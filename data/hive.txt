TITLE: Loading Files into Hive Tables
DESCRIPTION: Syntax for loading data files into Hive tables with support for local and HDFS paths, partitioning, and overwrite options.

LANGUAGE: sql
CODE:
LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]

LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] [INPUTFORMAT 'inputformat' SERDE 'serde']

----------------------------------------

TITLE: Basic SELECT Statement Syntax in HiveQL
DESCRIPTION: Demonstrates the complete syntax structure for SELECT statements in Hive, including WITH clause, table references, and various filtering and grouping options.

LANGUAGE: HiveQL
CODE:
[WITH CommonTableExpression (, CommonTableExpression)*]
SELECT [ALL | DISTINCT] select_expr, select_expr, ...
  FROM table_reference
  [WHERE where_condition]
  [GROUP BY col_list]
  [ORDER BY col_list]
  [CLUSTER BY col_list
    | [DISTRIBUTE BY col_list] [SORT BY col_list]
  ]
 [LIMIT [offset,] rows]

----------------------------------------

TITLE: Join Operation in Hive
DESCRIPTION: Example of joining two tables (user and page_view) on a common column.

LANGUAGE: HiveQL
CODE:
INSERT OVERWRITE TABLE pv_users
SELECT pv.*, u.gender, u.age
FROM user u JOIN page_view pv ON (pv.userid = u.id)
WHERE pv.date = '2008-03-03';

----------------------------------------

TITLE: WHERE Clause Example in HiveQL
DESCRIPTION: Shows how to filter query results using the WHERE clause with multiple conditions.

LANGUAGE: HiveQL
CODE:
SELECT * FROM sales WHERE amount > 10 AND region = "US"

----------------------------------------

TITLE: Installing Hive from tarball
DESCRIPTION: Steps to download, unpack and set up environment variables for a Hive release tarball

LANGUAGE: Shell
CODE:
$ tar -xzvf hive-x.y.z.tar.gz

$ cd hive-x.y.z
$ export HIVE_HOME={{pwd}}

$ export PATH=$HIVE_HOME/bin:$PATH

----------------------------------------

TITLE: Creating Hive tables
DESCRIPTION: SQL commands to create simple and partitioned Hive tables

LANGUAGE: SQL
CODE:
CREATE TABLE pokes (foo INT, bar STRING);

CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING);

----------------------------------------

TITLE: Insert Values Example
DESCRIPTION: Examples of directly inserting values into Hive tables using INSERT INTO VALUES syntax

LANGUAGE: sql
CODE:
CREATE TABLE students (name VARCHAR(64), age INT, gpa DECIMAL(3, 2))
  CLUSTERED BY (age) INTO 2 BUCKETS STORED AS ORC;

INSERT INTO TABLE students
  VALUES ('fred flintstone', 35, 1.28), ('barney rubble', 32, 2.32);

----------------------------------------

TITLE: Connecting to Hive via JDBC in Java
DESCRIPTION: This Java code demonstrates how to connect to Hive using JDBC, create a table, load data, and execute queries. It includes error handling and covers basic JDBC operations.

LANGUAGE: Java
CODE:
import java.sql.SQLException;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.Statement;
import java.sql.DriverManager;

public class HiveJdbcClient {
  private static String driverName = "org.apache.hadoop.hive.jdbc.HiveDriver";

  public static void main(String[] args) throws SQLException {
    try {
      Class.forName(driverName);
    } catch (ClassNotFoundException e) {
      // TODO Auto-generated catch block
      e.printStackTrace();
      System.exit(1);
    }
    Connection con = DriverManager.getConnection("jdbc:hive://localhost:10000/default", "", "");
    Statement stmt = con.createStatement();
    String tableName = "testHiveDriverTable";
    stmt.executeQuery("drop table " + tableName);
    ResultSet res = stmt.executeQuery("create table " + tableName + " (key int, value string)");
    // show tables
    String sql = "show tables '" + tableName + "'";
    System.out.println("Running: " + sql);
    res = stmt.executeQuery(sql);
    if (res.next()) {
      System.out.println(res.getString(1));
    }
    // describe table
    sql = "describe " + tableName;
    System.out.println("Running: " + sql);
    res = stmt.executeQuery(sql);
    while (res.next()) {
      System.out.println(res.getString(1) + "\t" + res.getString(2));
    }

    // load data into table
    // NOTE: filepath has to be local to the hive server
    // NOTE: /tmp/a.txt is a ctrl-A separated file with two fields per line
    String filepath = "/tmp/a.txt";
    sql = "load data local inpath '" + filepath + "' into table " + tableName;
    System.out.println("Running: " + sql);
    res = stmt.executeQuery(sql);

    // select * query
    sql = "select * from " + tableName;
    System.out.println("Running: " + sql);
    res = stmt.executeQuery(sql);
    while (res.next()) {
      System.out.println(String.valueOf(res.getInt(1)) + "\t" + res.getString(2));
    }

    // regular hive query
    sql = "select count(1) from " + tableName;
    System.out.println("Running: " + sql);
    res = stmt.executeQuery(sql);
    while (res.next()) {
      System.out.println(res.getString(1));
    }
  }
}

----------------------------------------

TITLE: Hive Join Syntax
DESCRIPTION: Defines the syntax for various types of joins in Hive, including INNER, LEFT/RIGHT/FULL OUTER, LEFT SEMI, and CROSS joins.

LANGUAGE: SQL
CODE:
join_table:
    table_reference [INNER] JOIN table_factor [join_condition]
  | table_reference {LEFT|RIGHT|FULL} [OUTER] JOIN table_reference join_condition
  | table_reference LEFT SEMI JOIN table_reference join_condition
  | table_reference CROSS JOIN table_reference [join_condition] (as of Hive 0.10)

table_reference:
    table_factor
  | join_table

table_factor:
    tbl_name [alias]
  | table_subquery alias
  | ( table_references )

join_condition:
    ON expression

----------------------------------------

TITLE: Loading data into Hive tables
DESCRIPTION: Commands to load data from local and HDFS files into Hive tables

LANGUAGE: SQL
CODE:
LOAD DATA LOCAL INPATH './examples/files/kv1.txt' OVERWRITE INTO TABLE pokes;

LOAD DATA LOCAL INPATH './examples/files/kv2.txt' OVERWRITE INTO TABLE invites PARTITION (ds='2008-08-15');

LOAD DATA INPATH '/user/myname/kv2.txt' OVERWRITE INTO TABLE invites PARTITION (ds='2008-08-15');

----------------------------------------

TITLE: Creating a Partitioned Table in Hive
DESCRIPTION: Example of creating a partitioned table called 'page_view' with various column types and partitioned by date and country.

LANGUAGE: HiveQL
CODE:
CREATE TABLE page_view(viewTime INT, userid BIGINT,
                    page_url STRING, referrer_url STRING,
                    ip STRING COMMENT 'IP Address of the User')
COMMENT 'This is the page view table'
PARTITIONED BY(dt STRING, country STRING)
STORED AS SEQUENCEFILE;

----------------------------------------

TITLE: Example Hive queries
DESCRIPTION: Sample Hive queries demonstrating selects, filters, joins, and transformations

LANGUAGE: SQL
CODE:
SELECT a.foo FROM invites a WHERE a.ds='2008-08-15';

INSERT OVERWRITE DIRECTORY '/tmp/hdfs_out' SELECT a.* FROM invites a WHERE a.ds='2008-08-15';

FROM pokes t1 JOIN invites t2 ON (t1.bar = t2.bar) INSERT OVERWRITE TABLE events SELECT t1.bar, t1.foo, t2.foo;

FROM invites a INSERT OVERWRITE TABLE events SELECT TRANSFORM(a.foo, a.bar) AS (oof, rab) USING '/bin/cat' WHERE a.ds > '2008-08-09';

----------------------------------------

TITLE: Map-side Join with MAPJOIN Hint
DESCRIPTION: Shows how to use the MAPJOIN hint to perform a join as a map-only job when one table is small enough to fit in memory.

LANGUAGE: SQL
CODE:
SELECT /*+ MAPJOIN(b) */ a.key, a.value
FROM a JOIN b ON a.key = b.key

----------------------------------------

TITLE: Create Table
DESCRIPTION: Syntax for creating a new table in Hive with column definitions, storage format, partitioning, bucketing and other properties

LANGUAGE: sql
CODE:
CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name
  [(col_name data_type [column_constraint_specification] [COMMENT col_comment], ... [constraint_specification])]
  [COMMENT table_comment]
  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
  [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
  [SKEWED BY (col_name, col_name, ...)
     ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)
     [STORED AS DIRECTORIES]]
  [
   [ROW FORMAT row_format] 
   [STORED AS file_format]
     | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)]
  ]
  [LOCATION hdfs_path]
  [TBLPROPERTIES (property_name=property_value, ...)]   
  [AS select_statement];

----------------------------------------

TITLE: Hive Interactive Shell Commands
DESCRIPTION: Examples of various commands that can be used in the Hive interactive shell mode.

LANGUAGE: sql
CODE:
hive> set mapred.reduce.tasks=32;
hive> set;
hive> select a.* from tab1;
hive> !ls;
hive> dfs -ls;

----------------------------------------

TITLE: Simple Query in Hive
DESCRIPTION: Example of a simple SELECT query to filter active users.

LANGUAGE: HiveQL
CODE:
INSERT OVERWRITE TABLE user_active
SELECT user.*
FROM user
WHERE user.active = 1;

----------------------------------------

TITLE: Creating and Using a Materialized View in Hive
DESCRIPTION: Shows the creation of a materialized view 'mv1' and a query that can be rewritten using this view.

LANGUAGE: SQL
CODE:
CREATE MATERIALIZED VIEW mv1
AS
SELECT empid, deptname, hire_date
FROM emps JOIN depts
ON (emps.deptno = depts.deptno)
WHERE hire_date >= '2016-01-01';

-- Query that can be rewritten using mv1
SELECT empid, deptname
FROM emps
JOIN depts
ON (emps.deptno = depts.deptno)
WHERE hire_date >= '2018-01-01'
AND hire_date <= '2018-03-31';

----------------------------------------

TITLE: Basic Window Function Examples
DESCRIPTION: Collection of basic window function examples including PARTITION BY and ORDER BY clauses.

LANGUAGE: sql
CODE:
SELECT a, COUNT(b) OVER (PARTITION BY c)
FROM T;

LANGUAGE: sql
CODE:
SELECT a, COUNT(b) OVER (PARTITION BY c, d)
FROM T;

LANGUAGE: sql
CODE:
SELECT a, SUM(b) OVER (PARTITION BY c ORDER BY d)
FROM T;

----------------------------------------

TITLE: Creating table for Apache weblog data
DESCRIPTION: Example of creating a Hive table to store Apache weblog data using RegexSerDe

LANGUAGE: SQL
CODE:
CREATE TABLE apachelog (
  host STRING,
  identity STRING,
  user STRING,
  time STRING,
  request STRING,
  status STRING,
  size STRING,
  referer STRING,
  agent STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
  "input.regex" = "([^]*) ([^]*) ([^]*) (-|\\[^\\]*\\]) ([^ \"]*|\"[^\"]*\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\".*\") ([^ \"]*|\".*\")?)?"
)
STORED AS TEXTFILE;

----------------------------------------

TITLE: Creating a Materialized View in Hive
DESCRIPTION: Demonstrates the syntax for creating a materialized view in Hive, including options for partitioning, clustering, and storage format.

LANGUAGE: SQL
CODE:
CREATE MATERIALIZED VIEW [IF NOT EXISTS] [db_name.]materialized_view_name
[DISABLE REWRITE]
[COMMENT materialized_view_comment]
[PARTITIONED ON (col_name, ...)]
[CLUSTERED ON (col_name, ...) | DISTRIBUTED ON (col_name, ...) SORTED ON (col_name, ...)]
[
[ROW FORMAT row_format]
[STORED AS file_format]
| STORED BY 'storage.handler.class.name'
 [WITH SERDEPROPERTIES (...)]
]
[LOCATION hdfs_path]
[TBLPROPERTIES (property_name=property_value, ...)]
AS
<query>;

----------------------------------------

TITLE: Connecting to HiveServer2 using Beeline CLI
DESCRIPTION: Example of using Beeline command-line interface to connect to HiveServer2 and execute a query.

LANGUAGE: shell
CODE:
% bin/beeline 
beeline> !connect jdbc:hive2://localhost:10000 scott tiger
Connecting to jdbc:hive2://localhost:10000
Connected to: Hive (version 0.10.0)
0: jdbc:hive2://localhost:10000> show tables;
+-------------------+
|     tab_name      |
+-------------------+
| primitives        |
| src               |
| src1              |
+-------------------+
9 rows selected (1.079 seconds)

----------------------------------------

TITLE: Complex Join Expressions
DESCRIPTION: Demonstrates various complex join expressions, including multi-table joins and different join types.

LANGUAGE: SQL
CODE:
SELECT a.* FROM a JOIN b ON (a.id = b.id)

SELECT a.* FROM a JOIN b ON (a.id = b.id AND a.department = b.department)

SELECT a.* FROM a LEFT OUTER JOIN b ON (a.id <> b.id)

SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2)

----------------------------------------

TITLE: Window Specification Syntax in Hive
DESCRIPTION: Defines the supported window specification formats for ROWS and RANGE clauses in Hive's OVER statements.

LANGUAGE: sql
CODE:
(ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)
(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)
(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING

----------------------------------------

TITLE: Advanced Window Function Examples
DESCRIPTION: Examples of complex window functions with multiple partitions and window specifications.

LANGUAGE: sql
CODE:
SELECT a, SUM(b) OVER (PARTITION BY c ORDER BY d ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)
FROM T;

LANGUAGE: sql
CODE:
SELECT a, AVG(b) OVER (PARTITION BY c ORDER BY d ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)
FROM T;

----------------------------------------

TITLE: Aggregation Query in Hive
DESCRIPTION: Example of performing count and grouping operations.

LANGUAGE: HiveQL
CODE:
INSERT OVERWRITE TABLE pv_gender_sum
SELECT pv_users.gender, count (DISTINCT pv_users.userid)
FROM pv_users
GROUP BY pv_users.gender;

----------------------------------------

TITLE: Creating a Clustered and Bucketed Table in Hive
DESCRIPTION: Example of creating a table with custom row format, clustered by userid, sorted by viewTime, and bucketed into 32 buckets.

LANGUAGE: HiveQL
CODE:
CREATE TABLE page_view(viewTime INT, userid BIGINT,
                    page_url STRING, referrer_url STRING,
                    ip STRING COMMENT 'IP Address of the User')
COMMENT 'This is the page view table'
PARTITIONED BY(dt STRING, country STRING)
CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS
ROW FORMAT DELIMITED
        FIELDS TERMINATED BY '1'
        COLLECTION ITEMS TERMINATED BY '2'
        MAP KEYS TERMINATED BY '3'
STORED AS SEQUENCEFILE;

----------------------------------------

TITLE: Setting Hive Environment Variables in Bash
DESCRIPTION: Commands to set HIVE_HOME environment variable and add Hive's bin directory to the system PATH. This allows Hive commands to be executed from any location.

LANGUAGE: bash
CODE:
$ cd hive-x.y.z
$ export HIVE_HOME={{pwd}}
$ export PATH=$HIVE_HOME/bin:$PATH

----------------------------------------

TITLE: Example Query with Multiple Group By and Join Operations
DESCRIPTION: SQL query demonstrating correlation optimization potential between JOIN and GROUP BY operations on the same key column.

LANGUAGE: sql
CODE:
SELECT tmp1.key, count(*)
FROM (SELECT key, avg(value) AS avg
      FROM t1
      GROUP BY /*AGG1*/ key) tmp1
JOIN /*JOIN1*/ t1 ON (tmp1.key = t2.key)
WHERE t1.value > tmp1.avg
GROUP BY /*AGG2*/ tmp1.key;

----------------------------------------

TITLE: Using posexplode Function in Hive SQL
DESCRIPTION: Examples of using the posexplode function to transform an array into multiple rows with position information. Shows different syntax options including using it in a SELECT statement and with LATERAL VIEW.

LANGUAGE: SQL
CODE:
select posexplode(array('A','B','C'));
select posexplode(array('A','B','C')) as (pos,val);
select tf.* from (select 0) t lateral view posexplode(array('A','B','C')) tf;
select tf.* from (select 0) t lateral view posexplode(array('A','B','C')) tf as pos,val;

----------------------------------------

TITLE: Exploding an Array in Hive SQL
DESCRIPTION: Examples of using the explode function to transform an array into multiple rows. Shows different syntax options including using it in a SELECT statement and with LATERAL VIEW.

LANGUAGE: SQL
CODE:
select explode(array('A','B','C'));
select explode(array('A','B','C')) as col;
select tf.* from (select 0) t lateral view explode(array('A','B','C')) tf;
select tf.* from (select 0) t lateral view explode(array('A','B','C')) tf as col;

----------------------------------------

TITLE: Connecting to Hive via Thrift in Python
DESCRIPTION: This Python script demonstrates how to connect to a Hive server using Thrift, execute queries, and fetch results. It includes error handling and shows basic Thrift client operations.

LANGUAGE: Python
CODE:
#!/usr/bin/env python

import sys

from hive import ThriftHive
from hive.ttypes import HiveServerException
from thrift import Thrift
from thrift.transport import TSocket
from thrift.transport import TTransport
from thrift.protocol import TBinaryProtocol

try:
    transport = TSocket.TSocket('localhost', 10000)
    transport = TTransport.TBufferedTransport(transport)
    protocol = TBinaryProtocol.TBinaryProtocol(transport)

    client = ThriftHive.Client(protocol)
    transport.open()

    client.execute("CREATE TABLE r(a STRING, b INT, c DOUBLE)")
    client.execute("LOAD TABLE LOCAL INPATH '/path' INTO TABLE r")
    client.execute("SELECT * FROM r")
    while (1):
      row = client.fetchOne()
      if (row == None):
        break
      print row
    client.execute("SELECT * FROM r")
    print client.fetchAll()

    transport.close()

except Thrift.TException, tx:
    print '%s' % (tx.message)

----------------------------------------

TITLE: Create Database
DESCRIPTION: Syntax for creating a new database/schema in Hive with optional comment, location and properties

LANGUAGE: sql
CODE:
CREATE [REMOTE] (DATABASE|SCHEMA) [IF NOT EXISTS] database_name
  [COMMENT database_comment]
  [LOCATION hdfs_path]
  [MANAGEDLOCATION hdfs_path]
  [WITH DBPROPERTIES (property_name=property_value, ...)];

----------------------------------------

TITLE: Altering Table Compaction Settings
DESCRIPTION: Examples showing how to modify compaction settings for an existing table

LANGUAGE: SQL
CODE:
ALTER TABLE table_name COMPACT 'minor' 
   WITH OVERWRITE TBLPROPERTIES ("compactor.mapreduce.map.memory.mb"="3072");  -- specify compaction map job properties
ALTER TABLE table_name COMPACT 'major'
   WITH OVERWRITE TBLPROPERTIES ("tblprops.orc.compress.size"="8192");         -- change any other Hive table properties

----------------------------------------

TITLE: Initializing Hive Metastore and Running HiveServer2
DESCRIPTION: Commands to initialize the Hive metastore schema and start HiveServer2.

LANGUAGE: bash
CODE:
$HIVE_HOME/bin/schematool -dbType derby -initSchema --verbose

$HIVE_HOME/bin/hiveserver2

$HIVE_HOME/bin/beeline -u 'jdbc:hive2://localhost:10000/' -n yourusername

----------------------------------------

TITLE: Configuring Spark as Hive Execution Engine
DESCRIPTION: Configuration setting to use Spark as the execution engine in Hive.

LANGUAGE: properties
CODE:
set hive.execution.engine=spark;

----------------------------------------

TITLE: Creating Tables for Materialized View Example in Hive
DESCRIPTION: Demonstrates the creation of 'emps' and 'depts' tables used in the first materialized view example.

LANGUAGE: SQL
CODE:
CREATE TABLE emps (
empid INT,
deptno INT,
name VARCHAR(256),
salary FLOAT,
hire_date TIMESTAMP)
STORED AS ORC
TBLPROPERTIES ('transactional'='true');

CREATE TABLE depts (
deptno INT,
deptname VARCHAR(256),
locationid INT)
STORED AS ORC
TBLPROPERTIES ('transactional'='true');

----------------------------------------

TITLE: Creating Table with CSV SerDe in Apache Hive
DESCRIPTION: This snippet demonstrates how to create a table in Hive using the CSV SerDe. It specifies the SerDe class and custom properties for separator, quote, and escape characters.

LANGUAGE: SQL
CODE:
CREATE TABLE my_table(a string, b string, ...)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
   "separatorChar" = "\t",
   "quoteChar"     = "'",
   "escapeChar"    = "\\"
)   
STORED AS TEXTFILE;

----------------------------------------

TITLE: CTE Basic Syntax Definition in Hive
DESCRIPTION: Defines the basic syntax structure for Common Table Expressions in Hive, showing the grammar rules for withClause and cteClause.

LANGUAGE: sql
CODE:
withClause: cteClause (, cteClause)*
cteClause: cte_name AS (select statment)

----------------------------------------

TITLE: Basic UNION Syntax in Apache Hive SQL
DESCRIPTION: Demonstrates the basic syntax for UNION operations in Hive SQL, combining results from multiple SELECT statements. Supports UNION ALL for keeping duplicates and UNION DISTINCT (default) for removing duplicates.

LANGUAGE: SQL
CODE:
select_statement UNION [ALL | DISTINCT] select_statement UNION [ALL | DISTINCT] select_statement ...

----------------------------------------

TITLE: Starting HiveServer2 from command line
DESCRIPTION: Commands to start HiveServer2 from the command line using the hiveserver2 script or hive service command.

LANGUAGE: Bash
CODE:
$HIVE_HOME/bin/hiveserver2

LANGUAGE: Bash
CODE:
$HIVE_HOME/bin/hive --service hiveserver2

----------------------------------------

TITLE: Accessing Fields Using StructObjectInspector in Hive
DESCRIPTION: Shows how operators use the StructObjectInspector to access individual fields in a deserialized record using the getStructFieldData() method.

LANGUAGE: Java
CODE:
StructObjectInspector.getStructFieldData()

----------------------------------------

TITLE: Creating Iceberg Table in Hive
DESCRIPTION: Demonstrates how to create an Iceberg table using the CREATE TABLE command with the STORED BY ICEBERG clause.

LANGUAGE: SQL
CODE:
CREATE TABLE TBL_ICE (ID INT) STORED BY ICEBERG;

----------------------------------------

TITLE: Example Configuration Settings
DESCRIPTION: Example values showing how configuration properties are set in hive-site.xml

LANGUAGE: xml
CODE:
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>hive.exec.scratchdir</name>
    <value>/tmp/hive-${user.name}</value>
  </property>

  <property>
    <name>hive.metastore.warehouse.dir</name> 
    <value>/user/hive/warehouse</value>
  </property>
</configuration>

----------------------------------------

TITLE: Creating ORC Table in HiveQL
DESCRIPTION: Example of creating a table stored in ORC format without compression. Demonstrates table creation syntax and ORC-specific table properties.

LANGUAGE: hql
CODE:
create table Addresses (
  name string,
  street string,
  city string,
  state string,
  zip int
) stored as orc tblproperties ("orc.compress"="NONE");

----------------------------------------

TITLE: Mixed Static and Dynamic Partition Insert
DESCRIPTION: Example demonstrating insert with both static and dynamic partition columns. Static partition has fixed value while dynamic partition is determined at runtime.

LANGUAGE: hql
CODE:
INSERT OVERWRITE TABLE T PARTITION (ds='2010-03-03', hr) 
SELECT key, value, /*ds,*/ hr FROM srcpart WHERE ds is not null and hr>10;

----------------------------------------

TITLE: Creating Bucketed Table in Hive
DESCRIPTION: Creates a bucketed table with user information that is partitioned by date and clustered into 256 buckets based on user_id. The table includes fields for user_id, firstname, and lastname.

LANGUAGE: hql
CODE:
CREATE TABLE user_info_bucketed(user_id BIGINT, firstname STRING, lastname STRING)
COMMENT 'A bucketed copy of user_info'
PARTITIONED BY(ds STRING)
CLUSTERED BY(user_id) INTO 256 BUCKETS;

----------------------------------------

TITLE: Create Table with ACID Properties Example
DESCRIPTION: Example of creating a transactional Hive table with compaction configuration properties

LANGUAGE: SQL
CODE:
CREATE TABLE table_name (
  id                int,
  name              string
)
CLUSTERED BY (id) INTO 2 BUCKETS STORED AS ORC
TBLPROPERTIES ("transactional"="true",
  "compactor.mapreduce.map.memory.mb"="2048",     -- specify compaction map job properties
  "compactorthreshold.hive.compactor.delta.num.threshold"="4",  -- trigger minor compaction if there are more than 4 delta directories
  "compactorthreshold.hive.compactor.delta.pct.threshold"="0.5" -- trigger major compaction if the ratio of size of delta files to
                                                                   -- size of base files is greater than 50%
);

----------------------------------------

TITLE: Complex Avro Schema Definition
DESCRIPTION: Example of a complex Avro schema definition that includes various data types including records, arrays, maps, unions, and fixed types.

LANGUAGE: json
CODE:
{
  "namespace": "com.linkedin.haivvreo",
  "name": "test_serializer",
  "type": "record",
  "fields": [
    { "name":"string1", "type":"string" },
    { "name":"int1", "type":"int" },
    { "name":"tinyint1", "type":"int" },
    { "name":"smallint1", "type":"int" },
    { "name":"bigint1", "type":"long" },
    { "name":"boolean1", "type":"boolean" },
    { "name":"float1", "type":"float" },
    { "name":"double1", "type":"double" },
    { "name":"list1", "type":{"type":"array", "items":"string"} },
    { "name":"map1", "type":{"type":"map", "values":"int"} },
    { "name":"struct1", "type":{"type":"record", "name":"struct1_name", "fields": [
          { "name":"sInt", "type":"int" }, { "name":"sBoolean", "type":"boolean" }, { "name":"sString", "type":"string" } ] } },
    { "name":"union1", "type":["float", "boolean", "string"] },
    { "name":"enum1", "type":{"type":"enum", "name":"enum1_values", "symbols":["BLUE","RED", "GREEN"]} },
    { "name":"nullableint", "type":["int", "null"] },
    { "name":"bytes1", "type":"bytes" },
    { "name":"fixed1", "type":{"type":"fixed", "name":"threebytes", "size":3} }
  ] }

----------------------------------------

TITLE: Querying Iceberg Table in Hive
DESCRIPTION: Demonstrates a simple SELECT query on an Iceberg table, showing that standard HiveQL syntax applies.

LANGUAGE: SQL
CODE:
SELECT * FROM TBL_ICE WHERE ID > 5;

----------------------------------------

TITLE: Using Python client driver for HiveServer2
DESCRIPTION: Example of using the pyhs2 Python client driver to connect to HiveServer2 and execute queries.

LANGUAGE: Python
CODE:
import pyhs2

with pyhs2.connect(host='localhost',
                   port=10000,
                   authMechanism="PLAIN",
                   user='root',
                   password='test',
                   database='default') as conn:
    with conn.cursor() as cur:
    	#Show databases
    	print cur.getDatabases()

    	#Execute query
        cur.execute("select * from table")
 
        #Return column info from query
        print cur.getSchema()

        #Fetch table results
        for i in cur.fetch():
            print i

----------------------------------------

TITLE: Example Hive SQL Queries
DESCRIPTION: Sample SQL queries demonstrating table creation, partition management, and data manipulation in Hive.

LANGUAGE: sql
CODE:
show tables;
create table hive_example(a string, b int) partitioned by(c int);
alter table hive_example add partition(c=1);
insert into hive_example partition(c=1) values('a', 1), ('a', 2),('b',3);
select count(distinct a) from hive_example;
select sum(b) from hive_example;

----------------------------------------

TITLE: Running Hive Queries from Command Line
DESCRIPTION: Examples of running Hive queries directly from the command line using various options.

LANGUAGE: bash
CODE:
$HIVE_HOME/bin/hive -e 'select a.col from tab1 a'

$HIVE_HOME/bin/hive -e 'select a.col from tab1 a' --hiveconf hive.exec.scratchdir=/home/my/hive_scratch  --hiveconf mapred.reduce.tasks=32

$HIVE_HOME/bin/hive -S -e 'select a.col from tab1 a' > a.txt

$HIVE_HOME/bin/hive -f /home/my/hive-script.sql

$HIVE_HOME/bin/hive -f hdfs://<namenode>:<port>/hive-script.sql
$HIVE_HOME/bin/hive -f s3://mys3bucket/s3-script.sql 

$HIVE_HOME/bin/hive -i /home/my/hive-init.sql

----------------------------------------

TITLE: Implementing Hive Streaming in Non-secure Mode
DESCRIPTION: Example showing how to stream records into Hive using multiple transactions and batches in a non-secure environment. Demonstrates creating connections, writing delimited data, and managing transactions across multiple threads.

LANGUAGE: Java
CODE:
///// Stream five records in two transactions /////

// Assumed HIVE table Schema:
create table alerts ( id int , msg string )
     partitioned by (continent string, country string)
     clustered by (id) into 5 buckets
     stored as orc tblproperties("transactional"="true"); // currently ORC is required for streaming


//-------   MAIN THREAD  ------- //
String dbName = "testing";
String tblName = "alerts";
ArrayList<String> partitionVals = new ArrayList<String>(2);
partitionVals.add("Asia");
partitionVals.add("India");
String serdeClass = "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe";

HiveEndPoint hiveEP = new HiveEndPoint("thrift://x.y.com:9083", dbName, tblName, partitionVals);

.. spin up threads ..

//-------   Thread 1  -------//
StreamingConnection connection = hiveEP.newConnection(true);
DelimitedInputWriter writer =
                     new DelimitedInputWriter(fieldNames,",", hiveEP);
TransactionBatch txnBatch = connection.fetchTransactionBatch(10, writer);

///// Batch 1 - First TXN
txnBatch.beginNextTransaction();
txnBatch.write("1,Hello streaming".getBytes());
txnBatch.write("2,Welcome to streaming".getBytes());
txnBatch.commit();

----------------------------------------

TITLE: Dynamic Partition Insert Example
DESCRIPTION: Example showing how to dynamically insert data into partitioned tables based on column values

LANGUAGE: sql
CODE:
FROM page_view_stg pvs
INSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country)
       SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip, pvs.cnt

----------------------------------------

TITLE: Creating an HBase-backed Hive Table
DESCRIPTION: Example of creating a Hive table that is backed by an HBase table, specifying the storage handler and column mapping.

LANGUAGE: HiveQL
CODE:
CREATE TABLE hbase_table_1(key int, value string) 
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf1:val")
TBLPROPERTIES ("hbase.table.name" = "xyz", "hbase.mapred.output.outputtable" = "xyz");

----------------------------------------

TITLE: Subquery with UNION ALL in FROM Clause for Apache Hive SQL
DESCRIPTION: Demonstrates a more complex subquery in the FROM clause that uses UNION ALL to combine results from two different calculations on separate tables.

LANGUAGE: SQL
CODE:
SELECT t3.col
FROM (
  SELECT a+b AS col
  FROM t1
  UNION ALL
  SELECT c+d AS col
  FROM t2
) t3

----------------------------------------

TITLE: Creating Avro Table Using STORED AS AVRO in Hive 0.14+
DESCRIPTION: Modern way to create Avro tables in Hive 0.14 and later versions using the simplified STORED AS AVRO syntax with automatic schema generation.

LANGUAGE: sql
CODE:
CREATE TABLE kst (
    string1 string,
    string2 string,
    int1 int,
    boolean1 boolean,
    long1 bigint,
    float1 float,
    double1 double,
    inner_record1 struct<int_in_inner_record1:int,string_in_inner_record1:string>,
    enum1 string,
    array1 array<string>,
    map1 map<string,string>,
    union1 uniontype<float,boolean,string>,
    fixed1 binary,
    null1 void,
    unionnullint int,
    bytes1 binary)
  PARTITIONED BY (ds string)
  STORED AS AVRO;

----------------------------------------

TITLE: TPC-DS Query Example in Hive SQL
DESCRIPTION: Sample TPC-DS query demonstrating revenue ratio calculation with joins across web_sales, item, and date_dim tables. The query includes filtering, grouping, windowing functions and ordering operations.

LANGUAGE: sql
CODE:
select
  i_item_desc
  ,i_category
  ,i_class
  ,i_current_price
  ,i_item_id
  ,itemrevenue
  ,itemrevenue*100/sum(itemrevenue) over
    (partition by i_class) as revenueratio
from
  (select
     i_item_desc
     ,i_category
     ,i_class
     ,i_current_price
     ,i_item_id
     ,sum(ws_ext_sales_price) as itemrevenue
   from
     web_sales
     join item on (web_sales.ws_item_sk = item.i_item_sk)
     join date_dim on (web_sales.ws_sold_date_sk = date_dim.d_date_sk)
   where
     i_category in ('1', '2', '3')
     and year(d_date) = 2001 and month(d_date) = 10
   group by
     i_item_id
     ,i_item_desc
     ,i_category
     ,i_class
     ,i_current_price) tmp
order by
  i_category
  ,i_class
  ,i_item_id
  ,i_item_desc
  ,revenueratio;

----------------------------------------

TITLE: Materialized View Maintenance Commands
DESCRIPTION: Commands for rebuilding and maintaining materialized views to keep them in sync with source data.

LANGUAGE: sql
CODE:
ALTER MATERIALIZED VIEW [db_name.]materialized_view_name REBUILD;

----------------------------------------

TITLE: Unpacking Hive Tarball in Bash
DESCRIPTION: Command to extract the contents of a Hive tarball. Replace 'x.y.z' with the specific version number of Hive being installed.

LANGUAGE: bash
CODE:
$ tar -xzvf hive-x.y.z.tar.gz

----------------------------------------

TITLE: ORDER BY Syntax in Hive QL
DESCRIPTION: Defines the syntax for ORDER BY clause in Hive, including column ordering and null handling options. Supports both ascending and descending orders with optional NULLS FIRST/LAST specifications.

LANGUAGE: sql
CODE:
colOrder: ( ASC | DESC )
colNullOrder: (NULLS FIRST | NULLS LAST)
orderBy: ORDER BY colName colOrder? colNullOrder? (',' colName colOrder? colNullOrder?)*
query: SELECT expression (',' expression)* FROM src orderBy

----------------------------------------

TITLE: Explode with Lateral View Example
DESCRIPTION: Demonstrates how to use lateral view with explode() function to transform array elements into separate rows.

LANGUAGE: sql
CODE:
SELECT pageid, adid
FROM pageAds LATERAL VIEW explode(adid_list) adTable AS adid;

----------------------------------------

TITLE: Defining Transform/Map-Reduce Syntax in Hive
DESCRIPTION: Formal syntax definition for TRANSFORM, MAP, and REDUCE operations in Hive, including row format specifications and query structure.

LANGUAGE: hql
CODE:
clusterBy: CLUSTER BY colName (',' colName)*
distributeBy: DISTRIBUTE BY colName (',' colName)*
sortBy: SORT BY colName (ASC | DESC)? (',' colName (ASC | DESC)?)*

rowFormat
  : ROW FORMAT
    (DELIMITED [FIELDS TERMINATED BY char] 
               [COLLECTION ITEMS TERMINATED BY char]
               [MAP KEYS TERMINATED BY char]
               [ESCAPED BY char]
               [LINES SEPARATED BY char]
     | 
     SERDE serde_name [WITH SERDEPROPERTIES 
                            property_name=property_value, 
                            property_name=property_value, ...])

----------------------------------------

TITLE: Creating PostgreSQL Connector in Hive
DESCRIPTION: Shows how to create a PostgreSQL connector with basic authentication using username and password. This connector enables connection to a PostgreSQL database running on localhost.

LANGUAGE: sql
CODE:
CREATE CONNECTOR pg_local TYPE 'postgres' URL 'jdbc:<postgresql://localhost:5432>' WITH DCPROPERTIES ("hive.sql.dbcp.username"="postgres", "hive.sql.dbcp.password"="postgres");

----------------------------------------

TITLE: Hive JDBC Connection Examples
DESCRIPTION: Examples of JDBC connection URLs for both remote and local Hive connections

LANGUAGE: Java
CODE:
jdbc:hive://hostname:port/databasename
jdbc:hive://

----------------------------------------

TITLE: Creating Parquet Table in Hive 0.13 and Later
DESCRIPTION: HiveQL syntax for creating a Parquet table in Hive version 0.13 and later. This example shows the simplified syntax for creating a partitioned table with various data types using the native Parquet support.

LANGUAGE: HiveQL
CODE:
CREATE TABLE parquet_test (
 id int,
 str string,
 mp MAP<STRING,STRING>,
 lst ARRAY<STRING>,
 strct STRUCT<A:STRING,B:STRING>) 
PARTITIONED BY (part string)
STORED AS PARQUET;

----------------------------------------

TITLE: Managing Druid Kafka Ingestion
DESCRIPTION: ALTER TABLE commands to start, stop and reset Druid's Kafka ingestion process.

LANGUAGE: SQL
CODE:
ALTER TABLE druid_kafka_test SET TBLPROPERTIES('druid.kafka.ingestion' = 'START');
ALTER TABLE druid_kafka_test SET TBLPROPERTIES('druid.kafka.ingestion' = 'STOP');
ALTER TABLE druid_kafka_test SET TBLPROPERTIES('druid.kafka.ingestion' = 'RESET');

----------------------------------------

TITLE: Basic Scheduled Query Example
DESCRIPTION: Demonstrates creating a basic scheduled query that inserts data into a table every 10 minutes, including monitoring and execution commands.

LANGUAGE: sql
CODE:
create table t (a integer);

create scheduled query sc1 cron '0 */10 * * * ? *' as insert into t values (1);
alter scheduled query sc1 enabled;

select * from information_schema.scheduled_queries s where schedule_name='sc1';

alter scheduled query sc1 execute;

select * from information_schema.scheduled_executions s where schedule_name='sc1' order by scheduled_execution_id desc limit 1;

----------------------------------------

TITLE: Implementing a Custom UDTF in Java for Apache Hive
DESCRIPTION: This code snippet demonstrates how to create a custom User-Defined Table Function (UDTF) by extending the GenericUDTF abstract class. The example UDTF, GenericUDTFCount2, counts the number of rows processed and outputs the count twice.

LANGUAGE: Java
CODE:
package org.apache.hadoop.hive.contrib.udtf.example;

import java.util.ArrayList;

import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;

/**
 * GenericUDTFCount2 outputs the number of rows seen, twice. It's output twice
 * to test outputting of rows on close with lateral view.
 *
 */
public class GenericUDTFCount2 extends GenericUDTF {

  Integer count = Integer.valueOf(0);
  Object forwardObj[] = new Object[1];

  @Override
  public void close() throws HiveException {
    forwardObj[0] = count;
    forward(forwardObj);
    forward(forwardObj);
  }

  @Override
  public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
    ArrayList<String> fieldNames = new ArrayList<String>();
    ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();
    fieldNames.add("col1");
    fieldOIs.add(PrimitiveObjectInspectorFactory.javaIntObjectInspector);
    return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames,
        fieldOIs);
  }

  @Override
  public void process(Object[] args) throws HiveException {
    count = Integer.valueOf(count.intValue() + 1);
  }

}

----------------------------------------

TITLE: Hive Streaming API Usage Example
DESCRIPTION: Complete example demonstrating how to use the Hive Streaming API with both static and dynamic partitioning, including transaction management and record writing.

LANGUAGE: java
CODE:
///// Stream five records in two transactions /////
 
// Assumed HIVE table Schema:
create table alerts ( id int , msg string )
     partitioned by (continent string, country string)
     clustered by (id) into 5 buckets
     stored as orc tblproperties("transactional"="true"); // currently ORC is required for streaming
 
 
//-------   MAIN THREAD  ------- //
String dbName = "testing";
String tblName = "alerts";

.. spin up thread 1 ..
// static partition values
ArrayList<String> partitionVals = new ArrayList<String>(2);
partitionVals.add("Asia");
partitionVals.add("India");

// create delimited record writer whose schema exactly matches table schema
StrictDelimitedInputWriter writer = StrictDelimitedInputWriter.newBuilder()
                                      .withFieldDelimiter(',')
                                      .build();
// create and open streaming connection (default.src table has to exist already)
StreamingConnection connection = HiveStreamingConnection.newBuilder()
                                    .withDatabase(dbName)
                                    .withTable(tblName)
                                    .withStaticPartitionValues(partitionVals)
                                    .withAgentInfo("example-agent-1")
                                    .withRecordWriter(writer)
                                    .withHiveConf(hiveConf)
                                    .connect();
// begin a transaction, write records and commit 1st transaction
connection.beginTransaction();
connection.write("1,val1".getBytes());
connection.write("2,val2".getBytes());
connection.commitTransaction();
// begin another transaction, write more records and commit 2nd transaction
connection.beginTransaction();
connection.write("3,val3".getBytes());
connection.write("4,val4".getBytes());
connection.commitTransaction();
// close the streaming connection
connection.close();

.. spin up thread 2 ..
// dynamic partitioning
// create delimited record writer whose schema exactly matches table schema
StrictDelimitedInputWriter writer = StrictDelimitedInputWriter.newBuilder()
                                      .withFieldDelimiter(',')
                                      .build();
// create and open streaming connection (default.src table has to exist already)
StreamingConnection connection = HiveStreamingConnection.newBuilder()
                                    .withDatabase(dbName)
                                    .withTable(tblName)
                                    .withAgentInfo("example-agent-1")
                                    .withRecordWriter(writer)
                                    .withHiveConf(hiveConf)
                                    .connect();
// begin a transaction, write records and commit 1st transaction
connection.beginTransaction();
// dynamic partition mode where last 2 columns are partition values
connection.write("11,val11,Asia,China".getBytes());
connection.write("12,val12,Asia,India".getBytes());
connection.commitTransaction();
// begin another transaction, write more records and commit 2nd transaction
connection.beginTransaction();
connection.write("13,val13,Europe,Germany".getBytes());
connection.write("14,val14,Asia,India".getBytes());
connection.commitTransaction();
// close the streaming connection
connection.close();

----------------------------------------

TITLE: Implementing Hive Streaming in Secure Mode
DESCRIPTION: Example demonstrating how to stream data to Hive using Kerberos authentication in a secure environment. Shows connection setup with security credentials and transaction management.

LANGUAGE: Java
CODE:
import org.apache.hadoop.security.UserGroupInformation;

HiveEndPoint hiveEP2 = ... ;
UserGroupInformation ugi = .. authenticateWithKerberos(principal,keytab);
StreamingConnection secureConn = hiveEP2.newConnection(true, null, ugi);

DelimitedInputWriter writer3 = new DelimitedInputWriter(fieldNames, ",", hiveEP2);

TransactionBatch txnBatch3= secureConn.fetchTransactionBatch(10, writer3);

///// Batch 1 - First TXN – over secure connection
txnBatch3.beginNextTransaction();
txnBatch3.write("28,Eric Baldeschwieler".getBytes());
txnBatch3.write("29,Ari Zilka".getBytes());
txnBatch3.commit();

txnBatch3.close();
secureConn.close();

----------------------------------------

TITLE: CTE in Select Statements Examples
DESCRIPTION: Demonstrates various ways to use CTEs in SELECT statements, including basic queries, FROM style queries, chaining CTEs, and UNION operations.

LANGUAGE: sql
CODE:
with q1 as ( select key from src where key = '5')
select *
from q1;

-- from style
with q1 as (select * from src where key= '5')
from q1
select *;
 
-- chaining CTEs
with q1 as ( select key from q2 where key = '5'),
q2 as ( select key from src where key = '5')
select * from (select key from q1) a;
 
-- union example
with q1 as (select * from src where key= '5'),
q2 as (select * from src s2 where key = '4')
select * from q1 union all select * from q2;

----------------------------------------

TITLE: Map-side Aggregation Configuration
DESCRIPTION: Example showing how to enable map-side aggregation for improved query performance with GROUP BY operations.

LANGUAGE: sql
CODE:
set hive.map.aggr=true;
SELECT COUNT(*) FROM table2;

----------------------------------------

TITLE: Basic Exchange Partition Syntax in Hive
DESCRIPTION: Demonstrates the basic syntax for the EXCHANGE PARTITION command in Hive

LANGUAGE: sql
CODE:
ALTER TABLE <dest_table> EXCHANGE PARTITION (<[partial] partition spec>) WITH TABLE <src_table>

----------------------------------------

TITLE: Data Ingestion Schedule
DESCRIPTION: Shows how to implement scheduled incremental data ingestion from a source table to a target table while tracking offsets.

LANGUAGE: sql
CODE:
create scheduled query ingest every 10 minutes defined as
from (select id==offset as first,* from s
join t_offset on id>=offset) s1
insert into t select id,cnt where first = false
insert overwrite table t_offset select max(s1.id);

----------------------------------------

TITLE: Filtering Partitions in Apache Hive SQL
DESCRIPTION: Demonstrates how to construct a partition filter using multiple conditions with logical operators. The example shows filtering on 'country' and 'state' partition columns using AND/OR operators with nested expressions.

LANGUAGE: sql
CODE:
country = "USA" AND (state = "CA" OR state = "AZ")

----------------------------------------

TITLE: Dynamic Partition Insert with All DP Columns
DESCRIPTION: Example showing dynamic partition insert where all partition columns are determined at execution time. Only allowed in nonstrict mode.

LANGUAGE: hql
CODE:
INSERT OVERWRITE TABLE T PARTITION (ds, hr) 
SELECT key, value, ds, hr FROM srcpart WHERE ds is not null and hr>10;

----------------------------------------

TITLE: SORT BY Syntax in Hive QL
DESCRIPTION: Defines the syntax for SORT BY clause which performs per-reducer sorting. Supports multiple columns with optional sort direction specification.

LANGUAGE: sql
CODE:
colOrder: ( ASC | DESC )
sortBy: SORT BY colName colOrder? (',' colName colOrder?)*
query: SELECT expression (',' expression)* FROM src sortBy

----------------------------------------

TITLE: Schema-less Transform Example
DESCRIPTION: Example showing how to use TRANSFORM without explicit schema definition in Hive.

LANGUAGE: hql
CODE:
FROM (
  FROM pv_users
  MAP pv_users.userid, pv_users.date
  USING 'map_script'
  CLUSTER BY key) map_output
INSERT OVERWRITE TABLE pv_users_reduced
  REDUCE map_output.key, map_output.value
  USING 'reduce_script'
  AS date, count;

----------------------------------------

TITLE: Using stack Function in Hive SQL
DESCRIPTION: Examples of using the stack function to transform a set of columns into multiple rows. Shows different syntax options including using it in a SELECT statement and with LATERAL VIEW.

LANGUAGE: SQL
CODE:
select stack(2,'A',10,date '2015-01-01','B',20,date '2016-01-01');
select stack(2,'A',10,date '2015-01-01','B',20,date '2016-01-01') as (col0,col1,col2);
select tf.* from (select 0) t lateral view stack(2,'A',10,date '2015-01-01','B',20,date '2016-01-01') tf;
select tf.* from (select 0) t lateral view stack(2,'A',10,date '2015-01-01','B',20,date '2016-01-01') tf as col0,col1,col2;

----------------------------------------

TITLE: Basic Hive Docker Setup Commands
DESCRIPTION: Essential commands for pulling and running Hive 4.0.0 in a Docker container with basic configuration.

LANGUAGE: bash
CODE:
docker pull apache/hive:4.0.0

export HIVE_VERSION=4.0.0

docker run -d -p 10000:10000 -p 10002:10002 --env SERVICE_NAME=hiveserver2 --name hive4 apache/hive:${HIVE_VERSION}

docker exec -it hiveserver2 beeline -u 'jdbc:hive2://hiveserver2:10000/'

----------------------------------------

TITLE: Enabling Automatic Join Conversion in Hive SQL
DESCRIPTION: This SQL command enables the automatic conversion of common joins to map joins in Hive. When set to true, Hive will attempt to convert joins to map joins when possible for better performance.

LANGUAGE: sql
CODE:
set hive.auto.convert.join = true;

----------------------------------------

TITLE: Rewritten Query Using Druid-Stored Materialized View in Hive
DESCRIPTION: Demonstrates how Hive rewrites the query using the Druid-stored materialized view 'mv3'.

LANGUAGE: SQL
CODE:
SELECT floor(time to month),
SUM(c_added)
FROM mv3
GROUP BY floor(time to month);

----------------------------------------

TITLE: Displaying Hive CLI Usage Information
DESCRIPTION: Shows the usage information and available options for the Hive CLI command.

LANGUAGE: bash
CODE:
usage: hive
 -d,--define <key=value>          Variable substitution to apply to Hive
                                  commands. e.g. -d A=B or --define A=B
 -e <quoted-query-string>         SQL from command line
 -f <filename>                    SQL from files
 -H,--help                        Print help information
 -h <hostname>                    Connecting to Hive Server on remote host
    --hiveconf <property=value>   Use value for given property
    --hivevar <key=value>         Variable substitution to apply to hive
                                  commands. e.g. --hivevar A=B
 -i <filename>                    Initialization SQL file
 -p <port>                        Connecting to Hive Server on port number
 -S,--silent                      Silent mode in interactive shell
 -v,--verbose                     Verbose mode (echo executed SQL to the
                                  console)

----------------------------------------

TITLE: Creating a CredentialProvider for Hive metastore password
DESCRIPTION: Example of using the Hadoop credential command to create a JCEKS keystore file containing the Hive metastore password.

LANGUAGE: shell
CODE:
$ hadoop credential create javax.jdo.option.ConnectionPassword -provider jceks://file/usr/lib/hive/conf/hive.jceks
Enter password: 
Enter password again: 
javax.jdo.option.ConnectionPassword has been successfully created.
org.apache.hadoop.security.alias.JavaKeyStoreProvider has been updated.

----------------------------------------

TITLE: Creating an External Kudu Table in Hive
DESCRIPTION: Demonstrates how to create an external Hive table that references an existing Kudu table. It specifies the table structure, storage handler, and table properties including the Kudu table name and master addresses.

LANGUAGE: SQL
CODE:
CREATE EXTERNAL TABLE kudu_table (foo INT, bar STRING, baz DOUBLE)
STORED BY 'org.apache.hadoop.hive.kudu.KuduStorageHandler'
TBLPROPERTIES (
  "kudu.table_name"="default.kudu_table", 
  "kudu.master_addresses"="localhost:7051"
);

----------------------------------------

TITLE: Creating Avro Table with Schema URL in Hive
DESCRIPTION: Example of creating a Hive table that uses AvroSerDe with an external schema URL. This demonstrates basic table creation with partitioning and SerDe configuration.

LANGUAGE: sql
CODE:
CREATE TABLE kst
  PARTITIONED BY (ds string)
  ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
  STORED AS INPUTFORMAT
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
  OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
  TBLPROPERTIES (
    'avro.schema.url'='http://schema_provider/kst.avsc');

----------------------------------------

TITLE: Distinct User Count by Gender
DESCRIPTION: Shows how to count distinct users grouped by gender with results being written to a table.

LANGUAGE: sql
CODE:
INSERT OVERWRITE TABLE pv_gender_sum
SELECT pv_users.gender, count (DISTINCT pv_users.userid)
FROM pv_users
GROUP BY pv_users.gender;

----------------------------------------

TITLE: Hive Schema Tool Command Usage
DESCRIPTION: Usage instructions for the 'schematool' command, showing available options for schema manipulation.

LANGUAGE: bash
CODE:
$ schematool -help
usage: schemaTool
 -dbType <databaseType>             Metastore database type
 -driver <driver>                   Driver name for connection
 -dryRun                            List SQL scripts (no execute)
 -help                              Print this message
 -info                              Show config and schema details
 -initSchema                        Schema initialization
 -initSchemaTo <initTo>             Schema initialization to a version
 -metaDbType <metaDatabaseType>     Used only if upgrading the system catalog for hive
 -passWord <password>               Override config file password
 -upgradeSchema                     Schema upgrade
 -upgradeSchemaFrom <upgradeFrom>   Schema upgrade from a version
 -url <url>                         Connection url to the database
 -userName <user>                   Override config file user name
 -verbose                           Only print SQL statements
(Additional catalog related options added in Hive 3.0.0 (HIVE-19135] release are below.
 -createCatalog <catalog>       Create catalog with given name
 -catalogLocation <location>        Location of new catalog, required when adding a catalog
 -catalogDescription <description>  Description of new catalog
 -ifNotExists                       If passed then it is not an error to create an existing catalog
 -moveDatabase <database>                     Move a database between catalogs.  All tables under it would still be under it as part of new catalog. Argument is the database name. Requires --fromCatalog and --toCatalog parameters as well
 -moveTable  <table>                Move a table to a different database.  Argument is the table name. Requires --fromCatalog, --toCatalog, --fromDatabase, and --toDatabase 
 -toCatalog  <catalog>              Catalog a moving database or table is going to.  This is required if you are moving a database or table.
 -fromCatalog <catalog>             Catalog a moving database or table is coming from.  This is required if you are moving a database or table.
 -toDatabase  <database>            Database a moving table is going to.  This is required if you are moving a table.
 -fromDatabase <database>           Database a moving table is coming from.  This is required if you are moving a table.

----------------------------------------

TITLE: Creating and Altering Partitioned Views in Apache Hive
DESCRIPTION: Demonstrates the syntax for creating a partitioned view and adding or dropping partitions. The CREATE VIEW statement includes options for partitioning, while ALTER VIEW statements allow for partition management.

LANGUAGE: SQL
CODE:
CREATE VIEW [IF NOT EXISTS] view_name [(column_name [COMMENT column_comment], ...) ]
[COMMENT table_comment]
[PARTITIONED ON (col1, col2, ...)]
[TBLPROPERTIES ...]
AS SELECT ...

ALTER VIEW view_name ADD [IF NOT EXISTS] partition_spec partition_spec ...

ALTER VIEW view_name DROP [IF EXISTS] partition_spec, partition_spec, ...

partition_spec:
  : PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, ...)

----------------------------------------

TITLE: Hive GROUP BY Basic Syntax
DESCRIPTION: Defines the fundamental syntax structure for GROUP BY clauses in Hive queries, showing the grammar for groupByClause, groupByExpression, and groupByQuery.

LANGUAGE: sql
CODE:
groupByClause: GROUP BY groupByExpression (, groupByExpression)*

groupByExpression: expression

groupByQuery: SELECT expression (, expression)* FROM src groupByClause?

----------------------------------------

TITLE: Querying Lock Status in Hive SQL
DESCRIPTION: SQL commands for viewing current locks on tables and partitions. These commands help debug concurrency issues and monitor lock status.

LANGUAGE: sql
CODE:
SHOW LOCKS <TABLE_NAME>;
SHOW LOCKS <TABLE_NAME> EXTENDED;
SHOW LOCKS <TABLE_NAME> PARTITION (<PARTITION_DESC>);
SHOW LOCKS <TABLE_NAME> PARTITION (<PARTITION_DESC>) EXTENDED;

----------------------------------------

TITLE: Supported Partition Filter Operators in Hive
DESCRIPTION: Complete list of operators supported for partition column filtering in Apache Hive, including comparison operators, logical operators, and the LIKE operator for string pattern matching.

LANGUAGE: sql
CODE:
=
<
<=
>
>=
<>
AND
OR
LIKE

----------------------------------------

TITLE: Subquery Syntax in FROM Clause for Apache Hive SQL
DESCRIPTION: Demonstrates the syntax for using subqueries in the FROM clause of a Hive SQL query. It shows two variations, with and without the 'AS' keyword for naming the subquery.

LANGUAGE: SQL
CODE:
SELECT ... FROM (subquery) name ...
SELECT ... FROM (subquery) AS name ...   (Note: Only valid starting with Hive 0.13.0)

----------------------------------------

TITLE: Creating External Table with JDBC Storage Handler in Hive
DESCRIPTION: Demonstrates basic syntax for creating an external table using JdbcStorageHandler to connect to a MySQL database. Includes essential table properties for configuration.

LANGUAGE: HiveQL
CODE:
CREATE EXTERNAL TABLE student_jdbc
(
  name string,
  age int,
  gpa double
)
STORED BY 'org.apache.hive.storage.jdbc.JdbcStorageHandler'
TBLPROPERTIES (
    "hive.sql.database.type" = "MYSQL",
    "hive.sql.jdbc.driver" = "com.mysql.jdbc.Driver",
    "hive.sql.jdbc.url" = "jdbc:mysql://localhost/sample",
    "hive.sql.dbcp.username" = "hive",
    "hive.sql.dbcp.password" = "hive",
    "hive.sql.table" = "STUDENT",
    "hive.sql.dbcp.maxActive" = "1"
);

----------------------------------------

TITLE: Securing JDBC Password Using Hadoop Credential Store
DESCRIPTION: Shows how to securely store and use database passwords using Hadoop's credential store instead of plain text passwords in table properties.

LANGUAGE: bash
CODE:
hadoop credential create host1.password -provider jceks://hdfs/user/foo/test.jceks -v passwd1
hadoop credential create host2.password -provider jceks://hdfs/user/foo/test.jceks -v passwd2

----------------------------------------

TITLE: Creating Custom UDF in Java for Apache Hive
DESCRIPTION: Example implementation of a custom UDF that converts text to lowercase. The class extends UDF and implements an evaluate method that takes Text input and returns lowercase Text output.

LANGUAGE: java
CODE:
package com.example.hive.udf;

import org.apache.hadoop.hive.ql.exec.UDF;
import org.apache.hadoop.io.Text;

public final class Lower extends UDF {
  public Text evaluate(final Text s) {
    if (s == null) { return null; }
    return new Text(s.toString().toLowerCase());
  }
}

----------------------------------------

TITLE: Configuring LDAP Group DN Pattern in Hive XML
DESCRIPTION: Example of setting the hive.server2.authentication.ldap.groupDNPattern property in hive-site.xml to specify the pattern for group distinguished names in LDAP.

LANGUAGE: xml
CODE:
<property>
  <name>
    hive.server2.authentication.ldap.groupDNPattern
  </name>
  <value>CN=%s,OU=Groups,DC=apache,DC=org</value>
</property>

----------------------------------------

TITLE: Hive Complex Type Array Access
DESCRIPTION: Demonstrates array element access in Hive where A[0] retrieves the first element from array A.

LANGUAGE: hive
CODE:
A[0]  -- returns 'foo' from array ['foo', 'bar']

----------------------------------------

TITLE: Materialized View Management Operations
DESCRIPTION: Basic management operations for materialized views including dropping views, showing existing views, and viewing view details.

LANGUAGE: sql
CODE:
-- Drops a materialized view
DROP MATERIALIZED VIEW [db_name.]materialized_view_name;
-- Shows materialized views (with optional filters)
SHOW MATERIALIZED VIEWS [IN database_name] ['identifier_with_wildcards'];
-- Shows information about a specific materialized view
DESCRIBE [EXTENDED | FORMATTED] [db_name.]materialized_view_name;

----------------------------------------

TITLE: Advanced Metastore Configuration
DESCRIPTION: Commands for setting up a standalone metastore with PostgreSQL integration and volume mounting for data persistence.

LANGUAGE: bash
CODE:
docker run -d -p 9083:9083 --env SERVICE_NAME=metastore --env DB_DRIVER=postgres \
     --env SERVICE_OPTS="-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore_db -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=password" \
     --mount source=warehouse,target=/opt/hive/data/warehouse \
     --mount type=bind,source=`mvn help:evaluate -Dexpression=settings.localRepository -q -DforceStdout`/org/postgresql/postgresql/42.5.1/postgresql-42.5.1.jar,target=/opt/hive/lib/postgres.jar \
     --name metastore-standalone apache/hive:4.0.0

----------------------------------------

TITLE: Creating a Remote Table with SSH Tunneling in Hive SQL
DESCRIPTION: SQL statement for creating a remote table connection using SSH tunneling for secure access through bastion hosts.

LANGUAGE: SQL
CODE:
CREATE REMOTE TABLE tbl_name
VIA 'org.apache.hadoop.hive.metastore.SSHThriftHiveMetastoreClientFactory'
WITH TBLPROPERTIES (
  'hive.metastore.uris' = 'thrift://metastore.domain:9083'
  'ssh.tunnel.route' = 'bastionuser@bastion-host.domain -> user@cluster-node.domain'
  'ssh.tunnel.private.keys' = '/home/user/.ssh/bastionuser-key-pair.pem,/home/user/.ssh/user-key-pair.pem'
  'ssh.tunnel.known.hosts' = '/home/user/.ssh/known_hosts'
);

----------------------------------------

TITLE: Applying Subclauses to Individual SELECTs in UNION
DESCRIPTION: Demonstrates how to apply ORDER BY, LIMIT, and other subclauses to individual SELECT statements within a UNION operation by enclosing them in parentheses.

LANGUAGE: SQL
CODE:
SELECT key FROM (SELECT key FROM src ORDER BY key LIMIT 10)subq1
UNION
SELECT key FROM (SELECT key FROM src1 ORDER BY key LIMIT 10)subq2

----------------------------------------

TITLE: Creating ACID Table with Custom Compaction Settings
DESCRIPTION: Example showing how to create a transactional Hive table with custom compaction thresholds and MapReduce settings

LANGUAGE: SQL
CODE:
CREATE TABLE table_name (
  id                int,
  name              string
)
CLUSTERED BY (id) INTO 2 BUCKETS STORED AS ORC
TBLPROPERTIES ("transactional"="true",
  "compactor.mapreduce.map.memory.mb"="2048",     -- specify compaction map job properties
  "compactorthreshold.hive.compactor.delta.num.threshold"="4",  -- trigger minor compaction if there are more than 4 delta directories
  "compactorthreshold.hive.compactor.delta.pct.threshold"="0.5" -- trigger major compaction if the ratio of size of delta files to
                                                                   -- size of base files is greater than 50%
);

----------------------------------------

TITLE: Multiple Lateral Views Example
DESCRIPTION: Demonstrates how to use multiple lateral view clauses in a single query to process nested arrays.

LANGUAGE: sql
CODE:
SELECT * FROM exampleTable
LATERAL VIEW explode(col1) myTable1 AS myCol1
LATERAL VIEW explode(myCol1) myTable2 AS myCol2;

----------------------------------------

TITLE: Creating Iceberg V2 Table in Hive
DESCRIPTION: Demonstrates how to create an Iceberg table with format version 2 using table properties.

LANGUAGE: SQL
CODE:
CREATE TABLE V2_TABLE (ID INT) STORED BY ICEBERG TBLPROPERTIES ('format-version'='2');

----------------------------------------

TITLE: Configuring LDAP User DN Pattern in Hive XML
DESCRIPTION: Example of setting the hive.server2.authentication.ldap.userDNPattern property in hive-site.xml to specify the pattern for user distinguished names in LDAP.

LANGUAGE: xml
CODE:
<property>
  <name>
    hive.server2.authentication.ldap.userDNPattern
  </name>
  <value>
    CN=%s,CN=Users,DC=apache,DC=org
  </value>
</property>

----------------------------------------

TITLE: Comparing TIMESTAMP Types in SQL Query Results
DESCRIPTION: This SQL snippet demonstrates how different TIMESTAMP types behave when inserting and querying a timestamp value '1969-07-20 16:17:39' in Washington D.C. and then querying it from Paris. It shows the different representations based on timestamp semantics.

LANGUAGE: sql
CODE:
| SQL type | Semantics | Result | Explanation |
| --- | --- | --- | --- |
| TIMESTAMP [WITHOUT TIME ZONE] | LocalDateTime | 1969-07-20 16:17:39 | Displayed like the original timestamp literal. |
| TIMESTAMP WITH LOCAL TIME ZONE | Instant | 1969-07-20 21:17:39 | Differs from the original timestamp literal, but refers to the same time instant. |
| TIMESTAMP WITH TIME ZONE | OffsetDateTime | 1969-07-20 16:17:39 (UTC -04:00) | Displayed like the original literal but showing the time zone offset as well. |

----------------------------------------

TITLE: Java API - Streaming Mutation
DESCRIPTION: Operation-based API for inserting, updating, and deleting records in transactional tables using Hive's ACID feature. Handles large-volume mutations atomically in single long-lived transactions.



----------------------------------------

TITLE: SORT BY Example with Multiple Columns
DESCRIPTION: Demonstrates sorting by multiple columns with different sort directions within each reducer.

LANGUAGE: sql
CODE:
SELECT key, value FROM src SORT BY key ASC, value DESC

----------------------------------------

TITLE: Bucket Sampling Query Example in Hive SQL
DESCRIPTION: Example showing how to sample the 3rd bucket out of 32 buckets using random distribution.

LANGUAGE: sql
CODE:
SELECT *
FROM source TABLESAMPLE(BUCKET 3 OUT OF 32 ON rand()) s;

----------------------------------------

TITLE: Querying with GROUPING__ID Function in Hive SQL
DESCRIPTION: Demonstrates using the GROUPING__ID function with GROUP BY and ROLLUP to differentiate between NULL values in data and NULL values from aggregation.

LANGUAGE: SQL
CODE:
SELECT key, value, GROUPING__ID, count(*)
FROM T1
GROUP BY key, value WITH ROLLUP;

----------------------------------------

TITLE: Configuring Hive Authorization in XML
DESCRIPTION: Essential XML configuration settings in hive-site.xml for enabling Hive authorization and setting default table owner grants.

LANGUAGE: xml
CODE:
<property>
  <name>hive.security.authorization.enabled</name>
  <value>true</value>
  <description>enable or disable the hive client authorization</description>
</property>

<property>
  <name>hive.security.authorization.createtable.owner.grants</name>
  <value>ALL</value>
  <description>the privileges automatically granted to the owner whenever a table gets created. 
   An example like "select,drop" will grant select and drop privilege to the owner of the table</description>
</property>

----------------------------------------

TITLE: Creating Druid Table with Schema Definition
DESCRIPTION: Creates a Hive table with explicit schema definition that will be stored in Druid.

LANGUAGE: SQL
CODE:
CREATE EXTERNAL TABLE druid_table_1
(`__time` TIMESTAMP, `dimension1` STRING, `dimension2` STRING, `metric1` INT, `metric2` FLOAT)
STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler';

----------------------------------------

TITLE: Showing Grants in Hive SQL
DESCRIPTION: SQL command to show privileges granted to a user or role on a specific table or all objects.

LANGUAGE: SQL
CODE:
SHOW GRANT [principal_specification] ON (ALL | [TABLE] table_or_view_name);

----------------------------------------

TITLE: Analyzing Contextual N-grams in Apache Hive
DESCRIPTION: This query shows how to use context_ngrams() to find the top 100 words following the phrase "i love" in Twitter data. It demonstrates the use of a context array with null placeholders.

LANGUAGE: SQL
CODE:
SELECT context_ngrams(sentences(lower(tweet)), array("i","love",null), 100, [, 1000]) FROM twitter;

----------------------------------------

TITLE: Partition-Based Query Examples in HiveQL
DESCRIPTION: Demonstrates how to query partitioned tables using date ranges in WHERE and JOIN conditions.

LANGUAGE: HiveQL
CODE:
SELECT page_views.*
FROM page_views
WHERE page_views.date >= '2008-03-01' AND page_views.date <= '2008-03-31'

LANGUAGE: HiveQL
CODE:
SELECT page_views.*
FROM page_views JOIN dim_users
  ON (page_views.user_id = dim_users.id AND page_views.date >= '2008-03-01' AND page_views.date <= '2008-03-31')

----------------------------------------

TITLE: Remote Database Operations in Hive
DESCRIPTION: Demonstrates various database operations including creating a remote database with keystore authentication and basic database usage commands.

LANGUAGE: sql
CODE:
CREATE REMOTE DATABASE pg_hive_testing USING pg_local WITH DBPROPERTIES ("connector.remoteDbName"="hive_hms_testing");

// USING keystore instead of cleartext passwords in DCPROPERTIES
CREATE CONNECTOR pg_local_ks TYPE 'postgres' URL 'jdbc:<postgresql://localhost:5432/hive_hms_testing>' WITH DCPROPERTIES("hive.sql.dbcp.username"="postgres","hive.sql.dbcp.password.keystore"="jceks://app/local/hive/secrets.jceks" "hive.sql.dbcp.password.key"="postgres.credential");
CREATE REMOTE DATABASE pg_ks_local USING pg_local_ks("connector.remoteDbName"="hive_hms_testing");

USE pg_hive_testing;
SHOW TABLES;
DESCRIBE [formatted] <tablename>;
SELECT <col1> from <tablename> where <filter1> and <filter2>;

----------------------------------------

TITLE: Using Variables in Hive Queries and Commands
DESCRIPTION: A comprehensive example showing various ways to set, retrieve, and use variables in Hive, including system properties and nested variables.

LANGUAGE: sql
CODE:
set zzz=5;
set zzz;

set system:xxx=5;
set system:xxx;

set system:yyy=${system:xxx};
set system:yyy;

set go=${hiveconf:zzz};
set go;

set hive.variable.substitute=false;
set raw=${hiveconf:zzz};
set raw;

set hive.variable.substitute=true;

EXPLAIN SELECT * FROM src where key=${hiveconf:zzz};
SELECT * FROM src where key=${hiveconf:zzz};

set a=1;
set b=a;
set c=${hiveconf:${hiveconf:b}};
set c;

set jar=../lib/derby.jar;

add file ${hiveconf:jar};
list file;
delete file ${hiveconf:jar};
list file;

----------------------------------------

TITLE: Analyzing Column Statistics in HiveQL
DESCRIPTION: HiveQL command to compute statistics on one or more columns in a Hive table or partition.

LANGUAGE: SQL
CODE:
analyze table t [partition p] compute statistics for [columns c,...];

----------------------------------------

TITLE: Creating External Accumulo-Backed Table
DESCRIPTION: SQL statement to create an external Hive table that references an existing Accumulo table, decoupling the lifecycle of the Hive and Accumulo tables.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE countries(key string, name string, country string, country_id int)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES ("accumulo.columns.mapping" = ":rowID,info:name,info:country,info:country_id");

----------------------------------------

TITLE: Demonstrating UNION ALL Query in Apache Hive SQL
DESCRIPTION: This SQL snippet shows a query structure that can benefit from the Union optimization. It includes a UNION ALL operation between two subqueries, wrapped in an outer select statement.

LANGUAGE: SQL
CODE:
select * from   
 (subq1  
 UNION ALL  
 sub2) u;

----------------------------------------

TITLE: Example Hive SQL Query with Map Join Hint
DESCRIPTION: This SQL query demonstrates how to use a map join hint in Hive. The hint specifies that table 'a' should be treated as the small table in a map join operation.

LANGUAGE: sql
CODE:
select /*+mapjoin(a)*/ * from src1 x join src2 y on x.key=y.key;

----------------------------------------

TITLE: Concurrency Note for Hive Streaming API
DESCRIPTION: Important note about thread-safety considerations when using the Hive Streaming API. The APIs are not thread-safe and must be called from the same thread.

LANGUAGE: java
CODE:
* CONCURRENCY NOTE: The streaming connection APIs and record writer APIs are not thread-safe. Streaming connection creation,  
* begin/commit/abort transactions, write and close has to be called in the same thread. If close() or  
* abortTransaction() has to be triggered from a separate thread it has to be co-ordinated via external variables or  
* synchronization mechanism

----------------------------------------

TITLE: HAVING Clause Examples in HiveQL
DESCRIPTION: Shows how to use HAVING clause for filtering grouped results, with both direct and subquery approaches.

LANGUAGE: HiveQL
CODE:
SELECT col1 FROM t1 GROUP BY col1 HAVING SUM(col2) > 10

LANGUAGE: HiveQL
CODE:
SELECT col1 FROM (SELECT col1, SUM(col2) AS col2sum FROM t1 GROUP BY col1) t2 WHERE t2.col2sum > 10

----------------------------------------

TITLE: Grant/Revoke SQL Syntax for Hive Authorization
DESCRIPTION: SQL syntax for granting and revoking privileges in Hive, including object types, privilege levels, and options.

LANGUAGE: SQL
CODE:
GRANT
    priv_type [(column_list)]
      [, priv_type [(column_list)]] ...
    ON [object_type] priv_level
    TO user [, user] ...
WITH ADMIN OPTION

object_type:
    TABLE

priv_level:
    *
  | *.*
  | db_name.*
  | db_name.tbl_name
  | tbl_name

REVOKE
    priv_type [(column_list)]
      [, priv_type [(column_list)]] ...
    ON [object_type] priv_level
    FROM user [, user] ...

REVOKE ALL PRIVILEGES, GRANT OPTION
    FROM user [, user] ...

DENY  
	priv_type [(column_list)]
      [, priv_type [(column_list)]] ...
    ON [object_type] priv_level
    FROM user [, user] ...

----------------------------------------

TITLE: Using Column Aliases for Schema Matching in UNION
DESCRIPTION: Demonstrates the use of column aliases to ensure schema matching between different SELECT statements in a UNION operation, which is required to avoid semantic errors.

LANGUAGE: SQL
CODE:
INSERT OVERWRITE TABLE target_table
  SELECT name, id, category FROM source_table_1
  UNION ALL
  SELECT name, id, "Category159" as category FROM source_table_2

----------------------------------------

TITLE: UNION within FROM Clause in Apache Hive SQL
DESCRIPTION: Shows how to use UNION within a FROM clause for additional processing on the combined result set. This example combines video and comment publishing events from different tables.

LANGUAGE: SQL
CODE:
SELECT u.id, actions.date
FROM (
    SELECT av.uid AS uid 
    FROM action_video av 
    WHERE av.date = '2008-06-03' 
    UNION ALL 
    SELECT ac.uid AS uid 
    FROM action_comment ac 
    WHERE ac.date = '2008-06-03' 
 ) actions JOIN users u ON (u.id = actions.uid)

----------------------------------------

TITLE: Supported EXISTS Subquery Example
DESCRIPTION: Example of a valid EXISTS subquery that will be supported in the implementation.

LANGUAGE: sql
CODE:
SELECT EXISTS(SELECT p_size FROM part)
FROM part

----------------------------------------

TITLE: Inserting Data into Kudu Table via Hive
DESCRIPTION: Shows examples of inserting data into a Kudu table using Hive INSERT statements. These operations actually perform Kudu UPSERT operations to avoid primary key constraint issues.

LANGUAGE: SQL
CODE:
INSERT INTO kudu_table SELECT * FROM other_table;

INSERT INTO TABLE kudu_table
VALUES (1, 'test 1', 1.1), (2, 'test 2', 2.2);

----------------------------------------

TITLE: Initializing Hive Schema
DESCRIPTION: Example of initializing the Hive schema for a new setup using the Derby database.

LANGUAGE: bash
CODE:
$ schematool -dbType derby -initSchema
Metastore connection URL:        jdbc:derby:;databaseName=metastore_db;create=true
Metastore Connection Driver :    org.apache.derby.jdbc.EmbeddedDriver
Metastore connection User:       APP
Starting metastore schema initialization to 0.13.0
Initialization script hive-schema-0.13.0.derby.sql
Initialization script completed
schemaTool completed

----------------------------------------

TITLE: Materialized View with HLL Sketches
DESCRIPTION: Creates a materialized view to store precomputed HLL sketches for optimized distinct count queries

LANGUAGE: sql
CODE:
create materialized view mv_1 as
  select category, ds_hll_sketch(id) from sketch_input group by category;

set hive.optimize.bi.enabled=true;
select category,count(distinct id) from sketch_input group by category;
select count(distinct id) from sketch_input;

----------------------------------------

TITLE: Aggregation with Lateral View
DESCRIPTION: Shows how to combine lateral view with aggregation to count occurrences of array elements.

LANGUAGE: sql
CODE:
SELECT adid, count(1)
FROM pageAds LATERAL VIEW explode(adid_list) adTable AS adid
GROUP BY adid;

----------------------------------------

TITLE: Overwriting Data in Iceberg Table in Hive
DESCRIPTION: Demonstrates how to overwrite data in an Iceberg table using the INSERT OVERWRITE statement.

LANGUAGE: SQL
CODE:
INSERT OVERWRITE TBL_ICE SELECT * FROM TABLE1;

----------------------------------------

TITLE: Loading Data with HCatLoader in Pig
DESCRIPTION: Example of using HCatLoader to read data from an HCatalog-managed table in a Pig script. This demonstrates the basic usage pattern for HCatLoader.

LANGUAGE: Pig
CODE:
A = LOAD 'tablename' USING org.apache.hive.hcatalog.pig.HCatLoader();

----------------------------------------

TITLE: Creating a Denormalized Materialized View in Hive
DESCRIPTION: Demonstrates the creation of a materialized view 'mv2' that denormalizes the database contents.

LANGUAGE: SQL
CODE:
CREATE MATERIALIZED VIEW mv2
AS
SELECT <dims>,
lo_revenue,
lo_extendedprice * lo_discount AS d_price,
lo_revenue - lo_supplycost
FROM customer, dates, lineorder, part, supplier
WHERE lo_orderdate = d_datekey
AND lo_partkey = p_partkey
AND lo_suppkey = s_suppkey
AND lo_custkey = c_custkey;

----------------------------------------

TITLE: Testing ODBC Connection with ISQL
DESCRIPTION: Command to test the ODBC driver connection using the ISQL command-line tool.

LANGUAGE: bash
CODE:
$ isql -v Hive

----------------------------------------

TITLE: GenericUDTF Abstract Class Definition in Java for Apache Hive
DESCRIPTION: This code snippet shows the definition of the GenericUDTF abstract class, which serves as the base for implementing custom User-Defined Table Functions in Apache Hive. It includes abstract methods for initialization, processing, and closing, as well as utility methods for collecting output.

LANGUAGE: Java
CODE:
package org.apache.hadoop.hive.ql.udf.generic;

import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;

/**
 * A Generic User-defined Table Generating Function (UDTF)
 * 
 * Generates a variable number of output rows for a single input row. Useful for
 * explode(array)...
 */

public abstract class GenericUDTF {
  Collector collector = null;

  /**
 * Initialize this GenericUDTF. This will be called only once per instance.
 * 
 * @param args
 *          An array of ObjectInspectors for the arguments
 * @return A StructObjectInspector for output. The output struct represents a
 *         row of the table where the fields of the stuct are the columns. The
 *         field names are unimportant as they will be overridden by user
 *         supplied column aliases.
   */
  public abstract StructObjectInspector initialize(ObjectInspector[] argOIs)
      throws UDFArgumentException;

  /**
 * Give a set of arguments for the UDTF to process.
 * 
 * @param o
 *          object array of arguments
   */
  public abstract void process(Object[] args) throws HiveException;

  /**
 * Called to notify the UDTF that there are no more rows to process.
 * Clean up code or additional forward() calls can be made here.
   */
  public abstract void close() throws HiveException;

  /**
 * Associates a collector with this UDTF. Can't be specified in the
 * constructor as the UDTF may be initialized before the collector has been
 * constructed.
 * 
 * @param collector
   */
  public final void setCollector(Collector collector) {
    this.collector = collector;
  }

  /**
 * Passes an output row to the collector.
 * 
 * @param o
 * @throws HiveException
   */
  protected final void forward(Object o) throws HiveException {
    collector.collect(o);
  }

}

----------------------------------------

TITLE: Querying N-grams from Twitter Data using Apache Hive
DESCRIPTION: This SQL query demonstrates how to use the context_ngrams() function in Hive to find the top 100 bigrams from a Twitter dataset. It combines lowercase conversion, sentence splitting, and n-gram analysis.

LANGUAGE: SQL
CODE:
SELECT context_ngrams(sentences(lower(tweet)), 2, 100 [, 1000]) FROM twitter;

----------------------------------------

TITLE: HCatOutputFormat API Implementation in Java
DESCRIPTION: Core API methods for HCatOutputFormat including setOutput, setSchema, and getTableSchema for writing data to HCatalog-managed tables.

LANGUAGE: Java
CODE:
  /**
   * Set the information about the output to write for the job. This queries the metadata
   * server to find the StorageHandler to use for the table. It throws an error if the
   * partition is already published.
   * @param job the job object
   * @param outputJobInfo the table output information for the job
   * @throws IOException the exception in communicating with the metadata server
   */
  @SuppressWarnings("unchecked")
  public static void setOutput(Job job, OutputJobInfo outputJobInfo) throws IOException;

  /**
   * Set the schema for the data being written out to the partition. The
   * table schema is used by default for the partition if this is not called.
   * @param job the job object
   * @param schema the schema for the data
   * @throws IOException
   */
  public static void setSchema(final Job job, final HCatSchema schema) throws IOException;

  /**
   * Get the table schema for the table specified in the HCatOutputFormat.setOutput call
   * on the specified job context.
   * @param context the context
   * @return the table schema
   * @throws IOException if HCatOutputFormat.setOutput has not been called
   *                     for the passed context
   */
  public static HCatSchema getTableSchema(JobContext context) throws IOException;

----------------------------------------

TITLE: Creating a Table with Decimal Columns in Hive
DESCRIPTION: Example of creating a Hive table with decimal columns, demonstrating the syntax for specifying precision and scale.

LANGUAGE: SQL
CODE:
CREATE TABLE foo (
  a DECIMAL, -- Defaults to decimal(10,0)
  b DECIMAL(9, 7)
)

----------------------------------------

TITLE: Deleting Data from Iceberg Table in Hive
DESCRIPTION: Illustrates how to delete data from an Iceberg table using the DELETE statement with a WHERE clause.

LANGUAGE: SQL
CODE:
DELETE FROM TBL_ICE WHERE ID=5;

----------------------------------------

TITLE: Populating Bucketed Table in Hive
DESCRIPTION: Demonstrates how to properly populate a bucketed table using the hive.enforce.bucketing setting (required for Hive 0.x and 1.x) and INSERT OVERWRITE statement with partition specification.

LANGUAGE: hql
CODE:
set hive.enforce.bucketing = true;  -- (Note: Not needed in Hive 2.x onward)
FROM user_id
INSERT OVERWRITE TABLE user_info_bucketed
PARTITION (ds='2009-02-25')
SELECT userid, firstname, lastname WHERE ds='2009-02-25';

----------------------------------------

TITLE: Using inline Function in Hive SQL
DESCRIPTION: Examples of using the inline function to transform an array of structs into multiple rows. Shows different syntax options including using it in a SELECT statement and with LATERAL VIEW.

LANGUAGE: SQL
CODE:
select inline(array(struct('A',10,date '2015-01-01'),struct('B',20,date '2016-02-02')));
select inline(array(struct('A',10,date '2015-01-01'),struct('B',20,date '2016-02-02'))) as (col1,col2,col3);
select tf.* from (select 0) t lateral view inline(array(struct('A',10,date '2015-01-01'),struct('B',20,date '2016-02-02'))) tf;
select tf.* from (select 0) t lateral view inline(array(struct('A',10,date '2015-01-01'),struct('B',20,date '2016-02-02'))) tf as col1,col2,col3;

----------------------------------------

TITLE: Building Hive Client with Ant
DESCRIPTION: Commands to compile the C++ Hive client component using Ant build system. Requires Thrift home directory to be specified.

LANGUAGE: bash
CODE:
$ ant compile-cpp -Dthrift.home=<THRIFT_HOME>

----------------------------------------

TITLE: Create Table As Select with Dynamic Partitions
DESCRIPTION: Examples showing CTAS syntax with dynamic partitions, including cases for all dynamic columns and mixed static/dynamic columns.

LANGUAGE: hql
CODE:
CREATE TABLE T (key int, value string) PARTITIONED BY (ds string, hr int) AS 
SELECT key, value, ds, hr+1 hr1 FROM srcpart WHERE ds is not null and hr>10;

LANGUAGE: hql
CODE:
CREATE TABLE T (key int, value string) PARTITIONED BY (ds string, hr int) AS 
SELECT key, value, "2010-03-03", hr+1 hr1 FROM srcpart WHERE ds is not null and hr>10;

----------------------------------------

TITLE: Materialized View Rebuild Schedule
DESCRIPTION: Demonstrates scheduling automatic rebuilds of materialized views to maintain data freshness.

LANGUAGE: sql
CODE:
CREATE TABLE emps (
  empid INT,
  deptno INT,
  name VARCHAR(256),
  salary FLOAT,
  hire_date TIMESTAMP)
STORED AS ORC
TBLPROPERTIES ('transactional'='true');

CREATE MATERIALIZED VIEW mv1 AS
  SELECT empid, deptname, hire_date FROM emps
    JOIN depts ON (emps.deptno = depts.deptno)
    WHERE hire_date >= '2016-01-01 00:00:00';

create scheduled query mv_rebuild cron '0 */10 * * * ? *' defined as 
  alter materialized view mv1 rebuild;

----------------------------------------

TITLE: Setting Hive scratch directory in hive-site.xml
DESCRIPTION: Example of setting the scratch directory configuration variable in the hive-site.xml configuration file.

LANGUAGE: xml
CODE:
<property>
  <name>hive.exec.scratchdir</name>
  <value>/tmp/mydir</value>
  <description>Scratch space for Hive jobs</description>
</property>

----------------------------------------

TITLE: Configuring SSL with self-signed certificates for HiveServer2
DESCRIPTION: Steps to create and verify self-signed SSL certificates for use with HiveServer2, including generating keys, exporting certificates, and adding to truststore.

LANGUAGE: Bash
CODE:
keytool -genkey -alias example.com -keyalg RSA -keystore keystore.jks -keysize 2048
keytool -list -keystore keystore.jks
keytool -export -alias example.com -file example.com.crt -keystore keystore.jks
keytool -import -trustcacerts -alias example.com -file example.com.crt -keystore truststore.jks
keytool -list -keystore truststore.jks

----------------------------------------

TITLE: Creating an Index in Hive SQL
DESCRIPTION: SQL syntax for creating an index in Hive, including options for specifying the index handler, deferring the build, setting properties, and controlling the storage format.

LANGUAGE: SQL
CODE:
CREATE INDEX index_name
ON TABLE base_table_name (col_name, ...)
AS 'index.handler.class.name'
[WITH DEFERRED REBUILD]
[IDXPROPERTIES (property_name=property_value, ...)]
[IN TABLE index_table_name]
[PARTITIONED BY (col_name, ...)]
[
   [ ROW FORMAT ...] STORED AS ...
   | STORED BY ...
]
[LOCATION hdfs_path]
[TBLPROPERTIES (...)]
[COMMENT "index comment"]

----------------------------------------

TITLE: Updating Data in Iceberg Table in Hive
DESCRIPTION: Shows how to update data in an Iceberg table using the UPDATE statement with SET and WHERE clauses.

LANGUAGE: SQL
CODE:
UPDATE TBL_ICE WHERE ID=8 SET ID=2;

----------------------------------------

TITLE: Basic EXPLAIN Syntax in Hive
DESCRIPTION: Shows the basic syntax for the EXPLAIN command in Hive, including optional clauses.

LANGUAGE: SQL
CODE:
EXPLAIN [EXTENDED|CBO|AST|DEPENDENCY|AUTHORIZATION|LOCKS|VECTORIZATION|ANALYZE] query

----------------------------------------

TITLE: Creating External Hive Table Linked to Druid Datasource
DESCRIPTION: Creates an external Hive table that maps to an existing Druid datasource using the DruidStorageHandler.

LANGUAGE: SQL
CODE:
CREATE EXTERNAL TABLE druid_table_1
STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'
TBLPROPERTIES ("druid.datasource" = "wikiticker");

----------------------------------------

TITLE: Implementing Map Aggregation without Data Skew in Hive
DESCRIPTION: Implementation plan for GROUP BY WITH ROLLUP when map-side aggregation is enabled and data skew is not present. Uses hash-based group by operator for partial aggregations followed by merge-partial operations.

LANGUAGE: text
CODE:
Mapper:\n*Hash-based group by operator to perform partial aggregations\n*Reduce sink operator, performs some partial aggregations\n\nReducer:\n*MergePartial (list-based) group by operator to perform final aggregations

----------------------------------------

TITLE: Creating Basic Accumulo-Backed Hive Table
DESCRIPTION: SQL statement to create a Hive table backed by Accumulo with column mapping configurations that specify how Hive columns map to Accumulo key-value pairs.

LANGUAGE: sql
CODE:
CREATE TABLE accumulo_table(rowid STRING, name STRING, age INT, weight DOUBLE, height INT)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES('accumulo.columns.mapping' = ':rowid,person:name,person:age,person:weight,person:height');

----------------------------------------

TITLE: Creating and Populating Sample Table
DESCRIPTION: Creates a transactional ORC table with sample data for demonstrating sketch operations

LANGUAGE: sql
CODE:
create table sketch_input (id int, category char(1))
STORED AS ORC
TBLPROPERTIES ('transactional'='true');

insert into table sketch_input values
  (1,'a'),(1, 'a'), (2, 'a'), (3, 'a'), (4, 'a'), (5, 'a'), (6, 'a'), (7, 'a'), (8, 'a'), (9, 'a'), (10, 'a'),
  (6,'b'),(6, 'b'), (7, 'b'), (8, 'b'), (9, 'b'), (10, 'b'), (11, 'b'), (12, 'b'), (13, 'b'), (14, 'b'), (15, 'b');

----------------------------------------

TITLE: Creating Text File Index in Hive
DESCRIPTION: Shows how to create an index stored as a delimited text file.

LANGUAGE: hql
CODE:
CREATE INDEX table06_index ON TABLE table06 (column7) AS 'COMPACT' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE;

----------------------------------------

TITLE: Creating and Analyzing Tables with Top K Statistics
DESCRIPTION: Example SQL commands for creating tables and analyzing Top K statistics in Hive, demonstrating both partitioned and non-partitioned table scenarios.

LANGUAGE: sql
CODE:
CREATE TABLE table1 (key STRING, value STRING) PARTITIONED BY (ds STRING);
INSERT OVERWRITE TABLE table1 PARTITION (ds='2012-09-07') SELECT * FROM table_src;

DESCRIBE FORMATTED table1 partition (ds='2012-09-07');

----------------------------------------

TITLE: Basic Partition Exchange Example in Hive
DESCRIPTION: Shows how to create two tables partitioned by 'ds' and exchange a partition between them. Creates tables T1 and T2, adds a partition to T1, then moves it to T2.

LANGUAGE: sql
CODE:
--Create two tables, partitioned by ds
CREATE TABLE T1(a string, b string) PARTITIONED BY (ds string);
CREATE TABLE T2(a string, b string) PARTITIONED BY (ds string);
ALTER TABLE T1 ADD PARTITION (ds='1');

--Move partition from T1 to T2
ALTER TABLE T2 EXCHANGE PARTITION (ds='1') WITH TABLE T1;

----------------------------------------

TITLE: Creating Table with Spatial Column in HQL
DESCRIPTION: Example of creating a table with a GEOMETRY type column for spatial data storage. Uses tab-delimited format for field separation.

LANGUAGE: HQL
CODE:
CREATE TABLE IF NOT EXISTS spatial_table (\n  tile_id STRING,\n  d_id STRING,\n  rec_id STRING,\n  outline GEOMETRY\n) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

----------------------------------------

TITLE: Implementing Map Aggregation with Data Skew in Hive
DESCRIPTION: Implementation plan for GROUP BY WITH ROLLUP when both map-side aggregation and data skew handling are enabled. Uses a two-phase reduction process with intermediate partial aggregations.

LANGUAGE: text
CODE:
Mapper 1:\n*Hash-based group by operator to perform partial aggregations\n*Reduce sink operator to spray by the group by and distinct keys\n\nReducer 1:\n*Partials (list-based) group by operator\n\nMapper 2:\n*Reduce sink operator, performs partial aggregations\n\nReducer 2:\n*Final (list-based) group by operator

----------------------------------------

TITLE: Configuring Hive-site.xml for Replication
DESCRIPTION: XML configuration for enabling metastore event persistence in hive-site.xml. Sets the event listener and time-to-live for database events.

LANGUAGE: xml
CODE:
  <property>
    <name>hive.metastore.event.listeners</name>
    <value>org.apache.hive.hcatalog.listener.DbNotificationListener</value>
  </property>
  <property>
    <name>hive.metastore.event.db.listener.timetolive</name>
    <value>86400s</value>
  </property>

----------------------------------------

TITLE: Explaining Left Outer Join with Where Predicate on Null Supplying Table
DESCRIPTION: This HiveQL query demonstrates a left outer join with a where predicate on the null supplying table. The explain plan shows that the where predicate is not pushed down, as expected for Case W2.

LANGUAGE: HiveQL
CODE:
explain
select s1.key, s2.key 
from src s1 left join src s2 
where s2.key > '2';

----------------------------------------

TITLE: Implementing Partition Pruning for MapJoin in Apache Hive
DESCRIPTION: Pseudo-code outlining the steps to implement partition pruning optimization for MapJoin operations in Apache Hive. This includes compile-time analysis of join conditions, runtime value set generation, and partition descriptor modification.

LANGUAGE: pseudo
CODE:
1. Walk through Task DAG looking for MapredTask. Perform #2 - #6 for each such MapRedTask.

2. Skip Task if it contains backup join plan (i.e if not MAPJOIN_ONLY_NOBACKUP or if backupTask is not null).

3. With in the task Look for pattern "TS.*MAPJOIN". Perform #4 - #6 for each MAPJOIN operator.

4. Flag a Map-Join Operator as candidate for Partition Pruning

   4.1 Collect small tables that might participate in Big Table pruning

   4.2 Collect join keys from big table which is not wrapped in functions

   4.3 Find join keys from #4.1 that is not wrapped in function using partition pruner candidate set

   4.4 Find BigTable Columns from partition-pruner-bigtable-candidate set that is partitioned

   4.5 Check if any of the partition pruner element could potentially mutate the value before hitting the join conditions

   4.6 Add partition-pruner-bigtable-candidate list and set-generation-key-map to the existing list of values in the PhysicalCtx

5. Modify corresponding LocalMRTask to introduce the new PartitionPrunerSink Operator

6. In the MapRedTask corresponding to MapJoin, add TableScan for the bigTable to the prelaunch operator list

7. At execution time call prelaunch on each task

----------------------------------------

TITLE: Creating Index in New Table in Hive
DESCRIPTION: Shows how to create an index and store it in a specific target table.

LANGUAGE: hql
CODE:
CREATE INDEX table04_index ON TABLE table04 (column5) AS 'COMPACT' WITH DEFERRED REBUILD IN TABLE table04_index_table;

----------------------------------------

TITLE: SQL Test Structure Guidelines
DESCRIPTION: Common SQL patterns and directives to use when creating Hive test cases. Includes recommendations for table management, query organization, and cleanup procedures.

LANGUAGE: sql
CODE:
DROP TABLE [table_name]; -- Start with cleanup\nSELECT ... FROM src ... ORDER BY ...; -- Use existing data sources\nDROP TABLE [table_name]; -- End with cleanup

----------------------------------------

TITLE: Creating a List Bucketing Table in Hive SQL
DESCRIPTION: SQL syntax for creating a table with list bucketing. Specifies skewed columns and values, with the option to store as directories.

LANGUAGE: SQL
CODE:
CREATE TABLE <T> (SCHEMA) SKEWED BY (keys) ON ('c1', 'c2') [STORED AS DIRECTORIES];

----------------------------------------

TITLE: Simplified Remote Table Creation in Hive SQL
DESCRIPTION: Simplified SQL statement for creating a remote table connection, using the current database and assuming matching table names.

LANGUAGE: SQL
CODE:
CREATE REMOTE TABLE tbl_name
WITH TBLPROPERTIES (
  'hive.metastore.uris' = 'thrift://remote-hms:9083'
);

----------------------------------------

TITLE: HBase Table Creation and Data Sort Configuration
DESCRIPTION: SQL commands to configure and execute the data sorting process for HBase bulk loading, including compression and partitioning settings.

LANGUAGE: sql
CODE:
set hive.execution.engine=mr;
set mapred.reduce.tasks=12;
set hive.mapred.partitioner=org.apache.hadoop.mapred.lib.TotalOrderPartitioner;
set total.order.partitioner.path=/tmp/hb_range_key_list;
set hfile.compression=gz;

create table hbsort(transaction_id string, user_name string, amount double, ...)
stored as
INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.hbase.HiveHFileOutputFormat'
TBLPROPERTIES ('hfile.family.path' = '/tmp/hbsort/cf');

insert overwrite table hbsort
select transaction_id, user_name, amount, ...
from transactions
cluster by transaction_id;

----------------------------------------

TITLE: Using Reflect UDF with Java Methods in Hive SQL
DESCRIPTION: Demonstrates various examples of using the reflect() function to call Java String and Math methods directly from Hive SQL. The example shows string operations, mathematical calculations, and type conversions. The query returns primitive types that Hive can serialize.

LANGUAGE: sql
CODE:
SELECT reflect("java.lang.String", "valueOf", 1),
       reflect("java.lang.String", "isEmpty"),
       reflect("java.lang.Math", "max", 2, 3),
       reflect("java.lang.Math", "min", 2, 3),
       reflect("java.lang.Math", "round", 2.5),
       reflect("java.lang.Math", "exp", 1.0),
       reflect("java.lang.Math", "floor", 1.9)
FROM src LIMIT 1;

1	true	3	2	3	2.7182818284590455	1.0

----------------------------------------

TITLE: Partial Partition Specification Exchange in Hive
DESCRIPTION: Demonstrates exchanging multiple partitions using partial partition specification. Creates tables with multiple partition columns and exchanges all partitions where ds='1'.

LANGUAGE: sql
CODE:
--Create two tables with multiple partition columns.
CREATE TABLE T1 (a string, b string) PARTITIONED BY (ds string, hr string);
CREATE TABLE T2 (a string, b string) PARTITIONED BY (ds string, hr string);
ALTER TABLE T1 ADD PARTITION (ds = '1', hr = '00');
ALTER TABLE T1 ADD PARTITION (ds = '1', hr = '01');
ALTER TABLE T1 ADD PARTITION (ds = '1', hr = '03');

--Alter the table, moving all the three partitions data where ds='1' from table T1 to table T2 (ds=1) 
ALTER TABLE T2 EXCHANGE PARTITION (ds='1') WITH TABLE T1;

----------------------------------------

TITLE: Direct HLL Distinct Count Queries
DESCRIPTION: Shows how to compute distinct counts using HLL sketches without intermediate tables

LANGUAGE: sql
CODE:
select category, ds_hll_estimate(ds_hll_sketch(id)) from sketch_input group by category;
select ds_hll_estimate(ds_hll_sketch(id)) from sketch_input;

----------------------------------------

TITLE: Creating RCFile Index in Hive
DESCRIPTION: Demonstrates creating an index stored in RCFile format.

LANGUAGE: hql
CODE:
CREATE INDEX table05_index ON TABLE table05 (column6) AS 'COMPACT' STORED AS RCFILE;

----------------------------------------

TITLE: Defining Column Statistics Thrift API Methods
DESCRIPTION: Thrift API method definitions for updating, retrieving, and deleting column statistics for tables and partitions in Hive.

LANGUAGE: Thrift
CODE:
bool update_table_column_statistics(1:ColumnStatistics stats_obj) throws (1:NoSuchObjectException o1,   
 2:InvalidObjectException o2, 3:MetaException o3, 4:InvalidInputException o4)  
 bool update_partition_column_statistics(1:ColumnStatistics stats_obj) throws (1:NoSuchObjectException o1,   
 2:InvalidObjectException o2, 3:MetaException o3, 4:InvalidInputException o4)

ColumnStatistics get_table_column_statistics(1:string db_name, 2:string tbl_name, 3:string col_name) throws  
 (1:NoSuchObjectException o1, 2:MetaException o2, 3:InvalidInputException o3, 4:InvalidObjectException o4)   
 ColumnStatistics get_partition_column_statistics(1:string db_name, 2:string tbl_name, 3:string part_name,  
 4:string col_name) throws (1:NoSuchObjectException o1, 2:MetaException o2,   
 3:InvalidInputException o3, 4:InvalidObjectException o4)

bool delete_partition_column_statistics(1:string db_name, 2:string tbl_name, 3:string part_name, 4:string col_name) throws   
 (1:NoSuchObjectException o1, 2:MetaException o2, 3:InvalidObjectException o3,   
 4:InvalidInputException o4)  
 bool delete_table_column_statistics(1:string db_name, 2:string tbl_name, 3:string col_name) throws   
 (1:NoSuchObjectException o1, 2:MetaException o2, 3:InvalidObjectException o3,   
 4:InvalidInputException o4)

----------------------------------------

TITLE: Table Bucket Sampling Syntax in Hive SQL
DESCRIPTION: Basic syntax for bucket-based table sampling, which allows querying a subset of data based on bucketing.

LANGUAGE: sql
CODE:
table_sample: TABLESAMPLE (BUCKET x OUT OF y [ON colname])

----------------------------------------

TITLE: Setting Hive Metastore Client Cache Configuration
DESCRIPTION: Configuration setting to enable Caffeine Cache for the Metastore client. This setting is managed through the MetastoreConf configuration file and defaults to true.

LANGUAGE: xml
CODE:
hive.metastore.client.cache.v2.enabled

----------------------------------------

TITLE: Setting Small Table File Size Threshold for Map Joins in Hive SQL
DESCRIPTION: This SQL command sets the maximum file size (in bytes) for small tables that can be used in map joins. If the total size of small tables exceeds this threshold, Hive will use a common join instead.

LANGUAGE: sql
CODE:
set hive.mapjoin.smalltable.filesize = 30000000;

----------------------------------------

TITLE: Basic Rebalance Compaction in Hive SQL
DESCRIPTION: Executes a basic rebalance compaction operation on a table to redistribute data evenly across implicit bucket files. This operation requires an exclusive write lock on the table.

LANGUAGE: sql
CODE:
ALTER TABLE table_name COMPACT 'REBALANCE';

----------------------------------------

TITLE: Creating Partition Column Statistics Table in Metastore
DESCRIPTION: SQL command to create the PART_COL_STATS table for storing partition column statistics in the Hive metastore.

LANGUAGE: SQL
CODE:
CREATE TABLE PART_COL_STATS  
 (
 CS_ID NUMBER NOT NULL,  
 PART_ID NUMBER NOT NULL,

DB_NAME VARCHAR(128) NOT NULL,  
 COLUMN_NAME VARCHAR(128) NOT NULL,  
 COLUMN_TYPE VARCHAR(128) NOT NULL,  
 TABLE_NAME VARCHAR(128) NOT NULL,  
 PART_NAME VARCHAR(128) NOT NULL,

LOW_VALUE RAW,  
 HIGH_VALUE RAW,  
 NUM_NULLS BIGINT,  
 NUM_DISTINCTS BIGINT,

BIT_VECTOR, BLOB,  /* introduced in [HIVE-16997](https://issues.apache.org/jira/browse/HIVE-16997) in Hive 3.0.0 */

AVG_COL_LEN DOUBLE,  
 MAX_COL_LEN BIGINT,  
 NUM_TRUES BIGINT,  
 NUM_FALSES BIGINT,  
 LAST_ANALYZED BIGINT NOT NULL)

----------------------------------------

TITLE: Creating Index with Properties in Hive
DESCRIPTION: Demonstrates creating an index with custom index properties.

LANGUAGE: hql
CODE:
CREATE INDEX table07_index ON TABLE table07 (column8) AS 'COMPACT' IDXPROPERTIES ("prop1"="value1", "prop2"="value2");

----------------------------------------

TITLE: Extracting N-grams from Kafka Data in Apache Hive
DESCRIPTION: This query demonstrates how to use the ngrams() function with the explode() function to extract and display the top 10 bigrams from a Kafka data source.

LANGUAGE: SQL
CODE:
SELECT explode(ngrams(sentences(lower(val)), 2, 10)) AS x FROM kafka;

----------------------------------------

TITLE: LIMIT Clause Examples in HiveQL
DESCRIPTION: Demonstrates various ways to limit query results using the LIMIT clause, including offset and row count specifications.

LANGUAGE: HiveQL
CODE:
SELECT * FROM customers LIMIT 5

LANGUAGE: HiveQL
CODE:
SELECT * FROM customers ORDER BY create_date LIMIT 5

LANGUAGE: HiveQL
CODE:
SELECT * FROM customers ORDER BY create_date LIMIT 2,5

----------------------------------------

TITLE: Multi-Table Dependency Example in Hive
DESCRIPTION: Complete example showing how to create and manage dependencies across multiple tables, including table creation, partition management, and dependency changes.

LANGUAGE: hql
CODE:
create table T (key string, value string) partitioned by (ds string, hr string);

create dependent table Tdep partitioned by (ds string) depends on table T;

alter table Tdep add partition (ds=1);

create table T2 (key string, value string) partitioned by (ds string, hr string, min string);

alter table Tdep depends on table T2;

alter table Tdep add partition (ds=1);

----------------------------------------

TITLE: Creating a Druid-Stored Materialized View in Hive
DESCRIPTION: Shows how to create a materialized view that is stored in Druid using a custom storage handler.

LANGUAGE: SQL
CODE:
CREATE MATERIALIZED VIEW druid_wiki_mv
STORED AS 'org.apache.hadoop.hive.druid.DruidStorageHandler'
AS
SELECT __time, page, user, c_added, c_removed
FROM src;

----------------------------------------

TITLE: Granting and Revoking Hive Roles
DESCRIPTION: SQL syntax for granting and revoking roles to users, groups, or other roles with optional admin privileges.

LANGUAGE: sql
CODE:
GRANT ROLE role_name [, role_name] ...
TO principal_specification [, principal_specification] ...
[WITH ADMIN OPTION]

REVOKE [ADMIN OPTION FOR] ROLE role_name [, role_name] ...
FROM principal_specification [, principal_specification] ...

----------------------------------------

TITLE: HLL Distinct Count with Intermediate Table
DESCRIPTION: Demonstrates using HLL sketches with an intermediate table to compute distinct value counts by category

LANGUAGE: sql
CODE:
create temporary table sketch_intermediate (category char(1), sketch binary);
insert into sketch_intermediate select category, ds_hll_sketch(id) from sketch_input group by category;

select category, ds_hll_estimate(sketch) from sketch_intermediate;

select ds_hll_estimate(ds_hll_union(sketch)) from sketch_intermediate;

----------------------------------------

TITLE: Filesystem Layout Example for ACID Table
DESCRIPTION: Shows the directory structure of an ACID-enabled Hive table including base and delta directories

LANGUAGE: shell
CODE:
hive> dfs -ls -R /user/hive/warehouse/t;
drwxr-xr-x   - ekoifman staff          0 2016-06-09 17:03 /user/hive/warehouse/t/base_0000022
-rw-r--r--   1 ekoifman staff        602 2016-06-09 17:03 /user/hive/warehouse/t/base_0000022/bucket_00000
drwxr-xr-x   - ekoifman staff          0 2016-06-09 17:06 /user/hive/warehouse/t/delta_0000023_0000023_0000
-rw-r--r--   1 ekoifman staff        611 2016-06-09 17:06 /user/hive/warehouse/t/delta_0000023_0000023_0000/bucket_00000
drwxr-xr-x   - ekoifman staff          0 2016-06-09 17:07 /user/hive/warehouse/t/delta_0000024_0000024_0000
-rw-r--r--   1 ekoifman staff        610 2016-06-09 17:07 /user/hive/warehouse/t/delta_0000024_0000024_0000/bucket_00000

----------------------------------------

TITLE: Serializing Output Data in Hive
DESCRIPTION: Illustrates how Hive's execution engine passes the deserialized Object and ObjectInspector to Serde.serialize() for converting the record to the appropriate output format.

LANGUAGE: Java
CODE:
Serde.serialize(Object, ObjectInspector)

----------------------------------------

TITLE: Creating Index with Table Properties in Hive
DESCRIPTION: Shows how to create an index with custom table properties.

LANGUAGE: hql
CODE:
CREATE INDEX table08_index ON TABLE table08 (column9) AS 'COMPACT' TBLPROPERTIES ("prop3"="value3", "prop4"="value4");

----------------------------------------

TITLE: Adding and Deleting Ivy Resources in Hive
DESCRIPTION: Examples of adding and deleting resources using Ivy URLs in Hive 1.2.0 and later versions.

LANGUAGE: sql
CODE:
hive>ADD JAR ivy://org.apache.pig:pig:0.10.0?exclude=org.apache.hadoop:avro;
hive>ADD JAR ivy://org.apache.pig:pig:0.10.0?exclude=org.apache.hadoop:avro&transitive=false;

hive>ADD JAR ivy://org.apache.pig:pig:0.10.0
hive>ADD JAR ivy://org.apache.pig:pig:0.11.1.15
hive>DELETE JAR ivy://org.apache.pig:pig:0.10.0

----------------------------------------

TITLE: Deferred Index Build Operations in Hive
DESCRIPTION: Shows how to create an index with deferred rebuild, rebuild it later, show formatted output, and drop the index.

LANGUAGE: hql
CODE:
CREATE INDEX table02_index ON TABLE table02 (column3) AS 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX table02_index ON table2 REBUILD;
SHOW FORMATTED INDEX ON table02;
DROP INDEX table02_index ON table02;

----------------------------------------

TITLE: Setting Druid Broker Address in Hive Configuration
DESCRIPTION: Sets the Hive configuration property to specify the Druid broker address for connecting to Druid.

LANGUAGE: SQL
CODE:
SET hive.druid.broker.address.default=10.5.0.10:8082;

----------------------------------------

TITLE: Creating Remote Database in Hive
DESCRIPTION: Demonstrates the syntax for creating a remote database in Hive using a connector. This command maps a remote database to a Hive database.

LANGUAGE: sql
CODE:
CREATE REMOTE DATABASE postgres_db1 USING <connectorName> WITH DBPROPERTIES ('connector.remoteDbName'='db1');

----------------------------------------

TITLE: Building and Running Hugo Server
DESCRIPTION: Command to build and start the Hugo server for local development, including sample output.

LANGUAGE: bash
CODE:
>>> hugo server -D

                   | EN
+------------------+----+
  Pages            | 10
  Paginator pages  |  0
  Non-page files   |  0
  Static files     |  3
  Processed images |  0
  Aliases          |  1
  Sitemaps         |  1
  Cleaned          |  0

Total in 11 ms
Watching for changes in /Users/bep/quickstart/{content,data,layouts,static,themes}
Watching for config changes in /Users/bep/quickstart/config.toml
Environment: "development"
Serving pages from memory
Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender
Web Server is available at http://localhost:1313/ (bind address 127.0.0.1)
Press Ctrl+C to stop

----------------------------------------

TITLE: Invoking Serde.deserialize() for Input Processing in Hive
DESCRIPTION: Demonstrates how Hive's execution engine uses the configured InputFormat to read a record of data and then invokes Serde.deserialize() to perform deserialization of the record.

LANGUAGE: Java
CODE:
Serde.deserialize()

----------------------------------------

TITLE: Marking Partition Set as Complete in HCatalog using Java
DESCRIPTION: Demonstrates how to mark a set of partitions as complete in HCatalog, which triggers a notification for data readers.

LANGUAGE: java
CODE:
HiveMetaStoreClient msc = new HiveMetaStoreClient(conf);

// Create a map, specifying partition key names and values
Map<String,String> partMap = new HashMap<String, String>();
partMap.put("date","20110711");
partMap.put("country","*");

// Mark the partition as "done"
msc.markPartitionForEvent("mydb", "mytbl", partMap, PartitionEventType.LOAD_DONE);

----------------------------------------

TITLE: Configuring Metastore Connection URL for MySQL
DESCRIPTION: Example configuration for connecting Apache Hive Metastore to a MySQL database. This snippet shows the JDBC connection URL format and driver class name required for MySQL integration.

LANGUAGE: XML
CODE:
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://<HOST>:<PORT>/<SCHEMA></value>
</property>
<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
</property>

----------------------------------------

TITLE: Building Hive Client with Maven
DESCRIPTION: Commands to compile the ODBC driver using Maven build system. Requires Thrift and Boost home directories.

LANGUAGE: bash
CODE:
$ cd odbc
$ mvn compile -Podbc,hadoop-1 -Dthrift.home=/usr/local -Dboost.home=/usr/local

----------------------------------------

TITLE: Complex DISTRIBUTE BY with Transform
DESCRIPTION: Shows how to use DISTRIBUTE BY and SORT BY in conjunction with MapReduce transformations.

LANGUAGE: sql
CODE:
  FROM (
    FROM pv_users
    MAP ( pv_users.userid, pv_users.date )
    USING 'map_script'
    AS c1, c2, c3
    DISTRIBUTE BY c2
    SORT BY c2, c1) map_output
  INSERT OVERWRITE TABLE pv_users_reduced
    REDUCE ( map_output.c1, map_output.c2, map_output.c3 )
    USING 'reduce_script'
    AS date, count;

----------------------------------------

TITLE: Dynamic Partitioning with Data Spanning Multiple Partitions in Pig
DESCRIPTION: This Pig script shows how HCatalog handles dynamic partitioning when data spans multiple partitions, automatically determining how to distribute the data.

LANGUAGE: pig
CODE:
store A into 'mytable' using HCatStorer();

store A into 'mytable' using HCatStorer("a=1");

split A into A1 if b='1', A2 if b='2';
store A1 into 'mytable' using HCatStorer("a=1, b=1");
store A2 into 'mytable' using HCatStorer("a=1, b=2");

----------------------------------------

TITLE: Hive Table Query Examples
DESCRIPTION: Demonstrates case-insensitive nature of Hive SQL queries

LANGUAGE: SQL
CODE:
SELECT * FROM MyTable WHERE myColumn = 3
select * from mytable where mycolumn = 3

----------------------------------------

TITLE: Creating a Hive Table with Avro Data in HBase
DESCRIPTION: Example of creating a Hive table that maps to Avro-serialized data stored in HBase columns.

LANGUAGE: HiveQL
CODE:
CREATE EXTERNAL TABLE test_hbase_avro
ROW FORMAT SERDE 'org.apache.hadoop.hive.hbase.HBaseSerDe' 
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' 
WITH SERDEPROPERTIES (
	"hbase.columns.mapping" = ":key,test_col_fam:test_col", 
	"test_col_fam.test_col.serialization.type" = "avro",
	"test_col_fam.test_col.avro.schema.url" = "hdfs://testcluster/tmp/schema.avsc")
TBLPROPERTIES (
    "hbase.table.name" = "hbase_avro_table",
    "hbase.mapred.output.outputtable" = "hbase_avro_table",
    "hbase.struct.autogenerate"="true");

----------------------------------------

TITLE: Rename Table During Import Example
DESCRIPTION: Example showing how to rename a table during the import process.

LANGUAGE: sql
CODE:
export table department to 'hdfs_exports_location/department';
import table imported_dept from 'hdfs_exports_location/department';

----------------------------------------

TITLE: Retrieving HCatalog Notification Topic Name in Java
DESCRIPTION: Gets the topic name for a specific table from the Hive metastore. This topic name is used to subscribe to notifications for that table.

LANGUAGE: java
CODE:
HiveMetaStoreClient msc = new HiveMetaStoreClient(hiveConf);
String topicName = msc.getTable("mydb",
                   "myTbl").getParameters().get(HCatConstants.HCAT_MSGBUS_TOPIC_NAME);

----------------------------------------

TITLE: Typed Output Transform Example
DESCRIPTION: Examples showing how to specify data types for TRANSFORM output columns in Hive.

LANGUAGE: hql
CODE:
SELECT TRANSFORM(stuff)
USING 'script'
AS (thing1 INT, thing2 INT)

----------------------------------------

TITLE: Configuring Hive Metastore Security Settings in XML
DESCRIPTION: Sample hive-site.xml configuration showing default security settings for Hive metastore authorization and authentication. Includes properties for authorization manager, authenticator manager, and pre-event listeners with their default values and descriptions.

LANGUAGE: xml
CODE:
<property>
  <name>hive.security.metastore.authorization.manager</name>
  <value>org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider</value>
  <description>authorization manager class name to be used in the metastore for authorization.
  The user defined authorization class should implement interface
  org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider.
  </description>
 </property>

<property>
  <name>hive.security.metastore.authenticator.manager</name>
  <value>org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator</value>
  <description>authenticator manager class name to be used in the metastore for authentication.
  The user defined authenticator should implement interface 
  org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider.
  </description>
</property>

<property>
  <name>hive.metastore.pre.event.listeners</name>
  <value> </value>
  <description>pre-event listener classes to be loaded on the metastore side to run code
  whenever databases, tables, and partitions are created, altered, or dropped.
  Set to org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener
  if metastore-side authorization is desired.
  </description>
</property>

----------------------------------------

TITLE: Configuring SAML Authentication in Hive XML
DESCRIPTION: XML configuration snippets for enabling SAML authentication in Hive, including setting the authentication mode, IDP metadata path, and other SAML-related properties.

LANGUAGE: xml
CODE:
<property>\n  <name>hive.server2.authentication</name>\n  <value>SAML</value>\n</property>\n\n<property>\n  <name>hive.server2.saml2.idp.metadata</name>\n  <value>path_to_idp_metadata.xml</value>\n</property>\n\n<property>\n  <name>hive.server2.saml2.sp.entity.id</name>\n  <value>test_sp_entity_id</value>\n</property>\n\n<property>\n  <name>hive.server2.saml2.group.attribute.name</name>\n  <value>group_attribute_name</value>\n</property>\n\n<property>\n  <name>hive.server2.saml2.group.filter</name>\n  <value>comma_separated_group_names</value>\n</property>\n\n<property>\n  <name>hive.server2.saml2.sp.callback.url</name>\n  <value>callback_url_of_hiveserver2</value>\n</property>

----------------------------------------

TITLE: Multi-Table Dynamic Partition Insert
DESCRIPTION: Example showing multi-table insert with mixed static and dynamic partitions across different target tables.

LANGUAGE: hql
CODE:
FROM S
INSERT OVERWRITE TABLE T PARTITION (ds='2010-03-03', hr) 
SELECT key, value, ds, hr FROM srcpart WHERE ds is not null and hr>10
INSERT OVERWRITE TABLE R PARTITION (ds='2010-03-03, hr=12)
SELECT key, value, ds, hr from srcpart where ds is not null and hr = 12;

----------------------------------------

TITLE: Regex Pattern for Hive Patch Naming Convention
DESCRIPTION: Regular expression pattern defining the allowed formats for Hive patch file names. Supports JIRA issue numbers, optional revision numbers, branch specifications, and file extensions.

LANGUAGE: regex
CODE:
HIVE-XXXX(.XX)?(-branch)?.patch(.txt)?\n(HIVE-XXXX\.)?DXXXX(.XX)?.patch(.txt)?

----------------------------------------

TITLE: Querying Iceberg Metadata Tables in Hive
DESCRIPTION: Shows how to query various metadata tables associated with an Iceberg table using the dot notation syntax.

LANGUAGE: SQL
CODE:
SELECT * from db_name.tbl_name.entries;
SELECT * from db_name.tbl_name.files;
SELECT * from db_name.tbl_name.history;
SELECT * from db_name.tbl_name.snapshots;
SELECT * from db_name.tbl_name.manifests;
SELECT * from db_name.tbl_name.partitions;
SELECT * from db_name.tbl_name.all_data_files;
SELECT * from db_name.tbl_name.all_manifests;
SELECT * from db_name.tbl_name.all_entries;
SELECT * from db_name.tbl_name.data_files;
SELECT * from db_name.tbl_name.delete_files;
SELECT * from db_name.tbl_name.metadata_log_entries;
SELECT * from db_name.tbl_name.refs;
SELECT * from db_name.tbl_name.all_delete_files;
SELECT * from db_name.tbl_name.all_files;

----------------------------------------

TITLE: Creating a Hive Table with Composite HBase Row Key
DESCRIPTION: Example of creating a Hive table with a composite row key mapped to an HBase table.

LANGUAGE: HiveQL
CODE:
CREATE EXTERNAL TABLE delimited_example(key struct<f1:string, f2:string>, value string) 
ROW FORMAT DELIMITED 
COLLECTION ITEMS TERMINATED BY '~' 
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' 
WITH SERDEPROPERTIES ( 
  'hbase.columns.mapping'=':key,f:c1');

----------------------------------------

TITLE: Executing Jenkins Script for Apache Hive Patch Testing
DESCRIPTION: This script automates the process of testing patches for Apache Hive. It sets up the environment, validates the patch, and executes tests using PTestClient. The script handles JIRA integration, patch downloading, and result reporting.

LANGUAGE: bash
CODE:
env
set -e
set -x
. ${HOME}/toolchain/toolchain.sh
export PATH=$JAVA_HOME/bin:$MAVEN_HOME/bin:$PATH
ROOT=$PWD
BRANCH=trunk
export JIRA_NAME="HIVE-${ISSUE_NUM}"
echo $JIRA_NAME
export JIRA_ROOT_URL="https://issues.apache.org"
test -d hive/build/ || mkdir -p hive/build/
cd hive/build/
rm -rf ptest2
svn co http://svn.apache.org/repos/asf/hive/trunk/testutils/ptest2/ ptest2
cd ptest2
# process the jira
JIRA_TEXT=$(mktemp)
trap "rm -f $JIRA_TEXT" EXIT
curl -s -S --location --retry 3 "${JIRA_ROOT_URL}/jira/browse/${JIRA_NAME}" > $JIRA_TEXT
if grep -q "NO PRECOMMIT TESTS" $JIRA_TEXT
then
  echo "Test $JIRA_NAME has tag NO PRECOMMIT TESTS"
  exit 0
fi
# ensure the patch is actually in the correct state
if ! grep -q 'Patch Available' $JIRA_TEXT
then
  echo "$JIRA_NAME is not \"Patch Available\". Exiting."
  exit 1
fi
# pull attachments from JIRA (hack stolen from hadoop since rest api doesn't show attachments)
PATCH_URL=$(grep -o '"/jira/secure/attachment/[0-9]*/[^"]*' $JIRA_TEXT | \
  grep -v -e 'htm[l]*$' | sort | tail -1 | \
  grep -o '/jira/secure/attachment/[0-9]*/[^"]*')
if [[ -z "$PATCH_URL" ]]
then
  echo "Unable to find attachment for $JIRA_NAME"
  exit 1
fi
# ensure attachment has not already been tested
ATTACHMENT_ID=$(basename $(dirname $PATCH_URL))
if grep -q "ATTACHMENT ID: $ATTACHMENT_ID" $JIRA_TEXT
then
  echo "Attachment $ATTACHMENT_ID is already tested for $JIRA_NAME"
  exit 1
fi
# validate the patch name, parse branch if needed
shopt -s nocasematch
PATCH_NAME=$(basename $PATCH_URL)
# Test examples:
# HIVE-123.patch HIVE-123.1.patch HIVE-123.D123.patch HIVE-123.D123.1.patch HIVE-123-tez.patch HIVE-123.1-tez.patch
# HIVE-XXXX.patch, HIVE-XXXX.XX.patch  HIVE-XXXX.XX-branch.patch HIVE-XXXX-branch.patch
if [[ $PATCH_NAME =~ ^HIVE-[0-9]+(\.[0-9]+)?(-[a-z0-9-]+)?\.(patch|patch.\txt)$ ]]
then
  if [[ -n "${BASH_REMATCH[2]}" ]]
  then
    BRANCH=${BASH_REMATCH[2]#*-}
  else
    echo "Assuming branch $BRANCH"
  fi
# HIVE-XXXX.DXXXX.patch or HIVE-XXXX.DXXXX.XX.patch
elif [[ $PATCH_NAME =~ ^(HIVE-[0-9]+\.)?D[0-9]+(\.[0-9]+)?\.(patch|patch.\txt)$ ]]
then
  echo "Assuming branch $BRANCH"
else
  echo "Patch $PATCH_NAME does not appear to be a patch"
  exit 1
fi
shopt -u nocasematch
# append mr1 if needed
if [[ $BRANCH =~ (mr1|mr2)$ ]]
then
  profile=$BRANCH
else
  profile=${BRANCH}-mr1
fi
# sanity check the profile
case "$profile" in
  trunk-mr1);;trunk-mr2);;maven-mr1);;*)
  echo "Unknown profile '$profile'"
  exit 1
esac
TEST_OPTS=""
if grep -q "CLEAR LIBRARY CACHE" $JIRA_TEXT
then
  echo "Clearing library cache before starting test"
  TEST_OPTS="--clearLibraryCache"
fi
mvn clean package -DskipTests -Drat.numUnapprovedLicenses=1000 -Dmaven.repo.local=$WORKSPACE/.m2
set +e
java -cp "target/hive-ptest-1.0-classes.jar:target/lib/*" org.apache.hive.ptest.api.client.PTestClient --endpoint http://ec2-174-129-184-35.compute-1.amazonaws.com --command testStart --profile $profile --password xxx --outputDir target/ --testHandle "${BUILD_TAG##jenkins-}" --patch "${JIRA_ROOT_URL}${PATCH_URL}" --jira "$JIRA_NAME" --clearLibraryCache
ret=$?
cd target/
if [[ -f test-results.tar.gz ]]
then
  rm -rf $ROOT/hive/build/test-results/
  tar zxf test-results.tar.gz -C $ROOT/hive/build/
fi
exit $ret

----------------------------------------

TITLE: Retrieving Attribute Values with XPath in Apache Hive SQL
DESCRIPTION: Shows how to use the xpath UDF to get a list of attribute values from an XML string. The function returns an array of strings containing the matched attribute values.

LANGUAGE: SQL
CODE:
select xpath('<a><b id="foo">b1</b><b id="bar">b2</b></a>','//@id') from src limit 1 ;

----------------------------------------

TITLE: Creating Hive Database via WebHCat REST API - Curl Example
DESCRIPTION: Demonstrates how to create a new Hive database using a PUT request to the WebHCat API with custom parameters including comment, location, and properties.

LANGUAGE: bash
CODE:
% curl -s -X PUT -HContent-type:application/json \
       -d '{ "comment":"Hello there",
             "location":"hdfs://localhost:9000/user/hive/my_warehouse",
             "properties":{"a":"b"}}' \
       'http://localhost:50111/templeton/v1/ddl/database/newdb?user.name=rachel'

----------------------------------------

TITLE: Join Optimization with STREAMTABLE Hint
DESCRIPTION: Shows how to use the STREAMTABLE hint to optimize join performance by specifying which table to stream through the reducers.

LANGUAGE: SQL
CODE:
SELECT /*+ STREAMTABLE(a) */ a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1)

----------------------------------------

TITLE: Configuring Hive Archive Settings
DESCRIPTION: Essential configuration settings required before using Hive archiving functionality. Includes settings for enabling archiving, parent directory configuration, and controlling archive file size.

LANGUAGE: sql
CODE:
hive> set hive.archive.enabled=true;
hive> set hive.archive.har.parentdir.settable=true;
hive> set har.partfile.size=1099511627776;

----------------------------------------

TITLE: Configuring Database Schema for Different DBMS Systems
DESCRIPTION: Examples of creating and configuring database schemas in different DBMS systems including MariaDB, MS SQL, Oracle, and PostgreSQL.

LANGUAGE: SQL
CODE:
CREATE SCHEMA bob;
CREATE TABLE bob.country
(
    id   int,
    name varchar(20)
);

insert into bob.country
values (1, 'India');
insert into bob.country
values (2, 'Russia');
insert into bob.country
values (3, 'USA');

----------------------------------------

TITLE: Configuring Hive for TiDB Connection
DESCRIPTION: XML configuration for Hive to connect to TiDB as the Metastore database, including connection details and driver settings.

LANGUAGE: XML
CODE:
<configuration>
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://host:port/hive</value>
    <description>TiDB address</description>
  </property>

  <property>  
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive</value>
    <description>TiDB username</description>
  </property>

  <property>  
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>123456</value>
    <description>TiDB password</description>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
  </property>

  <property>
    <name>hive.metastore.uris</name>
    <value>thrift://localhost:9083</value>
  </property>

  <property>
    <name>hive.metastore.schema.verification</name>
    <value>false</value>
  </property>
</configuration>

----------------------------------------

TITLE: Retrieving Simple Table Description with cURL
DESCRIPTION: Example of a basic API call to retrieve table description using the WebHCat REST endpoint. Shows how to query the default database for a specific table's structure.

LANGUAGE: curl
CODE:
curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/my_table?user.name=ctdean'

----------------------------------------

TITLE: Preparing HCatWriter Context in Java
DESCRIPTION: This code demonstrates how to obtain a WriterContext from an HCatWriter instance.

LANGUAGE: Java
CODE:
HCatWriter writer = DataTransferFactory.getHCatWriter(entity, config);
WriterContext info = writer.prepareWrite();

----------------------------------------

TITLE: GroupByAge MapReduce Example with HCatalog
DESCRIPTION: Complete MapReduce program example showing how to read from and write to HCatalog tables, implementing a group-by operation on age data.

LANGUAGE: Java
CODE:
public class GroupByAge extends Configured implements Tool {

    public static class Map extends
            Mapper<WritableComparable, HCatRecord, IntWritable, IntWritable> {

        int age;

        @Override
        protected void map(
                WritableComparable key,
                HCatRecord value,
                org.apache.hadoop.mapreduce.Mapper<WritableComparable, HCatRecord,
                        IntWritable, IntWritable>.Context context)
                throws IOException, InterruptedException {
            age = (Integer) value.get(1);
            context.write(new IntWritable(age), new IntWritable(1));
        }
    }

    public static class Reduce extends Reducer<IntWritable, IntWritable,
    WritableComparable, HCatRecord> {

      @Override
      protected void reduce(
              IntWritable key,
              java.lang.Iterable<IntWritable> values,
              org.apache.hadoop.mapreduce.Reducer<IntWritable, IntWritable,
                      WritableComparable, HCatRecord>.Context context)
              throws IOException, InterruptedException {
          int sum = 0;
          Iterator<IntWritable> iter = values.iterator();
          while (iter.hasNext()) {
              sum++;
              iter.next();
          }
          HCatRecord record = new DefaultHCatRecord(2);
          record.set(0, key.get());
          record.set(1, sum);

          context.write(null, record);
        }
    }

    public int run(String[] args) throws Exception {
        Configuration conf = getConf();
        args = new GenericOptionsParser(conf, args).getRemainingArgs();

        String inputTableName = args[0];
        String outputTableName = args[1];
        String dbName = null;

        Job job = new Job(conf, "GroupByAge");
        HCatInputFormat.setInput(job, InputJobInfo.create(dbName,
                inputTableName, null));
        // initialize HCatOutputFormat

        job.setInputFormatClass(HCatInputFormat.class);
        job.setJarByClass(GroupByAge.class);
        job.setMapperClass(Map.class);
        job.setReducerClass(Reduce.class);
        job.setMapOutputKeyClass(IntWritable.class);
        job.setMapOutputValueClass(IntWritable.class);
        job.setOutputKeyClass(WritableComparable.class);
        job.setOutputValueClass(DefaultHCatRecord.class);
        HCatOutputFormat.setOutput(job, OutputJobInfo.create(dbName,
                outputTableName, null));
        HCatSchema s = HCatOutputFormat.getTableSchema(job);
        System.err.println("INFO: output schema explicitly set for writing:"
                + s);
        HCatOutputFormat.setSchema(job, s);
        job.setOutputFormatClass(HCatOutputFormat.class);
        return (job.waitForCompletion(true) ? 0 : 1);
    }

    public static void main(String[] args) throws Exception {
        int exitCode = ToolRunner.run(new GroupByAge(), args);
        System.exit(exitCode);
    }
}

----------------------------------------

TITLE: Extracting Text with Conditional XPath in Apache Hive SQL
DESCRIPTION: Demonstrates using xpath UDF with a conditional XPath expression to retrieve text from nodes with specific attribute values. Returns an array of matching text strings.

LANGUAGE: SQL
CODE:
SELECT xpath ('<a><b class="bb">b1</b><b>b2</b><b>b3</b><c class="bb">c1</c><c>c2</c></a>', 'a/*[@class="bb"]/text()') FROM src LIMIT 1 ;

----------------------------------------

TITLE: Transform Example with Map and Reduce Scripts
DESCRIPTION: Example showing how to use TRANSFORM with custom map and reduce scripts in Hive queries.

LANGUAGE: hql
CODE:
FROM (
  FROM pv_users
  MAP pv_users.userid, pv_users.date
  USING 'map_script'
  AS dt, uid
  CLUSTER BY dt) map_output
INSERT OVERWRITE TABLE pv_users_reduced
  REDUCE map_output.dt, map_output.uid
  USING 'reduce_script'
  AS date, count;

----------------------------------------

TITLE: Creating Compressed SequenceFile Table in Hive
DESCRIPTION: This snippet shows how to create a table stored as a SequenceFile, which can be split and distributed across map jobs. It includes setting compression options and inserting data from an existing table.

LANGUAGE: SQL
CODE:
CREATE TABLE raw (line STRING)
   ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n';

CREATE TABLE raw_sequence (line STRING)
   STORED AS SEQUENCEFILE;

LOAD DATA LOCAL INPATH '/tmp/weblogs/20090603-access.log.gz' INTO TABLE raw;

SET hive.exec.compress.output=true;
SET io.seqfile.compression.type=BLOCK; -- NONE/RECORD/BLOCK (see below)
INSERT OVERWRITE TABLE raw_sequence SELECT * FROM raw;

----------------------------------------

TITLE: WebHCat Get Columns API Response Format - JSON Example
DESCRIPTION: Example JSON response showing the structure of column information returned by the WebHCat API, including column names, types, and optional comments.

LANGUAGE: json
CODE:
{
 "columns": [
   {
     "name": "id",
     "type": "bigint"
   },
   {
     "name": "user",
     "comment": "The user name",
     "type": "string"
   },
   {
     "name": "my_p",
     "type": "string"
   },
   {
     "name": "my_q",
     "type": "string"
   }
 ],
 "database": "default",
 "table": "my_table"
}

----------------------------------------

TITLE: Query Rewriting Example Using Druid-Stored Materialized View in Hive
DESCRIPTION: Shows a query that can be rewritten using the Druid-stored materialized view 'mv3'.

LANGUAGE: SQL
CODE:
SELECT floor(time to month),
SUM(characters_added) AS c_added
FROM wiki
GROUP BY floor(time to month);

----------------------------------------

TITLE: Setting Hive Environment Variables
DESCRIPTION: Bash commands to set HADOOP_HOME and JAVA_HOME environment variables in the Hive configuration file.

LANGUAGE: Bash
CODE:
export HADOOP_HOME=...
export JAVA_HOME=...

----------------------------------------

TITLE: Hive Startup Commands
DESCRIPTION: Commands to start Hive and initialize the metastore

LANGUAGE: bash
CODE:
cd /opt/hadoop/hive
bin/hive
hive> show tables;

----------------------------------------

TITLE: Setting Findbugs Environment Variable in Bash
DESCRIPTION: Sets the FINDBUGS_HOME environment variable to the installation directory of Findbugs. This is required for running Findbugs checks with Yetus.

LANGUAGE: bash
CODE:
export FINDBUGS_HOME=~/dev/upstream/findbugs-3.0.1/

----------------------------------------

TITLE: Retrieving Single Node Text with xpath_string in Apache Hive SQL
DESCRIPTION: Shows the usage of xpath_string UDF to extract text from a specific XML node. The function returns the text of the first matching node as a string.

LANGUAGE: SQL
CODE:
SELECT xpath_string ('<a><b>bb</b><c>cc</c></a>', 'a/b') FROM src LIMIT 1 ;

----------------------------------------

TITLE: Transform Example with TypedBytes SerDe
DESCRIPTION: Example demonstrating TRANSFORM usage with TypedBytes SerDe and custom record reader.

LANGUAGE: hql
CODE:
FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  USING '/bin/cat'
  AS (tkey, tvalue) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDREADER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader'
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tkey, tvalue

----------------------------------------

TITLE: Applying Subclauses to Entire UNION Result
DESCRIPTION: Shows how to apply ORDER BY, LIMIT, and other subclauses to the entire UNION result by placing them after the last SELECT statement.

LANGUAGE: SQL
CODE:
SELECT key FROM src
UNION
SELECT key FROM src1 
ORDER BY key LIMIT 10

----------------------------------------

TITLE: WebHCat Get Partitions API Response Format
DESCRIPTION: Sample JSON response showing partition information including partition values (columnName and columnValue pairs) and the formatted partition name. The response also includes database and table identifiers.

LANGUAGE: json
CODE:
{
  "partitions": [
    {
      "values": [
        {
          "columnName": "dt",
          "columnValue": "20120101"
        },
        {
          "columnName": "country",
          "columnValue": "US"
        }
      ],
      "name": "dt='20120101',country='US'"
    }
  ],
  "database": "default",
  "table": "my_table"
}

----------------------------------------

TITLE: Generating Histograms by Gender in Apache Hive
DESCRIPTION: This query demonstrates how to use the histogram_numeric() function to generate age histograms grouped by gender from a users table.

LANGUAGE: SQL
CODE:
SELECT histogram_numeric(age) FROM users GROUP BY gender;

----------------------------------------

TITLE: Inserting Data with DEFAULT Constraint in Hive SQL
DESCRIPTION: Shows how INSERT statements will use DEFAULT values when a column is not explicitly specified. If a value is provided, including NULL, it will override the DEFAULT.

LANGUAGE: sql
CODE:
INSERT INTO <tableName>(co1, col3) values(<val1> , <val2>)

LANGUAGE: sql
CODE:
INSERT INTO <tableName>(col1, col2, col3) values (<val1>, <val2>, <val3>)

----------------------------------------

TITLE: JSON Response for WebHCat GetColumn API
DESCRIPTION: This JSON object represents the response from the WebHCat API when describing a column. It includes the database name, table name, and detailed information about the specified column including its name, comment, and data type.

LANGUAGE: json
CODE:
{
 "database": "default",
 "table": "test_table",
 "column": {
   "name": "price",
   "comment": "The unit price",
   "type": "float"
 }
}

----------------------------------------

TITLE: Creating Star Schema Tables for Materialized View Example in Hive
DESCRIPTION: Shows the creation of tables based on the SSB benchmark for the second materialized view example.

LANGUAGE: SQL
CODE:
CREATE TABLE `customer`(
`c_custkey` BIGINT,
`c_name` STRING,
`c_address` STRING,
`c_city` STRING,
`c_nation` STRING,
`c_region` STRING,
`c_phone` STRING,
`c_mktsegment` STRING,
PRIMARY KEY (`c_custkey`) DISABLE RELY)
STORED AS ORC
TBLPROPERTIES ('transactional'='true');

CREATE TABLE `dates`(
`d_datekey` BIGINT,
`d_date` STRING,
`d_dayofweek` STRING,
`d_month` STRING,
`d_year` INT,
`d_yearmonthnum` INT,
`d_yearmonth` STRING,
`d_daynuminweek` INT,
`d_daynuminmonth` INT,
`d_daynuminyear` INT,
`d_monthnuminyear` INT,
`d_weeknuminyear` INT,
`d_sellingseason` STRING,
`d_lastdayinweekfl` INT,
`d_lastdayinmonthfl` INT,
`d_holidayfl` INT,
`d_weekdayfl`INT,
PRIMARY KEY (`d_datekey`) DISABLE RELY)
STORED AS ORC
TBLPROPERTIES ('transactional'='true');

CREATE TABLE `part`(
`p_partkey` BIGINT,
`p_name` STRING,
`p_mfgr` STRING,
`p_category` STRING,
`p_brand1` STRING,
`p_color` STRING,
`p_type` STRING,
`p_size` INT,
`p_container` STRING,
PRIMARY KEY (`p_partkey`) DISABLE RELY)
STORED AS ORC
TBLPROPERTIES ('transactional'='true');

CREATE TABLE `supplier`(
`s_suppkey` BIGINT,
`s_name` STRING,
`s_address` STRING,
`s_city` STRING,
`s_nation` STRING,
`s_region` STRING,
`s_phone` STRING,
PRIMARY KEY (`s_suppkey`) DISABLE RELY)
STORED AS ORC
TBLPROPERTIES ('transactional'='true');

CREATE TABLE `lineorder`(
`lo_orderkey` BIGINT,
`lo_linenumber` int,
`lo_custkey` BIGINT not null DISABLE RELY,
`lo_partkey` BIGINT not null DISABLE RELY,
`lo_suppkey` BIGINT not null DISABLE RELY,
`lo_orderdate` BIGINT not null DISABLE RELY,
`lo_ordpriority` STRING,
`lo_shippriority` STRING,
`lo_quantity` DOUBLE,
`lo_extendedprice` DOUBLE,
`lo_ordtotalprice` DOUBLE,
`lo_discount` DOUBLE,
`lo_revenue` DOUBLE,
`lo_supplycost` DOUBLE,
`lo_tax` DOUBLE,
`lo_commitdate` BIGINT,
`lo_shipmode` STRING,
PRIMARY KEY (`lo_orderkey`) DISABLE RELY,
CONSTRAINT fk1 FOREIGN KEY (`lo_custkey`) REFERENCES `customer_n1`(`c_custkey`) DISABLE RELY,
CONSTRAINT fk2 FOREIGN KEY (`lo_orderdate`) REFERENCES `dates_n0`(`d_datekey`) DISABLE RELY,
CONSTRAINT fk3 FOREIGN KEY (`lo_partkey`) REFERENCES `ssb_part_n0`(`p_partkey`) DISABLE RELY,
CONSTRAINT fk4 FOREIGN KEY (`lo_suppkey`) REFERENCES `supplier_n0`(`s_suppkey`) DISABLE RELY)
STORED AS ORC
TBLPROPERTIES ('transactional'='true');

----------------------------------------

TITLE: Evaluating Boolean XPath Expressions in Apache Hive SQL
DESCRIPTION: Demonstrates the use of xpath_boolean UDF to evaluate XPath expressions that return true/false. Returns true if the expression is true or if a matching node is found.

LANGUAGE: SQL
CODE:
SELECT xpath_boolean ('<a><b>b</b></a>', 'a/b') FROM src LIMIT 1 ;

----------------------------------------

TITLE: CLUSTER BY and DISTRIBUTE BY Examples
DESCRIPTION: Demonstrates various ways to use CLUSTER BY and DISTRIBUTE BY for controlling data distribution and sorting across reducers.

LANGUAGE: sql
CODE:
SELECT col1, col2 FROM t1 CLUSTER BY col1

SELECT col1, col2 FROM t1 DISTRIBUTE BY col1

SELECT col1, col2 FROM t1 DISTRIBUTE BY col1 SORT BY col1 ASC, col2 DESC

----------------------------------------

TITLE: Creating Table with DEFAULT Constraint in Hive SQL
DESCRIPTION: Demonstrates the proposed syntax for creating a table with a DEFAULT constraint on a column. The DEFAULT value can be a literal, date/time function, CURRENT_USER(), NULL, or a CAST expression.

LANGUAGE: sql
CODE:
CREATE TABLE <tableName> (<columnName> <dataType> DEFAULT <defaultValue>)

----------------------------------------

TITLE: Successful Database Description Response
DESCRIPTION: Example JSON response showing successful database description retrieval. Contains database location, parameters, comment, and name.

LANGUAGE: json
CODE:
{
 "location":"hdfs://localhost:9000/warehouse/newdb.db",
 "params":"{a=b}",
 "comment":"Hello there",
 "database":"newdb"
}

----------------------------------------

TITLE: Starting Derby Network Server
DESCRIPTION: Commands to start Derby in network server mode with specific host configuration

LANGUAGE: bash
CODE:
cd /opt/hadoop/db-derby-10.4.1.3-bin/data
nohup /opt/hadoop/db-derby-10.4.1.3-bin/startNetworkServer -h 0.0.0.0 &

----------------------------------------

TITLE: Implementing custom RelOptCost for Hive
DESCRIPTION: Java code snippet showing a custom HiveVolcanoCost class extending VolcanoCost to implement cost comparison logic for Hive query optimization

LANGUAGE: Java
CODE:
Class HiveVolcanoCost extends VolcanoCost {

Double m_sizeOfTuple;

   @Override
   public boolean isLe(RelOptCost other) {
      VolcanoCost that = (VolcanoCost) other;
      if (((this.dCpu + this.dIo) < (that.dCpu + that.dIo))
         || ((this.dCpu + this.dIo) == (that.dCpu + that.dIo)
            && this.dRows <= that.dRows)) {
         return true;
      } else {
         return false;
      }
   }
}

----------------------------------------

TITLE: Creating UDF with JAR Specification
DESCRIPTION: SQL command to create a UDF while specifying the required JAR file location in HDFS.

LANGUAGE: sql
CODE:
CREATE FUNCTION myfunc AS 'myclass' USING JAR 'hdfs:///path/to/jar';

----------------------------------------

TITLE: Directory Structure Setup for Hadoop and Derby
DESCRIPTION: Shows the basic directory structure for Hadoop, Derby, and Hive installation

LANGUAGE: plaintext
CODE:
/opt/hadoop/hadoop-0.17.2.1
/opt/hadoop/db-derby-10.4.1.3-bin
/opt/hadoop/hive

----------------------------------------

TITLE: Derby Environment Configuration
DESCRIPTION: Environment variable settings for Derby installation

LANGUAGE: bash
CODE:
DERBY_INSTALL=/opt/hadoop/db-derby-10.4.1.3-bin
DERBY_HOME=/opt/hadoop/db-derby-10.4.1.3-bin
export DERBY_INSTALL
export DERBY_HOME

----------------------------------------

TITLE: Performing Numeric Operations with XPath in Apache Hive SQL
DESCRIPTION: Shows how to use xpath_int UDF to perform mathematical operations on XML data. Returns an integer result of the XPath expression evaluation.

LANGUAGE: SQL
CODE:
SELECT xpath_int ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'sum(a/*)') FROM src LIMIT 1 ;

----------------------------------------

TITLE: Block Sampling Syntax in Hive SQL
DESCRIPTION: Syntax for block-based sampling which samples based on data size percentage.

LANGUAGE: sql
CODE:
block_sample: TABLESAMPLE (n PERCENT)

----------------------------------------

TITLE: Managing Hive Privileges
DESCRIPTION: SQL commands for granting and revoking specific privileges on Hive objects.

LANGUAGE: sql
CODE:
GRANT
    priv_type [(column_list)]
      [, priv_type [(column_list)]] ...
    [ON object_specification]
    TO principal_specification [, principal_specification] ...
    [WITH GRANT OPTION]

REVOKE [GRANT OPTION FOR]
    priv_type [(column_list)]
      [, priv_type [(column_list)]] ...
    [ON object_specification]
    FROM principal_specification [, principal_specification] ...

----------------------------------------

TITLE: Configuring CredentialProvider in Hive XML
DESCRIPTION: Example of configuring Hive to use a JCEKS keystore file as a CredentialProvider for passwords.

LANGUAGE: xml
CODE:
<!-- Configure credential store for passwords-->
<property>
  <name>hadoop.security.credential.provider.path</name>
  <value>jceks://file/usr/lib/hive/conf/hive.jceks</value>
</property>

----------------------------------------

TITLE: Querying Table Information with JDOQL
DESCRIPTION: Commands to retrieve table column statistics information from the metastore using JDOQL queries. These commands must be run with the correct HIVE_CONF_DIR setting.

LANGUAGE: bash
CODE:
HIVE_CONF_DIR=/etc/hive/conf/conf.server/ hive --service metatool -executeJDOQL 'select dbName+"."+tableName+"::"+colName+"="+numDVs from org.apache.hadoop.hive.metastore.model.MTableColumnStatistics'

LANGUAGE: bash
CODE:
HIVE_CONF_DIR=/etc/hive/conf/conf.server/ hive --service metatool -executeJDOQL 'select dbName+"."+tableName+"("+partitionName+")::"+colName+"="+numDVs from org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics'

----------------------------------------

TITLE: Enabling Correlation Optimizer Configuration
DESCRIPTION: Configuration setting to enable the Correlation Optimizer feature in Hive.

LANGUAGE: sql
CODE:
set hive.optimize.correlation=true;

----------------------------------------

TITLE: MySQL Connection Fix Command
DESCRIPTION: SQL command to fix MySQL connection timeout issues by setting the global wait status

LANGUAGE: SQL
CODE:
set global wait_status=120;

----------------------------------------

TITLE: Configuring Hive for Kudu Integration
DESCRIPTION: Shows how to set the Kudu master addresses in Hive configuration using the hive command. This configuration is used when the kudu.master_addresses table property is not set.

LANGUAGE: bash
CODE:
hive -hiveconf hive.kudu.master.addresses.default=localhost:7051

----------------------------------------

TITLE: Managing Hive Resources
DESCRIPTION: Examples of adding, listing, and deleting resources in Hive, including files, JARs, and archives.

LANGUAGE: sql
CODE:
hive> add FILE /tmp/tt.py;
hive> list FILES;
/tmp/tt.py
hive> select from networks a 
               MAP a.networkid 
               USING 'python tt.py' as nn where a.ds = '2009-01-04' limit 10;

----------------------------------------

TITLE: Row-Based Sampling Syntax in Hive SQL
DESCRIPTION: Syntax for sampling based on row count per split.

LANGUAGE: sql
CODE:
block_sample: TABLESAMPLE (n ROWS)

----------------------------------------

TITLE: Transaction and Mutation Process Steps
DESCRIPTION: The key sequence of steps required to apply mutations using the Streaming Mutation API, from creating a MutatorClient to closing the transaction.

LANGUAGE: java
CODE:
1. Create MutatorClient
2. Open Transaction
3. Get AcidTables
4. Begin transaction
5. Create MutatorCoordinator(s)
6. Compute mutation set
7. Collect affected partitions (optional)
8. Append bucket IDs to insertion records
9. Group and sort data
10. Issue mutation events
11. Close coordinators
12. Abort or commit transaction
13. Close mutation client
14. Create affected partitions (optional)

----------------------------------------

TITLE: JSON Response for Table Properties in WebHCat
DESCRIPTION: This JSON output shows the response structure when retrieving table properties. It includes a list of properties as key-value pairs, along with the database and table names.

LANGUAGE: json
CODE:
{
 "properties": {
   "fruit": "apple",
   "last_modified_by": "ctdean",
   "hcat.osd": "org.apache.hcatalog.rcfile.RCFileOutputDriver",
   "color": "blue",
   "last_modified_time": "1331620706",
   "hcat.isd": "org.apache.hcatalog.rcfile.RCFileInputDriver",
   "transient_lastDdlTime": "1331620706",
   "comment": "Best table made today",
   "country": "Albania"
 },
 "table": "test_table",
 "database": "default"
}

----------------------------------------

TITLE: Creating a Remote Table for Netflix Iceberg in Hive SQL
DESCRIPTION: Example SQL statement for creating a remote table connection to a Netflix Iceberg table.

LANGUAGE: SQL
CODE:
CREATE REMOTE TABLE tbl_name
VIA 'xxx.yyy.iceberg.hive.zzz.IcebergTableHiveClientFactory'
WITH TBLPROPERTIES (
  'iceberg.table.path' = 'an-atomic-store:/tables/tbl_name'
)

----------------------------------------

TITLE: Rebalance Compaction with Custom Bucket Count in Hive SQL
DESCRIPTION: Performs rebalance compaction while specifying the desired number of implicit buckets. This allows controlling the granularity of data distribution.

LANGUAGE: sql
CODE:
ALTER TABLE table_name COMPACT 'REBALANCE' CLUSTERED INTO n BUCKETS;

----------------------------------------

TITLE: Manual Pool Assignment Using ALTER TABLE Command
DESCRIPTION: SQL command to manually assign a compaction request to a specific pool. This assignment overrides any pool settings defined through properties.

LANGUAGE: sql
CODE:
ALTER TABLE COMPACT table_name POOL 'pool_name';

----------------------------------------

TITLE: Successful Response Format for GetTables
DESCRIPTION: Example JSON response showing successful retrieval of table listings from a database

LANGUAGE: json
CODE:
{
 "tables": [
   "my_table",
   "my_table_2",
   "my_table_3"
 ],
 "database": "default"
}

----------------------------------------

TITLE: Converting Existing Table to Iceberg in Hive
DESCRIPTION: Shows how to convert an existing external Hive table to an Iceberg table using the ALTER TABLE statement.

LANGUAGE: SQL
CODE:
ALTER TABLE TABLE1 CONVERT TO ICEBERG TBLPROPERTIES ('format-version'='2');

----------------------------------------

TITLE: JDBC Client Sample for HiveServer2
DESCRIPTION: Java code demonstrating how to connect to HiveServer2 using JDBC, create a table, and execute queries.

LANGUAGE: Java
CODE:
import java.sql.SQLException;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.Statement;
import java.sql.DriverManager;

public class HiveJdbcClient {
  private static String driverName = "org.apache.hive.jdbc.HiveDriver";

  public static void main(String[] args) throws SQLException {
    try {
      Class.forName(driverName);
    } catch (ClassNotFoundException e) {
      e.printStackTrace();
      System.exit(1);
    }
    Connection con = DriverManager.getConnection("jdbc:hive2://localhost:10000/default", "hive", "");
    Statement stmt = con.createStatement();
    String tableName = "testHiveDriverTable";
    stmt.execute("drop table if exists " + tableName);
    stmt.execute("create table " + tableName + " (key int, value string)");
    // show tables
    String sql = "show tables '" + tableName + "'";
    System.out.println("Running: " + sql);
    ResultSet res = stmt.executeQuery(sql);
    if (res.next()) {
      System.out.println(res.getString(1));
    }
    // describe table
    sql = "describe " + tableName;
    System.out.println("Running: " + sql);
    res = stmt.executeQuery(sql);
    while (res.next()) {
      System.out.println(res.getString(1) + "\t" + res.getString(2));
    }
  }
}

----------------------------------------

TITLE: Building Apache Hive Website Locally with CMS Build Scripts
DESCRIPTION: This Perl command builds the Apache Hive website locally using the CMS build scripts. It specifies the source and target directories for the build process.

LANGUAGE: perl
CODE:
buildsite/build_site.pl --source-base hive-site --target-base hive-website

----------------------------------------

TITLE: Querying XML with XPath in Apache Hive SQL
DESCRIPTION: Demonstrates using the xpath UDF to retrieve a list of node text values from an XML string. The function returns an array of strings matching the XPath expression.

LANGUAGE: SQL
CODE:
select xpath('<a><b>b1</b><b>b2</b></a>','a/*/text()') from src limit 1 ;

----------------------------------------

TITLE: Error Response JSON Format
DESCRIPTION: Example of error response when attempting to access a non-existent table through the API.

LANGUAGE: json
CODE:
{
  "error": "Table xtest_table does not exist",
  "errorCode": 404,
  "database": "default",
  "table": "xtest_table"
}

----------------------------------------

TITLE: Using Custom UDF in Hive Query
DESCRIPTION: Example of using the custom lowercase UDF in a Hive SQL query to transform and group data.

LANGUAGE: sql
CODE:
select my_lower(title), sum(freq) from titles group by my_lower(title);

----------------------------------------

TITLE: Unsupported Complex Expression Subqueries
DESCRIPTION: Examples of subqueries that will not be supported due to being part of complex expressions or CASE statements.

LANGUAGE: sql
CODE:
-- subquery in non-simple expression
SELECT 1 + (SELECT SUM(ship_charge) FROM orders), customer.customer_num FROM customer
 
-- subquery in CASE
SELECT CASE WHEN (select count(*) from store_sales 
                  where ss_quantity between 1 and 20) > 409437
            THEN (select avg(ss_ext_list_price) 
                  from store_sales 
                  where ss_quantity between 1 and 20) 
            ELSE (select avg(ss_net_paid_inc_tax)
                  from store_sales
                  where ss_quantity between 1 and 20) end bucket1
FROM reason
WHERE r_reason_sk = 1

----------------------------------------

TITLE: Configuring HiveConf for ExIm on S3
DESCRIPTION: XML configuration for whitelisting S3 URI schemes in Hive's import/export operations when replicating to AWS/EMR/S3 environments.

LANGUAGE: xml
CODE:
  <property>
    <name>hive.exim.uri.scheme.whitelist</name>
    <value>hdfs,s3a</value>
  </property>

----------------------------------------

TITLE: Creating a Druid-Stored Materialized View with Rollup in Hive
DESCRIPTION: Demonstrates the creation of a materialized view 'mv3' that rolls up wiki edit events by minute and stores them in Druid.

LANGUAGE: SQL
CODE:
CREATE MATERIALIZED VIEW mv3
STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'
AS
SELECT floor(time to minute) as `__time`, page,
SUM(characters_added) AS c_added,
SUM(characters_removed) AS c_removed
FROM wiki
GROUP BY floor(time to minute), page;

----------------------------------------

TITLE: Loading Data into Iceberg Table in Hive
DESCRIPTION: Demonstrates how to load data into an Iceberg table from a local file using the LOAD DATA statement.

LANGUAGE: SQL
CODE:
LOAD DATA LOCAL INPATH '/data/files/doctors.avro' OVERWRITE INTO TABLE ice_avro;

----------------------------------------

TITLE: Using Kerberos with Pre-Authenticated Subject in JDBC
DESCRIPTION: Java code snippet showing how to use a pre-authenticated Kerberos subject to connect to HiveServer2.

LANGUAGE: Java
CODE:
static Connection getConnection( Subject signedOnUserSubject ) throws Exception{
       Connection conn = (Connection) Subject.doAs(signedOnUserSubject, new PrivilegedExceptionAction<Object>()
           {
               public Object run()
               {
                       Connection con = null;
                       String JDBC_DB_URL = "jdbc:hive2://HiveHost:10000/default;" ||
                                              "principal=hive/localhost.localdomain@EXAMPLE.COM;" || 
                                              "kerberosAuthType=fromSubject";
                       try {
                               Class.forName(JDBC_DRIVER);
                               con =  DriverManager.getConnection(JDBC_DB_URL);
                       } catch (SQLException e) {
                               e.printStackTrace();
                       } catch (ClassNotFoundException e) {
                               e.printStackTrace();
                       }
                       return con;
               }
           });
       return conn;
}

----------------------------------------

TITLE: Upgrading Hive Schema
DESCRIPTION: Example of upgrading the Hive schema from version 0.10.0 to the current version.

LANGUAGE: bash
CODE:
$ schematool -dbType derby -upgradeSchemaFrom 0.10.0
Metastore connection URL:        jdbc:derby:;databaseName=metastore_db;create=true
Metastore Connection Driver :    org.apache.derby.jdbc.EmbeddedDriver
Metastore connection User:       APP
Starting upgrade metastore schema from version 0.10.0 to 0.13.0
Upgrade script upgrade-0.10.0-to-0.11.0.derby.sql
Completed upgrade-0.10.0-to-0.11.0.derby.sql
Upgrade script upgrade-0.11.0-to-0.12.0.derby.sql
Completed upgrade-0.11.0-to-0.12.0.derby.sql
Upgrade script upgrade-0.12.0-to-0.13.0.derby.sql
Completed upgrade-0.12.0-to-0.13.0.derby.sql
schemaTool completed

----------------------------------------

TITLE: Row-Based Sampling Example in Hive SQL
DESCRIPTION: Example showing how to sample 10 rows per split using row-based sampling.

LANGUAGE: sql
CODE:
SELECT * FROM source TABLESAMPLE(10 ROWS);

----------------------------------------

TITLE: WebHCat API JSON Response
DESCRIPTION: Example JSON response from the WebHCat API showing job ID and execution information

LANGUAGE: json
CODE:
{
 "id": "job_201111101627_0018",
 "info": {
          "stdout": "templeton-job-id:job_201111101627_0018
                    ",
          "stderr": "",
          "exitcode": 0
         }
}

----------------------------------------

TITLE: Connecting to Hive via Thrift in PHP
DESCRIPTION: This PHP script demonstrates how to connect to a Hive server using Thrift, execute a query, and fetch results. It includes the necessary setup for Thrift and shows basic client operations.

LANGUAGE: PHP
CODE:
<?php
// set THRIFT_ROOT to php directory of the hive distribution
$GLOBALS['THRIFT_ROOT'] = '/lib/php/';
// load the required files for connecting to Hive
require_once $GLOBALS['THRIFT_ROOT'] . 'packages/hive_service/ThriftHive.php';
require_once $GLOBALS['THRIFT_ROOT'] . 'transport/TSocket.php';
require_once $GLOBALS['THRIFT_ROOT'] . 'protocol/TBinaryProtocol.php';
// Set up the transport/protocol/client
$transport = new TSocket('localhost', 10000);
$protocol = new TBinaryProtocol($transport);
$client = new ThriftHiveClient($protocol);
$transport->open();

// run queries, metadata calls etc
$client->execute('SELECT * from src');
var_dump($client->fetchAll());
$transport->close();

----------------------------------------

TITLE: Creating Permanent UDF in Hive
DESCRIPTION: SQL command to register a custom UDF as a permanent function in a specific database in Hive.

LANGUAGE: sql
CODE:
create function my_db.my_lower as 'com.example.hive.udf.Lower';

----------------------------------------

TITLE: Error JSON Response for Invalid WebHCat API Version
DESCRIPTION: This JSON output demonstrates an error response from the WebHCat API when an invalid version (v2) is specified in the request URL. It indicates that the requested URI is not valid.

LANGUAGE: json
CODE:
{
  "error": "null for uri: http://localhost:50111/templeton/v2"
}

----------------------------------------

TITLE: Setting up Tez for Hadoop
DESCRIPTION: Commands to download, extract, and configure Tez for use with Hadoop, including HDFS setup.

LANGUAGE: bash
CODE:
wget https://dlcdn.apache.org/tez/0.10.3/apache-tez-0.10.3-bin.tar.gz
tar -xzvf apache-tez-0.10.3-bin.tar.gz
cd apache-tez-0.10.3-bin
tar zcvf ../apache-tez-0.10.3-bin.tar.gz * && cd ..

$HADOOP_HOME/sbin/start-dfs.sh
$HADOOP_HOME/bin/hadoop fs -mkdir -p /apps/tez
$HADOOP_HOME/bin/hadoop fs -put apache-tez-0.10.3-bin.tar.gz /apps/tez
$HADOOP_HOME/bin/hadoop fs -put apache-tez-0.10.3-bin /apps/tez
$HADOOP_HOME/bin/hadoop fs -ls /apps/tez
$HADOOP_HOME/sbin/stop-all.sh

export TEZ_HOME=/yourpathtotez/apache-tez-0.10.3-bin
export HADOOP_CLASSPATH=$TEZ_HOME/*:$TEZ_HOME/conf

----------------------------------------

TITLE: Archive Partition Command Syntax
DESCRIPTION: Generic SQL syntax for archiving a Hive table partition using the ALTER TABLE command.

LANGUAGE: sql
CODE:
ALTER TABLE table_name ARCHIVE PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, ...)

----------------------------------------

TITLE: Dry Run Upgrade for Hive Schema
DESCRIPTION: Example of performing a dry run upgrade to list required scripts without executing them.

LANGUAGE: bash
CODE:
$ build/dist/bin/schematool -dbType derby -upgradeSchemaFrom 0.7.0 -dryRun
Metastore Connection Driver :    org.apache.derby.jdbc.EmbeddedDriver
Metastore connection User:       APP
Starting upgrade metastore schema from version 0.7.0 to 0.13.0
Upgrade script upgrade-0.7.0-to-0.8.0.derby.sql
Upgrade script upgrade-0.8.0-to-0.9.0.derby.sql
Upgrade script upgrade-0.9.0-to-0.10.0.derby.sql
Upgrade script upgrade-0.10.0-to-0.11.0.derby.sql
Upgrade script upgrade-0.11.0-to-0.12.0.derby.sql
Upgrade script upgrade-0.12.0-to-0.13.0.derby.sql
schemaTool completed

----------------------------------------

TITLE: Retrieving Hive Schema Information
DESCRIPTION: This example shows how to get schema information using the schematool command.

LANGUAGE: bash
CODE:
$ schematool -dbType derby -info 
Metastore connection URL:	 jdbc:derby:;databaseName=metastore_db;create=true
Metastore connection Driver :	 org.apache.derby.jdbc.EmbeddedDriver
Metastore connection User:	 APP
Hive distribution version:	 4.0.0-beta-2
Metastore schema version:	 4.0.0-beta-2

----------------------------------------

TITLE: Detailed JSON Response for Job Listing in Apache Hive WebHCat
DESCRIPTION: Example JSON response from the GET /jobs endpoint with fields=*, showing comprehensive job details including status, profile, and user arguments.

LANGUAGE: JSON
CODE:
[{"id":"job_201304291205_0016",
  "detail":{
    "status":{
      "jobACLs":{
        "MODIFY_JOB":{"allAllowed":false,"aclstring":" "},
        "VIEW_JOB":{"allAllowed":false,"aclstring":" "}},
      "runState":2,
      "startTime":1367264912274,
      "schedulingInfo":"NA",
      "failureInfo":"NA",
      "jobPriority":"NORMAL",
      "username":"daijy",
      "jobID":{"id":16,"jtIdentifier":"201304291205"},
      "jobId":"job_201304291205_0016",
      "jobComplete":true},
    "profile":{
      "user":"daijy",
      "jobFile":"hdfs://localhost:8020/Users/daijy/hadoop-1.0.3/tmp/mapred/staging/
          daijy/.staging/job_201304291205_0016/job.xml",
      "url":"http://localhost:50030/jobdetails.jsp?jobid=job_201304291205_0016",
      "queueName":"default",
      "jobName":"word count",
      "jobID":{"id":16,"jtIdentifier":"201304291205"},
      "jobId":"job_201304291205_0016"},
      "id":"job_201304291205_0016",
      "parentId":"job_201304291205_0015",
      "percentComplete":"map 100% reduce 100%",
      "exitValue":0,
      "user":"daijy",
      "callback":"http://daijymacpro.local:57815/templeton/$jobId",
      "completed": "done",
      "userargs" => {
        "callback"  => null,
        "define"    => [],
        "enablelog" => "false",
        "execute"   => "select a,rand(b) from mynums",
        "file"      => null,
        "files"     => [],
        "statusdir" => null,
        "user.name" => "hadoopqa",
      },
    }]

----------------------------------------

TITLE: Enabling Vectorized Execution in Hive SQL
DESCRIPTION: SQL commands to enable and disable vectorized query execution in Hive.

LANGUAGE: sql
CODE:
set hive.vectorized.execution.enabled = true;

----------------------------------------

TITLE: Creating Hive Database and User in TiDB
DESCRIPTION: SQL commands to create a database for Hive Metastore, create a user, and grant privileges in TiDB.

LANGUAGE: SQL
CODE:
-- Create a database for Hive Metastore.
create database hive;
-- Create a user and password for Hive Metastore.
create user 'hive'@'%' identified by '123456';
-- Grant privileges to the user.
grant all privileges on hive.* to 'hive'@'%' identified by '123456';
-- Flush privileges.
flush privileges;

----------------------------------------

TITLE: Illustrating String Literals in Apache Hive SQL
DESCRIPTION: This snippet demonstrates how string literals can be expressed in Hive using either single quotes or double quotes. Hive uses C-style escaping within the strings.

LANGUAGE: SQL
CODE:
'Single quoted string'
"Double quoted string"

----------------------------------------

TITLE: Deleting Hadoop Job Using cURL
DESCRIPTION: Example cURL command demonstrating how to delete a Hadoop job by its ID using the WebHCat REST API.

LANGUAGE: bash
CODE:
% curl -s -X DELETE 'http://localhost:50111/templeton/v1/jobs/job_201111111311_0009?user.name=ctdean'

----------------------------------------

TITLE: Starting Hive Metastore Server
DESCRIPTION: Commands to start the Hive metastore Thrift server, showing both the modern and legacy methods

LANGUAGE: bash
CODE:
hive --service metastore

LANGUAGE: bash
CODE:
$JAVA_HOME/bin/java  -Xmx1024m -Dlog4j.configuration=file://$HIVE_HOME/conf/hms-log4j.properties -Djava.library.path=$HADOOP_HOME/lib/native/Linux-amd64-64/ -cp $CLASSPATH org.apache.hadoop.hive.metastore.HiveMetaStore

----------------------------------------

TITLE: Archive Partition Example
DESCRIPTION: Practical example of archiving a specific partition in a Hive table using date and hour partitions.

LANGUAGE: sql
CODE:
ALTER TABLE srcpart ARCHIVE PARTITION(ds='2008-04-08', hr='12')

----------------------------------------

TITLE: Configuring Delete Modes for Iceberg Table in Hive
DESCRIPTION: Demonstrates how to set Copy-on-Write mode for delete, update, and merge operations on an Iceberg table using table properties.

LANGUAGE: SQL
CODE:
CREATE TABLE tbl_x (id int) STORED BY ICEBERG TBLPROPERTIES (
    'write.delete.mode'='copy-on-write',
    'write.update.mode'='copy-on-write',
    'write.merge.mode'='copy-on-write'
);

----------------------------------------

TITLE: Creating partitioned external Hive table over S3 data
DESCRIPTION: Shows how to create a partitioned external Hive table over S3 data and add a partition. This example uses a nested directory structure in S3.

LANGUAGE: sql
CODE:
create external table pkv (key int, values string) partitioned by (insertdate string);
alter table pkv add partition (insertdate='2008-01-01') location 's3n://data.s3ndemo.hive/pkv/2008-01-01';

----------------------------------------

TITLE: Writing Data with HCatWriter in Java
DESCRIPTION: This snippet illustrates how to use HCatWriter to write data on slave nodes.

LANGUAGE: Java
CODE:
HCatWriter writer = DataTransferFactory.getHCatWriter(context);
writer.write(hCatRecordItr);

----------------------------------------

TITLE: Obtaining HCatReader Instance in Java
DESCRIPTION: This code shows how to obtain an instance of HCatReader using a ReadEntity and cluster configuration.

LANGUAGE: Java
CODE:
HCatReader reader = DataTransferFactory.getHCatReader(entity, config);

----------------------------------------

TITLE: CAST Format Syntax
DESCRIPTION: Basic syntax for using CAST with FORMAT option for datetime conversions in Hive.

LANGUAGE: sql
CODE:
CAST(<timestamp/date> AS <varchar/char/string> [FORMAT <template>])  
CAST(<varchar/char/string> AS <timestamp/date> [FORMAT <template>])

----------------------------------------

TITLE: Deleting a Job with cURL in WebHCat API
DESCRIPTION: Example cURL command to delete a job using the WebHCat API. It sends a DELETE request to the queue/:jobid endpoint with the job ID and user name as parameters.

LANGUAGE: bash
CODE:
% curl -s -X DELETE 'http://localhost:50111/templeton/v1/queue/job_201111111311_0009?user.name=ctdean'

----------------------------------------

TITLE: Creating Druid Table with Kafka Ingestion
DESCRIPTION: Creates a Hive table that will ingest data from Kafka into Druid with specified ingestion properties.

LANGUAGE: SQL
CODE:
CREATE EXTERNAL TABLE druid_kafka_table_1(`__time` timestamp,`dimension1` string, `dimension1` string, `metric1` int, `metric2` double ....)
        STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'
        TBLPROPERTIES (
        "kafka.bootstrap.servers" = "localhost:9092",
        "kafka.topic" = "topic1",
        "druid.kafka.ingestion.useEarliestOffset" = "true",
        "druid.kafka.ingestion.maxRowsInMemory" = "5",
        "druid.kafka.ingestion.startDelay" = "PT1S",
        "druid.kafka.ingestion.period" = "PT1S",
        "druid.kafka.ingestion.consumer.retries" = "2"
        );

----------------------------------------

TITLE: EXPLAIN AST Query Example in Hive
DESCRIPTION: Demonstrates using the AST clause with EXPLAIN to output the query's Abstract Syntax Tree.

LANGUAGE: SQL
CODE:
EXPLAIN AST
FROM src INSERT OVERWRITE TABLE dest_g1 SELECT src.key, sum(substr(src.value,4)) GROUP BY src.key;

----------------------------------------

TITLE: Unarchive Partition Example
DESCRIPTION: Example command showing how to unarchive a previously archived partition to restore original files.

LANGUAGE: sql
CODE:
ALTER TABLE srcpart UNARCHIVE PARTITION(ds='2008-04-08', hr='12')

----------------------------------------

TITLE: Running Hive Integration Tests
DESCRIPTION: Commands to navigate to integration tests directory and run tests with Iceberg support.

LANGUAGE: shell
CODE:
mvn clean test -pl itest -Piceberg

----------------------------------------

TITLE: Basic SELECT Subquery Example
DESCRIPTION: Example of a basic subquery in SELECT statement that is currently not supported in Hive but planned for implementation.

LANGUAGE: sql
CODE:
SELECT customer.customer_num,
	(SELECT SUM(ship_charge) 
		FROM orders
		WHERE customer.customer_num = orders.customer_num
	) AS total_ship_chg
FROM customer

----------------------------------------

TITLE: WebHCat Database Creation API Response
DESCRIPTION: Example JSON response returned by the WebHCat API after successful database creation, containing the created database name.

LANGUAGE: json
CODE:
{
 "database":"newdb"
}

----------------------------------------

TITLE: Creating external Hive table over S3 data
DESCRIPTION: Demonstrates creating an external Hive table that references data stored in S3. This example creates a simple key-value table.

LANGUAGE: sql
CODE:
create external table kv (key int, values string)  location 's3n://data.s3ndemo.hive/kv';

----------------------------------------

TITLE: Storage Configuration Properties
DESCRIPTION: Properties for controlling storage behavior in HCatalog including external location specification and dynamic partitioning patterns.

LANGUAGE: properties
CODE:
hcat.pig.storer.external.location=not set
hcat.dynamic.partitioning.custom.pattern=not set
hcat.append.limit=not set
hcat.input.ignore.invalid.path=false

----------------------------------------

TITLE: Running HWI as Background Process
DESCRIPTION: Command to run the Hive Web Interface as a background process with output redirection.

LANGUAGE: bash
CODE:
nohup bin/hive --service hwi > /dev/null 2> /dev/null &

----------------------------------------

TITLE: Constructing JDBC Connection URLs for SAML Authentication in Hive
DESCRIPTION: Examples of JDBC connection URLs for Hive using SAML authentication, including browser-based and token-based authentication modes.

LANGUAGE: java
CODE:
jdbc:hive2://HiveServer2-host:10001/default;transportMode=http;httpPath=cliservice;auth=browser

LANGUAGE: java
CODE:
jdbc:hive2://HiveServer2-host:10001/default;transportMode=http;httpPath=cliservice;auth=token;token=<token_string>

LANGUAGE: java
CODE:
jdbc:hive2://HiveServer2-host:10001/default;transportMode=http;httpPath=cliservice;auth=browser;samlResponsePort=12345;samlResponseTimeout=120

----------------------------------------

TITLE: Creating a Remote Table in Hive SQL
DESCRIPTION: Example SQL statement for creating a remote table in Hive, specifying the local and remote table names, connection method, and configuration properties.

LANGUAGE: SQL
CODE:
CREATE REMOTE TABLE local_db.local_tbl
CONNECTED TO remote_db.remote_tbl
VIA 'org.apache.hadoop.hive.metastore.ThriftHiveMetastoreClientFactory'
WITH TBLPROPERTIES (
  'hive.metastore.uris' = 'thrift://remote-hms:9083'
);

----------------------------------------

TITLE: Creating a Table with Binary Column in Hive SQL
DESCRIPTION: This SQL snippet demonstrates how to create a table in Hive with a binary column. It shows the syntax for declaring a column of type 'binary' alongside a string column.

LANGUAGE: sql
CODE:
create table binary_table (a string, b binary);

----------------------------------------

TITLE: HCatalog Environment Setup
DESCRIPTION: Demonstrates how to invoke the HCatalog CLI by setting the required environment variables and paths.

LANGUAGE: bash
CODE:
HIVE_HOME=hive_home hcat_home/bin/hcat

----------------------------------------

TITLE: Successful JSON Response for Table Rename
DESCRIPTION: Example JSON output returned by the WebHCat API upon successful renaming of a Hive table.

LANGUAGE: json
CODE:
{
 "table": "test_table_2",
 "database": "default"
}

----------------------------------------

TITLE: Implementing ScanPlan for HBase Query Execution
DESCRIPTION: Class representing a single HBase scan operation with start/end markers and filters. Includes implementation of AND/OR operations for combining scan plans.

LANGUAGE: java
CODE:
ScanPlan extends FilterPlan {
    private ScanMarker startMarker;
    private ScanMarker endMarker;
    private ScanFilter filter;

    public FilterPlan and(FilterPlan other) {
        // calls this.and(otherScanPlan) on each scan plan in other
    }

    private ScanPlan and(ScanPlan other) {
        // combines start marker and end marker and filters of this and other
    }

    public FilterPlan or(FilterPlan other) {
        // just create a new FilterPlan from other, with this additional plan
    }
}

----------------------------------------

TITLE: Defining Column Statistics Thrift Structs
DESCRIPTION: Thrift struct definitions for various column statistics data types, including boolean, numeric, string, and date statistics.

LANGUAGE: Thrift
CODE:
struct BooleanColumnStatsData {  
 1: required i64 numTrues,  
 2: required i64 numFalses,  
 3: required i64 numNulls  
 }

struct DoubleColumnStatsData {  
 1: required double lowValue,  
 2: required double highValue,  
 3: required i64 numNulls,  
 4: required i64 numDVs,

5: optional string bitVectors

}

struct LongColumnStatsData {  
 1: required i64 lowValue,  
 2: required i64 highValue,  
 3: required i64 numNulls,  
 4: required i64 numDVs,

5: optional string bitVectors  
 }

struct StringColumnStatsData {  
 1: required i64 maxColLen,  
 2: required double avgColLen,  
 3: required i64 numNulls,  
 4: required i64 numDVs,

5: optional string bitVectors  
 }

struct BinaryColumnStatsData {  
 1: required i64 maxColLen,  
 2: required double avgColLen,  
 3: required i64 numNulls  
 }

struct Decimal {  
1: required binary unscaled,  
3: required i16 scale  
}

struct DecimalColumnStatsData {  
1: optional Decimal lowValue,  
2: optional Decimal highValue,  
3: required i64 numNulls,  
4: required i64 numDVs,  
5: optional string bitVectors  
}

struct Date {  
1: required i64 daysSinceEpoch  
}

struct DateColumnStatsData {  
1: optional Date lowValue,  
2: optional Date highValue,  
3: required i64 numNulls,  
4: required i64 numDVs,  
5: optional string bitVectors  
}

----------------------------------------

TITLE: UDAF Resolver Type Checking
DESCRIPTION: Demonstrates type checking implementation in the resolver class to validate input parameters

LANGUAGE: Java
CODE:
public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info) throws SemanticException {
  TypeInfo [] parameters = info.getParameters();
  if (parameters.length != 2) {
    throw new UDFArgumentTypeException(parameters.length - 1,
        "Please specify exactly two arguments.");
  }
  
  // validate the first parameter, which is the expression to compute over
  if (parameters[0].getCategory() != ObjectInspector.Category.PRIMITIVE) {
    throw new UDFArgumentTypeException(0,
        "Only primitive type arguments are accepted but "
        + parameters[0].getTypeName() + " was passed as parameter 1.");
  }
  switch (((PrimitiveTypeInfo) parameters[0]).getPrimitiveCategory()) {
  case BYTE:
  case SHORT:
  case INT:
  case LONG:
  case FLOAT:
  case DOUBLE:
    break;
  case STRING:
  case BOOLEAN:
  default:
    throw new UDFArgumentTypeException(0,
        "Only numeric type arguments are accepted but "
        + parameters[0].getTypeName() + " was passed as parameter 1.");
  }

  // validate the second parameter, which is the number of histogram bins
  if (parameters[1].getCategory() != ObjectInspector.Category.PRIMITIVE) {
    throw new UDFArgumentTypeException(1,
        "Only primitive type arguments are accepted but "
        + parameters[1].getTypeName() + " was passed as parameter 2.");
  }
  if( ((PrimitiveTypeInfo) parameters[1]).getPrimitiveCategory()
      != PrimitiveObjectInspector.PrimitiveCategory.INT) {
    throw new UDFArgumentTypeException(1,
        "Only an integer argument is accepted as parameter 2, but "
        + parameters[1].getTypeName() + " was passed instead.");
  }
  return new GenericUDAFHistogramNumericEvaluator();
}

----------------------------------------

TITLE: Uploading Pig Archive to HDFS
DESCRIPTION: This command uploads a Pig tar.gz file to HDFS. It's necessary for making Pig available in the Hadoop distributed cache for WebHCat to use.

LANGUAGE: bash
CODE:
hadoop fs -put /tmp/pig-0.11.1.tar.gz /apps/templeton/pig-0.11.1.tar.gz

----------------------------------------

TITLE: Spatial Join Example with Results
DESCRIPTION: Complete example of spatial join query between two tables using st_intersects predicate, including the command and sample output.

LANGUAGE: HQL
CODE:
SELECT ta.rec_id, tb.rec_id FROM ta JOIN tb ON (st_intersects(ta.outline, tb.outline) = TRUE);

----------------------------------------

TITLE: Creating a Remote Database in Hive SQL
DESCRIPTION: SQL statement for creating a remote database connection in Hive, allowing access to all tables within the remote database.

LANGUAGE: SQL
CODE:
CREATE REMOTE DATABASE local_db_name
CONNECTED TO remote_db_name
VIA 'org.apache.hadoop.hive.metastore.ThriftHiveMetastoreClientFactory'
WITH DBPROPERTIES (
  'hive.metastore.uris' = 'thrift://remote-hms:9083'
);

----------------------------------------

TITLE: Defining gRPC Service in Protobuf for Hive Metastore
DESCRIPTION: Example of defining a gRPC service for Hive Metastore using Protocol Buffers. It shows a simple getTable method definition.

LANGUAGE: protobuf
CODE:
service HiveMetaStoreGrpc {
    rpc getTable(Table) returns (GetTableResponse);
}

----------------------------------------

TITLE: Hive LZO Compression Settings
DESCRIPTION: Configuration parameters for enabling LZO compression in Hive queries.

LANGUAGE: sql
CODE:
SET mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.LzoCodec
SET hive.exec.compress.output=true
SET mapreduce.output.fileoutputformat.compress=true

----------------------------------------

TITLE: Creating Clustered Hive Table - Advanced Example
DESCRIPTION: Example of creating a Hive table with clustering, custom SerDe, and advanced formatting options using the WebHCat REST API.

LANGUAGE: bash
CODE:
% curl -s -X PUT -HContent-type:application/json -d '{
  "comment": "Best table made today",
  "columns": [
    { "name": "id", "type": "bigint"},
    { "name": "price", "type": "float", "comment": "The unit price" } ],
  "partitionedBy": [
    { "name": "country", "type": "string" } ],
  "clusteredBy": {
    "columnNames": ["id"],
    "sortedBy": [
      { "columnName": "id", "order": "ASC" } ],
    "numberOfBuckets": 10 },
  "format": {
    "storedAs": "rcfile",
    "rowFormat": {
      "fieldsTerminatedBy": "\u0001",
      "serde": {
        "name": "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe",
        "properties": {
          "key": "value" } } } }
  } ' \
  'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table_c?user.name=ctdean'

----------------------------------------

TITLE: Setting Python Path Environment Variable
DESCRIPTION: Example of setting PYTHONPATH to include locally installed Python modules

LANGUAGE: bash
CODE:
export PYTHONPATH="${PYTHONPATH}:${HOME}/.python_modules/lib/python2.6/site-packages"

----------------------------------------

TITLE: Building Hive Test Plugin with Ant
DESCRIPTION: Ant command to build the test plugin, which creates a jar file and metadata in the build directory.

LANGUAGE: bash
CODE:
ant -Dhive.install.dir=../..

----------------------------------------

TITLE: JSON Response for Extended Job Listing in Apache Hive WebHCat
DESCRIPTION: Example JSON response from the GET /jobs endpoint with showall=true, showing multiple job IDs.

LANGUAGE: JSON
CODE:
[
{"id":"job_201304291205_0014","detail":null},
{"id":"job_201111111311_0015","detail":null},
]

----------------------------------------

TITLE: Altering Metastore Tables for View Support
DESCRIPTION: SQL statements to alter the TBLS table in the Hive metastore to add columns for storing view definitions and table types.

LANGUAGE: SQL
CODE:
ALTER TABLE TBLS ADD COLUMN VIEW_ORIGINAL_TEXT MEDIUMTEXT;
ALTER TABLE TBLS ADD COLUMN VIEW_EXPANDED_TEXT MEDIUMTEXT;
ALTER TABLE TBLS ADD COLUMN TBL_TYPE VARCHAR(128);

----------------------------------------

TITLE: Post-0.8.0 Execution Plan
DESCRIPTION: Explain plan output after Hive 0.8.0 showing filter pushdown to both tables due to join condition transitivity

LANGUAGE: text
CODE:
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        invites
          TableScan
            alias: invites
            Filter Operator
              predicate:
                  expr: (ds = '2011-01-01')
                  type: boolean
              Reduce Output Operator
                key expressions:
                      expr: ds
                      type: string
                sort order: +
                Map-reduce partition columns:
                      expr: ds
                      type: string
                tag: 0
                value expressions:
                      expr: ds
                      type: string
        invites2
          TableScan
            alias: invites2
            Filter Operator
              predicate:
                  expr: (ds = '2011-01-01')
                  type: boolean
              Reduce Output Operator
                key expressions:
                      expr: ds
                      type: string
                sort order: +
                Map-reduce partition columns:
                      expr: ds
                      type: string
                tag: 1

----------------------------------------

TITLE: Using Hive Variables with Command Line Options
DESCRIPTION: Shows how to set and use Hive variables using the --hiveconf option, which is more efficient for multiple operations.

LANGUAGE: bash
CODE:
$ bin/hive --hiveconf a=b -e 'set a; set hiveconf:a; \
create table if not exists b (col int); describe ${hiveconf:a}'

----------------------------------------

TITLE: Retrieving Extended Table Description with cURL
DESCRIPTION: Example of an API call to retrieve extended table information including partitioning details and storage formats.

LANGUAGE: curl
CODE:
curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table?user.name=ctdean&format=extended'

----------------------------------------

TITLE: Table Property Update Response - JSON
DESCRIPTION: Example JSON response from the WebHCat API after successfully setting a table property, showing the database, table, and property names

LANGUAGE: json
CODE:
{
 "property": "fruit",
 "table": "test_table",
 "database": "default"
}

----------------------------------------

TITLE: Simple Filter Expression Example
DESCRIPTION: Example showing the string representation of a filter condition used in Hive's filter pushdown mechanism.

LANGUAGE: sql
CODE:
((key >= 100) and (key < 200))

----------------------------------------

TITLE: Implementing getTable Method in gRPC Server for Hive Metastore
DESCRIPTION: Sample implementation of the getTable method in the gRPC server, showing how to translate between gRPC and Thrift requests/responses.

LANGUAGE: java
CODE:
// Returns gRPC Response, and takes in gRPC GetTableRequest
public GetTableResponse getTable(GetTableRequest grpcTableRequest) {
  // convertToThriftRequest is implemented by a different library, not in Hive code
  Table thriftGetTableRequest = convertToThriftRequest(grpcTableRequest);
  // Result is a Thrift API object    
  GetTableResult result = HiveMetaStoreThriftServer.get_table(thriftTable);
  return convertToThriftResponse(result);
}

----------------------------------------

TITLE: Running Hive Unit Tests
DESCRIPTION: Example commands showing how to run Hive unit tests with different options using Ant

LANGUAGE: bash
CODE:
ant package test
ant test -Dtestcase=TestCliDriver
ant test -Dtestcase=TestCliDriver -Dqfile=groupby1.q

----------------------------------------

TITLE: File Movement Commands in SVN
DESCRIPTION: Example commands for moving files in SVN including file renaming and content replacement

LANGUAGE: bash
CODE:
$ svn mv MyCLass.java MyClass.java
$ perl -i -pe 's<at:var at:name="MyCLass" />MyClass@g' MyClass.java

----------------------------------------

TITLE: Disabling Hive Variable Substitution
DESCRIPTION: Shows how to disable variable substitution in Hive if it causes issues with existing scripts.

LANGUAGE: sql
CODE:
set hive.variable.substitute=false;

----------------------------------------

TITLE: Implementing IStatsAggregator Interface in Java
DESCRIPTION: Java interface definition for aggregating Top K statistics in Hive, adding the aggregateStatsTopK method to read multiple entries from temporary storage.

LANGUAGE: java
CODE:
public interface IStatsAggregator {
  /**
 * This method aggregates top K statistics.
   *
 * */
  public List<String> aggregateStatsTopK(String keyPrefix, String statType);
}

----------------------------------------

TITLE: Upgrading Hive Schema from Specific Version
DESCRIPTION: This example shows how to upgrade the Hive schema from version 3.1.0 to the latest version using Derby as the database.

LANGUAGE: bash
CODE:
$ schematool -dbType derby -upgradeSchemaFrom 3.1.0 Upgrading from the user input version 3.1.0
Metastore connection URL:	 jdbc:derby:;databaseName=metastore_db;create=true
Metastore connection Driver :	 org.apache.derby.jdbc.EmbeddedDriver
Metastore connection User:	 APP
Starting upgrade metastore schema from version 3.1.0 to 4.0.0-beta-2
Upgrade script upgrade-3.1.0-to-3.2.0.derby.sql
Completed upgrade-3.1.0-to-3.2.0.derby.sql
...
Completed upgrade-4.0.0-beta-1-to-4.0.0-beta-2.derby.sql

----------------------------------------

TITLE: Disabling Directory Storage for List Bucketing in Hive SQL
DESCRIPTION: SQL syntax for disabling the 'stored as directories' option for list bucketing, while keeping the table skewed.

LANGUAGE: SQL
CODE:
ALTER TABLE <T> (SCHEMA) NOT STORED AS DIRECTORIES;

----------------------------------------

TITLE: Explain Plan Output for Vectorized Query
DESCRIPTION: Sample explain plan output showing vectorized execution details and execution stages.

LANGUAGE: plaintext
CODE:
STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        alltypesorc
          TableScan
            alias: vectorizedtable
             Statistics: Num rows: 1 Data size: 95 Basic stats: COMPLETE Column stats: COMPLETE
            Select Operator
              Statistics: Num rows: 1 Data size: 95 Basic stats: COMPLETE Column stats: COMPLETE
              Group By Operator
                aggregations: count()
                mode: hash
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  sort order: 
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                  value expressions: _col0 (type: bigint)
      Execution mode: vectorized
      Reduce Operator Tree:
        Group By Operator
          aggregations: count(VALUE._col0)
          mode: mergepartial
          outputColumnNames: _col0
          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
          File Output Operator
            compressed: false
            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
            table:
                input format: org.apache.hadoop.mapred.TextInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

----------------------------------------

TITLE: Plugin Build Configuration
DESCRIPTION: Ant build file configuration for a Hive plugin, defining plugin properties and importing PDK build script.

LANGUAGE: xml
CODE:
<project name="pdktest" default="package">
  <property name="plugin.libname" value="pdk-test-udf"/>
  <property name="plugin.title" value="Hive PDK Test UDF Library"/>
  <property name="plugin.version" value="0.1"/>
  <property name="plugin.vendor" value="Apache Software Foundation"/>
  <property name="function.sql.prefix" value="tp_"/>
  <import file="${hive.install.dir}/scripts/pdk/build-plugin.xml"/>
</project>

----------------------------------------

TITLE: Initializing gRPC Client for Hive Metastore
DESCRIPTION: Example of creating and using a gRPC client to communicate with the Hive Metastore gRPC server.

LANGUAGE: java
CODE:
ManagedChannel channel = ManagedChannelBuilder.forTarget(target)
    // Channels are secure by default (via SSL/TLS). For the example we disable TLS to avoid
    // needing certificates.
    .usePlaintext()
    .build();
HiveMetaStoreGrpc.HiveMetastore BlockingStub blockingStub = HiveMetaStoreGrpc.newBlockingStub(channel);
blockingStub.getTable(getTableRequest);

----------------------------------------

TITLE: Describing Remote Table in Apache Hive
DESCRIPTION: This snippet demonstrates how to describe a remote table using the HIVEJDBC connector. It shows the table structure and properties of the remote table.

LANGUAGE: SQL
CODE:
USE hiveserver_remote;
describe formatted test_emr_tbl;

----------------------------------------

TITLE: Bitmap Index Operations in Hive
DESCRIPTION: Demonstrates bitmap index creation with deferred rebuild, rebuilding, and management operations.

LANGUAGE: hql
CODE:
CREATE INDEX table03_index ON TABLE table03 (column4) AS 'BITMAP' WITH DEFERRED REBUILD;
ALTER INDEX table03_index ON table03 REBUILD;
SHOW FORMATTED INDEX ON table03;
DROP INDEX table03_index ON table03;

----------------------------------------

TITLE: Storing Data to Specific Partitions in Pig
DESCRIPTION: This Pig script demonstrates how to store data into specific partitions using HCatStorer. It splits the data based on region and stores each partition separately.

LANGUAGE: pig
CODE:
A = load 'raw' using HCatLoader(); 
... 
split Z into for_us if region='us', for_eu if region='eu', for_asia if region='asia'; 
store for_us into 'processed' using HCatStorer("ds=20110110, region=us"); 
store for_eu into 'processed' using HCatStorer("ds=20110110, region=eu"); 
store for_asia into 'processed' using HCatStorer("ds=20110110, region=asia"); 

----------------------------------------

TITLE: REGEX Column Specification in HiveQL
DESCRIPTION: Shows how to use regex-based column specification for selecting columns matching a pattern.

LANGUAGE: HiveQL
CODE:
SELECT `(ds|hr)?+.+` FROM sales

----------------------------------------

TITLE: Derby JAR File Installation
DESCRIPTION: Commands for copying required Derby JAR files to Hive and Hadoop directories

LANGUAGE: bash
CODE:
cp /opt/hadoop/db-derby-10.4.1.3-bin/lib/derbyclient.jar /opt/hadoop/hive/lib
cp /opt/hadoop/db-derby-10.4.1.3-bin/lib/derbytools.jar /opt/hadoop/hive/lib

----------------------------------------

TITLE: Setting Custom Locations for List Bucketing in Hive SQL
DESCRIPTION: SQL syntax for specifying custom storage locations for list bucketing keys.

LANGUAGE: SQL
CODE:
ALTER TABLE <T> (SCHEMA) SET SKEWED LOCATION (key1="loc1", key2="loc2");

----------------------------------------

TITLE: Checking WebHCat Status with cURL
DESCRIPTION: This snippet demonstrates how to check the status of a locally installed WebHCat server using cURL. It sends a GET request to the status endpoint and receives a JSON response indicating the server status and version.

LANGUAGE: bash
CODE:
% curl -i http://localhost:50111/templeton/v1/status

----------------------------------------

TITLE: Setting Map Join Memory Usage Threshold in Hive SQL
DESCRIPTION: This SQL command sets the maximum memory usage threshold for the MapJoin local task. The value is a decimal between 0 and 1, representing the percentage of available memory.

LANGUAGE: sql
CODE:
set hive.mapjoin.localtask.max.memory.usage = 0.999;

----------------------------------------

TITLE: JSON Response for WebHCat Partition Creation
DESCRIPTION: This JSON object represents the response from the WebHCat API after successfully creating a partition. It includes the partition name, table name, and database name.

LANGUAGE: json
CODE:
{
 "partition": "country='algeria'",
 "table": "test_table",
 "database": "default"
}

----------------------------------------

TITLE: Publishing Apache Hive Website using CMS CLI
DESCRIPTION: This command uses curl to download and execute the Apache CMS CLI script for publishing the Hive website. It requires the user's Apache ID for authentication.

LANGUAGE: bash
CODE:
curl -sL http://s.apache.org/cms-cli | perl

----------------------------------------

TITLE: Running Unit Tests
DESCRIPTION: Commands to build Hive and run specific unit tests

LANGUAGE: bash
CODE:
mvn clean install -DskipTests
mvn test -Dtest=SomeTest

----------------------------------------

TITLE: Launching Hive CLI in Bash
DESCRIPTION: Command to start the Hive Command Line Interface (CLI) from the Hive home directory.

LANGUAGE: bash
CODE:
$ bin/hive

----------------------------------------

TITLE: Current Database Query in HiveQL
DESCRIPTION: Demonstrates how to query the current database name using the current_database() function.

LANGUAGE: HiveQL
CODE:
SELECT current_database()

----------------------------------------

TITLE: Setting and Using Hive Variables in Bash
DESCRIPTION: Demonstrates how to set a variable in bash and use it in a Hive query. This method is inefficient for multiple operations due to Hive's startup time.

LANGUAGE: bash
CODE:
$ a=b
$ hive -e " describe $a "

----------------------------------------

TITLE: Querying LLAP Peers via REST API
DESCRIPTION: Example of querying the LLAP peers endpoint to get information about cluster nodes including identity, ports, and resources.

LANGUAGE: json
CODE:
{
  "dynamic" : true,
  "identity" : "718264f1-722e-40f1-8265-ac25587bf336",
  "peers" : [ 
 {
    "identity" : "940d6838-4dd7-4e85-95cc-5a6a2c537c04",
    "host" : "sandbox121.hortonworks.com",
    "management-port" : 15004,
    "rpc-port" : 15001,
    "shuffle-port" : 15551,
    "resource" : {
      "vcores" : 24,
      "memory" : 128000
    },
    "host" : "sandbox121.hortonworks.com"
  }, 
]
}

----------------------------------------

TITLE: CTE in Views, CTAS, and Insert Statements
DESCRIPTION: Shows how to use CTEs in different SQL operations including INSERT statements, CREATE TABLE AS SELECT (CTAS), and CREATE VIEW statements with handling of name collisions.

LANGUAGE: sql
CODE:
-- insert example
create table s1 like src;
with q1 as ( select key, value from src where key = '5')
from q1
insert overwrite table s1
select *;

-- ctas example
create table s2 as
with q1 as ( select key from src where key = '4')
select * from q1;

-- view example
create view v1 as
with q1 as ( select key from src where key = '5')
select * from q1;
select * from v1;
 
-- view example, name collision
create view v1 as
with q1 as ( select key from src where key = '5')
select * from q1;
with q1 as ( select key from src where key = '4')
select * from v1;

----------------------------------------

TITLE: Successful DDL Response
DESCRIPTION: Example JSON response from a successful DDL command showing the stdout containing table list, stderr with execution logs, and exitcode of 0 indicating success.

LANGUAGE: json
CODE:
{
 "stdout": "important_table\n            my_other_table\n            my_table\n            my_table_2\n            pokes\n            ",
 "stderr": "WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated...\n            Hive history file=/tmp/ctdean/hive_job_log_ctdean_201111111258_2117356679.txt\n            OK\n            Time taken: 1.202 seconds\n            ",
 "exitcode": 0
}

----------------------------------------

TITLE: Displaying HWI Service Help
DESCRIPTION: Command to display help information for the Hive Web Interface service.

LANGUAGE: bash
CODE:
bin/hive --service hwi --help

----------------------------------------

TITLE: IndexSearchCondition Class Implementation
DESCRIPTION: Support class for representing search conditions in index-based filtering, containing column references, comparison operators, and constant values.

LANGUAGE: java
CODE:
public class IndexSearchCondition
{
  /**
 * Constructs a search condition, which takes the form
 * <pre>column-ref comparison-op constant-value</pre>.
   *
 * @param columnDesc column being compared
   *
 * @param comparisonOp comparison operator, e.g. "="
 * (taken from GenericUDFBridge.getUdfName())
   *
 * @param constantDesc constant value to search for
   *
 * @Param comparisonExpr the original comparison expression
   */
  public IndexSearchCondition(
    ExprNodeColumnDesc columnDesc,
    String comparisonOp,
    ExprNodeConstantDesc constantDesc,
    ExprNodeDesc comparisonExpr);
}

----------------------------------------

TITLE: Simple Table Query in HiveQL
DESCRIPTION: Shows how to retrieve all columns and rows from a table using the basic SELECT statement.

LANGUAGE: HiveQL
CODE:
SELECT * FROM t1

----------------------------------------

TITLE: Starting HWI Service with Ant
DESCRIPTION: Command to start the Hive Web Interface service using Apache Ant library.

LANGUAGE: bash
CODE:
export ANT_LIB=/opt/ant/lib
bin/hive --service hwi

----------------------------------------

TITLE: Using RCFileCat for Data Reading in Hive
DESCRIPTION: Command line usage for reading and displaying data from RC files. Supports options for specifying start offset, length of data to read, and verbose output mode. Output is formatted with tab-separated columns and newline-separated rows.

LANGUAGE: bash
CODE:
hive --rcfilecat [--start=start_offset] [--length=len] [--verbose] fileName

--start=start_offset           Start offset to begin reading in the file
--length=len                   Length of data to read from the file
--verbose                      Prints periodic stats about the data read,
                               how many records, how many bytes, scan rate

----------------------------------------

TITLE: WebHCat MapReduce Job Response Format
DESCRIPTION: Example JSON response from WebHCat after submitting a MapReduce job

LANGUAGE: json
CODE:
{
 "id": "job_201111121211_0001",
 "info": {
          "stdout": "templeton-job-id:job_201111121211_0001
                    ",
          "stderr": "",
          "exitcode": 0
         }
}

----------------------------------------

TITLE: Retrieving Job Status with Curl
DESCRIPTION: Example curl command to retrieve job status information from the WebHCat API by specifying a job ID.

LANGUAGE: curl
CODE:
% curl -s 'http://localhost:50111/templeton/v1/jobs/job_201112212038_0004?user.name=ctdean'

----------------------------------------

TITLE: Configuring Local Hive Environment
DESCRIPTION: Environment configuration for running Hive without a Hadoop cluster using local filesystem

LANGUAGE: bash
CODE:
export HIVE_OPTS='--hiveconf mapred.job.tracker=local --hiveconf fs.default.name=file:///tmp \
    --hiveconf hive.metastore.warehouse.dir=file:///tmp/warehouse \
    --hiveconf javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=/tmp/metastore_db;create=true'

----------------------------------------

TITLE: Setting SerDe Properties in Hive
DESCRIPTION: Example showing how to set serialization encoding properties for a Hive table using ALTER TABLE command

LANGUAGE: SQL
CODE:
ALTER TABLE person SET SERDEPROPERTIES ('serialization.encoding'='GBK');

----------------------------------------

TITLE: JSON Response for WebHCat Database List
DESCRIPTION: This JSON output shows the structure of the response from the GET ddl/database endpoint. It includes a 'databases' array containing the names of the matching databases.

LANGUAGE: json
CODE:
{
 "databases": [
   "newdb",
   "newdb2"
 ]
}

----------------------------------------

TITLE: Clustered Table Sampling Examples in Hive SQL
DESCRIPTION: Examples demonstrating sampling with different bucket ratios on clustered tables.

LANGUAGE: sql
CODE:
TABLESAMPLE(BUCKET 3 OUT OF 16 ON id)

LANGUAGE: sql
CODE:
TABLESAMPLE(BUCKET 3 OUT OF 64 ON id)

----------------------------------------

TITLE: Rebuilding Partitioned Index in Hive
DESCRIPTION: Shows how to rebuild an index for a specific partition.

LANGUAGE: hql
CODE:
ALTER INDEX table10_index ON table10 PARTITION (columnX='valueQ', columnY='valueR') REBUILD;

----------------------------------------

TITLE: Querying LLAP Status via REST API
DESCRIPTION: Example of querying the LLAP daemon status endpoint to retrieve runtime information including uptime and build details.

LANGUAGE: json
CODE:
{
  "status" : "STARTED",
  "uptime" : 139093,
  "build" : "2.1.0-SNAPSHOT from 77474581df4016e3899a986e079513087a945674 by gopal source checksum a9caa5faad5906d5139c33619f1368bb"
}

----------------------------------------

TITLE: Configuring Pig Executable Path in WebHCat
DESCRIPTION: Example showing how to specify the Pig executable path using environment variables in WebHCat configuration. The path is constructed using the PIG_HOME environment variable.

LANGUAGE: xml
CODE:
${env.PIG_HOME}/bin/pig

----------------------------------------

TITLE: Deleting Hive Table Partition using cURL
DESCRIPTION: Example cURL command to delete a partition from a Hive table. The command performs a DELETE request to the WebHCat API endpoint with the database name, table name, and encoded partition value.

LANGUAGE: curl
CODE:
% curl -s -X DELETE \
       'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/partition/country=%27algeria%27?user.name=ctdean'

----------------------------------------

TITLE: Setting Table Property Using cURL - Bash
DESCRIPTION: Example cURL command demonstrating how to set a table property named 'fruit' with value 'apples' on table 'test_table' in 'default' database

LANGUAGE: bash
CODE:
% curl -s -X PUT -HContent-type:application/json -d '{ "value": "apples" }' \
  'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/property/fruit?user.name=ctdean'

----------------------------------------

TITLE: JSON Response for Creating a Column in HCatalog Table
DESCRIPTION: This JSON output shows the response received after successfully creating a new column using the WebHCat API. It confirms the column name, table name, and database name where the column was added.

LANGUAGE: json
CODE:
{
 "column": "brand",
 "table": "test_table",
 "database": "default"
}

----------------------------------------

TITLE: Block Sampling Query Example in Hive SQL
DESCRIPTION: Example showing how to sample 0.1% of data using block sampling.

LANGUAGE: sql
CODE:
SELECT *
FROM source TABLESAMPLE(0.1 PERCENT) s;

----------------------------------------

TITLE: Basic Index Operations in Hive
DESCRIPTION: Demonstrates basic index creation, showing, and dropping operations using COMPACT index type.

LANGUAGE: hql
CODE:
CREATE INDEX table01_index ON TABLE table01 (column2) AS 'COMPACT';
SHOW INDEX ON table01;
DROP INDEX table01_index ON table01;

----------------------------------------

TITLE: Dynamic Partition Insert in Hive
DESCRIPTION: Example of inserting data into dynamic partitions based on input column values.

LANGUAGE: HiveQL
CODE:
FROM page_view_stg pvs
INSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country)
       SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip, pvs.country

----------------------------------------

TITLE: Data Analysis with Hive (Using HCatalog)
DESCRIPTION: Simplified Hive queries with HCatalog integration, eliminating manual table structure modifications.

LANGUAGE: sql
CODE:
select advertiser_id, count(clicks)
from processedevents
where date = '20100819'
group by advertiser_id;

----------------------------------------

TITLE: Defining HiveServer2 Thrift API Structures and Services
DESCRIPTION: This Thrift definition file specifies the structures, enums, and service interface for the proposed HiveServer2 API. It includes definitions for data types, operation handles, session management, and various database operations.

LANGUAGE: Thrift
CODE:
// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// required for GetQueryPlan()
include "ql/if/queryplan.thrift"

namespace java org.apache.hive.service
namespace cpp Apache.Hive

// List of protocol versions. A new token should be
// added to the end of this list every time a change is made.
enum ProtocolVersion {
  HIVE_SERVER2_PROTOCOL_V1
}

enum TType {
  BOOLEAN_TYPE,
  TINYINT_TYPE,
  SMALLINT_TYPE,
  INT_TYPE,
  BIGINT_TYPE,
  FLOAT_TYPE,
  DOUBLE_TYPE,
  STRING_TYPE,
  TIMESTAMP_TYPE,
  BINARY_TYPE,
  ARRAY_TYPE,
  MAP_TYPE,
  STRUCT_TYPE,
  UNION_TYPE,
  USER_DEFINED_TYPE
}
  
const set<TType> PRIMITIVE_TYPES = [
  TType.BOOLEAN_TYPE
  TType.TINYINT_TYPE
  TType.SMALLINT_TYPE
  TType.INT_TYPE
  TType.BIGINT_TYPE
  TType.FLOAT_TYPE
  TType.DOUBLE_TYPE
  TType.STRING_TYPE
  TType.TIMESTAMP_TYPE
  TType.BINARY_TYPE
]

const set<TType> COMPLEX_TYPES = [
  TType.ARRAY_TYPE
  TType.MAP_TYPE
  TType.STRUCT_TYPE
  TType.UNION_TYPE
  TType.USER_DEFINED_TYPE
]

const set<TType> COLLECTION_TYPES = [
  TType.ARRAY_TYPE
  TType.MAP_TYPE
]

const map<TType,string> TYPE_NAMES = {
  TType.BOOLEAN_TYPE: "BOOLEAN",
  TType.TINYINT_TYPE: "TINYINT",
  TType.SMALLINT_TYPE: "SMALLINT",
  TType.INT_TYPE: "INT",
  TType.BIGINT_TYPE: "BIGINT",
  TType.FLOAT_TYPE: "FLOAT",
  TType.DOUBLE_TYPE: "DOUBLE",
  TType.STRING_TYPE: "STRING",
  TType.TIMESTAMP_TYPE: "TIMESTAMP",
  TType.BINARY_TYPE: "BINARY",
  TType.ARRAY_TYPE: "ARRAY",
  TType.MAP_TYPE: "MAP",
  TType.STRUCT_TYPE: "STRUCT",
  TType.UNION_TYPE: "UNIONTYPE"
}

// Thrift does not support recursively defined types or forward declarations,
// which makes it difficult to represent Hive's nested types.
// To get around these limitations TTypeDesc employs a type list that maps
// integer "pointers" to TTypeEntry objects. The following examples show
// how different types are represented using this scheme:
//
// "INT":
// TTypeDesc {
//   types = [
//     TTypeEntry.primitive_entry {
//       type = INT_TYPE
//     }
//   ]
// }
//
// "ARRAY<INT>":
// TTypeDesc {
//   types = [
//     TTypeEntry.array_entry {
//       object_type_ptr = 1
//     },
//     TTypeEntry.primitive_entry {
//       type = INT_TYPE
//     }
//   ]
// }
//
// "MAP<INT,STRING>":
// TTypeDesc {
//   types = [
//     TTypeEntry.map_entry {
//       key_type_ptr = 1
//       value_type_ptr = 2
//     },
//     TTypeEntry.primitive_entry {
//       type = INT_TYPE
//     },
//     TTypeEntry.primitive_entry {
//       type = STRING_TYPE
//     }
//   ]
// }

typedef i32 TTypeEntryPtr

// Type entry for a primitive type.
struct TPrimitiveTypeEntry {
  // The primitive type token. This must satisfy the condition
  // that type is in the PRIMITIVE_TYPES set.
  1: required TType type
}

// Type entry for an ARRAY type.
struct TArrayTypeEntry {
  1: required TTypeEntryPtr object_type_ptr
}

// Type entry for a MAP type.
struct TMapTypeEntry {
  1: required TTypeEntryPtr key_type_ptr
  2: required TTypeEntryPtr value_type_ptr
}

// Type entry for a STRUCT type.
struct TStructTypeEntry {
  1: required map<string, TTypeEntryPtr> name_to_type_ptr
}

// Type entry for a UNIONTYPE type.
struct TUnionTypeEntry {
  1: required map<string, TTypeEntryPtr> name_to_type_ptr
}

struct TUserDefinedTypeEntry {
  // The fully qualified name of the class implementing this type.
  1: required string typeClassName
}

// We use a union here since Thrift does not support inheritance.
union TTypeEntry {
  1: TPrimitiveTypeEntry primitive_entry
  2: TArrayTypeEntry array_entry
  3: TMapTypeEntry map_entry
  4: TStructTypeEntry struct_entry
  5: TUnionTypeEntry union_entry
  6: TUserDefinedTypeEntry user_defined_type_entry
}

// Type descriptor for columns.
struct TTypeDesc {
  // The "top" type is always the first element of the list.
  // If the top type is an ARRAY, MAP, STRUCT, or UNIONTYPE
  // type, then subsequent elements represent nested types.
  1: required list<TTypeEntry> types
}

// A result set column descriptor.
struct TColumnDesc {
  // The name of the column
  1: required string col_name

  // The type descriptor for this column
  2: required TTypeDesc type_desc
  
  // The ordinal position of this column in the schema
  3: required i32 position

  4: optional string comment
}

// Metadata used to describe the schema (column names, types, comments)
// of result sets.
struct TTableSchema {
  1: required list<TColumnDesc> columns
}

// A single column value in a result set.
// Note that Hive's type system is richer than Thrift's,
// so in some cases we have to map multiple Hive types
// to the same Thrift type. On the client-side this is
// disambiguated by looking at the Schema of the
// result set.
union TColumnValue {
  1: bool   bool_val      // BOOLEAN
  2: byte   byte_val      // TINYINT
  3: i16    i16_val       // SMALLINT
  4: i32    i32_val       // INT
  5: i64    i64_val       // BIGINT, TIMESTAMP
  6: double double_val    // FLOAT, DOUBLE
  7: string string_val    // STRING, LIST, MAP, STRUCT, UNIONTYPE, BINARY
}

// Represents a row in a rowset.
struct TRow {
  1: list<TColumnValue> colVals
}

// Represents a rowset
struct TRowSet {
  // The starting row offset of this rowset.
  1: i64 start_row_offset
  2: list<TRow> rows
}

// The return status code contained in each response.
enum TStatusCode {
  SUCCESS,
  SUCCESS_WITH_INFO,
  SQL_STILL_EXECUTING,
  ERROR,
  INVALID_HANDLE
}

// The return status of a remote request
struct TStatus {
  1: required TStatusCode status_code

  // If status is SUCCESS_WITH_INFO, info_msgs may be populated with
  // additional diagnostic information.
  2: optional list<string> info_msgs

  // If status is ERROR, then the following fields may be set
  3: optional string sql_state  // as defined in the ISO/IEF CLI specification
  4: optional i32 error_code    // internal error code
  5: optional string error_message
}

// The state of an operation (i.e. a query or other
// asynchronous operation that generates a result set)
// on the server.
enum TOperationState {
  // The operation is running. In this state the result
  // set is not available.
  RUNNING,

  // The operation has completed. When an operation is in
  // this state it's result set may be fetched.
  FINISHED,

  // The operation was canceled by a client
  CANCELED,

  // The operation failed due to an error
  ERROR
}

// A string identifier. This is interpreted literally.
typedef string TIdentifier

// A search pattern.
//
// Valid search pattern characters:
// '_': Any single character.
// '%': Any sequence of zero or more characters.
// '\': Escape character used to include special characters,
//      e.g. '_', '%', '\'. If a '\' precedes a non-special
//      character it has no special meaning and is interpreted
//      literally.
typedef string TPattern

// A search pattern or identifier. Used as input
// parameter for many of the catalog functions.
union TPatternOrIdentifier {
  1: TIdentifier identifier
  2: TPattern pattern
}

struct THandleIdentifier {
  // 16 byte globally unique identifier
  // This is the public ID of the handle and
  // can be used for reporting.
  1: binary guid,

  // 16 byte secret generated by the server
  // and used to verify that the handle is not
  // being hijacked by another user.
  2: binary secret,
}

// Client-side handle to persistent
// session information on the server-side.
struct TSessionHandle {
  1: required THandleIdentifier sess_id
}

// The subtype of an OperationHandle.
enum TOperationType {
  EXECUTE_STATEMENT,
  GET_TYPE_INFO,
  GET_TABLES,
  GET_COLUMNS,
  GET_FUNCTIONS,
}

// Client-side reference to a task running
// asynchronously on the server.
struct TOperationHandle {
  1: required THandleIdentifier op_id
  2: required TOperationType op_type
}

// OpenSession()
//
// Open a session (connection) on the server against
// which operations may be executed. 
struct TOpenSessionReq {
  // The version of the HiveServer2 protocol that the client is using.
  1: required ProtocolVersion client_protocol = ProtocolVersion.HIVE_SERVER2_PROTOCOL_V1
  
  // Username and password for authentication.
  // Depending on the authentication scheme being used,
  // this information may instead be provided by a lower
  // protocol layer, in which case these fields may be
  // left unset.
  2: optional string username
  3: optional string password

  // Configuration overlay which is applied when the session is
  // first created.
  4: optional map<string, string> configuration
}

struct TOpenSessionResp {
  1: required TStatus status

  // The protocol version that the server is using.
  2: ProtocolVersion server_protocol = ProtocolVersion.HIVE_SERVER2_PROTOCOL_V1

  // Session Handle
  3: TSessionHandle session_handle

  // The configuration settings for this session.
  4: map<string, string> configuration
}

// CloseSession()
//
// Closes the specified session and frees any resources
// currently allocated to that session. Any open
// operations in that session will be canceled.
struct TCloseSessionReq {
  1: required TSessionHandle session_handle
}

struct TCloseSessionResp {
  1: required TStatus status
}

// GetInfo()
//
// This function is based on ODBC's SQLGetInfo() function.
// The function returns general information about the data source
// using the same keys as ODBC.
struct TGetInfoReq {
  // The sesssion to run this request against
  1: required TSessionHandle session_handle

  // List of keys for which info is requested. If unset or empty,
  // the response message will contain the values of all Info
  // properties.
  2: optional list<string> info_keys
}

struct TGetInfoResp {
  1: required TStatus status

  // key/value pairs representing Info values
  2: map<string, string> info
}

// ExecuteStatement()
//
// Execute a statement.
// The returned OperationHandle can be used to check on the
// status of the statement, and to fetch results once the
// statement has finished executing.
struct TExecuteStatementReq {
  // The session to exexcute the statement against
  1: required TSessionHandle session

  // The statement to be executed (DML, DDL, SET, etc)
  2: required string statement

  // Configuration properties that are overlayed on top of the
  // the existing session configuration before this statement
  // is executed. These properties apply to this statement
  // only and will not affect the subsequent state of the Session.
  3: map<string, string> conf_overlay
}

struct TExecuteStatementResp {
  1: required TStatus status
  2: TOperationHandle op_handle
}

// GetTypeInfo()
//
// Get information about types supported by the HiveServer instance.
// The information is returned as a result set which can be fetched
// using the OperationHandle provided in the response.
//
// Refer to the documentation for ODBC's SQLGetTypeInfo function for
// for the format of the result set.
struct TGetTypeInfoReq {
  // The session to run this request against.
  1: required TSessionHandle session
}

struct TGetTypeInfoResp {
  1: required TStatus status
  2: TOperationHandle op_handle
}  

// GetTables()
//
// Returns a list of tables with catalog, schema, and table
// type information. The information is returned as a result
// set which can be fetched using the OperationHandle
// provided in the response.
//
// Result Set Columns:
//
// col1
// name: TABLE_CAT
// type: STRING
// desc: Catalog name. NULL if not applicable.
//
// col2
// name: TABLE_SCHEM
// type: STRING
// desc: Schema name.
//
// col3
// name: TABLE_NAME
// type: STRING
// desc: Table name.
//
// col4
// name: TABLE_TYPE
// type: STRING
// desc: The table type, e.g. "TABLE", "VIEW", etc.
//
// col5
// name: REMARKS
// type: STRING
// desc: Comments about the table
//
struct TGetTablesReq {
  // Session to run this request against
  1: required TSessionHandle session

  // Name of the catalog or a search pattern.
  2: optional TPatternOrIdentifier catalog_name

  // Name of the schema or a search pattern.
  3: required TPatternOrIdentifier schema_name

  // Name of the table or a search pattern.
  4: required TPatternOrIdentifier table_name

  // List of table types to match
  // e.g. "TABLE", "VIEW", "SYSTEM TABLE", "GLOBAL TEMPORARY",
  // "LOCAL TEMPORARY", "ALIAS", "SYNONYM", etc.
  5: list<string> table_types
}

struct TGetTablesResp {
  1: required TStatus status
  2: TOperationHandle op_handle
}

// GetColumns()
//
// Returns a list of columns in the specified tables.
// The information is returned as a result set which can be fetched
// using the OperationHandle provided in the response.
//
// Result Set Columns are the same as those for the ODBC SQLColumns
// function.
//
struct TGetColumnsReq {
  // Session to run this request against
  1: required TSessionHandle session

  // Name of the catalog. Must not contain a search pattern.
  2: optional TIdentifier catalog_name

  // Schema name or search pattern
  3: required TPatternOrIdentifier schema_name

  // Table name or search pattern
  4: required TPatternOrIdentifier table_name

  // Column name or search pattern
  5: required TPatternOrIdentifier column_name
}

struct TGetColumnsResp {
  1: required TStatus status
  2: optional TOperationHandle op_handle
}

// GetFunctions()
//
// Returns a list of functions supported by the data source.
//
// Result Set Columns:
//
// col1
// name: FUNCTION_NAME
// type: STRING
// desc: The name of the function.
//
// col2
// name: DESCRIPTION
// type: STRING
// desc: A description of the function.
//
// col3
// name: FUNCTION_TYPE
// type: STRING
// desc: The function type: "UDF", "UDAF", or "UDTF".
//
// col4
// name: FUNCTION_CLASS
// type: STRING
// desc: The fully qualified name of the class
//       that implements the specified function.
//
struct TGetFunctionsReq {
  // Session to run this request against
  1: required TSessionHandle session
}

struct TGetFunctionsResp {
  1: required TStatus status
  2: optional TOperationHandle op_handle
}
  

// GetOperationStatus()
//
// Get the status of an operation running on the server.
struct TGetOperationStatusReq {
  // Session to run this request against
  1: required TOperationHandle op_handle
}

struct TGetOperationStatusResp {
  1: required TStatus status
  2: TOperationState op_state
}

// CancelOperation()
//
// Cancels processing on the specified operation handle and
// frees any resources which were allocated.
struct TCancelOperationReq {
  // Operation to cancel
  1: required TOperationHandle op_handle
}

struct TCancelOperationResp {
  1: required TStatus status
}

// GetQueryPlan()
//
// Given an OperationHandle corresponding to an ExecuteStatement
// request, this function returns the queryplan for that
// statement annotated with counter information.
struct TGetQueryPlanReq {
  1: required TOperationHandle op_handle
}

struct TGetQueryPlanResp {
  1: required TStatus status
  2: optional queryplan.QueryPlan query_plan
}

// CloseOperation()
//
// Given an operation in the FINISHED, CANCELED,
// or ERROR states, CloseOperation() will free
// all of the resources which were allocated on
// the server to service the operation.
struct TCloseOperationReq {
  1: required TOperationHandle op_handle
}

struct TCloseOperationResp {
  1: required TStatus status
}

// GetResultSetMetadata()
//
// Retrieves schema information for the specified operation
struct TGetResultSetMetadataReq {
  // Operation for which to fetch result set schema information
  1: required TOperationHandle op_handle
}

struct TGetResultSetMetadataResp {
  1: required TStatus status
  2: optional TTableSchema schema
}

enum TFetchOrientation {
  // Get the next rowset. The fetch offset is ignored.
  FETCH_NEXT,

  // Get the previous rowset. The fetch offset is ignored.
  // NOT SUPPORTED
  FETCH_PRIOR,

  // Return the rowset at the given fetch offset relative
  // to the curren rowset.
  // NOT SUPPORTED
  FETCH_RELATIVE,

  // Return the rowset at the specified fetch offset.
  // NOT SUPPORTED
  FETCH_ABSOLUTE,

  // Get the first rowset in the result set.
  FETCH_FIRST,

  // Get the last rowset in the result set.
  // NOT SUPPORTED
  FETCH_LAST
}

// FetchResults()
//
// Fetch rows from the server corresponding to
// a particular OperationHandle.
struct TFetchResultsReq {
  // Operation from which to fetch results.
  1: required TOperationHandle op_handle

  // The fetch orientation. For V1 this must be either
  // FETCH_NEXT or FETCH_FIRST. Defaults to FETCH_NEXT.
  2: TFetchOrientation orientation = TFetchOrientation.FETCH_NEXT
  
  // Max number of rows that should be returned in
  // the rowset.
  3: i64 max_rows
}

struct TFetchResultsResp {
  1: required TStatus status

  // TRUE if there are more rows left to fetch from the server.
  2: bool has_more_rows

  // The rowset. This is optional so that we have the
  // option in the future of adding alternate formats for
  // representing result set data, e.g. delimited strings,
  // binary encoded, etc.
  3: optional TRowSet results
}

service TSQLService {

  TOpenSessionResp OpenSession(1:TOpenSessionReq req);

  TCloseSessionResp CloseSession(1:TCloseSessionReq req);

  TGetInfoResp GetInfo(1:TGetInfoReq req);

  TExecuteStatementResp ExecuteStatement(1:TExecuteStatementReq req);

  TGetTypeInfoResp GetTypeInfo(1:TGetTypeInfoReq req);

  TGetTablesResp GetTables(1:TGetTablesReq req);

  TGetColumnsResp GetColumns(1:TGetColumnsReq req);

  TGetFunctionsResp GetFunctions(1:TGetFunctionsReq req);

  TGetOperationStatusResp GetOperationStatus(1:TGetOperationStatusReq req);
  
  TCancelOperationResp CancelOperation(1:TCancelOperationReq req);

  TGetQueryPlanResp GetQueryPlan(1:TGetQueryPlanReq req);

  TGetResultSetMetadataResp GetResultSetMetadata(1:TGetResultSetMetadataReq req);
  
  TFetchResultsResp FetchResults(1:TFetchResultsReq req);
}

----------------------------------------

TITLE: MapReduce Environment Setup for HCatalog
DESCRIPTION: Shell script commands to set up the environment variables and classpath for running MapReduce jobs with HCatalog.

LANGUAGE: Shell
CODE:
export HADOOP_HOME=<path_to_hadoop_install>
export HCAT_HOME=<path_to_hcat_install>
export HIVE_HOME=<path_to_hive_install>
export LIB_JARS=$HCAT_HOME/share/hcatalog/hcatalog-core-0.5.0.jar,
$HIVE_HOME/lib/hive-metastore-0.10.0.jar,
$HIVE_HOME/lib/libthrift-0.7.0.jar,
$HIVE_HOME/lib/hive-exec-0.10.0.jar,
$HIVE_HOME/lib/libfb303-0.7.0.jar,
$HIVE_HOME/lib/jdo2-api-2.3-ec.jar,
$HIVE_HOME/lib/slf4j-api-1.6.1.jar

export HADOOP_CLASSPATH=$HCAT_HOME/share/hcatalog/hcatalog-core-0.5.0.jar:
$HIVE_HOME/lib/hive-metastore-0.10.0.jar:
$HIVE_HOME/lib/libthrift-0.7.0.jar:
$HIVE_HOME/lib/hive-exec-0.10.0.jar:
$HIVE_HOME/lib/libfb303-0.7.0.jar:
$HIVE_HOME/lib/jdo2-api-2.3-ec.jar:
$HIVE_HOME/conf:$HADOOP_HOME/conf:
$HIVE_HOME/lib/slf4j-api-1.6.1.jar

----------------------------------------

TITLE: Updating NameNode Location with MetaTool
DESCRIPTION: Example command showing how to update the NameNode location from namenode2:8020 to localhost:9000 using the metatool service with table and serde property keys for Avro schema URLs.

LANGUAGE: bash
CODE:
./hive --service metatool -updateLocation hdfs://localhost:9000 hdfs://namenode2:8020 -tablePropKey avro.schema.url -serdePropKey avro.schema.url

----------------------------------------

TITLE: WebHCat Version API Response Format
DESCRIPTION: JSON response format from the WebHCat version API showing the supported versions array and current version. The response includes all available API versions and indicates which version is currently active.

LANGUAGE: json
CODE:
{
 "supportedVersions": [
   "v1"
 ],
 "version": "v1"
}

----------------------------------------

TITLE: Byte Length Sampling Syntax in Hive SQL
DESCRIPTION: Syntax for sampling based on byte length with size units.

LANGUAGE: sql
CODE:
block_sample: TABLESAMPLE (ByteLengthLiteral)

ByteLengthLiteral : (Digit)+ ('b' | 'B' | 'k' | 'K' | 'm' | 'M' | 'g' | 'G')

----------------------------------------

TITLE: Maven Build Commands for Hive
DESCRIPTION: Maven commands for building Hive source code without running tests

LANGUAGE: bash
CODE:
mvn clean install -DskipTests
cd itests
mvn clean install -DskipTests

----------------------------------------

TITLE: Ideal HBase Bulk Load SQL Operation
DESCRIPTION: Example of how HBase bulk loading should ideally work, showing table creation and data insertion with HBaseStorageHandler. This is currently not implemented but represents the target functionality.

LANGUAGE: sql
CODE:
CREATE TABLE new_hbase_table(rowkey string, x int, y int) 
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:x,cf:y");

SET hive.hbase.bulk=true;

INSERT OVERWRITE TABLE new_hbase_table
SELECT rowkey_expression, x, y FROM ...any_hive_query...;

----------------------------------------

TITLE: Error Tolerance Properties
DESCRIPTION: Properties for configuring error tolerance thresholds in HCatRecordReader.

LANGUAGE: properties
CODE:
hcat.input.bad.record.threshold=0.0001f
hcat.input.bad.record.min=2

----------------------------------------

TITLE: Ordered Rebalance Compaction in Hive SQL
DESCRIPTION: Executes rebalance compaction with custom ordering of data using an ORDER BY clause. This allows reorganizing data based on specific columns during the redistribution process.

LANGUAGE: sql
CODE:
ALTER TABLE table_name COMPACT 'REBALANCE' ORDER BY column_name DESC;

----------------------------------------

TITLE: Configuring Proxy User in core-site.xml
DESCRIPTION: This XML snippet shows how to configure proxy user settings in the Hadoop core-site.xml file. It specifies the groups and hosts allowed for a particular proxy user.

LANGUAGE: xml
CODE:
<property>
  <name>hadoop.proxyuser.USER.groups</name>
  <value>group1,group2</value>
</property>
<property>
  <name>hadoop.proxyuser.USER.hosts</name>
  <value>host1,host2</value>
</property>

----------------------------------------

TITLE: Setting Up HCatalog Output for MapReduce with Specific Partition
DESCRIPTION: This Java code demonstrates how to set up HCatalog output for MapReduce with a specific partition. It creates a map of partition values and sets the output information for the job.

LANGUAGE: java
CODE:
Map<String, String> partitionValues = new HashMap<String, String>();
partitionValues.put("a", "1");
partitionValues.put("b", "1");
HCatTableInfo info = HCatTableInfo.getOutputTableInfo(dbName, tblName, partitionValues);
HCatOutputFormat.setOutput(job, info);

----------------------------------------

TITLE: Hadoop File System Setup Commands
DESCRIPTION: Commands to upload JAR files to HDFS before executing MapReduce job

LANGUAGE: bash
CODE:
% hadoop fs -put wordcount.jar .
% hadoop fs -put transform.jar .

% hadoop fs -ls .
Found 2 items
-rw-r--r--   1 ctdean supergroup         23 2011-11-11 13:29 /user/ctdean/wordcount.jar
-rw-r--r--   1 ctdean supergroup         28 2011-11-11 13:29 /user/ctdean/transform.jar

----------------------------------------

TITLE: Deleting an HCatalog Table using WebHCat API with Curl
DESCRIPTION: This curl command demonstrates how to delete a table named 'test_table' from the 'default' database using the WebHCat API. It includes the required user.name parameter for authentication.

LANGUAGE: shell
CODE:
% curl -s -X DELETE 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table?user.name=ctdean'

----------------------------------------

TITLE: Byte Length Sampling Example in Hive SQL
DESCRIPTION: Example showing how to sample 100MB of data using byte length sampling.

LANGUAGE: sql
CODE:
SELECT *
FROM source TABLESAMPLE(100M) s;

----------------------------------------

TITLE: Eclipse Project Generation Commands
DESCRIPTION: Commands for generating Eclipse project files for Hive development

LANGUAGE: bash
CODE:
$ mkdir workspace
$ cd workspace
$ git clone https://github.com/apache/hive.git
$ cd hive
$ mvn clean package eclipse:clean eclipse:eclipse -Pitests -DskipTests -DdownloadSources -DdownloadJavadocs

----------------------------------------

TITLE: REPL LOAD Command Syntax
DESCRIPTION: The syntax for the new REPL LOAD command, used to load replication dumps into a target cluster with options for database renaming and configuration.

LANGUAGE: SQL
CODE:
REPL LOAD {<dbname>} FROM <dirname> {WITH ('key1'='value1', 'key2'='value2')};

----------------------------------------

TITLE: Creating a ReadEntity for HCatReader in Java
DESCRIPTION: This snippet demonstrates how to create a ReadEntity object for reading from a specific database and table in HCatalog.

LANGUAGE: Java
CODE:
ReadEntity.Builder builder = new ReadEntity.Builder();
ReadEntity entity = builder.withDatabase("mydb").withTable("mytbl").build();

----------------------------------------

TITLE: WebHCat MapReduce Job Submission cURL Command
DESCRIPTION: Example cURL command to submit a MapReduce job through WebHCat REST API

LANGUAGE: bash
CODE:
% curl -s -d jar=wordcount.jar \
       -d class=org.myorg.WordCount \
       -d libjars=transform.jar \
       -d arg=wordcount/input \
       -d arg=wordcount/output \
       'http://localhost:50111/templeton/v1/mapreduce/jar?user.name=ekoifman'

----------------------------------------

TITLE: Storing Data with Partial Partition Specification in Pig
DESCRIPTION: These Pig statements demonstrate different ways to store data with partial or no partition specifications, allowing HCatalog to handle dynamic partitioning.

LANGUAGE: pig
CODE:
store A into 'mytable' using HCatStorer("a=1, b=1");

store A into 'mytable' using HCatStorer();

store A into 'mytable' using HCatStorer("a=1");

store A into 'mytable' using HCatStorer("b=1");

----------------------------------------

TITLE: Configuring Hive to use EC2 Hadoop cluster
DESCRIPTION: Sets Hive configuration to use a Hadoop cluster running on EC2. Requires the public hostname of the EC2 master node.

LANGUAGE: sql
CODE:
set fs.default.name=hdfs://ec2-12-34-56-78.compute-1.amazonaws.com:50001;
set mapred.job.tracker=ec2-12-34-56-78.compute-1.amazonaws.com:50002;

----------------------------------------

TITLE: Retrieving Job IDs using WebHCat API - Curl Example
DESCRIPTION: Example curl command to retrieve job IDs from the WebHCat API endpoint. The request includes the required user.name parameter.

LANGUAGE: curl
CODE:
% curl -s 'http://localhost:50111/templeton/v1/queue?user.name=ctdean'

----------------------------------------

TITLE: Sample Data Setup Commands
DESCRIPTION: Hadoop filesystem commands to set up test data and Pig script for the example

LANGUAGE: bash
CODE:
% cat fake-passwd
ctdean:Chris Dean:secret
pauls:Paul Stolorz:good
carmas:Carlos Armas:evil
dra:Deirdre McClure:marvelous

% hadoop fs -put id.pig .
% hadoop fs -put fake-passwd passwd

----------------------------------------

TITLE: Sampling Data in Hive
DESCRIPTION: Example of using TABLESAMPLE to query a sample of data from a bucketed table.

LANGUAGE: HiveQL
CODE:
INSERT OVERWRITE TABLE pv_gender_sum_sample
SELECT pv_gender_sum.*
FROM pv_gender_sum TABLESAMPLE(BUCKET 3 OUT OF 32);

----------------------------------------

TITLE: Log Error for Missing Version Information
DESCRIPTION: Log output showing an error message when version information is not found in the metastore.

LANGUAGE: text
CODE:
...
Caused by: MetaException(message:Version information not found in metastore. )
...

----------------------------------------

TITLE: Conditional Index Drop in Hive
DESCRIPTION: Demonstrates how to safely drop an index if it exists.

LANGUAGE: hql
CODE:
DROP INDEX IF EXISTS table09_index ON table09;

----------------------------------------

TITLE: Viewing Granted Roles in Hive
DESCRIPTION: SQL command for displaying role grants for specific users, groups, or roles.

LANGUAGE: sql
CODE:
SHOW ROLE GRANT principal_specification

----------------------------------------

TITLE: Starting Hive Client for Testing
DESCRIPTION: Bash command to launch the Hive client for testing the Metastore connection.

LANGUAGE: Bash
CODE:
${HIVE_HOME}/bin/hive

----------------------------------------

TITLE: Initializing JMS Connection for HCatalog Notifications in Java
DESCRIPTION: Creates a connection to the JMS message bus for receiving HCatalog notifications. Uses ActiveMQ as the JMS provider.

LANGUAGE: java
CODE:
ConnectionFactory connFac = new ActiveMQConnectionFactory(amqurl);
Connection conn = connFac.createConnection();
conn.start();

----------------------------------------

TITLE: Creating and populating a temporary HDFS table
DESCRIPTION: Demonstrates creating a table in HDFS on EC2 and populating it with joined data from S3. This can be used as a performance optimization for frequently accessed data.

LANGUAGE: sql
CODE:
create table cust_order (nationkey string, acctbal double, mktsegment string, orderstatus string, totalprice double);
from customer c left outer join orders o on (c.c_custkey = o.o_custkey)
  insert overwrite table cust_order
  select c.c_nationkey, c.c_acctbal, c.c_mktsegment, o.o_orderstatus, o.o_totalprice;

----------------------------------------

TITLE: Executing Hive Query via WebHCat API with Curl
DESCRIPTION: Example curl command to execute a Hive query using the WebHCat API. It demonstrates sending a SELECT query and specifying an output directory for status information.

LANGUAGE: bash
CODE:
% curl -s -d execute="select+*+from+pokes;" \
       -d statusdir="pokes.output" \
       'http://localhost:50111/templeton/v1/hive?user.name=ekoifman'

----------------------------------------

TITLE: Uncorrelated Subquery in WHERE Clause for Apache Hive SQL
DESCRIPTION: Shows an example of an uncorrelated subquery in the WHERE clause using the IN operator. This type of subquery is supported in Hive 0.13 and later versions.

LANGUAGE: SQL
CODE:
SELECT *
FROM A
WHERE A.a IN (SELECT foo FROM B);

----------------------------------------

TITLE: Restarting Jenkins Service in Bash
DESCRIPTION: Command to restart the Jenkins service on the Hive PTest2 Infrastructure master. This is typically used when changes are made to Jenkins configuration or when troubleshooting Jenkins-related issues.

LANGUAGE: bash
CODE:
sudo service jenkins restart

----------------------------------------

TITLE: Inserting Data into HBase via Hive
DESCRIPTION: Example of inserting data from a Hive table into an HBase-backed table.

LANGUAGE: HiveQL
CODE:
INSERT OVERWRITE TABLE hbase_table_1 SELECT * FROM pokes WHERE foo=98;

----------------------------------------

TITLE: Setting Role in Hive SQL
DESCRIPTION: SQL command to set the current role for the session. Can set to a specific role, ALL roles, or NONE.

LANGUAGE: SQL
CODE:
SET ROLE (role_name|ALL|NONE);

----------------------------------------

TITLE: WebHCat API response for Hadoop streaming job submission
DESCRIPTION: JSON response from the WebHCat API after submitting a Hadoop streaming MapReduce job, including job ID and execution information.

LANGUAGE: json
CODE:
{
 "id": "job_201111111311_0008",
 "info": {
          "stdout": "packageJobJar: [] [/Users/ctdean/var/hadoop/hadoop-0.20.205.0/share/hadoop/contrib/streaming/hadoop-streaming-0.20.205.0.jar...
                    templeton-job-id:job_201111111311_0008
                    ",
          "stderr": "11/11/11 13:26:43 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments
                    11/11/11 13:26:43 INFO mapred.FileInputFormat: Total input paths to process : 2
                    ",
          "exitcode": 0
         }
}

----------------------------------------

TITLE: Successful JSON Response for WebHCat Supported Response Types
DESCRIPTION: This JSON output represents a successful response from the WebHCat API, listing the supported response types. In this case, it shows that 'application/json' is the only supported response type.

LANGUAGE: json
CODE:
{
  "responseTypes": [
    "application/json"
  ]
}

----------------------------------------

TITLE: Execute DDL Command with cURL
DESCRIPTION: Example of using curl to execute a 'show tables' DDL command through the WebHCat API. The command is sent as a POST request with the exec parameter containing the DDL statement.

LANGUAGE: bash
CODE:
% curl -s -d 'exec=show tables;' \
       'http://localhost:50111/templeton/v1/ddl?user.name=ekoifman'

----------------------------------------

TITLE: Error Response Format for GetTables
DESCRIPTION: Example JSON response showing error format when database doesn't exist

LANGUAGE: json
CODE:
{
  "errorDetail": "
    org.apache.hadoop.hive.ql.metadata.HiveException: ERROR: The database defaultsd does not exist.
        at org.apache.hadoop.hive.ql.exec.DDLTask.switchDatabase(DDLTask.java:3122)
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:224)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:134)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1332)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1123)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:931)
        at org.apache.hcatalog.cli.HCatDriver.run(HCatDriver.java:42)
        at org.apache.hcatalog.cli.HCatCli.processCmd(HCatCli.java:247)
        at org.apache.hcatalog.cli.HCatCli.processLine(HCatCli.java:203)
        at org.apache.hcatalog.cli.HCatCli.main(HCatCli.java:162)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
    ",
  "error": "FAILED: Error in metadata: ERROR: The database defaultsd does not exist.",
  "errorCode": 500,
  "database": "defaultsd"
}

----------------------------------------

TITLE: Fetching WebHCat Supported Response Types with Curl
DESCRIPTION: This curl command sends a GET request to the WebHCat API endpoint to retrieve the list of supported response types. It uses the local development server on port 50111.

LANGUAGE: bash
CODE:
% curl -s 'http://localhost:50111/templeton/v1'

----------------------------------------

TITLE: Exploding a Map in Hive SQL
DESCRIPTION: Examples of using the explode function to transform a map into multiple rows. Shows different syntax options including using it in a SELECT statement and with LATERAL VIEW.

LANGUAGE: SQL
CODE:
select explode(map('A',10,'B',20,'C',30));
select explode(map('A',10,'B',20,'C',30)) as (key,value);
select tf.* from (select 0) t lateral view explode(map('A',10,'B',20,'C',30)) tf;
select tf.* from (select 0) t lateral view explode(map('A',10,'B',20,'C',30)) tf as key,value;

----------------------------------------

TITLE: Managing Hive PTest2 WebServer in Bash
DESCRIPTION: Commands for stopping, starting, restarting the Hive PTest2 WebServer, and updating it with the latest test infrastructure code. These commands are essential for maintaining and updating the Hive PTest2 WebServer component of the infrastructure.

LANGUAGE: bash
CODE:
$ sudo /usr/local/hiveptest/bin/stop-server.sh 
$ sudo /usr/local/hiveptest/bin/start-server.sh 
$ sudo /usr/local/hiveptest/bin/restart-server.sh 
$ sudo /usr/local/hiveptest/bin/update.sh 

----------------------------------------

TITLE: Inserting DEFAULT Values in Apache Hive SQL
DESCRIPTION: Examples of using the proposed DEFAULT keyword in INSERT INTO statements. This allows inserting default values without explicitly specifying the column schema.

LANGUAGE: sql
CODE:
INSERT INTO TABLE1 VALUES(DEFAULT, DEFAULT)

LANGUAGE: sql
CODE:
INSERT INTO TABLE1(COL1) VALUES(DEFAULT)

----------------------------------------

TITLE: Creating a Role in Hive SQL
DESCRIPTION: SQL command to create a new role. Only users with admin privileges can execute this.

LANGUAGE: SQL
CODE:
CREATE ROLE role_name;

----------------------------------------

TITLE: Inserting Data into Iceberg Table in Hive
DESCRIPTION: Shows how to insert data into an Iceberg table using the INSERT INTO statement with VALUES clause.

LANGUAGE: SQL
CODE:
INSERT INTO TBL_ICE VALUES (1),(2),(3),(4);

----------------------------------------

TITLE: Testing Local Patches
DESCRIPTION: Commands for testing patches from local filesystem, including multiple patch application

LANGUAGE: bash
CODE:
hive_repo/testutils/ptest/hivetest.py --test --patch /path/to/my.patch
hive_repo/testutils/ptest/hivetest.py --test --patch first.patch second.patch

----------------------------------------

TITLE: Creating Basic Hive Table - Curl Example
DESCRIPTION: Example of creating a Hive table with basic columns, partitioning, and RCFile storage format using the WebHCat REST API.

LANGUAGE: bash
CODE:
% curl -s -X PUT -HContent-type:application/json -d '{
 "comment": "Best table made today",
 "columns": [
   { "name": "id", "type": "bigint" },
   { "name": "price", "type": "float", "comment": "The unit price" } ],
 "partitionedBy": [
   { "name": "country", "type": "string" } ],
 "format": { "storedAs": "rcfile" } }' \
 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table?user.name=ctdean'

----------------------------------------

TITLE: Executing GET Request for Table Properties in WebHCat
DESCRIPTION: This curl command demonstrates how to make a GET request to the WebHCat API to retrieve all properties of a table named 'test_table' in the 'default' database.

LANGUAGE: bash
CODE:
% curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/property?user.name=ctdean'

----------------------------------------

TITLE: Successful JSON Response for Get Table Property in Apache Hive WebHCat API
DESCRIPTION: This JSON output shows the successful response structure when retrieving a table property value using the WebHCat API. It includes the property name-value pair, table name, and database name.

LANGUAGE: json
CODE:
{
 "property": {
   "fruit": "apple"
 },
 "table": "test_table",
 "database": "default"
}

----------------------------------------

TITLE: Creating a WriteEntity for HCatWriter in Java
DESCRIPTION: This snippet shows how to create a WriteEntity object for writing to a specific database and table in HCatalog.

LANGUAGE: Java
CODE:
WriteEntity.Builder builder = new WriteEntity.Builder();
WriteEntity entity = builder.withDatabase("mydb").withTable("mytbl").build();

----------------------------------------

TITLE: Disabling List Bucketing on a Table in Hive SQL
DESCRIPTION: SQL syntax for disabling list bucketing on a table by removing the skewed designation.

LANGUAGE: SQL
CODE:
ALTER TABLE <T> (SCHEMA) NOT SKEWED;

----------------------------------------

TITLE: Managing Hive Roles with SQL
DESCRIPTION: SQL commands for creating and dropping roles in Hive's authorization system.

LANGUAGE: sql
CODE:
CREATE ROLE role_name

DROP ROLE role_name

----------------------------------------

TITLE: Creating Iceberg Table with ORC Format in Hive
DESCRIPTION: Shows how to create an Iceberg table using the ORC file format instead of the default Parquet format.

LANGUAGE: SQL
CODE:
CREATE TABLE ORC_TABLE (ID INT) STORED BY ICEBERG STORED AS ORC;

----------------------------------------

TITLE: Running Hive Tests with Differential Revision
DESCRIPTION: Command to test a patch from Phabricator using the test script

LANGUAGE: bash
CODE:
hive_repo/testutils/ptest/hivetest.py --test --revision D123

----------------------------------------

TITLE: Simple COUNT Query
DESCRIPTION: Demonstrates a basic counting operation using GROUP BY to count total rows in a table.

LANGUAGE: sql
CODE:
SELECT COUNT(*) FROM table2;

----------------------------------------

TITLE: Retrieving Full Job Details with GET /jobs in Apache Hive WebHCat
DESCRIPTION: This API call retrieves full details for all jobs by setting the fields parameter to '*'.

LANGUAGE: HTTP
CODE:
GET /templeton/v1/jobs?user.name=daijy&fields=*

----------------------------------------

TITLE: Sample Data Format for LZO Compression
DESCRIPTION: Example data format showing ID, first name, and last name columns that will be compressed using LZO.

LANGUAGE: text
CODE:
19630001     john          lennon
19630002     paul          mccartney
19630003     george        harrison
19630004     ringo         starr


----------------------------------------

TITLE: Configuring HBase Metastore in Hive XML Configuration
DESCRIPTION: XML configuration properties required in hive-site.xml to enable HBase as the metastore implementation. Sets the rawstore implementation class and enables fastpath optimization.

LANGUAGE: xml
CODE:
<property>
    <name>hive.metastore.rawstore.impl</name>
    <value>org.apache.hadoop.hive.metastore.hbase.HBaseStore</value>
  </property>
  <property>
    <name>hive.metastore.fastpath</name>
    <value>true</value>
  </property>

----------------------------------------

TITLE: Simplified Remote Database Creation in Hive SQL
DESCRIPTION: Simplified SQL statement for creating a remote database connection, using default values for the connection method and remote database name.

LANGUAGE: SQL
CODE:
CREATE REMOTE DATABASE db_name
WITH DBPROPERTIES (
  'hive.metastore.uris' = 'thrift://remote-hms:9083'
);

----------------------------------------

TITLE: Java API - Streaming Data Ingest
DESCRIPTION: Operation-based API for writing continuous data streams into transactional tables using Hive's ACID feature. Handles small batch inserts with short-lived transactions.



----------------------------------------

TITLE: Rewritten Query Using Materialized View in Hive
DESCRIPTION: Demonstrates how Hive rewrites the query using the materialized view 'mv1'.

LANGUAGE: SQL
CODE:
SELECT empid, deptname
FROM mv1
WHERE hire_date >= '2018-01-01'
AND hire_date <= '2018-03-31';

----------------------------------------

TITLE: Database Deletion Error Response - JSON
DESCRIPTION: Example JSON error response when attempting to delete a non-existent database through the WebHCat API.

LANGUAGE: json
CODE:
{
  "errorDetail": "
    NoSuchObjectException(message:There is no database named my_db)
        at org.apache.hadoop.hive.metastor...
    ",
  "error": "There is no database named newdb",
  "errorCode": 404,
  "database": "newdb"
}

----------------------------------------

TITLE: Hive Site XML Configuration
DESCRIPTION: XML configuration for connecting Hive to network Derby server

LANGUAGE: xml
CODE:
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:derby://hadoop1:1527/metastore_db;create=true</value>
  <description>JDBC connect string for a JDBC metastore</description>
</property>

<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>org.apache.derby.jdbc.ClientDriver</value>
  <description>Driver class name for a JDBC metastore</description>
</property>

----------------------------------------

TITLE: Interactive Hive Shell Commands
DESCRIPTION: Example usage of various interactive shell commands in Hive CLI including source, show tables, exit, configuration settings, and resource management.

LANGUAGE: sql
CODE:
hive> source /root/test.sql;
hive> show tables;
test1
test2
hive> exit;
hive> quit;
hive> set;
hive> set hive.cli.print.header=true;
hive> set -v;
hive> reset;
hive> add file /opt/a.txt;
Added resources: [/opt/a.txt]
hive> list files;
/opt/a.txt
hive> delete file /opt/a.txt;
hive> add jar /usr/share/vnc/classes/vncviewer.jar;
Added [/usr/share/vnc/classes/vncviewer.jar]to class path
Added resources:[/usr/share/vnc/classes/vncviewer.jar]
hive> list jars;
/usr/share/vnc/classes/vncviewer.jar
hive> delete jar /usr/share/vnc/classes/vncviewer.jar;
hive> !ls;
bin
conf
hive> dfs -ls / ;
Found 2 items
drwx-wx-wx  - root supergroup  0   2015-08-12 19:06 /tmp
drwxr-xr-x  - root supergroup  0   2015-08-12 19:43 /user
hive> select * from pokes;
OK
pokes.foo   pokes.bar
238         val_238
86          val_86
311         val_311
hive>source /opt/s.sql;

----------------------------------------

TITLE: Creating LZO-Compressed Hive External Table
DESCRIPTION: SQL command to create an external Hive table that uses LZO compression format.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE IF NOT EXISTS hive_table_name (column_1  datatype_1......column_N datatype_N)
PARTITIONED BY (partition_col_1 datatype_1 ....col_P  datatype_P)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS INPUTFORMAT  \"com.hadoop.mapred.DeprecatedLzoTextInputFormat\"
          OUTPUTFORMAT \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\";

----------------------------------------

TITLE: MovieLens data analysis example
DESCRIPTION: Example of loading and analyzing MovieLens user rating data in Hive

LANGUAGE: SQL
CODE:
CREATE TABLE u_data (
  userid INT,
  movieid INT,
  rating INT,
  unixtime STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '<path>/u.data'
OVERWRITE INTO TABLE u_data;

SELECT COUNT(*) FROM u_data;

LANGUAGE: Python
CODE:
import sys
import datetime

for line in sys.stdin:
  line = line.strip()
  userid, movieid, rating, unixtime = line.split('\t')
  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()
  print '\t'.join([userid, movieid, rating, str(weekday)])

LANGUAGE: SQL
CODE:
CREATE TABLE u_data_new (
  userid INT,
  movieid INT,
  rating INT,
  weekday INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t';

add FILE weekday_mapper.py;

INSERT OVERWRITE TABLE u_data_new
SELECT
  TRANSFORM (userid, movieid, rating, unixtime)
  USING 'python weekday_mapper.py'
  AS (userid, movieid, rating, weekday)
FROM u_data;

SELECT weekday, COUNT(*)
FROM u_data_new
GROUP BY weekday;

----------------------------------------

TITLE: Hadoop LZO Index Generation Command
DESCRIPTION: Command to create LZO index files using the Hadoop LZO jar file.

LANGUAGE: bash
CODE:
hadoop jar /path/to/jar/hadoop-lzo-cdh4-0.4.15-gplextras.jar com.hadoop.compression.lzo.LzoIndexer  /path/to/HDFS/dir/containing/lzo/files

----------------------------------------

TITLE: Launching Pentaho Report Designer for Hive JDBC in Bash
DESCRIPTION: This command launches the Pentaho Report Designer with the modified configuration for Hive JDBC integration.

LANGUAGE: bash
CODE:
$ sh reporter-designer.sh

----------------------------------------

TITLE: WebHCat Delete Job Response Format
DESCRIPTION: Example JSON response returned when deleting a Hadoop job, showing job status, profile information, and completion details.

LANGUAGE: json
CODE:
{
 "status": {
            "startTime": 1321047216471,
            "username": "ctdean",
            "jobID": {
                      "jtIdentifier": "201111111311",
                      "id": 9
                     },
            "jobACLs": {
                       },
            "schedulingInfo": "NA",
            "failureInfo": "NA",
            "jobId": "job_201111111311_0009",
            "jobPriority": "NORMAL",
            "runState": 1,
            "jobComplete": false
           },
 "profile": {
             "url": "http://localhost:50030/jobdetails.jsp?jobid=job_201111111311_0009",
             "user": "ctdean",
             "jobID": {
                       "jtIdentifier": "201111111311",
                       "id": 9
                      },
             "queueName": "default",
             "jobFile": "hdfs://localhost:9000/tmp/hadoop-ctdean/mapred/staging/ctdean/.staging/job_201111111311_0009/job.xml",
             "jobName": "streamjob3322518350676530377.jar",
             "jobId": "job_201111111311_0009"
            }
 "id": "job_201111111311_0009",
 "parentId": "job_201111111311_0008",
 "percentComplete": "10% complete",
 "exitValue": 0,
 "user": "ctdean",
 "callback": null,
 "completed": "false"
}

----------------------------------------

TITLE: JPOX Properties Configuration
DESCRIPTION: JPOX configuration settings for Derby connection (for versions prior to Hive 5.0)

LANGUAGE: plaintext
CODE:
javax.jdo.PersistenceManagerFactoryClass=org.jpox.PersistenceManagerFactoryImpl
org.jpox.autoCreateSchema=false
org.jpox.validateTables=false
org.jpox.validateColumns=false
org.jpox.validateConstraints=false
org.jpox.storeManagerType=rdbms
org.jpox.autoCreateSchema=true
org.jpox.autoStartMechanismMode=checked
org.jpox.transactionIsolation=read_committed
javax.jdo.option.DetachAllOnCommit=true
javax.jdo.option.NontransactionalRead=true
javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.ClientDriver
javax.jdo.option.ConnectionURL=jdbc:derby://hadoop1:1527/metastore_db;create=true
javax.jdo.option.ConnectionUserName=APP
javax.jdo.option.ConnectionPassword=mine

----------------------------------------

TITLE: Configuring Protobuf for ARM
DESCRIPTION: Steps to download, configure, and install Protobuf 2.5.0, including ARM-specific modifications.

LANGUAGE: bash
CODE:
wget https://github.com/google/protobuf/releases/download/v2.5.0/protobuf-2.5.0.tar.bz2
tar -xvf protobuf-2.5.0.tar.bz2
cd protobuf-2.5.0
./configure

# On ARM, edit src/google/protobuf/stubs/platform_macros.h

make
make check
sudo make install

----------------------------------------

TITLE: Case W2: Where Predicate Example - Left Outer Join with Null Supplying Table
DESCRIPTION: Shows a left outer join with where predicate on the null supplying table (s2). Includes explain plan showing filter operator placement after join.

LANGUAGE: sql
CODE:
explain
select s1.key, s2.key 
from src s1 left join src s2 
where s2.key > '2';

----------------------------------------

TITLE: Dynamic Partitioning in Pig
DESCRIPTION: This Pig script shows how to use dynamic partitioning by not specifying partition values when storing data. HCatalog will automatically determine the partitions based on the data.

LANGUAGE: pig
CODE:
A = load 'raw' using HCatLoader(); 
... 
store Z into 'processed' using HCatStorer(); 

----------------------------------------

TITLE: Hive Query with LZO Compression Disabled
DESCRIPTION: Configuration parameters for disabling compression when using custom Java implementation.

LANGUAGE: sql
CODE:
SET hive.exec.compress.output=false
SET mapreduce.output.fileoutputformat.compress=false

----------------------------------------

TITLE: Enabling BI Mode in Hive
DESCRIPTION: Configuration setting to enable Business Intelligence mode for automatic sketch-based query optimizations

LANGUAGE: sql
CODE:
set hive.optimize.bi.enabled=true;

----------------------------------------

TITLE: Updating with DEFAULT Values in Apache Hive SQL
DESCRIPTION: Example of using the proposed DEFAULT keyword in an UPDATE statement. This allows setting columns to their default values during an update operation.

LANGUAGE: sql
CODE:
UPDATE TABLE1 SET COL1=DEFAULT, COL2=DEFAULT WHERE <CONDITION>

----------------------------------------

TITLE: Schema Version Error in Hive Log
DESCRIPTION: This snippet shows the error message in the Hive log when version information is not found in the metastore.

LANGUAGE: text
CODE:
...
Caused by: MetaException(message:Version information not found in metastore. )
...

----------------------------------------

TITLE: Creating Feature Branch
DESCRIPTION: Commands to create and push a feature branch for development

LANGUAGE: bash
CODE:
# fetch all changes - so you will create your feature branch on top of the current master
git fetch --all
# create a feature branch This branch name can be anything - including the ticket id may help later on identifying the branch.
git branch HIVE-9999-something apache/master
git checkout HIVE-9999-something
# push your feature branch to your github fork - and set that branch as upstream to this branch
git push GITHUB_USER -u HEAD

----------------------------------------

TITLE: Registering Temporary UDF in Hive
DESCRIPTION: SQL command to register a custom UDF as a temporary function in Hive, making it available for the current session.

LANGUAGE: sql
CODE:
create temporary function my_lower as 'com.example.hive.udf.Lower';

----------------------------------------

TITLE: Enabling Reoptimize Strategy in Apache Hive SQL
DESCRIPTION: This snippet shows how to enable both the Overlay and Reoptimize strategies for query reexecution in Hive.

LANGUAGE: sql
CODE:
set hive.query.reexecution.strategies=overlay,reoptimize;

----------------------------------------

TITLE: Tabular Data Structure Example
DESCRIPTION: Example table showing object counts and sizes for metastore cache memory footprint analysis

LANGUAGE: markdown
CODE:
| object | count | Avg size (byte) |
| --- | --- | --- |
| table | 895 | 1576 |
| partition | 97,863 | 591 |
| storagedescriptor | 412 | 680 |

----------------------------------------

TITLE: Creating Remote Database in Apache Hive
DESCRIPTION: This snippet shows how to create a remote database in Hive using the previously created HIVEJDBC connector. It maps a remote database to a local Hive database.

LANGUAGE: SQL
CODE:
CREATE REMOTE DATABASE hiveserver_remote USING hiveserver_connector   
 WITH DBPROPERTIES ("connector.remoteDbName"="default");

----------------------------------------

TITLE: Hive Import Command Syntax
DESCRIPTION: Basic syntax for the IMPORT command to import table data and metadata from a specified location. Supports external tables, custom locations, and partition specifications.

LANGUAGE: sql
CODE:
IMPORT [[EXTERNAL] TABLE new_or_original_tablename [PARTITION (part_column="value"[, ...])]] 
  FROM 'source_path' 
  [LOCATION 'import_target_path']

----------------------------------------

TITLE: Initializing Hive Schema
DESCRIPTION: This example demonstrates how to initialize the Hive schema to the current version for a new setup using Derby as the database.

LANGUAGE: bash
CODE:
$ schematool -dbType derby -initSchema Initializing the schema to: 4.0.0-beta-2
Metastore connection URL:	 jdbc:derby:;databaseName=metastore_db;create=true
Metastore connection Driver :	 org.apache.derby.jdbc.EmbeddedDriver
Metastore connection User:	 APP
Starting metastore schema initialization to 4.0.0-beta-2
Initialization script hive-schema-4.0.0-beta-2.derby.sql
Initialization script completed

----------------------------------------

TITLE: Fetching Apache Hive Source Code
DESCRIPTION: Instructions for obtaining Hive source code either by downloading the source tar or cloning from Git with a specific release tag.

LANGUAGE: shell
CODE:
git clone --branch rel/release-4.0.0 https://github.com/apache/hive.git

----------------------------------------

TITLE: Demonstrating Overlay Strategy in Apache Hive SQL
DESCRIPTION: This snippet shows how to use the Overlay strategy for query reexecution in Hive. It sets up reexecution parameters, creates a table, inserts data, and runs an assertion query.

LANGUAGE: sql
CODE:
set zzz=1;
set reexec.overlay.zzz=2;

set hive.query.reexecution.enabled=true;
set hive.query.reexecution.strategies=overlay;

create table t(a int);
insert into t values (1);
select assert_true(${hiveconf:zzz} > a) from t group by a;

----------------------------------------

TITLE: Git Remote Setup
DESCRIPTION: Commands to set up Git remotes for contributing to Hive

LANGUAGE: bash
CODE:
# clone the apache/hive repo from github
git clone --origin apache https://github.com/apache/hive
cd hive
# add your own fork as a remote
git remote add GITHUB_USER git@github.com:GITHUB_USER/hive

----------------------------------------

TITLE: Creating a Wiki Edit Events Table in Hive
DESCRIPTION: Shows the creation of a 'wiki' table that stores edit events for a website.

LANGUAGE: SQL
CODE:
CREATE TABLE `wiki` (
`time` TIMESTAMP,
`page` STRING,
`user` STRING,
`characters_added` BIGINT,
`characters_removed` BIGINT)
STORED AS ORC
TBLPROPERTIES ('transactional'='true');

----------------------------------------

TITLE: Using RCFileCat for Metadata Analysis in Hive
DESCRIPTION: Command line usage for analyzing metadata of RC files, introduced in version 0.11.0. Provides options to display column sizes in either raw or human-friendly format, showing both compressed and uncompressed sizes for each column.

LANGUAGE: bash
CODE:
hive --rcfilecat [--column-sizes | --column-sizes-pretty] fileName

----------------------------------------

TITLE: Subscribing to HCatalog Notification Topic in Java
DESCRIPTION: Creates a JMS session and subscribes to the HCatalog notification topic for a specific table.

LANGUAGE: java
CODE:
Session session = conn.createSession(true, Session.SESSION_TRANSACTED);
Destination hcatTopic = session.createTopic(topicName);
MessageConsumer consumer = session.createConsumer(hcatTopic);
consumer.setMessageListener(this);

----------------------------------------

TITLE: Alter Table Compaction Example
DESCRIPTION: Examples of altering table compaction properties

LANGUAGE: SQL
CODE:
ALTER TABLE table_name COMPACT 'minor' 
   WITH OVERWRITE TBLPROPERTIES ("compactor.mapreduce.map.memory.mb"="3072");  -- specify compaction map job properties
ALTER TABLE table_name COMPACT 'major'
   WITH OVERWRITE TBLPROPERTIES ("tblprops.orc.compress.size"="8192");         -- change any other Hive table properties

----------------------------------------

TITLE: Configuring Spark YARN Jar Caching
DESCRIPTION: XML configuration to allow YARN to cache Spark dependency jars on nodes.

LANGUAGE: xml
CODE:
<property>
  <name>spark.yarn.jar</name>
  <value>hdfs://xxxx:8020/spark-assembly.jar</value>
</property>

LANGUAGE: xml
CODE:
<property>
  <name>spark.yarn.jars</name>
  <value>hdfs://xxxx:8020/spark-jars/*</value>
</property>

----------------------------------------

TITLE: Configuring Compaction Worker Pool in Hive Properties
DESCRIPTION: Property setting to assign databases, tables and partitions to specific compaction pools. This configuration can be set at database or table level.

LANGUAGE: properties
CODE:
hive.compactor.worker.pool={pool_name}

----------------------------------------

TITLE: Configuring LDAP Group Filter in Hive XML
DESCRIPTION: Example of setting the hive.server2.authentication.ldap.groupFilter property in hive-site.xml to specify allowed LDAP groups for authentication.

LANGUAGE: xml
CODE:
<property>
  <name>hive.server2.authentication.ldap.groupFilter</name>
  <value>group1,group2</value>
</property>

----------------------------------------

TITLE: Moving Database between Catalogs
DESCRIPTION: This example demonstrates how to move a database from the default Hive catalog to a custom Spark catalog.

LANGUAGE: bash
CODE:
build/dist/bin/schematool -moveDatabase db1 -fromCatalog hive -toCatalog spark

----------------------------------------

TITLE: Configuring Pool Timeout in Hive Properties
DESCRIPTION: Property setting to define the timeout period after which compaction requests fall back to the default pool if not processed by labeled pools.

LANGUAGE: properties
CODE:
hive.compactor.worker.pool.timeout

----------------------------------------

TITLE: Statistics Aggregator Interface Implementation in Java
DESCRIPTION: Interface definition for implementing statistics aggregation functionality in Hive. Provides methods for initialization, aggregating statistics, and cleanup.

LANGUAGE: java
CODE:
package org.apache.hadoop.hive.ql.stats;

import org.apache.hadoop.conf.Configuration;

public interface IStatsAggregator {
  public boolean init(Configuration hconf);
  public String aggregateStats(String rowID, String key);
  public boolean terminate();
}

----------------------------------------

TITLE: Multiple Aggregations Example
DESCRIPTION: Demonstrates multiple aggregation functions used together with the same DISTINCT column.

LANGUAGE: sql
CODE:
INSERT OVERWRITE TABLE pv_gender_agg
SELECT pv_users.gender, count(DISTINCT pv_users.userid), count(*), sum(DISTINCT pv_users.userid)
FROM pv_users
GROUP BY pv_users.gender;

----------------------------------------

TITLE: Receiving HCatalog Partition Set Notifications in Java
DESCRIPTION: Shows how to receive and process notifications for completed partition sets in HCatalog using JMS.

LANGUAGE: java
CODE:
public void onMessage(Message msg) {

                                
  MapMessage mapMsg = (MapMessage)msg;
  Enumeration<String> keys = mapMsg.getMapNames();
  
  // Enumerate over all keys. This will print key-value pairs specifying the  
  // particular partition 44which was marked done. In this case, it will print:
  // date : 20110711
  // country: *

  while(keys.hasMoreElements()){
    String key = keys.nextElement();
    System.out.println(key + " : " + mapMsg.getString(key));
  }
  System.out.println("Message: "+msg);

----------------------------------------

TITLE: Specifying user.name in GET request for WebHCat API
DESCRIPTION: This snippet demonstrates how to include the user.name parameter in a GET request to the WebHCat API for retrieving table information. It uses curl to make the HTTP request.

LANGUAGE: bash
CODE:
% curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/my_table?user.name=ctdean'

----------------------------------------

TITLE: Example Code References
DESCRIPTION: References to example implementations showing actual usage of the Streaming Mutation API in test cases.

LANGUAGE: java
CODE:
See:
- ExampleUseCase.java
- TestMutations.testUpdatesAndDeletes()

----------------------------------------

TITLE: Recommended Hive on Spark Configuration Settings
DESCRIPTION: Recommended configuration settings for optimizing Hive on Spark performance.

LANGUAGE: sql
CODE:
mapreduce.input.fileinputformat.split.maxsize=750000000
hive.vectorized.execution.enabled=true

hive.cbo.enable=true
hive.optimize.reducededuplication.min.reducer=4
hive.optimize.reducededuplication=true
hive.orc.splits.include.file.footer=false
hive.merge.mapfiles=true
hive.merge.sparkfiles=false
hive.merge.smallfiles.avgsize=16000000
hive.merge.size.per.task=256000000
hive.merge.orcfile.stripe.level=true
hive.auto.convert.join=true
hive.auto.convert.join.noconditionaltask=true
hive.auto.convert.join.noconditionaltask.size=894435328
hive.optimize.bucketmapjoin.sortedmerge=false
hive.map.aggr.hash.percentmemory=0.5
hive.map.aggr=true
hive.optimize.sort.dynamic.partition=false
hive.stats.autogather=true
hive.stats.fetch.column.stats=true
hive.vectorized.execution.reduce.enabled=false
hive.vectorized.groupby.checkinterval=4096
hive.vectorized.groupby.flush.percent=0.1
hive.compute.query.using.stats=true
hive.limit.pushdown.memory.usage=0.4
hive.optimize.index.filter=true
hive.exec.reducers.bytes.per.reducer=67108864
hive.smbjoin.cache.rows=10000
hive.exec.orc.default.stripe.size=67108864
hive.fetch.task.conversion=more
hive.fetch.task.conversion.threshold=1073741824
hive.fetch.task.aggr=false
mapreduce.input.fileinputformat.list-status.num-threads=5
spark.kryo.referenceTracking=false
spark.kryo.classesToRegister=org.apache.hadoop.hive.ql.io.HiveKey,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch

----------------------------------------

TITLE: Analyzing Contextual N-grams from Kafka Data in Apache Hive
DESCRIPTION: This query shows how to use context_ngrams() with explode() to find the top 10 words following "he" in Kafka data, demonstrating contextual n-gram analysis.

LANGUAGE: SQL
CODE:
SELECT explode(context_ngrams(sentences(lower(val)), array("he", null), 10)) AS x FROM kafka;

----------------------------------------

TITLE: Hive Schema Tool Command Usage
DESCRIPTION: This snippet outlines the usage and available options for the schematool command in Hive.

LANGUAGE: bash
CODE:
$ usage: schemaTool
 -alterCatalog <arg>                Alter a catalog, requires
                                    --catalogLocation and/or
                                    --catalogDescription parameter as well
 -catalogDescription <arg>          Description of new catalog
 -catalogLocation <arg>             Location of new catalog, required when
                                    adding a catalog
 -createCatalog <arg>               Create a catalog, requires
                                    --catalogLocation parameter as well
 -createLogsTable <arg>             Create table for Hive
                                    warehouse/compute logs
 -createUser                        Create the Hive user, set hiveUser to
                                    the db admin user and the hive
                                    password to the db admin password with
                                    this
 -dbOpts <databaseOpts>             Backend DB specific options
 -dbType <databaseType>             Metastore database type
 -driver <driver>                   driver name for connection
 -dropAllDatabases                  Drop all Hive databases (with
                                    CASCADE). This will remove all managed
                                    data!
 -dryRun                            list SQL scripts (no execute)
 -fromCatalog <arg>                 Catalog a moving database or table is
                                    coming from.  This is required if you
                                    are moving a database or table.
 -fromDatabase <arg>                Database a moving table is coming
                                    from.  This is required if you are
                                    moving a table.
 -help                              print this message
 -hiveDb <arg>                      Hive database (for use with
                                    createUser)
 -hivePassword <arg>                Hive password (for use with
                                    createUser)
 -hiveUser <arg>                    Hive user (for use with createUser)
 -ifNotExists                       If passed then it is not an error to
                                    create an existing catalog
 -info                              Show config and schema details
 -initOrUpgradeSchema               Initialize or upgrade schema to latest
                                    version
 -initSchema                        Schema initialization
 -initSchemaTo <initTo>             Schema initialization to a version
 -mergeCatalog <arg>                Merge databases from a catalog into
                                    other, Argument is the source catalog
                                    name Requires --toCatalog to indicate
                                    the destination catalog
 -metaDbType <metaDatabaseType>     Used only if upgrading the system
                                    catalog for hive
 -moveDatabase <arg>                Move a database between catalogs.
                                    Argument is the database name.
                                    Requires --fromCatalog and --toCatalog
                                    parameters as well
 -moveTable <arg>                   Move a table to a different database.
                                    Argument is the table name. Requires
                                    --fromCatalog, --toCatalog,
                                    --fromDatabase, and --toDatabase
                                    parameters as well.
 -passWord <password>               Override config file password
 -retentionPeriod <arg>             Specify logs table retention period
 -servers <serverList>              a comma-separated list of servers used
                                    in location validation in the format
                                    of scheme://authority (e.g.
                                    hdfs://localhost:8000)
 -toCatalog <arg>                   Catalog a moving database or table is
                                    going to.  This is required if you are
                                    moving a database or table.
 -toDatabase <arg>                  Database a moving table is going to.
                                    This is required if you are moving a
                                    table.
 -upgradeSchema                     Schema upgrade
 -upgradeSchemaFrom <upgradeFrom>   Schema upgrade from a version
 -url <url>                         connection url to the database
 -userName <user>                   Override config file user name
 -validate                          Validate the database
 -verbose                           only print SQL statements
 -yes                               Don't ask for confirmation when using
                                    -dropAllDatabases.

----------------------------------------

TITLE: Configuring Hive Test Properties
DESCRIPTION: Property file configuration defining which test files are executed by different test drivers in Apache Hive. Includes settings for MinimrCliDriver, MiniTezCliDriver, and BeeLine tests.

LANGUAGE: properties
CODE:
minimr.query.files: Tests run by TestMinimrCliDriver (positive)
minimr.query.negative.files: Tests run by TestMinimrCliDriver (negative)
minitez.query.files.shared: Tests run by TestMinimrCliDriver and TestMiniTezCliDriver
minitez.query.files: Tests run by TestMiniTezCliDriver
beeline.positive.exclude: Tests excluded when running with TestBeeLineDriver

----------------------------------------

TITLE: Data Processing with Pig (Without HCatalog)
DESCRIPTION: Traditional Pig script for data processing without using HCatalog, requiring manual path specifications and data availability checks.

LANGUAGE: pig
CODE:
A = load '/data/rawevents/20100819/data' as (alpha:int, beta:chararray, ...);
B = filter A by bot_finder(zeta) = 0;
...
store Z into 'data/processedevents/20100819/data';

----------------------------------------

TITLE: Preparing HCatReader Context in Java
DESCRIPTION: This snippet illustrates how to obtain a ReaderContext from an HCatReader instance.

LANGUAGE: Java
CODE:
ReaderContext cntxt = reader.prepareRead();

----------------------------------------

TITLE: WebHCat Status Response in JSON
DESCRIPTION: This JSON output represents the response from the WebHCat status endpoint, indicating the server status and API version.

LANGUAGE: json
CODE:
{
 "status": "ok",
 "version": "v1"
}

----------------------------------------

TITLE: Setting Hive Execution Engine to Spark
DESCRIPTION: SQL command to configure Hive to use Spark as the execution engine.

LANGUAGE: sql
CODE:
set hive.execution.engine=spark;

----------------------------------------

TITLE: Setting Hive scratch directory in CLI
DESCRIPTION: Example of using the set command in the Hive CLI to set the scratch directory configuration variable for the current session.

LANGUAGE: shell
CODE:
set hive.exec.scratchdir=/tmp/mydir;

----------------------------------------

TITLE: Creating Table Column Statistics Table in Metastore
DESCRIPTION: SQL command to create the TAB_COL_STATS table for storing table column statistics in the Hive metastore.

LANGUAGE: SQL
CODE:
CREATE TABLE TAB_COL_STATS  
 (
 CS_ID NUMBER NOT NULL,  
 TBL_ID NUMBER NOT NULL,  
 COLUMN_NAME VARCHAR(128) NOT NULL,  
 COLUMN_TYPE VARCHAR(128) NOT NULL,  
 TABLE_NAME VARCHAR(128) NOT NULL,  
 DB_NAME VARCHAR(128) NOT NULL,

LOW_VALUE RAW,  
 HIGH_VALUE RAW,  
 NUM_NULLS BIGINT,  
 NUM_DISTINCTS BIGINT,

BIT_VECTOR, BLOB,  /* introduced in [HIVE-16997](https://issues.apache.org/jira/browse/HIVE-16997) in Hive 3.0.0 */

AVG_COL_LEN DOUBLE,  
 MAX_COL_LEN BIGINT,  
 NUM_TRUES BIGINT,  
 NUM_FALSES BIGINT,  
 LAST_ANALYZED BIGINT NOT NULL)

----------------------------------------

TITLE: Querying Partitioned Views in Apache Hive
DESCRIPTION: Shows an example of how Hive internally compiles a query to select from a partitioned view. This is used to provide dependency information for hooks during ALTER VIEW ADD PARTITION operations.

LANGUAGE: SQL
CODE:
SELECT * FROM view_name
WHERE view_partition_col1 = 'val1' AND view_partition_col=2 = 'val2' ...

----------------------------------------

TITLE: Initializing Hive Schema to Specific Version
DESCRIPTION: This example demonstrates how to initialize the Hive schema to a specific version (3.1.0) using Derby as the database.

LANGUAGE: bash
CODE:
$ schematool -dbType derby -initSchemaTo 3.1.0 
Metastore connection URL:	 jdbc:derby:;databaseName=metastore_db;create=true
Metastore connection Driver :	 org.apache.derby.jdbc.EmbeddedDriver
Metastore connection User:	 APP
Starting metastore schema initialization to 3.1.0
Initialization script hive-schema-3.1.0.derby.sql

----------------------------------------

TITLE: Generating Numeric Histogram from Normal Distribution in Apache Hive
DESCRIPTION: This query demonstrates how to use histogram_numeric() with explode() to generate and display a 10-bin histogram from a normally distributed dataset.

LANGUAGE: SQL
CODE:
SELECT explode(histogram_numeric(val, 10)) AS x FROM normal;

----------------------------------------

TITLE: Configuring Hadoop Cluster Connection Properties
DESCRIPTION: Configuration variables required to connect Hive CLI to different Hadoop clusters. These properties specify the filesystem and job tracker locations.

LANGUAGE: properties
CODE:
fs.default.name
mapred.job.tracker

----------------------------------------

TITLE: Connecting to Accumulo via Hive CLI
DESCRIPTION: Command to launch Hive CLI with Accumulo connection parameters including instance name, ZooKeeper configuration, and authentication credentials.

LANGUAGE: bash
CODE:
hive -hiveconf accumulo.instance.name=accumulo -hiveconf accumulo.zookeepers=localhost -hiveconf accumulo.user.name=hive -hiveconf accumulo.user.pass=hive

----------------------------------------

TITLE: WebHCat API Response - JSON Output
DESCRIPTION: Example JSON response returned by the WebHCat API after successfully creating a new table, showing the created table and database names.

LANGUAGE: json
CODE:
{
 "table": "test_table_2",
 "database": "default"
}

----------------------------------------

TITLE: Querying Job Status with WebHCat REST API using cURL
DESCRIPTION: Example cURL command to retrieve job status information from WebHCat API by providing a job ID.

LANGUAGE: curl
CODE:
% curl -s 'http://localhost:50111/templeton/v1/queue/job_201112212038_0004?user.name=ctdean'

----------------------------------------

TITLE: Connecting to Beeline
DESCRIPTION: Command to connect to the Beeline CLI within the Docker container.

LANGUAGE: shell
CODE:
docker exec -it hiveserver2 beeline -u 'jdbc:hive2://hiveserver2:10000/'

----------------------------------------

TITLE: Dry Run of Hive Schema Upgrade
DESCRIPTION: This example demonstrates a dry run of the schema upgrade process, listing the required scripts without executing them.

LANGUAGE: bash
CODE:
$ schematool -dbType derby -upgradeSchemaFrom 3.1.0 -dryRun Upgrading from the user input version 3.1.0
Metastore connection URL:	 jdbc:derby:;databaseName=metastore_db;create=true
Metastore connection Driver :	 org.apache.derby.jdbc.EmbeddedDriver
Metastore connection User:	 APP
Starting upgrade metastore schema from version 3.1.0 to 4.0.0-beta-2
Upgrade script upgrade-3.1.0-to-3.2.0.derby.sql
Upgrade script upgrade-3.2.0-to-4.0.0-alpha-1.derby.sql
Upgrade script upgrade-4.0.0-alpha-1-to-4.0.0-alpha-2.derby.sql
Upgrade script upgrade-4.0.0-alpha-2-to-4.0.0-beta-1.derby.sql
Upgrade script upgrade-4.0.0-beta-1-to-4.0.0-beta-2.derby.sql

----------------------------------------

TITLE: Hive Environment Configuration
DESCRIPTION: Environment variable setup for Hadoop path in Hive

LANGUAGE: bash
CODE:
HADOOP=/opt/hadoop/hadoop-0.17.2.1/bin/hadoop
export HADOOP

----------------------------------------

TITLE: Executing TPCH query and storing results in HDFS
DESCRIPTION: Demonstrates running a complex query on S3 data using Hive and EC2, with results stored in HDFS. This example uses a modified version of TPCH query 1.

LANGUAGE: sql
CODE:
insert overwrite directory '/tmp/tpcresults-1.sql' 
  select l_returnflag, l_linestatus, sum ( l_quantity ) as sum_qty, sum ( l_extendedprice ) as sum_base_price,
  sum ( l_extendedprice * ( 1 - l_discount )) as sub_disc_price, 
  sum ( l_extendedprice * ( 1 - l_discount ) * ( 1 + l_tax )) as sum_charge,
  avg ( l_quantity ) as avg_qty, avg ( l_extendedprice ) as avg_price, 
  avg ( l_discount ) as avg_disc, count ( 1 ) as count_order
  from lineitem where l_shipdate <= to_date('1998-12-01') group by l_returnflag, l_linestatus;

----------------------------------------

TITLE: Creating Indexed Accumulo Table
DESCRIPTION: Example of creating a Hive table with Accumulo indexing support, which improves query performance by creating a separate index table for field values.

LANGUAGE: sql
CODE:
CREATE TABLE company_stats (
   rowid string,
   active_entry boolean,
   num_offices tinyint,
   num_personel smallint,
   total_manhours int,
   num_shareholders bigint,
   eff_rating float,
   err_rating double,
   yearly_production decimal,
   start_date date,
   address varchar(100),
   phone char(13),
   last_update timestamp )
ROW FORMAT SERDE 'org.apache.hadoop.hive.accumulo.serde.AccumuloSerDe'
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES (
   "accumulo.columns.mapping" = ":rowID,a:act,a:off,a:per,a:mhs,a:shs,a:eff,a:err,a:yp,a:sd,a:addr,a:ph,a:lu",
   "accumulo.table.name"="company_stats",
   "accumulo.indextable.name"="company_stats_idx"
 );

----------------------------------------

TITLE: Retrieving Partition Details using WebHCat API - Curl Example
DESCRIPTION: Example curl command to retrieve partition information from a Hive table. The command calls the WebHCat REST API endpoint with database, table and partition parameters.

LANGUAGE: curl
CODE:
% curl -s \
   'http://localhost:50111/templeton/v1/ddl/database/default/table/mytest/partition/country=%27US%27?user.name=ctdean'

----------------------------------------

TITLE: Setting and Retrieving Hive Variables
DESCRIPTION: Demonstrates how to set and retrieve Hive variables using the hiveconf namespace.

LANGUAGE: sql
CODE:
set x=myvalue;

-- Retrieve the variable
${hiveconf:x}

----------------------------------------

TITLE: Creating a Partition with WebHCat API using cURL
DESCRIPTION: This curl command demonstrates how to create a partition in an HCatalog table using the WebHCat API. It sends a PUT request with JSON payload to specify the partition location.

LANGUAGE: bash
CODE:
% curl -s -X PUT -HContent-type:application/json -d '{"location": "loc_a"}' \
       'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/partition/country=%27algeria%27?user.name=ctdean'

----------------------------------------

TITLE: Reading Data with HCatReader in Java
DESCRIPTION: This code demonstrates how to use HCatReader to read data in parallel on slave nodes.

LANGUAGE: Java
CODE:
for(InputSplit split : readCntxt.getSplits()){
HCatReader reader = DataTransferFactory.getHCatReader(split,
readerCntxt.getConf());
       Iterator<HCatRecord> itr = reader.read();
       while(itr.hasNext()){
              HCatRecord read = itr.next();
          }
}

----------------------------------------

TITLE: Moving Table between Catalogs and Databases
DESCRIPTION: This example shows how to move a table from the Hive catalog to the Spark catalog, including creating the target database if it doesn't exist.

LANGUAGE: bash
CODE:
# Create the desired target database in spark catalog if it doesn't already exist.
beeline ... -e "create database if not exists newdb";
schematool -moveDatabase newdb -fromCatalog hive -toCatalog spark

# Now move the table to target db under the spark catalog.
schematool -moveTable table1 -fromCatalog hive -toCatalog spark  -fromDatabase db1 -toDatabase newdb

----------------------------------------

TITLE: Adding Required JAR Files to HDFS
DESCRIPTION: Commands for uploading necessary HBase and Hive handler JAR files to HDFS for bulk loading operation.

LANGUAGE: bash
CODE:
hadoop dfs -put /usr/lib/hive/lib/hbase-VERSION.jar /user/hive/hbase-VERSION.jar
hadoop dfs -put /usr/lib/hive/lib/hive-hbase-handler-VERSION.jar /user/hive/hive-hbase-handler-VERSION.jar

----------------------------------------

TITLE: WebHCat Job Status Response Format (Hive 0.12.0+)
DESCRIPTION: Enhanced JSON response structure for Hive 0.12.0 and later versions, including additional user arguments in the response.

LANGUAGE: json
CODE:
{
 "status": {
            "startTime": 1324529476131,
            "username": "ctdean",
            "jobID": {
                      "jtIdentifier": "201112212038",
                      "id": 4
                     },
            "jobACLs": {
                       },
            "schedulingInfo": "NA",
            "failureInfo": "NA",
            "jobId": "job_201112212038_0004",
            "jobPriority": "NORMAL",
            "runState": 2,
            "jobComplete": true
           },
 "profile": {
             "url": "http://localhost:50030/jobdetails.jsp?jobid=job_201112212038_0004",
             "jobID": {
                       "jtIdentifier": "201112212038",
                        "id": 4
                      },
             "user": "ctdean",
             "queueName": "default",
             "jobFile": "hdfs://localhost:9000/tmp/hadoop-ctdean/mapred/staging/ctdean/.staging/job_201112212038_0004/job.xml",
             "jobName": "PigLatin:DefaultJobName",
             "jobId": "job_201112212038_0004"
            },
 "id": "job_201112212038_0004",
 "parentId": "job_201112212038_0003",
 "percentComplete": "100% complete",
 "exitValue": 0,
 "user": "ctdean",
 "callback": null,
 "completed": "done",
 "userargs" => {
                "callback"  => null,
                "define"    => [],
                "enablelog" => "false",
                "execute"   => "select a,rand(b) from mynums",
                "file"      => null,
                "files"     => [],
                "statusdir" => null,
                "user.name" => "hadoopqa",
               }
}

----------------------------------------

TITLE: Error JSON Response for Get Table Property in Apache Hive WebHCat API
DESCRIPTION: This JSON output demonstrates the error response structure when attempting to retrieve a property for a non-existent table using the WebHCat API. It includes an error message, error code, and the requested database and table names.

LANGUAGE: json
CODE:
{
  "error": "Table test_table does not exist",
  "errorCode": 404,
  "database": "default",
  "table": "test_table"
}

----------------------------------------

TITLE: Markdown Table of Hive SQL Feature Support
DESCRIPTION: A markdown table showing links to supported SQL features documentation for different Apache Hive versions

LANGUAGE: markdown
CODE:
| Version | Supported SQL Features |
| --- | --- |
| Apache Hive 2.1 | [Supported SQL Features](https://cwiki.apache.org/confluence/display/Hive/Supported+Features%3A+Apache+Hive+2.1) |
| Apache Hive 2.3 | [Supported SQL Features](https://cwiki.apache.org/confluence/display/Hive/Supported+Features%3A+Apache+Hive+2.3) |
| Apache Hive 3.1 | [Supported SQL Features]({{< ref "97551656" >}}) |

----------------------------------------

TITLE: Querying Hadoop Version via WebHCat API - Curl Example
DESCRIPTION: Example curl command to retrieve the Hadoop version information from WebHCat REST API. The request includes a required user.name parameter.

LANGUAGE: bash
CODE:
% curl -s 'http://localhost:50111/templeton/v1/version/hadoop?user.name=ekoifman'

----------------------------------------

TITLE: Displaying HiveServer2 usage message
DESCRIPTION: Command to display the usage message for HiveServer2, showing available options and syntax.

LANGUAGE: Bash
CODE:
$HIVE_HOME/bin/hive --service hiveserver2 -H

----------------------------------------

TITLE: EXPLAIN DEPENDENCY Query Example in Hive
DESCRIPTION: Shows how to use the DEPENDENCY clause with EXPLAIN to get information about query inputs.

LANGUAGE: SQL
CODE:
EXPLAIN DEPENDENCY
  SELECT key, count(1) FROM srcpart WHERE ds IS NOT NULL GROUP BY key

----------------------------------------

TITLE: Advanced Query with Multiple Join Types and Window Functions
DESCRIPTION: Advanced SQL query demonstrating complex join patterns including LEFT SEMI JOINs and multiple table relationships.

LANGUAGE: sql
CODE:
SELECT count(distinct ws1.ws_order_number) as order_count,
       sum(ws1.ws_ext_ship_cost) as total_shipping_cost,
       sum(ws1.ws_net_profit) as total_net_profit
FROM web_sales ws1
JOIN /*MJ1*/ customer_address ca ON (ws1.ws_ship_addr_sk = ca.ca_address_sk)
JOIN /*MJ2*/ web_site s ON (ws1.ws_web_site_sk = s.web_site_sk)
JOIN /*MJ3*/ date_dim d ON (ws1.ws_ship_date_sk = d.d_date_sk)
LEFT SEMI JOIN /*JOIN4*/ (SELECT ws2.ws_order_number as ws_order_number
               	          FROM web_sales ws2 JOIN /*JOIN1*/ web_sales ws3
               	          ON (ws2.ws_order_number = ws3.ws_order_number)
               	          WHERE ws2.ws_warehouse_sk <> ws3.ws_warehouse_sk) ws_wh1
ON (ws1.ws_order_number = ws_wh1.ws_order_number)
LEFT SEMI JOIN /*JOIN4*/ (SELECT wr_order_number
               	          FROM web_returns wr
               	          JOIN /*JOIN3*/ (SELECT ws4.ws_order_number as ws_order_number
                                          FROM web_sales ws4 JOIN /*JOIN2*/ web_sales ws5
                                          ON (ws4.ws_order_number = ws5.ws_order_number)
                                          WHERE ws4.ws_warehouse_sk <> ws5.ws_warehouse_sk) ws_wh2
                          ON (wr.wr_order_number = ws_wh2.ws_order_number)) tmp1
ON (ws1.ws_order_number = tmp1.wr_order_number)
WHERE d.d_date >= '2001-05-01' and
      d.d_date <= '2001-06-30' and
      ca.ca_state = 'NC' and
      s.web_company_name = 'pri';

----------------------------------------

TITLE: Advanced Query with Multiple Join Types and Window Functions
DESCRIPTION: Advanced SQL query demonstrating complex join patterns including LEFT SEMI JOINs and multiple table relationships.

LANGUAGE: sql
CODE:
SELECT count(distinct ws1.ws_order_number) as order_count,
       sum(ws1.ws_ext_ship_cost) as total_shipping_cost,
       sum(ws1.ws_net_profit) as total_net_profit
FROM web_sales ws1
JOIN /*MJ1*/ customer_address ca ON (ws1.ws_ship_addr_sk = ca.ca_address_sk)
JOIN /*MJ2*/ web_site s ON (ws1.ws_web_site_sk = s.web_site_sk)
JOIN /*MJ3*/ date_dim d ON (ws1.ws_ship_date_sk = d.d_date_sk)
LEFT SEMI JOIN /*JOIN4*/ (SELECT ws2.ws_order_number as ws_order_number
               	          FROM web_sales ws2 JOIN /*JOIN1*/ web_sales ws3
               	          ON (ws2.ws_order_number = ws3.ws_order_number)
               	          WHERE ws2.ws_warehouse_sk <> ws3.ws_warehouse_sk) ws_wh1
ON (ws1.ws_order_number = ws_wh1.ws_order_number)
LEFT SEMI JOIN /*JOIN4*/ (SELECT wr_order_number
               	          FROM web_returns wr
               	          JOIN /*JOIN3*/ (SELECT ws4.ws_order_number as ws_order_number
                                          FROM web_sales ws4 JOIN /*JOIN2*/ web_sales ws5
                                          ON (ws4.ws_order_number = ws5.ws_order_number)
                                          WHERE ws4.ws_warehouse_sk <> ws5.ws_warehouse_sk) ws_wh2
                          ON (wr.wr_order_number = ws_wh2.ws_order_number)) tmp1
ON (ws1.ws_order_number = tmp1.wr_order_number)
WHERE d.d_date >= '2001-05-01' and
      d.d_date <= '2001-06-30' and
      ca.ca_state = 'NC' and
      s.web_company_name = 'pri';

----------------------------------------

TITLE: ORC File Dump Utility Commands
DESCRIPTION: Command line syntax for the ORC file dump utility across different Hive versions. Shows various options for analyzing ORC files including metadata dumps, row index inspection, and file recovery.

LANGUAGE: bash
CODE:
// Hive version 0.11 through 0.14:
hive --orcfiledump <location-of-orc-file>

// Hive version 1.1.0 and later:
hive --orcfiledump [-d] [--rowindex <col_ids>] <location-of-orc-file>

// Hive version 1.2.0 and later:
hive --orcfiledump [-d] [-t] [--rowindex <col_ids>] <location-of-orc-file>

// Hive version 1.3.0 and later:
hive --orcfiledump [-j] [-p] [-d] [-t] [--rowindex <col_ids>] [--recover] [--skip-dump] 
    [--backup-path <new-path>] <location-of-orc-file-or-directory>

----------------------------------------

TITLE: Installing Python Dependencies Locally
DESCRIPTION: Commands for installing Python modules in user's home directory when root access is unavailable

LANGUAGE: bash
CODE:
easy_install --prefix "~/.python_modules" argparse
easy_install --prefix "~/.python_modules" mako

----------------------------------------

TITLE: Querying Table Columns via WebHCat API - Curl Example
DESCRIPTION: Example curl command demonstrating how to retrieve column information from a table named 'my_table' in the 'default' database.

LANGUAGE: bash
CODE:
% curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/my_table/column?user.name=ctdean'

----------------------------------------

TITLE: Spatial Join Query in HQL
DESCRIPTION: Example of a spatial JOIN query using ST_INTERSECTS predicate to compare spatial columns between two tables.

LANGUAGE: HQL
CODE:
SELECT * FROM a JOIN b on ST_INTERSECTS(a.spatialcolumn, b.spatialcolumn) = TRUE;

----------------------------------------

TITLE: Retrieving Hive Version using WebHCat API with curl
DESCRIPTION: This curl command demonstrates how to make a GET request to the WebHCat API endpoint for retrieving the Hive version. It includes the required user.name parameter.

LANGUAGE: curl
CODE:
% curl -s 'http://localhost:50111/templeton/v1/version/hive?user.name=ekoifman'

----------------------------------------

TITLE: Starting HiveServer in Hive 0.8 and Later
DESCRIPTION: Commands to start the Thrift HiveServer in Apache Hive versions 0.8 and later. Includes usage help and the actual start command.

LANGUAGE: bash
CODE:
$ build/dist/bin/hive --service hiveserver --help
usage: hiveserver
 -h,--help                        Print help information
    --hiveconf <property=value>   Use value for given property
    --maxWorkerThreads <arg>      maximum number of worker threads,
                                  default:2147483647
    --minWorkerThreads <arg>      minimum number of worker threads,
                                  default:100
 -p <port>                        Hive Server port number, default:10000
 -v,--verbose                     Verbose mode

$ bin/hive --service hiveserver

----------------------------------------

TITLE: Starting HiveServer in Hive 0.8 and Later
DESCRIPTION: Commands to start the Thrift HiveServer in Apache Hive versions 0.8 and later. Includes usage help and the actual start command.

LANGUAGE: bash
CODE:
$ build/dist/bin/hive --service hiveserver --help
usage: hiveserver
 -h,--help                        Print help information
    --hiveconf <property=value>   Use value for given property
    --maxWorkerThreads <arg>      maximum number of worker threads,
                                  default:2147483647
    --minWorkerThreads <arg>      minimum number of worker threads,
                                  default:100
 -p <port>                        Hive Server port number, default:10000
 -v,--verbose                     Verbose mode

$ bin/hive --service hiveserver

----------------------------------------

TITLE: Filesystem Layout Example for ACID Table
DESCRIPTION: Example showing the directory structure of a transactional Hive table including base and delta files

LANGUAGE: bash
CODE:
hive> dfs -ls -R /user/hive/warehouse/t;
drwxr-xr-x   - ekoifman staff          0 2016-06-09 17:03 /user/hive/warehouse/t/base_0000022
-rw-r--r--   1 ekoifman staff        602 2016-06-09 17:03 /user/hive/warehouse/t/base_0000022/bucket_00000
drwxr-xr-x   - ekoifman staff          0 2016-06-09 17:06 /user/hive/warehouse/t/delta_0000023_0000023_0000
-rw-r--r--   1 ekoifman staff        611 2016-06-09 17:06 /user/hive/warehouse/t/delta_0000023_0000023_0000/bucket_00000
drwxr-xr-x   - ekoifman staff          0 2016-06-09 17:07 /user/hive/warehouse/t/delta_0000024_0000024_0000
-rw-r--r--   1 ekoifman staff        610 2016-06-09 17:07 /user/hive/warehouse/t/delta_0000024_0000024_0000/bucket_00000

----------------------------------------

TITLE: Using Custom Map/Reduce Scripts in Hive
DESCRIPTION: Example of using TRANSFORM clause to embed custom mapper and reducer scripts in a Hive query.

LANGUAGE: HiveQL
CODE:
FROM (
    FROM pv_users
    MAP pv_users.userid, pv_users.date
    USING 'map_script'
    AS dt, uid
    CLUSTER BY dt) map_output

INSERT OVERWRITE TABLE pv_users_reduced
    REDUCE map_output.dt, map_output.uid
    USING 'reduce_script'
    AS date, count;

----------------------------------------

TITLE: Querying Database Description with cURL
DESCRIPTION: Example curl command to retrieve database description from WebHCat API endpoint. The command makes a GET request to the database description endpoint with the database name 'newdb' and user authentication parameter.

LANGUAGE: bash
CODE:
% curl -s 'http://localhost:50111/templeton/v1/ddl/database/newdb?user.name=ctdean'

----------------------------------------

TITLE: Executing Hive Query with Incompatible Schema
DESCRIPTION: Example of a failed Hive query execution due to incompatible metastore schema version.

LANGUAGE: bash
CODE:
$ build/dist/bin/hive -e "show tables"
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient

----------------------------------------

TITLE: WebHCat Hadoop Version API Response Format
DESCRIPTION: Example JSON response from the WebHCat version/hadoop endpoint showing the Hadoop module version information.

LANGUAGE: json
CODE:
[
{"module":"hadoop","version":"2.4.1-SNAPSHOT}
]

----------------------------------------

TITLE: Moving Database Between Catalogs
DESCRIPTION: Example of moving a database from the default Hive catalog to a custom Spark catalog.

LANGUAGE: bash
CODE:
build/dist/bin/schematool -moveDatabase db1 -fromCatalog hive -toCatalog spark

----------------------------------------

TITLE: Moving Database Between Catalogs
DESCRIPTION: Example of moving a database from the default Hive catalog to a custom Spark catalog.

LANGUAGE: bash
CODE:
build/dist/bin/schematool -moveDatabase db1 -fromCatalog hive -toCatalog spark

----------------------------------------

TITLE: Viewing Granted Privileges in Hive
DESCRIPTION: SQL command for displaying privilege grants for specific users, groups, or roles on specific objects.

LANGUAGE: sql
CODE:
SHOW GRANT principal_specification
[ON object_specification [(column_list)]]

----------------------------------------

TITLE: Loading Data into Hive Table from Local File
DESCRIPTION: Example of loading data from a local file into a Hive table partition.

LANGUAGE: HiveQL
CODE:
LOAD DATA LOCAL INPATH /tmp/pv_2008-06-08_us.txt INTO TABLE page_view PARTITION(date='2008-06-08', country='US')

----------------------------------------

TITLE: Supported Scalar Subquery Example
DESCRIPTION: Example of a valid scalar subquery that will be supported in the implementation.

LANGUAGE: sql
CODE:
SELECT customer.customer_num,
	(SELECT SUM(ship_charge) 
		FROM orders
		WHERE customer.customer_num = orders.customer_num
	) AS total_ship_chg
FROM customer

----------------------------------------

TITLE: Retrieving Table Partitions using WebHCat API - Curl Example
DESCRIPTION: Example curl command to retrieve partition information from a table named 'my_table' in the 'default' database. The request requires user authentication via user.name parameter.

LANGUAGE: curl
CODE:
% curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/my_table/partition?user.name=ctdean'

----------------------------------------

TITLE: Creating Table with MultiDelimitSerDe in Hive
DESCRIPTION: Example of creating a Hive table using MultiDelimitSerDe with custom delimiters for fields, collections, and map key-value pairs. The SerDe supports multiple-character field delimiters and single-character delimiters for collections and map keys.

LANGUAGE: sql
CODE:
CREATE TABLE test (
 id string,
 hivearray array<binary>,
 hivemap map<string,int>) 
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.MultiDelimitSerDe'                  
WITH SERDEPROPERTIES ("field.delim"="[,]","collection.delim"=":","mapkey.delim"="@");

----------------------------------------

TITLE: Creating a Column in HCatalog Table using WebHCat API - Curl Command
DESCRIPTION: This curl command demonstrates how to create a new column named 'brand' of type 'string' in the 'test_table' of the 'default' database using the WebHCat API. It includes the required parameters and authentication.

LANGUAGE: bash
CODE:
% curl -s -X PUT -HContent-type:application/json \
       -d '{"type": "string", "comment": "The brand name"}' \
       'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/column/brand?user.name=ctdean'

----------------------------------------

TITLE: Case W1: Where Predicate Example - Left Outer Join with Preserved Row Table
DESCRIPTION: Demonstrates a left outer join with where predicate on the preserved row table (s1). Shows the explain plan with filter operator placement.

LANGUAGE: sql
CODE:
explain 
select s1.key, s2.key 
from src s1 left join src s2 
where s1.key > '2';

----------------------------------------

TITLE: Initializing Hive System Schema
DESCRIPTION: This example shows how to initialize the Hive system schema using Derby as the metastore database.

LANGUAGE: bash
CODE:
$ ./schematool -dbType hive -metaDbType derby -initSchema  --verbose -url="jdbc:hive2://localhost:10000"

----------------------------------------

TITLE: Pre-0.8.0 Error Message
DESCRIPTION: Error message shown when running the query in strict mode before Hive 0.8.0

LANGUAGE: text
CODE:
Error in semantic analysis: No Partition Predicate Found for Alias "invites2" Table "invites2"

----------------------------------------

TITLE: Renaming a Hive Table Using WebHCat API
DESCRIPTION: Curl command to send a POST request for renaming a Hive table named 'test_table' to 'test_table_2' in the 'default' database.

LANGUAGE: bash
CODE:
% curl -s -d rename=test_table_2 \
       'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table?user.name=ekoifman'

----------------------------------------

TITLE: Case J2: Join Predicate Example - Left Outer Join with Null Supplying Table
DESCRIPTION: Shows a left outer join query where the join predicate is on the null supplying table (s2). Includes detailed explain plan output.

LANGUAGE: sql
CODE:
explain 
select s1.key, s2.key 
from src s1 left join src s2 on s2.key > '2';

----------------------------------------

TITLE: WebHCat Partition Details API Response Format
DESCRIPTION: Example JSON response showing the structure of partition details including location, formats, columns, ownership and partition information.

LANGUAGE: json
CODE:
{
  "partitioned": true,
  "location": "hdfs://ip-10-77-6-151.ec2.internal:8020/apps/hive/warehouse/mytest/loc1",
  "outputFormat": "org.apache.hadoop.hive.ql.io.RCFileOutputFormat",
  "columns": [
    {
      "name": "i",
      "type": "int"
    },
    {
      "name": "j",
      "type": "bigint"
    },
    {
      "name": "ip",
      "comment": "IP Address of the User",
      "type": "string"
    }
  ],
  "owner": "rachel",
  "partitionColumns": [
    {
      "name": "country",
      "type": "string"
    }
  ],
  "inputFormat": "org.apache.hadoop.hive.ql.io.RCFileInputFormat",
  "database": "default",
  "table": "mytest",
  "partition": "country='US'"
}

----------------------------------------

TITLE: Creating Parquet Table in Hive 0.10-0.12
DESCRIPTION: HiveQL syntax for creating a Parquet table in Hive versions 0.10 to 0.12. This example demonstrates the creation of a partitioned table with various data types using the Parquet SerDe and input/output formats.

LANGUAGE: HiveQL
CODE:
CREATE TABLE parquet_test (
 id int,
 str string,
 mp MAP<STRING,STRING>,
 lst ARRAY<STRING>,
 strct STRUCT<A:STRING,B:STRING>) 
PARTITIONED BY (part string)
ROW FORMAT SERDE 'parquet.hive.serde.ParquetHiveSerDe'
 STORED AS
 INPUTFORMAT 'parquet.hive.DeprecatedParquetInputFormat'
 OUTPUTFORMAT 'parquet.hive.DeprecatedParquetOutputFormat';

----------------------------------------

TITLE: Creating Parquet Table in Hive 0.10-0.12
DESCRIPTION: HiveQL syntax for creating a Parquet table in Hive versions 0.10 to 0.12. This example demonstrates the creation of a partitioned table with various data types using the Parquet SerDe and input/output formats.

LANGUAGE: HiveQL
CODE:
CREATE TABLE parquet_test (
 id int,
 str string,
 mp MAP<STRING,STRING>,
 lst ARRAY<STRING>,
 strct STRUCT<A:STRING,B:STRING>) 
PARTITIONED BY (part string)
ROW FORMAT SERDE 'parquet.hive.serde.ParquetHiveSerDe'
 STORED AS
 INPUTFORMAT 'parquet.hive.DeprecatedParquetInputFormat'
 OUTPUTFORMAT 'parquet.hive.DeprecatedParquetOutputFormat';

----------------------------------------

TITLE: Building Apache Hive Distribution
DESCRIPTION: Maven command to build Hive distribution with Iceberg support and integration tests, skipping unit tests.

LANGUAGE: shell
CODE:
mvn clean install -DskipTests -Pdist -Piceberg -Pitests

----------------------------------------

TITLE: Initial Query Setup in Hive
DESCRIPTION: Example SQL commands showing table creation and join query that previously caused errors in strict mode before Hive 0.8.0

LANGUAGE: sql
CODE:
set hive.mapred.mode=strict;
create table invites (foo int, bar string) partitioned by (ds string);
create table invites2 (foo int, bar string) partitioned by (ds string);
select count(*) from invites join invites2 on invites.ds=invites2.ds where invites.ds='2011-01-01';

----------------------------------------

TITLE: WebHCat API Curl Request
DESCRIPTION: Example curl command demonstrating how to submit a Pig job through WebHCat API

LANGUAGE: bash
CODE:
% curl -s -d file=id.pig \
       -d arg=-v \
       'http://localhost:50111/templeton/v1/pig?user.name=ekoifman'

----------------------------------------

TITLE: Error Response Example
DESCRIPTION: Example JSON response when table creation fails due to unsupported clustering operation.

LANGUAGE: json
CODE:
{
  "statement": "use default; create table test_table_c(id bigint, price float comment ...",
  "error": "unable to create table: test_table_c",
  "exec": {
    "stdout": "",
    "stderr": "WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated...\n        Hive history file=/tmp/ctdean/hive_job_log_ctdean_201204051335_2016086186.txt\n        SLF4J: Class path contains multiple SLF4J bindings.\n        SLF4J: Found binding in ...\n        SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n        OK\n        Time taken: 0.448 seconds\n        FAILED: Error in semantic analysis: Operation not supported. HCatalog doesn't allow Clustered By in create table.\n        ",
    "exitcode": 10
  }
}

----------------------------------------

TITLE: Retrieving Table Property Value using cURL in Apache Hive WebHCat API
DESCRIPTION: This curl command demonstrates how to retrieve the value of a specific property for a table in Apache Hive using the WebHCat API. It includes the necessary URL structure and parameters.

LANGUAGE: shell
CODE:
% curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/property/fruit?user.name=ctdean'

----------------------------------------

TITLE: Hive Index Handler Java Interface
DESCRIPTION: Java interface definition for custom index handlers in Hive, including methods for validation, build plan generation, and determining if an index table is used.

LANGUAGE: Java
CODE:
public interface HiveIndexHandler extends Configurable
{
  boolean usesIndexTable();

  void analyzeIndexDefinition(
    org.apache.hadoop.hive.metastore.api.Table baseTable,
    org.apache.hadoop.hive.metastore.api.Index index,
    org.apache.hadoop.hive.metastore.api.Table indexTable)
      throws HiveException;

  List<Task<?>> generateIndexBuildTaskList(
    org.apache.hadoop.hive.metastore.api.Table baseTable,
    org.apache.hadoop.hive.metastore.api.Index index,
    List<org.apache.hadoop.hive.metastore.api.Partition> partitions,
    org.apache.hadoop.hive.metastore.api.Table indexTable)
      throws HiveException;
}

----------------------------------------

TITLE: Hive Index Handler Java Interface
DESCRIPTION: Java interface definition for custom index handlers in Hive, including methods for validation, build plan generation, and determining if an index table is used.

LANGUAGE: Java
CODE:
public interface HiveIndexHandler extends Configurable
{
  boolean usesIndexTable();

  void analyzeIndexDefinition(
    org.apache.hadoop.hive.metastore.api.Table baseTable,
    org.apache.hadoop.hive.metastore.api.Index index,
    org.apache.hadoop.hive.metastore.api.Table indexTable)
      throws HiveException;

  List<Task<?>> generateIndexBuildTaskList(
    org.apache.hadoop.hive.metastore.api.Table baseTable,
    org.apache.hadoop.hive.metastore.api.Index index,
    List<org.apache.hadoop.hive.metastore.api.Partition> partitions,
    org.apache.hadoop.hive.metastore.api.Table indexTable)
      throws HiveException;
}

----------------------------------------

TITLE: Running Hive Unit Tests
DESCRIPTION: Maven command to run unit tests with Iceberg support enabled.

LANGUAGE: shell
CODE:
mvn clean install -Piceberg

----------------------------------------

TITLE: Plotting Histogram Data with Gnuplot
DESCRIPTION: This Gnuplot command shows how to visualize histogram data generated by Hive's histogram_numeric() function, assuming the data has been parsed into a text file of (x,y) pairs.

LANGUAGE: Gnuplot
CODE:
plot 'data.txt' u 1:2 w impulses lw 5

----------------------------------------

TITLE: Starting HiveServer in Hive 0.7 and Earlier
DESCRIPTION: Commands to start the Thrift HiveServer in Apache Hive versions 0.7 and earlier. Shows how to specify the server port and start the service.

LANGUAGE: bash
CODE:
$ build/dist/bin/hive --service hiveserver --help
usage HIVE_PORT=xxxx ./hive --service hiveserver
  HIVE_PORT : Specify the server port

$ bin/hive --service hiveserver

----------------------------------------

TITLE: Installing Python Dependencies Globally
DESCRIPTION: Commands for installing required Python modules 'argparse' and 'mako' globally using easy_install

LANGUAGE: bash
CODE:
sudo easy_install argparse
sudo easy_install mako

----------------------------------------

TITLE: Describing a Column using WebHCat API with Curl
DESCRIPTION: This curl command demonstrates how to make a GET request to the WebHCat API to describe a single column (price) in a specific table (test_table) within the default database.

LANGUAGE: bash
CODE:
% curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/column/price?user.name=ctdean'

----------------------------------------

TITLE: Building Hive from source
DESCRIPTION: Commands to clone the Hive repository and build it using Maven

LANGUAGE: Shell
CODE:
$ git clone https://git-wip-us.apache.org/repos/asf/hive.git
$ cd hive
$ mvn clean package -Pdist [-DskipTests -Dmaven.javadoc.skip=true]

----------------------------------------

TITLE: Building Hive from source
DESCRIPTION: Commands to clone the Hive repository and build it using Maven

LANGUAGE: Shell
CODE:
$ git clone https://git-wip-us.apache.org/repos/asf/hive.git
$ cd hive
$ mvn clean package -Pdist [-DskipTests -Dmaven.javadoc.skip=true]

----------------------------------------

TITLE: Using GROUPING Function with Multiple Arguments in Hive SQL
DESCRIPTION: Shows how to use the GROUPING function with multiple column combinations to indicate whether expressions in a GROUP BY clause are aggregated.

LANGUAGE: SQL
CODE:
SELECT key, value, GROUPING__ID,
  grouping(key, value), grouping(value, key), grouping(key), grouping(value),
  count(*)
FROM T1
GROUP BY key, value WITH ROLLUP;

----------------------------------------

TITLE: Using GROUPING Function with Multiple Arguments in Hive SQL
DESCRIPTION: Shows how to use the GROUPING function with multiple column combinations to indicate whether expressions in a GROUP BY clause are aggregated.

LANGUAGE: SQL
CODE:
SELECT key, value, GROUPING__ID,
  grouping(key, value), grouping(value, key), grouping(key), grouping(value),
  count(*)
FROM T1
GROUP BY key, value WITH ROLLUP;

----------------------------------------

TITLE: Hive Statistics Analysis SQL Commands
DESCRIPTION: Example SQL commands for analyzing and computing statistics on Hive tables and partitions.

LANGUAGE: sql
CODE:
ANALYZE TABLE Table1 PARTITION(ds='2008-04-09', hr=11) COMPUTE STATISTICS;

ANALYZE TABLE Table1 PARTITION(ds='2008-04-09', hr=11) COMPUTE STATISTICS FOR COLUMNS;

ANALYZE TABLE Table1 PARTITION(ds='2008-04-09', hr) COMPUTE STATISTICS;

ANALYZE TABLE Table1 PARTITION(ds, hr) COMPUTE STATISTICS;

ANALYZE TABLE Table1 COMPUTE STATISTICS;

----------------------------------------

TITLE: Rot13 UDF Implementation
DESCRIPTION: Example UDF implementation with PDK annotations for testing, demonstrating rot13 string transformation with unit test cases.

LANGUAGE: java
CODE:
package org.apache.hive.pdktest;

import org.apache.hive.pdk.HivePdkUnitTest;
import org.apache.hive.pdk.HivePdkUnitTests;

import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.exec.UDF;
import org.apache.hadoop.io.Text;

/**
 * Example UDF for rot13 transformation.
 */
@Description(name = "rot13",
  value = "_FUNC_(str) - Returns str with all characters transposed via rot13",
  extended = "Example:\n"
  + "  > SELECT _FUNC_('Facebook') FROM src LIMIT 1;\n" + "  'Snprobbx'")
@HivePdkUnitTests(
    setup = "create table rot13_data(s string); "
    + "insert overwrite table rot13_data select 'Facebook' from onerow;",
    cleanup = "drop table if exists rot13_data;",
    cases = {
      @HivePdkUnitTest(
        query = "SELECT tp_rot13('Mixed Up!') FROM onerow;",
        result = "Zvkrq Hc!"),
      @HivePdkUnitTest(
        query = "SELECT tp_rot13(s) FROM rot13_data;",
        result = "Snprobbx")
    }
  )
public class Rot13 extends UDF {
  private Text t = new Text();

  public Rot13() {
  }

  public Text evaluate(Text s) {
    StringBuilder out = new StringBuilder(s.getLength());
    char[] ca = s.toString().toCharArray();
    for (char c : ca) {
      if (c >= 'a' && c <= 'm') {
        c += 13;
      } else if (c >= 'n' && c <= 'z') {
        c -= 13;
      } else if (c >= 'A' && c <= 'M') {
        c += 13;
      } else if (c >= 'N' && c <= 'Z') {
        c -= 13;
      }
      out.append(c);
    }
    t.set(out.toString());
    return t;
  }
}

----------------------------------------

TITLE: Error Response for Non-existent Database
DESCRIPTION: Example JSON error response when attempting to describe a non-existent database. Returns 404 error code and descriptive message.

LANGUAGE: json
CODE:
{
  "error": "No such database: newdb",
  "errorCode": 404
}

----------------------------------------

TITLE: JSON Response for Successful Table Deletion in WebHCat
DESCRIPTION: This JSON output shows the response returned by the WebHCat API after successfully deleting a table. It includes the name of the deleted table and the database it was in.

LANGUAGE: json
CODE:
{
 "table": "test_table",
 "database": "default"
}

----------------------------------------

TITLE: Explaining Left Outer Join with Join Predicate on Null Supplying Table
DESCRIPTION: This HiveQL query shows a left outer join with a join predicate on the null supplying table. The explain plan demonstrates that the join predicate is pushed down to the null supplying table, as expected for Case J2.

LANGUAGE: HiveQL
CODE:
explain 
select s1.key, s2.key 
from src s1 left join src s2 on s2.key > '2';

----------------------------------------

TITLE: Creating Basic Partitioned Table in Hive
DESCRIPTION: Creates a partitioned table T with string key-value pairs, partitioned by date (ds) and hour (hr).

LANGUAGE: hql
CODE:
create table T (key string, value string) partitioned by (ds string, hr string);

----------------------------------------

TITLE: Statistics Publisher Interface Implementation in Java
DESCRIPTION: Interface definition for implementing statistics publishing functionality in Hive. Provides methods for initialization, publishing individual statistics, and cleanup.

LANGUAGE: java
CODE:
package org.apache.hadoop.hive.ql.stats;

import org.apache.hadoop.conf.Configuration;

public interface IStatsPublisher {
  public boolean init(Configuration hconf);
  public boolean publishStat(String rowID, String key, String value);
  public boolean terminate();
}

----------------------------------------

TITLE: Basic UDAF Class Structure
DESCRIPTION: Shows the basic skeleton structure of a Generic UDAF class with resolver and evaluator implementations

LANGUAGE: Java
CODE:
public class GenericUDAFHistogramNumeric extends AbstractGenericUDAFResolver {
  static final Log LOG = LogFactory.getLog(GenericUDAFHistogramNumeric.class.getName());

  @Override
  public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info) throws SemanticException {
    // Type-checking goes here!

    return new GenericUDAFHistogramNumericEvaluator();
  }

  public static class GenericUDAFHistogramNumericEvaluator extends GenericUDAFEvaluator {
    // UDAF logic goes here!
  }
}

----------------------------------------

TITLE: LLAP Status CLI Command Usage
DESCRIPTION: Command line syntax for checking LLAP application status with various options for monitoring and configuration.

LANGUAGE: bash
CODE:
/current/hive-server2-hive2/bin/hive --service llapstatus --name {llap_app_name} [-f] [-w] [-i] [-t]

----------------------------------------

TITLE: Successful Database Deletion Response - JSON
DESCRIPTION: Example JSON response returned when a database is successfully deleted through the WebHCat API.

LANGUAGE: json
CODE:
{
 "database":"newdb"
}

----------------------------------------

TITLE: Configuring Storage-Based Authorization in Hive
DESCRIPTION: XML configuration snippets to enable storage-based authorization in Hive by setting the authorization manager and enabling authorization.

LANGUAGE: XML
CODE:
<property>
  <name>hive.security.authorization.enabled</name>
  <value>true</value>
  <description>enable or disable the hive client authorization</description>
</property>

<property>
  <name>hive.security.authorization.manager</name>
  <value>org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider</value>
  <description>the hive client authorization manager class name.
  The user defined authorization class should implement interface
  org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider.
  </description>
</property>

----------------------------------------

TITLE: HBase Storage Handler Example in Hive SQL
DESCRIPTION: Example of creating a table using the HBase storage handler with custom SerDe properties for column mapping and table name configuration.

LANGUAGE: sql
CODE:
CREATE TABLE hbase_table_1(key int, value string) 
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
"hbase.columns.mapping" = "cf:string",
"hbase.table.name" = "hbase_table_0"
);

----------------------------------------

TITLE: Hive Site XML Configuration
DESCRIPTION: XML configuration to add necessary JAR files to Hive's auxiliary classpath.

LANGUAGE: xml
CODE:
<property>
  <name>hive.aux.jars.path</name>
  <value>/user/hive/hbase-VERSION.jar,/user/hive/hive-hbase-handler-VERSION.jar</value>
</property>

----------------------------------------

TITLE: Pulling Apache Hive Docker Image
DESCRIPTION: Command to pull the Apache Hive Docker image from DockerHub. This example uses version 4.0.0.

LANGUAGE: shell
CODE:
docker pull apache/hive:4.0.0

----------------------------------------

TITLE: JSON Response for WebHCat Delete Job Operation
DESCRIPTION: Example JSON response from the WebHCat API when deleting a job. It includes job status information, profile details, and other metadata about the deleted job.

LANGUAGE: json
CODE:
{
 "status": {
            "startTime": 1321047216471,
            "username": "ctdean",
            "jobID": {
                      "jtIdentifier": "201111111311",
                      "id": 9
                     },
            "jobACLs": {
                       },
            "schedulingInfo": "NA",
            "failureInfo": "NA",
            "jobId": "job_201111111311_0009",
            "jobPriority": "NORMAL",
            "runState": 1,
            "jobComplete": false
           },
 "profile": {
             "url": "http://localhost:50030/jobdetails.jsp?jobid=job_201111111311_0009",
             "user": "ctdean",
             "jobID": {
                       "jtIdentifier": "201111111311",
                       "id": 9
                      },
             "queueName": "default",
             "jobFile": "hdfs://localhost:9000/tmp/hadoop-ctdean/mapred/staging/ctdean/.staging/job_201111111311_0009/job.xml",
             "jobName": "streamjob3322518350676530377.jar",
             "jobId": "job_201111111311_0009"
            }
 "id": "job_201111111311_0009",
 "parentId": "job_201111111311_0008",
 "percentComplete": "10% complete",
 "exitValue": 0,
 "user": "ctdean",
 "callback": null,
 "completed": "false"
}

----------------------------------------

TITLE: Rebuilding a Materialized View in Hive
DESCRIPTION: Shows the syntax for triggering a rebuild operation on a materialized view.

LANGUAGE: SQL
CODE:
ALTER MATERIALIZED VIEW [db_name.]materialized_view_name REBUILD;

----------------------------------------

TITLE: Example EXPLAIN Query in Hive
DESCRIPTION: Demonstrates an example EXPLAIN query that shows the execution plan for inserting data into a table.

LANGUAGE: SQL
CODE:
EXPLAIN
FROM src INSERT OVERWRITE TABLE dest_g1 SELECT src.key, sum(substr(src.value,4)) GROUP BY src.key;

----------------------------------------

TITLE: Column Type Conversion in UNION Operations
DESCRIPTION: Illustrates explicit type casting for combining columns of different types in a UNION operation, which is necessary for types from different groups (e.g., string and date) since Hive 2.2.0.

LANGUAGE: SQL
CODE:
SELECT name, id, cast('2001-01-01' as date) d FROM source_table_1
UNION ALL
SELECT name, id, hiredate as d FROM source_table_2

----------------------------------------

TITLE: Building Hive from Source with Ant in Bash
DESCRIPTION: Ant command to build Hive from source code. This creates a 'build/dist' directory containing all necessary files to run Hive.

LANGUAGE: bash
CODE:
$ ant package

----------------------------------------

TITLE: WebHCat Delete Partition Response
DESCRIPTION: JSON response returned by the WebHCat API after successfully deleting a partition. Contains confirmation details including the partition name, table name, and database name.

LANGUAGE: json
CODE:
{
 "partition": "country='algeria'",
 "table": "test_table",
 "database": "default"
}

----------------------------------------

TITLE: Setting up SSH tunnel for Hive-EC2 communication
DESCRIPTION: Creates an SSH tunnel to allow Hive to communicate with the EC2 Hadoop cluster. This command should be run in a separate terminal window.

LANGUAGE: bash
CODE:
ssh -i <path to Hadoop private key path> -D 2600 ec2-12-34-56-78.compute-1.amazonaws.com

----------------------------------------

TITLE: Setting up SSH tunnel for Hive-EC2 communication
DESCRIPTION: Creates an SSH tunnel to allow Hive to communicate with the EC2 Hadoop cluster. This command should be run in a separate terminal window.

LANGUAGE: bash
CODE:
ssh -i <path to Hadoop private key path> -D 2600 ec2-12-34-56-78.compute-1.amazonaws.com

----------------------------------------

TITLE: EXPLAIN CBO Query Example in Hive
DESCRIPTION: Shows an example of using the CBO clause with EXPLAIN to output the plan generated by the Calcite optimizer.

LANGUAGE: SQL
CODE:
EXPLAIN CBO
WITH customer_total_return AS
(SELECT sr_customer_sk AS ctr_customer_sk,
  sr_store_sk AS ctr_store_sk,
  SUM(SR_FEE) AS ctr_total_return
  FROM store_returns, date_dim
  WHERE sr_returned_date_sk = d_date_sk
    AND d_year =2000
  GROUP BY sr_customer_sk, sr_store_sk)
SELECT c_customer_id
FROM customer_total_return ctr1, store, customer
WHERE ctr1.ctr_total_return > (SELECT AVG(ctr_total_return)*1.2
FROM customer_total_return ctr2
WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk)
  AND s_store_sk = ctr1.ctr_store_sk
  AND s_state = 'NM'
  AND ctr1.ctr_customer_sk = c_customer_sk
ORDER BY c_customer_id
LIMIT 100

----------------------------------------

TITLE: Simple Subquery Example in FROM Clause for Apache Hive SQL
DESCRIPTION: Illustrates a basic example of using a subquery in the FROM clause of a Hive SQL query. The subquery performs a calculation and aliases the result.

LANGUAGE: SQL
CODE:
SELECT col 
FROM (
  SELECT a+b AS col
  FROM t1
) t2

----------------------------------------

TITLE: Checking Out Hive Source from SVN in Bash
DESCRIPTION: SVN command to download a specific branch of Hive source code. Replace '#.#' with the desired Hive version number.

LANGUAGE: bash
CODE:
$ svn co http://svn.apache.org/repos/asf/hive/branches/branch-#.# hive

----------------------------------------

TITLE: Setting up test data for Hadoop streaming job
DESCRIPTION: Bash commands to create sample input files and upload them to HDFS for use in the streaming MapReduce job example.

LANGUAGE: bash
CODE:
% cat mydata/file01 mydata/file02
Hello World Bye World
Hello Hadoop Goodbye Hadoop

% hadoop fs -put mydata/ .

% hadoop fs -ls mydata
Found 2 items
-rw-r--r--   1 ctdean supergroup         23 2011-11-11 13:29 /user/ctdean/mydata/file01
-rw-r--r--   1 ctdean supergroup         28 2011-11-11 13:29 /user/ctdean/mydata/file02

----------------------------------------

TITLE: Configuring Hadoop Core Site XML
DESCRIPTION: XML configuration for Hadoop core-site.xml, including filesystem and proxy user settings.

LANGUAGE: xml
CODE:
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>

    <property>
      <name>hadoop.proxyuser.yourusername.groups</name>
      <value>*</value>
    </property>

    <property>
      <name>hadoop.proxyuser.yourusername.hosts</name>
      <value>*</value>
    </property>
</configuration>

----------------------------------------

TITLE: Configuring Hadoop Core Site XML
DESCRIPTION: XML configuration for Hadoop core-site.xml, including filesystem and proxy user settings.

LANGUAGE: xml
CODE:
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>

    <property>
      <name>hadoop.proxyuser.yourusername.groups</name>
      <value>*</value>
    </property>

    <property>
      <name>hadoop.proxyuser.yourusername.hosts</name>
      <value>*</value>
    </property>
</configuration>

----------------------------------------

TITLE: Altering a Table to Use List Bucketing in Hive SQL
DESCRIPTION: SQL syntax for altering an existing table to use list bucketing. Can convert a non-skewed table to skewed or modify skewed columns/values.

LANGUAGE: SQL
CODE:
ALTER TABLE <T> (SCHEMA) SKEWED BY (keys) ON ('c1', 'c2') [STORED AS DIRECTORIES];

----------------------------------------

TITLE: Position-Based DISTRIBUTE BY Example
DESCRIPTION: Demonstrates how to use position-based column references in DISTRIBUTE BY clause when specific configuration options are enabled.

LANGUAGE: sql
CODE:
set hive.orderby.position.alias=true;
set hive.cbo.enable=true;
SELECT age, name, birthdate FROM author DISTRIBUTE BY 3, 1;

----------------------------------------

TITLE: Building and Signing Hive Release Artifacts
DESCRIPTION: Commands for building, signing and verifying main Hive release artifacts

LANGUAGE: bash
CODE:
mvn install -Pdist,iceberg -DskipTests -Dmaven.javadoc.skip=true -DcreateChecksum=true

cd packaging/target
shasum -a 256 apache-hive-X.Y.Z-bin.tar.gz > apache-hive-X.Y.Z-bin.tar.gz.sha256
shasum -a 256 apache-hive-X.Y.Z-src.tar.gz > apache-hive-X.Y.Z-src.tar.gz.sha256

gpg --armor --output apache-hive-X.Y.Z-bin.tar.gz.asc --detach-sig apache-hive-X.Y.Z-bin.tar.gz
gpg --armor --output apache-hive-X.Y.Z-src.tar.gz.asc --detach-sig apache-hive-X.Y.Z-src.tar.gz

----------------------------------------

TITLE: Verifying Hadoop streaming job results
DESCRIPTION: Bash commands to check the output of the Hadoop streaming MapReduce job in HDFS, listing the output directory and displaying the contents of the result file.

LANGUAGE: bash
CODE:
% hadoop fs -ls mycounts
Found 3 items
-rw-r--r--   1 ctdean supergroup          0 2011-11-11 13:27 /user/ctdean/mycounts/_SUCCESS
drwxr-xr-x   - ctdean supergroup          0 2011-11-11 13:26 /user/ctdean/mycounts/_logs
-rw-r--r--   1 ctdean supergroup         10 2011-11-11 13:27 /user/ctdean/mycounts/part-00000

% hadoop fs -cat mycounts/part-00000
      8

----------------------------------------

TITLE: Hive Export Command Syntax
DESCRIPTION: Basic syntax for the EXPORT command to export table data and metadata to a specified location. Includes optional partition specification and replication event ID.

LANGUAGE: sql
CODE:
EXPORT TABLE tablename [PARTITION (part_column="value"[, ...])] 
  TO 'export_target_path' [ FOR replication('eventid') ]

----------------------------------------

TITLE: Hive Export Command Syntax
DESCRIPTION: Basic syntax for the EXPORT command to export table data and metadata to a specified location. Includes optional partition specification and replication event ID.

LANGUAGE: sql
CODE:
EXPORT TABLE tablename [PARTITION (part_column="value"[, ...])] 
  TO 'export_target_path' [ FOR replication('eventid') ]

----------------------------------------

TITLE: Outer Lateral View Examples
DESCRIPTION: Shows the usage of OUTER keyword with lateral view to handle empty arrays and NULL values.

LANGUAGE: sql
CODE:
SELEC * FROM src LATERAL VIEW explode(array()) C AS a limit 10;

LANGUAGE: sql
CODE:
SELECT * FROM src LATERAL VIEW OUTER explode(array()) C AS a limit 10;

----------------------------------------

TITLE: Basic Lateral View Syntax in Hive
DESCRIPTION: Defines the basic syntax structure for using lateral view with table generating functions in Hive queries.

LANGUAGE: sql
CODE:
lateralView: LATERAL VIEW udtf(expression) tableAlias AS columnAlias (',' columnAlias)*
fromClause: FROM baseTable (lateralView)*

----------------------------------------

TITLE: Hadoop Download Fix Commands
DESCRIPTION: Shell commands to fix Hadoop archive download issues

LANGUAGE: bash
CODE:
cd ~/.ant/cache/hadoop/core/sources
wget http://archive.apache.org/dist/hadoop/core/hadoop-0.20.1/hadoop-0.20.1.tar.gz

----------------------------------------

TITLE: Submitting Hadoop streaming job via WebHCat API
DESCRIPTION: Curl command to submit a Hadoop streaming MapReduce job through the WebHCat API, specifying input, output, mapper, and reducer.

LANGUAGE: bash
CODE:
% curl -s -d input=mydata \
       -d output=mycounts \
       -d mapper=/bin/cat \
       -d reducer="/usr/bin/wc -w" \
       'http://localhost:50111/templeton/v1/mapreduce/streaming?user.name=ekoifman'

----------------------------------------

TITLE: Managing Materialized Views in Hive
DESCRIPTION: Demonstrates various operations for managing materialized views, including dropping, showing, and describing views.

LANGUAGE: SQL
CODE:
-- Drops a materialized view
DROP MATERIALIZED VIEW [db_name.]materialized_view_name;
-- Shows materialized views (with optional filters)
SHOW MATERIALIZED VIEWS [IN database_name] ['identifier_with_wildcards'];
-- Shows information about a specific materialized view
DESCRIBE [EXTENDED | FORMATTED] [db_name.]materialized_view_name;

----------------------------------------

TITLE: Managing Materialized Views in Hive
DESCRIPTION: Demonstrates various operations for managing materialized views, including dropping, showing, and describing views.

LANGUAGE: SQL
CODE:
-- Drops a materialized view
DROP MATERIALIZED VIEW [db_name.]materialized_view_name;
-- Shows materialized views (with optional filters)
SHOW MATERIALIZED VIEWS [IN database_name] ['identifier_with_wildcards'];
-- Shows information about a specific materialized view
DESCRIBE [EXTENDED | FORMATTED] [db_name.]materialized_view_name;

----------------------------------------

TITLE: Implicit Join Notation in Hive 0.13.0+
DESCRIPTION: Demonstrates the implicit join notation supported in Hive 0.13.0 and later, allowing comma-separated table lists in the FROM clause.

LANGUAGE: SQL
CODE:
SELECT *   
FROM table1 t1, table2 t2, table3 t3   
WHERE t1.id = t2.id AND t2.id = t3.id AND t1.zipcode = '02535';

----------------------------------------

TITLE: REPL DUMP Command Syntax
DESCRIPTION: The syntax for the new REPL DUMP command, used to generate replication dumps with various options for controlling the scope and content of the dump.

LANGUAGE: SQL
CODE:
REPL DUMP <repl_policy> {REPLACE <old_repl_policy>} {FROM <init-evid> {TO <end-evid>} {LIMIT <num-evids>} } {WITH ('key1'='value1', 'key2'='value2')};

----------------------------------------

TITLE: Installing Java 8 on Apple ARM
DESCRIPTION: Commands to install Java 8 on Apple ARM chipsets using Homebrew.

LANGUAGE: bash
CODE:
brew install homebrew/cask-versions/adoptopenjdk8 --cask
brew untap adoptopenjdk/openjdk

----------------------------------------

TITLE: Installing Java 8 on Apple ARM
DESCRIPTION: Commands to install Java 8 on Apple ARM chipsets using Homebrew.

LANGUAGE: bash
CODE:
brew install homebrew/cask-versions/adoptopenjdk8 --cask
brew untap adoptopenjdk/openjdk

----------------------------------------

TITLE: Configuring HWI Properties in XML
DESCRIPTION: XML configuration properties for Hive Web Interface defining the host address, port, and WAR file location.

LANGUAGE: xml
CODE:
<property>
  <name>hive.hwi.listen.host</name>
  <value>0.0.0.0</value>
  <description>This is the host address the Hive Web Interface will listen on</description>
</property>

<property>
  <name>hive.hwi.listen.port</name>
  <value>9999</value>
  <description>This is the port the Hive Web Interface will listen on</description>
</property>

<property>
  <name>hive.hwi.war.file</name>
  <value>${HIVE_HOME}/lib/hive-hwi-<version>.war</value>
  <description>This is the WAR file with the jsp content for Hive Web Interface</description>
</property>

----------------------------------------

TITLE: Show Process List in HiveServer2
DESCRIPTION: Displays the output of 'show processlist' command which shows active operations on HiveServer2, including user sessions, queries, and their execution states.

LANGUAGE: sql
CODE:
0: jdbc:hive2://localhost:10000> show processlist;
+------------+------------+-------------------+---------------------------------------+--------------------------+------------------------+----------------------------------------------------+----------+-------------------+-------------------+---------------+
| User Name  |  Ip Addr   | Execution Engine  |              Session Id               | Session Active Time (s)  | Session Idle Time (s)  |                      Query ID                      |  State   | Opened Timestamp  | Elapsed Time (s)  |  Runtime (s)  |
+------------+------------+-------------------+---------------------------------------+--------------------------+------------------------+----------------------------------------------------+----------+-------------------+-------------------+---------------+
| hive       | 127.0.0.1  | mr                | 66df357a-90bf-43cb-847f-279aa6df1c24  | 113                      | 7                      | rtrivedi_20240709124106_d0f00d7a-6fab-4fcd-9f41-d53bb296275d | RUNNING  | 1720546866774     | 16                | Not finished  |
| hive       | 127.0.0.1  | mr                | 7daa873e-bb46-462e-bc38-94cb8d3e7c17  | 83                       | 29                     | rtrivedi_20240709124106_2dc53c4c-e522-4aed-a969-3d48ac01ba81 | RUNNING  | 1720546866774     | 17                | Not finished  |
+------------+------------+-------------------+---------------------------------------+--------------------------+------------------------+----------------------------------------------------+----------+-------------------+-------------------+---------------+

----------------------------------------

TITLE: Setting Hive scratch directory via command line option
DESCRIPTION: Example of using the --hiveconf option when launching Hive to set the scratch directory configuration variable for the entire session.

LANGUAGE: shell
CODE:
bin/hive --hiveconf hive.exec.scratchdir=/tmp/mydir

----------------------------------------

TITLE: Creating a Table with Union Type in Hive
DESCRIPTION: Example of creating a Hive table with a column of UNIONTYPE, demonstrating the syntax for specifying multiple possible data types.

LANGUAGE: SQL
CODE:
CREATE TABLE union_test(foo UNIONTYPE<int, double, array<string>, struct<a:int,b:string>>);

----------------------------------------

TITLE: Example Hive Commands for Replication Events
DESCRIPTION: A series of Hive commands demonstrating partition operations that might occur during replication, including adding and dropping partitions.

LANGUAGE: SQL
CODE:
CREATE TABLE blah (a int) PARTITIONED BY (p string);  
INSERT INTO TABLE blah [PARTITION (p="a") VALUES 5;  
INSERT INTO TABLE blah [PARTITION (p="b") VALUES 10;  
INSERT INTO TABLE blah [PARTITION (p="a") VALUES 15;

----------------------------------------

TITLE: Installing Hive from Tarball
DESCRIPTION: Commands to download, extract, and set up Apache Hive from a tarball release.

LANGUAGE: bash
CODE:
wget https://dlcdn.apache.org/hive/hive-4.0.0/apache-hive-4.0.0-bin.tar.gz
tar -xzvf apache-hive-4.0.0-bin.tar.gz 

cd apache-hive-4.0.0-bin
export HIVE_HOME=/yourpathtohive/apache-hive-4.0.0-bin 
export PATH=$HIVE_HOME/bin:$PATH

mkdir /yourpathtoexternaltables/warehouse

----------------------------------------

TITLE: Installing Hive from Tarball
DESCRIPTION: Commands to download, extract, and set up Apache Hive from a tarball release.

LANGUAGE: bash
CODE:
wget https://dlcdn.apache.org/hive/hive-4.0.0/apache-hive-4.0.0-bin.tar.gz
tar -xzvf apache-hive-4.0.0-bin.tar.gz 

cd apache-hive-4.0.0-bin
export HIVE_HOME=/yourpathtohive/apache-hive-4.0.0-bin 
export PATH=$HIVE_HOME/bin:$PATH

mkdir /yourpathtoexternaltables/warehouse

----------------------------------------

TITLE: Java API - HCatClient
DESCRIPTION: Operation-based Java API for DDL operations. Uses HCatClientHMSImpl as the primary implementation, interfacing directly with the Metastore.



----------------------------------------

TITLE: Distinct Aggregation with Window Functions
DESCRIPTION: Examples of using DISTINCT with window functions for aggregation operations.

LANGUAGE: sql
CODE:
COUNT(DISTINCT a) OVER (PARTITION BY c)

LANGUAGE: sql
CODE:
COUNT(DISTINCT a) OVER (PARTITION BY c ORDER BY d ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)

----------------------------------------

TITLE: Git Operations for Storage API Release
DESCRIPTION: Commands for checking out master branch, creating release branches and tags for Storage API component

LANGUAGE: bash
CODE:
git checkout master
git checkout -b storage-branch-X.Y origin/master
git commit -a -m "Preparing for storage-api X.Y.Z release"
git push -u origin storage-branch-X.Y
git tag -a storage-release-X.Y.Z-rcR -m "Hive Storage API X.Y.Z-rcR release."
git push origin storage-release-X.Y.Z-rcR

----------------------------------------

TITLE: Altering Decimal Column Definition in Hive
DESCRIPTION: Example of using ALTER TABLE to change the precision and scale of a decimal column in Hive.

LANGUAGE: SQL
CODE:
ALTER TABLE foo CHANGE COLUMN dec_column_name dec_column_name DECIMAL(38,18);

----------------------------------------

TITLE: Configuring LDAP User Filter in Hive XML
DESCRIPTION: Example of setting the hive.server2.authentication.ldap.userFilter property in hive-site.xml to specify allowed LDAP users for authentication.

LANGUAGE: xml
CODE:
<property>
  <name>
    hive.server2.authentication.ldap.userFilter
  </name>
  <value>
    hive-admin,hive,hivetest,hive-user
  </value>
</property>

----------------------------------------

TITLE: Configuring Hive XML Settings
DESCRIPTION: XML configuration for Hive, including Tez settings and metastore configuration.

LANGUAGE: xml
CODE:
<configuration>
    <property>
        <name>hive.tez.container.size</name>
        <value>1024</value>
    </property>

    <property>
        <name>hive.metastore.warehouse.external.dir</name>
        <value>/yourpathtowarehousedirectory/warehouse</value>
    </property>

    <property>
        <name>hive.execution.engine</name>
        <value>tez</value>
    </property>

    <property>
        <name>tez.lib.uris</name>
        <value>hdfs://localhost:9000/apps/tez/apache-tez-0.10.3-bin.tar.gz,hdfs://localhost:9000/apps/tez/apache-tez-0.10.3-bin/lib,hdfs://localhost:9000/apps/tez/apache-tez-0.10.3-bin</value>
    </property>

    <property>
        <name>tez.configuration</name>
        <value>/yourpathtotez/apache-tez-0.10.3-bin/conf/tez-site.xml</value>
    </property>

    <property>
        <name>tez.use.cluster.hadoop-libs</name>
        <value>true</value>
    </property>
</configuration>

----------------------------------------

TITLE: Configuring Hive XML Settings
DESCRIPTION: XML configuration for Hive, including Tez settings and metastore configuration.

LANGUAGE: xml
CODE:
<configuration>
    <property>
        <name>hive.tez.container.size</name>
        <value>1024</value>
    </property>

    <property>
        <name>hive.metastore.warehouse.external.dir</name>
        <value>/yourpathtowarehousedirectory/warehouse</value>
    </property>

    <property>
        <name>hive.execution.engine</name>
        <value>tez</value>
    </property>

    <property>
        <name>tez.lib.uris</name>
        <value>hdfs://localhost:9000/apps/tez/apache-tez-0.10.3-bin.tar.gz,hdfs://localhost:9000/apps/tez/apache-tez-0.10.3-bin/lib,hdfs://localhost:9000/apps/tez/apache-tez-0.10.3-bin</value>
    </property>

    <property>
        <name>tez.configuration</name>
        <value>/yourpathtotez/apache-tez-0.10.3-bin/conf/tez-site.xml</value>
    </property>

    <property>
        <name>tez.use.cluster.hadoop-libs</name>
        <value>true</value>
    </property>
</configuration>

----------------------------------------

TITLE: REST API - WebHCat
DESCRIPTION: REST-based operation API for HCatalog. Currently not actively maintained and may not be supported in future releases.



----------------------------------------

TITLE: Dropping an Index in Hive SQL
DESCRIPTION: SQL command to drop an existing index from a table.

LANGUAGE: SQL
CODE:
DROP INDEX index_name ON table_name

----------------------------------------

TITLE: Building and Signing Storage API Release Artifacts
DESCRIPTION: Commands for building, signing and verifying Storage API release artifacts

LANGUAGE: bash
CODE:
wget https://github.com/apache/hive/archive/storage-release-X.Y.Z-rcR.tar.gz
tar xzvf storage-release-X.Y.Z-rcR.tar.gz
mv storage-release-X.Y.Z-rcR/storage-api hive-storage-X.Y.Z
tar czvf hive-storage-X.Y.Z-rcR.tar.gz hive-storage-X.Y.Z
shasum -a 256 hive-storage-X.Y.Z-rcR.tar.gz > hive-storage-X.Y.Z-rcR.tar.gz.sha256
gpg --armor --detach-sig hive-storage-X.Y.Z-rcR.tar.gz

----------------------------------------

TITLE: Advancing Write ID for Managed Tables
DESCRIPTION: Simple utility call to advance the write ID for a managed table during write operations.

LANGUAGE: java
CODE:
AcidUtils.advanceWriteId(conf, tbl);

----------------------------------------

TITLE: Setting TiDB Configuration for Hive
DESCRIPTION: SQL command to set a global configuration in TiDB to skip isolation level checks for Hive compatibility.

LANGUAGE: SQL
CODE:
set global tidb_skip_isolation_level_check=1;

----------------------------------------

TITLE: Specifying user.name in POST request for WebHCat API
DESCRIPTION: This example shows how to include the user.name parameter in a POST request to the WebHCat API for renaming a table. It uses curl to send the POST request with form data.

LANGUAGE: bash
CODE:
% curl -s -d user.name=ctdean \
       -d rename=test_table_2 \
       'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table'

----------------------------------------

TITLE: Database Schema for Column Type Qualifiers
DESCRIPTION: Example schema showing potential column definitions for storing type qualifier information in a database, including character length, numeric precision, and collation settings.

LANGUAGE: sql
CODE:
CHARACTER_MAXIMUM_LENGTH  |  bigint(21) unsigned  |  YES  |   |  NULL  |   |
CHARACTER_OCTET_LENGTH  |  bigint(21) unsigned  |  YES  |   |  NULL  |   |
NUMERIC_PRECISION  |  bigint(21) unsigned  |  YES  |   |  NULL  |   |
NUMERIC_SCALE  |  bigint(21) unsigned  |  YES  |   |  NULL  |   |
CHARACTER_SET_NAME  |  varchar(32)  |  YES  |   |  NULL  |   |
COLLATION_NAME  |  varchar(32)  |  YES  |   |  NULL  |   |

----------------------------------------

TITLE: Adding JAR to Hive Classpath
DESCRIPTION: Commands for adding custom JAR files to Hive's classpath and listing currently added JARs.

LANGUAGE: sql
CODE:
add jar my_jar.jar;

LANGUAGE: sql
CODE:
add jar /tmp/my_jar.jar;

LANGUAGE: sql
CODE:
list jars;

----------------------------------------

TITLE: HiveStoragePredicateHandler Interface Definition
DESCRIPTION: Interface for storage handlers to negotiate filter decomposition with Hive, allowing partial pushdown of predicates to storage layer.

LANGUAGE: java
CODE:
public interface HiveStoragePredicateHandler {
  public DecomposedPredicate decomposePredicate(
    JobConf jobConf,
    Deserializer deserializer,
    ExprNodeDesc predicate);

  public static class DecomposedPredicate {
    public ExprNodeDesc pushedPredicate;
    public ExprNodeDesc residualPredicate;
  }
}

----------------------------------------

TITLE: Creating HIVEJDBC Connector in Apache Hive
DESCRIPTION: This snippet demonstrates how to create a HIVEJDBC connector in Apache Hive. It specifies the connector type, JDBC URL, and authentication credentials.

LANGUAGE: SQL
CODE:
CREATE CONNECTOR hiveserver_connector TYPE 'hivejdbc' URL 'jdbc:hive2://<maskedhost>:10000'   
 WITH DCPROPERTIES ("hive.sql.dbcp.username"="hive", "hive.sql.dbcp.password"="hive");

----------------------------------------

TITLE: Retrieving Hive Schema Information
DESCRIPTION: Example of retrieving schema information using the schema tool.

LANGUAGE: bash
CODE:
$ schematool -dbType derby -info
Metastore connection URL:        jdbc:derby:;databaseName=metastore_db;create=true
Metastore Connection Driver :    org.apache.derby.jdbc.EmbeddedDriver
Metastore connection User:       APP
Hive distribution version:       0.13.0
Metastore schema version:        0.13.0
schemaTool completed

----------------------------------------

TITLE: View Description Format in Hive
DESCRIPTION: Example showing the metadata format for views in Hive, including the original and expanded text of the view query.

LANGUAGE: text
CODE:
# View Information	 	 
View Original Text: 	SELECT value FROM src WHERE key=86	 
View Expanded Text: 	SELECT `src`.`value` FROM `src` WHERE `src`.`key`=86

----------------------------------------

TITLE: UDF Definition Types
DESCRIPTION: Example showing different types of UDFs and their basic interfaces

LANGUAGE: java
CODE:
org.apache.hadoop.hive.ql.exec.UDF;

----------------------------------------

TITLE: Comparing ngrams() and context_ngrams() in Apache Hive
DESCRIPTION: These two queries demonstrate the equivalence of ngrams() and context_ngrams() functions when used without specific context. The ngrams() function is noted to be slightly faster in practice.

LANGUAGE: SQL
CODE:
SELECT ngrams(sentences(lower(tweet)), 2, 100 [, 1000]) FROM twitter;
SELECT context_ngrams(sentences(lower(tweet)), array(null,null), 100, [, 1000]) FROM twitter;

----------------------------------------

TITLE: Querying Remote Table with Predicate in Apache Hive
DESCRIPTION: This snippet demonstrates how to query a remote table with a predicate using the HIVEJDBC connector in Hive.

LANGUAGE: SQL
CODE:
select * from test_emr_tbl where tblkey > 1;

----------------------------------------

TITLE: Configuring Hive JDBC Connection in Pentaho
DESCRIPTION: This code snippet shows the JDBC connection parameters for connecting to Hive from Pentaho Report Designer. It specifies the URL, driver name, and empty username and password fields.

LANGUAGE: java
CODE:
URL: jdbc:hive://localhost:10000/default
Driver name: org.apache.hadoop.hive.jdbc.HiveDriver
Username and password are empty

----------------------------------------

TITLE: Configuring Hive JDBC Connection in Pentaho
DESCRIPTION: This code snippet shows the JDBC connection parameters for connecting to Hive from Pentaho Report Designer. It specifies the URL, driver name, and empty username and password fields.

LANGUAGE: java
CODE:
URL: jdbc:hive://localhost:10000/default
Driver name: org.apache.hadoop.hive.jdbc.HiveDriver
Username and password are empty

----------------------------------------

TITLE: Storing query results directly in S3
DESCRIPTION: Shows how to modify a Hive query to store results directly in S3 instead of HDFS. This can be useful for persisting results after the EC2 cluster is terminated.

LANGUAGE: sql
CODE:
insert overwrite directory 's3n://target-bucket/tpcresults-1.sql';

----------------------------------------

TITLE: Testing HiveServer with Ant
DESCRIPTION: Ant commands to run JDBC and HiveServer tests in standalone mode to verify the server is working correctly. Assumes the server is running on localhost:10000.

LANGUAGE: bash
CODE:
$ ant test -Dtestcase=TestJdbcDriver -Dstandalone=true
$ ant test -Dtestcase=TestHiveServer -Dstandalone=true

----------------------------------------

TITLE: Unsupported Multi-Row Scalar Subquery
DESCRIPTION: Example of an invalid scalar subquery that could return multiple rows.

LANGUAGE: sql
CODE:
SELECT customer.customer_num,
	(SELECT ship_charge 
		FROM orders
		WHERE customer.customer_num = orders.customer_num
	) AS total_ship_chg
FROM customer

----------------------------------------

TITLE: CAST Format Examples
DESCRIPTION: Examples showing how to use CAST FORMAT for date formatting and parsing.

LANGUAGE: sql
CODE:
select cast(dt as string format 'DD-MM-YYYY')  
select cast('01-05-2017' as date format 'DD-MM-YYYY')

----------------------------------------

TITLE: Running Individual Tests in Maven
DESCRIPTION: Commands for executing specific test classes or methods using Maven

LANGUAGE: bash
CODE:
mvn test -Dtest=ClassName
mvn test -Dtest=ClassName#methodName
mvn test -Dtest='org.apache.hive.beeline.*'

----------------------------------------

TITLE: Multiple Partition Columns Exchange in Hive
DESCRIPTION: Shows how to exchange partitions with multiple partition columns specified. Creates tables with two partition columns and exchanges a specific partition.

LANGUAGE: sql
CODE:
-- Create two tables with multiple partition columns.
CREATE TABLE T1 (a int) PARTITIONED BY (d1 int, d2 int);
CREATE TABLE T2 (a int) PARTITIONED BY (d1 int, d2 int);
ALTER TABLE T1 ADD PARTITION (d1=1, d2=2);

-- Alter the table, moving partition data d1=1, d2=2 from table T1 to table T2
ALTER TABLE T2 EXCHANGE PARTITION (d1 = 1, d2 = 2) WITH TABLE T1;

----------------------------------------

TITLE: Configuring Fall Back Authorizer in HiveServer2
DESCRIPTION: XML configuration required in hiveserver2-site.xml to enable Fall Back Authorizer, which provides basic security controls like preventing local file access and restricting certain commands to admin users. Requires Hive 2.3.4 or 3.1.1 or later.

LANGUAGE: xml
CODE:
<property>
  <name>hive.security.authorization.enabled</name>
  <value>true</value>
</property>
<property>
  <name>hive.security.authorization.manager</name>
  <value>org.apache.hadoop.hive.ql.security.authorization.plugin.fallback.FallbackHiveAuthorizerFactory</value>
</property>

----------------------------------------

TITLE: Data Processing with Pig (Using HCatalog)
DESCRIPTION: Pig script using HCatalog loader and storer for improved data management and automatic notifications.

LANGUAGE: pig
CODE:
A = load 'rawevents' using org.apache.hive.hcatalog.pig.HCatLoader();
B = filter A by date = '20100819' and by bot_finder(zeta) = 0;
...
store Z into 'processedevents' using org.apache.hive.hcatalog.pig.HCatStorer("date=20100819");

----------------------------------------

TITLE: Unsupported Multi-Column Subquery
DESCRIPTION: Example of an invalid scalar subquery that returns multiple columns.

LANGUAGE: sql
CODE:
SELECT customer.customer_num,
	(SELECT ship_charge, customer_num
		FROM orders LIMIT 1
	) AS total_ship_chg
FROM customer

----------------------------------------

TITLE: Query Rewriting Example Using Denormalized Materialized View in Hive
DESCRIPTION: Shows a query that can be rewritten using the denormalized materialized view 'mv2'.

LANGUAGE: SQL
CODE:
SELECT SUM(lo_extendedprice * lo_discount)
FROM lineorder, dates
WHERE lo_orderdate = d_datekey
AND d_year = 2013
AND lo_discount between 1 and 3;

----------------------------------------

TITLE: Configuring ODBC Driver in odbc.ini
DESCRIPTION: Configuration entries required in odbc.ini file to connect the Hive ODBC driver to a driver manager.

LANGUAGE: ini
CODE:
[Hive]
Driver = <path_to_libodbchive.so>
Description = Hive Driver v1
DATABASE = default
HOST = <Hive_server_address>
PORT = <Hive_server_port>
FRAMED = 0

----------------------------------------

TITLE: Uploading Hadoop Streaming Jar to HDFS
DESCRIPTION: This command uploads the Hadoop Streaming jar file to HDFS. The exact path may vary depending on the Hadoop version being used.

LANGUAGE: bash
CODE:
hadoop fs -put hadoop-2.1.0/share/hadoop/tools/lib/hadoop-streaming-2.1.0.jar \
  /apps/templeton/hadoop-streaming.jar

----------------------------------------

TITLE: Type Casting with SORT BY
DESCRIPTION: Shows how to handle type casting when using SORT BY after transform operations to ensure proper numeric sorting.

LANGUAGE: sql
CODE:
FROM (FROM (FROM src
            SELECT TRANSFORM(value)
            USING 'mapper'
            AS value, count) mapped
      SELECT cast(value as double) AS value, cast(count as int) AS count
      SORT BY value, count) sorted
SELECT TRANSFORM(value, count)
USING 'reducer'
AS whatever

----------------------------------------

TITLE: HCatInputFormat API Implementation in Java
DESCRIPTION: Core API methods for HCatInputFormat including setInput, setOutputSchema, and getTableSchema for reading data from HCatalog-managed tables.

LANGUAGE: Java
CODE:
  /**
   * Set the input to use for the Job. This queries the metadata server with
   * the specified partition predicates, gets the matching partitions, puts
   * the information in the conf object. The inputInfo object is updated with
   * information needed in the client context
   * @param job the job object
   * @param inputJobInfo the input info for table to read
   * @throws IOException the exception in communicating with the metadata server
   */
  public static void setInput(Job job,
      InputJobInfo inputJobInfo) throws IOException;

  /**
   * Set the schema for the HCatRecord data returned by HCatInputFormat.
   * @param job the job object
   * @param hcatSchema the schema to use as the consolidated schema
   */
  public static void setOutputSchema(Job job,HCatSchema hcatSchema)
    throws IOException;

  /**
   * Get the HCatTable schema for the table specified in the HCatInputFormat.setInput
   * call on the specified job context. This information is available only after
   * HCatInputFormat.setInput has been called for a JobContext.
   * @param context the context
   * @return the table schema
   * @throws IOException if HCatInputFormat.setInput has not been called
   *                     for the current context
   */
  public static HCatSchema getTableSchema(JobContext context)
    throws IOException;

----------------------------------------

TITLE: Running Multiple Test Instances
DESCRIPTION: Example of running multiple test instances simultaneously using HIVE_PTEST_SUFFIX

LANGUAGE: bash
CODE:
HIVE_PTEST_SUFFIX=first_run hive_repo/testutils/ptest/hivetest.py --test &
HIVE_PTEST_SUFFIX=second_run hive_repo/testutils/ptest/hivetest.py --test &

----------------------------------------

TITLE: Rewritten Query Using Denormalized Materialized View in Hive
DESCRIPTION: Demonstrates how Hive rewrites the query using the denormalized materialized view 'mv2'.

LANGUAGE: SQL
CODE:
SELECT SUM(d_price)
FROM mv2
WHERE d_year = 2013
AND lo_discount between 1 and 3;

----------------------------------------

TITLE: Multi-Group-By Insert Example
DESCRIPTION: Shows how to perform multiple GROUP BY operations in a single query, writing results to different destinations.

LANGUAGE: sql
CODE:
FROM pv_users 
INSERT OVERWRITE TABLE pv_gender_sum
  SELECT pv_users.gender, count(DISTINCT pv_users.userid) 
  GROUP BY pv_users.gender 
INSERT OVERWRITE DIRECTORY '/user/facebook/tmp/pv_age_sum'
  SELECT pv_users.age, count(DISTINCT pv_users.userid) 
  GROUP BY pv_users.age;

----------------------------------------

TITLE: Derby Installation Commands
DESCRIPTION: Commands for downloading and extracting Derby database files

LANGUAGE: bash
CODE:
cd /opt/hadoop
<download>
tar -xzf db-derby-10.4.1.3-bin.tar.gz
mkdir db-derby-10.4.1.3-bin/data

----------------------------------------

TITLE: Unqualified Column References in Join Conditions (Hive 0.13.0+)
DESCRIPTION: Shows how unqualified column references can be used in join conditions, with Hive resolving them against the inputs to a Join.

LANGUAGE: SQL
CODE:
CREATE TABLE a (k1 string, v1 string);
CREATE TABLE b (k2 string, v2 string);

SELECT k1, v1, k2, v2
FROM a JOIN b ON k1 = k2;

----------------------------------------

TITLE: Building Apache Hive and Generating Eclipse Project Files
DESCRIPTION: This command cleans the project, packages it, and generates the necessary Eclipse project files. It should be run from the top-level directory of the Hive source code.

LANGUAGE: bash
CODE:
$ ant clean package eclipse-files

----------------------------------------

TITLE: Demonstrating Complex Monolithic Hive Query Structure
DESCRIPTION: This snippet illustrates a complex, nested Hive SQL query to demonstrate the need for modularization in testing. It shows multiple levels of subqueries, joins, and aggregations.

LANGUAGE: sql
CODE:
SELECT ... FROM (                  -- Query 1
  SELECT ... FROM (                --  Query 2
    SELECT ... FROM (              --   Query 3
      SELECT ... FROM a WHERE ...  --    Query 4
    ) A LEFT JOIN (                --   Query 3
      SELECT ... FROM b            --    Query 5
    ) B ON (...)                   --   Query 3 
  ) ab FULL OUTER JOIN (           --  Query 2
    SELECT ... FROM c WHERE ...    --   Query 6
  ) C ON (...)                     --  Query 2
) abc LEFT JOIN (                  -- Query 1
  SELECT ... FROM d WHERE ...      --  Query 7
) D ON (...)                       -- Query 1
GROUP BY ...;                      -- Query 1

----------------------------------------

TITLE: Creating Iceberg V2 Table with ORC Format in Hive
DESCRIPTION: Illustrates creating an Iceberg V2 table with ORC file format, combining version and format specifications.

LANGUAGE: SQL
CODE:
CREATE TABLE V2_ORC_TABLE (ID INT) STORED BY ICEBERG STORED AS ORC TBLPROPERTIES ('format-version'='2');

----------------------------------------

TITLE: Basic Export/Import Example
DESCRIPTION: Simple example showing how to export a department table and import it back.

LANGUAGE: sql
CODE:
export table department to 'hdfs_exports_location/department';
import from 'hdfs_exports_location/department';

----------------------------------------

TITLE: Launching Hive Metastore Service
DESCRIPTION: Bash command to start the Hive Metastore service using the Hive binary.

LANGUAGE: Bash
CODE:
${HIVE_HOME}/bin/hive --service metastore

----------------------------------------

TITLE: Retrieving Transaction Write IDs in Hive
DESCRIPTION: Code snippet demonstrating how to obtain ValidTxnWriteIdList and table-specific write IDs from HiveTxnManager for cache validation.

LANGUAGE: java
CODE:
HiveTxnManager txnMgr = TxnManagerFactory.getTxnManagerFactory().getTxnManager(conf);
ValidTxnList txnIds = txnMgr.getValidTxns(); // get global transaction state
ValidTxnWriteIdList
txnWriteIdsTblIds = txnMgr.getValidWriteIdsTableIds(txnTables, txnString); // map global transaction state to table specific write id
int tblId = txnWriteIdsTblIds.getTableId(fullTableName);
ValidWriteIdList writeids = txnWriteIds.getTableValidWriteIdList(fullTableName); // get table specific writeid

----------------------------------------

TITLE: Creating Dependent Table in Hive
DESCRIPTION: Demonstrates creation of a dependent table that inherits schema from another table and establishes explicit dependencies. The table is partitioned by date and depends on table T.

LANGUAGE: hql
CODE:
create dependent table Tdep partitioned by (ds string) depends on table T;

----------------------------------------

TITLE: Uploading Hive Archive to HDFS
DESCRIPTION: This command uploads a Hive tar.gz file to the Hadoop Distributed File System (HDFS). It's part of the setup process for making Hive available in the Hadoop distributed cache.

LANGUAGE: bash
CODE:
hadoop fs -put /tmp/hive-0.11.0.tar.gz /apps/templeton/hive-0.11.0.tar.gz

----------------------------------------

TITLE: Creating Iceberg Table using CTLT in Hive
DESCRIPTION: Illustrates creating an Iceberg table using the CREATE TABLE LIKE TABLE (CTLT) statement, copying the schema from an existing table.

LANGUAGE: SQL
CODE:
CREATE TABLE_CTLT LIKE SRCTABLE STORED BY ICEBERG;

----------------------------------------

TITLE: Partition Export/Import Example
DESCRIPTION: Example demonstrating how to export and import specific partitions of a table.

LANGUAGE: sql
CODE:
export table employee partition (emp_country="in", emp_state="ka") to 'hdfs_exports_location/employee';
import from 'hdfs_exports_location/employee';

----------------------------------------

TITLE: HiveServer2 Interface Definition
DESCRIPTION: Location of the Thrift IDL file that defines the service interface for TCLIService

LANGUAGE: text
CODE:
https://github.com/apache/hive/blob/master/service-rpc/if/TCLIService.thrift

----------------------------------------

TITLE: Creating Table and Loading Compressed Data in Hive
DESCRIPTION: This snippet demonstrates how to create a table in Hive and load compressed Gzip or Bzip2 data directly into it. The compression is automatically detected and decompressed during query execution.

LANGUAGE: SQL
CODE:
CREATE TABLE raw (line STRING)
   ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n';

LOAD DATA LOCAL INPATH '/tmp/weblogs/20090603-access.log.gz' INTO TABLE raw;

----------------------------------------

TITLE: Configuring Spark Application Settings for Hive
DESCRIPTION: SQL commands to configure Spark application settings for use with Hive, including master URL, event logging, executor memory and serializer.

LANGUAGE: sql
CODE:
set spark.master=<Spark Master URL>
set spark.eventLog.enabled=true;
set spark.eventLog.dir=<Spark event log folder (must exist)>
set spark.executor.memory=512m;              
set spark.serializer=org.apache.spark.serializer.KryoSerializer;

----------------------------------------

TITLE: Implementing gRPC Server Class for Hive Metastore
DESCRIPTION: Java class definition for the HiveMetaStoreGrpcServer, extending the generated gRPC base class to implement the server-side logic.

LANGUAGE: java
CODE:
private class HiveMetaStoreGrpcServer extends HiveMetaStoreGrpc.HiveMetaStoreImplBase

----------------------------------------

TITLE: Viewing Column Statistics in HiveQL
DESCRIPTION: HiveQL command to view column statistics for a specific table and column.

LANGUAGE: SQL
CODE:
describe formatted [table_name] [column_name];

----------------------------------------

TITLE: Replication Event Handling SQL Commands
DESCRIPTION: SQL commands used for handling different replication events in Hive, including database operations, table operations, partition operations, and data insertion. These commands are part of the export/import mechanism used in Hive's replication system.

LANGUAGE: sql
CODE:
DROP DATABASE CASCADE
DROP TABLE ... FOR REPLICATION('id')
EXPORT ... FOR REPLICATION
EXPORT ... FOR METADATA REPLICATION
IMPORT
ALTER TABLE ... DROP PARTITION(...) FOR REPLICATION('id')

----------------------------------------

TITLE: Adding New Markdown Content
DESCRIPTION: Command to create a new markdown file for the Hive documentation site using Hugo.

LANGUAGE: bash
CODE:
hugo new general/Downloads.md

----------------------------------------

TITLE: Creating Hive Table with Map Type and Binary Serialization
DESCRIPTION: Example showing how to create a Hive table with a map column type that uses binary serialization for storing values in Accumulo.

LANGUAGE: sql
CODE:
CREATE TABLE hive_map(key int, value map<string,int>) 
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES (
  "accumulo.columns.mapping" = ":rowID,cf:*",
  "accumulo.default.storage" = "binary"
);

----------------------------------------

TITLE: Enabling Beeline-based Hive CLI
DESCRIPTION: Environment variable setting to enable the new Beeline-based Hive CLI implementation instead of the deprecated version.

LANGUAGE: bash
CODE:
export USE_DEPRECATED_CLI=false

----------------------------------------

TITLE: Failed DDL Response
DESCRIPTION: Example JSON response from a failed DDL command showing empty stdout, stderr containing error message, and non-zero exitcode indicating failure.

LANGUAGE: json
CODE:
{
  "stdout": "",
  "stderr": "WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated...\n            Hive history file=/tmp/ctdean/hive_job_log_ctdean_201204051246_689834593.txt\n            FAILED: Parse Error: line 1:5 Failed to recognize predicate 'tab'...\n\n            ",
  "exitcode": 11
}

----------------------------------------

TITLE: Verifying Hugo Installation
DESCRIPTION: Command to check the installed version of Hugo.

LANGUAGE: bash
CODE:
hugo version

----------------------------------------

TITLE: Creating Local Copy of Remote Table in Apache Hive
DESCRIPTION: This snippet shows how to create a local copy of a remote table using a CREATE TABLE AS SELECT (CTAS) statement in Hive.

LANGUAGE: SQL
CODE:
create table default.emr_clone as select * from test_emr_tbl;

----------------------------------------

TITLE: Vectorized Query Example and Table Creation
DESCRIPTION: Example showing table creation, data insertion, and executing a vectorized query with explain plan.

LANGUAGE: sql
CODE:
create table vectorizedtable(state string,id int) stored as orc;

insert into vectorizedtable values('haryana',1);
set hive.vectorized.execution.enabled = true;
explain select count(*) from vectorizedtable;

----------------------------------------

TITLE: Custom Import Location Example
DESCRIPTION: Example showing how to specify a custom target location during import.

LANGUAGE: sql
CODE:
export table department to 'hdfs_exports_location/department';
import table department from 'hdfs_exports_location/department' 
       location 'import_target_location/department';

----------------------------------------

TITLE: Defining FilterPlan Abstract Class for HBase Query Plans
DESCRIPTION: Abstract class representing a filter plan for HBase queries. Provides methods for combining filters using AND/OR operations and retrieving scan plans.

LANGUAGE: java
CODE:
public static abstract class FilterPlan {
    abstract FilterPlan and(FilterPlan other);
    abstract FilterPlan or(FilterPlan other);
    abstract List<ScanPlan> getPlans();
}

----------------------------------------

TITLE: HTML Link Implementation
DESCRIPTION: HTML anchor tags for linking to Google Analytics and their privacy policy pages.

LANGUAGE: html
CODE:
<a href="http://www.google.com/analytics/">Google Analytics</a>
<a href="http://www.google.com/privacy.html">privacy policy</a>

----------------------------------------

TITLE: HiveServer2 Setup with Remote Metastore
DESCRIPTION: Commands for configuring HiveServer2 with a remote metastore connection and data persistence.

LANGUAGE: bash
CODE:
docker run -d -p 10000:10000 -p 10002:10002 --env SERVICE_NAME=hiveserver2 \
     --env SERVICE_OPTS="-Dhive.metastore.uris=thrift://metastore:9083" \
     --mount source=warehouse,target=/opt/hive/data/warehouse \
     --env IS_RESUME="true" \
     --name hiveserver2 apache/hive:4.0.0

----------------------------------------

TITLE: Manual Skewed Join Optimization Queries in Hive SQL
DESCRIPTION: Two SQL queries that manually handle skewed data by splitting the join operation into two parts: one for the skewed key (id=1) and another for all other keys.

LANGUAGE: sql
CODE:
select A.id from A join B on A.id = B.id where A.id <> 1;
select A.id from A join B on A.id = B.id where A.id = 1 and B.id = 1;

----------------------------------------

TITLE: UDAF Evaluator Class Structure
DESCRIPTION: Shows the skeleton structure of a UDAF evaluator class with required method implementations

LANGUAGE: Java
CODE:
public static class GenericUDAFHistogramNumericEvaluator extends GenericUDAFEvaluator {

  // For PARTIAL1 and COMPLETE: ObjectInspectors for original data
  private PrimitiveObjectInspector inputOI;
  private PrimitiveObjectInspector nbinsOI;

  // For PARTIAL2 and FINAL: ObjectInspectors for partial aggregations (list of doubles)
  private StandardListObjectInspector loi;

  @Override
  public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException {
    super.init(m, parameters);
    // return type goes here
  }

  @Override
  public Object terminatePartial(AggregationBuffer agg) throws HiveException {
    // return value goes here
  }

  @Override
  public Object terminate(AggregationBuffer agg) throws HiveException {
    // final return value goes here
  }

  @Override
  public void merge(AggregationBuffer agg, Object partial) throws HiveException {
  }

  @Override
  public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {
  }

  // Aggregation buffer definition and manipulation methods 
  static class StdAgg implements AggregationBuffer {
  };

  @Override
  public AggregationBuffer getNewAggregationBuffer() throws HiveException {
  }

  @Override
  public void reset(AggregationBuffer agg) throws HiveException {
  }    
}

----------------------------------------

TITLE: Creating Tables with Storage Handlers in Hive SQL
DESCRIPTION: SQL syntax for creating tables with storage handlers in Hive. Shows the complete CREATE TABLE syntax with the STORED BY clause for specifying custom storage handlers.

LANGUAGE: sql
CODE:
CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name
  [(col_name data_type [COMMENT col_comment], ...)]
  [COMMENT table_comment]
  [PARTITIONED BY (col_name data_type [col_comment], col_name data_type [COMMENT col_comment], ...)]
  [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name, ...)] INTO num_buckets BUCKETS]
  [
   [ROW FORMAT row_format] [STORED AS file_format]
   | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)]
  ]
  [LOCATION hdfs_path]
  [AS select_statement]

----------------------------------------

TITLE: Running HiveServer2 with Remote Metastore
DESCRIPTION: Docker command to run HiveServer2 with a specified remote Metastore and volume for data persistence.

LANGUAGE: shell
CODE:
docker run -d -p 10000:10000 -p 10002:10002 --env SERVICE_NAME=hiveserver2 \
   --env SERVICE_OPTS="-Dhive.metastore.uris=thrift://metastore:9083" \
   --mount source=warehouse,target=/opt/hive/data/warehouse \
   --env IS_RESUME="true" \
   --name hiveserver2 apache/hive:${HIVE_VERSION}

----------------------------------------

TITLE: Moving Table Between Catalogs and Databases
DESCRIPTION: Example of moving a table from the Hive catalog to the Spark catalog, including creating the target database.

LANGUAGE: bash
CODE:
# Create the desired target database in spark catalog if it doesn't already exist.
beeline ... -e "create database if not exists newdb";
schematool -moveDatabase newdb -fromCatalog hive -toCatalog spark

# Now move the table to target db under the spark catalog.
schematool -moveTable table1 -fromCatalog hive -toCatalog spark  -fromDatabase db1 -toDatabase newdb

----------------------------------------

TITLE: Launching HiveServer2 with Embedded Metastore
DESCRIPTION: Docker command to run HiveServer2 with an embedded Metastore using Derby as the metastore database.

LANGUAGE: shell
CODE:
docker run -d -p 10000:10000 -p 10002:10002 --env SERVICE_NAME=hiveserver2 --name hive4 apache/hive:${HIVE_VERSION}

----------------------------------------

TITLE: Deleting Hive Database via WebHCat API - Curl Example
DESCRIPTION: Example curl command demonstrating how to delete a database named 'newdb' using the WebHCat REST API endpoint.

LANGUAGE: curl
CODE:
% curl -s -X DELETE "http://localhost:50111/templeton/v1/ddl/database/newdb?user.name=ctdean"

----------------------------------------

TITLE: Storage Handler Interface Implementation in Java
DESCRIPTION: Java interface definition for implementing custom storage handlers in Hive. Includes methods for specifying input/output formats, SerDe class, and configuration properties.

LANGUAGE: java
CODE:
package org.apache.hadoop.hive.ql.metadata;

import java.util.Map;

import org.apache.hadoop.conf.Configurable;
import org.apache.hadoop.hive.metastore.HiveMetaHook;
import org.apache.hadoop.hive.ql.plan.TableDesc;
import org.apache.hadoop.hive.serde2.SerDe;
import org.apache.hadoop.mapred.InputFormat;
import org.apache.hadoop.mapred.OutputFormat;

public interface HiveStorageHandler extends Configurable {
  public Class<? extends InputFormat> getInputFormatClass();
  public Class<? extends OutputFormat> getOutputFormatClass();
  public Class<? extends SerDe> getSerDeClass();
  public HiveMetaHook getMetaHook();
  public void configureTableJobProperties(
    TableDesc tableDesc,
    Map<String, String> jobProperties);
}

----------------------------------------

TITLE: Changing Metastore Port
DESCRIPTION: Command to start the Hive metastore service on a custom port

LANGUAGE: bash
CODE:
hive --service metastore -p <port_num>

----------------------------------------

TITLE: Cloning Hive Repository
DESCRIPTION: Command to clone the Apache Hive source code repository from GitHub

LANGUAGE: bash
CODE:
git clone https://github.com/apache/hive

----------------------------------------

TITLE: REPL STATUS Command Syntax
DESCRIPTION: The syntax for the new REPL STATUS command, used to check the replication status of a database.

LANGUAGE: SQL
CODE:
REPL STATUS <dbname>;

----------------------------------------

TITLE: Exporting Hive Version Environment Variable
DESCRIPTION: Sets an environment variable for the Hive version to be used in subsequent commands.

LANGUAGE: shell
CODE:
export HIVE_VERSION=4.0.0

----------------------------------------

TITLE: IndexPredicateAnalyzer Class Implementation
DESCRIPTION: Core class for analyzing and managing index predicates in Hive's filter pushdown system. Handles comparison operators and column name allowances for index searches.

LANGUAGE: java
CODE:
public class IndexPredicateAnalyzer
{
  public IndexPredicateAnalyzer();

  /**
 * Registers a comparison operator as one which can be satisfied
 * by an index search.  Unless this is called, analyzePredicate
 * will never find any indexable conditions.
   *
 * @param udfName name of comparison operator as returned
 * by either {@link GenericUDFBridge#getUdfName} (for simple UDF's)
 * or udf.getClass().getName() (for generic UDF's).
   */
  public void addComparisonOp(String udfName);

  /**
 * Clears the set of column names allowed in comparisons.  (Initially, all
 * column names are allowed.)
   */
  public void clearAllowedColumnNames();

  /**
 * Adds a column name to the set of column names allowed.
   *
 * @param columnName name of column to be allowed
   */
  public void allowColumnName(String columnName);

  /**
 * Analyzes a predicate.
   *
 * @param predicate predicate to be analyzed
   *
 * @param searchConditions receives conditions produced by analysis
   *
 * @return residual predicate which could not be translated to
 * searchConditions
   */
  public ExprNodeDesc analyzePredicate(
    ExprNodeDesc predicate,
    final List<IndexSearchCondition> searchConditions);

  /**
 * Translates search conditions back to ExprNodeDesc form (as
 * a left-deep conjunction).
   *
 * @param searchConditions (typically produced by analyzePredicate)
   *
 * @return ExprNodeDesc form of search conditions
   */
  public ExprNodeDesc translateSearchConditions(
    List<IndexSearchCondition> searchConditions);
}

----------------------------------------

TITLE: HiveMetaHook Interface Implementation in Java
DESCRIPTION: Java interface definition for implementing metadata hooks in Hive storage handlers. Includes methods for handling table creation and deletion events.

LANGUAGE: java
CODE:
package org.apache.hadoop.hive.metastore;

import org.apache.hadoop.hive.metastore.api.MetaException;
import org.apache.hadoop.hive.metastore.api.Partition;
import org.apache.hadoop.hive.metastore.api.Table;

public interface HiveMetaHook {
  public void preCreateTable(Table table)
    throws MetaException;
  public void rollbackCreateTable(Table table)
    throws MetaException;
  public void commitCreateTable(Table table)
    throws MetaException;
  public void preDropTable(Table table)
    throws MetaException;
  public void rollbackDropTable(Table table)
    throws MetaException;
  public void commitDropTable(Table table, boolean deleteData)
    throws MetaException;

----------------------------------------

TITLE: DataNucleus Configuration XML
DESCRIPTION: XML configuration for setting up DataNucleus auto start mechanism for the Hive metastore

LANGUAGE: xml
CODE:
 <property>
    <name>datanucleus.autoStartMechanism</name>
    <value>SchemaTable</value>
  </property>

----------------------------------------

TITLE: Running Metastore with Postgres and Volume
DESCRIPTION: Docker command to run Metastore with an external Postgres database and volume for data persistence.

LANGUAGE: shell
CODE:
docker run -d -p 9083:9083 --env SERVICE_NAME=metastore --env DB_DRIVER=postgres \
   --env SERVICE_OPTS="-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore_db -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=password" \
   --mount source=warehouse,target=/opt/hive/data/warehouse \
   --mount type=bind,source=`mvn help:evaluate -Dexpression=settings.localRepository -q -DforceStdout`/org/postgresql/postgresql/42.5.1/postgresql-42.5.1.jar,target=/opt/hive/lib/postgres.jar \
   --name metastore-standalone apache/hive:${HIVE_VERSION}

----------------------------------------

TITLE: Invalid Subpartition Configuration
DESCRIPTION: Example showing invalid case where static partition is specified as subpartition of dynamic partition. This is not allowed due to directory hierarchy constraints.

LANGUAGE: hql
CODE:
-- throw an exception
INSERT OVERWRITE TABLE T PARTITION (ds, hr = 11) 
SELECT key, value, ds/*, hr*/ FROM srcpart WHERE ds is not null and hr=11;

----------------------------------------

TITLE: Supported IN Subquery Example
DESCRIPTION: Example of a valid IN subquery that will be supported in the implementation.

LANGUAGE: sql
CODE:
SELECT p_size IN (
		SELECT MAX(p_size) FROM part)
FROM part

----------------------------------------

TITLE: Creating User Table with Custom Accumulo Table Name
DESCRIPTION: Example of creating a Hive table that maps to a custom-named Accumulo table with specific column family and qualifier mappings.

LANGUAGE: sql
CODE:
CREATE TABLE users(key int, userid int, username string) 
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES ("accumulo.columns.mapping" = ":rowID,f:userid,f:nickname")
WITH TBLPROPERTIES ("accumulo.table.name" = "hive_users");

----------------------------------------

TITLE: Configuring Markdown Frontmatter
DESCRIPTION: YAML frontmatter configuration for the privacy policy page, setting the title, date, and draft status.

LANGUAGE: markdown
CODE:
---
title: "Privacy Policy"
date: 2022-09-13T19:35:56+05:30
draft: false
---

----------------------------------------

TITLE: Example Hive SQL Queries
DESCRIPTION: A set of example SQL queries to demonstrate basic Hive operations like creating tables, inserting data, and querying.

LANGUAGE: sql
CODE:
show tables;
create table hive_example(a string, b int) partitioned by(c int);
alter table hive_example add partition(c=1);
insert into hive_example partition(c=1) values('a', 1), ('a', 2),('b',3);
select count(distinct a) from hive_example;
select sum(b) from hive_example;

----------------------------------------

TITLE: Creating and Dropping Views in Hive SQL
DESCRIPTION: Syntax for creating and dropping views in Hive SQL. Includes optional column names and comments for the view definition.

LANGUAGE: SQL
CODE:
CREATE VIEW [IF NOT EXISTS] view_name [(column_name [COMMENT column_comment], ...) ]
[COMMENT table_comment]
AS SELECT ...

DROP VIEW view_name

----------------------------------------

TITLE: Installing Hugo on macOS
DESCRIPTION: Command to install Hugo on macOS using Homebrew package manager.

LANGUAGE: bash
CODE:
brew install hugo

----------------------------------------

TITLE: Running Pig with HCatalog Support
DESCRIPTION: Command to run Pig with HCatalog support enabled, which automatically includes necessary jars and configurations.

LANGUAGE: Shell
CODE:
pig -useHCatalog

----------------------------------------

TITLE: Setting Materialized View Rewriting Time Window in Hive
DESCRIPTION: Demonstrates how to set the time window for materialized view rewriting to allow for some staleness in the data.

LANGUAGE: HiveQL
CODE:
SET hive.materializedview.rewriting.time.window=10min;

----------------------------------------

TITLE: Initializing Hugo Front Matter Template
DESCRIPTION: A template for Hugo content pages that sets up the basic front matter. The title is derived from the page name with hyphens replaced by spaces and proper title casing applied. Includes automatic date insertion and draft status.

LANGUAGE: hugo
CODE:
---
title: "{{ replace .Name "-" " " | title }}"
date: {{ .Date }}
draft: true
---

----------------------------------------

TITLE: Setting Up Environment Variables for HCatalog in Pig
DESCRIPTION: Shell commands to set up environment variables for using HCatalog with Pig when not using the -useHCatalog flag. This includes setting paths for Hadoop, Hive, and HCatalog installations.

LANGUAGE: Shell
CODE:
export HADOOP_HOME=<path_to_hadoop_install>

export HIVE_HOME=<path_to_hive_install>

export HCAT_HOME=<path_to_hcat_install>

export PIG_CLASSPATH=$HCAT_HOME/share/hcatalog/hcatalog-core*.jar:\
$HCAT_HOME/share/hcatalog/hcatalog-pig-adapter*.jar:\
$HIVE_HOME/lib/hive-metastore-*.jar:$HIVE_HOME/lib/libthrift-*.jar:\
$HIVE_HOME/lib/hive-exec-*.jar:$HIVE_HOME/lib/libfb303-*.jar:\
$HIVE_HOME/lib/jdo2-api-*-ec.jar:$HIVE_HOME/conf:$HADOOP_HOME/conf:\
$HIVE_HOME/lib/slf4j-api-*.jar

export PIG_OPTS=-Dhive.metastore.uris=thrift://<hostname>:<port>

----------------------------------------

TITLE: Querying File Names and Block Offsets in Hive
DESCRIPTION: Example showing how to select input file name and block offset along with data columns

LANGUAGE: hql
CODE:
select INPUT__FILE__NAME, key, BLOCK__OFFSET__INSIDE__FILE from src;

----------------------------------------

TITLE: HTML License Comment Block
DESCRIPTION: Apache License 2.0 header comment block explaining the licensing terms for the documentation

LANGUAGE: html
CODE:
<!---
  Licensed to the Apache Software Foundation (ASF) under one
  or more contributor license agreements.  See the NOTICE file
  distributed with this work for additional information
  regarding copyright ownership.  The ASF licenses this file
  to you under the Apache License, Version 2.0 (the
  "License"); you may not use this file except in compliance
  with the License.  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing,
  software distributed under the License is distributed on an
  "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
  KIND, either express or implied.  See the License for the
  specific language governing permissions and limitations
  under the License. -->

----------------------------------------

TITLE: Range Partitioning Query
DESCRIPTION: SQL query to sample and partition data for efficient sorting, using row sequencing and table sampling.

LANGUAGE: sql
CODE:
add jar lib/hive-contrib-0.7.0.jar;
set mapred.reduce.tasks=1;
create temporary function row_sequence as 
'org.apache.hadoop.hive.contrib.udf.UDFRowSequence';
select transaction_id from
(select transaction_id
from transactions
tablesample(bucket 1 out of 10000 on transaction_id) s 
order by transaction_id 
limit 10000000) x
where (row_sequence() % 910000)=0
order by transaction_id
limit 11;

----------------------------------------

TITLE: Enabling/Disabling Materialized View Rewriting in Hive
DESCRIPTION: Shows how to enable or disable a materialized view for query rewriting.

LANGUAGE: SQL
CODE:
ALTER MATERIALIZED VIEW [db_name.]materialized_view_name ENABLE|DISABLE REWRITE;

----------------------------------------

TITLE: Basic HCatalog CLI Usage
DESCRIPTION: Shows the basic usage syntax for the HCatalog command line interface with available options.

LANGUAGE: bash
CODE:
Usage:  hcat  { -e "<query>" | -f <filepath> }  [-g <group>] [-p <perms>] [-D<name>=<value>]

----------------------------------------

TITLE: Hive Map Value Access
DESCRIPTION: Shows how to retrieve values from a Hive map using key-based access.

LANGUAGE: hive
CODE:
M['all']  -- returns 'foobar' from map {'f' -> 'foo', 'b' -> 'bar', 'all' -> 'foobar'}

----------------------------------------

TITLE: Default CSV SerDe Properties in Apache Hive
DESCRIPTION: This snippet shows the default values for separator, quote, and escape characters used by the CSV SerDe if not explicitly specified.

LANGUAGE: SQL
CODE:
DEFAULT_ESCAPE_CHARACTER \
DEFAULT_QUOTE_CHARACTER  "
DEFAULT_SEPARATOR        ,

----------------------------------------

TITLE: Case J1: Join Predicate Example - Left Outer Join with Preserved Row Table
DESCRIPTION: Demonstrates a left outer join query where the join predicate is on the preserved row table (s1). Shows the explain plan with stage dependencies and operator trees.

LANGUAGE: sql
CODE:
explain 
select s1.key, s2.key 
from src s1 left join src s2 on s1.key > '2';

----------------------------------------

TITLE: Executing Hive Query with Incompatible Schema
DESCRIPTION: This snippet demonstrates the error that occurs when executing a Hive query against an incompatible metastore schema.

LANGUAGE: bash
CODE:
$ build/dist/bin/hive -e "show tables"
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient