TITLE: Sanity Check with Playwright
DESCRIPTION: Initial crawler setup to verify the scraping environment by printing category content from the start URL. Uses PlaywrightCrawler to navigate the page and extract category elements.

LANGUAGE: javascript
CODE:
document.querySelectorAll('.collection-block-item');

----------------------------------------

TITLE: Complete Crawlee Data Extraction and Saving Example (JavaScript)
DESCRIPTION: A full example of a Crawlee script that extracts data from a website and saves it using Dataset.pushData(). This includes the crawler setup, data extraction logic, and data saving.

LANGUAGE: javascript
CODE:
{Example}

----------------------------------------

TITLE: Complete E-commerce Crawler Implementation
DESCRIPTION: Full implementation of a Playwright crawler that handles both category and product detail pages, including pagination logic.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        if (request.label === 'DETAIL') {
            // We're not doing anything with the details yet.
        } else if (request.label === 'CATEGORY') {
            // We are now on a category page. We can use this to paginate through and enqueue all products,
            // as well as any subsequent pages we find

            await page.waitForSelector('.product-item > a');
            await enqueueLinks({
                selector: '.product-item > a',
                label: 'DETAIL', // <= note the different label
            });

            // Now we need to find the "Next" button and enqueue the next page of results (if it exists)
            const nextButton = await page.$('a.pagination__next');
            if (nextButton) {
                await enqueueLinks({
                    selector: 'a.pagination__next',
                    label: 'CATEGORY', // <= note the same label
                });
            }
        } else {
            // This means we're on the start page, with no label.
            // On this page, we just want to enqueue all the category pages.

            await page.waitForSelector('.collection-block-item');
            await enqueueLinks({
                selector: '.collection-block-item',
                label: 'CATEGORY',
            });
        }
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: DevTools Element Query Selector Example
DESCRIPTION: JavaScript query selector command to find collection block items in browser DevTools

LANGUAGE: typescript
CODE:
document.querySelectorAll('.collection-block-item');

----------------------------------------

TITLE: Complete E-commerce Crawler Implementation
DESCRIPTION: Full implementation of a Playwright crawler that handles both category and product detail pages, including pagination logic.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        if (request.label === 'DETAIL') {
            // We're not doing anything with the details yet.
        } else if (request.label === 'CATEGORY') {
            // We are now on a category page. We can use this to paginate through and enqueue all products,
            // as well as any subsequent pages we find

            await page.waitForSelector('.product-item > a');
            await enqueueLinks({
                selector: '.product-item > a',
                label: 'DETAIL', // <= note the different label
            });

            // Now we need to find the "Next" button and enqueue the next page of results (if it exists)
            const nextButton = await page.$('a.pagination__next');
            if (nextButton) {
                await enqueueLinks({
                    selector: 'a.pagination__next',
                    label: 'CATEGORY', // <= note the same label
                });
            }
        } else {
            // This means we're on the start page, with no label.
            // On this page, we just want to enqueue all the category pages.

            await page.waitForSelector('.collection-block-item');
            await enqueueLinks({
                selector: '.collection-block-item',
                label: 'CATEGORY',
            });
        }
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Complete Store Crawling Implementation
DESCRIPTION: Full implementation of a crawler that handles both category and product detail pages. Uses conditional logic based on request labels to handle different page types and includes pagination handling.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        if (request.label === 'DETAIL') {
            // We're not doing anything with the details yet.
        } else if (request.label === 'CATEGORY') {
            // We are now on a category page. We can use this to paginate through and enqueue all products,
            // as well as any subsequent pages we find

            await page.waitForSelector('.product-item > a');
            await enqueueLinks({
                selector: '.product-item > a',
                label: 'DETAIL', // <= note the different label
            });

            // Now we need to find the "Next" button and enqueue the next page of results (if it exists)
            const nextButton = await page.$('a.pagination__next');
            if (nextButton) {
                await enqueueLinks({
                    selector: 'a.pagination__next',
                    label: 'CATEGORY', // <= note the same label
                });
            }
        } else {
            // This means we're on the start page, with no label.
            // On this page, we just want to enqueue all the category pages.

            await page.waitForSelector('.collection-block-item');
            await enqueueLinks({
                selector: '.collection-block-item',
                label: 'CATEGORY',
            });
        }
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Complete Crawlee Scraping Example with Data Saving (JavaScript)
DESCRIPTION: This is the final code example for the tutorial, showing a complete Crawlee scraping script that includes data saving functionality. It uses PlaywrightCrawler to scrape data and Dataset.pushData() to store the results.

LANGUAGE: javascript
CODE:
{Example}

----------------------------------------

TITLE: Exporting Dataset to CSV using Crawlee in TypeScript
DESCRIPTION: This code snippet demonstrates how to use Crawlee to export an entire dataset to a single CSV file. It utilizes the `CheerioCrawler`, `Dataset`, and `KeyValueStore` classes from Crawlee. The crawler processes a list of URLs, extracts title and URL information, and then exports the collected data to a CSV file.

LANGUAGE: typescript
CODE:
import { CheerioCrawler, Dataset, KeyValueStore } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        await Dataset.pushData({
            url: request.url,
            title,
        });

        await enqueueLinks();
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run(['https://crawlee.dev']);

// Export the entire default dataset to a single CSV file
const csv = await Dataset.exportToValue('CSV');
await KeyValueStore.setRecord({ key: 'OUTPUT', value: csv, contentType: 'text/csv' });

console.log('Crawl finished.');

----------------------------------------

TITLE: Complete Crawlee Scraping Example with Data Saving (JavaScript)
DESCRIPTION: This is the final code example for the tutorial, showing a complete Crawlee scraping script that includes data extraction and saving using Dataset.pushData(). It uses PlaywrightCrawler to scrape book information from a website.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);

        if (request.label === 'START') {
            await enqueueLinks({
                selector: 'a[href*="/books/"]',
                label: 'DETAIL',
            });
        }

        if (request.label === 'DETAIL') {
            const results = await page.evaluate(() => {
                const bookDetail = {};
                bookDetail.url = window.location.href;
                bookDetail.title = document.querySelector('h1').textContent;
                bookDetail.author = document.querySelector('.author').textContent;
                const ratingElement = document.querySelector('.rating_stars');
                bookDetail.rating = ratingElement ? Number(ratingElement.getAttribute('title').match(/\d+/)[0]) : null;
                return bookDetail;
            });

            await Dataset.pushData(results);
        }
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run(['https://books.toscrape.com']);

----------------------------------------

TITLE: Complete Store Crawling Implementation
DESCRIPTION: Full implementation of a PlaywrightCrawler that handles category pages, product details, and pagination using labeled requests and specific selectors.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        if (request.label === 'DETAIL') {
            // We're not doing anything with the details yet.
        } else if (request.label === 'CATEGORY') {
            // We are now on a category page. We can use this to paginate through and enqueue all products,
            // as well as any subsequent pages we find

            await page.waitForSelector('.product-item > a');
            await enqueueLinks({
                selector: '.product-item > a',
                label: 'DETAIL', // <= note the different label
            });

            // Now we need to find the "Next" button and enqueue the next page of results (if it exists)
            const nextButton = await page.$('a.pagination__next');
            if (nextButton) {
                await enqueueLinks({
                    selector: 'a.pagination__next',
                    label: 'CATEGORY', // <= note the same label
                });
            }
        } else {
            // This means we're on the start page, with no label.
            // On this page, we just want to enqueue all the category pages.

            await page.waitForSelector('.collection-block-item');
            await enqueueLinks({
                selector: '.collection-block-item',
                label: 'CATEGORY',
            });
        }
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Scraping Hacker News with PlaywrightCrawler
DESCRIPTION: Demonstrates recursive web scraping of Hacker News using PlaywrightCrawler and RequestQueue. The script starts from a single URL, finds and enqueues links to subsequent pages, and stores results in a default dataset. Intended for use with the apify/actor-node-playwright-chrome Docker image when running on Apify Platform.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, RequestQueue } from 'crawlee';

const queue = await RequestQueue.open();
await queue.addRequest({ url: 'https://news.ycombinator.com' });

const crawler = new PlaywrightCrawler({
    requestQueue: queue,
    // maxRequestsPerCrawl: 20, // Limit the number of requests
    async requestHandler({ request, page, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        // Save results
        await this.push({
            url: request.url,
            title: title,
            //...
        });

        // Extract links to next pages
        // 1. Add all links from page
        await enqueueLinks({
            // globs: ["https://news.ycombinator.com/*"], // Optionally limit the search
        });
    },
});

await crawler.run();

----------------------------------------

TITLE: Basic Crawlee Web Scraping Example
DESCRIPTION: Example demonstrating how to use PlaywrightCrawler to crawl web pages, extract titles, save data, and follow links

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

// PlaywrightCrawler crawls the web using a headless
// browser controlled by the Playwright library.
const crawler = new PlaywrightCrawler({
    // Use the requestHandler to process each of the crawled pages.
    async requestHandler({ request, page, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.loadedUrl} is '${title}'`);

        // Save results as JSON to ./storage/datasets/default
        await Dataset.pushData({ title, url: request.loadedUrl });

        // Extract links from the current page
        // and add them to the crawling queue.
        await enqueueLinks();
    },
    // Uncomment this option to see the browser window.
    // headless: false,
});

// Add first URL to the queue and start the crawl.
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Complete E-commerce Crawling Implementation
DESCRIPTION: Full implementation of a crawler that handles both category and product detail pages, including pagination. Uses different labels to manage crawling flow and selective link enqueueing based on page context.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        if (request.label === 'DETAIL') {
            // We're not doing anything with the details yet.
        } else if (request.label === 'CATEGORY') {
            // We are now on a category page. We can use this to paginate through and enqueue all products,
            // as well as any subsequent pages we find

            await page.waitForSelector('.product-item > a');
            await enqueueLinks({
                selector: '.product-item > a',
                label: 'DETAIL', // <= note the different label
            });

            // Now we need to find the "Next" button and enqueue the next page of results (if it exists)
            const nextButton = await page.$('a.pagination__next');
            if (nextButton) {
                await enqueueLinks({
                    selector: 'a.pagination__next',
                    label: 'CATEGORY', // <= note the same label
                });
            }
        } else {
            // This means we're on the start page, with no label.
            // On this page, we just want to enqueue all the category pages.

            await page.waitForSelector('.collection-block-item');
            await enqueueLinks({
                selector: '.collection-block-item',
                label: 'CATEGORY',
            });
        }
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Scraping Hacker News with PlaywrightCrawler in JavaScript
DESCRIPTION: This code demonstrates how to use PlaywrightCrawler and RequestQueue to recursively scrape the Hacker News website. It starts with a single URL, finds links to next pages, enqueues them, and continues until no more desired links are available. The results are stored in the default dataset.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset, RequestQueue } from 'crawlee';

// Create an instance of the PlaywrightCrawler class - a crawler
// that automatically loads the URLs in headless Chrome / Playwright.
const crawler = new PlaywrightCrawler({
    // Use the requestQueue to automatically manage the crawler's requests
    requestQueue: await RequestQueue.open(),

    // This function will be called for each URL to crawl.
    // Here you can write the Playwright code to extract data from the page.
    async requestHandler({ request, page, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        // A function to be evaluated by Playwright within the browser context.
        const data = await page.$$eval('.athing', ($posts) => {
            const scrapedData = [];
            // We're getting the title, rank and URL of each post on Hacker News.
            $posts.forEach(($post) => {
                scrapedData.push({
                    title: $post.querySelector('.title a').innerText,
                    rank: $post.querySelector('.rank').innerText,
                    href: $post.querySelector('.title a').href,
                });
            });
            return scrapedData;
        });

        // Store the results to the default dataset.
        await Dataset.pushData(data);

        // Find a link to the next page and enqueue it if it exists.
        await enqueueLinks({
            selector: '.morelink',
        });
    },
    // Let's limit our crawls to make the example quick and safe.
    maxRequestsPerCrawl: 20,
});

// Add first URL to the queue and start the crawl.
await crawler.run(['https://news.ycombinator.com/']);

----------------------------------------

TITLE: Crawling and Parsing Websites with CheerioCrawler in JavaScript
DESCRIPTION: This code demonstrates how to use CheerioCrawler to crawl URLs from an external file, make HTTP requests, parse HTML with Cheerio, and extract the page title and h1 tags. It utilizes the Crawlee framework and Cheerio library for efficient web scraping.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';
import { readFile } from 'fs/promises';

// Read the list of URLs from a file
const urls = await readFile('urls.txt', 'utf8')
    .then((text) => text.split('\n').filter((line) => line.trim() !== ''));

// Create an instance of the CheerioCrawler class - a crawler
// that automatically loads the URLs and parses their HTML using the Cheerio library.
const crawler = new CheerioCrawler({
    // Let's limit the number of concurrent connections to 10
    maxConcurrency: 10,
    // Here we use the requestHandler to process each of the crawled pages
    async requestHandler({ $, request, enqueueLinks, log }) {
        // Extract data from the page using Cheerio
        const title = $('title').text();
        const h1texts = $('h1')
            .map((_, el) => $(el).text())
            .get();

        // Save the results to dataset. In local configuration,
        // the data will be stored in ./storage/datasets/default
        await Dataset.pushData({
            url: request.url,
            title,
            h1texts,
        });

        // Extract links from the current page
        // and add them to the crawling queue.
        await enqueueLinks();
    },
});

// Run the crawler with initial urls
await crawler.run(urls);

----------------------------------------

TITLE: Session Management with Multiple Crawlee Crawlers
DESCRIPTION: Comprehensive guide showing SessionPool implementation across BasicCrawler, HttpCrawler, CheerioCrawler, JSDOMCrawler, PlaywrightCrawler, PuppeteerCrawler, and standalone usage. The SessionPool helps manage proxy rotation, cookie handling, and custom settings to prevent blocking and ensure even IP distribution.

LANGUAGE: javascript
CODE:
{HttpSource}

LANGUAGE: javascript
CODE:
{CheerioSource}

LANGUAGE: javascript
CODE:
{JSDOMSource}

LANGUAGE: javascript
CODE:
{PlaywrightSource}

LANGUAGE: javascript
CODE:
{PuppeteerSource}

LANGUAGE: javascript
CODE:
{StandaloneSource}

----------------------------------------

TITLE: Crawling Links with Cheerio Crawler
DESCRIPTION: Implementation of a web crawler using Cheerio Crawler to navigate and extract links from web pages. It demonstrates how to use enqueueLinks() to automatically add new URLs to the crawling queue and process them recursively.

LANGUAGE: typescript
CODE:
{CheerioSource}

----------------------------------------

TITLE: Building CheerioCrawler with RequestQueue
DESCRIPTION: Shows how to create a CheerioCrawler instance with a RequestQueue and a request handler that extracts webpage titles using Cheerio.

LANGUAGE: typescript
CODE:
// Add import of CheerioCrawler
import { RequestQueue, CheerioCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

// Create the crawler and add the queue with our URL
// and a request handler to process the page.
const crawler = new CheerioCrawler({
    requestQueue,
    // The `$` argument is the Cheerio object
    // which contains parsed HTML of the website.
    async requestHandler({ $, request }) {
        // Extract <title> text with Cheerio.
        // See Cheerio documentation for API docs.
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

// Start the crawler and wait for it to finish
await crawler.run();

----------------------------------------

TITLE: Crawling Multiple URLs with Puppeteer Crawler in JavaScript
DESCRIPTION: This code snippet shows how to use Puppeteer Crawler to crawl multiple specified URLs. It includes setup for the crawler and defines the request handler. Note: This requires the 'apify/actor-node-puppeteer-chrome' image on the Apify Platform.

LANGUAGE: javascript
CODE:
// Code content not available in the provided text

----------------------------------------

TITLE: Crawling and Parsing Websites with CheerioCrawler in JavaScript
DESCRIPTION: This code demonstrates how to use CheerioCrawler to crawl URLs from an external file, make HTTP requests, parse HTML with Cheerio, and extract the page title and h1 tags. It utilizes the Crawlee framework and Cheerio library for efficient web scraping.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';
import { readFile } from 'fs/promises';

// Read the list of URLs from a file
const urls = await readFile('urls.txt', 'utf8')
    .then((text) => text.split('\n').filter((line) => line.trim() !== ''));

// Create an instance of the CheerioCrawler class - a crawler
// that automatically loads the URLs and parses their HTML using the Cheerio library.
const crawler = new CheerioCrawler({
    // Let's limit the number of concurrent connections to 10
    maxConcurrency: 10,
    // Here we use the requestHandler to process each of the crawled pages
    async requestHandler({ $, request, enqueueLinks, log }) {
        // Extract data from the page using Cheerio
        const title = $('title').text();
        const h1texts = $('h1')
            .map((_, el) => $(el).text())
            .get();

        // Save the results to dataset. In local configuration,
        // the data will be stored in ./storage/datasets/default
        await Dataset.pushData({
            url: request.url,
            title,
            h1texts,
        });

        // Extract links from the current page
        // and add them to the crawling queue.
        await enqueueLinks();
    },
});

// Run the crawler with initial urls
await crawler.run(urls);

----------------------------------------

TITLE: Basic HTTP Request with BasicCrawler
DESCRIPTION: Demonstrates how to send a basic HTTP request using BasicCrawler's sendRequest function with Got Scraping.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest();
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Scraping Hacker News with PlaywrightCrawler in TypeScript
DESCRIPTION: This code demonstrates how to use PlaywrightCrawler to scrape Hacker News. It initializes a RequestQueue, sets up the crawler with custom handling logic, and processes the scraped data. The crawler follows pagination links and stores results in the default dataset.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

// Create an instance of the PlaywrightCrawler class - a crawler
// that automatically loads the URLs in headless Chrome / Playwright.
const crawler = new PlaywrightCrawler({
    // Use the requestHandler to process each of the crawled pages.
    async requestHandler({ request, page, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        // Save results as JSON to ./storage/datasets/default
        await Dataset.pushData({
            url: request.url,
            title,
        });

        // Extract links from the current page
        // and add them to the crawling queue.
        await enqueueLinks({
            globs: ['https://news.ycombinator.com/*'],
            label: 'detail',
        });
    },
    // Uncomment this option to see the browser window.
    // headless: false,
});

// Add first URL to the queue and start the crawl.
await crawler.run(['https://news.ycombinator.com/']);

----------------------------------------

TITLE: Customizing Browser Fingerprints with PlaywrightCrawler
DESCRIPTION: This snippet demonstrates how to customize browser fingerprints using PlaywrightCrawler in Crawlee. It sets specific operating system, browser, and language preferences for the fingerprint generation.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                operatingSystems: ['windows'],
                browsers: [{ name: 'firefox', minVersion: 80 }],
                languages: ['en-US', 'en']
            }
        }
    },
    // ...
});

----------------------------------------

TITLE: Scraping Hacker News with PuppeteerCrawler in JavaScript
DESCRIPTION: This code demonstrates how to use PuppeteerCrawler and RequestQueue to recursively scrape the Hacker News website. It extracts article titles, ranks, and scores, storing the results in a dataset. The crawler handles pagination by finding and enqueueing links to next pages.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, RequestQueue, Dataset } from 'crawlee';

const crawler = new PuppeteerCrawler({
    requestQueue: await RequestQueue.open(),
    async requestHandler({ page, request, enqueueLinks }) {
        console.log(`Processing ${request.url}`);

        // Extract data from the page
        const pageData = await page.evaluate(() => {
            const data = [];
            document.querySelectorAll('.athing').forEach((el) => {
                data.push({
                    title: el.querySelector('.title a').innerText,
                    rank: Number(el.querySelector('.rank').innerText.replace('.', '')),
                    href: el.querySelector('.title a').href,
                });
            });
            return data;
        });

        // Add score to each article
        for (const article of pageData) {
            const scoreElement = await page.$(`#score_${article.id}`);
            if (scoreElement) {
                article.score = Number((await page.evaluate(el => el.innerText, scoreElement)).replace(' points', ''));
            }
        }

        // Save the data to dataset
        await Dataset.pushData(pageData);

        // Enqueue all links to next pages
        await enqueueLinks({
            selector: '.morelink',
        });
    },
    maxRequestsPerCrawl: 10, // Limit the number of requests
});

// Run the crawler with initial request
await crawler.run(['https://news.ycombinator.com/']);

----------------------------------------

TITLE: Category Page Crawler Implementation
DESCRIPTION: Implementation of a Playwright crawler that processes category pages and enqueues category links using specific selectors.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        // Wait for the category cards to render,
        // otherwise enqueueLinks wouldn't enqueue anything.
        await page.waitForSelector('.collection-block-item');

        // Add links to the queue, but only from
        // elements matching the provided selector.
        await enqueueLinks({
            selector: '.collection-block-item',
            label: 'CATEGORY',
        });
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Building a Basic CheerioCrawler
DESCRIPTION: Shows how to create a CheerioCrawler instance with a RequestQueue and request handler to extract page titles. Demonstrates the complete flow of setting up and running a crawler.

LANGUAGE: typescript
CODE:
// Add import of CheerioCrawler
import { RequestQueue, CheerioCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

// Create the crawler and add the queue with our URL
// and a request handler to process the page.
const crawler = new CheerioCrawler({
    requestQueue,
    // The `$` argument is the Cheerio object
    // which contains parsed HTML of the website.
    async requestHandler({ $, request }) {
        // Extract <title> text with Cheerio.
        // See Cheerio documentation for API docs.
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

// Start the crawler and wait for it to finish
await crawler.run();

----------------------------------------

TITLE: Category Page Crawling with PlaywrightCrawler
DESCRIPTION: Implementation of a crawler that specifically targets category pages using selectors and request labels. Demonstrates waiting for elements and selective link enqueueing.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);

        // Only run this logic on the main category listing, not on sub-pages.
        if (request.label !== 'CATEGORY') {

          // Wait for the category cards to render,
          // otherwise enqueueLinks wouldn't enqueue anything.
          await page.waitForSelector('.collection-block-item');

          // Add links to the queue, but only from
          // elements matching the provided selector.
          await enqueueLinks({
              selector: '.collection-block-item',
              label: 'CATEGORY',
          });
        }
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Basic HTTP Request with BasicCrawler
DESCRIPTION: Demonstrates how to send a basic HTTP request using BasicCrawler's sendRequest function with Got Scraping.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest();
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Building a CheerioCrawler with RequestQueue in Crawlee
DESCRIPTION: This code demonstrates how to create a CheerioCrawler, set up a RequestQueue, and define a request handler to extract and log the title of a webpage. It shows the complete process of initializing and running a basic crawler.

LANGUAGE: typescript
CODE:
// Add import of CheerioCrawler
import { RequestQueue, CheerioCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

// Create the crawler and add the queue with our URL
// and a request handler to process the page.
const crawler = new CheerioCrawler({
    requestQueue,
    // The `$` argument is the Cheerio object
    // which contains parsed HTML of the website.
    async requestHandler({ $, request }) {
        // Extract <title> text with Cheerio.
        // See Cheerio documentation for API docs.
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

// Start the crawler and wait for it to finish
await crawler.run();

----------------------------------------

TITLE: Configuring SessionPool in Crawlee Crawlers
DESCRIPTION: Example implementations showing how to configure and use SessionPool with various Crawlee crawler types including BasicCrawler, HttpCrawler, CheerioCrawler, JSDOMCrawler, PlaywrightCrawler, PuppeteerCrawler, and standalone usage. SessionPool helps manage proxy rotation, cookie handling, and session persistence.

LANGUAGE: javascript
CODE:
// Code snippets are referenced from external files:
// BasicSource
// HttpSource
// CheerioSource
// JSDOMSource
// PlaywrightSource
// PuppeteerSource
// StandaloneSource

----------------------------------------

TITLE: Complete Store Crawling with PlaywrightCrawler in Crawlee
DESCRIPTION: This comprehensive example shows how to crawl an entire online store, including category pages, product detail pages, and pagination. It uses request labels to differentiate between different types of pages and applies appropriate crawling logic for each.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        if (request.label === 'DETAIL') {
            // We're not doing anything with the details yet.
        } else if (request.label === 'CATEGORY') {
            // We are now on a category page. We can use this to paginate through and enqueue all products,
            // as well as any subsequent pages we find

            await page.waitForSelector('.product-item > a');
            await enqueueLinks({
                selector: '.product-item > a',
                label: 'DETAIL', // <= note the different label
            });

            // Now we need to find the "Next" button and enqueue the next page of results (if it exists)
            const nextButton = await page.$('a.pagination__next');
            if (nextButton) {
                await enqueueLinks({
                    selector: 'a.pagination__next',
                    label: 'CATEGORY', // <= note the same label
                });
            }
        } else {
            // This means we're on the start page, with no label.
            // On this page, we just want to enqueue all the category pages.

            await page.waitForSelector('.collection-block-item');
            await enqueueLinks({
                selector: '.collection-block-item',
                label: 'CATEGORY',
            });
        }
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Implementing Recursive Web Scraping with PuppeteerCrawler
DESCRIPTION: Demonstrates setting up a PuppeteerCrawler instance to recursively crawl Hacker News. Uses RequestQueue for managing crawl requests and stores results in a local dataset. Includes configuration for running on the Apify Platform using the apify/actor-node-puppeteer-chrome image.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, RequestQueue } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://news.ycombinator.com' });

const crawler = new PuppeteerCrawler({
    requestQueue,
    async requestHandler({ request, page, enqueueLinks }) {
        // Add all links from page to RequestQueue
        await enqueueLinks({
            globs: ['https://news.ycombinator.com/*'],
            label: 'link',
        });

        // Extract data from the page
        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);
    },
});

await crawler.run();

----------------------------------------

TITLE: Crawling URLs with CheerioCrawler in JavaScript
DESCRIPTION: This code snippet shows how to set up and use CheerioCrawler to crawl a list of URLs from an external file. It demonstrates parsing HTML with Cheerio, extracting page titles and h1 tags, and handling the crawling process.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';
import { readFile } from 'fs/promises';

// Define a function to process each page
const processPage = async ({ $, request }) => {
    // Extract data from the page
    const title = $('title').text();
    const h1texts = $('h1')
        .map((_, el) => $(el).text())
        .get();

    // Log the extracted data
    console.log(`Title: ${title}`);
    console.log(`H1 texts: ${h1texts.join(', ')}`);

    // Save the results to the default dataset
    await Dataset.pushData({
        url: request.url,
        title,
        h1texts,
    });
};

// Read the list of URLs from a file
const urlList = await readFile('urls.txt', 'utf8')
    .then((text) => text.split('\n').filter((line) => line.trim() !== ''));

// Create an instance of CheerioCrawler
const crawler = new CheerioCrawler({
    // Function called for each crawled page
    requestHandler: processPage,
    // Let's limit paralelism to 1 request
    maxRequestsPerMinute: 60,
});

// Add URLs to the crawler's queue
await crawler.addRequests(urlList);

// Run the crawler
await crawler.run();

console.log('Crawler finished.');

----------------------------------------

TITLE: Implementing CheerioCrawler for Web Scraping in JavaScript
DESCRIPTION: Example showing how to use CheerioCrawler to crawl URLs, parse HTML with Cheerio, and extract page titles and h1 tags. This demonstrates core web scraping functionality using plain HTTP requests and Cheerio's HTML parsing capabilities.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    // Function called for each URL
    async requestHandler({ $, request, enqueueLinks, log }) {
        const title = $('title').text();
        log.info(`Title of ${request.loadedUrl} is '${title}'`);

        // Save results as JSON to ./storage/datasets/default
        await Dataset.pushData({
            url: request.loadedUrl,
            title,
            headings: $('h1').toArray().map(h1 => $(h1).text()),
        });

        // Extract links from the current page
        // and add them to the crawling queue
        await enqueueLinks();
    },
});

// Add first URL to the queue and start the crawl
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Complete Product Data Scraping with Playwright in JavaScript
DESCRIPTION: This code snippet combines all the previous snippets to scrape complete product information including URL, manufacturer, title, SKU, current price, and stock availability using Playwright.

LANGUAGE: javascript
CODE:
const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']
const manufacturer = urlPart.split('-')[0]; // 'sennheiser'

const title = await page.locator('.product-meta h1').textContent();
const sku = await page.locator('span.product-meta__sku-number').textContent();

const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Selective Link Crawling with CheerioCrawler in JavaScript
DESCRIPTION: Demonstrates using CheerioCrawler to crawl website links that match specific patterns using the globs property. The crawler adds matching links to a RequestQueue for processing based on defined glob patterns.

LANGUAGE: javascript
CODE:
<RunnableCodeBlock className="language-js" type="cheerio">
	{CrawlSource}
</RunnableCodeBlock>

----------------------------------------

TITLE: Implementing Recursive Web Crawling with PuppeteerCrawler
DESCRIPTION: Example showing how to set up and execute a recursive web crawl using PuppeteerCrawler. This implementation is compatible with the Apify Platform when used with the apify/actor-node-puppeteer-chrome Docker image.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, Dataset } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ request, page, enqueueLinks }) {
        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);
        
        await Dataset.pushData({
            url: request.url,
            title,
        });

        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10,
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Crawling Hacker News with PuppeteerCrawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to set up a PuppeteerCrawler to scrape the Hacker News website. It uses RequestQueue for managing URLs, extracts data from each page, and stores results in a dataset. The crawler recursively follows pagination links until no more are available.

LANGUAGE: JavaScript
CODE:
import { PuppeteerCrawler, Dataset, RequestQueue } from 'crawlee';

// Create an instance of the RequestQueue class
const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://news.ycombinator.com/' });

// Create an instance of the PuppeteerCrawler class
const crawler = new PuppeteerCrawler({
    requestQueue,
    // Function called for each URL
    async requestHandler({ request, page, enqueueLinks }) {
        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);

        // Extract data from the page using Puppeteer
        const data = await page.$$eval('.athing', ($posts) => {
            return $posts.map($post => {
                const id = $post.id;
                const rank = $post.querySelector('.rank')?.innerText;
                const title = $post.querySelector('.title a')?.innerText;
                const url = $post.querySelector('.title a')?.href;

                return {
                    id,
                    rank,
                    title,
                    url,
                };
            })
        });

        // Save the data to dataset
        await Dataset.pushData(data);

        // Add links to RequestQueue
        await enqueueLinks({
            globs: ['https://news.ycombinator.com/news?p=*'],
            label: 'LIST',
        });
    },
    // Function to handle errors
    async failedRequestHandler({ request }) {
        console.log(`Request ${request.url} failed too many times`);
    },
});

// Run the crawler
await crawler.run();

----------------------------------------

TITLE: Implementing Web Scraping with CheerioCrawler
DESCRIPTION: Example demonstrates how to use CheerioCrawler to crawl URLs, parse HTML using Cheerio library, and extract data. The code shows how to load URLs from an external file and extract page titles and h1 tags from each webpage.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, log } from 'crawlee';
import { readFile } from 'fs/promises';

// Add URLs to a list
const urls = JSON.parse(
    await readFile(new URL('./urls.json', import.meta.url), 'utf8')
);

// Create an instance of the CheerioCrawler class
const crawler = new CheerioCrawler({
    // Let's limit the number of concurrently processed requests
    maxConcurrency: 5,
    // Function called for each URL
    async requestHandler({ $, request }) {
        const title = $('title').text();
        log.info(`URL: ${request.url} TITLE: ${title}`);

        // Extract h1 texts
        const h1texts = $('h1')
            .map((_, el) => $(el).text())
            .get();

        // Save results as JSON to ./storage/datasets/default
        await Dataset.pushData({
            url: request.url,
            title,
            h1texts,
        });
    },
});

// Add first URL to the queue and start the crawl.
await crawler.run(urls);

----------------------------------------

TITLE: Implementing PuppeteerCrawler for Web Scraping in TypeScript
DESCRIPTION: This code snippet demonstrates how to set up and use PuppeteerCrawler to scrape the Hacker News website. It includes configuration for the crawler, request queue handling, and data extraction logic.

LANGUAGE: typescript
CODE:
import { Dataset, createPuppeteerRouter, PuppeteerCrawler } from 'crawlee';

// createPuppeteerRouter() is only a helper to get better
// intellisense when creating the router.
// Feel free to use regular function instead.
const router = createPuppeteerRouter();

router.addDefaultHandler(async ({ page, enqueueLinks, log }) => {
    const title = await page.title();
    log.info(`Title of ${page.url()} is '${title}'`);

    // Save results as JSON to ./storage/datasets/default
    await Dataset.pushData({
        title,
        url: page.url(),
    });

    // Extract links from the current page
    // and add them to the crawling queue.
    await enqueueLinks({
        globs: ['https://news.ycombinator.com/*'],
        label: 'detail',
    });
});

router.addHandler('detail', async ({ request, page, log }) => {
    const title = await page.title();
    log.info(`Title of ${request.url} is '${title}'`);

    // Save results as JSON to ./storage/datasets/default
    await Dataset.pushData({
        title,
        url: request.url,
        // Include the hostname property to show in
        // the dataset what domains we crawled.
        hostname: new URL(request.url).hostname,
    });
});

// Create a PuppeteerCrawler
const crawler = new PuppeteerCrawler({
    requestHandler: router,
    // Uncomment this option to see the browser window.
    // headless: false,
});

// Add first URL to the queue and start the crawl.
await crawler.run(['https://news.ycombinator.com/']);

----------------------------------------

TITLE: Initializing CheerioCrawler for Web Scraping with JavaScript
DESCRIPTION: Example showing how to set up and use CheerioCrawler to crawl URLs, make HTTP requests, and parse HTML using the Cheerio library. The crawler extracts page titles and h1 tags from each crawled page.

LANGUAGE: javascript
CODE:
import { Dataset, createCheerioRouter, CheerioCrawler } from 'crawlee';

const router = createCheerioRouter();

router.addDefaultHandler(async ({ $, request, enqueueLinks, log }) => {
    const title = $('title').text();
    log.info(`Title of ${request.loadedUrl} is '${title}'`);

    const h1texts = [];
    $('h1').each((_, el) => {
        h1texts.push($(el).text());
    });

    // Save results to dataset
    await Dataset.pushData({
        url: request.loadedUrl,
        title,
        h1texts,
    });

    // Extract links from the current page
    // and add them to the crawling queue.
    await enqueueLinks();
});

const crawler = new CheerioCrawler({
    // Let's crawl only JavaScript category on Wikipedia
    requestHandler: router,
    maxRequestsPerCrawl: 20, // Limitation for only 20 requests (do not use if you want to crawl all links)
});

await crawler.run(['https://www.wikipedia.org']);

----------------------------------------

TITLE: Complete Product Data Extraction Script
DESCRIPTION: Complete script combining all scraping operations to extract product information including URL, manufacturer, title, SKU, price and stock status.

LANGUAGE: javascript
CODE:
const urlPart = request.url.split('/').slice(-1);
const manufacturer = urlPart.split('-')[0];

const title = await page.locator('.product-meta h1').textContent();
const sku = await page.locator('span.product-meta__sku-number').textContent();

const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Scraping Hacker News with PlaywrightCrawler in TypeScript
DESCRIPTION: This code demonstrates how to use PlaywrightCrawler and RequestQueue to recursively scrape the Hacker News website. It starts with a single URL, finds links to next pages, enqueues them, and continues until no more desired links are available. The results are stored in the default dataset.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

const crawler = new PlaywrightCrawler({
    // Use the requestHandler to process each of the crawled pages.
    async requestHandler({ request, page, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        // Save results as JSON to ./storage/datasets/default
        await Dataset.pushData({
            url: request.url,
            title,
        });

        // Extract links from the current page
        // and add them to the crawling queue.
        await enqueueLinks({
            globs: ['https://news.ycombinator.com/*'],
            label: 'detail',
        });
    },
    // Uncomment this option to see the browser window.
    // headless: false,
});

// Add first URL to the queue and start the crawl.
await crawler.run(['https://news.ycombinator.com/']);

----------------------------------------

TITLE: Complete Crawlee Data Extraction and Saving Example (JavaScript)
DESCRIPTION: This is the final code example that includes the entire process of extracting data and saving it using Crawlee. It uses PlaywrightCrawler and Dataset.pushData().

LANGUAGE: javascript
CODE:
{Example}

----------------------------------------

TITLE: Scraping Hacker News with PuppeteerCrawler in JavaScript/TypeScript
DESCRIPTION: This code snippet demonstrates how to use PuppeteerCrawler and RequestQueue to recursively scrape the Hacker News website. It starts with a single URL, finds links to next pages, enqueues them, and continues until no more desired links are available. The results are stored in the default dataset.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, Dataset, log } from 'crawlee';
import { EnqueueLinks } from '@crawlee/puppeteer';

const crawler = new PuppeteerCrawler({
    async requestHandler({ request, page, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        // Save results as JSON to ./storage/datasets/default
        await Dataset.pushData({
            url: request.url,
            title,
        });

        // Extract links to next pages
        await enqueueLinks({
            globs: ['https://news.ycombinator.com/*'],
            label: 'list',
        });
    },
    maxRequestsPerCrawl: 20, // Limit the number of requests
});

await crawler.run(['https://news.ycombinator.com/']);

log.info('Crawler finished.');

----------------------------------------

TITLE: Configuring Multi-stage Dockerfile for Crawlee Actor
DESCRIPTION: Complete Dockerfile configuration that sets up a Crawlee actor with Node.js, Playwright, and Chrome. Uses a multi-stage build process to optimize the final image size and includes proper dependency management, build steps, and production configuration.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16 AS builder

COPY --chown=myuser package*.json ./

RUN npm install --include=dev --audit=false

COPY --chown=myuser . ./

RUN npm run build

FROM apify/actor-node-playwright-chrome:16

COPY --from=builder --chown=myuser /home/myuser/dist ./dist

COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent

----------------------------------------

TITLE: Crawling Website Links with Cheerio
DESCRIPTION: Demonstrates how to crawl all links on a website using Cheerio Crawler. Uses enqueueLinks() to add new URLs to the RequestQueue and processes them recursively. Includes configurable maxRequestsPerCrawl limit.

LANGUAGE: typescript
CODE:
import { CheerioSpider, createCheerioRouter } from 'crawlee';

const router = createCheerioRouter();

router.addDefaultHandler(async ({ enqueueLinks, $, log }) => {
    log.info(`enqueueing new URLs`);
    await enqueueLinks();
});

const spider = new CheerioSpider({
    // maxRequestsPerCrawl: 20,
    requestHandler: router,
});

await spider.run(['https://example.com']);

----------------------------------------

TITLE: Modified Crawlee Main Script with Apify Integration
DESCRIPTION: Example of how to modify a Crawlee crawler script to work with the Apify Platform by adding Actor.init() and Actor.exit() calls

LANGUAGE: typescript
CODE:
import { Actor } from 'apify';
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

await Actor.init();

log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

await Actor.exit();

----------------------------------------

TITLE: Scraping Hacker News with PlaywrightCrawler in JavaScript
DESCRIPTION: This code sets up a PlaywrightCrawler to recursively scrape Hacker News. It extracts titles, ranks, and scores from each page, handles pagination, and stores results in a dataset. The crawler uses a RequestQueue to manage URLs and continues until no more links are available.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset, RequestQueue } from 'crawlee';

// Create an instance of the PlaywrightCrawler class - a crawler
// that automatically loads the URLs in headless Chrome / Playwright.
const crawler = new PlaywrightCrawler({
    // Use the requestQueue to add additional requests
    requestQueue: await RequestQueue.open(),

    // Here you can set options that are passed to the Playwright browser.
    launchContext: {
        launchOptions: {
            headless: true,
        },
    },

    // Stop crawling after several pages
    maxRequestsPerCrawl: 50,

    // This function will be called for each URL to crawl.
    // Here you can write the Playwright scripts you are familiar with,
    // with the exception that browsers and pages are automatically managed by the Apify SDK.
    async requestHandler({ request, page, log }) {
        log.info(`Processing ${request.url}...`);

        // A function to be evaluated by Playwright within the browser context.
        const data = await page.$$eval('.athing', ($posts) => {
            const scrapedData = [];

            // We're getting the title, rank and ID of each post on Hacker News.
            $posts.forEach(($post) => {
                scrapedData.push({
                    title: $post.querySelector('.title a').innerText,
                    rank: $post.querySelector('.rank').innerText,
                    id: $post.id,
                });
            });

            return scrapedData;
        });

        // Add all of the post data to the dataset
        await Dataset.pushData(data);

        // Find a link to the next page and enqueue it if it exists.
        const infos = await page.$$eval('.subtext', ($subtext) => {
            const scrapedInfos = [];
            $subtext.forEach(($s) => {
                const points = $s.querySelector('.score')?.innerText;
                scrapedInfos.push({
                    points: points ? Number(points.replace(' points', '')) : null,
                });
            });
            return scrapedInfos;
        });

        data.forEach((item, i) => (item.points = infos[i].points));

        // Save the scraped data to dataset.
        await Dataset.pushData(data);

        // Find the next page and enqueue it
        const nextPageHref = await page
            .$eval('.morelink', (el) => el.href)
            .catch(() => null);

        if (nextPageHref) {
            await crawler.requestQueue.addRequest({ url: nextPageHref });
        }
    },

    // This function is called if the page processing failed more than maxRequestRetries + 1 times.
    failedRequestHandler({ request, log }) {
        log.info(`Request ${request.url} failed twice.`);
    },
});

// Run the crawler and wait for it to finish.
await crawler.run(['https://news.ycombinator.com/']);

console.log('Crawler finished.');

----------------------------------------

TITLE: URL List Crawling with JSDOMCrawler in JavaScript
DESCRIPTION: Shows how to use JSDOMCrawler to process multiple URLs from an external file, parse HTML content using JSDOM, and extract page titles and h1 tags from each page.

LANGUAGE: javascript
CODE:
{JSDOMCrawlerSource}

----------------------------------------

TITLE: Initializing Playwright Crawler for Web Scraping
DESCRIPTION: Example demonstrating how to set up PlaywrightCrawler with RequestQueue to recursively scrape the Hacker News website using headless Chrome/Playwright. The crawler processes links and stores results in a default dataset.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

// Create an instance of the PlaywrightCrawler class - a crawler
// that automatically loads the URLs in headless Chrome / Playwright.
const crawler = new PlaywrightCrawler({
    // Here you can set options that are passed to the launchPersistentContext() function.
    launchContext: {
        launchOptions: {
            headless: true,
        },
    },

    // Stop crawling after several pages
    maxRequestsPerCrawl: 50,

    // This function will be called for each URL to crawl.
    // Here you can write the Playwright scripts you are familiar with,
    // with the exception that browsers and pages are automatically managed by the Crawler.
    async requestHandler({ request, page, enqueueLinks, log }) {
        // Add all links from page to RequestQueue
        await enqueueLinks({
            globs: ['https://news.ycombinator.com/*'],
            label: 'detail',
        });

        // Extract data from a page using Playwright-native queries
        const title = await page.locator('.title a').first().textContent();

        // Save the data to dataset
        await Dataset.pushData({
            url: request.url,
            title,
        });
    },
});

// Add first URL to traverse to the queue and start the crawl.
await crawler.run(['https://news.ycombinator.com']);

----------------------------------------

TITLE: Complete Product Data Scraping Implementation
DESCRIPTION: Complete implementation combining all scraping operations to extract product details from a page.

LANGUAGE: javascript
CODE:
const urlPart = request.url.split('/').slice(-1);
const manufacturer = urlPart.split('-')[0];

const title = await page.locator('.product-meta h1').textContent();
const sku = await page.locator('span.product-meta__sku-number').textContent();

const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Implementing Recursive Web Scraping with PuppeteerCrawler
DESCRIPTION: Shows how to set up a PuppeteerCrawler instance to recursively crawl Hacker News, starting from a single URL. The crawler finds and enqueues links to subsequent pages and stores results in a default dataset. Designed to run in a local environment or on the Apify Platform using the apify/actor-node-puppeteer-chrome image.

LANGUAGE: typescript
CODE:
{CrawlSource}

----------------------------------------

TITLE: Implementing Basic Web Crawler with Crawlee
DESCRIPTION: Example showing how to use BasicCrawler to download web pages using HTTP requests and store their raw HTML content. The crawler utilizes the sendRequest utility function from got-scraping module and saves data to the default dataset storage.

LANGUAGE: typescript
CODE:
import { BasicCrawler, Dataset } from 'crawlee';

// Create an instance of the BasicCrawler.
const crawler = new BasicCrawler({
    // Let's limit our requests to only 10 per minute
    maxRequestsPerMinute: 10,
    // Function called for each URL
    async requestHandler({ request, sendRequest }) {
        const response = await sendRequest();

        // Save the HTML content and URL to the default dataset
        await Dataset.pushData({
            url: request.url,
            html: response.body,
        });
    },
});

// Define the list of URLs to crawl
await crawler.run([
    'http://crawlee.dev',
    'https://crawlee.dev/docs/quick-start',
    'https://crawlee.dev/docs/introduction',
]);

----------------------------------------

TITLE: Initializing RequestQueue and Adding URL in Crawlee
DESCRIPTION: Creates a RequestQueue instance and adds a URL to be crawled. This demonstrates the basic setup for managing crawl targets.

LANGUAGE: typescript
CODE:
import { RequestQueue } from 'crawlee';

// First you create the request queue instance.
const requestQueue = await RequestQueue.open();
// And then you add one or more requests to it.
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

----------------------------------------

TITLE: Implementing Basic Web Crawling with Crawlee in TypeScript
DESCRIPTION: This code snippet demonstrates how to use Crawlee's BasicCrawler to download web pages, process their content, and store the results. It includes error handling and uses the Dataset class to save the crawled data.

LANGUAGE: typescript
CODE:
import { BasicCrawler, Dataset } from 'crawlee';

// Create an instance of the BasicCrawler class - a generic
// crawler that you can use to crawl any web pages.
const crawler = new BasicCrawler({
    // This function will be called for each URL to crawl.
    // It accepts a single parameter, which is an object with options as:
    // async requestHandler({ request, sendRequest, log, crawler }) {}
    async requestHandler({ request, sendRequest, log }) {
        const { url } = request;
        log.info(`Processing ${url}...`);

        // Fetch the page HTML via Crawlee's sendRequest utility method
        // By default, the method will use a configured SessionPool,
        // so each call will use a different proxy
        const { body } = await sendRequest({ url });

        // Store the results to the default dataset. In local configuration,
        // the data will be stored as JSON files in ./storage/datasets/default
        await Dataset.pushData({
            url,
            html: body,
        });
    },

    // This function is called if the function passed to `requestHandler`
    // throws an exception.
    async failedRequestHandler({ request, error }) {
        console.error(`Request ${request.url} failed twice.`);
    },
});

// Run the crawler and wait for it to finish.
await crawler.run([
    'https://crawlee.dev',
    'https://example.com',
    'https://apify.com',
]);

----------------------------------------

TITLE: Implementing Parallel Scraper with Child Processes in JavaScript
DESCRIPTION: This code creates a parallel scraper using Node.js child processes. It forks multiple instances of the scraper, each using a shared request queue. The code includes logic for both parent and worker processes.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { Actor } from 'apify';
import { getOrInitQueue } from './requestQueue.mjs';
import { router } from './routes.mjs';
import { fork } from 'child_process';

Configuration.set('persistStateIntervalMillis', 100000);

if (process.env.IS_WORKER_THREAD) {
    Configuration.set('purgeOnStart', false);
    const requestQueue = await getOrInitQueue(false);
    const config = new Configuration({
        storageClientOptions: {
            localDataDirectory: `./storage/worker-${process.env.WORKER_INDEX}`,
        },
    });

    const crawler = new CheerioCrawler(
        {
            requestQueue,
            requestHandler: router,
            maxConcurrency: 5,
        },
        config
    );

    Configuration.setOption('requestLocking.enabledForCrawlers', ['CheerioCrawler']);

    await crawler.run();
    process.exit(0);
} else {
    const workerCount = 2;
    const workers = [];

    for (let i = 0; i < workerCount; i++) {
        workers.push(
            new Promise((resolve) => {
                const worker = fork(process.argv[1], [], {
                    env: { ...process.env, IS_WORKER_THREAD: '1', WORKER_INDEX: i },
                });
                worker.on('message', (message) => {
                    Actor.pushData(message);
                });
                worker.on('exit', resolve);
            })
        );
    }

    await Promise.all(workers);
}

----------------------------------------

TITLE: Implementing HTTP Crawler with HttpCrawler in TypeScript
DESCRIPTION: This code snippet demonstrates the setup and execution of an HTTP crawler using the HttpCrawler class. It reads URLs from a file, makes HTTP requests, and saves the HTML content of each page. The crawler is configured with various options including max requests per crawl and request handler function.

LANGUAGE: typescript
CODE:
import { HttpCrawler, Dataset } from 'crawlee';
import { readFile } from 'fs/promises';

const crawler = new HttpCrawler({
    // Use the requestHandler to process each of the crawled pages.
    async requestHandler({ request, body, log }) {
        log.info(`Processing ${request.url}...`);

        // Save results as JSON to ./storage/datasets/default
        await Dataset.pushData({
            url: request.url,
            html: body,
        });
    },
    // Limit the number of requests
    maxRequestsPerCrawl: 20,
});

// Add URLs to crawl
const listOfUrls = await readFile('urls.txt', 'utf8');
for (const url of listOfUrls.split('\n')) {
    await crawler.addRequests([url]);
}

// Run the crawler
await crawler.run();

----------------------------------------

TITLE: Implementing Basic Web Scraping with Crawlee's BasicCrawler in TypeScript
DESCRIPTION: This code demonstrates how to use Crawlee's BasicCrawler to download web pages and store their HTML content. It uses the sendRequest utility function to make HTTP requests and stores the results in the default dataset.

LANGUAGE: typescript
CODE:
import { BasicCrawler, Dataset } from 'crawlee';

// Create an instance of the BasicCrawler class - a crawler that uses raw HTTP requests
const crawler = new BasicCrawler({
    // Define a list of URLs to crawl
    requestList: [
        { url: 'http://example.com/page-1' },
        { url: 'http://example.com/page-2' },
        { url: 'http://example.com/page-3' },
    ],
    // This function will be called for each URL to crawl.
    async requestHandler({ sendRequest, request, log }) {
        // sendRequest() is an example of how you can use the context aware HTTP clients
        const { body, statusCode } = await sendRequest();

        log.info(`Received response: ${statusCode}`)

        // Store the results as dataset
        await Dataset.pushData({
            url: request.url,
            html: body,
        });
    },
});

await crawler.run();

----------------------------------------

TITLE: Using SessionPool with Different Crawlee Crawlers
DESCRIPTION: Examples demonstrating how to implement SessionPool with various Crawlee crawler types. SessionPool helps manage proxy IP rotation, cookies, and session-specific settings while preventing IP blocking and ensuring even distribution of requests across available IPs.

LANGUAGE: javascript
CODE:
{BasicSource}

LANGUAGE: javascript
CODE:
{HttpSource}

LANGUAGE: javascript
CODE:
{CheerioSource}

LANGUAGE: javascript
CODE:
{JSDOMSource}

LANGUAGE: javascript
CODE:
{PlaywrightSource}

LANGUAGE: javascript
CODE:
{PuppeteerSource}

LANGUAGE: javascript
CODE:
{StandaloneSource}

----------------------------------------

TITLE: Using SessionPool with BasicCrawler in Crawlee
DESCRIPTION: This snippet demonstrates how to configure and use SessionPool with BasicCrawler in Crawlee. It shows how to create a session pool, set up proxy configuration, and use the session in the crawler's request handler.

LANGUAGE: js
CODE:
import { BasicCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new BasicCrawler({
    // The `useSessionPool` enables the `SessionPool` functionality for the crawler
    useSessionPool: true,
    // Configures the proxy to be used by the `SessionPool`
    proxyConfiguration,
    async requestHandler({ session, sendRequest }) {
        // Use `session.userData` to store custom data
        session.userData.numberOfRetries = session.userData.numberOfRetries ?? 0;

        try {
            const { body } = await sendRequest({
                ...(session.userData.numberOfRetries && {
                    // We want to retry the failed requests from the same IP so we keep the same proxy URL
                    proxyUrl: session.userData.proxyUrl,
                }),
            });

            // Process the response...

            // Increase the number of processed requests in the session
            session.userData.numberOfRetries++;
        } catch (err) {
            // Retire the proxy URL if we get blocked
            session.retire();
        }
    },
});

await crawler.run([
    'https://crawlee.dev',
]);

----------------------------------------

TITLE: Using SessionPool with BasicCrawler in Crawlee
DESCRIPTION: This snippet demonstrates how to configure and use SessionPool with BasicCrawler in Crawlee. It shows how to create a session pool, set up proxy configuration, and use the session in the crawler's request handler.

LANGUAGE: js
CODE:
import { BasicCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new BasicCrawler({
    // The `useSessionPool` enables the `SessionPool` functionality for the crawler
    useSessionPool: true,
    // Configures the proxy to be used by the `SessionPool`
    proxyConfiguration,
    async requestHandler({ session, sendRequest }) {
        // Use `session.userData` to store custom data
        session.userData.numberOfRetries = session.userData.numberOfRetries ?? 0;

        try {
            const { body } = await sendRequest({
                ...(session.userData.numberOfRetries && {
                    // We want to retry the failed requests from the same IP so we keep the same proxy URL
                    proxyUrl: session.userData.proxyUrl,
                }),
            });

            // Process the response...

            // Increase the number of processed requests in the session
            session.userData.numberOfRetries++;
        } catch (err) {
            // Retire the proxy URL if we get blocked
            session.retire();
        }
    },
});

await crawler.run([
    'https://crawlee.dev',
]);

----------------------------------------

TITLE: Implementing Route Handlers for Web Scraping
DESCRIPTION: Defines route handlers for different page types (detail pages, category pages, and default handler) using Crawlee's router. Each handler implements specific scraping logic for its page type and manages navigation flow.

LANGUAGE: javascript
CODE:
import { createPlaywrightRouter, Dataset } from 'crawlee';

// createPlaywrightRouter() is only a helper to get better
// intellisense and typings. You can use Router.create() too.
export const router = createPlaywrightRouter();

// This replaces the request.label === DETAIL branch of the if clause.
router.addHandler('DETAIL', async ({ request, page, log }) => {
    log.debug(`Extracting data: ${request.url}`);
    const urlPart = request.url.split('/').slice(-1);
    const manufacturer = urlPart[0].split('-')[0];

    const title = await page.locator('.product-meta h1').textContent();
    const sku = await page
        .locator('span.product-meta__sku-number')
        .textContent();

    const priceElement = page
        .locator('span.price')
        .filter({
            hasText: '$',
        })
        .first();

    const currentPriceString = await priceElement.textContent();
    const rawPrice = currentPriceString.split('$')[1];
    const price = Number(rawPrice.replaceAll(',', ''));

    const inStockElement = page
        .locator('span.product-form__inventory')
        .filter({
            hasText: 'In stock',
        })
        .first();

    const inStock = (await inStockElement.count()) > 0;

    const results = {
        url: request.url,
        manufacturer,
        title,
        sku,
        currentPrice: price,
        availableInStock: inStock,
    };

    log.debug(`Saving data: ${request.url}`);
    await Dataset.pushData(results);
});

router.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {
    log.debug(`Enqueueing pagination for: ${request.url}`);
    await page.waitForSelector('.product-item > a');
    await enqueueLinks({
        selector: '.product-item > a',
        label: 'DETAIL',
    });

    const nextButton = await page.$('a.pagination__next');
    if (nextButton) {
        await enqueueLinks({
            selector: 'a.pagination__next',
            label: 'CATEGORY',
        });
    }
});

router.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {
    log.debug(`Enqueueing categories from page: ${request.url}`);
    await page.waitForSelector('.collection-block-item');
    await enqueueLinks({
        selector: '.collection-block-item',
        label: 'CATEGORY',
    });
});

----------------------------------------

TITLE: Implementing Cheerio Crawler for Web Scraping in JavaScript
DESCRIPTION: Example code demonstrating how to use CheerioCrawler to crawl URLs, make HTTP requests, and parse HTML content using the Cheerio library. The crawler extracts page titles and h1 tags from each page it visits.

LANGUAGE: javascript
CODE:
<RunnableCodeBlock className="language-js" type="cheerio">
	{CheerioCrawlerSource}
</RunnableCodeBlock>

----------------------------------------

TITLE: Comprehensive Crawling of E-commerce Store with PlaywrightCrawler
DESCRIPTION: This snippet showcases a more comprehensive crawling strategy for an e-commerce store. It handles the start page, category pages, and product detail pages. It demonstrates how to use labels to differentiate between page types and how to handle pagination.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        if (request.label === 'DETAIL') {
            // We're not doing anything with the details yet.
        } else if (request.label === 'CATEGORY') {
            // We are now on a category page. We can use this to paginate through and enqueue all products,
            // as well as any subsequent pages we find

            await page.waitForSelector('.product-item > a');
            await enqueueLinks({
                selector: '.product-item > a',
                label: 'DETAIL', // <= note the different label
            });

            // Now we need to find the "Next" button and enqueue the next page of results (if it exists)
            const nextButton = await page.$('a.pagination__next');
            if (nextButton) {
                await enqueueLinks({
                    selector: 'a.pagination__next',
                    label: 'CATEGORY', // <= note the same label
                });
            }
        } else {
            // This means we're on the start page, with no label.
            // On this page, we just want to enqueue all the category pages.

            await page.waitForSelector('.collection-block-item');
            await enqueueLinks({
                selector: '.collection-block-item',
                label: 'CATEGORY',
            });
        }
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Simplified CheerioCrawler Setup with Implicit RequestQueue in Crawlee
DESCRIPTION: This snippet demonstrates a more concise way to set up a CheerioCrawler using the implicit RequestQueue. It uses the crawler.run() method to directly add URLs for crawling, simplifying the initialization process.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Implementing Router for Crawlee Scraping Project in JavaScript
DESCRIPTION: Defines a Router for a Crawlee project with handlers for detail pages, category pages, and a default handler. It includes data extraction, pagination handling, and link enqueuing.

LANGUAGE: javascript
CODE:
import { createPlaywrightRouter, Dataset } from 'crawlee';

export const router = createPlaywrightRouter();

router.addHandler('DETAIL', async ({ request, page, log }) => {
    log.debug(`Extracting data: ${request.url}`);
    const urlPart = request.url.split('/').slice(-1);
    const manufacturer = urlPart[0].split('-')[0];

    const title = await page.locator('.product-meta h1').textContent();
    const sku = await page
        .locator('span.product-meta__sku-number')
        .textContent();

    const priceElement = page
        .locator('span.price')
        .filter({
            hasText: '$',
        })
        .first();

    const currentPriceString = await priceElement.textContent();
    const rawPrice = currentPriceString.split('$')[1];
    const price = Number(rawPrice.replaceAll(',', ''));

    const inStockElement = page
        .locator('span.product-form__inventory')
        .filter({
            hasText: 'In stock',
        })
        .first();

    const inStock = (await inStockElement.count()) > 0;

    const results = {
        url: request.url,
        manufacturer,
        title,
        sku,
        currentPrice: price,
        availableInStock: inStock,
    };

    log.debug(`Saving data: ${request.url}`);
    await Dataset.pushData(results);
});

router.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {
    log.debug(`Enqueueing pagination for: ${request.url}`);

    await page.waitForSelector('.product-item > a');
    await enqueueLinks({
        selector: '.product-item > a',
        label: 'DETAIL',
    });

    const nextButton = await page.$('a.pagination__next');
    if (nextButton) {
        await enqueueLinks({
            selector: 'a.pagination__next',
            label: 'CATEGORY',
        });
    }
});

router.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {
    log.debug(`Enqueueing categories from page: ${request.url}`);

    await page.waitForSelector('.collection-block-item');
    await enqueueLinks({
        selector: '.collection-block-item',
        label: 'CATEGORY',
    });
});

----------------------------------------

TITLE: Using SessionPool with BasicCrawler in Crawlee
DESCRIPTION: This snippet demonstrates how to configure and use SessionPool with BasicCrawler in Crawlee. It shows how to create a session pool, set up proxy configuration, and use the session in the crawler's request handler.

LANGUAGE: js
CODE:
import { BasicCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new BasicCrawler({
    // The `useSessionPool` enables the `SessionPool` functionality for the crawler
    useSessionPool: true,
    // Configures the proxy to be used by the `SessionPool`
    proxyConfiguration,
    async requestHandler({ session, sendRequest }) {
        // Use `session.userData` to store custom data
        session.userData.numberOfRetries = session.userData.numberOfRetries ?? 0;

        try {
            const { body } = await sendRequest({
                ...(session.userData.numberOfRetries && {
                    // We want to retry the failed requests from the same IP so we keep the same proxy URL
                    proxyUrl: session.userData.proxyUrl,
                }),
            });

            // Process the response...

            // Increase the number of processed requests in the session
            session.userData.numberOfRetries++;
        } catch (err) {
            // Retire the proxy URL if we get blocked
            session.retire();
        }
    },
});

await crawler.run([
    'https://crawlee.dev',
]);

----------------------------------------

TITLE: Installing Crawlee with CLI
DESCRIPTION: Use the Crawlee CLI to quickly set up a new crawler project with all necessary dependencies and boilerplate code.

LANGUAGE: bash
CODE:
npx crawlee create my-crawler

----------------------------------------

TITLE: Scraping Hacker News with PlaywrightCrawler in JavaScript
DESCRIPTION: This code sets up a PlaywrightCrawler to recursively scrape Hacker News. It extracts titles, ranks, and scores from each page, handles pagination, and stores results in a dataset. The crawler uses a RequestQueue to manage URLs and continues until no more links are available.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset, RequestQueue } from 'crawlee';

// Create an instance of the PlaywrightCrawler class - a crawler
// that automatically loads the URLs in headless Chrome / Playwright.
const crawler = new PlaywrightCrawler({
    // Use the requestQueue to add additional requests
    requestQueue: await RequestQueue.open(),

    // Here you can set options that are passed to the Playwright browser.
    launchContext: {
        launchOptions: {
            headless: true,
        },
    },

    // Stop crawling after several pages
    maxRequestsPerCrawl: 50,

    // This function will be called for each URL to crawl.
    // Here you can write the Playwright scripts you are familiar with,
    // with the exception that browsers and pages are automatically managed by the Apify SDK.
    async requestHandler({ request, page, log }) {
        log.info(`Processing ${request.url}...`);

        // A function to be evaluated by Playwright within the browser context.
        const data = await page.$$eval('.athing', ($posts) => {
            const scrapedData = [];

            // We're getting the title, rank and ID of each post on Hacker News.
            $posts.forEach(($post) => {
                scrapedData.push({
                    title: $post.querySelector('.title a').innerText,
                    rank: $post.querySelector('.rank').innerText,
                    id: $post.id,
                });
            });

            return scrapedData;
        });

        // Add all of the post data to the dataset
        await Dataset.pushData(data);

        // Find a link to the next page and enqueue it if it exists.
        const infos = await page.$$eval('.subtext', ($subtext) => {
            const scrapedInfos = [];
            $subtext.forEach(($s) => {
                const points = $s.querySelector('.score')?.innerText;
                scrapedInfos.push({
                    points: points ? Number(points.replace(' points', '')) : null,
                });
            });
            return scrapedInfos;
        });

        data.forEach((item, i) => (item.points = infos[i].points));

        // Save the scraped data to dataset.
        await Dataset.pushData(data);

        // Find the next page and enqueue it
        const nextPageHref = await page
            .$eval('.morelink', (el) => el.href)
            .catch(() => null);

        if (nextPageHref) {
            await crawler.requestQueue.addRequest({ url: nextPageHref });
        }
    },

    // This function is called if the page processing failed more than maxRequestRetries + 1 times.
    failedRequestHandler({ request, log }) {
        log.info(`Request ${request.url} failed twice.`);
    },
});

// Run the crawler and wait for it to finish.
await crawler.run(['https://news.ycombinator.com/']);

console.log('Crawler finished.');

----------------------------------------

TITLE: Downloading Large Files with Node.js Streams using Crawlee's FileDownload Crawler
DESCRIPTION: This code demonstrates how to use Crawlee's FileDownload crawler to efficiently download large files using Node.js streams. It logs the download progress and stores the downloaded data in the key-value store. For local configurations, the data is saved as files in the './storage/key_value_stores/default' directory.

LANGUAGE: TypeScript
CODE:
import { FileDownload } from 'crawlee';

const crawler = new FileDownload({
    maxRequestsPerMinute: 60,
    maxConcurrency: 10,
    // In a local configuration, data will be stored in ./storage/key_value_stores/default
    // When deployed to the Apify platform, the data will be available under the "default" key-value store
    requestHandlerTimeoutSecs: 1800, // 30 minutes
    // To avoid storage overflow, keep only 100,000 most recent records in the dataset/KVS
    maxOutputItems: 100_000,
});

await crawler.run([
    {
        url: 'https://file-examples.com/wp-content/storage/2017/02/zip_10MB.zip',
        label: 'zip_10MB',
    },
    {
        url: 'https://file-examples.com/wp-content/storage/2017/04/file_example_MP4_1920_18MG.mp4',
        label: 'MP4_1920_18MG',
    },
]);

console.log('Crawler finished.')

----------------------------------------

TITLE: Downloading Files with Streams using FileDownload Crawler in JavaScript/TypeScript
DESCRIPTION: This code snippet demonstrates how to use the FileDownload crawler class to download files using Node.js streams. It shows how to configure the crawler, handle file downloads, log progress, and store the downloaded data in a key-value store. The example is particularly useful for efficiently handling large file downloads in web crawling scenarios.

LANGUAGE: JavaScript
CODE:
import { FileDownload } from 'crawlee';
import { createWriteStream } from 'fs';
import { pipeline } from 'stream/promises';

const crawler = new FileDownload({
    maxRequestsPerMinute: 5,
    // Using small value to better demonstrate the progress logging
    streamProgressFinishedInterval: 1000,
    savedFilesDirectory: './savedFiles',
    async requestHandler({ request, sendRequest, log, pushData }) {
        log.info(`Downloading file: ${request.url}`);

        const response = await sendRequest();
        const contentLength = Number(response.headers.get('content-length'));

        // Create a writable stream to save the file
        const writer = createWriteStream(`./savedFiles/${request.url.split('/').pop()}`);

        await pipeline(
            response.body,
            async function* (source) {
                let bytesDownloaded = 0;
                for await (const chunk of source) {
                    bytesDownloaded += chunk.length;
                    const progress = (bytesDownloaded / contentLength) * 100;
                    log.info(`Download progress: ${progress.toFixed(2)}%`);
                    yield chunk;
                }
            },
            writer
        );

        log.info('File downloaded successfully!');
        await pushData({ url: request.url, savedPath: writer.path });
    },
});

await crawler.run([
    'https://apify.com/docs/crawlee-brand-assets.zip',
    'https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4',
]);


----------------------------------------

TITLE: Scraping Hacker News with PuppeteerCrawler in JavaScript
DESCRIPTION: This code demonstrates how to set up a PuppeteerCrawler to scrape the Hacker News website. It uses a RequestQueue to manage URLs, extracts article details, and stores results in a dataset. The crawler recursively follows 'More' links to scrape multiple pages.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, Dataset, RequestQueue } from 'crawlee';

// Get the starting URL from the environment or use the default
const startUrls = [process.env.START_URL || 'https://news.ycombinator.com'];

// Create an instance of the PuppeteerCrawler class
const crawler = new PuppeteerCrawler({
    // Initialize the request queue
    requestQueue: await RequestQueue.open(),

    // Function called for each URL
    async requestHandler({ request, page, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        // Extract data from the page using Puppeteer
        const data = await page.$$eval('.athing', ($posts) => {
            return $posts.map((el) => {
                const rank = el.querySelector('.rank')?.innerText;
                const rowTitle = el.querySelector('.title a');
                const titleUrl = rowTitle?.href;
                const titleText = rowTitle?.innerText;

                return {
                    rank,
                    title: titleText,
                    url: titleUrl,
                };
            });
        });

        // Save the data to a named dataset
        await Dataset.pushData(data);

        // Enqueue the 'More' button
        await enqueueLinks({
            selector: '.morelink',
        });
    },
    // Function to handle errors
    failedRequestHandler({ request, error, log }) {
        log.error(`Request ${request.url} failed with error: ${error.message}`);
    },
});

// Run the crawler with the provided start URLs
await crawler.run(startUrls);

----------------------------------------

TITLE: Configuring PlaywrightCrawler for Hacker News Scraping
DESCRIPTION: Demonstrates setting up a PlaywrightCrawler instance to recursively crawl Hacker News. The crawler processes each page, extracts article information, finds pagination links, and stores results in a dataset. Uses headless Chrome via Playwright and handles request queuing.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

// Create an instance of the PlaywrightCrawler class - a crawler
// that automatically loads the URLs in headless Chrome / Playwright.
const crawler = new PlaywrightCrawler({
    // Use the requestHandler to process each of the crawled pages.
    async requestHandler({ request, page, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        // Extract data from the page using Playwright API.
        const data = {
            title,
            url: request.url,
            // Get text content of the whole page.
            text: await page.textContent('*'),
        };

        // Save the data to dataset.
        await Dataset.pushData(data);

        // Extract links from the current page
        // and add them to the crawling queue.
        await enqueueLinks({
            globs: ['https://news.ycombinator.com/*'],
            exclude: ['/user?*', '/hide?*'],
        });
    },
    // Uncomment this option to see the browser window.
    // headless: false,
});

// Add first URL to the queue and start the crawl.
await crawler.run(['https://news.ycombinator.com']);

----------------------------------------

TITLE: Automating GitHub Search Form with PuppeteerCrawler
DESCRIPTION: Implementation of a web crawler that automates GitHub's repository search form using PuppeteerCrawler. The crawler fills in search parameters including search term, repository owner, start date, and programming language, then submits the form and saves the search results to a dataset. Requires PuppeteerCrawler and runs in headless Chrome.

LANGUAGE: typescript
CODE:
import { Dataset, PuppeteerCrawler } from 'crawlee';

const searchQuery = 'apify-client';
const repoOwner = 'apify';
const language = 'TypeScript';
const startDate = '2019-01-01';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, log }) {
        log.info('Filling in the GitHub search form...');

        // Fill in search term into the search box and search for repositories
        await page.type('[data-target="qbsearch-input.inputButton"]', searchQuery);
        await page.keyboard.press('Enter');

        // Wait for results page to load
        await page.waitForSelector('.codesearch-results');

        // Click the 'Repositories' tab to search in repos
        await page.click('[data-ga-click="Search results, search click, Tabs - Repositories clicked"]');

        // Fill in Owner filter to only show results from the specified owner
        await page.type('#search_from', repoOwner);

        // Apply Search filters
        await page.click('.select-menu-button.text-center[data-toggle-for="search-filters"]');

        // Fill in language filter
        await page.click('[data-target="search-filter.showFilterItem"][data-filter-item-type="language"]');
        await page.type('#search-select-language', language);
        await page.keyboard.press('Enter');

        // Fill in date range filter
        await page.click('[data-target="search-filter.showFilterItem"][data-filter-item-type="created"]');
        await page.type('#search-select-created', startDate);
        await page.keyboard.press('Enter');
        await page.click('.select-menu-list button[type="submit"]');

        // Get results
        const results = await page.$$eval('.repo-list-item', (elements) => {
            return elements.map((el) => ({
                title: el.querySelector('a').textContent.trim(),
                description: el.querySelector('p')?.textContent.trim(),
                stars: el.querySelector('.muted-link')?.textContent.trim(),
                updated: el.querySelector('relative-time')?.getAttribute('datetime'),
            }));
        });

        // Store the results
        await Dataset.pushData(results);
    },
});

await crawler.run(['https://github.com/search']);


----------------------------------------

TITLE: Implementing Parallel Scraper with Child Processes in JavaScript
DESCRIPTION: Creates a parallel scraper that forks itself into multiple worker processes. Each worker process uses the shared request queue to scrape product details concurrently.

LANGUAGE: javascript
CODE:
import { fork } from 'child_process';
import { CheerioCrawler, Dataset, log, Configuration } from 'crawlee';
import { router } from './routes.mjs';
import { getOrInitQueue } from './requestQueue.mjs';

if (process.env.IS_WORKER_THREAD) {
    Configuration.set('purgeOnStart', false);
    const requestQueue = await getOrInitQueue(false);

    const config = new Configuration({
        storageClientOptions: {
            localDataDirectory: `./storage/worker-${process.env.WORKER_INDEX}`,
        },
    });

    const crawler = new CheerioCrawler({
        maxConcurrency: 5,
        requestQueue,
        requestHandler: router,
    }, config);

    await crawler.run();
    process.exit(0);
} else {
    await getOrInitQueue(true);
    const dataset = await Dataset.open();
    const workerCount = 2;
    const workers = [];

    for (let i = 0; i < workerCount; i++) {
        const worker = fork(new URL(import.meta.url).pathname, {
            env: { ...process.env, IS_WORKER_THREAD: '1', WORKER_INDEX: i.toString() },
        });

        workers.push(new Promise((resolve) => {
            worker.on('message', (message) => {
                dataset.pushData(message);
            });

            worker.on('exit', (code) => {
                log.info(`Worker ${i} exited with code ${code}`);
                resolve();
            });
        }));
    }

    await Promise.all(workers);
    log.info('All workers finished');
}


----------------------------------------

TITLE: Basic JSDOM Crawler Implementation
DESCRIPTION: Demonstrates basic usage of JSDOMCrawler including initialization and crawling a website. The example shows how to create a crawler instance, define a request handler that extracts page title, and start crawling from a specific URL.

LANGUAGE: javascript
CODE:
const crawler = new JSDOMCrawler({
    async requestHandler({ request, window }) {
        await Dataset.pushData({
            url: request.url,
            title: window.document.title,
        });
    },
});

await crawler.run([
    'http://crawlee.dev',
]);

----------------------------------------

TITLE: Implementing Basic Web Crawler with Crawlee
DESCRIPTION: Demonstrates how to use BasicCrawler to download web pages using HTTP requests and store their raw HTML and URLs in a dataset. The crawler uses the sendRequest utility function which internally leverages the got-scraping module for making HTTP requests.

LANGUAGE: typescript
CODE:
{BasicCrawlerSource}

----------------------------------------

TITLE: Implementing Basic Web Crawling with CheerioCrawler
DESCRIPTION: Complete example demonstrating how to initialize and run a CheerioCrawler instance, including data extraction using Cheerio selectors and saving results to a dataset

LANGUAGE: typescript
CODE:
const crawler = new CheerioCrawler({
    requestList,
    async requestHandler({ request, response, body, contentType, $ }) {
        const data = [];

        // Do some data extraction from the page with Cheerio.
        $('.some-collection').each((index, el) => {
            data.push({ title: $(el).find('.some-title').text() });
        });

        // Save the data to dataset.
        await Dataset.pushData({
            url: request.url,
            html: body,
            data,
        })
    },
});

await crawler.run([
    'http://www.example.com/page-1',
    'http://www.example.com/page-2',
]);

----------------------------------------

TITLE: Implementing Basic Web Crawler with Crawlee
DESCRIPTION: A basic implementation of a web crawler using Crawlee's BasicCrawler class. The crawler sends HTTP requests to download web pages and stores their raw HTML and URLs in a default dataset. Uses got-scraping internally for making HTTP requests.

LANGUAGE: typescript
CODE:
import { BasicCrawler, Dataset } from 'crawlee';

// Create an instance of the BasicCrawler class - a crawler that automatically
// downloads and processes web pages.
const crawler = new BasicCrawler({
    // Let's restrict our crawls to a single day
    maxRequestsPerCrawl: 10,
    // This function will be called for each URL to crawl.
    // It accepts a single parameter, which is an object with options as you see them
    async requestHandler({ request, sendRequest }) {
        // Using the crawler.sendRequest utility function, we can make HTTP requests
        // and handle the response. It uses `got-scraping` internally, a library
        // designed to prevent detection of your crawler.
        const response = await sendRequest();

        // Save the HTML content of the page to the dataset. In local configuration,
        // the data will be stored as JSON files in ./storage/datasets/default
        await Dataset.pushData({
            url: request.url,
            html: response.body,
        });
    },
});

// Add first URL to the queue and start the crawl.
await crawler.run([
    'http://example.com/page-1',
    'http://example.com/page-2',
    'http://example.com/page-3',
]);

----------------------------------------

TITLE: Implementing Router for Crawlee in JavaScript
DESCRIPTION: Defines a router for handling different types of pages in a web scraping scenario. It includes handlers for detail pages, category pages, and a default handler for the start page. The router uses Crawlee's createPlaywrightRouter and demonstrates data extraction and link enqueuing.

LANGUAGE: javascript
CODE:
import { createPlaywrightRouter, Dataset } from 'crawlee';

export const router = createPlaywrightRouter();

router.addHandler('DETAIL', async ({ request, page, log }) => {
    log.debug(`Extracting data: ${request.url}`);
    const urlPart = request.url.split('/').slice(-1);
    const manufacturer = urlPart[0].split('-')[0];

    const title = await page.locator('.product-meta h1').textContent();
    const sku = await page
        .locator('span.product-meta__sku-number')
        .textContent();

    const priceElement = page
        .locator('span.price')
        .filter({
            hasText: '$',
        })
        .first();

    const currentPriceString = await priceElement.textContent();
    const rawPrice = currentPriceString.split('$')[1];
    const price = Number(rawPrice.replaceAll(',', ''));

    const inStockElement = page
        .locator('span.product-form__inventory')
        .filter({
            hasText: 'In stock',
        })
        .first();

    const inStock = (await inStockElement.count()) > 0;

    const results = {
        url: request.url,
        manufacturer,
        title,
        sku,
        currentPrice: price,
        availableInStock: inStock,
    };

    log.debug(`Saving data: ${request.url}`);
    await Dataset.pushData(results);
});

router.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {
    log.debug(`Enqueueing pagination for: ${request.url}`);

    await page.waitForSelector('.product-item > a');
    await enqueueLinks({
        selector: '.product-item > a',
        label: 'DETAIL',
    });

    const nextButton = await page.$('a.pagination__next');
    if (nextButton) {
        await enqueueLinks({
            selector: 'a.pagination__next',
            label: 'CATEGORY',
        });
    }
});

router.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {
    log.debug(`Enqueueing categories from page: ${request.url}`);

    await page.waitForSelector('.collection-block-item');
    await enqueueLinks({
        selector: '.collection-block-item',
        label: 'CATEGORY',
    });
});

----------------------------------------

TITLE: Initializing and Running PlaywrightCrawler in JavaScript
DESCRIPTION: This snippet demonstrates how to create and use a PlaywrightCrawler instance. It defines request handlers for successful and failed requests, and then runs the crawler with specific URLs.

LANGUAGE: javascript
CODE:
const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        // This function is called to extract data from a single web page
        // 'page' is an instance of Playwright.Page with page.goto(request.url) already called
        // 'request' is an instance of Request class with information about the page to load
        await Dataset.pushData({
            title: await page.title(),
            url: request.url,
            succeeded: true,
        })
    },
    async failedRequestHandler({ request }) {
        // This function is called when the crawling of a request failed too many times
        await Dataset.pushData({
            url: request.url,
            succeeded: false,
            errors: request.errorMessages,
        })
    },
});

await crawler.run([
    'http://www.example.com/page-1',
    'http://www.example.com/page-2',
]);

----------------------------------------

TITLE: Interacting with React Calculator using JSDOMCrawler
DESCRIPTION: Demonstrates how to use JSDOMCrawler to interact with a React calculator application. The script navigates to the calculator, performs button clicks (1 + 1 =), and extracts the calculation result.

LANGUAGE: typescript
CODE:
{JSDOMCrawlerRunScriptSource}

----------------------------------------

TITLE: HTTP Crawler Configuration in JavaScript
DESCRIPTION: Example showing HttpCrawler implementation that reads URLs from an external file, processes them using HTTP requests, and saves the resulting HTML content. The code demonstrates integration with Crawlee's HTTP crawler functionality.

LANGUAGE: javascript
CODE:
import RunnableCodeBlock from '@site/src/components/RunnableCodeBlock';
import ApiLink from '@site/src/components/ApiLink';
import HttpCrawlerSource from '!!raw-loader!roa-loader!./http_crawler.ts';

----------------------------------------

TITLE: Configuring Maximum Requests Per Minute in Crawlee
DESCRIPTION: Demonstrates how to set a limit on the number of requests made per minute using the maxRequestsPerMinute option in CheerioCrawler. This helps control the request rate to avoid overwhelming target websites.

LANGUAGE: javascript
CODE:
const crawler = new CheerioCrawler({
    maxRequestsPerMinute: 120,
    // ...
});

----------------------------------------

TITLE: Implementing BasicCrawler for Web Scraping in TypeScript
DESCRIPTION: This code snippet demonstrates how to use the BasicCrawler class from Crawlee to crawl web pages, send HTTP requests, and store the results in a dataset. It includes error handling and logging.

LANGUAGE: typescript
CODE:
import { BasicCrawler, Dataset } from 'crawlee';

// Create an instance of the BasicCrawler class - a configurable
// crawler that runs HTTP requests.
const crawler = new BasicCrawler({
    // Let's limit our crawls to only 10 requests (do not use this in production).
    maxRequestsPerCrawl: 10,

    // Basic crawler automatically respects robots.txt rules
    // and download delay.
    requestHandler: async ({ request, sendRequest, log }) => {
        // 'sendRequest()' is an example of a helper function that
        // comes with the crawler. It's abstracts away the details
        // of sending a request, such as retries and error handling.
        const { body } = await sendRequest(request);
        const title = body.match(/<title>([^<]*)<\/title>/i)?.[1];

        log.info(`Title of ${request.url}: ${title}`);

        // Store the results to the default dataset. In local configuration,
        // the data will be stored as JSON files in ./storage/datasets/default
        await Dataset.pushData({
            url: request.url,
            title,
            html: body,
        });
    },

    // This function is called if the function passed to `requestHandler`
    // throws an exception.
    failedRequestHandler: async ({ request, log }) => {
        log.error(`Request ${request.url} failed too many times.`);

        await Dataset.pushData({
            '#debug': Crawlee.utilities.createRequestDebugInfo(request),
        });
    },
});

// Add first URL to the queue and start the crawl.
await crawler.run(['https://crawlee.dev']);


----------------------------------------

TITLE: Implementing CheerioCrawler for Web Scraping in JavaScript
DESCRIPTION: Example demonstrates using CheerioCrawler to crawl URLs from an external file, make HTTP requests, and parse HTML using Cheerio library to extract page titles and h1 tags. The crawler utilizes Cheerio's jQuery-like syntax for HTML parsing and data extraction.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

// Create an instance of the CheerioCrawler class
const crawler = new CheerioCrawler({
    // Function called for each URL
    async requestHandler({ $, request }) {
        // Extract data from the page using Cheerio
        const title = $('title').text();
        const h1texts = $('h1')
            .map((_, el) => $(el).text())
            .get();

        // Save the results to the default dataset
        await Dataset.pushData({
            url: request.url,
            title,
            h1texts,
        });
    },
});

// Add URLs to the crawler's queue
await crawler.addRequests([
    'https://crawlee.dev',
]);

// Run the crawler
await crawler.run();

----------------------------------------

TITLE: Implementing Parallel Scraper with Child Processes in JavaScript
DESCRIPTION: Creates a parallel scraper that forks itself into multiple worker processes. Each worker process uses the shared request queue to scrape product details concurrently.

LANGUAGE: javascript
CODE:
import { fork } from 'child_process';
import { CheerioCrawler, Dataset, log, Configuration } from 'crawlee';
import { router } from './routes.mjs';
import { getOrInitQueue } from './requestQueue.mjs';

if (process.env.IS_WORKER_THREAD) {
    Configuration.set('purgeOnStart', false);
    const requestQueue = await getOrInitQueue(false);

    const config = new Configuration({
        storageClientOptions: {
            localDataDirectory: `./storage/worker-${process.env.WORKER_INDEX}`,
        },
    });

    const crawler = new CheerioCrawler({
        maxConcurrency: 5,
        requestQueue,
        requestHandler: router,
    }, config);

    await crawler.run();
    process.exit(0);
} else {
    await getOrInitQueue(true);
    const dataset = await Dataset.open();
    const workerCount = 2;
    const workers = [];

    for (let i = 0; i < workerCount; i++) {
        const worker = fork(new URL(import.meta.url).pathname, {
            env: { ...process.env, IS_WORKER_THREAD: '1', WORKER_INDEX: i.toString() },
        });

        workers.push(new Promise((resolve) => {
            worker.on('message', (message) => {
                dataset.pushData(message);
            });

            worker.on('exit', (code) => {
                log.info(`Worker ${i} exited with code ${code}`);
                resolve();
            });
        }));
    }

    await Promise.all(workers);
    log.info('All workers finished');
}


----------------------------------------

TITLE: Implementing Web Crawling with CheerioCrawler
DESCRIPTION: Shows how to use CheerioCrawler to crawl URLs and extract data using Cheerio library. The code demonstrates parsing HTML content to extract page titles and h1 tags from web pages. It utilizes the Cheerio library for HTML parsing and manipulation.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, downloadListOfUrls } from 'crawlee';

// Create an instance of the crawler
const crawler = new CheerioCrawler({
    // Function called for each URL
    async requestHandler({ $, request, enqueueLinks, log }) {
        const title = $('title').text();
        log.info(`Title of ${request.url}: ${title}`);

        // Extract all h1 texts
        const h1texts = [];
        $('h1').each((_, el) => {
            h1texts.push($(el).text());
        });
        log.info('h1 texts:', h1texts);
    },
});

// Add list of URLs to crawl
await crawler.addRequests([
    'https://crawlee.dev',
]);

// Run the crawler
await crawler.run();

----------------------------------------

TITLE: Implementing Recursive Web Scraping with PuppeteerCrawler
DESCRIPTION: Demonstrates how to set up a PuppeteerCrawler to recursively scrape Hacker News using headless Chrome. The crawler starts with a single URL, discovers and enqueues links to subsequent pages, and stores results in a default dataset. Includes RequestQueue integration for handling discovered URLs.

LANGUAGE: javascript
CODE:
// For the latest version of this example, see
// https://apify.com/apify/example-puppeteer-crawler

import { PuppeteerCrawler, RequestQueue } from 'crawlee';

// Create a RequestQueue instance
const requestQueue = await RequestQueue.open();

// Add the initial request
await requestQueue.addRequest({ url: 'https://news.ycombinator.com' });

// Create a PuppeteerCrawler instance
const crawler = new PuppeteerCrawler({
    requestQueue,
    maxRequestsPerCrawl: 50,
    async requestHandler({ request, page, enqueueLinks, pushData }) {
        const title = await page.title();
        const links = await page.$$eval('.titleline > a', (nodes) => nodes.map((n) => n.href));
        const titles = await page.$$eval('.titleline > a', (nodes) => nodes.map((n) => n.innerText));

        // Save results
        await pushData({
            title,
            url: request.url,
            titleCount: titles.length,
            links,
            titles,
        });

        // Add new requests to the queue
        await enqueueLinks({
            selector: '.morelink',
        });
    },
});

// Start the crawler
await crawler.run();

----------------------------------------

TITLE: URL List Crawling with JSDOM Parser
DESCRIPTION: Shows how to use JSDOMCrawler to process multiple URLs from an external file, parse HTML content using JSDOM, and extract specific elements like page titles and h1 tags.

LANGUAGE: typescript
CODE:
{JSDOMCrawlerSource}

----------------------------------------

TITLE: Complete Product Data Extraction Script
DESCRIPTION: Complete script combining all extraction methods to scrape product details including URL, manufacturer, title, SKU, price and stock status.

LANGUAGE: javascript
CODE:
const urlPart = request.url.split('/').slice(-1);
const manufacturer = urlPart.split('-')[0];

const title = await page.locator('.product-meta h1').textContent();
const sku = await page.locator('span.product-meta__sku-number').textContent();

const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Implementing Recursive Web Crawler with PuppeteerCrawler
DESCRIPTION: Demonstrates how to create a web crawler that recursively scrapes Hacker News using PuppeteerCrawler and RequestQueue. The crawler starts from a single URL, finds and enqueues links to next pages, and stores results in a default dataset. Requires the apify/actor-node-puppeteer-chrome image when running on Apify Platform.

LANGUAGE: typescript
CODE:
import { Dataset, PuppeteerCrawler, RequestQueue, log } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://news.ycombinator.com'];

const crawler = new PuppeteerCrawler({
    // Use the requestQueue to handle the recursive crawling
    requestQueue: await RequestQueue.open(),

    // Use default browser.
    browserPoolOptions: {},

    // Increase the timeout for processing of each page.
    requestHandlerTimeoutSecs: 30,

    // Limit the number of pages that can be processed in parallel.
    maxConcurrency: 10,

    // Define the page handler.
    async requestHandler({ request, page, enqueueLinks, log }) {
        // Extract data from the page using Puppeteer.
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        // Find all links to next pages,
        // select only those that match our regular expression.
        await enqueueLinks({
            globs: ['https://news.ycombinator.com/*'],
            label: 'LIST',
        });

        // Save some data about the page to dataset.
        await Dataset.pushData({
            url: request.url,
            title,
        });
    },

    // This function is called if the page processing failed more than maxRequestRetries+1 times.
    failedRequestHandler({ request, log }) {
        log.warning(`Request ${request.url} failed too many times.`);
    },
});

// Add first URL to the queue and start the crawl.
await crawler.run(startUrls);

----------------------------------------

TITLE: Selective Link Crawling using CheerioCrawler and Globs
DESCRIPTION: Shows how to use CheerioCrawler with enqueueLinks() method and globs property to selectively crawl links matching specific patterns. The example connects to a RequestQueue and processes matched URLs.

LANGUAGE: typescript
CODE:
import { CheerioCrawler, enqueueLinks } from 'crawlee';

// Create an instance of the crawler
const crawler = new CheerioCrawler({
    // Define the crawler's behavior for each page it visits
    async requestHandler({ $, request, enqueueLinks }) {
        // Print the URL of the page the crawler is currently on
        console.log(`Processing: ${request.url}`);

        // Extract the page title
        const title = $('title').text();
        console.log(`Title: ${title}`);

        // Add new links from current page to the crawler's RequestQueue
        // Only links matching the specified globs will be included
        await enqueueLinks({
            // Only add links that match these patterns
            globs: ['http?(s)://example.com/*/*']
        });
    }
});

// Start the crawler with the initial URL
await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Extracting All Links with Cheerio
DESCRIPTION: Example demonstrating how to find all anchor elements with href attributes and extract their URLs into an array using Cheerio's chaining methods.

LANGUAGE: javascript
CODE:
$('a[href]')
    .map((i, el) => $(el).attr('href'))
    .get();

----------------------------------------

TITLE: Crawling All Links with Cheerio Crawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use Cheerio Crawler to crawl all links on a website. It uses the enqueueLinks() method to add new links to the RequestQueue as the crawler navigates from page to page.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, enqueueLinks } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limit to 10 requests
});

// Run the crawler with initial URL
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Standalone SessionPool Usage
DESCRIPTION: Demonstrates how to use SessionPool independently without a crawler for manual session management.

LANGUAGE: javascript
CODE:
{StandaloneSource}

----------------------------------------

TITLE: Checking Stock Availability
DESCRIPTION: Demonstrates how to check if a product is in stock by looking for specific element and text.

LANGUAGE: javascript
CODE:
const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Initializing ProxyConfiguration in Crawlee
DESCRIPTION: This snippet demonstrates how to create a ProxyConfiguration instance with custom proxy URLs and obtain a new proxy URL.

LANGUAGE: javascript
CODE:
import { ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ]
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Setting up Docker Environment for Crawlee Actor
DESCRIPTION: Configures a Docker container for running Crawlee actors using Node.js 20. The build process is optimized using layer caching for package installation and includes only production dependencies to maintain a smaller image size.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:20

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm start --silent

----------------------------------------

TITLE: Multi-stage Docker Build for Crawlee Actor
DESCRIPTION: A complete Dockerfile that creates a production-ready Crawlee actor environment. Uses a multi-stage build process to first compile the application and then create a minimal production image with only the necessary dependencies. Includes Playwright and Chrome for browser automation.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:20 AS builder

COPY --chown=myuser package*.json ./

RUN npm install --include=dev --audit=false

COPY --chown=myuser . ./

RUN npm run build

FROM apify/actor-node-playwright-chrome:20

COPY --from=builder --chown=myuser /home/myuser/dist ./dist

COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent

----------------------------------------

TITLE: Configuring Docker Environment for Crawlee Actor
DESCRIPTION: Dockerfile that sets up a Node.js environment for running Crawlee actors. It uses a base image from Apify, installs production dependencies, and configures the runtime environment. The build process is optimized using Docker layer caching by separating dependency installation from source code copying.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm start --silent

----------------------------------------

TITLE: Crawling with PlaywrightCrawler in JavaScript
DESCRIPTION: Example code for performing a recursive crawl of the Crawlee website using PlaywrightCrawler. It shows how to set up the crawler with Playwright, define the request handler, and start the crawling process.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        await Dataset.pushData({
            url: request.url,
            title,
        });
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Initializing Shared Request Queue with Locking Support
DESCRIPTION: Creates a reusable request queue with locking support for parallel scraping. Includes initialization and reset functionality.

LANGUAGE: javascript
CODE:
import { Configuration } from 'crawlee';
import { enableRequestLocking } from '@crawlee/memory-storage/experimental';

export async function getOrInitQueue(purge = false) {
    enableRequestLocking();
    const config = new Configuration();
    const requestQueue = await config.getRequestQueue();
    if (purge) {
        await requestQueue.drop();
    }
    return requestQueue;
}

----------------------------------------

TITLE: Complete Crawlee Scraping Example with Data Saving
DESCRIPTION: This is the final code example showing a complete Crawlee scraping script that extracts data from a website and saves it using Dataset.pushData().

LANGUAGE: javascript
CODE:
{Example}

----------------------------------------

TITLE: Installing Crawlee CLI and Project Setup
DESCRIPTION: Commands to install Crawlee using the CLI tool and create a new crawler project

LANGUAGE: bash
CODE:
npx crawlee create my-crawler

----------------------------------------

TITLE: Extracting Product Title with Playwright in JavaScript
DESCRIPTION: This code snippet shows how to use Playwright to extract the product title from an HTML element with a specific class.

LANGUAGE: javascript
CODE:
const title = await page.locator('.product-meta h1').textContent();

----------------------------------------

TITLE: Initializing Basic Proxy Configuration in Crawlee
DESCRIPTION: Shows how to create a basic proxy configuration using ProxyConfiguration class with custom proxy URLs. The code demonstrates initializing proxy settings and obtaining a new proxy URL.

LANGUAGE: javascript
CODE:
import { ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ]
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Complete Product Data Extraction with Playwright
DESCRIPTION: This code combines all the previous snippets to extract the full set of product data including URL, manufacturer, title, SKU, price, and stock availability.

LANGUAGE: javascript
CODE:
const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']
const manufacturer = urlPart.split('-')[0]; // 'sennheiser'

const title = await page.locator('.product-meta h1').textContent();
const sku = await page.locator('span.product-meta__sku-number').textContent();

const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Crawling All Links with Puppeteer Crawler in Crawlee
DESCRIPTION: This snippet shows how to use Puppeteer Crawler in Crawlee to crawl all links on a website. It configures the crawler, defines a request handler that extracts page title and URL, and uses enqueueLinks() to add new links to the queue.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, Dataset } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        await Dataset.pushData({
            url: request.url,
            title,
        });

        await enqueueLinks();
    },
    maxRequestsPerCrawl: 20, // Limit to 20 requests
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Crawling All Links with Playwright Crawler in Crawlee
DESCRIPTION: This snippet illustrates how to use Playwright Crawler in Crawlee to crawl all links on a website. It uses enqueueLinks() to add new links to the RequestQueue and limits the crawl to 100 pages. It requires the apify/actor-node-playwright-chrome image for deployment on the Apify Platform.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, enqueueLinks } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 100, // Limitation for only 100 requests (do not use if you want to crawl all links)
});

// Run the crawler with initial request
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Comparing DOM Selection in Browser JS vs Cheerio
DESCRIPTION: Demonstrates the difference between native browser JavaScript DOM selection and Cheerio's jQuery-like syntax for extracting title text and href attributes.

LANGUAGE: typescript
CODE:
// Return the text content of the <title> element.
document.querySelector('title').textContent; // plain JS
$('title').text(); // Cheerio

// Return an array of all 'href' links on the page.
Array.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS
$('[href]')
    .map((i, el) => $(el).attr('href'))
    .get(); // Cheerio

----------------------------------------

TITLE: Complete Product Data Extraction
DESCRIPTION: Complete code combining all scraping operations to extract product information from the page.

LANGUAGE: javascript
CODE:
const urlPart = request.url.split('/').slice(-1);
const manufacturer = urlPart.split('-')[0];

const title = await page.locator('.product-meta h1').textContent();
const sku = await page.locator('span.product-meta__sku-number').textContent();

const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Crawling All Links with Cheerio Crawler in Crawlee
DESCRIPTION: This snippet demonstrates how to use Cheerio Crawler in Crawlee to crawl all links on a website. It uses enqueueLinks() to add new links to the RequestQueue and limits the crawl to 100 pages.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, enqueueLinks } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 100, // Limitation for only 100 requests (do not use if you want to crawl all links)
});

// Run the crawler with initial request
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Scraping Hacker News with PlaywrightCrawler in JavaScript
DESCRIPTION: This code demonstrates how to use PlaywrightCrawler and RequestQueue to recursively scrape the Hacker News website. It starts with the initial URL, extracts data from each page, and follows pagination links. The results are stored in the default dataset.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

const crawler = new PlaywrightCrawler({
    // Use the requestHandler to process each of the crawled pages.
    async requestHandler({ request, page, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        // Save results as JSON to ./storage/datasets/default
        await Dataset.pushData({
            url: request.url,
            title,
        });

        // Extract links from the current page
        // and add them to the crawling queue.
        await enqueueLinks({
            globs: ['https://news.ycombinator.com/*'],
            label: 'detail',
        });
    },
    // Uncomment this option to see the browser window.
    // headless: false,
});

// Add first URL to the queue and start the crawl.
await crawler.run(['https://news.ycombinator.com/']);

----------------------------------------

TITLE: Basic Request List Implementation in Crawlee
DESCRIPTION: Demonstrates how to initialize and use a RequestList with PuppeteerCrawler to process a predefined list of URLs. Shows basic setup of sources array and crawler configuration.

LANGUAGE: javascript
CODE:
import { RequestList, PuppeteerCrawler } from 'crawlee';

// Prepare the sources array with URLs to visit
const sources = [
    { url: 'http://www.example.com/page-1' },
    { url: 'http://www.example.com/page-2' },
    { url: 'http://www.example.com/page-3' },
];

// Open the request list.
// List name is used to persist the sources and the list state in the key-value store
const requestList = await RequestList.open('my-list', sources);

// The crawler will automatically process requests from the list
// It's used the same way for Cheerio /Playwright crawlers.
const crawler = new PuppeteerCrawler({
    requestList,
    async requestHandler({ page, request }) {
        // Process the page (extract data, take page screenshot, etc).
        // No more requests could be added to the request list here
    },
});

----------------------------------------

TITLE: Downloading Files with Crawlee FileDownload Crawler
DESCRIPTION: This code snippet shows how to use the FileDownload crawler class from Crawlee to download files from specified URLs and save them to the default key-value store. It demonstrates downloading different file types including PNG, PDF, and ZIP.

LANGUAGE: javascript
CODE:
import { FileDownload } from 'crawlee';

await FileDownload.download([
    { url: 'https://www.iana.org/_img/2013.1/iana-logo-header.svg' },
    { url: 'https://www.iana.org/assignments/access-types/access-types.txt' },
    { url: 'https://www.iana.org/assignments/http-status-codes/http-status-codes.pdf' },
    { url: 'https://www.iana.org/assignments/http-status-codes/http-status-codes.xml' },
    { url: 'https://www.iana.org/assignments/http-status-codes/http-status-codes.xhtml' },
    { url: 'https://www.iana.org/assignments/http-status-codes/http-status-codes-1.csv' },
    { url: 'https://www.iana.org/assignments/language-subtag-registry/language-subtag-registry' },
    { url: 'https://www.iana.org/assignments/dns-parameters/dns-parameters.xhtml' },
    { url: 'https://www.iana.org/assignments/ipv4-address-space/ipv4-address-space.xml' },
    { url: 'https://www.iana.org/protocols/ApprovedProtocols.zip' }
]);

----------------------------------------

TITLE: Complete Product Scraping Logic with Playwright in JavaScript
DESCRIPTION: This code snippet combines all the previous snippets to create a complete scraping logic for extracting product information using Playwright.

LANGUAGE: javascript
CODE:
const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']
const manufacturer = urlPart.split('-')[0]; // 'sennheiser'

const title = await page.locator('.product-meta h1').textContent();
const sku = await page.locator('span.product-meta__sku-number').textContent();

const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Crawling URLs with CheerioCrawler in JavaScript
DESCRIPTION: This code demonstrates how to use CheerioCrawler to crawl a list of URLs from an external file, parse the HTML using Cheerio, and extract the page title and h1 tags. It utilizes the Crawlee library and handles rate limiting and concurrency.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, downloadListOfUrls } from 'crawlee';

// Create an instance of the CheerioCrawler class - a crawler
// that uses Cheerio under the hood to parse HTML.
const crawler = new CheerioCrawler({
    // Let's limit the number of requests we'll send to
    // each hostname per minute. That way we don't overload
    // the websites and they will be more likely to keep serving us.
    maxRequestsPerMinute: 60,

    // This function will be called for each URL to crawl.
    // It accepts a single parameter, which is an object with
    // the following fields:
    // - request: an instance of the Request class with information such as URL and HTTP method
    // - $: the cheerio object containing parsed HTML
    // - body: the raw HTML string
    // - json: the parsed JSON object if the response is JSON
    // - contentType: the Content-Type header of the web server's response
    async requestHandler({ $, request }) {
        // Extract the page title
        const title = $('title').text();

        // Extract all text content from h1 elements
        const h1Texts = $('h1').map((_, el) => $(el).text()).get();

        // Print the extracted data
        console.log(`URL: ${request.url}\nTitle: ${title}`);
        console.log('h1 texts:', h1Texts);
    },
});

// This array will contain the URLs to crawl
let urls = [];

// Download the list of URLs and add them to the crawler's queue
await downloadListOfUrls({ url: 'https://example.com/list-of-urls.txt' })
    .then((downloadedUrls) => {
        console.log(`Downloaded ${downloadedUrls.length} URLs`);
        urls = downloadedUrls;
    });

// Run the crawler with the list of URLs
await crawler.run(urls);

----------------------------------------

TITLE: Working with Datasets in Crawlee
DESCRIPTION: Examples of dataset operations including writing single and multiple rows to both default and named datasets. Shows how to store structured data in an append-only format.

LANGUAGE: javascript
CODE:
import { Dataset } from 'crawlee';

// Write a single row to the default dataset
await Dataset.pushData({ col1: 123, col2: 'val2' });

// Open a named dataset
const dataset = await Dataset.open('some-name');

// Write a single row
await dataset.pushData({ foo: 'bar' });

// Write multiple rows
await dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);

----------------------------------------

TITLE: Creating a CheerioCrawler with RequestQueue in Crawlee
DESCRIPTION: Shows how to set up a CheerioCrawler with a RequestQueue and a requestHandler. The crawler visits the specified URL, extracts the page title using Cheerio, and logs it.

LANGUAGE: typescript
CODE:
import { RequestQueue, CheerioCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

const crawler = new CheerioCrawler({
    requestQueue,
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run();

----------------------------------------

TITLE: Implementing Custom HTTP Client with Fetch in Crawlee
DESCRIPTION: Example implementation of a BaseHttpClient interface using the standard fetch API. Shows required method implementations for handling HTTP requests and response processing.

LANGUAGE: typescript
CODE:
import { type BaseHttpClient } from 'crawlee';

export class FetchHttpClient implements BaseHttpClient {
    async get() {
        // implementation
    }

    async head() {
        // implementation
    }

    async post() {
        // implementation
    }

    async put() {
        // implementation
    }

    async patch() {
        // implementation
    }

    async delete() {
        // implementation
    }
}


----------------------------------------

TITLE: Crawling URLs with CheerioCrawler and Extracting Data using Cheerio in JavaScript
DESCRIPTION: This code demonstrates how to use CheerioCrawler to crawl a list of URLs from an external file, load each URL using HTTP requests, parse the HTML using Cheerio, and extract the page title and all h1 tags. It requires the Crawlee library and Cheerio.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';
import { readFile } from 'fs/promises';

// Load the list of URLs from a file
const urls = await readFile('urls.txt', 'utf8')
    .then((text) => text.split('\n').filter((line) => line.trim() !== ''));

// Create an instance of the CheerioCrawler class - a crawler
// that uses Cheerio to parse HTML.
const crawler = new CheerioCrawler({
    // Let's limit the number of requests
    maxRequestsPerCrawl: 20,
    // Function called for each URL
    async requestHandler({ request, $ }) {
        // Extract data from the page using Cheerio
        const title = $('title').text();
        const h1texts = $('h1')
            .map((_, el) => $(el).text())
            .get();

        // Save the results to the default dataset
        await Dataset.pushData({
            url: request.url,
            title,
            h1texts,
        });
    },
});

// Run the crawler
await crawler.run(urls);

----------------------------------------

TITLE: Implementing Crawlee Router with Route Handlers
DESCRIPTION: Defines route handlers for different page types (detail pages, category pages, and default handling) using Crawlee's router system. Includes data extraction logic and pagination handling.

LANGUAGE: javascript
CODE:
import { createPlaywrightRouter, Dataset } from 'crawlee';

export const router = createPlaywrightRouter();

router.addHandler('DETAIL', async ({ request, page, log }) => {
    log.debug(`Extracting data: ${request.url}`);
    const urlPart = request.url.split('/').slice(-1);
    const manufacturer = urlPart[0].split('-')[0];

    const title = await page.locator('.product-meta h1').textContent();
    const sku = await page
        .locator('span.product-meta__sku-number')
        .textContent();

    const priceElement = page
        .locator('span.price')
        .filter({
            hasText: '$',
        })
        .first();

    const currentPriceString = await priceElement.textContent();
    const rawPrice = currentPriceString.split('$')[1];
    const price = Number(rawPrice.replaceAll(',', ''));

    const inStockElement = page
        .locator('span.product-form__inventory')
        .filter({
            hasText: 'In stock',
        })
        .first();

    const inStock = (await inStockElement.count()) > 0;

    const results = {
        url: request.url,
        manufacturer,
        title,
        sku,
        currentPrice: price,
        availableInStock: inStock,
    };

    log.debug(`Saving data: ${request.url}`);
    await Dataset.pushData(results);
});

router.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {
    log.debug(`Enqueueing pagination for: ${request.url}`);
    await page.waitForSelector('.product-item > a');
    await enqueueLinks({
        selector: '.product-item > a',
        label: 'DETAIL',
    });

    const nextButton = await page.$('a.pagination__next');
    if (nextButton) {
        await enqueueLinks({
            selector: 'a.pagination__next',
            label: 'CATEGORY',
        });
    }
});

router.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {
    log.debug(`Enqueueing categories from page: ${request.url}`);
    await page.waitForSelector('.collection-block-item');
    await enqueueLinks({
        selector: '.collection-block-item',
        label: 'CATEGORY',
    });
});

----------------------------------------

TITLE: Multi-stage Dockerfile Build for Crawlee Actor
DESCRIPTION: Configures a Docker environment for running Crawlee actors using Node.js and Playwright with Chrome. Uses a multi-stage build process to optimize image size and includes development dependencies only in the build stage. The final image includes only production dependencies and built artifacts.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16 AS builder

COPY --chown=myuser package*.json ./

RUN npm install --include=dev --audit=false

COPY --chown=myuser . ./

RUN npm run build

FROM apify/actor-node-playwright-chrome:16

COPY --from=builder --chown=myuser /home/myuser/dist ./dist

COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent

----------------------------------------

TITLE: Initializing RequestQueue in Crawlee
DESCRIPTION: Creates a RequestQueue instance and adds a URL to crawl. This demonstrates the basic setup for queuing URLs to be processed by a crawler.

LANGUAGE: typescript
CODE:
import { RequestQueue } from 'crawlee';

// First you create the request queue instance.
const requestQueue = await RequestQueue.open();
// And then you add one or more requests to it.
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

----------------------------------------

TITLE: Downloading Files with Node.js Streams using Crawlee's FileDownload Crawler
DESCRIPTION: This code snippet demonstrates how to use Crawlee's FileDownload crawler to download large files efficiently using Node.js streams. It sets up the crawler, defines the download logic, and handles the file storage in the key-value store.

LANGUAGE: typescript
CODE:
import { FileDownload } from 'crawlee';

const crawler = new FileDownload({
    // Function for storing downloaded files
    // The output of this function will be saved as the file content
    async processFileDownload({ response, key }) {
        // Log the progress of the download
        const contentLength = response.headers.get('content-length');
        let downloadedLength = 0;
        response.body.on('data', (chunk) => {
            downloadedLength += chunk.length;
            if (contentLength) {
                const progress = Math.round((downloadedLength / Number(contentLength)) * 100);
                console.log(`Downloaded ${progress}% of ${key}`);
            }
        });

        // Return the response body as a Buffer
        return response.body;
    },
});

await crawler.addRequests([
    { url: 'https://via.placeholder.com/3000', label: 'FILE', userData: { key: '3000x3000.png' } },
]);

await crawler.run();

console.log('Crawler finished.');

----------------------------------------

TITLE: Crawling with PuppeteerCrawler in JavaScript
DESCRIPTION: Example code for performing a recursive crawl of the Crawlee website using PuppeteerCrawler. It demonstrates how to set up the crawler with Puppeteer, define the request handler, and start the crawling process.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, Dataset } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        await Dataset.pushData({
            url: request.url,
            title,
        });
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Crawling All Links with Playwright Crawler in JavaScript
DESCRIPTION: This code snippet illustrates how to use Playwright Crawler to crawl all links on a website. It employs the enqueueLinks() method to add new links to the RequestQueue as the crawler moves through the site.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, enqueueLinks } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limit to 10 requests
});

// Run the crawler with initial URL
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Implementing PlaywrightCrawler with RequestQueue
DESCRIPTION: Example code showing how to create a web scraper using PlaywrightCrawler and RequestQueue to crawl Hacker News. The crawler recursively follows links and stores results in a local JSON dataset.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, RequestQueue } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://news.ycombinator.com' });

const crawler = new PlaywrightCrawler({
    requestQueue,
    maxRequestsPerCrawl: 50,
    async requestHandler({ request, page, enqueueLinks }) {
        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);

        // Extract data from the page using Playwright API
        const data = {
            title,
            url: request.url,
            // Add more data here
        };

        // Save the data to dataset
        await Dataset.pushData(data);

        // Find all links and add them to the crawling queue
        await enqueueLinks();
    },
});

await crawler.run();

----------------------------------------

TITLE: Initializing PlaywrightCrawler for Hacker News Scraping in JavaScript
DESCRIPTION: Sets up a PlaywrightCrawler to scrape Hacker News, handling pagination and storing results. Uses RequestQueue for managing URLs and Dataset for storing scraped data.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset, RequestQueue } from 'crawlee';

// Create an instance of the PlaywrightCrawler class - a crawler
// that automatically loads the URLs in headless Chrome / Playwright.
const crawler = new PlaywrightCrawler({
    // Use the requestQueue to add more URLs to crawl.
    requestQueue: await RequestQueue.open(),

    // Here you can set options that are passed to the Playwright browser.
    launchContext: {
        launchOptions: {
            headless: true,
        },
    },

    // Stop crawling after several pages
    maxRequestsPerCrawl: 50,

    // This function will be called for each URL to crawl.
    // Here you can write the Playwright scripts you are familiar with,
    // with the exception that browsers and pages are automatically managed by the Crawlee.
    // The function accepts a single parameter, which is an object with the following fields:
    // - request: an instance of the Request class with information such as URL and HTTP method
    // - page: Playwright's Page object (see https://playwright.dev/docs/api/class-page)
    async requestHandler({ request, page, enqueueLinks, log }) {
        log.info(`Processing ${request.url}...`);

        // A function to be evaluated by Playwright within the browser context.
        const data = await page.$$eval('.athing', ($posts) => {
            const scrapedData = [];

            // We're getting the title, rank and link from each post on Hacker News.
            $posts.forEach(($post) => {
                scrapedData.push({
                    title: $post.querySelector('.title a').innerText,
                    rank: $post.querySelector('.rank').innerText,
                    href: $post.querySelector('.title a').href,
                });
            });

            return scrapedData;
        });

        // Store the results to the default dataset.
        await Dataset.pushData(data);

        // Find a link to the next page and enqueue it if it exists.
        const infos = await page.$$eval('.morelink', ($links) => {
            return $links.map((link) => {
                return {
                    text: link.innerText,
                    href: link.href,
                };
            });
        });

        for (const link of infos) {
            if (link.text === 'More') {
                await enqueueLinks({
                    urls: [link.href],
                    label: 'LIST',
                });
            }
        }
    },

    // This function is called if the page processing failed more than maxRequestRetries+1 times.
    failedRequestHandler({ request, log }) {
        log.info(`Request ${request.url} failed twice.`);
    },
});

// Run the crawler and wait for it to finish.
await crawler.run(['https://news.ycombinator.com/']);

console.log('Crawler finished.');

----------------------------------------

TITLE: Simplified CheerioCrawler Implementation
DESCRIPTION: Demonstrates a more concise way to initialize and run a CheerioCrawler using the crawler.run() method with direct URL input, eliminating the need for explicit RequestQueue management.

LANGUAGE: typescript
CODE:
// You don't need to import RequestQueue anymore
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

// Start the crawler with the provided URLs
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Configuring Crawlee using crawlee.json
DESCRIPTION: Example of basic Crawlee configuration using a JSON file to set persistStateIntervalMillis and logLevel parameters.

LANGUAGE: json
CODE:
{
  "persistStateIntervalMillis": 10000,
  "logLevel": "DEBUG"
}

----------------------------------------

TITLE: Complete E-commerce Store Crawler with Pagination
DESCRIPTION: Full implementation of a crawler that handles category pages, product detail pages, and pagination. Uses different labels to manage crawling flow and specific selectors for different types of links.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        if (request.label === 'DETAIL') {
            // We're not doing anything with the details yet.
        } else if (request.label === 'CATEGORY') {
            // We are now on a category page. We can use this to paginate through and enqueue all products,
            // as well as any subsequent pages we find

            await page.waitForSelector('.product-item > a');
            await enqueueLinks({
                selector: '.product-item > a',
                label: 'DETAIL', // <= note the different label
            });

            // Now we need to find the "Next" button and enqueue the next page of results (if it exists)
            const nextButton = await page.$('a.pagination__next');
            if (nextButton) {
                await enqueueLinks({
                    selector: 'a.pagination__next',
                    label: 'CATEGORY', // <= note the same label
                });
            }
        } else {
            // This means we're on the start page, with no label.
            // On this page, we just want to enqueue all the category pages.

            await page.waitForSelector('.collection-block-item');
            await enqueueLinks({
                selector: '.collection-block-item',
                label: 'CATEGORY',
            });
        }
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Configuring Dockerfile for Crawlee Project with Node.js and Playwright
DESCRIPTION: This Dockerfile sets up an environment for a Crawlee project. It uses a base image with Node.js and Playwright, installs NPM packages, copies project files, and configures the run command. The setup optimizes for build speed and minimal image size.

LANGUAGE: Dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16

COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD npm start --silent

----------------------------------------

TITLE: Complete AWS Lambda Handler for Cheerio Crawler with Data Return
DESCRIPTION: This is the final version of the Lambda handler function. It initializes the Cheerio Crawler, runs it, and returns the scraped data as the Lambda function's response.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);

    return {
        statusCode: 200,
        body: await crawler.getData(),
    }
};

----------------------------------------

TITLE: Implementing Parallel Scraper with Worker Processes
DESCRIPTION: Main scraper implementation that spawns multiple worker processes to handle scraping tasks in parallel using Node.js child processes.

LANGUAGE: javascript
CODE:
import { fork } from 'child_process';
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.mjs';
import { getOrInitQueue } from './requestQueue.mjs';

Configuration.set('purgeOnStart', false);
const requestQueue = await getOrInitQueue(false);

if (process.env.IS_WORKER_THREAD) {
    const config = new Configuration({
        storageClientOptions: {
            localDataDirectory: `./storage/worker-${process.env.WORKER_INDEX}`,
        },
    });
    const crawler = new PlaywrightCrawler({
        requestQueue,
        maxConcurrency: 5,
        requestHandler: router,
    }, config);
    await crawler.run();
    process.exit(0);
}

----------------------------------------

TITLE: Crawling All Links with Playwright Crawler in TypeScript
DESCRIPTION: This snippet illustrates how to use Playwright Crawler to crawl all links on a website. It configures a PlaywrightCrawler, uses the enqueueLinks() method to add new links to the queue, and retrieves the page title using Playwright's API.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, enqueueLinks } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limit to 10 requests
});

// Run the crawler
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Implementing Recursive Web Scraping with PuppeteerCrawler
DESCRIPTION: Implementation of a web crawler using PuppeteerCrawler and RequestQueue to recursively scrape Hacker News. The crawler processes URLs, extracts data, and stores results in a dataset while managing pagination.

LANGUAGE: typescript
CODE:
import { Dataset, createPuppeteerRouter, PuppeteerCrawler } from 'crawlee';

const router = createPuppeteerRouter();

router.addDefaultHandler(async ({ page, enqueueLinks }) => {
    const title = await page.title();
    console.log(`Title of ${page.url()} is '${title}'`);

    await Dataset.pushData({
        url: page.url(),
        title,
    });

    await enqueueLinks({
        globs: ['https://news.ycombinator.com/*'],
        label: 'detail',
    });
});

const crawler = new PuppeteerCrawler({
    requestHandler: router,

    // Uncomment this option to see the browser window.
    // headless: false,

    // Comment this option to scrape the full website.
    maxRequestsPerCrawl: 20,
});

await crawler.run(['https://news.ycombinator.com']);

----------------------------------------

TITLE: Sanity Check with Playwright and Cheerio
DESCRIPTION: This code snippet shows an alternative approach using PlaywrightCrawler with Cheerio for HTML parsing. It crawls the warehouse store website, extracts category information, and demonstrates how to use Cheerio for easier HTML manipulation.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';
import cheerio from 'cheerio';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${page.url()} is '${title}'`);

        const html = await page.content();
        const $ = cheerio.load(html);

        $('.collection-block-item').each((index, el) => {
            console.log('Category text:', $(el).text());
        });

        // Add all links from page to crawler's queue
        await enqueueLinks();
    },
});

// Start the crawler with the provided URLs
await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

await Dataset.open().exportToJSON('results.json');

----------------------------------------

TITLE: Implementing Basic Web Crawling with Crawlee in TypeScript
DESCRIPTION: This code snippet demonstrates how to use Crawlee's BasicCrawler to crawl web pages, send HTTP requests, and store the resulting data. It includes error handling and utilizes Crawlee's built-in storage for saving the crawled content.

LANGUAGE: typescript
CODE:
import { BasicCrawler, Dataset } from 'crawlee';

// Create an instance of the BasicCrawler class - a crawler that uses raw HTTP requests
const crawler = new BasicCrawler({
    // Let's limit our crawler to only 10 requests
    maxRequestsPerCrawl: 10,

    // This function will be called for each URL to crawl.
    // It accepts a single parameter, which is an object with options as properties:
    // - request: an instance of the Request class with information such as URL and HTTP method
    // - sendRequest: a helper function for sending proper HTTP requests
    // - crawler: the BasicCrawler instance being used
    async requestHandler({ request, sendRequest, crawler }) {
        const { body } = await sendRequest();

        // Save data to default dataset
        await Dataset.pushData({
            url: request.url,
            html: body,
        });

        // Log the result
        console.log(`HTML from ${request.url} saved to dataset.`);
    },

    // This function is called if the function passed as `requestHandler` throws an exception.
    // It accepts a single parameter, which is an object with options as properties:
    // - request: an instance of the Request class with information such as URL and HTTP method
    // - error: the instance of the thrown error
    // - crawler: the BasicCrawler instance being used
    async failedRequestHandler({ request, error }) {
        // Log the error
        console.log(`Request ${request.url} failed with error: ${error.message}`);
    },
});

// Run the crawler with initial request
await crawler.run([
    'https://crawlee.dev',
    'https://crawlee.dev/docs/quick-start',
    'https://crawlee.dev/docs/introduction/first-crawler',
    'https://crawlee.dev/docs/introduction/web-scraping-for-beginners',
]);

----------------------------------------

TITLE: Initializing Crawlee Playwright Crawler with Router
DESCRIPTION: Sets up the main Playwright crawler instance using a router for request handling and configures logging level. This is the entry point of the crawler that initiates the scraping process.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

// This is better set with CRAWLEE_LOG_LEVEL env var
// or a configuration option. This is just for show 😈
log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    // Instead of the long requestHandler with
    // if clauses we provide a router instance.
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Implementing BasicCrawler in Crawlee
DESCRIPTION: This code snippet demonstrates how to use Crawlee's BasicCrawler to download web pages and store their HTML content. It sets up a crawler, defines a request list, and processes each request to save the page's URL and content to a dataset.

LANGUAGE: typescript
CODE:
import { BasicCrawler, Dataset } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log, request }) {
        const { body } = await sendRequest();
        log.info(`Downloading ${request.url}...`);
        await Dataset.pushData({
            url: request.url,
            html: body,
        });
    },
});

await crawler.run([
    'https://crawlee.dev',
    'https://crawlee.dev/docs/quick-start',
    'https://crawlee.dev/docs/introduction/first-crawler',
]);

----------------------------------------

TITLE: Crawling Sitemap with Puppeteer Crawler in JavaScript
DESCRIPTION: This code snippet shows how to use Puppeteer Crawler to crawl a sitemap. It uses the Sitemap utility to download and process sitemap URLs, then crawls each URL using PuppeteerCrawler. The crawler extracts the page title and saves it to the dataset.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, Dataset } from 'crawlee';
import { Sitemap } from '@crawlee/utils';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url} is '${title}'`);

        await Dataset.pushData({
            url: request.url,
            title,
        });
    },
});

const sitemapUrl = 'https://crawlee.dev/sitemap.xml';

const sitemap = await Sitemap.load(sitemapUrl);
await crawler.run(sitemap.urls);

----------------------------------------

TITLE: Creating CheerioCrawler for Web Server Integration
DESCRIPTION: Initializes a CheerioCrawler with keepAlive option set to true. This keeps the crawler running and waiting for new requests even when the queue is empty.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, log } from 'crawlee';

const crawler = new CheerioCrawler({
    keepAlive: true,
    requestHandler: async ({ request, $ }) => {
        const title = $('title').text();
        // We will send the response here later
        log.info(`Page title: ${title} on ${request.url}`);
    },
});

----------------------------------------

TITLE: Complete Product Data Scraping with Playwright
DESCRIPTION: This code combines all the previous snippets to scrape comprehensive product data including URL, manufacturer, title, SKU, price, and stock availability.

LANGUAGE: javascript
CODE:
const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']
const manufacturer = urlPart.split('-')[0]; // 'sennheiser'

const title = await page.locator('.product-meta h1').textContent();
const sku = await page.locator('span.product-meta__sku-number').textContent();

const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Implementing Parallel Scraper Logic with Child Processes in JavaScript
DESCRIPTION: Creates a parallel scraper that forks multiple child processes to handle scraping tasks concurrently, using a shared request queue.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Configuration } from 'crawlee';
import { fork } from 'child_process';
import { router } from './routes.mjs';
import { getOrInitQueue } from './requestQueue.mjs';

if (process.env.IS_WORKER_THREAD) {
    // Worker process logic
    Configuration.set('purgeOnStart', false);
    const requestQueue = await getOrInitQueue(false);

    const config = new Configuration({
        storageClientOptions: {
            localDataDirectory: `./storage/worker-${process.env.WORKER_INDEX}`,
        },
    });

    const crawler = new PlaywrightCrawler({
        requestQueue,
        maxConcurrency: 5,
        requestHandler: router,
    }, config);

    await crawler.run();
    process.exit(0);
} else {
    // Parent process logic
    const numWorkers = 2;
    const workers = [];

    for (let i = 0; i < numWorkers; i++) {
        const worker = fork(new URL(import.meta.url).pathname, {
            env: { ...process.env, IS_WORKER_THREAD: '1', WORKER_INDEX: i.toString() },
        });

        workers.push(new Promise((resolve) => {
            worker.on('exit', resolve);
        }));

        worker.on('message', (message) => {
            console.log(`Received data from worker ${i}:`, message);
        });
    }

    await Promise.all(workers);
    console.log('All workers finished');
}

----------------------------------------

TITLE: Crawling All Links with Puppeteer Crawler in Crawlee
DESCRIPTION: This code snippet shows how to use Puppeteer Crawler in Crawlee to crawl all links on a website. It sets up a RequestQueue, creates a PuppeteerCrawler, and uses the enqueueLinks() method to add new links to the queue as it crawls.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, enqueueLinks } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limit to 10 requests
});

// Run the crawler
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Implementing Router for Crawlee in JavaScript
DESCRIPTION: Creates a PlaywrightRouter with handlers for different page types (DETAIL, CATEGORY, and default). Each handler processes specific page elements and enqueues further links as needed.

LANGUAGE: javascript
CODE:
import { createPlaywrightRouter, Dataset } from 'crawlee';

export const router = createPlaywrightRouter();

router.addHandler('DETAIL', async ({ request, page, log }) => {
    log.debug(`Extracting data: ${request.url}`);
    const urlPart = request.url.split('/').slice(-1);
    const manufacturer = urlPart[0].split('-')[0];

    const title = await page.locator('.product-meta h1').textContent();
    const sku = await page
        .locator('span.product-meta__sku-number')
        .textContent();

    const priceElement = page
        .locator('span.price')
        .filter({
            hasText: '$',
        })
        .first();

    const currentPriceString = await priceElement.textContent();
    const rawPrice = currentPriceString.split('$')[1];
    const price = Number(rawPrice.replaceAll(',', ''));

    const inStockElement = page
        .locator('span.product-form__inventory')
        .filter({
            hasText: 'In stock',
        })
        .first();

    const inStock = (await inStockElement.count()) > 0;

    const results = {
        url: request.url,
        manufacturer,
        title,
        sku,
        currentPrice: price,
        availableInStock: inStock,
    };

    log.debug(`Saving data: ${request.url}`);
    await Dataset.pushData(results);
});

router.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {
    log.debug(`Enqueueing pagination for: ${request.url}`);

    await page.waitForSelector('.product-item > a');
    await enqueueLinks({
        selector: '.product-item > a',
        label: 'DETAIL',
    });

    const nextButton = await page.$('a.pagination__next');
    if (nextButton) {
        await enqueueLinks({
            selector: 'a.pagination__next',
            label: 'CATEGORY',
        });
    }
});

router.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {
    log.debug(`Enqueueing categories from page: ${request.url}`);

    await page.waitForSelector('.collection-block-item');
    await enqueueLinks({
        selector: '.collection-block-item',
        label: 'CATEGORY',
    });
});

----------------------------------------

TITLE: Complete E-commerce Crawler Implementation
DESCRIPTION: Full implementation of a PlaywrightCrawler that handles both category pages and product detail pages, including pagination.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        if (request.label === 'DETAIL') {
            // We're not doing anything with the details yet.
        } else if (request.label === 'CATEGORY') {
            // We are now on a category page. We can use this to paginate through and enqueue all products,
            // as well as any subsequent pages we find

            await page.waitForSelector('.product-item > a');
            await enqueueLinks({
                selector: '.product-item > a',
                label: 'DETAIL', // <= note the different label
            });

            // Now we need to find the "Next" button and enqueue the next page of results (if it exists)
            const nextButton = await page.$('a.pagination__next');
            if (nextButton) {
                await enqueueLinks({
                    selector: 'a.pagination__next',
                    label: 'CATEGORY', // <= note the same label
                });
            }
        } else {
            // This means we're on the start page, with no label.
            // On this page, we just want to enqueue all the category pages.

            await page.waitForSelector('.collection-block-item');
            await enqueueLinks({
                selector: '.collection-block-item',
                label: 'CATEGORY',
            });
        }
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Crawling All Links with Cheerio Crawler in Crawlee
DESCRIPTION: This snippet demonstrates how to use the Cheerio Crawler in Crawlee to crawl all links on a website. It sets up a RequestQueue, creates a CheerioCrawler, and uses the enqueueLinks() method to add new links to the queue.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, enqueueLinks } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limitation for only 10 requests (do not use if you want to crawl all links)
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Initializing Crawlee Playwright Crawler with Router
DESCRIPTION: Sets up the main Playwright crawler instance using a router for request handling and configures logging level. This is the entry point of the crawler that initiates the scraping process.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

// This is better set with CRAWLEE_LOG_LEVEL env var
// or a configuration option. This is just for show 😈
log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    // Instead of the long requestHandler with
    // if clauses we provide a router instance.
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Automating GitHub Repository Search Form with PuppeteerCrawler
DESCRIPTION: Implementation of a web crawler that automates GitHub repository search by filling and submitting a form. The crawler sets search parameters including search term, repository owner, start date, and programming language, then saves the results to either Apify platform dataset or local storage.

LANGUAGE: javascript
CODE:
{CrawlSource}

----------------------------------------

TITLE: Basic Warehouse Store URL Structure Example
DESCRIPTION: Example of pagination URL structure used in the warehouse store website

LANGUAGE: text
CODE:
https://warehouse-theme-metal.myshopify.com/collections/headphones?page=2

----------------------------------------

TITLE: Automating GitHub Repository Search with PuppeteerCrawler in Crawlee
DESCRIPTION: This code demonstrates how to use PuppeteerCrawler to automate a GitHub repository search. It fills in search parameters, submits the form, extracts results, and saves them to a dataset. The crawler handles pagination and uses Puppeteer for browser automation.

LANGUAGE: typescript
CODE:
import { PuppeteerCrawler, Dataset } from 'crawlee';
import { faker } from '@faker-js/faker';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, log }) {
        await page.goto('https://github.com/search/advanced');

        // Fill form fields
        await page.type('#search_keyword', faker.lorem.word());
        await page.type('#search_user', 'apify');
        await page.type('#search_from', '2023-01-01');
        await page.select('#search_language', 'JavaScript');

        // Submit form
        await Promise.all([
            page.waitForNavigation(),
            page.click('#search_form > div.form-actions > button'),
        ]);

        let hasNextPage = true;
        let pageCounter = 1;

        // Extract search results
        while (hasNextPage) {
            log.info(`Extracting page ${pageCounter}`);

            const repositories = await page.$$eval('ul.repo-list li', (repos) => {
                return repos.map((repo) => {
                    const titleElement = repo.querySelector('div.f4 a');
                    const descriptionElement = repo.querySelector('p.mb-1');
                    const metaElement = repo.querySelector('div.flex-wrap');

                    return {
                        title: titleElement ? titleElement.textContent.trim() : null,
                        description: descriptionElement ? descriptionElement.textContent.trim() : null,
                        meta: metaElement ? metaElement.textContent.trim() : null,
                    };
                });
            });

            // Save results to dataset
            await Dataset.pushData(repositories);

            // Check for next page
            hasNextPage = await page.$('a.next_page') !== null;

            // If there is a next page, click it
            if (hasNextPage) {
                await Promise.all([
                    page.waitForNavigation(),
                    page.click('a.next_page'),
                ]);
                pageCounter++;
            }
        }
    },
});

await crawler.run(['https://github.com/search/advanced']);


----------------------------------------

TITLE: Complete Product Data Scraping with Playwright in JavaScript
DESCRIPTION: This comprehensive code snippet combines all the previous scraping techniques to extract full product information including URL, manufacturer, title, SKU, price, and stock availability.

LANGUAGE: javascript
CODE:
const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']
const manufacturer = urlPart.split('-')[0]; // 'sennheiser'

const title = await page.locator('.product-meta h1').textContent();
const sku = await page.locator('span.product-meta__sku-number').textContent();

const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Configuring Puppeteer Recursive Crawler
DESCRIPTION: Implementation example showing how to set up and run a recursive web crawler using PuppeteerCrawler. Includes configuration for running on the Apify Platform using the apify/actor-node-puppeteer-chrome Docker image.

LANGUAGE: javascript
CODE:
import RunnableCodeBlock from '@site/src/components/RunnableCodeBlock';
import ApiLink from '@site/src/components/ApiLink';
import CrawlSource from '!!raw-loader!roa-loader!./puppeteer_recursive_crawl.ts';

----------------------------------------

TITLE: Batch URL Processing with JSDOMCrawler
DESCRIPTION: Demonstrates how to use JSDOMCrawler to process multiple URLs from an external file, parse HTML content using jsdom, and extract specific elements like page titles and h1 tags.

LANGUAGE: typescript
CODE:
{JSDOMCrawlerSource}

----------------------------------------

TITLE: Complete Crawlee Scraping Example (JavaScript)
DESCRIPTION: This is the final code for the Crawlee scraping tutorial. It includes crawler setup, data extraction, and saving the results to a dataset.

LANGUAGE: javascript
CODE:
{Example}

----------------------------------------

TITLE: Crawling Multiple URLs with Cheerio
DESCRIPTION: Example showing how to crawl a list of URLs using Cheerio Crawler in Crawlee. Cheerio provides a fast and lightweight way to parse HTML without a browser environment.

LANGUAGE: javascript
CODE:
{CheerioSource}

----------------------------------------

TITLE: Implementing Router for Crawlee in JavaScript
DESCRIPTION: Defines a Router for a Crawlee project with handlers for different page types (DETAIL, CATEGORY, and default). Each handler extracts specific data or enqueues links based on the page content.

LANGUAGE: javascript
CODE:
import { createPlaywrightRouter, Dataset } from 'crawlee';

export const router = createPlaywrightRouter();

router.addHandler('DETAIL', async ({ request, page, log }) => {
    log.debug(`Extracting data: ${request.url}`);
    const urlPart = request.url.split('/').slice(-1);
    const manufacturer = urlPart[0].split('-')[0];

    const title = await page.locator('.product-meta h1').textContent();
    const sku = await page
        .locator('span.product-meta__sku-number')
        .textContent();

    const priceElement = page
        .locator('span.price')
        .filter({
            hasText: '$',
        })
        .first();

    const currentPriceString = await priceElement.textContent();
    const rawPrice = currentPriceString.split('$')[1];
    const price = Number(rawPrice.replaceAll(',', ''));

    const inStockElement = page
        .locator('span.product-form__inventory')
        .filter({
            hasText: 'In stock',
        })
        .first();

    const inStock = (await inStockElement.count()) > 0;

    const results = {
        url: request.url,
        manufacturer,
        title,
        sku,
        currentPrice: price,
        availableInStock: inStock,
    };

    log.debug(`Saving data: ${request.url}`);
    await Dataset.pushData(results);
});

router.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {
    log.debug(`Enqueueing pagination for: ${request.url}`);

    await page.waitForSelector('.product-item > a');
    await enqueueLinks({
        selector: '.product-item > a',
        label: 'DETAIL',
    });

    const nextButton = await page.$('a.pagination__next');
    if (nextButton) {
        await enqueueLinks({
            selector: 'a.pagination__next',
            label: 'CATEGORY',
        });
    }
});

router.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {
    log.debug(`Enqueueing categories from page: ${request.url}`);

    await page.waitForSelector('.collection-block-item');
    await enqueueLinks({
        selector: '.collection-block-item',
        label: 'CATEGORY',
    });
});

----------------------------------------

TITLE: Demonstrating Key-Value Store Operations in Crawlee
DESCRIPTION: Example showing basic operations with key-value stores including reading input, writing output, and managing named stores. Demonstrates automatic JSON conversion and value handling.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';

// Get the INPUT from the default key-value store
const input = await KeyValueStore.getInput();

// Write the OUTPUT to the default key-value store
await KeyValueStore.setValue('OUTPUT', { myResult: 123 });

// Open a named key-value store
const store = await KeyValueStore.open('some-name');

// Write a record to the named key-value store.
// JavaScript object is automatically converted to JSON,
// strings and binary buffers are stored as they are
await store.setValue('some-key', { foo: 'bar' });

// Read a record from the named key-value store.
// Note that JSON is automatically parsed to a JavaScript object,
// text data is returned as a string, and other data is returned as binary buffer
const value = await store.getValue('some-key');

// Delete a record from the named key-value store
await store.setValue('some-key', null);

----------------------------------------

TITLE: Comparing Cheerio and Browser JavaScript DOM Manipulation
DESCRIPTION: This snippet demonstrates how to perform common DOM operations using both Cheerio and plain browser JavaScript. It shows how to select the title element and extract all href attributes from a page.

LANGUAGE: typescript
CODE:
// Return the text content of the <title> element.
document.querySelector('title').textContent; // plain JS
$('title').text(); // Cheerio

// Return an array of all 'href' links on the page.
Array.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS
$('[href]')
    .map((i, el) => $(el).attr('href'))
    .get(); // Cheerio

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Router
DESCRIPTION: Sets up the main crawler instance using PlaywrightCrawler and configures logging levels. The crawler uses a router for request handling instead of inline logic.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

// This is better set with CRAWLEE_LOG_LEVEL env var
// or a configuration option. This is just for show 😈
log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    // Instead of the long requestHandler with
    // if clauses we provide a router instance.
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Implementing Router for Crawlee Web Scraping in JavaScript
DESCRIPTION: Defines a router for handling different types of pages in a web scraping project. It includes handlers for detail pages, category pages, and a default handler for the start page.

LANGUAGE: javascript
CODE:
import { createPlaywrightRouter, Dataset } from 'crawlee';

export const router = createPlaywrightRouter();

router.addHandler('DETAIL', async ({ request, page, log }) => {
    log.debug(`Extracting data: ${request.url}`);
    const urlPart = request.url.split('/').slice(-1);
    const manufacturer = urlPart[0].split('-')[0];

    const title = await page.locator('.product-meta h1').textContent();
    const sku = await page
        .locator('span.product-meta__sku-number')
        .textContent();

    const priceElement = page
        .locator('span.price')
        .filter({
            hasText: '$',
        })
        .first();

    const currentPriceString = await priceElement.textContent();
    const rawPrice = currentPriceString.split('$')[1];
    const price = Number(rawPrice.replaceAll(',', ''));

    const inStockElement = page
        .locator('span.product-form__inventory')
        .filter({
            hasText: 'In stock',
        })
        .first();

    const inStock = (await inStockElement.count()) > 0;

    const results = {
        url: request.url,
        manufacturer,
        title,
        sku,
        currentPrice: price,
        availableInStock: inStock,
    };

    log.debug(`Saving data: ${request.url}`);
    await Dataset.pushData(results);
});

router.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {
    log.debug(`Enqueueing pagination for: ${request.url}`);

    await page.waitForSelector('.product-item > a');
    await enqueueLinks({
        selector: '.product-item > a',
        label: 'DETAIL',
    });

    const nextButton = await page.$('a.pagination__next');
    if (nextButton) {
        await enqueueLinks({
            selector: 'a.pagination__next',
            label: 'CATEGORY',
        });
    }
});

router.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {
    log.debug(`Enqueueing categories from page: ${request.url}`);

    await page.waitForSelector('.collection-block-item');
    await enqueueLinks({
        selector: '.collection-block-item',
        label: 'CATEGORY',
    });
});

----------------------------------------

TITLE: Standalone SessionPool Usage in Crawlee
DESCRIPTION: This example illustrates standalone usage of SessionPool in Crawlee for managing sessions manually. It demonstrates creating a session pool, adding sessions, and using them for requests.

LANGUAGE: js
CODE:
import { SessionPool } from 'crawlee';

const sessionPool = new SessionPool({
    maxPoolSize: 25,
});

// Create a session
const session1 = await sessionPool.getSession();
console.log(session1.id); // e.g. session_01

// Retrieve the same session
const session1again = await sessionPool.getSession();
console.log(session1again.id); // session_01

// Create second session
const session2 = await sessionPool.getSession();
console.log(session2.id); // session_02

// Set session as blocked
session2.markBad();

// Create another session
const session3 = await sessionPool.getSession();
console.log(session3.id); // session_03

// Remove retired sessions
await sessionPool.teardown();

----------------------------------------

TITLE: Standalone SessionPool Usage in Crawlee
DESCRIPTION: This example illustrates standalone usage of SessionPool in Crawlee for managing sessions manually. It demonstrates creating a session pool, adding sessions, and using them for requests.

LANGUAGE: js
CODE:
import { SessionPool } from 'crawlee';

const sessionPool = new SessionPool({
    maxPoolSize: 25,
});

// Create a session
const session1 = await sessionPool.getSession();
console.log(session1.id); // e.g. session_01

// Retrieve the same session
const session1again = await sessionPool.getSession();
console.log(session1again.id); // session_01

// Create second session
const session2 = await sessionPool.getSession();
console.log(session2.id); // session_02

// Set session as blocked
session2.markBad();

// Create another session
const session3 = await sessionPool.getSession();
console.log(session3.id); // session_03

// Remove retired sessions
await sessionPool.teardown();

----------------------------------------

TITLE: Recursive Website Crawling with PuppeteerCrawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to set up and execute a recursive website crawl using PuppeteerCrawler. It includes configuration for the crawler, request handling, and data extraction logic.

LANGUAGE: JavaScript
CODE:
import { PuppeteerCrawler, Dataset } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ request, page, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        // Save results as JSON to ./storage/datasets/default
        await Dataset.pushData({
            url: request.url,
            title,
        });

        // Extract links from the current page
        // and add them to the crawling queue.
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limit the crawl to 10 requests
});

// Add first URL to the queue and start the crawl.
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Crawling Sitemap with Cheerio Crawler
DESCRIPTION: Implementation of sitemap crawling using Cheerio Crawler. Uses the Sitemap utility class to download and process sitemap URLs with Cheerio's lightweight DOM parsing.

LANGUAGE: javascript
CODE:
{CheerioSource}

----------------------------------------

TITLE: Implementing Sitemap Crawling with Cheerio Crawler
DESCRIPTION: Example showing how to use Cheerio Crawler to process sitemap URLs using the Sitemap utility class from @crawlee/utils. This implementation is best for simple HTML scraping without JavaScript rendering.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, CheerioRootAPIProvider } from 'crawlee';
import { Sitemap } from '@crawlee/utils';

const crawler = new CheerioCrawler({
    async requestHandler({ log }) {
        log.info('Got a page!');
    },
});

const sitemap = new Sitemap({
    provider: new CheerioRootAPIProvider(),
});

await sitemap.downloadSitemaps(['https://example.com/sitemap.xml']);
const urls = await sitemap.getAllURLs();

await crawler.run(urls);

----------------------------------------

TITLE: Downloading Files with Crawlee FileDownload Crawler
DESCRIPTION: Demonstrates how to download binary files using Crawlee's FileDownload crawler class and save them to the default key-value store. Files are stored locally in ./storage/key_value_stores/default when running in local configuration.

LANGUAGE: javascript
CODE:
import RunnableCodeBlock from '@site/src/components/RunnableCodeBlock';
import ApiLink from '@site/src/components/ApiLink';
import FileDownloadSource from '!!raw-loader!roa-loader!./file_download.ts';

----------------------------------------

TITLE: Loading and Parsing HTML with CheerioCrawler in JavaScript
DESCRIPTION: Example showing how to use CheerioCrawler to crawl URLs from an external file, make HTTP requests, and parse HTML content using Cheerio library. The code extracts page titles and h1 tags from each crawled page.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

// Create an instance of the CheerioCrawler class - a crawler
// that automatically loads the URLs and parses their HTML using the Cheerio library.
const crawler = new CheerioCrawler({
    // The crawler will automatically process each URL from the RequestList
    // and call the following function on each HTML page.
    async requestHandler({ $, request, enqueueLinks, log }) {
        // Extract data from the page using Cheerio.
        const title = $('title').text();
        const h1texts = [];
        $('h1').each((index, el) => {
            h1texts.push($(el).text());
        });

        // Save the data to dataset.
        await Dataset.pushData({
            url: request.url,
            title,
            h1texts,
        });

        // Extract links from the page and add them to the crawling queue.
        await enqueueLinks();
    },
});

// Start the crawler and wait for it to finish.
await crawler.run(['https://crawlee.dev']);


----------------------------------------

TITLE: Configuring Dockerfile for Crawlee Project with Node.js
DESCRIPTION: This Dockerfile sets up a Docker image for a Crawlee project. It uses the apify/actor-node:20 base image, copies package files, installs NPM packages excluding dev and optional dependencies, copies the source code, and sets the command to run the project.

LANGUAGE: Dockerfile
CODE:
FROM apify/actor-node:20

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm start --silent

----------------------------------------

TITLE: Configuring Maximum Requests Per Minute in Crawlee
DESCRIPTION: This snippet demonstrates how to set the maxRequestsPerMinute option in a CheerioCrawler to limit the rate of requests. It sets a maximum of 120 requests per minute.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    maxRequestsPerMinute: 120,
    // ... other options
});

----------------------------------------

TITLE: Complete E-commerce Store Crawler with Crawlee
DESCRIPTION: Full implementation of a PlaywrightCrawler that handles both category and product detail pages, including pagination handling and proper request labeling.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        if (request.label === 'DETAIL') {
            // We're not doing anything with the details yet.
        } else if (request.label === 'CATEGORY') {
            // We are now on a category page. We can use this to paginate through and enqueue all products,
            // as well as any subsequent pages we find

            await page.waitForSelector('.product-item > a');
            await enqueueLinks({
                selector: '.product-item > a',
                label: 'DETAIL', // <= note the different label
            });

            // Now we need to find the "Next" button and enqueue the next page of results (if it exists)
            const nextButton = await page.$('a.pagination__next');
            if (nextButton) {
                await enqueueLinks({
                    selector: 'a.pagination__next',
                    label: 'CATEGORY', // <= note the same label
                });
            }
        } else {
            // This means we're on the start page, with no label.
            // On this page, we just want to enqueue all the category pages.

            await page.waitForSelector('.collection-block-item');
            await enqueueLinks({
                selector: '.collection-block-item',
                label: 'CATEGORY',
            });
        }
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Scraping Dynamic Content with PlaywrightCrawler in Crawlee
DESCRIPTION: This snippet shows how to use PlaywrightCrawler to scrape JavaScript-rendered content. Playwright automatically waits for elements to appear in the DOM.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        // Extract text content of the first actor card
        const actorText = await page.textContent('.ActorStoreItem');
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Creating a Request Queue with Locking Support in JavaScript
DESCRIPTION: This code snippet defines a function to initialize or retrieve a request queue that supports locking. It uses the Crawlee library and handles both local and cloud environments.

LANGUAGE: javascript
CODE:
import { RequestQueue } from 'crawlee';
import { Actor } from 'apify';

export async function getOrInitQueue(purgeOnStart = false) {
    if (Actor.isAtHome()) {
        return await Actor.openRequestQueue();
    }

    return await RequestQueue.open(null, { purgeOnStart });
}

----------------------------------------

TITLE: Configuring PlaywrightCrawler with Firefox Browser
DESCRIPTION: Example showing how to initialize and configure PlaywrightCrawler to use Firefox browser in headless mode. The code demonstrates the basic setup and browser configuration. References the Hacker News website as a practical use case.

LANGUAGE: typescript
CODE:
{PlaywrightFirefoxSource}

----------------------------------------

TITLE: Downloading Files with Streams using Crawlee's FileDownload Crawler
DESCRIPTION: This code snippet demonstrates how to use the FileDownload crawler from Crawlee to download large files efficiently using Node.js streams. It shows how to configure the crawler, set up progress logging, and store the downloaded files in a key-value store.

LANGUAGE: typescript
CODE:
import { FileDownload } from 'crawlee';

const crawler = new FileDownload({
    maxRequestsPerCrawl: 2,
    async requestHandler({ log, incomingMessage, abortController }) {
        log.info('Starting file download');

        const totalBytes = parseInt(incomingMessage.headers['content-length'] ?? '0', 10);
        let downloadedBytes = 0;

        incomingMessage.on('data', (chunk) => {
            downloadedBytes += chunk.length;
            const percentage = ((downloadedBytes / totalBytes) * 100).toFixed(2);
            log.info(`Downloaded ${percentage}%`);
        });

        incomingMessage.on('end', () => {
            log.info('File download completed!');
        });

        // Artificially abort the download after 500ms
        setTimeout(() => {
            abortController.abort();
        }, 500);

        return incomingMessage;
    },
});

await crawler.run([
    'https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4',
    'https://cdn.apify.com/v2/key-value-stores/default/records/FILE_1',
]);


----------------------------------------

TITLE: Implementing BasicCrawler for Web Scraping in JavaScript
DESCRIPTION: This snippet demonstrates how to create and use a BasicCrawler instance to crawl web pages, fetch their HTML content, and store it in a dataset. It shows the setup of the crawler with a custom requestHandler and how to run the crawler with initial URLs.

LANGUAGE: javascript
CODE:
import { BasicCrawler, Dataset } from 'crawlee';

// Create a crawler instance
const crawler = new BasicCrawler({
    async requestHandler({ request, sendRequest }) {
        // 'request' contains an instance of the Request class
        // Here we simply fetch the HTML of the page and store it to a dataset
        const { body } = await sendRequest({
            url: request.url,
            method: request.method,
            body: request.payload,
            headers: request.headers,
        });

        await Dataset.pushData({
            url: request.url,
            html: body,
        })
    },
});

// Enqueue the initial requests and run the crawler
await crawler.run([
    'http://www.example.com/page-1',
    'http://www.example.com/page-2',
]);

----------------------------------------

TITLE: Sanity Check with PlaywrightCrawler
DESCRIPTION: Basic crawler setup to verify the scraping environment by printing category content from a Shopify store. Uses PlaywrightCrawler to access JavaScript-rendered content and demonstrates selector usage.

LANGUAGE: javascript
CODE:
document.querySelectorAll('.collection-block-item');

----------------------------------------

TITLE: Crawling All Links with Cheerio Crawler in Crawlee
DESCRIPTION: This code snippet demonstrates how to use Cheerio Crawler in Crawlee to crawl all links on a website. It initializes a RequestQueue, creates a CheerioCrawler, and uses the enqueueLinks() method to add new links to the queue.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, enqueueLinks } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limitation for only 10 requests (do not use if you want to crawl all links)
});

// Run the crawler
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Exporting Dataset to CSV in Crawlee (TypeScript)
DESCRIPTION: This code snippet demonstrates how to use Crawlee to scrape product information from a website, store it in a dataset, and then export the entire dataset to a single CSV file. It utilizes the CheerioCrawler and the Dataset.exportToValue function.

LANGUAGE: typescript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks, log }) {
        const title = $('title').text();
        log.info(`Title of ${request.url} is '${title}'`);

        // Save results
        await Dataset.pushData({
            title,
            url: request.url,
        });

        // Enqueue the homepage menu items
        await enqueueLinks({
            selector: '.nav a',
        });
    },
    maxRequestsPerCrawl: 20, // Limitation for only 20 requests (do not use if you want to crawl all links)
});

await crawler.run(['https://crawlee.dev']);

// Export
const csv = await Dataset.exportToValue('csv');
const store = await Actor.openKeyValueStore();
await store.setValue('RESULTS.csv', csv, { contentType: 'text/csv' });

----------------------------------------

TITLE: Filling and Submitting GitHub Search Form with PuppeteerCrawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use PuppeteerCrawler to automate a GitHub repository search. It fills in search criteria, submits the form, extracts results, and saves them to a dataset. The crawler handles pagination and processes multiple result pages.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, Dataset } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, log }) {
        log.info('Searching for repositories...');

        // Fill in the search form
        await page.type('input[name="q"]', 'crawler');
        await page.type('#search_from', 'apozdniakov');
        await page.type('#search_date', '>=2023-01-01');
        await page.select('select#search_language', 'JavaScript');

        // Submit the form
        await Promise.all([
            page.waitForNavigation(),
            page.click('button[type="submit"]'),
        ]);

        // Extract the data
        const repositories = await page.$$eval('.repo-list-item', (items) =>
            items.map((item) => ({
                title: item.querySelector('a').innerText,
                url: item.querySelector('a').href,
                description: item.querySelector('p.col-9').innerText,
            }))
        );

        // Save the data to dataset
        await Dataset.pushData(repositories);

        // Recursively crawl through all the pages
        const nextButton = await page.$('a.next_page');
        if (nextButton) {
            log.info('Crawling the next page...');
            await Promise.all([
                page.waitForNavigation(),
                nextButton.click(),
            ]);
        }
    },
});

await crawler.run(['https://github.com/search']);


----------------------------------------

TITLE: Importing React Components and Code Sources for Crawlee Crawlers
DESCRIPTION: This snippet imports React components for creating tabs and a runnable code block. It also imports the source code for Cheerio, Puppeteer, and Playwright crawlers using raw-loader and roa-loader.

LANGUAGE: javascript
CODE:
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import RunnableCodeBlock from '@site/src/components/RunnableCodeBlock';

import CheerioSource from '!!raw-loader!roa-loader!./crawl_multiple_urls_cheerio.ts';
import PuppeteerSource from '!!raw-loader!roa-loader!./crawl_multiple_urls_puppeteer.ts';
import PlaywrightSource from '!!raw-loader!roa-loader!./crawl_multiple_urls_playwright.ts';

----------------------------------------

TITLE: Crawling Sitemap with Playwright Crawler in Crawlee
DESCRIPTION: This snippet illustrates how to use Playwright Crawler to crawl a sitemap. It downloads the sitemap URLs, creates a RequestQueue, and processes each URL to extract the page title using Playwright's browser automation.

LANGUAGE: javascript
CODE:
import { PlaywrightRouter, PlaywrightCrawler } from 'crawlee';
import { downloadListOfUrls } from '@crawlee/utils';

const router = new PlaywrightRouter();

router.addDefaultHandler(async ({ page, request, log }) => {
    const title = await page.title();
    log.info(`Title of ${request.url} is: ${title}`);
});

const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

const SITEMAP_URL = 'https://crawlee.dev/sitemap.xml';

await crawler.run([
    ...await downloadListOfUrls({ url: SITEMAP_URL }),
]);


----------------------------------------

TITLE: Crawling Sitemap with Cheerio Crawler in JavaScript
DESCRIPTION: This snippet demonstrates how to use the Cheerio Crawler to crawl a sitemap. It utilizes the Sitemap utility class to download and process sitemap URLs, then uses the CheerioCrawler to scrape the content of each page.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, downloadListOfUrls } from 'crawlee';
import { Sitemap } from '@crawlee/utils';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

const { downloadListOfUrls } = Sitemap;

const run = async () => {
    const urls = await downloadListOfUrls({
        url: 'https://apify.com/sitemap.xml',
    });

    await crawler.run(urls);
};

run();

----------------------------------------

TITLE: Crawling All Links with Cheerio Crawler in Crawlee
DESCRIPTION: This snippet demonstrates how to use Cheerio Crawler in Crawlee to crawl all links on a website. It sets up a basic crawler configuration, defines a request handler, and uses enqueueLinks() to add new links to the queue.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        await Dataset.pushData({
            url: request.url,
            title,
        });

        await enqueueLinks();
    },
    maxRequestsPerCrawl: 20, // Limit to 20 requests
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Crawling All Links with Cheerio Crawler in Crawlee
DESCRIPTION: This snippet demonstrates how to use Cheerio Crawler in Crawlee to crawl all links on a website. It initializes a RequestQueue, creates a CheerioCrawler, and uses the enqueueLinks() method to add new links to the queue.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, enqueueLinks } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limit the crawler to only 10 requests
});

// Run the crawler with initial request
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Using Camoufox with PlaywrightCrawler for Cloudflare Challenge
DESCRIPTION: This snippet demonstrates how to use Camoufox, a custom stealthy build of Firefox, with PlaywrightCrawler to handle Cloudflare challenges.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, handleCloudflareChallenge } from 'crawlee';
import { CamoufoxLauncher } from 'camoufox';

const crawler = new PlaywrightCrawler({
    launchContext: {
        // Use Camoufox launcher instead of the default one
        launcher: new CamoufoxLauncher(),
    },
    async requestHandler({ page, enqueueLinks }) {
        // Handle Cloudflare challenge if necessary
        await handleCloudflareChallenge({ page });

        // Crawl normally
        await enqueueLinks();
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Crawling Sitemap with Puppeteer Crawler in JavaScript
DESCRIPTION: This snippet shows how to use the Puppeteer Crawler to crawl a sitemap. It uses the Sitemap utility class to download and process sitemap URLs, then employs the PuppeteerCrawler to navigate and scrape each page.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';
import { Sitemap } from '@crawlee/utils';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        const title = await page.title();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

const { downloadListOfUrls } = Sitemap;

const run = async () => {
    const urls = await downloadListOfUrls({
        url: 'https://apify.com/sitemap.xml',
    });

    await crawler.run(urls);
};

run();

----------------------------------------

TITLE: Saving Data to Default Dataset in Crawlee
DESCRIPTION: This code snippet demonstrates how to save scraped data to the default dataset in Crawlee. It uses the Dataset.pushData() method to add items to the dataset. If the dataset doesn't exist, it will be automatically created.

LANGUAGE: javascript
CODE:
import { Dataset, CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks, log }) {
        const title = $('title').text();
        log.info(`Title of ${request.url}: ${title}`);

        // Save data to default dataset
        await Dataset.pushData({
            url: request.url,
            title,
        });

        // Enqueue all links from page
        await enqueueLinks();
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Comprehensive Crawling of E-commerce Store (TypeScript)
DESCRIPTION: This code snippet showcases a more advanced crawling strategy for an e-commerce store. It handles the initial category page, product listing pages with pagination, and enqueues links to product detail pages. The crawler uses different labels to distinguish between page types and adjust its behavior accordingly.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        if (request.label === 'DETAIL') {
            // We're not doing anything with the details yet.
        } else if (request.label === 'CATEGORY') {
            // We are now on a category page. We can use this to paginate through and enqueue all products,
            // as well as any subsequent pages we find

            await page.waitForSelector('.product-item > a');
            await enqueueLinks({
                selector: '.product-item > a',
                label: 'DETAIL', // <= note the different label
            });

            // Now we need to find the "Next" button and enqueue the next page of results (if it exists)
            const nextButton = await page.$('a.pagination__next');
            if (nextButton) {
                await enqueueLinks({
                    selector: 'a.pagination__next',
                    label: 'CATEGORY', // <= note the same label
                });
            }
        } else {
            // This means we're on the start page, with no label.
            // On this page, we just want to enqueue all the category pages.

            await page.waitForSelector('.collection-block-item');
            await enqueueLinks({
                selector: '.collection-block-item',
                label: 'CATEGORY',
            });
        }
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: HTTP Crawler URL Processing with Crawlee
DESCRIPTION: Shows the implementation of an HTTP crawler that reads URLs from an external file, processes them using HTTP requests, and saves the resulting HTML. The example utilizes the HttpCrawler class from the Crawlee framework.

LANGUAGE: javascript
CODE:
{HttpCrawlerSource}

----------------------------------------

TITLE: Crawling Specific Links with CheerioCrawler in JavaScript
DESCRIPTION: This code snippet shows how to set up a CheerioCrawler to crawl specific links on a website. It uses the globs property to filter links matching certain patterns and demonstrates how to process the crawled data.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}`);

        await Dataset.pushData({
            title,
            url: request.url,
        });

        // Extract links from the current page
        // and add them to the crawling queue.
        await enqueueLinks({
            globs: ['https://apify.com/*/*'],
            exclude: ['**/log-in'],
        });
    },
    maxRequestsPerCrawl: 20, // Limitation for only 20 requests (do not use if you want to crawl all links)
});

// Run the crawler
await crawler.run(['https://apify.com/']);

console.log('Crawler finished.');

----------------------------------------

TITLE: Automating GitHub Repository Search with PuppeteerCrawler in TypeScript
DESCRIPTION: This code snippet uses PuppeteerCrawler to automate a GitHub repository search. It fills out a form with search criteria, submits it, extracts search results, and saves them to a dataset. The crawler handles pagination and demonstrates form interaction in a headless browser environment.

LANGUAGE: typescript
CODE:
import { PuppeteerCrawler, Dataset } from 'crawlee';
import { faker } from '@faker-js/faker';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, log }) {
        if (page.url() === 'https://github.com/search/advanced') {
            log.info('Filling in search form...');
            await page.type('#search_form input[name="q"]', 'crawler');
            await page.type('#search_form input[name="user"]', 'apify');
            await page.type('#search_form input[name="created"]', '>2022-01-01');
            await page.select('#search_language', 'JavaScript');
            await Promise.all([
                page.waitForNavigation(),
                page.click('#search_form button[type="submit"]'),
            ]);
        } else {
            log.info('Extracting data...');
            const repos = await page.$$eval('.repo-list-item', (items) => {
                return items.map((item) => {
                    const repo = item.querySelector('a[data-hydro-click]');
                    const description = item.querySelector('.col-12');
                    const stars = item.querySelector('.muted-link');
                    const topics = item.querySelectorAll('.topic-tag');
                    return {
                        title: repo?.textContent.trim(),
                        description: description?.textContent.trim(),
                        stars: stars?.textContent.trim(),
                        topics: Array.from(topics).map((t) => t.textContent.trim()),
                    };
                });
            });
            await Dataset.pushData(repos);

            const nextButton = await page.$('a.next_page');
            if (nextButton) {
                log.info('Clicking next page...');
                await Promise.all([
                    page.waitForNavigation(),
                    nextButton.click(),
                ]);
            }
        }
    },
    maxRequestsPerCrawl: 10,
});

await crawler.run(['https://github.com/search/advanced']);

console.log('Crawler finished.');


----------------------------------------

TITLE: Implementing Router for Crawlee Web Scraping
DESCRIPTION: Defines a router for handling different types of pages in a web scraping project. It includes handlers for detail pages, category pages, and a default handler for the start page.

LANGUAGE: javascript
CODE:
import { createPlaywrightRouter, Dataset } from 'crawlee';

export const router = createPlaywrightRouter();

router.addHandler('DETAIL', async ({ request, page, log }) => {
    log.debug(`Extracting data: ${request.url}`);
    const urlPart = request.url.split('/').slice(-1);
    const manufacturer = urlPart[0].split('-')[0];

    const title = await page.locator('.product-meta h1').textContent();
    const sku = await page
        .locator('span.product-meta__sku-number')
        .textContent();

    const priceElement = page
        .locator('span.price')
        .filter({
            hasText: '$',
        })
        .first();

    const currentPriceString = await priceElement.textContent();
    const rawPrice = currentPriceString.split('$')[1];
    const price = Number(rawPrice.replaceAll(',', ''));

    const inStockElement = page
        .locator('span.product-form__inventory')
        .filter({
            hasText: 'In stock',
        })
        .first();

    const inStock = (await inStockElement.count()) > 0;

    const results = {
        url: request.url,
        manufacturer,
        title,
        sku,
        currentPrice: price,
        availableInStock: inStock,
    };

    log.debug(`Saving data: ${request.url}`);
    await Dataset.pushData(results);
});

router.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {
    log.debug(`Enqueueing pagination for: ${request.url}`);

    await page.waitForSelector('.product-item > a');
    await enqueueLinks({
        selector: '.product-item > a',
        label: 'DETAIL',
    });

    const nextButton = await page.$('a.pagination__next');
    if (nextButton) {
        await enqueueLinks({
            selector: 'a.pagination__next',
            label: 'CATEGORY',
        });
    }
});

router.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {
    log.debug(`Enqueueing categories from page: ${request.url}`);

    await page.waitForSelector('.collection-block-item');
    await enqueueLinks({
        selector: '.collection-block-item',
        label: 'CATEGORY',
    });
});

----------------------------------------

TITLE: Capturing Screenshots with PuppeteerCrawler using page.screenshot()
DESCRIPTION: This snippet demonstrates how to capture screenshots of multiple web pages using PuppeteerCrawler with the page.screenshot() method. It creates a crawler, defines a request handler that takes screenshots, and starts the crawler with a list of URLs.

LANGUAGE: javascript
CODE:
import { KeyValueStore, PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);

        const screenshot = await page.screenshot();
        const key = new URL(request.url).pathname;
        await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });
    },
});

await crawler.run([
    'https://crawlee.dev',
    'https://apify.com',
]);

----------------------------------------

TITLE: Capturing Screenshots with PuppeteerCrawler using page.screenshot()
DESCRIPTION: This snippet demonstrates how to capture screenshots of multiple web pages using PuppeteerCrawler with the page.screenshot() method. It creates a crawler, defines a request handler that takes screenshots, and starts the crawler with a list of URLs.

LANGUAGE: javascript
CODE:
import { KeyValueStore, PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);

        const screenshot = await page.screenshot();
        const key = new URL(request.url).pathname;
        await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });
    },
});

await crawler.run([
    'https://crawlee.dev',
    'https://apify.com',
]);

----------------------------------------

TITLE: Configuring SessionPool for CheerioCrawler in Crawlee
DESCRIPTION: This code snippet illustrates the setup of SessionPool with CheerioCrawler in Crawlee. It includes proxy configuration and session handling for Cheerio-based web scraping.

LANGUAGE: javascript
CODE:
{CheerioSource}

----------------------------------------

TITLE: Creating Multi-Stage Docker Build for Crawlee Project
DESCRIPTION: This Dockerfile defines a multi-stage build process for a Crawlee project. It first creates a builder stage to install dependencies and build the project, then creates a final stage with only the necessary runtime files and dependencies. This approach optimizes the final image size and improves security by excluding development dependencies.

LANGUAGE: Dockerfile
CODE:
# Specify the base Docker image. You can read more about
# the available images at https://crawlee.dev/js/docs/guides/docker-images
# You can also use any other image from Docker Hub.
FROM apify/actor-node:16 AS builder

# Copy just package.json and package-lock.json
# to speed up the build using Docker layer cache.
COPY package*.json ./

# Install all dependencies. Don't audit to speed up the installation.
RUN npm install --include=dev --audit=false

# Next, copy the source files using the user set
# in the base image.
COPY . ./

# Install all dependencies and build the project.
# Don't audit to speed up the installation.
RUN npm run build

# Create final image
FROM apify/actor-node:16

# Copy only built JS files from builder image
COPY --from=builder /usr/src/app/dist ./dist

# Copy just package.json and package-lock.json
# to speed up the build using Docker layer cache.
COPY package*.json ./

# Install NPM packages, skip optional and development dependencies to
# keep the image small. Avoid logging too much and print the dependency
# tree for debugging
RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

# Next, copy the remaining files and directories with the source code.
# Since we do this after NPM install, quick build will be really fast
# for most source file changes.
COPY . ./


# Run the image.
CMD npm run start:prod --silent

----------------------------------------

TITLE: Crawling Links with Playwright Crawler
DESCRIPTION: Implementation showing how to crawl website links using Playwright Crawler. Requires apify/actor-node-playwright-chrome image when running on Apify Platform. Offers modern browser automation capabilities.

LANGUAGE: javascript
CODE:
PlaywrightSource

----------------------------------------

TITLE: Disabling Browser Fingerprints in PuppeteerCrawler
DESCRIPTION: Example showing how to disable browser fingerprints in PuppeteerCrawler by setting useFingerprints option to false in browserPoolOptions.

LANGUAGE: javascript
CODE:
{PuppeteerFingerprintsOffSource}

----------------------------------------

TITLE: Crawling All Links with Puppeteer Crawler in Crawlee
DESCRIPTION: This snippet shows how to use Puppeteer Crawler in Crawlee to crawl all links on a website. It uses enqueueLinks() to add new links to the RequestQueue and limits the crawl to 100 pages. It requires the apify/actor-node-puppeteer-chrome image for deployment on the Apify Platform.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, enqueueLinks } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 100, // Limitation for only 100 requests (do not use if you want to crawl all links)
});

// Run the crawler with initial request
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Router
DESCRIPTION: Sets up the main crawler instance using PlaywrightCrawler with a router for request handling. Includes logging configuration for better debugging.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

// This is better set with CRAWLEE_LOG_LEVEL env var
// or a configuration option. This is just for show 😈
log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    // Instead of the long requestHandler with
    // if clauses we provide a router instance.
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Comparing Cheerio and Browser JavaScript for DOM Manipulation
DESCRIPTION: This snippet demonstrates how to perform common DOM operations using both Cheerio and plain browser JavaScript. It shows how to select the title element and extract text, as well as how to find all href links on a page.

LANGUAGE: typescript
CODE:
// Return the text content of the <title> element.
document.querySelector('title').textContent; // plain JS
$('title').text(); // Cheerio

// Return an array of all 'href' links on the page.
Array.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS
$('[href]')
    .map((i, el) => $(el).attr('href'))
    .get(); // Cheerio

----------------------------------------

TITLE: Creating a CheerioCrawler with RequestQueue in Crawlee
DESCRIPTION: This code snippet shows how to create a CheerioCrawler instance, configure it with a RequestQueue, and define a requestHandler to process the crawled pages. It extracts the page title using Cheerio and logs it to the console.

LANGUAGE: javascript
CODE:
import { RequestQueue, CheerioCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

const crawler = new CheerioCrawler({
    requestQueue,
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run();

----------------------------------------

TITLE: Scraping with PuppeteerCrawler (JavaScript)
DESCRIPTION: Example of using PuppeteerCrawler to scrape JavaScript-rendered content from Apify Store. This code shows how to explicitly wait for elements to appear in Puppeteer before extracting data.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // Wait for the actor cards to render
        await page.waitForSelector('.ActorStoreItem');
        // Extract text content of the first actor card
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Interactive Web Automation with JSDOM Crawler
DESCRIPTION: Demonstrates how to use JSDOMCrawler to interact with a React calculator application. The script performs button clicks to calculate 1+1 and extracts the result.

LANGUAGE: typescript
CODE:
{JSDOMCrawlerRunScriptSource}

----------------------------------------

TITLE: Implementing Parallel Scraper Logic with Child Processes in Crawlee
DESCRIPTION: This code sets up a parallel scraper using Node.js child processes. It forks multiple instances of the scraper, each processing URLs from a shared request queue, and handles data communication between parent and child processes.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';
import { CheerioCrawler, Configuration } from 'crawlee';
import { fork } from 'child_process';
import { fileURLToPath } from 'url';
import { router } from './routes.mjs';
import { getOrInitQueue } from './requestQueue.mjs';

const WORKER_COUNT = 2;

if (process.env.IS_WORKER_THREAD) {
    // Enable the request locking experiment
    Configuration.getGlobalConfig().set('experiments.requestLocking', true);
    
    Configuration.set('purgeOnStart', false);
    const requestQueue = await getOrInitQueue(false);
    const config = new Configuration({
        storageClientOptions: {
            localDataDirectory: `./storage/worker-${process.env.WORKER_INDEX}`,
        },
    });

    const crawler = new CheerioCrawler({
        requestQueue,
        requestHandler: router,
        maxConcurrency: 5,
    }, config);

    await crawler.run();
    process.exit(0);
} else {
    await Actor.init();

    const workerPromises = [];

    for (let i = 0; i < WORKER_COUNT; i++) {
        const workerPromise = new Promise((resolve) => {
            const worker = fork(fileURLToPath(import.meta.url), [], {
                env: { ...process.env, IS_WORKER_THREAD: '1', WORKER_INDEX: i.toString() },
            });

            worker.on('message', (message) => {
                if (message && message.type === 'CRAWLER_DATA') {
                    Actor.pushData(message.data);
                }
            });

            worker.on('exit', (code) => {
                console.log(`Worker ${i} exited with code ${code}`);
                resolve();
            });
        });

        workerPromises.push(workerPromise);
    }

    await Promise.all(workerPromises);
    await Actor.exit();
}

----------------------------------------

TITLE: Using Request Queue with Crawler in Crawlee
DESCRIPTION: Demonstrates how to use a request queue with a crawler in Crawlee. The crawler automatically processes requests from the queue and adds new URLs to it.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, enqueueLinks }) {
        // Add new requests to the queue
        await enqueueLinks();
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Installing Crawlee Dependencies
DESCRIPTION: Package installation commands for different crawler types including optional browser automation dependencies

LANGUAGE: bash
CODE:
npm install crawlee
npm install crawlee playwright
npm install crawlee puppeteer

----------------------------------------

TITLE: Comprehensive E-commerce Crawling with PlaywrightCrawler
DESCRIPTION: Implements a PlaywrightCrawler to crawl both category and product detail pages of an e-commerce site. It uses different selectors and labels to distinguish between page types and handle pagination.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        if (request.label === 'DETAIL') {
            // We're not doing anything with the details yet.
        } else if (request.label === 'CATEGORY') {
            // We are now on a category page. We can use this to paginate through and enqueue all products,
            // as well as any subsequent pages we find

            await page.waitForSelector('.product-item > a');
            await enqueueLinks({
                selector: '.product-item > a',
                label: 'DETAIL', // <= note the different label
            });

            // Now we need to find the "Next" button and enqueue the next page of results (if it exists)
            const nextButton = await page.$('a.pagination__next');
            if (nextButton) {
                await enqueueLinks({
                    selector: 'a.pagination__next',
                    label: 'CATEGORY', // <= note the same label
                });
            }
        } else {
            // This means we're on the start page, with no label.
            // On this page, we just want to enqueue all the category pages.

            await page.waitForSelector('.collection-block-item');
            await enqueueLinks({
                selector: '.collection-block-item',
                label: 'CATEGORY',
            });
        }
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Implementing Basic Web Crawling with Crawlee in TypeScript
DESCRIPTION: This code snippet demonstrates how to use Crawlee's BasicCrawler to download web pages and store their content. It uses the sendRequest utility function to make HTTP requests and saves the raw HTML and URLs to a default dataset.

LANGUAGE: typescript
CODE:
import { BasicCrawler, Dataset } from 'crawlee';

// Create a BasicCrawler instance
const crawler = new BasicCrawler({
    // Function called for each URL
    async requestHandler({ sendRequest, log }) {
        // Fetch the page HTML
        const { body } = await sendRequest();

        // Log the URL and HTML length
        log.info(`Crawled ${sendRequest.url} (${body.length} characters)`)

        // Save results to default dataset
        await Dataset.pushData({
            url: sendRequest.url,
            html: body,
        });
    },
});

// Define the list of URLs to crawl
await crawler.addRequests([
    'https://crawlee.dev',
    'https://example.com',
    'https://example.com/test',
]);

// Run the crawler
await crawler.run();

----------------------------------------

TITLE: Configuring SessionPool for CheerioCrawler in Crawlee
DESCRIPTION: This code snippet illustrates the setup of SessionPool with CheerioCrawler in Crawlee. It includes proxy configuration and session handling for Cheerio-based web scraping.

LANGUAGE: javascript
CODE:
{CheerioSource}

----------------------------------------

TITLE: Sitemap Crawling with Cheerio
DESCRIPTION: Implementation of sitemap crawling using Cheerio Crawler, which is suitable for static content scraping. Uses downloadListOfUrls utility to fetch sitemap URLs and processes them using Cheerio's lightweight DOM parsing.

LANGUAGE: javascript
CODE:
{CheerioSource}

----------------------------------------

TITLE: Customizing Browser Fingerprints with PuppeteerCrawler in Crawlee
DESCRIPTION: This code snippet shows how to customize browser fingerprints using PuppeteerCrawler in Crawlee. It configures specific options for fingerprint generation, including browser, operating system, and screen size.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    // ... other options
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                browsers: ["chrome"],
                devices: ["desktop"],
                operatingSystems: ["windows"],
                locales: ["en-US"],
                screens: ["1920x1080"],
            },
        },
    },
});

----------------------------------------

TITLE: Implementing Skip Navigation with PlaywrightCrawler
DESCRIPTION: Shows how to use Request#skipNavigation option with PlaywrightCrawler to efficiently handle CDN resources. The example demonstrates fetching and saving images directly without full page navigation while continuing to crawl other pages normally.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request, sendRequest, log }) {
        // Get all image elements on the page
        const images = await page.$$eval('img', (imgs) => {
            return imgs.map((img) => ({
                url: img.src,
                alt: img.alt,
            }));
        });

        // For each image, create a new Request with skipNavigation enabled
        for (const image of images) {
            // We create a new request for each image
            // with skipNavigation enabled
            const imageBuffer = await sendRequest({
                url: image.url,
                skipNavigation: true, // <-- this is the important part
            });

            // Store the image somewhere
            await Dataset.pushData({
                url: image.url,
                alt: image.alt,
                image: imageBuffer.body,
            });
        }

        // Continue crawling the website
        const links = await page.$$eval('a', (els) => els.map((el) => el.href));
        await crawler.addRequests(links);
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Crawling Same Hostname Links with CheerioCrawler in Crawlee
DESCRIPTION: This snippet shows how to use CheerioCrawler to crawl links with the same hostname as the current page. It uses the 'same-hostname' strategy for enqueueLinks(), which is the default behavior.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks, log }) {
        log.info('Enqueueing same hostname links.');
        await enqueueLinks({
            strategy: EnqueueStrategy.SameHostname,
            // You can also use the string 'same-hostname'
            // strategy: 'same-hostname',
            // Or you can omit the strategy option entirely
            // as it is the default behavior
        });
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Multi-stage Docker Build Configuration for Crawlee Actor
DESCRIPTION: Defines a multi-stage Docker build process that first builds the application in a builder stage and then creates a optimized production image. The configuration handles dependency installation, source code compilation, and sets up the runtime environment while maintaining efficient layer caching and minimal image size.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:20 AS builder

COPY package*.json ./

RUN npm install --include=dev --audit=false

COPY . ./

RUN npm run build

FROM apify/actor-node:20

COPY --from=builder /usr/src/app/dist ./dist

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm run start:prod --silent

----------------------------------------

TITLE: Demonstrating Key-Value Store Operations in Crawlee
DESCRIPTION: This snippet shows basic operations for working with key-value stores in Crawlee, including getting input, setting output, opening a named store, and performing read/write operations.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';

// Get the INPUT from the default key-value store
const input = await KeyValueStore.getInput();

// Write the OUTPUT to the default key-value store
await KeyValueStore.setValue('OUTPUT', { myResult: 123 });

// Open a named key-value store
const store = await KeyValueStore.open('some-name');

// Write a record to the named key-value store.
// JavaScript object is automatically converted to JSON,
// strings and binary buffers are stored as they are
await store.setValue('some-key', { foo: 'bar' });

// Read a record from the named key-value store.
// Note that JSON is automatically parsed to a JavaScript object,
// text data is returned as a string, and other data is returned as binary buffer
const value = await store.getValue('some-key');

// Delete a record from the named key-value store
await store.setValue('some-key', null);

----------------------------------------

TITLE: Handling JSON Responses with BasicCrawler in TypeScript
DESCRIPTION: This snippet shows how to configure sendRequest to handle JSON responses. It sets the responseType option to 'json', allowing for easy parsing of JSON data returned from the request.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ responseType: 'json' });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Crawling Sitemap with Cheerio Crawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use Cheerio Crawler to crawl a sitemap. It uses the Sitemap utility to download and process sitemap URLs, then crawls each URL using CheerioC rawler. The crawler extracts the page title and saves it to the dataset.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';
import { Sitemap } from '@crawlee/utils';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks, log }) {
        const title = $('title').text();
        log.info(`Title of ${request.url} is '${title}'`);

        await Dataset.pushData({
            url: request.url,
            title,
        });
    },
});

const sitemapUrl = 'https://crawlee.dev/sitemap.xml';

const sitemap = await Sitemap.load(sitemapUrl);
await crawler.run(sitemap.urls);

----------------------------------------

TITLE: Using actor-node-puppeteer-chrome Docker Image
DESCRIPTION: Shows how to use the Apify Docker image that includes Puppeteer and Chrome, suitable for CheerioCrawler and PuppeteerCrawler.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-puppeteer-chrome:16

----------------------------------------

TITLE: Scraping Dynamic Content with PlaywrightCrawler
DESCRIPTION: This snippet shows how to use PlaywrightCrawler to scrape JavaScript-rendered content. Playwright automatically waits for elements to appear before scraping.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        // Extract text content of the first actor card
        const actorText = await page.textContent('.ActorStoreItem');
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Configuring Header Generation in BasicCrawler
DESCRIPTION: This snippet demonstrates how to configure header generation options in BasicCrawler. It shows how to set specific devices, locales, operating systems, and browsers for generating browser-like headers in requests.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            headerGeneratorOptions: {
                devices: ['mobile', 'desktop'],
                locales: ['en-US'],
                operatingSystems: ['windows', 'macos', 'android', 'ios'],
                browsers: ['chrome', 'edge', 'firefox', 'safari'],
            },
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Building Docker Image for Crawlee Actor
DESCRIPTION: This Dockerfile specifies the steps to create a Docker image for running a Crawlee actor. It uses the apify/actor-node:16 base image, installs NPM packages, copies the source code, and sets the command to run the actor.

LANGUAGE: Dockerfile
CODE:
FROM apify/actor-node:16

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm start --silent

----------------------------------------

TITLE: Comparing Cheerio and Browser JavaScript DOM Manipulation
DESCRIPTION: This snippet demonstrates how to perform common DOM operations using both Cheerio and plain browser JavaScript. It shows how to select the title element and extract its text, as well as how to find all href links on a page.

LANGUAGE: typescript
CODE:
// Return the text content of the <title> element.
document.querySelector('title').textContent; // plain JS
$('title').text(); // Cheerio

// Return an array of all 'href' links on the page.
Array.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS
$('[href]')
    .map((i, el) => $(el).attr('href'))
    .get(); // Cheerio

----------------------------------------

TITLE: Initializing ProxyConfiguration in Crawlee
DESCRIPTION: This snippet demonstrates how to create a ProxyConfiguration instance with custom proxy URLs and obtain a new proxy URL.

LANGUAGE: javascript
CODE:
import { ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ]
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Scraping Dynamic Content with PlaywrightCrawler
DESCRIPTION: This snippet shows how to use PlaywrightCrawler to scrape JavaScript-rendered content, demonstrating Playwright's automatic waiting for elements to appear.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        // Extract text content of the first actor card
        const actorText = await page.textContent('.ActorStoreItem');
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Configuring SessionPool with PlaywrightCrawler in Crawlee
DESCRIPTION: This example demonstrates how to set up and use SessionPool with PlaywrightCrawler in Crawlee. It includes configuration for proxy usage and session pool options.

LANGUAGE: js
CODE:
import { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new PlaywrightCrawler({
    // Use the proxy configuration
    proxyConfiguration,
    // Set up the session pool options
    sessionPoolOptions: {
        maxPoolSize: 100,
    },
    // This function is called for each URL
    async requestHandler({ session, request, page }) {
        const title = await page.title();
        // ...
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Saving Data to Default Dataset in Crawlee
DESCRIPTION: This code snippet demonstrates how to save scraped data to the default dataset using Crawlee. It shows the process of creating a CheerioCrawler, defining the request handler, and pushing data to the dataset.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks, log }) {
        const title = $('title').text();
        log.info(`Title of ${request.url} is '${title}'`);

        await Dataset.pushData({
            url: request.url,
            title,
        });

        await enqueueLinks();
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Sanity Check Crawler with Playwright
DESCRIPTION: Creates a basic Playwright crawler to visit the start URL and print the text content of all category cards. This code helps verify the initial setup and page structure assumptions.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page }) {
        const categories = await page.$$('.collection-block-item');
        for (const category of categories) {
            console.log(await category.textContent());
        }
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Configuring SessionPool with CheerioCrawler in Crawlee
DESCRIPTION: This snippet illustrates the configuration of SessionPool with CheerioCrawler in Crawlee. It shows how to set up proxy configuration and handle blocked sessions.

LANGUAGE: js
CODE:
import { CheerioCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new CheerioCrawler({
    proxyConfiguration,
    async requestHandler({ $, request, session }) {
        const title = $('title').text();
        console.log(`The title of ${request.url} is: ${title}`);

        const body = $('body').text();
        if (body.includes('blocked')) {
            session.retire();
        }
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Configuring Crawlee with Custom Configuration
DESCRIPTION: Example of creating and using a custom Configuration instance for Crawlee, which is then passed to the crawler.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Create new configuration
const config = new Configuration({
    // Set the 'persistStateIntervalMillis' option to 10 seconds
    persistStateIntervalMillis: 10_000,
});

// Now we need to pass the configuration to the crawler
const crawler = new CheerioCrawler({}, config);

crawler.router.addDefaultHandler(async ({ request }) => {
    // for the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // for the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Crawling Specific Links with CheerioCrawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use CheerioCrawler to crawl specific links on a website. It uses the globs property in the enqueueLinks() method to filter links based on patterns, and processes the crawled data.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}`);

        await Dataset.pushData({
            title,
            url: request.url,
        });

        await enqueueLinks({
            globs: ['https://crawlee.dev/**'],
            label: 'DETAIL',
        });
    },
});

await crawler.run(['https://crawlee.dev']);


----------------------------------------

TITLE: Configuring Request Rate Limiting in Crawlee
DESCRIPTION: Demonstrates how to set the maximum number of requests per minute for a CheerioCrawler instance. This helps control the crawler's request rate to avoid overwhelming target websites.

LANGUAGE: javascript
CODE:
const crawler = new CheerioCrawler({
    // This will limit our crawler to doing 100 requests per minute
    maxRequestsPerMinute: 100,
    // ... other options
});

----------------------------------------

TITLE: Demonstrating Key-Value Store Operations in Crawlee
DESCRIPTION: This code snippet shows basic operations for working with key-value stores in Crawlee, including getting input, setting output, opening named stores, and performing CRUD operations on key-value pairs.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';

// Get the INPUT from the default key-value store
const input = await KeyValueStore.getInput();

// Write the OUTPUT to the default key-value store
await KeyValueStore.setValue('OUTPUT', { myResult: 123 });

// Open a named key-value store
const store = await KeyValueStore.open('some-name');

// Write a record to the named key-value store.
// JavaScript object is automatically converted to JSON,
// strings and binary buffers are stored as they are
await store.setValue('some-key', { foo: 'bar' });

// Read a record from the named key-value store.
// Note that JSON is automatically parsed to a JavaScript object,
// text data is returned as a string, and other data is returned as binary buffer
const value = await store.getValue('some-key');

// Delete a record from the named key-value store
await store.setValue('some-key', null);

----------------------------------------

TITLE: Scraping Dynamic Content with PlaywrightCrawler
DESCRIPTION: This snippet shows how to use PlaywrightCrawler to scrape JavaScript-rendered content. Playwright automatically waits for elements to appear before scraping.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        // Extract text content of the first actor card
        const actorText = await page.textContent('.ActorStoreItem');
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Implementing Basic Web Crawler with Crawlee
DESCRIPTION: Demonstrates how to create a basic web crawler using Crawlee's BasicCrawler. The script downloads web pages using HTTP requests via the sendRequest utility function and stores the raw HTML and URLs in a default dataset directory.

LANGUAGE: typescript
CODE:
import { BasicCrawler, Dataset } from 'crawlee';

// Create a crawler
const crawler = new BasicCrawler({
    // Function called for each URL
    async requestHandler({ sendRequest, log }) {
        const { body } = await sendRequest();
        log.info('Length of downloaded page:', body.length);
        await Dataset.pushData({
            url: request.url,
            html: body,
        });
    },
});

// Add URLs to crawler
await crawler.addRequests([
    'http://example.com/page-1',
    'http://example.com/page-2',
    'http://example.com/page-3',
]);

// Run the crawler
await crawler.run();

----------------------------------------

TITLE: Configuring Crawlee using crawlee.json
DESCRIPTION: This snippet shows how to create a crawlee.json file to set configuration options for Crawlee. It demonstrates setting the persistStateIntervalMillis and logLevel options.

LANGUAGE: json
CODE:
{
  "persistStateIntervalMillis": 10000,
  "logLevel": "DEBUG"
}

----------------------------------------

TITLE: Customizing Browser Fingerprints with PuppeteerCrawler in Crawlee
DESCRIPTION: This snippet shows how to customize browser fingerprints using PuppeteerCrawler in Crawlee. It configures specific options for fingerprint generation, including browser, operating system, and screen size.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    // ...
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                browsers: [
                    { name: 'firefox', minVersion: 80 },
                    { name: 'chrome', minVersion: 87 },
                ],
                operatingSystems: ['windows', 'linux'],
                screens: ['1920x1080', '1440x900'],
            },
        },
    },
});

----------------------------------------

TITLE: Configuring Crawlee using crawlee.json
DESCRIPTION: This snippet shows how to create a crawlee.json file to set configuration options for Crawlee. It demonstrates setting the persistStateIntervalMillis and logLevel options.

LANGUAGE: json
CODE:
{
  "persistStateIntervalMillis": 10000,
  "logLevel": "DEBUG"
}

----------------------------------------

TITLE: Customizing Browser Fingerprints with PlaywrightCrawler
DESCRIPTION: This snippet demonstrates how to customize browser fingerprints when using PlaywrightCrawler in Crawlee. It shows how to specify browser, operating system, and other parameters for fingerprint generation.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    // ...
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                browsers: [
                    { name: 'firefox', minVersion: 88 },
                    { name: 'chrome', minVersion: 90 },
                ],
                devices: [
                    'desktop',
                ],
                operatingSystems: [
                    'windows',
                ],
            },
        },
    },
});

----------------------------------------

TITLE: Building a CheerioCrawler with RequestQueue in Crawlee
DESCRIPTION: Shows how to create a CheerioCrawler instance, initialize it with a RequestQueue, and define a request handler to extract the page title. This example demonstrates the basic structure of a Crawlee crawler.

LANGUAGE: typescript
CODE:
import { RequestQueue, CheerioCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

const crawler = new CheerioCrawler({
    requestQueue,
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run();

----------------------------------------

TITLE: Customizing Browser Fingerprints with PlaywrightCrawler in Crawlee
DESCRIPTION: This code snippet demonstrates how to customize browser fingerprints for PlaywrightCrawler in Crawlee. It sets specific options for the fingerprint generation, including browser, operating system, and screen size.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                browsers: ["chrome"],
                operatingSystems: ["windows"],
                devices: ["desktop"],
                locales: ["en-US"],
                screen: {
                    minWidth: 1920,
                    minHeight: 1080,
                },
            },
        },
    },
    // ...
});

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Router - JavaScript
DESCRIPTION: Sets up the main crawler instance using PlaywrightCrawler with a router for request handling and configures logging level.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

// This is better set with CRAWLEE_LOG_LEVEL env var
// or a configuration option. This is just for show 😈
log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    // Instead of the long requestHandler with
    // if clauses we provide a router instance.
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Capturing Screenshots with PuppeteerCrawler Page API
DESCRIPTION: Implementation showing how to capture screenshots across multiple pages using PuppeteerCrawler with page.screenshot(). Screenshots are saved to a key-value store using URL-based keys.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, KeyValueStore } from '@crawlee/puppeteer';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // Create key from url
        const key = request.url.replace(/[:/]/g, '_');

        // Capture screenshot
        const screenshot = await page.screenshot();

        // Save screenshot to named key-value store
        await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Configuration in JavaScript
DESCRIPTION: This snippet shows how to initialize a PlaywrightCrawler with a custom Configuration to disable storage persistence. It sets up the crawler with a router for request handling and defines start URLs.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: JSON Response Handling
DESCRIPTION: Shows how to configure sendRequest to handle JSON responses instead of default text.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ responseType: 'json' });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Fetching HTML from a Single URL using got-scraping in JavaScript
DESCRIPTION: This snippet demonstrates how to use the got-scraping package to fetch the HTML content of a specified URL. It uses Crawlee's CheerioCrawler and processes the fetched data to extract specific information.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';
import { gotScraping } from 'got-scraping';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        const h1 = $('h1').text();

        await Dataset.pushData({
            title,
            h1,
            url: request.url
        });
    }
});

const url = 'https://crawlee.dev';

const { body: html } = await gotScraping(url);

await crawler.run([{
    url,
    userData: {
        html
    }
}]);

console.log('Crawler finished.');

----------------------------------------

TITLE: Implementing Custom HTTP Client with Fetch API in TypeScript
DESCRIPTION: This code snippet demonstrates a skeleton implementation of a custom HTTP client using the standard fetch interface. It conforms to the BaseHttpClient interface required by Crawlee.

LANGUAGE: typescript
CODE:
import { BaseHttpClient, HttpResponse } from '@crawlee/core';

class CustomHttpClient implements BaseHttpClient {
    async request(url: string, options?: RequestInit): Promise<HttpResponse> {
        const response = await fetch(url, options);

        return {
            body: await response.text(),
            statusCode: response.status,
            headers: Object.fromEntries(response.headers.entries()),
            url: response.url,
        };
    }

    async destroy() {
        // Clean up any resources
    }
}


----------------------------------------

TITLE: Express Server Integration for GCP Cloud Run
DESCRIPTION: Complete setup showing how to wrap the Crawlee crawler in an Express server for GCP Cloud Run deployment. Includes server initialization, route handling, and proper port configuration using environment variables.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import express from 'express';
const app = express();

const startUrls = ['https://crawlee.dev'];

app.get('/', async (req, res) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));
    
    await crawler.run(startUrls);    

    return res.send(await crawler.getData());
});

app.listen(parseInt(process.env.PORT) || 3000);

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Router in JavaScript
DESCRIPTION: Sets up the main crawler instance using PlaywrightCrawler with a router for request handling. Demonstrates proper logging configuration and crawler initialization.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

// This is better set with CRAWLEE_LOG_LEVEL env var
// or a configuration option. This is just for show 😈
log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    // Instead of the long requestHandler with
    // if clauses we provide a router instance.
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Basic Got Scraping Usage with BasicCrawler
DESCRIPTION: Demonstrates how to use sendRequest function with BasicCrawler for making HTTP requests. The example shows the basic setup for handling requests and logging responses.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest();
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Crawling Sitemap with Cheerio Crawler in Crawlee
DESCRIPTION: This code snippet demonstrates how to use Cheerio Crawler to crawl a sitemap. It utilizes the Sitemap utility class to download and process the sitemap, then uses CheerioScraper to crawl the extracted URLs.

LANGUAGE: javascript
CODE:
import { CheerioScraper, createCheerioRouter } from 'crawlee';
import { Sitemap } from '@crawlee/utils';

const router = createCheerioRouter();

router.addDefaultHandler(async ({ enqueueLinks, log }) => {
    log.info('Crawling!!');
    await enqueueLinks();
});

const crawler = new CheerioScraper({
    requestHandler: router,
});

const sitemap = new Sitemap({ url: 'https://crawlee.dev/sitemap.xml' });
await sitemap.download();

const urls = await sitemap.getURLs();
await crawler.run(urls);


----------------------------------------

TITLE: Simplified CheerioCrawler Setup in Crawlee
DESCRIPTION: This snippet shows a more concise way to set up a CheerioCrawler in Crawlee. It uses the crawler's built-in RequestQueue and the run method to add URLs directly, simplifying the initialization process.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Complete Store Crawling Implementation
DESCRIPTION: Full implementation of a PlaywrightCrawler that handles both category and product detail pages, including pagination handling and different request labels.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        if (request.label === 'DETAIL') {
            // We're not doing anything with the details yet.
        } else if (request.label === 'CATEGORY') {
            // We are now on a category page. We can use this to paginate through and enqueue all products,
            // as well as any subsequent pages we find

            await page.waitForSelector('.product-item > a');
            await enqueueLinks({
                selector: '.product-item > a',
                label: 'DETAIL', // <= note the different label
            });

            // Now we need to find the "Next" button and enqueue the next page of results (if it exists)
            const nextButton = await page.$('a.pagination__next');
            if (nextButton) {
                await enqueueLinks({
                    selector: 'a.pagination__next',
                    label: 'CATEGORY', // <= note the same label
                });
            }
        } else {
            // This means we're on the start page, with no label.
            // On this page, we just want to enqueue all the category pages.

            await page.waitForSelector('.collection-block-item');
            await enqueueLinks({
                selector: '.collection-block-item',
                label: 'CATEGORY',
            });
        }
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Configuring PlaywrightCrawler with Firefox in Crawlee
DESCRIPTION: This code snippet demonstrates how to set up PlaywrightCrawler to use a headless Firefox browser. It includes configuration for the browser, request handling, and data processing.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';
import { firefox } from 'playwright';

const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        launchOptions: {
            launcher: firefox,
        },
    },
    async requestHandler({ page, request, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.loadedUrl} is '${title}'`);

        await Dataset.pushData({
            url: request.loadedUrl,
            title,
        });

        await enqueueLinks();
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Configuring SessionPool for PuppeteerCrawler in Crawlee
DESCRIPTION: This code example illustrates how to set up SessionPool with PuppeteerCrawler in Crawlee. It covers proxy configuration and session management for Puppeteer-based browser automation.

LANGUAGE: javascript
CODE:
{PuppeteerSource}

----------------------------------------

TITLE: Configuring SessionPool with BasicCrawler in Crawlee
DESCRIPTION: This snippet demonstrates how to set up and use SessionPool with BasicCrawler in Crawlee. It includes configuration for proxy usage and session pool options.

LANGUAGE: js
CODE:
import { BasicCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new BasicCrawler({
    // Use the proxy configuration
    proxyConfiguration,
    // Set up the session pool options
    sessionPoolOptions: {
        maxPoolSize: 100,
    },
    // This function is called for each URL
    async requestHandler({ session, request, sendRequest }) {
        const { response, body } = await sendRequest();
        // ...
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Crawling Multiple URLs with Cheerio Crawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use Cheerio Crawler to crawl a list of specified URLs. It extracts the title from each page and stores the results.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks, log }) {
        const title = $('title').text();
        log.info(`Title of ${request.url} is '${title}'`);

        await Dataset.pushData({
            url: request.url,
            title,
        });
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run([
    'https://crawlee.dev',
    'https://apify.com',
    'https://sdk.apify.com',
]);


----------------------------------------

TITLE: Configuring Dockerfile for Crawlee Project with Node.js and Playwright
DESCRIPTION: This Dockerfile sets up a container for a Crawlee project. It uses a base image with Node.js and Playwright, installs npm dependencies excluding dev and optional packages, and copies the project files. The final command runs the project using npm start.

LANGUAGE: Dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16

COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD npm start --silent

----------------------------------------

TITLE: Standalone SessionPool Usage in Crawlee
DESCRIPTION: This code snippet demonstrates standalone usage of SessionPool in Crawlee for managing sessions manually. It includes configuration for proxy rotation and session management without using a crawler.

LANGUAGE: js
CODE:
import { SessionPool, ProxyConfiguration, sleep } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const sessionPool = await SessionPool.open();

// Create a new session
const session = await sessionPool.getSession();

// Use the session...

// Mark the session as working (successful)
session.markGood();

// Or retire the session on error
// session.markBad();

// Close the session pool
await sessionPool.teardown();

----------------------------------------

TITLE: Crawling Specific Links with CheerioCrawler in JavaScript
DESCRIPTION: This code snippet shows how to set up a CheerioCrawler to crawl specific links on a website. It uses the 'globs' option to filter links based on patterns and demonstrates how to process the crawled data.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        await Dataset.pushData({ title, url: request.url });

        // Only add links to articles
        await enqueueLinks({
            globs: ['https://blog.apify.com/page/*', 'https://blog.apify.com/author/*'],
        });
    },
    maxRequestsPerCrawl: 20, // Limit the number of requests
});

await crawler.run(['https://blog.apify.com/']);


----------------------------------------

TITLE: Extracting All Links from a Page using Cheerio
DESCRIPTION: This code snippet shows how to use Cheerio to find all <a> elements with an href attribute on a page and extract their href values into an array.

LANGUAGE: javascript
CODE:
$('a[href]')
    .map((i, el) => $(el).attr('href'))
    .get();

----------------------------------------

TITLE: Customizing Browser Fingerprints with PlaywrightCrawler in Crawlee
DESCRIPTION: This code snippet demonstrates how to customize browser fingerprints using PlaywrightCrawler in Crawlee. It sets specific options for the fingerprint generation, including browser, operating system, and screen size.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    // ...
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                browsers: [
                    { name: 'firefox', minVersion: 88 },
                    { name: 'chrome', minVersion: 94 },
                ],
                devices: [
                    'desktop',
                ],
                operatingSystems: [
                    'windows',
                ],
                locales: ['en-US', 'en-GB'],
                // screen size is required option
                screenWidth: 1920,
                screenHeight: 1080,
            },
        },
    },
});

----------------------------------------

TITLE: Complete AWS Lambda Handler for Cheerio Crawler with Data Return
DESCRIPTION: This is the final version of the Lambda handler function. It initializes the Cheerio Crawler, runs it, and returns the scraped data as the Lambda function's response.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);

    return {
        statusCode: 200,
        body: await crawler.getData(),
    }
};

----------------------------------------

TITLE: Initializing RequestQueue and Adding Requests in Crawlee
DESCRIPTION: This snippet shows how to create a RequestQueue instance and add a URL to it for crawling. It demonstrates the basic setup for queueing URLs in a Crawlee crawler.

LANGUAGE: typescript
CODE:
import { RequestQueue } from 'crawlee';

// First you create the request queue instance.
const requestQueue = await RequestQueue.open();
// And then you add one or more requests to it.
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

----------------------------------------

TITLE: Scraping and Parsing Product Price with Playwright
DESCRIPTION: This code demonstrates how to locate, extract, and parse the current price of a product using Playwright and string manipulation.

LANGUAGE: javascript
CODE:
const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

----------------------------------------

TITLE: Initializing ProxyConfiguration in Crawlee
DESCRIPTION: This snippet demonstrates how to create a ProxyConfiguration instance with custom proxy URLs. It shows the basic setup for using proxies in Crawlee.

LANGUAGE: javascript
CODE:
import { ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ]
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Configuring Header Generation with sendRequest in Crawlee
DESCRIPTION: Shows how to configure header generation options with the sendRequest function in a BasicCrawler's requestHandler.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            headerGeneratorOptions: {
                devices: ['mobile', 'desktop'],
                locales: ['en-US'],
                operatingSystems: ['windows', 'macos', 'android', 'ios'],
                browsers: ['chrome', 'edge', 'firefox', 'safari'],
            },
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Router in JavaScript
DESCRIPTION: Sets up the main crawler instance using PlaywrightCrawler with a router for request handling and configures logging level for debugging.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

// This is better set with CRAWLEE_LOG_LEVEL env var
// or a configuration option. This is just for show 😈
log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    // Instead of the long requestHandler with
    // if clauses we provide a router instance.
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Configuring SessionPool with BasicCrawler in Crawlee
DESCRIPTION: This snippet demonstrates how to set up and use SessionPool with BasicCrawler in Crawlee. It includes configuration for proxy usage and session pool management.

LANGUAGE: js
CODE:
import { BasicCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new BasicCrawler({
    // Activates the Session pool
    useSessionPool: true,
    // Overrides default Session pool configuration
    sessionPoolOptions: {
        maxPoolSize: 100,
        sessionOptions: {
            // All sessions will use this proxy configuration
            proxyConfiguration,
        },
    },
    async requestHandler({ session, request }) {
        // ...
    },
});

await crawler.run();

----------------------------------------

TITLE: Crawling Multiple URLs with Puppeteer Crawler
DESCRIPTION: Implementation of a web crawler using Puppeteer Crawler for JavaScript-rendered pages. Requires the apify/actor-node-puppeteer-chrome image when running on Apify Platform.

LANGUAGE: javascript
CODE:
{PuppeteerSource}

----------------------------------------

TITLE: Automating GitHub Search Form with PuppeteerCrawler
DESCRIPTION: Demonstrates how to automate GitHub repository search by filling and submitting a form using PuppeteerCrawler. The code handles form input for search terms, repository owner, start date, and language selection, then extracts and saves the search results to a dataset.

LANGUAGE: typescript
CODE:
import { Dataset, PuppeteerCrawler, createPuppeteerRouter } from 'crawlee';

const router = createPuppeteerRouter();

router.addDefaultHandler(async ({ page, log }) => {
    log.info('Filling form...');

    // Move to search repositories
    await page.goto('https://github.com/search/advanced');

    // Fill the form
    await page.type('#search_keywords', 'language learning');
    await page.type('#search_user', 'microsoft');
    await page.type('#search_from', '2022-01-01');
    await page.select('#search_language', 'JavaScript');

    // Submit the form
    await Promise.all([
        page.waitForNavigation(),
        page.click('.btn-primary'),
    ]);

    // Extract the data
    const repositories = await page.$$eval('.repo-list-item', (elements) => elements.map((el) => ({
        title: el.querySelector('a').textContent.trim(),
        description: el.querySelector('p')?.textContent?.trim() || null,
        language: el.querySelector('[itemprop="programmingLanguage"]')?.textContent?.trim() || null,
        updated: el.querySelector('relative-time')?.getAttribute('datetime') || null,
    })));

    // Save the results
    await Dataset.pushData(repositories);
});

const crawler = new PuppeteerCrawler({
    requestHandler: router,
    // Uncomment this option to see the browser window.
    // headless: false,
    startUrls: [
        'https://github.com/search/advanced',
    ],
});

await crawler.run();

----------------------------------------

TITLE: Finding All Links on a Page with Cheerio
DESCRIPTION: This snippet demonstrates how to use Cheerio to find all <a> elements with an href attribute and extract the href values into an array.

LANGUAGE: javascript
CODE:
$('a[href]')
    .map((i, el) => $(el).attr('href'))
    .get();

----------------------------------------

TITLE: Configuring Playwright Crawler with Firefox Browser
DESCRIPTION: Example implementation of PlaywrightCrawler using Firefox browser in headless mode. This setup is specifically configured for use with the Apify Platform using the apify/actor-node-playwright-firefox Docker image.

LANGUAGE: javascript
CODE:
The code content would be sourced from playwright_crawler_firefox.ts which is not visible in the provided content

----------------------------------------

TITLE: Basic Usage of LinkeDOMCrawler
DESCRIPTION: Shows a simple example of how to use LinkeDOMCrawler to crawl a website and push data to a Dataset. This snippet demonstrates the creation of a crawler instance, defining a requestHandler, and running the crawler with a list of URLs.

LANGUAGE: javascript
CODE:
const crawler = new LinkeDOMCrawler({
    async requestHandler({ request, window }) {
        await Dataset.pushData({
            url: request.url,
            title: window.document.title,
        });
    },
});

await crawler.run([
    'http://crawlee.dev',
]);

----------------------------------------

TITLE: Installing and Authenticating with Apify CLI
DESCRIPTION: Commands to install the Apify CLI globally and log in with your API token

LANGUAGE: bash
CODE:
npm install -g apify-cli
apify login -t YOUR_API_TOKEN

----------------------------------------

TITLE: Skipping Navigation for Image Requests with PlaywrightCrawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use PlaywrightCrawler to crawl a website, identify image URLs, and fetch them directly without navigation. It utilizes the Request#skipNavigation option and sendRequest method to efficiently handle CDN-delivered images.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ request, page, sendRequest, log }) {
        log.info(`Processing ${request.url}`);

        // Extract image URLs from the page
        const imageUrls = await page.evaluate(() => {
            const imgElements = document.querySelectorAll('img');
            return Array.from(imgElements).map(img => img.src);
        });

        // Process each image URL
        for (const imageUrl of imageUrls) {
            // Create a new request for the image with skipNavigation set to true
            const imageRequest = {
                url: imageUrl,
                skipNavigation: true, // This is the key part!
            };

            // Fetch the image data without navigation
            const imageResponse = await sendRequest(imageRequest);

            // Save the image data to the default key-value store
            await Dataset.pushData({
                url: imageUrl,
                data: imageResponse.body,
            });
        }
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Creating AWS Lambda Handler for Cheerio Crawler
DESCRIPTION: Wraps the crawler initialization and execution in an AWS Lambda handler function for serverless deployment.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
};

----------------------------------------

TITLE: Crawling Links with CheerioCrawler in JavaScript/TypeScript
DESCRIPTION: Implementation showing how to use CheerioCrawler's enqueueLinks method with glob patterns to selectively crawl website links. Uses RequestQueue for managing crawl requests and demonstrates pattern-based link filtering.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        await Dataset.pushData({
            url: request.url,
            title,
        });

        // Only add links to the queue that match the glob pattern
        await enqueueLinks({
            globs: ['**/cultures/*'],
        });
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Implementing HTTP Crawler with Crawlee in TypeScript
DESCRIPTION: This code snippet demonstrates how to set up and use HttpCrawler from Crawlee to crawl URLs from a text file, make HTTP requests, and save the HTML content. It includes error handling and uses the Dataset class for data storage.

LANGUAGE: typescript
CODE:
import { Dataset, HttpCrawler, log } from 'crawlee';
import { readFile } from 'fs/promises';

// Create an instance of the HttpCrawler class
const crawler = new HttpCrawler({
    // Use the requestHandler to process each URL
    async requestHandler({ request, body, enqueueLinks, log }) {
        const title = body.match(/<title>([^<]*)<\/title>/)?.[1];
        log.info(`Title: ${title}`, { url: request.url });

        // Save results to dataset
        await Dataset.pushData({
            url: request.url,
            title,
            html: body,
        });

        // Add links found on page to the crawling queue
        await enqueueLinks();
    },

    // Uncomment this option to see the browser console output
    // headless: false,
});

// Define a list of start URLs
const urlList = await readFile('urls.txt', 'utf8')
    .then((content) => content.split('\n'))
    .catch((err) => {
        log.error('Could not read the URLs file', err);
        process.exit(1);
    });

// Add URLs to the crawling queue
await crawler.addRequests(urlList);

// Run the crawler
await crawler.run();

----------------------------------------

TITLE: Configuring SessionPool with CheerioCrawler in Crawlee
DESCRIPTION: This code demonstrates the setup and usage of SessionPool with CheerioCrawler in Crawlee. It includes configuration for proxy usage and session pool options.

LANGUAGE: js
CODE:
import { CheerioCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new CheerioCrawler({
    // Use the proxy configuration
    proxyConfiguration,
    // Set up the session pool options
    sessionPoolOptions: {
        maxPoolSize: 100,
    },
    // This function is called for each URL
    async requestHandler({ session, request, $ }) {
        const title = $('title').text();
        // ...
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Configuring SessionPool with PlaywrightCrawler in Crawlee
DESCRIPTION: This example demonstrates how to configure and use SessionPool with PlaywrightCrawler in Crawlee. It includes setup for proxy configuration and session pool management.

LANGUAGE: js
CODE:
import { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new PlaywrightCrawler({
    // Use the proxyConfiguration
    proxyConfiguration,
    async requestHandler({ session, page }) {
        const title = await page.title();
        console.log(`The title of ${session.id} is: ${title}`);
    },
});

await crawler.run(['http://example.com/1', 'http://example.com/2', 'http://example.com/3']);

----------------------------------------

TITLE: Crawling All Links with Puppeteer Crawler in Crawlee
DESCRIPTION: This snippet shows how to use Puppeteer Crawler in Crawlee to crawl all links on a website. It sets up a PuppeteerCrawler, uses the enqueueLinks() method to add new links to the queue, and demonstrates how to extract the page title using Puppeteer.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, enqueueLinks } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limit the crawler to only 10 requests
});

// Run the crawler with initial request
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Demonstrating Dataset Operations in Crawlee
DESCRIPTION: This code snippet illustrates basic operations for working with datasets in Crawlee, including pushing data to the default dataset, opening named datasets, and writing single or multiple rows of data.

LANGUAGE: javascript
CODE:
import { Dataset } from 'crawlee';

// Write a single row to the default dataset
await Dataset.pushData({ col1: 123, col2: 'val2' });

// Open a named dataset
const dataset = await Dataset.open('some-name');

// Write a single row
await dataset.pushData({ foo: 'bar' });

// Write multiple rows
await dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);

----------------------------------------

TITLE: Configuring Crawlee using global Configuration
DESCRIPTION: This snippet demonstrates how to use the global Configuration instance to set Crawlee options. It sets the persistStateIntervalMillis option and creates a CheerioCrawler.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Get the global configuration
const config = Configuration.getGlobalConfig();
// Set the 'persistStateIntervalMillis' option
// of global configuration to 10 seconds
config.set('persistStateIntervalMillis', 10_000);

// Note, that we are not passing the configuration to the crawler
// as it's using the global configuration
const crawler = new CheerioCrawler();

crawler.router.addDefaultHandler(async ({ request }) => {
    // For the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // For the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Automating GitHub Repository Search with PuppeteerCrawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use PuppeteerCrawler to automate a GitHub repository search. It fills out a search form with specific criteria, submits it, extracts the results, and saves them to a dataset. The crawler uses Puppeteer to interact with the page elements and handle navigation.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, Dataset } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, log }) {
        log.info('Navigating to GitHub');
        await page.goto('https://github.com/search/advanced');

        log.info('Filling in search form');
        await page.type('#adv_code_search input.js-advanced-search-input', 'apify');
        await page.type('#search_from', 'apify');
        await page.type('#search_date', '>2015-10-10');
        await page.select('#search_language', 'JavaScript');

        log.info('Submitting form');
        await Promise.all([
            page.waitForNavigation(),
            page.click('#search_form .js-advanced-search-submit'),
        ]);

        log.info('Extracting data from results');
        const results = await page.$$eval('.repo-list-item', (items) =>
            items.map((item) => ({
                url: item.querySelector('a').href,
                title: item.querySelector('a').textContent.trim(),
                description: item.querySelector('.mb-1').textContent.trim(),
            }))
        );

        log.info(`Found ${results.length} results`);
        await Dataset.pushData(results);
    },
});

await crawler.run(['https://github.com/search/advanced']);


----------------------------------------

TITLE: Disabling Browser Fingerprints with PuppeteerCrawler
DESCRIPTION: This snippet shows how to disable the use of browser fingerprints when using PuppeteerCrawler in Crawlee by setting the useFingerprints option to false.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    // ...
    browserPoolOptions: {
        useFingerprints: false,
    },
});

----------------------------------------

TITLE: Configuring PlaywrightCrawler with Firefox for Web Scraping in JavaScript
DESCRIPTION: This code snippet demonstrates how to set up and use PlaywrightCrawler with a headless Firefox browser for web scraping. It includes configuration for the browser, crawler settings, and a basic crawling function.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';
import { firefox } from 'playwright';

const crawler = new PlaywrightCrawler({
    // Use Firefox browser
    browserPoolOptions: {
        launchOptions: {
            launcher: firefox,
        },
    },
    async requestHandler({ page, request, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        // Save results to default dataset
        await Dataset.pushData({
            url: request.url,
            title,
        });

        // Extract links from the current page
        // and add them to the crawling queue.
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limit the number of requests
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Configuring Multi-stage Docker Build for Crawlee Actor
DESCRIPTION: Complete Dockerfile configuration that sets up a Crawlee actor using a multi-stage build process. The first stage handles dependency installation and project building, while the second stage creates an optimized runtime environment with minimal dependencies.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:20 AS builder

COPY package*.json ./

RUN npm install --include=dev --audit=false

COPY . ./

RUN npm run build

FROM apify/actor-node:20

COPY --from=builder /usr/src/app/dist ./dist

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm run start:prod --silent

----------------------------------------

TITLE: Setting Maximum Requests Per Crawl
DESCRIPTION: Example showing how to limit the number of requests a crawler will process using maxRequestsPerCrawl option.

LANGUAGE: typescript
CODE:
const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 20,
    // ...
});

----------------------------------------

TITLE: Interacting with React Calculator using JSDOMCrawler
DESCRIPTION: Demonstrates how to use JSDOMCrawler to interact with a React calculator application. The script navigates to the calculator, performs arithmetic operations by clicking buttons, and extracts the result.

LANGUAGE: typescript
CODE:
{JSDOMCrawlerRunScriptSource}

----------------------------------------

TITLE: Crawling All Links with Playwright Crawler in Crawlee
DESCRIPTION: This snippet illustrates how to use Playwright Crawler in Crawlee to crawl all links on a website. It employs the enqueueLinks() method to add new links to the RequestQueue and logs the title of each crawled page.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, enqueueLinks } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 20, // Limit to 20 requests
});

// Run the crawler with initial URL
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Implementing Basic Web Crawler with Crawlee
DESCRIPTION: A minimal example showing how to create a basic web crawler using Crawlee's BasicCrawler. The crawler performs HTTP requests to download web pages and stores their HTML content and URLs in a local dataset. Uses got-scraping internally for making HTTP requests.

LANGUAGE: typescript
CODE:
import { BasicCrawler, EnqueueLinksOptions } from 'crawlee';
import { BasicCrawlingContext } from 'crawlee';

// Create a crawler
const crawler = new BasicCrawler({
    // Define the request handler
    async requestHandler({ request, sendRequest, log }) {
        // Fetch the page HTML using the sendRequest utility
        const response = await sendRequest();
        
        // Store the HTML and URL to the default dataset
        await Dataset.pushData({
            url: request.url,
            html: response.body,
        });
        
        log.info(`Processed ${request.url}...`);
    },
});

// Add requests to the queue
await crawler.addRequests([
    { url: 'http://example.com/page-1' },
    { url: 'http://example.com/page-2' },
    { url: 'http://example.com/page-3' },
]);

// Run the crawler
await crawler.run();

----------------------------------------

TITLE: Initializing ProxyConfiguration in Crawlee
DESCRIPTION: This snippet demonstrates how to create a ProxyConfiguration instance with custom proxy URLs and obtain a new proxy URL.

LANGUAGE: javascript
CODE:
import { ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ]
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Accessing Page Title with Browser JavaScript and JSDOM
DESCRIPTION: Demonstrates how to retrieve the page title using browser JavaScript and JSDOM. This snippet highlights the similarity between browser and JSDOM APIs.

LANGUAGE: javascript
CODE:
// Return the page title
document.title; // browsers
window.document.title; // JSDOM

----------------------------------------

TITLE: Configuring Crawlee using crawlee.json
DESCRIPTION: This snippet shows how to use a crawlee.json file to set configuration options for Crawlee. It demonstrates setting the persistStateIntervalMillis and logLevel options.

LANGUAGE: json
CODE:
{
  "persistStateIntervalMillis": 10000,
  "logLevel": "DEBUG"
}

----------------------------------------

TITLE: Saving Data to Crawlee Datasets - JavaScript
DESCRIPTION: Example showing how to save data to a default dataset in Crawlee. The dataset is automatically created if it doesn't exist. Data can also be saved to custom datasets using Dataset.open().

LANGUAGE: javascript
CODE:
import { Dataset } from 'crawlee';

await Dataset.pushData({
    title: 'Crawlee & Cheerio Scraper',
    url: 'https://crawlee.dev',
    date: new Date(),
});

----------------------------------------

TITLE: Crawling Specific Links with CheerioCrawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use CheerioCrawler to crawl specific links on a website. It uses the globs property in the enqueueLinks() method to filter links based on patterns, and processes the crawled data.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Save results to default dataset
        await Dataset.pushData({
            url: request.url,
            title,
        });

        // Add all links from page to RequestQueue
        await enqueueLinks({
            globs: ['https://crawlee.dev/**'],
            exclude: ['.pdf'],
        });
    },
    maxRequestsPerCrawl: 20, // Limitation for only 20 requests (do not use if you want to crawl all links)
});

// Run the crawler
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Installing Apify SDK v1
DESCRIPTION: Installation commands for Apify SDK v1 with either Puppeteer or Playwright support

LANGUAGE: bash
CODE:
npm install apify puppeteer

LANGUAGE: bash
CODE:
npm install apify playwright

----------------------------------------

TITLE: GitHub Repository Search Form Automation using PuppeteerCrawler
DESCRIPTION: A script that uses PuppeteerCrawler to automate GitHub repository searches by filling form fields including search term, owner, date, and language. Results are saved to either Apify platform's dataset or local JSON files.

LANGUAGE: javascript
CODE:
import { Dataset, PuppeteerCrawler, log } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page }) {
        // Fill and submit the form
        await page.type('input[name="q"]', 'crawler');
        await page.type('#search_from', 'apify');
        await page.type('#search_date', '2015-01-01');
        await page.select('select#search_language', 'JavaScript');

        await Promise.all([
            page.waitForNavigation(),
            page.click('#form-submit'),
        ]);

        // Extract the data
        const repos = await page.$$eval('.repo-list-item', (elements) => {
            const data = [];
            for (const el of elements) {
                data.push({
                    title: el.querySelector('h3').innerText.trim(),
                    description: el.querySelector('p').innerText.trim(),
                    language: el.querySelector('[itemprop="programmingLanguage"]')?.innerText.trim(),
                    stars: Number(el.querySelector('.muted-link').innerText.trim()),
                    url: el.querySelector('h3 a').href,
                });
            }
            return data;
        });

        // Save the data
        await Dataset.pushData(repos);
    },
});

await crawler.run(['https://github.com/search']);


----------------------------------------

TITLE: Using RequestQueueV2 with CheerioCrawler
DESCRIPTION: Shows how to combine RequestQueueV2 with a crawler while enabling the request locking experiment.

LANGUAGE: typescript
CODE:
import { CheerioCrawler, RequestQueueV2 } from 'crawlee';

const queue = await RequestQueueV2.open('my-locking-queue');

const crawler = new CheerioCrawler({
    experiments: {
        requestLocking: true,
    },
    requestQueue: queue,
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

await crawler.run();

----------------------------------------

TITLE: Demonstrating Key-Value Store Operations in Crawlee
DESCRIPTION: This snippet shows basic operations of key-value stores in Crawlee, including getting input, setting output, opening a named store, and performing read/write operations.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';

// Get the INPUT from the default key-value store
const input = await KeyValueStore.getInput();

// Write the OUTPUT to the default key-value store
await KeyValueStore.setValue('OUTPUT', { myResult: 123 });

// Open a named key-value store
const store = await KeyValueStore.open('some-name');

// Write a record to the named key-value store.
// JavaScript object is automatically converted to JSON,
// strings and binary buffers are stored as they are
await store.setValue('some-key', { foo: 'bar' });

// Read a record from the named key-value store.
// Note that JSON is automatically parsed to a JavaScript object,
// text data is returned as a string, and other data is returned as binary buffer
const value = await store.getValue('some-key');

// Delete a record from the named key-value store
await store.setValue('some-key', null);

----------------------------------------

TITLE: Crawling Same Hostname Links in Crawlee
DESCRIPTION: Example demonstrating how to crawl links from the same hostname using CheerioCrawler. This is the default strategy that matches relative URLs and URLs pointing to the same hostname, excluding subdomains.

LANGUAGE: javascript
CODE:
{SameHostnameSource}

----------------------------------------

TITLE: Crawling Multiple URLs with Playwright Crawler in Crawlee
DESCRIPTION: This code snippet illustrates the use of the Playwright Crawler in Crawlee for crawling multiple URLs. It includes a tip for running the example on the Apify Platform using the correct Docker image.

LANGUAGE: javascript
CODE:
{PlaywrightSource}

----------------------------------------

TITLE: Configuring preNavigationHooks in LinkeDOMCrawler
DESCRIPTION: Demonstrates how to use preNavigationHooks to adjust gotOptions before navigation in LinkeDOMCrawler. This allows for customization of the HTTP request options.

LANGUAGE: javascript
CODE:
preNavigationHooks: [
    (crawlingContext, gotOptions) => {
        // ...
    },
]

----------------------------------------

TITLE: Crawling All Links with Puppeteer Crawler in Crawlee
DESCRIPTION: This snippet shows how to use Puppeteer Crawler in Crawlee to crawl all links on a website. It utilizes the enqueueLinks() method to add new links to the RequestQueue and logs the title of each crawled page.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, enqueueLinks } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 20, // Limit to 20 requests
});

// Run the crawler with initial URL
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Using Crawlee with crawlee.json configuration
DESCRIPTION: This JavaScript example demonstrates how to use Crawlee with configuration options set in crawlee.json. It creates a CheerioCrawler and uses sleep to simulate processing time.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, sleep } from 'crawlee';
// We are not importing nor passing
// the Configuration to the crawler.
// We are not assigning any env vars either.
const crawler = new CheerioCrawler();

crawler.router.addDefaultHandler(async ({ request }) => {
    // for the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // for the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Multiple URL Crawling with Puppeteer Crawler
DESCRIPTION: Implementation of a Puppeteer-based crawler for processing multiple URLs. Requires apify/actor-node-puppeteer-chrome image for Platform execution. Supports JavaScript rendering and browser automation.

LANGUAGE: javascript
CODE:
{PuppeteerSource}

----------------------------------------

TITLE: Crawling Sitemap with Playwright Crawler in Crawlee
DESCRIPTION: This code snippet illustrates how to use Playwright Crawler to crawl a sitemap. It employs the Sitemap utility class to download and process the sitemap, then uses PlaywrightScraper to crawl the extracted URLs.

LANGUAGE: javascript
CODE:
import { PlaywrightScraper, createPlaywrightRouter } from 'crawlee';
import { Sitemap } from '@crawlee/utils';

const router = createPlaywrightRouter();

router.addDefaultHandler(async ({ enqueueLinks, log }) => {
    log.info('Crawling!!');
    await enqueueLinks();
});

const crawler = new PlaywrightScraper({
    requestHandler: router,
});

const sitemap = new Sitemap({ url: 'https://crawlee.dev/sitemap.xml' });
await sitemap.download();

const urls = await sitemap.getURLs();
await crawler.run(urls);


----------------------------------------

TITLE: Configuring Advanced autoscaledPoolOptions in CheerioCrawler
DESCRIPTION: This snippet illustrates how to set advanced autoscaledPoolOptions in a CheerioCrawler for fine-tuning the autoscaling behavior of the crawler.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    autoscaledPoolOptions: {
        desiredConcurrency: 10,
        desiredConcurrencyRatio: 0.98,
        scaleUpStepRatio: 0.01,
        scaleDownStepRatio: 0.01,
        maybeRunIntervalSecs: 1,
        loggingIntervalSecs: 30,
        autoscaleIntervalSecs: 15,
    },
    // ...
});

----------------------------------------

TITLE: Scraping JavaScript-Rendered Content with PuppeteerCrawler
DESCRIPTION: This snippet demonstrates how to use PuppeteerCrawler to scrape JavaScript-rendered content, explicitly waiting for elements to appear before extracting data.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // Wait for the actor cards to render
        await page.waitForSelector('.ActorStoreItem');
        // Extract text content of the first actor card
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Cookie Jar Implementation
DESCRIPTION: Shows how to use a custom cookie jar with BasicCrawler for managing cookies.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';
import { CookieJar } from 'tough-cookie';

const cookieJar = new CookieJar();

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ cookieJar });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Initializing ProxyConfiguration in Crawlee
DESCRIPTION: Creates a new ProxyConfiguration instance with a list of proxy URLs and demonstrates how to get a new proxy URL.

LANGUAGE: javascript
CODE:
import { ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ]
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Implementing HTTP Crawler with Crawlee in JavaScript
DESCRIPTION: This code demonstrates how to use the HttpCrawler class from Crawlee to crawl a list of URLs. It reads URLs from a text file, processes each URL with a HTTP request, and saves the HTML content to a dataset. The crawler is configured with various options including maximum concurrency and request handling.

LANGUAGE: javascript
CODE:
import { HttpCrawler, Dataset } from 'crawlee';
import { readFile } from 'fs/promises';

// Create an instance of the HttpCrawler class
const crawler = new HttpCrawler({
    // Function called for each URL
    async requestHandler({ request, body, $ }) {
        const title = $('title').text();
        console.log(`Title of ${request.url} is: ${title}`);

        // Save results to dataset
        await Dataset.pushData({
            url: request.url,
            title,
            html: body,
        });
    },
    // Function to handle failed requests
    failedRequestHandler({ request }) {
        console.log(`Request ${request.url} failed too many times`);
    },
    maxRequestsPerCrawl: 20,
    maxConcurrency: 10,
});

// Assuming you have a file containing URLs, one per line
const content = await readFile('urls.txt', 'utf8');
const urls = content.trim().split('\n');

// Add URLs to the crawler's queue
await crawler.addRequests(urls);

// Run the crawler
await crawler.run();

----------------------------------------

TITLE: Setting Up Tiered Proxies in ProxyConfiguration for JavaScript
DESCRIPTION: Configures ProxyConfiguration with tiered proxy URLs. This setup allows automatic switching between different proxy tiers based on blocking behavior.

LANGUAGE: javascript
CODE:
const proxyConfiguration = new ProxyConfiguration({
    tieredProxyUrls: [
        [null], // At first, we try to connect without a proxy
        ['http://okay-proxy.com'],
        ['http://slightly-better-proxy.com', 'http://slightly-better-proxy-2.com'],
        ['http://very-good-and-expensive-proxy.com'],
    ]
});

----------------------------------------

TITLE: Crawling Sitemap with Cheerio Crawler
DESCRIPTION: Demonstrates sitemap crawling implementation using Cheerio Crawler in Crawlee. Uses downloadListOfUrls utility to fetch sitemap URLs.

LANGUAGE: typescript
CODE:
{CheerioSource}

----------------------------------------

TITLE: Explicit Request Queue Usage with Crawler in Crawlee
DESCRIPTION: Shows how to explicitly create and use a Request Queue with a Crawler in Crawlee. It demonstrates creating a queue, adding requests, and using it with a PuppeteerCrawler.

LANGUAGE: javascript
CODE:
import { RequestQueue, PuppeteerCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

const crawler = new PuppeteerCrawler({
    requestQueue,
    async requestHandler({ page, request, enqueueLinks }) {
        // Process the page...
        await enqueueLinks();
    },
});

await crawler.run();

----------------------------------------

TITLE: Session Pool Implementation with JSDOMCrawler
DESCRIPTION: Example demonstrating SessionPool configuration with JSDOMCrawler for DOM-based crawling

LANGUAGE: javascript
CODE:
{JSDOMSource}

----------------------------------------

TITLE: Session Management with CheerioCrawler
DESCRIPTION: Example of session management implementation with CheerioCrawler. Demonstrates how to handle sessions and proxy rotation in a Cheerio-based web scraping context.

LANGUAGE: javascript
CODE:
{CheerioSource}

----------------------------------------

TITLE: Configuring Express Server with PlaywrightCrawler for GCP Cloud Run in JavaScript
DESCRIPTION: This code sets up an Express server to handle HTTP requests for the PlaywrightCrawler. It creates a route that initializes the crawler, runs it with the specified start URLs, and returns the crawled data. The server listens on the port specified by the GCP environment variable.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import express from 'express';
const app = express();

const startUrls = ['https://crawlee.dev'];


app.get('/', async (req, res) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));
    
    await crawler.run(startUrls);    

    return res.send(await crawler.getData());
});

app.listen(parseInt(process.env.PORT) || 3000);

----------------------------------------

TITLE: Crawling Sitemap with Puppeteer Crawler in JavaScript
DESCRIPTION: This code snippet shows how to use Puppeteer Crawler to download and crawl URLs from a sitemap. It uses the downloadListOfUrls utility and processes each URL to extract the title of the page using Puppeteer.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from '@crawlee/puppeteer';
import { downloadListOfUrls } from '@crawlee/utils';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        const title = await page.title();
        console.log(`The title of "${request.url}" is: ${title}`);
    },
});

const { log } = crawler;

const listOfUrls = await downloadListOfUrls({
    url: 'https://crawlee.dev/sitemap.xml',
});

await crawler.run(listOfUrls);

log.info('Crawler finished.');

----------------------------------------

TITLE: Session Pool Implementation with CheerioCrawler
DESCRIPTION: Example showing SessionPool usage with CheerioCrawler for managing sessions during HTML parsing

LANGUAGE: javascript
CODE:
{CheerioSource}

----------------------------------------

TITLE: Installing TypeScript Compiler for Crawlee Project
DESCRIPTION: Command to install TypeScript as a development dependency for a Crawlee project.

LANGUAGE: shell
CODE:
npm install --dev typescript

----------------------------------------

TITLE: Browser Pool Configuration Example
DESCRIPTION: Shows how to configure BrowserPool with lifecycle hooks and options

LANGUAGE: javascript
CODE:
const crawler = new Apify.PuppeteerCrawler({
    browserPoolOptions: {
        retireBrowserAfterPageCount: 10,
        preLaunchHooks: [
            async (pageId, launchContext) => {
                const { request } = crawler.crawlingContexts.get(pageId);
                if (request.userData.useHeadful === true) {
                    launchContext.launchOptions.headless = false;
                }
            }
        ]
    }
})

----------------------------------------

TITLE: Capturing Screenshots with PuppeteerCrawler using utils.puppeteer.saveSnapshot()
DESCRIPTION: This snippet shows how to capture screenshots of multiple web pages using PuppeteerCrawler with the utils.puppeteer.saveSnapshot() utility function. It sets up a crawler to visit multiple URLs and use the utility function to save snapshots of each page.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';
import { PuppeteerCrawler } from 'crawlee';

await Actor.init();

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, crawler }) {
        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);

        await crawler.utils.puppeteer.saveSnapshot(page);
    },
});

await crawler.run(['https://crawlee.dev', 'https://apify.com']);

await Actor.exit();

----------------------------------------

TITLE: Setting Up Tiered Proxies with ProxyConfiguration in JavaScript
DESCRIPTION: This snippet illustrates how to configure tiered proxy URLs for automatic switching based on blocking behavior. It demonstrates a progressive proxy usage strategy.

LANGUAGE: javascript
CODE:
const proxyConfiguration = new ProxyConfiguration({
    tieredProxyUrls: [
        [null], // At first, we try to connect without a proxy
        ['http://okay-proxy.com'],
        ['http://slightly-better-proxy.com', 'http://slightly-better-proxy-2.com'],
        ['http://very-good-and-expensive-proxy.com'],
    ]
});

----------------------------------------

TITLE: Crawling Sitemap with Playwright Crawler in JavaScript
DESCRIPTION: This code snippet illustrates how to use Playwright Crawler to download and crawl URLs from a sitemap. It employs the downloadListOfUrls utility and processes each URL to extract the title of the page using Playwright.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from '@crawlee/playwright';
import { downloadListOfUrls } from '@crawlee/utils';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        const title = await page.title();
        console.log(`The title of "${request.url}" is: ${title}`);
    },
});

const { log } = crawler;

const listOfUrls = await downloadListOfUrls({
    url: 'https://crawlee.dev/sitemap.xml',
});

await crawler.run(listOfUrls);

log.info('Crawler finished.');

----------------------------------------

TITLE: Configuring SessionPool with PuppeteerCrawler in Crawlee
DESCRIPTION: This snippet shows how to configure and use SessionPool with PuppeteerCrawler in Crawlee. It includes setup for proxy configuration and session pool options.

LANGUAGE: js
CODE:
import { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new PuppeteerCrawler({
    // Use the proxy configuration
    proxyConfiguration,
    // Set up the session pool options
    sessionPoolOptions: {
        maxPoolSize: 100,
    },
    // This function is called for each URL
    async requestHandler({ session, request, page }) {
        const title = await page.title();
        // ...
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Initializing ProxyConfiguration with Static Proxy List in JavaScript
DESCRIPTION: This snippet demonstrates how to create a ProxyConfiguration instance with a static list of proxy URLs. It shows how to rotate through provided proxies, including the option to use no proxy.

LANGUAGE: javascript
CODE:
import { ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ]
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Configuring Crawlee using global Configuration
DESCRIPTION: Example of using the global Configuration instance to set Crawlee options programmatically. It sets the persistStateIntervalMillis option and uses it with a CheerioCrawler.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Get the global configuration
const config = Configuration.getGlobalConfig();
// Set the 'persistStateIntervalMillis' option
// of global configuration to 10 seconds
config.set('persistStateIntervalMillis', 10_000);

// Note, that we are not passing the configuration to the crawler
// as it's using the global configuration
const crawler = new CheerioCrawler();

crawler.router.addDefaultHandler(async ({ request }) => {
    // For the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // For the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Scraping Dynamic Content with PuppeteerCrawler in Crawlee
DESCRIPTION: This snippet demonstrates using PuppeteerCrawler to scrape JavaScript-rendered content. Unlike Playwright, Puppeteer requires explicit waiting for elements to appear.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // Wait for the content to load
        await page.waitForSelector('.ActorStoreItem');
        // Extract text content of the first actor card
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Customizing Browser Fingerprints with PuppeteerCrawler
DESCRIPTION: Example showing how to customize browser fingerprints for PuppeteerCrawler. Demonstrates setting specific browser and operating system parameters to generate targeted fingerprints.

LANGUAGE: javascript
CODE:
{PuppeteerSource}

----------------------------------------

TITLE: Integrating AWS Chromium with Crawlee
DESCRIPTION: Implementation showing how to configure Crawlee to use AWS-compatible Chromium with proper launch options and hardware configurations.

LANGUAGE: javascript
CODE:
// For more information, see https://crawlee.dev/
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
    launchContext: {
        launchOptions: {
             executablePath: await aws_chromium.executablePath(),
             args: aws_chromium.args,
             headless: true
        }
    }
}, new Configuration({
    persistStorage: false,
}));

----------------------------------------

TITLE: Proxy Configuration Setup
DESCRIPTION: Example of configuring and using Apify Proxy with specific groups and country selection

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
    countryCode: 'US',
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: React Calculator Interaction with JSDOMCrawler
DESCRIPTION: Script that demonstrates interacting with a React calculator application using JSDOMCrawler. It performs basic arithmetic operations by simulating button clicks and extracts the result.

LANGUAGE: typescript
CODE:
import { JSDOMCrawler, Dataset } from 'crawlee';

const crawler = new JSDOMCrawler({
    async requestHandler({ window, enqueueLinks, log }) {
        const document = window.document;

        // Click calculator buttons
        document.querySelector('button[data-value="1"]')?.click();
        document.querySelector('button[data-value="+"]')?.click();
        document.querySelector('button[data-value="1"]')?.click();
        document.querySelector('button[data-value="="]')?.click();

        // Extract result
        const result = document.querySelector('.component-display')?.textContent;
        log.info(`Calculator result: ${result}`);
    },
});

// Add URLs to crawler
await crawler.run(['https://ahfarmer.github.io/calculator/']);

----------------------------------------

TITLE: Configuring Crawlee with Global Configuration
DESCRIPTION: Example of using the Configuration class to set global configuration options for Crawlee programmatically.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Get the global configuration
const config = Configuration.getGlobalConfig();
// Set the 'persistStateIntervalMillis' option
// of global configuration to 10 seconds
config.set('persistStateIntervalMillis', 10_000);

// Note, that we are not passing the configuration to the crawler
// as it's using the global configuration
const crawler = new CheerioCrawler();

crawler.router.addDefaultHandler(async ({ request }) => {
    // For the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // For the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Explicit Request Queue Usage with Crawler in Crawlee
DESCRIPTION: Shows how to explicitly create and use a Request Queue with a Crawler in Crawlee. This approach allows for more control over queue management.

LANGUAGE: javascript
CODE:
import { RequestQueue, PuppeteerCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

const crawler = new PuppeteerCrawler({
    requestQueue,
    async requestHandler({ page, request, enqueueLinks }) {
        await enqueueLinks();
    },
});

await crawler.run();

----------------------------------------

TITLE: Customizing Browser Fingerprints with PlaywrightCrawler
DESCRIPTION: Example showing how to customize browser fingerprints in PlaywrightCrawler using browserPoolOptions. This configuration specifies operating systems, browsers, and other fingerprint parameters.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        fingerprintOptions: {
            // Allows to specify the operating systems the fingerprints
            // should be generated for
            operatingSystems: ['windows', 'linux'],
            // Specifies the browser and its versions for generating fingerprints
            browsers: [
                { name: 'firefox', minVersion: 88 },
                { name: 'chrome', minVersion: 88 }
            ]
        }
    }
});

----------------------------------------

TITLE: Session Management with CheerioCrawler
DESCRIPTION: Example of session management implementation with CheerioCrawler. Demonstrates how to handle sessions and proxy rotation in a Cheerio-based web scraping context.

LANGUAGE: javascript
CODE:
{CheerioSource}

----------------------------------------

TITLE: Configuring Crawlee using crawlee.json
DESCRIPTION: Example of a crawlee.json file used to set global configuration options for Crawlee, such as persistStateIntervalMillis and logLevel.

LANGUAGE: json
CODE:
{
  "persistStateIntervalMillis": 10000,
  "logLevel": "DEBUG"
}

----------------------------------------

TITLE: Disabling Browser Fingerprints in PuppeteerCrawler
DESCRIPTION: This snippet shows how to disable browser fingerprints in PuppeteerCrawler by setting the useFingerprints option to false.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    // ...
    browserPoolOptions: {
        useFingerprints: false,
    },
});

----------------------------------------

TITLE: Using SessionPool with HttpCrawler in Crawlee
DESCRIPTION: This example shows how to set up SessionPool with HttpCrawler in Crawlee. It demonstrates proxy configuration, session management, and handling of retry attempts for failed requests.

LANGUAGE: js
CODE:
import { HttpCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new HttpCrawler({
    // The `useSessionPool` enables the `SessionPool` functionality for the crawler
    useSessionPool: true,
    // By default, `HttpCrawler` retries failed requests
    // so there's no need to do it manually
    maxRequestRetries: 5,
    // Configures the proxy to be used by the `SessionPool`
    proxyConfiguration,
    async requestHandler({ session, json }) {
        // Use `session.userData` to store custom data
        session.userData.numberOfRetries = session.userData.numberOfRetries ?? 0;

        const data = await json();
        // Process the data...

        // Increase the number of processed requests in the session
        session.userData.numberOfRetries++;
    },
    failedRequestHandler({ session }) {
        // Retire the proxy URL if we get blocked
        session.retire();
    },
});

await crawler.run([
    'https://api.example.com/v1/test',
]);

----------------------------------------

TITLE: Creating a Shared Request Queue with Locking Support in Crawlee
DESCRIPTION: This code snippet defines a function to initialize or retrieve a request queue that supports locking. It uses the Crawlee SDK and handles both local and Apify platform environments.

LANGUAGE: javascript
CODE:
import { RequestQueue } from 'crawlee';
import { Actor } from 'apify';

export async function getOrInitQueue(purgeIfExists = false) {
    const queue = await (Actor.isAtHome()
        ? Actor.openRequestQueue()
        : RequestQueue.open('default'));

    if (purgeIfExists) {
        await queue.drop();
    }

    return queue;
}

----------------------------------------

TITLE: Scraping JavaScript-Rendered Content with PlaywrightCrawler
DESCRIPTION: This snippet shows how to use PlaywrightCrawler to scrape JavaScript-rendered content, automatically waiting for elements to appear before extracting data.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        // Wait for the actor cards to render
        await page.waitForSelector('.ActorStoreItem');
        // Extract text content of the first actor card
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Capturing Screenshots with PuppeteerCrawler using page.screenshot()
DESCRIPTION: This snippet demonstrates how to capture screenshots of multiple web pages using PuppeteerCrawler with page.screenshot(). It creates a PuppeteerCrawler instance, defines a handler function to capture screenshots, and starts the crawler with a list of URLs.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, KeyValueStore } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);

        const screenshotBuffer = await page.screenshot();

        // Save screenshot to default key-value store
        const key = request.url.replace(/[:/]/g, '_');
        await KeyValueStore.setValue(key, screenshotBuffer, {
            contentType: 'image/png',
        });
    },
});

await crawler.run([
    'https://crawlee.dev',
    'https://apify.com',
]);

----------------------------------------

TITLE: Basic Request Queue Operations in Crawlee
DESCRIPTION: Demonstrates basic operations with a request queue in Crawlee, including opening a queue, adding requests, and processing them.

LANGUAGE: javascript
CODE:
import { RequestQueue } from 'crawlee';

const requestQueue = await RequestQueue.open();

await requestQueue.addRequest({ url: 'https://example.com' });

let request = await requestQueue.fetchNextRequest();
if (request) {
    // Process the request...
    await requestQueue.markRequestHandled(request);
}

// Later we can close the queue
await requestQueue.drop();

----------------------------------------

TITLE: Demonstrating Key-Value Store Operations in Crawlee
DESCRIPTION: Shows basic operations of key-value stores including reading input, writing output, and managing named stores. Demonstrates how to store and retrieve different data types automatically handled by the system.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';

// Get the INPUT from the default key-value store
const input = await KeyValueStore.getInput();

// Write the OUTPUT to the default key-value store
await KeyValueStore.setValue('OUTPUT', { myResult: 123 });

// Open a named key-value store
const store = await KeyValueStore.open('some-name');

// Write a record to the named key-value store.
// JavaScript object is automatically converted to JSON,
// strings and binary buffers are stored as they are
await store.setValue('some-key', { foo: 'bar' });

// Read a record from the named key-value store.
// Note that JSON is automatically parsed to a JavaScript object,
// text data is returned as a string, and other data is returned as binary buffer
const value = await store.getValue('some-key');

// Delete a record from the named key-value store
await store.setValue('some-key', null);

----------------------------------------

TITLE: Configuring Docker Environment for Crawlee Actor
DESCRIPTION: Dockerfile that sets up a Node.js environment with Playwright and Chrome for Crawlee projects. It implements efficient layer caching by separating dependency installation from source code copying. The configuration uses the apify/actor-node-playwright-chrome:20 base image and includes optimized npm installation steps.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:20

COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD npm start --silent

----------------------------------------

TITLE: Configuring Headful Browser Mode
DESCRIPTION: TypeScript configuration option to enable visible browser window during crawling

LANGUAGE: typescript
CODE:
// Uncomment this option to see the browser window.
headless: false

----------------------------------------

TITLE: Puppeteer Chrome Docker Configuration
DESCRIPTION: Dockerfile setup for Node.js with Puppeteer and Chrome browser support, including XVFB for headless/headful operation.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-puppeteer-chrome:16

----------------------------------------

TITLE: Configuring SessionPool with PlaywrightCrawler in Crawlee
DESCRIPTION: This code snippet shows how to configure and use SessionPool with PlaywrightCrawler in Crawlee. It includes setup for proxy rotation and session management.

LANGUAGE: js
CODE:
import { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new PlaywrightCrawler({
    proxyConfiguration,
    async requestHandler({ page, session }) {
        // Use 'session' here...
        const pageTitle = await page.title();
        // Process the page content here...
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Setting up HTTP Server with Node.js for Crawlee
DESCRIPTION: Creates a basic HTTP server using Node.js built-in 'http' module. The server listens on port 3000 and logs incoming requests.

LANGUAGE: javascript
CODE:
import { createServer } from 'http';
import { log } from 'crawlee';

const server = createServer(async (req, res) => {
    log.info(`Request received: ${req.method} ${req.url}`);

    res.writeHead(200, { 'Content-Type': 'text/plain' });
    // We will return the page title here later instead
    res.end('Hello World\n');
});

server.listen(3000, () => {
    log.info('Server is listening for user requests');
});

----------------------------------------

TITLE: Implementing Firefox Browser with Playwright Crawler in NodeJS
DESCRIPTION: Code example demonstrating the configuration and usage of PlaywrightCrawler with Firefox browser. The code includes necessary imports and configuration for headless Firefox browser setup.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';

const crawler = new PlaywrightCrawler({
    browserType: 'firefox',
    // Proxy configuration
    proxyConfiguration: new ProxyConfiguration({ proxyUrls: ['...'] })
});

----------------------------------------

TITLE: Crawling Sitemaps with Puppeteer Crawler in JavaScript
DESCRIPTION: This code snippet shows how to use Puppeteer Crawler to crawl sitemaps. It sets up the crawler, defines the request handler, and uses the Sitemap utility to process sitemap URLs. It's designed to run on the Apify Platform using the apify/actor-node-puppeteer-chrome image.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';
import { Sitemap } from '@crawlee/utils';

const crawler = new PuppeteerCrawler({
    // By default, the crawler only supports limited number of
    // concurrently processed requests. To download all URLs from the
    // sitemap, we need to configure higher limit.
    maxRequestsPerCrawl: 500,
});

crawler.router.addHandler('START', async ({ enqueueLinks }) => {
    const sitemap = await Sitemap.load({ url: 'https://crawlee.dev/sitemap.xml' });
    await enqueueLinks({ urls: sitemap.urls });
});

crawler.router.addDefaultHandler(async ({ log, request, page }) => {
    const title = await page.title();
    log.info(`Title of ${request.loadedUrl} is '${title}'`);
});

await crawler.run(['START']);

----------------------------------------

TITLE: Combining Request Queue and Request List in Crawlee
DESCRIPTION: Shows how to use both Request Queue and Request List together in a Crawlee crawler. This approach is useful for handling both initial and dynamically added URLs.

LANGUAGE: javascript
CODE:
import { RequestList, RequestQueue, CheerioCrawler } from 'crawlee';

const requestList = await RequestList.open('my-list', [
    { url: 'https://crawlee.dev' },
    { url: 'https://crawlee.dev/docs' },
    { url: 'https://crawlee.dev/api' },
]);

const requestQueue = await RequestQueue.open();

const crawler = new CheerioCrawler({
    requestList,
    requestQueue,
    async requestHandler({ $, request, enqueueLinks }) {
        // Process the page...

        // We can add more pages to the queue
        await enqueueLinks();
    },
});

await crawler.run();

----------------------------------------

TITLE: Playwright-based Sitemap Crawling
DESCRIPTION: Implementation showing sitemap crawling using Playwright Crawler, offering modern browser automation features. Requires the apify/actor-node-playwright-chrome Docker image when running on the Apify Platform.

LANGUAGE: javascript
CODE:
{PlaywrightSource}

----------------------------------------

TITLE: Enabling Request Locking in Crawlee Crawler
DESCRIPTION: Shows how to enable the request locking experiment in a CheerioCrawler instance. This allows the crawler to use the new request locking API for parallel crawling.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    experiments: {
        requestLocking: true,
    },
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Implementing RequestQueueV2 for Request Locking
DESCRIPTION: Demonstrates how to use RequestQueueV2 for implementing request locking outside of crawlers. This allows for creating and managing request queues that support the locking mechanism.

LANGUAGE: typescript
CODE:
import { RequestQueueV2 } from 'crawlee';

const queue = await RequestQueueV2.open('my-locking-queue');
await queue.addRequests([
    { url: 'https://crawlee.dev' },
    { url: 'https://crawlee.dev/js/docs' },
    { url: 'https://crawlee.dev/js/api' },
]);

----------------------------------------

TITLE: Complete Product Data Extraction with Playwright
DESCRIPTION: This code snippet combines all the previous extraction methods to scrape complete product information including URL, manufacturer, title, SKU, price, and stock availability.

LANGUAGE: javascript
CODE:
const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']
const manufacturer = urlPart.split('-')[0]; // 'sennheiser'

const title = await page.locator('.product-meta h1').textContent();
const sku = await page.locator('span.product-meta__sku-number').textContent();

const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Wrapping PlaywrightCrawler in Express Server for GCP Cloud Run in JavaScript
DESCRIPTION: This code demonstrates how to wrap a PlaywrightCrawler in an Express server for deployment on GCP Cloud Run. It includes setting up the server, handling requests, and listening on the appropriate port.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import express from 'express';
const app = express();

const startUrls = ['https://crawlee.dev'];

app.get('/', async (req, res) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));
    
    await crawler.run(startUrls);    

    return res.send(await crawler.getData());
});

app.listen(parseInt(process.env.PORT) || 3000);

----------------------------------------

TITLE: Using SessionPool with CheerioCrawler
DESCRIPTION: Example of integrating SessionPool with CheerioCrawler for managing sessions during web scraping.

LANGUAGE: javascript
CODE:
{CheerioSource}

----------------------------------------

TITLE: Implementing Router Handlers for Web Scraping in JavaScript
DESCRIPTION: Defines route handlers for different page types including product details, category pages, and a default handler. Each handler extracts specific data or enqueues additional links for crawling.

LANGUAGE: javascript
CODE:
import { createPlaywrightRouter, Dataset } from 'crawlee';

// createPlaywrightRouter() is only a helper to get better
// intellisense and typings. You can use Router.create() too.
export const router = createPlaywrightRouter();

// This replaces the request.label === DETAIL branch of the if clause.
router.addHandler('DETAIL', async ({ request, page, log }) => {
    log.debug(`Extracting data: ${request.url}`);
    const urlPart = request.url.split('/').slice(-1);
    const manufacturer = urlPart[0].split('-')[0];

    const title = await page.locator('.product-meta h1').textContent();
    const sku = await page
        .locator('span.product-meta__sku-number')
        .textContent();

    const priceElement = page
        .locator('span.price')
        .filter({
            hasText: '$',
        })
        .first();

    const currentPriceString = await priceElement.textContent();
    const rawPrice = currentPriceString.split('$')[1];
    const price = Number(rawPrice.replaceAll(',', ''));

    const inStockElement = page
        .locator('span.product-form__inventory')
        .filter({
            hasText: 'In stock',
        })
        .first();

    const inStock = (await inStockElement.count()) > 0;

    const results = {
        url: request.url,
        manufacturer,
        title,
        sku,
        currentPrice: price,
        availableInStock: inStock,
    };

    log.debug(`Saving data: ${request.url}`);
    await Dataset.pushData(results);
});

router.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {
    log.debug(`Enqueueing pagination for: ${request.url}`);
    await page.waitForSelector('.product-item > a');
    await enqueueLinks({
        selector: '.product-item > a',
        label: 'DETAIL',
    });

    const nextButton = await page.$('a.pagination__next');
    if (nextButton) {
        await enqueueLinks({
            selector: 'a.pagination__next',
            label: 'CATEGORY',
        });
    }
});

router.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {
    log.debug(`Enqueueing categories from page: ${request.url}`);
    await page.waitForSelector('.collection-block-item');
    await enqueueLinks({
        selector: '.collection-block-item',
        label: 'CATEGORY',
    });
});

----------------------------------------

TITLE: Storage Cleanup in Crawlee
DESCRIPTION: Shows how to clean up default storage directories in Crawlee using the purgeDefaultStorages helper function.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Initializing Shared Request Queue with Locking Support in JavaScript
DESCRIPTION: Creates a function to initialize or retrieve a request queue that supports locking. This queue is used to store and manage URLs for parallel scraping.

LANGUAGE: javascript
CODE:
import { RequestQueue } from 'crawlee';
import { RequestLockingQueue } from '@crawlee/memory-storage';

let requestQueue;

export async function getOrInitQueue(shouldReset = false) {
    if (!requestQueue || shouldReset) {
        requestQueue = await RequestQueue.open('default', {
            requestLockingImplementation: {
                new: () => new RequestLockingQueue(),
            },
        });
    }
    return requestQueue;
}

----------------------------------------

TITLE: Capturing Screenshot with Puppeteer Using utils.puppeteer.saveSnapshot()
DESCRIPTION: This snippet shows how to capture a screenshot using the utils.puppeteer.saveSnapshot() method. It launches a browser, creates a page, navigates to a URL, and uses the saveSnapshot utility to capture and save the screenshot.

LANGUAGE: javascript
CODE:
import { launchPuppeteer } from 'crawlee/lib/puppeteer/puppeteer_launcher';
import { utils } from 'crawlee';

const browser = await launchPuppeteer();
const page = await browser.newPage();

const url = 'https://crawlee.dev';
await page.goto(url);

await utils.puppeteer.saveSnapshot(page, { key: 'my-screenshot' });

await browser.close();

----------------------------------------

TITLE: Adding Data to Default Dataset in Crawlee
DESCRIPTION: Example showing how to save data to the default dataset in Crawlee. The dataset will be automatically created if it doesn't exist. Data can also be saved to custom datasets using Dataset.open().

LANGUAGE: javascript
CODE:
{AddDataToDatasetSource}

----------------------------------------

TITLE: Crawling Multiple URLs with Playwright Crawler in JavaScript
DESCRIPTION: This code snippet illustrates how to use Playwright Crawler to crawl a list of specified URLs. It captures screenshots of each page and stores the page title and URL.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';
import { writeFileSync } from 'fs';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url} is '${title}'`);

        const screenshotBuffer = await page.screenshot();
        writeFileSync(`screenshot-${Math.random()}.png`, screenshotBuffer);

        await Dataset.pushData({
            title,
            url: request.url,
        });
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run([
    'https://crawlee.dev',
    'https://crawlee.dev/docs/quick-start',
    'https://crawlee.dev/docs/introduction/first-scraper',
]);

----------------------------------------

TITLE: Using Crawling Context IDs in Apify SDK v1
DESCRIPTION: Shows how to use Crawling Context IDs for cross-context access in handler functions.

LANGUAGE: javascript
CODE:
let masterContextId;
const handlePageFunction = async ({ id, page, request, crawler }) => {
    if (request.userData.masterPage) {
        masterContextId = id;
        // Prepare the master page.
    } else {
        const masterContext = crawler.crawlingContexts.get(masterContextId);
        const masterPage = masterContext.page;
        const masterRequest = masterContext.request;
        // Now we can manipulate the master data from another handlePageFunction.
    }
}

----------------------------------------

TITLE: Configuring Header Generation in BasicCrawler with TypeScript
DESCRIPTION: This snippet shows how to configure header generation options in BasicCrawler. It demonstrates setting up specific devices, locales, operating systems, and browsers for generating realistic browser headers.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            headerGeneratorOptions: {
                devices: ['mobile', 'desktop'],
                locales: ['en-US'],
                operatingSystems: ['windows', 'macos', 'android', 'ios'],
                browsers: ['chrome', 'edge', 'firefox', 'safari'],
            },
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Using sendRequest with BasicCrawler in Crawlee
DESCRIPTION: Demonstrates how to use the sendRequest function within a BasicCrawler's requestHandler to make HTTP requests.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest();
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Configuring PlaywrightCrawler with AWS Chromium in JavaScript
DESCRIPTION: Updates the Crawlee code to use the Chromium path from @sparticuz/chromium and sets necessary arguments for AWS Lambda execution.

LANGUAGE: javascript
CODE:
// For more information, see https://crawlee.dev/
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
    launchContext: {
        launchOptions: {
             executablePath: await aws_chromium.executablePath(),
             args: aws_chromium.args,
             headless: true
        }
    }
}, new Configuration({
    persistStorage: false,
}));

----------------------------------------

TITLE: Complete package.json Configuration
DESCRIPTION: Full package.json configuration for a TypeScript project using Crawlee, including all necessary scripts and dependencies.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "type": "module",
    "main": "dist/main.js",
    "dependencies": {
        "crawlee": "3.0.0"
    },
    "devDependencies": {
        "@apify/tsconfig": "^0.1.0",
        "ts-node": "^10.8.0",
        "typescript": "^4.7.4"
    },
    "scripts": {
        "start": "npm run start:dev",
        "start:prod": "node dist/main.js",
        "start:dev": "ts-node-esm -T src/main.ts",
        "build": "tsc"
    }
}

----------------------------------------

TITLE: Setting Maximum Requests Limit in Cheerio Crawler
DESCRIPTION: Configuration example showing how to limit the maximum number of requests per crawl using maxRequestsPerCrawl option.

LANGUAGE: typescript
CODE:
const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 20,
    // ...
});

----------------------------------------

TITLE: Configuring Header Generator Options in Got Scraping with TypeScript
DESCRIPTION: Shows how to set custom header generator options for Got Scraping to generate specific browser fingerprints.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            headerGeneratorOptions: {
                devices: ['mobile', 'desktop'],
                locales: ['en-US'],
                operatingSystems: ['windows', 'macos', 'android', 'ios'],
                browsers: ['chrome', 'edge', 'firefox', 'safari'],
            },
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Using Proxy with BasicCrawler
DESCRIPTION: Demonstrates how to configure a proxy server when making requests with BasicCrawler.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            proxyUrl: 'http://auto:password@proxy.apify.com:8000',
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Configuring CheerioCrawler for GCP Functions
DESCRIPTION: Updates the main.js file to use a separate Configuration instance with persistStorage set to false, which is necessary for running in a serverless environment.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Explicit Request Queue Usage with Crawler in Crawlee
DESCRIPTION: Shows how to explicitly create and use a request queue with a Crawlee crawler. This approach allows more control over queue management.

LANGUAGE: javascript
CODE:
import { RequestQueue, PuppeteerCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

const crawler = new PuppeteerCrawler({
    requestQueue,
    async requestHandler({ page, enqueueLinks }) {
        await enqueueLinks();
    },
});

await crawler.run();

----------------------------------------

TITLE: Standalone Session Management in Crawlee
DESCRIPTION: This snippet demonstrates standalone usage of SessionPool in Crawlee for manual session management. It includes creating a session pool, adding sessions, and using them for requests.

LANGUAGE: js
CODE:
import { SessionPool } from 'crawlee';
import { gotScraping } from 'got-scraping';

const sessionPool = new SessionPool({
    maxPoolSize: 25,
});

// Create random session
const randomSession = await sessionPool.getSession();

// Create session with specific session id
const namedSession = await sessionPool.getSession({ sessionId: 'my-named-session' });

// Create session with custom user data
const sessionWithData = await sessionPool.getSession({
    userData: {
        country: 'US',
        foods: ['burger', 'pizza', 'fries'],
    },
});

// Use the session
const response = await gotScraping({
    url: 'https://example.com',
    proxyUrl: randomSession.proxyUrl,
    headers: {
        'User-Agent': randomSession.userData.userAgent,
    },
});

// Mark session as blocked if needed
if (response.statusCode === 403) {
    randomSession.markBad();
}

// Destroy sessions
await sessionPool.teardown();

----------------------------------------

TITLE: Dockerfile and Package.json Best Practices
DESCRIPTION: Illustrates the recommended way to specify Docker image and dependency versions in Dockerfile and package.json.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16

LANGUAGE: json
CODE:
{
    "dependencies": {
        "crawlee": "^3.0.0",
        "playwright": "*"
    }
}

----------------------------------------

TITLE: JSON Response Type Configuration
DESCRIPTION: Demonstrates how to configure sendRequest to handle JSON response types.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ responseType: 'json' });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Implementing Router for Crawlee Web Scraping in JavaScript
DESCRIPTION: Defines a Router for handling different types of pages in a web scraping project. It includes handlers for detail pages, category pages, and a default handler for the start page.

LANGUAGE: javascript
CODE:
import { createPlaywrightRouter, Dataset } from 'crawlee';

export const router = createPlaywrightRouter();

router.addHandler('DETAIL', async ({ request, page, log }) => {
    log.debug(`Extracting data: ${request.url}`);
    const urlPart = request.url.split('/').slice(-1);
    const manufacturer = urlPart[0].split('-')[0];

    const title = await page.locator('.product-meta h1').textContent();
    const sku = await page
        .locator('span.product-meta__sku-number')
        .textContent();

    const priceElement = page
        .locator('span.price')
        .filter({
            hasText: '$',
        })
        .first();

    const currentPriceString = await priceElement.textContent();
    const rawPrice = currentPriceString.split('$')[1];
    const price = Number(rawPrice.replaceAll(',', ''));

    const inStockElement = page
        .locator('span.product-form__inventory')
        .filter({
            hasText: 'In stock',
        })
        .first();

    const inStock = (await inStockElement.count()) > 0;

    const results = {
        url: request.url,
        manufacturer,
        title,
        sku,
        currentPrice: price,
        availableInStock: inStock,
    };

    log.debug(`Saving data: ${request.url}`);
    await Dataset.pushData(results);
});

router.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {
    log.debug(`Enqueueing pagination for: ${request.url}`);

    await page.waitForSelector('.product-item > a');
    await enqueueLinks({
        selector: '.product-item > a',
        label: 'DETAIL',
    });

    const nextButton = await page.$('a.pagination__next');
    if (nextButton) {
        await enqueueLinks({
            selector: 'a.pagination__next',
            label: 'CATEGORY',
        });
    }
});

router.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {
    log.debug(`Enqueueing categories from page: ${request.url}`);

    await page.waitForSelector('.collection-block-item');
    await enqueueLinks({
        selector: '.collection-block-item',
        label: 'CATEGORY',
    });
});

----------------------------------------

TITLE: Retrieving Page Title with JSDOM in JavaScript
DESCRIPTION: Demonstrates how to get the page title using JSDOM, comparing it with browser JavaScript. This snippet shows the similarity between browser and JSDOM APIs for accessing document properties.

LANGUAGE: javascript
CODE:
// Return the page title
document.title; // browsers
window.document.title; // JSDOM

----------------------------------------

TITLE: Dataset Operations in Crawlee
DESCRIPTION: Demonstrates basic operations for working with datasets in Crawlee, including writing single and multiple rows of data.

LANGUAGE: javascript
CODE:
import { Dataset } from 'crawlee';

// Write a single row to the default dataset
await Dataset.pushData({ col1: 123, col2: 'val2' });

// Open a named dataset
const dataset = await Dataset.open('some-name');

// Write a single row
await dataset.pushData({ foo: 'bar' });

// Write multiple rows
await dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);

----------------------------------------

TITLE: Using Crawlee with crawlee.json configuration
DESCRIPTION: This JavaScript code demonstrates how to use Crawlee with configuration options set in crawlee.json. It creates a CheerioCrawler and adds request handlers without explicitly setting configuration in the code.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, sleep } from 'crawlee';
// We are not importing nor passing
// the Configuration to the crawler.
// We are not assigning any env vars either.
const crawler = new CheerioCrawler();

crawler.router.addDefaultHandler(async ({ request }) => {
    // for the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // for the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Configuring ProxyConfiguration with Custom Proxy Function in JavaScript
DESCRIPTION: Demonstrates how to use a custom function to select proxy URLs based on the request URL and session ID. This allows for more complex proxy selection logic.

LANGUAGE: javascript
CODE:
const proxyConfiguration = new ProxyConfiguration({
    newUrlFunction: (sessionId, { request }) => {
        if (request?.url.includes('crawlee.dev')) {
            return null; // for crawlee.dev, we don't use a proxy
        }

        return 'http://proxy-1.com'; // for all other URLs, we use this proxy
    }
});

----------------------------------------

TITLE: Disabling Browser Fingerprints with PuppeteerCrawler in Crawlee
DESCRIPTION: This code snippet shows how to disable browser fingerprints when using PuppeteerCrawler in Crawlee. It sets the 'useFingerprints' option to false in the browserPoolOptions.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    // ...
    browserPoolOptions: {
        useFingerprints: false,
    },
});

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Router in JavaScript
DESCRIPTION: Sets up a PlaywrightCrawler instance using a Router for request handling. It configures logging and runs the crawler with a starting URL.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Dataset Operations in Crawlee
DESCRIPTION: Demonstrates basic dataset operations including writing single and multiple rows of data.

LANGUAGE: javascript
CODE:
import { Dataset } from 'crawlee';

// Write a single row to the default dataset
await Dataset.pushData({ col1: 123, col2: 'val2' });

// Open a named dataset
const dataset = await Dataset.open('some-name');

// Write a single row
await dataset.pushData({ foo: 'bar' });

// Write multiple rows
await dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);

----------------------------------------

TITLE: Initializing BasicCrawler with Got Scraping in TypeScript
DESCRIPTION: Demonstrates how to use the sendRequest function within a BasicCrawler's requestHandler to send HTTP requests using Got Scraping.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest();
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Implementing Crawlee Router with Route Handlers
DESCRIPTION: Defines route handlers for different page types (detail, category, and default pages) using Crawlee's router. Each handler processes specific page types and extracts or enqueues data accordingly.

LANGUAGE: javascript
CODE:
import { createPlaywrightRouter, Dataset } from 'crawlee';

// createPlaywrightRouter() is only a helper to get better
// intellisense and typings. You can use Router.create() too.
export const router = createPlaywrightRouter();

// This replaces the request.label === DETAIL branch of the if clause.
router.addHandler('DETAIL', async ({ request, page, log }) => {
    log.debug(`Extracting data: ${request.url}`);
    const urlPart = request.url.split('/').slice(-1);
    const manufacturer = urlPart[0].split('-')[0];

    const title = await page.locator('.product-meta h1').textContent();
    const sku = await page
        .locator('span.product-meta__sku-number')
        .textContent();

    const priceElement = page
        .locator('span.price')
        .filter({
            hasText: '$',
        })
        .first();

    const currentPriceString = await priceElement.textContent();
    const rawPrice = currentPriceString.split('$')[1];
    const price = Number(rawPrice.replaceAll(',', ''));

    const inStockElement = page
        .locator('span.product-form__inventory')
        .filter({
            hasText: 'In stock',
        })
        .first();

    const inStock = (await inStockElement.count()) > 0;

    const results = {
        url: request.url,
        manufacturer,
        title,
        sku,
        currentPrice: price,
        availableInStock: inStock,
    };

    log.debug(`Saving data: ${request.url}`);
    await Dataset.pushData(results);
});

router.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {
    log.debug(`Enqueueing pagination for: ${request.url}`);
    await page.waitForSelector('.product-item > a');
    await enqueueLinks({
        selector: '.product-item > a',
        label: 'DETAIL',
    });

    const nextButton = await page.$('a.pagination__next');
    if (nextButton) {
        await enqueueLinks({
            selector: 'a.pagination__next',
            label: 'CATEGORY',
        });
    }
});

router.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {
    log.debug(`Enqueueing categories from page: ${request.url}`);
    await page.waitForSelector('.collection-block-item');
    await enqueueLinks({
        selector: '.collection-block-item',
        label: 'CATEGORY',
    });
});

----------------------------------------

TITLE: Implementing Skip Navigation with PlaywrightCrawler
DESCRIPTION: Shows how to use Request#skipNavigation and sendRequest to efficiently fetch and save CDN resources without full crawler navigation. This optimization is particularly useful for handling static assets like images served through CDNs.

LANGUAGE: javascript
CODE:
{
SkipNavigationSource
}

----------------------------------------

TITLE: Demonstrating Dataset Operations in Crawlee
DESCRIPTION: This snippet illustrates basic operations of datasets in Crawlee, including writing single and multiple rows to both default and named datasets.

LANGUAGE: javascript
CODE:
import { Dataset } from 'crawlee';

// Write a single row to the default dataset
await Dataset.pushData({ col1: 123, col2: 'val2' });

// Open a named dataset
const dataset = await Dataset.open('some-name');

// Write a single row
await dataset.pushData({ foo: 'bar' });

// Write multiple rows
await dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);

----------------------------------------

TITLE: Header Generator Configuration
DESCRIPTION: Demonstrates how to configure browser fingerprint generation options

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            headerGeneratorOptions: {
                devices: ['mobile', 'desktop'],
                locales: ['en-US'],
                operatingSystems: ['windows', 'macos', 'android', 'ios'],
                browsers: ['chrome', 'edge', 'firefox', 'safari'],
            },
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Importing Dataset Module in Crawlee (TypeScript)
DESCRIPTION: Imports the PlaywrightCrawler and Dataset modules from Crawlee. This is necessary for using the Dataset.pushData() function to save extracted data.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

----------------------------------------

TITLE: Configuring maxRequestsPerMinute in CheerioCrawler
DESCRIPTION: This snippet demonstrates how to set the maxRequestsPerMinute option in a CheerioCrawler to limit the number of requests made per minute.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    maxRequestsPerMinute: 120,
    // ...
});

----------------------------------------

TITLE: Implementing HTTP Crawler with Crawlee
DESCRIPTION: Example showing how to build a crawler using HttpCrawler to load URLs from an external file, make HTTP requests, and save HTML content. The code demonstrates basic crawler setup and HTML processing functionality.

LANGUAGE: javascript
CODE:
import { HttpCrawler, Dataset } from 'crawlee';

// Create an instance of the HttpCrawler class
const crawler = new HttpCrawler({
    // Function called for each URL
    async requestHandler({ request, body, $ }) {
        // Extract data from the loaded page and save it to the dataset
        await Dataset.pushData({
            url: request.url,
            html: body
        });
    },
});

// Add requests to the queue and start the crawl
await crawler.run([
    'http://example.com/page-1',
    'http://example.com/page-2',
    'http://example.com/page-3',
]);

----------------------------------------

TITLE: Installing Crawlee with Browser Dependencies
DESCRIPTION: Commands to install Crawlee with Playwright or Puppeteer dependencies

LANGUAGE: bash
CODE:
npm install crawlee playwright
# or npm install @crawlee/playwright playwright

----------------------------------------

TITLE: Basic Cheerio Crawler Implementation in TypeScript
DESCRIPTION: Simple crawler implementation that downloads a single page's HTML and extracts its title using Cheerio.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Scraping with PlaywrightCrawler (Successful for JS-rendered content)
DESCRIPTION: This snippet shows how to use PlaywrightCrawler to successfully scrape JavaScript-rendered content by waiting for elements to appear before extracting data.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        // Wait for the actor cards to render
        await page.waitForSelector('.ActorStoreItem');
        
        // Extract text content of the first actor card
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Basic Request Queue Operations in Crawlee
DESCRIPTION: Demonstrates basic operations with a request queue in Crawlee, including opening a queue, adding requests, and fetching requests.

LANGUAGE: javascript
CODE:
import { RequestQueue } from 'crawlee';

const requestQueue = await RequestQueue.open();

// Add requests to queue
await requestQueue.addRequest({ url: 'https://example.com/1' });
await requestQueue.addRequest({ url: 'https://example.com/2' });

// Get request from queue
const request1 = await requestQueue.fetchNextRequest();
const request2 = await requestQueue.fetchNextRequest();

// Mark requests as handled
await requestQueue.markRequestHandled(request1);
await requestQueue.markRequestHandled(request2);

// Check if queue is empty
const isEmpty = await requestQueue.isEmpty();
console.log(`Is queue empty? ${isEmpty}`);

----------------------------------------

TITLE: Crawling Same Hostname Links with CheerioCrawler in Crawlee
DESCRIPTION: This snippet shows how to use CheerioCrawler to crawl links with the same hostname as the starting URL. It uses the 'same-hostname' enqueue strategy, which is the default.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks }) {
        // Add new links from same hostname to RequestQueue
        await enqueueLinks({
            strategy: 'same-hostname',
            label: 'detail',
        });
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Simplified CheerioCrawler Implementation
DESCRIPTION: Shows a more concise way to create a CheerioCrawler using the run method's URL parameter instead of explicitly creating a RequestQueue.

LANGUAGE: typescript
CODE:
// You don't need to import RequestQueue anymore
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

// Start the crawler with the provided URLs
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: HTML Link Example
DESCRIPTION: Example of an HTML anchor tag with href attribute that the crawler will process.

LANGUAGE: html
CODE:
<a href="https://crawlee.dev/js/docs/introduction">This is a link to Crawlee introduction</a>

----------------------------------------

TITLE: Configuring Request Rate Limiting in Crawlee
DESCRIPTION: Example demonstrating how to set maximum requests per minute for a CheerioCrawler to control request rate. Uses maxRequestsPerMinute parameter to limit request frequency.

LANGUAGE: javascript
CODE:
const crawler = new CheerioCrawler({
    // Only allow 120 requests per minute
    maxRequestsPerMinute: 120,
    // ... other options
});

----------------------------------------

TITLE: Simplified CheerioCrawler Implementation
DESCRIPTION: Shows a more concise way to create a CheerioCrawler using the run method's URL parameter instead of explicitly creating a RequestQueue.

LANGUAGE: typescript
CODE:
// You don't need to import RequestQueue anymore
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

// Start the crawler with the provided URLs
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Skipping Navigation for Image Requests in PlaywrightCrawler
DESCRIPTION: This code snippet demonstrates how to use PlaywrightCrawler to crawl a website, identify image URLs, and fetch them directly without navigation. It utilizes the Request#skipNavigation option and sendRequest method to efficiently handle CDN-delivered images.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ request, page, sendRequest, log }) {
        log.info(`Processing ${request.url}`);

        if (request.skipNavigation) {
            // We don't need to navigate anywhere, we can just send a GET request
            const imageBuffer = await sendRequest({ url: request.url, responseType: 'buffer' });

            // Save the image to the default key-value store
            await Dataset.pushData({
                url: request.url,
                image: imageBuffer,
            });

            return;
        }

        // Find all image urls on the page
        const imageUrls = await page.evaluate(() => {
            const imgElements = document.querySelectorAll('img');
            return Array.from(imgElements).map((img) => img.src);
        });

        // Add them to the queue, but skip navigation
        for (const url of imageUrls) {
            await crawler.addRequests([{
                url,
                skipNavigation: true,
            }]);
        }
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Scraping and Parsing Product Price with Playwright
DESCRIPTION: This code demonstrates how to locate the price element, extract the text content, and parse it into a numeric value using Playwright.

LANGUAGE: javascript
CODE:
const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

----------------------------------------

TITLE: Extracting Text Content from an HTML Element with Cheerio
DESCRIPTION: This code snippet demonstrates how to use Cheerio to find the first <h2> element on a page and extract its text content.

LANGUAGE: javascript
CODE:
$('h2').text()

----------------------------------------

TITLE: Implementing Express Server for GCP Cloud Run with PlaywrightCrawler
DESCRIPTION: Creates an Express HTTP server that wraps the PlaywrightCrawler implementation for GCP Cloud Run deployment. Includes configuration for port listening and stateless request handling.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import express from 'express';
const app = express();

const startUrls = ['https://crawlee.dev'];

app.get('/', async (req, res) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));
    
    await crawler.run(startUrls);    

    return res.send(await crawler.getData());
});

app.listen(parseInt(process.env.PORT) || 3000);

----------------------------------------

TITLE: Failed Cheerio Crawler Output
DESCRIPTION: Shows the empty output from the CheerioCrawler attempt, demonstrating why it fails with JavaScript-rendered content.

LANGUAGE: log
CODE:
ACTOR:

----------------------------------------

TITLE: Adjusting Crawlee Code for Apify Platform Deployment
DESCRIPTION: Modifications to the main Crawlee script to integrate with the Apify Platform, including initializing and exiting the Actor.

LANGUAGE: typescript
CODE:
import { Actor } from 'apify';
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

await Actor.init();

log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

await Actor.exit();

----------------------------------------

TITLE: Disabling Browser Fingerprints with PlaywrightCrawler
DESCRIPTION: This code snippet demonstrates how to disable browser fingerprints in PlaywrightCrawler by setting the useFingerprints option to false in the browserPoolOptions.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    // ...
    browserPoolOptions: {
        useFingerprints: false,
    },
});

----------------------------------------

TITLE: Implementing Request List Operations in Crawlee
DESCRIPTION: Shows how to initialize and use Request List with PuppeteerCrawler for processing a predefined set of URLs. Demonstrates the basic setup for handling multiple pages in a static list.

LANGUAGE: javascript
CODE:
import { RequestList, PuppeteerCrawler } from 'crawlee';

// Prepare the sources array with URLs to visit
const sources = [
    { url: 'http://www.example.com/page-1' },
    { url: 'http://www.example.com/page-2' },
    { url: 'http://www.example.com/page-3' },
];

// Open the request list.
// List name is used to persist the sources and the list state in the key-value store
const requestList = await RequestList.open('my-list', sources);

// The crawler will automatically process requests from the list
// It's used the same way for Cheerio /Playwright crawlers.
const crawler = new PuppeteerCrawler({
    requestList,
    async requestHandler({ page, request }) {
        // Process the page (extract data, take page screenshot, etc).
        // No more requests could be added to the request list here
    },
});

----------------------------------------

TITLE: Configuring Advanced Autoscaling Options in Crawlee
DESCRIPTION: Detailed example of configuring advanced autoscaling pool options for fine-tuned control over crawler scaling behavior.

LANGUAGE: javascript
CODE:
const crawler = new CheerioCrawler({
    autoscaledPoolOptions: {
        desiredConcurrency: 10,
        desiredConcurrencyRatio: 0.95,
        scaleUpStepRatio: 0.05,
        scaleDownStepRatio: 0.05,
        maybeRunIntervalSecs: 0.5,
        loggingIntervalSecs: 60,
        autoscaleIntervalSecs: 10,
        maxTasksPerMinute: 120
    },
    // ... other options
});

----------------------------------------

TITLE: Actor Creation and Execution
DESCRIPTION: Commands to create and run a new Apify actor locally

LANGUAGE: bash
CODE:
apify create my-hello-world
cd my-hello-world
apify run

----------------------------------------

TITLE: sendRequest API Implementation in Crawlee
DESCRIPTION: Shows the implementation of the sendRequest function in Crawlee. It uses gotScraping with various options including URL, method, headers, proxy settings, and cookie handling. This function is the core of making HTTP requests in Crawlee's BasicCrawler.

LANGUAGE: typescript
CODE:
async sendRequest(overrideOptions?: GotOptionsInit) => {
    return gotScraping({
        url: request.url,
        method: request.method,
        body: request.payload,
        headers: request.headers,
        proxyUrl: crawlingContext.proxyInfo?.url,
        sessionToken: session,
        responseType: 'text',
        ...overrideOptions,
        retry: {
            limit: 0,
            ...overrideOptions?.retry,
        },
        cookieJar: {
            getCookieString: (url: string) => session!.getCookieString(url),
            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),
            ...overrideOptions?.cookieJar,
        },
    });
}

----------------------------------------

TITLE: Capturing Screenshot with Puppeteer Using page.screenshot()
DESCRIPTION: This snippet demonstrates how to capture a screenshot of a web page using Puppeteer's page.screenshot() method. It launches a browser, creates a new page, navigates to a URL, takes a screenshot, and saves it to a key-value store.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';
import { launchPuppeteer } from 'crawlee/lib/puppeteer/puppeteer_launcher';

const browser = await launchPuppeteer();
const page = await browser.newPage();

const url = 'https://crawlee.dev';
await page.goto(url);

const screenshot = await page.screenshot();

const key = `screenshot-${new URL(url).hostname}`;
await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });

await browser.close();

----------------------------------------

TITLE: Configuring Advanced Autoscaled Pool Options in Crawlee
DESCRIPTION: Demonstrates advanced configuration of the autoscaling pool, including desired concurrency, scaling ratios, and various timing intervals for optimal crawler performance.

LANGUAGE: javascript
CODE:
const crawler = new CheerioCrawler({
    autoscaledPoolOptions: {
        desiredConcurrency: 10,
        desiredConcurrencyRatio: 0.95,
        scaleUpStepRatio: 0.05,
        scaleDownStepRatio: 0.05,
        maybeRunIntervalSecs: 0.5,
        loggingIntervalSecs: 60,
        autoscaleIntervalSecs: 10
    },
    // ...
});

----------------------------------------

TITLE: Using Crawlee with crawlee.json configuration
DESCRIPTION: This JavaScript example demonstrates how to use Crawlee with configuration options set in crawlee.json. It creates a CheerioCrawler and adds request handlers without explicitly setting configuration options in the code.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, sleep } from 'crawlee';
// We are not importing nor passing
// the Configuration to the crawler.
// We are not assigning any env vars either.
const crawler = new CheerioCrawler();

crawler.router.addDefaultHandler(async ({ request }) => {
    // for the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // for the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Crawling Multiple URLs with Cheerio Crawler
DESCRIPTION: Implementation of a web crawler using Cheerio Crawler, which is suitable for basic HTML scraping without JavaScript rendering needs.

LANGUAGE: javascript
CODE:
{CheerioSource}

----------------------------------------

TITLE: Configuring Advanced Autoscaling Pool Options in Crawlee
DESCRIPTION: Shows how to fine-tune the autoscaling behavior of the crawler using advanced pool options for precise control over scaling mechanics.

LANGUAGE: javascript
CODE:
const crawler = new CheerioCrawler({
    autoscaledPoolOptions: {
        desiredConcurrency: 10,
        desiredConcurrencyRatio: 0.95,
        scaleUpStepRatio: 0.05,
        scaleDownStepRatio: 0.05,
        maybeRunIntervalSecs: 0.5,
        loggingIntervalSecs: 60,
        autoscaleIntervalSecs: 10,
        maxTasksPerMinute: 120
    },
    // ...
});

----------------------------------------

TITLE: Crawling Sitemap with Cheerio Crawler in Crawlee
DESCRIPTION: This snippet demonstrates how to use Cheerio Crawler to crawl a sitemap. It downloads the sitemap URLs, creates a RequestQueue, and processes each URL to extract the page title.

LANGUAGE: javascript
CODE:
import { CheerioRouter, CheerioCrawler } from 'crawlee';
import { downloadListOfUrls } from '@crawlee/utils';

const router = new CheerioRouter();

router.addDefaultHandler(async ({ $, request, log }) => {
    const title = $('title').text();
    log.info(`Title of ${request.url} is: ${title}`);
});

const crawler = new CheerioCrawler({
    requestHandler: router,
});

const SITEMAP_URL = 'https://crawlee.dev/sitemap.xml';

await crawler.run([
    ...await downloadListOfUrls({ url: SITEMAP_URL }),
]);


----------------------------------------

TITLE: Crawling Links with Pattern Matching in CheerioCrawler
DESCRIPTION: Demonstrates using CheerioCrawler to crawl web pages and enqueue links that match specific glob patterns. The crawler processes the target URLs and extracts specified data using Cheerio selectors.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        await Dataset.pushData({
            url: request.url,
            title,
        });

        // Only add links that end with `.html` or `/` to the queue
        await enqueueLinks({
            globs: ['**/*.html', '**/'],
        });
    },
});

// Add the first URL to the queue and start the crawl
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Creating and Running a New Actor
DESCRIPTION: Commands to create a new actor project and run it locally using Apify CLI

LANGUAGE: bash
CODE:
apify create my-hello-world
cd my-hello-world
apify run

----------------------------------------

TITLE: Crawling Links with Cheerio Crawler
DESCRIPTION: Implementation showing how to crawl website links using Cheerio Crawler. This approach is best for simple HTML scraping without JavaScript rendering.

LANGUAGE: javascript
CODE:
CheerioSource

----------------------------------------

TITLE: HTML Link Example
DESCRIPTION: Example of an HTML anchor tag with href attribute that enqueueLinks function targets by default.

LANGUAGE: html
CODE:
<a href="https://crawlee.dev/js/docs/introduction">This is a link to Crawlee introduction</a>

----------------------------------------

TITLE: Configuring BrowserPool Options
DESCRIPTION: Example of configuring BrowserPool options including lifecycle hooks in a PuppeteerCrawler.

LANGUAGE: javascript
CODE:
const crawler = new Apify.PuppeteerCrawler({
    browserPoolOptions: {
        retireBrowserAfterPageCount: 10,
        preLaunchHooks: [
            async (pageId, launchContext) => {
                const { request } = crawler.crawlingContexts.get(pageId);
                if (request.userData.useHeadful === true) {
                    launchContext.launchOptions.headless = false;
                }
            }
        ]
    }
})

----------------------------------------

TITLE: Initializing ProxyConfiguration in Crawlee
DESCRIPTION: Creates a new ProxyConfiguration instance with a list of proxy URLs and demonstrates how to get a new proxy URL.

LANGUAGE: javascript
CODE:
import { ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ]
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Basic Link Enqueuing with Crawlee
DESCRIPTION: Simple example of using the enqueueLinks() function without parameters to crawl links.

LANGUAGE: javascript
CODE:
await enqueueLinks();

----------------------------------------

TITLE: Configuring Advanced Autoscaling Pool Options in Crawlee
DESCRIPTION: Demonstrates advanced configuration of the autoscaling pool, including scaling ratios, intervals, and logging settings for fine-tuned crawler performance.

LANGUAGE: javascript
CODE:
const crawler = new CheerioCrawler({
    autoscaledPoolOptions: {
        desiredConcurrency: 10,
        desiredConcurrencyRatio: 0.95,
        scaleUpStepRatio: 0.05,
        scaleDownStepRatio: 0.05,
        maybeRunIntervalSecs: 0.5,
        loggingIntervalSecs: 60,
        autoscaleIntervalSecs: 10,
        maxTasksPerMinute: 200
    },
    // ... other options
});

----------------------------------------

TITLE: HTML Link Example
DESCRIPTION: Example of an HTML anchor tag with href attribute that enqueueLinks function targets by default.

LANGUAGE: html
CODE:
<a href="https://crawlee.dev/js/docs/introduction">This is a link to Crawlee introduction</a>

----------------------------------------

TITLE: Capturing Screenshot with Crawlee's utils.puppeteer.saveSnapshot()
DESCRIPTION: This snippet shows how to capture a screenshot using Crawlee's utils.puppeteer.saveSnapshot() utility. It launches a browser, creates a new page, navigates to a URL, and saves the screenshot using the saveSnapshot() function.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';
import { launchPuppeteer } from 'crawlee';

const browser = await launchPuppeteer();
const page = await browser.newPage();
await page.goto('https://example.com');

// Save screenshot and HTML to default key-value store
const key = 'my-screenshot';
await utils.puppeteer.saveSnapshot(page, { key });

await browser.close();

----------------------------------------

TITLE: Fetching HTML from Single URL using Got-Scraping
DESCRIPTION: Demonstrates how to use the got-scraping package to fetch HTML content from a specific webpage. The example shows basic web scraping functionality and mentions the ability to accept user input for URLs instead of hardcoding them.

LANGUAGE: javascript
CODE:
import { gotScraping } from 'got-scraping';

const url = 'https://example.com';
const response = await gotScraping(url);
const html = response.body;

----------------------------------------

TITLE: Initializing ProxyConfiguration with Static Proxy List in JavaScript
DESCRIPTION: Creates a ProxyConfiguration instance with a static list of proxy URLs. Crawlee will rotate through these proxies in a round-robin fashion.

LANGUAGE: javascript
CODE:
import { ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ]
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Implementing Parallel Scraper with Child Processes in JavaScript
DESCRIPTION: Creates a parallel scraper that forks itself into multiple worker processes. Each worker process uses the shared request queue to fetch and process URLs concurrently.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, experiments } from 'crawlee';
import { createRequire } from 'module';
import { getOrInitQueue } from './requestQueue.mjs';
import { router } from './routes.mjs';

const require = createRequire(import.meta.url);
const { fork } = require('child_process');

experiments.requestLocking.enable();

if (process.env.IS_WORKER_THREAD) {
    // Worker process logic
    Configuration.set('purgeOnStart', false);
    const requestQueue = await getOrInitQueue(false);
    const config = new Configuration({
        storageClientOptions: {
            localDataDirectory: `./storage/worker-${process.env.WORKER_INDEX}`,
        },
    });

    const crawler = new CheerioCrawler({
        requestQueue,
        maxConcurrency: 5,
    }, config);

    crawler.addHandler('DETAIL', async ({ $, request, log }) => {
        // ... (detail scraping logic)
        process.send({ data: productData });
    });

    await crawler.run();
    process.exit(0);
} else {
    // Parent process logic
    const dataset = await Dataset.open();
    const workerCount = 2;
    const workers = [];

    for (let i = 0; i < workerCount; i++) {
        const worker = fork(import.meta.url, [], {
            env: { ...process.env, IS_WORKER_THREAD: '1', WORKER_INDEX: i.toString() },
        });

        const workerPromise = new Promise((resolve) => {
            worker.on('message', async (message) => {
                if (message.data) {
                    await dataset.pushData(message.data);
                }
            });
            worker.on('exit', resolve);
        });

        workers.push(workerPromise);
    }

    await Promise.all(workers);
    console.log('All workers finished');
}

----------------------------------------

TITLE: Key-Value Store Operations in Crawlee
DESCRIPTION: Demonstrates basic operations with key-value stores including reading input, writing output, and managing named stores

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';

// Get the INPUT from the default key-value store
const input = await KeyValueStore.getInput();

// Write the OUTPUT to the default key-value store
await KeyValueStore.setValue('OUTPUT', { myResult: 123 });

// Open a named key-value store
const store = await KeyValueStore.open('some-name');

// Write a record to the named key-value store.
// JavaScript object is automatically converted to JSON,
// strings and binary buffers are stored as they are
await store.setValue('some-key', { foo: 'bar' });

// Read a record from the named key-value store.
// Note that JSON is automatically parsed to a JavaScript object,
// text data is returned as a string, and other data is returned as binary buffer
const value = await store.getValue('some-key');

// Delete a record from the named key-value store
await store.setValue('some-key', null);

----------------------------------------

TITLE: Standalone SessionPool Usage in Crawlee
DESCRIPTION: This example demonstrates how to use SessionPool independently of a crawler in Crawlee. It shows manual session management, including creating sessions, making requests, and handling session data.

LANGUAGE: js
CODE:
import { SessionPool, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const sessionPool = await SessionPool.open({
    maxPoolSize: 100,
    sessionOptions: {
        maxAgeSecs: 3000,
        maxUsageCount: 5,
    },
    // Configures the proxy to be used by the `SessionPool`
    proxyConfiguration,
});

const session = await sessionPool.getSession();
session.userData.requests = session.userData.requests || 0;

const { statusCode } = await fetch('https://example.com', {
    headers: session.headers,
    proxyUrl: session.proxyUrl,
});

if (statusCode >= 500) {
    session.markBad();
    // Gets a new session
} else {
    session.userData.requests++;
}

if (session.userData.request >= 5) {
    session.retire();
}

----------------------------------------

TITLE: Modifying Detail Route Handler for Parallel Scraping in JavaScript
DESCRIPTION: This code snippet shows how to modify the DETAIL route handler in a Crawlee scraper to send data back to the parent process instead of using the default storage. This is necessary when using child processes for parallel scraping.

LANGUAGE: javascript
CODE:
DETAIL: async ({ $, request, log }) => {
    const title = $('h1').text().trim();
    const price = $('p.price_color').text().trim();
    const availability = $('p.availability').text().trim();
    const description = $('#product_description')?.next('p')?.text()?.trim() ?? null;

    log.info(`Scraped data`, { url: request.url, title });

    // Instead of context.pushData, we use process.send
    process.send({
        url: request.url,
        title,
        price,
        availability,
        description,
    });
}

----------------------------------------

TITLE: Configuring Package.json for TypeScript Build in Crawlee Project
DESCRIPTION: This JSON snippet shows the necessary configuration in package.json for building TypeScript and specifying the main entry point.

LANGUAGE: json
CODE:
{
    "scripts": {
        "build": "tsc"
    },
    "main": "dist/main.js"
}

----------------------------------------

TITLE: Capturing Screenshots with PuppeteerCrawler using page.screenshot()
DESCRIPTION: This snippet demonstrates how to capture screenshots of multiple web pages using PuppeteerCrawler with page.screenshot(). It creates a crawler that visits multiple URLs, takes screenshots, and saves them to a key-value store.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, KeyValueStore } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);

        const screenshotBuffer = await page.screenshot();
        const key = `${request.id}.png`;
        await KeyValueStore.setValue(key, screenshotBuffer, { contentType: 'image/png' });
    },
});

await crawler.run(['https://crawlee.dev', 'https://apify.com']);

----------------------------------------

TITLE: Capturing Screenshot with Puppeteer using utils.puppeteer.saveSnapshot()
DESCRIPTION: This snippet shows how to capture a screenshot using the utils.puppeteer.saveSnapshot() method from Crawlee. It launches a browser, creates a new page, navigates to a URL, and saves a snapshot using the utility function.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';
import { launch } from 'puppeteer';
import { utils } from 'crawlee';

const browser = await launch();
const page = await browser.newPage();
await page.goto('https://example.com');

await utils.puppeteer.saveSnapshot(page, { key: 'my-screenshot' });

await browser.close();

----------------------------------------

TITLE: Customizing Browser Fingerprints with PlaywrightCrawler
DESCRIPTION: This snippet demonstrates how to customize browser fingerprints using PlaywrightCrawler in Crawlee. It sets specific options for the fingerprint generation, including browser, operating system, and screen size.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                browsers: ["chrome"],
                devices: ["desktop"],
                operatingSystems: ["windows"],
                locales: ["en-US"],
                // you can specify any options from the Fingerprint-Generator library
            },
        },
    },
    // ... other options
});

----------------------------------------

TITLE: Configuring Dockerfile for Crawlee Project with Node.js and Playwright
DESCRIPTION: This Dockerfile sets up a Docker image for a Crawlee project. It uses a base image with Node.js and Playwright, installs npm dependencies, copies the project files, and specifies the command to run the project. The build process is optimized for caching and minimal image size.

LANGUAGE: Dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:20

COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD npm start --silent

----------------------------------------

TITLE: Crawling All Links Strategy Implementation
DESCRIPTION: Demonstrates how to crawl all links found on a website regardless of their domain using CheerioCrawler. The crawler will process any URLs it finds, even if they lead to external sites.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks }) {
        // Add all discovered links to the queue, regardless of their domain
        await enqueueLinks({
            strategy: EnqueueStrategy.All,
        });
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: URL List Crawling with JSDOMCrawler
DESCRIPTION: Script that crawls a list of URLs using JSDOMCrawler, parsing HTML content with jsdom to extract page titles and h1 tags. Demonstrates basic web scraping functionality with DOM manipulation.

LANGUAGE: typescript
CODE:
import { JSDOMCrawler, Dataset } from 'crawlee';

const crawler = new JSDOMCrawler({
    async requestHandler({ window, enqueueLinks, log }) {
        const document = window.document;

        const title = document.querySelector('title')?.textContent;
        const h1texts = Array.from(document.querySelectorAll('h1'))
            .map((el) => el.textContent);

        log.info(`Title: ${title}`);
        log.info(`H1 texts: ${h1texts}`);

        await Dataset.pushData({
            title,
            h1texts,
        });
    },
});

// Add URLs to crawler
await crawler.run(['http://example.com']);

----------------------------------------

TITLE: Extracting All Links from a Web Page using Cheerio
DESCRIPTION: This code snippet shows how to use Cheerio to find all <a> elements with an href attribute on a page and extract their href values into an array.

LANGUAGE: javascript
CODE:
$('a[href]')
    .map((i, el) => $(el).attr('href'))
    .get();

----------------------------------------

TITLE: Using Proxy with sendRequest in Crawlee's BasicCrawler
DESCRIPTION: Demonstrates how to use a proxy server with the sendRequest function in Crawlee's BasicCrawler. This example shows how to manually pass the proxyUrl option to hide the real IP address when making requests.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            proxyUrl: 'http://auto:password@proxy.apify.com:8000',
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Using enqueueLinks for Custom Link Selection in Crawlee
DESCRIPTION: Demonstrates how to use the enqueueLinks function with a custom selector to find specific links on a page.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    selector: 'div.has-link'
});

----------------------------------------

TITLE: Exporting Dataset to CSV Using Crawlee
DESCRIPTION: Shows how to use Dataset.exportToValue() to export the entire default dataset to a single CSV file and store it in a key-value store named 'my-data'.

LANGUAGE: javascript
CODE:
import { Dataset, KeyValueStore } from 'crawlee';

// Export entire dataset to a single CSV file
const csvData = await Dataset.exportToValue('CSV');

// Open the default key-value store
const store = await KeyValueStore.open();

// Save the CSV file to the store
await store.setValue('my-data', csvData);

----------------------------------------

TITLE: Implementing JSDOM Crawler for Web Scraping in JavaScript
DESCRIPTION: Demonstrates using JSDOMCrawler to fetch URLs, load them via HTTP requests, and parse HTML using jsdom DOM implementation. The code extracts page titles and h1 tags from each crawled webpage.

LANGUAGE: javascript
CODE:
import { JSDOMCrawler, Dataset } from 'crawlee';
import { readFileSync } from 'fs';

const crawler = new JSDOMCrawler({
    // Function called for each URL
    async requestHandler({ window, $ }) {
        const pageTitle = $('title').text();
        const h1texts = $('h1').map((_, el) => $(el).text()).get();

        // Save data to default dataset
        await Dataset.pushData({
            pageTitle,
            h1texts,
        });
    },
});

// Add URLs to crawl
const sources = JSON.parse(readFileSync('sources.json', 'utf8'));
await crawler.addRequests(sources);

// Run the crawler
await crawler.run();

----------------------------------------

TITLE: Package.json Configuration for Apify Docker Images
DESCRIPTION: Example of how to configure package.json when using Apify Docker images, using asterisk for automation library version.

LANGUAGE: json
CODE:
{
    "dependencies": {
        "crawlee": "^3.0.0",
        "playwright": "*"
    }
}

----------------------------------------

TITLE: Customizing Browser Fingerprints with PlaywrightCrawler
DESCRIPTION: Demonstrates how to customize browser fingerprints in PlaywrightCrawler by specifying operating system, browser versions, and other parameters to avoid detection.

LANGUAGE: javascript
CODE:
{PlaywrightSource}

----------------------------------------

TITLE: Updating Crawlee Configuration for AWS Lambda
DESCRIPTION: Code snippet showing how to update the Crawlee configuration to use a separate storage instance for each crawler in the Lambda environment.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Capturing Screenshot with Puppeteer using page.screenshot()
DESCRIPTION: This snippet demonstrates how to capture a screenshot of a web page using Puppeteer's page.screenshot() method. It launches a browser, creates a new page, navigates to a URL, takes a screenshot, and saves it to a key-value store.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';
import { launch } from 'puppeteer';

const browser = await launch();
const page = await browser.newPage();
await page.goto('https://crawlee.dev');

const screenshot = await page.screenshot();
const key = 'my-promo-image.png';
await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });

await browser.close();

----------------------------------------

TITLE: Crawling Multiple URLs with Cheerio Crawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use Cheerio Crawler to crawl multiple specified URLs. It sets up the crawler, defines the request handler, and starts the crawl.

LANGUAGE: javascript
CODE:
// Code content not available in the provided text

----------------------------------------

TITLE: CheerioCrawler Log Output
DESCRIPTION: Log entries showing the CheerioCrawler visiting and extracting titles from crawlee.dev documentation pages. Shows successful crawling of the main page and documentation sections.

LANGUAGE: log
CODE:
INFO  CheerioCrawler: Starting the crawl
INFO  CheerioCrawler: Title of https://crawlee.dev/ is 'Crawlee · Build reliable crawlers. Fast. | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/guides is 'Guides | Crawlee'

----------------------------------------

TITLE: Parsing Product Price with Playwright
DESCRIPTION: Complex price extraction logic that finds price element, filters for correct text, and converts string price to number format.

LANGUAGE: javascript
CODE:
const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

----------------------------------------

TITLE: sendRequest API Implementation in TypeScript
DESCRIPTION: This code snippet shows the implementation of the sendRequest function. It uses got-scraping to make HTTP requests and includes various options like URL, method, headers, proxy settings, and cookie handling.

LANGUAGE: typescript
CODE:
async sendRequest(overrideOptions?: GotOptionsInit) => {
    return gotScraping({
        url: request.url,
        method: request.method,
        body: request.payload,
        headers: request.headers,
        proxyUrl: crawlingContext.proxyInfo?.url,
        sessionToken: session,
        responseType: 'text',
        ...overrideOptions,
        retry: {
            limit: 0,
            ...overrideOptions?.retry,
        },
        cookieJar: {
            getCookieString: (url: string) => session!.getCookieString(url),
            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),
            ...overrideOptions?.cookieJar,
        },
    });
}

----------------------------------------

TITLE: Finding All Links on a Page with JSDOMCrawler
DESCRIPTION: This snippet demonstrates how to use JSDOMCrawler to find all <a> elements with href attributes on a page and extract their href values into an array. It utilizes the querySelector and map methods.

LANGUAGE: javascript
CODE:
Array.from(document.querySelectorAll('a[href]')).map((a) => a.href);

----------------------------------------

TITLE: Key-Value Store Operations in Crawlee
DESCRIPTION: Demonstrates basic operations with key-value stores including reading input, writing output, and managing named stores

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';

// Get the INPUT from the default key-value store
const input = await KeyValueStore.getInput();

// Write the OUTPUT to the default key-value store
await KeyValueStore.setValue('OUTPUT', { myResult: 123 });

// Open a named key-value store
const store = await KeyValueStore.open('some-name');

// Write a record to the named key-value store.
// JavaScript object is automatically converted to JSON,
// strings and binary buffers are stored as they are
await store.setValue('some-key', { foo: 'bar' });

// Read a record from the named key-value store.
// Note that JSON is automatically parsed to a JavaScript object,
// text data is returned as a string, and other data is returned as binary buffer
const value = await store.getValue('some-key');

// Delete a record from the named key-value store
await store.setValue('some-key', null);

----------------------------------------

TITLE: Crawling Category Pages with PlaywrightCrawler in Crawlee
DESCRIPTION: This snippet demonstrates how to use PlaywrightCrawler to crawl category pages of an online store. It uses the enqueueLinks function with a specific selector to target category links and labels them for later processing.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        // Wait for the category cards to render,
        // otherwise enqueueLinks wouldn't enqueue anything.
        await page.waitForSelector('.collection-block-item');

        // Add links to the queue, but only from
        // elements matching the provided selector.
        await enqueueLinks({
            selector: '.collection-block-item',
            label: 'CATEGORY',
        });
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Standalone Session Pool Implementation
DESCRIPTION: Example showing how to use SessionPool independently without a crawler for manual session management

LANGUAGE: javascript
CODE:
{StandaloneSource}

----------------------------------------

TITLE: Accessing Page Title with Browser JavaScript and JSDOM
DESCRIPTION: Demonstrates how to access the page title using both browser JavaScript and JSDOM syntax, showing the difference between the two approaches.

LANGUAGE: typescript
CODE:
// Return the page title
document.title; // browsers
window.document.title; // JSDOM

----------------------------------------

TITLE: Customizing Browser Fingerprints in PlaywrightCrawler
DESCRIPTION: This snippet demonstrates how to customize browser fingerprints in PlaywrightCrawler. It sets specific options for operating system, browser, and user agent.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    // ...
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                operatingSystems: ['windows'],
                browsers: [{ name: 'firefox', minVersion: 90 }],
                devices: ['desktop'],
                locales: ['en-US', 'en-GB'],
            },
        },
    },
});

----------------------------------------

TITLE: Complete Package.json Configuration for Crawlee TypeScript Project
DESCRIPTION: This JSON shows the complete package.json configuration for a Crawlee TypeScript project, including dependencies, scripts, and module type settings.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "type": "module",
    "main": "dist/main.js",
    "dependencies": {
        "crawlee": "3.0.0"
    },
    "devDependencies": {
        "@apify/tsconfig": "^0.1.0",
        "@types/node": "^18.14.0",
        "ts-node": "^10.8.0",
        "typescript": "^4.7.4"
    },
    "scripts": {
        "start": "npm run start:dev",
        "start:prod": "node dist/main.js",
        "start:dev": "ts-node-esm -T src/main.ts",
        "build": "tsc"
    }
}

----------------------------------------

TITLE: Crawling Multiple URLs with Playwright Crawler in JavaScript
DESCRIPTION: This code snippet illustrates how to use Playwright Crawler to crawl multiple specified URLs. It sets up the crawler and defines the request handler. Note: This requires the 'apify/actor-node-playwright-chrome' image on the Apify Platform.

LANGUAGE: javascript
CODE:
// Code content not available in the provided text

----------------------------------------

TITLE: Crawling Same Domain Links Including Subdomains
DESCRIPTION: Illustrates crawling links that share the same domain name, including links from any subdomain. Matches relative URLs and those pointing to any subdomain of the main domain.

LANGUAGE: javascript
CODE:
{SameDomainSource}

----------------------------------------

TITLE: TypeScript Configuration File
DESCRIPTION: TSConfig setup extending @apify/tsconfig with ES2022 module and target settings for top-level await support

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist"
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Customizing Browser Fingerprints with PlaywrightCrawler
DESCRIPTION: This snippet demonstrates how to customize browser fingerprints using PlaywrightCrawler in Crawlee. It sets specific options for the browser, operating system, and other parameters to generate tailored fingerprints.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    // ...
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                browsers: [
                    { name: 'firefox', minVersion: 88 },
                    { name: 'chrome', minVersion: 94 },
                ],
                devices: [
                    'desktop',
                ],
                operatingSystems: [
                    'windows',
                ],
            },
        },
    },
});

----------------------------------------

TITLE: Configuring Docker Environment for Crawlee Node.js Actor
DESCRIPTION: Sets up a Docker environment for running Crawlee actors based on apify/actor-node:16 image. Implements optimized dependency installation and file copying to leverage Docker layer caching and minimize image size. Excludes development and optional dependencies while maintaining essential functionality.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm start --silent

----------------------------------------

TITLE: Exporting Dataset to CSV File using Crawlee
DESCRIPTION: This code snippet demonstrates how to use Crawlee's Dataset API to export an entire dataset to a single CSV file. It utilizes the `exportToValue` function and stores the result in the default key-value store.

LANGUAGE: javascript
CODE:
import { Dataset, KeyValueStore } from 'crawlee';

await Dataset.open();

// Export entire dataset to CSV
const csv = await Dataset.exportToValue('csv');

// Save the CSV file to the default key-value store
await KeyValueStore.setValue('DATASET.csv', csv, { contentType: 'text/csv' });

----------------------------------------

TITLE: Saving Data to Default Dataset in Crawlee
DESCRIPTION: This snippet demonstrates how to save data to the default dataset in Crawlee. If the dataset doesn't exist, it will be created automatically. The code uses the Dataset.pushData() method to add items to the dataset.

LANGUAGE: javascript
CODE:
import { Dataset } from 'crawlee';

await Dataset.pushData({
    title: 'Crawlee',
    description: 'Crawlee is a web scraping and browser automation library.',
    url: 'https://crawlee.dev',
});

----------------------------------------

TITLE: sendRequest API Implementation in TypeScript
DESCRIPTION: This code snippet shows the implementation of the sendRequest function. It uses got-scraping to make HTTP requests and includes various options like URL, method, headers, proxy settings, and cookie handling.

LANGUAGE: typescript
CODE:
async sendRequest(overrideOptions?: GotOptionsInit) => {
    return gotScraping({
        url: request.url,
        method: request.method,
        body: request.payload,
        headers: request.headers,
        proxyUrl: crawlingContext.proxyInfo?.url,
        sessionToken: session,
        responseType: 'text',
        ...overrideOptions,
        retry: {
            limit: 0,
            ...overrideOptions?.retry,
        },
        cookieJar: {
            getCookieString: (url: string) => session!.getCookieString(url),
            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),
            ...overrideOptions?.cookieJar,
        },
    });
}

----------------------------------------

TITLE: Configuring Docker Environment for Crawlee Actor
DESCRIPTION: Configures a Docker container for running Crawlee actors with Node.js and Playwright. The setup process includes copying package files, installing dependencies, and setting up the execution environment. It uses layer caching for optimized builds and minimal image size.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16

COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD npm start --silent

----------------------------------------

TITLE: Using actor-node Docker Image
DESCRIPTION: Example of how to use the smallest Apify Docker image based on Alpine Linux, suitable for CheerioCrawler.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16

----------------------------------------

TITLE: Scraping Product Title with Playwright
DESCRIPTION: Shows how to extract the product title using Playwright's locator to find the h1 element within product-meta class.

LANGUAGE: javascript
CODE:
const title = await page.locator('.product-meta h1').textContent();

----------------------------------------

TITLE: Screenshot Capture with PuppeteerCrawler
DESCRIPTION: Example of capturing screenshots across multiple pages using PuppeteerCrawler with page.screenshot(). Screenshots are saved with URL-based keys.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, KeyValueStore } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // Create a unique key for the screenshot based on URL
        const key = `screenshot-${request.id}`;

        // Capture the screenshot
        const screenshot = await page.screenshot();

        // Save the screenshot to the default key-value store
        await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Authenticating with Apify CLI
DESCRIPTION: Command to log into the Apify Platform using the CLI tool. Requires an Apify account and personal access token.

LANGUAGE: bash
CODE:
apify login

----------------------------------------

TITLE: Complete AWS Lambda Handler for Cheerio Crawler with Data Return
DESCRIPTION: This is the final version of the AWS Lambda handler function. It initializes the Cheerio crawler, runs it, and returns the scraped data as the Lambda function's response.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);

    return {
        statusCode: 200,
        body: await crawler.getData(),
    }
};

----------------------------------------

TITLE: Creating Express Server Handler for Cloud Run Deployment
DESCRIPTION: Implements an Express HTTP server that wraps the crawler functionality. The server listens on the port specified by GCP's PORT environment variable and handles incoming HTTP requests in a stateless manner.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import express from 'express';
const app = express();

const startUrls = ['https://crawlee.dev'];

app.get('/', async (req, res) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));
    
    await crawler.run(startUrls);    

    return res.send(await crawler.getData());
});

app.listen(parseInt(process.env.PORT) || 3000);

----------------------------------------

TITLE: Sanity Check with Playwright Crawler
DESCRIPTION: This code snippet demonstrates a basic setup for crawling a warehouse store website using PlaywrightCrawler. It visits the start URL and prints the text content of all category elements on the page.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${page.url()} is '${title}'`);

        // Extract text content from all elements with class 'collection-block-item'
        const categoryElements = await page.$$('.collection-block-item');
        for (const element of categoryElements) {
            const text = await element.textContent();
            console.log('Category text:', text);
        }

        // Add all links from page to crawler's queue
        await enqueueLinks();
    },
});

// Start the crawler with the provided URLs
await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

await Dataset.open().exportToJSON('results.json');

----------------------------------------

TITLE: Implementing Cheerio Crawler for HTML Parsing in JavaScript
DESCRIPTION: Example showing how to use CheerioCrawler to process URLs from an external file, make HTTP requests, and parse HTML content using the Cheerio library to extract page titles and h1 tags.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        const h1texts = [];
        $('h1').each((i, el) => {
            h1texts.push($(el).text());
        });

        console.log(
            `The title of ${request.url} is: ${title}\n`,
            `The page contains ${h1texts.length} <h1> tags.\n`,
            'Their text content is: ', h1texts,
        );
    }
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Modifying Detail Route Handler for Parallel Scraping in JavaScript
DESCRIPTION: Updates the DETAIL route handler to send scraped data back to the parent process using process.send instead of pushing directly to the dataset. This change is necessary for parallel scraping with child processes.

LANGUAGE: javascript
CODE:
router.addHandler('DETAIL', async ({ request, log, $ }) => {
    log.info(`Scraping ${request.url}`);
    const title = $('h1').text().trim();
    const price = $('p.price_color').text().trim();
    const availability = $('p.availability').text().trim();

    process.send({
        url: request.url,
        title,
        price,
        availability,
    });
});

----------------------------------------

TITLE: Custom Configuration Implementation in Crawlee
DESCRIPTION: Example showing how to create and use a custom Configuration instance with a CheerioCrawler.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

const config = new Configuration({
    persistStateIntervalMillis: 10_000,
});

const crawler = new CheerioCrawler({}, config);

crawler.router.addDefaultHandler(async ({ request }) => {
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Disabling Browser Fingerprints with PlaywrightCrawler
DESCRIPTION: This snippet demonstrates how to disable browser fingerprints in PlaywrightCrawler. It sets the 'useFingerprints' option to false in the browserPoolOptions.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    // ...
    browserPoolOptions: {
        useFingerprints: false,
    },
});

----------------------------------------

TITLE: Using actor-node-puppeteer-chrome Docker Image
DESCRIPTION: Demonstrates the use of a Docker image that includes Puppeteer and Chrome, suitable for CheerioCrawler and PuppeteerCrawler.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-puppeteer-chrome:16

----------------------------------------

TITLE: Crawling All Links with CheerioCrawler in Crawlee
DESCRIPTION: This snippet demonstrates how to use CheerioCrawler to crawl all links found on a website, regardless of their domain. It uses the 'all' enqueue strategy.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks, log }) {
        log.info('Crawling');        
        await enqueueLinks({
            strategy: EnqueueStrategy.All,
            transformRequestFunction(req) {
                // ...modify request object...
                return req;
            },
        });
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Basic Crawler Implementation with crawlee.json Configuration
DESCRIPTION: Example showing a CheerioCrawler implementation that relies on crawlee.json configuration without explicit configuration in code.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, sleep } from 'crawlee';

const crawler = new CheerioCrawler();

crawler.router.addDefaultHandler(async ({ request }) => {
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Using All Strategy in Crawlee's enqueueLinks
DESCRIPTION: Shows how to configure enqueueLinks to follow all links regardless of domain using the 'all' strategy.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    strategy: 'all', // wander the internet
});

----------------------------------------

TITLE: Configuring tsconfig.json for Crawlee TypeScript Project
DESCRIPTION: TypeScript configuration file (tsconfig.json) setup for a Crawlee project, extending @apify/tsconfig and enabling top-level await.

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist"
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Using actor-node-puppeteer-chrome Docker Image
DESCRIPTION: Demonstrates the use of a Docker image that includes Puppeteer and Chrome, suitable for CheerioCrawler and PuppeteerCrawler.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-puppeteer-chrome:16

----------------------------------------

TITLE: Scraping and Processing Product Price
DESCRIPTION: Shows how to extract and process the product price by filtering elements and converting string to number.

LANGUAGE: javascript
CODE:
const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

----------------------------------------

TITLE: Configuring Package.json Build Scripts
DESCRIPTION: Basic package.json configuration for TypeScript build process and main entry point

LANGUAGE: json
CODE:
{
    "scripts": {
        "build": "tsc"
    },
    "main": "dist/main.js"
}

----------------------------------------

TITLE: Complete Package.json Configuration
DESCRIPTION: Full package.json configuration including all dependencies and scripts for development and production

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "type": "module",
    "main": "dist/main.js",
    "dependencies": {
        "crawlee": "3.0.0"
    },
    "devDependencies": {
        "@apify/tsconfig": "^0.1.0",
        "@types/node": "^18.14.0",
        "ts-node": "^10.8.0",
        "typescript": "^4.7.4"
    },
    "scripts": {
        "start": "npm run start:dev",
        "start:prod": "node dist/main.js",
        "start:dev": "ts-node-esm -T src/main.ts",
        "build": "tsc"
    }
}

----------------------------------------

TITLE: Disabling Browser Fingerprints with PuppeteerCrawler
DESCRIPTION: This snippet shows how to disable browser fingerprints in PuppeteerCrawler. It sets the 'useFingerprints' option to false in the browserPoolOptions.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    // ...
    browserPoolOptions: {
        useFingerprints: false,
    },
});

----------------------------------------

TITLE: Multi-stage Docker Build Configuration for Crawlee
DESCRIPTION: Implements a two-stage Docker build process for a Crawlee project. The first stage builds the application with development dependencies, while the second stage creates a production-ready image with minimal dependencies. Includes optimizations for Docker layer caching and dependency management.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16 AS builder

COPY package*.json ./

RUN npm install --include=dev --audit=false

COPY . ./

RUN npm run build

FROM apify/actor-node:16

COPY --from=builder /usr/src/app/dist ./dist

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm run start:prod --silent

----------------------------------------

TITLE: Scraping Product Title
DESCRIPTION: Shows how to extract the product title using Playwright's locator to find the h1 element within product-meta class.

LANGUAGE: javascript
CODE:
const title = await page.locator('.product-meta h1').textContent();

----------------------------------------

TITLE: Skipping Navigation with PlaywrightCrawler in Crawlee
DESCRIPTION: Demonstrates how to use Request#skipNavigation with sendRequest to fetch and save CDN-served images without full crawler navigation. This functionality works across all Crawlee crawlers.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';
import { writeFile } from 'fs/promises';

const crawler = new PlaywrightCrawler({
    async requestHandler({ request, sendRequest, log }) {
        if (request.skipNavigation) {
            log.info('Skipping navigation and saving the image directly');
            
            const imageBuffer = await sendRequest();
            await writeFile('image.jpg', imageBuffer);
            return;
        }

        log.info('Processing a listed product...');
    }
});

await crawler.addRequests([
    {
        url: 'https://example.com/image-from-cdn.jpg',
        skipNavigation: true,
    },
    { url: 'https://example.com/product/1' },
]);

----------------------------------------

TITLE: Interacting with React Calculator using JSDOMCrawler
DESCRIPTION: Demonstrates how to use JSDOMCrawler to interact with a React calculator application. The script performs button clicks (1 + 1 =) and extracts the calculation result using jsdom DOM implementation.

LANGUAGE: typescript
CODE:
{JSDOMCrawlerRunScriptSource}

----------------------------------------

TITLE: Capturing Screenshots with PuppeteerCrawler Using utils.puppeteer.saveSnapshot()
DESCRIPTION: This snippet shows how to capture screenshots of multiple web pages using PuppeteerCrawler with the context-aware utils.puppeteer.saveSnapshot() utility. It sets up a crawler that visits multiple URLs and uses the saveSnapshot utility to capture and save screenshots.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks, log, pushData }) {
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        await enqueueLinks();

        // This will save a screenshot with default options to key value store
        await this.utils.puppeteer.saveSnapshot({
            key: `my-page-${request.id}`,
            screenshotQuality: 60,
        });

        await pushData({ title, url: request.url });
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Checking NPM Version
DESCRIPTION: Command to verify NPM package manager installation

LANGUAGE: bash
CODE:
npm -v

----------------------------------------

TITLE: Extracting Manufacturer from URL
DESCRIPTION: Demonstrates how to extract the manufacturer name from a product URL by splitting the string.

LANGUAGE: javascript
CODE:
const urlPart = request.url.split('/').slice(-1);
const manufacturer = urlPart[0].split('-')[0];

----------------------------------------

TITLE: Performing Recursive Web Crawling with PuppeteerCrawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to set up and execute a recursive web crawl using PuppeteerCrawler from Crawlee. It includes configuration for the crawler, request handling, and data processing.

LANGUAGE: javascript
CODE:
{CrawlSource}

----------------------------------------

TITLE: Using Proxy with sendRequest in Crawlee
DESCRIPTION: Demonstrates how to use a proxy server with sendRequest in a BasicCrawler's requestHandler.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            proxyUrl: 'http://auto:password@proxy.apify.com:8000',
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Crawling Sitemap with Puppeteer Crawler in Crawlee
DESCRIPTION: This snippet shows how to use Puppeteer Crawler to crawl a sitemap. It downloads the sitemap URLs, creates a RequestQueue, and processes each URL to extract the page title using browser automation.

LANGUAGE: javascript
CODE:
import { PuppeteerRouter, PuppeteerCrawler } from 'crawlee';
import { downloadListOfUrls } from '@crawlee/utils';

const router = new PuppeteerRouter();

router.addDefaultHandler(async ({ page, request, log }) => {
    const title = await page.title();
    log.info(`Title of ${request.url} is: ${title}`);
});

const crawler = new PuppeteerCrawler({
    requestHandler: router,
});

const SITEMAP_URL = 'https://crawlee.dev/sitemap.xml';

await crawler.run([
    ...await downloadListOfUrls({ url: SITEMAP_URL }),
]);


----------------------------------------

TITLE: Initializing CheerioCrawler with Unique Configuration for AWS Lambda
DESCRIPTION: This snippet demonstrates how to create a CheerioCrawler instance with a unique Configuration object, setting persistStorage to false for Lambda compatibility.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Configuring PlaywrightCrawler with AWS Chromium Settings
DESCRIPTION: JavaScript code snippet demonstrating how to configure PlaywrightCrawler with the Chromium executable path and arguments from @sparticuz/chromium for AWS Lambda compatibility.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
    launchContext: {
        launchOptions: {
             executablePath: await aws_chromium.executablePath(),
             args: aws_chromium.args,
             headless: true
        }
    }
}, new Configuration({
    persistStorage: false,
}));

----------------------------------------

TITLE: Crawling Multiple URLs with JSDOMCrawler in TypeScript
DESCRIPTION: This snippet shows how to use JSDOMCrawler to crawl a list of URLs from an external file. It loads each URL using an HTTP request, parses the HTML with jsdom, and extracts the page title and all h1 tags.

LANGUAGE: typescript
CODE:
import { JSDOMCrawler, Dataset } from 'crawlee';
import { readFile } from 'fs/promises';

const crawler = new JSDOMCrawler({
    async requestHandler({ window, request, log }) {
        const { document } = window;
        const title = document.querySelector('title')?.textContent;
        const h1 = document.querySelector('h1')?.textContent;

        log.info(`The title of "${request.url}" is: ${title}`);
        log.info(`The first <h1> tag of "${request.url}" is: ${h1}`);

        await Dataset.pushData({
            url: request.url,
            title,
            h1,
        });
    },
});

const urls = JSON.parse(await readFile('urls.json', 'utf8'));
await crawler.run(urls);

----------------------------------------

TITLE: Failing to Scrape Dynamic Content with PlaywrightCrawler (No Wait)
DESCRIPTION: This snippet shows an incorrect way to use PlaywrightCrawler without waiting for elements to render, which results in a failure to scrape the content.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        // This will fail because the element might not be rendered yet
        const actorText = await page.textContent('.ActorStoreItem');
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Getting Public URL for Stored Item
DESCRIPTION: Example of getting a public URL for an item stored in Key-Value Store

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'apify';

const store = await KeyValueStore.open();
await store.setValue('your-file', { foo: 'bar' });
const url = store.getPublicUrl('your-file');
// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file

----------------------------------------

TITLE: Fetching HTML Content with got-scraping
DESCRIPTION: Demonstrates how to use the got-scraping package to retrieve HTML content from a specified URL. The example shows basic web scraping setup with dynamic content handling.

LANGUAGE: javascript
CODE:
import { gotScraping } from 'got-scraping';

const response = await gotScraping('https://example.com');
console.log(response.body);

----------------------------------------

TITLE: Configuring INPUT.json File Path in Crawlee
DESCRIPTION: Demonstrates the file path structure for providing input to a Crawlee actor through the INPUT.json file in the default key-value store.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json

----------------------------------------

TITLE: Failing to Scrape Dynamic Content with PlaywrightCrawler (No Wait)
DESCRIPTION: This snippet shows an incorrect way to use PlaywrightCrawler without waiting for elements to render, which results in a failure to scrape the content.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        // This will fail because the element might not be rendered yet
        const actorText = await page.textContent('.ActorStoreItem');
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Capturing Screenshots with PuppeteerCrawler Using page.screenshot()
DESCRIPTION: This example demonstrates how to capture screenshots of multiple web pages using PuppeteerCrawler with the page.screenshot() method. It sets up a crawler that visits multiple URLs, takes screenshots, and saves them to a key-value store.

LANGUAGE: javascript
CODE:
import { KeyValueStore, PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);

        const screenshot = await page.screenshot();
        const key = `screenshot-${new URL(request.url).hostname}`;
        await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });
    },
});

await crawler.run(['https://crawlee.dev', 'https://apify.com']);

----------------------------------------

TITLE: Building CheerioCrawler with RequestQueue
DESCRIPTION: Demonstrates how to create a CheerioCrawler instance with a RequestQueue and request handler to extract page titles using Cheerio.

LANGUAGE: typescript
CODE:
// Add import of CheerioCrawler
import { RequestQueue, CheerioCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

// Create the crawler and add the queue with our URL
// and a request handler to process the page.
const crawler = new CheerioCrawler({
    requestQueue,
    // The `$` argument is the Cheerio object
    // which contains parsed HTML of the website.
    async requestHandler({ $, request }) {
        // Extract <title> text with Cheerio.
        // See Cheerio documentation for API docs.
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

// Start the crawler and wait for it to finish
await crawler.run();

----------------------------------------

TITLE: Scraping Product Title with Playwright
DESCRIPTION: This code uses Playwright to locate and extract the product title from a webpage using a CSS selector.

LANGUAGE: javascript
CODE:
const title = await page.locator('.product-meta h1').textContent();

----------------------------------------

TITLE: Extracting Product SKU with Playwright
DESCRIPTION: This snippet shows how to use Playwright to locate and extract the product SKU from a specific HTML element.

LANGUAGE: javascript
CODE:
const sku = await page.locator('span.product-meta__sku-number').textContent();

----------------------------------------

TITLE: Customizing Browser Fingerprints with PuppeteerCrawler
DESCRIPTION: This snippet shows how to customize browser fingerprints using PuppeteerCrawler in Crawlee. It sets specific operating system, browser, and language preferences for the fingerprint generation.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                operatingSystems: ['windows'],
                browsers: [{ name: 'firefox', minVersion: 80 }],
                languages: ['en-US', 'en']
            }
        }
    },
    // ...
});

----------------------------------------

TITLE: Configuring Docker Environment for Crawlee Actor
DESCRIPTION: Dockerfile that sets up a Node.js environment for running Crawlee actors. It uses apify/actor-node:20 as base image, installs production dependencies, copies source code, and configures the container startup command.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:20

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm start --silent

----------------------------------------

TITLE: Using Pre-release Docker Image
DESCRIPTION: Shows how to use a pre-release (beta) version of an Apify Docker image, both with and without specifying the automation library version.

LANGUAGE: dockerfile
CODE:
# Without library version.
FROM apify/actor-node:20-beta

LANGUAGE: dockerfile
CODE:
# With library version.
FROM apify/actor-node-playwright-chrome:20-1.10.0-beta

----------------------------------------

TITLE: Using Dataset Reduce Method in Crawlee
DESCRIPTION: Shows how to use the Dataset reduce method to calculate the total number of headers across all scraped pages in the dataset.

LANGUAGE: javascript
CODE:
import { Dataset, KeyValueStore } from 'crawlee';

const dataset = await Dataset.open();
const pagesHeadingCount = await dataset.reduce((memo, value) => {
    return memo + value.headingCount;
}, 0);

// Save the result to the default key-value store
await KeyValueStore.setValue('total-pages-heading-count', pagesHeadingCount);

----------------------------------------

TITLE: Extracting All Links from a Page using JSDOM
DESCRIPTION: This code snippet demonstrates how to use JSDOM to find all <a> elements with href attributes on a page and extract their href values into an array. It utilizes querySelector and Array methods for efficient extraction.

LANGUAGE: javascript
CODE:
Array.from(document.querySelectorAll('a[href]')).map((a) => a.href);

----------------------------------------

TITLE: Configuring PlaywrightCrawler with Unique Storage in JavaScript
DESCRIPTION: Code snippet showing how to initialize a PlaywrightCrawler with a new Configuration instance to ensure unique storage for each crawler instance in Lambda.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Purging Default Storage in Crawlee
DESCRIPTION: Shows how to clean up default storage directories using the purgeDefaultStorages helper function

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Configuring Docker Environment for Crawlee Actor
DESCRIPTION: Defines a Docker build process optimized for Crawlee actors. Uses layer caching for faster builds, installs production dependencies only, and sets up the execution environment. The build process is designed to be efficient for development iterations while maintaining a small image size.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm start --silent

----------------------------------------

TITLE: Creating Input JSON File for Crawlee Project
DESCRIPTION: This bash snippet shows the file path where an INPUT.json file should be created to provide input to a Crawlee actor. The file should be placed in the project's key-value store.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json

----------------------------------------

TITLE: Crawling Multiple URLs with Cheerio Crawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use Cheerio Crawler to crawl a list of specified URLs. It extracts the title and URL from each page and stores the results.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks, log }) {
        const title = $('title').text();
        log.info(`Title of ${request.url} is '${title}'`);

        await Dataset.pushData({
            title,
            url: request.url,
        });
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run([
    'https://crawlee.dev',
    'https://crawlee.dev/docs/quick-start',
    'https://crawlee.dev/docs/introduction/first-scraper',
]);

----------------------------------------

TITLE: Implementing Recursive Web Crawling with PuppeteerCrawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to set up and execute a recursive web crawl using PuppeteerCrawler. It includes configuration for the crawler, request handling, and data extraction logic.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, Dataset } from 'crawlee';

const crawler = new PuppeteerCrawler({
    // Function called for each URL
    async requestHandler({ request, page, enqueueLinks }) {
        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);
        await Dataset.pushData({ title, url: request.url });

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    // Function for ignoring navigation errors
    failedRequestHandler({ request }) {
        console.log(`Request ${request.url} failed too many times`);
    },
    maxRequestsPerCrawl: 100, // Limit the number of requests
});

// Run the crawler with initial URL
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Setting Min and Max Concurrency in Crawlee
DESCRIPTION: This example shows how to configure the minConcurrency and maxConcurrency options in a CheerioCrawler. It sets a minimum of 5 and a maximum of 10 concurrent requests.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    minConcurrency: 5,
    maxConcurrency: 10,
    // ... other options
});

----------------------------------------

TITLE: Using actor-node-playwright-firefox Docker Image
DESCRIPTION: This snippet shows how to use the Apify Docker image that includes Playwright with Firefox pre-installed.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-firefox:16

----------------------------------------

TITLE: Creating Tabbed Interface for Crawlee Crawler Examples
DESCRIPTION: This JSX snippet creates a tabbed interface using React components. It displays code examples for Cheerio, Puppeteer, and Playwright crawlers, with additional tips for running Puppeteer and Playwright crawlers on the Apify Platform.

LANGUAGE: jsx
CODE:
<Tabs groupId="crawler-type">

<TabItem value="cheerio_crawler" label="Cheerio Crawler" default>

<RunnableCodeBlock className="language-js" type="cheerio">
	{CheerioSource}
</RunnableCodeBlock>

</TabItem>

<TabItem value="puppeteer_crawler" label="Puppeteer Crawler">

:::tip

To run this example on the Apify Platform, select the `apify/actor-node-puppeteer-chrome` image for your Dockerfile.

:::

<RunnableCodeBlock className="language-js" type="puppeteer">
	{PuppeteerSource}
</RunnableCodeBlock>

</TabItem>

<TabItem value="playwright_crawler" label="Playwright Crawler">

:::tip

To run this example on the Apify Platform, select the `apify/actor-node-playwright-chrome` image for your Dockerfile.

:::

<RunnableCodeBlock className="language-js" type="playwright">
	{PlaywrightSource}
</RunnableCodeBlock>

</TabItem>

</Tabs>

----------------------------------------

TITLE: Using CookieJar with BasicCrawler in Crawlee
DESCRIPTION: This example shows how to use a CookieJar with BasicCrawler for managing cookies. It creates a new CookieJar instance and passes it to the sendRequest function to handle cookie storage and retrieval.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';
import { CookieJar } from 'tough-cookie';

const cookieJar = new CookieJar();

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ cookieJar });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Basic Cheerio Crawler Implementation in TypeScript
DESCRIPTION: Simple crawler that downloads a single page's HTML and extracts its title using Cheerio.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Capturing Screenshots with PuppeteerCrawler using utils.puppeteer.saveSnapshot()
DESCRIPTION: This snippet shows how to capture screenshots of multiple web pages using PuppeteerCrawler and the context-aware utils.puppeteer.saveSnapshot() utility. It defines a crawler that visits multiple URLs and saves snapshots using the utility function.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, puppeteerUtils }) {
        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);

        await puppeteerUtils.saveSnapshot();
    },
});

await crawler.run(['https://example.com', 'https://example.com/about']);

----------------------------------------

TITLE: Updating package.json for GCP Cloud Functions
DESCRIPTION: Set the "main" field in package.json to point to the entry file for the cloud function.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "version": "1.0.0",
    "main": "src/main.js",
    ...
}

----------------------------------------

TITLE: Complete package.json Configuration for Crawlee TypeScript Project
DESCRIPTION: Full package.json setup for a Crawlee TypeScript project, including dependencies, scripts, and module configuration.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "type": "module",
    "main": "dist/main.js",
    "dependencies": {
        "crawlee": "3.0.0"
    },
    "devDependencies": {
        "@apify/tsconfig": "^0.1.0",
        "@types/node": "^18.14.0",
        "ts-node": "^10.8.0",
        "typescript": "^4.7.4"
    },
    "scripts": {
        "start": "npm run start:dev",
        "start:prod": "node dist/main.js",
        "start:dev": "ts-node-esm -T src/main.ts",
        "build": "tsc"
    }
}

----------------------------------------

TITLE: Crawling All Links with Cheerio Crawler in Crawlee
DESCRIPTION: This code snippet demonstrates how to use Cheerio Crawler in Crawlee to crawl all links on a website. It sets up a RequestQueue, creates a CheerioCrawler, and uses the enqueueLinks() method to add new links to the queue as it crawls.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, enqueueLinks } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limit to 10 requests
});

// Run the crawler
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: CheerioCrawler Log Output
DESCRIPTION: Log entries showing CheerioCrawler execution, including the starting message and successful page visits with extracted titles from crawlee.dev documentation pages.

LANGUAGE: log
CODE:
INFO  CheerioCrawler: Starting the crawl
INFO  CheerioCrawler: Title of https://crawlee.dev/ is 'Crawlee · Build reliable crawlers. Fast. | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/guides is 'Guides | Crawlee'

----------------------------------------

TITLE: Initializing RequestQueue in Crawlee
DESCRIPTION: Demonstrates how to create a RequestQueue instance and add a URL request to it. This is the basic setup for managing URLs to be crawled.

LANGUAGE: typescript
CODE:
import { RequestQueue } from 'crawlee';

// First you create the request queue instance.
const requestQueue = await RequestQueue.open();
// And then you add one or more requests to it.
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

----------------------------------------

TITLE: Limiting Crawlee Crawler Requests
DESCRIPTION: Demonstrates how to set a maximum limit on the number of requests a crawler will process using the maxRequestsPerCrawl option.

LANGUAGE: typescript
CODE:
const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 20,
    // ...
});

----------------------------------------

TITLE: Configuring Docker Environment for Crawlee Actor
DESCRIPTION: Sets up a Docker container with Node.js, Playwright, and Chrome for running Crawlee actors. The configuration optimizes build performance through layer caching and reduces image size by excluding unnecessary dependencies. It includes steps for package installation, dependency verification, and source code copying.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16

COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD npm start --silent

----------------------------------------

TITLE: Cheerio-based Sitemap Crawler Implementation
DESCRIPTION: Uses Cheerio Crawler to process URLs from a sitemap using the downloadListOfUrls utility from @crawlee/utils. Cheerio provides a lightweight solution for HTML parsing without a browser engine.

LANGUAGE: typescript
CODE:
{CheerioSource}

----------------------------------------

TITLE: Custom Link Selector with enqueueLinks
DESCRIPTION: Example showing how to override the default link selector in enqueueLinks function.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    selector: 'div.has-link'
});

----------------------------------------

TITLE: Scraping Static Content with CheerioCrawler in TypeScript
DESCRIPTION: This snippet demonstrates an attempt to scrape JavaScript-rendered content using CheerioCrawler, which fails because it can't execute client-side JavaScript.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        // Extract text content of an actor card
        const actorText = $('.ActorStoreItem').text();
        console.log(`ACTOR: ${actorText}`);
    }
})

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Finding All Links on a Page using Cheerio
DESCRIPTION: This snippet shows how to find all <a> elements with an href attribute on a page and extract their href values into an array using Cheerio.

LANGUAGE: javascript
CODE:
$('a[href]')
    .map((i, el) => $(el).attr('href'))
    .get();

----------------------------------------

TITLE: Configuring Dockerfile for Crawlee Project with Node.js and Playwright
DESCRIPTION: This Dockerfile sets up a container for a Crawlee project. It uses an Apify base image with Node.js and Playwright, installs NPM dependencies, copies project files, and configures the container to run the project. It optimizes build speed using Docker layer caching.

LANGUAGE: Dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16

COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD npm start --silent

----------------------------------------

TITLE: Starting Crawlee Project
DESCRIPTION: Commands to navigate to project directory and start the crawler

LANGUAGE: bash
CODE:
cd my-crawler
npm start

----------------------------------------

TITLE: Capturing Screenshots with Puppeteer Utils
DESCRIPTION: Demonstrates using utils.puppeteer.saveSnapshot() to capture screenshots with additional utilities and automatic key generation.

LANGUAGE: javascript
CODE:
import { utils } from 'crawlee';
import { launch } from 'puppeteer';

const browser = await launch();
const page = await browser.newPage();
await page.goto('https://crawlee.dev');

// Capture and save the screenshot using the utility function
await utils.puppeteer.saveSnapshot(page, { key: 'my-screenshot' });

await browser.close();

----------------------------------------

TITLE: Crawling with CheerioCrawler in JavaScript
DESCRIPTION: Example code for performing a recursive crawl of the Crawlee website using CheerioCrawler.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        await Dataset.pushData({
            url: request.url,
            title,
        });
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Configuring Docker Environment for Crawlee Actor
DESCRIPTION: Sets up a Docker container with Node.js, Playwright, and Chrome for running Crawlee actors. The configuration optimizes build performance through layer caching and reduces image size by excluding unnecessary dependencies. It includes steps for package installation, dependency verification, and source code copying.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16

COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD npm start --silent

----------------------------------------

TITLE: Combining RequestQueueV2 with CheerioCrawler
DESCRIPTION: Demonstrates how to use a custom RequestQueueV2 instance with CheerioCrawler while enabling the request locking experiment.

LANGUAGE: typescript
CODE:
import { CheerioCrawler, RequestQueueV2 } from 'crawlee';

const queue = await RequestQueueV2.open('my-locking-queue');

const crawler = new CheerioCrawler({
    experiments: {
        requestLocking: true,
    },
    requestQueue: queue,
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

await crawler.run();

----------------------------------------

TITLE: Configuring Docker Environment for Crawlee Actor
DESCRIPTION: Dockerfile that sets up a Node.js environment for Crawlee web scraping. Uses layer caching for efficient builds by separating dependency installation from source code copying. Based on apify/actor-node:16 image with production-only NPM packages.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm start --silent

----------------------------------------

TITLE: Cookie Jar Implementation
DESCRIPTION: Shows how to use a custom cookie jar with BasicCrawler's sendRequest function

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';
import { CookieJar } from 'tough-cookie';

const cookieJar = new CookieJar();

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ cookieJar });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Comparing DOM Manipulation: Browser JS vs Cheerio
DESCRIPTION: Demonstrates the syntax differences between browser JavaScript and Cheerio for common DOM operations like selecting elements and extracting attributes.

LANGUAGE: typescript
CODE:
// Return the text content of the <title> element.
document.querySelector('title').textContent; // plain JS
$('title').text(); // Cheerio

// Return an array of all 'href' links on the page.
Array.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS
$('[href]')
    .map((i, el) => $(el).attr('href'))
    .get(); // Cheerio

----------------------------------------

TITLE: Purging Default Storages in Crawlee
DESCRIPTION: This snippet demonstrates how to explicitly purge default storages in Crawlee using the purgeDefaultStorages() helper function.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Crawling a Single URL using got-scraping in JavaScript
DESCRIPTION: This code snippet demonstrates how to use the got-scraping package to fetch the HTML content of a single URL. It sets up a basic crawler, defines the starting URL, and logs the title of the page.

LANGUAGE: JavaScript
CODE:
import { gotScraping } from 'got-scraping';

const url = 'https://crawlee.dev';

console.log('Fetching URL:', url);

const response = await gotScraping(url);
const html = response.body;

// Here you can add your own parsing logic
console.log('HTML:', html);

// For example, you can use cheerio to parse the HTML
import cheerio from 'cheerio';
const $ = cheerio.load(html);
const pageTitle = $('title').text();
console.log('Page title:', pageTitle);

----------------------------------------

TITLE: Scraping Dynamic Content with PuppeteerCrawler
DESCRIPTION: This snippet demonstrates how to use PuppeteerCrawler to scrape JavaScript-rendered content, explicitly waiting for elements to appear before extracting data.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // With Puppeteer, we need to wait explicitly
        await page.waitForSelector('.ActorStoreItem');
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Extracting Manufacturer from URL in JavaScript
DESCRIPTION: This snippet demonstrates how to extract the manufacturer name from a product URL by splitting the string.

LANGUAGE: javascript
CODE:
// request.url = https://warehouse-theme-metal.myshopify.com/products/sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440

const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']
const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'

----------------------------------------

TITLE: JSON Response Handling
DESCRIPTION: Demonstrates how to configure the crawler to handle JSON responses instead of default text responses.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ responseType: 'json' });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Comparing Cheerio and Browser JavaScript for DOM Manipulation
DESCRIPTION: This snippet demonstrates how to perform common DOM operations using both Cheerio and plain browser JavaScript. It shows how to select the title element and extract all href links from a page.

LANGUAGE: typescript
CODE:
// Return the text content of the <title> element.
document.querySelector('title').textContent; // plain JS
$('title').text(); // Cheerio

// Return an array of all 'href' links on the page.
Array.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS
$('[href]')
    .map((i, el) => $(el).attr('href'))
    .get(); // Cheerio

----------------------------------------

TITLE: Logging into Apify Platform using CLI
DESCRIPTION: Commands to install Apify CLI and log in with an API token.

LANGUAGE: bash
CODE:
npm install -g apify-cli
apify login -t YOUR_API_TOKEN

----------------------------------------

TITLE: Implementing Request Skip Navigation in PlaywrightCrawler
DESCRIPTION: Demonstrates how to skip full browser navigation for CDN images while still saving them using PlaywrightCrawler. This optimization technique combines Request#skipNavigation with sendRequest to efficiently handle resource downloads without full page loads.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, KeyValueStore } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ request, sendRequest, log }) {
        // If the URL is an image from a CDN...
        if (request.url.includes('cdn') && request.url.match(/\.(jpg|jpeg|png|webp)$/)) {
            log.info('Skipping navigation for CDN image');
            // We can skip the navigation and just download the image directly
            const response = await sendRequest({
                ...request,
                skipNavigation: true,
            });

            const buffer = await response.body;
            const store = await KeyValueStore.open();
            await store.setValue('my-image', buffer, {
                contentType: response.headers['content-type'],
            });
            return;
        }

        // Handle other URLs normally
        log.info('Crawling', { url: request.url });
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Building a CheerioCrawler with RequestQueue
DESCRIPTION: Implements a complete CheerioCrawler that processes a URL from a RequestQueue and extracts the page title using Cheerio parser.

LANGUAGE: typescript
CODE:
// Add import of CheerioCrawler
import { RequestQueue, CheerioCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

// Create the crawler and add the queue with our URL
// and a request handler to process the page.
const crawler = new CheerioCrawler({
    requestQueue,
    // The `$` argument is the Cheerio object
    // which contains parsed HTML of the website.
    async requestHandler({ $, request }) {
        // Extract <title> text with Cheerio.
        // See Cheerio documentation for API docs.
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

// Start the crawler and wait for it to finish
await crawler.run();

----------------------------------------

TITLE: Adjusting Crawlee Code for Apify Platform
DESCRIPTION: Modifications to the main.js file to integrate with the Apify Platform. Includes importing Actor from Apify SDK and adding Actor.init() and Actor.exit() calls.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

await Actor.init();

log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

await Actor.exit();

----------------------------------------

TITLE: Crawling Sitemap with Cheerio Crawler in JavaScript
DESCRIPTION: This snippet demonstrates how to use Cheerio Crawler to crawl a sitemap. It initializes the crawler, sets up the router, and processes the sitemap URLs.

LANGUAGE: javascript
CODE:
import { CheerioSitemap } from 'crawlee';

await CheerioSitemap.run({
    sitemap: 'https://crawlee.dev/sitemap.xml',
    // exclude robots.txt
    ignoreRobotsTxt: true,
    // crawl only 10 pages
    maxRequestsPerCrawl: 10,
    async requestHandler({ $, request, enqueueLinks, log }) {
        const title = $('title').text();
        log.info(`Title of ${request.loadedUrl} is '${title}'`);

        // save some results
        await Dataset.pushData({
            url: request.loadedUrl,
            title,
        });

        // crawl all links on the extracted URLs
        await enqueueLinks();
    },
});


----------------------------------------

TITLE: Configuring Package.json for Development and Production
DESCRIPTION: JSON configuration for package.json to set up scripts for development and production environments.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "type": "module",
    "main": "dist/main.js",
    "dependencies": {
        "crawlee": "3.0.0"
    },
    "devDependencies": {
        "@apify/tsconfig": "^0.1.0",
        "@types/node": "^18.14.0",
        "ts-node": "^10.8.0",
        "typescript": "^4.7.4"
    },
    "scripts": {
        "start": "npm run start:dev",
        "start:prod": "node dist/main.js",
        "start:dev": "ts-node-esm -T src/main.ts",
        "build": "tsc"
    }
}

----------------------------------------

TITLE: Dockerfile for Playwright Chrome Image
DESCRIPTION: Example Dockerfile using the Apify Playwright Chrome image with Node.js 20.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:20

----------------------------------------

TITLE: Custom Link Selection in enqueueLinks
DESCRIPTION: Example of using a custom selector with enqueueLinks to find specific elements.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    selector: 'div.has-link'
});

----------------------------------------

TITLE: Initializing and Running PuppeteerCrawler in JavaScript
DESCRIPTION: This snippet demonstrates how to create and use a PuppeteerCrawler instance. It defines requestHandler and failedRequestHandler functions, and then runs the crawler with a list of URLs.

LANGUAGE: javascript
CODE:
const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // This function is called to extract data from a single web page
        // 'page' is an instance of Puppeteer.Page with page.goto(request.url) already called
        // 'request' is an instance of Request class with information about the page to load
        await Dataset.pushData({
            title: await page.title(),
            url: request.url,
            succeeded: true,
        })
    },
    async failedRequestHandler({ request }) {
        // This function is called when the crawling of a request failed too many times
        await Dataset.pushData({
            url: request.url,
            succeeded: false,
            errors: request.errorMessages,
        })
    },
});

await crawler.run([
    'http://www.example.com/page-1',
    'http://www.example.com/page-2',
]);

----------------------------------------

TITLE: Filtering URLs with Globs in Crawlee
DESCRIPTION: Demonstrates how to use glob patterns to filter URLs when enqueuing links in Crawlee.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
});

----------------------------------------

TITLE: Complete Lambda Handler with Data Return
DESCRIPTION: Final implementation of the Lambda handler that includes crawler execution and returns scraped data in the required AWS Lambda response format.

LANGUAGE: javascript
CODE:
// For more information, see https://crawlee.dev/
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);

    return {
        statusCode: 200,
        body: await crawler.getData(),
    }
};

----------------------------------------

TITLE: Extracting and Processing Product Price
DESCRIPTION: Shows how to extract, filter, and convert a product's price from string format to a number, handling currency symbols and formatting.

LANGUAGE: javascript
CODE:
const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

----------------------------------------

TITLE: Complete Lambda Handler with Data Return
DESCRIPTION: Final implementation of the Lambda handler that includes crawler execution and returns scraped data in the required AWS Lambda response format.

LANGUAGE: javascript
CODE:
// For more information, see https://crawlee.dev/
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);

    return {
        statusCode: 200,
        body: await crawler.getData(),
    }
};

----------------------------------------

TITLE: Setting maxRequestsPerMinute in Crawlee CheerioCrawler
DESCRIPTION: This snippet demonstrates how to set the maxRequestsPerMinute option in a CheerioCrawler to limit the number of requests made per minute.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    maxRequestsPerMinute: 120,
    // ...
});

----------------------------------------

TITLE: Building Crawlee Actor Docker Image with Node.js and Playwright
DESCRIPTION: This Dockerfile creates a containerized environment for a Crawlee actor. It uses a multi-stage build to optimize the final image size, installs dependencies, builds the project, and sets up the runtime with Playwright and Chrome support.

LANGUAGE: Dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16 AS builder

COPY --chown=myuser package*.json ./

RUN npm install --include=dev --audit=false

COPY --chown=myuser . ./

RUN npm run build

FROM apify/actor-node-playwright-chrome:16

COPY --from=builder --chown=myuser /home/myuser/dist ./dist

COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent

----------------------------------------

TITLE: Running a Crawlee Project
DESCRIPTION: Commands to navigate to the project directory and start the Crawlee crawler.

LANGUAGE: bash
CODE:
cd my-crawler
npm start

----------------------------------------

TITLE: Failed Scraping with PlaywrightCrawler (No Wait)
DESCRIPTION: This snippet shows an attempt to scrape dynamic content with PlaywrightCrawler without waiting, which results in an error as the elements are not yet rendered.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        // This will fail because the content is not loaded yet
        const actorText = await page.textContent('.ActorStoreItem');
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Installing Crawlee Manually
DESCRIPTION: Manually install Crawlee and its dependencies using npm for different crawler types.

LANGUAGE: bash
CODE:
npm install crawlee

LANGUAGE: bash
CODE:
npm install crawlee playwright

LANGUAGE: bash
CODE:
npm install crawlee puppeteer

----------------------------------------

TITLE: TypeScript Configuration for Crawlee
DESCRIPTION: Example tsconfig.json for Crawlee projects using TypeScript with recommended settings from @apify/tsconfig.

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist",
        "lib": ["DOM"]
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Configuring Apify Proxy
DESCRIPTION: Examples of creating and configuring proxy connections using Apify Proxy service

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
    countryCode: 'US',
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Capturing Screenshots with PuppeteerCrawler Using utils.saveSnapshot()
DESCRIPTION: This snippet shows how to capture screenshots of multiple web pages when using PuppeteerCrawler with the context-aware utils.saveSnapshot() utility. It automatically saves screenshots and HTML snapshots to the default key-value store.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from '@crawlee/puppeteer';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, log, crawler }) {
        // Capture and save screenshot
        await crawler.utils.saveSnapshot(page, {
            key: `screenshot-${new URL(request.url).hostname}`,
            saveHtml: false,
        });
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Handling JSON Responses with sendRequest in Crawlee
DESCRIPTION: Shows how to configure sendRequest to handle JSON responses in a BasicCrawler's requestHandler.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ responseType: 'json' });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Explicit Request Queue Usage with Crawler in Crawlee
DESCRIPTION: Shows how to explicitly use a request queue with a crawler in Crawlee. The queue is created and managed separately from the crawler.

LANGUAGE: javascript
CODE:
import { RequestQueue, PuppeteerCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

const crawler = new PuppeteerCrawler({
    requestQueue,
    async requestHandler({ page, request, enqueueLinks }) {
        // Process the page (extract data, take page screenshot, etc).
        await enqueueLinks();
    },
});

await crawler.run();

----------------------------------------

TITLE: Initializing RequestQueue and Adding a Request in Crawlee
DESCRIPTION: This snippet demonstrates how to create a RequestQueue instance and add a new request to it. It's a basic setup for crawling a single URL.

LANGUAGE: typescript
CODE:
import { RequestQueue } from 'crawlee';

// First you create the request queue instance.
const requestQueue = await RequestQueue.open();
// And then you add one or more requests to it.
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

----------------------------------------

TITLE: Configuring tsconfig.json for Crawlee TypeScript Project
DESCRIPTION: Set up TypeScript configuration file for a Crawlee project, extending @apify/tsconfig and enabling top-level await.

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist"
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Example Dataset JSON Structure in Crawlee
DESCRIPTION: Shows the structure of a JSON array representing scraped data stored in the default dataset. Each object contains a URL and a count of heading elements.

LANGUAGE: json
CODE:
[
    {
        "url": "https://crawlee.dev/",
        "headingCount": 11
    },
    {
        "url": "https://crawlee.dev/storage",
        "headingCount": 8
    },
    {
        "url": "https://crawlee.dev/proxy",
        "headingCount": 4
    }
]

----------------------------------------

TITLE: Initializing Apify Project Configuration
DESCRIPTION: Command to initialize Apify project configuration and create necessary files

LANGUAGE: bash
CODE:
apify init

----------------------------------------

TITLE: Creating Express Server for Crawlee Crawler in JavaScript
DESCRIPTION: This code sets up an Express server to handle HTTP requests for the Crawlee crawler. It initializes the PlaywrightCrawler, runs it with the provided start URLs, and returns the crawled data as the response.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import express from 'express';
const app = express();

const startUrls = ['https://crawlee.dev'];

app.get('/', async (req, res) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));
    
    await crawler.run(startUrls);    

    return res.send(await crawler.getData());
});

app.listen(parseInt(process.env.PORT) || 3000);

----------------------------------------

TITLE: Apify CLI Authentication
DESCRIPTION: Command to log into the Apify Platform using the CLI tool

LANGUAGE: bash
CODE:
apify login

----------------------------------------

TITLE: Configuring Chromium for AWS Lambda Environment
DESCRIPTION: Code snippet demonstrating how to configure Chromium for the AWS Lambda environment using the @sparticuz/chromium package.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
    launchContext: {
        launchOptions: {
             executablePath: await aws_chromium.executablePath(),
             args: aws_chromium.args,
             headless: true
        }
    }
}, new Configuration({
    persistStorage: false,
}));

----------------------------------------

TITLE: Web Scraping with JSDOMCrawler
DESCRIPTION: Shows how to use JSDOMCrawler to crawl multiple URLs from an external file, parse HTML content using jsdom, and extract specific data including page titles and h1 tags.

LANGUAGE: typescript
CODE:
{JSDOMCrawlerSource}

----------------------------------------

TITLE: Using Crawlee with crawlee.json configuration in JavaScript
DESCRIPTION: This example demonstrates how to use Crawlee with a configuration set in crawlee.json. It creates a CheerioCrawler and adds handlers for two requests, showing how the configuration affects the crawler's behavior.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, sleep } from 'crawlee';
// We are not importing nor passing
// the Configuration to the crawler.
// We are not assigning any env vars either.
const crawler = new CheerioCrawler();

crawler.router.addDefaultHandler(async ({ request }) => {
    // for the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // for the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Checking Product Stock Availability with Playwright
DESCRIPTION: This snippet demonstrates how to check if a product is in stock by looking for a specific element on the page.

LANGUAGE: javascript
CODE:
const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Basic URL Structure Example
DESCRIPTION: Example of a paginated URL structure for the warehouse store website

LANGUAGE: plaintext
CODE:
https://warehouse-theme-metal.myshopify.com/collections/headphones?page=2

----------------------------------------

TITLE: Map Method Result Example
DESCRIPTION: Example output of the map method showing the array of heading counts greater than 5.

LANGUAGE: javascript
CODE:
[11, 8]

----------------------------------------

TITLE: Using SessionPool with PlaywrightCrawler
DESCRIPTION: Guide for implementing SessionPool with PlaywrightCrawler for browser automation with session handling.

LANGUAGE: javascript
CODE:
{PlaywrightSource}

----------------------------------------

TITLE: Map Method Result Example
DESCRIPTION: Example output of the map method showing the array of heading counts greater than 5.

LANGUAGE: javascript
CODE:
[11, 8]

----------------------------------------

TITLE: Docker Multi-stage Build Configuration
DESCRIPTION: Dockerfile configuration for building and running TypeScript project in production

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16 AS builder

COPY . ./
RUN npm install --include=dev \
    && npm run build

FROM apify/actor-node:16
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/dist ./dist

RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional

CMD npm run start:prod

----------------------------------------

TITLE: Using transformRequestFunction with enqueueLinks in Crawlee
DESCRIPTION: Shows how to use the transformRequestFunction to have fine-grained control over which requests are enqueued and how they are modified.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
    transformRequestFunction(req) {
        // ignore all links ending with `.pdf`
        if (req.url.endsWith('.pdf')) return false;
        return req;
    },
});

----------------------------------------

TITLE: Fetching JSON with sendRequest in Crawlee
DESCRIPTION: Shows how to set the responseType to 'json' when using sendRequest to fetch JSON data.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ responseType: 'json' });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Using Dataset Map Method in Crawlee
DESCRIPTION: Demonstrates how to use the Dataset map method to transform scraped data, specifically checking for pages with more than 5 header elements.

LANGUAGE: javascript
CODE:
import { Dataset, KeyValueStore } from 'crawlee';

const dataset = await Dataset.open();
const moreThan5headers = await dataset.map((item) => {
    return item.headingCount > 5 ? item.headingCount : null;
});

// Remove null values
const result = moreThan5headers.filter((item) => item !== null);

// Save the result to the default key-value store
await KeyValueStore.setValue('pages-with-more-than-5-headers', result);

----------------------------------------

TITLE: Configuring Dockerfile for Crawlee Project with Node.js
DESCRIPTION: This Dockerfile sets up a Docker image for a Crawlee project. It uses the apify/actor-node:16 base image, copies package files, installs NPM packages excluding optional and development dependencies, copies the source code, and specifies the command to run the image.

LANGUAGE: Dockerfile
CODE:
FROM apify/actor-node:16

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm start --silent

----------------------------------------

TITLE: Fetching JSON with sendRequest in Crawlee
DESCRIPTION: Shows how to set the responseType to 'json' when using sendRequest to fetch JSON data.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ responseType: 'json' });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Integrating AWS Chromium with Crawlee
DESCRIPTION: Configuration for integrating @sparticuz/chromium with Crawlee, including necessary browser launch options for AWS Lambda environment.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
    launchContext: {
        launchOptions: {
             executablePath: await aws_chromium.executablePath(),
             args: aws_chromium.args,
             headless: true
        }
    }
}, new Configuration({
    persistStorage: false,
}));

----------------------------------------

TITLE: URL Pattern Filtering
DESCRIPTION: Example of using glob patterns to filter URLs during the crawling process.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
});

----------------------------------------

TITLE: Using Pre-release Docker Image
DESCRIPTION: Shows how to use a pre-release version of an Apify Docker image, both with and without a specific library version.

LANGUAGE: dockerfile
CODE:
# Without library version.
FROM apify/actor-node:16-beta

LANGUAGE: dockerfile
CODE:
# With library version.
FROM apify/actor-node-playwright-chrome:16-1.10.0-beta

----------------------------------------

TITLE: Configuring Apify Proxy for Web Scraping
DESCRIPTION: Shows how to set up and use Apify Proxy for web scraping tasks. This example configures the proxy to use residential IPs from the US.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
    countryCode: 'US',
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Capturing Screenshot with Puppeteer using utils.puppeteer.saveSnapshot()
DESCRIPTION: This snippet shows how to capture a screenshot using the utils.puppeteer.saveSnapshot() utility function. It launches a browser, creates a new page, navigates to a URL, and uses the utility function to save a snapshot of the page.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';
import { launchPuppeteer, utils } from 'crawlee';

await Actor.init();

const browser = await launchPuppeteer();
const page = await browser.newPage();

const url = 'https://crawlee.dev';
await page.goto(url);

await utils.puppeteer.saveSnapshot(page, { key: 'my-screenshot' });

await browser.close();
await Actor.exit();

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Configuration on GCP
DESCRIPTION: Sets up a basic PlaywrightCrawler instance with persistent storage disabled for Cloud Run deployment. This configuration is necessary for stateless execution in cloud environments.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Configuring SessionPool with HttpCrawler in Crawlee
DESCRIPTION: This example shows how to configure and use SessionPool with HttpCrawler in Crawlee. It includes setting up proxy configuration and handling blocked sessions.

LANGUAGE: js
CODE:
import { HttpCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new HttpCrawler({
    proxyConfiguration,
    async requestHandler({ response, session }) {
        if (response.statusCode === 403) {
            session.retire();
        }

        // Process the response...
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Using Dataset Reduce Method in Crawlee
DESCRIPTION: This snippet shows how to use the Dataset reduce method to calculate the total number of headers across all pages in the dataset. It accumulates the heading counts from all items into a single value.

LANGUAGE: javascript
CODE:
import { Dataset, KeyValueStore } from 'crawlee';

const dataset = await Dataset.open();
const pagesHeadingCount = await dataset.reduce((memo, value) => {
    return memo + value.headingCount;
}, 0);

const kvStore = await KeyValueStore.open();
await kvStore.setValue('TOTAL_HEADINGS', pagesHeadingCount);

----------------------------------------

TITLE: Using RequestQueueV2 with CheerioCrawler
DESCRIPTION: Demonstrates how to use a RequestQueueV2 instance with a CheerioCrawler while enabling the request locking experiment. This setup allows for parallel processing of requests from the shared queue.

LANGUAGE: typescript
CODE:
import { CheerioCrawler, RequestQueueV2 } from 'crawlee';

const queue = await RequestQueueV2.open('my-locking-queue');

const crawler = new CheerioCrawler({
    experiments: {
        requestLocking: true,
    },
    requestQueue: queue,
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

await crawler.run();

----------------------------------------

TITLE: Browser Pool Configuration
DESCRIPTION: Example showing how to configure browser pool options and lifecycle hooks in the new API.

LANGUAGE: javascript
CODE:
const crawler = new Apify.PuppeteerCrawler({
    browserPoolOptions: {
        retireBrowserAfterPageCount: 10,
        preLaunchHooks: [
            async (pageId, launchContext) => {
                const { request } = crawler.crawlingContexts.get(pageId);
                if (request.userData.useHeadful === true) {
                    launchContext.launchOptions.headless = false;
                }
            }
        ]
    }
})

----------------------------------------

TITLE: Extracting Product Manufacturer from URL
DESCRIPTION: Demonstrates how to parse a product's manufacturer name from its URL by splitting the URL string.

LANGUAGE: javascript
CODE:
const urlPart = request.url.split('/').slice(-1);
const manufacturer = urlPart[0].split('-')[0];

----------------------------------------

TITLE: Configuring CheerioCrawler for GCP Cloud Functions
DESCRIPTION: Updates the main.js file to use a separate Configuration instance with persistStorage set to false, which is necessary for running on GCP Cloud Functions.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Using SessionPool with BasicCrawler in Crawlee
DESCRIPTION: This snippet demonstrates how to configure and use SessionPool with BasicCrawler in Crawlee. It includes setting up a proxy configuration and handling session creation and rotation.

LANGUAGE: javascript
CODE:
{BasicSource}

----------------------------------------

TITLE: Modifying Crawlee Detail Route for Parallel Processing in JavaScript
DESCRIPTION: Updates the DETAIL route handler to send scraped data back to the parent process instead of using the default storage method.

LANGUAGE: javascript
CODE:
router.addHandler('DETAIL', async ({ request, page, log }) => {
    const urlParts = request.url.split('/');
    const bookId = urlParts[urlParts.length - 2];

    const title = await page.locator('h1').textContent();
    const price = await page.locator('.price_color').textContent();

    log.info(`Scraped book ${bookId} - ${title}`);

    // Instead of context.pushData, use process.send
    if (process.send) {
        process.send({ id: bookId, title, price });
    }
});

----------------------------------------

TITLE: Configuring Headful Browser in TypeScript
DESCRIPTION: TypeScript code snippet to configure Crawlee to use a headful (visible) browser for debugging purposes.

LANGUAGE: typescript
CODE:
// Uncomment this option to see the browser window.
headless: false

----------------------------------------

TITLE: Customizing Browser Fingerprints in PuppeteerCrawler
DESCRIPTION: This snippet shows how to customize browser fingerprints in PuppeteerCrawler. It sets specific options for operating system, browser, and user agent.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    // ...
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                operatingSystems: ['windows'],
                browsers: [{ name: 'firefox', minVersion: 90 }],
                devices: ['desktop'],
                locales: ['en-US', 'en-GB'],
            },
        },
    },
});

----------------------------------------

TITLE: Customizing Browser Fingerprints in PuppeteerCrawler
DESCRIPTION: This snippet shows how to customize browser fingerprints in PuppeteerCrawler. It sets specific options for operating system, browser, and user agent.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    // ...
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                operatingSystems: ['windows'],
                browsers: [{ name: 'firefox', minVersion: 90 }],
                devices: ['desktop'],
                locales: ['en-US', 'en-GB'],
            },
        },
    },
});

----------------------------------------

TITLE: Creating INPUT.json File for Actor Input
DESCRIPTION: This bash snippet shows the file path where the INPUT.json file should be created to provide input to the actor. The file should be placed in the default key-value store of the project.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json

----------------------------------------

TITLE: Finding All Links on a Page with Cheerio
DESCRIPTION: This snippet demonstrates how to use Cheerio to find all <a> elements with an href attribute and extract the href values into an array.

LANGUAGE: javascript
CODE:
$('a[href]')
    .map((i, el) => $(el).attr('href'))
    .get();

----------------------------------------

TITLE: Implementing Recursive Crawl with PuppeteerCrawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to set up and execute a recursive web crawl using PuppeteerCrawler. It includes configuration for the crawler, request queue setup, and handling of crawled pages.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, Dataset } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ request, page, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        // Save results as JSON to ./storage/datasets/default
        await Dataset.pushData({
            url: request.url,
            title,
        });

        // Extract links from the current page
        // and add them to the crawling queue.
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 20, // Limit the crawler to only 20 requests
});

// Add first URL to the queue and start the crawl.
await crawler.run(['https://crawlee.dev']);


----------------------------------------

TITLE: Configuring Dockerfile for Crawlee Project with Node.js and Playwright
DESCRIPTION: This Dockerfile sets up an environment for a Crawlee project using Node.js and Playwright with Chrome. It installs npm dependencies, copies project files, and configures the container to run the project on startup.

LANGUAGE: Dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:20

COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD npm start --silent

----------------------------------------

TITLE: Configuring Dockerfile for Crawlee Project with Node.js and Playwright
DESCRIPTION: This Dockerfile sets up an environment for a Crawlee project using Node.js and Playwright with Chrome. It installs npm dependencies, copies project files, and configures the container to run the project on startup.

LANGUAGE: Dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:20

COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD npm start --silent

----------------------------------------

TITLE: Configuring SessionPool with HttpCrawler in Crawlee
DESCRIPTION: This example shows how to configure and use SessionPool with HttpCrawler in Crawlee. It includes setup for proxy configuration and session pool options.

LANGUAGE: js
CODE:
import { HttpCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new HttpCrawler({
    // Use the proxy configuration
    proxyConfiguration,
    // Set up the session pool options
    sessionPoolOptions: {
        maxPoolSize: 100,
    },
    // This function is called for each URL
    async requestHandler({ session, request, body }) {
        // ...
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Map Method Result Example
DESCRIPTION: Sample output from the Dataset.map() method showing filtered heading counts greater than 5.

LANGUAGE: javascript
CODE:
[11, 8]

----------------------------------------

TITLE: Importing Dataset Module in Crawlee
DESCRIPTION: Shows how to import the required Dataset module along with PlaywrightCrawler from Crawlee library.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

----------------------------------------

TITLE: Map Method Result Example
DESCRIPTION: Sample output from the Dataset.map() method showing filtered heading counts greater than 5.

LANGUAGE: javascript
CODE:
[11, 8]

----------------------------------------

TITLE: Basic Crawler Implementation with crawlee.json
DESCRIPTION: Example showing a CheerioCrawler implementation that relies on crawlee.json configuration without explicit configuration in code.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, sleep } from 'crawlee';
const crawler = new CheerioCrawler();

crawler.router.addDefaultHandler(async ({ request }) => {
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Configuring Apify Proxy
DESCRIPTION: Examples of creating and configuring Apify Proxy for use with specific proxy groups and countries

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration();
const proxyUrl = await proxyConfiguration.newUrl();

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
    countryCode: 'US',
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Installing @apify/tsconfig for Crawlee TypeScript Project
DESCRIPTION: Command to install @apify/tsconfig as a development dependency for a Crawlee TypeScript project.

LANGUAGE: shell
CODE:
npm install --save-dev @apify/tsconfig

----------------------------------------

TITLE: Getting Text Content with Cheerio
DESCRIPTION: Shows how to extract text content from an H2 element using Cheerio selector.

LANGUAGE: javascript
CODE:
$('h2').text()

----------------------------------------

TITLE: Sanity Check Crawler with Playwright and Cheerio
DESCRIPTION: Creates a Playwright crawler that uses Cheerio for HTML parsing to extract and print the text content of all category elements on the page.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';
import { load } from 'cheerio';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, body, log }) {
        const title = await page.title();
        log.info(`Title of ${page.url()} is '${title}'`);

        const $ = load(body);
        $('.collection-block-item').each((_, el) => {
            console.log($(el).text());
        });
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Sample Dataset Structure in JSON
DESCRIPTION: Example of the dataset structure containing URLs and heading counts stored in the project's default dataset directory.

LANGUAGE: json
CODE:
[
    {
        "url": "https://crawlee.dev/",
        "headingCount": 11
    },
    {
        "url": "https://crawlee.dev/storage",
        "headingCount": 8
    },
    {
        "url": "https://crawlee.dev/proxy",
        "headingCount": 4
    }
]

----------------------------------------

TITLE: Configuring Package.json for TypeScript Build
DESCRIPTION: JSON snippet for package.json to add a TypeScript build script and specify the main entry point.

LANGUAGE: json
CODE:
{
    "scripts": {
        "build": "tsc"
    },
    "main": "dist/main.js"
}

----------------------------------------

TITLE: URL Pattern Filtering with enqueueLinks
DESCRIPTION: Example of using glob patterns to filter URLs during the crawling process.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
});

----------------------------------------

TITLE: Extracting Manufacturer from URL in JavaScript
DESCRIPTION: This snippet demonstrates how to extract the manufacturer name from a product URL by splitting the string.

LANGUAGE: javascript
CODE:
// request.url = https://warehouse-theme-metal.myshopify.com/products/sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440

const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']
const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'

----------------------------------------

TITLE: Configuring Apify Proxy
DESCRIPTION: Examples of creating and configuring Apify Proxy for use with specific proxy groups and countries.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration();
const proxyUrl = await proxyConfiguration.newUrl();

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
    countryCode: 'US',
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Using SessionPool with PuppeteerCrawler in Crawlee
DESCRIPTION: This code snippet shows how to use SessionPool with PuppeteerCrawler in Crawlee. It covers proxy configuration, session management, and web page interactions using Puppeteer.

LANGUAGE: js
CODE:
import { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new PuppeteerCrawler({
    // The `useSessionPool` enables the `SessionPool` functionality for the crawler
    useSessionPool: true,
    // By default, `PuppeteerCrawler` retries failed requests
    // so there's no need to do it manually
    maxRequestRetries: 5,
    // Configures the proxy to be used by the `SessionPool`
    proxyConfiguration,
    async requestHandler({ page, session }) {
        // Use `session.userData` to store custom data
        session.userData.numberOfRetries = session.userData.numberOfRetries ?? 0;

        const title = await page.title();
        // Process the page...

        // Increase the number of processed requests in the session
        session.userData.numberOfRetries++;
    },
    failedRequestHandler({ session }) {
        // Retire the proxy URL if we get blocked
        session.retire();
    },
});

await crawler.run([
    'https://crawlee.dev',
]);

----------------------------------------

TITLE: Initializing CheerioCrawler with GCP Configuration
DESCRIPTION: Sets up the CheerioCrawler with disabled persistent storage for GCP compatibility.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Failing to Scrape Dynamic Content with PuppeteerCrawler (No Wait)
DESCRIPTION: This snippet shows how attempting to scrape JavaScript-rendered content without explicitly waiting for elements in Puppeteer leads to errors.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // This will fail because the element might not be in the DOM yet
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Crawling URLs with HttpCrawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use the HttpCrawler class to crawl a list of URLs from an external file. It makes HTTP requests to each URL, processes the responses, and saves the HTML content to a dataset. The crawler is configured with various options including max requests, session pool, and request handler.

LANGUAGE: javascript
CODE:
import { HttpCrawler, Dataset } from 'crawlee';
import { readFile } from 'fs/promises';

// Add URLs to a list
const urls = [
    'https://crawlee.dev',
    'https://apify.com',
];

// Create an instance of the HttpCrawler class
const crawler = new HttpCrawler({
    // Limit the number of requests
    maxRequestsPerCrawl: 20,
    // Use the default session pool that automatically manages concurrency
    useSessionPool: true,
    // Set the maximum concurrency to 5
    sessionPoolOptions: {
        maxPoolSize: 5,
    },
    // Function called for each URL
    async requestHandler({ request, body, $ }) {
        // Save the HTML content to a dataset
        await Dataset.pushData({
            url: request.url,
            html: body,
        });
    },
});

// Load URLs from an external file
const loadUrlsFromFile = async (path) => {
    const content = await readFile(path, 'utf8');
    return content.trim().split('\n');
};

// Add URLs to the crawler's request queue
const addUrls = async () => {
    // Load URLs from a file
    const urlsFromFile = await loadUrlsFromFile('./urls.txt');
    // Combine URLs from the file with the predefined list
    const allUrls = [...urls, ...urlsFromFile];
    // Add the URLs to the crawler's request queue
    await crawler.addRequests(allUrls);
};

// Run the crawler
const runCrawler = async () => {
    await addUrls();
    await crawler.run();
};

runCrawler();

----------------------------------------

TITLE: Running Crawlee Code as an Actor using Actor.main()
DESCRIPTION: Shows how to wrap Crawlee code in the Actor.main() function to run it as an actor on the Apify platform. This example uses CheerioCrawler to scrape a website.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';
import { CheerioCrawler, Dataset } from 'crawlee';

await Actor.main(async () => {
    const crawler = new CheerioCrawler({
        async requestHandler({ $, request, enqueueLinks, log }) {
            const title = $('title').text();
            log.info(`Title of ${request.url} is '${title}'`);
            await Dataset.pushData({ title, url: request.url });
            await enqueueLinks();
        },
        maxRequestsPerCrawl: 20,
    });

    await crawler.run(['https://crawlee.dev']);
});

----------------------------------------

TITLE: Configuring Minimum and Maximum Concurrency in CheerioCrawler
DESCRIPTION: Shows how to set the minimum and maximum concurrency for parallel requests in a CheerioCrawler. This allows fine-tuning of the crawler's performance based on system resources and target website limitations.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    minConcurrency: 1,
    maxConcurrency: 100,
    // ...
});

----------------------------------------

TITLE: Exporting Dataset to CSV File Using Crawlee
DESCRIPTION: This code snippet demonstrates how to use the 'exportToValue' function in Crawlee to export the entire default dataset to a single CSV file. The exported file is then stored in the default key-value store.

LANGUAGE: JavaScript
CODE:
import { Dataset, KeyValueStore } from 'crawlee';

await Dataset.open();

// Export entire dataset to CSV
const csv = await Dataset.exportToValue('CSV');

// Save the CSV file to the default key-value store
await KeyValueStore.setValue('DATASET.csv', csv, { contentType: 'text/csv' });

----------------------------------------

TITLE: Updating package.json for GCP Cloud Functions
DESCRIPTION: Modifies the package.json file to set the main entry point for the GCP Cloud Function.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "version": "1.0.0",
    "main": "src/main.js",
    ...
}

----------------------------------------

TITLE: Crawling All Links with Cheerio Crawler
DESCRIPTION: Implementation of a CheerioCrawler that enqueues all discovered links regardless of their domain using the 'all' strategy. This crawler processes any URLs found during crawling.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks }) {
        // Add all discovered links to the crawling queue,
        // regardless of the domain
        await enqueueLinks({
            strategy: EnqueueStrategy.All,
        });
    },
});

----------------------------------------

TITLE: Using Pre-release Docker Image
DESCRIPTION: This snippet shows how to use a pre-release version of an Apify Docker image, both with and without a specific library version.

LANGUAGE: dockerfile
CODE:
# Without library version.
FROM apify/actor-node:16-beta

LANGUAGE: dockerfile
CODE:
# With library version.
FROM apify/actor-node-playwright-chrome:16-1.10.0-beta

----------------------------------------

TITLE: Cross-Context Access Using Context IDs
DESCRIPTION: Shows how to track and access different crawling contexts using the new context ID system.

LANGUAGE: javascript
CODE:
let masterContextId;
const handlePageFunction = async ({ id, page, request, crawler }) => {
    if (request.userData.masterPage) {
        masterContextId = id;
        // Prepare the master page.
    } else {
        const masterContext = crawler.crawlingContexts.get(masterContextId);
        const masterPage = masterContext.page;
        const masterRequest = masterContext.request;
        // Now we can manipulate the master data from another handlePageFunction.
    }
}

----------------------------------------

TITLE: Initializing CheerioCrawler with GCP Configuration
DESCRIPTION: Sets up the CheerioCrawler with disabled storage persistence for GCP compatibility.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Initializing Cheerio Crawler with Unique Configuration for AWS Lambda
DESCRIPTION: This snippet demonstrates how to create a CheerioCrawler instance with a unique Configuration object, setting persistStorage to false for Lambda compatibility.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Session Management with PlaywrightCrawler
DESCRIPTION: Example of session management using PlaywrightCrawler. Demonstrates how to handle sessions and proxy rotation in a Playwright-based browser automation context.

LANGUAGE: javascript
CODE:
{PlaywrightSource}

----------------------------------------

TITLE: Initializing ProxyConfiguration in Crawlee
DESCRIPTION: This snippet demonstrates how to create a new ProxyConfiguration instance with custom proxy URLs and obtain a new proxy URL.

LANGUAGE: javascript
CODE:
import { ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ]
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Accessing Crawling Context in Apify SDK v1
DESCRIPTION: Demonstrates how to access and use the new Crawling Context object in handler functions.

LANGUAGE: javascript
CODE:
const handlePageFunction = async (crawlingContext1) => {
    crawlingContext1.hasOwnProperty('proxyInfo') // true
}

const handleFailedRequestFunction = async (crawlingContext2) => {
    crawlingContext2.hasOwnProperty('proxyInfo') // true
}

// All contexts are the same object.
crawlingContext1 === crawlingContext2 // true

----------------------------------------

TITLE: Explicit Request Queue Usage with Crawler in Crawlee
DESCRIPTION: Shows how to explicitly create and use a Request Queue with a Crawler in Crawlee. It demonstrates creating a named queue, adding requests, and configuring the crawler to use it.

LANGUAGE: javascript
CODE:
import { RequestQueue, PuppeteerCrawler } from 'crawlee';

// Create a named request queue
const requestQueue = await RequestQueue.open('my-queue');

// Add requests to the queue
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

const crawler = new PuppeteerCrawler({
    requestQueue,
    async requestHandler({ page, request, enqueueLinks }) {
        // Add all links from page to the queue
        await enqueueLinks();
    },
});

// Run the crawler
await crawler.run();

----------------------------------------

TITLE: Reducing Dataset to Calculate Total Header Count
DESCRIPTION: This code snippet shows how to use the Dataset reduce method to calculate the total number of headers across all pages.

LANGUAGE: javascript
CODE:
import { Dataset, KeyValueStore } from 'crawlee';

const dataset = await Dataset.open();
const pagesHeadingCount = await dataset.reduce((memo, value) => {
    return memo + value.headingCount;
}, 0);

// Save the result to the default key-value store
const store = await KeyValueStore.open();
await store.setValue('total-heading-count', pagesHeadingCount);

----------------------------------------

TITLE: Importing Crawlee Dataset Components
DESCRIPTION: Import statement for PlaywrightCrawler and Dataset from Crawlee package.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

----------------------------------------

TITLE: Scraping Static Content with CheerioCrawler in TypeScript
DESCRIPTION: This snippet demonstrates an attempt to scrape JavaScript-rendered content using CheerioCrawler, which fails because CheerioCrawler cannot execute client-side JavaScript.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        // Extract text content of an actor card
        const actorText = $('.ActorStoreItem').text();
        console.log(`ACTOR: ${actorText}`);
    }
})

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Using Dataset Map Method in Crawlee
DESCRIPTION: Demonstrates how to use the Dataset map method to filter pages with more than 5 header elements. The result is an array of headingCount values greater than 5.

LANGUAGE: javascript
CODE:
import { Dataset, KeyValueStore } from 'crawlee';

const dataset = await Dataset.open();
const moreThan5headers = await dataset.map((item) => {
    if (item.headingCount > 5) {
        return item.headingCount;
    }
});

const kvStore = await KeyValueStore.open();
await kvStore.setValue('header-count', moreThan5headers);

----------------------------------------

TITLE: Filtering URLs with Globs in Crawlee
DESCRIPTION: Demonstrates how to use globs with enqueueLinks to filter URLs for crawling.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
});

----------------------------------------

TITLE: Complete Crawlee Scraping Script with Data Saving (JavaScript)
DESCRIPTION: This is the final version of the Crawlee scraping script that includes data extraction and saving. It uses PlaywrightCrawler to scrape web pages and Dataset.pushData() to save the extracted data.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        console.log(`Title of ${request.loadedUrl} is '${title}'`);

        const h1 = await page.$eval('h1', (el) => el.textContent);
        const body = await page.$eval('body', (el) => el.innerHTML);

        await Dataset.pushData({
            title,
            h1,
            body,
            url: request.loadedUrl,
        });

        await enqueueLinks();
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Checking Product Stock Status with Playwright
DESCRIPTION: Demonstrates how to check if a product is in stock by verifying the existence of specific elements using Playwright locators.

LANGUAGE: javascript
CODE:
const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Using Dataset Map Method in Crawlee
DESCRIPTION: Demonstrates how to use the Dataset map method to filter pages with more than 5 header elements. The result is an array of headingCount values greater than 5.

LANGUAGE: javascript
CODE:
import { Dataset, KeyValueStore } from 'crawlee';

const dataset = await Dataset.open();
const moreThan5headers = await dataset.map((item) => {
    if (item.headingCount > 5) {
        return item.headingCount;
    }
});

const kvStore = await KeyValueStore.open();
await kvStore.setValue('header-count', moreThan5headers);

----------------------------------------

TITLE: Initializing Cheerio Crawler with Unique Configuration for AWS Lambda
DESCRIPTION: This snippet demonstrates how to create a CheerioCrawler instance with a unique Configuration object, setting persistStorage to false for Lambda compatibility.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Using actor-node-playwright Docker Image
DESCRIPTION: Example of how to use the Apify Docker image that includes all Playwright browsers, suitable for development and testing with multiple browsers.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright:16

----------------------------------------

TITLE: Scraping Product Title with Playwright
DESCRIPTION: This code uses Playwright to locate and extract the product title from an HTML element with a specific class.

LANGUAGE: javascript
CODE:
const title = await page.locator('.product-meta h1').textContent();

----------------------------------------

TITLE: Configuring CheerioCrawler for GCP Cloud Functions
DESCRIPTION: Update the main.js file to use a separate Configuration instance with persistStorage set to false, which is necessary for cloud function environments.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Using SessionPool with JSDOMCrawler in Crawlee
DESCRIPTION: This snippet illustrates how to use SessionPool with JSDOMCrawler in Crawlee. It covers proxy configuration, session management, and processing of web pages using JSDOM.

LANGUAGE: js
CODE:
import { JSDOMCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new JSDOMCrawler({
    // The `useSessionPool` enables the `SessionPool` functionality for the crawler
    useSessionPool: true,
    // By default, `JSDOMCrawler` retries failed requests
    // so there's no need to do it manually
    maxRequestRetries: 5,
    // Configures the proxy to be used by the `SessionPool`
    proxyConfiguration,
    async requestHandler({ window, session }) {
        // Use `session.userData` to store custom data
        session.userData.numberOfRetries = session.userData.numberOfRetries ?? 0;

        const title = window.document.querySelector('title').textContent;
        // Process the page...

        // Increase the number of processed requests in the session
        session.userData.numberOfRetries++;
    },
    failedRequestHandler({ session }) {
        // Retire the proxy URL if we get blocked
        session.retire();
    },
});

await crawler.run([
    'https://crawlee.dev',
]);

----------------------------------------

TITLE: Initializing ProxyConfiguration in Crawlee
DESCRIPTION: This snippet demonstrates how to create a new ProxyConfiguration instance with custom proxy URLs and obtain a new proxy URL.

LANGUAGE: javascript
CODE:
import { ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ]
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Using RequestQueueV2 in CheerioCrawler with Request Locking (TypeScript)
DESCRIPTION: This snippet demonstrates how to use a RequestQueueV2 instance in a CheerioCrawler while enabling the request locking experiment. It shows creating the queue, enabling the experiment, and passing the queue to the crawler.

LANGUAGE: typescript
CODE:
import { CheerioCrawler, RequestQueueV2 } from 'crawlee';

const queue = await RequestQueueV2.open('my-locking-queue');

const crawler = new CheerioCrawler({
    experiments: {
        requestLocking: true,
    },
    requestQueue: queue,
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

await crawler.run();

----------------------------------------

TITLE: Basic Usage of sendRequest with BasicCrawler
DESCRIPTION: Shows how to initialize BasicCrawler and use the sendRequest function to make HTTP requests.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest();
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Purging Default Storages in Crawlee
DESCRIPTION: This snippet demonstrates how to explicitly purge default storages in Crawlee using the purgeDefaultStorages() helper function.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Extracting Page Links Using JSDOM in JavaScript
DESCRIPTION: Shows how to find and extract all href attributes from anchor tags on a page using JSDOM's querySelector functionality and Array manipulation.

LANGUAGE: javascript
CODE:
Array.from(document.querySelectorAll('a[href]')).map((a) => a.href);

----------------------------------------

TITLE: Purging Default Storages in Crawlee
DESCRIPTION: This snippet demonstrates how to explicitly purge default storages in Crawlee using the purgeDefaultStorages() helper function.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Creating AWS Lambda Handler Function for Crawlee Cheerio Crawler
DESCRIPTION: This code demonstrates how to wrap the Crawlee crawler logic in an AWS Lambda handler function. It creates a new crawler instance for each Lambda execution to maintain statelessness.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
};

----------------------------------------

TITLE: Cheerio-based Sitemap Crawling
DESCRIPTION: Implementation showing how to crawl a sitemap using the Cheerio Crawler, which is lightweight and suitable for basic HTML parsing. Uses the Sitemap utility class from @crawlee/utils to handle sitemap processing.

LANGUAGE: javascript
CODE:
{CheerioSource}

----------------------------------------

TITLE: Cookie Jar Implementation
DESCRIPTION: Shows how to use a custom cookie jar for managing cookies in requests.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';
import { CookieJar } from 'tough-cookie';

const cookieJar = new CookieJar();

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ cookieJar });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Configuring package.json for TypeScript Build
DESCRIPTION: JSON configuration in package.json to define build script and main entry point for a TypeScript project.

LANGUAGE: json
CODE:
{
    "scripts": {
        "build": "tsc"
    },
    "main": "dist/main.js"
}

----------------------------------------

TITLE: Basic Cheerio Crawler Implementation in TypeScript
DESCRIPTION: Simple implementation of CheerioCrawler that downloads a single page and prints its title.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Configuring PlaywrightCrawler with Firefox Browser
DESCRIPTION: Example showing how to configure and use PlaywrightCrawler with headless Firefox browser. This setup is specifically designed for use with the Apify Platform using the apify/actor-node-playwright-firefox Docker image.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    // ...
    browserType: 'firefox',
});

----------------------------------------

TITLE: Capturing Screenshots with PuppeteerCrawler Using utils.saveSnapshot()
DESCRIPTION: This snippet shows how to capture screenshots of multiple web pages using PuppeteerCrawler and the context-aware utils.saveSnapshot() utility. It creates a PuppeteerCrawler instance, defines a handler function that captures and saves snapshots for each page, and runs the crawler.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, utils }) {
        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);

        const key = `${Math.random()}.png`; // Or use URL
        await utils.saveSnapshot(page, { key });
    },
    maxRequestsPerCrawl: 10,
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Extracting Page Links with JSDOM
DESCRIPTION: Shows how to find and extract all href attributes from anchor tags on a page using JSDOM's querySelector.

LANGUAGE: javascript
CODE:
Array.from(document.querySelectorAll('a[href]')).map((a) => a.href);

----------------------------------------

TITLE: Customizing Browser Fingerprints with PlaywrightCrawler
DESCRIPTION: This snippet demonstrates how to customize browser fingerprints using PlaywrightCrawler in Crawlee. It shows how to set specific browser and operating system parameters for fingerprint generation.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    // ...
    browserPoolOptions: {
        useFingerprints: true, // this is the default
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                browsers: [
                    { name: 'firefox', minVersion: 88 },
                    { name: 'chrome', minVersion: 90 },
                ],
                operatingSystems: ['windows'],
            },
        },
    },
});

await crawler.run();

----------------------------------------

TITLE: Configuring INPUT.json File Path for Crawlee Actor
DESCRIPTION: Demonstrates the file path structure for providing input to a Crawlee actor through the INPUT.json file in the default key-value store.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json

----------------------------------------

TITLE: Customizing Browser Fingerprints with PlaywrightCrawler
DESCRIPTION: This snippet demonstrates how to customize browser fingerprints using PlaywrightCrawler in Crawlee. It shows how to set specific browser and operating system parameters for fingerprint generation.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    // ...
    browserPoolOptions: {
        useFingerprints: true, // this is the default
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                browsers: [
                    { name: 'firefox', minVersion: 88 },
                    { name: 'chrome', minVersion: 90 },
                ],
                operatingSystems: ['windows'],
            },
        },
    },
});

await crawler.run();

----------------------------------------

TITLE: Managing Dataset Operations in Crawlee
DESCRIPTION: Demonstrates how to work with datasets for storing structured data, including writing single and multiple rows to both default and named datasets.

LANGUAGE: javascript
CODE:
import { Dataset } from 'crawlee';

// Write a single row to the default dataset
await Dataset.pushData({ col1: 123, col2: 'val2' });

// Open a named dataset
const dataset = await Dataset.open('some-name');

// Write a single row
await dataset.pushData({ foo: 'bar' });

// Write multiple rows
await dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);

----------------------------------------

TITLE: Crawling All Links with Playwright Crawler in Crawlee
DESCRIPTION: This snippet illustrates how to use Playwright Crawler in Crawlee to crawl all links on a website. It configures a PlaywrightCrawler, utilizes the enqueueLinks() method to add new links to the queue, and shows how to extract the page title using Playwright.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, enqueueLinks } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limit the crawler to only 10 requests
});

// Run the crawler with initial request
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Finding All Links on a Web Page with Cheerio
DESCRIPTION: This code snippet shows how to use Cheerio to find all <a> elements with an href attribute on a page and extract their href values into an array.

LANGUAGE: javascript
CODE:
$('a[href]')
    .map((i, el) => $(el).attr('href'))
    .get();

----------------------------------------

TITLE: Scraping with PuppeteerCrawler (Successful for JS-rendered content)
DESCRIPTION: This snippet demonstrates using PuppeteerCrawler to successfully scrape JavaScript-rendered content, explicitly waiting for elements to appear before extraction.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // Wait for the actor cards to render
        await page.waitForSelector('.ActorStoreItem');
        
        // Extract text content of the first actor card
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Storage Cleanup in Crawlee
DESCRIPTION: Shows how to purge default storage directories in Crawlee.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Logging in to Apify Platform via CLI
DESCRIPTION: Command to log in to the Apify Platform using the Apify CLI. Requires a personal access token from the Apify account.

LANGUAGE: bash
CODE:
apify login

----------------------------------------

TITLE: Configuring SessionPool with JSDOMCrawler in Crawlee
DESCRIPTION: This example demonstrates how to set up and use SessionPool with JSDOMCrawler in Crawlee. It includes proxy configuration and handling of blocked sessions.

LANGUAGE: js
CODE:
import { JSDOMCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new JSDOMCrawler({
    proxyConfiguration,
    async requestHandler({ window, request, session }) {
        const title = window.document.querySelector('title').textContent;
        console.log(`The title of ${request.url} is: ${title}`);

        const body = window.document.body.textContent;
        if (body.includes('blocked')) {
            session.retire();
        }
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Purging Default Storages in Crawlee
DESCRIPTION: Demonstrates how to explicitly purge default storages in Crawlee using the purgeDefaultStorages() helper function.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Enqueuing Links with Crawlee
DESCRIPTION: Basic example of using the enqueueLinks function in Crawlee to add links to the crawling queue.

LANGUAGE: javascript
CODE:
await enqueueLinks();

----------------------------------------

TITLE: Using Dataset Reduce Method in Crawlee
DESCRIPTION: Shows how to use the Dataset reduce method to calculate the total number of headers across all scraped pages. The result is a single value representing the sum of all headingCount values.

LANGUAGE: javascript
CODE:
import { Dataset, KeyValueStore } from 'crawlee';

const dataset = await Dataset.open();
const pagesHeadingCount = await dataset.reduce((memo, value) => {
    return memo + value.headingCount;
}, 0);

const kvStore = await KeyValueStore.open();
await kvStore.setValue('total-header-count', pagesHeadingCount);

----------------------------------------

TITLE: Configuring INPUT.json File Path for Crawlee Actor
DESCRIPTION: Demonstrates the file path structure for providing input to a Crawlee actor through the INPUT.json file in the default key-value store.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json

----------------------------------------

TITLE: Complete Package.json Configuration for Crawlee TypeScript Project
DESCRIPTION: Full package.json configuration for a Crawlee TypeScript project, including dependencies, scripts, and module type settings.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "type": "module",
    "main": "dist/main.js",
    "dependencies": {
        "crawlee": "3.0.0"
    },
    "devDependencies": {
        "@apify/tsconfig": "^0.1.0",
        "@types/node": "^18.14.0",
        "ts-node": "^10.8.0",
        "typescript": "^4.7.4"
    },
    "scripts": {
        "start": "npm run start:dev",
        "start:prod": "node dist/main.js",
        "start:dev": "ts-node-esm -T src/main.ts",
        "build": "tsc"
    }
}

----------------------------------------

TITLE: Using actor-node-playwright Docker Image
DESCRIPTION: Shows how to use a Docker image that includes all Playwright browsers, suitable for development or testing with multiple browsers.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright:16

----------------------------------------

TITLE: Implementing GCP Cloud Function Handler
DESCRIPTION: Creates a handler function that wraps the crawler execution and returns data through the GCP function response object.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (req, res) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
    
    return res.send(await crawler.getData())
}

----------------------------------------

TITLE: Complete AWS Lambda Handler for CheerioCrawler with Data Return
DESCRIPTION: This snippet shows the complete AWS Lambda handler function for a CheerioCrawler, including crawler initialization, execution, and returning the scraped data in the Lambda response.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);

    return {
        statusCode: 200,
        body: await crawler.getData(),
    }
};

----------------------------------------

TITLE: Complete AWS Lambda Handler for Crawlee with Data Return
DESCRIPTION: This is the final version of the Lambda handler function. It initializes the CheerioCrawler, runs it, and returns the scraped data as the Lambda response.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);

    return {
        statusCode: 200,
        body: await crawler.getData(),
    }
};

----------------------------------------

TITLE: Dataset Structure Example in JSON
DESCRIPTION: Example JSON structure showing scraped data containing URLs and heading counts from different pages. This represents the data stored in the default dataset directory.

LANGUAGE: json
CODE:
[
    {
        "url": "https://crawlee.dev/",
        "headingCount": 11
    },
    {
        "url": "https://crawlee.dev/storage",
        "headingCount": 8
    },
    {
        "url": "https://crawlee.dev/proxy",
        "headingCount": 4
    }
]

----------------------------------------

TITLE: Basic Request Queue Operations in Crawlee
DESCRIPTION: Demonstrates basic operations with the Request Queue in Crawlee, including opening a queue, adding requests, and fetching requests.

LANGUAGE: javascript
CODE:
import { RequestQueue } from 'crawlee';

// Open a named request queue
const queue = await RequestQueue.open('my-queue');

// Add requests to the queue
await queue.addRequest({ url: 'https://example.com/foo' });
await queue.addRequest({ url: 'https://example.com/bar' });

// Get request from queue
const request1 = await queue.fetchNextRequest();
const request2 = await queue.fetchNextRequest();

// Mark requests as handled
await queue.markRequestHandled(request1);
await queue.markRequestHandled(request2);

// Reclaim failed requests back to the queue
await queue.reclaimRequest(request1, { forefront: true });

----------------------------------------

TITLE: Saving Data with Dataset.pushData() in JavaScript
DESCRIPTION: This code demonstrates how to save extracted data using the Dataset.pushData() method in Crawlee.

LANGUAGE: javascript
CODE:
await Dataset.pushData(results);

----------------------------------------

TITLE: Enqueuing Links with Crawlee
DESCRIPTION: Basic example of using enqueueLinks() function in Crawlee to add links to the crawling queue.

LANGUAGE: javascript
CODE:
await enqueueLinks();

----------------------------------------

TITLE: Storage Directory Structure Example
DESCRIPTION: Shows the file path structure for key-value store data storage in Crawlee.

LANGUAGE: plaintext
CODE:
{CRAWLEE_STORAGE_DIR}/key_value_stores/{STORE_ID}/{KEY}.{EXT}

----------------------------------------

TITLE: Crawling All Links Strategy Implementation in Crawlee
DESCRIPTION: Demonstrates how to crawl all links found on a website regardless of their domain using CheerioCrawler and the 'all' enqueue strategy. This approach will follow any URLs discovered during crawling.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks }) {
        // Add all discovered links to the crawling queue,
        // regardless of their domain
        await enqueueLinks({
            strategy: EnqueueStrategy.All,
        });
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Direct Puppeteer Screenshot Capture
DESCRIPTION: Demonstrates how to capture a screenshot of a webpage using Puppeteer's page.screenshot() method directly. The screenshot is saved to a key-value store with a key derived from the page URL.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';
import { chromium } from 'puppeteer';

const browser = await chromium.launch();
const page = await browser.newPage();
await page.goto('https://crawlee.dev');

const screenshotBuffer = await page.screenshot();
const key = (new URL('https://crawlee.dev')).host;
await KeyValueStore.setValue(key, screenshotBuffer, { contentType: 'image/png' });

await browser.close();

----------------------------------------

TITLE: Initializing RequestQueue and Adding a URL in Crawlee
DESCRIPTION: Demonstrates how to create a RequestQueue instance and add a URL to it for crawling.

LANGUAGE: typescript
CODE:
import { RequestQueue } from 'crawlee';

// First you create the request queue instance.
const requestQueue = await RequestQueue.open();
// And then you add one or more requests to it.
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

----------------------------------------

TITLE: HTML Link Example
DESCRIPTION: Example of an HTML anchor tag with href attribute that the crawler would process.

LANGUAGE: html
CODE:
<a href="https://crawlee.dev/js/docs/introduction">This is a link to Crawlee introduction</a>

----------------------------------------

TITLE: Implementing Recursive Web Scraping with PuppeteerCrawler in JavaScript
DESCRIPTION: This code demonstrates how to use PuppeteerCrawler and RequestQueue to recursively scrape the Hacker News website. It starts with a single URL, finds links to next pages, enqueues them, and stores results in a default dataset. The crawler continues until no more desired links are available.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, Dataset, RequestQueue } from 'crawlee';

// Create an instance of the RequestQueue class
const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://news.ycombinator.com/' });

// Create an instance of the PuppeteerCrawler class
const crawler = new PuppeteerCrawler({
    requestQueue,
    async requestHandler({ request, page, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        // Save results to dataset
        await Dataset.pushData({ title, url: request.url });

        // Extract links to other pages and add them to the crawling queue
        await enqueueLinks({
            globs: ['https://news.ycombinator.com/*'],
            exclude: [{ globs: ['*item?id=*'] }],
        });
    },
    maxRequestsPerCrawl: 50, // Limitation for only 50 requests (do not use if you want to crawl all links)
});

// Run the crawler
await crawler.run();

----------------------------------------

TITLE: Configuring Package.json for TypeScript Build
DESCRIPTION: Add build script and main entry point in package.json for TypeScript compilation.

LANGUAGE: json
CODE:
{
    "scripts": {
        "build": "tsc"
    },
    "main": "dist/main.js"
}

----------------------------------------

TITLE: Demonstrating Key-Value Store Operations in Crawlee
DESCRIPTION: This snippet shows basic operations for working with key-value stores in Crawlee, including getting input, setting output, opening named stores, and reading/writing/deleting records.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';

// Get the INPUT from the default key-value store
const input = await KeyValueStore.getInput();

// Write the OUTPUT to the default key-value store
await KeyValueStore.setValue('OUTPUT', { myResult: 123 });

// Open a named key-value store
const store = await KeyValueStore.open('some-name');

// Write a record to the named key-value store.
// JavaScript object is automatically converted to JSON,
// strings and binary buffers are stored as they are
await store.setValue('some-key', { foo: 'bar' });

// Read a record from the named key-value store.
// Note that JSON is automatically parsed to a JavaScript object,
// text data is returned as a string, and other data is returned as binary buffer
const value = await store.getValue('some-key');

// Delete a record from the named key-value store
await store.setValue('some-key', null);

----------------------------------------

TITLE: Demonstrating Key-Value Store Operations in Crawlee
DESCRIPTION: This snippet shows basic operations for working with key-value stores in Crawlee, including getting input, setting output, opening named stores, and reading/writing/deleting records.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';

// Get the INPUT from the default key-value store
const input = await KeyValueStore.getInput();

// Write the OUTPUT to the default key-value store
await KeyValueStore.setValue('OUTPUT', { myResult: 123 });

// Open a named key-value store
const store = await KeyValueStore.open('some-name');

// Write a record to the named key-value store.
// JavaScript object is automatically converted to JSON,
// strings and binary buffers are stored as they are
await store.setValue('some-key', { foo: 'bar' });

// Read a record from the named key-value store.
// Note that JSON is automatically parsed to a JavaScript object,
// text data is returned as a string, and other data is returned as binary buffer
const value = await store.getValue('some-key');

// Delete a record from the named key-value store
await store.setValue('some-key', null);

----------------------------------------

TITLE: Crawling Same Domain Links with CheerioCrawler in Crawlee
DESCRIPTION: This snippet illustrates how to use CheerioCrawler to crawl links within the same domain, including subdomains. It uses the 'same-domain' enqueue strategy.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks }) {
        // Add new links from same domain to RequestQueue
        await enqueueLinks({
            strategy: EnqueueStrategy.SameDomain,
            label: 'detail',
        });
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Crawling All Links with Playwright Crawler in Crawlee
DESCRIPTION: This code snippet illustrates how to use the PlaywrightCrawler to crawl all links on a website. It configures the crawler, sets up a request queue, and uses the enqueueLinks() method to add new links for crawling. It's intended to run on the Apify Platform using a specific Docker image.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, enqueueLinks } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limitation for only 10 requests (do not use if you want to crawl all links)
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Deploying Crawlee Project to Apify Platform
DESCRIPTION: Command to deploy the Crawlee project to the Apify Platform, creating an archive, uploading it, and initiating a Docker build.

LANGUAGE: bash
CODE:
apify push

----------------------------------------

TITLE: Failed Attempt to Scrape JavaScript-Rendered Content with PlaywrightCrawler
DESCRIPTION: This snippet shows an unsuccessful attempt to scrape JavaScript-rendered content using PlaywrightCrawler without waiting for the element to appear. It results in an error due to the element not being present in the DOM yet.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        // Try to extract text content of the first actor card without waiting
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Using Request Queue with Crawler in Crawlee
DESCRIPTION: Demonstrates how to use a request queue implicitly with a Crawlee crawler. The crawler automatically manages the queue, adding new requests discovered during crawling.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, enqueueLinks }) {
        // Add new requests to the automatically managed queue
        await enqueueLinks();
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Accepting and Logging User Input in Crawlee
DESCRIPTION: This code snippet demonstrates how to accept user input in a Crawlee project and log it to the console. It uses the Actor.getInput() method to retrieve the input and console.log() to display it.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

await Actor.init();

const input = await Actor.getInput();
console.log('Input:');
console.log(JSON.stringify(input, null, 2));

await Actor.exit();

----------------------------------------

TITLE: Using Custom HTTP Client with BasicCrawler in TypeScript
DESCRIPTION: This code snippet shows how to instantiate the custom HTTP client and pass it to a BasicCrawler constructor. It demonstrates the usage of the custom client in a Crawlee crawler setup.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    httpClient: new CustomHttpClient(),
    // ...
});


----------------------------------------

TITLE: Configuring Production Script for Crawlee TypeScript Project
DESCRIPTION: This JSON snippet adds a script to package.json for running the compiled JavaScript code in production.

LANGUAGE: json
CODE:
{
    "scripts": {
        "start:prod": "node dist/main.js"
    }
}

----------------------------------------

TITLE: Disabling Browser Fingerprints with PuppeteerCrawler
DESCRIPTION: This code snippet shows how to disable the use of browser fingerprints in PuppeteerCrawler by setting the useFingerprints option to false.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    browserPoolOptions: {
        useFingerprints: false
    },
    // ...
});

----------------------------------------

TITLE: Using actor-node-playwright-firefox Docker Image
DESCRIPTION: Shows how to use a Docker image with Playwright and Firefox pre-installed.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-firefox:16

----------------------------------------

TITLE: Integrating AWS Chromium Setup
DESCRIPTION: Extended configuration showing how to integrate @sparticuz/chromium with Crawlee, including proper launch options for AWS Lambda environment.

LANGUAGE: javascript
CODE:
// For more information, see https://crawlee.dev/
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
    launchContext: {
        launchOptions: {
             executablePath: await aws_chromium.executablePath(),
             args: aws_chromium.args,
             headless: true
        }
    }
}, new Configuration({
    persistStorage: false,
}));

----------------------------------------

TITLE: Creating AWS Lambda Handler Function for Cheerio Crawler
DESCRIPTION: This code wraps the Cheerio crawler logic in a handler function, which is the entry point for AWS Lambda execution. It ensures a new crawler instance is created for each Lambda invocation.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
};

----------------------------------------

TITLE: Using actor-node Docker Image
DESCRIPTION: Example of using the smallest Apify Docker image based on Alpine Linux, suitable for CheerioCrawler but not for browser-based crawlers.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:20

----------------------------------------

TITLE: Specifying Node.js Version in Dockerfile
DESCRIPTION: Example of how to specify a Node.js version when using an Apify Docker image.

LANGUAGE: dockerfile
CODE:
# Use Node.js 20
FROM apify/actor-node:20

----------------------------------------

TITLE: Using actor-node Docker Image
DESCRIPTION: Example of using the smallest Apify Docker image based on Alpine Linux, suitable for CheerioCrawler but not for browser-based crawlers.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:20

----------------------------------------

TITLE: Complete Crawlee Data Extraction and Saving Example (JavaScript)
DESCRIPTION: A complete example of a Crawlee script that extracts data from a website and saves it using Dataset.pushData(). This code demonstrates the entire process from setting up the crawler to saving the extracted data.

LANGUAGE: javascript
CODE:
{Example}

----------------------------------------

TITLE: Using actor-node-playwright Docker Image
DESCRIPTION: Demonstrates how to use the comprehensive Apify Docker image that includes all Playwright browsers.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright:16

----------------------------------------

TITLE: Using Custom Browser Modules in Apify SDK v1
DESCRIPTION: Shows how to use custom Puppeteer and Playwright modules when launching browsers.

LANGUAGE: javascript
CODE:
const puppeteer = require('puppeteer');
const playwright = require('playwright');

await Apify.launchPuppeteer({
    launcher: puppeteer
})

await Apify.launchPlaywright({
    launcher: playwright.chromium
})

----------------------------------------

TITLE: Disabling Browser Fingerprints with PuppeteerCrawler in Crawlee
DESCRIPTION: This code snippet shows how to disable browser fingerprints for PuppeteerCrawler in Crawlee by setting the useFingerprints option to false.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    browserPoolOptions: {
        useFingerprints: false,
    },
    // ...
});

----------------------------------------

TITLE: Using actor-node-playwright Docker Image
DESCRIPTION: Shows how to use the comprehensive Apify Docker image that includes all Playwright browsers, suitable for development and testing with multiple browsers.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright:20

----------------------------------------

TITLE: Using Request Queue with Crawler in Crawlee
DESCRIPTION: Demonstrates how to use a Request Queue with a Crawler in Crawlee. It shows how to create a PuppeteerCrawler instance and add requests to the queue.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // Process the page...
        await crawler.addRequests(['https://crawlee.dev']);
    },
});

await crawler.addRequests(['https://crawlee.dev']);
await crawler.run();

----------------------------------------

TITLE: Crawlee Scraper Log Output
DESCRIPTION: Example log output from a Crawlee scraper showing the titles of scraped pages.

LANGUAGE: log
CODE:
INFO  PlaywrightCrawler: Starting the crawl
INFO  PlaywrightCrawler: Title of https://crawlee.dev/ is 'Crawlee · Build reliable crawlers. Fast. | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core is '@crawlee/core | API | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core/changelog is 'Changelog | API | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'

----------------------------------------

TITLE: Capturing Single Page Screenshot with Puppeteer in JavaScript
DESCRIPTION: This snippet demonstrates how to capture a screenshot of a single web page using Puppeteer's page.screenshot() method. It launches a browser, creates a new page, navigates to a URL, takes a screenshot, and saves it to a key-value store.

LANGUAGE: javascript
CODE:
import { launchPuppeteer } from 'crawlee';
import { KeyValueStore } from 'crawlee';

const browser = await launchPuppeteer();
const page = await browser.newPage();

await page.goto('https://example.com');

const screenshot = await page.screenshot();
const key = 'my-screenshot';

await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });

await browser.close();

----------------------------------------

TITLE: Exporting Dataset to CSV File Using Crawlee
DESCRIPTION: This code snippet demonstrates how to use Crawlee's Dataset API to export an entire dataset to a single CSV file. It utilizes the `exportToValue` function to export the default dataset and store it in the default key-value store.

LANGUAGE: javascript
CODE:
import { Dataset, KeyValueStore } from 'crawlee';

await Dataset.open();

// Export entire dataset to CSV file
const csv = await Dataset.exportToValue('CSV');

// Save the CSV file to key-value store
await KeyValueStore.setValue('DATASET.csv', csv, { contentType: 'text/csv' });

----------------------------------------

TITLE: Configuring TypeScript Compiler Options
DESCRIPTION: JSON configuration for tsconfig.json to set up TypeScript compiler options and project structure.

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist"
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Extracting Manufacturer from URL in JavaScript
DESCRIPTION: Demonstrates how to extract the manufacturer name from a product URL by splitting the string.

LANGUAGE: javascript
CODE:
// request.url = https://warehouse-theme-metal.myshopify.com/products/sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440

const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']
const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'

----------------------------------------

TITLE: Crawling Same Hostname Links Strategy Implementation
DESCRIPTION: Shows how to crawl links that match the same hostname as the source URL using CheerioCrawler. This is the default strategy that only processes URLs from the same hostname.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks }) {
        // Only add links that share the same hostname as the origin
        await enqueueLinks({
            strategy: EnqueueStrategy.SameHostname,
        });
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Crawling Same Domain Links Strategy in Crawlee
DESCRIPTION: Illustrates how to crawl links that share the same domain name, including subdomains, using CheerioCrawler. This strategy allows crawling across different subdomains of the same root domain.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks }) {
        // Add links that match the same domain name,
        // including subdomains
        await enqueueLinks({
            strategy: EnqueueStrategy.SameDomain,
        });
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Crawling URLs with Cheerio Crawler
DESCRIPTION: Implementation of a web crawler using Cheerio Crawler to process multiple URLs. Cheerio provides a lightweight and fast solution for parsing static HTML content.

LANGUAGE: javascript
CODE:
{CheerioSource}

----------------------------------------

TITLE: Crawlee Results Output Format
DESCRIPTION: Example of JSON output format for scraped data stored in the default dataset

LANGUAGE: json
CODE:
{
    "url": "https://crawlee.dev/",
    "title": "Crawlee · The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee"
}

----------------------------------------

TITLE: Finding All Links on a Page with JSDOM
DESCRIPTION: Extracts all href attributes from anchor tags on a page using JSDOM's querySelector functionality. Returns an array of all href values found on the page.

LANGUAGE: javascript
CODE:
Array.from(document.querySelectorAll('a[href]')).map((a) => a.href);

----------------------------------------

TITLE: Using Request List in Crawlee
DESCRIPTION: Shows how to create and use a Request List with a PuppeteerCrawler in Crawlee. The Request List is used for processing a predefined set of URLs.

LANGUAGE: javascript
CODE:
import { RequestList, PuppeteerCrawler } from 'crawlee';

// Prepare the sources array with URLs to visit
const sources = [
    { url: 'http://www.example.com/page-1' },
    { url: 'http://www.example.com/page-2' },
    { url: 'http://www.example.com/page-3' },
];

// Open the request list.
// List name is used to persist the sources and the list state in the key-value store
const requestList = await RequestList.open('my-list', sources);

// The crawler will automatically process requests from the list
// It's used the same way for Cheerio /Playwright crawlers.
const crawler = new PuppeteerCrawler({
    requestList,
    async requestHandler({ page, request }) {
        // Process the page (extract data, take page screenshot, etc).
        // No more requests could be added to the request list here
    },
});

----------------------------------------

TITLE: Successful Actor Content Scraping Log
DESCRIPTION: Shows the successful output when using a headless browser to scrape JavaScript-rendered content.

LANGUAGE: log
CODE:
ACTOR: Web Scraperapify/web-scraperCrawls arbitrary websites using [...]

----------------------------------------

TITLE: Request Transformation Logic
DESCRIPTION: Example of using transformRequestFunction to filter and modify requests before they are enqueued.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
    transformRequestFunction(req) {
        // ignore all links ending with `.pdf`
        if (req.url.endsWith('.pdf')) return false;
        return req;
    },
});

----------------------------------------

TITLE: Configuring enqueueLinks for Same-Domain Crawling in Crawlee
DESCRIPTION: Shows how to set up enqueueLinks to crawl links within the same domain, including subdomains.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    strategy: 'same-domain'
});

----------------------------------------

TITLE: Configuring Development Script for Crawlee TypeScript Project
DESCRIPTION: This JSON snippet adds a script to package.json for running the TypeScript project in development mode using ts-node.

LANGUAGE: json
CODE:
{
    "scripts": {
        "start:dev": "ts-node-esm -T src/main.ts"
    }
}

----------------------------------------

TITLE: Cross-Context Access Using Context IDs
DESCRIPTION: Demonstration of how to access and manipulate data across different crawling contexts using context IDs.

LANGUAGE: javascript
CODE:
let masterContextId;
const handlePageFunction = async ({ id, page, request, crawler }) => {
    if (request.userData.masterPage) {
        masterContextId = id;
        // Prepare the master page.
    } else {
        const masterContext = crawler.crawlingContexts.get(masterContextId);
        const masterPage = masterContext.page;
        const masterRequest = masterContext.request;
        // Now we can manipulate the master data from another handlePageFunction.
    }
}

----------------------------------------

TITLE: Extracting Product SKU with Playwright
DESCRIPTION: This snippet shows how to use Playwright to find and extract the product SKU from a specific element on the page.

LANGUAGE: javascript
CODE:
const sku = await page.locator('span.product-meta__sku-number').textContent();

----------------------------------------

TITLE: Crawling All Links Strategy with CheerioCrawler
DESCRIPTION: Demonstrates how to crawl all links on a website regardless of their domain using the 'all' strategy. This will match and enqueue any URLs found during crawling, even if they point to external websites.

LANGUAGE: javascript
CODE:
{AllLinksSource}

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Configuration in Node.js
DESCRIPTION: Sets up a basic PlaywrightCrawler instance with persistent storage disabled using Crawlee's Configuration class.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Configuring Crawlee with Custom Storage Settings
DESCRIPTION: Initial code setup showing how to initialize PlaywrightCrawler with a custom Configuration instance to prevent storage interference between crawler instances

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Configuring Request Rate Limiting in Crawlee
DESCRIPTION: This snippet demonstrates how to set the maximum number of requests per minute for a CheerioCrawler in Crawlee. It limits the crawler to 120 requests per minute.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    maxRequestsPerMinute: 120,
    // ...
});

----------------------------------------

TITLE: Using actor-node-puppeteer-chrome Docker Image
DESCRIPTION: Example of how to use the Apify Docker image that includes Puppeteer and Chrome, suitable for CheerioCrawler and PuppeteerCrawler.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-puppeteer-chrome:16

----------------------------------------

TITLE: Using actor-node-playwright-chrome Docker Image
DESCRIPTION: Example of using the Apify Docker image that includes Playwright and Chrome, suitable for CheerioCrawler and PlaywrightCrawler.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:20

----------------------------------------

TITLE: Basic Link Enqueuing in Crawlee
DESCRIPTION: Simple example of using enqueueLinks() function in Crawlee for basic link crawling.

LANGUAGE: javascript
CODE:
await enqueueLinks();

----------------------------------------

TITLE: Configuring Request Rate Limiting in Crawlee
DESCRIPTION: This snippet demonstrates how to set the maximum number of requests per minute for a CheerioCrawler in Crawlee. It limits the crawler to 120 requests per minute.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    maxRequestsPerMinute: 120,
    // ...
});

----------------------------------------

TITLE: Creating and Running a Basic Actor
DESCRIPTION: Commands to create and run a new actor project using Apify CLI.

LANGUAGE: bash
CODE:
apify create my-hello-world
cd my-hello-world
apify run

----------------------------------------

TITLE: Production Script Configuration
DESCRIPTION: Package.json script for running compiled JavaScript in production

LANGUAGE: json
CODE:
{
    "scripts": {
        "start:prod": "node dist/main.js"
    }
}

----------------------------------------

TITLE: Playwright Firefox Configuration
DESCRIPTION: Docker configuration for Node.js with Playwright and Firefox browser support.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-firefox:16

----------------------------------------

TITLE: Capturing Screenshots with PuppeteerCrawler Using page.screenshot()
DESCRIPTION: This snippet demonstrates how to capture screenshots of multiple web pages when using PuppeteerCrawler. It uses page.screenshot() method and saves screenshots to a key-value store with keys derived from page URLs.

LANGUAGE: javascript
CODE:
import { KeyValueStore, PuppeteerCrawler } from '@crawlee/puppeteer';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // Capture screenshot
        const screenshotBuffer = await page.screenshot();

        // Save screenshot to default key-value store
        const kvStore = await KeyValueStore.open();
        const key = `screenshot-${new URL(request.url).hostname}`;
        await kvStore.setValue(key, screenshotBuffer, { contentType: 'image/png' });
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Crawling Same Domain Links with CheerioCrawler in Crawlee
DESCRIPTION: This snippet illustrates how to use the CheerioCrawler to crawl links within the same domain, including subdomains. It uses the 'same-domain' enqueue strategy.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks }) {
        // Add all links from page to the queue,
        // if they are on the same domain
        await enqueueLinks({
            strategy: EnqueueStrategy.SameDomain,
            // or the string "same-domain"
        });
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Multiple URL Crawling with Cheerio Crawler
DESCRIPTION: Implementation of a Cheerio-based crawler for processing multiple URLs. Uses Cheerio for HTML parsing which is memory efficient for static pages.

LANGUAGE: javascript
CODE:
{CheerioSource}

----------------------------------------

TITLE: Installing and Zipping Dependencies for AWS Lambda
DESCRIPTION: Commands to install the @sparticuz/chromium package and zip the node_modules folder for uploading as a Lambda Layer.

LANGUAGE: bash
CODE:
# Install the package
npm i -S @sparticuz/chromium

# Zip the dependencies
zip -r dependencies.zip ./node_modules

----------------------------------------

TITLE: Configuring PuppeteerCrawler for Recursive Web Crawling
DESCRIPTION: Example demonstrating recursive website crawling setup using PuppeteerCrawler. This implementation shows how to configure and execute a recursive crawler that can navigate through website structures. Note that when running on Apify Platform, the apify/actor-node-puppeteer-chrome Docker image should be used.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    requestHandler: async ({ page, enqueueLinks }) => {
        // Add links to queue for recursive crawling
        await enqueueLinks();
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Basic Request Queue Operations with Crawler
DESCRIPTION: Demonstrates automatic request queue handling when using a crawler without explicit queue configuration. Requests are added and processed automatically by the crawler.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // Process pages, extract data, etc.
        // More requests can be added to the queue with `crawler.addRequests()`
    },
});

// Add requests
await crawler.addRequests([
    'http://example.com/page-1',
    { url: 'http://example.com/page-2', userData: { foo: 'bar' } },
]);

// Run the crawler
await crawler.run();

----------------------------------------

TITLE: Pre-release Image Configuration
DESCRIPTION: Docker configuration examples for using pre-release versions of images

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:20-beta

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:20-1.10.0-beta

----------------------------------------

TITLE: Crawling Specific Links with CheerioCrawler in JavaScript
DESCRIPTION: This code snippet shows how to use CheerioCrawler to crawl specific links on a website. It uses the 'globs' property to filter links and only add those matching the specified pattern to the RequestQueue. The crawler starts with the Apple homepage and crawls product pages.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    // Function called for each URL
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);

        // Save results
        await Dataset.pushData({
            url: request.url,
            title,
        });

        // Extract links to crawler
        // but only add those matching 'https://apps.apple.com/**/**'
        await enqueueLinks({
            globs: ['https://apps.apple.com/**/**'],
        });
    },
    // Let's limit our crawling to make the example quick and safe
    maxRequestsPerCrawl: 20,
});

// Start the crawler with the provided URLs
await crawler.run(['https://apps.apple.com/us']);


----------------------------------------

TITLE: Crawling Multiple URLs with Puppeteer Crawler in JavaScript
DESCRIPTION: This code snippet shows how to use Puppeteer Crawler to crawl a list of specified URLs. It captures screenshots, extracts the title and URL from each page, and stores the results.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, Dataset } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url} is '${title}'`);

        await Dataset.pushData({
            title,
            url: request.url,
            screenshot: await page.screenshot(),
        });
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run([
    'https://crawlee.dev',
    'https://apify.com',
]);

----------------------------------------

TITLE: Checking Product Stock Availability
DESCRIPTION: Demonstrates how to check if a product is in stock by looking for specific text in inventory element.

LANGUAGE: javascript
CODE:
const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Getting Text Content with Cheerio
DESCRIPTION: Simple example showing how to extract text content from an h2 element using Cheerio's jQuery-like syntax.

LANGUAGE: javascript
CODE:
$('h2').text()

----------------------------------------

TITLE: Customizing Browser Fingerprints with PlaywrightCrawler in Crawlee
DESCRIPTION: This snippet demonstrates how to customize browser fingerprints using PlaywrightCrawler in Crawlee. It sets specific options for the fingerprint generation, including browser, operating system, and screen size.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    // ...
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                browsers: [
                    { name: 'firefox', minVersion: 80 },
                    { name: 'chrome', minVersion: 87 },
                ],
                operatingSystems: ['windows', 'linux'],
                screens: ['1920x1080', '1440x900'],
            },
        },
    },
});

----------------------------------------

TITLE: Fetching Single URL with got-scraping in Crawlee
DESCRIPTION: Example shows how to use got-scraping package to fetch HTML content from a single webpage. The code demonstrates basic web scraping functionality using Crawlee's infrastructure.

LANGUAGE: javascript
CODE:
import { gotScraping } from 'got-scraping';

const response = await gotScraping('https://example.com');
console.log(response.body);

----------------------------------------

TITLE: Dockerfile Best Practice for Apify Images
DESCRIPTION: Demonstrates the recommended way to specify an Apify Docker image in a Dockerfile, using both Node.js and automation library version tags.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16

----------------------------------------

TITLE: Using Apify Proxy in Crawlee
DESCRIPTION: JavaScript code to create a proxy configuration for Apify Proxy and generate a new proxy URL.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration();
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Installing Apify TypeScript Configuration for Crawlee Project
DESCRIPTION: This command installs the Apify TypeScript configuration package as a development dependency for a Crawlee project.

LANGUAGE: shell
CODE:
npm install --save-dev @apify/tsconfig

----------------------------------------

TITLE: Creating and Running an Apify Actor
DESCRIPTION: Commands to create a new actor project and run it locally using Apify CLI

LANGUAGE: bash
CODE:
apify create my-hello-world
cd my-hello-world
apify run

----------------------------------------

TITLE: Scraping Product Title with Playwright
DESCRIPTION: This code uses Playwright to locate and extract the product title from the page using a CSS selector.

LANGUAGE: javascript
CODE:
const title = await page.locator('.product-meta h1').textContent();

----------------------------------------

TITLE: Crawling Same Hostname Links with CheerioCrawler
DESCRIPTION: Shows how to crawl links that share the same hostname as the starting URL. This default strategy matches relative URLs and absolute URLs pointing to the same hostname, excluding subdomains.

LANGUAGE: javascript
CODE:
{SameHostnameSource}

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Configuration
DESCRIPTION: Sets up a PlaywrightCrawler instance with disabled storage persistence using Crawlee's Configuration class. This is the basic setup needed for GCP Cloud Run compatibility.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Complete AWS Lambda Handler Implementation
DESCRIPTION: Full implementation of AWS Lambda handler function with Crawlee crawler configuration and response handling

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
        launchContext: {
            launchOptions: {
                executablePath: await aws_chromium.executablePath(),
                args: aws_chromium.args,
                headless: true
            }
        }
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);

    return {
        statusCode: 200,
        body: await crawler.getData(),
    };
}

----------------------------------------

TITLE: Dockerfile Best Practice for Apify Images
DESCRIPTION: Demonstrates the recommended way to specify an Apify Docker image in a Dockerfile, using both Node.js and automation library version tags.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16

----------------------------------------

TITLE: Installing Apify SDK and CLI Dependencies
DESCRIPTION: Commands to install the Apify SDK as a project dependency and the Apify CLI as a global tool for Crawlee projects.

LANGUAGE: bash
CODE:
npm install apify

LANGUAGE: bash
CODE:
npm install -g apify-cli

----------------------------------------

TITLE: Storage Cleanup in Crawlee
DESCRIPTION: Example showing how to purge default storage directories using the purgeDefaultStorages helper function.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Installing Crawlee with CLI
DESCRIPTION: Commands to install and start a new Crawlee project using the CLI tool

LANGUAGE: bash
CODE:
npx crawlee create my-crawler
cd my-crawler && npm start

----------------------------------------

TITLE: Displaying Dataset Storage Path
DESCRIPTION: Shows the default storage location for dataset items within the project folder structure. Each item is saved as a separate file in this directory.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/datasets/default/

----------------------------------------

TITLE: Production Script Configuration
DESCRIPTION: Package.json script configuration for production deployment

LANGUAGE: json
CODE:
{
    "scripts": {
        "start:prod": "node dist/main.js"
    }
}

----------------------------------------

TITLE: Configuring Dockerfile for Crawlee Node.js Project
DESCRIPTION: This Dockerfile sets up a Docker image for a Crawlee project. It uses a Node.js base image, installs dependencies, copies source code, and configures the run command. The build process is optimized for caching and minimal image size.

LANGUAGE: Dockerfile
CODE:
# Specify the base Docker image. You can read more about
# the available images at https://crawlee.dev/js/docs/guides/docker-images
# You can also use any other image from Docker Hub.
FROM apify/actor-node:20

# Copy just package.json and package-lock.json
# to speed up the build using Docker layer cache.
COPY package*.json ./

# Install NPM packages, skip optional and development dependencies to
# keep the image small. Avoid logging too much and print the dependency
# tree for debugging
RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

# Next, copy the remaining files and directories with the source code.
# Since we do this after NPM install, quick build will be really fast
# for most source file changes.
COPY . ./


# Run the image.
CMD npm start --silent

----------------------------------------

TITLE: Multi-Stage Docker Build for Crawlee Actor
DESCRIPTION: Creates a Docker container for running Crawlee actors with Node.js and Playwright-Chrome. Uses a multi-stage build to optimize image size and includes development dependencies only in the build stage. The final image contains only production dependencies and built artifacts.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16 AS builder

COPY --chown=myuser package*.json ./

RUN npm install --include=dev --audit=false

COPY --chown=myuser . ./

RUN npm run build

FROM apify/actor-node-playwright-chrome:16

COPY --from=builder --chown=myuser /home/myuser/dist ./dist

COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent

----------------------------------------

TITLE: Getting Text Content with Cheerio
DESCRIPTION: Shows how to extract text content from an h2 element using Cheerio's jQuery-like syntax

LANGUAGE: javascript
CODE:
$('h2').text()

----------------------------------------

TITLE: Configuring Package.json for GCP Deployment
DESCRIPTION: Updates the package.json configuration to specify the main entry point for the GCP function.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "version": "1.0.0",
    "main": "src/main.js",
    ...
}

----------------------------------------

TITLE: SendRequest Implementation Details
DESCRIPTION: Shows the internal implementation of the sendRequest function, including how it handles options like proxies, cookies, and retry logic.

LANGUAGE: typescript
CODE:
async sendRequest(overrideOptions?: GotOptionsInit) => {
    return gotScraping({
        url: request.url,
        method: request.method,
        body: request.payload,
        headers: request.headers,
        proxyUrl: crawlingContext.proxyInfo?.url,
        sessionToken: session,
        responseType: 'text',
        ...overrideOptions,
        retry: {
            limit: 0,
            ...overrideOptions?.retry,
        },
        cookieJar: {
            getCookieString: (url: string) => session!.getCookieString(url),
            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),
            ...overrideOptions?.cookieJar,
        },
    });
}

----------------------------------------

TITLE: Installing Apify SDK v1 Dependencies
DESCRIPTION: Commands for installing Apify SDK v1 with either Puppeteer or Playwright support.

LANGUAGE: bash
CODE:
npm install apify puppeteer

LANGUAGE: bash
CODE:
npm install apify playwright

----------------------------------------

TITLE: Disabling Browser Fingerprints with PuppeteerCrawler in Crawlee
DESCRIPTION: This code snippet shows how to disable the use of browser fingerprints when using PuppeteerCrawler in Crawlee. It sets the 'useFingerprints' option to false in the browserPoolOptions.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    browserPoolOptions: {
        useFingerprints: false,
    },
    // ...
});

----------------------------------------

TITLE: Sample Crawlee Results Output
DESCRIPTION: Example JSON output showing the structure of crawled data stored in the default dataset

LANGUAGE: json
CODE:
{
    "url": "https://crawlee.dev/",
    "title": "Crawlee · The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee"
}

----------------------------------------

TITLE: Deploying to Apify Platform
DESCRIPTION: Command to publish the project to the Apify Platform, creating an Actor.

LANGUAGE: bash
CODE:
apify push

----------------------------------------

TITLE: Setting Request Limit in Cheerio Crawler
DESCRIPTION: Configuration example showing how to limit the maximum number of requests per crawl using maxRequestsPerCrawl option.

LANGUAGE: typescript
CODE:
const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 20,
    // ...
});

----------------------------------------

TITLE: Creating CheerioCrawler with RequestQueue in Crawlee
DESCRIPTION: This code sets up a CheerioCrawler with a RequestQueue and a requestHandler. It demonstrates how to initialize a crawler, process a webpage, and extract its title using Cheerio.

LANGUAGE: typescript
CODE:
import { RequestQueue, CheerioCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

const crawler = new CheerioCrawler({
    requestQueue,
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run();

----------------------------------------

TITLE: Browser Fingerprint Configuration
DESCRIPTION: Example of configuring browser fingerprints in PlaywrightCrawler.

LANGUAGE: typescript
CODE:
const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        useFingerprints: false,
    },
});

----------------------------------------

TITLE: Advanced URL Filtering with Patterns
DESCRIPTION: Example of using globs pattern matching for URL filtering with enqueueLinks.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
});

----------------------------------------

TITLE: Storage Cleanup in Crawlee
DESCRIPTION: Demonstrates how to clean up default storage directories

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Using sendRequest with BasicCrawler in TypeScript
DESCRIPTION: Demonstrates how to use the sendRequest function within a BasicCrawler's requestHandler. This example shows the basic usage of sendRequest to make HTTP requests and log the response body.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest();
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Implementing Recursive Web Scraping with PuppeteerCrawler
DESCRIPTION: Demonstrates setting up a PuppeteerCrawler to recursively scrape Hacker News, including handling pagination and storing results. Uses RequestQueue for URL management and stores data in the default dataset. Requires puppeteer and the apify/actor-node-puppeteer-chrome environment.

LANGUAGE: typescript
CODE:
import { Dataset, createPuppeteerRouter, PuppeteerCrawler } from 'crawlee';

const router = createPuppeteerRouter();

router.addDefaultHandler(async ({ request, page, enqueueLinks }) => {
    const title = await page.title();
    console.log(`Title of ${request.url}: ${title}`);

    await Dataset.pushData({
        url: request.url,
        title,
    });

    await enqueueLinks({
        globs: ['https://news.ycombinator.com/*'],
        label: 'detail',
    });
});

const crawler = new PuppeteerCrawler({
    requestHandler: router,
});

await crawler.run(['https://news.ycombinator.com']);

----------------------------------------

TITLE: Extracting Product Title with Playwright
DESCRIPTION: This code snippet shows how to use Playwright to extract the product title from a webpage using a CSS selector.

LANGUAGE: javascript
CODE:
const title = await page.locator('.product-meta h1').textContent();

----------------------------------------

TITLE: Customizing Header Generation with sendRequest in Crawlee
DESCRIPTION: Shows how to customize the header generation options when using sendRequest for more specific browser fingerprinting.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            headerGeneratorOptions: {
                devices: ['mobile', 'desktop'],
                locales: ['en-US'],
                operatingSystems: ['windows', 'macos', 'android', 'ios'],
                browsers: ['chrome', 'edge', 'firefox', 'safari'],
            },
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Setting Apify Token in Configuration
DESCRIPTION: Example of configuring Apify token programmatically using Actor configuration

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const sdk = new Actor({ token: 'your_api_token' });

----------------------------------------

TITLE: Configuring TypeScript for Crawlee Project
DESCRIPTION: This JSON configuration sets up TypeScript compiler options for a Crawlee project, including module settings and output directory.

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist"
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Configuring minConcurrency and maxConcurrency in CheerioCrawler
DESCRIPTION: Shows how to set the minimum and maximum concurrency for parallel requests in CheerioCrawler.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    minConcurrency: 10,
    maxConcurrency: 100,
    // ...
});

----------------------------------------

TITLE: Checking Product Stock Status
DESCRIPTION: Demonstrates how to determine if a product is in stock by checking for the presence of specific elements.

LANGUAGE: javascript
CODE:
const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Using Request List in Crawlee
DESCRIPTION: Shows how to create and use a request list with a crawler in Crawlee. The request list is used for processing a predefined set of URLs.

LANGUAGE: javascript
CODE:
import { RequestList, PuppeteerCrawler } from 'crawlee';

// Prepare the sources array with URLs to visit
const sources = [
    { url: 'http://www.example.com/page-1' },
    { url: 'http://www.example.com/page-2' },
    { url: 'http://www.example.com/page-3' },
];

// Open the request list.
// List name is used to persist the sources and the list state in the key-value store
const requestList = await RequestList.open('my-list', sources);

// The crawler will automatically process requests from the list
// It's used the same way for Cheerio /Playwright crawlers.
const crawler = new PuppeteerCrawler({
    requestList,
    async requestHandler({ page, request }) {
        // Process the page (extract data, take page screenshot, etc).
        // No more requests could be added to the request list here
    },
});

----------------------------------------

TITLE: Crawler Configuration Example
DESCRIPTION: Example of configuring a PlaywrightCrawler with browser fingerprint options

LANGUAGE: typescript
CODE:
const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        useFingerprints: false,
    },
});

----------------------------------------

TITLE: Complete AWS Lambda Handler Implementation
DESCRIPTION: Final implementation of the AWS Lambda handler function that initializes and runs the crawler, then returns the results in the expected Lambda response format

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
        launchContext: {
            launchOptions: {
                executablePath: await aws_chromium.executablePath(),
                args: aws_chromium.args,
                headless: true
            }
        }
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);

    return {
        statusCode: 200,
        body: await crawler.getData(),
    };
}

----------------------------------------

TITLE: Complete Package.json Configuration for Crawlee TypeScript Project
DESCRIPTION: Full package.json configuration including dependencies, scripts, and module type for a Crawlee TypeScript project.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "type": "module",
    "main": "dist/main.js",
    "dependencies": {
        "crawlee": "3.0.0"
    },
    "devDependencies": {
        "@apify/tsconfig": "^0.1.0",
        "@types/node": "^18.14.0",
        "ts-node": "^10.8.0",
        "typescript": "^4.7.4"
    },
    "scripts": {
        "start": "npm run start:dev",
        "start:prod": "node dist/main.js",
        "start:dev": "ts-node-esm -T src/main.ts",
        "build": "tsc"
    }
}

----------------------------------------

TITLE: Initializing RequestQueue in Crawlee
DESCRIPTION: Demonstrates how to create a RequestQueue instance and add a URL to it for crawling. This is the basic setup required for starting a crawler.

LANGUAGE: typescript
CODE:
import { RequestQueue } from 'crawlee';

// First you create the request queue instance.
const requestQueue = await RequestQueue.open();
// And then you add one or more requests to it.
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

----------------------------------------

TITLE: Sample Crawlee Results Output
DESCRIPTION: Example JSON output showing the structure of crawled data stored in the default dataset

LANGUAGE: json
CODE:
{
    "url": "https://crawlee.dev/",
    "title": "Crawlee · The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee"
}

----------------------------------------

TITLE: Storage Cleanup in Crawlee
DESCRIPTION: Shows how to purge default storage directories using the purgeDefaultStorages helper function

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Using actor-node-playwright-chrome Docker Image
DESCRIPTION: Shows how to use the Apify Docker image that includes Playwright and Chrome, suitable for CheerioCrawler and PlaywrightCrawler.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16

----------------------------------------

TITLE: Updating package.json for GCP Functions
DESCRIPTION: Modifies the package.json file to set the main entry point for the GCP function.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "version": "1.0.0",
    "main": "src/main.js",
    ...
}

----------------------------------------

TITLE: Creating Dockerfile for Crawlee TypeScript Project
DESCRIPTION: Multi-stage Dockerfile for building and running a Crawlee TypeScript project in production.

LANGUAGE: dockerfile
CODE:
# using multistage build, as we need dev deps to build the TS source code
FROM apify/actor-node:16 AS builder

# copy all files, install all dependencies (including dev deps) and build the project
COPY . ./
RUN npm install --include=dev \
    && npm run build

# create final image
FROM apify/actor-node:16
# copy only necessary files
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/dist ./dist

# install only prod deps
RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional

# run compiled code
CMD npm run start:prod

----------------------------------------

TITLE: Creating a RequestQueue with Locking Support
DESCRIPTION: This code shows how to create a RequestQueue that supports locking by using the RequestQueueV2 class instead of the standard RequestQueue. It also demonstrates adding requests to the queue.

LANGUAGE: typescript
CODE:
import { RequestQueueV2 } from 'crawlee';

const queue = await RequestQueueV2.open('my-locking-queue');
await queue.addRequests([
    { url: 'https://crawlee.dev' },
    { url: 'https://crawlee.dev/js/docs' },
    { url: 'https://crawlee.dev/js/api' },
]);

----------------------------------------

TITLE: Complete AWS Lambda Handler with Response
DESCRIPTION: Final implementation of the AWS Lambda handler including data return functionality and proper configuration for stateless operation.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);

    return {
        statusCode: 200,
        body: await crawler.getData(),
    }
};

----------------------------------------

TITLE: Using Custom HTTP Client with BasicCrawler
DESCRIPTION: Demonstrates how to instantiate and configure a custom HTTP client with BasicCrawler, including setting up the crawler instance with the custom client implementation.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';
import { FetchHttpClient } from './http-client';

const crawler = new BasicCrawler({
    httpClient: new FetchHttpClient(),
    // ...
});


----------------------------------------

TITLE: Crawling Sitemap with Playwright Crawler in JavaScript
DESCRIPTION: This snippet illustrates how to use Playwright Crawler to download and process URLs from a sitemap. It includes a note about using the appropriate Docker image on the Apify Platform.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from '@crawlee/playwright';
import { downloadListOfUrls } from '@crawlee/utils';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        const title = await page.title();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
    maxRequestsPerCrawl: 20, // Limit the number of requests
});

const listOfUrls = await downloadListOfUrls({ url: 'https://apify.com/sitemap.xml' });
await crawler.addRequests(listOfUrls);

await crawler.run();

----------------------------------------

TITLE: Using Custom CookieJar with sendRequest in Crawlee
DESCRIPTION: Demonstrates how to use a custom CookieJar with the sendRequest function in a BasicCrawler's requestHandler.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';
import { CookieJar } from 'tough-cookie';

const cookieJar = new CookieJar();

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ cookieJar });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Storage Directory Structure Example
DESCRIPTION: Shows the directory structure pattern used by Crawlee for storing key-value data.

LANGUAGE: plaintext
CODE:
{CRAWLEE_STORAGE_DIR}/key_value_stores/{STORE_ID}/{KEY}.{EXT}

----------------------------------------

TITLE: Initializing RequestQueue and Adding URL in Crawlee
DESCRIPTION: This snippet demonstrates how to create a RequestQueue instance and add a URL to it for crawling. It's a basic setup for managing crawl targets in Crawlee.

LANGUAGE: typescript
CODE:
import { RequestQueue } from 'crawlee';

// First you create the request queue instance.
const requestQueue = await RequestQueue.open();
// And then you add one or more requests to it.
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

----------------------------------------

TITLE: Browser Fingerprint Configuration
DESCRIPTION: Example of configuring browser fingerprints in PlaywrightCrawler

LANGUAGE: typescript
CODE:
const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        useFingerprints: false,
    },
});

----------------------------------------

TITLE: Using HttpCrawler for URL Crawling - JavaScript
DESCRIPTION: Example showing the implementation of HttpCrawler to process URLs from an external source, make HTTP requests, and save the resulting HTML content. The code demonstrates basic web crawling functionality.

LANGUAGE: javascript
CODE:
import { HttpCrawler, Dataset } from 'crawlee';

// Create an instance of the HttpCrawler class - dump html files
const crawler = new HttpCrawler({
    // Function called for each URL
    async requestHandler({ request, body }) {
        // Save data to dataset
        await Dataset.pushData({
            url: request.url,
            html: body,
        });
    },
});

// Add URLs to the queue and run the crawler
await crawler.run(['http://example.com/page-1']);

----------------------------------------

TITLE: Configuring TypeScript for Crawlee Project
DESCRIPTION: TypeScript configuration file (tsconfig.json) setup for a Crawlee project, extending @apify/tsconfig and setting compiler options for ES2022 modules.

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist"
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Storage Cleanup in Crawlee
DESCRIPTION: Shows how to purge default storage directories using the purgeDefaultStorages helper function.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Accessing KeyValue Store Public URL
DESCRIPTION: Example of storing data and getting a public URL for a KeyValue Store item on Apify Platform

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'apify';

const store = await KeyValueStore.open();
await store.setValue('your-file', { foo: 'bar' });
const url = store.getPublicUrl('your-file');
// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file

----------------------------------------

TITLE: Request Transform Function
DESCRIPTION: Example of using transformRequestFunction to filter and modify requests before they are enqueued.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
    transformRequestFunction(req) {
        // ignore all links ending with `.pdf`
        if (req.url.endsWith('.pdf')) return false;
        return req;
    },
});

----------------------------------------

TITLE: Deploying an Actor to Apify Platform
DESCRIPTION: Command to deploy an actor project to the Apify platform using Apify CLI.

LANGUAGE: bash
CODE:
apify push

----------------------------------------

TITLE: Installing Node.js Type Declarations for Crawlee TypeScript Project
DESCRIPTION: This command installs type declarations for Node.js, enabling better type-checking in a Crawlee TypeScript project.

LANGUAGE: shell
CODE:
npm install --save-dev @types/node

----------------------------------------

TITLE: Logging in to Apify Platform using CLI
DESCRIPTION: Command to log in to the Apify Platform using the Apify CLI, requiring a personal access token.

LANGUAGE: bash
CODE:
apify login

----------------------------------------

TITLE: Getting Public URL for Stored Item
DESCRIPTION: Example of storing data in Key-Value Store and getting its public URL on Apify Platform

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'apify';

const store = await KeyValueStore.open();
await store.setValue('your-file', { foo: 'bar' });
const url = store.getPublicUrl('your-file');
// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file

----------------------------------------

TITLE: Failed Scraping with PuppeteerCrawler (No Wait)
DESCRIPTION: This snippet demonstrates an attempt to scrape dynamic content with PuppeteerCrawler without waiting, which results in an error as the elements are not yet rendered.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // This will fail because the content is not loaded yet
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Failed Scraping with PuppeteerCrawler (No Wait)
DESCRIPTION: This snippet demonstrates an attempt to scrape dynamic content with PuppeteerCrawler without waiting, which results in an error as the elements are not yet rendered.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // This will fail because the content is not loaded yet
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Browser Fingerprint Configuration
DESCRIPTION: Example showing how to configure browser fingerprints in PlaywrightCrawler.

LANGUAGE: typescript
CODE:
const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        useFingerprints: false,
    },
});

----------------------------------------

TITLE: Configuring TypeScript for Crawlee Project
DESCRIPTION: TypeScript configuration file (tsconfig.json) setup for a Crawlee project, extending @apify/tsconfig and setting compiler options for ES2022 modules.

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist"
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Installing Node.js Type Declarations
DESCRIPTION: Install type declarations for Node.js to enable type-checking for Node.js features.

LANGUAGE: shell
CODE:
npm install --save-dev @types/node

----------------------------------------

TITLE: Using actor-node-playwright-webkit Docker Image
DESCRIPTION: Example of using the Apify Docker image that includes Playwright and WebKit.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-webkit:20

----------------------------------------

TITLE: Using Dataset Map Method in Crawlee
DESCRIPTION: This snippet demonstrates how to use the Dataset map method to transform the dataset. It checks if there are more than 5 header elements on each page and returns an array of heading counts that meet this condition.

LANGUAGE: javascript
CODE:
import { Dataset, KeyValueStore } from 'crawlee';

const dataset = await Dataset.open();
const moreThan5headers = await dataset.map((item) => {
    return item.headingCount > 5 ? item.headingCount : null;
});

const kvStore = await KeyValueStore.open();
await kvStore.setValue('MORE_THAN_5', moreThan5headers);

----------------------------------------

TITLE: Sample Dataset Structure in JSON
DESCRIPTION: Example dataset showing scraped URLs and their heading counts stored in the project's default dataset directory.

LANGUAGE: json
CODE:
[
    {
        "url": "https://crawlee.dev/",
        "headingCount": 11
    },
    {
        "url": "https://crawlee.dev/storage",
        "headingCount": 8
    },
    {
        "url": "https://crawlee.dev/proxy",
        "headingCount": 4
    }
]

----------------------------------------

TITLE: Dockerfile for Playwright WebKit Crawlee Project
DESCRIPTION: Example Dockerfile for a Crawlee project using Playwright with WebKit browser.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-webkit:16

----------------------------------------

TITLE: Implementing Camoufox for Cloudflare Challenge
DESCRIPTION: Shows integration with Camoufox, a stealthy Firefox build, to handle Cloudflare challenges using the handleCloudflareChallenge helper.

LANGUAGE: typescript
CODE:
{PlaywrightCamoufox}

----------------------------------------

TITLE: Installing TypeScript Compiler
DESCRIPTION: Command to install TypeScript compiler as a development dependency

LANGUAGE: shell
CODE:
npm install --save-dev typescript

----------------------------------------

TITLE: Creating a Shared Request Queue with Locking Support in JavaScript
DESCRIPTION: Implements a function to initialize or retrieve a request queue that supports locking, essential for parallel scraping operations.

LANGUAGE: javascript
CODE:
import { RequestQueue } from 'crawlee';
import { Configuration } from 'crawlee';

// Enable the request locking experiment
Configuration.experimentRequestLocking = true;

let requestQueue;

/**
 * Gets or initializes the request queue.
 * @param {boolean} [purge=false] - Whether to purge the queue before initializing.
 * @returns {Promise<RequestQueue>} The initialized request queue.
 */
export async function getOrInitQueue(purge = false) {
    if (!requestQueue) {
        requestQueue = await RequestQueue.open('request-queue');
        if (purge) {
            await requestQueue.drop();
            requestQueue = await RequestQueue.open('request-queue');
        }
    }
    return requestQueue;
}

----------------------------------------

TITLE: Capturing Screenshot with Puppeteer's page.screenshot()
DESCRIPTION: This snippet demonstrates how to capture a screenshot of a web page using Puppeteer's page.screenshot() method. It launches a browser, creates a new page, navigates to a URL, and saves the screenshot to a key-value store.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';
import puppeteer from 'puppeteer';

const browser = await puppeteer.launch();
const page = await browser.newPage();
await page.goto('https://example.com');

const screenshotBuffer = await page.screenshot();

// Save screenshot to default key-value store
const kvStore = await KeyValueStore.open();
await kvStore.setValue('my-screenshot', screenshotBuffer, {
    contentType: 'image/png',
});

await browser.close();

----------------------------------------

TITLE: Modifying Crawlee Script for Apify Platform
DESCRIPTION: Example of how to modify a Crawlee crawler script to work with the Apify Platform by adding Actor initialization and exit calls.

LANGUAGE: typescript
CODE:
import { Actor } from 'apify';
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

await Actor.init();

log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

await Actor.exit();

----------------------------------------

TITLE: Comparing Cheerio and Browser JavaScript for DOM Manipulation
DESCRIPTION: This snippet demonstrates how to use Cheerio's $ function to manipulate the DOM, compared to plain JavaScript methods. It shows examples of selecting elements and extracting data.

LANGUAGE: typescript
CODE:
// Return the text content of the <title> element.
document.querySelector('title').textContent; // plain JS
$('title').text(); // Cheerio

// Return an array of all 'href' links on the page.
Array.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS
$('[href]')
    .map((i, el) => $(el).attr('href'))
    .get(); // Cheerio

----------------------------------------

TITLE: Getting Public URL for Stored Item
DESCRIPTION: Example of storing a value in KeyValueStore and getting its public URL on the Apify Platform.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'apify';

const store = await KeyValueStore.open();
await store.setValue('your-file', { foo: 'bar' });
const url = store.getPublicUrl('your-file');
// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file

----------------------------------------

TITLE: Session Pool Implementation with PlaywrightCrawler
DESCRIPTION: Example showing SessionPool integration with PlaywrightCrawler for browser automation

LANGUAGE: javascript
CODE:
{PlaywrightSource}

----------------------------------------

TITLE: Automating GitHub Repository Search with PuppeteerCrawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use PuppeteerCrawler to automate a GitHub repository search. It fills in search parameters, submits the form, extracts search results, and saves them to a dataset. The crawler handles pagination and uses a request queue for efficient crawling.

LANGUAGE: javascript
CODE:
import { Dataset, createPuppeteerRouter, PuppeteerCrawler } from 'crawlee';

const router = createPuppeteerRouter();

router.addDefaultHandler(async ({ page, crawler, request }) => {
    await page.type('#query-builder-test', 'crawler');
    await page.type('#owner', 'apify');
    await page.type('#created', '2022-06-01');
    await page.select('#language', 'JavaScript');

    await Promise.all([
        page.waitForNavigation(),
        page.click('form button[type="submit"]'),
    ]);

    const results = await page.evaluate(() => {
        const repositories = [];
        for (const result of document.querySelectorAll('.repo-list-item')) {
            repositories.push({
                name: result.querySelector('h3').innerText.trim(),
                url: result.querySelector('h3 a').href,
                description: result.querySelector('h3 + p')?.innerText?.trim(),
            });
        }
        return repositories;
    });

    await Dataset.pushData(results);

    const nextButton = await page.$('a.next_page');
    if (nextButton) {
        await crawler.addRequests([{ url: await nextButton.evaluate((el) => el.href) }]);
    }
});

const crawler = new PuppeteerCrawler({
    requestHandler: router,
    preNavigationHooks: [({ page }) => page.setExtraHTTPHeaders({ 'Accept-Language': 'en-US' })],
    // proxyConfiguration: new ProxyConfiguration({ proxyUrls: ['...'] }),
});

await crawler.run(['https://github.com/search']);


----------------------------------------

TITLE: Using sendRequest with Custom Header Generator Options
DESCRIPTION: Shows how to use sendRequest with custom header generator options. This example demonstrates setting specific options for generating browser-like headers to improve request authenticity.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            headerGeneratorOptions: {
                devices: ['mobile', 'desktop'],
                locales: ['en-US'],
                operatingSystems: ['windows', 'macos', 'android', 'ios'],
                browsers: ['chrome', 'edge', 'firefox', 'safari'],
            },
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Configuring Crawlee using global Configuration
DESCRIPTION: This snippet demonstrates how to use the global Configuration instance to set Crawlee options programmatically. It sets the persistStateIntervalMillis option and creates a CheerioCrawler.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Get the global configuration
const config = Configuration.getGlobalConfig();
// Set the 'persistStateIntervalMillis' option
// of global configuration to 10 seconds
config.set('persistStateIntervalMillis', 10_000);

// Note, that we are not passing the configuration to the crawler
// as it's using the global configuration
const crawler = new CheerioCrawler();

crawler.router.addDefaultHandler(async ({ request }) => {
    // For the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // For the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: TypeScript Configuration for Crawlee
DESCRIPTION: Sample TypeScript configuration file for Crawlee projects with recommended settings

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist",
        "lib": ["DOM"]
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Using Proxy with BasicCrawler and Got Scraping in TypeScript
DESCRIPTION: Demonstrates how to use a proxy server with BasicCrawler and Got Scraping by passing the proxyUrl option to sendRequest.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            proxyUrl: 'http://auto:password@proxy.apify.com:8000',
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Enabling Request Locking in CheerioCrawler
DESCRIPTION: Shows how to enable request locking experiment in a CheerioCrawler instance by setting the experiments.requestLocking flag to true.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    experiments: {
        requestLocking: true,
    },
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Getting Public URL for Key-Value Store Item
DESCRIPTION: Generate a public URL for an item stored in a Key-Value Store on the Apify Platform.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'apify';

const store = await KeyValueStore.open();
await store.setValue('your-file', { foo: 'bar' });
const url = store.getPublicUrl('your-file');
// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file

----------------------------------------

TITLE: Crawlee Request Handler Example
DESCRIPTION: Example showing new request handler patterns and context usage in Crawlee v3

LANGUAGE: typescript
CODE:
const crawler = new CheerioCrawler({
    async requestHandler({ log, request }) {
        log.info(`Opened ${request.loadedUrl}`);
    },
});

----------------------------------------

TITLE: Configuring Apify SDK with API Token
DESCRIPTION: JavaScript code to initialize the Apify SDK with an API token

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const sdk = new Actor({ token: 'your_api_token' });

----------------------------------------

TITLE: Multi-stage Dockerfile Build for Crawlee Actor
DESCRIPTION: A Dockerfile that creates a production-ready container for Crawlee actors using a multi-stage build process. The first stage builds the application, while the second stage creates a minimal production image. Includes optimizations for Docker layer caching and minimal image size by excluding development dependencies.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:20 AS builder

COPY package*.json ./

RUN npm install --include=dev --audit=false

COPY . ./

RUN npm run build

FROM apify/actor-node:20

COPY --from=builder /usr/src/app/dist ./dist

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm run start:prod --silent

----------------------------------------

TITLE: Creating a Request Queue with Locking Support
DESCRIPTION: Shows how to create and initialize a RequestQueueV2 instance that supports request locking. The queue can be used to store and manage crawling requests with locking capabilities.

LANGUAGE: typescript
CODE:
import { RequestQueueV2 } from 'crawlee';

const queue = await RequestQueueV2.open('my-locking-queue');
await queue.addRequests([
    { url: 'https://crawlee.dev' },
    { url: 'https://crawlee.dev/js/docs' },
    { url: 'https://crawlee.dev/js/api' },
]);

----------------------------------------

TITLE: Running Headful Browser with PuppeteerCrawler
DESCRIPTION: Example of using PuppeteerCrawler with a visible browser window for development purposes.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    // ... other options
    headless: false,
    requestHandlerTimeoutSecs: 300,
    navigationTimeoutSecs: 300,
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Comparing Cheerio and Browser JavaScript for DOM Manipulation
DESCRIPTION: This snippet demonstrates how to perform common DOM operations using both Cheerio and plain browser JavaScript, highlighting the similarities and differences in syntax.

LANGUAGE: typescript
CODE:
// Return the text content of the <title> element.
document.querySelector('title').textContent; // plain JS
$('title').text(); // Cheerio

// Return an array of all 'href' links on the page.
Array.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS
$('[href]')
    .map((i, el) => $(el).attr('href'))
    .get(); // Cheerio

----------------------------------------

TITLE: Browser Pool Configuration with Lifecycle Hooks
DESCRIPTION: Example of configuring browser pool options and lifecycle hooks in PuppeteerCrawler.

LANGUAGE: javascript
CODE:
const crawler = new Apify.PuppeteerCrawler({
    browserPoolOptions: {
        retireBrowserAfterPageCount: 10,
        preLaunchHooks: [
            async (pageId, launchContext) => {
                const { request } = crawler.crawlingContexts.get(pageId);
                if (request.userData.useHeadful === true) {
                    launchContext.launchOptions.headless = false;
                }
            }
        ]
    }
})

----------------------------------------

TITLE: Package.json Development Scripts
DESCRIPTION: Configuration for development environment with ts-node-esm

LANGUAGE: json
CODE:
{
    "scripts": {
        "start:dev": "ts-node-esm -T src/main.ts"
    }
}

----------------------------------------

TITLE: Installing TypeScript Compiler for Crawlee Project
DESCRIPTION: Command to install TypeScript compiler as a development dependency in a Crawlee project.

LANGUAGE: shell
CODE:
npm install --save-dev typescript

----------------------------------------

TITLE: Package.json Build Configuration
DESCRIPTION: Basic package.json configuration for TypeScript build script and main entry point

LANGUAGE: json
CODE:
{
    "scripts": {
        "build": "tsc"
    },
    "main": "dist/main.js"
}

----------------------------------------

TITLE: Checking Product Stock Availability with Playwright in JavaScript
DESCRIPTION: This snippet demonstrates how to use Playwright to check if a product is in stock by searching for a specific element on the webpage.

LANGUAGE: javascript
CODE:
const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Finding All Links on a Page with JSDOM
DESCRIPTION: Shows how to extract all href attributes from anchor tags using querySelector and Array manipulation.

LANGUAGE: javascript
CODE:
Array.from(document.querySelectorAll('a[href]')).map((a) => a.href);

----------------------------------------

TITLE: Crawling Multiple URLs with Playwright Crawler in JavaScript
DESCRIPTION: This code snippet illustrates how to use Playwright Crawler to crawl multiple specified URLs. It captures screenshots of each page and stores the page title and URL.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url} is '${title}'`);

        await Dataset.pushData({
            title,
            url: request.url,
            screenshot: await page.screenshot(),
        });
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run([
    'https://crawlee.dev',
    'https://crawlee.dev/docs/quick-start',
    'https://crawlee.dev/docs/introduction/first-steps',
]);

----------------------------------------

TITLE: Adapting Category Route Handler for Parallel Scraping in JavaScript
DESCRIPTION: Modifies the CATEGORY route handler to enqueue product URLs to the shared request queue instead of immediately scraping them. This prepares the URLs for parallel processing.

LANGUAGE: javascript
CODE:
import { getOrInitQueue } from './requestQueue.mjs';

// ...

router.addHandler('CATEGORY', async ({ $, log, crawler }) => {
    const requestQueue = await getOrInitQueue();

    const products = $('a.product_pod');
    log.info(`Found ${products.length} products.`);

    const productLinks = [];
    for (const product of products) {
        const productUrl = new URL($(product).attr('href'), request.url).href;
        productLinks.push({
            url: productUrl,
            userData: {
                label: 'DETAIL',
            },
        });
    }

    await Promise.all(productLinks.map(link => requestQueue.addRequest(link)));

    // Handle pagination
    const nextPageUrl = $('li.next a').attr('href');
    if (nextPageUrl) {
        await crawler.addRequests([{
            url: new URL(nextPageUrl, request.url).href,
            userData: {
                label: 'CATEGORY',
            },
        }]);
    }
});

----------------------------------------

TITLE: Crawlee Docker Build Configuration
DESCRIPTION: Multi-stage Dockerfile setup for building and running Crawlee projects with proper dependency management.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16 AS builder

COPY . ./
RUN npm install --include=dev \
    && npm run build

FROM apify/actor-node:16
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/README.md ./
COPY --from=builder /usr/src/app/dist ./dist
COPY --from=builder /usr/src/app/apify.json ./apify.json
COPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json

RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional \
    && echo "Installed NPM packages:" \
    && (npm list --only=prod --no-optional --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

CMD npm run start:prod

----------------------------------------

TITLE: Accessing Page Title with JSDOM in JavaScript
DESCRIPTION: Demonstrates how to retrieve the page title using JSDOM, comparing it with browser JavaScript. This snippet shows the difference in syntax between browser and JSDOM environments.

LANGUAGE: javascript
CODE:
// Return the page title
document.title; // browsers
window.document.title; // JSDOM

----------------------------------------

TITLE: Installing and Packaging Browser Dependencies for AWS Lambda
DESCRIPTION: Commands to install the @sparticuz/chromium package and create a zip archive of node_modules for AWS Lambda Layer deployment.

LANGUAGE: bash
CODE:
# Install the package
npm i -S @sparticuz/chromium

# Zip the dependencies
zip -r dependencies.zip ./node_modules

----------------------------------------

TITLE: Crawling All Links with CheerioCrawler - Crawlee
DESCRIPTION: Demonstrates how to crawl all links found on a webpage regardless of their domain using the 'all' enqueue strategy. This approach will process any URLs discovered during crawling, even if they point to external websites.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks, log }) {
        log.info('Enqueueing all links.');
        await enqueueLinks({
            strategy: EnqueueStrategy.All,
        });
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Crawling Sitemaps with Cheerio Crawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use Cheerio Crawler to crawl sitemaps. It initializes the crawler, sets up the request handler, and uses the Sitemap utility to process sitemap URLs.

LANGUAGE: javascript
CODE:
import { CheerioScraper } from 'crawlee';
import { Sitemap } from '@crawlee/utils';

const crawler = new CheerioScraper({
    // By default, the crawler only supports limited number of
    // concurrently processed requests. To download all URLs from the
    // sitemap, we need to configure higher limit.
    maxRequestsPerCrawl: 500,
});

crawler.router.addHandler('START', async ({ enqueueLinks }) => {
    const sitemap = await Sitemap.load({ url: 'https://crawlee.dev/sitemap.xml' });
    await enqueueLinks({ urls: sitemap.urls });
});

crawler.router.addDefaultHandler(async ({ log, request, $ }) => {
    const title = $('title').text();
    log.info(`Title of ${request.loadedUrl} is '${title}'`);
});

await crawler.run(['START']);

----------------------------------------

TITLE: Scraping Hacker News with PlaywrightCrawler in JavaScript
DESCRIPTION: This code demonstrates how to use PlaywrightCrawler to scrape Hacker News. It starts with the main page, extracts article details, and follows pagination links. The crawler uses a RequestQueue to manage URLs and stores results in a default dataset.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

// Create an instance of the PlaywrightCrawler class - a crawler
// that automatically loads the URLs in headless Chrome / Playwright.
const crawler = new PlaywrightCrawler({
    // Use the requestHandler to process each of the crawled pages.
    async requestHandler({ request, page, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        // Extract data from the page using Playwright API.
        const data = await page.evaluate(() => {
            const results = [];
            document.querySelectorAll('.athing').forEach((el) => {
                results.push({
                    title: el.querySelector('.title a').innerText,
                    rank: el.querySelector('.rank').innerText,
                    href: el.querySelector('.title a').href,
                });
            });
            return results;
        });

        // Save the data to dataset.
        await Dataset.pushData(data);

        // Extract links from the current page
        // and add them to the crawling queue.
        await enqueueLinks({
            selector: '.morelink',
        });
    },
    // Uncomment this option to see the browser window.
    // headless: false,
});

// Add first URL to the queue and start the crawl.
await crawler.run(['https://news.ycombinator.com/']);

----------------------------------------

TITLE: Extracting Manufacturer from URL in JavaScript
DESCRIPTION: Demonstrates how to extract the manufacturer name from a product URL by splitting the URL string.

LANGUAGE: javascript
CODE:
const urlPart = request.url.split('/').slice(-1);
const manufacturer = urlPart[0].split('-')[0];

----------------------------------------

TITLE: Configuring TypeScript Compiler Options
DESCRIPTION: Set up tsconfig.json with compiler options for Crawlee project, including ES2022 module and target.

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist"
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Installing and Packaging Browser Dependencies for AWS Lambda
DESCRIPTION: Commands for installing the @sparticuz/chromium package and creating a zip archive of node_modules for AWS Lambda Layer deployment.

LANGUAGE: bash
CODE:
# Install the package
npm i -S @sparticuz/chromium

# Zip the dependencies
zip -r dependencies.zip ./node_modules

----------------------------------------

TITLE: Configuring Crawlee using crawlee.json
DESCRIPTION: Example of basic Crawlee configuration using a JSON file. Shows how to set persistStateIntervalMillis and logLevel options.

LANGUAGE: json
CODE:
{
  "persistStateIntervalMillis": 10000,
  "logLevel": "DEBUG"
}

----------------------------------------

TITLE: Enabling System Information V2 Experiment in CheerioCrawler
DESCRIPTION: This snippet demonstrates how to enable the System Information V2 experiment in a CheerioCrawler instance. It sets the 'systemInfoV2' configuration option to true and creates a basic crawler that logs the title of each crawled page.

LANGUAGE: typescript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';

Configuration.set('systemInfoV2', true);

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Configuring Apify Token in JavaScript
DESCRIPTION: Setting up an Apify SDK instance with an API token in JavaScript.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const sdk = new Actor({ token: 'your_api_token' });

----------------------------------------

TITLE: Installing and Packaging Browser Dependencies for AWS Lambda
DESCRIPTION: Commands for installing the @sparticuz/chromium package and creating a zip archive of node_modules for AWS Lambda Layer deployment.

LANGUAGE: bash
CODE:
# Install the package
npm i -S @sparticuz/chromium

# Zip the dependencies
zip -r dependencies.zip ./node_modules

----------------------------------------

TITLE: Configuring Dockerfile for Crawlee Project with Node.js
DESCRIPTION: This Dockerfile sets up a Docker image for a Crawlee project. It uses the apify/actor-node:16 base image, copies package files, installs NPM packages, copies the source code, and specifies the command to run the project.

LANGUAGE: Dockerfile
CODE:
FROM apify/actor-node:16

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm start --silent

----------------------------------------

TITLE: Saving Data with Dataset.pushData() in Crawlee (JavaScript)
DESCRIPTION: This code demonstrates how to use Dataset.pushData() to save extracted data. It replaces a console.log() call with the data saving function.

LANGUAGE: javascript
CODE:
await Dataset.pushData(results);

----------------------------------------

TITLE: Using Pre-release Docker Image
DESCRIPTION: Examples of how to use pre-release versions of Apify Docker images, with and without specifying an automation library version.

LANGUAGE: dockerfile
CODE:
# Without library version.
FROM apify/actor-node:20-beta

LANGUAGE: dockerfile
CODE:
# With library version.
FROM apify/actor-node-playwright-chrome:20-1.10.0-beta

----------------------------------------

TITLE: Transforming Requests in Crawlee's enqueueLinks
DESCRIPTION: Shows how to use the transformRequestFunction to modify or skip requests before they are enqueued in Crawlee.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
    transformRequestFunction(req) {
        // ignore all links ending with `.pdf`
        if (req.url.endsWith('.pdf')) return false;
        return req;
    },
});

----------------------------------------

TITLE: Cross-Context Access with Crawling Context IDs
DESCRIPTION: Example showing how to use crawling context IDs to maintain references between different page handlers

LANGUAGE: javascript
CODE:
let masterContextId;
const handlePageFunction = async ({ id, page, request, crawler }) => {
    if (request.userData.masterPage) {
        masterContextId = id;
        // Prepare the master page.
    } else {
        const masterContext = crawler.crawlingContexts.get(masterContextId);
        const masterPage = masterContext.page;
        const masterRequest = masterContext.request;
        // Now we can manipulate the master data from another handlePageFunction.
    }
}

----------------------------------------

TITLE: Accessing Public URL of Stored Item
DESCRIPTION: Example of storing a value in a Key-Value Store and retrieving its public URL for sharing.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'apify';

const store = await KeyValueStore.open();
await store.setValue('your-file', { foo: 'bar' });
const url = store.getPublicUrl('your-file');
// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file

----------------------------------------

TITLE: Docker Multistage Build Configuration
DESCRIPTION: Dockerfile showing multistage build setup for TypeScript Crawlee projects

LANGUAGE: dockerfile
CODE:
# using multistage build, as we need dev deps to build the TS source code
FROM apify/actor-node:16 AS builder

# copy all files, install all dependencies (including dev deps) and build the project
COPY . ./
RUN npm install --include=dev \
    && npm run build

# create final image
FROM apify/actor-node:16
# copy only necessary files
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/README.md ./
COPY --from=builder /usr/src/app/dist ./dist
COPY --from=builder /usr/src/app/apify.json ./apify.json
COPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json

# install only prod deps
RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional \
    && echo "Installed NPM packages:" \
    && (npm list --only=prod --no-optional --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

# run compiled code
CMD npm run start:prod

----------------------------------------

TITLE: Docker Multi-stage Build Configuration
DESCRIPTION: Dockerfile configuration using multi-stage build for optimized production deployment

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:20 AS builder

COPY . ./
RUN npm install --include=dev \
    && npm run build

FROM apify/actor-node:20
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/dist ./dist

RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional

CMD npm run start:prod

----------------------------------------

TITLE: Production Runtime Configuration
DESCRIPTION: Package.json script for running compiled code in production

LANGUAGE: json
CODE:
{
    "scripts": {
        "start:prod": "node dist/main.js"
    }
}

----------------------------------------

TITLE: Cookie Jar Implementation
DESCRIPTION: Example of using a custom cookie jar with tough-cookie for managing cookies in requests.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';
import { CookieJar } from 'tough-cookie';

const cookieJar = new CookieJar();

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ cookieJar });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Initializing Cheerio Crawler with Custom Configuration
DESCRIPTION: Sets up a basic Cheerio crawler with custom configuration to prevent storage persistence on Lambda's read-only filesystem.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Crawlee Docker Configuration
DESCRIPTION: Multi-stage Dockerfile setup for building and running Crawlee projects.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:20 AS builder

COPY . ./
RUN npm install --include=dev \
    && npm run build

FROM apify/actor-node:20
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/README.md ./
COPY --from=builder /usr/src/app/dist ./dist
COPY --from=builder /usr/src/app/apify.json ./apify.json
COPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json

RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional \
    && echo "Installed NPM packages:" \
    && (npm list --only=prod --no-optional --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

CMD npm run start:prod

----------------------------------------

TITLE: Standalone SessionPool Usage in Crawlee
DESCRIPTION: This example demonstrates standalone usage of SessionPool in Crawlee for managing sessions manually. It includes creating a session pool, adding sessions, and using them for requests.

LANGUAGE: js
CODE:
import { SessionPool } from 'crawlee';

const sessionPool = new SessionPool({
    maxPoolSize: 100,
});

// Create a session
const session1 = await sessionPool.getSession();
session1.userData.foo = 'bar';

// Session 1 becomes available again
await sessionPool.putSession(session1);

// Get session 1 or 2 (random)
const session = await sessionPool.getSession();

// Do something with the session...

// Destroy session
await sessionPool.teardown();

----------------------------------------

TITLE: Crawling All Links with CheerioCrawler in Crawlee
DESCRIPTION: This snippet demonstrates how to use the CheerioCrawler to crawl all links found on a website, regardless of their domain. It uses the 'all' enqueue strategy.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks }) {
        // Add all links from page to the queue,
        // regardless of their domain
        await enqueueLinks({
            strategy: EnqueueStrategy.All,
            // or the string "all"
        });
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Crawling Specific Links with CheerioCrawler in JavaScript
DESCRIPTION: This code snippet shows how to set up a CheerioCrawler to crawl specific links on a website. It uses the globs property in the enqueueLinks method to only add links to the RequestQueue if they match the specified pattern. The crawler processes each page, extracts the title, and logs it along with the URL.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);

        await Dataset.pushData({
            url: request.url,
            title,
        });

        // Extract links from the current page
        // and add them to the crawling queue.
        await enqueueLinks({
            globs: ['https://crawlee.dev/**'],
            label: 'detail',
        });
    },
    // Let's limit our crawls to make our examples run faster
    maxRequestsPerCrawl: 20,
});

// Add first URL to the queue and start the crawl.
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Purging Default Storages in Crawlee
DESCRIPTION: This snippet demonstrates how to explicitly purge default storages in Crawlee using the purgeDefaultStorages() helper function.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: PuppeteerCrawler with Utils Screenshot
DESCRIPTION: Demonstrates using the context-aware saveSnapshot() utility within PuppeteerCrawler for capturing screenshots across multiple pages.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ puppeteerUtils }) {
        await puppeteerUtils.saveSnapshot();
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Crawling Context Updates in Handler Functions
DESCRIPTION: Example showing the evolution from separate argument objects to a unified crawling context in handler functions.

LANGUAGE: javascript
CODE:
const handlePageFunction = async (crawlingContext1) => {
    crawlingContext1.hasOwnProperty('proxyInfo') // true
}

const handleFailedRequestFunction = async (crawlingContext2) => {
    crawlingContext2.hasOwnProperty('proxyInfo') // true
}

// All contexts are the same object.
crawlingContext1 === crawlingContext2 // true

----------------------------------------

TITLE: Implementing Sitemap Crawling with Playwright Crawler
DESCRIPTION: Example showing Playwright Crawler implementation for processing sitemap URLs with modern browser automation capabilities. This is ideal for complex web applications requiring modern browser features.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, PlaywrightRootAPIProvider } from 'crawlee';
import { Sitemap } from '@crawlee/utils';

const crawler = new PlaywrightCrawler({
    async requestHandler({ log }) {
        log.info('Got a page!');
    },
});

const sitemap = new Sitemap({
    provider: new PlaywrightRootAPIProvider(),
});

await sitemap.downloadSitemaps(['https://example.com/sitemap.xml']);
const urls = await sitemap.getAllURLs();

await crawler.run(urls);

----------------------------------------

TITLE: Scraping Product Title with Playwright in JavaScript
DESCRIPTION: This code uses Playwright to locate and extract the product title from a webpage using a CSS selector.

LANGUAGE: javascript
CODE:
const title = await page.locator('.product-meta h1').textContent();

----------------------------------------

TITLE: Configuring Package.json for TypeScript Build in Crawlee
DESCRIPTION: JSON configuration for package.json to set up TypeScript build script and main entry point for a Crawlee project.

LANGUAGE: json
CODE:
{
    "scripts": {
        "build": "tsc"
    },
    "main": "dist/main.js"
}

----------------------------------------

TITLE: Demonstrating Dataset Operations in Crawlee
DESCRIPTION: This snippet illustrates basic operations of datasets in Crawlee, including writing single and multiple rows to both default and named datasets.

LANGUAGE: javascript
CODE:
import { Dataset } from 'crawlee';

// Write a single row to the default dataset
await Dataset.pushData({ col1: 123, col2: 'val2' });

// Open a named dataset
const dataset = await Dataset.open('some-name');

// Write a single row
await dataset.pushData({ foo: 'bar' });

// Write multiple rows
await dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);

----------------------------------------

TITLE: Skipping Navigation for Image Requests with PlaywrightCrawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use PlaywrightCrawler to crawl a website, skip navigation for image requests, and save them directly to a key-value store. It utilizes the Request#skipNavigation option and the sendRequest method to efficiently handle CDN-delivered images.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ request, sendRequest, log }) {
        log.info(`Processing ${request.url}`);

        if (request.skipNavigation) {
            log.info('Skipping navigation and saving image directly');
            const imageBuffer = await sendRequest({ url: request.url });
            await Dataset.pushData({
                url: request.url,
                image: imageBuffer.toString('base64'),
            });
            return;
        }

        // Regular crawling logic here
    },
});

await crawler.run(['https://example.com']);

// Enqueue an image to be processed
await crawler.addRequests([{
    url: 'https://example.com/image.jpg',
    skipNavigation: true,
}]);

----------------------------------------

TITLE: Capturing Screenshot with Puppeteer using page.screenshot()
DESCRIPTION: This snippet demonstrates how to capture a screenshot of a web page using Puppeteer's page.screenshot() method. It launches a browser, creates a new page, navigates to a URL, takes a screenshot, and saves it to a key-value store.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';
import { launchPuppeteer } from 'crawlee';

await Actor.init();

const browser = await launchPuppeteer();
const page = await browser.newPage();

const url = 'https://crawlee.dev';
await page.goto(url);

const screenshot = await page.screenshot();

const key = `${new URL(url).hostname}.png`;
await Actor.setValue(key, screenshot, { contentType: 'image/png' });

await browser.close();
await Actor.exit();

----------------------------------------

TITLE: Logging into Apify CLI
DESCRIPTION: Commands to install Apify CLI and log in with an API token.

LANGUAGE: bash
CODE:
npm install -g apify-cli
apify login -t YOUR_API_TOKEN

----------------------------------------

TITLE: Development Script Configuration
DESCRIPTION: Package.json script for running TypeScript code in development mode using ts-node

LANGUAGE: json
CODE:
{
    "scripts": {
        "start:dev": "ts-node-esm -T src/main.ts"
    }
}

----------------------------------------

TITLE: TypeScript Configuration
DESCRIPTION: TSConfig setup extending @apify/tsconfig with ES2022 module and target settings

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist"
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Initializing CheerioCrawler with Custom Configuration for AWS Lambda
DESCRIPTION: This snippet shows how to create a CheerioCrawler instance with a custom Configuration object to use in-memory storage for AWS Lambda compatibility.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Crawlee Scraper Log Output
DESCRIPTION: Example log output from a running Crawlee scraper, showing crawled URLs and page titles.

LANGUAGE: log
CODE:
INFO  PlaywrightCrawler: Starting the crawl
INFO  PlaywrightCrawler: Title of https://crawlee.dev/ is 'Crawlee · Build reliable crawlers. Fast. | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core is '@crawlee/core | API | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core/changelog is 'Changelog | API | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'

----------------------------------------

TITLE: Installing and Logging into Apify CLI
DESCRIPTION: Commands to install the Apify CLI globally and log in using an API token.

LANGUAGE: bash
CODE:
npm install -g apify-cli
apify login -t YOUR_API_TOKEN

----------------------------------------

TITLE: JSON Response Handling
DESCRIPTION: Shows how to configure sendRequest to handle JSON responses instead of default text responses.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ responseType: 'json' });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Request List Implementation
DESCRIPTION: Shows how to initialize and use a Request List for processing a known set of URLs. The Request List is immutable after creation and optimized for handling large numbers of initial URLs.

LANGUAGE: javascript
CODE:
import { RequestList, PuppeteerCrawler } from 'crawlee';

// Prepare the sources array with URLs to visit
const sources = [
    { url: 'http://www.example.com/page-1' },
    { url: 'http://www.example.com/page-2' },
    { url: 'http://www.example.com/page-3' },
];

// Open the request list.
// List name is used to persist the sources and the list state in the key-value store
const requestList = await RequestList.open('my-list', sources);

// The crawler will automatically process requests from the list
// It's used the same way for Cheerio /Playwright crawlers.
const crawler = new PuppeteerCrawler({
    requestList,
    async requestHandler({ page, request }) {
        // Process the page (extract data, take page screenshot, etc).
        // No more requests could be added to the request list here
    },
});

----------------------------------------

TITLE: Package.json Build Configuration
DESCRIPTION: Basic package.json configuration for TypeScript build process and main entry point

LANGUAGE: json
CODE:
{
    "scripts": {
        "build": "tsc"
    },
    "main": "dist/main.js"
}

----------------------------------------

TITLE: Getting Public URL for Key-Value Store Item
DESCRIPTION: JavaScript code to store an item in a Key-Value Store and get its public URL.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'apify';

const store = await KeyValueStore.open();
await store.setValue('your-file', { foo: 'bar' });
const url = store.getPublicUrl('your-file');
// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file

----------------------------------------

TITLE: TypeScript Configuration for Crawlee
DESCRIPTION: TypeScript configuration file setup for Crawlee projects using @apify/tsconfig.

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist",
        "lib": ["DOM"]
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Dockerfile Best Practice for Apify Images
DESCRIPTION: Example of best practice for specifying Apify Docker image in a Dockerfile, using Node.js version tag.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16

----------------------------------------

TITLE: Category Page Crawling with Playwright and Crawlee
DESCRIPTION: Implementation of a crawler that specifically targets category pages using PlaywrightCrawler. Uses selectors to find category links and labels requests for identification.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);

        // Only run this logic on the main category listing, not on sub-pages.
        if (request.label !== 'CATEGORY') {

          // Wait for the category cards to render,
          // otherwise enqueueLinks wouldn't enqueue anything.
          await page.waitForSelector('.collection-block-item');

          // Add links to the queue, but only from
          // elements matching the provided selector.
          await enqueueLinks({
              selector: '.collection-block-item',
              label: 'CATEGORY',
          });
        }
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Storage Cleanup in Crawlee
DESCRIPTION: Demonstrates how to clean up default storages in Crawlee using the purgeDefaultStorages helper function.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Cookie Context Manager Implementation
DESCRIPTION: Async context manager for handling cookie dialogs across multiple pages

LANGUAGE: python
CODE:
from playwright.async_api import TimeoutError as PlaywrightTimeoutError

@asynccontextmanager
async def accept_cookies(page: Page):
    task = asyncio.create_task(page.get_by_test_id('dialog-accept-button').click())
    try:
        yield
    finally:
        if not task.done():
            task.cancel()

        with suppress(asyncio.CancelledError, PlaywrightTimeoutError):
            await task

----------------------------------------

TITLE: Cross-Context Access Using Context IDs
DESCRIPTION: Example showing how to maintain state across different crawling contexts using the new context ID system.

LANGUAGE: javascript
CODE:
let masterContextId;
const handlePageFunction = async ({ id, page, request, crawler }) => {
    if (request.userData.masterPage) {
        masterContextId = id;
        // Prepare the master page.
    } else {
        const masterContext = crawler.crawlingContexts.get(masterContextId);
        const masterPage = masterContext.page;
        const masterRequest = masterContext.request;
        // Now we can manipulate the master data from another handlePageFunction.
    }
}

----------------------------------------

TITLE: Crawling All Links with Puppeteer Crawler in Crawlee
DESCRIPTION: This code snippet shows how to use Puppeteer Crawler in Crawlee to crawl all links on a website. It sets up a PuppeteerCrawler, uses the enqueueLinks() method to add new links to the queue, and demonstrates how to extract the page title.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, enqueueLinks } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limitation for only 10 requests (do not use if you want to crawl all links)
});

// Run the crawler
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Configuring Apify Proxy
DESCRIPTION: Examples of setting up and using Apify Proxy with specific configurations

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\nconst proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Customizing Browser Fingerprints with PuppeteerCrawler
DESCRIPTION: This snippet shows how to customize browser fingerprints using PuppeteerCrawler in Crawlee. It demonstrates setting specific browser and operating system parameters for fingerprint generation.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    // ...
    browserPoolOptions: {
        useFingerprints: true, // this is the default
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                browsers: [
                    { name: 'firefox', minVersion: 88 },
                    { name: 'chrome', minVersion: 90 },
                ],
                operatingSystems: ['windows'],
            },
        },
    },
});

await crawler.run();

----------------------------------------

TITLE: Sanity Check with PlaywrightCrawler in Crawlee
DESCRIPTION: This code snippet demonstrates a basic sanity check using PlaywrightCrawler to verify the setup and print the text content of category elements on the start page.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page }) => {
        const categories = await page.$$('.collection-block-item');
        for (const category of categories) {
            const categoryText = await category.textContent();
            console.log(categoryText);
        }
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Saving Data to Default Dataset in Crawlee
DESCRIPTION: This code snippet demonstrates how to save scraped data to the default dataset using Crawlee. It shows the process of creating a CheerioCrawler, defining the requestHandler, and pushing data to the dataset.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks, log }) {
        const title = $('title').text();
        log.info(`Title of ${request.url} is '${title}'`);

        await Dataset.pushData({
            url: request.url,
            title,
        });

        await enqueueLinks();
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Cookie Dialog Handler Implementation
DESCRIPTION: Router implementation for handling cookie acceptance dialog

LANGUAGE: python
CODE:
from crawlee.router import Router
from crawlee.playwright_crawler import PlaywrightCrawlingContext

router = Router[PlaywrightCrawlingContext]()

@router.default_handler
async def default_handler(context: PlaywrightCrawlingContext) -> None:
    """Default request handler."""
    await context.page.get_by_test_id('dialog-accept-button').click()

----------------------------------------

TITLE: Installing Crawlee Package
DESCRIPTION: Examples of installing Crawlee packages and their dependencies for different use cases.

LANGUAGE: bash
CODE:
npm install crawlee

# For just cheerio support
npm install @crawlee/cheerio

# For playwright
npm install crawlee playwright
# or npm install @crawlee/playwright playwright

----------------------------------------

TITLE: Finding All Links on a Page using Cheerio
DESCRIPTION: This code snippet uses Cheerio to find all <a> elements with an href attribute on a page and extract the href values into an array.

LANGUAGE: javascript
CODE:
$('a[href]')
    .map((i, el) => $(el).attr('href'))
    .get();

----------------------------------------

TITLE: Installing Node.js Type Declarations
DESCRIPTION: Command to install type declarations for Node.js as a development dependency.

LANGUAGE: shell
CODE:
npm install --save-dev @types/node

----------------------------------------

TITLE: Installing TypeScript Compiler for Crawlee Project
DESCRIPTION: Command to install TypeScript compiler as a development dependency in a Crawlee project.

LANGUAGE: shell
CODE:
npm install --save-dev typescript

----------------------------------------

TITLE: Element Not Found Error Log
DESCRIPTION: Shows the error message when attempting to access elements before they're rendered in the DOM.

LANGUAGE: log
CODE:
ERROR [...] Error: failed to find element matching selector ".ActorStoreItem"

----------------------------------------

TITLE: Installing Crawlee Manually for Different Crawler Types
DESCRIPTION: npm commands to manually install Crawlee for different crawler types: CheerioCrawler, PlaywrightCrawler, and PuppeteerCrawler.

LANGUAGE: bash
CODE:
npm install crawlee

LANGUAGE: bash
CODE:
npm install crawlee playwright

LANGUAGE: bash
CODE:
npm install crawlee puppeteer

----------------------------------------

TITLE: Creating New Crawlee Project
DESCRIPTION: Command to create a new Crawlee project using the CLI tool

LANGUAGE: bash
CODE:
npx crawlee create my-crawler

----------------------------------------

TITLE: Modifying Crawlee Script for Apify Platform
DESCRIPTION: Example of how to modify a Crawlee script to work with Apify Platform by adding Actor initialization and exit calls.

LANGUAGE: typescript
CODE:
import { Actor } from 'apify';
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

await Actor.init();

log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

await Actor.exit();

----------------------------------------

TITLE: Adding AWS Lambda Handler Function
DESCRIPTION: Wrapping the crawler logic in an AWS Lambda handler function to make it executable in the Lambda environment.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
};

----------------------------------------

TITLE: Configuring Crawlee with crawlee.json
DESCRIPTION: Example of a crawlee.json file used to set global configuration options for Crawlee. This file should be placed in the root of the project.

LANGUAGE: json
CODE:
{
  "persistStateIntervalMillis": 10000,
  "logLevel": "DEBUG"
}

----------------------------------------

TITLE: Crawling Links with Puppeteer Crawler
DESCRIPTION: Implementation showing how to crawl website links using Puppeteer Crawler. Requires apify/actor-node-puppeteer-chrome image when running on Apify Platform. Suitable for JavaScript-rendered pages.

LANGUAGE: javascript
CODE:
PuppeteerSource

----------------------------------------

TITLE: Reduce Method Result Example
DESCRIPTION: Example output of the Dataset reduce method showing the total count of headers across all pages.

LANGUAGE: javascript
CODE:
23

----------------------------------------

TITLE: Element Not Found Error Log
DESCRIPTION: Shows the error message when attempting to access elements before they're rendered in the DOM.

LANGUAGE: log
CODE:
ERROR [...] Error: failed to find element matching selector ".ActorStoreItem"

----------------------------------------

TITLE: Installing TypeScript Compiler in Node.js Project
DESCRIPTION: Command to install TypeScript as a development dependency in a Node.js project.

LANGUAGE: shell
CODE:
npm install --save-dev typescript

----------------------------------------

TITLE: Setting Apify Token in Configuration
DESCRIPTION: Example of setting the Apify API token using the Configuration instance. This allows access to Apify platform features within the script.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const sdk = new Actor({ token: 'your_api_token' });

----------------------------------------

TITLE: Browser Pool Configuration
DESCRIPTION: Example of configuring the new BrowserPool with lifecycle hooks and custom browser management.

LANGUAGE: javascript
CODE:
const crawler = new Apify.PuppeteerCrawler({
    browserPoolOptions: {
        retireBrowserAfterPageCount: 10,
        preLaunchHooks: [
            async (pageId, launchContext) => {
                const { request } = crawler.crawlingContexts.get(pageId);
                if (request.userData.useHeadful === true) {
                    launchContext.launchOptions.headless = false;
                }
            }
        ]
    }
})

----------------------------------------

TITLE: Using actor-node-playwright-firefox Docker Image
DESCRIPTION: Example of how to use the Apify Docker image that includes Playwright and Firefox.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-firefox:16

----------------------------------------

TITLE: Example Dataset Output
DESCRIPTION: Sample JSON output format showing the structure of scraped data stored by Crawlee.

LANGUAGE: json
CODE:
{
    "url": "https://crawlee.dev/",
    "title": "Crawlee · The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee"
}

----------------------------------------

TITLE: Creating New Crawlee Project
DESCRIPTION: Command to create a new Crawlee project using the CLI tool

LANGUAGE: bash
CODE:
npx crawlee create my-crawler

----------------------------------------

TITLE: Configuring Loopback Addresses for macOS in Crawlee Project
DESCRIPTION: Commands to set up additional loopback addresses in macOS for proxy tests. These need to be run once per system startup.

LANGUAGE: shell
CODE:
sudo ifconfig lo0 alias 127.0.0.2 up
sudo ifconfig lo0 alias 127.0.0.3 up
sudo ifconfig lo0 alias 127.0.0.4 up

----------------------------------------

TITLE: Crawling Context Implementation
DESCRIPTION: Demonstrates the new unified crawling context object that provides consistent access to crawler properties across handler functions.

LANGUAGE: javascript
CODE:
const handlePageFunction = async (crawlingContext1) => {
    crawlingContext1.hasOwnProperty('proxyInfo') // true
}

const handleFailedRequestFunction = async (crawlingContext2) => {
    crawlingContext2.hasOwnProperty('proxyInfo') // true
}

// All contexts are the same object.
crawlingContext1 === crawlingContext2 // true

----------------------------------------

TITLE: Exporting Dataset to CSV with Crawlee
DESCRIPTION: Demonstrates how to export an entire dataset to a single CSV file using the Dataset.exportToValue function. The exported data is stored in the default key-value store for later access.

LANGUAGE: javascript
CODE:
<RunnableCodeBlock className="language-js" type="cheerio">
	{CrawlSource}
</RunnableCodeBlock>

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Custom Configuration in JavaScript
DESCRIPTION: This snippet demonstrates how to create a PlaywrightCrawler instance with a custom Configuration object that disables persistent storage. It sets up the crawler with a router for request handling and defines the start URLs.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Using Dataset Map Method in Crawlee
DESCRIPTION: This code snippet demonstrates the usage of the Dataset map method to transform the dataset and create a new array containing header counts greater than 5.

LANGUAGE: javascript
CODE:
import { Dataset, KeyValueStore } from 'crawlee';

const dataset = await Dataset.open();
const moreThan5headers = await dataset.map((item) => {
    if (item.headingCount > 5) {
        return item.headingCount;
    }
});

const kvStore = await KeyValueStore.open();
await kvStore.setValue('more-than-5-headers', moreThan5headers);

----------------------------------------

TITLE: Node.js Base Image Configuration
DESCRIPTION: Basic Docker configuration for Node.js environment without browsers

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:20

----------------------------------------

TITLE: Adjusting Test Timeout in Vitest for Crawlee
DESCRIPTION: Example of how to adjust test timeout settings in Vitest, specifically for Windows platform.

LANGUAGE: typescript
CODE:
if (os.platform() === 'win32') {
    vitest.setConfig({
        testTimeout: 100_000,
    });
}

----------------------------------------

TITLE: Installing Crawlee Project Template
DESCRIPTION: Command to create a new Crawlee project using pipx

LANGUAGE: bash
CODE:
pipx run crawlee create nike-crawler

----------------------------------------

TITLE: Multiple Browser Management with Browser Pool
DESCRIPTION: Example demonstrating how to manage multiple browser types (Chromium, Firefox, WebKit) using Browser Pool

LANGUAGE: javascript
CODE:
import { BrowserPool, PlaywrightPlugin } from '@crawlee/browser-pool';
import playwright from 'playwright';

const browserPool = new BrowserPool({
    browserPlugins: [
        new PlaywrightPlugin(playwright.chromium),
        new PlaywrightPlugin(playwright.firefox),
        new PlaywrightPlugin(playwright.webkit),
    ],
});

// Open 4 pages in 3 browsers. The browsers are launched
// in a round-robin fashion based on the plugin order.
const chromiumPage = await browserPool.newPage();
const firefoxPage = await browserPool.newPage();
const webkitPage = await browserPool.newPage();
const chromiumPage2 = await browserPool.newPage();

// Don't forget to close pages / destroy pool when you're done.

----------------------------------------

TITLE: Crawling Same Hostname Links
DESCRIPTION: Shows how to crawl links that match the same hostname as the source URL. This is the default strategy that matches relative URLs and those pointing to the same hostname, excluding subdomains.

LANGUAGE: javascript
CODE:
{SameHostnameSource}

----------------------------------------

TITLE: Creating and Running an Actor Locally
DESCRIPTION: Commands to create and run a new Apify actor project locally

LANGUAGE: bash
CODE:
apify create my-hello-world
cd my-hello-world
apify run

----------------------------------------

TITLE: Custom Link Selector Configuration
DESCRIPTION: Example of using a custom selector with enqueueLinks to find specific elements.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    selector: 'div.has-link'
});

----------------------------------------

TITLE: Installing Apify TypeScript Configuration
DESCRIPTION: Install @apify/tsconfig as a development dependency for TypeScript configuration.

LANGUAGE: shell
CODE:
npm install --save-dev @apify/tsconfig

----------------------------------------

TITLE: Installing playwright-extra and stealth plugin
DESCRIPTION: This bash command installs the necessary packages for using playwright-extra with the stealth plugin.

LANGUAGE: bash
CODE:
npm install playwright-extra puppeteer-extra-plugin-stealth

----------------------------------------

TITLE: Crawlee Execution Log
DESCRIPTION: Example log output showing crawler execution and webpage titles

LANGUAGE: log
CODE:
INFO  PlaywrightCrawler: Starting the crawl
INFO  PlaywrightCrawler: Title of https://crawlee.dev/ is 'Crawlee · Build reliable crawlers. Fast. | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core is '@crawlee/core | API | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core/changelog is 'Changelog | API | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'

----------------------------------------

TITLE: Filtering URLs with Globs in Crawlee
DESCRIPTION: Demonstrates how to use globs to filter URLs when enqueueing links in Crawlee.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
});

----------------------------------------

TITLE: Implementing Express Server for GCP Cloud Run
DESCRIPTION: Creates an Express HTTP server that wraps the crawler functionality and handles incoming requests. The server listens on the port specified by GCP's PORT environment variable.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import express from 'express';
const app = express();

const startUrls = ['https://crawlee.dev'];

app.get('/', async (req, res) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));
    
    await crawler.run(startUrls);    

    return res.send(await crawler.getData());
});

app.listen(parseInt(process.env.PORT) || 3000);

----------------------------------------

TITLE: Basic Got Scraping Usage with BasicCrawler
DESCRIPTION: Demonstrates how to use sendRequest function within BasicCrawler to make HTTP requests.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest();
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Configuring Crawlee using crawlee.json
DESCRIPTION: This snippet shows how to use a crawlee.json file to set configuration options for Crawlee. It demonstrates setting the persistStateIntervalMillis and logLevel options.

LANGUAGE: json
CODE:
{
  "persistStateIntervalMillis": 10000,
  "logLevel": "DEBUG"
}

----------------------------------------

TITLE: Managing Crawler Instances in TypeScript
DESCRIPTION: Handles crawler instance creation and request processing by either retrieving an existing crawler or creating a new one based on proxy configuration.

LANGUAGE: typescript
CODE:
const key = JSON.stringify(crawlerOptions);
const crawler = crawlers.has(key) ? crawlers.get(key)! : await createAndStartCrawler(crawlerOptions);

await crawler.addRequests([request]);

----------------------------------------

TITLE: Failed Scraping Output Log
DESCRIPTION: Shows the empty output from the CheerioCrawler attempt, demonstrating why JavaScript rendering is necessary.

LANGUAGE: log
CODE:
ACTOR:

----------------------------------------

TITLE: Basic Got Scraping Usage with BasicCrawler
DESCRIPTION: Demonstrates how to use sendRequest function within BasicCrawler to make HTTP requests.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest();
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Docker Multi-stage Build Configuration
DESCRIPTION: Dockerfile configuration using multi-stage build to optimize the production image size by excluding development dependencies

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16 AS builder

COPY . ./
RUN npm install --include=dev \
    && npm run build

FROM apify/actor-node:16
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/dist ./dist

RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional

CMD npm run start:prod

----------------------------------------

TITLE: Logging into Apify Platform using CLI
DESCRIPTION: Instructions for logging into the Apify platform using the Apify CLI tool. This allows automatic credential injection when running scrapers.

LANGUAGE: bash
CODE:
npm install -g apify-cli
apify login -t YOUR_API_TOKEN

----------------------------------------

TITLE: Using enqueueLinks with All Strategy in Crawlee
DESCRIPTION: Example of using enqueueLinks with the 'all' strategy to follow every link, regardless of domain.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    strategy: 'all', // wander the internet
});

----------------------------------------

TITLE: Saving Extracted Data with Dataset.pushData() (JavaScript)
DESCRIPTION: Uses the Dataset.pushData() function to save the extracted data (stored in the 'results' variable) to the default Dataset. This replaces the previous console.log() call.

LANGUAGE: javascript
CODE:
await Dataset.pushData(results);

----------------------------------------

TITLE: Extracting Product SKU with Playwright in JavaScript
DESCRIPTION: This snippet shows how to use Playwright to find and extract the product SKU from a webpage using a specific CSS selector.

LANGUAGE: javascript
CODE:
const sku = await page.locator('span.product-meta__sku-number').textContent();

----------------------------------------

TITLE: Logging in to Apify Platform using CLI
DESCRIPTION: Command to log in to the Apify Platform using the Apify CLI. Requires a personal access token from the Apify console.

LANGUAGE: bash
CODE:
apify login

----------------------------------------

TITLE: Customizing Link Selection in Crawlee's enqueueLinks
DESCRIPTION: Shows how to override the default link selection in Crawlee's enqueueLinks function by specifying a custom selector.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    selector: 'div.has-link'
});

----------------------------------------

TITLE: Dataset Storage Path Structure
DESCRIPTION: Shows the directory structure where dataset items are stored as individual files within the project folder.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/datasets/default/

----------------------------------------

TITLE: Creating a Request Queue with Locking Support
DESCRIPTION: This snippet shows how to create a request queue that supports locking using the RequestQueueV2 class. It demonstrates importing the class and adding requests to the queue.

LANGUAGE: typescript
CODE:
import { RequestQueueV2 } from 'crawlee';

const queue = await RequestQueueV2.open('my-locking-queue');
await queue.addRequests([
    { url: 'https://crawlee.dev' },
    { url: 'https://crawlee.dev/js/docs' },
    { url: 'https://crawlee.dev/js/api' },
]);

----------------------------------------

TITLE: Configuring Docker Environment for Crawlee Project
DESCRIPTION: Dockerfile that sets up a containerized environment for running Crawlee projects. It uses a pre-configured base image with Node.js and Playwright, installs production dependencies, and configures the runtime environment.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16

COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD npm start --silent

----------------------------------------

TITLE: Comparing Browser and JSDOM Title Access
DESCRIPTION: Demonstrates the difference between accessing page title in browsers versus JSDOM environment.

LANGUAGE: typescript
CODE:
// Return the page title
document.title; // browsers
window.document.title; // JSDOM

----------------------------------------

TITLE: Crawling with PuppeteerCrawler in JavaScript
DESCRIPTION: Example code for performing a recursive crawl of the Crawlee website using PuppeteerCrawler.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, Dataset } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        await Dataset.pushData({
            url: request.url,
            title,
        });
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Using sendRequest with JSON Response Type
DESCRIPTION: Shows how to use sendRequest with a JSON response type. This example demonstrates setting the responseType option to 'json' to parse the response as JSON.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ responseType: 'json' });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Installing Crawlee Manually
DESCRIPTION: NPM commands for manually installing Crawlee and its optional browser automation dependencies

LANGUAGE: bash
CODE:
npm install crawlee

LANGUAGE: bash
CODE:
npm install crawlee playwright

LANGUAGE: bash
CODE:
npm install crawlee puppeteer

----------------------------------------

TITLE: Deploying Crawlee Project to Apify Platform
DESCRIPTION: Command to push the Crawlee project to the Apify Platform, creating a new Actor.

LANGUAGE: bash
CODE:
apify push

----------------------------------------

TITLE: Using transformRequestFunction in Crawlee
DESCRIPTION: Shows how to use the transformRequestFunction to have fine-grained control over which requests are enqueued and how they are modified.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
    transformRequestFunction(req) {
        // ignore all links ending with `.pdf`
        if (req.url.endsWith('.pdf')) return false;
        return req;
    },
});

----------------------------------------

TITLE: Sample Crawlee Log Output
DESCRIPTION: Example log output from a running Crawlee scraper, showing crawled pages and their titles.

LANGUAGE: log
CODE:
INFO  PlaywrightCrawler: Starting the crawl
INFO  PlaywrightCrawler: Title of https://crawlee.dev/ is 'Crawlee · Build reliable crawlers. Fast. | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core is '@crawlee/core | API | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core/changelog is 'Changelog | API | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'

----------------------------------------

TITLE: Installing Apify TypeScript Configuration for Crawlee Project
DESCRIPTION: Command to install @apify/tsconfig as a development dependency for a Crawlee TypeScript project.

LANGUAGE: shell
CODE:
npm install --save-dev @apify/tsconfig

----------------------------------------

TITLE: Dockerfile for Node.js Crawlee Project
DESCRIPTION: Example Dockerfile for a Node.js Crawlee project using the actor-node image.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16

----------------------------------------

TITLE: Setting enqueueLinks Strategy to 'All' in Crawlee
DESCRIPTION: Shows how to configure 'enqueueLinks' to follow every link, regardless of its domain.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    strategy: 'all', // wander the internet
});

----------------------------------------

TITLE: Creating a Request Queue with Locking Support
DESCRIPTION: This snippet shows how to create a request queue that supports locking using the RequestQueueV2 class. It demonstrates importing the class and adding requests to the queue.

LANGUAGE: typescript
CODE:
import { RequestQueueV2 } from 'crawlee';

const queue = await RequestQueueV2.open('my-locking-queue');
await queue.addRequests([
    { url: 'https://crawlee.dev' },
    { url: 'https://crawlee.dev/js/docs' },
    { url: 'https://crawlee.dev/js/api' },
]);

----------------------------------------

TITLE: Accessing Apify platform storage
DESCRIPTION: JavaScript code demonstrating how to access and use Apify platform storage both locally and remotely.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';
import { Dataset } from 'crawlee';

// or Dataset.open('my-local-data')
const localDataset = await Actor.openDataset('my-local-data');
// but here we need the `Actor` class
const remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });

----------------------------------------

TITLE: Scraping Static Content with CheerioCrawler in TypeScript
DESCRIPTION: This snippet demonstrates an attempt to scrape JavaScript-rendered content using CheerioCrawler, which fails because it can't execute client-side JavaScript.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        // Extract text content of an actor card
        const actorText = $('.ActorStoreItem').text();
        console.log(`ACTOR: ${actorText}`);
    }
})

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Implementing Crawlee Router with Multiple Handlers
DESCRIPTION: Defines route handlers for different page types using Crawlee's router. Includes handlers for product details, category pages, and a default handler for the start URL. Each handler implements specific scraping logic for its respective page type.

LANGUAGE: javascript
CODE:
import { createPlaywrightRouter, Dataset } from 'crawlee';

// createPlaywrightRouter() is only a helper to get better
// intellisense and typings. You can use Router.create() too.
export const router = createPlaywrightRouter();

// This replaces the request.label === DETAIL branch of the if clause.
router.addHandler('DETAIL', async ({ request, page, log }) => {
    log.debug(`Extracting data: ${request.url}`);
    const urlPart = request.url.split('/').slice(-1);
    const manufacturer = urlPart[0].split('-')[0];

    const title = await page.locator('.product-meta h1').textContent();
    const sku = await page
        .locator('span.product-meta__sku-number')
        .textContent();

    const priceElement = page
        .locator('span.price')
        .filter({
            hasText: '$',
        })
        .first();

    const currentPriceString = await priceElement.textContent();
    const rawPrice = currentPriceString.split('$')[1];
    const price = Number(rawPrice.replaceAll(',', ''));

    const inStockElement = page
        .locator('span.product-form__inventory')
        .filter({
            hasText: 'In stock',
        })
        .first();

    const inStock = (await inStockElement.count()) > 0;

    const results = {
        url: request.url,
        manufacturer,
        title,
        sku,
        currentPrice: price,
        availableInStock: inStock,
    };

    log.debug(`Saving data: ${request.url}`);
    await Dataset.pushData(results);
});

router.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {
    log.debug(`Enqueueing pagination for: ${request.url}`);
    await page.waitForSelector('.product-item > a');
    await enqueueLinks({
        selector: '.product-item > a',
        label: 'DETAIL',
    });

    const nextButton = await page.$('a.pagination__next');
    if (nextButton) {
        await enqueueLinks({
            selector: 'a.pagination__next',
            label: 'CATEGORY',
        });
    }
});

router.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {
    log.debug(`Enqueueing categories from page: ${request.url}`);
    await page.waitForSelector('.collection-block-item');
    await enqueueLinks({
        selector: '.collection-block-item',
        label: 'CATEGORY',
    });
});

----------------------------------------

TITLE: Installing ts-node for Development
DESCRIPTION: Install ts-node as a development dependency to run TypeScript code directly during development.

LANGUAGE: shell
CODE:
npm install --save-dev ts-node

----------------------------------------

TITLE: Installing and Logging into Apify CLI
DESCRIPTION: Commands to install the Apify CLI globally and log in with an API token

LANGUAGE: bash
CODE:
npm install -g apify-cli
apify login -t YOUR_API_TOKEN

----------------------------------------

TITLE: Enqueuing Links with Crawlee
DESCRIPTION: Basic example of using the enqueueLinks function in Crawlee.

LANGUAGE: javascript
CODE:
await enqueueLinks();

----------------------------------------

TITLE: Package Dependencies Configuration
DESCRIPTION: Configures package.json dependencies for Docker setup with browser automation libraries.

LANGUAGE: json
CODE:
{
    "dependencies": {
        "crawlee": "^3.0.0",
        "playwright": "*"
    }
}

----------------------------------------

TITLE: Crawling All Links with Puppeteer Crawler in Crawlee
DESCRIPTION: This snippet shows how to use the Puppeteer Crawler in Crawlee to crawl all links on a website. It creates a PuppeteerCrawler, sets up a RequestQueue, and uses the enqueueLinks() method to add new links to the queue.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, enqueueLinks } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limitation for only 10 requests (do not use if you want to crawl all links)
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Installing Apify SDK and CLI for Crawlee Deployment
DESCRIPTION: Commands to install the Apify SDK as a project dependency and the Apify CLI as a global tool for deployment.

LANGUAGE: bash
CODE:
npm install apify

LANGUAGE: bash
CODE:
npm install -g apify-cli

----------------------------------------

TITLE: Building a CheerioCrawler with RequestQueue in Crawlee
DESCRIPTION: Sets up a CheerioCrawler with a RequestQueue and a requestHandler. The crawler visits the specified URL, extracts the page title using Cheerio, and logs it.

LANGUAGE: typescript
CODE:
// Add import of CheerioCrawler
import { RequestQueue, CheerioCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

// Create the crawler and add the queue with our URL
// and a request handler to process the page.
const crawler = new CheerioCrawler({
    requestQueue,
    // The `$` argument is the Cheerio object
    // which contains parsed HTML of the website.
    async requestHandler({ $, request }) {
        // Extract <title> text with Cheerio.
        // See Cheerio documentation for API docs.
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

// Start the crawler and wait for it to finish
await crawler.run();

----------------------------------------

TITLE: Demonstrating Dataset Operations in Crawlee
DESCRIPTION: This snippet illustrates basic operations for working with datasets in Crawlee, including writing single and multiple rows to both default and named datasets.

LANGUAGE: javascript
CODE:
import { Dataset } from 'crawlee';

// Write a single row to the default dataset
await Dataset.pushData({ col1: 123, col2: 'val2' });

// Open a named dataset
const dataset = await Dataset.open('some-name');

// Write a single row
await dataset.pushData({ foo: 'bar' });

// Write multiple rows
await dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);

----------------------------------------

TITLE: Crawling Sitemap with Playwright Crawler in JavaScript
DESCRIPTION: This code snippet illustrates how to use Playwright Crawler to crawl a sitemap. It uses the Sitemap utility to download and process sitemap URLs, then crawls each URL using PlaywrightCrawler. The crawler extracts the page title and saves it to the dataset.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';
import { Sitemap } from '@crawlee/utils';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url} is '${title}'`);

        await Dataset.pushData({
            url: request.url,
            title,
        });
    },
});

const sitemapUrl = 'https://crawlee.dev/sitemap.xml';

const sitemap = await Sitemap.load(sitemapUrl);
await crawler.run(sitemap.urls);

----------------------------------------

TITLE: Dockerfile for Playwright Firefox Crawlee Project
DESCRIPTION: Example Dockerfile for a Crawlee project using Playwright with Firefox browser.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-firefox:16

----------------------------------------

TITLE: Package.json Configuration for Crawlee
DESCRIPTION: Example package.json configuration for Crawlee, using asterisk to ensure pre-installed versions of automation libraries are used.

LANGUAGE: json
CODE:
{
    "dependencies": {
        "crawlee": "^3.0.0",
        "playwright": "*"
    }
}

----------------------------------------

TITLE: Basic Proxy Configuration in Crawlee
DESCRIPTION: Demonstrates the basic setup of proxy configuration using ProxyConfiguration class with custom proxy URLs.

LANGUAGE: javascript
CODE:
import { ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ]
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Using sendRequest with Custom Cookie Jar
DESCRIPTION: Demonstrates how to use sendRequest with a custom cookie jar. This example shows how to create and use a CookieJar from the tough-cookie package to manage cookies in requests.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';
import { CookieJar } from 'tough-cookie';

const cookieJar = new CookieJar();

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ cookieJar });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Sample Dataset Structure in JSON
DESCRIPTION: Example JSON structure showing scraped data stored in the default dataset, containing URLs and their corresponding heading counts.

LANGUAGE: json
CODE:
[
    {
        "url": "https://crawlee.dev/",
        "headingCount": 11
    },
    {
        "url": "https://crawlee.dev/storage",
        "headingCount": 8
    },
    {
        "url": "https://crawlee.dev/proxy",
        "headingCount": 4
    }
]

----------------------------------------

TITLE: Deploying Project to Apify Platform
DESCRIPTION: Command to deploy the project to the Apify Platform, which creates an archive and initiates a Docker build.

LANGUAGE: bash
CODE:
apify push

----------------------------------------

TITLE: Basic Cheerio Crawler Setup in TypeScript
DESCRIPTION: Initial setup of a CheerioCrawler that crawls a single URL and extracts the page title.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Creating New Crawlee Project
DESCRIPTION: Command to create a new Crawlee project using the Crawlee CLI via npx

LANGUAGE: bash
CODE:
npx crawlee create my-crawler

----------------------------------------

TITLE: Installing Crawlee Dependencies
DESCRIPTION: Package installation commands for different crawler types including optional dependencies

LANGUAGE: bash
CODE:
npm install crawlee
npm install crawlee playwright
npm install crawlee puppeteer

----------------------------------------

TITLE: Installing puppeteer-extra and stealth plugin
DESCRIPTION: This bash command installs the necessary packages for using puppeteer-extra with the stealth plugin.

LANGUAGE: bash
CODE:
npm install puppeteer-extra puppeteer-extra-plugin-stealth

----------------------------------------

TITLE: Complete AWS Lambda Handler Implementation
DESCRIPTION: Full implementation of AWS Lambda handler function incorporating Crawlee crawler with Chromium configuration and response handling.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
        launchContext: {
            launchOptions: {
                executablePath: await aws_chromium.executablePath(),
                args: aws_chromium.args,
                headless: true
            }
        }
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);

    return {
        statusCode: 200,
        body: await crawler.getData(),
    };
}

----------------------------------------

TITLE: DevTools Element Query
DESCRIPTION: JavaScript query selector command for inspecting collection elements in browser DevTools

LANGUAGE: javascript
CODE:
document.querySelectorAll('.collection-block-item');

----------------------------------------

TITLE: Setting up Node.js HTTP Server
DESCRIPTION: Basic setup of a Node.js HTTP server using the built-in http module. Creates a server that listens on port 3000 and handles incoming requests.

LANGUAGE: javascript
CODE:
import { createServer } from 'http';
import { log } from 'crawlee';

const server = createServer(async (req, res) => {
    log.info(`Request received: ${req.method} ${req.url}`);

    res.writeHead(200, { 'Content-Type': 'text/plain' });
    // We will return the page title here later instead
    res.end('Hello World\n');
});

server.listen(3000, () => {
    log.info('Server is listening for user requests');
});

----------------------------------------

TITLE: Storage Cleanup in Crawlee
DESCRIPTION: Shows how to purge default storage directories in Crawlee using the purgeDefaultStorages helper function.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Browser Pool Configuration with Lifecycle Hooks
DESCRIPTION: Example of configuring BrowserPool with lifecycle hooks and custom options in PuppeteerCrawler

LANGUAGE: javascript
CODE:
const crawler = new Apify.PuppeteerCrawler({
    browserPoolOptions: {
        retireBrowserAfterPageCount: 10,
        preLaunchHooks: [
            async (pageId, launchContext) => {
                const { request } = crawler.crawlingContexts.get(pageId);
                if (request.userData.useHeadful === true) {
                    launchContext.launchOptions.headless = false;
                }
            }
        ]
    }
})

----------------------------------------

TITLE: Sanity Check with PlaywrightCrawler and Cheerio
DESCRIPTION: An alternative setup using PlaywrightCrawler with Cheerio for HTML parsing. This approach demonstrates how to combine Playwright for browser automation with Cheerio for efficient HTML parsing.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';
import { load } from 'cheerio';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${page.url()} is '${title}'`);

        const html = await page.content();
        const $ = load(html);

        $('.collection-block-item').each((_, el) => {
            const category = $(el).text();
            Dataset.pushData({ category });
        });

        await enqueueLinks({
            selector: '.collection-block-item',
            label: 'CATEGORY',
        });
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);


----------------------------------------

TITLE: URL Pattern Filtering
DESCRIPTION: Example of filtering URLs using glob patterns in enqueueLinks.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*']
});

----------------------------------------

TITLE: Authenticating with Apify Platform
DESCRIPTION: Command to log into the Apify Platform using the CLI tool

LANGUAGE: bash
CODE:
apify login

----------------------------------------

TITLE: Using actor-node-playwright-webkit Docker Image
DESCRIPTION: Shows how to use the Apify Docker image that includes Playwright and WebKit.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-webkit:16

----------------------------------------

TITLE: Extracting Product SKU with Playwright
DESCRIPTION: Shows how to get a product SKU using Playwright's locator to find specific span element.

LANGUAGE: javascript
CODE:
const sku = await page.locator('span.product-meta__sku-number').textContent();

----------------------------------------

TITLE: Extracting Product SKU with Playwright
DESCRIPTION: Shows how to get a product SKU using Playwright's locator to find specific span element.

LANGUAGE: javascript
CODE:
const sku = await page.locator('span.product-meta__sku-number').textContent();

----------------------------------------

TITLE: Extracting Product SKU with Playwright
DESCRIPTION: Shows how to get a product SKU using Playwright's locator to find specific span element.

LANGUAGE: javascript
CODE:
const sku = await page.locator('span.product-meta__sku-number').textContent();

----------------------------------------

TITLE: Installing Apify SDK Dependency
DESCRIPTION: Command to install the Apify SDK as a project dependency for integrating with Apify Platform storage services.

LANGUAGE: bash
CODE:
npm install apify

----------------------------------------

TITLE: Accessing Page Title with Browser JavaScript and JSDOM
DESCRIPTION: Demonstrates how to access the page title using both browser JavaScript and JSDOM approaches. Shows the syntax difference between browser-side and JSDOM implementations.

LANGUAGE: typescript
CODE:
// Return the page title
document.title; // browsers
window.document.title; // JSDOM

----------------------------------------

TITLE: Using actor-node-puppeteer-chrome Docker Image
DESCRIPTION: Example of using the Apify Docker image that includes Puppeteer and Chrome, suitable for CheerioCrawler and PuppeteerCrawler.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-puppeteer-chrome:20

----------------------------------------

TITLE: Checking Node.js Version in Bash
DESCRIPTION: Command to verify the installed version of Node.js.

LANGUAGE: bash
CODE:
node -v

----------------------------------------

TITLE: Configuring Crawlee using crawlee.json
DESCRIPTION: Example showing basic configuration options in crawlee.json file to set persistStateIntervalMillis and logLevel.

LANGUAGE: json
CODE:
{
  "persistStateIntervalMillis": 10000,
  "logLevel": "DEBUG"
}

----------------------------------------

TITLE: Key-Value Store Operations in Crawlee
DESCRIPTION: Shows basic operations for working with key-value stores in Crawlee, including reading input, writing output, and managing named stores.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';

// Get the INPUT from the default key-value store
const input = await KeyValueStore.getInput();

// Write the OUTPUT to the default key-value store
await KeyValueStore.setValue('OUTPUT', { myResult: 123 });

// Open a named key-value store
const store = await KeyValueStore.open('some-name');

// Write a record to the named key-value store.
// JavaScript object is automatically converted to JSON,
// strings and binary buffers are stored as they are
await store.setValue('some-key', { foo: 'bar' });

// Read a record from the named key-value store.
// Note that JSON is automatically parsed to a JavaScript object,
// text data is returned as a string, and other data is returned as binary buffer
const value = await store.getValue('some-key');

// Delete a record from the named key-value store
await store.setValue('some-key', null);

----------------------------------------

TITLE: Using Key-Value Store Public URLs
DESCRIPTION: Example showing how to get a public URL for a stored item in Key-Value Store

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'apify';

const store = await KeyValueStore.open();
await store.setValue('your-file', { foo: 'bar' });
const url = store.getPublicUrl('your-file');
// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file

----------------------------------------

TITLE: Sample Dataset Structure in JSON
DESCRIPTION: Example JSON structure showing scraped data stored in the default dataset, containing URLs and heading counts from web pages.

LANGUAGE: json
CODE:
[
    {
        "url": "https://crawlee.dev/",
        "headingCount": 11
    },
    {
        "url": "https://crawlee.dev/storage",
        "headingCount": 8
    },
    {
        "url": "https://crawlee.dev/proxy",
        "headingCount": 4
    }
]

----------------------------------------

TITLE: Configuring Browser Fingerprints
DESCRIPTION: Example showing how to disable dynamic fingerprints in Crawlee browser configuration.

LANGUAGE: typescript
CODE:
const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        useFingerprints: false,
    },
});

----------------------------------------

TITLE: Installing Crawlee and Playwright for PlaywrightCrawler
DESCRIPTION: Command to manually install Crawlee and Playwright for use with PlaywrightCrawler.

LANGUAGE: bash
CODE:
npm install crawlee playwright

----------------------------------------

TITLE: Initializing Apify Project Configuration
DESCRIPTION: Command to initialize the Apify project configuration, creating necessary files for deployment.

LANGUAGE: bash
CODE:
apify init

----------------------------------------

TITLE: Custom Link Selector Configuration
DESCRIPTION: Example of using a custom selector with enqueueLinks to target specific elements.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    selector: 'div.has-link'
});

----------------------------------------

TITLE: Configuring Advanced autoscaledPoolOptions in CheerioCrawler
DESCRIPTION: This snippet illustrates how to set advanced autoscaledPoolOptions for fine-tuning the autoscaling behavior of a CheerioCrawler.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    autoscaledPoolOptions: {
        desiredConcurrency: 30,
        desiredConcurrencyRatio: 0.9,
        scaleUpStepRatio: 0.05,
        scaleDownStepRatio: 0.05,
        maybeRunIntervalSecs: 1,
        loggingIntervalSecs: 60,
        autoscaleIntervalSecs: 15,
        maxTasksPerMinute: 300,
    },
    // ...
});

----------------------------------------

TITLE: Basic Request Queue File Path Structure
DESCRIPTION: Shows the directory structure for request queue storage on local disk

LANGUAGE: text
CODE:
{CRAWLEE_STORAGE_DIR}/request_queues/{QUEUE_ID}/entries.json

----------------------------------------

TITLE: Using Custom RequestQueueV2 with CheerioCrawler
DESCRIPTION: Shows how to integrate a custom RequestQueueV2 instance with CheerioCrawler while enabling the request locking experiment. This setup is required when using a custom request queue with locking support.

LANGUAGE: typescript
CODE:
import { CheerioCrawler, RequestQueueV2 } from 'crawlee';

const queue = await RequestQueueV2.open('my-locking-queue');

const crawler = new CheerioCrawler({
    experiments: {
        requestLocking: true,
    },
    requestQueue: queue,
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

await crawler.run();

----------------------------------------

TITLE: Configuring Crawlee using Custom Configuration
DESCRIPTION: Example of creating a custom Configuration instance and passing it to a CheerioCrawler. It demonstrates setting the persistStateIntervalMillis option and using it with the crawler.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Create new configuration
const config = new Configuration({
    // Set the 'persistStateIntervalMillis' option to 10 seconds
    persistStateIntervalMillis: 10_000,
});

// Now we need to pass the configuration to the crawler
const crawler = new CheerioCrawler({}, config);

crawler.router.addDefaultHandler(async ({ request }) => {
    // for the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // for the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Comparing Cheerio and Browser JavaScript for DOM Manipulation
DESCRIPTION: This snippet demonstrates how to perform common DOM operations using both Cheerio and plain browser JavaScript, highlighting the similarities and differences between the two approaches.

LANGUAGE: typescript
CODE:
// Return the text content of the <title> element.
document.querySelector('title').textContent; // plain JS
$('title').text(); // Cheerio

// Return an array of all 'href' links on the page.
Array.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS
$('[href]')
    .map((i, el) => $(el).attr('href'))
    .get(); // Cheerio

----------------------------------------

TITLE: Request Transform Function
DESCRIPTION: Example of using transformRequestFunction to modify or filter requests before they are enqueued.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
    transformRequestFunction(req) {
        // ignore all links ending with `.pdf`
        if (req.url.endsWith('.pdf')) return false;
        return req;
    },
});

----------------------------------------

TITLE: Installing Apify Dependencies
DESCRIPTION: Commands for installing the Apify SDK as a project dependency and Apify CLI as a global tool

LANGUAGE: bash
CODE:
npm install apify

LANGUAGE: bash
CODE:
npm install -g apify-cli

----------------------------------------

TITLE: Basic Crawler Implementation with crawlee.json
DESCRIPTION: Demonstrates how to implement a basic crawler that uses the configuration from crawlee.json without explicit configuration code.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, sleep } from 'crawlee';

const crawler = new CheerioCrawler();

crawler.router.addDefaultHandler(async ({ request }) => {
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Installing Puppeteer-Extra Dependencies
DESCRIPTION: Command to install puppeteer-extra and its stealth plugin using npm package manager.

LANGUAGE: bash
CODE:
npm install puppeteer-extra puppeteer-extra-plugin-stealth

----------------------------------------

TITLE: Extracting Product SKU
DESCRIPTION: Demonstrates how to extract a product's SKU from a span element with a specific class.

LANGUAGE: javascript
CODE:
const sku = await page.locator('span.product-meta__sku-number').textContent();

----------------------------------------

TITLE: Initializing Apify Project Configuration
DESCRIPTION: Command to initialize the Apify project configuration, creating necessary files for deployment.

LANGUAGE: bash
CODE:
apify init

----------------------------------------

TITLE: Creating INPUT.json File for Actor Input
DESCRIPTION: This bash snippet shows the file path where the INPUT.json file should be created to provide input to the actor. The file should be placed in the project's key-value store directory.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json

----------------------------------------

TITLE: Crawlee Log Output
DESCRIPTION: Log entries from CheerioCrawler showing the crawler starting and visiting various pages on crawlee.dev, extracting the page titles.

LANGUAGE: text
CODE:
INFO  CheerioCrawler: Starting the crawl
INFO  CheerioCrawler: Title of https://crawlee.dev/ is 'Crawlee · Build reliable crawlers. Fast. | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/guides is 'Guides | Crawlee'

----------------------------------------

TITLE: Disabling Browser Fingerprints with PlaywrightCrawler in Crawlee
DESCRIPTION: This code snippet demonstrates how to disable the use of browser fingerprints when using PlaywrightCrawler in Crawlee. It sets the 'useFingerprints' option to false in the browserPoolOptions.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        useFingerprints: false,
    },
    // ...
});

----------------------------------------

TITLE: Installing TypeScript Dependencies
DESCRIPTION: Commands for installing TypeScript compiler and Node.js type definitions as development dependencies

LANGUAGE: shell
CODE:
npm install --save-dev typescript

LANGUAGE: shell
CODE:
npm install --save-dev @types/node

----------------------------------------

TITLE: Configuring enqueueLinks for Same-Domain Crawling
DESCRIPTION: Demonstrates how to set up enqueueLinks to include subdomains in the crawling process using the 'same-domain' strategy.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    strategy: 'same-domain'
});

----------------------------------------

TITLE: Installing Node.js Type Declarations
DESCRIPTION: Command to install type declarations for Node.js as a development dependency.

LANGUAGE: shell
CODE:
npm install --dev @types/node

----------------------------------------

TITLE: Storage Cleanup Implementation
DESCRIPTION: Shows how to purge default storage directories using the purgeDefaultStorages helper function

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Purging Default Storages in Crawlee
DESCRIPTION: This snippet demonstrates how to explicitly purge default storages in Crawlee using the purgeDefaultStorages() helper function.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Complete Package.json Configuration
DESCRIPTION: Full package.json configuration including all scripts and dependencies for both development and production

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "type": "module",
    "main": "dist/main.js",
    "dependencies": {
        "crawlee": "3.0.0"
    },
    "devDependencies": {
        "@apify/tsconfig": "^0.1.0",
        "@types/node": "^18.14.0",
        "ts-node": "^10.8.0",
        "typescript": "^4.7.4"
    },
    "scripts": {
        "start": "npm run start:dev",
        "start:prod": "node dist/main.js",
        "start:dev": "ts-node-esm -T src/main.ts",
        "build": "tsc"
    }
}

----------------------------------------

TITLE: Disabling Browser Fingerprints with PlaywrightCrawler in Crawlee
DESCRIPTION: This code snippet demonstrates how to disable browser fingerprints for PlaywrightCrawler in Crawlee by setting the useFingerprints option to false.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        useFingerprints: false,
    },
    // ...
});

----------------------------------------

TITLE: Using sendRequest with Proxy in BasicCrawler
DESCRIPTION: Demonstrates how to use sendRequest with a proxy URL in a BasicCrawler. This example shows how to manually pass the proxyUrl option when making a request.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            proxyUrl: 'http://auto:password@proxy.apify.com:8000',
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Using actor-node-playwright-firefox Docker Image
DESCRIPTION: Shows how to use the Apify Docker image with Playwright and Firefox pre-installed.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-firefox:20

----------------------------------------

TITLE: Failing to Scrape Dynamic Content with PuppeteerCrawler (No Wait)
DESCRIPTION: This snippet demonstrates how PuppeteerCrawler fails to scrape JavaScript-rendered content when not waiting for elements to appear, resulting in an error.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // This will fail because we're not waiting for the content to load
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Running Headful Browser with PlaywrightCrawler in JavaScript
DESCRIPTION: Example of running PlaywrightCrawler with a visible browser window for development purposes.

LANGUAGE: javascript
CODE:
const crawler = new PlaywrightCrawler({
    launchContext: {
        launchOptions: {
            headless: false,
        },
    },
    // ... other options
});

----------------------------------------

TITLE: Docker Build Configuration
DESCRIPTION: Multi-stage Dockerfile setup for building TypeScript Crawlee projects.

LANGUAGE: dockerfile
CODE:
# using multistage build, as we need dev deps to build the TS source code
FROM apify/actor-node:16 AS builder

# copy all files, install all dependencies (including dev deps) and build the project
COPY . ./
RUN npm install --include=dev \
    && npm run build

# create final image
FROM apify/actor-node:16
# copy only necessary files
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/README.md ./
COPY --from=builder /usr/src/app/dist ./dist
COPY --from=builder /usr/src/app/apify.json ./apify.json
COPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json

# install only prod deps
RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional \
    && echo "Installed NPM packages:" \
    && (npm list --only=prod --no-optional --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

# run compiled code
CMD npm run start:prod

----------------------------------------

TITLE: Using State Management in ParselCrawler
DESCRIPTION: Illustrates how to use the new use_state helper for managing persistent state across crawler runs.

LANGUAGE: python
CODE:
import asyncio

from crawlee import Request
from crawlee.configuration import Configuration
from crawlee.crawlers import ParselCrawler, ParselCrawlingContext


async def main() -> None:
    # Create a crawler with purge_on_start disabled to retain state across runs.
    crawler = ParselCrawler(
        configuration=Configuration(purge_on_start=False),
    )

    @crawler.router.default_handler
    async def handler(context: ParselCrawlingContext) -> None:
        context.log.info(f'Crawling {context.request.url}')

        # Retrieve or initialize the state with a default value.
        state = await context.use_state('state', default_value={'runs': 0})

        # Increment the run count.
        state['runs'] += 1

    # Create a request with always_enqueue enabled to bypass deduplication and ensure it is processed.
    request = Request.from_url('https://crawlee.dev/', always_enqueue=True)

    # Run the crawler with the start request.
    await crawler.run([request])

    # Fetch the persisted state from the key-value store.
    kvs = await crawler.get_key_value_store()
    state = await kvs.get_auto_saved_value('state')
    crawler.log.info(f'Final state after run: {state}')


if __name__ == '__main__':
    asyncio.run(main())

----------------------------------------

TITLE: Using actor-node Docker Image
DESCRIPTION: Demonstrates how to use the smallest Apify Docker image based on Alpine Linux, suitable for CheerioCrawler.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16

----------------------------------------

TITLE: Specifying Node.js Version in Dockerfile
DESCRIPTION: Demonstrates how to use a specific Node.js version in a Dockerfile for an Apify actor.

LANGUAGE: dockerfile
CODE:
# Use Node.js 20
FROM apify/actor-node:20

----------------------------------------

TITLE: Configuring Advanced Autoscaling Options in Crawlee
DESCRIPTION: This snippet demonstrates how to set advanced autoscaling pool options for a CheerioCrawler in Crawlee. It includes various parameters to fine-tune the autoscaling behavior.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    autoscaledPoolOptions: {
        desiredConcurrency: 10,
        desiredConcurrencyRatio: 0.90,
        scaleUpStepRatio: 0.05,
        scaleDownStepRatio: 0.05,
        maybeRunIntervalSecs: 0.5,
        loggingIntervalSecs: 60,
        autoscaleIntervalSecs: 10,
        maxTasksPerMinute: 60,
    },
    // ...
});

----------------------------------------

TITLE: Using RequestQueueV2 with CheerioCrawler
DESCRIPTION: Shows how to combine a RequestQueueV2 instance with a CheerioCrawler while enabling the request locking experiment. Demonstrates proper configuration for using locking-enabled queues.

LANGUAGE: typescript
CODE:
import { CheerioCrawler, RequestQueueV2 } from 'crawlee';

const queue = await RequestQueueV2.open('my-locking-queue');

const crawler = new CheerioCrawler({
    experiments: {
        requestLocking: true,
    },
    requestQueue: queue,
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

await crawler.run();

----------------------------------------

TITLE: Using Request Queue with Crawler in Crawlee
DESCRIPTION: Demonstrates how to use the Request Queue with a Crawler in Crawlee. It shows setting up a PuppeteerCrawler with a default request queue and adding requests to it.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        // Add all links from page to the queue
        await enqueueLinks();
    },
});

// Add requests to the queue
await crawler.addRequests(['https://crawlee.dev']);

// Run the crawler
await crawler.run();

----------------------------------------

TITLE: Dataset Storage Structure Example
DESCRIPTION: Shows the file system path structure for datasets in Crawlee

LANGUAGE: plaintext
CODE:
{CRAWLEE_STORAGE_DIR}/datasets/{DATASET_ID}/{INDEX}.json

----------------------------------------

TITLE: Installing Node.js Type Declarations
DESCRIPTION: Command to install type declarations for Node.js as a development dependency.

LANGUAGE: shell
CODE:
npm install --dev @types/node

----------------------------------------

TITLE: Transforming Requests in Crawlee
DESCRIPTION: Shows how to use the transformRequestFunction to modify or filter requests before they are enqueued in Crawlee.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
    transformRequestFunction(req) {
        // ignore all links ending with `.pdf`
        if (req.url.endsWith('.pdf')) return false;
        return req;
    },
});

----------------------------------------

TITLE: Installing Apify SDK and CLI
DESCRIPTION: Commands to install the Apify SDK as a project dependency and the Apify CLI as a global tool for deployment.

LANGUAGE: bash
CODE:
npm install apify

LANGUAGE: bash
CODE:
npm install -g apify-cli

----------------------------------------

TITLE: Creating GCP Cloud Function Handler
DESCRIPTION: Implement the main handler function for the GCP Cloud Function with request/response handling and crawler execution.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (req, res) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
    
    return res.send(await crawler.getData())
}

----------------------------------------

TITLE: Installing Playwright Extra Dependencies
DESCRIPTION: Command to install the required npm packages for Playwright Extra implementation with stealth plugin

LANGUAGE: bash
CODE:
npm install playwright-extra puppeteer-extra-plugin-stealth

----------------------------------------

TITLE: Crawlee Execution Log Output
DESCRIPTION: Example log output showing crawler execution and page titles being scraped

LANGUAGE: log
CODE:
INFO  PlaywrightCrawler: Starting the crawl
INFO  PlaywrightCrawler: Title of https://crawlee.dev/ is 'Crawlee · Build reliable crawlers. Fast. | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core is '@crawlee/core | API | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core/changelog is 'Changelog | API | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'

----------------------------------------

TITLE: Installing and Logging in with Apify CLI
DESCRIPTION: Commands to install Apify CLI globally and log in using an API token

LANGUAGE: bash
CODE:
npm install -g apify-cli
apify login -t YOUR_API_TOKEN

----------------------------------------

TITLE: Using actor-node-playwright-webkit Docker Image
DESCRIPTION: This snippet demonstrates how to use the Apify Docker image that includes Playwright with WebKit pre-installed.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-webkit:16

----------------------------------------

TITLE: Displaying Dataset Storage Location using Bash
DESCRIPTION: This snippet shows the directory path where each item in the dataset will be saved as a separate file.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/datasets/default/

----------------------------------------

TITLE: Crawling URLs with Puppeteer Crawler
DESCRIPTION: Implementation of a web crawler using Puppeteer Crawler to process multiple URLs. This version requires the apify/actor-node-puppeteer-chrome image when running on the Apify Platform and provides full browser automation capabilities.

LANGUAGE: javascript
CODE:
{PuppeteerSource}

----------------------------------------

TITLE: Initializing CheerioCrawler with GCP Configuration
DESCRIPTION: Sets up the CheerioCrawler with disabled storage persistence for GCP compatibility.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Crawling Sitemap with Puppeteer Crawler
DESCRIPTION: Shows how to crawl sitemaps using Puppeteer Crawler in Crawlee. Requires apify/actor-node-puppeteer-chrome image when running on Apify Platform.

LANGUAGE: typescript
CODE:
{PuppeteerSource}

----------------------------------------

TITLE: Implementing HTTP Crawler with Crawlee
DESCRIPTION: Shows how to use HttpCrawler to create a web crawler that processes URLs from an external file and saves HTML content. The code demonstrates URL loading using HTTP requests and basic crawler configuration.

LANGUAGE: javascript
CODE:
import { HttpCrawler, Dataset } from 'crawlee';
import { readFileSync } from 'fs';

// Read the list of URLs from a file
const sources = JSON.parse(readFileSync('./sources.json', 'utf8'));

// Create an instance of the HttpCrawler class
const crawler = new HttpCrawler({
    // Function called for each URL
    async requestHandler({ request, body }) {
        // Save data to dataset
        await Dataset.pushData({
            url: request.url,
            html: body,
        });
    },
});

// Add URLs to the crawler's queue
await crawler.addRequests(sources);

// Run the crawler
await crawler.run();

----------------------------------------

TITLE: Using actor-node Docker Image
DESCRIPTION: Shows how to use the smallest Apify Docker image based on Alpine Linux, suitable for CheerioCrawler.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16

----------------------------------------

TITLE: Creating AWS Lambda Handler Function for Cheerio Crawler
DESCRIPTION: This code wraps the Cheerio Crawler logic in an async handler function, which is the entry point for AWS Lambda execution. It creates a new crawler instance for each Lambda invocation to maintain statelessness.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
};

----------------------------------------

TITLE: Enabling Headful Browser Mode
DESCRIPTION: Configuration option to enable visible browser window during crawling

LANGUAGE: typescript
CODE:
// Uncomment this option to see the browser window.
headless: false

----------------------------------------

TITLE: Configuring minConcurrency and maxConcurrency in CheerioCrawler
DESCRIPTION: This example shows how to set the minConcurrency and maxConcurrency options in a CheerioCrawler to control the number of parallel requests.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    minConcurrency: 10,
    maxConcurrency: 100,
    // ...
});

----------------------------------------

TITLE: Setting Advanced AutoscaledPool Options in CheerioCrawler
DESCRIPTION: Demonstrates how to configure advanced autoscaledPoolOptions in CheerioCrawler for fine-tuning the autoscaling behavior.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    autoscaledPoolOptions: {
        desiredConcurrency: 10,
        desiredConcurrencyRatio: 0.9,
        scaleUpStepRatio: 0.05,
        scaleDownStepRatio: 0.05,
        maybeRunIntervalSecs: 1,
        loggingIntervalSecs: 60,
        autoscaleIntervalSecs: 10,
        maxTasksPerMinute: 240,
    },
    // ...
});

----------------------------------------

TITLE: Accessing AutoscaledPool in Crawling Context
DESCRIPTION: Example showing how to access the AutoscaledPool through the crawler property in the crawling context.

LANGUAGE: javascript
CODE:
const handlePageFunction = async ({ request, page, crawler }) => {
    await crawler.requestQueue.addRequest({ url: 'https://example.com' });
    await crawler.autoscaledPool.pause();
}

----------------------------------------

TITLE: Setting Up RequestQueueV2 with Locking Support
DESCRIPTION: Demonstrates how to create and use RequestQueueV2 for request locking support outside of crawlers.

LANGUAGE: typescript
CODE:
import { RequestQueueV2 } from 'crawlee';

const queue = await RequestQueueV2.open('my-locking-queue');
await queue.addRequests([
    { url: 'https://crawlee.dev' },
    { url: 'https://crawlee.dev/js/docs' },
    { url: 'https://crawlee.dev/js/api' },
]);

----------------------------------------

TITLE: Setting up Express Server with PlaywrightCrawler for GCP Cloud Run in JavaScript
DESCRIPTION: This code sets up an Express server to handle HTTP requests for a PlaywrightCrawler in GCP Cloud Run. It creates a route that initializes and runs the crawler, then returns the crawled data. The server listens on the port specified by the GCP environment variable.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import express from 'express';
const app = express();

const startUrls = ['https://crawlee.dev'];

app.get('/', async (req, res) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));
    
    await crawler.run(startUrls);    

    return res.send(await crawler.getData());
});

app.listen(parseInt(process.env.PORT) || 3000);

----------------------------------------

TITLE: Logging into Apify Platform using CLI
DESCRIPTION: Commands to install Apify CLI and log in with an API token

LANGUAGE: bash
CODE:
npm install -g apify-cli
apify login -t YOUR_API_TOKEN

----------------------------------------

TITLE: Map Method Result Example
DESCRIPTION: Example output showing the result of mapping operation that filters header counts greater than 5.

LANGUAGE: javascript
CODE:
[11, 8]

----------------------------------------

TITLE: Example HTML Link Structure
DESCRIPTION: Sample HTML showing the link structure that the crawler will parse.

LANGUAGE: html
CODE:
<a href="https://crawlee.dev/js/docs/introduction">This is a link to Crawlee introduction</a>

----------------------------------------

TITLE: Selecting Elements with Query Selector in Browser Console
DESCRIPTION: JavaScript code to select all collection block items on a page using document.querySelectorAll()

LANGUAGE: javascript
CODE:
document.querySelectorAll('.collection-block-item');

----------------------------------------

TITLE: Installing Apify SDK and CLI for Crawlee Deployment
DESCRIPTION: Commands to install the Apify SDK as a project dependency and the Apify CLI as a global tool for deploying Crawlee projects to the Apify Platform.

LANGUAGE: bash
CODE:
npm install apify

LANGUAGE: bash
CODE:
npm install -g apify-cli

----------------------------------------

TITLE: Starting the Crawler
DESCRIPTION: Navigate to the project directory and start the crawler using npm.

LANGUAGE: bash
CODE:
cd my-crawler && npm start

----------------------------------------

TITLE: Installing Apify CLI Globally
DESCRIPTION: Command to install the Apify CLI tool globally for managing Apify projects and deployments.

LANGUAGE: bash
CODE:
npm install -g apify-cli

----------------------------------------

TITLE: Creating a handler function for GCP Cloud Functions
DESCRIPTION: Wraps the crawler execution in an async handler function that takes request and response objects, and exports it for use by GCP.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (req, res) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
    
    return res.send(await crawler.getData())
}

----------------------------------------

TITLE: Finding All Links on a Page with JSDOMCrawler
DESCRIPTION: This code snippet demonstrates how to use JSDOMCrawler to find all <a> elements with href attributes on a page and extract their href values into an array.

LANGUAGE: javascript
CODE:
Array.from(document.querySelectorAll('a[href]')).map((a) => a.href);

----------------------------------------

TITLE: Initializing Apify Project Configuration
DESCRIPTION: Command to initialize the Apify project configuration, creating necessary .actor directory and actor.json file.

LANGUAGE: bash
CODE:
apify init

----------------------------------------

TITLE: Direct Puppeteer Screenshot Capture
DESCRIPTION: Demonstrates capturing a screenshot using Puppeteer's page.screenshot() method directly. The screenshot is saved to a key-value store with a key derived from the page URL.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';
import puppeteer from 'puppeteer';

const browser = await puppeteer.launch();
const page = await browser.newPage();
await page.goto('https://example.com');

// Get a reasonable key for the screenshot
const key = (new URL('https://example.com')).hostname;

// Save the screenshot to the default key-value store
await KeyValueStore.setValue(
    `${key}.jpg`,
    await page.screenshot(),
    { contentType: 'image/jpeg' },
);

await browser.close();

----------------------------------------

TITLE: Running Crawlee Code as an Apify Actor
DESCRIPTION: Example of how to structure Crawlee code to run as an Apify actor using Actor.main().

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';
import { CheerioCrawler, Dataset } from 'crawlee';

await Actor.main(async () => {
    const crawler = new CheerioCrawler({
        async requestHandler({ $, request }) {
            const title = $('title').text();
            await Dataset.pushData({
                url: request.url,
                title
            });
        },
        maxRequestsPerCrawl: 20,
    });

    await crawler.run(['https://crawlee.dev']);
});

----------------------------------------

TITLE: Capturing Screenshot Using Puppeteer Utils Snapshot
DESCRIPTION: Example showing how to capture a webpage screenshot using the utils.puppeteer.saveSnapshot() utility function. This method provides a simpler interface for saving screenshots.

LANGUAGE: javascript
CODE:
const url = 'https://crawlee.dev';

// Launch browser and navigate to the page
const browser = await launchPuppeteer();
const page = await browser.newPage();
await page.goto(url);

// Save page screenshot
await utils.puppeteer.saveSnapshot(page);

await browser.close();

----------------------------------------

TITLE: Locating Saved Dataset Files in Crawlee Project
DESCRIPTION: This bash snippet shows the directory path where individual dataset item files are stored in a Crawlee project.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/datasets/default/

----------------------------------------

TITLE: Installing Crawlee Dependencies
DESCRIPTION: NPM commands for installing Crawlee with different crawler types (Cheerio, Playwright, Puppeteer)

LANGUAGE: bash
CODE:
npm install crawlee
npm install crawlee playwright
npm install crawlee puppeteer

----------------------------------------

TITLE: Complete Package.json Configuration
DESCRIPTION: Complete package.json configuration including all dependencies and scripts for both development and production.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "type": "module",
    "main": "dist/main.js",
    "dependencies": {
        "crawlee": "3.0.0"
    },
    "devDependencies": {
        "@apify/tsconfig": "^0.1.0",
        "@types/node": "^18.14.0",
        "ts-node": "^10.8.0",
        "typescript": "^4.7.4"
    },
    "scripts": {
        "start": "npm run start:dev",
        "start:prod": "node dist/main.js",
        "start:dev": "ts-node-esm -T src/main.ts",
        "build": "tsc"
    }
}

----------------------------------------

TITLE: Configuring Pre-Navigation Hooks in JSDOM Crawler
DESCRIPTION: Example showing how to configure pre-navigation hooks to modify Got options before making requests. These hooks allow customization of the HTTP request behavior.

LANGUAGE: javascript
CODE:
preNavigationHooks: [
    (crawlingContext, gotOptions) => {
        // ...
    },
]

----------------------------------------

TITLE: Configuring Apify Token in JavaScript
DESCRIPTION: Example of setting the Apify API token using the Configuration instance in JavaScript.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const sdk = new Actor({ token: 'your_api_token' });

----------------------------------------

TITLE: Selecting Node.js Version in Docker Image
DESCRIPTION: Demonstrates how to specify a Node.js version when using an Apify Docker image.

LANGUAGE: dockerfile
CODE:
# Use Node.js 16
FROM apify/actor-node:16

----------------------------------------

TITLE: HTML Link Structure Example
DESCRIPTION: Example of HTML anchor tag structure that contains links for crawling.

LANGUAGE: html
CODE:
<a href="https://crawlee.dev/js/docs/introduction">This is a link to Crawlee introduction</a>

----------------------------------------

TITLE: Implementing Recursive Web Scraping with PuppeteerCrawler
DESCRIPTION: Implementation of a web crawler using PuppeteerCrawler and RequestQueue to recursively scrape Hacker News. The crawler processes pages using headless Chrome, extracts data, and stores results in a local dataset. It handles pagination by finding and enqueueing links to subsequent pages.

LANGUAGE: javascript
CODE:
import { Dataset, createPuppeteerRouter, PuppeteerCrawler } from 'crawlee';

const router = createPuppeteerRouter();

router.addDefaultHandler(async ({ page, enqueueLinks, log }) => {
    const title = await page.title();
    log.info(`Title of ${page.url()} is '${title}'`);

    await Dataset.pushData({
        title,
        url: page.url(),
    });

    await enqueueLinks({
        globs: ['https://news.ycombinator.com/*'],
        label: 'detail',
    });
});

const crawler = new PuppeteerCrawler({
    requestHandler: router,
    maxRequestsPerCrawl: 20,
});

await crawler.run(['https://news.ycombinator.com']);

----------------------------------------

TITLE: HTML Link Structure Example
DESCRIPTION: Example of HTML anchor tag structure that contains links for crawling.

LANGUAGE: html
CODE:
<a href="https://crawlee.dev/js/docs/introduction">This is a link to Crawlee introduction</a>

----------------------------------------

TITLE: TypeScript Configuration for Crawlee
DESCRIPTION: Required TypeScript configuration for using Crawlee with ES2022 modules.

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist",
        "lib": ["DOM"]
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: AWS Lambda Handler Implementation for Cheerio Crawler
DESCRIPTION: Implementation of AWS Lambda handler function wrapping the Cheerio crawler initialization and execution.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
};

----------------------------------------

TITLE: Dataset Storage Directory Structure
DESCRIPTION: Shows the directory structure pattern used by Crawlee for storing dataset records.

LANGUAGE: plaintext
CODE:
{CRAWLEE_STORAGE_DIR}/datasets/{DATASET_ID}/{INDEX}.json

----------------------------------------

TITLE: Configuring Dockerfile for Crawlee Actor with Node.js and Playwright
DESCRIPTION: This Dockerfile sets up an environment for a Crawlee actor. It uses a base image with Node.js and Playwright, installs NPM packages, copies project files, and specifies the run command. The setup optimizes for build speed and minimal image size.

LANGUAGE: Dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:20

COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD npm start --silent

----------------------------------------

TITLE: Context Handling in Pre-v1 vs v1
DESCRIPTION: Comparison of handler argument handling between pre-v1 and v1 versions, demonstrating the new unified crawling context approach

LANGUAGE: javascript
CODE:
const handlePageFunction = async (args1) => {
    args1.hasOwnProperty('proxyInfo') // true
}

const handleFailedRequestFunction = async (args2) => {
    args2.hasOwnProperty('proxyInfo') // false
}

args1 === args2 // false

LANGUAGE: javascript
CODE:
const handlePageFunction = async (crawlingContext1) => {
    crawlingContext1.hasOwnProperty('proxyInfo') // true
}

const handleFailedRequestFunction = async (crawlingContext2) => {
    crawlingContext2.hasOwnProperty('proxyInfo') // true
}

// All contexts are the same object.
crawlingContext1 === crawlingContext2 // true

----------------------------------------

TITLE: Logging in to Apify Platform
DESCRIPTION: Command to log in to the Apify Platform using the Apify CLI. Requires a personal access token from the Apify console.

LANGUAGE: bash
CODE:
apify login

----------------------------------------

TITLE: Extracting Product SKU with Playwright
DESCRIPTION: Demonstrates how to get the product SKU using a specific class selector.

LANGUAGE: javascript
CODE:
const sku = await page.locator('span.product-meta__sku-number').textContent();

----------------------------------------

TITLE: Installing Apify SDK v1 Dependencies
DESCRIPTION: Commands for installing Apify SDK v1 with either Puppeteer or Playwright as the browser automation library.

LANGUAGE: bash
CODE:
npm install apify puppeteer

LANGUAGE: bash
CODE:
npm install apify playwright

----------------------------------------

TITLE: Request Transform Function
DESCRIPTION: Example of using transformRequestFunction to filter and modify requests before they are enqueued.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
    transformRequestFunction(req) {
        // ignore all links ending with `.pdf`
        if (req.url.endsWith('.pdf')) return false;
        return req;
    },
});

----------------------------------------

TITLE: Saving Data with Dataset.pushData() in Crawlee (JavaScript)
DESCRIPTION: This code demonstrates how to use Dataset.pushData() to save the scraped results. It replaces the console.log() call with Dataset.pushData(), which stores the data in Crawlee's default Dataset.

LANGUAGE: javascript
CODE:
await Dataset.pushData(results);

----------------------------------------

TITLE: Sanity Check with PlaywrightCrawler
DESCRIPTION: A basic crawler setup using PlaywrightCrawler to visit the start URL and print the text content of all category elements on the page.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, log }) {
        const title = await page.title();
        log.info(`Title of ${page.url()} is '${title}'`);

        const categoryElements = await page.$$('.collection-block-item');

        for (const element of categoryElements) {
            const categoryText = await element.textContent();
            await Dataset.pushData({
                categoryText
            });
        }
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Authenticating with Apify Platform
DESCRIPTION: Command to log in to the Apify Platform using the CLI tool

LANGUAGE: bash
CODE:
apify login

----------------------------------------

TITLE: Installing Crawlee with CLI
DESCRIPTION: Commands to install and start a new Crawlee project using the CLI tool

LANGUAGE: bash
CODE:
npx crawlee create my-crawler
cd my-crawler && npm start

----------------------------------------

TITLE: Deploying Crawlee Project to Apify Platform
DESCRIPTION: Command to deploy the Crawlee project to the Apify Platform, creating an Actor.

LANGUAGE: bash
CODE:
apify push

----------------------------------------

TITLE: Selecting Node.js Version in Docker Image
DESCRIPTION: Demonstrates how to specify a Node.js version when using an Apify Docker image.

LANGUAGE: dockerfile
CODE:
# Use Node.js 16
FROM apify/actor-node:16

----------------------------------------

TITLE: Installing Puppeteer Extra Dependencies
DESCRIPTION: Command line instructions for installing the required Puppeteer Extra packages and stealth plugin using npm.

LANGUAGE: bash
CODE:
npm install puppeteer-extra puppeteer-extra-plugin-stealth

----------------------------------------

TITLE: Reduce Method Result Example
DESCRIPTION: Example output from the Dataset reduce method showing the total count of headers across all pages.

LANGUAGE: javascript
CODE:
23

----------------------------------------

TITLE: Installing Puppeteer Extra Dependencies
DESCRIPTION: Command line instructions for installing the required Puppeteer Extra packages and stealth plugin using npm.

LANGUAGE: bash
CODE:
npm install puppeteer-extra puppeteer-extra-plugin-stealth

----------------------------------------

TITLE: Configuring CheerioCrawler for GCP Cloud Functions
DESCRIPTION: Updates the main.js file to use a separate Configuration instance with persistStorage set to false, which is necessary for running in a serverless environment.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Using SessionPool with PuppeteerCrawler
DESCRIPTION: Example showing SessionPool integration with PuppeteerCrawler for headless browser scraping.

LANGUAGE: javascript
CODE:
{PuppeteerSource}

----------------------------------------

TITLE: Interacting with React Calculator using JSDOMCrawler
DESCRIPTION: Example showing how to use JSDOMCrawler to interact with a React calculator application, performing basic arithmetic operations by simulating button clicks and extracting results.

LANGUAGE: typescript
CODE:
{JSDOMCrawlerRunScriptSource}

----------------------------------------

TITLE: Configuring PlaywrightCrawler with AWS Chromium Settings
DESCRIPTION: JavaScript code snippet demonstrating how to set up PlaywrightCrawler with the correct Chromium executable path and arguments for AWS Lambda environment.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
    launchContext: {
        launchOptions: {
             executablePath: await aws_chromium.executablePath(),
             args: aws_chromium.args,
             headless: true
        }
    }
}, new Configuration({
    persistStorage: false,
}));

----------------------------------------

TITLE: Configuring Package.json for Development and Production
DESCRIPTION: Set up npm scripts for development and production environments in package.json.

LANGUAGE: json
CODE:
{
    "scripts": {
        "start:dev": "ts-node-esm -T src/main.ts",
        "start:prod": "node dist/main.js"
    }
}

----------------------------------------

TITLE: Interacting with React Calculator using JSDOMCrawler
DESCRIPTION: Demonstrates how to use JSDOMCrawler to interact with a React calculator application. The script navigates to the calculator, performs button clicks to calculate 1+1, and extracts the result.

LANGUAGE: typescript
CODE:
{JSDOMCrawlerRunScriptSource}

----------------------------------------

TITLE: Docker Image Configurations
DESCRIPTION: Various Docker image configurations for different use cases including Node.js, Puppeteer, and Playwright setups.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-puppeteer-chrome:16

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright:16

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-firefox:16

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-webkit:16

----------------------------------------

TITLE: Crawling All Links with Cheerio Crawler in Crawlee
DESCRIPTION: This snippet demonstrates how to use Cheerio Crawler in Crawlee to crawl all links on a website. It uses the enqueueLinks() method to add new links to the RequestQueue and logs the title of each crawled page.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, enqueueLinks } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks, log }) {
        const title = $('title').text();
        log.info(`Title of ${request.url}: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 20, // Limit to 20 requests
});

// Run the crawler with initial URL
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: URL Pattern Filtering
DESCRIPTION: Example of filtering URLs using glob patterns in enqueueLinks configuration.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
});

----------------------------------------

TITLE: Reduce Method Result Example
DESCRIPTION: Sample output from the Dataset.reduce() method showing the total count of headers across all pages.

LANGUAGE: javascript
CODE:
23

----------------------------------------

TITLE: Using Request Queue with Crawler in Crawlee
DESCRIPTION: Demonstrates how to use a request queue with a crawler in Crawlee. The crawler automatically processes requests from the queue.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        // Process the page (extract data, take page screenshot, etc).
        await enqueueLinks();
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: TypeScript Configuration
DESCRIPTION: TypeScript configuration file setup for Crawlee projects using @apify/tsconfig.

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist",
        "lib": ["DOM"]
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Integrating AWS Chromium with Crawlee
DESCRIPTION: Implementation showing how to configure Crawlee to use the AWS-optimized Chromium browser with proper launch options.

LANGUAGE: javascript
CODE:
// For more information, see https://crawlee.dev/
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
    launchContext: {
        launchOptions: {
             executablePath: await aws_chromium.executablePath(),
             args: aws_chromium.args,
             headless: true
        }
    }
}, new Configuration({
    persistStorage: false,
}));

----------------------------------------

TITLE: Configuring package.json for Crawlee TypeScript Production
DESCRIPTION: JSON configuration for package.json to set up a production start script for a compiled Crawlee TypeScript project.

LANGUAGE: json
CODE:
{
    "scripts": {
        "start:prod": "node dist/main.js"
    }
}

----------------------------------------

TITLE: Configuring LaunchContext in Apify SDK v1
DESCRIPTION: Demonstrates how to use the new launchContext object for configuring browser launch options.

LANGUAGE: javascript
CODE:
const crawler = new Apify.PuppeteerCrawler({
    launchContext: {
        useChrome: true, // Apify option
        launchOptions: {
            headless: false // Puppeteer option
        }
    }
})

----------------------------------------

TITLE: Creating a New Crawlee Project Using CLI
DESCRIPTION: Command to create a new Crawlee project named 'my-crawler' using the Crawlee CLI via npx.

LANGUAGE: bash
CODE:
npx crawlee create my-crawler

----------------------------------------

TITLE: Saving Data with Dataset.pushData() (JavaScript)
DESCRIPTION: This code demonstrates how to save extracted data using the Dataset.pushData() method. It replaces a console.log() call with the data saving function.

LANGUAGE: javascript
CODE:
await Dataset.pushData(results);

----------------------------------------

TITLE: Cleaning Storage in Crawlee
DESCRIPTION: Demonstrates how to clean up default storage directories in Crawlee using the purgeDefaultStorages helper function.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Importing Dataset from Crawlee in TypeScript
DESCRIPTION: This snippet shows how to import the Dataset class from Crawlee along with PlaywrightCrawler.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

----------------------------------------

TITLE: BrowserPool Configuration with Lifecycle Hooks
DESCRIPTION: Configuration example for BrowserPool showing how to use lifecycle hooks to customize browser behavior

LANGUAGE: javascript
CODE:
const crawler = new Apify.PuppeteerCrawler({
    browserPoolOptions: {
        retireBrowserAfterPageCount: 10,
        preLaunchHooks: [
            async (pageId, launchContext) => {
                const { request } = crawler.crawlingContexts.get(pageId);
                if (request.userData.useHeadful === true) {
                    launchContext.launchOptions.headless = false;
                }
            }
        ]
    }
})

----------------------------------------

TITLE: Terminating Crawlee
DESCRIPTION: Keyboard shortcut to stop the crawler execution

LANGUAGE: text
CODE:
CTRL+C

----------------------------------------

TITLE: Docker Multi-stage Build Configuration
DESCRIPTION: Dockerfile configuration using multi-stage build for optimized production deployment.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16 AS builder

COPY . ./
RUN npm install --include=dev \
    && npm run build

FROM apify/actor-node:16
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/dist ./dist

RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional

CMD npm run start:prod

----------------------------------------

TITLE: Configuring Apify Proxy
DESCRIPTION: JavaScript code to set up and use Apify Proxy with specific groups and country

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
    countryCode: 'US',
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Installing and Logging in with Apify CLI
DESCRIPTION: Commands to install Apify CLI globally and log in to your Apify account using an API token

LANGUAGE: bash
CODE:
npm install -g apify-cli\napify login -t YOUR_API_TOKEN

----------------------------------------

TITLE: Capturing Screenshots with PuppeteerCrawler using utils.saveSnapshot()
DESCRIPTION: This snippet shows how to capture screenshots of multiple web pages using PuppeteerCrawler with the context-aware utils.saveSnapshot() utility. It creates a crawler that visits multiple URLs and saves snapshots using the utility function.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, log, saveSnapshot }) {
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        await saveSnapshot();
    },
});

await crawler.run(['https://crawlee.dev', 'https://apify.com']);

----------------------------------------

TITLE: Disabling Browser Fingerprints with PlaywrightCrawler
DESCRIPTION: This code snippet demonstrates how to disable the use of browser fingerprints in PlaywrightCrawler by setting the useFingerprints option to false.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        useFingerprints: false,
    },
    // ... other options
});

----------------------------------------

TITLE: Enqueuing Category Links in Crawlee (TypeScript)
DESCRIPTION: This snippet demonstrates how to use PlaywrightCrawler to crawl category pages of an e-commerce store. It uses the enqueueLinks function with a specific selector to target category links and labels them for later processing.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        // Wait for the category cards to render,
        // otherwise enqueueLinks wouldn't enqueue anything.
        await page.waitForSelector('.collection-block-item');

        // Add links to the queue, but only from
        // elements matching the provided selector.
        await enqueueLinks({
            selector: '.collection-block-item',
            label: 'CATEGORY',
        });
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Custom Selector for enqueueLinks
DESCRIPTION: Example showing how to override the default element selection in enqueueLinks using a custom selector.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    selector: 'div.has-link'
});

----------------------------------------

TITLE: Key-Value Store Operations in Crawlee
DESCRIPTION: Demonstrates basic operations with key-value stores including reading input, writing output, and managing named stores.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';

// Get the INPUT from the default key-value store
const input = await KeyValueStore.getInput();

// Write the OUTPUT to the default key-value store
await KeyValueStore.setValue('OUTPUT', { myResult: 123 });

// Open a named key-value store
const store = await KeyValueStore.open('some-name');

// Write a record to the named key-value store.
// JavaScript object is automatically converted to JSON,
// strings and binary buffers are stored as they are
await store.setValue('some-key', { foo: 'bar' });

// Read a record from the named key-value store.
// Note that JSON is automatically parsed to a JavaScript object,
// text data is returned as a string, and other data is returned as binary buffer
const value = await store.getValue('some-key');

// Delete a record from the named key-value store
await store.setValue('some-key', null);

----------------------------------------

TITLE: Example Dataset Output
DESCRIPTION: Sample JSON output showing the structure of scraped data stored by Crawlee

LANGUAGE: json
CODE:
{
    "url": "https://crawlee.dev/",
    "title": "Crawlee · The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee"
}

----------------------------------------

TITLE: Initializing Basic Proxy Configuration in Crawlee
DESCRIPTION: Basic setup of ProxyConfiguration class with multiple proxy URLs for rotation. Shows how to create a proxy configuration instance and obtain a new proxy URL.

LANGUAGE: javascript
CODE:
import { ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ]
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Configuring package.json for TypeScript Build in Crawlee Project
DESCRIPTION: Add build script and main entry point in package.json for a TypeScript-based Crawlee project.

LANGUAGE: json
CODE:
{
    "scripts": {
        "build": "tsc"
    },
    "main": "dist/main.js"
}

----------------------------------------

TITLE: Using Request List with Crawler in Crawlee
DESCRIPTION: Shows how to create and use a request list with a Crawlee crawler. This is useful for processing a known set of URLs without dynamic additions.

LANGUAGE: javascript
CODE:
import { RequestList, PuppeteerCrawler } from 'crawlee';

const sources = [
    { url: 'http://www.example.com/page-1' },
    { url: 'http://www.example.com/page-2' },
    { url: 'http://www.example.com/page-3' },
];

const requestList = await RequestList.open('my-list', sources);

const crawler = new PuppeteerCrawler({
    requestList,
    async requestHandler({ page, request }) {
        // Process the page (extract data, take page screenshot, etc).
        // No more requests could be added to the request list here
    },
});

----------------------------------------

TITLE: Playwright Browser-Specific Configurations
DESCRIPTION: Configures Docker images for specific browsers in Playwright (Chrome, Firefox, WebKit).

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-firefox:16

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-webkit:16

----------------------------------------

TITLE: Custom Configuration Implementation
DESCRIPTION: Example demonstrating how to create and use a custom Configuration instance with specific parameters for a crawler.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

const config = new Configuration({
    persistStateIntervalMillis: 10_000,
});

const crawler = new CheerioCrawler({}, config);

crawler.router.addDefaultHandler(async ({ request }) => {
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Custom Selector for enqueueLinks
DESCRIPTION: Example showing how to override the default element selection in enqueueLinks using a custom selector.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    selector: 'div.has-link'
});

----------------------------------------

TITLE: Configuring Crawlee using crawlee.json
DESCRIPTION: This snippet shows how to use a crawlee.json file to set configuration options for Crawlee. It demonstrates setting the persistStateIntervalMillis and logLevel options.

LANGUAGE: json
CODE:
{
  "persistStateIntervalMillis": 10000,
  "logLevel": "DEBUG"
}

----------------------------------------

TITLE: Purging Default Storages in Crawlee
DESCRIPTION: Shows how to explicitly purge default storages in Crawlee using the purgeDefaultStorages() helper function.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Running Headful Browser with PuppeteerCrawler
DESCRIPTION: Example of running a headful (visible) browser using PuppeteerCrawler. It demonstrates how to configure the crawler to run in headful mode and includes a delay for demonstration purposes.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    headless: false,
    requestHandler: async ({ page, request }) => {
        await page.waitForTimeout(5000);
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Development Environment Setup
DESCRIPTION: Installing and configuring ts-node for development workflow

LANGUAGE: shell
CODE:
npm install --save-dev ts-node

----------------------------------------

TITLE: Configuring Pre-release Docker Image
DESCRIPTION: Examples of using pre-release beta versions of Docker images with and without library version specification.

LANGUAGE: dockerfile
CODE:
# Without library version.
FROM apify/actor-node:16-beta

LANGUAGE: dockerfile
CODE:
# With library version.
FROM apify/actor-node-playwright-chrome:16-1.10.0-beta

----------------------------------------

TITLE: Advanced Autoscaled Pool Configuration in CheerioCrawler
DESCRIPTION: Demonstrates advanced configuration options for the autoscaled pool in CheerioCrawler. These settings allow for fine-grained control over how the crawler scales its operations based on system resources and performance.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    autoscaledPoolOptions: {
        desiredConcurrency: 50,
        desiredConcurrencyRatio: 0.9,
        scaleUpStepRatio: 0.05,
        scaleDownStepRatio: 0.05,
        maybeRunIntervalSecs: 0.5,
        loggingIntervalSecs: 60,
        autoscaleIntervalSecs: 10,
        maxTasksPerMinute: 60,
    },
    // ...
});

----------------------------------------

TITLE: TypeScript Configuration for Crawlee
DESCRIPTION: TypeScript configuration file setup for Crawlee projects with recommended settings

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist",
        "lib": ["DOM"]
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Demonstrating Key-Value Store Operations in Crawlee
DESCRIPTION: This snippet shows basic operations of key-value stores in Crawlee, including getting input, setting output, opening a named store, and performing read/write operations.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';

// Get the INPUT from the default key-value store
const input = await KeyValueStore.getInput();

// Write the OUTPUT to the default key-value store
await KeyValueStore.setValue('OUTPUT', { myResult: 123 });

// Open a named key-value store
const store = await KeyValueStore.open('some-name');

// Write a record to the named key-value store.
// JavaScript object is automatically converted to JSON,
// strings and binary buffers are stored as they are
await store.setValue('some-key', { foo: 'bar' });

// Read a record from the named key-value store.
// Note that JSON is automatically parsed to a JavaScript object,
// text data is returned as a string, and other data is returned as binary buffer
const value = await store.getValue('some-key');

// Delete a record from the named key-value store
await store.setValue('some-key', null);

----------------------------------------

TITLE: Importing Dataset Module in Crawlee (TypeScript)
DESCRIPTION: This snippet shows how to import the Dataset module along with PlaywrightCrawler from Crawlee. These imports are necessary for saving extracted data.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

----------------------------------------

TITLE: Key-Value Store Operations in Crawlee
DESCRIPTION: Demonstrates basic operations with key-value stores including reading input, writing output, and managing named stores.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';

// Get the INPUT from the default key-value store
const input = await KeyValueStore.getInput();

// Write the OUTPUT to the default key-value store
await KeyValueStore.setValue('OUTPUT', { myResult: 123 });

// Open a named key-value store
const store = await KeyValueStore.open('some-name');

// Write a record to the named key-value store.
// JavaScript object is automatically converted to JSON,
// strings and binary buffers are stored as they are
await store.setValue('some-key', { foo: 'bar' });

// Read a record from the named key-value store.
// Note that JSON is automatically parsed to a JavaScript object,
// text data is returned as a string, and other data is returned as binary buffer
const value = await store.getValue('some-key');

// Delete a record from the named key-value store
await store.setValue('some-key', null);

----------------------------------------

TITLE: Creating and Running an Actor Locally
DESCRIPTION: Commands to create and run a new actor project using Apify CLI

LANGUAGE: bash
CODE:
apify create my-hello-world
cd my-hello-world
apify run

----------------------------------------

TITLE: Crawling Context Migration in JavaScript
DESCRIPTION: Example showing the transition from separate handler arguments to unified crawling context in SDK v1

LANGUAGE: javascript
CODE:
const handlePageFunction = async (crawlingContext1) => {
    crawlingContext1.hasOwnProperty('proxyInfo') // true
}

const handleFailedRequestFunction = async (crawlingContext2) => {
    crawlingContext2.hasOwnProperty('proxyInfo') // true
}

// All contexts are the same object.
crawlingContext1 === crawlingContext2 // true

----------------------------------------

TITLE: Specifying Node.js Version in Dockerfile
DESCRIPTION: Example of how to specify a Node.js version when using an Apify Docker image.

LANGUAGE: dockerfile
CODE:
# Use Node.js 20
FROM apify/actor-node:20

----------------------------------------

TITLE: Configuring package.json for Production Script in Crawlee
DESCRIPTION: JSON configuration in package.json to add a production script for running compiled JavaScript code in a Crawlee project.

LANGUAGE: json
CODE:
{
    "scripts": {
        "start:prod": "node dist/main.js"
    }
}

----------------------------------------

TITLE: Configuring CheerioCrawler with Keep-Alive
DESCRIPTION: Creation of a CheerioCrawler instance with keepAlive option enabled to maintain the crawler running for incoming requests. The crawler extracts page titles from requested URLs.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, log } from 'crawlee';

const crawler = new CheerioCrawler({
    keepAlive: true,
    requestHandler: async ({ request, $ }) => {
        const title = $('title').text();
        // We will send the response here later
        log.info(`Page title: ${title} on ${request.url}`);
    },
});

----------------------------------------

TITLE: Main Scraper Implementation
DESCRIPTION: Main function that handles URL encoding and crawler initialization for LinkedIn job scraping

LANGUAGE: python
CODE:
from crawlee.playwright_crawler import PlaywrightCrawler
from .routes import router
import urllib.parse

async def main(title: str, location: str, data_name: str) -> None:
    base_url = "https://www.linkedin.com/jobs/search"

    # URL encode the parameters
    params = {
        "keywords": title,
        "location": location,
        "trk": "public_jobs_jobs-search-bar_search-submit",
        "position": "1",
        "pageNum": "0"
    }

    encoded_params = urlencode(params)

    # Encode parameters into a query string
    query_string = '?' + encoded_params

    # Combine base URL with the encoded query string
    encoded_url = urljoin(base_url, "") + query_string

    # Initialize the crawler
    crawler = PlaywrightCrawler(
        request_handler=router,
    )

    # Run the crawler with the initial list of URLs
    await crawler.run([encoded_url])

    # Save the data in a CSV file
    output_file = f"{data_name}.csv"
    await crawler.export_data(output_file)

----------------------------------------

TITLE: Using Request Queue with Crawler in Crawlee
DESCRIPTION: Demonstrates how to use the Request Queue implicitly with a Crawler in Crawlee. The crawler automatically processes requests from the queue.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        // Add new requests to the queue
        await enqueueLinks();

        // Process the page (extract data, take page screenshot, etc).
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Disabling Browser Fingerprints with PuppeteerCrawler
DESCRIPTION: This snippet shows how to disable the use of browser fingerprints in PuppeteerCrawler by setting the useFingerprints option to false in the browserPoolOptions.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    // ...
    browserPoolOptions: {
        useFingerprints: false,
    },
});

await crawler.run();

----------------------------------------

TITLE: Automating GitHub Repository Search Forms with PuppeteerCrawler
DESCRIPTION: Example showing how to use PuppeteerCrawler to automate GitHub repository search by filling form fields including search term, owner, date, and language. The code demonstrates form interaction, submission, and result extraction with results saved to either Apify platform dataset or local storage.

LANGUAGE: typescript
CODE:
import { PuppeteerCrawler, Dataset } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page }) {
        // Navigate to the GitHub advanced search page
        await page.goto('https://github.com/search/advanced');

        // Fill in the search form with your desired inputs
        await page.type('#adv_code_search input.js-advanced-search-input', 'language');
        await page.type('#search_from', 'apify');
        await page.type('#search_date', '>2015-01-01');
        await page.select('select#search_language', 'JavaScript');

        // Submit the form
        await page.click('#adv_code_search button[type="submit"]');

        // Wait for the results
        await page.waitForSelector('.repo-list-item');

        // Extract the data
        const repositories = await page.$$eval('.repo-list-item', (elements) => {
            return elements.map((el) => ({
                title: el.querySelector('a').textContent.trim(),
                description: el.querySelector('p')?.textContent?.trim(),
                url: el.querySelector('a').href,
            }));
        });

        // Save the results
        await Dataset.pushData(repositories);
    },
});

await crawler.run(['https://github.com/search/advanced']);

----------------------------------------

TITLE: Using actor-node-playwright Docker Image
DESCRIPTION: Example of using the Apify Docker image that includes all Playwright browsers, suitable for development and testing with multiple browsers.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright:20

----------------------------------------

TITLE: Automating GitHub Repository Search Forms with PuppeteerCrawler
DESCRIPTION: Example showing how to use PuppeteerCrawler to automate GitHub repository search by filling form fields including search term, owner, date, and language. The code demonstrates form interaction, submission, and result extraction with results saved to either Apify platform dataset or local storage.

LANGUAGE: typescript
CODE:
import { PuppeteerCrawler, Dataset } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page }) {
        // Navigate to the GitHub advanced search page
        await page.goto('https://github.com/search/advanced');

        // Fill in the search form with your desired inputs
        await page.type('#adv_code_search input.js-advanced-search-input', 'language');
        await page.type('#search_from', 'apify');
        await page.type('#search_date', '>2015-01-01');
        await page.select('select#search_language', 'JavaScript');

        // Submit the form
        await page.click('#adv_code_search button[type="submit"]');

        // Wait for the results
        await page.waitForSelector('.repo-list-item');

        // Extract the data
        const repositories = await page.$$eval('.repo-list-item', (elements) => {
            return elements.map((el) => ({
                title: el.querySelector('a').textContent.trim(),
                description: el.querySelector('p')?.textContent?.trim(),
                url: el.querySelector('a').href,
            }));
        });

        // Save the results
        await Dataset.pushData(repositories);
    },
});

await crawler.run(['https://github.com/search/advanced']);

----------------------------------------

TITLE: Using actor-node-playwright-webkit Docker Image
DESCRIPTION: Example of how to use the Apify Docker image that includes Playwright and WebKit.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-webkit:16

----------------------------------------

TITLE: Checking NPM Version in Bash
DESCRIPTION: Command to verify the installed version of NPM (Node Package Manager).

LANGUAGE: bash
CODE:
npm -v

----------------------------------------

TITLE: Playwright Multi-Browser Image Configuration
DESCRIPTION: Configures Docker image with support for all Playwright browsers (Chromium, Chrome, Firefox, WebKit).

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright:16

----------------------------------------

TITLE: Scraping with PlaywrightCrawler (No Wait - Unsuccessful)
DESCRIPTION: This snippet shows an unsuccessful attempt to scrape JavaScript-rendered content using PlaywrightCrawler without waiting for elements to render, resulting in an error.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        // Try to extract text content of the first actor card without waiting
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Using enqueueLinks for Custom Link Selection in Crawlee
DESCRIPTION: Shows how to use the enqueueLinks function with a custom selector to find specific links on a page.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    selector: 'div.has-link'
});

----------------------------------------

TITLE: Crawling All Links with Puppeteer Crawler in TypeScript
DESCRIPTION: This snippet shows how to use Puppeteer Crawler to crawl all links on a website. It sets up a PuppeteerCrawler, uses the enqueueLinks() method to add new links to the queue, and extracts the page title using Puppeteer's API.

LANGUAGE: typescript
CODE:
import { PuppeteerCrawler, enqueueLinks } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limit to 10 requests
});

// Run the crawler
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Crawling All Links with Puppeteer Crawler in TypeScript
DESCRIPTION: This snippet shows how to use Puppeteer Crawler to crawl all links on a website. It sets up a PuppeteerCrawler, uses the enqueueLinks() method to add new links to the queue, and extracts the page title using Puppeteer's API.

LANGUAGE: typescript
CODE:
import { PuppeteerCrawler, enqueueLinks } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limit to 10 requests
});

// Run the crawler
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Building Crawlee Actor Docker Image
DESCRIPTION: This Dockerfile creates a Docker image for a Crawlee actor using Node.js and Playwright. It utilizes a multi-stage build process to optimize the final image size and includes only the necessary dependencies and built files.

LANGUAGE: Dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16 AS builder

COPY --chown=myuser package*.json ./

RUN npm install --include=dev --audit=false

COPY --chown=myuser . ./

RUN npm run build

FROM apify/actor-node-playwright-chrome:16

COPY --from=builder --chown=myuser /home/myuser/dist ./dist

COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent

----------------------------------------

TITLE: Basic Node.js Docker Image Configuration
DESCRIPTION: Basic Dockerfile configuration for Node.js 16 environment without browsers.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16

----------------------------------------

TITLE: Request Transformation Function
DESCRIPTION: Example of using transformRequestFunction to filter and modify requests before they are enqueued.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
    transformRequestFunction(req) {
        // ignore all links ending with `.pdf`
        if (req.url.endsWith('.pdf')) return false;
        return req;
    },
});

----------------------------------------

TITLE: Configuring CheerioCrawler for GCP Cloud Functions
DESCRIPTION: Update the main.js file to use a separate Configuration instance with persistStorage set to false, which is necessary for running in a serverless environment.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Example URL Structure for Pagination
DESCRIPTION: Example of how pagination URLs are structured in the warehouse store

LANGUAGE: text
CODE:
https://warehouse-theme-metal.myshopify.com/collections/headphones?page=2

----------------------------------------

TITLE: Storage URL Generation
DESCRIPTION: Example of generating a public URL for items stored in Key-Value Store

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'apify';

const store = await KeyValueStore.open();
await store.setValue('your-file', { foo: 'bar' });
const url = store.getPublicUrl('your-file');
// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file

----------------------------------------

TITLE: Extracting Manufacturer from URL in JavaScript
DESCRIPTION: This snippet demonstrates how to extract the manufacturer name from a product URL by splitting the string.

LANGUAGE: javascript
CODE:
// request.url = https://warehouse-theme-metal.myshopify.com/products/sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440

const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']
const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'

----------------------------------------

TITLE: Using Request Queue with Crawler in Crawlee
DESCRIPTION: Demonstrates how to use a Request Queue with a Crawler in Crawlee. It shows how to add initial URLs to the queue and process them using a PuppeteerCrawler.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        // Add all links from page to request queue
        await enqueueLinks();

        // Process the page (extract data, take page screenshot, etc.)
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Upgrading Crawlee using pip
DESCRIPTION: Command to upgrade Crawlee to the latest version from PyPI using pip.

LANGUAGE: shell
CODE:
pip install --upgrade crawlee

----------------------------------------

TITLE: Using Apify Platform Storage in JavaScript
DESCRIPTION: Example of getting a public URL for a stored item in Apify Key-Value Store.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'apify';

const store = await KeyValueStore.open();
await store.setValue('your-file', { foo: 'bar' });
const url = store.getPublicUrl('your-file');
// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file

----------------------------------------

TITLE: Using a Custom Request Queue with Locking in Crawlers
DESCRIPTION: This snippet illustrates how to use a custom request queue that supports locking in a crawler. It shows creating the queue, enabling the experiment in the crawler options, and passing the queue to the crawler.

LANGUAGE: typescript
CODE:
import { CheerioCrawler, RequestQueueV2 } from 'crawlee';

const queue = await RequestQueueV2.open('my-locking-queue');

const crawler = new CheerioCrawler({
    experiments: {
        requestLocking: true,
    },
    requestQueue: queue,
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

await crawler.run();

----------------------------------------

TITLE: Scraping Netflix Genres and Shows with Cheerio
DESCRIPTION: JavaScript code using Cheerio to parse HTML and extract genre and show information from Netflix.

LANGUAGE: javascript
CODE:
const $ = await parseWithCheerio();

const data = $('[data-uia="collections-row"]')
  .map((_, el) => {
    const genre = $(el)
      .find('[data-uia="collections-row-title"]')
      .text()
      .trim();
    const items = $(el)
      .find('[data-uia="collections-title"]')
      .map((_, itemEl) => $(itemEl).text().trim())
      .get();
    return { genre, items };
  })
  .get();

const genres = data.map((d) => d.genre);
const shows = data.map((d) => d.items);

----------------------------------------

TITLE: Changelog Version Link Definition
DESCRIPTION: Markdown version link comparing differences between release versions on GitHub

LANGUAGE: markdown
CODE:
# [3.13.0](https://github.com/apify/crawlee/compare/v3.12.2...v3.13.0) (2025-03-04)

----------------------------------------

TITLE: Skipping Navigation for Image Requests with PlaywrightCrawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use PlaywrightCrawler to crawl a website, skip navigation for image requests, and save them directly to a key-value store. It uses the Request#skipNavigation option and sendRequest method to efficiently handle CDN-delivered images.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, KeyValueStore } from 'crawlee';
import { parseHTML } from 'linkedom';

const crawler = new PlaywrightCrawler({
    async requestHandler({ request, page, sendRequest, log }) {
        const title = await page.title();
        log.info(`Title of ${request.loadedUrl} is '${title}'`);

        const html = await page.content();
        const { document } = parseHTML(html);

        const imageUrls = Array.from(document.querySelectorAll('img'))
            .map((img) => img.src)
            .filter(Boolean);

        for (const imageUrl of imageUrls) {
            await crawler.addRequests([{
                url: imageUrl,
                label: 'IMAGE',
                skipNavigation: true, // This is the important part
            }]);
        }
    },

    async failedRequestHandler({ request, log }) {
        log.error(`Request ${request.url} failed`);
    },
});

crawler.router.addHandler('IMAGE', async ({ request, sendRequest, log }) => {
    const response = await sendRequest();
    const buffer = await response.buffer();

    log.info(`Saving image ${request.url} to key-value store`);
    await KeyValueStore.setValue(request.url, buffer, { contentType: response.headers['content-type'] });
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Session Management with PuppeteerCrawler
DESCRIPTION: Implementation of session management using PuppeteerCrawler. Shows how to configure and use SessionPool with Puppeteer for browser automation with session handling.

LANGUAGE: javascript
CODE:
{PuppeteerSource}

----------------------------------------

TITLE: Selecting Node.js Version in Dockerfile
DESCRIPTION: This snippet demonstrates how to specify a particular Node.js version when using an Apify Docker image.

LANGUAGE: dockerfile
CODE:
# Use Node.js 16
FROM apify/actor-node:16

----------------------------------------

TITLE: Dockerfile for Playwright-based Crawlee Project
DESCRIPTION: Example Dockerfile for a Crawlee project using Playwright with all supported browsers.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright:16

----------------------------------------

TITLE: Using actor-node-playwright-chrome Docker Image
DESCRIPTION: Example of how to use the Apify Docker image that includes Playwright and Chrome, suitable for CheerioCrawler and PlaywrightCrawler.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16

----------------------------------------

TITLE: Finding All Links on a Page with JSDOMCrawler
DESCRIPTION: This snippet demonstrates how to use the Element API to find all <a> elements with an href attribute on a page and extract their URLs into an array. It utilizes querySelector and map functions.

LANGUAGE: javascript
CODE:
Array.from(document.querySelectorAll('a[href]')).map((a) => a.href);

----------------------------------------

TITLE: Docker Configuration for Crawlee
DESCRIPTION: Multi-stage Dockerfile setup for building and running Crawlee projects

LANGUAGE: dockerfile
CODE:
# using multistage build, as we need dev deps to build the TS source code
FROM apify/actor-node:16 AS builder

# copy all files, install all dependencies (including dev deps) and build the project
COPY . ./
RUN npm install --include=dev \
    && npm run build

# create final image
FROM apify/actor-node:16
# copy only necessary files
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/README.md ./
COPY --from=builder /usr/src/app/dist ./dist
COPY --from=builder /usr/src/app/apify.json ./apify.json
COPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json

# install only prod deps
RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional \
    && echo "Installed NPM packages:" \
    && (npm list --only=prod --no-optional --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

# run compiled code
CMD npm run start:prod

----------------------------------------

TITLE: Package.json Browser Version Configuration
DESCRIPTION: Package.json configuration showing how to properly specify browser automation library versions.

LANGUAGE: json
CODE:
{
    "dependencies": {
        "crawlee": "^3.0.0",
        "playwright": "*"
    }
}

----------------------------------------

TITLE: Setting Maximum Requests Per Minute in Crawlee
DESCRIPTION: This snippet demonstrates how to set the maximum number of requests per minute for a CheerioCrawler in Crawlee. It limits the crawler to 120 requests per minute.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    // other options
    maxRequestsPerMinute: 120,
});

----------------------------------------

TITLE: Configuring Crawlee using custom Configuration in JavaScript
DESCRIPTION: This example demonstrates how to create a custom Configuration instance and pass it to a CheerioCrawler. It sets the persistStateIntervalMillis to 10000 and shows how the crawler uses this configuration.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Create new configuration
const config = new Configuration({
    // Set the 'persistStateIntervalMillis' option to 10 seconds
    persistStateIntervalMillis: 10_000,
});

// Now we need to pass the configuration to the crawler
const crawler = new CheerioCrawler({}, config);

crawler.router.addDefaultHandler(async ({ request }) => {
    // for the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // for the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Importing Crawlers in Crawlee v0.5
DESCRIPTION: Demonstrates the new import structure for crawler classes in Crawlee v0.5, showing the difference from previous versions.

LANGUAGE: python
CODE:
- from crawlee.beautifulsoup_crawler import BeautifulSoupCrawler, BeautifulSoupCrawlingContext
+ from crawlee.crawlers import BeautifulSoupCrawler, BeautifulSoupCrawlingContext

----------------------------------------

TITLE: Extracting Product SKU using Playwright
DESCRIPTION: Demonstrates extracting a product SKU from a span element with a specific class using Playwright.

LANGUAGE: javascript
CODE:
const sku = await page.locator('span.product-meta__sku-number').textContent();

----------------------------------------

TITLE: Using actor-node-playwright Docker Image
DESCRIPTION: This snippet shows how to use the Apify Docker image that includes all Playwright browsers, suitable for development or testing with multiple browsers.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright:16

----------------------------------------

TITLE: Initializing RequestQueue in Crawlee
DESCRIPTION: Shows how to create a RequestQueue instance and add a URL request to it using Crawlee's RequestQueue class.

LANGUAGE: typescript
CODE:
import { RequestQueue } from 'crawlee';

// First you create the request queue instance.
const requestQueue = await RequestQueue.open();
// And then you add one or more requests to it.
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

----------------------------------------

TITLE: Cookie Jar Implementation
DESCRIPTION: Shows how to implement custom cookie handling using the tough-cookie package with BasicCrawler.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';
import { CookieJar } from 'tough-cookie';

const cookieJar = new CookieJar();

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ cookieJar });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Sanity Check Crawler with Playwright
DESCRIPTION: Creates a basic Playwright crawler to visit the start URL and print the text content of all category elements on the page.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, log }) {
        const title = await page.title();
        log.info(`Title of ${page.url()} is '${title}'`);

        const categoryElements = await page.$$('.collection-block-item');
        for (const element of categoryElements) {
            const text = await element.textContent();
            console.log(text);
        }
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Enabling Request Locking in CheerioCrawler
DESCRIPTION: Example showing how to enable request locking experiment in a CheerioCrawler instance. The crawler will process URLs with request locking enabled.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    experiments: {
        requestLocking: true,
    },
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Crawlee Results Output Format
DESCRIPTION: Example of the JSON output format that Crawlee produces when storing crawling results

LANGUAGE: json
CODE:
{
    "url": "https://crawlee.dev/",
    "title": "Crawlee · The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee"
}

----------------------------------------

TITLE: Adding Requests in Batches to Request Queue in Crawlee
DESCRIPTION: Demonstrates how to add multiple requests in batches to a request queue using the addRequests() function in Crawlee.

LANGUAGE: javascript
CODE:
import { RequestQueue, PuppeteerCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();

// Add multiple requests to the queue
await requestQueue.addRequests([
    { url: 'http://example.com/1' },
    { url: 'http://example.com/2' },
    { url: 'http://example.com/3' },
    { url: 'http://example.com/4' },
    { url: 'http://example.com/5' },
]);

const crawler = new PuppeteerCrawler({
    requestQueue,
    async requestHandler({ request }) {
        console.log(request.url);
    },
});

await crawler.run();

----------------------------------------

TITLE: Crawlee Execution Log Output
DESCRIPTION: Example log output showing crawler visiting and processing various pages

LANGUAGE: log
CODE:
INFO  PlaywrightCrawler: Starting the crawl
INFO  PlaywrightCrawler: Title of https://crawlee.dev/ is 'Crawlee · Build reliable crawlers. Fast. | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core is '@crawlee/core | API | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/api/core/changelog is 'Changelog | API | Crawlee'
INFO  PlaywrightCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'

----------------------------------------

TITLE: Basic Link Enqueuing with Crawlee
DESCRIPTION: Simple example of using enqueueLinks function without parameters

LANGUAGE: javascript
CODE:
await enqueueLinks();

----------------------------------------

TITLE: Crawling Multiple URLs with Playwright Crawler in JavaScript
DESCRIPTION: This code snippet illustrates how to use Playwright Crawler to crawl a list of specified URLs. It captures screenshots, extracts the title and URL from each page, and stores the results.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url} is '${title}'`);

        await Dataset.pushData({
            title,
            url: request.url,
            screenshot: await page.screenshot(),
        });
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run([
    'https://crawlee.dev',
    'https://apify.com',
]);

----------------------------------------

TITLE: Crawling All Links with CheerioCrawler in Crawlee
DESCRIPTION: This snippet demonstrates how to use CheerioCrawler to crawl all links found on a website, regardless of their domain. It uses the 'all' strategy for enqueueLinks().

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks, log }) {
        log.info('Enqueueing all links.');
        await enqueueLinks({
            strategy: EnqueueStrategy.All,
            // You can also use the string 'all'
            // strategy: 'all',
        });
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Scraping with CheerioCrawler (JavaScript)
DESCRIPTION: This snippet demonstrates an attempt to scrape JavaScript-rendered content using CheerioCrawler, which fails because it can't execute client-side JavaScript.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        // Extract text content of an actor card
        const actorText = $('.ActorStoreItem').text();
        console.log(`ACTOR: ${actorText}`);
    }
})

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Using Local and Remote Datasets with Apify
DESCRIPTION: JavaScript code demonstrating how to use both local and remote datasets when working with Apify.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';
import { Dataset } from 'crawlee';

// or Dataset.open('my-local-data')
const localDataset = await Actor.openDataset('my-local-data');
// but here we need the `Actor` class
const remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });

----------------------------------------

TITLE: Installing Playwright-Extra Dependencies
DESCRIPTION: Command to install playwright-extra and puppeteer stealth plugin using npm package manager.

LANGUAGE: bash
CODE:
npm install playwright-extra puppeteer-extra-plugin-stealth

----------------------------------------

TITLE: Using SessionPool with BasicCrawler
DESCRIPTION: Demonstrates how to use SessionPool with BasicCrawler for managing proxy rotations and session-specific settings.

LANGUAGE: javascript
CODE:
{BasicSource}

----------------------------------------

TITLE: Multi-stage Dockerfile Build for Crawlee Actor
DESCRIPTION: Configures a Docker build process with two stages: a builder stage for compilation and a final stage for production. Optimizes the build process using layer caching and minimizes the final image size by excluding development dependencies.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:20 AS builder

COPY package*.json ./

RUN npm install --include=dev --audit=false

COPY . ./

RUN npm run build

FROM apify/actor-node:20

COPY --from=builder /usr/src/app/dist ./dist

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm run start:prod --silent

----------------------------------------

TITLE: Sample Dataset Structure in JSON
DESCRIPTION: Example dataset structure showing scraped URLs and their heading counts stored in the project's default dataset directory.

LANGUAGE: json
CODE:
[
    {
        "url": "https://crawlee.dev/",
        "headingCount": 11
    },
    {
        "url": "https://crawlee.dev/storage",
        "headingCount": 8
    },
    {
        "url": "https://crawlee.dev/proxy",
        "headingCount": 4
    }
]

----------------------------------------

TITLE: Configuring Parallel Scraper Worker Process
DESCRIPTION: Setup code for worker processes including storage configuration, request locking, and communication handling

LANGUAGE: javascript
CODE:
Configuration.set('purgeOnStart', false);

const requestQueue = await getOrInitQueue(false);

const config = new Configuration({
    storageClientOptions: {
        localDataDirectory: `./storage/worker-${process.env.WORKER_INDEX}`,
    },
});

----------------------------------------

TITLE: Browser Pool Configuration and Lifecycle Hooks
DESCRIPTION: Example showing how to configure browser pool options and use lifecycle hooks in crawler initialization.

LANGUAGE: javascript
CODE:
const crawler = new Apify.PuppeteerCrawler({
    browserPoolOptions: {
        retireBrowserAfterPageCount: 10,
        preLaunchHooks: [
            async (pageId, launchContext) => {
                const { request } = crawler.crawlingContexts.get(pageId);
                if (request.userData.useHeadful === true) {
                    launchContext.launchOptions.headless = false;
                }
            }
        ]
    }
})

----------------------------------------

TITLE: Capturing Screenshot with Puppeteer using utils.puppeteer.saveSnapshot()
DESCRIPTION: This snippet shows how to capture a screenshot using the utils.puppeteer.saveSnapshot() method. It launches a browser, creates a new page, navigates to a URL, and saves a snapshot using the utility function.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';
import puppeteer from 'puppeteer';
import { utils } from '@crawlee/puppeteer';

const browser = await puppeteer.launch();
const page = await browser.newPage();
await page.goto('https://crawlee.dev');

await utils.puppeteer.saveSnapshot(page, { key: 'my-screenshot' });

await browser.close();

----------------------------------------

TITLE: Specifying Dependency Versions in package.json
DESCRIPTION: Demonstrates the recommended way to specify dependency versions in package.json when using Apify Docker images, using an asterisk for pre-installed libraries.

LANGUAGE: json
CODE:
{
    "dependencies": {
        "crawlee": "^3.0.0",
        "playwright": "*"
    }
}

----------------------------------------

TITLE: Standalone SessionPool Usage in Crawlee
DESCRIPTION: This snippet demonstrates how to use SessionPool standalone in Crawlee without a specific crawler. It shows manual session creation, rotation, and management for custom implementations.

LANGUAGE: javascript
CODE:
{StandaloneSource}

----------------------------------------

TITLE: Version Number Example
DESCRIPTION: Example of a specific beta version number format.

LANGUAGE: markdown
CODE:
0.15.1-beta.3

----------------------------------------

TITLE: Sample Dataset Structure in JSON
DESCRIPTION: Example dataset structure showing scraped URLs and their heading counts stored in the project's default dataset directory.

LANGUAGE: json
CODE:
[
    {
        "url": "https://crawlee.dev/",
        "headingCount": 11
    },
    {
        "url": "https://crawlee.dev/storage",
        "headingCount": 8
    },
    {
        "url": "https://crawlee.dev/proxy",
        "headingCount": 4
    }
]

----------------------------------------

TITLE: HTML Link Example
DESCRIPTION: Example of an HTML anchor tag with href attribute that represents a typical web link.

LANGUAGE: html
CODE:
<a href="https://crawlee.dev/js/docs/introduction">This is a link to Crawlee introduction</a>

----------------------------------------

TITLE: Crawlee Results Data Structure
DESCRIPTION: Example of the JSON output format stored by Crawlee in the storage directory

LANGUAGE: json
CODE:
{
    "url": "https://crawlee.dev/",
    "title": "Crawlee · The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee"
}

----------------------------------------

TITLE: Installing Crawlee CLI and Project Setup
DESCRIPTION: Commands to install Crawlee using the CLI tool and create a new crawler project

LANGUAGE: bash
CODE:
npx crawlee create my-crawler
cd my-crawler && npm start

----------------------------------------

TITLE: Initializing Apify Project Configuration
DESCRIPTION: Command to initialize the Apify project configuration, creating necessary files for deployment.

LANGUAGE: bash
CODE:
apify init

----------------------------------------

TITLE: Docker Multi-stage Build Configuration
DESCRIPTION: Dockerfile setup for building TypeScript project with development dependencies and creating optimized production image

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16 AS builder

COPY . ./
RUN npm install --include=dev \
    && npm run build

FROM apify/actor-node:16
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/dist ./dist

RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional

CMD npm run start:prod

----------------------------------------

TITLE: Cross-Context Access Example
DESCRIPTION: Demonstrates how to access and manipulate data across different crawling contexts using context IDs.

LANGUAGE: javascript
CODE:
let masterContextId;
const handlePageFunction = async ({ id, page, request, crawler }) => {
    if (request.userData.masterPage) {
        masterContextId = id;
        // Prepare the master page.
    } else {
        const masterContext = crawler.crawlingContexts.get(masterContextId);
        const masterPage = masterContext.page;
        const masterRequest = masterContext.request;
        // Now we can manipulate the master data from another handlePageFunction.
    }
}

----------------------------------------

TITLE: Automating GitHub Repository Search with PuppeteerCrawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use PuppeteerCrawler to automate a GitHub repository search. It fills out a search form with specified criteria, submits it, extracts the search results, and saves them to a dataset. The example uses Puppeteer for browser automation and Crawlee's API for crawling and data management.

LANGUAGE: javascript
CODE:
import { Dataset, PuppeteerCrawler } from 'crawlee';

// Create an instance of the PuppeteerCrawler class - a crawler
// that automatically loads the URLs in headless Chrome / Puppeteer.
const crawler = new PuppeteerCrawler({
    // Here you can set options that are passed to the launchPuppeteer() function.
    launchContext: {
        launchOptions: {
            headless: true,
            // Other Puppeteer options
        },
    },

    // Stop crawling after several pages
    maxRequestsPerCrawl: 10,

    // This function will be called for each URL to crawl.
    // Here you can write the Puppeteer scripts you are familiar with,
    // with the exception that browsers and pages are automatically managed by the Apify SDK.
    // The function accepts a single parameter, which is an object with the following fields:
    // - request: an instance of the Request class with information such as URL and HTTP method
    // - page: Puppeteer's Page object (see https://pptr.dev/#show=api-class-page)
    async requestHandler({ request, page, log }) {
        log.info(`Processing ${request.url}...`);

        // Fill in the search form and submit it.
        await page.type('#search_form_input_homepage', 'Crawlee');
        await page.click('#search_button_homepage');

        // Wait for the results page to load.
        await page.waitForSelector('.result__body');

        // Extract data from the page using Puppeteer.
        const resultsCount = await page.$eval('#search_form_input', (el) => el.value);
        const topResult = await page.$eval('.result__url', (el) => el.innerText);

        // Print out the data.
        log.info(`Found ${resultsCount} results. Top result: ${topResult}`);

        // Store the results.
        await Dataset.pushData({
            searchTerm: 'Crawlee',
            resultsCount,
            topResult,
        });
    },

    // This function is called if the page processing failed more than maxRequestRetries+1 times.
    failedRequestHandler({ request, log }) {
        log.info(`Request ${request.url} failed too many times.`);
    },
});

// Run the crawler and wait for it to finish.
await crawler.run(['https://github.com/search']);

log.info('Crawler finished.');

----------------------------------------

TITLE: Extracting Page Links with JSDOM
DESCRIPTION: Shows how to use querySelector to find all anchor tags with href attributes and extract their URLs into an array using the Element API.

LANGUAGE: javascript
CODE:
Array.from(document.querySelectorAll('a[href]')).map((a) => a.href);

----------------------------------------

TITLE: Running the Crawlee Project
DESCRIPTION: Commands to navigate to the project directory and start the Crawlee scraper.

LANGUAGE: bash
CODE:
cd my-crawler
npm start

----------------------------------------

TITLE: Running the Crawlee Project
DESCRIPTION: Commands to navigate to the project directory and start the Crawlee scraper.

LANGUAGE: bash
CODE:
cd my-crawler
npm start

----------------------------------------

TITLE: Creating GCP Cloud Function Handler
DESCRIPTION: Implements the main handler function for the GCP Cloud Function, including request processing and response handling.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (req, res) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
    
    return res.send(await crawler.getData())
}

----------------------------------------

TITLE: Puppeteer-based Sitemap Crawling
DESCRIPTION: Implementation demonstrating sitemap crawling using Puppeteer Crawler, which provides full browser automation capabilities. Requires the apify/actor-node-puppeteer-chrome Docker image when running on the Apify Platform.

LANGUAGE: javascript
CODE:
{PuppeteerSource}

----------------------------------------

TITLE: Complete AWS Lambda Handler Implementation for Crawlee
DESCRIPTION: Full implementation of AWS Lambda handler function for Crawlee, including browser configuration, crawler setup, and response handling.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
        launchContext: {
            launchOptions: {
                executablePath: await aws_chromium.executablePath(),
                args: aws_chromium.args,
                headless: true
            }
        }
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);

    return {
        statusCode: 200,
        body: await crawler.getData(),
    };
}

----------------------------------------

TITLE: Creating GCP Cloud Function Handler
DESCRIPTION: Implements the main handler function for the GCP Cloud Function, including request processing and response handling.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (req, res) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
    
    return res.send(await crawler.getData())
}

----------------------------------------

TITLE: Specifying Node.js Version in Dockerfile
DESCRIPTION: Example of specifying Node.js version 16 in a Dockerfile using the apify/actor-node image.

LANGUAGE: dockerfile
CODE:
# Use Node.js 16
FROM apify/actor-node:16

----------------------------------------

TITLE: Using SessionPool with HttpCrawler
DESCRIPTION: Shows implementation of SessionPool with HttpCrawler for handling HTTP requests with session management.

LANGUAGE: javascript
CODE:
{HttpSource}

----------------------------------------

TITLE: Creating a CheerioCrawler with RequestQueue in Crawlee
DESCRIPTION: Shows how to set up a CheerioCrawler with a RequestQueue and a basic requestHandler to extract page titles.

LANGUAGE: typescript
CODE:
import { RequestQueue, CheerioCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

const crawler = new CheerioCrawler({
    requestQueue,
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run();

----------------------------------------

TITLE: Cross-Context Access Using Context IDs
DESCRIPTION: Example showing how to access and manipulate data across different crawling contexts using context IDs

LANGUAGE: javascript
CODE:
let masterContextId;
const handlePageFunction = async ({ id, page, request, crawler }) => {
    if (request.userData.masterPage) {
        masterContextId = id;
        // Prepare the master page.
    } else {
        const masterContext = crawler.crawlingContexts.get(masterContextId);
        const masterPage = masterContext.page;
        const masterRequest = masterContext.request;
        // Now we can manipulate the master data from another handlePageFunction.
    }
}

----------------------------------------

TITLE: Configuring Crawlee using crawlee.json
DESCRIPTION: This snippet shows how to use a crawlee.json file to set configuration options for Crawlee. It demonstrates setting the persistStateIntervalMillis and logLevel options.

LANGUAGE: json
CODE:
{
  "persistStateIntervalMillis": 10000,
  "logLevel": "DEBUG"
}

----------------------------------------

TITLE: Skipping Navigation for Image Requests with PlaywrightCrawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use PlaywrightCrawler to crawl a website, skip navigation for image requests, and save them directly to a key-value store. It uses the Request#skipNavigation option and sendRequest method to efficiently handle CDN-delivered images.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, KeyValueStore } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ request, sendRequest, log }) {
        if (request.userData.dataType === 'IMAGE') {
            log.info(`Fetching image ${request.url}`);
            const imageBuffer = await sendRequest({
                url: request.url,
                responseType: 'buffer',
            });

            await KeyValueStore.setValue(request.userData.key, imageBuffer, { contentType: request.userData.contentType });
            return;
        }

        // Handle other requests...
    },
});

await crawler.run([
    {
        url: 'https://crawler.com',
    },
    {
        url: 'https://cdn.crawler.com/image.jpg',
        userData: {
            dataType: 'IMAGE',
            key: 'image',
            contentType: 'image/jpeg',
        },
        skipNavigation: true,
    },
]);

----------------------------------------

TITLE: Skipping Navigation for Image Requests with PlaywrightCrawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use PlaywrightCrawler to crawl a website, skip navigation for image requests, and save them directly to a key-value store. It uses the Request#skipNavigation option and sendRequest method to efficiently handle CDN-delivered images.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, KeyValueStore } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ request, sendRequest, log }) {
        if (request.userData.dataType === 'IMAGE') {
            log.info(`Fetching image ${request.url}`);
            const imageBuffer = await sendRequest({
                url: request.url,
                responseType: 'buffer',
            });

            await KeyValueStore.setValue(request.userData.key, imageBuffer, { contentType: request.userData.contentType });
            return;
        }

        // Handle other requests...
    },
});

await crawler.run([
    {
        url: 'https://crawler.com',
    },
    {
        url: 'https://cdn.crawler.com/image.jpg',
        userData: {
            dataType: 'IMAGE',
            key: 'image',
            contentType: 'image/jpeg',
        },
        skipNavigation: true,
    },
]);

----------------------------------------

TITLE: Simplified CheerioCrawler Implementation
DESCRIPTION: Demonstrates a more concise way to create a CheerioCrawler using the crawler.run() method with direct URL input, eliminating the need for explicit RequestQueue initialization.

LANGUAGE: typescript
CODE:
// You don't need to import RequestQueue anymore
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

// Start the crawler with the provided URLs
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Puppeteer-based Sitemap Crawler Implementation
DESCRIPTION: Implements sitemap crawling using Puppeteer Crawler, requiring the apify/actor-node-puppeteer-chrome image for deployment. Provides full browser automation capabilities for JavaScript-heavy sites.

LANGUAGE: typescript
CODE:
{PuppeteerSource}

----------------------------------------

TITLE: Setting Up Apify Token in Configuration
DESCRIPTION: Example of initializing the Actor SDK with an API token using Configuration.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const sdk = new Actor({ token: 'your_api_token' });

----------------------------------------

TITLE: Configuring Advanced Autoscaling Options in Crawlee CheerioCrawler
DESCRIPTION: Demonstrates how to set advanced autoscaling pool options for fine-tuning the scaling behavior of a CheerioCrawler, including concurrency ratios and scaling intervals.

LANGUAGE: javascript
CODE:
const crawler = new CheerioCrawler({
    // other options
    autoscaledPoolOptions: {
        desiredConcurrency: 10,
        desiredConcurrencyRatio: 0.90,
        scaleUpStepRatio: 0.05,
        scaleDownStepRatio: 0.05,
        maybeRunIntervalSecs: 0.5,
        loggingIntervalSecs: 60,
        autoscaleIntervalSecs: 10,
        maxTasksPerMinute: 120,
    },
});

----------------------------------------

TITLE: Crawling All Links with Puppeteer Crawler in JavaScript
DESCRIPTION: This code snippet shows how to use Puppeteer Crawler to crawl all links on a website. It utilizes the enqueueLinks() method to add new links to the RequestQueue as the crawler navigates through pages.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, enqueueLinks } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limit to 10 requests
});

// Run the crawler with initial URL
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Package.json for Docker Image
DESCRIPTION: Example package.json file showing how to specify dependencies when using Apify Docker images, using an asterisk for the automation library version.

LANGUAGE: json
CODE:
{
    "dependencies": {
        "crawlee": "^3.0.0",
        "playwright": "*"
    }
}

----------------------------------------

TITLE: Crawling with CheerioCrawler in JavaScript
DESCRIPTION: Example code for performing a recursive crawl of the Crawlee website using CheerioCrawler. It demonstrates how to set up the crawler, define the request handler, and start the crawling process.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        await Dataset.pushData({
            url: request.url,
            title,
        });
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Installing Crawlee Dependencies
DESCRIPTION: NPM commands for installing Crawlee with different crawler types and their dependencies

LANGUAGE: bash
CODE:
npm install crawlee
npm install crawlee playwright
npm install crawlee puppeteer

----------------------------------------

TITLE: Deploying to Apify Platform
DESCRIPTION: Command to deploy the project to the Apify Platform

LANGUAGE: bash
CODE:
apify push

----------------------------------------

TITLE: Installing TypeScript Dependencies
DESCRIPTION: Installation commands for TypeScript and Node.js type definitions as development dependencies.

LANGUAGE: shell
CODE:
npm install --save-dev typescript
npm install --save-dev @types/node

----------------------------------------

TITLE: Using actor-node-puppeteer-chrome Docker Image
DESCRIPTION: This snippet demonstrates how to use the Apify Docker image that includes Puppeteer and Chrome, suitable for CheerioCrawler and PuppeteerCrawler.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-puppeteer-chrome:16

----------------------------------------

TITLE: Creating and Running a Basic Actor
DESCRIPTION: Commands to create and run a new actor project locally using Apify CLI

LANGUAGE: bash
CODE:
apify create my-hello-world
cd my-hello-world
apify run

----------------------------------------

TITLE: Setting Maximum Requests Per Minute in CheerioCrawler
DESCRIPTION: Demonstrates how to limit the number of requests per minute in a CheerioCrawler instance. This helps control the crawler's throughput and prevents overwhelming the target website.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    maxRequestsPerMinute: 120,
    // ...
});

----------------------------------------

TITLE: Capturing Screenshots with PuppeteerCrawler using page.screenshot()
DESCRIPTION: This snippet demonstrates how to capture screenshots of multiple web pages using PuppeteerCrawler and page.screenshot(). It defines a crawler that visits multiple URLs, takes screenshots, and saves them to a key-value store.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, KeyValueStore } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);

        const screenshot = await page.screenshot();
        const key = `${request.id}.png`;
        await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });
    },
});

await crawler.run(['https://example.com', 'https://example.com/about']);

----------------------------------------

TITLE: Fetching Single URL with got-scraping in JavaScript
DESCRIPTION: Example showing how to use got-scraping npm package to fetch HTML content from a single web page. The code demonstrates basic web scraping functionality with reference to handling dynamic URLs through user input.

LANGUAGE: javascript
CODE:
import { gotScraping } from 'got-scraping';

const response = await gotScraping('https://example.com');
console.log(response.body);

----------------------------------------

TITLE: TypeScript Configuration for Crawlee
DESCRIPTION: Basic TypeScript configuration file for Crawlee projects, extending from @apify/tsconfig with ES2022 settings

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist",
        "lib": ["DOM"]
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Configuring SessionPool with CheerioCrawler in Crawlee
DESCRIPTION: This example demonstrates the setup and usage of SessionPool with CheerioCrawler in Crawlee. It includes configuration for proxy usage and session pool management.

LANGUAGE: js
CODE:
import { CheerioCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new CheerioCrawler({
    // Activates the Session pool
    useSessionPool: true,
    // Overrides default Session pool configuration
    sessionPoolOptions: {
        maxPoolSize: 100,
        sessionOptions: {
            // All sessions will use this proxy configuration
            proxyConfiguration,
        },
    },
    async requestHandler({ session, request }) {
        // ...
    },
});

await crawler.run();

----------------------------------------

TITLE: Scraping and Parsing Product Price with Playwright
DESCRIPTION: This code block shows how to locate the price element, extract the text content, and parse it into a number.

LANGUAGE: javascript
CODE:
const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

----------------------------------------

TITLE: Initializing Cheerio Crawler with Stateless Configuration
DESCRIPTION: Basic setup of a Cheerio crawler with Lambda-compatible configuration using in-memory storage to maintain statelessness.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Adjusting Crawlee Code for Apify Platform Deployment
DESCRIPTION: Modifications to the main Crawlee script to integrate with the Apify Platform. Includes initializing and exiting the Apify Actor, which configures Crawlee to use Apify API for storage.

LANGUAGE: typescript
CODE:
import { Actor } from 'apify';
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

await Actor.init();

log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

await Actor.exit();

----------------------------------------

TITLE: TypeScript Configuration for Crawlee
DESCRIPTION: TypeScript configuration file setup for Crawlee projects

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist",
        "lib": ["DOM"]
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Initializing BasicCrawler with sendRequest in TypeScript
DESCRIPTION: This snippet demonstrates how to initialize a BasicCrawler and use the sendRequest function to make HTTP requests. It showcases the basic usage of sendRequest within the crawler's requestHandler.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest();
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Crawling Sitemap with Playwright Crawler
DESCRIPTION: Implementation of sitemap crawling using Playwright Crawler. Requires apify/actor-node-playwright-chrome image for Platform deployment. Offers modern browser automation features for processing sitemap URLs.

LANGUAGE: javascript
CODE:
{PlaywrightSource}

----------------------------------------

TITLE: Implementing CheerioCrawler for Web Scraping in JavaScript
DESCRIPTION: This code demonstrates how to use CheerioCrawler to crawl URLs from an external file, make HTTP requests, parse HTML with Cheerio, and extract the page title and h1 tags. It showcases the setup of the crawler, request handling, and data processing.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';
import { readFile } from 'fs/promises';

// Create an instance of the CheerioCrawler class - a crawler
// that uses Cheerio under the hood to parse HTML.
const crawler = new CheerioCrawler({
    // Let's limit our crawls to make our
    // tests shorter and safer.
    maxRequestsPerCrawl: 20,

    // This function will be called for each URL to crawl.
    // It accepts a single parameter, which is an object with
    // the following fields:
    // - request: an instance of the Request class with information such as URL and HTTP method
    // - $: the cheerio object containing parsed HTML
    // - body: the raw body of the HTML page
    // - json: if JSON content type detected, this contains the parsed JSON
    async requestHandler({ request, $, crawler }) {
        // Extract the text content of the page's <title> element
        const title = $('title').text();

        // Extract all <h1> elements from the page as an array of strings
        const h1s = $('h1')
            .map((_, el) => $(el).text())
            .toArray();

        // Save the extracted data into the default dataset associated with the crawler run
        await Dataset.pushData({
            url: request.url,
            title,
            h1s,
        });

        // Find all links, ignore search results pagination links
        // and add them to the crawling queue.
        const enqueued = await crawler.addRequests(
            $('a')
                .map((_, el) => $(el).attr('href'))
                .toArray()
                .filter((link) => !link.includes('search'))
        );

        console.log(`Enqueued ${enqueued.length} URLs.`);
    },
});

async function main() {
    // Add URLs to the queue from an external file
    const content = await readFile('urls.txt', 'utf8');
    const urls = content.trim().split('\n');
    await crawler.addRequests(urls);

    // Run the crawler
    await crawler.run();

    console.log('Crawler finished.');
}

main();

----------------------------------------

TITLE: Installing Apify SDK v1 with Playwright
DESCRIPTION: Command to install Apify SDK v1.0.0 with Playwright support

LANGUAGE: bash
CODE:
npm install apify playwright

----------------------------------------

TITLE: CheerioCrawler Log Output - Crawlee Website Crawl
DESCRIPTION: Log entries showing the crawler starting and extracting page titles from multiple crawlee.dev URLs. The log shows successful crawling of the homepage, examples, quick-start, and guides pages.

LANGUAGE: log
CODE:
INFO  CheerioCrawler: Starting the crawl
INFO  CheerioCrawler: Title of https://crawlee.dev/ is 'Crawlee · Build reliable crawlers. Fast. | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/guides is 'Guides | Crawlee'

----------------------------------------

TITLE: Adjusting Crawlee Code for Apify Platform Deployment
DESCRIPTION: Modifications to the main Crawlee script to integrate with the Apify Platform. Includes initializing and exiting the Apify Actor, which configures Crawlee to use Apify API for storage.

LANGUAGE: typescript
CODE:
import { Actor } from 'apify';
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

await Actor.init();

log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

await Actor.exit();

----------------------------------------

TITLE: Logging CheerioCrawler Output for Crawlee Website
DESCRIPTION: This log snippet shows the output of a CheerioCrawler instance crawling the Crawlee documentation website. It includes the crawler start message and the titles of four pages that were crawled, along with their URLs.

LANGUAGE: plaintext
CODE:
INFO  CheerioCrawler: Starting the crawl
INFO  CheerioCrawler: Title of https://crawlee.dev/ is 'Crawlee · Build reliable crawlers. Fast. | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/guides is 'Guides | Crawlee'

----------------------------------------

TITLE: Implementing Custom HTTP Client with Fetch API in TypeScript
DESCRIPTION: Shows a skeleton implementation of a custom HTTP client using the Fetch API that conforms to the BaseHttpClient interface. This example demonstrates the required methods and structure for creating a custom HTTP client.

LANGUAGE: typescript
CODE:
import { BaseHttpClient, type Request } from '@crawlee/core';

export class FetchHttpClient extends BaseHttpClient {
    async send(request: Request): Promise<Response> {
        // Custom logic here
        return fetch(request);
    }
}

----------------------------------------

TITLE: Configuring Docker Environment for Crawlee Actor
DESCRIPTION: Sets up a Docker container for running Crawlee actors based on the apify/actor-node-playwright-chrome image. The build process is optimized using layer caching for npm packages and implements a clean installation excluding development dependencies.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16

# Copy just package.json and package-lock.json
# to speed up the build using Docker layer cache.
COPY --chown=myuser package*.json ./

# Install NPM packages, skip optional and development dependencies to
# keep the image small. Avoid logging too much and print the dependency
# tree for debugging
RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

# Next, copy the remaining files and directories with the source code.
# Since we do this after NPM install, quick build will be really fast
# for most source file changes.
COPY --chown=myuser . ./

# Run the image.
CMD npm start --silent

----------------------------------------

TITLE: Defining Crawlee Documentation Robots.txt Rules
DESCRIPTION: This robots.txt configuration allows all user agents to access the Crawlee documentation and specifies the locations of XML sitemaps for improved crawling and indexing.

LANGUAGE: robots.txt
CODE:
User-agent: *
Sitemap: https://crawlee.dev/sitemap.xml
Sitemap: https://crawlee.dev/python/sitemap.xml

----------------------------------------

TITLE: Using PreLaunchHooks in PuppeteerCrawler
DESCRIPTION: Example demonstrating the use of preLaunchHooks to replace the functionality of launchPuppeteerFunction.

LANGUAGE: javascript
CODE:
const maybeLaunchChrome = (pageId, launchContext) => {
    if (someVariable === 'chrome') {
        launchContext.useChrome = true;
    }
}

const crawler = new Apify.PuppeteerCrawler({
    browserPoolOptions: {
        preLaunchHooks: [maybeLaunchChrome]
    },
    // ...
})

----------------------------------------

TITLE: Performing Recursive Web Crawl with PuppeteerCrawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to set up and execute a recursive web crawl using the PuppeteerCrawler class. It includes configuration for the initial URL, request queue, and handling of crawled data. The crawler is set to follow links within the same domain for recursive crawling.

LANGUAGE: javascript
CODE:
{CrawlSource}

----------------------------------------

TITLE: Logging in to Apify Platform via CLI
DESCRIPTION: Command to log in to the Apify Platform using the Apify CLI. Requires a personal access token from the Apify account.

LANGUAGE: bash
CODE:
apify login

----------------------------------------

TITLE: Configuring Docker Environment for Crawlee Actor
DESCRIPTION: Sets up a Docker container for running Crawlee actors using the apify/actor-node:16 base image. Implements a multi-stage build process to optimize caching and minimize image size by selectively copying package files, installing production dependencies, and then copying the remaining source code.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm start --silent

----------------------------------------

TITLE: Installing Apify SDK v1 with Dependencies
DESCRIPTION: Package installation commands for Apify SDK v1 with either Puppeteer or Playwright support

LANGUAGE: bash
CODE:
npm install apify puppeteer

npm install apify playwright

----------------------------------------

TITLE: Capturing Screenshot Using Direct Puppeteer Page API
DESCRIPTION: Example demonstrating how to capture a webpage screenshot using Puppeteer's page.screenshot() method directly. The screenshot is saved to a key-value store with a key derived from the page URL.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from '@crawlee/core';

const url = 'https://crawlee.dev';

// Launch browser and navigate to the page
const browser = await launchPuppeteer();
const page = await browser.newPage();
await page.goto(url);

// Create key from url
const key = url.replace(/[:/]/g, '_');

// Capture screenshot
const screenshot = await page.screenshot();

// Save screenshot to named key-value store
await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });

await browser.close();

----------------------------------------

TITLE: Extracting All Links with Cheerio
DESCRIPTION: Demonstrates how to find all anchor elements with href attributes and extract their URLs into an array using Cheerio's methods.

LANGUAGE: javascript
CODE:
$('a[href]')
    .map((i, el) => $(el).attr('href'))
    .get();

----------------------------------------

TITLE: Combining Request Queue and Request List in Crawlee
DESCRIPTION: Shows how to combine Request Queue and Request List in Crawlee. It demonstrates creating both storage types and using them together with a PuppeteerCrawler.

LANGUAGE: javascript
CODE:
import { RequestList, RequestQueue, PuppeteerCrawler } from 'crawlee';

// Prepare a list of URLs for the Request List
const sources = [
    { url: 'http://www.example.com/page-1' },
    { url: 'http://www.example.com/page-2' },
    { url: 'http://www.example.com/page-3' },
];

// Open the Request List
const requestList = await RequestList.open('my-list', sources);

// Open the Request Queue
const requestQueue = await RequestQueue.open();

// Create a crawler that will process requests from both List and Queue
const crawler = new PuppeteerCrawler({
    requestList,
    requestQueue,
    async requestHandler({ page, request, enqueueLinks }) {
        // Process the page...

        // Add newly discovered links to the Queue
        await enqueueLinks();
    },
});

// Run the crawler
await crawler.run();

----------------------------------------

TITLE: Complete package.json Configuration for Crawlee TypeScript Project
DESCRIPTION: Full package.json configuration for a Crawlee TypeScript project, including dependencies, scripts for development and production, and module type declaration.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "type": "module",
    "main": "dist/main.js",
    "dependencies": {
        "crawlee": "3.0.0"
    },
    "devDependencies": {
        "@apify/tsconfig": "^0.1.0",
        "@types/node": "^18.14.0",
        "ts-node": "^10.8.0",
        "typescript": "^4.7.4"
    },
    "scripts": {
        "start": "npm run start:dev",
        "start:prod": "node dist/main.js",
        "start:dev": "ts-node-esm -T src/main.ts",
        "build": "tsc"
    }
}

----------------------------------------

TITLE: Integrating AWS Chromium with Crawlee
DESCRIPTION: Configuration of Crawlee to use AWS-compatible Chromium binary with appropriate launch options for Lambda environment.

LANGUAGE: javascript
CODE:
// For more information, see https://crawlee.dev/
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
    launchContext: {
        launchOptions: {
             executablePath: await aws_chromium.executablePath(),
             args: aws_chromium.args,
             headless: true
        }
    }
}, new Configuration({
    persistStorage: false,
}));

----------------------------------------

TITLE: Configuring Crawlee using custom Configuration
DESCRIPTION: This example shows how to create a custom Configuration instance and pass it to a CheerioCrawler. It sets the persistStateIntervalMillis option and demonstrates the crawler's behavior.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Create new configuration
const config = new Configuration({
    // Set the 'persistStateIntervalMillis' option to 10 seconds
    persistStateIntervalMillis: 10_000,
});

// Now we need to pass the configuration to the crawler
const crawler = new CheerioCrawler({}, config);

crawler.router.addDefaultHandler(async ({ request }) => {
    // for the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // for the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Extracting All Links from a Webpage using JSDOM
DESCRIPTION: Shows how to use querySelector to find all anchor elements with href attributes and extract their URLs into an array using JSDOM's Element API.

LANGUAGE: javascript
CODE:
Array.from(document.querySelectorAll('a[href]')).map((a) => a.href);

----------------------------------------

TITLE: Capturing Screenshot with Puppeteer using utils.puppeteer.saveSnapshot()
DESCRIPTION: This snippet shows how to capture a screenshot using the utils.puppeteer.saveSnapshot() method. It launches a browser, creates a new page, navigates to a URL, and saves a snapshot using the utility function.

LANGUAGE: javascript
CODE:
import { launchPuppeteer, utils } from 'crawlee';

const browser = await launchPuppeteer();
const page = await browser.newPage();
await page.goto('https://crawlee.dev');

await utils.puppeteer.saveSnapshot(page, { key: 'my-crawlee-snapshot' });

await browser.close();

----------------------------------------

TITLE: Disabling Browser Fingerprints with PuppeteerCrawler in Crawlee
DESCRIPTION: This code snippet shows how to disable browser fingerprints when using PuppeteerCrawler in Crawlee. It sets the useFingerprints option to false in the browserPoolOptions.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    // ... other options
    browserPoolOptions: {
        useFingerprints: false,
    },
});

----------------------------------------

TITLE: Installing and Logging in with Apify CLI
DESCRIPTION: Commands to install Apify CLI and log in with an API token for platform access

LANGUAGE: bash
CODE:
npm install -g apify-cli
apify login -t YOUR_API_TOKEN

----------------------------------------

TITLE: Crawling All Links without Domain Restriction
DESCRIPTION: Demonstrates how to crawl all links found on a page using Cheerio crawler, regardless of their domain. Uses the 'all' enqueue strategy to process any discovered URLs.

LANGUAGE: javascript
CODE:
{AllLinksSource}

----------------------------------------

TITLE: Sitemap Crawling with Playwright
DESCRIPTION: Implementation of sitemap crawling using Playwright Crawler, providing modern browser automation capabilities. Requires apify/actor-node-playwright-chrome Docker image for Platform deployment.

LANGUAGE: javascript
CODE:
{PlaywrightSource}

----------------------------------------

TITLE: Installing and Logging in with Apify CLI
DESCRIPTION: Commands to install Apify CLI and log in with an API token for platform access

LANGUAGE: bash
CODE:
npm install -g apify-cli
apify login -t YOUR_API_TOKEN

----------------------------------------

TITLE: Installing and Logging in with Apify CLI
DESCRIPTION: Commands to install Apify CLI and log in with an API token for platform access

LANGUAGE: bash
CODE:
npm install -g apify-cli
apify login -t YOUR_API_TOKEN

----------------------------------------

TITLE: Logging Crawlee Web Scraping Results
DESCRIPTION: This log output shows the results of a CheerioCrawler instance crawling the Crawlee website. It includes the starting message and the titles of four different pages that were crawled.

LANGUAGE: plaintext
CODE:
INFO  CheerioCrawler: Starting the crawl
INFO  CheerioCrawler: Title of https://crawlee.dev/ is 'Crawlee · Build reliable crawlers. Fast. | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/guides is 'Guides | Crawlee'

----------------------------------------

TITLE: Crawling All Links with Playwright Crawler in Crawlee
DESCRIPTION: This code snippet illustrates how to use Playwright Crawler in Crawlee to crawl all links on a website. It sets up a RequestQueue, creates a PlaywrightCrawler, and uses the enqueueLinks() method to add new links to the queue as it crawls.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, enqueueLinks } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limit to 10 requests
});

// Run the crawler
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Logging Crawlee Web Scraping Process
DESCRIPTION: This log snippet shows the output of a CheerioCrawler instance as it crawls the Crawlee documentation website. It logs the start of the crawl and the titles of four different pages that were visited.

LANGUAGE: plaintext
CODE:
INFO  CheerioCrawler: Starting the crawl
INFO  CheerioCrawler: Title of https://crawlee.dev/ is 'Crawlee · Build reliable crawlers. Fast. | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/guides is 'Guides | Crawlee'

----------------------------------------

TITLE: Configuring Crawlee using crawlee.json
DESCRIPTION: Example of a crawlee.json file used to set global configuration options for Crawlee. It demonstrates setting the persistStateIntervalMillis and logLevel parameters.

LANGUAGE: json
CODE:
{
  "persistStateIntervalMillis": 10000,
  "logLevel": "DEBUG"
}

----------------------------------------

TITLE: Custom Link Selector with enqueueLinks
DESCRIPTION: Example of using enqueueLinks with a custom CSS selector to find links within specific div elements.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    selector: 'div.has-link'
});

----------------------------------------

TITLE: Accepting and Logging User Input in JavaScript
DESCRIPTION: This code snippet demonstrates how to accept and log user input in a Crawlee project. It uses the Actor class from the 'apify' package to get input and log it.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

await Actor.init();

const input = await Actor.getInput();
console.log('Input:');
console.log(input);

await Actor.exit();

----------------------------------------

TITLE: Implementing Hacker News Scraping with PlaywrightCrawler
DESCRIPTION: A complete example showing how to recursively crawl Hacker News using PlaywrightCrawler and RequestQueue. The crawler processes pages, extracts data, and handles pagination. Results are stored in the default dataset as JSON files in the local storage directory.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        // If request is marked with label, we are now scraping a detail page
        if (request.label === 'detail') {
            const title = await page.title();
            const rank = await page.locator('.rank').textContent();
            const points = await page.locator('.score').textContent();
            const url = await page.locator('.titleline a').first().getAttribute('href');

            await Dataset.pushData({
                title,
                rank,
                points,
                url,
            });
            return;
        }

        // Else, we are now on the start page and enqueueing links
        await enqueueLinks({
            selector: '.titleline a',
            label: 'detail',
        });

        // Enqueue pagination
        await enqueueLinks({
            selector: '.morelink',
        });
    },
});

await crawler.run(['https://news.ycombinator.com']);

----------------------------------------

TITLE: Implementing Hacker News Scraping with PlaywrightCrawler
DESCRIPTION: A complete example showing how to recursively crawl Hacker News using PlaywrightCrawler and RequestQueue. The crawler processes pages, extracts data, and handles pagination. Results are stored in the default dataset as JSON files in the local storage directory.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        // If request is marked with label, we are now scraping a detail page
        if (request.label === 'detail') {
            const title = await page.title();
            const rank = await page.locator('.rank').textContent();
            const points = await page.locator('.score').textContent();
            const url = await page.locator('.titleline a').first().getAttribute('href');

            await Dataset.pushData({
                title,
                rank,
                points,
                url,
            });
            return;
        }

        // Else, we are now on the start page and enqueueing links
        await enqueueLinks({
            selector: '.titleline a',
            label: 'detail',
        });

        // Enqueue pagination
        await enqueueLinks({
            selector: '.morelink',
        });
    },
});

await crawler.run(['https://news.ycombinator.com']);

----------------------------------------

TITLE: Customizing Browser Fingerprints with PlaywrightCrawler
DESCRIPTION: This code snippet demonstrates how to customize browser fingerprints using PlaywrightCrawler in Crawlee. It shows how to set specific browser and operating system parameters for fingerprint generation.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    // ...
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                browsers: [
                    { name: 'firefox', minVersion: 88 },
                    { name: 'chrome', minVersion: 91 },
                ],
                operatingSystems: ['windows', 'linux'],
                devices: ['desktop'],
            },
        },
    },
});

----------------------------------------

TITLE: Building and Configuring Docker Image for Crawlee Actor
DESCRIPTION: Multi-stage Dockerfile that builds and configures a Crawlee actor environment. The first stage builds the application, while the second stage creates a minimal production image with only the necessary dependencies and built files.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16 AS builder

COPY package*.json ./

RUN npm install --include=dev --audit=false

COPY . ./

RUN npm run build

FROM apify/actor-node:16

COPY --from=builder /usr/src/app/dist ./dist

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm run start:prod --silent

----------------------------------------

TITLE: Installing Apify SDK and CLI Dependencies
DESCRIPTION: Commands to install the Apify SDK as a project dependency and the Apify CLI as a global tool for deployment.

LANGUAGE: bash
CODE:
npm install apify

LANGUAGE: bash
CODE:
npm install -g apify-cli

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Router in Crawlee
DESCRIPTION: Sets up a PlaywrightCrawler instance using a router for request handling. It demonstrates how to configure logging and start the crawler with a specific URL.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Crawler Configuration Example
DESCRIPTION: Example of configuring a PlaywrightCrawler with browser fingerprint options.

LANGUAGE: typescript
CODE:
const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        useFingerprints: false,
    },
});

----------------------------------------

TITLE: Multi-Environment Page Creation with Browser Pool
DESCRIPTION: Example showing how to create pages in multiple browser environments simultaneously using newPageWithEachPlugin

LANGUAGE: javascript
CODE:
import { BrowserPool, PlaywrightPlugin, PuppeteerPlugin } from '@crawlee/browser-pool';
import playwright from 'playwright';
import puppeteer from 'puppeteer';

const browserPool = new BrowserPool({
    browserPlugins: [
        new PlaywrightPlugin(playwright.chromium),
        new PuppeteerPlugin(puppeteer),
    ],
});

const pages = await browserPool.newPageWithEachPlugin();
const promises = pages.map(async page => {
    // Run some task with each page
    // pages are in order of plugins:
    // [playwrightPage, puppeteerPage]
    await page.close();
});
await Promise.all(promises);

// Continue with some more work.

----------------------------------------

TITLE: Combining Request Queue and Request List in Crawlee
DESCRIPTION: Shows how to use both request queue and request list together in a Crawlee crawler, which can be useful for specific use cases.

LANGUAGE: javascript
CODE:
import { RequestQueue, RequestList, PuppeteerCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();

const sources = [
    { url: 'http://www.example.com/page-1' },
    { url: 'http://www.example.com/page-2' },
    { url: 'http://www.example.com/page-3' },
];

const requestList = await RequestList.open('my-list', sources);

const crawler = new PuppeteerCrawler({
    requestList,
    requestQueue,
    async requestHandler({ page, request }) {
        // Process the page...
    },
});

await crawler.run();

----------------------------------------

TITLE: Initializing Empty Queue State in Main Scraper
DESCRIPTION: Pre-initialization code to ensure the request queue starts with a clean slate before crawler execution

LANGUAGE: javascript
CODE:
import { getOrInitQueue } from './requestQueue.mjs';

// Pre-initialize the queue so that we have a blank slate that will get filled out by the crawler
await getOrInitQueue(true);

----------------------------------------

TITLE: Installing and Packaging Browser Dependencies for AWS Lambda
DESCRIPTION: Commands to install @sparticuz/chromium package and create a zip archive of node_modules for AWS Lambda layer deployment.

LANGUAGE: bash
CODE:
# Install the package
npm i -S @sparticuz/chromium

# Zip the dependencies
zip -r dependencies.zip ./node_modules

----------------------------------------

TITLE: Saving Data with Dataset.pushData() in Crawlee (JavaScript)
DESCRIPTION: This code demonstrates how to save scraped data using the Dataset.pushData() method. It replaces a console.log statement with the actual data saving operation.

LANGUAGE: javascript
CODE:
await Dataset.pushData(results);

----------------------------------------

TITLE: SendRequest Implementation with Got Scraping
DESCRIPTION: Shows the internal implementation of sendRequest function using Got Scraping with various configuration options.

LANGUAGE: typescript
CODE:
async sendRequest(overrideOptions?: GotOptionsInit) => {
    return gotScraping({
        url: request.url,
        method: request.method,
        body: request.payload,
        headers: request.headers,
        proxyUrl: crawlingContext.proxyInfo?.url,
        sessionToken: session,
        responseType: 'text',
        ...overrideOptions,
        retry: {
            limit: 0,
            ...overrideOptions?.retry,
        },
        cookieJar: {
            getCookieString: (url: string) => session!.getCookieString(url),
            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),
            ...overrideOptions?.cookieJar,
        },
    });
}

----------------------------------------

TITLE: Crawler Configuration Example
DESCRIPTION: Example of configuring a PlaywrightCrawler with browser fingerprint options.

LANGUAGE: typescript
CODE:
const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        useFingerprints: false,
    },
});

----------------------------------------

TITLE: Crawling All Links with CheerioCrawler in Crawlee
DESCRIPTION: This code snippet demonstrates how to use the CheerioCrawler to crawl all links found on a website, regardless of their domain. It uses the 'all' enqueue strategy.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks }) {
        // Add all the links from the page to the crawler's RequestQueue
        // TypeScript: EnqueueStrategy.All
        await enqueueLinks({
            strategy: 'all',
            transformRequestFunction(req) {
                // Add custom handling of the scraped links here
                return req;
            },
        });
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Simplified CheerioCrawler Setup with Inline URL Addition in Crawlee
DESCRIPTION: Demonstrates a more concise way to set up a CheerioCrawler by directly adding URLs to crawl in the run() method, eliminating the need for explicit RequestQueue initialization.

LANGUAGE: typescript
CODE:
// You don't need to import RequestQueue anymore
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

// Start the crawler with the provided URLs
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Configuring Crawlee using custom Configuration instance
DESCRIPTION: This example shows how to create a custom Configuration instance and pass it to a CheerioCrawler. It sets the persistStateIntervalMillis option to 10 seconds.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Create new configuration
const config = new Configuration({
    // Set the 'persistStateIntervalMillis' option to 10 seconds
    persistStateIntervalMillis: 10_000,
});

// Now we need to pass the configuration to the crawler
const crawler = new CheerioCrawler({}, config);

crawler.router.addDefaultHandler(async ({ request }) => {
    // for the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // for the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Configuring Crawlee using custom Configuration instance
DESCRIPTION: This example shows how to create a custom Configuration instance and pass it to a CheerioCrawler. It sets the persistStateIntervalMillis option to 10 seconds.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Create new configuration
const config = new Configuration({
    // Set the 'persistStateIntervalMillis' option to 10 seconds
    persistStateIntervalMillis: 10_000,
});

// Now we need to pass the configuration to the crawler
const crawler = new CheerioCrawler({}, config);

crawler.router.addDefaultHandler(async ({ request }) => {
    // for the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // for the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: CheerioCrawler Log Output
DESCRIPTION: Log entries showing the execution of a CheerioCrawler instance crawling the Crawlee documentation website. Shows the crawler starting and successfully retrieving page titles from multiple documentation pages.

LANGUAGE: log
CODE:
INFO  CheerioCrawler: Starting the crawl
INFO  CheerioCrawler: Title of https://crawlee.dev/ is 'Crawlee · Build reliable crawlers. Fast. | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/guides is 'Guides | Crawlee'

----------------------------------------

TITLE: Saving Data with Dataset.pushData() in Crawlee (JavaScript)
DESCRIPTION: This code demonstrates how to save scraped data using the Dataset.pushData() method. It replaces a console.log statement with the actual data saving operation.

LANGUAGE: javascript
CODE:
await Dataset.pushData(results);

----------------------------------------

TITLE: Configuring Basic Request Queue Operations in Crawlee
DESCRIPTION: Demonstrates basic operations with Request Queue including initialization and request handling. Shows how to work with the default queue storage directory and manage requests.

LANGUAGE: text
CODE:
{CRAWLEE_STORAGE_DIR}/request_queues/{QUEUE_ID}/entries.json

----------------------------------------

TITLE: Using Proxy with BasicCrawler
DESCRIPTION: Shows how to configure proxy settings when making requests with BasicCrawler.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            proxyUrl: 'http://auto:password@proxy.apify.com:8000',
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Request Transform Function
DESCRIPTION: Example of using transformRequestFunction to filter and modify requests before they are enqueued.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
    transformRequestFunction(req) {
        // ignore all links ending with `.pdf`
        if (req.url.endsWith('.pdf')) return false;
        return req;
    },
});

----------------------------------------

TITLE: Modifying Detail Route Handler for Inter-Process Communication in JavaScript
DESCRIPTION: Updates the DETAIL route handler to use process.send for communicating scraped data back to the parent process, enabling centralized data collection in parallel scraping.

LANGUAGE: javascript
CODE:
router.addHandler('DETAIL', async ({ $, request, log }) => {
    const title = $('h1').text();
    const price = $('p.price_color').text();
    const availability = $('p.availability').text().trim();

    log.info(`Product: ${title}, price: ${price}, availability: ${availability}`);

    const productData = {
        title,
        price,
        availability,
        url: request.url,
    };

    if (process.send) {
        process.send({ data: productData });
    } else {
        // Fallback for non-worker environments
        await context.pushData(productData);
    }
});

----------------------------------------

TITLE: Basic URL Structure Example
DESCRIPTION: Example of pagination URL structure for the warehouse store

LANGUAGE: plaintext
CODE:
https://warehouse-theme-metal.myshopify.com/collections/headphones?page=2

----------------------------------------

TITLE: Basic URL Structure Example
DESCRIPTION: Example of pagination URL structure for the warehouse store

LANGUAGE: plaintext
CODE:
https://warehouse-theme-metal.myshopify.com/collections/headphones?page=2

----------------------------------------

TITLE: Using Pre-release Docker Image
DESCRIPTION: Shows how to use a pre-release version of an Apify Docker image, both with and without specifying the automation library version.

LANGUAGE: dockerfile
CODE:
# Without library version.
FROM apify/actor-node:20-beta

LANGUAGE: dockerfile
CODE:
# With library version.
FROM apify/actor-node-playwright-chrome:20-1.10.0-beta

----------------------------------------

TITLE: Dockerfile for Crawlee TypeScript Project
DESCRIPTION: Multi-stage Dockerfile for building and running Crawlee TypeScript projects

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16 AS builder

COPY . ./
RUN npm install --include=dev \
    && npm run build

FROM apify/actor-node:16
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/README.md ./
COPY --from=builder /usr/src/app/dist ./dist
COPY --from=builder /usr/src/app/apify.json ./apify.json
COPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json

RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional \
    && echo "Installed NPM packages:" \
    && (npm list --only=prod --no-optional --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

CMD npm run start:prod

----------------------------------------

TITLE: Map Method Result Example
DESCRIPTION: Example output showing the result of mapping operation that filters heading counts greater than 5. The result is stored in the key-value store.

LANGUAGE: javascript
CODE:
[11, 8]

----------------------------------------

TITLE: Getting Public URL for Storage Item
DESCRIPTION: Example of storing data in Key-Value Store and getting its public URL

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'apify';

const store = await KeyValueStore.open();
await store.setValue('your-file', { foo: 'bar' });
const url = store.getPublicUrl('your-file');
// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file

----------------------------------------

TITLE: Crawling All Links with Cheerio Crawler in Crawlee
DESCRIPTION: This code snippet demonstrates how to use the CheerioCrawler to crawl all links on a website. It initializes the crawler, sets up a request queue, and uses the enqueueLinks() method to add new links for crawling.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, enqueueLinks } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limitation for only 10 requests (do not use if you want to crawl all links)
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Crawling All Links with Cheerio Crawler in Crawlee
DESCRIPTION: This code snippet demonstrates how to use the CheerioCrawler to crawl all links on a website. It initializes the crawler, sets up a request queue, and uses the enqueueLinks() method to add new links for crawling.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, enqueueLinks } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limitation for only 10 requests (do not use if you want to crawl all links)
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Configuring Crawlee using custom Configuration
DESCRIPTION: Example of creating a custom Configuration instance and passing it to a CheerioCrawler. It sets the persistStateIntervalMillis option and demonstrates usage with a crawler.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Create new configuration
const config = new Configuration({
    // Set the 'persistStateIntervalMillis' option to 10 seconds
    persistStateIntervalMillis: 10_000,
});

// Now we need to pass the configuration to the crawler
const crawler = new CheerioCrawler({}, config);

crawler.router.addDefaultHandler(async ({ request }) => {
    // for the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // for the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Disabling Fingerprints in PuppeteerCrawler
DESCRIPTION: Demonstrates how to turn off browser fingerprinting functionality in PuppeteerCrawler.

LANGUAGE: javascript
CODE:
{PuppeteerFingerprintsOffSource}

----------------------------------------

TITLE: Using Crawlee with crawlee.json Configuration
DESCRIPTION: JavaScript example demonstrating how to use Crawlee with configuration options set in crawlee.json, without explicitly importing or passing Configuration to the crawler.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, sleep } from 'crawlee';
// We are not importing nor passing
// the Configuration to the crawler.
// We are not assigning any env vars either.
const crawler = new CheerioCrawler();

crawler.router.addDefaultHandler(async ({ request }) => {
    // for the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // for the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Configuring SessionPool with HttpCrawler in Crawlee
DESCRIPTION: This example shows how to configure and use SessionPool with HttpCrawler in Crawlee. It includes setup for proxy configuration and session pool management.

LANGUAGE: js
CODE:
import { HttpCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new HttpCrawler({
    // Use the proxyConfiguration
    proxyConfiguration,
    async requestHandler({ request, session, body }) {
        console.log(`The ${session.id} successfully crawled ${request.url}`);
        console.log(`Received body: ${body}`);
    },
});

await crawler.run(['http://example.com/1', 'http://example.com/2', 'http://example.com/3']);

----------------------------------------

TITLE: Configuring SessionPool with HttpCrawler in Crawlee
DESCRIPTION: This example shows how to configure and use SessionPool with HttpCrawler in Crawlee. It includes setup for proxy configuration and session pool management.

LANGUAGE: js
CODE:
import { HttpCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new HttpCrawler({
    // Use the proxyConfiguration
    proxyConfiguration,
    async requestHandler({ request, session, body }) {
        console.log(`The ${session.id} successfully crawled ${request.url}`);
        console.log(`Received body: ${body}`);
    },
});

await crawler.run(['http://example.com/1', 'http://example.com/2', 'http://example.com/3']);

----------------------------------------

TITLE: Configuring Crawlee using crawlee.json
DESCRIPTION: Example of a crawlee.json file used to set global configuration options for Crawlee. It demonstrates setting the persistStateIntervalMillis and logLevel options.

LANGUAGE: json
CODE:
{
  "persistStateIntervalMillis": 10000,
  "logLevel": "DEBUG"
}

----------------------------------------

TITLE: TypeScript Configuration
DESCRIPTION: Example TypeScript configuration for Crawlee projects using @apify/tsconfig.

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist",
        "lib": ["DOM"]
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Domain Strategy Configuration
DESCRIPTION: Configuration example for including subdomains in crawling using enqueueLinks strategy.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    strategy: 'same-domain'
});

----------------------------------------

TITLE: Disabling Browser Fingerprints in PuppeteerCrawler
DESCRIPTION: Example showing how to disable browser fingerprint generation in PuppeteerCrawler by setting useFingerprints to false in browserPoolOptions.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    browserPoolOptions: {
        useFingerprints: false
    }
});

----------------------------------------

TITLE: Implementing Crawlee Router with Route Handlers
DESCRIPTION: Defines route handlers for different page types (detail, category, and default) using Crawlee's router system. Includes logic for data extraction and link enqueueing.

LANGUAGE: javascript
CODE:
import { createPlaywrightRouter, Dataset } from 'crawlee';

// createPlaywrightRouter() is only a helper to get better
// intellisense and typings. You can use Router.create() too.
export const router = createPlaywrightRouter();

// This replaces the request.label === DETAIL branch of the if clause.
router.addHandler('DETAIL', async ({ request, page, log }) => {
    log.debug(`Extracting data: ${request.url}`);
    const urlPart = request.url.split('/').slice(-1);
    const manufacturer = urlPart[0].split('-')[0];

    const title = await page.locator('.product-meta h1').textContent();
    const sku = await page
        .locator('span.product-meta__sku-number')
        .textContent();

    const priceElement = page
        .locator('span.price')
        .filter({
            hasText: '$',
        })
        .first();

    const currentPriceString = await priceElement.textContent();
    const rawPrice = currentPriceString.split('$')[1];
    const price = Number(rawPrice.replaceAll(',', ''));

    const inStockElement = page
        .locator('span.product-form__inventory')
        .filter({
            hasText: 'In stock',
        })
        .first();

    const inStock = (await inStockElement.count()) > 0;

    const results = {
        url: request.url,
        manufacturer,
        title,
        sku,
        currentPrice: price,
        availableInStock: inStock,
    };

    log.debug(`Saving data: ${request.url}`);
    await Dataset.pushData(results);
});

router.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {
    log.debug(`Enqueueing pagination for: ${request.url}`);
    await page.waitForSelector('.product-item > a');
    await enqueueLinks({
        selector: '.product-item > a',
        label: 'DETAIL',
    });

    const nextButton = await page.$('a.pagination__next');
    if (nextButton) {
        await enqueueLinks({
            selector: 'a.pagination__next',
            label: 'CATEGORY',
        });
    }
});

router.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {
    log.debug(`Enqueueing categories from page: ${request.url}`);
    await page.waitForSelector('.collection-block-item');
    await enqueueLinks({
        selector: '.collection-block-item',
        label: 'CATEGORY',
    });
});

----------------------------------------

TITLE: Implementing Crawlee Router with Route Handlers
DESCRIPTION: Defines route handlers for different page types (detail, category, and default) using Crawlee's router system. Includes logic for data extraction and link enqueueing.

LANGUAGE: javascript
CODE:
import { createPlaywrightRouter, Dataset } from 'crawlee';

// createPlaywrightRouter() is only a helper to get better
// intellisense and typings. You can use Router.create() too.
export const router = createPlaywrightRouter();

// This replaces the request.label === DETAIL branch of the if clause.
router.addHandler('DETAIL', async ({ request, page, log }) => {
    log.debug(`Extracting data: ${request.url}`);
    const urlPart = request.url.split('/').slice(-1);
    const manufacturer = urlPart[0].split('-')[0];

    const title = await page.locator('.product-meta h1').textContent();
    const sku = await page
        .locator('span.product-meta__sku-number')
        .textContent();

    const priceElement = page
        .locator('span.price')
        .filter({
            hasText: '$',
        })
        .first();

    const currentPriceString = await priceElement.textContent();
    const rawPrice = currentPriceString.split('$')[1];
    const price = Number(rawPrice.replaceAll(',', ''));

    const inStockElement = page
        .locator('span.product-form__inventory')
        .filter({
            hasText: 'In stock',
        })
        .first();

    const inStock = (await inStockElement.count()) > 0;

    const results = {
        url: request.url,
        manufacturer,
        title,
        sku,
        currentPrice: price,
        availableInStock: inStock,
    };

    log.debug(`Saving data: ${request.url}`);
    await Dataset.pushData(results);
});

router.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {
    log.debug(`Enqueueing pagination for: ${request.url}`);
    await page.waitForSelector('.product-item > a');
    await enqueueLinks({
        selector: '.product-item > a',
        label: 'DETAIL',
    });

    const nextButton = await page.$('a.pagination__next');
    if (nextButton) {
        await enqueueLinks({
            selector: 'a.pagination__next',
            label: 'CATEGORY',
        });
    }
});

router.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {
    log.debug(`Enqueueing categories from page: ${request.url}`);
    await page.waitForSelector('.collection-block-item');
    await enqueueLinks({
        selector: '.collection-block-item',
        label: 'CATEGORY',
    });
});

----------------------------------------

TITLE: Installing TypeScript Compiler for Crawlee Project
DESCRIPTION: Install the TypeScript compiler as a development dependency for a Crawlee project.

LANGUAGE: shell
CODE:
npm install --save-dev typescript

----------------------------------------

TITLE: Basic URL Structure Example
DESCRIPTION: Example of a pagination URL structure showing how page numbers are appended to the base URL.

LANGUAGE: plaintext
CODE:
https://warehouse-theme-metal.myshopify.com/collections/headphones?page=2

----------------------------------------

TITLE: Purging Default Storages in Crawlee
DESCRIPTION: Demonstrates how to explicitly purge default storages in Crawlee using the purgeDefaultStorages() helper function.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Comparing Cheerio and Browser JavaScript for DOM Manipulation
DESCRIPTION: This snippet demonstrates how to use Cheerio's $ function to manipulate HTML elements, comparing it with plain browser JavaScript. It shows how to select the title element and extract href attributes from links.

LANGUAGE: typescript
CODE:
// Return the text content of the <title> element.
document.querySelector('title').textContent; // plain JS
$('title').text(); // Cheerio

// Return an array of all 'href' links on the page.
Array.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS
$('[href]')
    .map((i, el) => $(el).attr('href'))
    .get(); // Cheerio

----------------------------------------

TITLE: Initializing RequestQueue in Crawlee
DESCRIPTION: Creates a RequestQueue instance and adds a URL to crawl. This demonstrates the basic setup for managing crawling requests.

LANGUAGE: typescript
CODE:
import { RequestQueue } from 'crawlee';

// First you create the request queue instance.
const requestQueue = await RequestQueue.open();
// And then you add one or more requests to it.
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

----------------------------------------

TITLE: Building Basic CheerioCrawler with RequestQueue
DESCRIPTION: Creates a complete CheerioCrawler implementation that uses a RequestQueue to crawl a URL and extract its title using Cheerio parser.

LANGUAGE: typescript
CODE:
// Add import of CheerioCrawler
import { RequestQueue, CheerioCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

// Create the crawler and add the queue with our URL
// and a request handler to process the page.
const crawler = new CheerioCrawler({
    requestQueue,
    // The `$` argument is the Cheerio object
    // which contains parsed HTML of the website.
    async requestHandler({ $, request }) {
        // Extract <title> text with Cheerio.
        // See Cheerio documentation for API docs.
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

// Start the crawler and wait for it to finish
await crawler.run();

----------------------------------------

TITLE: Configuring Concurrency Limits in Crawlee
DESCRIPTION: This code shows how to set minimum and maximum concurrency for a CheerioCrawler in Crawlee. It sets a minimum of 5 concurrent requests and a maximum of 100.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    // other options
    minConcurrency: 5,
    maxConcurrency: 100,
});

----------------------------------------

TITLE: Configuring Concurrency Limits in Crawlee
DESCRIPTION: This code shows how to set minimum and maximum concurrency for a CheerioCrawler in Crawlee. It sets a minimum of 5 concurrent requests and a maximum of 100.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    // other options
    minConcurrency: 5,
    maxConcurrency: 100,
});

----------------------------------------

TITLE: Crawling Multiple URLs with Cheerio Crawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use Cheerio Crawler to crawl multiple specified URLs. It extracts the title and URL from each page and stores the results.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks, log }) {
        const title = $('title').text();
        log.info(`Title of ${request.url} is '${title}'`);

        await Dataset.pushData({
            title,
            url: request.url,
        });
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run([
    'https://crawlee.dev',
    'https://crawlee.dev/docs/quick-start',
    'https://crawlee.dev/docs/introduction/first-steps',
]);

----------------------------------------

TITLE: Configuring INPUT.json File Path in Crawlee
DESCRIPTION: Shows the file path structure for providing input to Crawlee actors through the default key-value store. The INPUT.json file must be placed in the project's storage directory.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json

----------------------------------------

TITLE: Initializing Basic Crawler with sendRequest in Crawlee
DESCRIPTION: Demonstrates a basic web crawler implementation using Crawlee's BasicCrawler class. The crawler downloads web pages using HTTP requests via the sendRequest utility function and stores the raw HTML and URLs in the default dataset directory.

LANGUAGE: typescript
CODE:
{BasicCrawlerSource}

----------------------------------------

TITLE: Extracting Product SKU with Playwright
DESCRIPTION: This snippet demonstrates how to use Playwright to find and extract the product SKU from the page.

LANGUAGE: javascript
CODE:
const sku = await page.locator('span.product-meta__sku-number').textContent();

----------------------------------------

TITLE: Updated Crawling Context Implementation
DESCRIPTION: Demonstrates the new unified crawling context approach where all handler functions share the same context object.

LANGUAGE: javascript
CODE:
const handlePageFunction = async (crawlingContext1) => {
    crawlingContext1.hasOwnProperty('proxyInfo') // true
}

const handleFailedRequestFunction = async (crawlingContext2) => {
    crawlingContext2.hasOwnProperty('proxyInfo') // true
}

// All contexts are the same object.
crawlingContext1 === crawlingContext2 // true

----------------------------------------

TITLE: Initializing Apify Project Configuration
DESCRIPTION: Command to initialize the Apify project configuration, creating necessary files for deployment to the Apify Platform.

LANGUAGE: bash
CODE:
apify init

----------------------------------------

TITLE: Installing and Packaging Browser Dependencies for AWS Lambda
DESCRIPTION: Commands for installing the @sparticuz/chromium package and zipping dependencies for AWS Lambda Layer deployment.

LANGUAGE: bash
CODE:
# Install the package
npm i -S @sparticuz/chromium

# Zip the dependencies
zip -r dependencies.zip ./node_modules

----------------------------------------

TITLE: Setting Apify API Token in Configuration
DESCRIPTION: Demonstrates how to set the Apify API token using the Configuration instance. This allows access to Apify platform features in your code.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const sdk = new Actor({ token: 'your_api_token' });

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Router in JavaScript
DESCRIPTION: Sets up a PlaywrightCrawler instance using a router for request handling and configures logging. This code demonstrates the main structure of the crawler.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Dataset Storage Path
DESCRIPTION: Shows the default storage path where Crawlee saves dataset files in the project directory.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/datasets/default/

----------------------------------------

TITLE: Domain Strategy Configuration with enqueueLinks
DESCRIPTION: Configuration example for including subdomains in the crawl using the same-domain strategy.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    strategy: 'same-domain'
});

----------------------------------------

TITLE: Disabling Browser Fingerprints in PlaywrightCrawler
DESCRIPTION: Example showing how to disable browser fingerprints in PlaywrightCrawler by setting useFingerprints option to false in browserPoolOptions.

LANGUAGE: javascript
CODE:
{PlaywrightFingerprintsOffSource}

----------------------------------------

TITLE: Using sendRequest with BasicCrawler in Crawlee
DESCRIPTION: Demonstrates how to use the sendRequest function within a BasicCrawler's requestHandler to make HTTP requests.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest();
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Using sendRequest with BasicCrawler in Crawlee
DESCRIPTION: Demonstrates how to use the sendRequest function within a BasicCrawler's requestHandler to make HTTP requests.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest();
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: TypeScript Configuration
DESCRIPTION: TSConfig setup extending @apify/tsconfig with ES2022 module and target settings

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist"
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Configuring Crawlee to Crawl All Links
DESCRIPTION: Shows how to set up enqueueLinks to follow all links regardless of domain using the 'all' strategy.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    strategy: 'all', // wander the internet
});

----------------------------------------

TITLE: Adding Requests in Batches to Request Queue in Crawlee
DESCRIPTION: Demonstrates how to add multiple requests to a Request Queue in a single operation using the addRequests() function in Crawlee.

LANGUAGE: javascript
CODE:
import { RequestQueue, PuppeteerCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();

// Add multiple requests to the queue in a single operation
await requestQueue.addRequests([
    { url: 'http://example.com/page-1' },
    { url: 'http://example.com/page-2' },
    { url: 'http://example.com/page-3' },
]);

const crawler = new PuppeteerCrawler({
    requestQueue,
    async requestHandler({ page, request, enqueueLinks }) {
        // Add newly discovered links to the queue
        await enqueueLinks();

        // Process the page
    },
});

await crawler.run();

----------------------------------------

TITLE: Getting Text Content with Cheerio
DESCRIPTION: Shows how to extract text content from an h2 element using Cheerio's selector syntax.

LANGUAGE: javascript
CODE:
$('h2').text()

----------------------------------------

TITLE: Installing Apify SDK v1 with Browser Dependencies
DESCRIPTION: Instructions for installing Apify SDK v1 with either Puppeteer or Playwright as browser automation dependencies.

LANGUAGE: bash
CODE:
npm install apify puppeteer

LANGUAGE: bash
CODE:
npm install apify playwright

----------------------------------------

TITLE: Checking Node.js Version
DESCRIPTION: Command to verify installed Node.js version, which should be 16.0 or higher

LANGUAGE: bash
CODE:
node -v

----------------------------------------

TITLE: Getting Public URL for Storage Item
DESCRIPTION: Example of generating a public URL for an item stored in Key-Value Store

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'apify';

const store = await KeyValueStore.open();
await store.setValue('your-file', { foo: 'bar' });
const url = store.getPublicUrl('your-file');
// https://api.apify.com/v2/key-value-stores/<your-store-id>/records/your-file

----------------------------------------

TITLE: Accepting and Logging User Input in JavaScript with Crawlee
DESCRIPTION: This code snippet demonstrates how to accept user input in a Crawlee project and log it to the console. It uses the Actor.getInput() method to retrieve the input and console.log() to display it.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

await Actor.init();

const input = await Actor.getInput();
console.log('Input:');
console.log(input);

await Actor.exit();

----------------------------------------

TITLE: Crawling Same Hostname Links with Cheerio Crawler
DESCRIPTION: Implementation of a CheerioCrawler that only enqueues links from the same hostname using the 'same-hostname' strategy. This is the default behavior that matches relative URLs and URLs pointing to the same hostname.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks }) {
        // Add links from the same hostname to the queue
        await enqueueLinks({
            strategy: EnqueueStrategy.SameHostname,
        });
    },
});

----------------------------------------

TITLE: Map Method Result Example
DESCRIPTION: Example output from the Dataset map method showing an array of heading counts where the count is greater than 5.

LANGUAGE: javascript
CODE:
[11, 8]

----------------------------------------

TITLE: Authenticating with Apify CLI
DESCRIPTION: Command to log in to the Apify Platform using the CLI tool.

LANGUAGE: bash
CODE:
apify login

----------------------------------------

TITLE: Custom Link Selection with enqueueLinks
DESCRIPTION: Example of using enqueueLinks with custom selector for finding links.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    selector: 'div.has-link'
});

----------------------------------------

TITLE: Installing @apify/tsconfig for Crawlee TypeScript Project
DESCRIPTION: Install @apify/tsconfig as a development dependency for a Crawlee TypeScript project.

LANGUAGE: shell
CODE:
npm install --save-dev @apify/tsconfig

----------------------------------------

TITLE: Configuring Crawlee with Custom Storage Settings
DESCRIPTION: Initial code setup showing how to initialize a PlaywrightCrawler with custom Configuration to prevent storage interference between Lambda instances.

LANGUAGE: javascript
CODE:
// For more information, see https://crawlee.dev/
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Configuring minConcurrency and maxConcurrency in Crawlee CheerioCrawler
DESCRIPTION: This code snippet shows how to set the minConcurrency and maxConcurrency options in a CheerioCrawler to control the number of parallel requests.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    minConcurrency: 1,
    maxConcurrency: 100,
    // ...
});

----------------------------------------

TITLE: Installing and Authenticating with Apify CLI
DESCRIPTION: Commands to install Apify CLI globally and authenticate with an API token

LANGUAGE: bash
CODE:
npm install -g apify-cli
apify login -t YOUR_API_TOKEN

----------------------------------------

TITLE: Configuring Crawlee using crawlee.json in JSON
DESCRIPTION: This snippet shows how to set configuration options using a crawlee.json file. It sets the persistStateIntervalMillis to 10000 and logLevel to DEBUG.

LANGUAGE: json
CODE:
{
  "persistStateIntervalMillis": 10000,
  "logLevel": "DEBUG"
}

----------------------------------------

TITLE: Configuring Package.json for GCP Deployment
DESCRIPTION: Updates the package.json configuration to specify the main entry point for the GCP function.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "version": "1.0.0",
    "main": "src/main.js",
    ...
}

----------------------------------------

TITLE: Initializing Bluesky API Scraper Class
DESCRIPTION: Defines a BlueskyApiScraper class with methods for creating and deleting API sessions, and initializing the crawler with proper configurations.

LANGUAGE: python
CODE:
import asyncio
import json
import os
import traceback

import httpx
from yarl import URL

from crawlee import ConcurrencySettings, Request
from crawlee.configuration import Configuration
from crawlee.crawlers import HttpCrawler, HttpCrawlingContext
from crawlee.http_clients import HttpxHttpClient
from crawlee.storages import Dataset

# Environment variables for authentication
# BLUESKY_APP_PASSWORD: App-specific password generated from Bluesky settings
# BLUESKY_IDENTIFIER: Your Bluesky handle (e.g., username.bsky.social)
BLUESKY_APP_PASSWORD = os.getenv('BLUESKY_APP_PASSWORD')
BLUESKY_IDENTIFIER = os.getenv('BLUESKY_IDENTIFIER')


class BlueskyApiScraper:
    """A scraper class for extracting data from Bluesky social network using their official API.

    This scraper manages authentication, concurrent requests, and data collection for both
    posts and user profiles. It uses separate datasets for storing post and user information.
    """

    def __init__(self) -> None:
        self._crawler: HttpCrawler | None = None

        self._users: Dataset | None = None
        self._posts: Dataset | None = None

        # Variables for storing session data
        self._service_endpoint: str | None = None
        self._user_did: str | None = None
        self._access_token: str | None = None
        self._refresh_token: str | None = None
        self._handle: str | None = None

    def create_session(self) -> None:
        """Create credentials for the session."""
        url = 'https://bsky.social/xrpc/com.atproto.server.createSession'
        headers = {
            'Content-Type': 'application/json',
        }
        data = {'identifier': BLUESKY_IDENTIFIER, 'password': BLUESKY_APP_PASSWORD}

        response = httpx.post(url, headers=headers, json=data)
        response.raise_for_status()

        data = response.json()

        self._service_endpoint = data['didDoc']['service'][0]['serviceEndpoint']
        self._user_did = data['didDoc']['id']
        self._access_token = data['accessJwt']
        self._refresh_token = data['refreshJwt']
        self._handle = data['handle']

    def delete_session(self) -> None:
        """Delete the current session."""
        url = f'{self._service_endpoint}/xrpc/com.atproto.server.deleteSession'
        headers = {'Content-Type': 'application/json', 'authorization': f'Bearer {self._refresh_token}'}

        response = httpx.post(url, headers=headers)
        response.raise_for_status()

    async def init_crawler(self) -> None:
        """Initialize the crawler."""
        if not self._user_did:
            raise ValueError('Session not created.')

        # Initialize the datasets purge the data if it is not empty
        self._users = await Dataset.open(name='users', configuration=Configuration(purge_on_start=True))
        self._posts = await Dataset.open(name='posts', configuration=Configuration(purge_on_start=True))

        # Initialize the crawler
        self._crawler = HttpCrawler(
            max_requests_per_crawl=100,
            http_client=HttpxHttpClient(
                # Set headers for API requests
                headers={
                    'Content-Type': 'application/json',
                    'Authorization': f'Bearer {self._access_token}',
                    'Connection': 'Keep-Alive',
                    'accept-encoding': 'gzip, deflate, br, zstd',
                }
            ),
            # Configuring concurrency of crawling requests
            concurrency_settings=ConcurrencySettings(
                min_concurrency=10,
                desired_concurrency=10,
                max_concurrency=30,
                max_tasks_per_minute=200,
            ),
        )

        self._crawler.router.default_handler(self._search_handler)  # Handler for search requests
        self._crawler.router.handler(label='user')(self._user_handler)  # Handler for user requests

----------------------------------------

TITLE: Configuring Apify Proxy
DESCRIPTION: Example demonstrating how to set up and use Apify Proxy with specific configurations

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
    countryCode: 'US',
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Custom Link Selection with enqueueLinks
DESCRIPTION: Example of using enqueueLinks with a custom selector to find specific links on the page.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    selector: 'div.has-link'
});

----------------------------------------

TITLE: Disabling Browser Fingerprints with PlaywrightCrawler
DESCRIPTION: This code snippet demonstrates how to disable the use of browser fingerprints in PlaywrightCrawler by setting the useFingerprints option to false.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        useFingerprints: false
    },
    // ...
});

----------------------------------------

TITLE: Enhanced GraphQL Query for Multilingual Post Data in Python
DESCRIPTION: This snippet showcases an improved GraphQL query that retrieves post data in multiple languages and includes the internal text of publications. It demonstrates how to modify request parameters to obtain more comprehensive data.

LANGUAGE: python
CODE:
import requests

url = "https://restoran.ua/graphql"

data = {
    "operationName": "Posts_PostsForView",
    "variables": {"sort": {"sortBy": ["startAt_DESC"]}},
    "query": """query Posts_PostsForView(
    $where: PostForViewWhereInput,
    $sort: PostForViewSortInput,
    $pagination: PaginationInput,
    $search: String,
    $token: String,
    $coordinates_slice: SliceInput)
    {
        PostsForView(
                where: $where
                sort: $sort
                pagination: $pagination
                search: $search
                token: $token
                ) {
                        id
                        uk_title: ukTitle
                        en_title: enTitle
                        summary: ukSummary
                        slug
                        startAt
                        endAt
                        newsFeed
                        events
                        journal
                        toProfessionals
                        photoHeader {
                            address: mobile
                            __typename
                            }
                        mixedBlocks {
                            index
                            en_text: enText
                            uk_text: ukText
                            __typename
                            }
                        coordinates(slice: $coordinates_slice) {
                            lng
                            lat
                            __typename
                            }
                        __typename
                    }
    }"""
}

response = requests.post(url, json=data)
print(response.json())

----------------------------------------

TITLE: TypeScript Configuration Setup
DESCRIPTION: TSConfig configuration extending @apify/tsconfig with ES2022 module system and browser support options

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist"
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Configuring Headful Browser Mode
DESCRIPTION: TypeScript configuration option to enable visible browser window during crawling

LANGUAGE: typescript
CODE:
// Uncomment this option to see the browser window.
headless: false

----------------------------------------

TITLE: File System Storage Path Structure in Crawlee
DESCRIPTION: Demonstrates the file system path structure used by Crawlee for storing key-value data locally.

LANGUAGE: plaintext
CODE:
{CRAWLEE_STORAGE_DIR}/key_value_stores/{STORE_ID}/{KEY}.{EXT}

----------------------------------------

TITLE: Custom Proxy Function Implementation
DESCRIPTION: Example of implementing a custom proxy selection function based on URL patterns.

LANGUAGE: javascript
CODE:
const proxyConfiguration = new ProxyConfiguration({
    newUrlFunction: (sessionId, { request }) => {
        if (request?.url.includes('crawlee.dev')) {
            return null; // for crawlee.dev, we don't use a proxy
        }

        return 'http://proxy-1.com'; // for all other URLs, we use this proxy
    }
});

----------------------------------------

TITLE: Wrapping Crawlee Code in AWS Lambda Handler
DESCRIPTION: Complete JavaScript code example showing how to wrap the Crawlee crawler initialization and execution within an AWS Lambda handler function for deployment.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
        launchContext: {
            launchOptions: {
                executablePath: await aws_chromium.executablePath(),
                args: aws_chromium.args,
                headless: true
            }
        }
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);

    return {
        statusCode: 200,
        body: await crawler.getData(),
    };
}

----------------------------------------

TITLE: Viewing Dataset Storage Location
DESCRIPTION: Shows the default storage path where dataset items are saved as individual files.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/datasets/default/

----------------------------------------

TITLE: Using Proxy with BasicCrawler
DESCRIPTION: Shows how to configure a proxy server when making requests with BasicCrawler.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            proxyUrl: 'http://auto:password@proxy.apify.com:8000',
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Basic Cheerio Crawler Implementation in TypeScript
DESCRIPTION: Initial implementation of a simple Cheerio crawler that downloads a single page and extracts its title.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Initializing Apify Project Configuration
DESCRIPTION: Command to initialize the Apify project configuration, creating the necessary files for deployment to the Apify Platform.

LANGUAGE: bash
CODE:
apify init

----------------------------------------

TITLE: Using Proxy with BasicCrawler
DESCRIPTION: Shows how to configure a proxy server when making requests with BasicCrawler.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            proxyUrl: 'http://auto:password@proxy.apify.com:8000',
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Saving Data with Dataset.pushData()
DESCRIPTION: Demonstrates how to save scraped results to the default Dataset using the pushData() method.

LANGUAGE: javascript
CODE:
await Dataset.pushData(results);

----------------------------------------

TITLE: Creating and Running an Apify Actor
DESCRIPTION: Commands to create and run a new actor project using Apify CLI

LANGUAGE: bash
CODE:
apify create my-hello-world\ncd my-hello-world\napify run

----------------------------------------

TITLE: Implementing AWS Lambda Handler for Cheerio Crawler
DESCRIPTION: Creates an AWS Lambda handler function that initializes and runs the Cheerio crawler. Demonstrates basic handler structure with event and context parameters.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
};

----------------------------------------

TITLE: Storage Directory Structure Example
DESCRIPTION: Shows the file path structure for key-value stores in Crawlee storage system

LANGUAGE: plaintext
CODE:
{CRAWLEE_STORAGE_DIR}/key_value_stores/{STORE_ID}/{KEY}.{EXT}

----------------------------------------

TITLE: Configuring Apify Proxy in JavaScript
DESCRIPTION: Example of creating a proxy configuration for Apify Proxy with specific settings.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
    countryCode: 'US',
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Creating AWS Lambda Handler for Cheerio Crawler
DESCRIPTION: Wraps the crawler initialization and execution in an AWS Lambda handler function, maintaining stateless execution.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
};

----------------------------------------

TITLE: Scraping Product Title with Playwright in JavaScript
DESCRIPTION: This code snippet shows how to use Playwright to extract the product title from a webpage using a CSS selector.

LANGUAGE: javascript
CODE:
const title = await page.locator('.product-meta h1').textContent();

----------------------------------------

TITLE: Scraping JavaScript-Rendered Content with PuppeteerCrawler
DESCRIPTION: This snippet demonstrates how to scrape JavaScript-rendered content using PuppeteerCrawler. It explicitly waits for the element to appear before extracting its text content.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // Wait for the actor cards to render
        await page.waitForSelector('.ActorStoreItem');
        // Extract text content of the first actor card
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Crawling All Links with Cheerio Crawler in TypeScript
DESCRIPTION: This snippet demonstrates how to use Cheerio Crawler to crawl all links on a website. It initializes a RequestQueue, creates a CheerioCrawler, and uses the enqueueLinks() method to add new links to the queue as it crawls.

LANGUAGE: typescript
CODE:
import { CheerioCrawler, enqueueLinks } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}`);

        // Add all links from page to RequestQueue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limit to 10 requests
});

// Run the crawler
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Adapting Crawlee Scraper Routes for Parallel Processing
DESCRIPTION: This code modifies the CATEGORY route handler to enqueue product URLs to a shared request queue instead of processing them immediately. It prepares the scraper for parallel execution.

LANGUAGE: javascript
CODE:
CATEGORY: async ({ enqueueLinks, log, request }) => {
    log.info(`Processing category: ${request.url}`);
    const queue = await getOrInitQueue();

    await enqueueLinks({
        globs: ['https://crawlee.dev/docs/**'],
        label: 'DETAIL',
        forefront: true,
        requestQueue: queue,
        transformRequestFunction: (req) => {
            req.uniqueKey = `${req.url}#${req.userData.name}`;
            return req;
        },
    });
},

----------------------------------------

TITLE: Installing Crawlee with CLI
DESCRIPTION: Commands to install and start a Crawlee project using the Crawlee CLI.

LANGUAGE: bash
CODE:
npx crawlee create my-crawler

LANGUAGE: bash
CODE:
cd my-crawler && npm start

----------------------------------------

TITLE: Capturing Screenshot Using Puppeteer's page.screenshot()
DESCRIPTION: This snippet demonstrates how to capture a screenshot of a web page using Puppeteer's page.screenshot() method. It launches a browser, navigates to a URL, takes a screenshot, and saves it to a key-value store.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';
import { launchPuppeteer } from 'crawlee/puppeteer';

const browser = await launchPuppeteer();
const page = await browser.newPage();

const url = 'https://crawlee.dev';
await page.goto(url);

const screenshot = await page.screenshot();
const key = `screenshot-${new Date().getTime()}.png`;

await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });

await browser.close();

----------------------------------------

TITLE: Header Generator Configuration
DESCRIPTION: Shows how to configure browser fingerprint generation with custom header generator options.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            headerGeneratorOptions: {
                devices: ['mobile', 'desktop'],
                locales: ['en-US'],
                operatingSystems: ['windows', 'macos', 'android', 'ios'],
                browsers: ['chrome', 'edge', 'firefox', 'safari'],
            },
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Basic Proxy Configuration in Crawlee
DESCRIPTION: Demonstrates how to initialize basic proxy configuration with static proxy URLs in Crawlee.

LANGUAGE: javascript
CODE:
import { ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ]
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Extracting Manufacturer from URL in JavaScript
DESCRIPTION: This snippet demonstrates how to extract the manufacturer name from a product URL using string manipulation in JavaScript.

LANGUAGE: javascript
CODE:
// request.url = https://warehouse-theme-metal.myshopify.com/products/sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440

const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']
const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'

----------------------------------------

TITLE: Capturing Screenshot Using PuppeteerCrawler with Utils
DESCRIPTION: Shows how to capture screenshots using the utils.puppeteer.saveSnapshot() utility function within a PuppeteerCrawler. This method provides additional context awareness and simplified screenshot capture.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, log, utils }) {
        const { url, userData: { label } } = request;
        log.info('Label:', { label });

        // Capture the screenshot
        await utils.puppeteer.saveSnapshot(page, { key: 'snapshot', saveHtml: false });
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Installing and Logging in with Apify CLI
DESCRIPTION: Commands to install the Apify CLI tool and log in with an API token

LANGUAGE: bash
CODE:
npm install -g apify-cli
apify login -t YOUR_API_TOKEN

----------------------------------------

TITLE: Customizing Browser Fingerprints with PuppeteerCrawler
DESCRIPTION: This snippet shows how to customize browser fingerprints using PuppeteerCrawler in Crawlee. It configures specific options for the browser, operating system, and other parameters to generate custom fingerprints.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    // ...
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                browsers: [
                    { name: 'firefox', minVersion: 88 },
                    { name: 'chrome', minVersion: 94 },
                ],
                devices: [
                    'desktop',
                ],
                operatingSystems: [
                    'windows',
                ],
            },
        },
    },
});

----------------------------------------

TITLE: Initializing Apify Project
DESCRIPTION: Command to initialize the project for Apify, creating necessary configuration files.

LANGUAGE: bash
CODE:
apify init

----------------------------------------

TITLE: Dataset Storage Location - Bash
DESCRIPTION: Shows the default storage location for dataset files within the project directory structure. Each item is saved as a separate file in this location.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/datasets/default/

----------------------------------------

TITLE: Initializing Apify Project
DESCRIPTION: Command to initialize the project for Apify, creating necessary configuration files.

LANGUAGE: bash
CODE:
apify init

----------------------------------------

TITLE: Modifying Crawlee Project for Apify Platform Deployment
DESCRIPTION: Code modifications to integrate Apify SDK into a Crawlee project, enabling use of Apify Platform storage and services when deployed.

LANGUAGE: typescript
CODE:
import { Actor } from 'apify';
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

await Actor.init();

log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

await Actor.exit();

----------------------------------------

TITLE: Crawling Multiple URLs with Puppeteer Crawler in JavaScript
DESCRIPTION: This code snippet shows how to use Puppeteer Crawler to crawl a list of specified URLs. It includes setup for the crawler and defines the request handler. Note that it requires the 'apify/actor-node-puppeteer-chrome' image when running on the Apify Platform.

LANGUAGE: javascript
CODE:
// The code for this snippet is not directly visible in the provided text.
// It is referenced as an external file: './crawl_multiple_urls_puppeteer.ts'

----------------------------------------

TITLE: Creating a New Crawlee Project using CLI
DESCRIPTION: Command to create a new Crawlee project using the Crawlee CLI via npx.

LANGUAGE: bash
CODE:
npx crawlee create my-crawler

----------------------------------------

TITLE: Configuring Apify Token Programmatically
DESCRIPTION: Example showing how to initialize Actor with an API token in code

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const sdk = new Actor({ token: 'your_api_token' });

----------------------------------------

TITLE: Using CookieJar with BasicCrawler in TypeScript
DESCRIPTION: This example demonstrates how to use a CookieJar with BasicCrawler for managing cookies. It imports the CookieJar from tough-cookie and passes it to sendRequest for handling cookie-based sessions.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';
import { CookieJar } from 'tough-cookie';

const cookieJar = new CookieJar();

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ cookieJar });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Logging in to Apify Platform via CLI
DESCRIPTION: Command to authenticate with the Apify Platform using the Apify CLI tool.

LANGUAGE: bash
CODE:
apify login

----------------------------------------

TITLE: Package.json Configuration for Apify Docker Images
DESCRIPTION: Shows the recommended package.json configuration when using Apify Docker images, using an asterisk for the automation library version.

LANGUAGE: json
CODE:
{
    "dependencies": {
        "crawlee": "^3.0.0",
        "playwright": "*"
    }
}

----------------------------------------

TITLE: Installing ts-node for Development in Crawlee Project
DESCRIPTION: Command to install ts-node as a development dependency for running TypeScript code directly in a Crawlee project.

LANGUAGE: shell
CODE:
npm install --save-dev ts-node

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Router in JavaScript
DESCRIPTION: Sets up a PlaywrightCrawler instance using a Router for request handling. It configures logging and starts the crawler with a specified URL.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Setting Request Limit in Cheerio Crawler
DESCRIPTION: Example showing how to limit the maximum number of requests per crawl using maxRequestsPerCrawl option.

LANGUAGE: typescript
CODE:
const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 20,
    // ...
});

----------------------------------------

TITLE: Failed Attempt to Scrape JavaScript-Rendered Content with PuppeteerCrawler
DESCRIPTION: This snippet demonstrates an unsuccessful attempt to scrape JavaScript-rendered content using PuppeteerCrawler without waiting for the element to appear. It results in an error due to the element not being present in the DOM yet.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // Try to extract text content of the first actor card without waiting
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Capturing Screenshots with PuppeteerCrawler using page.screenshot()
DESCRIPTION: This snippet demonstrates how to capture screenshots of multiple web pages using PuppeteerCrawler with page.screenshot(). It sets up a crawler to visit multiple URLs, take screenshots, and save them to a key-value store.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';
import { PuppeteerCrawler } from 'crawlee';

await Actor.init();

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);

        const screenshot = await page.screenshot();
        const key = `${new URL(request.url).hostname}.png`;
        await Actor.setValue(key, screenshot, { contentType: 'image/png' });
    },
});

await crawler.run(['https://crawlee.dev', 'https://apify.com']);

await Actor.exit();

----------------------------------------

TITLE: HTTP Crawler Implementation using Crawlee
DESCRIPTION: Demonstrates how to use HttpCrawler to crawl a list of URLs and save HTML content. The code shows the basic setup and usage of HttpCrawler class from Crawlee framework.

LANGUAGE: javascript
CODE:
import { HttpCrawler, Dataset } from 'crawlee';
import { readFile } from 'fs/promises';

// Create an instance of the HttpCrawler class - a crawler
// that automatically loads the URLs using plain HTTP requests
const crawler = new HttpCrawler({
    // Let's limit our crawls to make our example easier to run
    maxRequestsPerCrawl: 10,
    async requestHandler({ request, body, $ }) {
        // Save the HTML of each loaded page to a dataset
        await Dataset.pushData({
            url: request.url,
            html: body.toString(),
        });
    },
});

// Load the list of URLs from a text file
const urls = (await readFile('urls.txt', 'utf8')).split('\n');

// Run the crawler
await crawler.run(urls);

----------------------------------------

TITLE: HTTP Crawler Implementation using Crawlee
DESCRIPTION: Demonstrates how to use HttpCrawler to crawl a list of URLs and save HTML content. The code shows the basic setup and usage of HttpCrawler class from Crawlee framework.

LANGUAGE: javascript
CODE:
import { HttpCrawler, Dataset } from 'crawlee';
import { readFile } from 'fs/promises';

// Create an instance of the HttpCrawler class - a crawler
// that automatically loads the URLs using plain HTTP requests
const crawler = new HttpCrawler({
    // Let's limit our crawls to make our example easier to run
    maxRequestsPerCrawl: 10,
    async requestHandler({ request, body, $ }) {
        // Save the HTML of each loaded page to a dataset
        await Dataset.pushData({
            url: request.url,
            html: body.toString(),
        });
    },
});

// Load the list of URLs from a text file
const urls = (await readFile('urls.txt', 'utf8')).split('\n');

// Run the crawler
await crawler.run(urls);

----------------------------------------

TITLE: Crawling with CheerioCrawler
DESCRIPTION: Example of using CheerioCrawler to recursively crawl the Crawlee website and extract page titles.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        await Dataset.pushData({
            url: request.url,
            title,
        });
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Configuring Advanced Autoscaling Pool Options in Crawlee
DESCRIPTION: This snippet demonstrates how to set advanced autoscaling pool options for a CheerioCrawler in Crawlee. It includes various parameters to fine-tune the autoscaling behavior.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    // other options
    autoscaledPoolOptions: {
        desiredConcurrency: 10,
        desiredConcurrencyRatio: 0.9,
        scaleUpStepRatio: 0.05,
        scaleDownStepRatio: 0.05,
        maybeRunIntervalSecs: 1,
        loggingIntervalSecs: 60,
        autoscaleIntervalSecs: 10,
        maxTasksPerMinute: 200,
    },
});

----------------------------------------

TITLE: Header Generator Configuration
DESCRIPTION: Shows how to configure the header generator options for browser fingerprinting.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            headerGeneratorOptions: {
                devices: ['mobile', 'desktop'],
                locales: ['en-US'],
                operatingSystems: ['windows', 'macos', 'android', 'ios'],
                browsers: ['chrome', 'edge', 'firefox', 'safari'],
            },
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Configuring Header Generator Options in sendRequest
DESCRIPTION: Shows how to configure header generator options in sendRequest to customize the generated browser fingerprint. This example demonstrates setting specific devices, locales, operating systems, and browsers for the header generation.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            headerGeneratorOptions: {
                devices: ['mobile', 'desktop'],
                locales: ['en-US'],
                operatingSystems: ['windows', 'macos', 'android', 'ios'],
                browsers: ['chrome', 'edge', 'firefox', 'safari'],
            },
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Importing Dataset Module in Crawlee (TypeScript)
DESCRIPTION: This snippet shows how to import the Dataset module along with PlaywrightCrawler from Crawlee. This is necessary for saving scraped data.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

----------------------------------------

TITLE: Implementing HTTP Crawler with Cheerio
DESCRIPTION: Example demonstrating how to use HttpCrawler to crawl URLs from an external file, make HTTP requests, and save HTML content. The code utilizes Cheerio for HTML parsing and includes error handling.

LANGUAGE: javascript
CODE:
{HttpCrawlerSource}

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Configuration in JavaScript
DESCRIPTION: This snippet shows how to create a new PlaywrightCrawler instance with a custom Configuration to disable persistent storage. It sets up the crawler with a router for request handling and defines start URLs.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Configuring Apify Proxy with specific groups and country
DESCRIPTION: JavaScript code to create a proxy configuration for Apify Proxy with custom settings.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
    countryCode: 'US',
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Crawling URLs with Playwright Crawler
DESCRIPTION: Implementation of a web crawler using Playwright Crawler to process multiple URLs. This version requires the apify/actor-node-playwright-chrome image when running on the Apify Platform and offers modern browser automation features.

LANGUAGE: javascript
CODE:
{PlaywrightSource}

----------------------------------------

TITLE: Attempting to Scrape JavaScript-Rendered Content with CheerioCrawler
DESCRIPTION: This snippet demonstrates an unsuccessful attempt to scrape JavaScript-rendered content using CheerioCrawler. It fails because CheerioCrawler cannot execute client-side JavaScript.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        // Extract text content of an actor card
        const actorText = $('.ActorStoreItem').text();
        console.log(`ACTOR: ${actorText}`);
    }
})

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Using sendRequest with BasicCrawler in Crawlee
DESCRIPTION: Demonstrates how to use the sendRequest function within a BasicCrawler's requestHandler to make HTTP requests. The function uses got-scraping under the hood to mimic browser requests and avoid blocking.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest();
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Creating and Running a New Actor
DESCRIPTION: Commands to create a new actor project using Apify CLI and run it locally

LANGUAGE: bash
CODE:
apify create my-hello-world
cd my-hello-world
apify run

----------------------------------------

TITLE: Using Request List with Crawler in Crawlee
DESCRIPTION: Shows how to use a request list with a crawler in Crawlee. The list is created with predefined URLs and used by the crawler.

LANGUAGE: javascript
CODE:
import { RequestList, PuppeteerCrawler } from 'crawlee';

// Prepare the sources array with URLs to visit
const sources = [
    { url: 'http://www.example.com/page-1' },
    { url: 'http://www.example.com/page-2' },
    { url: 'http://www.example.com/page-3' },
];

// Open the request list.
// List name is used to persist the sources and the list state in the key-value store
const requestList = await RequestList.open('my-list', sources);

// The crawler will automatically process requests from the list
// It's used the same way for Cheerio /Playwright crawlers.
const crawler = new PuppeteerCrawler({
    requestList,
    async requestHandler({ page, request }) {
        // Process the page (extract data, take page screenshot, etc).
        // No more requests could be added to the request list here
    },
});

----------------------------------------

TITLE: Configuring Crawlee with Custom Storage Settings
DESCRIPTION: Basic Crawlee configuration setup with custom storage settings to prevent interference between crawler instances in Lambda environment.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Using Request List with Crawler in Crawlee
DESCRIPTION: Shows how to use a request list with a crawler in Crawlee. The list is created with predefined URLs and used by the crawler.

LANGUAGE: javascript
CODE:
import { RequestList, PuppeteerCrawler } from 'crawlee';

// Prepare the sources array with URLs to visit
const sources = [
    { url: 'http://www.example.com/page-1' },
    { url: 'http://www.example.com/page-2' },
    { url: 'http://www.example.com/page-3' },
];

// Open the request list.
// List name is used to persist the sources and the list state in the key-value store
const requestList = await RequestList.open('my-list', sources);

// The crawler will automatically process requests from the list
// It's used the same way for Cheerio /Playwright crawlers.
const crawler = new PuppeteerCrawler({
    requestList,
    async requestHandler({ page, request }) {
        // Process the page (extract data, take page screenshot, etc).
        // No more requests could be added to the request list here
    },
});

----------------------------------------

TITLE: Custom Link Selector Configuration
DESCRIPTION: Example of using a custom selector with enqueueLinks to find specific elements containing links.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    selector: 'div.has-link'
});

----------------------------------------

TITLE: Adjusting Crawlee Code for Apify Platform Deployment
DESCRIPTION: Modifications to the main Crawlee script to initialize and exit the Apify Actor, enabling integration with Apify Platform storages.

LANGUAGE: typescript
CODE:
import { Actor } from 'apify';
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

await Actor.init();

log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

await Actor.exit();

----------------------------------------

TITLE: Configuring Apify Proxy
DESCRIPTION: Examples of setting up and using Apify Proxy with specific configurations

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
    countryCode: 'US',
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Finding All Links on a Page with Cheerio
DESCRIPTION: This code snippet uses Cheerio to find all <a> elements with an href attribute on a page and extracts the href values into an array.

LANGUAGE: javascript
CODE:
$('a[href]')
    .map((i, el) => $(el).attr('href'))
    .get();

----------------------------------------

TITLE: Initializing Apify Project Configuration
DESCRIPTION: Command to initialize the Apify project configuration, creating necessary files for deployment to the Apify Platform.

LANGUAGE: bash
CODE:
apify init

----------------------------------------

TITLE: Configuring SessionPool with BasicCrawler in Crawlee
DESCRIPTION: This snippet demonstrates how to set up and use SessionPool with BasicCrawler in Crawlee. It shows configuration of proxy settings and handling of blocked sessions.

LANGUAGE: js
CODE:
import { BasicCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new BasicCrawler({
    // Note that a single ProxyConfiguration instance can be used by
    // multiple crawlers.
    proxyConfiguration,
    async requestHandler({ session, sendRequest }) {
        const { headers, body } = await sendRequest({
            url: 'https://api.example.com/some-api',
            proxyUrl: session.proxyUrl,
        });

        if (headers['X-Blocked'] === '1') {
            session.retire();
        }

        // Process the body...
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Configuring Scrapy for proxy rotation
DESCRIPTION: This code snippet demonstrates how to configure Scrapy for proxy rotation using the scrapy-rotating-proxies package. It shows the necessary settings to be added to the settings.py file.

LANGUAGE: python
CODE:
DOWNLOADER_MIDDLEWARES = {
    # Lower value means higher priority
    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 90,
    'scrapy_rotating_proxies.middlewares.RotatingProxyMiddleware': 610,
    'scrapy_rotating_proxies.middlewares.BanDetectionMiddleware': 620,
}

ROTATING_PROXY_LIST = [
    'proxy1.com:8000',
    'proxy2.com:8031',
    # Add more proxies as needed
]

----------------------------------------

TITLE: HTML Link Example
DESCRIPTION: Example of an HTML anchor tag structure that the crawler would process.

LANGUAGE: html
CODE:
<a href="https://crawlee.dev/js/docs/introduction">This is a link to Crawlee introduction</a>

----------------------------------------

TITLE: Attempting to Scrape JavaScript-Rendered Content with CheerioCrawler
DESCRIPTION: This snippet demonstrates an unsuccessful attempt to scrape JavaScript-rendered content using CheerioCrawler. It fails because CheerioCrawler cannot execute client-side JavaScript.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        // Extract text content of an actor card
        const actorText = $('.ActorStoreItem').text();
        console.log(`ACTOR: ${actorText}`);
    }
})

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Failing to Scrape Dynamic Content with PlaywrightCrawler (No Wait)
DESCRIPTION: This snippet shows how PlaywrightCrawler fails to scrape JavaScript-rendered content when not waiting for elements to appear, resulting in an error.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        // This will fail because we're not waiting for the content to load
        const actorText = await page.textContent('.ActorStoreItem');
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Configuring SessionPool with BasicCrawler in Crawlee
DESCRIPTION: This snippet demonstrates how to set up and use SessionPool with BasicCrawler in Crawlee. It shows configuration of proxy settings and handling of blocked sessions.

LANGUAGE: js
CODE:
import { BasicCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new BasicCrawler({
    // Note that a single ProxyConfiguration instance can be used by
    // multiple crawlers.
    proxyConfiguration,
    async requestHandler({ session, sendRequest }) {
        const { headers, body } = await sendRequest({
            url: 'https://api.example.com/some-api',
            proxyUrl: session.proxyUrl,
        });

        if (headers['X-Blocked'] === '1') {
            session.retire();
        }

        // Process the body...
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Setting minConcurrency and maxConcurrency in CheerioCrawler
DESCRIPTION: This code example shows how to configure the minimum and maximum concurrency for parallel requests in a CheerioCrawler.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    minConcurrency: 10,
    maxConcurrency: 100,
    // ...
});

----------------------------------------

TITLE: Sitemap Crawling with Playwright
DESCRIPTION: Implementation of sitemap crawling using Playwright Crawler. Requires apify/actor-node-playwright-chrome image when running on Apify Platform.

LANGUAGE: javascript
CODE:
{PlaywrightSource}

----------------------------------------

TITLE: Using sendRequest with BasicCrawler in Crawlee
DESCRIPTION: Demonstrates how to use the sendRequest function within a BasicCrawler's requestHandler to make HTTP requests. The function uses got-scraping under the hood to mimic browser requests and avoid blocking.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest();
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Header Generator Configuration
DESCRIPTION: Shows how to configure browser fingerprint generation options for requests.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            headerGeneratorOptions: {
                devices: ['mobile', 'desktop'],
                locales: ['en-US'],
                operatingSystems: ['windows', 'macos', 'android', 'ios'],
                browsers: ['chrome', 'edge', 'firefox', 'safari'],
            },
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Using Request List in Crawlee
DESCRIPTION: Shows how to use a Request List with a PuppeteerCrawler in Crawlee. It demonstrates creating a list of URLs and using it with a crawler.

LANGUAGE: javascript
CODE:
import { RequestList, PuppeteerCrawler } from 'crawlee';

// Prepare the sources array with URLs to visit
const sources = [
    { url: 'http://www.example.com/page-1' },
    { url: 'http://www.example.com/page-2' },
    { url: 'http://www.example.com/page-3' },
];

// Open the request list.
// List name is used to persist the sources and the list state in the key-value store
const requestList = await RequestList.open('my-list', sources);

// The crawler will automatically process requests from the list
// It's used the same way for Cheerio /Playwright crawlers.
const crawler = new PuppeteerCrawler({
    requestList,
    async requestHandler({ page, request }) {
        // Process the page (extract data, take page screenshot, etc).
        // No more requests could be added to the request list here
    },
});

----------------------------------------

TITLE: Configuring Crawlee with Custom Storage Settings
DESCRIPTION: Basic Crawlee configuration setup with custom storage settings to prevent interference between crawler instances in Lambda environment.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Enabling Request Locking in CheerioCrawler
DESCRIPTION: Demonstrates how to enable the request locking experiment in a CheerioCrawler instance using the experiments configuration option.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    experiments: {
        requestLocking: true,
    },
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: TypeScript Configuration
DESCRIPTION: TSConfig setup extending @apify/tsconfig with ES2022 module system and output directory configuration

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist"
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Verifying Node.js Installation
DESCRIPTION: Commands to verify Node.js and NPM versions are properly installed

LANGUAGE: bash
CODE:
node -v

LANGUAGE: bash
CODE:
npm -v

----------------------------------------

TITLE: Using Request Queue with Locking in Crawler
DESCRIPTION: Shows how to combine a custom RequestQueueV2 instance with a crawler while enabling the request locking experiment.

LANGUAGE: typescript
CODE:
import { CheerioCrawler, RequestQueueV2 } from 'crawlee';

const queue = await RequestQueueV2.open('my-locking-queue');

const crawler = new CheerioCrawler({
    experiments: {
        requestLocking: true,
    },
    requestQueue: queue,
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

await crawler.run();

----------------------------------------

TITLE: Configuring Crawlee using Global Configuration
DESCRIPTION: This example shows how to use the Configuration class to set global configuration options for Crawlee. It sets the persistStateIntervalMillis option and creates a CheerioCrawler that uses this global configuration.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Get the global configuration
const config = Configuration.getGlobalConfig();
// Set the 'persistStateIntervalMillis' option
// of global configuration to 10 seconds
config.set('persistStateIntervalMillis', 10_000);

// Note, that we are not passing the configuration to the crawler
// as it's using the global configuration
const crawler = new CheerioCrawler();

crawler.router.addDefaultHandler(async ({ request }) => {
    // For the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // For the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Demonstrating Dataset Operations in Crawlee
DESCRIPTION: This snippet illustrates basic operations for working with datasets in Crawlee, including writing single and multiple rows to both default and named datasets.

LANGUAGE: javascript
CODE:
import { Dataset } from 'crawlee';

// Write a single row to the default dataset
await Dataset.pushData({ col1: 123, col2: 'val2' });

// Open a named dataset
const dataset = await Dataset.open('some-name');

// Write a single row
await dataset.pushData({ foo: 'bar' });

// Write multiple rows
await dataset.pushData([{ foo: 'bar2', col2: 'val2' }, { col3: 123 }]);

----------------------------------------

TITLE: Using PreLaunchHooks in Apify SDK v1
DESCRIPTION: Shows how to use preLaunchHooks for customizing browser launch behavior in PuppeteerCrawler.

LANGUAGE: javascript
CODE:
const preLaunchHooks = [
    async function maybeLaunchChrome(pageId, launchContext) {
        const { request } = crawler.crawlingContexts.get(pageId);
        if (request.userData.useHeadful === true) {
            launchContext.launchOptions.headless = false;
        }
    }
]

const crawler = new Apify.PuppeteerCrawler({
    browserPoolOptions: {
        preLaunchHooks
    },
    // ...
})

----------------------------------------

TITLE: Crawling Multiple URLs with JSDOMCrawler in TypeScript
DESCRIPTION: This code snippet shows how to use JSDOMCrawler to crawl multiple URLs from an external file. It loads each URL, parses the HTML using jsdom, and extracts the page title and all h1 tags.

LANGUAGE: typescript
CODE:
import { JSDOMCrawler, Dataset } from 'crawlee';
import { readFileSync } from 'fs';

const crawler = new JSDOMCrawler({
    async requestHandler({ window, enqueueLinks, log }) {
        const { document } = window;
        log.info(`Processing ${document.title}`);

        // Extract data from the page
        const title = document.title;
        const h1Texts = Array.from(document.querySelectorAll('h1'))
            .map((el) => el.textContent);

        // Save the results
        await Dataset.pushData({
            title,
            h1Texts,
        });

        // Crawler will automatically ensure the one item is crawled at a time
        await enqueueLinks();
    },
});

// Add URLs to the crawler
const sources = readFileSync('urls.txt', 'utf8')
    .split('\n')
    .map((url) => url.trim())
    .filter((url) => url.length > 0);

await crawler.addRequests(sources);

await crawler.run();

----------------------------------------

TITLE: Configuring PlaywrightCrawler with Firefox Browser
DESCRIPTION: Example demonstrating how to set up PlaywrightCrawler to use Firefox browser for web scraping. Requires the apify/actor-node-playwright-firefox Docker image when deploying to Apify Platform.

LANGUAGE: javascript
CODE:

import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        launchOptions: {
            product: 'firefox',
        },
    },
    // ...
});


----------------------------------------

TITLE: Using Pre-release Docker Image
DESCRIPTION: Examples of using pre-release Docker images with beta tags for both Node.js and automation library versions.

LANGUAGE: dockerfile
CODE:
# Without library version.
FROM apify/actor-node:16-beta

LANGUAGE: dockerfile
CODE:
# With library version.
FROM apify/actor-node-playwright-chrome:16-1.10.0-beta

----------------------------------------

TITLE: Updating package.json for GCP Cloud Functions
DESCRIPTION: Modify the package.json file to set the main entry point to src/main.js for GCP Cloud Functions deployment.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "version": "1.0.0",
    "main": "src/main.js",
    ...
}

----------------------------------------

TITLE: Dockerfile for Playwright Chrome Crawlee Project
DESCRIPTION: Example Dockerfile for a Crawlee project using Playwright with Chrome browser.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16

----------------------------------------

TITLE: Adding AWS Lambda Handler Function
DESCRIPTION: Wrapping the crawler logic in an AWS Lambda handler function to make it executable in the Lambda environment.

LANGUAGE: javascript
CODE:
// For more information, see https://crawlee.dev/
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
};

----------------------------------------

TITLE: Deploying Crawlee Project to Apify Platform
DESCRIPTION: Command to deploy the Crawlee project to the Apify Platform, creating an archive, uploading it, and initiating a Docker build.

LANGUAGE: bash
CODE:
apify push

----------------------------------------

TITLE: Crawling Same Domain Links in Crawlee
DESCRIPTION: Example showing how to crawl links from the same domain including subdomains using CheerioCrawler. This strategy matches links to the same domain name regardless of subdomain.

LANGUAGE: javascript
CODE:
{SameDomainSource}

----------------------------------------

TITLE: Failed Cheerio Crawler Output
DESCRIPTION: Shows the empty output from the CheerioCrawler attempt, demonstrating why JavaScript rendering is needed.

LANGUAGE: log
CODE:
ACTOR:

----------------------------------------

TITLE: PuppeteerCrawler without Waiting (JavaScript)
DESCRIPTION: This snippet demonstrates an attempt to scrape with PuppeteerCrawler without waiting for elements, which results in an error as the elements are not yet rendered.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // This will fail because the element is not in the DOM yet
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Disabling Browser Fingerprints with PlaywrightCrawler
DESCRIPTION: This snippet demonstrates how to disable the use of browser fingerprints when using PlaywrightCrawler in Crawlee by setting the useFingerprints option to false.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    // ...
    browserPoolOptions: {
        useFingerprints: false,
    },
});

----------------------------------------

TITLE: Using Pre-release Docker Image
DESCRIPTION: Examples of how to use pre-release versions of Apify Docker images, both with and without specifying the automation library version.

LANGUAGE: dockerfile
CODE:
# Without library version.
FROM apify/actor-node:16-beta

LANGUAGE: dockerfile
CODE:
# With library version.
FROM apify/actor-node-playwright-chrome:16-1.10.0-beta

----------------------------------------

TITLE: Using Pre-release Docker Image
DESCRIPTION: Examples of using pre-release Docker images with beta tags for both Node.js and automation library versions.

LANGUAGE: dockerfile
CODE:
# Without library version.
FROM apify/actor-node:16-beta

LANGUAGE: dockerfile
CODE:
# With library version.
FROM apify/actor-node-playwright-chrome:16-1.10.0-beta

----------------------------------------

TITLE: Configuring Crawlee using Global Configuration
DESCRIPTION: This example shows how to use the Configuration class to set global configuration options for Crawlee. It sets the persistStateIntervalMillis option and creates a CheerioCrawler that uses this global configuration.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Get the global configuration
const config = Configuration.getGlobalConfig();
// Set the 'persistStateIntervalMillis' option
// of global configuration to 10 seconds
config.set('persistStateIntervalMillis', 10_000);

// Note, that we are not passing the configuration to the crawler
// as it's using the global configuration
const crawler = new CheerioCrawler();

crawler.router.addDefaultHandler(async ({ request }) => {
    // For the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // For the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Domain Strategy Configuration
DESCRIPTION: Configuration example for including subdomains in the crawl using enqueueLinks strategy.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    strategy: 'same-domain'
});

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Router in JavaScript
DESCRIPTION: Sets up a PlaywrightCrawler instance using a Router for request handling. It configures logging and runs the crawler on a specific URL.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Using Apify Platform Storage in Local Actor
DESCRIPTION: Demonstrates how to use Apify platform storage when developing and running an actor locally. This example shows opening a Key-Value Store and forcing cloud storage usage.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';
import { Dataset } from 'crawlee';

// or Dataset.open('my-local-data')
const localDataset = await Actor.openDataset('my-local-data');
// but here we need the `Actor` class
const remoteDataset = await Actor.openDataset('my-dataset', { forceCloud: true });

----------------------------------------

TITLE: Implementing Tiered Proxy Configuration in Crawlee
DESCRIPTION: Demonstrates how to set up and use tiered proxy configuration with CheerioCrawler. Shows proxy tier configuration, crawler initialization, and request handling. The code illustrates the fallback mechanism between different proxy tiers.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    tieredProxyUrls: [
        ['http://tier1-proxy1.example.com', 'http://tier1-proxy2.example.com'],
        ['http://tier2-proxy1.example.com', 'http://tier2-proxy2.example.com'],
        ['http://tier2-proxy1.example.com', 'http://tier3-proxy2.example.com'],
    ],
});

const crawler = new CheerioCrawler({
    proxyConfiguration,
    requestHandler: async ({ request, response }) => {
        // Handle the request
    },
});

await crawler.addRequests([
    { url: 'https://example.com/critical' },
    { url: 'https://example.com/important' },
    { url: 'https://example.com/regular' },
]);

await crawler.run();

----------------------------------------

TITLE: Enabling Request Locking in CheerioCrawler (TypeScript)
DESCRIPTION: This snippet demonstrates how to enable the request locking experiment in a CheerioCrawler instance. It sets the 'requestLocking' option to true in the experiments object.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    experiments: {
        requestLocking: true,
    },
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Importing got-scraping for Single URL Crawling in JavaScript
DESCRIPTION: Example showing how to use got-scraping package to fetch HTML content from a web page. The code demonstrates basic URL crawling functionality within the Crawlee framework.

LANGUAGE: javascript
CODE:
import { gotScraping } from 'got-scraping';

const url = 'https://example.com';
const response = await gotScraping(url);
const html = response.body;

----------------------------------------

TITLE: Configuring ProxyConfiguration with Custom Proxy Function in JavaScript
DESCRIPTION: This code shows how to use a custom function to select proxy URLs based on specific conditions. It demonstrates conditional proxy usage and session-based proxy selection.

LANGUAGE: javascript
CODE:
const proxyConfiguration = new ProxyConfiguration({
    newUrlFunction: (sessionId, { request }) => {
        if (request?.url.includes('crawlee.dev')) {
            return null; // for crawlee.dev, we don't use a proxy
        }

        return 'http://proxy-1.com'; // for all other URLs, we use this proxy
    }
});

----------------------------------------

TITLE: Logging into Apify CLI
DESCRIPTION: Commands to install Apify CLI and log in with an API token.

LANGUAGE: bash
CODE:
npm install -g apify-cli
apify login -t YOUR_API_TOKEN

----------------------------------------

TITLE: sendRequest API Implementation in Crawlee
DESCRIPTION: This code snippet shows the implementation of the sendRequest function in Crawlee. It uses got-scraping to make HTTP requests and includes various options for customizing the request, such as URL, method, headers, and proxy settings.

LANGUAGE: typescript
CODE:
async sendRequest(overrideOptions?: GotOptionsInit) => {
    return gotScraping({
        url: request.url,
        method: request.method,
        body: request.payload,
        headers: request.headers,
        proxyUrl: crawlingContext.proxyInfo?.url,
        sessionToken: session,
        responseType: 'text',
        ...overrideOptions,
        retry: {
            limit: 0,
            ...overrideOptions?.retry,
        },
        cookieJar: {
            getCookieString: (url: string) => session!.getCookieString(url),
            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),
            ...overrideOptions?.cookieJar,
        },
    });
}

----------------------------------------

TITLE: Finding All Links on a Page with Cheerio
DESCRIPTION: Demonstrates how to find all anchor elements with href attributes and extract their URLs into an array using Cheerio's chaining methods.

LANGUAGE: javascript
CODE:
$('a[href]')
    .map((i, el) => $(el).attr('href'))
    .get();

----------------------------------------

TITLE: PlaywrightCrawler without Waiting (JavaScript)
DESCRIPTION: Example demonstrating why waiting for elements is crucial in PlaywrightCrawler. This code will fail to extract data as it doesn't wait for the elements to render.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        // Try to extract the actor card content immediately
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Capturing Screenshot Using Puppeteer's page.screenshot()
DESCRIPTION: This snippet demonstrates how to capture a screenshot of a web page using Puppeteer's page.screenshot() method. It saves the screenshot to a key-value store with a key derived from the page URL.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';
import { PuppeteerLauncher } from '@crawlee/puppeteer';

const browser = await PuppeteerLauncher.launch();
const page = await browser.newPage();
await page.goto('https://example.com');

// Capture screenshot
const screenshotBuffer = await page.screenshot();

// Save screenshot to default key-value store
const kvStore = await KeyValueStore.open();
const key = `screenshot-${new URL(page.url()).hostname}`;
await kvStore.setValue(key, screenshotBuffer, { contentType: 'image/png' });

await browser.close();

----------------------------------------

TITLE: Using PlaywrightCrawler to Scrape Website Content in Python
DESCRIPTION: This code snippet demonstrates how to use Crawlee's PlaywrightCrawler to crawl a website, extract its title and content. It sets up a crawler instance, defines a request handler, and runs the crawler on a specific URL.

LANGUAGE: python
CODE:
import asyncio

from crawlee.playwright_crawler import PlaywrightCrawler, PlaywrightCrawlingContext


async def main() -> None:
    # Create a crawler instance
    crawler = PlaywrightCrawler(
        # headless=False,
        # browser_type='firefox',
    )

    @crawler.router.default_handler
    async def request_handler(context: PlaywrightCrawlingContext) -> None:
        data = {
            'request_url': context.request.url,
            'page_url': context.page.url,
            'page_title': await context.page.title(),
            'page_content': (await context.page.content())[:10000],
        }
        await context.push_data(data)

    await crawler.run(['https://crawlee.dev'])


if __name__ == '__main__':
    asyncio.run(main())

----------------------------------------

TITLE: Purging Default Storages in Crawlee
DESCRIPTION: Demonstrates how to explicitly purge default storages in Crawlee, which can be useful for cleaning up before starting a new crawl.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Basic Crawlee Scraper in TypeScript
DESCRIPTION: A simple Crawlee scraper that downloads the HTML of a single page, extracts the title, and prints it to the console.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Creating Apify Proxy Configuration
DESCRIPTION: JavaScript code to create and use Apify Proxy configuration with specific groups and country code.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
    countryCode: 'US',
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Request Handler Implementation with Pagination
DESCRIPTION: Handler function that processes API responses, manages pagination, and handles data extraction with respect to result limits.

LANGUAGE: javascript
CODE:
async requestHandler(context) {
    const { log, request, json } = context;
    const { userData } = request;
    const { itemsCounter = 0, resultsLimit = 0 } = userData;
    if (!json.data) {
        throw new Error('BLOCKED');
    }
    const { data } = json;
    const items = data.list;
    const counter = itemsCounter + items.length;
    const dataItems = items.slice(
        0,
        resultsLimit && counter > resultsLimit
            ? resultsLimit - itemsCounter
            : undefined,
    );
    await context.pushData(dataItems);
    const {
        pagination: { page, total },
    } = data;
    log.info(
        `Scraped ${dataItems.length} results out of ${total} from search page ${page}`,
    );
    const isResultsLimitNotReached =
        counter < Math.min(total, resultsLimit);
    if (isResultsLimitNotReached && data.pagination.has_more) {
        const nextUrl = new URL(request.url);
        nextUrl.searchParams.set('page', page + 1);
        await crawler.addRequests([
            {
                url: nextUrl.toString(),
                headers: request.headers,
                userData: {
                    ...request.userData,
                    itemsCounter: itemsCounter + dataItems.length,
                },
            },
        ]);
    }
}

----------------------------------------

TITLE: Using actor-node-playwright-webkit Docker Image
DESCRIPTION: Demonstrates using the Apify Docker image with Playwright and WebKit pre-installed.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-webkit:20

----------------------------------------

TITLE: Setting Maximum Requests per Crawl in Crawlee
DESCRIPTION: Demonstrates how to limit the number of requests processed by a Crawlee crawler using the maxRequestsPerCrawl option.

LANGUAGE: typescript
CODE:
const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 20,
    // ...
});

----------------------------------------

TITLE: Configuring ProxyConfiguration with Custom Proxy Function in JavaScript
DESCRIPTION: This code shows how to use a custom function to select proxy URLs based on specific conditions. It demonstrates conditional proxy usage and session-based proxy selection.

LANGUAGE: javascript
CODE:
const proxyConfiguration = new ProxyConfiguration({
    newUrlFunction: (sessionId, { request }) => {
        if (request?.url.includes('crawlee.dev')) {
            return null; // for crawlee.dev, we don't use a proxy
        }

        return 'http://proxy-1.com'; // for all other URLs, we use this proxy
    }
});

----------------------------------------

TITLE: Domain Strategy Configuration
DESCRIPTION: Configuration example for including subdomains in the crawl using the same-domain strategy.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    strategy: 'same-domain'
});

----------------------------------------

TITLE: Crawling Same Domain Strategy Implementation
DESCRIPTION: Illustrates how to crawl links from the same domain including subdomains using CheerioCrawler. This strategy allows processing URLs from any subdomain of the original domain.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks }) {
        // Add links that share the same domain name as the starting URL,
        // including any subdomains
        await enqueueLinks({
            strategy: EnqueueStrategy.SameDomain,
        });
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Configuring BrowserPool in Apify SDK v1
DESCRIPTION: Shows how to configure BrowserPool options and use lifecycle hooks in PuppeteerCrawler.

LANGUAGE: javascript
CODE:
const crawler = new Apify.PuppeteerCrawler({
    browserPoolOptions: {
        retireBrowserAfterPageCount: 10,
        preLaunchHooks: [
            async (pageId, launchContext) => {
                const { request } = crawler.crawlingContexts.get(pageId);
                if (request.userData.useHeadful === true) {
                    launchContext.launchOptions.headless = false;
                }
            }
        ]
    }
})

----------------------------------------

TITLE: Modified Crawlee Script with Apify Integration
DESCRIPTION: Example of a Crawlee crawler script modified to work with the Apify Platform, including Actor initialization and proper shutdown

LANGUAGE: typescript
CODE:
import { Actor } from 'apify';
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

await Actor.init();

log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

await Actor.exit();

----------------------------------------

TITLE: Creating a Request Queue with Locking Support
DESCRIPTION: Demonstrates how to create and use a RequestQueueV2 instance that supports request locking. Shows how to add multiple requests to the queue.

LANGUAGE: typescript
CODE:
import { RequestQueueV2 } from 'crawlee';

const queue = await RequestQueueV2.open('my-locking-queue');
await queue.addRequests([
    { url: 'https://crawlee.dev' },
    { url: 'https://crawlee.dev/js/docs' },
    { url: 'https://crawlee.dev/js/api' },
]);

----------------------------------------

TITLE: Setting maxRequestsPerMinute in CheerioCrawler
DESCRIPTION: Demonstrates how to set the maxRequestsPerMinute option in CheerioCrawler to limit the number of requests per minute to 120.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    maxRequestsPerMinute: 120,
    // ...
});

----------------------------------------

TITLE: Sitemap Crawling with Puppeteer
DESCRIPTION: Implementation of sitemap crawling using Puppeteer Crawler, which enables dynamic content scraping with a Chrome browser. Requires apify/actor-node-puppeteer-chrome Docker image for Platform deployment.

LANGUAGE: javascript
CODE:
{PuppeteerSource}

----------------------------------------

TITLE: Using actor-node-playwright-firefox Docker Image
DESCRIPTION: Demonstrates how to use the Apify Docker image that includes Playwright and Firefox.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-firefox:16

----------------------------------------

TITLE: Launching Puppeteer and Playwright with Custom Modules
DESCRIPTION: Examples demonstrating how to launch Puppeteer and Playwright with custom modules in Apify SDK v1.

LANGUAGE: javascript
CODE:
const puppeteer = require('puppeteer');
const playwright = require('playwright');

await Apify.launchPuppeteer();
// Is the same as:
await Apify.launchPuppeteer({
    launcher: puppeteer
})

await Apify.launchPlaywright();
// Is the same as:
await Apify.launchPlaywright({
    launcher: playwright.chromium
})

----------------------------------------

TITLE: Configuring Crawler Entry Point
DESCRIPTION: Main crawler configuration including headless browser setup and request limits

LANGUAGE: python
CODE:
from crawlee.playwright_crawler import PlaywrightCrawler

from .routes import router

async def main() -> None:
    """The crawler entry point."""
    crawler = PlaywrightCrawler(
        headless=False,
        request_handler=router,
        max_requests_per_crawl=100,
    )

    await crawler.run([
        'https://nike.com/,
    ])

----------------------------------------

TITLE: Finding All Links on a Page using Cheerio
DESCRIPTION: This code snippet shows how to use Cheerio to find all <a> elements with an href attribute on a page and extract the href values into an array.

LANGUAGE: javascript
CODE:
$('a[href]')
    .map((i, el) => $(el).attr('href'))
    .get();

----------------------------------------

TITLE: Basic Cheerio Crawler Implementation
DESCRIPTION: Demonstrates a basic CheerioCrawler setup that fails to extract JavaScript-rendered content from Apify Store.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        // Extract text content of an actor card
        const actorText = $('.ActorStoreItem').text();
        console.log(`ACTOR: ${actorText}`);
    }
})

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Interactive Web Scraping with JSDOMCrawler in TypeScript
DESCRIPTION: Demonstrates how to use JSDOMCrawler to interact with a React calculator application. The script performs button clicks to calculate 1+1 and extracts the result.

LANGUAGE: typescript
CODE:
{JSDOMCrawlerRunScriptSource}

----------------------------------------

TITLE: Implementing Data Extraction for Google Search Results
DESCRIPTION: Handler implementation to extract title, URL, and text content from Google search results using BeautifulSoup

LANGUAGE: python
CODE:
@crawler.router.default_handler
async def default_handler(context: BeautifulSoupCrawlingContext) -> None:
    """Default request handler."""
    context.log.info(f'Processing {context.request} ...')

    for item in context.soup.select("div#search div#rso div[data-hveid][lang]"):
        data = {
            'title': item.select_one("h3").get_text(),
            "url": item.select_one("a").get("href"),
            "text_widget": item.select_one("div[style*='line']").get_text(),
        }
        await context.push_data(data)

----------------------------------------

TITLE: Playwright Multi-browser Docker Configuration
DESCRIPTION: Dockerfile configuration for Node.js with multiple Playwright browsers pre-installed.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright:16

----------------------------------------

TITLE: Customizing Browser Fingerprints with PlaywrightCrawler in Crawlee
DESCRIPTION: This code snippet demonstrates how to customize browser fingerprints using PlaywrightCrawler in Crawlee. It sets specific options for the fingerprint generation, including browser, operating system, and screen size.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    // ... other options
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                browsers: ["chrome"],
                devices: ["desktop"],
                operatingSystems: ["windows"],
                locales: ["en-US"],
                screens: ["1920x1080"],
            },
        },
    },
});

----------------------------------------

TITLE: Customizing Browser Fingerprints with PlaywrightCrawler in Crawlee
DESCRIPTION: This code snippet demonstrates how to customize browser fingerprints using PlaywrightCrawler in Crawlee. It sets specific options for the fingerprint generation, including browser, operating system, and screen size.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    // ... other options
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                browsers: ["chrome"],
                devices: ["desktop"],
                operatingSystems: ["windows"],
                locales: ["en-US"],
                screens: ["1920x1080"],
            },
        },
    },
});

----------------------------------------

TITLE: URL Structure Example
DESCRIPTION: Example of pagination URL structure used in the e-commerce store.

LANGUAGE: text
CODE:
https://warehouse-theme-metal.myshopify.com/collections/headphones?page=2

----------------------------------------

TITLE: Adapting Category Route Handler for Parallel Processing
DESCRIPTION: Modified route handler that enqueues product URLs to a lockable request queue instead of processing them directly.

LANGUAGE: javascript
CODE:
export const router = createPlaywrightRouter();

router.addHandler('CATEGORY', async ({ page, request, enqueueLinks, log }) => {
    log.info(`Processing ${request.url}...`);
    await page.waitForSelector('.product-grid');
    await enqueueLinks({
        selector: 'a.product',
        label: 'DETAIL',
        transformRequestFunction: (req) => ({
            ...req,
            skipNavigation: true,
        }),
    });
});

----------------------------------------

TITLE: Installing Crawlee with CLI
DESCRIPTION: Commands to install and start a new Crawlee project using the CLI tool.

LANGUAGE: bash
CODE:
npx crawlee create my-crawler
cd my-crawler && npm start

----------------------------------------

TITLE: Accessing Page Title in Browser vs JSDOM
DESCRIPTION: Demonstrates the difference between accessing page title in browser JavaScript versus JSDOM environment.

LANGUAGE: typescript
CODE:
// Return the page title
document.title; // browsers
window.document.title; // JSDOM

----------------------------------------

TITLE: Configuring package.json for Production Script
DESCRIPTION: JSON configuration in package.json to add a production script for running compiled JavaScript code.

LANGUAGE: json
CODE:
{
    "scripts": {
        "start:prod": "node dist/main.js"
    }
}

----------------------------------------

TITLE: Playwright WebKit Configuration
DESCRIPTION: Docker configuration for Node.js with Playwright and WebKit browser support.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-webkit:16

----------------------------------------

TITLE: Accessing Page Title with Browser JavaScript and JSDOM
DESCRIPTION: Demonstrates how to retrieve the page title using both browser JavaScript and JSDOM. This shows the similarity between the two environments for basic DOM operations.

LANGUAGE: javascript
CODE:
// Return the page title
document.title; // browsers
window.document.title; // JSDOM

----------------------------------------

TITLE: Setting Concurrency Limits in Crawlee
DESCRIPTION: Demonstrates how to configure minimum and maximum concurrent requests for a crawler to control resource usage and performance.

LANGUAGE: javascript
CODE:
const crawler = new CheerioCrawler({
    // Start with 1 parallel request
    minConcurrency: 1,
    // Scale up to a maximum of 100 parallel requests
    maxConcurrency: 100,
    // ...
});

----------------------------------------

TITLE: Setting Maximum Requests Per Crawl
DESCRIPTION: Configuration example showing how to limit the number of requests a crawler will process.

LANGUAGE: typescript
CODE:
const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 20,
    // ...
});

----------------------------------------

TITLE: Crawling URLs and Extracting HTML Content with JSDOMCrawler
DESCRIPTION: Shows how to use JSDOMCrawler to process multiple URLs from an external file, make HTTP requests, parse HTML using jsdom, and extract specific data like page titles and h1 tags.

LANGUAGE: typescript
CODE:
{JSDOMCrawlerSource}

----------------------------------------

TITLE: Initializing Basic Proxy Configuration in Crawlee
DESCRIPTION: Basic setup of ProxyConfiguration class with multiple proxy URLs. Shows how to create a proxy configuration instance and get a new proxy URL.

LANGUAGE: javascript
CODE:
import { ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ]
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Implementing Express Server with Crawlee for GCP Cloud Run in JavaScript
DESCRIPTION: This code sets up an Express server to handle HTTP requests for the Crawlee crawler on GCP Cloud Run. It creates a route that initializes and runs the crawler, then returns the collected data. The server listens on the port specified by the GCP environment variable.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import express from 'express';
const app = express();

const startUrls = ['https://crawlee.dev'];

app.get('/', async (req, res) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));
    
    await crawler.run(startUrls);    

    return res.send(await crawler.getData());
});

app.listen(parseInt(process.env.PORT) || 3000);

----------------------------------------

TITLE: Implementing Express Server with Crawlee for GCP Cloud Run in JavaScript
DESCRIPTION: This code sets up an Express server to handle HTTP requests for the Crawlee crawler on GCP Cloud Run. It creates a route that initializes and runs the crawler, then returns the collected data. The server listens on the port specified by the GCP environment variable.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import express from 'express';
const app = express();

const startUrls = ['https://crawlee.dev'];

app.get('/', async (req, res) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));
    
    await crawler.run(startUrls);    

    return res.send(await crawler.getData());
});

app.listen(parseInt(process.env.PORT) || 3000);

----------------------------------------

TITLE: Scraping Dynamic Content with PuppeteerCrawler
DESCRIPTION: This snippet demonstrates using PuppeteerCrawler to scrape JavaScript-rendered content, showing how to explicitly wait for elements to appear in Puppeteer.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // Wait for the actor cards to render
        await page.waitForSelector('.ActorStoreItem');
        // Extract text content of the first actor card
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Using Global Configuration in Crawlee
DESCRIPTION: Example demonstrating how to use and modify the global Configuration instance in Crawlee.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

const config = Configuration.getGlobalConfig();
config.set('persistStateIntervalMillis', 10_000);

const crawler = new CheerioCrawler();

crawler.router.addDefaultHandler(async ({ request }) => {
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Deploying Crawlee Project to Apify Platform
DESCRIPTION: Command to deploy the Crawlee project to the Apify Platform, creating an archive, uploading it, and initiating a Docker build.

LANGUAGE: bash
CODE:
apify push

----------------------------------------

TITLE: Configuring Apify Token Programmatically
DESCRIPTION: Example of setting up Apify authentication using the Configuration instance

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const sdk = new Actor({ token: 'your_api_token' });

----------------------------------------

TITLE: Checking Product Stock Availability with Playwright in JavaScript
DESCRIPTION: This snippet shows how to determine if a product is in stock by checking for the presence of a specific element on the webpage using Playwright.

LANGUAGE: javascript
CODE:
const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Complete Product Data Extraction
DESCRIPTION: Combines all the individual scraping operations into a complete solution for extracting product information.

LANGUAGE: javascript
CODE:
const urlPart = request.url.split('/').slice(-1);
const manufacturer = urlPart.split('-')[0];

const title = await page.locator('.product-meta h1').textContent();
const sku = await page.locator('span.product-meta__sku-number').textContent();

const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Capturing Screenshots of Multiple Pages Using PuppeteerCrawler with utils.puppeteer.saveSnapshot()
DESCRIPTION: This snippet shows how to capture screenshots of multiple web pages using PuppeteerCrawler with utils.puppeteer.saveSnapshot(). It sets up a crawler to visit multiple URLs and use the saveSnapshot() utility to capture and save screenshots.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks, log, context }) {
        const title = await page.title();
        log.info(`Title of ${request.loadedUrl}: ${title}`);

        // Use the context-aware saveSnapshot() helper function
        await context.puppeteerUtils.saveSnapshot();
    },
});

await crawler.run(['https://crawlee.dev', 'https://apify.com']);

----------------------------------------

TITLE: Handling Google Search Results Pagination
DESCRIPTION: Implementation of pagination handling for Google search results using CSS selectors

LANGUAGE: python
CODE:
await context.enqueue_links(selector="div[role='navigation'] td[role='heading']:last-of-type > a")

----------------------------------------

TITLE: Installing ts-node for Development in Crawlee TypeScript Project
DESCRIPTION: This command installs ts-node as a development dependency, allowing direct execution of TypeScript files during development.

LANGUAGE: shell
CODE:
npm install --save-dev ts-node

----------------------------------------

TITLE: Specifying Dataset Storage Path in Crawlee
DESCRIPTION: Shows the default storage path where individual dataset items are saved as separate files within the project structure.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/datasets/default/

----------------------------------------

TITLE: Installing Crawlee using CLI
DESCRIPTION: Commands to install Crawlee using the Crawlee CLI, which sets up a new project with necessary dependencies and boilerplate code.

LANGUAGE: bash
CODE:
npx crawlee create my-crawler

LANGUAGE: bash
CODE:
cd my-crawler && npm start

----------------------------------------

TITLE: Comparing Browser and JSDOM Title Access
DESCRIPTION: Demonstrates the difference between accessing page title in browsers versus JSDOM environment. Shows equivalent commands for both contexts.

LANGUAGE: typescript
CODE:
// Return the page title
document.title; // browsers
window.document.title; // JSDOM

----------------------------------------

TITLE: Installing Crawlee and Puppeteer for PuppeteerCrawler
DESCRIPTION: Command to manually install Crawlee and Puppeteer for use with PuppeteerCrawler.

LANGUAGE: bash
CODE:
npm install crawlee puppeteer

----------------------------------------

TITLE: Terminating Crawlee Process
DESCRIPTION: Keyboard shortcut to stop the crawler execution

LANGUAGE: text
CODE:
CTRL+C

----------------------------------------

TITLE: Implementing Recursive Web Crawling with PuppeteerCrawler
DESCRIPTION: Example showing how to set up and execute recursive website crawling using PuppeteerCrawler. The code demonstrates the proper configuration for running on the Apify Platform using the apify/actor-node-puppeteer-chrome Docker image.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';
import { Actor } from 'apify';

await Actor.init();

const crawler = new PuppeteerCrawler({
    // ProxyConfiguration enables proxy usage
    proxyConfiguration: new ProxyConfiguration({ proxyUrls: ['...'] }),

    async requestHandler({ request, page, enqueueLinks }) {
        // Add all links from page to RequestQueue
        await enqueueLinks();

        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);
    },
});

// Add first URL to RequestQueue
await crawler.addRequests(['https://crawlee.dev']);

// Run the crawler
await crawler.run();

await Actor.exit();

----------------------------------------

TITLE: Exporting Dataset to CSV File with Crawlee in TypeScript
DESCRIPTION: This code snippet demonstrates how to use Crawlee's 'exportToValue' function to export the entire default dataset to a single CSV file. The exported file is then stored in a key-value store named 'my-data'.

LANGUAGE: typescript
CODE:
import { Dataset, KeyValueStore } from 'crawlee';

await Dataset.open();
const csv = await Dataset.exportToValue('csv');
const store = await KeyValueStore.open('my-data');
await store.setValue('DATASET.csv', csv);

----------------------------------------

TITLE: Saving Data with Dataset.pushData() in Crawlee (JavaScript)
DESCRIPTION: Uses Dataset.pushData() to save the extracted results. This function adds a new row to the dataset, with each property of the results object becoming a column in the dataset.

LANGUAGE: javascript
CODE:
await Dataset.pushData(results);

----------------------------------------

TITLE: Installing Apify SDK v1 with Puppeteer
DESCRIPTION: Command to install Apify SDK v1 with Puppeteer support.

LANGUAGE: bash
CODE:
npm install apify puppeteer

----------------------------------------

TITLE: Configuring Production Start Script for Crawlee TypeScript Project
DESCRIPTION: JSON configuration for package.json to add a production start script for running compiled JavaScript in a Crawlee project.

LANGUAGE: json
CODE:
{
    "scripts": {
        "start:prod": "node dist/main.js"
    }
}

----------------------------------------

TITLE: Configuring Production Start Script for Crawlee TypeScript Project
DESCRIPTION: JSON configuration for package.json to add a production start script for running compiled JavaScript in a Crawlee project.

LANGUAGE: json
CODE:
{
    "scripts": {
        "start:prod": "node dist/main.js"
    }
}

----------------------------------------

TITLE: Adjusting Crawlee Code for Apify Platform Deployment
DESCRIPTION: Modifications to the main Crawlee script to integrate with Apify SDK, including initializing and exiting the Actor for proper storage handling on the Apify Platform.

LANGUAGE: typescript
CODE:
import { Actor } from 'apify';
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

await Actor.init();

log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

await Actor.exit();

----------------------------------------

TITLE: Exporting Dataset to CSV in Crawlee
DESCRIPTION: Shows how to export the default dataset to a single CSV file using the exportToValue function. The exported data is stored in a key-value store named 'my-data'.

LANGUAGE: javascript
CODE:
import { Dataset } from 'crawlee';

await Dataset.exportToValue('my-data', {
    format: 'csv',
    storage: 'KEY_VALUE_STORE'
});

----------------------------------------

TITLE: Dataset Reduce Result Example
DESCRIPTION: Example output showing the total sum of all heading counts after reduction operation.

LANGUAGE: javascript
CODE:
23

----------------------------------------

TITLE: Configuring Crawlee using Global Configuration
DESCRIPTION: Example of using the Configuration class to set global configuration options for Crawlee, specifically setting the persistStateIntervalMillis option.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Get the global configuration
const config = Configuration.getGlobalConfig();
// Set the 'persistStateIntervalMillis' option
// of global configuration to 10 seconds
config.set('persistStateIntervalMillis', 10_000);

// Note, that we are not passing the configuration to the crawler
// as it's using the global configuration
const crawler = new CheerioCrawler();

crawler.router.addDefaultHandler(async ({ request }) => {
    // For the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // For the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Checking Stock Availability with Playwright
DESCRIPTION: Demonstrates how to check if a product is in stock by locating and counting specific HTML elements.

LANGUAGE: javascript
CODE:
const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Crawlee Browser Fingerprint Configuration
DESCRIPTION: Example of configuring browser fingerprints in Crawlee with PlaywrightCrawler

LANGUAGE: typescript
CODE:
const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        useFingerprints: false,
    },
});

----------------------------------------

TITLE: Extracting Product Title using Playwright
DESCRIPTION: Shows how to select and extract a product's title from an h1 element within a product-meta div.

LANGUAGE: javascript
CODE:
const title = await page.locator('.product-meta h1').textContent();

----------------------------------------

TITLE: Extracting Product SKU with Playwright
DESCRIPTION: Demonstrates retrieving the product SKU using a specific class selector.

LANGUAGE: javascript
CODE:
const sku = await page.locator('span.product-meta__sku-number').textContent();

----------------------------------------

TITLE: Installing Crawlee Dependencies
DESCRIPTION: NPM commands for installing Crawlee and its optional browser automation dependencies.

LANGUAGE: bash
CODE:
npm install crawlee
npm install crawlee playwright
npm install crawlee puppeteer

----------------------------------------

TITLE: Complete Google Search Scraper Implementation
DESCRIPTION: Final implementation of the Google Search scraper with support for multiple queries and ranking position tracking

LANGUAGE: python
CODE:
from crawlee.beautifulsoup_crawler import BeautifulSoupCrawler, BeautifulSoupCrawlingContext
from crawlee.http_clients.curl_impersonate import CurlImpersonateHttpClient
from crawlee import Request, ConcurrencySettings, HttpHeaders

from .routes import router

QUERIES = ["Apify", "Crawlee"]

CRAWL_DEPTH = 2


async def main() -> None:
    """The crawler entry point."""

    concurrency_settings = ConcurrencySettings(max_concurrency=5, max_tasks_per_minute=200)

    http_client = CurlImpersonateHttpClient(impersonate="chrome124",
                                            headers=HttpHeaders({"referer": "https://www.google.com/",
                                                     "accept-language": "en",
                                                     "accept-encoding": "gzip, deflate, br, zstd",
                                                     "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
                                            }))
    crawler = BeautifulSoupCrawler(
        request_handler=router,
        max_request_retries=1,
        concurrency_settings=concurrency_settings,
        http_client=http_client,
        max_requests_per_crawl=100,
        max_crawl_depth=CRAWL_DEPTH
    )

    requests_lists = [Request.from_url(f"https://www.google.com/search?q={query}", user_data = {"query": query}) for query in QUERIES]

    await crawler.run(requests_lists)

    await crawler.export_data_csv("google_ranked.csv")

----------------------------------------

TITLE: Configuring Multi-stage Docker Build for Crawlee Actor
DESCRIPTION: A complete Dockerfile that implements a multi-stage build process for a Crawlee actor. The build stage compiles the source code while the final stage creates a minimal production image with only the necessary dependencies and built artifacts.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16 AS builder

COPY package*.json ./

RUN npm install --include=dev --audit=false

COPY . ./

RUN npm run build

FROM apify/actor-node:16

COPY --from=builder /usr/src/app/dist ./dist

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm run start:prod --silent

----------------------------------------

TITLE: TypeScript Configuration for Crawlee
DESCRIPTION: TypeScript configuration file (tsconfig.json) setup for Crawlee projects.

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist",
        "lib": ["DOM"]
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: TypeScript Configuration for Crawlee
DESCRIPTION: TypeScript configuration file (tsconfig.json) setup for Crawlee projects.

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist",
        "lib": ["DOM"]
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Implementing Route Handlers for Web Scraping - JavaScript
DESCRIPTION: Defines route handlers for different page types (detail, category, default) using Crawlee's router. Each handler processes specific page types and extracts relevant data or enqueues new links.

LANGUAGE: javascript
CODE:
import { createPlaywrightRouter, Dataset } from 'crawlee';

export const router = createPlaywrightRouter();

router.addHandler('DETAIL', async ({ request, page, log }) => {
    log.debug(`Extracting data: ${request.url}`);
    const urlPart = request.url.split('/').slice(-1);
    const manufacturer = urlPart[0].split('-')[0];

    const title = await page.locator('.product-meta h1').textContent();
    const sku = await page
        .locator('span.product-meta__sku-number')
        .textContent();

    const priceElement = page
        .locator('span.price')
        .filter({
            hasText: '$',
        })
        .first();

    const currentPriceString = await priceElement.textContent();
    const rawPrice = currentPriceString.split('$')[1];
    const price = Number(rawPrice.replaceAll(',', ''));

    const inStockElement = page
        .locator('span.product-form__inventory')
        .filter({
            hasText: 'In stock',
        })
        .first();

    const inStock = (await inStockElement.count()) > 0;

    const results = {
        url: request.url,
        manufacturer,
        title,
        sku,
        currentPrice: price,
        availableInStock: inStock,
    };

    log.debug(`Saving data: ${request.url}`);
    await Dataset.pushData(results);
});

router.addHandler('CATEGORY', async ({ page, enqueueLinks, request, log }) => {
    log.debug(`Enqueueing pagination for: ${request.url}`);
    await page.waitForSelector('.product-item > a');
    await enqueueLinks({
        selector: '.product-item > a',
        label: 'DETAIL',
    });

    const nextButton = await page.$('a.pagination__next');
    if (nextButton) {
        await enqueueLinks({
            selector: 'a.pagination__next',
            label: 'CATEGORY',
        });
    }
});

router.addDefaultHandler(async ({ request, page, enqueueLinks, log }) => {
    log.debug(`Enqueueing categories from page: ${request.url}`);
    await page.waitForSelector('.collection-block-item');
    await enqueueLinks({
        selector: '.collection-block-item',
        label: 'CATEGORY',
    });
});

----------------------------------------

TITLE: Configuring Apify Proxy
DESCRIPTION: Examples of setting up and using Apify Proxy with different configurations

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration();
const proxyUrl = await proxyConfiguration.newUrl();

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
    countryCode: 'US',
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Installing Crawlee Manually
DESCRIPTION: NPM commands for manually installing Crawlee and its dependencies for different crawler types

LANGUAGE: bash
CODE:
npm install crawlee

LANGUAGE: bash
CODE:
npm install crawlee playwright

LANGUAGE: bash
CODE:
npm install crawlee puppeteer

----------------------------------------

TITLE: Accessing Page Title with JSDOMCrawler in JavaScript
DESCRIPTION: Demonstrates how to retrieve the page title using both browser JavaScript and JSDOM in JSDOMCrawler. This snippet shows the similarity between browser and JSDOM APIs.

LANGUAGE: javascript
CODE:
// Return the page title
document.title; // browsers
window.document.title; // JSDOM

----------------------------------------

TITLE: Installing TypeScript Dependencies
DESCRIPTION: Commands for installing TypeScript compiler and Node.js type definitions as development dependencies

LANGUAGE: shell
CODE:
npm install --save-dev typescript
npm install --save-dev @types/node

----------------------------------------

TITLE: Importing Dataset in Crawlee (TypeScript)
DESCRIPTION: This snippet shows how to import the PlaywrightCrawler and Dataset modules from Crawlee. These imports are necessary for creating a crawler and saving data.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

----------------------------------------

TITLE: Creating INPUT.json File for Actor Input
DESCRIPTION: This bash command shows the location where the INPUT.json file should be created to provide input to the actor. The file should be placed in the project's key-value store directory.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json

----------------------------------------

TITLE: Crawling Same Domain Links with CheerioCrawler in Crawlee
DESCRIPTION: This code snippet illustrates how to use CheerioCrawler to crawl links within the same domain, including subdomains. It uses the 'SameDomain' enqueue strategy to process URLs from the same domain during the crawl.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks }) {
        // Add new links from the page to the crawler's RequestQueue
        // Only links that share the same domain name will be added
        await enqueueLinks({
            strategy: EnqueueStrategy.SameDomain,
        });
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Basic Link Enqueueing with Crawlee
DESCRIPTION: Simple example of using enqueueLinks() function in Crawlee for basic link crawling

LANGUAGE: javascript
CODE:
await enqueueLinks();

----------------------------------------

TITLE: Extracting Manufacturer from URL with JavaScript String Manipulation
DESCRIPTION: Demonstrates how to extract the manufacturer name from a product URL by splitting the URL string.

LANGUAGE: javascript
CODE:
const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']
const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'

----------------------------------------

TITLE: Initializing RequestQueue and Adding URL in Crawlee
DESCRIPTION: Demonstrates how to create a RequestQueue instance and add a URL to it for crawling. This is a basic setup for queuing URLs to be processed by a crawler.

LANGUAGE: typescript
CODE:
import { RequestQueue } from 'crawlee';

// First you create the request queue instance.
const requestQueue = await RequestQueue.open();
// And then you add one or more requests to it.
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

----------------------------------------

TITLE: Extracting Page Links with JSDOM
DESCRIPTION: Uses querySelectorAll to find all anchor tags with href attributes and maps them to an array of URLs. Demonstrates DOM manipulation capabilities of JSDOM.

LANGUAGE: javascript
CODE:
Array.from(document.querySelectorAll('a[href]')).map((a) => a.href);

----------------------------------------

TITLE: Extracting Page Links with JSDOM
DESCRIPTION: Uses querySelectorAll to find all anchor tags with href attributes and maps them to an array of URLs. Demonstrates DOM manipulation capabilities of JSDOM.

LANGUAGE: javascript
CODE:
Array.from(document.querySelectorAll('a[href]')).map((a) => a.href);

----------------------------------------

TITLE: Configuring package.json for TypeScript Build in Crawlee
DESCRIPTION: JSON configuration in package.json for building TypeScript and specifying the main entry point for a Crawlee project.

LANGUAGE: json
CODE:
{
    "scripts": {
        "build": "tsc"
    },
    "main": "dist/main.js"
}

----------------------------------------

TITLE: Implementing puppeteer-extra with Crawlee
DESCRIPTION: TypeScript code demonstrating how to use puppeteer-extra with the stealth plugin in a Crawlee PuppeteerCrawler. It sets up the crawler with custom launcher options and includes a basic page processing function.

LANGUAGE: typescript
CODE:
import { PuppeteerCrawler } from 'crawlee';
import puppeteer from 'puppeteer-extra';
import StealthPlugin from 'puppeteer-extra-plugin-stealth';

puppeteer.use(StealthPlugin());

const crawler = new PuppeteerCrawler({
    // Instead of the default launcher, we use the puppeteer-extra instance
    launcher: puppeteer,
    launchContext: {
        // Don't forget to explicitly include the Chrome args you need
        launchOptions: {
            headless: 'new',
            args: [
                '--no-sandbox',
                '--disable-setuid-sandbox',
            ],
        },
    },
    async requestHandler({ page, request, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        // Do some scraping or enqueue more links
        await enqueueLinks();
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Category Page Crawler Implementation
DESCRIPTION: Implementation of a PlaywrightCrawler that crawls category pages using specific selectors and labels.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        // Wait for the category cards to render,
        // otherwise enqueueLinks wouldn't enqueue anything.
        await page.waitForSelector('.collection-block-item');

        // Add links to the queue, but only from
        // elements matching the provided selector.
        await enqueueLinks({
            selector: '.collection-block-item',
            label: 'CATEGORY',
        });
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Session Pool Implementation with PuppeteerCrawler
DESCRIPTION: Example demonstrating SessionPool usage with PuppeteerCrawler for headless browser crawling

LANGUAGE: javascript
CODE:
{PuppeteerSource}

----------------------------------------

TITLE: Creating GCP Cloud Function Handler for CheerioCrawler
DESCRIPTION: Wraps the crawler execution in an async handler function that takes req and res arguments, required for GCP Cloud Functions. The function is exported as a named export.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (req, res) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
    
    return res.send(await crawler.getData())
}

----------------------------------------

TITLE: Installing Crawlee via CLI
DESCRIPTION: Creates a new Crawlee project using the CLI tool with a getting started example

LANGUAGE: bash
CODE:
npx crawlee create my-crawler

LANGUAGE: bash
CODE:
cd my-crawler
npm start

----------------------------------------

TITLE: Extracting Manufacturer from URL with JavaScript String Manipulation
DESCRIPTION: Demonstrates how to extract the manufacturer name from a product URL by splitting the URL string.

LANGUAGE: javascript
CODE:
const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']
const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'

----------------------------------------

TITLE: Creating Dockerfile for Crawlee TypeScript Project
DESCRIPTION: Dockerfile configuration for building and running a Crawlee TypeScript project, using a multi-stage build process.

LANGUAGE: dockerfile
CODE:
# using multistage build, as we need dev deps to build the TS source code
FROM apify/actor-node:16 AS builder

# copy all files, install all dependencies (including dev deps) and build the project
COPY . ./
RUN npm install --include=dev \
    && npm run build

# create final image
FROM apify/actor-node:16
# copy only necessary files
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/dist ./dist

# install only prod deps
RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional

# run compiled code
CMD npm run start:prod

----------------------------------------

TITLE: Google Search Results Handler Implementation
DESCRIPTION: Implementation of the default handler for processing Google search results with error handling and ranking tracking

LANGUAGE: python
CODE:
from crawlee.beautifulsoup_crawler import BeautifulSoupCrawlingContext
from crawlee.router import Router

router = Router[BeautifulSoupCrawlingContext]()


@router.default_handler
async def default_handler(context: BeautifulSoupCrawlingContext) -> None:
    """Default request handler."""
    context.log.info(f'Processing {context.request.url} ...')

    order = context.request.user_data.get("last_order", 1)
    query = context.request.user_data.get("query")
    for item in context.soup.select("div#search div#rso div[data-hveid][lang]"):
        try:
            data = {
                "query": query,
                "order_no": order,
                'title': item.select_one("h3").get_text(),
                "url": item.select_one("a").get("href"),
                "text_widget": item.select_one("div[style*='line']").get_text(),
            }
            await context.push_data(data)
            order += 1
        except AttributeError as e:
            context.log.warning(f'Attribute error for query "{query}": {str(e)}')
        except Exception as e:
            context.log.error(f'Unexpected error for query "{query}": {str(e)}')

    await context.enqueue_links(selector="div[role='navigation'] td[role='heading']:last-of-type > a",
                                user_data={"last_order": order, "query": query})

----------------------------------------

TITLE: Docker Configuration for Crawlee
DESCRIPTION: Multi-stage Dockerfile setup for building and running Crawlee projects.

LANGUAGE: dockerfile
CODE:
# using multistage build, as we need dev deps to build the TS source code
FROM apify/actor-node:16 AS builder

# copy all files, install all dependencies (including dev deps) and build the project
COPY . ./
RUN npm install --include=dev \
    && npm run build

# create final image
FROM apify/actor-node:16
# copy only necessary files
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/README.md ./
COPY --from=builder /usr/src/app/dist ./dist
COPY --from=builder /usr/src/app/apify.json ./apify.json
COPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json

# install only prod deps
RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional \
    && echo "Installed NPM packages:" \
    && (npm list --only=prod --no-optional --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

# run compiled code
CMD npm run start:prod

----------------------------------------

TITLE: Cross-Context Access Using Crawling Context IDs
DESCRIPTION: Example demonstrating how to track and access crawling contexts across different handler functions using context IDs.

LANGUAGE: javascript
CODE:
let masterContextId;
const handlePageFunction = async ({ id, page, request, crawler }) => {
    if (request.userData.masterPage) {
        masterContextId = id;
        // Prepare the master page.
    } else {
        const masterContext = crawler.crawlingContexts.get(masterContextId);
        const masterPage = masterContext.page;
        const masterRequest = masterContext.request;
        // Now we can manipulate the master data from another handlePageFunction.
    }
}

----------------------------------------

TITLE: Session Pool Implementation with PuppeteerCrawler
DESCRIPTION: Example demonstrating SessionPool usage with PuppeteerCrawler for headless browser crawling

LANGUAGE: javascript
CODE:
{PuppeteerSource}

----------------------------------------

TITLE: Streamlit Web Application Implementation
DESCRIPTION: Web interface implementation using Streamlit to accept user inputs and run the scraper

LANGUAGE: python
CODE:
import streamlit as st
import subprocess

# Streamlit form for inputs
st.title("LinkedIn Job Scraper")

with st.form("scraper_form"):
    title = st.text_input("Job Title", value="backend developer")
    location = st.text_input("Job Location", value="newyork")
    data_name = st.text_input("Output File Name", value="backend_jobs")

    submit_button = st.form_submit_button("Run Scraper")

if submit_button:
    command = f"""poetry run python -m linkedin-scraper --title "{title}"  --location "{location}" --data_name "{data_name}" """

    with st.spinner("Crawling in progress..."):
         # Execute the command and display the results
        result = subprocess.run(command, shell=True, capture_output=True, text=True)

        st.write("Script Output:")
        st.text(result.stdout)

        if result.returncode == 0:
            st.success(f"Data successfully saved in {data_name}.csv")
        else:
            st.error(f"Error: {result.stderr}")

----------------------------------------

TITLE: Configuring PlaywrightCrawler with Persistent Storage Disabled in JavaScript
DESCRIPTION: This snippet demonstrates how to create a PlaywrightCrawler instance with a custom Configuration that disables persistent storage, which is necessary for cloud environments.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Saving Data with Dataset.pushData() in Crawlee (JavaScript)
DESCRIPTION: This code demonstrates how to save extracted data using the Dataset.pushData() function in Crawlee. It replaces a console.log() call with the data saving operation.

LANGUAGE: javascript
CODE:
await Dataset.pushData(results);

----------------------------------------

TITLE: Scraping with PuppeteerCrawler (No Wait - Unsuccessful)
DESCRIPTION: This snippet demonstrates an unsuccessful attempt to scrape JavaScript-rendered content using PuppeteerCrawler without waiting for elements to render, resulting in an error.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // Try to extract text content of the first actor card without waiting
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Crawling Same Domain Links with CheerioCrawler
DESCRIPTION: Illustrates crawling links that share the same domain name, including subdomains. This strategy matches relative URLs, absolute URLs, and URLs pointing to any subdomain of the main domain.

LANGUAGE: javascript
CODE:
{SameDomainSource}

----------------------------------------

TITLE: Basic Link Enqueuing with Crawlee
DESCRIPTION: Simple example of using the enqueueLinks() function without parameters to crawl links.

LANGUAGE: javascript
CODE:
await enqueueLinks();

----------------------------------------

TITLE: Building and Running Crawlee Project with Docker
DESCRIPTION: This Dockerfile sets up a multi-stage build process for a Crawlee project. It installs dependencies, builds the project, and creates a production-ready image with only the necessary files and packages. The final image is based on apify/actor-node:16 and runs the project using npm run start:prod.

LANGUAGE: Dockerfile
CODE:
FROM apify/actor-node:16 AS builder

COPY package*.json ./

RUN npm install --include=dev --audit=false

COPY . ./

RUN npm run build

FROM apify/actor-node:16

COPY --from=builder /usr/src/app/dist ./dist

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm run start:prod --silent

----------------------------------------

TITLE: Installing playwright-extra and stealth plugin
DESCRIPTION: Command to install playwright-extra and puppeteer-extra-plugin-stealth packages using npm.

LANGUAGE: bash
CODE:
npm install playwright-extra puppeteer-extra-plugin-stealth

----------------------------------------

TITLE: Creating Dockerfile for Crawlee TypeScript Project
DESCRIPTION: Set up a multi-stage Dockerfile for building and running a Crawlee TypeScript project in production.

LANGUAGE: dockerfile
CODE:
# using multistage build, as we need dev deps to build the TS source code
FROM apify/actor-node:20 AS builder

# copy all files, install all dependencies (including dev deps) and build the project
COPY . ./
RUN npm install --include=dev \
    && npm run build

# create final image
FROM apify/actor-node:20
# copy only necessary files
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/dist ./dist

# install only prod deps
RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional

# run compiled code
CMD npm run start:prod

----------------------------------------

TITLE: Creating Dockerfile for Crawlee TypeScript Project
DESCRIPTION: Set up a multi-stage Dockerfile for building and running a Crawlee TypeScript project in production.

LANGUAGE: dockerfile
CODE:
# using multistage build, as we need dev deps to build the TS source code
FROM apify/actor-node:20 AS builder

# copy all files, install all dependencies (including dev deps) and build the project
COPY . ./
RUN npm install --include=dev \
    && npm run build

# create final image
FROM apify/actor-node:20
# copy only necessary files
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/dist ./dist

# install only prod deps
RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional

# run compiled code
CMD npm run start:prod

----------------------------------------

TITLE: Installing Apify SDK and CLI for Crawlee Projects
DESCRIPTION: Commands to install the Apify SDK as a project dependency and the Apify CLI as a global tool for deploying Crawlee projects to the Apify Platform.

LANGUAGE: bash
CODE:
npm install apify

LANGUAGE: bash
CODE:
npm install -g apify-cli

----------------------------------------

TITLE: Launching Puppeteer with Custom Options in Apify SDK v1
DESCRIPTION: Demonstrates how to launch Puppeteer with custom options using the updated launch function.

LANGUAGE: javascript
CODE:
await Apify.launchPuppeteer({
    useChrome: true,
    launchOptions: {
        headless: true,
    }
})

----------------------------------------

TITLE: Extracting Product Title using Playwright Locator
DESCRIPTION: Shows how to extract a product title from an HTML page using Playwright's locator API with CSS selectors.

LANGUAGE: javascript
CODE:
const title = await page.locator('.product-meta h1').textContent();

----------------------------------------

TITLE: Number Parsing Utility Functions
DESCRIPTION: Utility functions to parse numerical values from strings and selectors

LANGUAGE: typescript
CODE:
const parseNumberValue = (rawString: string): number => {
    return Number(rawString.replace(/[^\d.]+/g, ''));
};

export const parseNumberFromSelector = ($: CheerioAPI, selector: string): number => {
    const rawValue = $(selector).first().text();
    return parseNumberValue(rawValue);
};

----------------------------------------

TITLE: Number Parsing Utility Functions
DESCRIPTION: Utility functions to parse numerical values from strings and selectors

LANGUAGE: typescript
CODE:
const parseNumberValue = (rawString: string): number => {
    return Number(rawString.replace(/[^\d.]+/g, ''));
};

export const parseNumberFromSelector = ($: CheerioAPI, selector: string): number => {
    const rawValue = $(selector).first().text();
    return parseNumberValue(rawValue);
};

----------------------------------------

TITLE: Installing Apify TypeScript Configuration
DESCRIPTION: Command to install Apify's TypeScript configuration as a development dependency.

LANGUAGE: shell
CODE:
npm install --save-dev @apify/tsconfig

----------------------------------------

TITLE: Launching Puppeteer with Updated Arguments
DESCRIPTION: Example showing how to launch Puppeteer with the updated argument structure in Apify SDK v1.

LANGUAGE: javascript
CODE:
await Apify.launchPuppeteer({
    useChrome: true,
    launchOptions: {
        headless: true,
    }
})

----------------------------------------

TITLE: Complete Product Scraping Logic with Playwright
DESCRIPTION: Combines all scraping steps to extract full product information including URL, manufacturer, title, SKU, price, and stock availability.

LANGUAGE: javascript
CODE:
const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']
const manufacturer = urlPart.split('-')[0]; // 'sennheiser'

const title = await page.locator('.product-meta h1').textContent();
const sku = await page.locator('span.product-meta__sku-number').textContent();

const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Accepting and Logging User Input in JavaScript with Crawlee
DESCRIPTION: This code snippet demonstrates how to accept user input in a Crawlee project and log it to the console. It uses the Apify SDK to retrieve input from the default key-value store.

LANGUAGE: javascript
CODE:
{AcceptInputSource}

----------------------------------------

TITLE: Captcha and Blocking Detection
DESCRIPTION: Implementation of blocking detection and handling mechanisms

LANGUAGE: typescript
CODE:
import { CheerioAPI } from 'cheerio';

const CAPTCHA_SELECTOR = '[action="/errors/validateCaptcha"]';

export const handleCaptchaBlocking = ($: CheerioAPI) => {
    const isCaptchaDisplayed = $(CAPTCHA_SELECTOR).length > 0;
    if (isCaptchaDisplayed) throw new Error('Captcha is displayed! Retrying...');
};

----------------------------------------

TITLE: Dockerfile for Crawlee TypeScript Project
DESCRIPTION: This Dockerfile sets up a multi-stage build process for a Crawlee TypeScript project, including building the TypeScript code and creating a production-ready image.

LANGUAGE: dockerfile
CODE:
# using multistage build, as we need dev deps to build the TS source code
FROM apify/actor-node:16 AS builder

# copy all files, install all dependencies (including dev deps) and build the project
COPY . ./
RUN npm install --include=dev \
    && npm run build

# create final image
FROM apify/actor-node:16
# copy only necessary files
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/dist ./dist

# install only prod deps
RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional

# run compiled code
CMD npm run start:prod

----------------------------------------

TITLE: Creating Dockerfile for TypeScript Crawlee Project
DESCRIPTION: Dockerfile configuration for building and running a TypeScript Crawlee project in a Docker container.

LANGUAGE: dockerfile
CODE:
# using multistage build, as we need dev deps to build the TS source code
FROM apify/actor-node:20 AS builder

# copy all files, install all dependencies (including dev deps) and build the project
COPY . ./
RUN npm install --include=dev \
    && npm run build

# create final image
FROM apify/actor-node:20
# copy only necessary files
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/dist ./dist

# install only prod deps
RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional

# run compiled code
CMD npm run start:prod

----------------------------------------

TITLE: Crawler Configuration with Browser Fingerprints
DESCRIPTION: Example of configuring PlaywrightCrawler with browser fingerprint options.

LANGUAGE: typescript
CODE:
const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        useFingerprints: false,
    },
});

----------------------------------------

TITLE: Installing Crawlee Package Dependencies
DESCRIPTION: Examples of installing Crawlee packages using npm, showing various installation options for different use cases

LANGUAGE: bash
CODE:
npm install crawlee

npm install @crawlee/cheerio

npm install crawlee playwright
# or npm install @crawlee/playwright playwright

----------------------------------------

TITLE: Configuring SessionPool with PuppeteerCrawler in Crawlee
DESCRIPTION: This code snippet illustrates the configuration and usage of SessionPool with PuppeteerCrawler in Crawlee. It includes setup for proxy rotation and session management.

LANGUAGE: js
CODE:
import { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new PuppeteerCrawler({
    proxyConfiguration,
    async requestHandler({ page, session }) {
        // Use 'session' here...
        const pageTitle = await page.title();
        // Process the page content here...
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Configuring Crawler for Sitemap-based Scraping
DESCRIPTION: Python code for configuring the Crawlee crawler to scrape Crunchbase using the sitemap approach, including concurrency settings and HTTP client setup.

LANGUAGE: python
CODE:
from crawlee import ConcurrencySettings, HttpHeaders
from crawlee.crawlers import ParselCrawler
from crawlee.http_clients import CurlImpersonateHttpClient

from .routes import router

async def main() -> None:
    concurrency_settings = ConcurrencySettings(max_concurrency=1, max_tasks_per_minute=50)

    http_client = CurlImpersonateHttpClient(
        impersonate='safari17_0',
        headers=HttpHeaders(
            {
                'accept-language': 'en',
                'accept-encoding': 'gzip, deflate, br, zstd',
            }
        ),
    )
    crawler = ParselCrawler(
        request_handler=router,
        max_request_retries=1,
        concurrency_settings=concurrency_settings,
        http_client=http_client,
        max_requests_per_crawl=30,
    )

    await crawler.run(['https://www.crunchbase.com/www-sitemaps/sitemap-index.xml'])

    await crawler.export_data_json('crunchbase_data.json')

----------------------------------------

TITLE: Browser Pool Configuration
DESCRIPTION: Shows how to configure Browser Pool options and lifecycle hooks in SDK v1

LANGUAGE: javascript
CODE:
const crawler = new Apify.PuppeteerCrawler({
    browserPoolOptions: {
        retireBrowserAfterPageCount: 10,
        preLaunchHooks: [
            async (pageId, launchContext) => {
                const { request } = crawler.crawlingContexts.get(pageId);
                if (request.userData.useHeadful === true) {
                    launchContext.launchOptions.headless = false;
                }
            }
        ]
    }
})

----------------------------------------

TITLE: Importing Dataset Module in Crawlee (TypeScript)
DESCRIPTION: Imports the PlaywrightCrawler and Dataset modules from Crawlee. This is necessary to use the Dataset.pushData() function for saving scraped data.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

----------------------------------------

TITLE: Crawling Multiple URLs with Playwright Crawler in JavaScript
DESCRIPTION: This code snippet illustrates how to use Playwright Crawler to crawl a list of specified URLs. It sets up the crawler and defines the request handler. When running on the Apify Platform, it requires the 'apify/actor-node-playwright-chrome' image.

LANGUAGE: javascript
CODE:
// The code for this snippet is not directly visible in the provided text.
// It is referenced as an external file: './crawl_multiple_urls_playwright.ts'

----------------------------------------

TITLE: Crawling Same Domain Links Strategy Implementation
DESCRIPTION: Illustrates how to crawl links from the same domain including subdomains using CheerioCrawler. This strategy will process URLs from both the main domain and any of its subdomains.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks }) {
        // Add all links that share the same domain name, including subdomains
        await enqueueLinks({
            strategy: EnqueueStrategy.SameDomain,
        });
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Crawling Category Pages with PlaywrightCrawler
DESCRIPTION: Implements a PlaywrightCrawler to crawl category pages of an e-commerce site. It uses enqueueLinks() with a specific selector to target category links and labels them for later processing.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        // Wait for the category cards to render,
        // otherwise enqueueLinks wouldn't enqueue anything.
        await page.waitForSelector('.collection-block-item');

        // Add links to the queue, but only from
        // elements matching the provided selector.
        await enqueueLinks({
            selector: '.collection-block-item',
            label: 'CATEGORY',
        });
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Crawling Links with Playwright Crawler
DESCRIPTION: Implementation of a web crawler using Playwright Crawler for modern web automation. Requires the apify/actor-node-playwright-chrome image and shows how to handle dynamic page content while crawling links.

LANGUAGE: typescript
CODE:
{PlaywrightSource}

----------------------------------------

TITLE: Installing puppeteer-extra and stealth plugin
DESCRIPTION: Command to install puppeteer-extra and puppeteer-extra-plugin-stealth packages using npm.

LANGUAGE: bash
CODE:
npm install puppeteer-extra puppeteer-extra-plugin-stealth

----------------------------------------

TITLE: Using HttpCrawler to Crawl and Save Web Pages
DESCRIPTION: This example shows how to create and use an HttpCrawler instance to crawl multiple web pages and save their content to a dataset. It demonstrates setting up the crawler with a request list and a request handler function.

LANGUAGE: javascript
CODE:
import { HttpCrawler, Dataset } from '@crawlee/http';

const crawler = new HttpCrawler({
    requestList,
    async requestHandler({ request, response, body, contentType }) {
        // Save the data to dataset.
        await Dataset.pushData({
            url: request.url,
            html: body,
        });
    },
});

await crawler.run([
    'http://www.example.com/page-1',
    'http://www.example.com/page-2',
]);

----------------------------------------

TITLE: Wrapping Crawlee Code in AWS Lambda Handler Function
DESCRIPTION: Final version of the code wrapped in an exported handler function for AWS Lambda execution, including crawler initialization and execution.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
        launchContext: {
            launchOptions: {
                executablePath: await aws_chromium.executablePath(),
                args: aws_chromium.args,
                headless: true
            }
        }
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);

    return {
        statusCode: 200,
        body: await crawler.getData(),
    };
}

----------------------------------------

TITLE: Starting Crawlee Project
DESCRIPTION: Commands to navigate to project directory and start the crawler

LANGUAGE: bash
CODE:
cd my-crawler
npm start

----------------------------------------

TITLE: Basic Cheerio Crawler Implementation
DESCRIPTION: Simple crawler that downloads a single page's HTML and extracts its title using Cheerio.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Domain Strategy Configuration
DESCRIPTION: Configuration example for including subdomains in the crawl using enqueueLinks strategy option.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    strategy: 'same-domain'
});

----------------------------------------

TITLE: Complete Netflix Scraper using Crawlee and Cheerio
DESCRIPTION: Full implementation of a Netflix scraper using Crawlee and Cheerio, including data extraction and storage.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, log, Dataset } from "crawlee";

const crawler = new CheerioCrawler({
  requestHandler: async ({ request, parseWithCheerio, pushData }) => {
    log.info(`Processing: ${request.url}`);

    const $ = await parseWithCheerio();

    const data = $('[data-uia="collections-row"]')
      .map((_, el) => {
        const genre = $(el)
          .find('[data-uia="collections-row-title"]')
          .text()
          .trim();
        const items = $(el)
          .find('[data-uia="collections-title"]')
          .map((_, itemEl) => $(itemEl).text().trim())
          .get();
        return { genre, items };
      })
      .get();

    const genres = data.map((d) => d.genre);
    const shows = data.map((d) => d.items);

    await pushData({
      genres: genres,
      shows: shows,
    });
  },

  maxRequestsPerCrawl: 20,
});

await crawler.run(["https://www.netflix.com/in/browse/genre/1191605"]);
await Dataset.exportToJSON("results");

----------------------------------------

TITLE: Purging Default Storages in Crawlee
DESCRIPTION: This snippet demonstrates how to explicitly purge default storages in Crawlee using the purgeDefaultStorages() helper function.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Complete Product Data Extraction Script
DESCRIPTION: Complete script combining all scraping operations to extract product information including URL, manufacturer, title, SKU, price, and stock status.

LANGUAGE: javascript
CODE:
const urlPart = request.url.split('/').slice(-1);
const manufacturer = urlPart.split('-')[0];

const title = await page.locator('.product-meta h1').textContent();
const sku = await page.locator('span.product-meta__sku-number').textContent();

const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Header Generator Options Configuration
DESCRIPTION: Demonstrates how to configure header generation options for browser fingerprinting.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            headerGeneratorOptions: {
                devices: ['mobile', 'desktop'],
                locales: ['en-US'],
                operatingSystems: ['windows', 'macos', 'android', 'ios'],
                browsers: ['chrome', 'edge', 'firefox', 'safari'],
            },
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Data Extraction Function
DESCRIPTION: Function to extract structured data from Google Maps listing elements including name, rating, reviews, and amenities

LANGUAGE: python
CODE:
async def _extract_listing_data(self, listing: ElementHandle) -> Optional[Dict]:
    """Extract structured data from a single listing element."""
    try:
        name_el = await listing.query_selector(".qBF1Pd")
        if not name_el:
            return None
        name = await name_el.inner_text()
        if name in self.processed_names:
            return None

        elements = {
            "rating": await listing.query_selector(".MW4etd"),
            "reviews": await listing.query_selector(".UY7F9"),
            "price": await listing.query_selector(".wcldff"),
            "link": await listing.query_selector("a.hfpxzc"),
            "address": await listing.query_selector(".W4Efsd:nth-child(2)"),
            "category": await listing.query_selector(".W4Efsd:nth-child(1)"),
        }

        amenities = []
        amenities_els = await listing.query_selector_all(".dc6iWb")
        for amenity in amenities_els:
            amenity_text = await amenity.get_attribute("aria-label")
            if amenity_text:
                amenities.append(amenity_text)

        place_data = {
            "name": name,
            "rating": await elements["rating"].inner_text() if elements["rating"] else None,
            "reviews": (await elements["reviews"].inner_text()).strip("()") if elements["reviews"] else None,
            "price": await elements["price"].inner_text() if elements["price"] else None,
            "address": await elements["address"].inner_text() if elements["address"] else None,
            "category": await elements["category"].inner_text() if elements["category"] else None,
            "amenities": amenities if amenities else None,
            "link": await elements["link"].get_attribute("href") if elements["link"] else None,
        }

        self.processed_names.add(name)
        return place_data
    except Exception as e:
        context.log.exception("Error extracting listing data")
        return None

----------------------------------------

TITLE: Storage Directory Structure Example
DESCRIPTION: Shows the file system path structure for key-value stores in Crawlee

LANGUAGE: plaintext
CODE:
{CRAWLEE_STORAGE_DIR}/key_value_stores/{STORE_ID}/{KEY}.{EXT}

----------------------------------------

TITLE: Installing Poetry and Setting Up Project Structure
DESCRIPTION: Commands for installing Poetry, creating the project folder, initializing the project, and setting up the file structure for a Crawlee Python project.

LANGUAGE: bash
CODE:
pipx install poetry
mkdir crunchbase-crawlee && cd crunchbase-crawlee
poetry init
poetry add crawlee[parsel,curl-impersonate]
mkdir crunchbase-crawlee && touch crunchbase-crawlee/{__init__.py,__main__.py,main.py,routes.py}

----------------------------------------

TITLE: Cookie Jar Implementation
DESCRIPTION: Demonstrates how to use a custom cookie jar with the sendRequest function using tough-cookie package.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';
import { CookieJar } from 'tough-cookie';

const cookieJar = new CookieJar();

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ cookieJar });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Basic Cheerio Crawler Implementation
DESCRIPTION: Initial implementation of a simple Cheerio crawler that downloads a single page and extracts its title.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Crawling Multiple URLs with Cheerio Crawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use Cheerio Crawler to crawl a list of specified URLs. It sets up the crawler, defines the request handler, and starts the crawl.

LANGUAGE: javascript
CODE:
// The code for this snippet is not directly visible in the provided text.
// It is referenced as an external file: './crawl_multiple_urls_cheerio.ts'

----------------------------------------

TITLE: Crawling Multiple URLs with Playwright
DESCRIPTION: Example showing URL crawling using Playwright Crawler in Crawlee. Requires apify/actor-node-playwright-chrome Docker image when running on Apify Platform. Playwright provides modern browser automation capabilities.

LANGUAGE: javascript
CODE:
{PlaywrightSource}

----------------------------------------

TITLE: Checking Product Stock Availability with Playwright
DESCRIPTION: This snippet demonstrates how to check if a product is in stock using Playwright by looking for a specific element on the page.

LANGUAGE: javascript
CODE:
const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Crawling Multiple URLs with Cheerio Crawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to use Cheerio Crawler to crawl a list of specified URLs. It extracts the title and URL from each page and stores the results.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks, log }) {
        const title = $('title').text();
        log.info(`Title of ${request.url} is '${title}'`);

        await Dataset.pushData({
            title,
            url: request.url,
        });
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run([
    'https://crawlee.dev',
    'https://apify.com',
]);

----------------------------------------

TITLE: Installing Crawlee using CLI
DESCRIPTION: Commands to install and run Crawlee using the CLI tool which sets up a new crawler project with necessary dependencies

LANGUAGE: bash
CODE:
npx crawlee create my-crawler

LANGUAGE: bash
CODE:
cd my-crawler && npm start

----------------------------------------

TITLE: Configuring Crawlee with Custom Storage
DESCRIPTION: Implementation of Crawlee configuration to ensure isolated storage for each crawler instance in Lambda environment

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Initializing Cheerio Crawler with Unique Configuration for AWS Lambda
DESCRIPTION: This snippet shows how to create a Cheerio crawler instance with a unique Configuration object, setting persistStorage to false for Lambda compatibility.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Using actor-node-playwright-webkit Docker Image
DESCRIPTION: Demonstrates the use of a Docker image with Playwright and WebKit pre-installed.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-webkit:16

----------------------------------------

TITLE: Initializing CheerioCrawler with Custom Configuration for AWS Lambda
DESCRIPTION: This snippet shows how to create a CheerioCrawler instance with a custom Configuration object that uses in-memory storage, which is necessary for AWS Lambda's read-only filesystem.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Failed Browser Crawler Output
DESCRIPTION: Shows the error message when attempting to access elements without proper waiting mechanisms in headless browsers.

LANGUAGE: log
CODE:
ERROR [...] Error: failed to find element matching selector ".ActorStoreItem"

----------------------------------------

TITLE: Configuring Apify Token Programmatically
DESCRIPTION: Example of setting up Apify token through Configuration instance

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';\n\nconst sdk = new Actor({ token: 'your_api_token' });

----------------------------------------

TITLE: Initializing CheerioCrawler with Custom Configuration
DESCRIPTION: Sets up a CheerioCrawler instance with persistent storage disabled for cloud function compatibility.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Initializing CheerioCrawler with Custom Configuration
DESCRIPTION: Sets up a CheerioCrawler instance with persistent storage disabled for cloud function compatibility.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Checking Product Stock Status with Playwright
DESCRIPTION: Demonstrates how to determine if a product is in stock by checking for the presence of specific elements.

LANGUAGE: javascript
CODE:
const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Basic Crawlee Crawler Implementation in TypeScript
DESCRIPTION: A simple Crawlee crawler that downloads the HTML of a single page, extracts the title, and prints it to the console.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Installing Crawlee
DESCRIPTION: npm command to install the Crawlee library for web scraping.

LANGUAGE: bash
CODE:
npm install crawlee

----------------------------------------

TITLE: Dockerfile for Crawlee TypeScript Project
DESCRIPTION: Multi-stage Dockerfile for building and running a Crawlee TypeScript project, including separate build and production stages.

LANGUAGE: dockerfile
CODE:
# using multistage build, as we need dev deps to build the TS source code
FROM apify/actor-node:16 AS builder

# copy all files, install all dependencies (including dev deps) and build the project
COPY . ./
RUN npm install --include=dev \
    && npm run build

# create final image
FROM apify/actor-node:16
# copy only necessary files
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/dist ./dist

# install only prod deps
RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional

# run compiled code
CMD npm run start:prod

----------------------------------------

TITLE: Configuring SessionPool with PlaywrightCrawler in Crawlee
DESCRIPTION: This example shows how to configure and use SessionPool with PlaywrightCrawler in Crawlee. It includes setup for proxy configuration and session pool management.

LANGUAGE: js
CODE:
import { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new PlaywrightCrawler({
    // To use the proxy IP rotation, you must turn the proxy usage on.
    useSessionPool: true,
    persistCookiesPerSession: true,
    proxyConfiguration,
    maxRequestsPerCrawl: 100,
    async requestHandler({ session, page, request }) {
        // ...
    },
});

await crawler.run();

----------------------------------------

TITLE: Configuring SessionPool with PlaywrightCrawler in Crawlee
DESCRIPTION: This example shows how to configure and use SessionPool with PlaywrightCrawler in Crawlee. It includes setup for proxy configuration and session pool management.

LANGUAGE: js
CODE:
import { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new PlaywrightCrawler({
    // To use the proxy IP rotation, you must turn the proxy usage on.
    useSessionPool: true,
    persistCookiesPerSession: true,
    proxyConfiguration,
    maxRequestsPerCrawl: 100,
    async requestHandler({ session, page, request }) {
        // ...
    },
});

await crawler.run();

----------------------------------------

TITLE: Configuring PlaywrightCrawler with Firefox for Web Scraping in JavaScript
DESCRIPTION: This code snippet shows how to initialize and configure PlaywrightCrawler to use a headless Firefox browser for web scraping. It sets up the crawler, defines the handling of each page, and starts the crawling process.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';
import { firefox } from 'playwright';

const crawler = new PlaywrightCrawler({
    // Use Firefox browser
    browserPoolOptions: {
        launcher: firefox,
    },
    async requestHandler({ page, request, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        // Save results
        await Dataset.pushData({
            title,
            url: request.url,
        });

        // Extract links to other pages
        // and add them to the crawling queue
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 10, // Limit to 10 requests
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Installing ts-node for Development
DESCRIPTION: Command to install ts-node for running TypeScript code directly during development

LANGUAGE: shell
CODE:
npm install --save-dev ts-node

----------------------------------------

TITLE: Installing ts-node for Development
DESCRIPTION: Command to install ts-node for running TypeScript code directly during development

LANGUAGE: shell
CODE:
npm install --save-dev ts-node

----------------------------------------

TITLE: Importing Dataset Module in Crawlee
DESCRIPTION: Demonstrates how to import the necessary modules from Crawlee for data storage functionality

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

----------------------------------------

TITLE: Custom Context Class Definition
DESCRIPTION: Defines a CustomContext class that extends BasicCrawlingContext and HttpCrawlingResult to handle crawling context with page data and enqueue links functionality.

LANGUAGE: python
CODE:
from __future__ import annotations

from dataclasses import dataclass
from typing import TYPE_CHECKING

from crawlee.basic_crawler import BasicCrawlingContext
from crawlee.http_crawler import HttpCrawlingResult

if TYPE_CHECKING:

    from collections.abc import Callable


@dataclass(frozen=True)
class CustomContext(HttpCrawlingResult, BasicCrawlingContext):
    """Crawling context used by CustomCrawler."""

    page_data: dict | None
    # not `EnqueueLinksFunction`` because we are breaking protocol since we are not working with HTML
    # and we are not using selectors
    enqueue_links: Callable

----------------------------------------

TITLE: Creating GCP Function Handler for CheerioCrawler
DESCRIPTION: Wraps the crawler execution in an async handler function that can be exported and used as the entry point for the GCP Cloud Function. It takes request and response objects as parameters and sends the crawler data as the response.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (req, res) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
    
    return res.send(await crawler.getData())
}

----------------------------------------

TITLE: Saving Data to Default Dataset in Crawlee using JavaScript
DESCRIPTION: This snippet demonstrates how to save data to the default dataset in Crawlee. It shows the process of pushing data to the dataset and handling the case when the dataset doesn't exist.

LANGUAGE: javascript
CODE:
import { Dataset } from 'crawlee';

await Dataset.pushData({
    title: 'Crawlee',
    url: 'https://crawlee.dev',
    modified: new Date(),
});

----------------------------------------

TITLE: Custom Configuration Implementation
DESCRIPTION: Demonstrates how to create and use a custom Configuration instance with specific crawler settings.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

const config = new Configuration({
    persistStateIntervalMillis: 10_000,
});

const crawler = new CheerioCrawler({}, config);

crawler.router.addDefaultHandler(async ({ request }) => {
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Configuring PlaywrightCrawler with Firefox Browser
DESCRIPTION: Example configuration for running PlaywrightCrawler with Firefox browser. Includes setting up browser parameters and integrating with the Apify Platform using the appropriate Docker image.

LANGUAGE: javascript
CODE:
{PlaywrightFirefoxSource}

----------------------------------------

TITLE: Customizing Browser Fingerprints with PlaywrightCrawler
DESCRIPTION: Example showing how to customize browser fingerprints for PlaywrightCrawler. Demonstrates setting specific browser and operating system parameters to generate targeted fingerprints.

LANGUAGE: javascript
CODE:
{PlaywrightSource}

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Router
DESCRIPTION: Sets up the main crawler instance using PlaywrightCrawler with a router for request handling and configures logging levels.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

// This is better set with CRAWLEE_LOG_LEVEL env var
// or a configuration option. This is just for show 😈
log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    // Instead of the long requestHandler with
    // if clauses we provide a router instance.
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Configuring Multi-Stage Docker Build for Crawlee Project
DESCRIPTION: This Dockerfile defines a multi-stage build process for a Crawlee-based actor. It uses apify/actor-node:16 as the base image, installs dependencies, builds the project, and creates a final image with only production-necessary files and packages.

LANGUAGE: Dockerfile
CODE:
FROM apify/actor-node:16 AS builder

COPY package*.json ./

RUN npm install --include=dev --audit=false

COPY . ./

RUN npm run build

FROM apify/actor-node:16

COPY --from=builder /usr/src/app/dist ./dist

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm run start:prod --silent

----------------------------------------

TITLE: DevTools Element Selection Query
DESCRIPTION: JavaScript query to select collection elements using CSS selectors in browser DevTools

LANGUAGE: javascript
CODE:
document.querySelectorAll('.collection-block-item');

----------------------------------------

TITLE: Using Pre-release Docker Image Tags
DESCRIPTION: Examples of how to use pre-release versions of Apify Docker images, with and without specifying the automation library version.

LANGUAGE: dockerfile
CODE:
# Without library version.
FROM apify/actor-node:20-beta

LANGUAGE: dockerfile
CODE:
# With library version.
FROM apify/actor-node-playwright-chrome:20-1.10.0-beta

----------------------------------------

TITLE: Initializing Cheerio Crawler with Custom Configuration
DESCRIPTION: Sets up a basic Cheerio crawler with a custom configuration to use in-memory storage for AWS Lambda compatibility.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Configuring Package.json for GCP Functions
DESCRIPTION: Updates the package.json configuration to specify the main entry point for the GCP function.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "version": "1.0.0",
    "main": "src/main.js",
    ...
}

----------------------------------------

TITLE: Category Page Crawling with Playwright
DESCRIPTION: Implementation of a PlaywrightCrawler that specifically targets category links using selectors and labels.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        // Wait for the category cards to render,
        // otherwise enqueueLinks wouldn't enqueue anything.
        await page.waitForSelector('.collection-block-item');

        // Add links to the queue, but only from
        // elements matching the provided selector.
        await enqueueLinks({
            selector: '.collection-block-item',
            label: 'CATEGORY',
        });
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Scraping with PlaywrightCrawler (JavaScript)
DESCRIPTION: Example of using PlaywrightCrawler to scrape JavaScript-rendered content from Apify Store. This code demonstrates waiting for elements to appear before extracting data.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        // Wait for the actor cards to render
        await page.waitForSelector('.ActorStoreItem');
        // Extract text content of the first actor card
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Scraping and Processing Product Price with Playwright
DESCRIPTION: Shows how to extract and process the product price, including filtering elements and converting string price to number.

LANGUAGE: javascript
CODE:
const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

----------------------------------------

TITLE: Cross-Context Access Using Context IDs
DESCRIPTION: Demonstration of how to maintain state across different crawling contexts using the new context ID system.

LANGUAGE: javascript
CODE:
let masterContextId;
const handlePageFunction = async ({ id, page, request, crawler }) => {
    if (request.userData.masterPage) {
        masterContextId = id;
        // Prepare the master page.
    } else {
        const masterContext = crawler.crawlingContexts.get(masterContextId);
        const masterPage = masterContext.page;
        const masterRequest = masterContext.request;
        // Now we can manipulate the master data from another handlePageFunction.
    }
}

----------------------------------------

TITLE: Response Management System Implementation
DESCRIPTION: Implements functions for storing and managing HTTP response objects, including success and error handling scenarios.

LANGUAGE: typescript
CODE:
const responses = new Map<string, ServerResponse>();

export function addResponse(responseId: string, response: ServerResponse) {
    responses.set(responseId, response);
}

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Router in JavaScript
DESCRIPTION: Sets up a PlaywrightCrawler instance using a router for request handling. It configures logging and runs the crawler on a specific URL.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: URL Pattern Example - Pagination Structure
DESCRIPTION: Example URL structure showing pagination pattern for the warehouse store product listings

LANGUAGE: plaintext
CODE:
https://warehouse-theme-metal.myshopify.com/collections/headphones?page=2

----------------------------------------

TITLE: Locating Crawlee Dataset Storage Directory
DESCRIPTION: This bash snippet shows the directory path where Crawlee stores individual files for each item in the default dataset. Each item is saved as a separate file in this location.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/datasets/default/

----------------------------------------

TITLE: Creating a new Crawlee project using pipx
DESCRIPTION: Command to create a new Crawlee project from a template using pipx with CLI extras.

LANGUAGE: shell
CODE:
pipx run 'crawlee[cli]' create my-crawler

----------------------------------------

TITLE: Installing Node.js Type Definitions
DESCRIPTION: Command to install TypeScript type definitions for Node.js

LANGUAGE: shell
CODE:
npm install --save-dev @types/node

----------------------------------------

TITLE: Saving Data with Dataset.pushData
DESCRIPTION: Code snippet showing how to save extracted results to the default Dataset storage

LANGUAGE: javascript
CODE:
await Dataset.pushData(results);

----------------------------------------

TITLE: Adapting Crawlee Routes for Parallel Scraping in JavaScript
DESCRIPTION: This code modifies the CATEGORY route handler to enqueue product URLs to a request queue that supports locking. It uses the Crawlee library and the previously defined getOrInitQueue function.

LANGUAGE: javascript
CODE:
import { getOrInitQueue } from './requestQueue.mjs';

// ...

export const router = createPlaywrightRouter();

router.addHandler('CATEGORY', async ({ request, page, log }) => {
    const requestQueue = await getOrInitQueue();

    log.info(`Extracting product links from ${request.url}`);
    const productLinks = await page.$$eval('a.product-item-photo', (links) =>
        links.map((link) => ({ url: link.href, userData: { label: 'DETAIL' } }))
    );

    log.info(`Extracted ${productLinks.length} product links`);
    for (const link of productLinks) {
        await requestQueue.addRequest(link);
    }

    // ...
});

// ...

----------------------------------------

TITLE: Successful Extraction Output
DESCRIPTION: Shows the successful (though unformatted) output when properly waiting for elements to render in the headless browser.

LANGUAGE: log
CODE:
ACTOR: Web Scraperapify/web-scraperCrawls arbitrary websites using [...]

----------------------------------------

TITLE: Configuring CheerioCrawler for GCP Cloud Functions
DESCRIPTION: Update the main.js file to use a separate Configuration instance with persistStorage set to false, and wrap the crawler call in a handler function.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (req, res) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
    
    return res.send(await crawler.getData())
}

----------------------------------------

TITLE: HTML Link Example
DESCRIPTION: Example of an HTML anchor tag with href attribute for demonstration purposes.

LANGUAGE: html
CODE:
<a href="https://crawlee.dev/js/docs/introduction">This is a link to Crawlee introduction</a>

----------------------------------------

TITLE: Crawling Multiple URLs with Puppeteer
DESCRIPTION: Implementation of multi-URL crawling using Puppeteer Crawler, which provides full browser automation capabilities. Requires the apify/actor-node-puppeteer-chrome image when running on Apify Platform.

LANGUAGE: javascript
CODE:
{PuppeteerSource}

----------------------------------------

TITLE: Specifying Dependency Versions in package.json
DESCRIPTION: Example of how to specify dependency versions in package.json when using Apify Docker images, using an asterisk for the pre-installed automation library.

LANGUAGE: json
CODE:
{
    "dependencies": {
        "crawlee": "^3.0.0",
        "playwright": "*"
    }
}

----------------------------------------

TITLE: Using Apify Proxy in Crawlee
DESCRIPTION: Example of creating and using an Apify Proxy configuration in Crawlee.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration();
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Crawling Context Migration Example
DESCRIPTION: Shows the difference between old and new handling of arguments in handler functions, demonstrating the unified crawling context approach

LANGUAGE: javascript
CODE:
const handlePageFunction = async (crawlingContext1) => {
    crawlingContext1.hasOwnProperty('proxyInfo') // true
}

const handleFailedRequestFunction = async (crawlingContext2) => {
    crawlingContext2.hasOwnProperty('proxyInfo') // true
}

// All contexts are the same object.
crawlingContext1 === crawlingContext2 // true

----------------------------------------

TITLE: Configuring Cheerio Crawler with In-Memory Storage
DESCRIPTION: Basic setup of CheerioCrawler with configuration for in-memory storage to handle Lambda's read-only filesystem.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Installing Node.js Type Declarations for Crawlee TypeScript Project
DESCRIPTION: Install type declarations for Node.js to enable type-checking in a Crawlee TypeScript project.

LANGUAGE: shell
CODE:
npm install --save-dev @types/node

----------------------------------------

TITLE: PlaywrightCrawler without Waiting (JavaScript)
DESCRIPTION: This snippet shows an attempt to scrape with PlaywrightCrawler without waiting for elements, which results in an error as the elements are not yet rendered.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        // This will fail because the element is not in the DOM yet
        const actorText = await page.textContent('.ActorStoreItem');
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Failed Extraction Output
DESCRIPTION: Shows the empty output from the CheerioCrawler attempt, demonstrating its inability to process JavaScript-rendered content.

LANGUAGE: log
CODE:
ACTOR:

----------------------------------------

TITLE: Checking Node.js Version in Bash
DESCRIPTION: Command to verify the installed version of Node.js.

LANGUAGE: bash
CODE:
node -v

----------------------------------------

TITLE: Crawling Single URL with Crawlee and got-scraping in JavaScript
DESCRIPTION: This code snippet demonstrates how to use Crawlee and got-scraping to fetch and process the HTML content of a single URL. It sets up a basic crawler, fetches the page content, and extracts specific information using Cheerio.

LANGUAGE: JavaScript
CODE:
import { gotScraping } from 'got-scraping';
import cheerio from 'cheerio';

const url = 'https://crawlee.dev';

console.log(`Fetching ${url}...`);
const response = await gotScraping(url);
const html = response.body;
const $ = cheerio.load(html);

const title = $('title').text();
const h1 = $('h1').text();
const metaDescription = $('meta[name="description"]').attr('content');

console.log('TITLE:', title);
console.log('H1:', h1);
console.log('META DESCRIPTION:', metaDescription);

console.log('Crawling finished.');

----------------------------------------

TITLE: Configuring Dockerfile for Apify Actor
DESCRIPTION: Defines the Dockerfile for running the Bluesky scraper as an Apify Actor, using the official Apify Docker image and UV for package management.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-python:3.13

ENV PATH='/app/.venv/bin:$PATH'

WORKDIR /app

COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

COPY pyproject.toml uv.lock ./

RUN uv sync --frozen --no-install-project --no-editable -q --no-dev

COPY . .

RUN uv sync --frozen --no-editable -q --no-dev

CMD ["bluesky-crawlee"]

----------------------------------------

TITLE: Dockerfile for Crawlee TypeScript Project
DESCRIPTION: Multi-stage Dockerfile for building and running a Crawlee TypeScript project in production, optimizing for smaller image size.

LANGUAGE: dockerfile
CODE:
# using multistage build, as we need dev deps to build the TS source code
FROM apify/actor-node:16 AS builder

# copy all files, install all dependencies (including dev deps) and build the project
COPY . ./
RUN npm install --include=dev \
    && npm run build

# create final image
FROM apify/actor-node:16
# copy only necessary files
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/dist ./dist

# install only prod deps
RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional

# run compiled code
CMD npm run start:prod

----------------------------------------

TITLE: Configuring Cheerio Crawler with In-Memory Storage
DESCRIPTION: Basic setup of CheerioCrawler with configuration for in-memory storage to handle Lambda's read-only filesystem.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Simplified CheerioCrawler Implementation
DESCRIPTION: Shows a more concise way to create a CheerioCrawler using the crawler.run() method with direct URL input, eliminating the need for manual RequestQueue initialization.

LANGUAGE: typescript
CODE:
// You don't need to import RequestQueue anymore
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

// Start the crawler with the provided URLs
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Example Dataset Structure in JSON
DESCRIPTION: Sample dataset showing scraped URLs and their heading counts stored in the project's default dataset directory.

LANGUAGE: json
CODE:
[
    {
        "url": "https://crawlee.dev/",
        "headingCount": 11
    },
    {
        "url": "https://crawlee.dev/storage",
        "headingCount": 8
    },
    {
        "url": "https://crawlee.dev/proxy",
        "headingCount": 4
    }
]

----------------------------------------

TITLE: Using PlaywrightCrawler with browserforge in Python
DESCRIPTION: Example demonstrating the use of PlaywrightCrawler with browserforge integration for improved fingerprinting and header management. It shows how to set up the crawler and log various browser and request details.

LANGUAGE: python
CODE:
import asyncio

from crawlee.crawlers import PlaywrightCrawler, PlaywrightCrawlingContext


async def main() -> None:
    # The browserforge fingerprints and headers are used by default.
    crawler = PlaywrightCrawler()

    @crawler.router.default_handler
    async def handler(context: PlaywrightCrawlingContext) -> None:
        url = context.request.url
        context.log.info(f'Crawling URL: {url}')

        # Decode and log the response body, which contains the headers we sent.
        headers = (await context.response.body()).decode()
        context.log.info(f'Response headers: {headers}')

        # Extract and log the User-Agent and UA data used in the browser context.
        ua = await context.page.evaluate('() => window.navigator.userAgent')
        ua_data = await context.page.evaluate('() => window.navigator.userAgentData')
        context.log.info(f'Navigator user-agent: {ua}')
        context.log.info(f'Navigator user-agent data: {ua_data}')

    # The endpoint httpbin.org/headers returns the request headers in the response body.
    await crawler.run(['https://www.httpbin.org/headers'])


if __name__ == '__main__':
    asyncio.run(main())

----------------------------------------

TITLE: Adjusting Crawlee Code for Apify Platform Deployment
DESCRIPTION: Modifications to the main Crawlee script to integrate with Apify SDK, including Actor initialization and exit calls.

LANGUAGE: typescript
CODE:
import { Actor } from 'apify';
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

await Actor.init();

log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

await Actor.exit();

----------------------------------------

TITLE: Adjusting Crawlee Code for Apify Platform Deployment
DESCRIPTION: Modifications to the main Crawlee script to integrate with Apify SDK, including Actor initialization and exit calls.

LANGUAGE: typescript
CODE:
import { Actor } from 'apify';
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

await Actor.init();

log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

await Actor.exit();

----------------------------------------

TITLE: Configuring PlaywrightCrawler with Firefox Browser
DESCRIPTION: Example demonstrating the setup of PlaywrightCrawler with Firefox browser integration. It includes browser configuration and crawler initialization for use with Apify platform.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

await PlaywrightCrawler.run({
    browserType: 'firefox',
    requestList: [
        'https://crawlee.dev',
    ],
    async requestHandler({ page, request, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url}: ${title}`);

        await Dataset.pushData({
            url: request.url,
            title,
        });
    },
});

----------------------------------------

TITLE: Migration Event Handler Setup
DESCRIPTION: Sets up an event listener to handle Actor migrations by adding timeouts to all active responses.

LANGUAGE: typescript
CODE:
Actor.on('migrating', ()=>{
    addTimeoutToAllResponses(60);
});

----------------------------------------

TITLE: Apify Project Initialization and Deployment
DESCRIPTION: Commands for initializing and deploying the Crawlee project to the Apify Platform

LANGUAGE: bash
CODE:
apify init

LANGUAGE: bash
CODE:
apify push

----------------------------------------

TITLE: Installing Apify SDK v1 Dependencies
DESCRIPTION: Commands for installing Apify SDK v1 with either Puppeteer or Playwright as the browser automation library

LANGUAGE: bash
CODE:
npm install apify puppeteer

LANGUAGE: bash
CODE:
npm install apify playwright

----------------------------------------

TITLE: Configuring Package.json for GCP Deployment
DESCRIPTION: Update the package.json configuration to specify the main entry point for the GCP function.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "version": "1.0.0",
    "main": "src/main.js",
    ...
}

----------------------------------------

TITLE: Configuring Package.json for GCP Deployment
DESCRIPTION: Update the package.json configuration to specify the main entry point for the GCP function.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "version": "1.0.0",
    "main": "src/main.js",
    ...
}

----------------------------------------

TITLE: sendRequest API Implementation in TypeScript
DESCRIPTION: Shows the implementation of the sendRequest function, which uses got-scraping to make HTTP requests. It includes default options and allows for overriding various parameters such as URL, method, headers, and proxy settings.

LANGUAGE: typescript
CODE:
async sendRequest(overrideOptions?: GotOptionsInit) => {
    return gotScraping({
        url: request.url,
        method: request.method,
        body: request.payload,
        headers: request.headers,
        proxyUrl: crawlingContext.proxyInfo?.url,
        sessionToken: session,
        responseType: 'text',
        ...overrideOptions,
        retry: {
            limit: 0,
            ...overrideOptions?.retry,
        },
        cookieJar: {
            getCookieString: (url: string) => session!.getCookieString(url),
            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),
            ...overrideOptions?.cookieJar,
        },
    });
}

----------------------------------------

TITLE: Installing and Zipping Chromium Dependencies for AWS Lambda
DESCRIPTION: Commands to install the @sparticuz/chromium package and zip the node_modules folder for use as a Lambda Layer.

LANGUAGE: bash
CODE:
# Install the package
npm i -S @sparticuz/chromium

# Zip the dependencies
zip -r dependencies.zip ./node_modules

----------------------------------------

TITLE: Capturing Screenshot with Puppeteer using page.screenshot()
DESCRIPTION: This snippet demonstrates how to capture a screenshot of a web page using Puppeteer's page.screenshot() method. It launches a browser, creates a new page, navigates to a URL, takes a screenshot, and saves it to a key-value store.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';
import { launch } from 'puppeteer';

const browser = await launch();
const page = await browser.newPage();
await page.goto('https://example.com');

const screenshot = await page.screenshot();
await KeyValueStore.setValue('my-screenshot', screenshot, { contentType: 'image/png' });

await browser.close();

----------------------------------------

TITLE: Disabling Browser Fingerprints with PuppeteerCrawler
DESCRIPTION: This snippet shows how to turn off browser fingerprints in PuppeteerCrawler by setting the useFingerprints option to false in the browserPoolOptions.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    browserPoolOptions: {
        useFingerprints: false,
    },
    // ... other options
});

----------------------------------------

TITLE: Creating a CheerioCrawler with RequestQueue in Crawlee
DESCRIPTION: This code creates a CheerioCrawler instance, sets up a RequestQueue, and defines a requestHandler to extract and log the title of the crawled webpage. It demonstrates the basic structure of a Crawlee crawler.

LANGUAGE: typescript
CODE:
import { RequestQueue, CheerioCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

const crawler = new CheerioCrawler({
    requestQueue,
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run();

----------------------------------------

TITLE: Creating a CheerioCrawler with RequestQueue in Crawlee
DESCRIPTION: This code creates a CheerioCrawler instance, sets up a RequestQueue, and defines a requestHandler to extract and log the title of the crawled webpage. It demonstrates the basic structure of a Crawlee crawler.

LANGUAGE: typescript
CODE:
import { RequestQueue, CheerioCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

const crawler = new CheerioCrawler({
    requestQueue,
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run();

----------------------------------------

TITLE: Proxy Configuration Example
DESCRIPTION: Shows how to configure a proxy server with BasicCrawler's sendRequest function

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            proxyUrl: 'http://auto:password@proxy.apify.com:8000',
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Installing Apify SDK v1 with Puppeteer
DESCRIPTION: Command to install Apify SDK v1 with Puppeteer support.

LANGUAGE: bash
CODE:
npm install apify puppeteer

----------------------------------------

TITLE: Sanity Check with PlaywrightCrawler and Cheerio in Crawlee
DESCRIPTION: This code snippet shows an alternative sanity check using PlaywrightCrawler with Cheerio for HTML parsing, printing the text content of category elements on the start page.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';
import { load } from 'cheerio';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page }) => {
        const html = await page.content();
        const $ = load(html);
        $('.collection-block-item').each((_, el) => {
            console.log($(el).text());
        });
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Wrapping Crawlee Crawler in Express Server for GCP Cloud Run Deployment
DESCRIPTION: This code demonstrates how to wrap a Crawlee crawler in an Express HTTP server for deployment to GCP Cloud Run. It sets up a GET route that runs the crawler and returns the crawled data, and listens on the port specified by GCP.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import express from 'express';
const app = express();

const startUrls = ['https://crawlee.dev'];

app.get('/', async (req, res) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));
    
    await crawler.run(startUrls);    

    return res.send(await crawler.getData());
});

app.listen(parseInt(process.env.PORT) || 3000);

----------------------------------------

TITLE: Manual Installation of Crawlee
DESCRIPTION: Installing Crawlee and Playwright dependencies manually via npm

LANGUAGE: bash
CODE:
npm install crawlee playwright

----------------------------------------

TITLE: Querying GraphQL API for Post Data in Python
DESCRIPTION: This snippet demonstrates how to make a POST request to a GraphQL API to retrieve post data. It includes the query structure and how to handle the response.

LANGUAGE: python
CODE:
import requests

url = "https://restoran.ua/graphql"

data = {
    "operationName": "Posts_PostsForView",
    "variables": {"sort": {"sortBy": ["startAt_DESC"]}},
    "query": """query Posts_PostsForView(
    $where: PostForViewWhereInput,
    $sort: PostForViewSortInput,
    $pagination: PaginationInput,
    $search: String,
    $token: String,
    $coordinates_slice: SliceInput)
    {
        PostsForView(
                where: $where
                sort: $sort
                pagination: $pagination
                search: $search
                token: $token
                ) {
                        id
                        title: ukTitle
                        summary: ukSummary
                        slug
                        startAt
                        endAt
                        newsFeed
                        events
                        journal
                        toProfessionals
                        photoHeader {
                            address: mobile
                            __typename
                            }
                        coordinates(slice: $coordinates_slice) {
                            lng
                            lat
                            __typename
                            }
                        __typename
                    }
    }"""
}

response = requests.post(url, json=data)

print(response.json())

----------------------------------------

TITLE: Setting Concurrency Limits in Crawlee
DESCRIPTION: Demonstrates how to configure minimum and maximum concurrency for parallel request handling in CheerioCrawler.

LANGUAGE: javascript
CODE:
new CheerioCrawler({
    minConcurrency: 1,
    maxConcurrency: 100,
    // ... other options
});

----------------------------------------

TITLE: Rendering Code Example - React/JSX
DESCRIPTION: JSX code block for rendering the dataset export example using the RunnableCodeBlock component with Cheerio syntax highlighting.

LANGUAGE: jsx
CODE:
<RunnableCodeBlock className="language-js" type="cheerio">
	{CrawlSource}
</RunnableCodeBlock>

----------------------------------------

TITLE: Defining Actor Input Schema
DESCRIPTION: Specifies the input schema for the Apify Actor, defining parameters such as Bluesky credentials, search queries, and crawling mode.

LANGUAGE: json
CODE:
{
  "title": "Bluesky - Crawlee",
  "type": "object",
  "schemaVersion": 1,
  "properties": {
    "identifier": {
      "title": "Bluesky identifier",
      "description": "Bluesky identifier for API login",
      "type": "string",
      "editor": "textfield",
      "isSecret": true
    },
    "appPassword": {
      "title": "Bluesky app password",
      "description": "Bluesky app password for API",
      "type": "string",
      "editor": "textfield",
      "isSecret": true
    },
    "maxRequestsPerCrawl": {
      "title": "Max requests per crawl",
      "description": "Maximum number of requests for crawling",
      "type": "integer"
    },
    "queries": {
      "title": "Queries",
      "type": "array",
      "description": "Search queries",
      "editor": "stringList",
      "prefill": [
        "apify"
      ],
      "example": [
        "apify",
        "crawlee"
      ]
    },
    "mode": {
      "title": "Mode",
      "type": "string",
      "description": "Collect posts or users who post on a topic",
      "enum": [
        "posts",
        "users"
      ],
      "default": "posts"
    }
  },
  "required": [
    "identifier",
    "appPassword",
    "queries",
    "mode"
  ]
}

----------------------------------------

TITLE: Docker Build Configuration
DESCRIPTION: Dockerfile setup for building TypeScript-based Crawlee projects using multi-stage builds.

LANGUAGE: dockerfile
CODE:
# using multistage build, as we need dev deps to build the TS source code
FROM apify/actor-node:20 AS builder

# copy all files, install all dependencies (including dev deps) and build the project
COPY . ./
RUN npm install --include=dev \
    && npm run build

# create final image
FROM apify/actor-node:20
# copy only necessary files
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/README.md ./
COPY --from=builder /usr/src/app/dist ./dist
COPY --from=builder /usr/src/app/apify.json ./apify.json
COPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json

# install only prod deps
RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional \
    && echo "Installed NPM packages:" \
    && (npm list --only=prod --no-optional --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

# run compiled code
CMD npm run start:prod

----------------------------------------

TITLE: Capturing Screenshot Using Direct Puppeteer Page API
DESCRIPTION: Demonstrates how to capture a screenshot of a webpage using Puppeteer's page.screenshot() method directly. The screenshot is saved to a key-value store with a key derived from the page URL.

LANGUAGE: javascript
CODE:
const { KeyValueStore } = require('crawlee');

const browser = await launchPuppeteer();
const page = await browser.newPage();
await page.goto('https://crawlee.dev');

// Create a unique key for the screenshot
const key = 'screenshot-crawlee.png';

// Save screenshot to default key-value store
await KeyValueStore.default().setValue(
    key,
    await page.screenshot(),
    { contentType: 'image/png' },
);

await browser.close();

----------------------------------------

TITLE: Using the Stop Method in ParselCrawler
DESCRIPTION: Shows how to use the new stop method to halt crawling when a specific condition is met.

LANGUAGE: python
CODE:
import asyncio

from crawlee.crawlers import ParselCrawler, ParselCrawlingContext


async def main() -> None:
    crawler = ParselCrawler()

    @crawler.router.default_handler
    async def handler(context: ParselCrawlingContext) -> None:
        context.log.info('Crawling: %s', context.request.url)

        # Extract and enqueue links from the page.
        await context.enqueue_links()

        title = context.selector.css('title::text').get()

        # Condition when you want to stop the crawler, e.g. you
        # have found what you were looking for.
        if 'Crawlee for Python' in title:
            context.log.info('Condition met, stopping the crawler.')
            await crawler.stop()

    await crawler.run(['https://crawlee.dev'])


if __name__ == '__main__':
    asyncio.run(main())

----------------------------------------

TITLE: Specifying Node.js Version in Dockerfile
DESCRIPTION: Example of how to specify a Node.js version when using an Apify Docker image.

LANGUAGE: dockerfile
CODE:
# Use Node.js 16
FROM apify/actor-node:16

----------------------------------------

TITLE: Cross-Context Access Using Context IDs
DESCRIPTION: Example of using crawling context IDs to maintain state and access data across different contexts

LANGUAGE: javascript
CODE:
let masterContextId;
const handlePageFunction = async ({ id, page, request, crawler }) => {
    if (request.userData.masterPage) {
        masterContextId = id;
        // Prepare the master page.
    } else {
        const masterContext = crawler.crawlingContexts.get(masterContextId);
        const masterPage = masterContext.page;
        const masterRequest = masterContext.request;
        // Now we can manipulate the master data from another handlePageFunction.
    }
}

----------------------------------------

TITLE: Creating Dockerfile for TypeScript Project
DESCRIPTION: Dockerfile configuration for building and running a TypeScript project using multi-stage build.

LANGUAGE: dockerfile
CODE:
# using multistage build, as we need dev deps to build the TS source code
FROM apify/actor-node:16 AS builder

# copy all files, install all dependencies (including dev deps) and build the project
COPY . ./
RUN npm install --include=dev \
    && npm run build

# create final image
FROM apify/actor-node:16
# copy only necessary files
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/dist ./dist

# install only prod deps
RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional

# run compiled code
CMD npm run start:prod

----------------------------------------

TITLE: Configuring SessionPool with PuppeteerCrawler in Crawlee
DESCRIPTION: This snippet demonstrates how to set up and use SessionPool with PuppeteerCrawler in Crawlee. It includes configuration for proxy usage and session pool management.

LANGUAGE: js
CODE:
import { PuppeteerCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new PuppeteerCrawler({
    // To use the proxy IP rotation, you must turn the proxy usage on.
    useSessionPool: true,
    persistCookiesPerSession: true,
    proxyConfiguration,
    maxRequestsPerCrawl: 100,
    async requestHandler({ session, page, request }) {
        // ...
    },
});

await crawler.run();

----------------------------------------

TITLE: Failed Scraping Attempt with PlaywrightCrawler (No Wait)
DESCRIPTION: This snippet shows an incorrect implementation using PlaywrightCrawler without waiting for elements to render, resulting in a failure to find the target element.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        // Try to extract text content of the first actor card without waiting
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Logging into Apify Platform using Configuration
DESCRIPTION: Example of logging into Apify Platform by setting the API token in the Actor configuration.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const sdk = new Actor({ token: 'your_api_token' });

----------------------------------------

TITLE: Basic Request Queue Operations in Crawlee
DESCRIPTION: Demonstrates basic operations with Request Queue in Crawlee, including opening a queue, adding requests, and fetching requests from the queue.

LANGUAGE: javascript
CODE:
import { RequestQueue } from 'crawlee';

// Open a named request queue
const queue = await RequestQueue.open('my-queue');

// Add requests to the queue
await queue.addRequest({ url: 'https://example.com/1' });
await queue.addRequest({ url: 'https://example.com/2' });
await queue.addRequest({ url: 'https://example.com/3' });

// Get request from queue
const request1 = await queue.fetchNextRequest();
console.log(request1.url);

// Get another request
const request2 = await queue.fetchNextRequest();
console.log(request2.url);

// Mark request as handled
await queue.markRequestHandled(request2);

----------------------------------------

TITLE: Configuring TypeScript Compiler Options
DESCRIPTION: JSON configuration for tsconfig.json to set up TypeScript compiler options for a Crawlee project.

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist"
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Session Pool Implementation with BasicCrawler
DESCRIPTION: Example showing how to implement SessionPool with BasicCrawler for managing proxy rotations and session states

LANGUAGE: javascript
CODE:
{BasicSource}

----------------------------------------

TITLE: Extracting Manufacturer from URL in JavaScript
DESCRIPTION: This snippet shows how to extract the manufacturer name from a product URL by splitting the string.

LANGUAGE: javascript
CODE:
// request.url = https://warehouse-theme-metal.myshopify.com/products/sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440

const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']
const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'

----------------------------------------

TITLE: Installing Apify SDK v1 with Puppeteer
DESCRIPTION: Command to install Apify SDK v1.0.0 with Puppeteer support

LANGUAGE: bash
CODE:
npm install apify puppeteer

----------------------------------------

TITLE: Configuring PlaywrightCrawler with Persistent Storage Disabled in JavaScript
DESCRIPTION: This snippet shows how to initialize a PlaywrightCrawler with a custom Configuration that disables persistent storage, which is necessary for serverless environments.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Configuring PlaywrightCrawler with Persistent Storage Disabled in JavaScript
DESCRIPTION: This snippet shows how to initialize a PlaywrightCrawler with a custom Configuration that disables persistent storage, which is necessary for serverless environments.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Router Implementation for Request Handling
DESCRIPTION: Defines router handlers for different types of requests including search pages and listing details.

LANGUAGE: python
CODE:
from crawlee.router import Router

from .constants import LISTING_PATH, SEARCH_PATH, TARGET_LOCATIONS
from .custom_crawler import CustomContext

router = Router[CustomContext]()

@router.default_handler
async def default_handler(context: CustomContext) -> None:
    """Handle the start URL to get the Build ID and create search links."""
    context.log.info(f'default_handler is processing {context.request.url}')

    await context.enqueue_links(
        path_template=SEARCH_PATH, items=TARGET_LOCATIONS, label='SEARCH', user_data={'page': 1}
    )

@router.handler('SEARCH')
async def search_handler(context: CustomContext) -> None:
    """Handle the SEARCH URL generates links to listings and to the next search page."""
    context.log.info(f'search_handler is processing {context.request.url}')

    max_pages = context.page_data['pageProps']['initialPageCount']
    current_page = context.request.user_data['page']
    if current_page < max_pages:
        await context.enqueue_links(
            path_template=SEARCH_PATH,
            items=[context.request.user_data['location']],
            label='SEARCH',
            user_data={'page': current_page + 1},
        )
    else:
        context.log.info(f'Last page for {context.request.user_data["location"]} location')

    listing_ids = [
        listing['property']['id']
        for group in context.page_data['pageProps']['initialListings']['groups']
        for listing in group['results']
        if listing.get('property')
    ]

    await context.enqueue_links(path_template=LISTING_PATH, items=listing_ids, label='LISTING')

----------------------------------------

TITLE: Importing Dataset Module in Crawlee (TypeScript)
DESCRIPTION: This snippet shows how to import the Dataset module along with PlaywrightCrawler from Crawlee. These imports are necessary for saving scraped data.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

----------------------------------------

TITLE: Installing TypeScript Compiler for Crawlee Project
DESCRIPTION: This command installs TypeScript as a development dependency for a Crawlee project.

LANGUAGE: shell
CODE:
npm install --save-dev typescript

----------------------------------------

TITLE: Cleaning Up Default Storages in Crawlee
DESCRIPTION: Demonstrates how to explicitly purge default storages in Crawlee using the purgeDefaultStorages() helper function.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Extracting Text Content from an Element using Cheerio
DESCRIPTION: This code snippet demonstrates how to use Cheerio to find the first <h2> element on a page and extract its text content.

LANGUAGE: javascript
CODE:
$('h2').text()

----------------------------------------

TITLE: Adding Requests in Batches to Request Queue in Crawlee
DESCRIPTION: Demonstrates how to add multiple requests to a request queue in a single batch operation using the addRequests() function in Crawlee.

LANGUAGE: javascript
CODE:
import { RequestQueue, PuppeteerCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();

// Add multiple requests to the queue in a single batch
await requestQueue.addRequests([
    { url: 'http://example.com/1' },
    { url: 'http://example.com/2' },
    { url: 'http://example.com/3' },
]);

const crawler = new PuppeteerCrawler({
    requestQueue,
    maxRequestsPerCrawl: 10,
    async requestHandler({ request }) {
        console.log(request.url);
    },
});

await crawler.run();

----------------------------------------

TITLE: Using Request Loaders with ParselCrawler
DESCRIPTION: Demonstrates how to use the new Request Loaders feature to combine multiple request sources for crawling.

LANGUAGE: python
CODE:
import asyncio

from crawlee.crawlers import ParselCrawler, ParselCrawlingContext
from crawlee.request_loaders import RequestList, RequestManagerTandem
from crawlee.storages import RequestQueue


async def main() -> None:
    rl = RequestList(
        [
            'https://crawlee.dev',
            'https://apify.com',
            # Long list of URLs...
        ],
    )

    rq = await RequestQueue.open()

    # Combine them into a single request source.
    tandem = RequestManagerTandem(rl, rq)

    crawler = ParselCrawler(request_manager=tandem)

    @crawler.router.default_handler
    async def handler(context: ParselCrawlingContext) -> None:
        context.log.info(f'Crawling {context.request.url}')
        # ...

    await crawler.run()


if __name__ == '__main__':
    asyncio.run(main())

----------------------------------------

TITLE: Displaying Dataset Storage Location in Bash
DESCRIPTION: This snippet shows the directory path where each item in the default dataset will be saved as a separate file.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/datasets/default/

----------------------------------------

TITLE: Crawling All Links with CheerioCrawler in Crawlee
DESCRIPTION: This code snippet demonstrates how to use CheerioCrawler to crawl all links found on a website, regardless of their domain. It uses the 'All' enqueue strategy to process any URLs encountered during the crawl.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks }) {
        // Add all the links from the page to the crawler's RequestQueue
        // includeUnique: false, allows to add the same url multiple times,
        // which we might want to allow in this case, as long as the content
        // is still unique.
        await enqueueLinks({
            strategy: EnqueueStrategy.All,
            includeUnique: false,
        });
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Installing Crawlee Dependencies with Python
DESCRIPTION: Commands to install Crawlee and its required dependencies using pipx, and create a new project using the Crawlee CLI

LANGUAGE: bash
CODE:
pipx install crawlee[beautifulsoup,curl-impersonate]
pipx run crawlee create crawlee-google-search
cd crawlee-google-search
poetry install

----------------------------------------

TITLE: Package Dependencies Configuration
DESCRIPTION: Example package.json configuration showing proper version handling for browser automation libraries.

LANGUAGE: json
CODE:
{
    "dependencies": {
        "crawlee": "^3.0.0",
        "playwright": "*"
    }
}

----------------------------------------

TITLE: Updated Launch Context Configuration
DESCRIPTION: Shows the new way to configure browser launch options with explicit separation of Apify and browser-specific options.

LANGUAGE: javascript
CODE:
const crawler = new Apify.PuppeteerCrawler({
    launchContext: {
        useChrome: true, // Apify option
        launchOptions: {
            headless: false // Puppeteer option
        }
    }
})

----------------------------------------

TITLE: Basic Link Enqueuing with Crawlee
DESCRIPTION: Simple example of using the enqueueLinks() function in Crawlee to add links to the crawling queue.

LANGUAGE: javascript
CODE:
await enqueueLinks();

----------------------------------------

TITLE: Building and Running Crawlee Project Docker Image
DESCRIPTION: This Dockerfile creates a multi-stage build for a Crawlee project. It first builds the application in a builder stage, then creates a lean production image with only the necessary files and dependencies. The final image is based on apify/actor-node:16 and runs the application using 'npm run start:prod'.

LANGUAGE: Dockerfile
CODE:
# Specify the base Docker image. You can read more about
# the available images at https://crawlee.dev/js/docs/guides/docker-images
# You can also use any other image from Docker Hub.
FROM apify/actor-node:16 AS builder

# Copy just package.json and package-lock.json
# to speed up the build using Docker layer cache.
COPY package*.json ./

# Install all dependencies. Don't audit to speed up the installation.
RUN npm install --include=dev --audit=false

# Next, copy the source files using the user set
# in the base image.
COPY . ./

# Install all dependencies and build the project.
# Don't audit to speed up the installation.
RUN npm run build

# Create final image
FROM apify/actor-node:16

# Copy only built JS files from builder image
COPY --from=builder /usr/src/app/dist ./dist

# Copy just package.json and package-lock.json
# to speed up the build using Docker layer cache.
COPY package*.json ./

# Install NPM packages, skip optional and development dependencies to
# keep the image small. Avoid logging too much and print the dependency
# tree for debugging
RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

# Next, copy the remaining files and directories with the source code.
# Since we do this after NPM install, quick build will be really fast
# for most source file changes.
COPY . ./


# Run the image.
CMD npm run start:prod --silent

----------------------------------------

TITLE: Setting up Node.js HTTP Server for Crawlee
DESCRIPTION: Basic HTTP server setup using Node.js built-in http module to handle incoming requests. The server listens on port 3000 and currently returns a simple 'Hello World' response.

LANGUAGE: javascript
CODE:
import { createServer } from 'http';
import { log } from 'crawlee';

const server = createServer(async (req, res) => {
    log.info(`Request received: ${req.method} ${req.url}`);

    res.writeHead(200, { 'Content-Type': 'text/plain' });
    // We will return the page title here later instead
    res.end('Hello World\n');
});

server.listen(3000, () => {
    log.info('Server is listening for user requests');
});

----------------------------------------

TITLE: Automating GitHub Repository Search Form with PuppeteerCrawler
DESCRIPTION: Implementation of a web crawler that automates GitHub's repository search form. The crawler fills in search parameters including search term, repository owner, start date, and language, then submits the form and saves the results to either Apify platform's dataset or local storage.

LANGUAGE: javascript
CODE:
import { Dataset, createPuppeteerRouter, PuppeteerCrawler } from 'crawlee';

const router = createPuppeteerRouter();

router.addDefaultHandler(async ({ page, log }) => {
    log.info('Clicking on advanced search');
    await page.click('a[href="/search/advanced?q="]');

    log.info('Filling form');
    await page.type('#search_from', '2022-05-01');
    await page.type('#search_language', 'JavaScript');
    await page.type('#search_user', 'apify');
    await page.type('#search_q', 'crawlee');

    log.info('Submitting form');
    await Promise.all([
        page.waitForNavigation(),
        page.click('.js-advanced-search-submit'),
    ]);

    log.info('Extracting data');
    const repositories = await page.$$eval('.repo-list-item', (items) => {
        return items.map((item) => ({
            url: item.querySelector('a').href,
            title: item.querySelector('a').textContent.trim(),
            description: item.querySelector('p')?.textContent?.trim(),
        }));
    });

    log.info(`Found ${repositories.length} repositories`);
    await Dataset.pushData(repositories);
});

const crawler = new PuppeteerCrawler({
    requestHandler: router,
    // Uncomment to hide the browser
    // headless: true,
});

await crawler.run(['https://github.com/search']);

----------------------------------------

TITLE: Filtering URLs with Globs in Crawlee
DESCRIPTION: Demonstrates how to use glob patterns to filter URLs in Crawlee's enqueueLinks function.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
});

----------------------------------------

TITLE: Deploying an actor to Apify platform
DESCRIPTION: Command to deploy the local actor code to the Apify platform.

LANGUAGE: bash
CODE:
apify push

----------------------------------------

TITLE: Crawling Multiple URLs with Cheerio Crawler in Crawlee
DESCRIPTION: This code snippet demonstrates how to use the Cheerio Crawler in Crawlee to crawl a list of specified URLs. It sets up the crawler, defines the request handler, and starts the crawl.

LANGUAGE: javascript
CODE:
{CheerioSource}

----------------------------------------

TITLE: Configuring Crawlee using custom Configuration
DESCRIPTION: This example shows how to create a custom Configuration instance and pass it to a CheerioCrawler. It sets the persistStateIntervalMillis option to 10 seconds.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Create new configuration
const config = new Configuration({
    // Set the 'persistStateIntervalMillis' option to 10 seconds
    persistStateIntervalMillis: 10_000,
});

// Now we need to pass the configuration to the crawler
const crawler = new CheerioCrawler({}, config);

crawler.router.addDefaultHandler(async ({ request }) => {
    // for the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // for the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Custom Configuration in JavaScript
DESCRIPTION: This snippet demonstrates how to create a new PlaywrightCrawler instance with a custom Configuration object that disables persistent storage. It sets up the crawler with a router for request handling and defines the start URLs.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Installing Playwright with Crawlee
DESCRIPTION: Commands to install Crawlee with Playwright for browser automation, showing both meta-package and specific package options.

LANGUAGE: bash
CODE:
npm install crawlee playwright
# or npm install @crawlee/playwright playwright

----------------------------------------

TITLE: Displaying Dataset Storage Path
DESCRIPTION: Shows the default storage path where dataset items are saved as individual files within the project structure.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/datasets/default/

----------------------------------------

TITLE: Crawling Context Handler Example
DESCRIPTION: Demonstrates the new unified crawling context between handler functions

LANGUAGE: javascript
CODE:
const handlePageFunction = async (crawlingContext1) => {
    crawlingContext1.hasOwnProperty('proxyInfo') // true
}

const handleFailedRequestFunction = async (crawlingContext2) => {
    crawlingContext2.hasOwnProperty('proxyInfo') // true
}

// All contexts are the same object.
crawlingContext1 === crawlingContext2 // true

----------------------------------------

TITLE: Proxy Configuration Example
DESCRIPTION: Shows how to configure a proxy server when making requests with BasicCrawler.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            proxyUrl: 'http://auto:password@proxy.apify.com:8000',
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Installing and Zipping Browser Dependencies for AWS Lambda
DESCRIPTION: Commands to install the @sparticuz/chromium package and zip the node_modules folder for use as a Lambda Layer.

LANGUAGE: bash
CODE:
# Install the package
npm i -S @sparticuz/chromium

# Zip the dependencies
zip -r dependencies.zip ./node_modules

----------------------------------------

TITLE: Initializing Shared Request Queue with Locking Support in JavaScript
DESCRIPTION: Creates a function to initialize or retrieve a shared request queue that supports request locking. This queue is used to store URLs for parallel scraping.

LANGUAGE: javascript
CODE:
import { RequestQueue } from 'crawlee';
import { Configuration, experiments } from 'crawlee';

experiments.requestLocking.enable();

let requestQueue;

async function getOrInitQueue(reinitialize = false) {
    if (requestQueue && !reinitialize) {
        return requestQueue;
    }

    requestQueue = await RequestQueue.open('request-queue');

    if (reinitialize) {
        await requestQueue.drop();
        requestQueue = await RequestQueue.open('request-queue');
    }

    return requestQueue;
}

export { getOrInitQueue };

----------------------------------------

TITLE: Sitemap Crawling with Cheerio
DESCRIPTION: Implementation of sitemap crawling using Cheerio Crawler. Uses downloadListOfUrls utility from @crawlee/utils to process sitemap URLs.

LANGUAGE: javascript
CODE:
{CheerioSource}

----------------------------------------

TITLE: Creating API Start URLs with Parameters
DESCRIPTION: Function that generates the initial API URL with query parameters for TikTok's Creative Radar API. Handles filtering options like days, country, industry, and results limit.

LANGUAGE: javascript
CODE:
export const createStartUrls = (input) => {
    const {
        days = '7',
        country = '',
        resultsLimit = 100,
        industry = '',
        isNewToTop100,
    } = input;

    const filterBy = isNewToTop100 ? 'new_on_board' : '';
    return [
        {
            url: `https://ads.tiktok.com/creative_radar_api/v1/popular_trend/hashtag/list?page=1&limit=50&period=${days}&country_code=${country}&filter_by=${filterBy}&sort_by=popular&industry_id=${industry}`,
            headers: {
                // required headers
            },
            userData: { resultsLimit },
        },
    ];
};

----------------------------------------

TITLE: Saving Scraped Data to JSON Files
DESCRIPTION: Implements the save_data method to write collected user and post data to separate JSON files.

LANGUAGE: python
CODE:
async def save_data(self) -> None:
    """Save the data."""
    if not self._users or not self._posts:
        raise ValueError('Datasets not initialized.')

    with open('users.json', 'w') as f:
        await self._users.write_to_json(f, indent=4)

    with open('posts.json', 'w') as f:
        await self._posts.write_to_json(f, indent=4)

----------------------------------------

TITLE: Cross-Context Access Example
DESCRIPTION: Shows how to access and manipulate data across different crawling contexts using context IDs.

LANGUAGE: javascript
CODE:
let masterContextId;
const handlePageFunction = async ({ id, page, request, crawler }) => {
    if (request.userData.masterPage) {
        masterContextId = id;
        // Prepare the master page.
    } else {
        const masterContext = crawler.crawlingContexts.get(masterContextId);
        const masterPage = masterContext.page;
        const masterRequest = masterContext.request;
        // Now we can manipulate the master data from another handlePageFunction.
    }
}

----------------------------------------

TITLE: Setting Maximum Requests per Crawl in Crawlee
DESCRIPTION: Example of how to set a maximum limit on the number of requests processed during a crawl using the maxRequestsPerCrawl option.

LANGUAGE: typescript
CODE:
const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 20,
    // ...
});

----------------------------------------

TITLE: Creating a RequestQueue with Locking Support (TypeScript)
DESCRIPTION: This snippet shows how to create a RequestQueue that supports locking by using the RequestQueueV2 class instead of the regular RequestQueue. It demonstrates importing the class and adding requests to the queue.

LANGUAGE: typescript
CODE:
import { RequestQueueV2 } from 'crawlee';

const queue = await RequestQueueV2.open('my-locking-queue');
await queue.addRequests([
    { url: 'https://crawlee.dev' },
    { url: 'https://crawlee.dev/js/docs' },
    { url: 'https://crawlee.dev/js/api' },
]);

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Configuration
DESCRIPTION: Sets up a PlaywrightCrawler instance with disabled storage persistence for cloud deployment. Demonstrates basic crawler configuration with a single start URL.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Using Vitest Mocked Function in Crawlee Tests
DESCRIPTION: Example of using Vitest's mocked function utility for type inference in tests.

LANGUAGE: typescript
CODE:
import os from 'node:os';

const mockedPlatform = vitest.mocked(os.platform);

----------------------------------------

TITLE: Failing to Scrape Dynamic Content with PuppeteerCrawler (No Wait)
DESCRIPTION: This snippet demonstrates an incorrect way to use PuppeteerCrawler without waiting for elements to render, which results in a failure to scrape the content.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // This will fail because the element might not be rendered yet
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Category Page Crawling with Playwright
DESCRIPTION: Implementation of a PlaywrightCrawler that crawls category pages using selective link enqueueing with custom selectors and labels.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        // Wait for the category cards to render,
        // otherwise enqueueLinks wouldn't enqueue anything.
        await page.waitForSelector('.collection-block-item');

        // Add links to the queue, but only from
        // elements matching the provided selector.
        await enqueueLinks({
            selector: '.collection-block-item',
            label: 'CATEGORY',
        });
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Example Dataset Structure in JSON
DESCRIPTION: This snippet shows the structure of the example dataset used in the demonstrations. It contains URL and heading count information for different web pages.

LANGUAGE: json
CODE:
[
    {
        "url": "https://crawlee.dev/",
        "headingCount": 11
    },
    {
        "url": "https://crawlee.dev/storage",
        "headingCount": 8
    },
    {
        "url": "https://crawlee.dev/proxy",
        "headingCount": 4
    }
]

----------------------------------------

TITLE: Skipping Navigation for Image Requests in PlaywrightCrawler
DESCRIPTION: This code snippet demonstrates how to use PlaywrightCrawler to crawl a website, skip navigation for image requests, and save the images directly to a key-value store. It utilizes the Request#skipNavigation option and sendRequest method.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';
import { readFile, writeFile } from 'fs/promises';

const crawler = new PlaywrightCrawler({
    async requestHandler({ request, page, enqueueLinks, log, sendRequest }) {
        const title = await page.title();
        log.info(`Title of ${request.loadedUrl} is '${title}'`);

        if (request.label === 'IMAGE') {
            const imageBuffer = await sendRequest({
                ...request,
                responseType: 'buffer',
            });
            await Dataset.pushData({
                url: request.url,
                imageLength: imageBuffer.byteLength,
            });
            return;
        }

        await Dataset.pushData({
            title,
            url: request.loadedUrl,
        });

        await enqueueLinks({
            globs: ['https://crawlee.dev/**'],
            label: 'DETAIL',
        });

        const imageSources = await page.evaluate(() => {
            return [...document.querySelectorAll('img')].map((img) => img.src);
        });

        for (const src of imageSources) {
            await crawler.addRequests([{
                url: src,
                label: 'IMAGE',
                skipNavigation: true
            }]);
        }
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run(['https://crawlee.dev']);


----------------------------------------

TITLE: Crawler Configuration with Request Limit
DESCRIPTION: Example showing how to limit the maximum number of requests per crawl using maxRequestsPerCrawl option.

LANGUAGE: typescript
CODE:
const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 20,
    // ...
});

----------------------------------------

TITLE: Handling JSON Responses in BasicCrawler
DESCRIPTION: This snippet demonstrates how to configure BasicCrawler to handle JSON responses. It sets the responseType option to 'json' when calling sendRequest to automatically parse the response as JSON.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ responseType: 'json' });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Implementing Express Server with PlaywrightCrawler for GCP Cloud Run
DESCRIPTION: Creates an Express HTTP server that wraps the PlaywrightCrawler implementation, handling incoming requests and returning crawler data. Includes port configuration for GCP Cloud Run environment.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import express from 'express';
const app = express();

const startUrls = ['https://crawlee.dev'];

app.get('/', async (req, res) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));
    
    await crawler.run(startUrls);    

    return res.send(await crawler.getData());
});

app.listen(parseInt(process.env.PORT) || 3000);

----------------------------------------

TITLE: Storage Cleanup in Crawlee
DESCRIPTION: Shows how to clean up default storage directories in Crawlee using the purgeDefaultStorages helper function.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: TypeScript Configuration
DESCRIPTION: TSConfig setup extending @apify/tsconfig with ES2022 module and target settings for top-level await support.

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist"
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Using Map of Crawling Contexts
DESCRIPTION: Example demonstrating how to use the Map of crawling contexts for cross-context access.

LANGUAGE: javascript
CODE:
let masterContextId;
const handlePageFunction = async ({ id, page, request, crawler }) => {
    if (request.userData.masterPage) {
        masterContextId = id;
        // Prepare the master page.
    } else {
        const masterContext = crawler.crawlingContexts.get(masterContextId);
        const masterPage = masterContext.page;
        const masterRequest = masterContext.request;
        // Now we can manipulate the master data from another handlePageFunction.
    }
}

----------------------------------------

TITLE: SendRequest Implementation
DESCRIPTION: Shows the internal implementation of the sendRequest function with all available options and default configurations.

LANGUAGE: typescript
CODE:
async sendRequest(overrideOptions?: GotOptionsInit) => {
    return gotScraping({
        url: request.url,
        method: request.method,
        body: request.payload,
        headers: request.headers,
        proxyUrl: crawlingContext.proxyInfo?.url,
        sessionToken: session,
        responseType: 'text',
        ...overrideOptions,
        retry: {
            limit: 0,
            ...overrideOptions?.retry,
        },
        cookieJar: {
            getCookieString: (url: string) => session!.getCookieString(url),
            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),
            ...overrideOptions?.cookieJar,
        },
    });
}

----------------------------------------

TITLE: Extracting Product SKU with Playwright
DESCRIPTION: Demonstrates locating and extracting the product SKU using Playwright's locator method.

LANGUAGE: javascript
CODE:
const sku = await page.locator('span.product-meta__sku-number').textContent();

----------------------------------------

TITLE: Example Dataset Structure in JSON
DESCRIPTION: This snippet shows the structure of the example dataset used in the demonstrations. It contains URL and heading count information for different web pages.

LANGUAGE: json
CODE:
[
    {
        "url": "https://crawlee.dev/",
        "headingCount": 11
    },
    {
        "url": "https://crawlee.dev/storage",
        "headingCount": 8
    },
    {
        "url": "https://crawlee.dev/proxy",
        "headingCount": 4
    }
]

----------------------------------------

TITLE: Crawling Links Using CheerioCrawler and Glob Patterns
DESCRIPTION: Example showing how to use CheerioCrawler to crawl specific links that match glob patterns. Demonstrates the usage of enqueueLinks method with globs property to filter and queue only matching URLs.

LANGUAGE: typescript
CODE:
import { CheerioCrawler, CheerioCrawlingContext, EnqueueLinksOptions } from 'crawlee';

const crawler = new CheerioCrawler({ ... });

crawler.addHandler('enqueueLinks', async ({ enqueueLinks, $ }: CheerioCrawlingContext) => {
    // Filter and queue matching links
    await enqueueLinks({
        globs: ['**/matching-pattern/*']
    });
});

----------------------------------------

TITLE: Demonstrating Key-Value Store Operations in Crawlee
DESCRIPTION: Examples showing how to perform basic operations with key-value stores including reading input, writing output, and managing named stores. Demonstrates automatic JSON conversion and various data type handling.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';

// Get the INPUT from the default key-value store
const input = await KeyValueStore.getInput();

// Write the OUTPUT to the default key-value store
await KeyValueStore.setValue('OUTPUT', { myResult: 123 });

// Open a named key-value store
const store = await KeyValueStore.open('some-name');

// Write a record to the named key-value store.
// JavaScript object is automatically converted to JSON,
// strings and binary buffers are stored as they are
await store.setValue('some-key', { foo: 'bar' });

// Read a record from the named key-value store.
// Note that JSON is automatically parsed to a JavaScript object,
// text data is returned as a string, and other data is returned as binary buffer
const value = await store.getValue('some-key');

// Delete a record from the named key-value store
await store.setValue('some-key', null);

----------------------------------------

TITLE: Implementing Search Handler for Bluesky Scraper
DESCRIPTION: Defines the _search_handler method to process search requests, save post data, and create user requests for further processing.

LANGUAGE: python
CODE:
async def _search_handler(self, context: HttpCrawlingContext) -> None:
    context.log.info(f'Processing search {context.request.url} ...')

    data = json.loads(context.http_response.read())

    if 'posts' not in data:
        context.log.warning(f'No posts found in response: {context.request.url}')
        return

    user_requests = {}
    posts = []

    profile_url = URL(f'{self._service_endpoint}/xrpc/app.bsky.actor.getProfile')

    for post in data['posts']:
        # Add user request if not already added in current context
        if post['author']['did'] not in user_requests:
            user_requests[post['author']['did']] = Request.from_url(
                url=str(profile_url.with_query(actor=post['author']['did'])),
                user_data={'label': 'user'},
            )

        posts.append(
            {
                'uri': post['uri'],
                'cid': post['cid'],
                'author_did': post['author']['did'],
                'created': post['record']['createdAt'],
                'indexed': post['indexedAt'],
                'reply_count': post['replyCount'],
                'repost_count': post['repostCount'],
                'like_count': post['likeCount'],
                'quote_count': post['quoteCount'],
                'text': post['record']['text'],
                'langs': '; '.join(post['record'].get('langs', [])),
                'reply_parent': post['record'].get('reply', {}).get('parent', {}).get('uri'),
                'reply_root': post['record'].get('reply', {}).get('root', {}).get('uri'),
            }
        )

    await self._posts.push_data(posts)  # Push a batch of posts to the dataset
    await context.add_requests(list(user_requests.values()))

    if cursor := data.get('cursor'):
        next_url = URL(context.request.url).update_query({'cursor': cursor})  # Use yarl for update the query string

        await context.add_requests([str(next_url)])

----------------------------------------

TITLE: Comparing Browser and JSDOM Title Access
DESCRIPTION: Demonstrates the difference between accessing page title in browsers versus JSDOM environment. Shows both standard browser JavaScript and JSDOM-specific syntax.

LANGUAGE: typescript
CODE:
// Return the page title
document.title; // browsers
window.document.title; // JSDOM

----------------------------------------

TITLE: Crawling Filtered Links with CheerioCrawler in JavaScript
DESCRIPTION: This code snippet shows how to set up a CheerioCrawler to crawl specific links on a website. It uses the globs property to filter URLs and the enqueueLinks() method to add matching links to the RequestQueue. The crawler processes each page, extracting and logging the title.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Dataset } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request, enqueueLinks }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}`);

        await Dataset.pushData({
            title,
            url: request.url,
        });

        await enqueueLinks({
            globs: ['https://crawlee.dev/**'],
            label: 'DETAIL',
        });
    },
});

await crawler.run(['https://crawlee.dev']);


----------------------------------------

TITLE: Configuring Crawlee with crawlee.json
DESCRIPTION: Example of a crawlee.json file used to set global configuration options for Crawlee. It demonstrates setting the persistStateIntervalMillis and logLevel parameters.

LANGUAGE: json
CODE:
{
  "persistStateIntervalMillis": 10000,
  "logLevel": "DEBUG"
}

----------------------------------------

TITLE: Implementing Recursive Web Crawling with PuppeteerCrawler in JavaScript
DESCRIPTION: This code snippet demonstrates how to set up and execute a recursive web crawl using the PuppeteerCrawler class. It includes configuration for the crawler, handling of visited pages, and extraction of data from each page.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, Dataset } from 'crawlee';

const crawler = new PuppeteerCrawler({
    // Function called for each URL
    async requestHandler({ request, page, enqueueLinks }) {
        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);

        // Save results to dataset
        await Dataset.pushData({
            url: request.url,
            title,
        });

        // Add newly found links to the queue
        await enqueueLinks();
    },
    // Function to handle errors
    failedRequestHandler({ request }) {
        console.log(`Request ${request.url} failed too many times`);
    },
    // Let's limit our crawls to make the example quick and safe
    maxRequestsPerCrawl: 20,
});

// Add first URL to the queue and start the crawl
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Using Crawlee with crawlee.json configuration
DESCRIPTION: This JavaScript example demonstrates how to use Crawlee with configuration options set in crawlee.json. It creates a CheerioCrawler and handles two requests with different delays.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, sleep } from 'crawlee';
// We are not importing nor passing
// the Configuration to the crawler.
// We are not assigning any env vars either.
const crawler = new CheerioCrawler();

crawler.router.addDefaultHandler(async ({ request }) => {
    // for the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // for the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Setting Apify API Token in Configuration
DESCRIPTION: JavaScript code to set the Apify API token using the Configuration instance.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const sdk = new Actor({ token: 'your_api_token' });

----------------------------------------

TITLE: Installing Crawlee Dependencies
DESCRIPTION: Package installation commands for different Crawlee crawler types using npm.

LANGUAGE: bash
CODE:
npm install crawlee
npm install crawlee playwright
npm install crawlee puppeteer

----------------------------------------

TITLE: Domain Strategy Configuration
DESCRIPTION: Configuration example for including subdomains in crawling using the same-domain strategy.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    strategy: 'same-domain'
});

----------------------------------------

TITLE: Configuring Response Type for JSON in Got Scraping with TypeScript
DESCRIPTION: Shows how to set the responseType option to 'json' when using Got Scraping with BasicCrawler to handle JSON responses.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ responseType: 'json' });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Setting Apify Token in Configuration
DESCRIPTION: Set your Apify API token using the Configuration instance for authentication.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const sdk = new Actor({ token: 'your_api_token' });

----------------------------------------

TITLE: JSON Response Handling
DESCRIPTION: Demonstrates how to configure sendRequest to handle JSON responses.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ responseType: 'json' });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Creating Request Queue with Locking Support in JavaScript
DESCRIPTION: Initializes a request queue with locking support for parallel scraping. The function ensures queue initialization and optionally purges existing data.

LANGUAGE: javascript
CODE:
import { getOrInitQueue } from './requestQueue.mjs';

----------------------------------------

TITLE: Creating and Running a Hello World Actor
DESCRIPTION: Bash commands to create and run a basic Apify actor project

LANGUAGE: bash
CODE:
apify create my-hello-world
cd my-hello-world
apify run

----------------------------------------

TITLE: Proxy Configuration Example
DESCRIPTION: Demonstrates how to configure a proxy server with BasicCrawler's sendRequest function.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            proxyUrl: 'http://auto:password@proxy.apify.com:8000',
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Extracting Manufacturer from URL in JavaScript
DESCRIPTION: Shows how to parse a manufacturer name from a product URL by splitting the URL string and extracting relevant segments.

LANGUAGE: javascript
CODE:
const urlPart = request.url.split('/').slice(-1);
const manufacturer = urlPart[0].split('-')[0];

----------------------------------------

TITLE: Setting up Express Server with PlaywrightCrawler for GCP Cloud Run in JavaScript
DESCRIPTION: This code sets up an Express server to handle HTTP requests for a PlaywrightCrawler in GCP Cloud Run. It initializes the crawler, runs it with start URLs, and returns the crawled data as the HTTP response.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import express from 'express';
const app = express();

const startUrls = ['https://crawlee.dev'];


app.get('/', async (req, res) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));
    
    await crawler.run(startUrls);    

    return res.send(await crawler.getData());
});

app.listen(parseInt(process.env.PORT) || 3000);

----------------------------------------

TITLE: Updating package.json for GCP Cloud Functions
DESCRIPTION: Modifies the package.json file to set the main entry point for the GCP Cloud Function.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "version": "1.0.0",
    "main": "src/main.js",
    ...
}

----------------------------------------

TITLE: Tiered Proxy Configuration
DESCRIPTION: Demonstrates setting up tiered proxy configuration with fallback options based on blocking behavior.

LANGUAGE: javascript
CODE:
const proxyConfiguration = new ProxyConfiguration({
    tieredProxyUrls: [
        [null], // At first, we try to connect without a proxy
        ['http://okay-proxy.com'],
        ['http://slightly-better-proxy.com', 'http://slightly-better-proxy-2.com'],
        ['http://very-good-and-expensive-proxy.com'],
    ]
});

----------------------------------------

TITLE: Importing BrowserCrawler Package in JavaScript
DESCRIPTION: Shows the package name for importing the BrowserCrawler functionality in a JavaScript project. This is the entry point for using the browser crawling capabilities of Crawlee.

LANGUAGE: markdown
CODE:
# `@crawlee/browser`

----------------------------------------

TITLE: Infinite Scroll Handler
DESCRIPTION: Function to handle Google Maps' infinite scrolling mechanism to load more results

LANGUAGE: python
CODE:
async def _load_more_items(self, page: Page) -> bool:
        """Scroll down to load more items."""
        try:
            feed = await page.query_selector('div[role="feed"]')
            if not feed:
                return False
            prev_scroll = await feed.evaluate("(element) => element.scrollTop")
            await feed.evaluate("(element) => element.scrollTop += 800")
            await page.wait_for_timeout(2000)

            new_scroll = await feed.evaluate("(element) => element.scrollTop")
            if new_scroll <= prev_scroll:
                return False
            await page.wait_for_timeout(1000)
            return True
        except Exception as e:
            context.log.exception("Error during scroll")
            return False

----------------------------------------

TITLE: Setting Concurrency Limits in Crawlee
DESCRIPTION: Shows how to configure minimum and maximum concurrent requests for a CheerioCrawler. These settings control how many parallel requests can run simultaneously.

LANGUAGE: javascript
CODE:
const crawler = new CheerioCrawler({
    // Start with minimum of 1 parallel request
    minConcurrency: 1,
    // Scale up to maximum of 100 parallel requests
    maxConcurrency: 100,
    // ... other options
});

----------------------------------------

TITLE: SendRequest API Implementation
DESCRIPTION: Details the implementation of the sendRequest function with all available configuration options including proxy support, cookies, and request parameters.

LANGUAGE: typescript
CODE:
async sendRequest(overrideOptions?: GotOptionsInit) => {
    return gotScraping({
        url: request.url,
        method: request.method,
        body: request.payload,
        headers: request.headers,
        proxyUrl: crawlingContext.proxyInfo?.url,
        sessionToken: session,
        responseType: 'text',
        ...overrideOptions,
        retry: {
            limit: 0,
            ...overrideOptions?.retry,
        },
        cookieJar: {
            getCookieString: (url: string) => session!.getCookieString(url),
            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),
            ...overrideOptions?.cookieJar,
        },
    });
}

----------------------------------------

TITLE: Creating a GCP Cloud Function handler for CheerioCrawler
DESCRIPTION: Wrap the crawler execution in an async function that handles the cloud function request and response. Export this function as a named export from the main module.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (req, res) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
    
    return res.send(await crawler.getData())
}

----------------------------------------

TITLE: SendRequest Implementation Details
DESCRIPTION: Shows the internal implementation of the sendRequest function including its configuration options and default parameters.

LANGUAGE: typescript
CODE:
async sendRequest(overrideOptions?: GotOptionsInit) => {
    return gotScraping({
        url: request.url,
        method: request.method,
        body: request.payload,
        headers: request.headers,
        proxyUrl: crawlingContext.proxyInfo?.url,
        sessionToken: session,
        responseType: 'text',
        ...overrideOptions,
        retry: {
            limit: 0,
            ...overrideOptions?.retry,
        },
        cookieJar: {
            getCookieString: (url: string) => session!.getCookieString(url),
            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),
            ...overrideOptions?.cookieJar,
        },
    });
}

----------------------------------------

TITLE: Sanity Check Crawler with Playwright and Cheerio
DESCRIPTION: Creates a Playwright crawler that uses Cheerio for HTML parsing. It visits the start URL, extracts the HTML, and then uses Cheerio to select and print the text content of all category cards.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';
import cheerio from 'cheerio';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page }) {
        const html = await page.content();
        const $ = cheerio.load(html);
        $('.collection-block-item').each((_, el) => {
            console.log($(el).text());
        });
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Creating React App with Vite
DESCRIPTION: Command to create a new React application using Vite as the build tool.

LANGUAGE: bash
CODE:
npx create-vite@latest

----------------------------------------

TITLE: Logging into Apify CLI
DESCRIPTION: Commands to install Apify CLI and log in with an API token.

LANGUAGE: bash
CODE:
npm install -g apify-cli
apify login -t YOUR_API_TOKEN

----------------------------------------

TITLE: Initializing Cheerio Crawler with Unique Configuration for AWS Lambda
DESCRIPTION: This snippet shows how to create a CheerioCrawler instance with a unique Configuration object, setting persistStorage to false for in-memory storage on Lambda's read-only filesystem.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Domain Strategy Configuration
DESCRIPTION: Configuration example for including subdomains in the crawl using enqueueLinks strategy.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    strategy: 'same-domain'
});

----------------------------------------

TITLE: sendRequest API Implementation in Crawlee
DESCRIPTION: Shows the implementation of the sendRequest function, including its default options and how it uses got-scraping under the hood.

LANGUAGE: typescript
CODE:
async sendRequest(overrideOptions?: GotOptionsInit) => {
    return gotScraping({
        url: request.url,
        method: request.method,
        body: request.payload,
        headers: request.headers,
        proxyUrl: crawlingContext.proxyInfo?.url,
        sessionToken: session,
        responseType: 'text',
        ...overrideOptions,
        retry: {
            limit: 0,
            ...overrideOptions?.retry,
        },
        cookieJar: {
            getCookieString: (url: string) => session!.getCookieString(url),
            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),
            ...overrideOptions?.cookieJar,
        },
    });
}

----------------------------------------

TITLE: Logging into Apify Platform using CLI
DESCRIPTION: Shows how to log into the Apify platform using the Apify CLI tool. This allows automatic credential injection when running scrapers.

LANGUAGE: bash
CODE:
npm install -g apify-cli
apify login -t YOUR_API_TOKEN

----------------------------------------

TITLE: Configuring INPUT.json File Path in Crawlee
DESCRIPTION: Demonstrates the file path structure for providing input to a Crawlee actor through the INPUT.json file in the default key-value store.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json

----------------------------------------

TITLE: Complete Package.json Configuration
DESCRIPTION: Full package.json configuration including all dependencies and scripts for development and production

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "type": "module",
    "main": "dist/main.js",
    "dependencies": {
        "crawlee": "3.0.0"
    },
    "devDependencies": {
        "@apify/tsconfig": "^0.1.0",
        "@types/node": "^18.14.0",
        "ts-node": "^10.8.0",
        "typescript": "^4.7.4"
    },
    "scripts": {
        "start": "npm run start:dev",
        "start:prod": "node dist/main.js",
        "start:dev": "ts-node-esm -T src/main.ts",
        "build": "tsc"
    }
}

----------------------------------------

TITLE: Scraping JavaScript-rendered websites with Scrapy and Playwright
DESCRIPTION: This code snippet demonstrates how to use Scrapy with the Playwright plugin to scrape JavaScript-rendered websites. It creates a spider that navigates to the Apify Store and extracts the title of the first actor.

LANGUAGE: python
CODE:
import scrapy

class ActorSpider(scrapy.Spider):
    name = 'actor_spider'
    start_urls = ['https://apify.com/store']

    def start_requests(self):
        for url in self.start_urls:
            yield scrapy.Request(
                url,
                meta={"playwright": True, "playwright_include_page": True},
                callback=self.parse_playwright
            )

    async def parse_playwright(self, response):
        page = response.meta['playwright_page']
        await page.wait_for_selector('.ActorStoreItem-title-wrapper')
        actor_card = await page.query_selector('.ActorStoreItem-title-wrapper')

        if actor_card:
            actor_text = await actor_card.text_content()
            yield {
                'actor': actor_text.strip() if actor_text else 'N/A'
            }

        await page.close()

----------------------------------------

TITLE: Passing Services Directly to ParselCrawler
DESCRIPTION: Demonstrates how to pass services directly to a ParselCrawler instance in Crawlee.

LANGUAGE: python
CODE:
import asyncio

from crawlee.configuration import Configuration
from crawlee.crawlers import ParselCrawler, ParselCrawlingContext
from crawlee.events import LocalEventManager
from crawlee.storage_clients import MemoryStorageClient


async def main() -> None:
    crawler = ParselCrawler(
        configuration=Configuration(),
        storage_client=MemoryStorageClient(),
        event_manager=LocalEventManager(),
    )

    # ...


if __name__ == '__main__':
    asyncio.run(main())

----------------------------------------

TITLE: Proxy Configuration Example
DESCRIPTION: Shows how to configure a proxy URL when using BasicCrawler with sendRequest.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            proxyUrl: 'http://auto:password@proxy.apify.com:8000',
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Saving Data with Dataset.pushData
DESCRIPTION: Example of saving extracted results to Crawlee's Dataset storage.

LANGUAGE: javascript
CODE:
await Dataset.pushData(results);

----------------------------------------

TITLE: Using actor-node-playwright-chrome Docker Image
DESCRIPTION: Demonstrates the use of a Docker image with Playwright and Chrome, suitable for CheerioCrawler and PlaywrightCrawler.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16

----------------------------------------

TITLE: Installing and Packaging Browser Dependencies for AWS Lambda
DESCRIPTION: Commands for installing the @sparticuz/chromium package and creating a zip archive of node_modules for AWS Lambda Layer deployment.

LANGUAGE: bash
CODE:
# Install the package
npm i -S @sparticuz/chromium

# Zip the dependencies
zip -r dependencies.zip ./node_modules

----------------------------------------

TITLE: Crawlee Browser Fingerprint Configuration
DESCRIPTION: Example of configuring browser fingerprints in Crawlee crawler.

LANGUAGE: typescript
CODE:
const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        useFingerprints: false,
    },
});

----------------------------------------

TITLE: Creating Input Configuration for Crawlee Actor
DESCRIPTION: Shows the file path structure for providing input to a Crawlee actor via INPUT.json file in the default key-value store directory

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json

----------------------------------------

TITLE: Setting Min/Max Concurrency in Crawlee
DESCRIPTION: Shows how to configure minimum and maximum concurrent requests in CheerioCrawler. These settings control the parallel request limits and scaling behavior.

LANGUAGE: javascript
CODE:
const crawler = new CheerioCrawler({
    minConcurrency: 1,
    maxConcurrency: 100,
    // ...
});

----------------------------------------

TITLE: Complete AWS Lambda Handler Implementation
DESCRIPTION: Full implementation of AWS Lambda handler function incorporating Crawlee crawler with AWS Chromium configuration and proper response handling.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
        launchContext: {
            launchOptions: {
                executablePath: await aws_chromium.executablePath(),
                args: aws_chromium.args,
                headless: true
            }
        }
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);

    return {
        statusCode: 200,
        body: await crawler.getData(),
    };
}

----------------------------------------

TITLE: Enabling Request Locking in CheerioCrawler
DESCRIPTION: Example showing how to enable request locking experiment in a CheerioCrawler instance. This configuration allows the crawler to use the new request locking API for parallel processing.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    experiments: {
        requestLocking: true,
    },
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Configuring SessionPool with CheerioCrawler in Crawlee
DESCRIPTION: This code snippet illustrates the configuration and usage of SessionPool with CheerioCrawler in Crawlee. It includes setup for proxy rotation and session management.

LANGUAGE: js
CODE:
import { CheerioCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new CheerioCrawler({
    proxyConfiguration,
    async requestHandler({ $, session }) {
        // Use 'session' here...
        const pageTitle = $('title').text();
        // Process the page content here...
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Installing Puppeteer Stealth Dependencies
DESCRIPTION: Command line instructions for installing the required packages puppeteer-extra and puppeteer-extra-plugin-stealth via npm

LANGUAGE: bash
CODE:
npm install puppeteer-extra puppeteer-extra-plugin-stealth

----------------------------------------

TITLE: Crawling Same Hostname Strategy Implementation
DESCRIPTION: Shows how to crawl links from the same hostname using CheerioCrawler. This is the default strategy that only processes URLs matching the original hostname.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks }) {
        // Only add links that share the same hostname as the starting URL
        await enqueueLinks({
            strategy: EnqueueStrategy.SameHostname,
        });
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Using Proxy with BasicCrawler in TypeScript
DESCRIPTION: This example demonstrates how to use a proxy server with BasicCrawler by passing the proxyUrl option to sendRequest. It shows how to set up a proxy for making requests through a specified server.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            proxyUrl: 'http://auto:password@proxy.apify.com:8000',
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Using CookieJar with sendRequest in Crawlee
DESCRIPTION: Demonstrates how to use a CookieJar from the tough-cookie package with sendRequest for managing cookies.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';
import { CookieJar } from 'tough-cookie';

const cookieJar = new CookieJar();

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ cookieJar });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Header Generator Configuration
DESCRIPTION: Demonstrates how to configure browser fingerprint generation for requests.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            headerGeneratorOptions: {
                devices: ['mobile', 'desktop'],
                locales: ['en-US'],
                operatingSystems: ['windows', 'macos', 'android', 'ios'],
                browsers: ['chrome', 'edge', 'firefox', 'safari'],
            },
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: JSON Response Handling
DESCRIPTION: Demonstrates how to configure the responseType to handle JSON responses instead of default text/HTML.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ responseType: 'json' });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Configuring Apify Proxy
DESCRIPTION: Example of setting up and using Apify Proxy with specific configurations

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
    countryCode: 'US',
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Configuring Input Path for Crawlee Actor
DESCRIPTION: Shows the file path structure for providing input to a Crawlee actor through the INPUT.json file in the default key-value store.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json

----------------------------------------

TITLE: Specifying Dataset Storage Path in Crawlee
DESCRIPTION: Shows the default storage path where dataset items are saved as individual files within the project structure.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/datasets/default/

----------------------------------------

TITLE: Processing API Search Results and Extracting Company Data
DESCRIPTION: Python code for handling Crunchbase API search results, extracting company data, and implementing pagination for search results.

LANGUAGE: python
CODE:
import json
from crawlee.crawlers import HttpCrawlingContext
from crawlee.router import Router
from crawlee import Request, HttpHeaders

router = Router[HttpCrawlingContext]()

@router.default_handler
async def default_handler(context: HttpCrawlingContext) -> None:
    data = json.loads(context.http_response.read())
    requests = [
        Request.from_url(
            url=f'https://api.crunchbase.com/api/v4/entities/organizations/{entity["identifier"]["permalink"]}?field_ids=short_description%2Clocation_identifiers%2Cwebsite_url',
            label='company',
        )
        for entity in data['entities']
    ]
    await context.add_requests(requests)

@router.handler('company')
async def company_handler(context: HttpCrawlingContext) -> None:
    data = json.loads(context.http_response.read())
    await context.push_data(
        {
            'Company Name': data['properties']['identifier']['value'],
            'Short Description': data['properties']['short_description'],
            'Website': data['properties'].get('website_url'),
            'Location': '; '.join([item['value'] for item in data['properties'].get('location_identifiers', [])]),
        }
    )

@router.handler('search')
async def search_handler(context: HttpCrawlingContext) -> None:
    data = json.loads(context.http_response.read())
    last_entity = None
    results = []
    for entity in data['entities']:
        last_entity = entity['uuid']
        results.append(
            {
                'Company Name': entity['properties']['identifier']['value'],
                'Short Description': entity['properties']['short_description'],
                'Website': entity['properties'].get('website_url'),
                'Location': '; '.join([item['value'] for item in entity['properties'].get('location_identifiers', [])]),
            }
        )
    if results:
        await context.push_data(results)
    if last_entity:
        payload = json.loads(context.request.payload)
        payload['after_id'] = last_entity
        payload = json.dumps(payload)
        await context.add_requests(
            [
                Request.from_url(
                    url='https://api.crunchbase.com/api/v4/searches/organizations',
                    method='POST',
                    payload=payload,
                    use_extended_unique_key=True,
                    headers=HttpHeaders({'Content-Type': 'application/json'}),
                    label='search',
                )
            ]
        )

----------------------------------------

TITLE: Using Proxy with BasicCrawler in TypeScript
DESCRIPTION: This example demonstrates how to use a proxy server with BasicCrawler by passing the proxyUrl option to sendRequest. It shows how to set up a proxy for making requests through a specified server.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            proxyUrl: 'http://auto:password@proxy.apify.com:8000',
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Importing Dataset Module in Crawlee (TypeScript)
DESCRIPTION: This snippet shows how to import the Dataset module along with PlaywrightCrawler from Crawlee. The Dataset module is used for storing scraped data.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

----------------------------------------

TITLE: Using CookieJar with sendRequest in Crawlee
DESCRIPTION: Demonstrates how to use a CookieJar with sendRequest in Crawlee's BasicCrawler. This example shows how to create and pass a CookieJar instance to manage cookies across requests.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';
import { CookieJar } from 'tough-cookie';

const cookieJar = new CookieJar();

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ cookieJar });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Configuring Crawlee with Custom Storage
DESCRIPTION: Initial code setup showing how to configure Crawlee with a new Configuration instance to ensure isolated storage per crawler instance.

LANGUAGE: javascript
CODE:
// For more information, see https://crawlee.dev/
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Setting up Express Server with PlaywrightCrawler for GCP Cloud Run in JavaScript
DESCRIPTION: This code demonstrates how to wrap a PlaywrightCrawler with an Express HTTP server for deployment on GCP Cloud Run. It sets up a GET route that runs the crawler and returns the scraped data. The server listens on the port specified by GCP's PORT environment variable.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import express from 'express';
const app = express();

const startUrls = ['https://crawlee.dev'];

app.get('/', async (req, res) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));
    
    await crawler.run(startUrls);    

    return res.send(await crawler.getData());
});

app.listen(parseInt(process.env.PORT) || 3000);

----------------------------------------

TITLE: Creating a CheerioCrawler with RequestQueue in Crawlee
DESCRIPTION: This code creates a CheerioCrawler instance, sets up a RequestQueue, and defines a requestHandler to extract the page title. It demonstrates the basic structure of a Crawlee crawler.

LANGUAGE: typescript
CODE:
import { RequestQueue, CheerioCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

const crawler = new CheerioCrawler({
    requestQueue,
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run();

----------------------------------------

TITLE: TypeScript Configuration for Crawlee
DESCRIPTION: TypeScript configuration setup for Crawlee projects using @apify/tsconfig.

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist",
        "lib": ["DOM"]
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Reduce Method Result Example
DESCRIPTION: Example output from the Dataset.reduce() method showing the total count of headers across all pages.

LANGUAGE: javascript
CODE:
23

----------------------------------------

TITLE: Failing to Scrape Dynamic Content with PlaywrightCrawler (No Wait)
DESCRIPTION: This snippet demonstrates how attempting to scrape JavaScript-rendered content without waiting for elements to appear leads to errors.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request }) {
        // This will fail because the element might not be in the DOM yet
        const actorText = await page.textContent('.ActorStoreItem');
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Creating a CheerioCrawler with RequestQueue in Crawlee
DESCRIPTION: This code creates a CheerioCrawler instance, sets up a RequestQueue, and defines a requestHandler to extract the page title. It demonstrates the basic structure of a Crawlee crawler.

LANGUAGE: typescript
CODE:
import { RequestQueue, CheerioCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

const crawler = new CheerioCrawler({
    requestQueue,
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run();

----------------------------------------

TITLE: Crawling Same Domain Links with CheerioCrawler in Crawlee
DESCRIPTION: This snippet illustrates how to use CheerioCrawler to crawl links within the same domain, including subdomains. It uses the 'same-domain' enqueue strategy.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks, log }) {
        log.info('Crawling');
        await enqueueLinks({
            strategy: EnqueueStrategy.SameDomain,
        });
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Crawling Multiple URLs with Playwright Crawler in Crawlee
DESCRIPTION: This snippet illustrates the use of Playwright Crawler for crawling multiple URLs. It demonstrates crawler configuration, request handling, and starting the crawler. It requires the 'apify/actor-node-playwright-chrome' image for the Dockerfile when running on the Apify Platform.

LANGUAGE: javascript
CODE:
{PlaywrightSource}

----------------------------------------

TITLE: Migration Timeout Handler Implementation
DESCRIPTION: Implements timeout handling for pending responses during server migrations to ensure clean termination of requests.

LANGUAGE: typescript
CODE:
export const addTimeoutToAllResponses = (timeoutInSeconds: number = 60) => {
    const migrationErrorMessage = {
        errorMessage: 'Actor had to migrate to another server. Please, retry your request.',
    };

    const responseKeys = Object.keys(responses);

    for (const key of responseKeys) {
        setTimeout(() => {
            sendErrorResponseById(key, JSON.stringify(migrationErrorMessage));
        }, timeoutInSeconds * 1000);
    }
};

----------------------------------------

TITLE: Installing Puppeteer-Extra Dependencies
DESCRIPTION: Command to install puppeteer-extra and its stealth plugin using npm package manager

LANGUAGE: bash
CODE:
npm install puppeteer-extra puppeteer-extra-plugin-stealth

----------------------------------------

TITLE: Running Headful Browser with PlaywrightCrawler
DESCRIPTION: Example of running a headful (visible) browser using PlaywrightCrawler. It shows how to configure the crawler to run in headful mode and includes a delay for demonstration purposes.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    headless: false,
    requestHandler: async ({ page, request }) => {
        await page.waitForTimeout(5000);
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Installing Crawlee with CLI
DESCRIPTION: Commands to install Crawlee using the Crawlee CLI, which sets up a new project with necessary dependencies and boilerplate code.

LANGUAGE: bash
CODE:
npx crawlee create my-crawler

LANGUAGE: bash
CODE:
cd my-crawler && npm start

----------------------------------------

TITLE: Combining Request Queue and Request List in Crawlee
DESCRIPTION: Shows how to use both request queue and request list together in a Crawlee crawler, demonstrating their combined functionality.

LANGUAGE: javascript
CODE:
import { RequestList, RequestQueue, PuppeteerCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
const requestList = await RequestList.open(null, [
    { url: 'http://example.com/1' },
    { url: 'http://example.com/2' },
    { url: 'http://example.com/3' },
    { url: 'http://example.com/4' },
    { url: 'http://example.com/5' },
]);

const crawler = new PuppeteerCrawler({
    requestList,
    requestQueue,
    async requestHandler({ request, enqueueLinks }) {
        console.log(request.url);
        await enqueueLinks();
    },
});

await crawler.run();

----------------------------------------

TITLE: Using Proxy with BasicCrawler
DESCRIPTION: Example of configuring a proxy server with BasicCrawler's sendRequest function.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({
            proxyUrl: 'http://auto:password@proxy.apify.com:8000',
        });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Capturing Multiple Page Screenshots with PuppeteerCrawler in JavaScript
DESCRIPTION: This snippet demonstrates how to capture screenshots of multiple web pages using PuppeteerCrawler with the page.screenshot() method. It creates a PuppeteerCrawler instance, defines a handler function that captures and saves screenshots for each visited page.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, KeyValueStore } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);

        const screenshot = await page.screenshot();
        const key = `screenshot-${Math.random()}`;
        await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });
    },
    maxRequestsPerCrawl: 10,
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Configuring Apify Proxy
DESCRIPTION: JavaScript code to create and use Apify Proxy configuration for IP rotation.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration();
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Installing Puppeteer-Extra Dependencies
DESCRIPTION: Command to install puppeteer-extra and its stealth plugin using npm package manager

LANGUAGE: bash
CODE:
npm install puppeteer-extra puppeteer-extra-plugin-stealth

----------------------------------------

TITLE: Package.json Build Configuration
DESCRIPTION: Basic package.json configuration for TypeScript build process and main entry point

LANGUAGE: json
CODE:
{
    "scripts": {
        "build": "tsc"
    },
    "main": "dist/main.js"
}

----------------------------------------

TITLE: Crawling Links with Puppeteer Crawler
DESCRIPTION: Implementation of a web crawler using Puppeteer Crawler for JavaScript-rendered pages. Uses the apify/actor-node-puppeteer-chrome image and demonstrates automated link discovery and processing.

LANGUAGE: typescript
CODE:
{PuppeteerSource}

----------------------------------------

TITLE: Creating AWS Lambda Handler Function for Cheerio Crawler
DESCRIPTION: This code wraps the Cheerio Crawler logic in an async handler function, which is the entry point for AWS Lambda execution. It instantiates a new crawler for each Lambda invocation to maintain statelessness.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
};

----------------------------------------

TITLE: Implementing Request Transformation with enqueueLinks in Crawlee
DESCRIPTION: Demonstrates how to use the transformRequestFunction to modify or filter requests before they are enqueued.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
    transformRequestFunction(req) {
        // ignore all links ending with `.pdf`
        if (req.url.endsWith('.pdf')) return false;
        return req;
    },
});

----------------------------------------

TITLE: Cookie Jar Implementation
DESCRIPTION: Shows how to use a custom cookie jar for managing cookies in requests.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';
import { CookieJar } from 'tough-cookie';

const cookieJar = new CookieJar();

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ cookieJar });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Configuring Package.json for GCP Deployment
DESCRIPTION: Package.json configuration setting the main entry point for the GCP Cloud Function deployment.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "version": "1.0.0",
    "main": "src/main.js",
    ...
}

----------------------------------------

TITLE: Enabling Request Locking in CheerioCrawler
DESCRIPTION: This snippet demonstrates how to enable the request locking experiment in a CheerioCrawler instance. It sets up a crawler with the experiment enabled and a basic request handler.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    experiments: {
        requestLocking: true,
    },
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Initializing CheerioCrawler with Custom Configuration
DESCRIPTION: Create a CheerioCrawler instance with persistStorage disabled for GCP compatibility.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Configuring PlaywrightCrawler with AWS-Compatible Chrome Options
DESCRIPTION: JavaScript code to set up PlaywrightCrawler with Chromium path and arguments optimized for AWS Lambda environment.

LANGUAGE: javascript
CODE:
// For more information, see https://crawlee.dev/
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
    launchContext: {
        launchOptions: {
             executablePath: await aws_chromium.executablePath(),
             args: aws_chromium.args,
             headless: true
        }
    }
}, new Configuration({
    persistStorage: false,
}));

----------------------------------------

TITLE: Using enqueueLinks with Advanced Filtering in Crawlee
DESCRIPTION: Shows how to use enqueueLinks with advanced filtering options like globs, regexps, and pseudoUrls.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
});

----------------------------------------

TITLE: Creating AWS Lambda Handler Function for Cheerio Crawler
DESCRIPTION: This code wraps the Cheerio Crawler logic in an async handler function, which is the entry point for AWS Lambda execution. It instantiates a new crawler for each Lambda invocation to maintain statelessness.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
};

----------------------------------------

TITLE: Configuring Chromium for AWS Lambda
DESCRIPTION: Setup of Chromium browser with AWS-specific configurations and parameters for Lambda environment

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
    launchContext: {
        launchOptions: {
             executablePath: await aws_chromium.executablePath(),
             args: aws_chromium.args,
             headless: true
        }
    }
}, new Configuration({
    persistStorage: false,
}));

----------------------------------------

TITLE: Crawling Same Hostname Links with CheerioCrawler in Crawlee
DESCRIPTION: This snippet shows how to use CheerioCrawler to crawl links with the same hostname as the starting URL. It uses the 'same-hostname' enqueue strategy, which is the default.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks, log }) {
        log.info('Crawling');
        await enqueueLinks();
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Configuring Package.json for TypeScript Build in Crawlee Project
DESCRIPTION: JSON configuration for package.json to set up TypeScript build script and main entry point for a Crawlee project.

LANGUAGE: json
CODE:
{
    "scripts": {
        "build": "tsc"
    },
    "main": "dist/main.js"
}

----------------------------------------

TITLE: Sample Crawlee output log
DESCRIPTION: Example of the console output when running a Crawlee scraper, showing the progress of the crawling process.

LANGUAGE: log
CODE:
INFO  CheerioCrawler: Starting the crawl
INFO  CheerioCrawler: Fetching origin URL: https://crawlee.dev
INFO  CheerioCrawler: Requesting: https://crawlee.dev
INFO  CheerioCrawler: Analysing: https://crawlee.dev
INFO  CheerioCrawler: Finished: https://crawlee.dev
INFO  CheerioCrawler: Requesting: https://crawlee.dev/docs/quick-start
INFO  CheerioCrawler: Analysing: https://crawlee.dev/docs/quick-start
INFO  CheerioCrawler: Finished: https://crawlee.dev/docs/quick-start
INFO  CheerioCrawler: Requesting: https://crawlee.dev/docs/introduction
INFO  CheerioCrawler: Analysing: https://crawlee.dev/docs/introduction
INFO  CheerioCrawler: Finished: https://crawlee.dev/docs/introduction
INFO  CheerioCrawler: Crawl finished

----------------------------------------

TITLE: Complete Crawlee Scraping Example with Data Saving (JavaScript)
DESCRIPTION: A full example of a Crawlee scraper that extracts data from a website and saves it using Dataset.pushData(). This example uses PlaywrightCrawler to navigate web pages and extract information.

LANGUAGE: javascript
CODE:
{Example}

----------------------------------------

TITLE: Interacting with React Calculator using JSDOMCrawler
DESCRIPTION: Demonstrates how to use JSDOMCrawler to interact with a React calculator app. The script navigates to the calculator, performs button clicks for '1 + 1 =' operation, and extracts the result using jsdom.

LANGUAGE: typescript
CODE:
${JSDOMCrawlerRunScriptSource}

----------------------------------------

TITLE: Reduce Method Result Example
DESCRIPTION: Example output showing the final reduced value representing the total count of headers across all pages.

LANGUAGE: javascript
CODE:
23

----------------------------------------

TITLE: Capturing Screenshot with Puppeteer Using page.screenshot()
DESCRIPTION: This snippet demonstrates how to capture a screenshot of a web page using Puppeteer's page.screenshot() method. It launches a browser, creates a new page, navigates to a URL, and saves the screenshot to a key-value store.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';
import { launchPuppeteer } from 'crawlee/puppeteer';

const browser = await launchPuppeteer();
const page = await browser.newPage();
await page.goto('https://crawlee.dev');

const screenshot = await page.screenshot();
const key = 'my-screenshot';

await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });

await browser.close();

----------------------------------------

TITLE: Browser Pool Firefox Enhancement
DESCRIPTION: Feature enhancement enabling tab-as-a-container support for Firefox browsers

LANGUAGE: markdown
CODE:
### Features

* enable tab-as-a-container for Firefox ([#1456](https://github.com/apify/crawlee/issues/1456))

----------------------------------------

TITLE: Installing Playwright Extra Dependencies
DESCRIPTION: Command line instructions for installing the required packages playwright-extra and puppeteer-extra-plugin-stealth via npm.

LANGUAGE: bash
CODE:
npm install playwright-extra puppeteer-extra-plugin-stealth

----------------------------------------

TITLE: Installing Crawlee CLI and Project Setup
DESCRIPTION: Commands to create a new Crawlee project using the CLI tool and start the crawler

LANGUAGE: bash
CODE:
npx crawlee create my-crawler
cd my-crawler && npm start

----------------------------------------

TITLE: Sample Dataset Structure in JSON
DESCRIPTION: Example JSON structure showing scraped data stored in the default dataset, containing URLs and their corresponding header counts.

LANGUAGE: json
CODE:
[
    {
        "url": "https://crawlee.dev/",
        "headingCount": 11
    },
    {
        "url": "https://crawlee.dev/storage",
        "headingCount": 8
    },
    {
        "url": "https://crawlee.dev/proxy",
        "headingCount": 4
    }
]

----------------------------------------

TITLE: Creating a Request Queue with Locking Support in JavaScript
DESCRIPTION: Defines a function to initialize or retrieve a request queue with locking support for parallel scraping. It uses the Crawlee RequestQueue and enables the experimental request locking feature.

LANGUAGE: javascript
CODE:
import { RequestQueue } from 'crawlee';
import { Configuration } from 'crawlee';

// Enable the request locking experiment
Configuration.set('experiments.requestLocking', true);

let requestQueue;

export async function getOrInitQueue(purge = false) {
    if (!requestQueue) {
        requestQueue = await RequestQueue.open();
        if (purge) {
            await requestQueue.drop();
            requestQueue = await RequestQueue.open();
        }
    }
    return requestQueue;
}

----------------------------------------

TITLE: Installing Crawlee Dependencies
DESCRIPTION: NPM commands for installing Crawlee and its optional browser automation dependencies

LANGUAGE: bash
CODE:
npm install crawlee

LANGUAGE: bash
CODE:
npm install crawlee playwright

LANGUAGE: bash
CODE:
npm install crawlee puppeteer

----------------------------------------

TITLE: Package.json Build Configuration
DESCRIPTION: Basic package.json configuration for TypeScript build script and main entry point.

LANGUAGE: json
CODE:
{
    "scripts": {
        "build": "tsc"
    },
    "main": "dist/main.js"
}

----------------------------------------

TITLE: Installing Crawlee Dependencies
DESCRIPTION: NPM commands for installing Crawlee and its optional browser automation dependencies

LANGUAGE: bash
CODE:
npm install crawlee

LANGUAGE: bash
CODE:
npm install crawlee playwright

LANGUAGE: bash
CODE:
npm install crawlee puppeteer

----------------------------------------

TITLE: URL Pattern Filtering
DESCRIPTION: Example of using globs to filter URLs during the crawling process.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
});

----------------------------------------

TITLE: Installing Puppeteer-Extra Dependencies
DESCRIPTION: Commands for installing puppeteer-extra and its stealth plugin using npm package manager.

LANGUAGE: bash
CODE:
npm install puppeteer-extra puppeteer-extra-plugin-stealth

----------------------------------------

TITLE: Configuring Apify Proxy with Specific Groups and Country
DESCRIPTION: Example of creating an Apify Proxy configuration with specific proxy groups and country selection.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
    countryCode: 'US',
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Simplified CheerioCrawler Setup with Inline URL Addition in Crawlee
DESCRIPTION: Demonstrates a more concise way to create a CheerioCrawler by directly adding URLs to crawl using the run() method, eliminating the need for explicit RequestQueue initialization.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Configuring Request Rate Limits in Crawlee
DESCRIPTION: Shows how to set the maximum number of requests per minute to control crawler throughput and prevent overwhelming target websites.

LANGUAGE: javascript
CODE:
const crawler = new CheerioCrawler({
    // Only allow 120 requests per minute
    maxRequestsPerMinute: 120,
    // ...
});

----------------------------------------

TITLE: Scraping Static Content with CheerioCrawler in TypeScript
DESCRIPTION: This snippet demonstrates an attempt to scrape JavaScript-rendered content using CheerioCrawler, which fails because it can't execute client-side JavaScript.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        // Extract text content of an actor card
        const actorText = $('.ActorStoreItem').text();
        console.log(`ACTOR: ${actorText}`);
    }
})

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Installing Crawlee CLI and Project Setup
DESCRIPTION: Commands to create a new Crawlee project using the CLI tool and start the crawler

LANGUAGE: bash
CODE:
npx crawlee create my-crawler
cd my-crawler && npm start

----------------------------------------

TITLE: Building a Basic CheerioCrawler
DESCRIPTION: Creates a complete CheerioCrawler implementation that processes a queue of URLs and extracts the title from each page. Shows the integration of RequestQueue with the crawler and basic HTML parsing.

LANGUAGE: typescript
CODE:
// Add import of CheerioCrawler
import { RequestQueue, CheerioCrawler } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

// Create the crawler and add the queue with our URL
// and a request handler to process the page.
const crawler = new CheerioCrawler({
    requestQueue,
    // The `$` argument is the Cheerio object
    // which contains parsed HTML of the website.
    async requestHandler({ $, request }) {
        // Extract <title> text with Cheerio.
        // See Cheerio documentation for API docs.
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

// Start the crawler and wait for it to finish
await crawler.run();

----------------------------------------

TITLE: Configuring Crawlee using Custom Configuration
DESCRIPTION: Example of creating a custom Configuration instance and passing it to a CheerioCrawler, setting the persistStateIntervalMillis option.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Create new configuration
const config = new Configuration({
    // Set the 'persistStateIntervalMillis' option to 10 seconds
    persistStateIntervalMillis: 10_000,
});

// Now we need to pass the configuration to the crawler
const crawler = new CheerioCrawler({}, config);

crawler.router.addDefaultHandler(async ({ request }) => {
    // for the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // for the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: DevTools Element Selection Query
DESCRIPTION: JavaScript query to select collection elements using DevTools console

LANGUAGE: javascript
CODE:
document.querySelectorAll('.collection-block-item');

----------------------------------------

TITLE: Crawling Context Migration Example
DESCRIPTION: Demonstrates the transition from separate handler arguments to unified Crawling Context in SDK v1.

LANGUAGE: javascript
CODE:
const handlePageFunction = async (crawlingContext1) => {
    crawlingContext1.hasOwnProperty('proxyInfo') // true
}

const handleFailedRequestFunction = async (crawlingContext2) => {
    crawlingContext2.hasOwnProperty('proxyInfo') // true
}

// All contexts are the same object.
crawlingContext1 === crawlingContext2 // true

----------------------------------------

TITLE: DevTools Element Selection Query
DESCRIPTION: JavaScript query to select collection elements using DevTools console

LANGUAGE: javascript
CODE:
document.querySelectorAll('.collection-block-item');

----------------------------------------

TITLE: Initializing RequestQueue and Adding URLs in Crawlee
DESCRIPTION: Demonstrates how to create a RequestQueue instance and add a URL to it for crawling. This is the basic setup for managing the list of pages to be crawled.

LANGUAGE: typescript
CODE:
import { RequestQueue } from 'crawlee';

// First you create the request queue instance.
const requestQueue = await RequestQueue.open();
// And then you add one or more requests to it.
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

----------------------------------------

TITLE: Crawling Context Usage in Handler Functions
DESCRIPTION: Example showing how crawling context is shared between handler functions in SDK v1, replacing the previous separate arguments approach.

LANGUAGE: javascript
CODE:
const handlePageFunction = async (crawlingContext1) => {
    crawlingContext1.hasOwnProperty('proxyInfo') // true
}

const handleFailedRequestFunction = async (crawlingContext2) => {
    crawlingContext2.hasOwnProperty('proxyInfo') // true
}

// All contexts are the same object.
crawlingContext1 === crawlingContext2 // true

----------------------------------------

TITLE: PuppeteerCrawler Screenshot Capture
DESCRIPTION: Example showing screenshot capture within a PuppeteerCrawler instance, using page.screenshot() for multiple pages. Screenshots are saved with URL-based keys.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // Get a reasonable key for the screenshot
        const key = (new URL(request.url)).hostname;

        // Save the screenshot to the default key-value store
        await KeyValueStore.setValue(
            `${key}.jpg`,
            await page.screenshot(),
            { contentType: 'image/jpeg' },
        );
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Capturing Screenshots with Direct Puppeteer Usage
DESCRIPTION: Shows how to capture a screenshot of a web page using Puppeteer's direct page.screenshot() method. The screenshot is saved to a key-value store with a key derived from the URL.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';
import { launch } from 'puppeteer';

const browser = await launch();
const page = await browser.newPage();
await page.goto('https://crawlee.dev');

// Create a unique key for the screenshot based on URL
const key = `screenshot-${new Date().toISOString()}`;

// Capture the screenshot
const screenshot = await page.screenshot();

// Save the screenshot to the default key-value store
await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });

await browser.close();

----------------------------------------

TITLE: Using Key-Value Store Public URLs
DESCRIPTION: Example of storing data and getting a public URL for stored items

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'apify';\n\nconst store = await KeyValueStore.open();\nawait store.setValue('your-file', { foo: 'bar' });\nconst url = store.getPublicUrl('your-file');

----------------------------------------

TITLE: Extracting and Parsing Product Price with Playwright in JavaScript
DESCRIPTION: This code snippet shows how to extract the product price, filter the correct element, and parse it into a number using Playwright.

LANGUAGE: javascript
CODE:
const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

----------------------------------------

TITLE: Configuring package.json for Development Script
DESCRIPTION: JSON configuration in package.json to add a development script using ts-node for running TypeScript code directly.

LANGUAGE: json
CODE:
{
    "scripts": {
        "start:dev": "ts-node-esm -T src/main.ts"
    }
}

----------------------------------------

TITLE: Scraping with CheerioCrawler (Unsuccessful for JS-rendered content)
DESCRIPTION: This snippet demonstrates using CheerioCrawler to attempt scraping JavaScript-rendered content, which fails because Cheerio cannot execute client-side JavaScript.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        // Extract text content of an actor card
        const actorText = $('.ActorStoreItem').text();
        console.log(`ACTOR: ${actorText}`);
    }
})

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Downloading Files with Crawlee in JavaScript
DESCRIPTION: This script demonstrates how to use the FileDownload crawler class from Crawlee to download files and save them to the default key-value store. It handles various file types including images and PDFs. The downloaded files are stored locally in the './storage/key_value_stores/default' directory when using local configuration.

LANGUAGE: javascript
CODE:
import { FileDownload } from 'crawlee';

const crawler = new FileDownload();

await crawler.run([
    'https://api.qrserver.com/v1/create-qr-code/?size=150x150&data=Example',
    'https://cdn.edgeai.net/public-demos/images/bounding-box/produce.jpg',
    'https://crawlee.dev/img/crawlee-illustration.png',
    'https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Mongodb.png/320px-Mongodb.png',
    'https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4',
    'https://www.learningcontainer.com/wp-content/uploads/2020/05/sample-zip-file.zip',
    'https://file-examples.com/storage/fe5461ad6642896d7d7c50d/2017/10/file-sample_150kB.pdf',
]);

console.log('Crawler finished.');

----------------------------------------

TITLE: Installing TypeScript Compiler
DESCRIPTION: Command to install TypeScript compiler as a development dependency in a Node.js project.

LANGUAGE: shell
CODE:
npm install --dev typescript

----------------------------------------

TITLE: Installing Crawlee Dependencies
DESCRIPTION: Package installation commands for different crawler types including optional dependencies

LANGUAGE: bash
CODE:
npm install crawlee
npm install crawlee playwright
npm install crawlee puppeteer

----------------------------------------

TITLE: Basic Link Enqueuing with Crawlee
DESCRIPTION: Simple example of using enqueueLinks() function without parameters to crawl links.

LANGUAGE: javascript
CODE:
await enqueueLinks();

----------------------------------------

TITLE: Updating Bluesky Scraper for Apify Actor
DESCRIPTION: Modifies the BlueskyApiScraper class to work as an Apify Actor, including handling input parameters and adapting data collection logic based on the specified mode.

LANGUAGE: python
CODE:
import asyncio
import json
import traceback
from dataclasses import dataclass

import httpx
from apify import Actor
from yarl import URL

from crawlee import ConcurrencySettings, Request
from crawlee.crawlers import HttpCrawler, HttpCrawlingContext
from crawlee.http_clients import HttpxHttpClient


@dataclass
class ActorInput:
    """Actor input schema."""
    identifier: str
    app_password: str
    queries: list[str]
    mode: str
    max_requests_per_crawl: Optional[int] = None


async def run() -> None:
    """Main execution function that orchestrates the crawling process.

    Creates a scraper instance, manages the session, and handles the complete
    crawling lifecycle including proper cleanup on completion or error.
    """
    async with Actor:
        raw_input = await Actor.get_input()
        actor_input = ActorInput(
            identifier=raw_input.get('indentifier', ''),
            app_password=raw_input.get('appPassword', ''),
            queries=raw_input.get('queries', []),
            mode=raw_input.get('mode', 'posts'),
            max_requests_per_crawl=raw_input.get('maxRequestsPerCrawl')
        )
        scraper = BlueskyApiScraper(actor_input.mode, actor_input.max_requests_per_crawl)
        try:
            scraper.create_session(actor_input.identifier, actor_input.app_password)

            await scraper.init_crawler()
            await scraper.crawl(actor_input.queries)
        except httpx.HTTPError as e:
            Actor.log.error(f'HTTP error occurred: {e}')
            raise
        except Exception as e:
            Actor.log.error(f'Unexpected error: {e}')
            traceback.print_exc()
        finally:
            scraper.delete_session()

def main() -> None:
    """Entry point for the scraper application."""
    asyncio.run(run())


class BlueskyApiScraper:
    """A scraper class for extracting data from Bluesky social network using their official API.

    This scraper manages authentication, concurrent requests, and data collection for both
    posts and user profiles. It uses separate datasets for storing post and user information.
    """

    def __init__(self, mode: str, max_request: int | None) -> None:
        self._crawler: HttpCrawler | None = None

        self.mode = mode
        self.max_request = max_request

        # Variables for storing session data
        self._service_endpoint: str | None = None
        self._user_did: str | None = None
        self._access_token: str | None = None
        self._refresh_token: str | None = None
        self._handle: str | None = None

    def create_session(self, identifier: str, password: str) -> None:
        """Create credentials for the session."""
        url = 'https://bsky.social/xrpc/com.atproto.server.createSession'
        headers = {
            'Content-Type': 'application/json',
        }
        data = {'identifier': identifier, 'password': password}

        response = httpx.post(url, headers=headers, json=data)
        response.raise_for_status()

        data = response.json()

        self._service_endpoint = data['didDoc']['service'][0]['serviceEndpoint']
        self._user_did = data['didDoc']['id']
        self._access_token = data['accessJwt']
        self._refresh_token = data['refreshJwt']
        self._handle = data['handle']

    async def _search_handler(self, context: HttpCrawlingContext) -> None:
        """Handle search requests based on mode."""
        context.log.info(f'Processing search {context.request.url} ...')

        data = json.loads(context.http_response.read())

        if 'posts' not in data:
            context.log.warning(f'No posts found in response: {context.request.url}')
            return

        user_requests = {}
        posts = []

        profile_url = URL(f'{self._service_endpoint}/xrpc/app.bsky.actor.getProfile')

        for post in data['posts']:
            if self.mode == 'users' and post['author']['did'] not in user_requests:
                user_requests[post['author']['did']] = Request.from_url(
                    url=str(profile_url.with_query(actor=post['author']['did'])),
                    user_data={'label': 'user'},
                )
            elif self.mode == 'posts':
                posts.append(
                    {
                        'uri': post['uri'],
                        'cid': post['cid'],
                        'author_did': post['author']['did'],
                        'created': post['record']['createdAt'],
                        'indexed': post['indexedAt'],
                        'reply_count': post['replyCount'],
                        'repost_count': post['repostCount'],
                        'like_count': post['likeCount'],
                        'quote_count': post['quoteCount'],
                        'text': post['record']['text'],
                        'langs': '; '.join(post['record'].get('langs', [])),
                        'reply_parent': post['record'].get('reply', {}).get('parent', {}).get('uri'),
                        'reply_root': post['record'].get('reply', {}).get('root', {}).get('uri'),
                    }
                )

        if self.mode == 'posts':
            await context.push_data(posts)
        else:
            await context.add_requests(list(user_requests.values()))

        if cursor := data.get('cursor'):
            next_url = URL(context.request.url).update_query({'cursor': cursor})
            await context.add_requests([str(next_url)])

    async def _user_handler(self, context: HttpCrawlingContext) -> None:
        """Handle user profile requests."""
        context.log.info(f'Processing user {context.request.url} ...')

        data = json.loads(context.http_response.read())

        user_item = {
            'did': data['did'],
            'created': data['createdAt'],
            'avatar': data.get('avatar'),
            'description': data.get('description'),
            'display_name': data.get('displayName'),
            'handle': data['handle'],
            'indexed': data.get('indexedAt'),
            'posts_count': data['postsCount'],
            'followers_count': data['followersCount'],
            'follows_count': data['followsCount'],
        }

        await context.push_data(user_item)

----------------------------------------

TITLE: Legacy Handler Arguments Example
DESCRIPTION: Demonstrates how handler arguments worked in previous versions with separate objects for different handlers.

LANGUAGE: javascript
CODE:
const handlePageFunction = async (args1) => {
    args1.hasOwnProperty('proxyInfo') // true
}

const handleFailedRequestFunction = async (args2) => {
    args2.hasOwnProperty('proxyInfo') // false
}

args1 === args2 // false

----------------------------------------

TITLE: Configuring tsconfig.json for Crawlee TypeScript Project
DESCRIPTION: TypeScript configuration file (tsconfig.json) setup for a Crawlee project, extending @apify/tsconfig and setting compiler options.

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist"
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Configuring maxRequestsPerMinute in Crawlee CheerioCrawler
DESCRIPTION: Sets the maximum number of requests per minute for a CheerioCrawler to control the rate of requests to the target website.

LANGUAGE: javascript
CODE:
const crawler = new CheerioCrawler({
    // other options
    maxRequestsPerMinute: 120,
});

----------------------------------------

TITLE: Crawling Multiple URLs with Puppeteer
DESCRIPTION: Example demonstrating URL crawling using Puppeteer Crawler in Crawlee. Requires apify/actor-node-puppeteer-chrome Docker image when running on Apify Platform. Puppeteer enables full browser automation.

LANGUAGE: javascript
CODE:
{PuppeteerSource}

----------------------------------------

TITLE: Configuring SessionPool with CheerioCrawler in Crawlee
DESCRIPTION: This example demonstrates the setup and usage of SessionPool with CheerioCrawler in Crawlee. It includes configuration for proxy usage and session pool management.

LANGUAGE: js
CODE:
import { CheerioCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new CheerioCrawler({
    // To use the proxy IP rotation, you must turn the proxy usage on.
    useSessionPool: true,
    persistCookiesPerSession: true,
    proxyConfiguration,
    maxRequestsPerCrawl: 100,
    async requestHandler({ session, $, request }) {
        // ...
    },
});

await crawler.run();

----------------------------------------

TITLE: Customizing Browser Fingerprints with PuppeteerCrawler in Crawlee
DESCRIPTION: This code snippet shows how to customize browser fingerprints for PuppeteerCrawler in Crawlee. It configures specific options for fingerprint generation, including browser, operating system, and screen size.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                browsers: ["chrome"],
                operatingSystems: ["windows"],
                devices: ["desktop"],
                locales: ["en-US"],
                screen: {
                    minWidth: 1920,
                    minHeight: 1080,
                },
            },
        },
    },
    // ...
});

----------------------------------------

TITLE: Using Dataset Reduce Method in Crawlee
DESCRIPTION: This code snippet shows how to use the Dataset reduce method to calculate the total number of headers across all scraped pages in the dataset.

LANGUAGE: javascript
CODE:
import { Dataset, KeyValueStore } from 'crawlee';

const dataset = await Dataset.open();
const pagesHeadingCount = await dataset.reduce((memo, value) => {
    return memo + value.headingCount;
}, 0);

const kvStore = await KeyValueStore.open();
await kvStore.setValue('pages-heading-count', pagesHeadingCount);

----------------------------------------

TITLE: Handling Compressed HTTP Responses in Python
DESCRIPTION: Demonstrates how to properly handle Brotli-compressed HTTP responses using httpx library with appropriate headers. Shows the importance of proper compression handling for efficient web scraping.

LANGUAGE: python
CODE:
import httpx

url = 'https://www.wayfair.com/'

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Accept-Encoding": "gzip, deflate, br, zstd",
    "Connection": "keep-alive",
}

response = httpx.get(url, headers=headers)

print(response.content[:10])

----------------------------------------

TITLE: Setting Concurrency Limits in CheerioCrawler
DESCRIPTION: Shows how to configure minimum and maximum concurrent requests for the crawler, controlling parallel processing capacity.

LANGUAGE: javascript
CODE:
const crawler = new CheerioCrawler({
    minConcurrency: 1,
    maxConcurrency: 100,
    // ...
});

----------------------------------------

TITLE: Configuring Crawlee with Custom Storage Settings
DESCRIPTION: Implementation of Crawlee configuration to ensure isolated storage for each crawler instance in Lambda environment.

LANGUAGE: javascript
CODE:
// For more information, see https://crawlee.dev/
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Crawling Links with Cheerio Crawler
DESCRIPTION: Demonstrates using Cheerio Crawler with enqueueLinks() to scrape all links on a website within the same subdomain. Utilizes Cheerio for parsing HTML and handling requests.

LANGUAGE: javascript
CODE:
{CheerioSource}

----------------------------------------

TITLE: Installing Crawlee Package
DESCRIPTION: Instructions for installing Crawlee and its dependencies using npm.

LANGUAGE: bash
CODE:
npm install crawlee
# or specific crawler
npm install @crawlee/cheerio
# with browser dependencies
npm install crawlee playwright

----------------------------------------

TITLE: Crawling All Links Strategy in Crawlee
DESCRIPTION: Example showing how to crawl all links found on a page regardless of domain using the All strategy with CheerioCrawler. This will enqueue any URLs found, even if they go to different websites.

LANGUAGE: javascript
CODE:
{AllLinksSource}

----------------------------------------

TITLE: Extracting Manufacturer from URL in JavaScript
DESCRIPTION: This snippet demonstrates how to extract the manufacturer name from a product URL by splitting the string.

LANGUAGE: javascript
CODE:
// request.url = https://warehouse-theme-metal.myshopify.com/products/sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440

const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']
const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'

----------------------------------------

TITLE: Configuring PlaywrightCrawler with Custom Storage for AWS Lambda
DESCRIPTION: JavaScript code to initialize a PlaywrightCrawler with a custom Configuration instance to prevent interference between Lambda instances.

LANGUAGE: javascript
CODE:
// For more information, see https://crawlee.dev/
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Installing Node.js Type Declarations for Crawlee TypeScript Project
DESCRIPTION: Command to install TypeScript declarations for Node.js as a development dependency in a Crawlee project.

LANGUAGE: shell
CODE:
npm install --dev @types/node

----------------------------------------

TITLE: Configuring Package.json for TypeScript Build in Crawlee Project
DESCRIPTION: JSON configuration in package.json to set up the build script and main entry point for a TypeScript Crawlee project.

LANGUAGE: json
CODE:
{
    "scripts": {
        "build": "tsc"
    },
    "main": "dist/main.js"
}

----------------------------------------

TITLE: Configuring Dockerfile for Crawlee Project with Node.js and Playwright
DESCRIPTION: This Dockerfile sets up an environment for a Crawlee project. It uses a base image with Node.js and Playwright, installs dependencies, copies project files, and specifies the run command. The build process is optimized for caching and minimal image size.

LANGUAGE: Dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:20

COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD npm start --silent

----------------------------------------

TITLE: Intercepting GraphQL Requests with mitmproxy
DESCRIPTION: Python script for mitmproxy that intercepts GraphQL requests and modifies the persistedQuery hash to trigger a PersistedQueryNotFound error, forcing the client to reveal the full query text.

LANGUAGE: python
CODE:
import json

def request(flow):
    try:
        dat = json.loads(flow.request.text)
        dat[0]["extensions"]["persistedQuery"]["sha256Hash"] = "0d9e" # any bogus hex string here
        flow.request.text = json.dumps(dat)
    except:
        pass

----------------------------------------

TITLE: Configuring SessionPool with JSDOMCrawler in Crawlee
DESCRIPTION: This snippet illustrates how to set up and use SessionPool with JSDOMCrawler in Crawlee. It includes configuration for proxy usage and session pool management.

LANGUAGE: js
CODE:
import { JSDOMCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new JSDOMCrawler({
    // Activates the Session pool
    useSessionPool: true,
    // Overrides default Session pool configuration
    sessionPoolOptions: {
        maxPoolSize: 100,
        sessionOptions: {
            // All sessions will use this proxy configuration
            proxyConfiguration,
        },
    },
    async requestHandler({ session, request }) {
        // ...
    },
});

await crawler.run();

----------------------------------------

TITLE: Saving Data with Dataset.pushData() in Crawlee (JavaScript)
DESCRIPTION: This code demonstrates how to save scraped data using the Dataset.pushData() function in Crawlee. It replaces the console.log() call with the data saving operation.

LANGUAGE: javascript
CODE:
await Dataset.pushData(results);

----------------------------------------

TITLE: Crawling Multiple URLs with Puppeteer Crawler in JavaScript
DESCRIPTION: This code snippet shows how to use Puppeteer Crawler to crawl a list of specified URLs. It captures screenshots of each page and stores the page title and URL.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, Dataset } from 'crawlee';
import { writeFileSync } from 'fs';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${request.url} is '${title}'`);

        const screenshotBuffer = await page.screenshot();
        writeFileSync(`screenshot-${Math.random()}.png`, screenshotBuffer);

        await Dataset.pushData({
            title,
            url: request.url,
        });
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run([
    'https://crawlee.dev',
    'https://crawlee.dev/docs/quick-start',
    'https://crawlee.dev/docs/introduction/first-scraper',
]);

----------------------------------------

TITLE: Combining Request Queue and Request List in Crawlee
DESCRIPTION: Illustrates how to use both Request Queue and Request List together in a Crawlee crawler, allowing for both predefined and dynamically added URLs.

LANGUAGE: javascript
CODE:
import { RequestList, RequestQueue, PuppeteerCrawler } from 'crawlee';

const requestList = await RequestList.open('my-list', [
    { url: 'http://example.com/page-1' },
    { url: 'http://example.com/page-2' },
    { url: 'http://example.com/page-3' },
]);

const requestQueue = await RequestQueue.open();

const crawler = new PuppeteerCrawler({
    requestList,
    requestQueue,
    async requestHandler({ page, request, enqueueLinks }) {
        // Add newly discovered links to the queue
        await enqueueLinks();

        // Process the page
    },
});

await crawler.run();

----------------------------------------

TITLE: Configuring SessionPool with JSDOMCrawler in Crawlee
DESCRIPTION: This snippet illustrates how to configure and use SessionPool with JSDOMCrawler in Crawlee. It includes setup for proxy configuration and session pool options.

LANGUAGE: js
CODE:
import { JSDOMCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new JSDOMCrawler({
    // Use the proxy configuration
    proxyConfiguration,
    // Set up the session pool options
    sessionPoolOptions: {
        maxPoolSize: 100,
    },
    // This function is called for each URL
    async requestHandler({ session, request, window }) {
        const title = window.document.querySelector('title').textContent;
        // ...
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Mapping Dataset to Check for Pages with More Than 5 Headers
DESCRIPTION: This code snippet demonstrates the use of the Dataset map method to filter pages with more than 5 headers.

LANGUAGE: javascript
CODE:
import { Dataset, KeyValueStore } from 'crawlee';

const dataset = await Dataset.open();
const moreThan5headers = await dataset.map((item) => {
    if (item.headingCount > 5) return item.headingCount;
});

// Save the result to the default key-value store
const store = await KeyValueStore.open();
await store.setValue('pages-with-more-than-5-headers', moreThan5headers);

----------------------------------------

TITLE: Updating package.json for GCP Cloud Functions
DESCRIPTION: Set the "main" field in package.json to point to the entry file for the Cloud Function.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "version": "1.0.0",
    "main": "src/main.js",
    ...
}

----------------------------------------

TITLE: Key-Value Store Operations in Crawlee
DESCRIPTION: Demonstrates basic operations with key-value stores including reading input, writing output, and managing named stores.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';

// Get the INPUT from the default key-value store
const input = await KeyValueStore.getInput();

// Write the OUTPUT to the default key-value store
await KeyValueStore.setValue('OUTPUT', { myResult: 123 });

// Open a named key-value store
const store = await KeyValueStore.open('some-name');

// Write a record to the named key-value store.
// JavaScript object is automatically converted to JSON,
// strings and binary buffers are stored as they are
await store.setValue('some-key', { foo: 'bar' });

// Read a record from the named key-value store.
// Note that JSON is automatically parsed to a JavaScript object,
// text data is returned as a string, and other data is returned as binary buffer
const value = await store.getValue('some-key');

// Delete a record from the named key-value store
await store.setValue('some-key', null);

----------------------------------------

TITLE: Configuring Advanced Autoscaled Pool Options in Crawlee CheerioCrawler
DESCRIPTION: This snippet demonstrates how to set advanced autoscaled pool options in a CheerioCrawler for fine-tuning the crawler's scaling behavior.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    autoscaledPoolOptions: {
        desiredConcurrency: 10,
        desiredConcurrencyRatio: 0.90,
        scaleUpStepRatio: 0.05,
        scaleDownStepRatio: 0.05,
        maybeRunIntervalSecs: 1,
        loggingIntervalSecs: 60 * 5,
        autoscaleIntervalSecs: 10,
        maxTasksPerMinute: 200,
    },
    // ...
});

----------------------------------------

TITLE: Installing ts-node for Development in Crawlee TypeScript Project
DESCRIPTION: Install ts-node as a development dependency to run TypeScript code directly during development of a Crawlee project.

LANGUAGE: shell
CODE:
npm install --save-dev ts-node

----------------------------------------

TITLE: Implementing HTTP Crawler with Crawlee in TypeScript
DESCRIPTION: This code snippet demonstrates how to use the HttpCrawler class from Crawlee to crawl a list of URLs. It sets up the crawler, defines the request handler to process each page, and configures the output.

LANGUAGE: typescript
CODE:
import { HttpCrawler, Dataset } from 'crawlee';

// Create an instance of the HttpCrawler class
const crawler = new HttpCrawler({
    // Function called for each URL
    async requestHandler({ request, body, $ }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}`);

        await Dataset.pushData({
            url: request.url,
            title,
            html: body,
        });
    },
    // Let's limit our crawls to make our
    // tests shorter and safer
    maxRequestsPerCrawl: 20,
});

// Run the crawler with initial request
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Development Environment Setup
DESCRIPTION: Installation and configuration of ts-node for development environment

LANGUAGE: shell
CODE:
npm install --save-dev ts-node

LANGUAGE: json
CODE:
{
    "scripts": {
        "start:dev": "ts-node-esm -T src/main.ts"
    }
}

----------------------------------------

TITLE: Crawling Same Domain Links in Crawlee
DESCRIPTION: Implementation using CheerioCrawler to crawl links from the same domain, including subdomains. Processes URLs that share the same domain name regardless of subdomain.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, EnqueueStrategy } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks }) {
        // Add links from any subdomain to the queue
        await enqueueLinks({
            strategy: EnqueueStrategy.SameDomain,
        });
    },
});

await crawler.run(['https://example.com']);

----------------------------------------

TITLE: Accessing Page Title with JSDOM in JavaScript
DESCRIPTION: Demonstrates how to retrieve the page title using JSDOM, comparing it with browser JavaScript. This snippet shows the similarity between browser and JSDOM APIs for accessing document properties.

LANGUAGE: javascript
CODE:
// Return the page title
document.title; // browsers
window.document.title; // JSDOM

----------------------------------------

TITLE: Sample Dataset JSON Structure
DESCRIPTION: This JSON structure represents a sample dataset containing scraped page information, including URL and heading count.

LANGUAGE: json
CODE:
[
    {
        "url": "https://crawlee.dev/",
        "headingCount": 11
    },
    {
        "url": "https://crawlee.dev/storage",
        "headingCount": 8
    },
    {
        "url": "https://crawlee.dev/proxy",
        "headingCount": 4
    }
]

----------------------------------------

TITLE: Creating Dockerfile for Crawlee TypeScript Project
DESCRIPTION: Dockerfile configuration for a Crawlee TypeScript project, using multi-stage build to optimize the final image size.

LANGUAGE: dockerfile
CODE:
# using multistage build, as we need dev deps to build the TS source code
FROM apify/actor-node:20 AS builder

# copy all files, install all dependencies (including dev deps) and build the project
COPY . ./
RUN npm install --include=dev \
    && npm run build

# create final image
FROM apify/actor-node:20
# copy only necessary files
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/dist ./dist

# install only prod deps
RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional

# run compiled code
CMD npm run start:prod

----------------------------------------

TITLE: Sanity Check with PlaywrightCrawler
DESCRIPTION: A basic setup using PlaywrightCrawler to visit the start URL and print the text content of all categories on the page. This code serves as a sanity check to ensure the crawler is set up correctly before implementing the full scraping logic.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, enqueueLinks, log }) {
        const title = await page.title();
        log.info(`Title of ${page.url()} is '${title}'`);

        const collectionLinks = await page.$$('.collection-block-item');

        for (const link of collectionLinks) {
            const category = await link.textContent();
            await Dataset.pushData({ category });
        }

        await enqueueLinks({
            selector: '.collection-block-item',
            label: 'CATEGORY',
        });
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);


----------------------------------------

TITLE: Disabling Browser Fingerprints with PlaywrightCrawler in Crawlee
DESCRIPTION: This code snippet demonstrates how to disable the use of browser fingerprints in PlaywrightCrawler. It sets the 'useFingerprints' option to false in the browserPoolOptions.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    // ...
    browserPoolOptions: {
        useFingerprints: false,
    },
});

----------------------------------------

TITLE: Using HTML to Text Helper in ParselCrawler
DESCRIPTION: Demonstrates how to use the new html_to_text helper in a ParselCrawler to extract clean text from HTML pages.

LANGUAGE: python
CODE:
import asyncio

from crawlee.crawlers import ParselCrawler, ParselCrawlingContext


async def main() -> None:
    crawler = ParselCrawler()

    @crawler.router.default_handler
    async def handler(context: ParselCrawlingContext) -> None:
        context.log.info('Crawling: %s', context.request.url)
        text = context.html_to_text()
        # Continue with the processing...

    await crawler.run(['https://crawlee.dev'])


if __name__ == '__main__':
    asyncio.run(main())

----------------------------------------

TITLE: Configuring SessionPool with HttpCrawler in Crawlee
DESCRIPTION: This snippet shows how to configure and use SessionPool with HttpCrawler in Crawlee. It includes setup for proxy usage and session pool management.

LANGUAGE: js
CODE:
import { HttpCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new HttpCrawler({
    // Activates the Session pool
    useSessionPool: true,
    // Overrides default Session pool configuration
    sessionPoolOptions: {
        maxPoolSize: 100,
        sessionOptions: {
            // All sessions will use this proxy configuration
            proxyConfiguration,
        },
    },
    async requestHandler({ session, request }) {
        // ...
    },
});

await crawler.run();

----------------------------------------

TITLE: Using actor-node-playwright-chrome Docker Image
DESCRIPTION: Demonstrates using the Apify Docker image with Playwright and Chrome, suitable for CheerioCrawler and PlaywrightCrawler with Chrome.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:20

----------------------------------------

TITLE: Crawling with PlaywrightCrawler
DESCRIPTION: Example of using PlaywrightCrawler to recursively crawl the Crawlee website and extract page titles.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, request, enqueueLinks }) {
        const title = await page.title();
        await Dataset.pushData({
            url: request.url,
            title,
        });
        await enqueueLinks();
    },
    maxRequestsPerCrawl: 20,
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Configuring CheerioCrawler for GCP Cloud Functions
DESCRIPTION: Update the main.js file to use a separate Configuration instance with persistStorage set to false, and wrap the crawler call in a handler function for GCP.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (req, res) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
    
    return res.send(await crawler.getData())
}

----------------------------------------

TITLE: Basic Cheerio Crawler Implementation in TypeScript
DESCRIPTION: Simple crawler that downloads a single page's HTML and extracts its title using Cheerio.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Creating handler function for GCP Cloud Functions
DESCRIPTION: Wrap the crawler execution in an async handler function that takes req and res arguments, and export it as a named export. This function will be the entry point for the GCP Cloud Function.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (req, res) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
    
    return res.send(await crawler.getData())
}

----------------------------------------

TITLE: Using actor-node-playwright-firefox Docker Image
DESCRIPTION: Example of using the Apify Docker image that includes Playwright and Firefox.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-firefox:20

----------------------------------------

TITLE: Initializing CheerioCrawler with Custom Configuration for AWS Lambda
DESCRIPTION: This snippet shows how to create a CheerioCrawler instance with a custom Configuration for AWS Lambda environment. It sets persistStorage to false to use in-memory storage due to Lambda's read-only filesystem.

LANGUAGE: javascript
CODE:
// For more information, see https://crawlee.dev/
import { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Creating INPUT.json File for Crawlee Actor Input
DESCRIPTION: This bash command shows the file path where an INPUT.json file should be created to provide input to a Crawlee actor. The file is placed in the default key-value store of the project.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json

----------------------------------------

TITLE: Configuring package.json for Development Script in Crawlee
DESCRIPTION: JSON configuration in package.json to add a development script using ts-node-esm for running TypeScript code directly in a Crawlee project.

LANGUAGE: json
CODE:
{
    "scripts": {
        "start:dev": "ts-node-esm -T src/main.ts"
    }
}

----------------------------------------

TITLE: Extracting Text Content from an Element using Cheerio
DESCRIPTION: This snippet demonstrates how to use Cheerio to find the first <h2> element on a page and extract its text content.

LANGUAGE: javascript
CODE:
$('h2').text()

----------------------------------------

TITLE: Complete Package.json Configuration
DESCRIPTION: Full package.json configuration including all required dependencies and scripts

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "type": "module",
    "main": "dist/main.js",
    "dependencies": {
        "crawlee": "3.0.0"
    },
    "devDependencies": {
        "@apify/tsconfig": "^0.1.0",
        "@types/node": "^18.14.0",
        "ts-node": "^10.8.0",
        "typescript": "^4.7.4"
    },
    "scripts": {
        "start": "npm run start:dev",
        "start:prod": "node dist/main.js",
        "start:dev": "ts-node-esm -T src/main.ts",
        "build": "tsc"
    }
}

----------------------------------------

TITLE: Complete Crawlee Scraper with Data Saving (JavaScript)
DESCRIPTION: This is the final version of the Crawlee scraper code, including data extraction and saving functionality using Dataset.pushData(). It demonstrates the complete flow of a basic web scraping task.

LANGUAGE: javascript
CODE:
{Example}

----------------------------------------

TITLE: Capturing Screenshots with PuppeteerCrawler Utils Snapshot
DESCRIPTION: Example demonstrating how to capture screenshots across multiple pages using PuppeteerCrawler with the context-aware saveSnapshot() utility function.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from '@crawlee/puppeteer';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, utils: { puppeteer } }) {
        // Save page screenshot
        await puppeteer.saveSnapshot(page);
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Complete package.json Configuration for Crawlee TypeScript Project
DESCRIPTION: Full package.json configuration for a Crawlee TypeScript project, including dependencies, scripts, and project metadata.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "type": "module",
    "main": "dist/main.js",
    "dependencies": {
        "crawlee": "3.0.0"
    },
    "devDependencies": {
        "@apify/tsconfig": "^0.1.0",
        "@types/node": "^18.14.0",
        "ts-node": "^10.8.0",
        "typescript": "^4.7.4"
    },
    "scripts": {
        "start": "npm run start:dev",
        "start:prod": "node dist/main.js",
        "start:dev": "ts-node-esm -T src/main.ts",
        "build": "tsc"
    }
}

----------------------------------------

TITLE: Basic Link Enqueueing in Crawlee
DESCRIPTION: Simple example of using enqueueLinks() function in Crawlee without parameters.

LANGUAGE: javascript
CODE:
await enqueueLinks();

----------------------------------------

TITLE: Configuring PuppeteerCrawler with Pre and Post Navigation Hooks
DESCRIPTION: Example showing how to configure PuppeteerCrawler with preNavigationHooks and postNavigationHooks.

LANGUAGE: javascript
CODE:
const preNavigationHooks = [
    async ({ page }) => makePageStealthy(page)
];

const postNavigationHooks = [
    async ({ page }) => page.evaluate(() => {
        window.foo = 'bar'
    })
]

const crawler = new Apify.PuppeteerCrawler({
    preNavigationHooks,
    postNavigationHooks,
    // ...
})

----------------------------------------

TITLE: Creating a new Crawlee project using uvx
DESCRIPTION: Command to create a new Crawlee project from a template using uvx with CLI extras.

LANGUAGE: shell
CODE:
uvx 'crawlee[cli]' create my-crawler

----------------------------------------

TITLE: Storage Cleanup in Crawlee
DESCRIPTION: Demonstrates how to purge default storage directories using the purgeDefaultStorages helper function while preserving the INPUT key.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Dockerfile and Package.json Best Practices
DESCRIPTION: This snippet demonstrates the recommended way to specify Docker image and dependency versions in Dockerfile and package.json for Apify projects.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16

LANGUAGE: json
CODE:
{
    "dependencies": {
        "crawlee": "^3.0.0",
        "playwright": "*"
    }
}

----------------------------------------

TITLE: Disabling Browser Fingerprints in PlaywrightCrawler
DESCRIPTION: Example demonstrating how to disable browser fingerprint generation in PlaywrightCrawler by setting useFingerprints to false in browserPoolOptions.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        useFingerprints: false
    }
});

----------------------------------------

TITLE: Configuring PlaywrightCrawler with Persistent Storage Disabled for GCP Cloud Run
DESCRIPTION: This snippet shows how to initialize a PlaywrightCrawler with a custom Configuration that disables persistent storage, which is necessary for running in a stateless environment like Cloud Run.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Configuring PuppeteerCrawler with Navigation Hooks in Apify SDK v1
DESCRIPTION: Shows how to use preNavigationHooks and postNavigationHooks in PuppeteerCrawler configuration.

LANGUAGE: javascript
CODE:
const preNavigationHooks = [
    async ({ page }) => makePageStealthy(page)
];

const postNavigationHooks = [
    async ({ page }) => page.evaluate(() => {
        window.foo = 'bar'
    })
]

const crawler = new Apify.PuppeteerCrawler({
    preNavigationHooks,
    postNavigationHooks,
    // ...
})

----------------------------------------

TITLE: Simplified CheerioCrawler Setup with Direct URL Input in Crawlee
DESCRIPTION: This snippet shows a more concise way to set up a CheerioCrawler by directly providing URLs to the run method, eliminating the need for explicit RequestQueue initialization.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Installing Crawlee Meta-Package
DESCRIPTION: Command to install the complete Crawlee package that includes all crawler implementations.

LANGUAGE: bash
CODE:
npm install crawlee

----------------------------------------

TITLE: Creating GCP Cloud Function Handler
DESCRIPTION: Complete implementation of the cloud function handler that initializes and runs the crawler, then returns the collected data.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (req, res) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
    
    return res.send(await crawler.getData())
}

----------------------------------------

TITLE: Creating GCP Cloud Function Handler
DESCRIPTION: Complete implementation of the cloud function handler that initializes and runs the crawler, then returns the collected data.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (req, res) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
    
    return res.send(await crawler.getData())
}

----------------------------------------

TITLE: Extracting and Parsing Product Price with Playwright
DESCRIPTION: This code snippet shows how to extract the product price, filter the correct element, and parse it into a number using Playwright.

LANGUAGE: javascript
CODE:
const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

----------------------------------------

TITLE: Customizing Browser Fingerprints with PuppeteerCrawler in Crawlee
DESCRIPTION: This code snippet shows how to customize browser fingerprints using PuppeteerCrawler in Crawlee. It configures specific options for fingerprint generation, including browser, operating system, and screen size.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    // ...
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                browsers: [
                    { name: 'firefox', minVersion: 88 },
                    { name: 'chrome', minVersion: 94 },
                ],
                devices: [
                    'desktop',
                ],
                operatingSystems: [
                    'windows',
                ],
                locales: ['en-US', 'en-GB'],
                // screen size is required option
                screenWidth: 1920,
                screenHeight: 1080,
            },
        },
    },
});

----------------------------------------

TITLE: Configuring tsconfig.json for Crawlee TypeScript Project
DESCRIPTION: TypeScript configuration file (tsconfig.json) setup for a Crawlee project, extending @apify/tsconfig and setting up ES2022 module and target.

LANGUAGE: json
CODE:
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist"
    },
    "include": [
        "./src/**/*"
    ]
}

----------------------------------------

TITLE: Importing Crawlee Dataset Module
DESCRIPTION: Shows how to import the required Playwright Crawler and Dataset modules from Crawlee for data storage functionality.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

----------------------------------------

TITLE: URL Crawling and Data Extraction using JSDOMCrawler
DESCRIPTION: Shows how to crawl multiple URLs from an external file using JSDOMCrawler. The script loads each URL via HTTP request, parses HTML using jsdom, and extracts page titles and h1 tags.

LANGUAGE: typescript
CODE:
{JSDOMCrawlerSource}

----------------------------------------

TITLE: Complete Amazon Product Scraper Implementation
DESCRIPTION: Full implementation of the product scraper including advanced fields like images and attributes

LANGUAGE: typescript
CODE:
import { CheerioCrawler, CheerioCrawlingContext, log } from 'crawlee';

const requestHandler = async (context: CheerioCrawlingContext) => {
    const { $, request } = context;
    const { url } = request;

    log.info(`Scraping product page`, { url });
    const extractedProduct = extractProductDetails($);

    log.info(`Scraped product details for "${extractedProduct.title}", saving...`, { url });
    crawler.pushData(extractedProduct);
};

const crawler = new CheerioCrawler({ requestHandler });

await crawler.run(['https://www.amazon.com/dp/B0BV7XQ9V9']);

----------------------------------------

TITLE: Extracting Text Content from an HTML Element using Cheerio
DESCRIPTION: This snippet demonstrates how to use Cheerio to find the first <h2> element on a page and extract its text content.

LANGUAGE: javascript
CODE:
$('h2').text()

----------------------------------------

TITLE: Installing Apify SDK v1 with Playwright
DESCRIPTION: Command to install Apify SDK v1 with Playwright support.

LANGUAGE: bash
CODE:
npm install apify playwright

----------------------------------------

TITLE: Configuring Apify SDK with API Token
DESCRIPTION: JavaScript code showing how to initialize the Apify SDK with an API token for authentication

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const sdk = new Actor({ token: 'your_api_token' });

----------------------------------------

TITLE: Extracting and Parsing Product Price with Playwright
DESCRIPTION: This code snippet shows how to extract the product price, filter the correct element, and parse it into a number using Playwright.

LANGUAGE: javascript
CODE:
const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

----------------------------------------

TITLE: Response Handling Functions in TypeScript
DESCRIPTION: Implements success and error response handling functions that manage the lifecycle of response objects and send data back to users.

LANGUAGE: typescript
CODE:
export const sendSuccResponseById = (responseId: string, result: unknown, contentType: string) => {
    const res = responses.get(responseId);
    if (!res) {
        log.info(`Response for request ${responseId} not found`);
        return;
    }

    res.writeHead(200, { 'Content-Type': contentType });
    res.end(result);
    responses.delete(responseId);
};

----------------------------------------

TITLE: Static Proxy List Configuration
DESCRIPTION: Shows how to configure a static list of proxy URLs including a null option for no proxy use.

LANGUAGE: javascript
CODE:
const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
        null // null means no proxy is used
    ]
});

----------------------------------------

TITLE: Configuring Apify SDK with API Token
DESCRIPTION: JavaScript code showing how to initialize the Apify SDK with an API token for authentication

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const sdk = new Actor({ token: 'your_api_token' });

----------------------------------------

TITLE: Crawler Configuration Examples
DESCRIPTION: Examples of configuring various Crawlee features like browser fingerprints, request handling, and state management.

LANGUAGE: typescript
CODE:
const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        useFingerprints: false,
    },
});

const crawler = new CheerioCrawler({
    async requestHandler({ log, request }) {
        log.info(`Opened ${request.loadedUrl}`);
    },
});

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ responseType: 'json' });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Executing Bluesky Crawler
DESCRIPTION: Defines the crawl method to start the crawling process with given search queries, and the main execution function to orchestrate the entire scraping process.

LANGUAGE: python
CODE:
async def crawl(self, queries: list[str]) -> None:
    """Crawl the given URL."""
    if not self._crawler:
        raise ValueError('Crawler not initialized.')

    search_url = URL(f'{self._service_endpoint}/xrpc/app.bsky.feed.searchPosts')

    await self._crawler.run([str(search_url.with_query(q=query)) for query in queries])

async def run() -> None:
    """Main execution function that orchestrates the crawling process.

    Creates a scraper instance, manages the session, and handles the complete
    crawling lifecycle including proper cleanup on completion or error.
    """
    scraper = BlueskyApiScraper()
    scraper.create_session()
    try:
        await scraper.init_crawler()
        await scraper.crawl(['python', 'apify', 'crawlee'])
        await scraper.save_data()
    except Exception:
        traceback.print_exc()
    finally:
        scraper.delete_session()


def main() -> None:
    """Entry point for the crawler application."""
    asyncio.run(run())

----------------------------------------

TITLE: Accessing Crawling Context in Handler Functions
DESCRIPTION: Example showing how to access and use the new Crawling Context in handler functions.

LANGUAGE: javascript
CODE:
const handlePageFunction = async (crawlingContext1) => {
    crawlingContext1.hasOwnProperty('proxyInfo') // true
}

const handleFailedRequestFunction = async (crawlingContext2) => {
    crawlingContext2.hasOwnProperty('proxyInfo') // true
}

// All contexts are the same object.
crawlingContext1 === crawlingContext2 // true

----------------------------------------

TITLE: Scraping and Processing Product Price
DESCRIPTION: Shows how to extract and process the product price by filtering elements, extracting the price text, and converting it to a number.

LANGUAGE: javascript
CODE:
const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

----------------------------------------

TITLE: Using CookieJar with Got Scraping in TypeScript
DESCRIPTION: Demonstrates how to use a CookieJar from the tough-cookie package with Got Scraping in a BasicCrawler to manage cookies.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';
import { CookieJar } from 'tough-cookie';

const cookieJar = new CookieJar();

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ cookieJar });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Browser vs JSDOM Page Title Comparison in TypeScript
DESCRIPTION: Demonstrates the difference between accessing page title in browser JavaScript versus JSDOM environment. Shows equivalent syntax for both contexts.

LANGUAGE: typescript
CODE:
// Return the page title
document.title; // browsers
window.document.title; // JSDOM

----------------------------------------

TITLE: Crawlee Results Sample
DESCRIPTION: Example JSON output showing the structure of crawled data stored by Crawlee.

LANGUAGE: json
CODE:
{
    "url": "https://crawlee.dev/",
    "title": "Crawlee · The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee"
}

----------------------------------------

TITLE: Crawling Sitemap with Puppeteer Crawler in JavaScript
DESCRIPTION: This snippet shows how to use Puppeteer Crawler to download and process URLs from a sitemap. It includes a note about using the appropriate Docker image on the Apify Platform.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from '@crawlee/puppeteer';
import { downloadListOfUrls } from '@crawlee/utils';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        const title = await page.title();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
    maxRequestsPerCrawl: 20, // Limit the number of requests
});

const listOfUrls = await downloadListOfUrls({ url: 'https://apify.com/sitemap.xml' });
await crawler.addRequests(listOfUrls);

await crawler.run();

----------------------------------------

TITLE: Installing TypeScript Compiler for Crawlee Project
DESCRIPTION: Install TypeScript as a development dependency in the project.

LANGUAGE: shell
CODE:
npm install --save-dev typescript

----------------------------------------

TITLE: Initial Crawler Setup and Connection
DESCRIPTION: Basic setup for PlaywrightCrawler including browser configuration and page handling

LANGUAGE: python
CODE:
from crawlee.playwright_crawler import PlaywrightCrawler
from datetime import timedelta
import asyncio

async def scrape_google_maps(context):
    """
    Establishes connection to Google Maps and handles the initial page load
    """
    page = context.page
    await page.goto(context.request.url)
    context.log.info(f"Processing: {context.request.url}")

async def main():
    """
    Configures and launches the crawler with custom settings
    """
    # Initialize crawler with browser visibility and timeout settings
    crawler = PlaywrightCrawler(
        headless=False,  # Shows the browser window while scraping
        request_handler_timeout=timedelta(
            minutes=5
        ),  # Allows plenty of time for page loading
    )

    # Tell the crawler how to handle each page it visits
    crawler.router.default_handler(scrape_google_maps)

    # Prepare the search URL
    search_query = "hotels in bengaluru"
    start_url = f"https://www.google.com/maps/search/{search_query.replace(' ', '+')}"

    # Start the scraping process
    await crawler.run([start_url])

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Installing Crawlee packages with npm
DESCRIPTION: Shows different ways to install Crawlee packages with npm, including the full meta-package and individual crawler modules.

LANGUAGE: bash
CODE:
npm install crawlee

npm install @crawlee/cheerio

npm install crawlee playwright
# or npm install @crawlee/playwright playwright

----------------------------------------

TITLE: URL Pattern Example
DESCRIPTION: Demonstrates the structure of paginated URLs in the target website.

LANGUAGE: text
CODE:
https://warehouse-theme-metal.myshopify.com/collections/headphones?page=2

----------------------------------------

TITLE: Extracting Text Content from an Element using Cheerio
DESCRIPTION: This code snippet demonstrates how to use Cheerio to find the first <h2> element on a page and extract its text content.

LANGUAGE: javascript
CODE:
$('h2').text()

----------------------------------------

TITLE: Installing puppeteer-extra and stealth plugin
DESCRIPTION: Command to install the required packages for using puppeteer-extra with the stealth plugin.

LANGUAGE: bash
CODE:
npm install puppeteer-extra puppeteer-extra-plugin-stealth

----------------------------------------

TITLE: Sanity Check with PlaywrightCrawler and Cheerio in Crawlee
DESCRIPTION: This code snippet shows a sanity check using PlaywrightCrawler with Cheerio to visit the start URL, parse the HTML content, and extract category information.

LANGUAGE: javascript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';
import cheerio from 'cheerio';

const crawler = new PlaywrightCrawler({
    async requestHandler({ page, log }) {
        const html = await page.content();
        const $ = cheerio.load(html);

        const categories = $('.collection-block-item');
        log.info(`Found ${categories.length} categories`);

        categories.each((index, el) => {
            const categoryData = $(el).text();
            log.info(`Category: ${categoryData}`);
        });
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

await Dataset.exportToJSON('categories');

----------------------------------------

TITLE: Custom Crawler Implementation
DESCRIPTION: Implements a custom crawler class that uses curl-impersonate for HTTP requests and handles JSON responses from a Next.js application.

LANGUAGE: python
CODE:
from __future__ import annotations

import logging
from re import search
from typing import TYPE_CHECKING, Any, Unpack

from crawlee import Request
from crawlee.basic_crawler import (
    BasicCrawler,
    BasicCrawlerOptions,
    BasicCrawlingContext,
    ContextPipeline,
)
from crawlee.errors import SessionError
from crawlee.http_clients.curl_impersonate import CurlImpersonateHttpClient
from crawlee.http_crawler import HttpCrawlingContext
from orjson import loads

from afs_crawlee.constants import BASE_TEMPLATE, HEADERS

from .custom_context import CustomContext

if TYPE_CHECKING:
    from collections.abc import AsyncGenerator, Iterable


class CustomCrawler(BasicCrawler[CustomContext]):
    """A crawler that fetches the request URL using `curl_impersonate` and parses the result with `orjson` and `re`."""

    def __init__(
        self,
        *,
        impersonate: str = 'chrome124',
        additional_http_error_status_codes: Iterable[int] = (),
        ignore_http_error_status_codes: Iterable[int] = (),
        **kwargs: Unpack[BasicCrawlerOptions[CustomContext]],
    ) -> None:

        self._build_id = None
        self._base_url = BASE_TEMPLATE

        kwargs['_context_pipeline'] = (
            ContextPipeline()
            .compose(self._make_http_request)
            .compose(self._handle_blocked_request)
            .compose(self._parse_http_response)
        )

        kwargs.setdefault(
            'http_client',
            CurlImpersonateHttpClient(
                additional_http_error_status_codes=additional_http_error_status_codes,
                ignore_http_error_status_codes=ignore_http_error_status_codes,
                impersonate=impersonate,
                headers=HEADERS,
            ),
        )

        kwargs.setdefault('_logger', logging.getLogger(__name__))

        super().__init__(**kwargs)

----------------------------------------

TITLE: Using BrowserController in Apify SDK v1
DESCRIPTION: Demonstrates correct usage of BrowserController for managing browser instances and cookies.

LANGUAGE: javascript
CODE:
const handlePageFunction = async ({ page, browserController }) => {
    // Correct usage. Allows graceful shutdown.
    await browserController.close();

    const cookies = [/* some cookie objects */];
    // Correct usage. Will work in both Puppeteer and Playwright.
    await browserController.setCookies(page, cookies);
}

----------------------------------------

TITLE: Map Method Result Example
DESCRIPTION: Example output of the Dataset map method showing an array of heading counts greater than 5.

LANGUAGE: javascript
CODE:
[11, 8]

----------------------------------------

TITLE: Integrating AWS Chromium with Crawlee
DESCRIPTION: Implementation of Crawlee crawler with AWS-specific Chromium configuration including executable path and launch arguments.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
    launchContext: {
        launchOptions: {
             executablePath: await aws_chromium.executablePath(),
             args: aws_chromium.args,
             headless: true
        }
    }
}, new Configuration({
    persistStorage: false,
}));

----------------------------------------

TITLE: Installing Crawlee Basic
DESCRIPTION: Commands for installing the base Crawlee package and specific crawler modules

LANGUAGE: bash
CODE:
npm install crawlee

npm install @crawlee/cheerio

npm install crawlee playwright
# or npm install @crawlee/playwright playwright

----------------------------------------

TITLE: Configuring Pre-Navigation Hooks in CheerioCrawler
DESCRIPTION: Example showing how to configure pre-navigation hooks to modify gotOptions before making requests

LANGUAGE: typescript
CODE:
preNavigationHooks: [
    (crawlingContext, gotOptions) => {
        // ...
    },
]

----------------------------------------

TITLE: Sitemap Crawling with Puppeteer
DESCRIPTION: Implementation of sitemap crawling using Puppeteer Crawler. Requires apify/actor-node-puppeteer-chrome image when running on Apify Platform.

LANGUAGE: javascript
CODE:
{PuppeteerSource}

----------------------------------------

TITLE: Using SessionPool with JSDOMCrawler
DESCRIPTION: Implementation of SessionPool with JSDOMCrawler for DOM-based web scraping with session management.

LANGUAGE: javascript
CODE:
{JSDOMSource}

----------------------------------------

TITLE: Modifying Crawlee Detail Route for Inter-Process Communication
DESCRIPTION: This code updates the DETAIL route handler to use process.send for communicating scraped data back to the parent process, instead of using the crawler's built-in data pushing mechanism.

LANGUAGE: javascript
CODE:
DETAIL: async ({ $, request, log }) => {
    const title = $('h1').text();
    const description = $('meta[name="description"]').attr('content');
    log.info(`Scraping detailed data for ${title}`);

    const scrapedData = {
        url: request.url,
        title,
        description,
    };

    if (process && process.send) {
        process.send({
            type: 'CRAWLER_DATA',
            data: scrapedData,
        });
    }
},

----------------------------------------

TITLE: Command Line Arguments Setup
DESCRIPTION: Implementation of command line argument parsing for the scraper application

LANGUAGE: python
CODE:
import asyncio
import argparse

from .main import main

def get_args():
    parser = argparse.ArgumentParser(description="Crawl LinkedIn job listings")

    parser.add_argument("--title", type=str, required=True, help="Job title")
    parser.add_argument("--location", type=str, required=True, help="Job location")
    parser.add_argument("--data_name", type=str, required=True, help="Name for the output CSV file")

    return parser.parse_args()

if __name__ == '__main__':
    args = get_args()
    asyncio.run(main(args.title, args.location, args.data_name))

----------------------------------------

TITLE: Installing Crawlee and Dependencies
DESCRIPTION: Examples of how to install Crawlee packages with their dependencies using npm

LANGUAGE: bash
CODE:
npm install crawlee

npm install @crawlee/cheerio

npm install crawlee playwright
# or npm install @crawlee/playwright playwright

----------------------------------------

TITLE: Checking Product Stock Status
DESCRIPTION: Demonstrates how to determine if a product is in stock by checking for the presence of a specific element.

LANGUAGE: javascript
CODE:
const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Extracting All Links with Cheerio
DESCRIPTION: Demonstrates how to find all anchor elements with href attributes and extract their URLs into an array using Cheerio methods.

LANGUAGE: javascript
CODE:
$('a[href]')
    .map((i, el) => $(el).attr('href'))
    .get();

----------------------------------------

TITLE: Using Service Locator Explicitly in Crawlee
DESCRIPTION: Shows how to explicitly set up services using the ServiceLocator in Crawlee.

LANGUAGE: python
CODE:
import asyncio

from crawlee import service_locator
from crawlee.configuration import Configuration
from crawlee.crawlers import ParselCrawler, ParselCrawlingContext
from crawlee.events import LocalEventManager
from crawlee.storage_clients import MemoryStorageClient


async def main() -> None:
    service_locator.set_configuration(Configuration())
    service_locator.set_storage_client(MemoryStorageClient())
    service_locator.set_event_manager(LocalEventManager())

    crawler = ParselCrawler()

    # ...


if __name__ == '__main__':
    asyncio.run(main())

----------------------------------------

TITLE: Using curl_cffi for Browser Impersonation
DESCRIPTION: Demonstrates how to use curl_cffi library to impersonate Chrome browser and bypass Cloudflare protection. Shows configuration for proper TLS fingerprinting.

LANGUAGE: python
CODE:
from curl_cffi import requests

url = 'https://www.g2.com/'

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Accept-Encoding": "gzip, deflate, br, zstd",
    "Connection": "keep-alive",
}

response = requests.get(url, headers=headers, impersonate="chrome124")

print(response)

----------------------------------------

TITLE: Importing Required Components - React/JSX
DESCRIPTION: Import statements for React components used in the documentation page, including RunnableCodeBlock and ApiLink components.

LANGUAGE: jsx
CODE:
import RunnableCodeBlock from '@site/src/components/RunnableCodeBlock';
import ApiLink from '@site/src/components/ApiLink';
import CrawlSource from '!!raw-loader!roa-loader!./export_entire_dataset.ts';

----------------------------------------

TITLE: Complete AWS Lambda Handler Implementation
DESCRIPTION: Final implementation combining all components into an AWS Lambda handler function with proper response formatting.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
        launchContext: {
            launchOptions: {
                executablePath: await aws_chromium.executablePath(),
                args: aws_chromium.args,
                headless: true
            }
        }
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);

    return {
        statusCode: 200,
        body: await crawler.getData(),
    };
}

----------------------------------------

TITLE: Example HTML Link Structure
DESCRIPTION: Example of HTML anchor tag structure that the crawler will parse.

LANGUAGE: html
CODE:
<a href="https://crawlee.dev/js/docs/introduction">This is a link to Crawlee introduction</a>

----------------------------------------

TITLE: Installing Crawlee Basic Package
DESCRIPTION: Command to install the basic Crawlee package via npm

LANGUAGE: bash
CODE:
npm install crawlee

----------------------------------------

TITLE: Importing Storage Clients in Crawlee v0.5
DESCRIPTION: Shows the updated import statement for storage client classes in Crawlee v0.5.

LANGUAGE: python
CODE:
- from crawlee.memory_storage_client import MemoryStorageClient
+ from crawlee.storage_clients import MemoryStorageClient

----------------------------------------

TITLE: Playwright Chrome Configuration
DESCRIPTION: Docker configuration specifically for Playwright with Chrome browser support.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node-playwright-chrome:16

----------------------------------------

TITLE: Checking Stock Availability with Playwright
DESCRIPTION: Demonstrates how to check if a product is in stock by looking for specific inventory status text.

LANGUAGE: javascript
CODE:
const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;

----------------------------------------

TITLE: Implementing User Handler for Bluesky Scraper
DESCRIPTION: Defines the _user_handler method to process user profile requests and save user data to the dataset.

LANGUAGE: python
CODE:
async def _user_handler(self, context: HttpCrawlingContext) -> None:
    context.log.info(f'Processing user {context.request.url} ...')

    data = json.loads(context.http_response.read())

    user_item = {
        'did': data['did'],
        'created': data['createdAt'],
        'avatar': data.get('avatar'),
        'description': data.get('description'),
        'display_name': data.get('displayName'),
        'handle': data['handle'],
        'indexed': data.get('indexedAt'),
        'posts_count': data['postsCount'],
        'followers_count': data['followersCount'],
        'follows_count': data['followsCount'],
    }

    await self._users.push_data(user_item)

----------------------------------------

TITLE: Dockerfile Setup for Crawlee
DESCRIPTION: Multi-stage Dockerfile configuration for building and running Crawlee projects.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16 AS builder

COPY . ./
RUN npm install --include=dev \
    && npm run build

FROM apify/actor-node:16
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/README.md ./
COPY --from=builder /usr/src/app/dist ./dist
COPY --from=builder /usr/src/app/apify.json ./apify.json
COPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json

RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional \
    && echo "Installed NPM packages:" \
    && (npm list --only=prod --no-optional --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

CMD npm run start:prod

----------------------------------------

TITLE: Configuring Apify Proxy
DESCRIPTION: Example of creating a proxy configuration for Apify Proxy, including selecting specific proxy groups and countries.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
    countryCode: 'US',
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Updating package.json for GCP Cloud Functions
DESCRIPTION: Sets the 'main' field in package.json to point to the main JavaScript file for the Cloud Function.

LANGUAGE: json
CODE:
{
    "name": "my-crawlee-project",
    "version": "1.0.0",
    "main": "src/main.js",
    ...
}

----------------------------------------

TITLE: Configuring Crawlee Crawler for Google Search
DESCRIPTION: Setup of BeautifulSoupCrawler with CurlImpersonateHttpClient and concurrency settings for Google search scraping

LANGUAGE: python
CODE:
from crawlee.beautifulsoup_crawler import BeautifulSoupCrawler
from crawlee.http_clients.curl_impersonate import CurlImpersonateHttpClient
from crawlee import ConcurrencySettings, HttpHeaders

async def main() -> None:
    concurrency_settings = ConcurrencySettings(max_concurrency=5, max_tasks_per_minute=200)

    http_client = CurlImpersonateHttpClient(impersonate="chrome124",
                                            headers=HttpHeaders({"referer": "https://www.google.com/",
                                                     "accept-language": "en",
                                                     "accept-encoding": "gzip, deflate, br, zstd",
                                                     "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
                                            }))

    crawler = BeautifulSoupCrawler(
        max_request_retries=1,
        concurrency_settings=concurrency_settings,
        http_client=http_client,
        max_requests_per_crawl=10,
        max_crawl_depth=5
    )

    await crawler.run(['https://www.google.com/search?q=Apify'])

----------------------------------------

TITLE: Basic Browser Pool Usage with Playwright
DESCRIPTION: Example showing how to initialize Browser Pool with Playwright plugin and perform basic page operations

LANGUAGE: javascript
CODE:
import { BrowserPool, PlaywrightPlugin } from '@crawlee/browser-pool';
import playwright from 'playwright';

const browserPool = new BrowserPool({
    browserPlugins: [new PlaywrightPlugin(playwright.chromium)],
});

// Launches Chromium with Playwright and returns a Playwright Page.
const page1 = await browserPool.newPage();
// You can interact with the page as you're used to.
await page1.goto('https://example.com');
// When you're done, close the page.
await page1.close();

// Opens a second page in the same browser.
const page2 = await browserPool.newPage();
// When everything's finished, tear down the pool.
await browserPool.destroy();

----------------------------------------

TITLE: Installing Crawlee Packages
DESCRIPTION: NPM commands for installing Crawlee and its required dependencies.

LANGUAGE: bash
CODE:
npm install crawlee
npm install @crawlee/cheerio
npm install crawlee playwright

----------------------------------------

TITLE: Initializing Actor with API Token
DESCRIPTION: Example of initializing an Apify Actor instance with an API token for authentication

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const sdk = new Actor({ token: 'your_api_token' });

----------------------------------------

TITLE: Installing Cheerio Crawler Package
DESCRIPTION: Command to install only the Cheerio crawler implementation for lightweight HTML parsing.

LANGUAGE: bash
CODE:
npm install @crawlee/cheerio

----------------------------------------

TITLE: Scraping with PuppeteerCrawler (JavaScript)
DESCRIPTION: This snippet demonstrates how to use PuppeteerCrawler to scrape JavaScript-rendered content, explicitly waiting for elements to appear before scraping.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        // Wait for the actor cards to render
        await page.waitForSelector('.ActorStoreItem');
        // Extract text content of the first actor card
        const actorText = await page.$eval('.ActorStoreItem', (el) => el.textContent);
        console.log(`ACTOR: ${actorText}`);
    }
});

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Installing Crawlee NPM Package
DESCRIPTION: Basic npm installation commands for Crawlee and its dependencies. Shows how to install the full package or individual components.

LANGUAGE: bash
CODE:
npm install crawlee
npm install @crawlee/cheerio
npm install crawlee playwright

----------------------------------------

TITLE: Installing Crawlee for Python v0.5 via pip
DESCRIPTION: Command to upgrade Crawlee to the latest version from PyPI using pip.

LANGUAGE: shell
CODE:
pip install --upgrade crawlee

----------------------------------------

TITLE: SendRequest Implementation Details
DESCRIPTION: Shows the internal implementation of sendRequest function with its configuration options.

LANGUAGE: typescript
CODE:
async sendRequest(overrideOptions?: GotOptionsInit) => {
    return gotScraping({
        url: request.url,
        method: request.method,
        body: request.payload,
        headers: request.headers,
        proxyUrl: crawlingContext.proxyInfo?.url,
        sessionToken: session,
        responseType: 'text',
        ...overrideOptions,
        retry: {
            limit: 0,
            ...overrideOptions?.retry,
        },
        cookieJar: {
            getCookieString: (url: string) => session!.getCookieString(url),
            setCookie: (rawCookie: string, url: string) => session!.setCookie(rawCookie, url),
            ...overrideOptions?.cookieJar,
        },
    });
}

----------------------------------------

TITLE: Capturing Screenshots of Multiple Pages Using PuppeteerCrawler with page.screenshot()
DESCRIPTION: This snippet demonstrates how to capture screenshots of multiple web pages using PuppeteerCrawler with page.screenshot(). It sets up a crawler to visit multiple URLs, take screenshots, and save them to a key-value store.

LANGUAGE: javascript
CODE:
import { KeyValueStore, PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);

        const screenshot = await page.screenshot();
        const key = `screenshot-${Math.random().toString().slice(2)}.png`;
        await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });
    },
});

await crawler.run(['https://crawlee.dev', 'https://apify.com']);

----------------------------------------

TITLE: Installing Crawlee Package
DESCRIPTION: Command to install the core Crawlee package via npm.

LANGUAGE: bash
CODE:
npm install crawlee

----------------------------------------

TITLE: Enabling Request Locking in CheerioCrawler
DESCRIPTION: This snippet demonstrates how to enable the request locking experiment in a CheerioCrawler instance. The experiment is enabled by setting the requestLocking option to true in the experiments object.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    experiments: {
        requestLocking: true,
    },
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Initializing Cheerio Crawler with Lambda Configuration
DESCRIPTION: Basic setup of a Cheerio crawler with Lambda-specific configuration to ensure stateless operation using in-memory storage.

LANGUAGE: javascript
CODE:
// For more information, see https://crawlee.dev/
import { CheerioCrawler, Configuration, ProxyConfiguration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new CheerioCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Update fingerprintGeneratorOptions Types
DESCRIPTION: Bug fix for updating fingerprint generator option types

LANGUAGE: markdown
CODE:
### Bug Fixes

* update `fingerprintGeneratorOptions` types ([#2705](https://github.com/apify/crawlee/issues/2705))

----------------------------------------

TITLE: Disabling Fingerprints in PlaywrightCrawler
DESCRIPTION: Example showing how to disable browser fingerprints entirely in PlaywrightCrawler by setting useFingerprints to false.

LANGUAGE: javascript
CODE:
{PlaywrightFingerprintsOffSource}

----------------------------------------

TITLE: Installing and Packaging Browser Dependencies
DESCRIPTION: Commands for installing the @sparticuz/chromium package and creating a zip archive of node_modules for AWS Lambda Layer deployment.

LANGUAGE: bash
CODE:
# Install the package
npm i -S @sparticuz/chromium

# Zip the dependencies
zip -r dependencies.zip ./node_modules

----------------------------------------

TITLE: Installing Crawlee with CLI
DESCRIPTION: Commands to install Crawlee using the Crawlee CLI, which sets up a new project with necessary dependencies and boilerplate code.

LANGUAGE: bash
CODE:
npx crawlee create my-crawler

LANGUAGE: bash
CODE:
cd my-crawler && npm start

----------------------------------------

TITLE: Implementing Sitemap Navigation and Data Extraction
DESCRIPTION: Python code for navigating Crunchbase's sitemap, handling gzipped sitemap files, and extracting company data from individual pages.

LANGUAGE: python
CODE:
from crawlee.crawlers import ParselCrawlingContext
from crawlee.router import Router
from crawlee import Request
from gzip import decompress
from parsel import Selector

router = Router[ParselCrawlingContext]()

@router.default_handler
async def default_handler(context: ParselCrawlingContext) -> None:
    requests = [
        Request.from_url(url, label='sitemap')
        for url in context.selector.xpath('//loc[contains(., "sitemap-organizations")]/text()').getall()
    ]
    await context.add_requests(requests, limit=1)

@router.handler('sitemap')
async def sitemap_handler(context: ParselCrawlingContext) -> None:
    data = context.http_response.read()
    data = decompress(data)
    selector = Selector(data.decode())
    requests = [Request.from_url(url, label='company') for url in selector.xpath('//loc/text()').getall()]
    await context.add_requests(requests)

@router.handler('company')
async def company_handler(context: ParselCrawlingContext) -> None:
    json_selector = context.selector.xpath('//*[@id="ng-state"]/text()')
    await context.push_data(
        {
            'Company Name': json_selector.jmespath('HttpState.*.data[].properties.identifier.value').get(),
            'Short Description': json_selector.jmespath('HttpState.*.data[].properties.short_description').get(),
            'Website': json_selector.jmespath('HttpState.*.data[].cards.company_about_fields2.website.value').get(),
            'Location': '; '.join(
                json_selector.jmespath(
                    'HttpState.*.data[].cards.company_about_fields2.location_identifiers[].value'
                ).getall()
            ),
        }
    )

----------------------------------------

TITLE: Capturing Screenshots with PuppeteerCrawler Using page.screenshot()
DESCRIPTION: This snippet demonstrates how to capture screenshots of multiple web pages using PuppeteerCrawler and page.screenshot(). It creates a PuppeteerCrawler instance, defines a handler function that captures and saves screenshots for each page, and runs the crawler.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler, KeyValueStore } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        const title = await page.title();
        console.log(`Title of ${request.url}: ${title}`);

        const screenshot = await page.screenshot();
        const key = `${Math.random()}.png`; // Or use URL
        await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });
    },
    maxRequestsPerCrawl: 10,
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Crawler Docker Configuration
DESCRIPTION: Multi-stage Dockerfile for building and running TypeScript Crawlee projects.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16 AS builder
COPY . ./
RUN npm install --include=dev \
    && npm run build
FROM apify/actor-node:16
COPY --from=builder /usr/src/app/package*.json ./
COPY --from=builder /usr/src/app/README.md ./
COPY --from=builder /usr/src/app/dist ./dist
COPY --from=builder /usr/src/app/apify.json ./apify.json
COPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json
RUN npm --quiet set progress=false \
    && npm install --only=prod --no-optional \
    && echo "Installed NPM packages:" \
    && (npm list --only=prod --no-optional --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version
CMD npm run start:prod

----------------------------------------

TITLE: Installing Apify SDK v1 with Playwright
DESCRIPTION: Command to install Apify SDK v1 with Playwright support.

LANGUAGE: bash
CODE:
npm install apify playwright

----------------------------------------

TITLE: Wrapping Crawlee Code in AWS Lambda Handler Function
DESCRIPTION: Encapsulates the Crawlee crawler setup and execution within an exported handler function for AWS Lambda integration.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
        launchContext: {
            launchOptions: {
                executablePath: await aws_chromium.executablePath(),
                args: aws_chromium.args,
                headless: true
            }
        }
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);

    return {
        statusCode: 200,
        body: await crawler.getData(),
    };
}

----------------------------------------

TITLE: Installing Apify SDK v1 with Dependencies
DESCRIPTION: Commands for installing Apify SDK v1 with either Puppeteer or Playwright browser automation libraries.

LANGUAGE: bash
CODE:
npm install apify puppeteer

LANGUAGE: bash
CODE:
npm install apify playwright

----------------------------------------

TITLE: Installing Crawlee SDK with Puppeteer
DESCRIPTION: Instructions for installing SDK v1 with Puppeteer support via npm.

LANGUAGE: bash
CODE:
npm install apify puppeteer

----------------------------------------

TITLE: Configuring Apify Proxy
DESCRIPTION: Examples of setting up and using Apify Proxy with custom configurations

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
    countryCode: 'US',
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Scraping Product Title with Playwright
DESCRIPTION: Shows how to extract a product title using Playwright's locator API with CSS selectors.

LANGUAGE: javascript
CODE:
const title = await page.locator('.product-meta h1').textContent();

----------------------------------------

TITLE: Exporting Entire Dataset to CSV File in Crawlee (TypeScript)
DESCRIPTION: This code snippet demonstrates how to use Crawlee's Dataset.exportToValue() function to export the entire default dataset to a single CSV file. The exported file is then stored in a key-value store named 'my-data' using KeyValueStore.open().

LANGUAGE: typescript
CODE:
import { Dataset, KeyValueStore } from 'crawlee';

const dataset = await Dataset.open();
const csv = await dataset.exportToValue('csv');

const store = await KeyValueStore.open('my-data');
await store.setValue('OUTPUT.csv', csv);

----------------------------------------

TITLE: Installing Browser Pool with Playwright
DESCRIPTION: NPM installation command for Browser Pool and Playwright dependencies

LANGUAGE: bash
CODE:
npm install @crawlee/browser-pool playwright

----------------------------------------

TITLE: Installing Crawlee Project
DESCRIPTION: Commands to create a new Crawlee project and install dependencies using Poetry

LANGUAGE: bash
CODE:
pipx run crawlee create linkedin-scraper
poetry install

----------------------------------------

TITLE: JSON Response Handling
DESCRIPTION: Shows how to configure the responseType to handle JSON responses instead of default text/HTML.

LANGUAGE: typescript
CODE:
import { BasicCrawler } from 'crawlee';

const crawler = new BasicCrawler({
    async requestHandler({ sendRequest, log }) {
        const res = await sendRequest({ responseType: 'json' });
        log.info('received body', res.body);
    },
});

----------------------------------------

TITLE: Skipping Navigation for Image Requests with PlaywrightCrawler in TypeScript
DESCRIPTION: This code snippet demonstrates how to use PlaywrightCrawler to crawl a website, skip navigation for image requests, and save them directly to a key-value store. It utilizes the Request#skipNavigation option and sendRequest method to efficiently handle CDN-delivered images.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, KeyValueStore } from 'crawlee';

const crawler = new PlaywrightCrawler({
    async requestHandler({ request, sendRequest, log }) {
        if (request.label === 'IMAGE') {
            log.info(`Fetching image ${request.url}`);
            const imageBuffer = await sendRequest({ url: request.url, responseType: 'buffer' });
            await KeyValueStore.setValue(request.userData.imageKey, imageBuffer, { contentType: 'image/png' });
            return;
        }

        // Standard crawling logic...
    },
});

await crawler.run([
    {
        url: 'https://example.com',
    },
    {
        url: 'https://example.com/image.png',
        label: 'IMAGE',
        skipNavigation: true,
        userData: {
            imageKey: 'my-image-in-key-value-store',
        },
    },
]);

----------------------------------------

TITLE: Node.js Base Image Configuration
DESCRIPTION: Configures base Node.js Docker image without browser support, suitable for CheerioCrawler.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:16

----------------------------------------

TITLE: Comparing Cheerio and Browser JavaScript for DOM Manipulation
DESCRIPTION: This snippet demonstrates how to perform common DOM operations using both Cheerio and plain browser JavaScript, highlighting the syntax differences and similarities.

LANGUAGE: typescript
CODE:
// Return the text content of the <title> element.
document.querySelector('title').textContent; // plain JS
$('title').text(); // Cheerio

// Return an array of all 'href' links on the page.
Array.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS
$('[href]')
    .map((i, el) => $(el).attr('href'))
    .get(); // Cheerio

----------------------------------------

TITLE: Creating and Running a Hello World Actor
DESCRIPTION: Bash commands to create and run a Hello World actor using Apify CLI.

LANGUAGE: bash
CODE:
apify create my-hello-world
cd my-hello-world
apify run

----------------------------------------

TITLE: Fetching Single URL with got-scraping
DESCRIPTION: Example showing how to use got-scraping package to fetch HTML content from a webpage. Uses RunnableCodeBlock component for demonstration with Cheerio parser.

LANGUAGE: javascript
CODE:
import RunnableCodeBlock from '@site/src/components/RunnableCodeBlock';
import ApiLink from '@site/src/components/ApiLink';
import CrawlSource from '!!raw-loader!roa-loader!./crawl_single_url.ts';

----------------------------------------

TITLE: Setting Request Limit in Crawler
DESCRIPTION: Example showing how to limit the maximum number of requests per crawl using maxRequestsPerCrawl option.

LANGUAGE: typescript
CODE:
const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 20,
    // ...
});

----------------------------------------

TITLE: Installing Crawlee with Cheerio
DESCRIPTION: Command to install the Cheerio-specific Crawlee package

LANGUAGE: bash
CODE:
npm install @crawlee/cheerio

----------------------------------------

TITLE: Installing Crawlee manually for CheerioCrawler
DESCRIPTION: Command to manually install Crawlee for use with CheerioCrawler.

LANGUAGE: bash
CODE:
npm install crawlee

----------------------------------------

TITLE: Installing Dependencies with pip
DESCRIPTION: Commands to install Crawlee with curl-impersonate support and the orjson package for high-performance JSON handling.

LANGUAGE: bash
CODE:
pip install crawlee[curl-impersonate]==0.3.5
pip install orjson>=3.10.7,<4.0.0"

----------------------------------------

TITLE: Installing Playwright-Extra Dependencies
DESCRIPTION: Command to install playwright-extra and puppeteer stealth plugin using npm package manager

LANGUAGE: bash
CODE:
npm install playwright-extra puppeteer-extra-plugin-stealth

----------------------------------------

TITLE: Beta Version Format Example
DESCRIPTION: Example format for beta version numbers showing version with beta suffix and counter.

LANGUAGE: markdown
CODE:
${VERSION}-beta.${COUNTER}

----------------------------------------

TITLE: Checking Node.js Version in Bash
DESCRIPTION: Command to check the installed version of Node.js.

LANGUAGE: bash
CODE:
node -v

----------------------------------------

TITLE: Extracting Product SKU with Playwright
DESCRIPTION: Demonstrates how to get the product SKU using a specific CSS class selector.

LANGUAGE: javascript
CODE:
const sku = await page.locator('span.product-meta__sku-number').textContent();

----------------------------------------

TITLE: Configuring Advanced Autoscaling Pool Options in Crawlee
DESCRIPTION: Shows advanced configuration of the autoscaling pool, including scaling ratios, intervals, and logging settings.

LANGUAGE: javascript
CODE:
new CheerioCrawler({
    autoscaledPoolOptions: {
        desiredConcurrency: 10,
        desiredConcurrencyRatio: 0.95,
        scaleUpStepRatio: 0.05,
        scaleDownStepRatio: 0.05,
        maybeRunIntervalSecs: 0.5,
        loggingIntervalSecs: 60,
        autoscaleIntervalSecs: 10,
        maxTasksPerMinute: 120
    },
    // ... other options
});

----------------------------------------

TITLE: Git Version Tag Format Example
DESCRIPTION: Example format for version tags used in GitHub releases, showing the proper format with v prefix.

LANGUAGE: markdown
CODE:
v${VERSION}

----------------------------------------

TITLE: Enabling Corepack for Yarn v4 in Crawlee Project
DESCRIPTION: Command to enable corepack, which is used to manage Yarn v4 dependencies in the project.

LANGUAGE: shell
CODE:
corepack enable

----------------------------------------

TITLE: Crawling Category Pages with PlaywrightCrawler
DESCRIPTION: This snippet demonstrates how to use PlaywrightCrawler to crawl category pages of an e-commerce store. It uses the enqueueLinks function with a specific selector to target category links.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler } from 'crawlee';

const crawler = new PlaywrightCrawler({
    requestHandler: async ({ page, request, enqueueLinks }) => {
        console.log(`Processing: ${request.url}`);
        // Wait for the category cards to render,
        // otherwise enqueueLinks wouldn't enqueue anything.
        await page.waitForSelector('.collection-block-item');

        // Add links to the queue, but only from
        // elements matching the provided selector.
        await enqueueLinks({
            selector: '.collection-block-item',
            label: 'CATEGORY',
        });
    },
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

----------------------------------------

TITLE: Installing Crawlee SDK with Playwright
DESCRIPTION: Instructions for installing SDK v1 with Playwright support via npm.

LANGUAGE: bash
CODE:
npm install apify playwright

----------------------------------------

TITLE: Installing Playwright Extra Dependencies
DESCRIPTION: Command line instructions for installing the required Playwright Extra packages and stealth plugin using npm.

LANGUAGE: bash
CODE:
npm install playwright-extra puppeteer-extra-plugin-stealth

----------------------------------------

TITLE: Configuring enqueueLinks for Same-Domain Crawling in Crawlee
DESCRIPTION: Demonstrates how to set up enqueueLinks to crawl links within the same domain, including subdomains.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    strategy: 'same-domain'
});

----------------------------------------

TITLE: Configuring enqueueLinks for Same-Domain Crawling in Crawlee
DESCRIPTION: Demonstrates how to set up enqueueLinks to crawl links within the same domain, including subdomains.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    strategy: 'same-domain'
});

----------------------------------------

TITLE: Customizing Browser Fingerprints with PuppeteerCrawler
DESCRIPTION: Shows configuration for customizing browser fingerprints in PuppeteerCrawler with specific browser and OS settings.

LANGUAGE: javascript
CODE:
{PuppeteerSource}

----------------------------------------

TITLE: Installing ts-node for Development
DESCRIPTION: Command to install ts-node as a development dependency for running TypeScript code directly.

LANGUAGE: shell
CODE:
npm install --dev ts-node

----------------------------------------

TITLE: Mocking Node.js OS Module in Vitest for Crawlee Tests
DESCRIPTION: Example of mocking the Node.js 'os' module in Vitest, including handling of default exports and individual function mocks.

LANGUAGE: typescript
CODE:
vitest.mock('node:os', async (importActual) => {
    const original = await importActual<
        typeof import('node:os') & { default: typeof import('node:os') }
    >();

    const platformMock = () => 'darwin';
    const freememMock = vitest.fn();

    return {
        ...original,
        platform: platformMock,
        freemem: freememMock,
        // Specifically, you'll need to also mock the `default` property of the module, as seen below
        default: {
            ...original.default,
            platform: platformMock,
            freemem: freememMock,
        },
    };
});

----------------------------------------

TITLE: Initializing and Deploying Apify Project
DESCRIPTION: Commands for initializing the project configuration and deploying to Apify Platform

LANGUAGE: bash
CODE:
apify init

LANGUAGE: bash
CODE:
apify push

----------------------------------------

TITLE: Initializing and Deploying Apify Project
DESCRIPTION: Commands for initializing the project configuration and deploying to Apify Platform

LANGUAGE: bash
CODE:
apify init

LANGUAGE: bash
CODE:
apify push

----------------------------------------

TITLE: Simplified CheerioCrawler Setup with Direct URL Input in Crawlee
DESCRIPTION: Demonstrates a more concise way to set up a CheerioCrawler by directly providing URLs to the run method.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Disabling Browser Fingerprints in Crawlee
DESCRIPTION: Example showing how to disable dynamic fingerprints in PlaywrightCrawler configuration.

LANGUAGE: typescript
CODE:
const crawler = new PlaywrightCrawler({
    browserPoolOptions: {
        useFingerprints: false,
    },
});

----------------------------------------

TITLE: Importing Dataset Module in Crawlee (TypeScript)
DESCRIPTION: Imports the PlaywrightCrawler and Dataset modules from Crawlee. This is necessary to use the Dataset.pushData() function for saving extracted data.

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

----------------------------------------

TITLE: Crawling Multiple URLs with Cheerio
DESCRIPTION: Example showing how to crawl multiple URLs using Cheerio Crawler, which is a fast and lightweight solution for HTML parsing. The code demonstrates basic crawler configuration and URL handling.

LANGUAGE: javascript
CODE:
{CheerioSource}

----------------------------------------

TITLE: Initializing Crawler Map in TypeScript
DESCRIPTION: Creates a Map to store multiple PlaywrightCrawler instances, where each key is a stringified proxy configuration.

LANGUAGE: typescript
CODE:
const crawlers = new Map<string, PlaywrightCrawler>();

----------------------------------------

TITLE: Injecting Global Variable using IIFE in JavaScript
DESCRIPTION: This snippet uses an immediately invoked function expression (IIFE) to inject a variable into the global scope of a web page. It defines a function that sets a global variable 'injectedVariable' to the value 42, then immediately executes this function with the 'window' object as its argument.

LANGUAGE: javascript
CODE:
(function inject(global) {
    global.injectedVariable = 42;
}(window));

----------------------------------------

TITLE: Puppeteer Screenshot with Utils
DESCRIPTION: Shows how to capture screenshots using the utils.puppeteer.saveSnapshot() utility function, which provides additional context-aware functionality for saving screenshots.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';
import { chromium } from 'puppeteer';
import { utils } from 'crawlee';

const browser = await chromium.launch();
const page = await browser.newPage();
await page.goto('https://crawlee.dev');

await utils.puppeteer.saveSnapshot(page, { key: 'screenshot-example' });

await browser.close();

----------------------------------------

TITLE: Creating and Running a New Actor with Apify CLI
DESCRIPTION: Commands to create a new actor project and run it locally using Apify CLI.

LANGUAGE: bash
CODE:
apify create my-hello-world
cd my-hello-world
apify run

----------------------------------------

TITLE: Testing for Class Names in Stack Traces with Vitest
DESCRIPTION: Example of how to test for class names in stack traces using Vitest, accounting for potential '_' prefixes.

LANGUAGE: typescript
CODE:
expect(/at _?BasicCrawler\.requestHandler/.test(stackTrace)).toBe(true);

----------------------------------------

TITLE: Using Async Hooks in Vitest for Crawlee Tests
DESCRIPTION: Example of using async hooks in Vitest as a replacement for callback-style hooks in Jest.

LANGUAGE: typescript
CODE:
beforeAll(async () => {
    await new Promise((resolve) => {
        // Do something
        resolve();
    });
});

----------------------------------------

TITLE: Importing Crawlee Dataset Components
DESCRIPTION: Import statement for PlaywrightCrawler and Dataset from Crawlee library

LANGUAGE: typescript
CODE:
import { PlaywrightCrawler, Dataset } from 'crawlee';

----------------------------------------

TITLE: Implementing BasicCrawler for Web Scraping in JavaScript
DESCRIPTION: This code demonstrates how to use BasicCrawler from Crawlee to crawl web pages, send HTTP requests, and store the resulting data. It uses the sendRequest utility function and stores the raw HTML and URL in the default dataset.

LANGUAGE: javascript
CODE:
import { BasicCrawler, Dataset } from 'crawlee';

const crawler = new BasicCrawler({
    // Function called for each URL
    async requestHandler({ sendRequest, log }) {
        const { body } = await sendRequest();

        log.info('Crawled');  // 'log' is provided by the crawler

        // Save results to dataset
        await Dataset.pushData({
            url: request.url,
            html: body,
        });
    },
});

// Run the crawler
await crawler.run([
    'https://crawlee.dev',
    'https://example.com',
]);

----------------------------------------

TITLE: Enabling System Information V2 Experiment in CheerioCrawler
DESCRIPTION: This code snippet demonstrates how to enable the System Information V2 experiment in Crawlee using the CheerioCrawler. It sets the 'systemInfoV2' configuration option to true and creates a basic crawler that logs page titles.

LANGUAGE: typescript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';

Configuration.set('systemInfoV2', true);

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Configuring Crawlee using Global Configuration
DESCRIPTION: Example of using the Configuration class to set global configuration options for Crawlee. It demonstrates setting the persistStateIntervalMillis option and using it with a CheerioCrawler.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Get the global configuration
const config = Configuration.getGlobalConfig();
// Set the 'persistStateIntervalMillis' option
// of global configuration to 10 seconds
config.set('persistStateIntervalMillis', 10_000);

// Note, that we are not passing the configuration to the crawler
// as it's using the global configuration
const crawler = new CheerioCrawler();

crawler.router.addDefaultHandler(async ({ request }) => {
    // For the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // For the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Configuring Crawlee using Global Configuration
DESCRIPTION: Example of using the Configuration class to set global configuration options for Crawlee. It demonstrates setting the persistStateIntervalMillis option and using it with a CheerioCrawler.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Get the global configuration
const config = Configuration.getGlobalConfig();
// Set the 'persistStateIntervalMillis' option
// of global configuration to 10 seconds
config.set('persistStateIntervalMillis', 10_000);

// Note, that we are not passing the configuration to the crawler
// as it's using the global configuration
const crawler = new CheerioCrawler();

crawler.router.addDefaultHandler(async ({ request }) => {
    // For the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // For the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Scraping and Parsing Current Price with Playwright
DESCRIPTION: Shows how to locate, extract, and parse the current price of a product using Playwright and string manipulation.

LANGUAGE: javascript
CODE:
const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

----------------------------------------

TITLE: Domain Strategy Configuration
DESCRIPTION: Configuration for including subdomains in the crawl using enqueueLinks strategy option.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    strategy: 'same-domain'
});

----------------------------------------

TITLE: Configuring Crawlee with Custom Configuration
DESCRIPTION: Example of creating a custom Configuration instance and passing it to a CheerioCrawler. It demonstrates setting the persistStateIntervalMillis option and using it with the crawler.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Create new configuration
const config = new Configuration({
    // Set the 'persistStateIntervalMillis' option to 10 seconds
    persistStateIntervalMillis: 10_000,
});

// Now we need to pass the configuration to the crawler
const crawler = new CheerioCrawler({}, config);

crawler.router.addDefaultHandler(async ({ request }) => {
    // for the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // for the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Configuring Crawlee with Custom Configuration
DESCRIPTION: Example of creating a custom Configuration instance and passing it to a CheerioCrawler. It demonstrates setting the persistStateIntervalMillis option and using it with the crawler.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Create new configuration
const config = new Configuration({
    // Set the 'persistStateIntervalMillis' option to 10 seconds
    persistStateIntervalMillis: 10_000,
});

// Now we need to pass the configuration to the crawler
const crawler = new CheerioCrawler({}, config);

crawler.router.addDefaultHandler(async ({ request }) => {
    // for the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // for the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Configuring SessionPool with HttpCrawler in Crawlee
DESCRIPTION: This snippet shows how to configure and use SessionPool with HttpCrawler in Crawlee. It includes setup for proxy configuration and session pool management.

LANGUAGE: js
CODE:
import { HttpCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new HttpCrawler({
    // To use the proxy IP rotation, you must turn the proxy usage on.
    useSessionPool: true,
    persistCookiesPerSession: true,
    proxyConfiguration,
    maxRequestsPerCrawl: 100,
    async requestHandler({ session, sendRequest }) {
        // ...
    },
});

await crawler.run();

----------------------------------------

TITLE: Configuring SessionPool with HttpCrawler in Crawlee
DESCRIPTION: This snippet shows how to configure and use SessionPool with HttpCrawler in Crawlee. It includes setup for proxy configuration and session pool management.

LANGUAGE: js
CODE:
import { HttpCrawler, ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy-1.com',
        'http://proxy-2.com',
    ],
});

const crawler = new HttpCrawler({
    // To use the proxy IP rotation, you must turn the proxy usage on.
    useSessionPool: true,
    persistCookiesPerSession: true,
    proxyConfiguration,
    maxRequestsPerCrawl: 100,
    async requestHandler({ session, sendRequest }) {
        // ...
    },
});

await crawler.run();

----------------------------------------

TITLE: Scraping Product Title with Playwright
DESCRIPTION: Shows how to use Playwright to locate and extract the product title from an HTML element.

LANGUAGE: javascript
CODE:
const title = await page.locator('.product-meta h1').textContent();

----------------------------------------

TITLE: Basic Cheerio Crawler Implementation in TypeScript
DESCRIPTION: A simple crawler implementation that downloads a single page's HTML and extracts its title using Cheerio.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Configuring Apify SDK with API Token
DESCRIPTION: Example of initializing the Apify SDK with an API token using the Configuration instance

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const sdk = new Actor({ token: 'your_api_token' });

----------------------------------------

TITLE: Browser Title Comparison in JSDOM vs Browsers
DESCRIPTION: Demonstrates the difference between accessing page title in browser JavaScript versus JSDOM environment.

LANGUAGE: typescript
CODE:
// Return the page title
document.title; // browsers
window.document.title; // JSDOM

----------------------------------------

TITLE: Crawling Same Hostname Links with CheerioCrawler in Crawlee
DESCRIPTION: This code snippet shows how to use CheerioCrawler to crawl links with the same hostname as the starting URL. It uses the 'SameHostname' enqueue strategy, which is the default behavior for enqueueLinks().

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks }) {
        // Add new links from the page to the crawler's RequestQueue
        // Only links that share the same hostname will be added
        await enqueueLinks();
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Simplified CheerioCrawler Implementation
DESCRIPTION: Shows a simplified way to create and run a CheerioCrawler using the crawler.run() method with direct URL input, bypassing manual RequestQueue setup.

LANGUAGE: typescript
CODE:
// You don't need to import RequestQueue anymore
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

// Start the crawler with the provided URLs
await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Configuring Input File Path in Crawlee Project
DESCRIPTION: Shows the file path structure for providing input to a Crawlee actor through the default key-value store using an INPUT.json file. The file must be placed in the project's storage directory.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json

----------------------------------------

TITLE: Implementing HTTP Crawler in Crawlee
DESCRIPTION: Shows how to use HttpCrawler to crawl URLs from an external file, make HTTP requests, and save the resulting HTML content. The code demonstrates the basic setup and usage of HttpCrawler from the Crawlee framework.

LANGUAGE: javascript
CODE:
import { Dataset, HttpCrawler, log } from 'crawlee';
import { readFileSync } from 'fs';

// Parse the input text file
const urls = readFileSync('./urls.txt', 'utf8')
    .split('\n')
    .map((url) => url.trim())
    .filter((url) => url.length > 0);

// Create the crawler
const crawler = new HttpCrawler({
    // Use default HttpCrawler options
    maxRequestsPerMinute: 60,
    async requestHandler({ request, body }) {
        log.info(`Processing ${request.url}...`);

        // Save the HTML to the default Dataset
        await Dataset.pushData({
            url: request.url,
            html: body.toString(),
        });
    },
});

// Add the URLs to the crawler's request queue
await crawler.run(urls);

----------------------------------------

TITLE: Creating AWS Lambda Handler Function for Crawlee Execution
DESCRIPTION: This code wraps the Crawlee logic in an async handler function, which is the entry point for AWS Lambda. It instantiates a new crawler for each Lambda invocation to maintain statelessness.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new CheerioCrawler({
        requestHandler: router,
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);
};

----------------------------------------

TITLE: Simplified CheerioCrawler Setup in Crawlee
DESCRIPTION: This snippet shows a more concise way to set up a CheerioCrawler. It uses the crawler's implicit RequestQueue and the run method to directly add URLs, simplifying the initialization process.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    }
})

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Configuring Puppeteer Recursive Crawler
DESCRIPTION: Example showing how to set up and execute a recursive website crawl using PuppeteerCrawler. The code is designed to run on the Apify Platform using the apify/actor-node-puppeteer-chrome Docker image.

LANGUAGE: javascript
CODE:
import ApiLink from '@site/src/components/ApiLink';
import CrawlSource from '!!raw-loader!roa-loader!./puppeteer_recursive_crawl.ts';

----------------------------------------

TITLE: Installing ts-node for Crawlee TypeScript Development
DESCRIPTION: Command to install ts-node as a development dependency for running TypeScript code directly during Crawlee project development.

LANGUAGE: shell
CODE:
npm install --save-dev ts-node

----------------------------------------

TITLE: Configuring Headful Mode in Crawlee TypeScript
DESCRIPTION: TypeScript code snippet to enable headful mode in Crawlee, which shows the browser window during scraping.

LANGUAGE: typescript
CODE:
// Uncomment this option to see the browser window.
headless: false

----------------------------------------

TITLE: Complete AWS Lambda Handler Implementation
DESCRIPTION: Final implementation of the AWS Lambda handler function that executes the Crawlee crawler and returns the results.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
        launchContext: {
            launchOptions: {
                executablePath: await aws_chromium.executablePath(),
                args: aws_chromium.args,
                headless: true
            }
        }
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);

    return {
        statusCode: 200,
        body: await crawler.getData(),
    };
}

----------------------------------------

TITLE: Configuring Advanced Autoscaled Pool Options in Crawlee
DESCRIPTION: This snippet demonstrates how to set advanced autoscaled pool options in a CheerioCrawler. It includes settings for desiredConcurrency, scaleUpStepRatio, scaleDownStepRatio, and various interval settings.

LANGUAGE: javascript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    autoscaledPoolOptions: {
        desiredConcurrency: 10,
        desiredConcurrencyRatio: 0.95,
        scaleUpStepRatio: 0.05,
        scaleDownStepRatio: 0.05,
        maybeRunIntervalSecs: 0.5,
        loggingIntervalSecs: 60,
        autoscaleIntervalSecs: 10,
        maxTasksPerMinute: 60,
    },
    // ... other options
});

----------------------------------------

TITLE: Configuring Apify Proxy with Specific Groups and Country
DESCRIPTION: JavaScript code to create Apify Proxy configuration with custom settings.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
    countryCode: 'US',
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Initializing PlaywrightCrawler with Configuration in Node.js
DESCRIPTION: Sets up a basic PlaywrightCrawler instance with persistent storage disabled using Crawlee's Configuration class.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
}, new Configuration({
    persistStorage: false,
}));

await crawler.run(startUrls);

----------------------------------------

TITLE: Initializing RequestQueue and Adding URLs in Crawlee
DESCRIPTION: This snippet demonstrates how to create a RequestQueue instance and add a URL to it using Crawlee. It's a basic setup for crawling a specific website.

LANGUAGE: typescript
CODE:
import { RequestQueue } from 'crawlee';

// First you create the request queue instance.
const requestQueue = await RequestQueue.open();
// And then you add one or more requests to it.
await requestQueue.addRequest({ url: 'https://crawlee.dev' });

----------------------------------------

TITLE: Setting Apify Token in Configuration
DESCRIPTION: JavaScript code to set the Apify API token using the Configuration instance.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const sdk = new Actor({ token: 'your_api_token' });

----------------------------------------

TITLE: Configuring Crawlee using global Configuration
DESCRIPTION: This snippet demonstrates how to use the global Configuration instance to set Crawlee options programmatically. It sets the persistStateIntervalMillis option to 10 seconds.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, Configuration, sleep } from 'crawlee';

// Get the global configuration
const config = Configuration.getGlobalConfig();
// Set the 'persistStateIntervalMillis' option
// of global configuration to 10 seconds
config.set('persistStateIntervalMillis', 10_000);

// Note, that we are not passing the configuration to the crawler
// as it's using the global configuration
const crawler = new CheerioCrawler();

crawler.router.addDefaultHandler(async ({ request }) => {
    // For the first request we wait for 5 seconds,
    // and add the second request to the queue
    if (request.url === 'https://www.example.com/1') {
        await sleep(5_000);
        await crawler.addRequests(['https://www.example.com/2'])
    }
    // For the second request we wait for 10 seconds,
    // and abort the run
    if (request.url === 'https://www.example.com/2') {
        await sleep(10_000);
        process.exit(0);
    }
});

await crawler.run(['https://www.example.com/1']);

----------------------------------------

TITLE: Installing and Logging into Apify CLI
DESCRIPTION: Commands to install Apify CLI globally and log in with API token

LANGUAGE: bash
CODE:
npm install -g apify-cli
apify login -t YOUR_API_TOKEN

----------------------------------------

TITLE: CheerioCrawler Log Output
DESCRIPTION: Log entries showing the execution of a CheerioCrawler instance crawling the crawlee.dev website, including the starting message and successful page title extractions from multiple documentation pages.

LANGUAGE: log
CODE:
INFO  CheerioCrawler: Starting the crawl
INFO  CheerioCrawler: Title of https://crawlee.dev/ is 'Crawlee · Build reliable crawlers. Fast. | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'
INFO  CheerioCrawler: Title of https://crawlee.dev/js/docs/guides is 'Guides | Crawlee'

----------------------------------------

TITLE: Configuring Crawlee for Apify Platform
DESCRIPTION: Modified main script that includes Apify Actor initialization and cleanup. Demonstrates how to integrate Crawlee with Apify Platform storage and services.

LANGUAGE: typescript
CODE:
import { Actor } from 'apify';
import { PlaywrightCrawler, log } from 'crawlee';
import { router } from './routes.mjs';

await Actor.init();

log.setLevel(log.LEVELS.DEBUG);

log.debug('Setting up crawler.');
const crawler = new PlaywrightCrawler({
    requestHandler: router,
});

await crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);

await Actor.exit();

----------------------------------------

TITLE: Installing Apify Dependencies via NPM
DESCRIPTION: Commands for installing the required Apify SDK and CLI tools. The SDK is installed as a project dependency while the CLI is installed globally.

LANGUAGE: bash
CODE:
npm install apify

LANGUAGE: bash
CODE:
npm install -g apify-cli

----------------------------------------

TITLE: Logging into Apify Platform via CLI
DESCRIPTION: Command to log into the Apify Platform using the Apify CLI. Requires a personal access token from the Apify console.

LANGUAGE: bash
CODE:
apify login

----------------------------------------

TITLE: Parsing Product Price
DESCRIPTION: Shows how to extract and parse the product price by filtering elements and converting string to number.

LANGUAGE: javascript
CODE:
const priceElement = page
    .locator('span.price')
    .filter({
        hasText: '$',
    })
    .first();

const currentPriceString = await priceElement.textContent();
const rawPrice = currentPriceString.split('$')[1];
const price = Number(rawPrice.replaceAll(',', ''));

----------------------------------------

TITLE: Customizing Browser Fingerprints with PuppeteerCrawler
DESCRIPTION: This snippet shows how to customize browser fingerprints when using PuppeteerCrawler in Crawlee. It demonstrates specifying browser, operating system, and other parameters for fingerprint generation.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    // ...
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                browsers: [
                    { name: 'firefox', minVersion: 88 },
                    { name: 'chrome', minVersion: 90 },
                ],
                devices: [
                    'desktop',
                ],
                operatingSystems: [
                    'windows',
                ],
            },
        },
    },
});

----------------------------------------

TITLE: Configuring INPUT.json File Path in Crawlee
DESCRIPTION: Shows the file path structure for providing input to Crawlee actors via an INPUT.json file in the default key-value store directory.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json

----------------------------------------

TITLE: Customizing Browser Fingerprints with PuppeteerCrawler
DESCRIPTION: This snippet shows how to customize browser fingerprints using PuppeteerCrawler in Crawlee. It configures specific options for fingerprint generation, including browser, operating system, and screen size.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    browserPoolOptions: {
        fingerprintOptions: {
            fingerprintGeneratorOptions: {
                browsers: ["chrome"],
                devices: ["desktop"],
                operatingSystems: ["windows"],
                locales: ["en-US"],
                // you can specify any options from the Fingerprint-Generator library
            },
        },
    },
    // ... other options
});

----------------------------------------

TITLE: Configuring package.json for Crawlee TypeScript Development
DESCRIPTION: JSON configuration for package.json to set up a development start script using ts-node for a Crawlee TypeScript project.

LANGUAGE: json
CODE:
{
    "scripts": {
        "start:dev": "ts-node-esm -T src/main.ts"
    }
}

----------------------------------------

TITLE: Using a Locking-Enabled RequestQueue in CheerioCrawler
DESCRIPTION: This example illustrates how to use a custom RequestQueue that supports locking with a CheerioCrawler. It includes enabling the request locking experiment and passing the custom queue to the crawler.

LANGUAGE: typescript
CODE:
import { CheerioCrawler, RequestQueueV2 } from 'crawlee';

const queue = await RequestQueueV2.open('my-locking-queue');

const crawler = new CheerioCrawler({
    experiments: {
        requestLocking: true,
    },
    requestQueue: queue,
    async requestHandler({ $, request }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

await crawler.run();

----------------------------------------

TITLE: Purging Default Storages in Crawlee
DESCRIPTION: Demonstrates how to explicitly purge default storages in Crawlee using the purgeDefaultStorages() helper function.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Adapting Category Handler for Parallel Scraping in JavaScript
DESCRIPTION: This code modifies the CATEGORY handler in a Crawlee scraper to enqueue product URLs to a shared request queue for parallel processing. It demonstrates how to adapt existing scraper logic for parallelization.

LANGUAGE: javascript
CODE:
CATEGORY: async ({ $, log, crawler }) => {
    const requestQueue = await getOrInitQueue();
    log.info(`Enqueueing product URLs from category page`);
    const products = $('a.product_pod');
    for (const product of products) {
        const $product = $(product);
        const url = new URL($product.attr('href'), 'https://books.toscrape.com/catalogue/');
        await requestQueue.addRequest({
            url: url.toString(),
            label: 'DETAIL',
        });
    }

    const nextButton = $('li.next a').attr('href');
    if (nextButton) {
        log.info('Enqueueing next page of products');
        await crawler.addRequests([{
            url: new URL(nextButton, 'https://books.toscrape.com/catalogue/').toString(),
            label: 'CATEGORY',
        }]);
    }
}

----------------------------------------

TITLE: Apify CLI Login
DESCRIPTION: Commands to install Apify CLI and log in with an API token

LANGUAGE: bash
CODE:
npm install -g apify-cli
apify login -t YOUR_API_TOKEN

----------------------------------------

TITLE: Adapting Category Route Handler for Parallel Scraping
DESCRIPTION: Modified route handler that enqueues product URLs to a locked request queue instead of processing them immediately

LANGUAGE: javascript
CODE:
import { getOrInitQueue } from './requestQueue.mjs';

----------------------------------------

TITLE: Installing @apify/tsconfig for Crawlee TypeScript Project
DESCRIPTION: Command to install @apify/tsconfig as a development dependency for a Crawlee TypeScript project.

LANGUAGE: shell
CODE:
npm install --save-dev @apify/tsconfig

----------------------------------------

TITLE: Integrating HTTP Server with Crawlee for Web Scraping
DESCRIPTION: Combines the HTTP server and CheerioCrawler to handle incoming requests, perform web scraping, and return results. It uses a RequestQueue to manage scraping tasks and maps HTTP responses to Crawlee Requests.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, RequestQueue, log } from 'crawlee';
import { createServer } from 'http';

const requestQueue = await RequestQueue.open();

const crawler = new CheerioCrawler({
    keepAlive: true,
    requestQueue,
    requestHandler: async ({ request, $, sendResponse }) => {
        const title = $('title').text();
        sendResponse(JSON.stringify({ title }));
    },
});

const server = createServer(async (req, res) => {
    const url = new URL(req.url, `http://${req.headers.host}`);
    const pageUrl = url.searchParams.get('url');

    if (!pageUrl) {
        res.writeHead(400, { 'Content-Type': 'application/json' });
        res.end(JSON.stringify({ error: 'Missing url parameter' }));
        return;
    }

    const sendResponse = (data) => {
        res.writeHead(200, { 'Content-Type': 'application/json' });
        res.end(data);
    };

    await requestQueue.addRequest({
        url: pageUrl,
        uniqueKey: `${Date.now()}`, // To avoid duplicate URL error
        userData: { sendResponse },
    });
});

await crawler.run();

server.listen(3000, () => {
    log.info('Server is listening for user requests');
});

----------------------------------------

TITLE: Creating Apify Proxy Configuration
DESCRIPTION: Set up Apify Proxy with specific proxy groups and country code for improved performance.

LANGUAGE: javascript
CODE:
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
    countryCode: 'US',
});
const proxyUrl = await proxyConfiguration.newUrl();

----------------------------------------

TITLE: Integrating HTTP Server with Crawlee for Web Scraping
DESCRIPTION: Combines the HTTP server and CheerioCrawler to handle incoming requests, scrape page titles, and return results. It uses a RequestQueue to manage scraping tasks and maps HTTP responses to Crawlee Requests.

LANGUAGE: javascript
CODE:
import { CheerioCrawler, RequestQueue, log } from 'crawlee';
import { createServer } from 'http';

const requestQueue = await RequestQueue.open();

const crawler = new CheerioCrawler({
    requestQueue,
    keepAlive: true,
    requestHandler: async ({ request, $, responseHandler }) => {
        const title = $('title').text();
        await responseHandler({ title });
    },
});

const server = createServer(async (req, res) => {
    const url = new URL(req.url, `http://${req.headers.host}`);
    const targetUrl = url.searchParams.get('url');

    if (!targetUrl) {
        res.writeHead(400, { 'Content-Type': 'text/plain' });
        res.end('Missing URL parameter\n');
        return;
    }

    const responsePromise = new Promise((resolve) => {
        void requestQueue.addRequest({
            url: targetUrl,
            userData: {
                responseHandler: resolve,
            },
        });
    });

    const { title } = await responsePromise;

    res.writeHead(200, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({ title }));
});

server.listen(3000, () => {
    log.info('Server is listening for user requests');
});

await crawler.run();

----------------------------------------

TITLE: Adapting Crawlee Routes for Parallel Scraping in JavaScript
DESCRIPTION: Modifies the CATEGORY route handler to enqueue product URLs to a shared request queue, preparing for parallel scraping.

LANGUAGE: javascript
CODE:
import { getOrInitQueue } from './requestQueue.mjs';

// ... other imports and code ...

export const router = createPlaywrightRouter();

router.addHandler('CATEGORY', async ({ request, page, log }) => {
    const requestQueue = await getOrInitQueue();

    const productLinks = await page.$$eval('a.product_pod', (elements) =>
        elements.map((el) => el.href)
    );

    log.info(`Enqueueing ${productLinks.length} products from ${request.url}`);

    for (const url of productLinks) {
        await requestQueue.addRequest({
            url,
            userData: {
                label: 'DETAIL',
            },
        });
    }

    // ... rest of the code ...
});

// ... other route handlers ...

----------------------------------------

TITLE: Configuring Docker Environment for Crawlee Actor
DESCRIPTION: Defines a Docker container setup for running Crawlee actors. Uses apify/actor-node:20 as base image, implements efficient dependency installation with npm, and configures the build process to optimize for layer caching. Includes steps for package installation, dependency verification, and final source code deployment.

LANGUAGE: dockerfile
CODE:
FROM apify/actor-node:20

COPY package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY . ./

CMD npm start --silent

----------------------------------------

TITLE: Installing Playwright-Extra Dependencies
DESCRIPTION: Command to install playwright-extra and puppeteer's stealth plugin using npm package manager

LANGUAGE: bash
CODE:
npm install playwright-extra puppeteer-extra-plugin-stealth

----------------------------------------

TITLE: PuppeteerCrawler Screenshot Capture
DESCRIPTION: Demonstrates screenshot capture in a crawler context using PuppeteerCrawler. Screenshots are captured for multiple pages during the crawling process.

LANGUAGE: javascript
CODE:
import { PuppeteerCrawler } from 'crawlee';

const crawler = new PuppeteerCrawler({
    async requestHandler({ page, request }) {
        const key = (new URL(request.url)).host;
        const screenshot = await page.screenshot();
        await KeyValueStore.setValue(key, screenshot, { contentType: 'image/png' });
    },
});

await crawler.run(['https://crawlee.dev']);

----------------------------------------

TITLE: Implementing Request List with PuppeteerCrawler
DESCRIPTION: Demonstrates how to initialize and use a RequestList with PuppeteerCrawler for processing a predefined set of URLs

LANGUAGE: javascript
CODE:
import { RequestList, PuppeteerCrawler } from 'crawlee';

// Prepare the sources array with URLs to visit
const sources = [
    { url: 'http://www.example.com/page-1' },
    { url: 'http://www.example.com/page-2' },
    { url: 'http://www.example.com/page-3' },
];

// Open the request list.
// List name is used to persist the sources and the list state in the key-value store
const requestList = await RequestList.open('my-list', sources);

// The crawler will automatically process requests from the list
// It's used the same way for Cheerio /Playwright crawlers.
const crawler = new PuppeteerCrawler({
    requestList,
    async requestHandler({ page, request }) {
        // Process the page (extract data, take page screenshot, etc).
        // No more requests could be added to the request list here
    },
});

----------------------------------------

TITLE: Configuring LaunchContext in PuppeteerCrawler
DESCRIPTION: Example of configuring launchContext in PuppeteerCrawler, replacing the old launchPuppeteerOptions.

LANGUAGE: javascript
CODE:
const crawler = new Apify.PuppeteerCrawler({
    launchContext: {
        useChrome: true, // Apify option
        launchOptions: {
            headless: false // Puppeteer option
        }
    }
})

----------------------------------------

TITLE: Checking Node.js Version in Bash
DESCRIPTION: Command to verify the installed version of Node.js.

LANGUAGE: bash
CODE:
node -v

----------------------------------------

TITLE: Installing Apify SDK v1 with Dependencies
DESCRIPTION: Commands for installing Apify SDK v1 with either Puppeteer or Playwright as browser automation dependencies.

LANGUAGE: bash
CODE:
npm install apify puppeteer

LANGUAGE: bash
CODE:
npm install apify playwright

----------------------------------------

TITLE: Accepting and Logging User Input in Crawlee
DESCRIPTION: This code snippet demonstrates how to accept user input in a Crawlee project and log it to the console. It uses the Actor.getInput() method to retrieve the input and console.log() to display it.

LANGUAGE: javascript
CODE:
{AcceptInputSource}

----------------------------------------

TITLE: Request Transform Function
DESCRIPTION: Example of using transformRequestFunction to filter out PDF files and modify requests before they are enqueued.

LANGUAGE: typescript
CODE:
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
    transformRequestFunction(req) {
        // ignore all links ending with `.pdf`
        if (req.url.endsWith('.pdf')) return false;
        return req;
    },
});

----------------------------------------

TITLE: Installing Project Dependencies
DESCRIPTION: Command to install project dependencies using Poetry

LANGUAGE: bash
CODE:
poetry install

----------------------------------------

TITLE: Installing Crawlee Package
DESCRIPTION: Commands for installing Crawlee and its dependencies using npm

LANGUAGE: bash
CODE:
npm install crawlee

npm install @crawlee/cheerio

npm install crawlee playwright
# or npm install @crawlee/playwright playwright

----------------------------------------

TITLE: React Component for Netflix Show Recommender
DESCRIPTION: React component implementing the UI for the Netflix show recommender, including genre selection and show display.

LANGUAGE: jsx
CODE:
import { useState } from "react";
import "./App.css";
import jsonData from "../storage/key_value_stores/default/results.json";

function HeaderAndSelector({ handleChange }) {
  return (
    <>
      <h1 className="header">Netflix Web Show Recommender</h1>
      <div className="genre-selector">
        <select onChange={handleChange} className="select-genre">
          <option value="">Select your genre</option>
          {jsonData[0].genres.map((genres, key) => {
            return (
              <option key={key} value={key}>
                {genres}
              </option>
            );
          })}
        </select>
      </div>
    </>
  );
}

function App() {
  const [count, setCount] = useState(null);

  const handleChange = (event) => {
    const value = event.target.value;
    if (value) setCount(parseInt(value));
  };

  const isValidCount = count !== null && count <= jsonData[0].shows.length;

  return (
    <div className="app-container">
      <HeaderAndSelector handleChange={handleChange} />
      <div className="shows-container">
        {isValidCount && (
          <>
            <div className="shows-list">
              <ul>
                {jsonData[0].shows[count].slice(0, 20).map((show, index) => (
                  <li key={index} className="show-item">
                    {show}
                  </li>
                ))}
              </ul>
            </div>
            <div className="shows-list">
              <ul>
                {jsonData[0].shows[count].slice(20).map((show, index) => (
                  <li key={index} className="show-item">
                    {show}
                  </li>
                ))}
              </ul>
            </div>
          </>
        )}
      </div>
    </div>
  );
}

export default App;

----------------------------------------

TITLE: Storage Cleanup Operations
DESCRIPTION: Demonstrates how to clean up storage directories and purge default storages before starting a new crawl.

LANGUAGE: javascript
CODE:
import { purgeDefaultStorages } from 'crawlee';

await purgeDefaultStorages();

----------------------------------------

TITLE: Configuring Crawlee for proxy rotation
DESCRIPTION: This code snippet shows how to configure Crawlee for proxy rotation using the ProxyConfiguration class. It demonstrates setting up proxy URLs and integrating them with a crawler.

LANGUAGE: javascript
CODE:
import { ProxyConfiguration } from 'crawlee';

const proxyConfiguration = new ProxyConfiguration({
    proxyUrls: [
        'http://proxy1.example.com',
        'http://proxy2.example.com',
    ]
});
const crawler = new CheerioCrawler({
    proxyConfiguration,
    // ...
});

----------------------------------------

TITLE: Specifying Dataset Storage Location in Crawlee
DESCRIPTION: This bash snippet shows the directory path where each item in the default dataset will be saved as a separate file when using Crawlee.

LANGUAGE: bash
CODE:
{PROJECT_FOLDER}/storage/datasets/default/

----------------------------------------

TITLE: Implementing Basic Product Details Extraction
DESCRIPTION: Core function to extract basic product details like title, price, and reviews using Cheerio selectors

LANGUAGE: typescript
CODE:
import { CheerioAPI } from 'cheerio';

type ProductDetails = {
    title: string;
    price: string;
    listPrice: string;
    reviewRating: string;
    reviewCount: string;
};

const SELECTORS = {
    TITLE: 'span#productTitle',
    PRICE: 'span.priceToPay',
    LIST_PRICE: 'span.basisPrice .a-offscreen',
    REVIEW_RATING: '#acrPopover a > span',
    REVIEW_COUNT: '#acrCustomerReviewText',
} as const;

export const extractProductDetails = ($: CheerioAPI): ProductDetails => {
    const title = $(SELECTORS.TITLE).text().trim();
    const price = $(SELECTORS.PRICE).first().text();
    const listPrice = $(SELECTORS.LIST_PRICE).first().text();
    const reviewRating = $(SELECTORS.REVIEW_RATING).first().text();
    const reviewCount = $(SELECTORS.REVIEW_COUNT).first().text();
    return { title, price, listPrice, reviewRating, reviewCount };
};

----------------------------------------

TITLE: Sample Dataset JSON Structure
DESCRIPTION: This JSON array represents a sample dataset structure containing URLs and header counts from scraped web pages. It is stored in the default dataset under the project's storage folder.

LANGUAGE: json
CODE:
[
    {
        "url": "https://crawlee.dev/",
        "headingCount": 11
    },
    {
        "url": "https://crawlee.dev/storage",
        "headingCount": 8
    },
    {
        "url": "https://crawlee.dev/proxy",
        "headingCount": 4
    }
]

----------------------------------------

TITLE: Running Headful Browser with PuppeteerCrawler in JavaScript
DESCRIPTION: Example of running PuppeteerCrawler with a visible browser window for development purposes.

LANGUAGE: javascript
CODE:
const crawler = new PuppeteerCrawler({
    launchContext: {
        launchOptions: {
            headless: false,
        },
    },
    // ... other options
});

----------------------------------------

TITLE: Installing and Zipping Dependencies for AWS Lambda
DESCRIPTION: Commands to install the @sparticuz/chromium package and zip the node_modules folder for use as a Lambda Layer.

LANGUAGE: bash
CODE:
# Install the package
npm i -S @sparticuz/chromium

# Zip the dependencies
zip -r dependencies.zip ./node_modules

----------------------------------------

TITLE: Sample Crawlee result JSON
DESCRIPTION: Example of the JSON output produced by a Crawlee scraper, showing the scraped data structure.

LANGUAGE: json
CODE:
{
    "url": "https://crawlee.dev/",
    "title": "Crawlee · The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee"
}

----------------------------------------

TITLE: Integrating AWS Chromium Configuration
DESCRIPTION: Extended crawler configuration incorporating AWS-specific Chromium settings including executable path and launch arguments.

LANGUAGE: javascript
CODE:
// For more information, see https://crawlee.dev/
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

const crawler = new PlaywrightCrawler({
    requestHandler: router,
    launchContext: {
        launchOptions: {
             executablePath: await aws_chromium.executablePath(),
             args: aws_chromium.args,
             headless: true
        }
    }
}, new Configuration({
    persistStorage: false,
}));

----------------------------------------

TITLE: Manual Installation of Crawlee
DESCRIPTION: NPM commands to manually install Crawlee and its dependencies for different crawler types.

LANGUAGE: bash
CODE:
npm install crawlee

LANGUAGE: bash
CODE:
npm install crawlee playwright

LANGUAGE: bash
CODE:
npm install crawlee puppeteer

----------------------------------------

TITLE: Wrapping Crawlee Code in AWS Lambda Handler Function
DESCRIPTION: Complete JavaScript code for an AWS Lambda function that runs a Crawlee crawler with browser support and returns the crawled data.

LANGUAGE: javascript
CODE:
import { Configuration, PlaywrightCrawler } from 'crawlee';
import { router } from './routes.js';
import aws_chromium from '@sparticuz/chromium';

const startUrls = ['https://crawlee.dev'];

export const handler = async (event, context) => {
    const crawler = new PlaywrightCrawler({
        requestHandler: router,
        launchContext: {
            launchOptions: {
                executablePath: await aws_chromium.executablePath(),
                args: aws_chromium.args,
                headless: true
            }
        }
    }, new Configuration({
        persistStorage: false,
    }));

    await crawler.run(startUrls);

    return {
        statusCode: 200,
        body: await crawler.getData(),
    };
}

----------------------------------------

TITLE: Puppeteer Screenshot with Crawler Utils
DESCRIPTION: Shows how to capture screenshots using the utils.puppeteer.saveSnapshot() utility function, which provides additional context awareness and simplified handling.

LANGUAGE: javascript
CODE:
import { KeyValueStore } from 'crawlee';
import puppeteer from 'puppeteer';

const browser = await puppeteer.launch();
const page = await browser.newPage();
const utils = new PuppeteerUtils({ page });

await page.goto('https://example.com');
await utils.saveSnapshot();

await browser.close();

----------------------------------------

TITLE: Installing Dependencies for Playwright on Arch Linux
DESCRIPTION: Commands to install necessary dependencies for Playwright on Arch Linux, which is not officially supported.

LANGUAGE: shell
CODE:
yay -S libffi7 icu66 libwebp052 flite-unpatched
sudo ln -s /usr/lib/libpcre.so /usr/lib/libpcre.so.3

----------------------------------------

TITLE: Running Tests in Crawlee Project
DESCRIPTION: Command to run tests before starting development work to ensure all tests pass and the setup is correct.

LANGUAGE: shell
CODE:
yarn test

----------------------------------------

TITLE: Installing Playwright Extra Dependencies
DESCRIPTION: Command line instructions for installing the required packages playwright-extra and puppeteer-extra-plugin-stealth via npm

LANGUAGE: bash
CODE:
npm install playwright-extra puppeteer-extra-plugin-stealth

----------------------------------------

TITLE: Reduce Method Result Example
DESCRIPTION: Example output showing the result of reduce operation that calculates the total number of headers across all pages. The result is stored in the key-value store.

LANGUAGE: javascript
CODE:
23

----------------------------------------

TITLE: Exporting Entire Dataset to CSV File in Crawlee using TypeScript
DESCRIPTION: This code snippet demonstrates how to use the 'exportToValue' function from Crawlee to export the entire default dataset to a single CSV file. The exported file is stored in the default key-value store.

LANGUAGE: typescript
CODE:
import { Dataset, KeyValueStore } from 'crawlee';

await Dataset.exportToValue('OUTPUT.csv', {
    format: 'csv',
    fields: ['title', 'url'],
});

const kvStore = await KeyValueStore.open();
const csvContent = await kvStore.getValue('OUTPUT.csv') as string;
console.log(csvContent);

----------------------------------------

TITLE: Fetching HTML Content with Got-Scraping
DESCRIPTION: Demonstrates how to use the got-scraping package to retrieve HTML content from a specified URL. The code shows basic web scraping functionality using JavaScript.

LANGUAGE: javascript
CODE:
import { gotScraping } from 'got-scraping';

const response = await gotScraping('https://example.com');
console.log(response.body);

----------------------------------------

TITLE: Failed Element Selection Error Log
DESCRIPTION: Demonstrates the error message when attempting to access elements before they are rendered in the DOM.

LANGUAGE: log
CODE:
ERROR [...] Error: failed to find element matching selector ".ActorStoreItem"

----------------------------------------

TITLE: Basic CheerioCrawler Implementation for JavaScript Content
DESCRIPTION: Demonstrates a basic attempt to scrape JavaScript-rendered content using CheerioCrawler, which fails because it cannot execute client-side JavaScript.

LANGUAGE: typescript
CODE:
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        // Extract text content of an actor card
        const actorText = $('.ActorStoreItem').text();
        console.log(`ACTOR: ${actorText}`);
    }
})

await crawler.run(['https://apify.com/store']);

----------------------------------------

TITLE: Checking Product Stock Availability with Playwright
DESCRIPTION: This snippet uses Playwright to check if a product is in stock by looking for a specific element on the page.

LANGUAGE: javascript
CODE:
const inStockElement = await page
    .locator('span.product-form__inventory')
    .filter({
        hasText: 'In stock',
    })
    .first();

const inStock = (await inStockElement.count()) > 0;