TITLE: Implementing Simple Calculator Tool in Python
DESCRIPTION: Creates a basic calculator tool that can perform addition, subtraction, multiplication, and division operations. Uses the @tool decorator to make it available to DB-GPT agents.

LANGUAGE: python
CODE:
from dbgpt.agent.resource import tool

@tool
def simple_calculator(first_number: int, second_number: int, operator: str) -> float:
    """Simple calculator tool. Just support +, -, *, /."""
    if isinstance(first_number, str):
        first_number = int(first_number)
    if isinstance(second_number, str):
        second_number = int(second_number)
    if operator == "+":
        return first_number + second_number
    elif operator == "-":
        return first_number - second_number
    elif operator == "*":
        return first_number * second_number
    elif operator == "/":
        return first_number / second_number
    else:
        raise ValueError(f"Invalid operator: {operator}")

----------------------------------------

TITLE: Listing All Trace Information in DB-GPT
DESCRIPTION: This command lists all trace information in DB-GPT, including trace IDs, span IDs, operation names, and conversation UIDs.

LANGUAGE: python
CODE:
dbgpt trace list

----------------------------------------

TITLE: Fine-tuning CodeLlama Model
DESCRIPTION: Configures and initiates fine-tuning of the CodeLlama-13b model using LoRA. Specifies model parameters, training settings, and output directories.

LANGUAGE: python
CODE:
train_args = {
            "model_name_or_path": "codellama/CodeLlama-13b-Instruct-hf",
            "do_train": True,
            "dataset": "example_text2sql_train",
            "max_source_length": 2048,
            "max_target_length": 512,
            "finetuning_type": "lora",
            "lora_target": "q_proj,v_proj",
            "template": "llama2",
            "lora_rank": 64,
            "lora_alpha": 32,
            "output_dir": "dbgpt_hub/output/adapter/CodeLlama-13b-sql-lora",
            "overwrite_cache": True,
            "overwrite_output_dir": True,
            "per_device_train_batch_size": 1,
            "gradient_accumulation_steps": 16,
            "lr_scheduler_type": "cosine_with_restarts",
            "logging_steps": 50,
            "save_steps": 2000,
            "learning_rate": 2e-4,
            "num_train_epochs": 8,
            "plot_loss": True,
            "bf16": True,
}

start_sft(train_args)

----------------------------------------

TITLE: Initializing Custom Summarizer Agent Class
DESCRIPTION: Defines the basic structure of a custom summarizer agent by inheriting from ConversableAgent.

LANGUAGE: python
CODE:
from dbgpt.agent import ConversableAgent

class MySummarizerAgent(ConversableAgent):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

----------------------------------------

TITLE: Implementing Multi-Round Chat with LLM using Python and AWEL
DESCRIPTION: Creates a DAG-based chat system using AWEL library with OpenAI integration. Implements a chat composer, LLM operator, and message handling with conversation history management. Uses in-memory storage for maintaining chat context and supports multiple conversation rounds.

LANGUAGE: python
CODE:
import asyncio
from dbgpt.core.awel import DAG, MapOperator, BaseOperator
from dbgpt.core import (
    ChatPromptTemplate,
    HumanPromptTemplate,
    InMemoryStorage,
    MessagesPlaceholder,
    ModelRequestContext,
    SystemPromptTemplate,
)
from dbgpt.core.operators import (
    ChatComposerInput,
    ChatHistoryPromptComposerOperator,
)
from dbgpt.model.proxy import OpenAILLMClient
from dbgpt.model.operators import LLMOperator

with DAG("multi_round_chat_with_lll_dag") as dag:
    prompt = ChatPromptTemplate(
        messages=[
            SystemPromptTemplate.from_template("You are a helpful chatbot."),
            MessagesPlaceholder(variable_name="chat_history"),
            HumanPromptTemplate.from_template("{user_input}"),
        ]
    )

    composer_operator = ChatHistoryPromptComposerOperator(
        prompt_template=prompt,
        keep_end_rounds=5,
        storage=InMemoryStorage(),
        message_storage=InMemoryStorage(),
    )
    
    input_task = MapOperator(
        lambda req: ChatComposerInput(
            context=ModelRequestContext(conv_uid=req["conv_uid"]),
            prompt_dict={"user_input": req["user_input"]},
            model_dict={"model": "gpt-3.5-turbo"},
        )
    )

    # Use LLMOperator to generate response.
    llm_task = LLMOperator(task_name="llm_task", llm_client=OpenAILLMClient())
    out_parse_task = MapOperator(lambda out: out.text)
    
    input_task >> composer_operator >> llm_task >> out_parse_task


async def main(task: BaseOperator):
    conv_uid = "conv_1234"
    first_user_input = "Who is elon musk?"
    second_user_input = "Is he rich?"
    
    print(f"First round\nUser: {first_user_input}")
    first_ai_response = await task.call({"conv_uid": conv_uid, "user_input": first_user_input})
    print(f"AI: {first_ai_response}")
    
    print(f"\nSecond round\nUser: {second_user_input}")
    second_ai_response = await task.call({"conv_uid": conv_uid, "user_input": second_user_input})
    print(f"AI: {second_ai_response}")

asyncio.run(main(out_parse_task))

----------------------------------------

TITLE: Generating Predictions with Fine-tuned Model
DESCRIPTION: Configures and initiates prediction using the fine-tuned CodeLlama model. Specifies input file, output directory, and model parameters.

LANGUAGE: python
CODE:
predict_args = {
            "model_name_or_path": "codellama/CodeLlama-13b-Instruct-hf",
            "template": "llama2",
            "finetuning_type": "lora",
            "checkpoint_dir": "dbgpt_hub/output/adapter/CodeLlama-13b-sql-lora",
            "predict_file_path": "dbgpt_hub/data/eval_data/dev_sql.json",
            "predict_out_dir": "dbgpt_hub/output/",
            "predicted_out_filename": "pred_sql.sql",
}
start_predict(predict_args)

----------------------------------------

TITLE: Listing Available Models via cURL
DESCRIPTION: cURL command to retrieve list of available models from the API server

LANGUAGE: bash
CODE:
curl http://127.0.0.1:8100/api/v1/models \\n-H "Authorization: Bearer EMPTY" \\n-H "Content-Type: application/json"

----------------------------------------

TITLE: Running DB-GPT with GPU Support
DESCRIPTION: Docker command for running DB-GPT with GPU support, mounting local models and configuration.

LANGUAGE: bash
CODE:
docker run --ipc host --gpus all \
  -it --rm \
  -p 5670:5670 \
  -v ./dbgpt-local-gpu.toml:/app/configs/dbgpt-local-gpu.toml \
  -v ./models:/app/models \
  --name dbgpt \
  eosphorosai/dbgpt \
  dbgpt start webserver --config /app/configs/dbgpt-local-gpu.toml

----------------------------------------

TITLE: Implementing an AI Agent with Database Integration in Python
DESCRIPTION: This comprehensive example demonstrates how to set up an AI agent that can interact with a database. It includes creating an OpenAI LLM client, setting up the agent context and memory, and initiating a chat to query the database for users under 18 years old.

LANGUAGE: python
CODE:
import asyncio
import os
from dbgpt.agent import AgentContext, AgentMemory, LLMConfig, UserProxyAgent
from dbgpt.agent.expand.data_scientist_agent import DataScientistAgent
from dbgpt.model.proxy import OpenAILLMClient

async def main():

    llm_client = OpenAILLMClient(
        model_alias="gpt-3.5-turbo",  # or other models, eg. "gpt-4o"
        api_base=os.getenv("OPENAI_API_BASE"),
        api_key=os.getenv("OPENAI_API_KEY"),
    )
    context: AgentContext = AgentContext(
        conv_id="test123", language="en", temperature=0.5, max_new_tokens=2048
    )
    agent_memory = AgentMemory()

    user_proxy = await UserProxyAgent().bind(agent_memory).bind(context).build()

    sql_boy = (
        await DataScientistAgent()
        .bind(context)
        .bind(LLMConfig(llm_client=llm_client))
        .bind(db_resource)
        .bind(agent_memory)
        .build()
    )

    await user_proxy.initiate_chat(
        recipient=sql_boy,
        reviewer=user_proxy,
        message="What is the name and age of the user with age less than 18",
    )

    ## dbgpt-vis message infos
    print(await agent_memory.gpts_memory.one_chat_completions("test123"))


if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Implementing Calculator Agent with DB-GPT
DESCRIPTION: Python implementation showing how to create a calculator using DB-GPT agents. The code sets up a code assistant agent and user proxy agent, configures them with OpenAI's LLM client, and demonstrates performing a multiplication calculation.

LANGUAGE: python
CODE:
import asyncio

from dbgpt.agent import AgentContext, AgentMemory, LLMConfig, UserProxyAgent
from dbgpt.agent.expand.code_assistant_agent import CodeAssistantAgent
from dbgpt.model.proxy import OpenAILLMClient


async def main():
    llm_client = OpenAILLMClient(model_alias="gpt-3.5-turbo")
    context: AgentContext = AgentContext(conv_id="test123")
    # Create an agent memory, default memory is ShortTermMemory
    agent_memory: AgentMemory = AgentMemory()

    # Create a code assistant agent
    coder = (
        await CodeAssistantAgent()
        .bind(context)
        .bind(LLMConfig(llm_client=llm_client))
        .bind(agent_memory)
        .build()
    )

    # Create a user proxy agent
    user_proxy = await UserProxyAgent().bind(context).bind(agent_memory).build()

    # Initiate a chat with the user proxy agent
    await user_proxy.initiate_chat(
        recipient=coder,
        reviewer=user_proxy,
        message="calculate the result of 321 * 123",
    )
    # Obtain conversation history messages between agents
    print(await agent_memory.gpts_memory.one_chat_completions("test123"))


if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Creating Agent Profile Using ProfileConfig in Python
DESCRIPTION: Demonstrates how to create an agent profile using the ProfileConfig class with parameters like name, role, goal, constraints, and description.

LANGUAGE: python
CODE:
from dbgpt.agent import ProfileConfig

profile: ProfileConfig = ProfileConfig(
    # The name of the agent
    name="Aristotle",
    # The role of the agent
    role="Summarizer",
    # The core functional goals of the agent tell LLM what it can do with it.
    goal=(
        "Summarize answer summaries based on user questions from provided "
        "resource information or from historical conversation memories."
    ),
    # Constraints of the agent
    constraints=[
        "Prioritize the summary of answers to user questions from the improved "
        "resource text. If no relevant information is found, summarize it from "
        "the historical dialogue memory given. It is forbidden to make up your "
        "own.",
        "You need to first detect user's question that you need to answer with "
        "your summarization.",
        "Extract the provided text content used for summarization.",
        "Then you need to summarize the extracted text content.",
        "Output the content of summarization ONLY related to user's question. "
        "The output language must be the same to user's question language.",
        "If you think the provided text content is not related to user "
        "questions at all, ONLY output 'Did not find the information you "
        "want.'!!.",
    ],
    # Introduction and description of the agent, used for task assignment and display.
    # If it is empty, the goal content will be used.
    desc=(
        "You can summarize provided text content according to user's questions"
        " and output the summarization."
    ),
    expand_prompt="Keep your answer concise",
    # Some examples in your prompt
    examples=""
)

----------------------------------------

TITLE: AWEL DSL Workflow Definition
DESCRIPTION: Example of using AWEL's DSL layer to define a RAG workflow using a SQL-like syntax. Includes HTTP request handling, embedding processing, data retrieval, and LLM interaction.

LANGUAGE: python
CODE:
CREATE WORKFLOW RAG AS
BEGIN
    DATA requestData = RECEIVE REQUEST FROM 
    		http_source("/examples/rags", method = "post");
        
    DATA processedData = TRANSFORM requestData USING embedding(model = "text2vec");
    DATA retrievedData = RETRIEVE DATA 
    		FROM vstore(database = "chromadb", key = processedData)
    		ON ERROR FAIL;
        
    DATA modelResult = APPLY LLM "vicuna-13b" 
    		WITH DATA retrievedData AND PARAMETERS (temperature = 0.7)
    		ON ERROR RETRY 2 TIMES;
        
    RESPOND TO http_source WITH modelResult
    		ON ERROR LOG "Failed to respond to request";
END;

----------------------------------------

TITLE: Memory Reading and Prompt Building Example in Python
DESCRIPTION: Comprehensive example demonstrating how to read memories and build prompts in an agent context, including custom templates and OpenAI integration.

LANGUAGE: python
CODE:
import os
import asyncio
from dbgpt.agent import (
    AgentContext,
    ShortTermMemory,
    AgentMemory,
    ConversableAgent,
    ProfileConfig,
    LLMConfig,
    BlankAction,
    UserProxyAgent,
)
from dbgpt.model.proxy import OpenAILLMClient

llm_client = OpenAILLMClient(
    model_alias="gpt-4o",
    api_base=os.getenv("OPENAI_API_BASE"),
    api_key=os.getenv("OPENAI_API_KEY"),
)

context: AgentContext = AgentContext(
    conv_id="test123",
    language="en",
    temperature=0.9,
    max_new_tokens=2048,
    verbose=True,
)

memory = ShortTermMemory(buffer_size=2)
agent_memory: AgentMemory = AgentMemory(memory=memory)

user_prompt_template = """\
{% if most_recent_memories %}\
Most recent observations:
{{ most_recent_memories }}
{% endif %}\

{% if question %}\
Question: {{ question }}
{% endif %}
"""

write_memory_template = """\
{% if question %}user: {{ question }} {% endif %}
{% if thought %}assistant: {{ thought }} {% endif %}\
"""

async def main():
    joy_profile = ProfileConfig(
        name="Joy",
        role="Comedians",
        user_prompt_template=user_prompt_template,
        write_memory_template=write_memory_template,
    )
    joy = (
        await ConversableAgent(profile=joy_profile)
        .bind(context)
        .bind(LLMConfig(llm_client=llm_client))
        .bind(agent_memory)
        .bind(BlankAction)
        .build()
    )
    user_proxy = await UserProxyAgent().bind(agent_memory).bind(context).build()
    await user_proxy.initiate_chat(
        recipient=joy,
        reviewer=user_proxy,
        message="My name is bob, please tell me a joke",
    )

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Starting DB-GPT Webserver
DESCRIPTION: Commands for starting the DB-GPT webserver with different configuration files

LANGUAGE: bash
CODE:
uv run dbgpt start webserver --config configs/dbgpt-proxy-openai.toml

LANGUAGE: bash
CODE:
uv run python packages/dbgpt-app/src/dbgpt_app/dbgpt_server.py --config configs/dbgpt-proxy-openai.toml

----------------------------------------

TITLE: Implementing WrappedAWELLayoutManager for Agent Planning in Python
DESCRIPTION: This code snippet demonstrates how to set up and use WrappedAWELLayoutManager in DB-GPT. It initializes various agents, including UserProxyAgent, ToolAssistantAgent, and SummaryAssistantAgent, and manages their execution sequence. The example shows how to query the weather in Beijing using these agents.

LANGUAGE: python
CODE:
import asyncio
import os

from dbgpt.agent import (
    AgentContext,
    AgentMemory,
    LLMConfig,
    UserProxyAgent,
    WrappedAWELLayoutManager,
)
from dbgpt.agent.expand.resources.search_tool import baidu_search
from dbgpt.agent.expand.summary_assistant_agent import SummaryAssistantAgent
from dbgpt.agent.expand.tool_assistant_agent import ToolAssistantAgent
from dbgpt.agent.resource import ToolPack
from dbgpt.model.proxy import OpenAILLMClient


async def main():
    llm_client = OpenAILLMClient(
        model_alias="gpt-4o",
        api_base=os.getenv("OPENAI_API_BASE"),
        api_key=os.getenv("OPENAI_API_KEY"),
    )
    context: AgentContext = AgentContext(
        conv_id="test123", language="en", temperature=0.5, max_new_tokens=2048
    )
    agent_memory = AgentMemory()

    user_proxy = await UserProxyAgent().bind(agent_memory).bind(context).build()

    tools = ToolPack([baidu_search])
    tool_engineer = (
        await ToolAssistantAgent()
        .bind(context)
        .bind(LLMConfig(llm_client=llm_client))
        .bind(agent_memory)
        .bind(tools)
        .build()
    )
    summarizer = (
        await SummaryAssistantAgent()
        .bind(context)
        .bind(agent_memory)
        .bind(LLMConfig(llm_client=llm_client))
        .build()
    )

    manager = (
        await WrappedAWELLayoutManager()
        .bind(context)
        .bind(agent_memory)
        .bind(LLMConfig(llm_client=llm_client))
        .build()
    )
    manager.hire([tool_engineer, summarizer])

    await user_proxy.initiate_chat(
        recipient=manager,
        reviewer=user_proxy,
        message="Query the weather in Beijing",
    )


if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Defining Agent Profile and Constraints
DESCRIPTION: Sets up the agent's profile including name, role, goals, and operational constraints.

LANGUAGE: python
CODE:
from dbgpt.agent import ConversableAgent, ProfileConfig

class MySummarizerAgent(ConversableAgent):
    profile: ProfileConfig = ProfileConfig(
        name="Aristotle",
        role="Summarizer",
        goal=("Summarize answer summaries based on user questions from provided "
              "resource information or from historical conversation memories."),
        desc=("You can summarize provided text content according to user's questions"
              " and output the summarization."),
        constraints=[
            "Prioritize the summary of answers to user questions from the improved resource"
            " text. If no relevant information is found, summarize it from the historical "
            "dialogue memory given. It is forbidden to make up your own.",
            "You need to first detect user's question that you need to answer with your"
            " summarization.",
            "Extract the provided text content used for summarization.",
            "Then you need to summarize the extracted text content.",
            "Output the content of summarization ONLY related to user's question. The "
            "output language must be the same to user's question language.",
            "If you think the provided text content is not related to user questions at "
            "all, ONLY output '{{ not_related_message }}'!!.",
        ]
    )
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

----------------------------------------

TITLE: Implementing AWEL DAG for SQL Generation
DESCRIPTION: Python implementation of a DAG workflow using AWEL to generate SQL queries via OpenAI's LLM. The workflow includes prompt building, request configuration, and LLM interaction steps.

LANGUAGE: python
CODE:
import asyncio
from dbgpt.core.awel import DAG
from dbgpt.core.operators import (
    PromptBuilderOperator,
    RequestBuilderOperator,
)
from dbgpt.model.proxy import OpenAILLMClient
from dbgpt.model.operators import LLMOperator

with DAG("simple_sdk_llm_example_dag") as dag:
    prompt_task = PromptBuilderOperator(
        "Write a SQL of {dialect} to query all data of {table_name}."
    )
    model_pre_handle_task = RequestBuilderOperator(model="gpt-3.5-turbo")
    llm_task = LLMOperator(OpenAILLMClient())
    prompt_task >> model_pre_handle_task >> llm_task
    
output = asyncio.run(
    llm_task.call({
        "dialect": "MySQL", 
        "table_name": "users"
    }
))
print(output)

----------------------------------------

TITLE: Creating Basic Agent Profile with Default Templates in Python
DESCRIPTION: Demonstrates how to create a basic agent profile configuration using ProfileConfig class with default templates. The profile includes agent name, role, goals and description.

LANGUAGE: python
CODE:
from dbgpt.agent import ProfileConfig
profile: ProfileConfig = ProfileConfig(
    name="Aristotle",
    role="Summarizer",
    goal=(
        "Summarize answer summaries based on user questions from provided "
        "resource information or from historical conversation memories."
    ),
    desc=(
        "You can summarize provided text content according to user's questions"
        " and output the summarization."
    ),
)

real_profile = profile.create_profile()

print(f"System Prompt Template: \n{real_profile.get_system_prompt_template()}")
print("#" * 50)
print(f"User Prompt Template: \n{real_profile.get_user_prompt_template()}")

----------------------------------------

TITLE: Creating a Simple AWEL DAG in Python
DESCRIPTION: This snippet demonstrates how to create a basic Directed Acyclic Graph (DAG) using AWEL in Python. It includes an InputOperator and a MapOperator to process and print a greeting message.

LANGUAGE: python
CODE:
import asyncio

from dbgpt.core.awel import DAG, MapOperator, InputOperator, SimpleCallDataInputSource

with DAG("awel_hello_world") as dag:
    input_task = InputOperator(
        input_source=SimpleCallDataInputSource()
    )
    task = MapOperator(map_function=lambda x: print(f"Hello, {x}!"))
    input_task >> task

dag.visualize_dag()
asyncio.run(task.call(call_data="world"))

----------------------------------------

TITLE: Using the Custom Summarizer Agent
DESCRIPTION: Provides a complete example of how to initialize and use the custom summarizer agent with OpenAI's LLM client.

LANGUAGE: python
CODE:
import asyncio

from dbgpt.agent import AgentContext, ConversableAgent, AgentMemory, LLMConfig, UserProxyAgent
from dbgpt.model.proxy import OpenAILLMClient

# ... (Previous class definitions)

async def main():
    llm_client = OpenAILLMClient(model_alias="gpt-3.5-turbo")
    context: AgentContext = AgentContext(conv_id="summarize")

    agent_memory: AgentMemory = AgentMemory()

    summarizer = (
        await MySummarizerAgent()
        .bind(context)
        .bind(LLMConfig(llm_client=llm_client))
        .bind(agent_memory)
        .build()
    )

    user_proxy = await UserProxyAgent().bind(agent_memory).bind(context).build()
  
    await user_proxy.initiate_chat(
        recipient=summarizer,
        reviewer=user_proxy,
        message="""I want to summarize advantages of Nuclear Power according to the following content.
            Nuclear power in space is the use of nuclear power in outer space, typically either small fission systems or radioactive decay for electricity or heat. Another use is for scientific observation, as in a Mössbauer spectrometer. The most common type is a radioisotope thermoelectric generator, which has been used on many space probes and on crewed lunar missions. Small fission reactors for Earth observation satellites, such as the TOPAZ nuclear reactor, have also been flown.[1] A radioisotope heater unit is powered by radioactive decay and can keep components from becoming too cold to function, potentially over a span of decades.[2]
            # ... (rest of the content)
            """,
    )
    print(await agent_memory.gpts_memory.one_chat_completions("summarize"))

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Implementing Result Correctness Check
DESCRIPTION: Adds a method to verify the correctness of the summarization results using LLM verification.

LANGUAGE: python
CODE:
from typing import Tuple, Optional
from dbgpt.agent import ConversableAgent, AgentMessage
from dbgpt.core import ModelMessageRoleType

CHECK_RESULT_SYSTEM_MESSAGE = (
    "You are an expert in analyzing the results of a summary task."
    "Your responsibility is to check whether the summary results can summarize the "
    "input provided by the user, and then make a judgment. You need to answer "
    "according to the following rules:\n"
    "    Rule 1: If you think the summary results can summarize the input provided"
    " by the user, only return True.\n"
    "    Rule 2: If you think the summary results can NOT summarize the input "
    "provided by the user, return False and the reason, split by | and ended "
    "by TERMINATE. For instance: False|Some important concepts in the input are "
    "not summarized. TERMINATE"
)

class MySummarizerAgent(ConversableAgent):
    # ... previous code ...

    async def correctness_check(
        self, message: AgentMessage
    ) -> Tuple[bool, Optional[str]]:
        current_goal = message.current_goal
        action_report = message.action_report
        task_result = ""
        if action_report:
            task_result = action_report.get("content", "")

        check_result, model = await self.thinking(
            messages=[
                AgentMessage(
                    role=ModelMessageRoleType.HUMAN,
                    content=(
                        "Please understand the following user input and summary results"
                        " and give your judgment:\n"
                        f"User Input: {current_goal}\n"
                        f"Summary Results: {task_result}"
                    ),
                )
            ],
            prompt=CHECK_RESULT_SYSTEM_MESSAGE,
        )
        
        fail_reason = ""
        if check_result and (
            "true" in check_result.lower() or "yes" in check_result.lower()
        ):
            success = True
        else:
            success = False
            try:
                _, fail_reason = check_result.split("|")
                fail_reason = (
                    "The summary results cannot summarize the user input due"
                    f" to: {fail_reason}. Please re-understand and complete the summary"
                    " task."
                )
            except Exception:
                fail_reason = (
                    "The summary results cannot summarize the user input. "
                    "Please re-understand and complete the summary task."
                )
        return success, fail_reason

----------------------------------------

TITLE: Non-Streaming Chat Completion with DB-GPT API using Python
DESCRIPTION: This code snippet demonstrates how to use the DB-GPT client library in Python to make a non-streaming chat completion request. It shows the asynchronous API call and how to retrieve the complete response.

LANGUAGE: python
CODE:
from dbgpt_client import Client

DBGPT_API_KEY = "dbgpt"
client = Client(api_key=DBGPT_API_KEY)
response = await client.chat(model="gpt-4o" ,messages="hello")

----------------------------------------

TITLE: Implementing Agent with Tools Integration in Python
DESCRIPTION: Shows complete implementation of an agent using tools with OpenAI LLM integration, including context setup and async execution.

LANGUAGE: python
CODE:
import asyncio
import os
from dbgpt.agent import AgentContext, AgentMemory, LLMConfig, UserProxyAgent
from dbgpt.agent.expand.tool_assistant_agent import ToolAssistantAgent
from dbgpt.model.proxy import OpenAILLMClient

async def main():

    llm_client = OpenAILLMClient(
        model_alias="gpt-3.5-turbo",  # or other models, eg. "gpt-4o"
        api_base=os.getenv("OPENAI_API_BASE"),
        api_key=os.getenv("OPENAI_API_KEY"),
    )
    context: AgentContext = AgentContext(
        conv_id="test123", language="en", temperature=0.5, max_new_tokens=2048
    )
    agent_memory = AgentMemory()

    user_proxy = await UserProxyAgent().bind(agent_memory).bind(context).build()

    tool_man = (
        await ToolAssistantAgent()
        .bind(context)
        .bind(LLMConfig(llm_client=llm_client))
        .bind(agent_memory)
        .bind(tools)
        .build()
    )

    await user_proxy.initiate_chat(
        recipient=tool_man,
        reviewer=user_proxy,
        message="Calculate the product of 10 and 99",
    )

    await user_proxy.initiate_chat(
        recipient=tool_man,
        reviewer=user_proxy,
        message="Count the number of files in /tmp",
    )

    # dbgpt-vis message infos
    print(await agent_memory.gpts_memory.one_chat_completions("test123"))
    
if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Basic Hello World DAG Implementation
DESCRIPTION: Simple implementation of a DAG with a single MapOperator that prints a hello world message

LANGUAGE: python
CODE:
from dbgpt.core.awel import DAG, MapOperator

with DAG("awel_hello_world") as dag:
    task = MapOperator(map_function=lambda x: print(f"Hello, {x}!"))
task._blocking_call(call_data="world")

----------------------------------------

TITLE: Streaming Chat Completion with DB-GPT API using Python
DESCRIPTION: This snippet demonstrates how to use the DB-GPT client library in Python to initiate a streaming chat completion. It shows the asynchronous API call and how to process the streamed response.

LANGUAGE: python
CODE:
from dbgpt_client import Client

DBGPT_API_KEY = "dbgpt"
client = Client(api_key=DBGPT_API_KEY)

async for data in client.chat_stream(
    model="gpt-4o",
    messages="hello",
):
    print(data)

----------------------------------------

TITLE: AgentFream Workflow Implementation
DESCRIPTION: Demonstrates using the AgentFream API to create a text processing and model interaction pipeline. Includes text vectorization, filtering, LLM processing, and result handling.

LANGUAGE: python
CODE:
af = AgentFream(HttpSource("/examples/run_code", method = "post"))
result = (
    af
    .text2vec(model="text2vec")
    .filter(vstore, store = "chromadb", db="default")
    .llm(model="vicuna-13b", temperature=0.7)
    .map(code_parse_func)
    .map(run_sql_func)
    .reduce(lambda a, b: a + b)
)
result.write_to_sink(type='source_slink')

----------------------------------------

TITLE: OpenAI SDK Integration Example
DESCRIPTION: Python code demonstrating how to use OpenAI SDK to interact with DB-GPT API for chat completion

LANGUAGE: python
CODE:
import openai\\nopenai.api_key = "EMPTY"\\nopenai.api_base = "http://127.0.0.1:8100/api/v1"\\nmodel = "glm-4-9b-chat"\\n\\ncompletion = openai.ChatCompletion.create(\\n  model=model,\\n  messages=[{"role": "user", "content": "hello"}]\\n)\\n# print the completion\\nprint(completion.choices[0].message.content)

----------------------------------------

TITLE: Implementing Dynamic Parameter Initialization
DESCRIPTION: Overrides the _init_reply_message method to handle dynamic parameters in the agent's prompt template.

LANGUAGE: python
CODE:
from dbgpt.agent import AgentMessage, ConversableAgent, ProfileConfig

NOT_RELATED_MESSAGE = "Did not find the information you want."

class MySummarizerAgent(ConversableAgent):
    # ... previous code ...

    def _init_reply_message(self, received_message: AgentMessage) -> AgentMessage:
        reply_message = super()._init_reply_message(received_message)
        reply_message.context = {"not_related_message": NOT_RELATED_MESSAGE}
        return reply_message

----------------------------------------

TITLE: Setting Up Automated Planning for Database Analysis in Python
DESCRIPTION: This snippet demonstrates how to set up an AutoPlanChatManager with a DataScientistAgent to automatically plan and execute a database analysis task. It uses OpenAI's API for language model interactions.

LANGUAGE: python
CODE:
import asyncio
import os

from dbgpt.agent import (
    AgentContext,
    AgentMemory,
    AutoPlanChatManager,
    LLMConfig,
    UserProxyAgent,
)
from dbgpt.agent.expand.data_scientist_agent import DataScientistAgent 
from dbgpt.model.proxy import OpenAILLMClient

async def main():
    llm_client = OpenAILLMClient(
        model_alias="gpt-3.5-turbo",
        api_base=os.getenv("OPENAI_API_BASE"),
        api_key=os.getenv("OPENAI_API_KEY"),
    )
    context: AgentContext = AgentContext(
        conv_id="test123", language="en", temperature=0.5, max_new_tokens=2048
    )
    agent_memory = AgentMemory()

    user_proxy = await UserProxyAgent().bind(agent_memory).bind(context).build()

    sql_boy = (
        await DataScientistAgent()
        .bind(context)
        .bind(LLMConfig(llm_client=llm_client))
        .bind(db_resource)
        .bind(agent_memory)
        .build()
    )
    manager = (
        await AutoPlanChatManager()
        .bind(context)
        .bind(agent_memory)
        .bind(LLMConfig(llm_client=llm_client))
        .build()
    )
    manager.hire([sql_boy])

    await user_proxy.initiate_chat(
        recipient=manager,
        reviewer=user_proxy,
        message="Analyze student scores from at least three dimensions",
    )

    print(await agent_memory.gpts_memory.one_chat_completions("test123"))

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Initializing Custom Summarizer Agent with Profile in Python
DESCRIPTION: Example of creating a custom agent class that extends ConversableAgent with a profile configuration defining the agent's name, role, goals and description.

LANGUAGE: python
CODE:
from dbgpt.agent import ConversableAgent, ProfileConfig

class MySummarizerAgent(ConversableAgent):
    profile: ProfileConfig = ProfileConfig(
        # The name of the agent
        name="Aristotle",
        # The role of the agent
        role="Summarizer",
        # The core functional goals of the agent tell LLM what it can do with it.
        goal=(
            "Summarize answer summaries based on user questions from provided "
            "resource information or from historical conversation memories."
        ),
        # Introduction and description of the agent, used for task assignment and display. 
        # If it is empty, the goal content will be used.
        desc=(
            "You can summarize provided text content according to user's questions"
            " and output the summarization."
        ),
    )
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

----------------------------------------

TITLE: Creating Dynamic Profile Configuration in Python
DESCRIPTION: Demonstrates how to create a ProfileConfig with dynamic fields using DynConfig. Shows configuration of a dynamic 'name' field that can be modified through environment variables.

LANGUAGE: python
CODE:
from dbgpt.agent import ProfileConfig, DynConfig

profile: ProfileConfig = ProfileConfig(
    # The name of the agent
    name=DynConfig(
        "Aristotle",
       key="summary_profile_name",
       provider="env"
    ),
    # The role of the agent
    role="Summarizer",
)

----------------------------------------

TITLE: Creating a Database Resource for AI Agent in Python
DESCRIPTION: This code creates a database resource using the RDBMSConnectorResource class, which can be used by AI agents to interact with the database. It requires a previously created database connector.

LANGUAGE: python
CODE:
from dbgpt.agent.resource import RDBMSConnectorResource

db_resource = RDBMSConnectorResource("user_manager", connector=connector)

----------------------------------------

TITLE: Creating a Function Tool with @tool Decorator in Python
DESCRIPTION: Demonstrates how to create a simple calculator tool using the @tool decorator. The function takes two numbers and an operator as input and performs the specified arithmetic operation.

LANGUAGE: python
CODE:
from dbgpt.agent.resource import tool

@tool
def simple_calculator(first_number: int, second_number: int, operator: str) -> float:
    """Simple calculator tool. Just support +, -, *, /."""
    if isinstance(first_number, str):
        first_number = int(first_number)
    if isinstance(second_number, str):
        second_number = int(second_number)
    if operator == "+":
        return first_number + second_number
    elif operator == "-":
        return first_number - second_number
    elif operator == "*":
        return first_number * second_number
    elif operator == "/":
        return first_number / second_number
    else:
        raise ValueError(f"Invalid operator: {operator}")

----------------------------------------

TITLE: Creating and Using a ToolPack in Python
DESCRIPTION: Demonstrates how to create multiple tools and wrap them into a ToolPack. This example includes a calculator tool and a directory file counter tool.

LANGUAGE: python
CODE:
import os
from typing_extensions import Annotated, Doc

from dbgpt.agent.resource import ToolPack

@tool
def count_directory_files(path: Annotated[str, Doc("The directory path")]) -> int:
    """Count the number of files in a directory."""
    if not os.path.isdir(path):
        raise ValueError(f"Invalid directory path: {path}")
    return len(os.listdir(path))

tools = ToolPack([simple_calculator, count_directory_files])

async def show_tool_pack_prompts():
    # Just show the default prompt type
    tool_pack_prompt = await tools.get_prompt()
    print(tool_pack_prompt)

if __name__ == "__main__":
    import asyncio
    asyncio.run(show_tool_pack_prompts())

----------------------------------------

TITLE: Binding Action to Agent and Adding Extended Parameters
DESCRIPTION: Demonstrates how to bind the custom action to the agent and add extended parameters for action execution.

LANGUAGE: python
CODE:
from typing import Dict, Any
from dbgpt.agent import ConversableAgent

class MySummarizerAgent(ConversableAgent):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._init_actions([SummaryAction])
    
    def prepare_act_param(self) -> Dict[str, Any]:
        return {"action_extra_param_key": "this is extra param"}

----------------------------------------

TITLE: Implementing Custom Prompt Templates with ProfileConfig in Python
DESCRIPTION: Demonstrates how to apply custom prompt templates to a ProfileConfig instance and generate formatted system and user prompts with specific questions.

LANGUAGE: python
CODE:
from dbgpt.agent import ProfileConfig

profile: ProfileConfig = ProfileConfig(
    name="Aristotle",
    role="Summarizer",
    goal=(
        "Summarize answer summaries based on user questions from provided "
        "resource information or from historical conversation memories."
    ),
    desc=(
        "You can summarize provided text content according to user's questions"
        " and output the summarization."
    ),
    system_prompt_template=my_system_prompt_template,
    user_prompt_template=my_user_prompt_template,
)

real_profile = profile.create_profile()

system_prompt = real_profile.format_system_prompt(question="What can you do?")
user_prompt = real_profile.format_user_prompt(question="What can you do?")
print(f"System Prompt: \n{system_prompt}")
print("#" * 50)
print(f"User Prompt: \n{user_prompt}")

----------------------------------------

TITLE: Creating Hybrid Memory with Custom Components in Python
DESCRIPTION: Shows how to create a Hybrid Memory instance with custom sensory, short-term, and long-term memory components.

LANGUAGE: python
CODE:
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

from dbgpt.agent import (
    SensoryMemory,
    EnhancedShortTermMemory,
    LongTermMemory,
    HybridMemory,
    AgentMemory,
)

executor = ThreadPoolExecutor()

sensor_memory = SensoryMemory(buffer_size=2)

short_term_memory = EnhancedShortTermMemory(
    embeddings=embeddings,
    buffer_size=2,
    enhance_similarity_threshold=0.7,
    enhance_threshold=3,
    executor=executor,
)

long_term_memory = LongTermMemory(
    executor=ThreadPoolExecutor(), vector_store=vector_store, _default_importance=0.5
)

hybrid_memory = HybridMemory(
    now=datetime.now(),
    sensory_memory=sensor_memory,
    short_term_memory=short_term_memory,
    long_term_memory=long_term_memory,
)

agent_memory: AgentMemory = AgentMemory(memory=hybrid_memory)

----------------------------------------

TITLE: Building Full-featured Docker Image
DESCRIPTION: Creates a CUDA-based Docker image with all available features including embedding capabilities.

LANGUAGE: bash
CODE:
bash docker/base/build_image.sh --install-mode full

----------------------------------------

TITLE: Defining VLLMDeployModelParameters Configuration in JSON
DESCRIPTION: This JSON object defines the configuration schema for VLLMDeployModelParameters. It includes various parameters for model deployment, such as name, path, device, provider, and numerous performance and runtime settings.

LANGUAGE: json
CODE:
{
  "name": "VLLMDeployModelParameters",
  "description": "Local deploy model parameters.",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "path",
      "type": "string",
      "required": false,
      "description": "The path of the model, if you want to deploy a local model."
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name."
    },
    {
      "name": "device",
      "type": "string",
      "required": false,
      "description": "Device to run model. If None, the device is automatically determined",
      "defaultValue": "auto"
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "vllm"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "100"
    },
    {
      "name": "prompt_template",
      "type": "string",
      "required": false,
      "description": "Prompt template. If None, the prompt template is automatically determined from model. Just for local deployment."
    },
    {
      "name": "context_length",
      "type": "integer",
      "required": false,
      "description": "The context length of the model. If None, it is automatically determined from model."
    },
    {
      "name": "trust_remote_code",
      "type": "boolean",
      "required": false,
      "description": "Trust remote code or not.",
      "defaultValue": "True"
    },
    {
      "name": "download_dir",
      "type": "string",
      "required": false,
      "description": "Directory to download and load the weights, default to the default cache dir of huggingface."
    },
    {
      "name": "load_format",
      "type": "string",
      "required": false,
      "description": "The format of the model weights to load.\n\n* \"auto\" will try to load the weights in the safetensors format and fall back to the pytorch bin format if safetensors format is not available.\n* \"pt\" will load the weights in the pytorch bin format.\n* \"safetensors\" will load the weights in the safetensors format.\n* \"npcache\" will load the weights in pytorch format and store a numpy cache to speed up the loading.\n* \"dummy\" will initialize the weights with random values, which is mainly for profiling.\n* \"tensorizer\" will load the weights using tensorizer from CoreWeave. See the Tensorize vLLM Model script in the Examples section for more information.\n* \"runai_streamer\" will load the Safetensors weights using Run:aiModel Streamer \n* \"bitsandbytes\" will load the weights using bitsandbytes quantization.\n",
      "defaultValue": "auto",
      "validValues": [
        "auto",
        "pt",
        "safetensors",
        "npcache",
        "dummy",
        "tensorizer",
        "runai_streamer",
        "bitsandbytes",
        "sharded_state",
        "gguf",
        "mistral"
      ]
    },
    {
      "name": "config_format",
      "type": "string",
      "required": false,
      "description": "The format of the model config to load.\n\n* \"auto\" will try to load the config in hf format if available else it will try to load in mistral format ",
      "defaultValue": "auto",
      "validValues": [
        "auto",
        "hf",
        "mistral"
      ]
    },
    {
      "name": "dtype",
      "type": "string",
      "required": false,
      "description": "Data type for model weights and activations.\n\n* \"auto\" will use FP16 precision for FP32 and FP16 models, and BF16 precision for BF16 models.\n* \"half\" for FP16. Recommended for AWQ quantization.\n* \"float16\" is the same as \"half\".\n* \"bfloat16\" for a balance between precision and range.\n* \"float\" is shorthand for FP32 precision.\n* \"float32\" for FP32 precision.",
      "defaultValue": "auto",
      "validValues": [
        "auto",
        "half",
        "float16",
        "bfloat16",
        "float",
        "float32"
      ]
    },
    {
      "name": "kv_cache_dtype",
      "type": "string",
      "required": false,
      "description": "Data type for kv cache storage. If \"auto\", will use model data type. CUDA 11.8+ supports fp8 (=fp8_e4m3) and fp8_e5m2. ROCm (AMD GPU) supports fp8 (=fp8_e4m3)",
      "defaultValue": "auto",
      "validValues": [
        "auto",
        "fp8",
        "fp8_e5m2",
        "fp8_e4m3"
      ]
    },
    {
      "name": "seed",
      "type": "integer",
      "required": false,
      "description": "Random seed for operations.",
      "defaultValue": "0"
    },
    {
      "name": "max_model_len",
      "type": "integer",
      "required": false,
      "description": "Model context length. If unspecified, will be automatically derived from the model config."
    },
    {
      "name": "distributed_executor_backend",
      "type": "string",
      "required": false,
      "description": "Backend to use for distributed model workers, either \"ray\" or \"mp\" (multiprocessing). If the product of pipeline_parallel_size and tensor_parallel_size is less than or equal to the number of GPUs available, \"mp\" will be used to keep processing on a single host. Otherwise, this will default to \"ray\" if Ray is installed and fail otherwise. Note that tpu only supports Ray for distributed inference.",
      "validValues": [
        "ray",
        "mp",
        "uni",
        "external_launcher"
      ]
    },
    {
      "name": "pipeline_parallel_size",
      "type": "integer",
      "required": false,
      "description": "Number of pipeline stages.",
      "defaultValue": "1"
    },
    {
      "name": "tensor_parallel_size",
      "type": "integer",
      "required": false,
      "description": "Number of tensor parallel replicas.",
      "defaultValue": "1"
    },
    {
      "name": "max_parallel_loading_workers",
      "type": "integer",
      "required": false,
      "description": "Load model sequentially in multiple batches, to avoid RAM OOM when using tensor parallel and large models."
    },
    {
      "name": "block_size",
      "type": "integer",
      "required": false,
      "description": "Token block size for contiguous chunks of tokens. This is ignored on neuron devices and set to ``--max-model-len``. On CUDA devices, only block sizes up to 32 are supported. On HPU devices, block size defaults to 128.",
      "validValues": [
        "8",
        "16",
        "32",
        "64",
        "128"
      ]
    },
    {
      "name": "enable_prefix_caching",
      "type": "boolean",
      "required": false,
      "description": "Enables automatic prefix caching. "
    },
    {
      "name": "swap_space",
      "type": "number",
      "required": false,
      "description": "CPU swap space size (GiB) per GPU.",
      "defaultValue": "4"
    },
    {
      "name": "cpu_offload_gb",
      "type": "number",
      "required": false,
      "description": "The space in GiB to offload to CPU, per GPU. Default is 0, which means no offloading. Intuitively, this argument can be seen as a virtual way to increase the GPU memory size. For example, if you have one 24 GB GPU and set this to 10, virtually you can think of it as a 34 GB GPU. Then you can load a 13B model with BF16 weight, which requires at least 26GB GPU memory. Note that this requires fast CPU-GPU interconnect, as part of the model is loaded from CPU memory to GPU memory on the fly in each model forward pass.",
      "defaultValue": "0"
    },
    {
      "name": "gpu_memory_utilization",
      "type": "number",
      "required": false,
      "description": "The fraction of GPU memory to be used for the model executor, which can range from 0 to 1. For example, a value of 0.5 would imply 50%% GPU memory utilization. If unspecified, will use the default value of 0.9. This is a per-instance limit, and only applies to the current vLLM instance.It does not matter if you have another vLLM instance running on the same GPU. For example, if you have two vLLM instances running on the same GPU, you can set the GPU memory utilization to 0.5 for each instance.",
      "defaultValue": "0.9"
    },
    {
      "name": "max_num_batched_tokens",
      "type": "integer",
      "required": false,
      "description": "Maximum number of batched tokens per iteration."
    },
    {
      "name": "max_num_seqs",
      "type": "integer",
      "required": false,
      "description": "Maximum number of sequences per iteration."
    },
    {
      "name": "max_logprobs",
      "type": "integer",
      "required": false,
      "description": "Max number of log probs to return logprobs is specified in SamplingParams.",
      "defaultValue": "20"
    },
    {
      "name": "revision",
      "type": "string",
      "required": false,
      "description": "The specific model version to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version."
    },
    {
      "name": "code_revision",
      "type": "string",
      "required": false,
      "description": "The specific revision to use for the model code on Hugging Face Hub. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version."
    },
    {
      "name": "tokenizer_revision",
      "type": "string",
      "required": false,
      "description": "Revision of the huggingface tokenizer to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version."
    },
    {
      "name": "tokenizer_mode",
      "type": "string",
      "required": false,
      "description": "The tokenizer mode.\n\n* \"auto\" will use the fast tokenizer if available.\n* \"slow\" will always use the slow tokenizer. \n* \"mistral\" will always use the `mistral_common` tokenizer.",
      "defaultValue": "auto",
      "validValues": [
        "auto",
        "slow",
        "mistral"
      ]
    },
    {
      "name": "quantization",
      "type": "string",
      "required": false,
      "description": "Method used to quantize the weights. If None, we first check the `quantization_config` attribute in the model config file. If that is None, we assume the model weights are not quantized and use `dtype` to determine the data type of the weights.",
      "validValues": [
        "aqlm",
        "awq",
        "deepspeedfp",
        "tpu_int8",
        "fp8",
        "ptpc_fp8",
        "fbgemm_fp8",
        "modelopt",
        "marlin",
        "gguf",
        "gptq_marlin_24",
        "gptq_marlin",
        "awq_marlin",
        "gptq",
        "compressed-tensors",
        "bitsandbytes",
        "qqq",
        "hqq",
        "experts_int8",
        "neuron_quant",
        "ipex",
        "quark",
        "moe_wna16"
      ]
    },
    {
      "name": "max_seq_len_to_capture",
      "type": "integer",
      "required": false,
      "description": "Maximum sequence length covered by CUDA graphs. When a sequence has context length larger than this, we fall back to eager mode. Additionally for encoder-decoder models, if the sequence length of the encoder input is larger than this, we fall back to the eager mode.",
      "defaultValue": "8192"
    },
    {
      "name": "worker_cls",
      "type": "string",
      "required": false,
      "description": "The worker class to use for distributed execution.",
      "defaultValue": "auto"
    },
    {
      "name": "extras",
      "type": "object",
      "required": false,
      "description": "Extra parameters, it will be passed to the vllm engine."
    }
  ]
}

----------------------------------------

TITLE: Creating ToolPack for Multiple Tools in Python
DESCRIPTION: Demonstrates how to combine multiple tools into a ToolPack for easier management and agent access.

LANGUAGE: python
CODE:
from dbgpt.agent.resource import ToolPack

tools = ToolPack([simple_calculator, count_directory_files])

----------------------------------------

TITLE: Generating SQL with LLM
DESCRIPTION: AWEL DAG implementation to generate SQL queries using an LLM based on user input and retrieved database schema.

LANGUAGE: python
CODE:
import json
from dbgpt.core import (
    ChatPromptTemplate,
    HumanPromptTemplate,
    SystemPromptTemplate,
    SQLOutputParser
)
from dbgpt.core.awel import DAG, InputOperator, InputSource, MapOperator, JoinOperator
from dbgpt.core.operators import PromptBuilderOperator, RequestBuilderOperator
from dbgpt.rag.operators import DBSchemaRetrieverOperator
from dbgpt.model.operators import LLMOperator

system_prompt = """You are a database expert. Please answer the user's question based on the database selected by the user and some of the available table structure definitions of the database.
Database name:
    {db_name}
Table structure definition:
    {table_info}
    
Constraint:
1.Please understand the user's intention based on the user's question, and use the given table structure definition to create a grammatically correct {dialect} sql. If sql is not required, answer the user's question directly.. 
2.Always limit the query to a maximum of {top_k} results unless the user specifies in the question the specific number of rows of data he wishes to obtain.
3.You can only use the tables provided in the table structure information to generate sql. If you cannot generate sql based on the provided table structure, please say: "The table structure information provided is not enough to generate sql queries." It is prohibited to fabricate information at will.
4.Please be careful not to mistake the relationship between tables and columns when generating SQL.
5.Please check the correctness of the SQL and ensure that the query performance is optimized under correct conditions.
6.Please choose the best one from the display methods given below for data rendering, and put the type name into the name parameter value that returns the required format. If you cannot find the most suitable one, use 'Table' as the display method.
the available data display methods are as follows: {display_type}
 
User Question:
    {user_input}
Please think step by step and respond according to the following JSON format:
    {response}
Ensure the response is correct json and can be parsed by Python json.loads.
"""

RESPONSE_FORMAT_SIMPLE = {
    "thoughts": "thoughts summary to say to user",
    "sql": "SQL Query to run",
    "display_type": "Data display method",
}

prompt = ChatPromptTemplate(
    messages=[
        SystemPromptTemplate.from_template(
            system_prompt,
            response_format=json.dumps(
                RESPONSE_FORMAT_SIMPLE, ensure_ascii=False, indent=4
            ),
        ),
        HumanPromptTemplate.from_template("{user_input}"),
    ]
)

with DAG("chat_data_dag") as chat_data_dag:
    input_task = InputOperator(input_source=InputSource.from_callable())
    retriever_task = DBSchemaRetrieverOperator(
        top_k=1,
        index_store=vector_store,
    )
    content_task = MapOperator(lambda cks: [c.content for c in cks]) 
    merge_task = JoinOperator(lambda table_info, ext_dict: {"table_info": table_info, **ext_dict}) 
    prompt_task = PromptBuilderOperator(prompt)
    req_build_task = RequestBuilderOperator(model="gpt-3.5-turbo")
    llm_task = LLMOperator(llm_client=llm_client) 
    sql_parse_task = SQLOutputParser()
 
    input_task >> MapOperator(lambda x: x["user_input"]) >> retriever_task >> content_task >> merge_task
    input_task >> merge_task
    merge_task >> prompt_task >> req_build_task >> llm_task >> sql_parse_task
 
result = asyncio.run(sql_parse_task.call({
    "user_input": "Query the name and age of users younger than 18 years old",
    "db_name": "user_management",
    "dialect": "SQLite",
    "top_k": 1,
    "display_type": display_type,
    "response": json.dumps(RESPONSE_FORMAT_SIMPLE, ensure_ascii=False, indent=4)
}))

print("Result:\n", result)

----------------------------------------

TITLE: Creating Community Knowledge Graph Connector
DESCRIPTION: Python function to initialize and configure the community knowledge graph connector with OpenAI embeddings

LANGUAGE: python
CODE:
from dbgpt.model.proxy.llms.chatgpt import OpenAILLMClient
from dbgpt.storage.knowledge_graph.community_summary import (
    CommunitySummaryKnowledgeGraph,
    CommunitySummaryKnowledgeGraphConfig,
)

llm_client = OpenAILLMClient()
model_name = "gpt-4o-mini"

def __create_community_kg_connector():
    """Create community knowledge graph connector."""
    return CommunitySummaryKnowledgeGraph(
        config=CommunitySummaryKnowledgeGraphConfig(
            name="community_graph_rag_test",
            embedding_fn=DefaultEmbeddingFactory.openai(),
            llm_client=llm_client,
            model_name=model_name,
            graph_store_type="TuGraphGraph",
        ),
    )

----------------------------------------

TITLE: Implementing RAG Workflow with AWEL DAG API
DESCRIPTION: Example showing how to create a Retrieval-Augmented Generation (RAG) workflow using AWEL's DAG API. The workflow includes HTTP triggers, request parsing, prompt management, chat history, embeddings, and model operations.

LANGUAGE: python
CODE:
with DAG("simple_rag_example") as dag:
    trigger_task = HttpTrigger(
        "/examples/simple_rag", methods="POST", request_body=ConversationVo
    )
    req_parse_task = RequestParseOperator()
    # TODO should register prompt template first
    prompt_task = PromptManagerOperator()
    history_storage_task = ChatHistoryStorageOperator()
    history_task = ChatHistoryOperator()
    embedding_task = EmbeddingEngingOperator()
    chat_task = BaseChatOperator()
    model_task = ModelOperator()
    output_parser_task = MapOperator(lambda out: out.to_dict()["text"])

    (
        trigger_task
        >> req_parse_task
        >> prompt_task
        >> history_storage_task
        >> history_task
        >> embedding_task
        >> chat_task
        >> model_task
        >> output_parser_task
    )

----------------------------------------

TITLE: Implementing Calculator with DB-GPT Agents
DESCRIPTION: Asynchronous Python code to create a code assistant agent and a user proxy agent, and initiate a chat to calculate 321 * 123. It demonstrates the full flow of agent interaction and code execution.

LANGUAGE: python
CODE:
import asyncio

from dbgpt.agent import LLMConfig, UserProxyAgent
from dbgpt.agent.expand.code_assistant_agent import CodeAssistantAgent


async def main():

    # Create a code assistant agent
    coder = (
        await CodeAssistantAgent()
        .bind(context)
        .bind(LLMConfig(llm_client=llm_client))
        .bind(agent_memory)
        .build()
    )

    # Create a user proxy agent
    user_proxy = await UserProxyAgent().bind(context).bind(agent_memory).build()

    # Initiate a chat with the user proxy agent
    await user_proxy.initiate_chat(
        recipient=coder,
        reviewer=user_proxy,
        message="Calculate the result of 321 * 123",
    )
    # Obtain conversation history messages between agents
    print(await agent_memory.gpts_memory.one_chat_completions("test123"))


if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Complete Even/Odd BranchOperator Example with JoinOperator in Python
DESCRIPTION: This comprehensive example demonstrates a full implementation of BranchOperator for even/odd number processing, including input handling, branching logic, and result combination using JoinOperator.

LANGUAGE: python
CODE:
import asyncio
from dbgpt.core.awel import (
    DAG, BranchOperator, MapOperator, JoinOperator, 
    InputOperator, SimpleCallDataInputSource,
    is_empty_data
)

def branch_even(x: int) -> bool:
    return x % 2 == 0

def branch_odd(x: int) -> bool:
    return not branch_even(x)

branch_mapping = {
    branch_even: "even_task",
    branch_odd: "odd_task"
}

def even_func(x: int) -> int:
    print(f"Branch even, {x} is even, multiply by 10")
    return x * 10

def odd_func(x: int) -> int:
    print(f"Branch odd, {x} is odd, multiply by itself")
    return x * x

def combine_function(x: int, y: int) -> int:
    print(f"Received {x} and {y}")
    # Return the first non-empty data
    return x if not is_empty_data(x) else y
    
with DAG("awel_branch_operator") as dag:
    input_task = InputOperator(input_source=SimpleCallDataInputSource())
    task = BranchOperator(branches=branch_mapping)
    even_task = MapOperator(task_name="even_task", map_function=even_func)
    odd_task = MapOperator(task_name="odd_task", map_function=odd_func)
    join_task = JoinOperator(combine_function=combine_function, can_skip_in_branch=False)
    input_task >> task >> even_task >> join_task
    input_task >> task >> odd_task >> join_task

print("First call, input is 5")
assert asyncio.run(join_task.call(call_data=5)) == 25
print("=" * 80)
print("Second call, input is 6")
assert asyncio.run(join_task.call(call_data=6)) == 60

----------------------------------------

TITLE: Creating Agent Memory with GptsMemory in Python
DESCRIPTION: Example showing how to create agent memory with both ShortTermMemory and GptsMemory for storing conversation and plan information.

LANGUAGE: python
CODE:
from dbgpt.agent import AgentMemory, ShortTermMemory, GptsMemory

# Create an agent memory, default memory is ShortTermMemory
memory = ShortTermMemory(buffer_size=5)
# Store the conversation and plan information
gpts_memory = GptsMemory()
agent_memory = AgentMemory(memory=memory, gpts_memory=gpts_memory)

----------------------------------------

TITLE: Creating RAG Program with AWEL in Python
DESCRIPTION: Builds a complete RAG program using AWEL, combining knowledge retrieval, prompt building, and LLM interaction. It uses various operators to create a DAG for the RAG workflow.

LANGUAGE: python
CODE:
from dbgpt.core.awel import InputOperator, JoinOperator, InputSource
from dbgpt.core.operators import PromptBuilderOperator, RequestBuilderOperator
from dbgpt.model.operators import LLMOperator

prompt = """Based on the known information below, provide users with professional and concise answers to their questions. 
If the answer cannot be obtained from the provided content, please say: 
"The information provided in the knowledge base is not sufficient to answer this question.". 
It is forbidden to make up information randomly. When answering, it is best to summarize according to points 1.2.3.
          known information: 
          {context}
          question:
          {question}
"""

with DAG("llm_rag_dag") as rag_dag:
    input_task = InputOperator(input_source=InputSource.from_callable())
    retriever_task = EmbeddingRetrieverOperator(
        top_k=3,
        index_store=vector_store,
    )
    content_task = MapOperator(lambda cks: "\n".join(c.content for c in cks))
    
    merge_task = JoinOperator(lambda context, question: {"context": context, "question": question})
    
    prompt_task = PromptBuilderOperator(prompt)
    # The model is gpt-3.5-turbo, you can replace it with other models.
    req_build_task = RequestBuilderOperator(model="gpt-3.5-turbo")
    llm_task = LLMOperator(llm_client=llm_client)
    result_task = MapOperator(lambda r: r.text)

    input_task >> retriever_task >> content_task >> merge_task
    input_task >> merge_task

    merge_task >> prompt_task >> req_build_task >> llm_task >> result_task

print(asyncio.run(result_task.call("What is the AWEL?")))

----------------------------------------

TITLE: Implementing Directory File Counter Tool in Python
DESCRIPTION: Creates a tool to count files in a specified directory using the @tool decorator. Includes type annotations and documentation.

LANGUAGE: python
CODE:
import os
from typing_extensions import Annotated, Doc

@tool
def count_directory_files(path: Annotated[str, Doc("The directory path")]) -> int:
    """Count the number of files in a directory."""
    if not os.path.isdir(path):
        raise ValueError(f"Invalid directory path: {path}")
    return len(os.listdir(path))

----------------------------------------

TITLE: Customizing Tool Parameters with @tool Decorator in Python
DESCRIPTION: Shows how to customize tool parameters using various methods, including direct argument specification, Pydantic models, and Annotated types with Doc.

LANGUAGE: python
CODE:
from dbgpt.agent.resource import tool
from pydantic import BaseModel, Field
from typing_extensions import Annotated, Doc

@tool(
    args={
        "a": {
            "type": "integer",
            "description": "First number to add",
            "required": True,
        },
        "b": {
            "type": "integer",
            "description": "Second number to add",
            "required": True,
        },
    }
)
def two_sum(a: int, b: int) -> int:
    """Add two numbers and return the sum."""
    return a + b

class ArgsSchema(BaseModel):
    a: int = Field(description="The first number.")
    b: int = Field(description="The second number.")

@tool(args_schema=ArgsSchema)
def two_sum_pydantic(a: int, b: int) -> int:
    """Add two numbers and return the sum."""
    return a + b

@tool
def two_sum_annotated(
    a: Annotated[int, Doc("The first number.")],
    b: Annotated[int, Doc("The second number.")],
) -> int:
    """Add two numbers and return the sum."""
    return a + b

----------------------------------------

TITLE: Complete Number Summation Example - Python
DESCRIPTION: Full example showing how to create a number stream and reduce it using custom operators, including a number producer and sum reducer with validation logic.

LANGUAGE: python
CODE:
import asyncio
from typing import AsyncIterator
from dbgpt.core.awel import DAG, ReduceStreamOperator, StreamifyAbsOperator

class NumberProducerOperator(StreamifyAbsOperator[int, int]):
    """Create a stream of numbers from 0 to `n-1`"""
    async def streamify(self, n: int) -> AsyncIterator[int]:
        for i in range(n):
            yield i
            
class MySumOperator(ReduceStreamOperator[int, int]):
    async def reduce(self, x: int, y: int) -> int:
        return x + y

with DAG("sum_numbers_dag") as dag:
    task = NumberProducerOperator()
    sum_task = MySumOperator()
    task >> sum_task

o1 = asyncio.run(sum_task.call(call_data=5))
if o1 == sum(range(5)):
    print(f"Success! n is 5, sum is {o1}")
else:
    print("Failed")
o2 = asyncio.run(sum_task.call(call_data=10))
if o2 == sum(range(10)):
    print(f"Success! n is 10, sum is {o2}")
else:
    print("Failed")

----------------------------------------

TITLE: Storing Database Schema in Vector Store
DESCRIPTION: AWEL DAG implementation to load and store the database schema into a vector store for efficient retrieval.

LANGUAGE: python
CODE:
import asyncio
import shutil
from dbgpt.core.awel import DAG, InputOperator
from dbgpt_ext.rag import ChunkParameters
from dbgpt.rag.operators import DBSchemaAssemblerOperator
from dbgpt.storage.vector_store.chroma_store import ChromaVectorConfig, ChromaStore

shutil.rmtree("/tmp/awel_with_data_vector_store", ignore_errors=True)

vector_store = ChromaStore(
    ChromaVectorConfig(
        persist_path="/tmp/tmp_ltm_vector_store",
        name="ltm_vector_store",
        embedding_fn=embeddings,
    )
)

with DAG("load_schema_dag") as load_schema_dag:
    input_task = InputOperator.dummy_input()
    assembler_task = DBSchemaAssemblerOperator(
        connector=db_conn,
        index_store=vector_store,
        chunk_parameters=ChunkParameters(chunk_strategy="CHUNK_BY_SIZE")
    )
    input_task >> assembler_task

chunks = asyncio.run(assembler_task.call())
print(chunks)

----------------------------------------

TITLE: Implementing OpenAI SDK Model API Call in Python
DESCRIPTION: Example demonstrating how to call DB-GPT models using the OpenAI SDK compatibility layer. Sets up the API connection and makes a simple chat completion request.

LANGUAGE: python
CODE:
import openai
openai.api_key = "EMPTY"
openai.api_base = "http://127.0.0.1:8100/api/v1"
model = "vicuna-13b-v1.5"

completion = openai.ChatCompletion.create(
  model=model,
  messages=[{"role": "user", "content": "hello"}]
)
# print the completion
print(completion.choices[0].message.content)

----------------------------------------

TITLE: Retrieving Database Schema from Vector Store
DESCRIPTION: AWEL DAG to retrieve relevant database schema information based on a user query.

LANGUAGE: python
CODE:
from dbgpt.core.awel import InputSource
from dbgpt.rag.operators import DBSchemaRetrieverOperator

with DAG("retrieve_schema_dag") as retrieve_schema_dag:
    input_task = InputOperator(input_source=InputSource.from_callable())
    retriever_task = DBSchemaRetrieverOperator(
        top_k=1,
        index_store=vector_store,
    )
    input_task >> retriever_task

chunks = asyncio.run(retriever_task.call("Query the name and age of users younger than 18 years old"))
print("Retrieved schema:\n", chunks)

----------------------------------------

TITLE: Implementing Long-term Memory
DESCRIPTION: Creates an agent memory instance with long-term memory capabilities using ThreadPoolExecutor and the configured vector store.

LANGUAGE: python
CODE:
from concurrent.futures import ThreadPoolExecutor
from dbgpt.agent import AgentMemory, LongTermMemory

# Create an agent memory, which contains a long-term memory
memory = LongTermMemory(
    executor=ThreadPoolExecutor(), vector_store=vector_store, _default_importance=0.5
)
agent_memory: AgentMemory = AgentMemory(memory=memory)

----------------------------------------

TITLE: Running DB-GPT with Proxy Model
DESCRIPTION: Docker command to run DB-GPT container with SiliconFlow API integration. Requires a valid API key from SiliconFlow.

LANGUAGE: bash
CODE:
docker run -it --rm -e SILICONFLOW_API_KEY=${SILICONFLOW_API_KEY} \
 -p 5670:5670 --name dbgpt eosphorosai/dbgpt-openai

----------------------------------------

TITLE: Mocking a Streaming LLM Service with StreamifyAbsOperator in Python
DESCRIPTION: This snippet demonstrates how to use StreamifyAbsOperator to mock a streaming LLM service. It creates a custom operator that yields predefined responses, simulating a streaming language model output.

LANGUAGE: python
CODE:
import asyncio
from typing import AsyncIterator, List
from dbgpt.core.awel import DAG, StreamifyAbsOperator

class MockLLMService(StreamifyAbsOperator[str, str]):
    """Mock a streaming LLM service"""
    def __init__(self, mock_data: List[str], **kwargs):
        self.mock_data = mock_data
        super().__init__(**kwargs)
        
    async def streamify(self, user_input: str) -> AsyncIterator[str]:
        for data in self.mock_data:
            yield data

with DAG("mock_llm_service_dag") as dag:
    task = MockLLMService(mock_data=["Hello, ", "how ", "can ", "I ", "help ", "you?"])

async def print_stream(t, user_input: str):
    # Call the streaming operator by `call_stream` method
    async for i in await t.call_stream(call_data=user_input):
        print(i, end="")

asyncio.run(print_stream(task, "Hi"))

----------------------------------------

TITLE: Implementing Graph RAG Retrieval and Chat
DESCRIPTION: Python implementation of knowledge retrieval and chat functionality using the graph database

LANGUAGE: python
CODE:
async def test_community_graph_rag():
    await __run_graph_rag(
        knowledge_file="examples/test_files/graphrag-mini.md",
        chunk_strategy="CHUNK_BY_MARKDOWN_HEADER",
        knowledge_graph=__create_community_kg_connector(),
        question="What's the relationship between TuGraph and DB-GPT ?",
    )

async def __run_graph_rag(knowledge_file, chunk_strategy, knowledge_graph, question):
    file_path = os.path.join(ROOT_PATH, knowledge_file).format()
    knowledge = KnowledgeFactory.from_file_path(file_path)
    try:
        chunk_parameters = ChunkParameters(chunk_strategy=chunk_strategy)
        assembler = await EmbeddingAssembler.aload_from_knowledge(
            knowledge=knowledge,
            chunk_parameters=chunk_parameters,
            index_store=knowledge_graph,
            retrieve_strategy=RetrieverStrategy.GRAPH,
        )
        await assembler.apersist()
        retriever = assembler.as_retriever(1)
        chunks = await retriever.aretrieve_with_scores(question, score_threshold=0.3)
        print(f"{await ask_chunk(chunks[0], question)}")
    finally:
        knowledge_graph.delete_vector_name(knowledge_graph.get_config().name)

----------------------------------------

TITLE: Complete Stream Transformation Example with DB-GPT
DESCRIPTION: Full example demonstrating stream transformation including number generation and doubling, with async iteration and DAG implementation.

LANGUAGE: python
CODE:
import asyncio
from typing import AsyncIterator
from dbgpt.core.awel import DAG, TransformStreamAbsOperator, StreamifyAbsOperator

class NumberProducerOperator(StreamifyAbsOperator[int, int]):
    """Create a stream of numbers from 0 to `n-1`"""
    async def streamify(self, n: int) -> AsyncIterator[int]:
        for i in range(n):
            yield i

class NumberDoubleOperator(TransformStreamAbsOperator[int, int]):
    async def transform_stream(self, it: AsyncIterator) -> AsyncIterator[int]:
        async for i in it:
            # Double the number
            yield i * 2

with DAG("numbers_dag") as dag:
    task = NumberProducerOperator()
    double_task = NumberDoubleOperator()
    task >> double_task
async def print_stream(t, n: int):
    # Call the streaming operator by `call_stream` method
    async for i in await t.call_stream(call_data=n):
        print(i)

asyncio.run(print_stream(double_task, 10))

----------------------------------------

TITLE: Creating Temporary SQLite Database for Student Data Analysis in Python
DESCRIPTION: This snippet creates a temporary SQLite database and populates it with tables for students, courses, and scores. It uses the SQLiteTempConnector to set up the database structure and insert sample data.

LANGUAGE: python
CODE:
from dbgpt.datasource.rdbms.conn_sqlite import SQLiteTempConnector

connector = SQLiteTempConnector.create_temporary_db()
connector.create_temp_tables(
    {
        "students": {
            "columns": {
                "student_id": "INTEGER PRIMARY KEY",
                "student_name": "TEXT",
                "major": "TEXT",
                "year_of_enrollment": "INTEGER",
                "student_age": "INTEGER",
            },
            "data": [
                (1, "Zhang San", "Computer Science", 2020, 20),
                (2, "Li Si", "Computer Science", 2021, 19),
                # ... more student data ...
            ],
        },
        "courses": {
            "columns": {
                "course_id": "INTEGER PRIMARY KEY",
                "course_name": "TEXT",
                "credit": "REAL",
            },
            "data": [
                (1, "Introduction to Computer Science", 3),
                (2, "Data Structures", 4),
                # ... more course data ...
            ],
        },
        "scores": {
            "columns": {
                "student_id": "INTEGER",
                "course_id": "INTEGER",
                "score": "INTEGER",
                "semester": "TEXT",
            },
            "data": [
                (1, 1, 90, "Fall 2020"),
                (1, 2, 85, "Spring 2021"),
                # ... more score data ...
            ],
        },
    }
)

----------------------------------------

TITLE: Constructing AWEL DAG Pipeline in Python
DESCRIPTION: Creates a DAG pipeline using HttpTrigger and RequestHandleOperator, defining the workflow for processing HTTP requests and generating responses.

LANGUAGE: python
CODE:
with DAG("simple_dag_example") as dag:
    trigger = HttpTrigger("/examples/hello", request_body=TriggerReqBody)
    map_node = RequestHandleOperator()
    trigger >> map_node

----------------------------------------

TITLE: Evaluation Request JSON for Recall Scene in DB-GPT
DESCRIPTION: This JSON snippet shows the structure of an evaluation request for the 'recall' scene. It includes scene details, context parameters, multiple evaluation metrics, and dataset information.

LANGUAGE: json
CODE:
{
  "scene_key": "recall",
  "scene_value":"2c76eea2-83b6-11ef-b482-acde48001122",
  "context":{"top_k":5, "prompt":"942acd7e33b54ce28565f89f9b278044","model":"zhipu_proxyllm"},
  "evaluate_metrics":["RetrieverHitRateMetric", "RetrieverMRRMetric", "RetrieverSimilarityMetric"],
  "datasets": [{
            "query": "what awel talked about",
            "doc_name":"awel.md"
        }]
}

----------------------------------------

TITLE: Implementing Custom TransformStreamAbsOperator in Python
DESCRIPTION: Example of creating a custom TransformStreamAbsOperator that doubles input numbers in a stream. Shows basic implementation by overriding the transform_stream method.

LANGUAGE: python
CODE:
from typing import AsyncIterator
from dbgpt.core.awel import DAG, TransformStreamAbsOperator

class NumberDoubleOperator(TransformStreamAbsOperator[int, int]):
    async def transform_stream(self, it: AsyncIterator) -> AsyncIterator[int]:
        async for i in it:
            # Double the number
            yield i * 2

with DAG("numbers_dag") as dag:
    task = NumberDoubleOperator()

----------------------------------------

TITLE: Creating a Custom Tool Class in Python
DESCRIPTION: Demonstrates how to create a custom tool class by inheriting from BaseTool. This example implements a two-sum tool that adds two numbers.

LANGUAGE: python
CODE:
from typing import Any, Dict, Optional
from dbgpt.agent.resource import BaseTool, ToolParameter

def _two_sum(a: int, b: int) -> int:
    return a + b

_TWO_SUM_ARGS = {
    "a": ToolParameter(
        name="a",
        type="integer",
        required=True,
        description="First number to add",
    ),
    "b": ToolParameter(
        name="b",
        type="integer",
        required=True,
        description="Second number to add",
    ),
}

class MyTwoSumTool(BaseTool):
    def __init__(self, name: Optional[str] = None) -> None:
        self._name = name or "two_sum"
        self._args = _TWO_SUM_ARGS

    @property
    def name(self) -> str:
        """Return the name of the tool."""
        return self._name

    @property
    def description(self) -> str:
        return "Add two numbers and return the sum."

    @property
    def args(self) -> Dict[str, ToolParameter]:
        return self._args

    @property
    def is_async(self) -> bool:
        """Return whether the tool is asynchronous."""
        return False

    def execute(
        self,
        *args,
        resource_name: Optional[str] = None,
        **kwargs,
    ) -> Any:
        return _two_sum(*args, **kwargs)

    async def async_execute(
        self,
        *args,
        resource_name: Optional[str] = None,
        **kwargs,
    ) -> Any:
        raise ValueError("The function is not asynchronous")

----------------------------------------

TITLE: Implementing Streaming HTTP Trigger in Python for DB-GPT
DESCRIPTION: This code snippet defines a DAG with a HttpTrigger and a custom NumberProducerOperator to stream numbers. It uses the dbgpt.core.awel module to set up the streaming functionality.

LANGUAGE: python
CODE:
from dbgpt._private.pydantic import BaseModel, Field
from dbgpt.core.awel import DAG, HttpTrigger, StreamifyAbsOperator, setup_dev_environment
from typing import AsyncIterator

class TriggerReqBody(BaseModel):
    n: int = Field(..., description="The number of integers to be streamed")

class NumberProducerOperator(StreamifyAbsOperator[TriggerReqBody, int]):
    """Create a stream of numbers from 0 to `n-1`"""
    async def streamify(self, req: TriggerReqBody) -> AsyncIterator[int]:
        for i in range(req.n):
            yield str(i) + "\n"

with DAG("awel_stream_numbers") as dag:
    trigger_task = HttpTrigger(
        endpoint="/awel_tutorial/stream_numbers", 
        methods="POST", 
        request_body=TriggerReqBody,
        status_code=200,
        streaming_predict_func=lambda x: True
    )
    task = NumberProducerOperator()
    trigger_task >> task

setup_dev_environment([dag], port=5555)

----------------------------------------

TITLE: Implementing Keyword Retrieval
DESCRIPTION: Example implementation of keyword-based document retrieval using Elasticsearch and DB-GPT's RAG framework

LANGUAGE: python
CODE:
import os

from dbgpt.configs.model_config import ROOT_PATH
from dbgpt_ext.rag import ChunkParameters
from dbgpt_ext.rag.assembler import EmbeddingAssembler
from dbgpt_ext.rag.knowledge import KnowledgeFactory

async def main():
    file_path = os.path.join(ROOT_PATH, "docs/docs/awel/awel.md")
    knowledge = KnowledgeFactory.from_file_path(file_path)
    keyword_store = _create_es_connector()
    chunk_parameters = ChunkParameters(chunk_strategy="CHUNK_BY_SIZE")
    # get embedding assembler
    assembler = EmbeddingAssembler.load_from_knowledge(
        knowledge=knowledge,
        chunk_parameters=chunk_parameters,
        index_store=keyword_store,
    )
    assembler.persist()
    # get embeddings retriever
    retriever = assembler.as_retriever(3)
    chunks = await retriever.aretrieve_with_scores("what is awel talk about", 0.3)
    print(f"keyword rag example results:{chunks}")

----------------------------------------

TITLE: Creating a Temporary SQLite Database Connector in Python
DESCRIPTION: This snippet demonstrates how to create a temporary SQLite database connector, create a table, and populate it with sample user data. The temporary database is created in memory and will be deleted after the program exits.

LANGUAGE: python
CODE:
from dbgpt.datasource.rdbms.conn_sqlite import SQLiteTempConnector

connector = SQLiteTempConnector.create_temporary_db()
connector.create_temp_tables(
    {
        "user": {
            "columns": {
                "id": "INTEGER PRIMARY KEY",
                "name": "TEXT",
                "age": "INTEGER",
            },
            "data": [
                (1, "Tom", 10),
                (2, "Jerry", 16),
                (3, "Jack", 18),
                (4, "Alice", 20),
                (5, "Bob", 22),
            ],
        }
    }
)

----------------------------------------

TITLE: Creating Basic Agent Memory in Python
DESCRIPTION: Example of creating a basic agent memory instance using ShortTermMemory and AgentMemory classes.

LANGUAGE: python
CODE:
from dbgpt.agent import AgentMemory, ShortTermMemory

# Create an agent memory, default memory is ShortTermMemory
memory = ShortTermMemory(buffer_size=5)
agent_memory = AgentMemory(memory=memory)

----------------------------------------

TITLE: Evaluation Request JSON for App Scene in DB-GPT
DESCRIPTION: This JSON snippet illustrates the structure of an evaluation request for the 'app' scene. It includes scene details, context parameters, evaluation metrics, and dataset information.

LANGUAGE: json
CODE:
{
  "scene_key": "app",
  "scene_value":"2c76eea2-83b6-11ef-b482-acde48001122",
  "context":{"top_k":5, "prompt":"942acd7e33b54ce28565f89f9b278044","model":"zhipu_proxyllm"},
  "sys_code":"xx",
  "evaluate_metrics":["AnswerRelevancyMetric"],
  "datasets": [{
            "query": "what awel talked about",
            "doc_name":"awel.md"
        }]
}

----------------------------------------

TITLE: Loading Knowledge and Storing in Vector Store with AWEL in Python
DESCRIPTION: Creates a DAG to load knowledge from a URL and store it in a ChromaStore vector store. It uses KnowledgeOperator and EmbeddingAssemblerOperator.

LANGUAGE: python
CODE:
import asyncio
import shutil
from dbgpt.core.awel import DAG
from dbgpt_ext.rag import ChunkParameters
from dbgpt.rag.knowledge import KnowledgeType
from dbgpt.rag.operators import EmbeddingAssemblerOperator, KnowledgeOperator
from dbgpt.storage.vector_store.chroma_store import ChromaStore, ChromaVectorConfig

# Delete old vector store directory(/tmp/awel_rag_test_vector_store)
shutil.rmtree("/tmp/awel_rag_test_vector_store", ignore_errors=True)

vector_store = ChromaStore(
    vector_store_config=ChromaVectorConfig(
        name="test_vstore",
        persist_path="/tmp/awel_rag_test_vector_store",
        embedding_fn=embeddings
    )
)

with DAG("load_knowledge_dag") as knowledge_dag:
    # Load knowledge from URL
    knowledge_task = KnowledgeOperator(knowledge_type=KnowledgeType.URL.name)
    assembler_task = EmbeddingAssemblerOperator(
        index_store=vector_store,
        chunk_parameters=ChunkParameters(chunk_strategy="CHUNK_BY_SIZE")
    )
    knowledge_task >> assembler_task

chunks = asyncio.run(assembler_task.call("https://docs.dbgpt.site/docs/awel/"))
print(f"Chunk length: {len(chunks)}")

----------------------------------------

TITLE: Retrieving Knowledge from Vector Store with AWEL in Python
DESCRIPTION: Creates a DAG to retrieve knowledge from the vector store using EmbeddingRetrieverOperator and MapOperator.

LANGUAGE: python
CODE:
from dbgpt.core.awel import MapOperator
from dbgpt.rag.operators import EmbeddingRetrieverOperator

with DAG("retriever_dag") as retriever_dag:
    retriever_task = EmbeddingRetrieverOperator(
        top_k=3,
        index_store=vector_store,
    )
    content_task = MapOperator(lambda cks: "\n".join(c.content for c in cks))
    retriever_task >> content_task

chunks = asyncio.run(content_task.call("What is the AWEL?"))
print(chunks)

----------------------------------------

TITLE: Integrating Custom Operators into AWEL DAG
DESCRIPTION: Extension of the AWEL DAG to incorporate custom post-processing operators for SQL query results.

LANGUAGE: python
CODE:
    # previous code ...
    two_sum_task = TwoSumOperator()
    decision_task = DataDecisionOperator(odd_task_name="odd_task", even_task_name="even_task")
    odd_task = OddOperator(task_name="odd_task")
    even_task = EvenOperator(task_name="even_task")
    merge_task = MergeOperator()
    
    db_query_task >> two_sum_task >> decision_task
    decision_task >> odd_task >> merge_task
    decision_task >> even_task >> merge_task


final_result = asyncio.run(merge_task.call({
    "user_input": "Query the name and age of users younger than 18 years old",
    "db_name": "user_management",
    "dialect": "SQLite",
    "top_k": 1,
    "display_type": display_type,
    "response": json.dumps(RESPONSE_FORMAT_SIMPLE, ensure_ascii=False, indent=4) 
}))
print("The final result is:")
print(final_result)

----------------------------------------

TITLE: Building OpenAI-optimized Docker Image
DESCRIPTION: Creates a CPU-based Docker image optimized for OpenAI API usage with basic functionality and RAG capabilities.

LANGUAGE: bash
CODE:
bash docker/base/build_image.sh --install-mode openai

----------------------------------------

TITLE: Streaming Chat Completion with DB-GPT API using cURL
DESCRIPTION: This snippet shows how to make a streaming chat completion request to the DB-GPT API using cURL. It includes the necessary headers and request body for authentication and specifying chat parameters.

LANGUAGE: shell
CODE:
DBGPT_API_KEY="dbgpt"

curl -X POST "http://localhost:5670/api/v2/chat/completions" \
   -H "Authorization: Bearer $DBGPT_API_KEY" \
   -H "accept: application/json" \
   -H "Content-Type: application/json" \
   -d "{\"messages\":\"Hello\",\"model\":\"gpt-4o\", \"stream\": true}"

----------------------------------------

TITLE: Basic HTTP Trigger Implementation
DESCRIPTION: Implementation of a simple HTTP trigger that responds with 'Hello, world!' using AWEL DAG framework. The code sets up an HTTP endpoint and maps the response using a MapOperator.

LANGUAGE: python
CODE:
from dbgpt.core.awel import DAG, HttpTrigger, MapOperator, setup_dev_environment

with DAG("awel_hello_world") as dag:
    trigger_task = HttpTrigger(endpoint="/awel_tutorial/hello_world")
    task = MapOperator(map_function=lambda x: f"Hello, world!")
    trigger_task >> task  
    
setup_dev_environment([dag], port=5555)

----------------------------------------

TITLE: Implementing HTTP POST Handler with AWEL HttpTrigger
DESCRIPTION: Creates a DAG with an HTTP endpoint that accepts POST requests containing name and age parameters. The handler validates the request body using Pydantic and returns a personalized greeting message in JSON format.

LANGUAGE: python
CODE:
from dbgpt._private.pydantic import BaseModel, Field
from dbgpt.core.awel import DAG, HttpTrigger, MapOperator, setup_dev_environment

class TriggerReqBody(BaseModel):
    name: str = Field(..., description="User name")
    age: int = Field(18, description="User age")

with DAG("awel_say_hello_post") as dag:
    trigger_task = HttpTrigger(
        endpoint="/awel_tutorial/say_hello_post", 
        methods="POST", 
        request_body=TriggerReqBody,
        status_code=200
    )
    task = MapOperator(
        map_function=lambda x: {"message": f"Hello, {x.name}! You are {x.age} years old."}
    )
    trigger_task >> task

setup_dev_environment([dag], port=5555)

----------------------------------------

TITLE: Non-Streaming Chat Completion with DB-GPT API using cURL
DESCRIPTION: This snippet demonstrates how to make a non-streaming chat completion request to the DB-GPT API using cURL. It includes the necessary headers and request body for authentication and specifying chat parameters.

LANGUAGE: shell
CODE:
DBGPT_API_KEY="dbgpt"

curl -X POST "http://localhost:5670/api/v2/chat/completions" \
   -H "Authorization: Bearer $DBGPT_API_KEY" \
   -H "accept: application/json" \
   -H "Content-Type: application/json" \
   -d "{\"messages\":\"Hello\",\"model\":\"gpt-4o\", \"stream\": false}"

----------------------------------------

TITLE: Implementing Custom Streaming Operators in Python
DESCRIPTION: This snippet demonstrates creating two custom streaming operators: NumberProducerOperator and NumberDoubleOperator. It shows how to implement streaming logic and chain operators in a DAG.

LANGUAGE: python
CODE:
import asyncio
from typing import AsyncIterator
from dbgpt.core.awel import DAG, StreamifyAbsOperator, TransformStreamAbsOperator

class NumberProducerOperator(StreamifyAbsOperator[int, int]):
    async def streamify(self, n: int) -> AsyncIterator[int]:
        for i in range(n):
            yield i

class NumberDoubleOperator(TransformStreamAbsOperator[int, int]):
    async def transform_stream(self, it: AsyncIterator) -> AsyncIterator[int]:
        async for i in it:
            # Double the number
            yield i * 2
            
with DAG("numbers_dag") as dag:
    task = NumberProducerOperator()
    double_task = NumberDoubleOperator()
    task >> double_task

async def helper_call_fn(t, n: int):
    # Call the streaming operator by `call_stream` method
    async for i in await t.call_stream(call_data=n):
        print(i)
        
asyncio.run(helper_call_fn(double_task, 10))

----------------------------------------

TITLE: Initializing Basic Short-term Memory in Python
DESCRIPTION: Creates a basic short-term memory instance with a specified buffer size and initializes an agent memory object. The memory operates on a FIFO (First In, First Out) basis with a configurable buffer size.

LANGUAGE: python
CODE:
from dbgpt.agent import AgentMemory, ShortTermMemory

# Create an agent memory, which contains a short-term memory
memory = ShortTermMemory(buffer_size=2)
agent_memory: AgentMemory = AgentMemory(memory=memory)

----------------------------------------

TITLE: Generating Prompts from Agent Profile in Python
DESCRIPTION: Demonstrates how to create profile configuration, generate a profile instance, and format system and user prompts from the profile settings.

LANGUAGE: python
CODE:
from dbgpt.agent import ProfileConfig

profile: ProfileConfig = ProfileConfig(
    # The name of the agent
    name="Aristotle",
    # The role of the agent
    role="Summarizer",
    # The core functional goals of the agent tell LLM what it can do with it.
    goal=(
        "Summarize answer summaries based on user questions from provided "
        "resource information or from historical conversation memories."
    ),
    # Introduction and description of the agent, used for task assignment and display. 
    # If it is empty, the goal content will be used.
    desc=(
        "You can summarize provided text content according to user's questions"
        " and output the summarization."
    ),
)

# Create a profile from the configuration
real_profile = profile.create_profile()
system_prompt = real_profile.format_system_prompt(question="What can you do?")
user_prompt = real_profile.format_user_prompt(question="What can you do?")

print(f"System Prompt: \n{system_prompt}")
print("#" * 50)
print(f"User Prompt: \n{user_prompt}")

----------------------------------------

TITLE: Defining Custom Prompt Templates in Python
DESCRIPTION: Shows how to create custom system and user prompt templates using Jinja2 syntax. The templates define the structure for agent interactions including role, name, goal and language preferences.

LANGUAGE: python
CODE:
my_system_prompt_template = """\
You are a {{ role }}, {% if name %}named {{ name }}, {% endif %}your goal is {{ goal }}.
Please think step by step to achieve the goal. You can use the resources given below. 
At the same time, please strictly abide by the constraints and specifications in IMPORTANT REMINDER.

*** IMPORTANT REMINDER ***
{% if language == 'zh' %}\
Please answer in simplified Chinese.
{% else %}\
Please answer in English.
{% endif %}\"""

my_user_prompt_template = "User question: {{ question }}"

----------------------------------------

TITLE: Generating Prompts from ProfileConfig in Python
DESCRIPTION: Shows how to generate system and user prompts from a created profile configuration.

LANGUAGE: python
CODE:
real_profile = profile.create_profile()
system_prompt = real_profile.format_system_prompt(question="What can you do?")
user_prompt = real_profile.format_user_prompt(question="What can you do?")
print(f"System Prompt: \n{system_prompt}")
print("#" * 50)
print(f"User Prompt: \n{user_prompt}")

----------------------------------------

TITLE: Implementing Hello World Custom Operator in Python
DESCRIPTION: This snippet shows how to create a custom HelloWorldOperator that inherits from MapOperator. It demonstrates the basic structure of a custom operator and how to use it within a DAG.

LANGUAGE: python
CODE:
import asyncio
from dbgpt.core.awel import DAG, MapOperator

class HelloWorldOperator(MapOperator[str, None]):
    async def map(self, x: str) -> None:
        print(f"Hello, {x}!")

with DAG("awel_hello_world") as dag:
    task = HelloWorldOperator()
    
asyncio.run(task.call(call_data="world"))

----------------------------------------

TITLE: Verifying Memory Retention in Python with DB-GPT Agents
DESCRIPTION: This function demonstrates how to create agents, initiate chats, and verify that the agent remembers previous conversations within the buffer size limit.

LANGUAGE: python
CODE:
import asyncio
from dbgpt.agent import (
    ConversableAgent,
    ProfileConfig,
    LLMConfig,
    BlankAction,
    UserProxyAgent,
)

async def verify_remember():
    joy = (
        await ConversableAgent(profile=ProfileConfig(name="Joy", role="Comedians"))
        .bind(context)
        .bind(LLMConfig(llm_client=llm_client))
        .bind(agent_memory)
        .bind(BlankAction)
        .build()
    )
    user_proxy = await UserProxyAgent().bind(agent_memory).bind(context).build()
    # The turns not more than 2, make sure the agent remembers the previous conversation
    for message in messages[:2]:
        await user_proxy.initiate_chat(
            recipient=joy,
            reviewer=user_proxy,
            message=message,
        )
    await user_proxy.initiate_chat(
        recipient=joy,
        reviewer=user_proxy,
        message="How old was I when I went to primary school?"
    )

if __name__ == "__main__":
    asyncio.run(verify_remember())

----------------------------------------

TITLE: Initializing OpenAI Embedding Model in Python
DESCRIPTION: Sets up an embedding model using OpenAI's API for converting text into vectors. Requires OpenAI API credentials through environment variables.

LANGUAGE: python
CODE:
import os
from dbgpt.rag.embedding import DefaultEmbeddingFactory

api_url = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1") + "/embeddings"
api_key = os.getenv("OPENAI_API_KEY")
embeddings = DefaultEmbeddingFactory.openai(api_url=api_url, api_key=api_key)

----------------------------------------

TITLE: Implementing Custom UnstreamifyAbsOperator in Python
DESCRIPTION: This snippet demonstrates how to create a custom UnstreamifyAbsOperator by overriding the unstreamify method. It shows a SumOperator that sums a stream of integers.

LANGUAGE: python
CODE:
from typing import AsyncIterator
from dbgpt.core.awel import DAG, UnstreamifyAbsOperator

class SumOperator(UnstreamifyAbsOperator[int, int]):
    """Unstreamify the stream of numbers"""
    async def unstreamify(self, it: AsyncIterator[int]) -> int:
        return sum([i async for i in it])

with DAG("sum_dag") as dag:
    task = SumOperator()

----------------------------------------

TITLE: Implementing HTTP Trigger for JSON Response in Python
DESCRIPTION: Python code to create an HTTP trigger that returns a JSON response. It uses the same AWEL DAG structure but modifies the output to return a dictionary.

LANGUAGE: python
CODE:
from dbgpt._private.pydantic import BaseModel, Field
from dbgpt.core.awel import DAG, HttpTrigger, MapOperator, setup_dev_environment

class TriggerReqBody(BaseModel):
    name: str = Field(..., description="User name")
    age: int = Field(18, description="User age")

with DAG("awel_say_hello_json") as dag:
    trigger_task = HttpTrigger(
        endpoint="/awel_tutorial/say_hello_json", 
        methods="GET", 
        request_body=TriggerReqBody,
    )
    task = MapOperator(
        map_function=lambda x: {"message": f"Hello, {x.name}! You are {x.age} years old."}
    )
    trigger_task >> task

setup_dev_environment([dag], port=5555)

----------------------------------------

TITLE: Using ProfileFactory with ProfileConfig in Python
DESCRIPTION: Shows how to use a custom ProfileFactory with ProfileConfig and generate prompts.

LANGUAGE: python
CODE:
from dbgpt.agent import ProfileConfig

profile: ProfileConfig = ProfileConfig(
    factory=MyProfileFactory(),
)

real_profile = profile.create_profile()
system_prompt = real_profile.format_system_prompt(question="What can you do?")
user_prompt = real_profile.format_user_prompt(question="What can you do?")
print(f"System Prompt: \n{system_prompt}")
print("#" * 50)
print(f"User Prompt: \n{user_prompt}")

----------------------------------------

TITLE: Rendering Feedback Serve Configuration in JSX
DESCRIPTION: This code snippet uses a custom React component called ConfigDetail to render the configuration details for the feedback serve module. It includes the configuration name, description, and parameters.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "ServeConfig",
  "description": "This configuration is for the feedback serve module.",
  "documentationUrl": null,
  "parameters": [
    {
      "name": "api_keys",
      "type": "string",
      "required": false,
      "description": "API keys for the endpoint, if None, allow all"
    }
  ]
}} />

----------------------------------------

TITLE: Implementing Claude AI Interaction Functions in Python
DESCRIPTION: Defines functions for sending prompts to Claude AI and processing the responses. It includes methods for single interactions and conversations with message history.

LANGUAGE: Python
CODE:
def send_prompt(prompt):
    response = client.completions.create(
        model=model,
        prompt=f"{HUMAN_PROMPT} {prompt}{AI_PROMPT}",
        max_tokens_to_sample=max_tokens_to_sample,
        temperature=temperature,
    )
    return response.completion

def send_prompt_with_history(prompt, history):
    messages = []
    for h in history:
        messages.append({"role": "user", "content": h["human"]})
        messages.append({"role": "assistant", "content": h["ai"]})
    messages.append({"role": "user", "content": prompt})
    
    response = client.messages.create(
        model=model,
        messages=messages,
        max_tokens=max_tokens_to_sample,
        temperature=temperature,
    )
    return response.content[0].text

----------------------------------------

TITLE: Displaying DB-GPT CLI Help Information in Python
DESCRIPTION: Shows the main help output for the DB-GPT command line interface, listing available commands and general options.

LANGUAGE: python
CODE:
~ dbgpt --help
Already connect 'dbgpt'
Usage: dbgpt [OPTIONS] COMMAND [ARGS]...

Options:
  --log-level TEXT  Log level
  --version         Show the version and exit.
  --help            Show this message and exit.

Commands:
  install    Install dependencies, plugins, etc.
  knowledge  Knowledge command line tool
  model      Clients that manage model serving
  start      Start specific server.
  stop       Stop specific server.
  trace      Analyze and visualize trace spans.

----------------------------------------

TITLE: Creating Custom ReduceStreamOperator Class - Python
DESCRIPTION: Demonstrates how to implement a custom ReduceStreamOperator by creating a subclass that defines a reduce method for summing integers.

LANGUAGE: python
CODE:
from dbgpt.core.awel import DAG, ReduceStreamOperator

class MySumOperator(ReduceStreamOperator[int, int]):
    async def reduce(self, x: int, y: int) -> int:
        return x + y

with DAG("awel_reduce_operator") as dag:
    task = MySumOperator()

----------------------------------------

TITLE: Preparing OpenAI Embedding Model for Hybrid Memory in Python
DESCRIPTION: Shows how to set up the OpenAI Embedding API for use with the Hybrid Memory system.

LANGUAGE: python
CODE:
import os
from dbgpt.rag.embedding import DefaultEmbeddingFactory

api_url = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1") + "/embeddings"
api_key = os.getenv("OPENAI_API_KEY")
embeddings = DefaultEmbeddingFactory.openai(api_url=api_url, api_key=api_key)

----------------------------------------

TITLE: Configuring RDBMS Datasource Parameters with React Component
DESCRIPTION: React/JSX component implementation for documenting RDBMS connection parameters. Includes configuration for database connection details, pool settings, and authentication options. Uses the ConfigDetail component to render parameter specifications.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "RDBMSDatasourceParameters",
  "description": "RDBMS datasource parameters.",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "port",
      "type": "integer",
      "required": true,
      "description": "Database port, e.g., 3306"
    },
    {
      "name": "user",
      "type": "string",
      "required": true,
      "description": "Database user to connect"
    },
    {
      "name": "database",
      "type": "string",
      "required": true,
      "description": "Database name"
    },
    {
      "name": "driver",
      "type": "string",
      "required": true,
      "description": "Database driver, e.g., mysql+pymysql"
    },
    {
      "name": "password",
      "type": "string",
      "required": false,
      "description": "Database password, you can write your password directly, of course, you can also use environment variables, such as ${env:DBGPT_DB_PASSWORD}",
      "defaultValue": "${env:DBGPT_DB_PASSWORD}"
    },
    {
      "name": "pool_size",
      "type": "integer",
      "required": false,
      "description": "Connection pool size, default 5",
      "defaultValue": "5"
    },
    {
      "name": "max_overflow",
      "type": "integer",
      "required": false,
      "description": "Max overflow connections, default 10",
      "defaultValue": "10"
    },
    {
      "name": "pool_timeout",
      "type": "integer",
      "required": false,
      "description": "Connection pool timeout, default 30",
      "defaultValue": "30"
    },
    {
      "name": "pool_recycle",
      "type": "integer",
      "required": false,
      "description": "Connection pool recycle, default 3600",
      "defaultValue": "3600"
    },
    {
      "name": "pool_pre_ping",
      "type": "boolean",
      "required": false,
      "description": "Connection pool pre ping, default True",
      "defaultValue": "True"
    },
    {
      "name": "host",
      "type": "string",
      "required": true,
      "description": "Database host, e.g., localhost"
    }
  ]
}} />

----------------------------------------

TITLE: Custom Memory Agent Implementation in Python
DESCRIPTION: Example of creating a custom agent by extending ConversableAgent with custom memory reading and writing implementations.

LANGUAGE: python
CODE:
from typing import Optional
from dbgpt.agent import (
    ConversableAgent,
    AgentMemoryFragment,
    ProfileConfig,
    BlankAction,
    ActionOutput,
)

write_memory_template = """\
{% if question %}user: {{ question }} {% endif %}
{% if thought %}assistant: {{ thought }} {% endif %}\
"""

class JoyAgent(ConversableAgent):
    profile: ProfileConfig = ProfileConfig(
        name="Joy",
        role="Comedians",
        write_memory_template=write_memory_template,
    )

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._init_actions([BlankAction])

    async def read_memories(self, question: str) -> str:
        memories = await self.memory.read(observation=question)
        recent_messages = [m.raw_observation for m in memories]
        return "".join(recent_messages)

    async def write_memories(
        self,
        question: str,
        ai_message: str,
        action_output: Optional[ActionOutput] = None,
        check_pass: bool = True,
        check_fail_reason: Optional[str] = None,
    ) -> None:
        if not action_output:
            raise ValueError("Action output is required to save to memory.")

        mem_thoughts = action_output.thoughts or ai_message
        memory_map = {
            "question": question,
            "thought": mem_thoughts,
        }
        write_memory_template = self.write_memory_template
        memory_content: str = self._render_template(write_memory_template, **memory_map)
        fragment = AgentMemoryFragment(memory_content)
        await self.memory.write(fragment)

----------------------------------------

TITLE: Building Llama-cpp Docker Image
DESCRIPTION: Creates a CUDA-based Docker image with Llama-cpp support and CUDA acceleration enabled.

LANGUAGE: bash
CODE:
bash docker/base/build_image.sh --install-mode llama-cpp

----------------------------------------

TITLE: Implementing BranchOperator with Branch Mapping in Python
DESCRIPTION: This snippet demonstrates how to create a BranchOperator using a dictionary of branch functions and task names. It includes a DAG setup with even and odd tasks.

LANGUAGE: python
CODE:
from dbgpt.core.awel import DAG, BranchOperator, MapOperator

def branch_even(x: int) -> bool:
    return x % 2 == 0

def branch_odd(x: int) -> bool:
    return not branch_even(x)

branch_mapping = {
    branch_even: "even_task",
    branch_odd: "odd_task"
}

with DAG("awel_branch_operator") as dag:
    task = BranchOperator(branches=branch_mapping)
    even_task = MapOperator(
        task_name="even_task", 
        map_function=lambda x: print(f"{x} is even")
    )
    odd_task = MapOperator(
        task_name="odd_task", 
        map_function=lambda x: print(f"{x} is odd")
    )

----------------------------------------

TITLE: Stream Chat App Python Client Implementation
DESCRIPTION: Python client example using dbgpt_client to make streaming chat requests with app integration.

LANGUAGE: python
CODE:
from dbgpt_client import Client

DBGPT_API_KEY = "dbgpt"
APP_ID="{YOUR_APP_ID}"

client = Client(api_key=DBGPT_API_KEY)

async for data in client.chat_stream(
    messages="Introduce AWEL", 
    model="gpt-4o", 
    chat_mode="chat_app", 
    chat_param=APP_ID
):
    print(data)

----------------------------------------

TITLE: Configuring ModelsDeployParameters Component in React/MDX
DESCRIPTION: React/MDX component that renders detailed configuration documentation for ModelsDeployParameters, including parameter definitions, types, and nested configurations for different model deployments.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "ModelsDeployParameters",
  "description": "ModelsDeployParameters(default_llm: Optional[str] = None, default_embedding: Optional[str] = None, default_reranker: Optional[str] = None, llms: List[dbgpt.core.interface.parameter.LLMDeployModelParameters] = <factory>, embeddings: List[dbgpt.core.interface.parameter.EmbeddingDeployModelParameters] = <factory>, rerankers: List[dbgpt.core.interface.parameter.RerankerDeployModelParameters] = <factory>)",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "default_embedding",
      "type": "string",
      "required": false,
      "description": "Default embedding model name, used to specify which model to use when you have multiple embedding models"
    },
    {
      "name": "default_reranker",
      "type": "string",
      "required": false,
      "description": "Default reranker model name, used to specify which model to use when you have multiple reranker models"
    },
    {
      "name": "llms",
      "type": "LLMDeployModelParameters",
      "required": false,
      "description": "LLM model deploy configuration. If you deploy in cluster mode, you just deploy one model.",
      "nestedTypes": [...],
      "defaultValue": "[]"
    },
    {
      "name": "embeddings",
      "type": "EmbeddingDeployModelParameters",
      "required": false,
      "description": "Embedding model deploy configuration. If you deploy in cluster mode, you just deploy one model.",
      "nestedTypes": [...],
      "defaultValue": "[]"
    },
    {
      "name": "rerankers",
      "type": "RerankerDeployModelParameters",
      "required": false,
      "description": "Reranker model deploy configuration. If you deploy in cluster mode, you just deploy one model.",
      "nestedTypes": [...],
      "defaultValue": "[]"
    },
    {
      "name": "default_llm",
      "type": "string",
      "required": false,
      "description": "Default LLM model name, used to specify which model to use when you have multiple LLMs"
    }
  ]
}} />

----------------------------------------

TITLE: Visualizing AWEL DAGs in Python
DESCRIPTION: Demonstrates how to visualize the created DAGs using the visualize_dag() method. This is useful for understanding the structure of the RAG workflow.

LANGUAGE: python
CODE:
knowledge_dag.visualize_dag()
rag_dag.visualize_dag()

# For Jupyter Notebook
display(knowledge_dag.show())
display(rag_dag.show())

----------------------------------------

TITLE: Configuring OpenAI Embeddings for Enhanced Memory
DESCRIPTION: Sets up OpenAI embeddings for enhanced short-term memory using environment variables for API configuration. Requires OpenAI API credentials and endpoint URL.

LANGUAGE: python
CODE:
import os
from dbgpt.rag.embedding import DefaultEmbeddingFactory

api_url = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1") + "/embeddings"
api_key = os.getenv("OPENAI_API_KEY")
embeddings = DefaultEmbeddingFactory.openai(api_url=api_url, api_key=api_key)

----------------------------------------

TITLE: Initializing LLM Config and Agent Memory Setup in Python
DESCRIPTION: Sets up the basic configuration for LLM client, agent context, and memory. Defines system and user prompt templates for agent conversations.

LANGUAGE: python
CODE:
import os
from dbgpt.agent import AgentContext, AgentMemory
from dbgpt.model.proxy import OpenAILLMClient

llm_client = OpenAILLMClient(
    model_alias="gpt-4o",
    api_base=os.getenv("OPENAI_API_BASE"),
    api_key=os.getenv("OPENAI_API_KEY"),
)

context: AgentContext = AgentContext(
    conv_id="test123",
    language="en",
    temperature=0.9,
    max_new_tokens=2048,
    max_chat_round=4,
)
# Create an agent memory, default memory is ShortTermMemory
agent_memory: AgentMemory = AgentMemory()


system_prompt_template = """
You are a {{ role }}, {% if name %}named {{ name }}, {% endif %}your goal is {{ goal }}.
*** IMPORTANT REMINDER ***
{% if language == 'zh' %}
Please answer in simplified Chinese.
{% else %}
Please answer in English.
{% endif %}
"""  # noqa

user_prompt_template = """
{% if most_recent_memories %}
Most recent observations:
{{ most_recent_memories }}
{% endif %}
{% if question %}
user: {{ question }}
{% endif %}
"""

----------------------------------------

TITLE: Listing Datasources using Python
DESCRIPTION: Demonstrates how to list all datasources using the DB-GPT client in Python.

LANGUAGE: python
CODE:
from dbgpt_client import Client
from dbgpt_client.datasource import list_datasource

DBGPT_API_KEY = "dbgpt"

client = Client(api_key=DBGPT_API_KEY)
res = await list_datasource(client=client)

----------------------------------------

TITLE: Implementing Custom MapOperator Class in Python
DESCRIPTION: Shows how to create a custom MapOperator by extending the base class. This implementation defines a typed operator that takes a string input and returns None, useful for cases requiring more complex logic.

LANGUAGE: python
CODE:
from dbgpt.core.awel import DAG, MapOperator

class MyMapOperator(MapOperator[str, None]):
    async def map(self, x: str) -> None:
        print(f"Hello, {x}!")

with DAG("awel_hello_world") as dag:
    task = MyMapOperator()

----------------------------------------

TITLE: Rendering ConfigDiagram with Relationship Data in JSX
DESCRIPTION: This JSX code renders the ConfigDiagram component, passing a complex object of relationship data as a prop. The data describes the connections between various configuration classes in the DB-GPT project.

LANGUAGE: JSX
CODE:
<ConfigDiagram relationships={[
  {
    "from": "ApplicationConfig",
    "to": "HookConfig",
    "label": "hooks"
  },
  {
    "from": "ApplicationConfig",
    "to": "SystemParameters",
    "label": "system"
  },
  // ... (many more relationship objects)
  {
    "from": "ApplicationConfig",
    "to": "LoggingParameters",
    "label": "log"
  }
]} />

----------------------------------------

TITLE: Configuring ModelAPIServerParameters Component in JSX
DESCRIPTION: React/JSX component implementation for displaying ModelAPIServerParameters configuration details with nested parameter definitions and documentation links.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "ModelAPIServerParameters",
  "description": "ModelAPIServerParameters(host: Optional[str] = '0.0.0.0', port: Optional[int] = 8100, daemon: Optional[bool] = False, log: dbgpt.util.utils.LoggingParameters = <factory>, trace: Optional[dbgpt.util.tracer.tracer_impl.TracerParameters] = None, controller_addr: Optional[str] = 'http://127.0.0.1:8000', api_keys: Optional[str] = None, embedding_batch_size: Optional[int] = None, ignore_stop_exceeds_error: Optional[bool] = False)",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "port",
      "type": "integer",
      "required": false,
      "description": "Model API server deploy port",
      "defaultValue": "8100"
    },
    {
      "name": "daemon",
      "type": "boolean",
      "required": false,
      "description": "Run the server as a daemon.",
      "defaultValue": "False"
    },
    {
      "name": "log",
      "type": "LoggingParameters",
      "required": false,
      "description": "Logging configuration",
      "nestedTypes": [
        {
          "type": "link",
          "text": "loggingparameters configuration",
          "url": "././utils_loggingparameters_4ba5c6"
        }
      ],
      "defaultValue": "LoggingParameters"
    },
    {
      "name": "trace",
      "type": "TracerParameters",
      "required": false,
      "description": "Tracer configuration",
      "nestedTypes": [
        {
          "type": "link",
          "text": "tracerparameters configuration",
          "url": "././tracer_impl_tracerparameters_f8f272"
        }
      ]
    },
    {
      "name": "controller_addr",
      "type": "string",
      "required": false,
      "description": "The Model controller address to connect",
      "defaultValue": "http://127.0.0.1:8000"
    },
    {
      "name": "api_keys",
      "type": "string",
      "required": false,
      "description": "Optional list of comma separated API keys"
    },
    {
      "name": "embedding_batch_size",
      "type": "integer",
      "required": false,
      "description": "Embedding batch size"
    },
    {
      "name": "ignore_stop_exceeds_error",
      "type": "boolean",
      "required": false,
      "description": "Ignore exceeds stop words error",
      "defaultValue": "False"
    },
    {
      "name": "host",
      "type": "string",
      "required": false,
      "description": "The host IP address to bind to.",
      "defaultValue": "0.0.0.0"
    }
  ]
}} />

----------------------------------------

TITLE: Implementing Enhanced Short-term Memory
DESCRIPTION: Creates an enhanced short-term memory instance with similarity-based enhancement and threading support. Configures memory parameters including buffer size, similarity threshold, and enhancement threshold.

LANGUAGE: python
CODE:
from concurrent.futures import ThreadPoolExecutor
from dbgpt.agent import AgentMemory, EnhancedShortTermMemory

# Create an agent memory, which contains a short-term memory
memory = EnhancedShortTermMemory(
    embeddings=embeddings,
    buffer_size=2,
    enhance_similarity_threshold=0.5,
    enhance_threshold=3,
    executor=ThreadPoolExecutor(),
)
agent_memory: AgentMemory = AgentMemory(memory=memory)

----------------------------------------

TITLE: Viewing Service Runtime Information in DB-GPT
DESCRIPTION: This command displays service runtime information for DB-GPT, including configuration details for the webserver, embedding model, model worker, and worker manager.

LANGUAGE: python
CODE:
dbgpt trace chat --hide_conv

----------------------------------------

TITLE: Implementing Basic ReduceStreamOperator with Function - Python
DESCRIPTION: Shows how to create a basic ReduceStreamOperator using a lambda function to perform addition operations within a DAG context.

LANGUAGE: python
CODE:
from dbgpt.core.awel import DAG, ReduceStreamOperator

with DAG("awel_reduce_operator") as dag:
    task = ReduceStreamOperator(reduce_function=lambda x, y: x + y)

----------------------------------------

TITLE: Rendering ModelCacheParameters Configuration using ConfigDetail Component in JSX
DESCRIPTION: This code snippet uses a custom ConfigDetail component to render the configuration details for ModelCacheParameters. It includes parameters for storage type, max memory, persist directory, and model cache enablement.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "ModelCacheParameters",
  "description": "Model cache configuration.",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "storage_type",
      "type": "string",
      "required": false,
      "description": "The storage type, default is memory",
      "defaultValue": "memory"
    },
    {
      "name": "max_memory_mb",
      "type": "integer",
      "required": false,
      "description": "The max memory in MB, default is 256",
      "defaultValue": "256"
    },
    {
      "name": "persist_dir",
      "type": "string",
      "required": false,
      "description": "The persist directory, default is model_cache",
      "defaultValue": "model_cache"
    },
    {
      "name": "enable_model_cache",
      "type": "boolean",
      "required": false,
      "description": "Whether to enable model cache, default is True",
      "defaultValue": "True"
    }
  ]
}} />

----------------------------------------

TITLE: Creating MySQL Table for DB-GPT Cluster Registry
DESCRIPTION: SQL script to create the required table for storing model registry information in MySQL database. The table handles model instance metadata including health status, connection details, and timestamps.

LANGUAGE: sql
CODE:
CREATE TABLE IF NOT EXISTS `dbgpt_cluster_registry_instance` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT 'Auto increment id',
  `model_name` varchar(128) NOT NULL COMMENT 'Model name',
  `host` varchar(128) NOT NULL COMMENT 'Host of the model',
  `port` int(11) NOT NULL COMMENT 'Port of the model',
  `weight` float DEFAULT 1.0 COMMENT 'Weight of the model',
  `check_healthy` tinyint(1) DEFAULT 1 COMMENT 'Whether to check the health of the model',
  `healthy` tinyint(1) DEFAULT 0 COMMENT 'Whether the model is healthy',
  `enabled` tinyint(1) DEFAULT 1 COMMENT 'Whether the model is enabled',
  `prompt_template` varchar(128) DEFAULT NULL COMMENT 'Prompt template for the model instance',
  `last_heartbeat` datetime DEFAULT NULL COMMENT 'Last heartbeat time of the model instance',
  `user_name` varchar(128) DEFAULT NULL COMMENT 'User name',
  `sys_code` varchar(128) DEFAULT NULL COMMENT 'System code',
  `gmt_created` datetime DEFAULT CURRENT_TIMESTAMP COMMENT 'Record creation time',
  `gmt_modified` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'Record update time',
  PRIMARY KEY (`id`),
  UNIQUE KEY `uk_model_instance` (`model_name`, `host`, `port`, `sys_code`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COMMENT='Cluster model instance table, for registering and managing model instances';

----------------------------------------

TITLE: Preparing OpenAI Embedding Model
DESCRIPTION: Code to initialize the OpenAI embedding model for use in the project.

LANGUAGE: python
CODE:
from dbgpt.rag.embedding import DefaultEmbeddingFactory

embeddings = DefaultEmbeddingFactory.openai()

----------------------------------------

TITLE: Rendering ServeConfig Configuration Details in React
DESCRIPTION: This code snippet uses a custom React component called ConfigDetail to display the configuration details for the ServeConfig. It includes the configuration name, description, and parameters such as API keys.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "ServeConfig",
  "description": "This configuration is for the libro serve module.",
  "documentationUrl": null,
  "parameters": [
    {
      "name": "api_keys",
      "type": "string",
      "required": false,
      "description": "API keys for the endpoint, if None, allow all"
    }
  ]
}} />

----------------------------------------

TITLE: Creating a SQLite Database Connector from File Path in Python
DESCRIPTION: This code snippet shows how to create a SQLite database connector by specifying the file path to an existing SQLite database. It requires the correct file path to be provided.

LANGUAGE: python
CODE:
from dbgpt.datasource.rdbms.conn_sqlite import SQLiteConnector

connector = SQLiteConnector.from_file_path("path/to/your/database.db")

----------------------------------------

TITLE: Viewing Latest Conversation Information in DB-GPT
DESCRIPTION: This command displays the latest conversation information in DB-GPT, including trace details, user input, chat mode, and model output.

LANGUAGE: python
CODE:
dbgpt trace chat --hide_run_params

----------------------------------------

TITLE: Retrieving a Datasource using cURL
DESCRIPTION: Shows how to retrieve information about a specific datasource using a cURL GET request, including authentication and datasource ID.

LANGUAGE: shell
CODE:
DBGPT_API_KEY=dbgpt
DATASOURCE_ID={YOUR_DATASOURCE_ID}

curl -X GET "http://localhost:5670/api/v2/serve/datasources/$DATASOURCE_ID" -H "Authorization: Bearer $DBGPT_API_KEY"

----------------------------------------

TITLE: Creating New Tables for DB-GPT v0.5.0 Upgrade in MySQL
DESCRIPTION: SQL statements to create new tables required for the DB-GPT v0.5.0 upgrade. This includes tables for serve flow, GPT apps, app collections, and app details.

LANGUAGE: sql
CODE:
-- dbgpt.dbgpt_serve_flow definition
CREATE TABLE `dbgpt_serve_flow` (
  `id` int NOT NULL AUTO_INCREMENT COMMENT 'Auto increment id',
  `uid` varchar(128) NOT NULL COMMENT 'Unique id',
  `dag_id` varchar(128) DEFAULT NULL COMMENT 'DAG id',
  `name` varchar(128) DEFAULT NULL COMMENT 'Flow name',
  `flow_data` text COMMENT 'Flow data, JSON format',
  `user_name` varchar(128) DEFAULT NULL COMMENT 'User name',
  `sys_code` varchar(128) DEFAULT NULL COMMENT 'System code',
  `gmt_created` datetime DEFAULT NULL COMMENT 'Record creation time',
  `gmt_modified` datetime DEFAULT NULL COMMENT 'Record update time',
  `flow_category` varchar(64) DEFAULT NULL COMMENT 'Flow category',
  `description` varchar(512) DEFAULT NULL COMMENT 'Flow description',
  `state` varchar(32) DEFAULT NULL COMMENT 'Flow state',
  `source` varchar(64) DEFAULT NULL COMMENT 'Flow source',
  `source_url` varchar(512) DEFAULT NULL COMMENT 'Flow source url',
  `version` varchar(32) DEFAULT NULL COMMENT 'Flow version',
  `label` varchar(128) DEFAULT NULL COMMENT 'Flow label',
  `editable` int DEFAULT NULL COMMENT 'Editable, 0: editable, 1: not editable',
  PRIMARY KEY (`id`),
  UNIQUE KEY `uk_uid` (`uid`),
  KEY `ix_dbgpt_serve_flow_sys_code` (`sys_code`),
  KEY `ix_dbgpt_serve_flow_uid` (`uid`),
  KEY `ix_dbgpt_serve_flow_dag_id` (`dag_id`),
  KEY `ix_dbgpt_serve_flow_user_name` (`user_name`),
  KEY `ix_dbgpt_serve_flow_name` (`name`)
) ENGINE=InnoDB AUTO_INCREMENT=15 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;

-- dbgpt.gpts_app definition
CREATE TABLE `gpts_app` (
  `id` int NOT NULL AUTO_INCREMENT COMMENT 'autoincrement id',
  `app_code` varchar(255) NOT NULL COMMENT 'Current AI assistant code',
  `app_name` varchar(255) NOT NULL COMMENT 'Current AI assistant name',
  `app_describe` varchar(2255) NOT NULL COMMENT 'Current AI assistant describe',
  `language` varchar(100) NOT NULL COMMENT 'gpts language',
  `team_mode` varchar(255) NOT NULL COMMENT 'Team work mode',
  `team_context` text COMMENT 'The execution logic and team member content that teams with different working modes rely on',
  `user_code` varchar(255) DEFAULT NULL COMMENT 'user code',
  `sys_code` varchar(255) DEFAULT NULL COMMENT 'system app code',
  `created_at` datetime DEFAULT NULL COMMENT 'create time',
  `updated_at` datetime DEFAULT NULL COMMENT 'last update time',
  `icon` varchar(1024) DEFAULT NULL COMMENT 'app icon, url',
  PRIMARY KEY (`id`),
  UNIQUE KEY `uk_gpts_app` (`app_name`)
) ENGINE=InnoDB AUTO_INCREMENT=39 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;

CREATE TABLE `gpts_app_collection` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT 'autoincrement id',
  `app_code` varchar(255) NOT NULL COMMENT 'Current AI assistant code',
  `user_code` int(11) NOT NULL COMMENT 'user code',
  `sys_code` varchar(255) NOT NULL COMMENT 'system app code',
  `created_at` datetime DEFAULT NULL COMMENT 'create time',
  `updated_at` datetime DEFAULT NULL COMMENT 'last update time',
  PRIMARY KEY (`id`),
  KEY `idx_app_code` (`app_code`),
  KEY `idx_user_code` (`user_code`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COMMENT="gpt collections";

-- dbgpt.gpts_app_detail definition
CREATE TABLE `gpts_app_detail` (
  `id` int NOT NULL AUTO_INCREMENT COMMENT 'autoincrement id',
  `app_code` varchar(255) NOT NULL COMMENT 'Current AI assistant code',
  `app_name` varchar(255) NOT NULL COMMENT 'Current AI assistant name',
  `agent_name` varchar(255) NOT NULL COMMENT ' Agent name',
  `node_id` varchar(255) NOT NULL COMMENT 'Current AI assistant Agent Node id',
  `resources` text COMMENT 'Agent bind  resource',
  `prompt_template` text COMMENT 'Agent bind  template',
  `llm_strategy` varchar(25) DEFAULT NULL COMMENT 'Agent use llm strategy',
  `llm_strategy_value` text COMMENT 'Agent use llm strategy value',
  `created_at` datetime DEFAULT NULL COMMENT 'create time',
  `updated_at` datetime DEFAULT NULL COMMENT 'last update time',
  PRIMARY KEY (`id`),
  UNIQUE KEY `uk_gpts_app_agent_node` (`app_name`,`agent_name`,`node_id`)
) ENGINE=InnoDB AUTO_INCREMENT=23 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;

----------------------------------------

TITLE: Initializing OpenAI LLM Client in Python
DESCRIPTION: Creates an OpenAI LLM client for use in the RAG program. Requires the openai library to be installed and the OPENAI_API_KEY environment variable to be set.

LANGUAGE: python
CODE:
from dbgpt.model.proxy import OpenAILLMClient

llm_client = OpenAILLMClient()

----------------------------------------

TITLE: Implementing Custom Summary Action
DESCRIPTION: Defines a custom action for summarization, including input model, resource needs, and execution logic.

LANGUAGE: python
CODE:
from typing import Optional
from pydantic import BaseModel, Field
from dbgpt.vis import Vis
from dbgpt.agent import Action, ActionOutput, AgentResource, ResourceType
from dbgpt.agent.util import cmp_string_equal

NOT_RELATED_MESSAGE = "Did not find the information you want."

class SummaryActionInput(BaseModel):
    summary: str = Field(
        ...,
        description="The summary content",
    )

class SummaryAction(Action[SummaryActionInput]):
    def __init__(self):
        super().__init__()

    @property
    def resource_need(self) -> Optional[ResourceType]:
        return None
    
    @property
    def render_protocol(self) -> Optional[Vis]:
        return None
    
    @property
    def out_model_type(self):
        return SummaryActionInput

    async def run(
        self,
        ai_message: str,
        resource: Optional[AgentResource] = None,
        rely_action_out: Optional[ActionOutput] = None,
        need_vis_render: bool = True,
        **kwargs,
    ) -> ActionOutput:
        try:
            param: SummaryActionInput = self._input_convert(ai_message, SummaryActionInput)
        except Exception:
            return ActionOutput(
                is_exe_success=False,
                content="The requested correctly structured answer could not be found, "
                f"ai message: {ai_message}",
            )
        if param.summary and cmp_string_equal(
            param.summary, 
            NOT_RELATED_MESSAGE,
            ignore_case=True,
            ignore_punctuation=True,
            ignore_whitespace=True,
        ):
            return ActionOutput(
                is_exe_success=False,
                content="the provided text content is not related to user questions at all."
                f"ai message: {ai_message}",
            )
        else:
            return ActionOutput(
                is_exe_success=True,
                content=param.summary,
            )

----------------------------------------

TITLE: Chat Completion Response Format
DESCRIPTION: This snippet illustrates the JSON format of a non-streaming chat completion response from the DB-GPT API. It includes details such as the conversation ID, model used, creation timestamp, and the complete assistant's reply.

LANGUAGE: json
CODE:
{
    "id": "a8321543-52e9-47a5-a0b6-3d997463f6a3",
    "object": "chat.completion",
    "created": 1710826792,
    "model": "gpt-4o",
    "choices": [
        {
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "Hello! How can I assist you today?"
            },
            "finish_reason": null
        }
    ],
    "usage": {
        "prompt_tokens": 0,
        "total_tokens": 0,
        "completion_tokens": 0
    }
}

----------------------------------------

TITLE: Retrieving a Datasource using Python
DESCRIPTION: Demonstrates how to retrieve information about a specific datasource using the DB-GPT client in Python.

LANGUAGE: python
CODE:
from dbgpt_client import Client
from dbgpt_client.datasource import get_datasource

DBGPT_API_KEY = "dbgpt"
datasource_id = "{your_datasource_id}"

client = Client(api_key=DBGPT_API_KEY)
res = await get_datasource(client=client, datasource_id=datasource_id)

----------------------------------------

TITLE: Starting Embedding Model Worker
DESCRIPTION: Command to start a text embedding model worker that connects to multiple controller instances for redundancy.

LANGUAGE: shell
CODE:
dbgpt start worker --model_name text2vec \
--model_path /app/models/text2vec-large-chinese \
--worker_type text2vec \
--port 8003 \
--controller_addr "http://server1:8000,http://server2:8000"

----------------------------------------

TITLE: Configuring Ollama Model Settings in Python
DESCRIPTION: Defines the configuration settings for the Ollama model, including the model name, API base URL, and various parameters for text generation.

LANGUAGE: python
CODE:
model_name = "llama2"
api_base = "http://localhost:11434"

model_params = {
    "stop": ["Human:", "Assistant:"],
    "num_predict": 128,
    "temperature": 0.5,
    "repeat_penalty": 1.1,
    "top_k": 40,
    "top_p": 0.9
}

----------------------------------------

TITLE: Configuring OpenAI Environment Variables
DESCRIPTION: Setting up environment variables for OpenAI API authentication and endpoint configuration

LANGUAGE: bash
CODE:
export OPENAI_API_KEY=sk-xx
export OPENAI_API_BASE=https://xx:80/v1

----------------------------------------

TITLE: Deploying Web Server for DB-GPT
DESCRIPTION: Command to start the DB-GPT web server with remote embedding and connection to multiple controller instances.

LANGUAGE: shell
CODE:
LLM_MODEL=glm-4-9b-chat EMBEDDING_MODEL=text2vec \
dbgpt start webserver \
--light \
--remote_embedding \
--controller_addr "http://server1:8000,http://server2:8000"

----------------------------------------

TITLE: Verifying Memory Forgetting in Python with DB-GPT Agents
DESCRIPTION: This function demonstrates how the agent forgets previous conversations when the number of messages exceeds the buffer size, showcasing the special feature of SensoryMemory.

LANGUAGE: python
CODE:
async def verify_forget():
    joy = (
        await ConversableAgent(profile=ProfileConfig(name="Joy", role="Comedians"))
        .bind(context)
        .bind(LLMConfig(llm_client=llm_client))
        .bind(agent_memory)
        .bind(BlankAction)
        .build()
    )
    user_proxy = await UserProxyAgent().bind(agent_memory).bind(context).build()
    for message in messages:
        await user_proxy.initiate_chat(
            recipient=joy,
            reviewer=user_proxy,
            message=message,
        )
    await user_proxy.initiate_chat(
        recipient=joy,
        reviewer=user_proxy,
        message="How old was I when I went to primary school?",
    )


if __name__ == "__main__":
    asyncio.run(verify_forget())

----------------------------------------

TITLE: Docker Compose Configuration for DB-GPT with Jaeger
DESCRIPTION: Docker Compose YAML configuration for setting up a DB-GPT cluster with Jaeger for observability, including services for Jaeger, DB-GPT controller, LLM worker, embedding worker, and webserver.

LANGUAGE: yaml
CODE:
version: '3.10'

services:
  jaeger:
    image: jaegertracing/all-in-one:1.58
    restart: unless-stopped
    networks:
      - dbgptnet
    ports:
      - "16686:16686"
      - "6831:6831"
      - "4318:4318"
      - "4317:4317"
      - "14268:14268"
    environment:
      - LOG_LEVEL=debug
      - SPAN_STORAGE_TYPE=badger
      - BADGER_EPHEMERAL=false
      - BADGER_DIRECTORY_VALUE=/badger/data
      - BADGER_DIRECTORY_KEY=/badger/key
    volumes:
      - jaeger-badger:/badger
    user: root
  controller:
    image: eosphorosai/dbgpt:latest
    command: dbgpt start controller
    restart: unless-stopped
    environment:
      - TRACER_TO_OPEN_TELEMETRY=True
      - OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://jaeger:4317
      - DBGPT_LOG_LEVEL=DEBUG
    networks:
      - dbgptnet
  llm-worker:
    image: eosphorosai/dbgpt:latest
    command: dbgpt start worker --model_type proxy --model_name chatgpt_proxyllm --model_path chatgpt_proxyllm --proxy_server_url ${OPENAI_API_BASE}/chat/completions --proxy_api_key ${OPENAI_API_KEY} --controller_addr http://controller:8000
    environment:
      - PROXYLLM_BACKEND=gpt-3.5-turbo
      - TRACER_TO_OPEN_TELEMETRY=True
      - OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://jaeger:4317
      - DBGPT_LOG_LEVEL=DEBUG
    depends_on:
      - controller
    restart: unless-stopped
    networks:
      - dbgptnet
    ipc: host
  embedding-worker:
    image: eosphorosai/dbgpt:latest
    command: dbgpt start worker --worker_type text2vec --model_name proxy_http_openapi --model_path proxy_http_openapi --proxy_server_url ${OPENAI_API_BASE}/embeddings --proxy_api_key ${OPENAI_API_KEY} --controller_addr http://controller:8000
    environment:
      - proxy_http_openapi_proxy_backend=text-embedding-3-small
      - TRACER_TO_OPEN_TELEMETRY=True
      - OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://jaeger:4317
      - DBGPT_LOG_LEVEL=DEBUG
    depends_on:
      - controller
    restart: unless-stopped
    networks:
      - dbgptnet
    ipc: host
  webserver:
    image: eosphorosai/dbgpt:latest
    command: dbgpt start webserver --light --remote_embedding --controller_addr http://controller:8000
    environment:
      - LLM_MODEL=chatgpt_proxyllm
      - EMBEDDING_MODEL=proxy_http_openapi
      - TRACER_TO_OPEN_TELEMETRY=True
      - OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://jaeger:4317
    depends_on:
      - controller
      - llm-worker
      - embedding-worker
    volumes:
      - dbgpt-data:/app/pilot/data
      - dbgpt-message:/app/pilot/message
    ports:
      - 5670:5670/tcp
    restart: unless-stopped
    networks:
      - dbgptnet
volumes:
  dbgpt-data:
  dbgpt-message:
  jaeger-badger:
networks:
  dbgptnet:
    driver: bridge
    name: dbgptnet

----------------------------------------

TITLE: Creating Knowledge Space with Python Client
DESCRIPTION: Example of creating a knowledge space using the Python client library with SpaceModel.

LANGUAGE: python
CODE:
from dbgpt_client import Client
from dbgpt_client.knowledge import create_space
from dbgpt_client.schema import SpaceModel

DBGPT_API_KEY = "dbgpt"

client = Client(api_key=DBGPT_API_KEY)
res = await create_space(client, SpaceModel(
    name="test_space",
    vector_type="Chroma",
    desc="for client space",
    owner="dbgpt"
))

----------------------------------------

TITLE: Configuring Remote Embedding API Connection
DESCRIPTION: Sets up connection to a remote embedding API server for cluster deployments. Requires API server URL and authentication key.

LANGUAGE: python
CODE:
from dbgpt.rag.embedding import DefaultEmbeddingFactory

embeddings = DefaultEmbeddingFactory.remote(
  api_url="http://localhost:8100/api/v1/embeddings",
  api_key="{your_api_key}",
  model_name="text2vec"
)

----------------------------------------

TITLE: Setting up OpenAI LLM Client and AgentContext in Python
DESCRIPTION: This code sets up an OpenAI LLM client and creates an AgentContext with specific parameters. It also defines a list of messages for testing memory retention.

LANGUAGE: python
CODE:
import os
from dbgpt.agent import AgentContext
from dbgpt.model.proxy import OpenAILLMClient

llm_client = OpenAILLMClient(
    model_alias="gpt-4o",
    api_base=os.getenv("OPENAI_API_BASE"),
    api_key=os.getenv("OPENAI_API_KEY"),
)

context: AgentContext = AgentContext(
    conv_id="test123",
    language="en",
    temperature=0.9,
    max_new_tokens=2048,
)

messages = [
    "When I was 4 years old, I went to primary school for the first time, please tell me a joke",
    "When I was 10 years old, I went to middle school for the first time, please tell me a joke",
    "When I was 16 years old, I went to high school for the first time, please tell me a joke",
    "When I was 18 years old, I went to college for the first time, please tell me a joke",
]

----------------------------------------

TITLE: Creating Hybrid Memory with Default Values in Python
DESCRIPTION: Demonstrates how to create a Hybrid Memory instance using default values, which utilizes OpenAI Embedding API and ChromaStore.

LANGUAGE: python
CODE:
import shutil
from dbgpt.agent import HybridMemory, AgentMemory

# Delete old vector store directory(/tmp/tmp_ltm_vector_stor)
shutil.rmtree("/tmp/tmp_ltm_vector_store", ignore_errors=True)
hybrid_memory = HybridMemory.from_chroma(
    vstore_name="agent_memory", vstore_path="/tmp/tmp_ltm_vector_store"
)

agent_memory: AgentMemory = AgentMemory(memory=hybrid_memory)

----------------------------------------

TITLE: Initializing DB-GPT Python Client
DESCRIPTION: Shows how to initialize the DB-GPT client in Python using an API key.

LANGUAGE: python
CODE:
from dbgpt_client import Client

DBGPT_API_KEY = "dbgpt"
client = Client(api_key=DBGPT_API_KEY)

----------------------------------------

TITLE: Generating Tool Prompts in Python
DESCRIPTION: Shows how to generate prompts for a tool, including both default and OpenAI-style prompts. This is useful for understanding how the tool will be presented to the language model.

LANGUAGE: python
CODE:
async def show_prompts():
    from dbgpt.agent.resource import BaseTool
    tool: BaseTool= simple_calculator._tool

    tool_prompt = await tool.get_prompt()
    openai_tool_prompt = await tool.get_prompt(prompt_type="openai")
    print(f"Tool Prompt: \n{tool_prompt}\n")
    print(f"OpenAI Tool Prompt: \n{openai_tool_prompt}\n")


if __name__ == "__main__":
    import asyncio
    asyncio.run(show_prompts())

----------------------------------------

TITLE: Generating Prompts from Dynamic Profile
DESCRIPTION: Shows how to create a profile from the configuration and generate system and user prompts. This code demonstrates the practical usage of the dynamic profile configuration.

LANGUAGE: python
CODE:
real_profile = profile.create_profile()
system_prompt = real_profile.format_system_prompt(question="What can you do?")
user_prompt = real_profile.format_user_prompt(question="What can you do?")
print(f"System Prompt: \n{system_prompt}")
print("#" * 50)
print(f"User Prompt: \n{user_prompt}")

----------------------------------------

TITLE: List Apps Endpoint
DESCRIPTION: GET endpoint for retrieving a list of all available apps. Requires API key authentication.

LANGUAGE: python
CODE:
GET /api/v2/serve/apps

----------------------------------------

TITLE: Stream Data Processing with InputOperator
DESCRIPTION: Demonstrates how to handle streaming data using InputOperator with an async generator as the input source.

LANGUAGE: python
CODE:
import asyncio
from dbgpt.core.awel import DAG, InputOperator, SimpleInputSource

async def stream_data():
    for i in range(10):
        yield i

with DAG("awel_input_operator") as dag:
    input_source = SimpleInputSource(data=stream_data())
    input_task = InputOperator(input_source=input_source)

async def print_stream(t: InputOperator):
    async for i in await t.call_stream():
        print(i)

asyncio.run(print_stream(input_task))

----------------------------------------

TITLE: List Apps Endpoint
DESCRIPTION: GET endpoint for retrieving a list of all available apps. Requires API key authentication.

LANGUAGE: python
CODE:
GET /api/v2/serve/apps

----------------------------------------

TITLE: Creating a MySQL Database Connector in Python
DESCRIPTION: This snippet demonstrates how to create a MySQL database connector by providing the necessary connection details such as host, port, user, password, and database name. It also includes an example of setting additional engine arguments.

LANGUAGE: python
CODE:
from dbgpt.datasource.rdbms.conn_mysql import MySQLConnector

connector = MySQLConnector.from_uri_db(
    host="localhost",
    port=3307,
    user="root",
    pwd="********",
    db_name="user_manager",
    engine_args={"connect_args": {"charset": "utf8mb4"}},
)

----------------------------------------

TITLE: Initializing SensoryMemory and AgentMemory in Python
DESCRIPTION: This snippet demonstrates how to create instances of SensoryMemory and AgentMemory. It sets up a memory buffer with a size of 2 for the sensory memory component.

LANGUAGE: python
CODE:
from dbgpt.agent import AgentMemory, SensoryMemory

# Create an agent memory, which contains a sensory memory
memory = SensoryMemory(buffer_size=2)
agent_memory: AgentMemory = AgentMemory(memory=memory)

----------------------------------------

TITLE: Listing Datasources using cURL
DESCRIPTION: Shows how to list all datasources using a cURL GET request, including authentication.

LANGUAGE: shell
CODE:
DBGPT_API_KEY=dbgpt

curl -X GET "http://localhost:5670/api/v2/serve/datasources" -H "Authorization: Bearer $DBGPT_API_KEY"

----------------------------------------

TITLE: Preparing ChromaStore Vector Store for Hybrid Memory in Python
DESCRIPTION: Demonstrates the setup of a ChromaStore vector store for use in the Hybrid Memory system.

LANGUAGE: python
CODE:
import shutil
from dbgpt.storage.vector_store.chroma_store import ChromaVectorConfig, ChromaStore

# Delete old vector store directory(/tmp/tmp_ltm_vector_stor)
shutil.rmtree("/tmp/tmp_ltm_vector_store", ignore_errors=True)
vector_store = ChromaStore(
    ChromaVectorConfig(
        embedding_fn=embeddings,
        vector_store_config=ChromaVectorConfig(
            name="ltm_vector_store",
            persist_path="/tmp/tmp_ltm_vector_store",
        ),
    )
)

----------------------------------------

TITLE: Creating Temporary SQLite Database
DESCRIPTION: Python code to create a temporary SQLite database with a sample 'user' table for demonstration purposes.

LANGUAGE: python
CODE:
from dbgpt.datasource.rdbms.conn_sqlite import SQLiteTempConnector

db_conn = SQLiteTempConnector.create_temporary_db()
db_conn.create_temp_tables(
    {
        "user": {
            "columns": {
                "id": "INTEGER PRIMARY KEY",
                "name": "TEXT",
                "age": "INTEGER",
            },
            "data": [
                (1, "Tom", 10),
                (2, "Jerry", 16),
                (3, "Jack", 18),
                (4, "Alice", 20),
                (5, "Bob", 22),
            ],
        }
    }
)

----------------------------------------

TITLE: RAG Knowledge Embedding Response Format
DESCRIPTION: Example JSON response structure showing the embedded document chunks with metadata, including content, chunk IDs, and scoring information.

LANGUAGE: json
CODE:
[
    {
        "content": "\"What is AWEL?\": Agentic Workflow Expression Language(AWEL) is a set of intelligent agent workflow expression language specially designed for large model application\ndevelopment. It provides great functionality and flexibility. Through the AWEL API, you can focus on the development of business logic for LLMs applications\nwithout paying attention to cumbersome model and environment details.  \nAWEL adopts a layered API design. AWEL's layered API design architecture is shown in the figure below.  \n<p align=\"left\">\n<img src={'/img/awel.png'} width=\"480px\"/>\n</p>",
        "metadata": {
            "Header1": "What is AWEL?",
            "source": "../../docs/docs/awel/awel.md"
        },
        "chunk_id": "c1ffa671-76d0-4c7a-b2dd-0b08dfd37712",
        "chunk_name": "",
        "score": 0.0,
        "summary": "",
        "separator": "\n",
        "retriever": null
    },...
  ]

----------------------------------------

TITLE: Configuring Chroma Vector Store
DESCRIPTION: Sets up a ChromaStore instance for vector storage with specified configuration including embedding function and persistence path.

LANGUAGE: python
CODE:
import shutil
from dbgpt.storage.vector_store.chroma_store import ChromaVectorConfig, ChromaStore

# Delete old vector store directory(/tmp/tmp_ltm_vector_stor)
shutil.rmtree("/tmp/tmp_ltm_vector_store", ignore_errors=True)
vector_store = ChromaStore(
    ChromaVectorConfig(
        embedding_fn=embeddings,
        vector_store_config=ChromaVectorConfig(
            name="ltm_vector_store",
            persist_path="/tmp/tmp_ltm_vector_store",
        ),
    )
)

----------------------------------------

TITLE: Viewing Docker-Compose Deployment Output
DESCRIPTION: Example output showing successful deployment of DB-GPT containers using Docker-Compose. Indicates creation of network and starting of db and webserver containers.

LANGUAGE: bash
CODE:
[+] Running 3/3
 ✔ Network dbgptnet              Created                                            0.0s 
 ✔ Container db-gpt-db-1         Started                                            0.2s 
 ✔ Container db-gpt-webserver-1  Started                                            0.2s 

----------------------------------------

TITLE: AWEL API Endpoints List
DESCRIPTION: Endpoints for AWEL trigger examples including RAG and chat functionalities.

LANGUAGE: python
CODE:
api/v1/awel/trigger/examples/simple_rag
api/v1/awel/trigger/examples/simple_chat
api/v1/awel/trigger/examples/hello

----------------------------------------

TITLE: Starting Model Workers
DESCRIPTION: Commands to start different types of model workers including GLM-4, Vicuna, text embedding, and reranking models with specific configurations

LANGUAGE: shell
CODE:
dbgpt start worker --model_name glm-4-9b-chat \
--model_path /app/models/glm-4-9b-chat \
--port 8001 \
--controller_addr http://127.0.0.1:8000

LANGUAGE: shell
CODE:
dbgpt start worker --model_name vicuna-13b-v1.5 \
--model_path /app/models/vicuna-13b-v1.5 \
--port 8002 \
--controller_addr http://127.0.0.1:8000

LANGUAGE: shell
CODE:
dbgpt start worker --model_name text2vec \
--model_path /app/models/text2vec-large-chinese \
--worker_type text2vec \
--port 8003 \
--controller_addr http://127.0.0.1:8000

LANGUAGE: shell
CODE:
dbgpt start worker --worker_type text2vec \
--rerank \
--model_path /app/models/bge-reranker-base \
--model_name bge-reranker-base \
--port 8004 \
--controller_addr http://127.0.0.1:8000

----------------------------------------

TITLE: Executing Generated SQL Query
DESCRIPTION: Extension of the AWEL DAG to execute the generated SQL query on the database and retrieve results.

LANGUAGE: python
CODE:
from dbgpt.datasource.operators import DatasourceOperator

    # previous code ...
    db_query_task = DatasourceOperator(connector=db_conn)
    sql_parse_task >> MapOperator(lambda x: x["sql"]) >> db_query_task
    
    db_result = asyncio.run(db_query_task.call({
        "user_input": "Query the name and age of users younger than 18 years old",
        "db_name": "user_management",
        "dialect": "SQLite",
        "top_k": 1,
        "display_type": display_type,
        "response": json.dumps(RESPONSE_FORMAT_SIMPLE, ensure_ascii=False, indent=4)
    }))
    print("The result of the query is:")
    print(db_result)

----------------------------------------

TITLE: Stream Chat App cURL Implementation
DESCRIPTION: cURL example for making streaming chat requests to the chat completions endpoint with app integration.

LANGUAGE: shell
CODE:
DBGPT_API_KEY=dbgpt
APP_ID={YOUR_APP_ID}

curl -X POST "http://localhost:5670/api/v2/chat/completions" \
   -H "Authorization: Bearer $DBGPT_API_KEY" \
   -H "accept: application/json" \
   -H "Content-Type: application/json" \
   -d "{\"messages\":\"Hello\",\"model\":\"gpt-4o\", \"chat_mode\": \"chat_app\", \"chat_param\": \"$APP_ID\"}"

----------------------------------------

TITLE: Sample Response from Knowledge Graph Processing API
DESCRIPTION: This bash snippet shows an example of the JSON response returned by the knowledge graph processing API, including content, metadata, and chunk information.

LANGUAGE: bash
CODE:
[
    {
        "content": "\"What is AWEL?\": Agentic Workflow Expression Language(AWEL) is a set of intelligent agent workflow expression language specially designed for large model application\ndevelopment. It provides great functionality and flexibility. Through the AWEL API, you can focus on the development of business logic for LLMs applications\nwithout paying attention to cumbersome model and environment details.  \nAWEL adopts a layered API design. AWEL's layered API design architecture is shown in the figure below.  \n<p align=\"left\">\n<img src={'/img/awel.png'} width=\"480px\"/>\n</p>",
        "metadata": {
            "Header1": "What is AWEL?",
            "source": "../../docs/docs/awel/awel.md"
        },
        "chunk_id": "c1ffa671-76d0-4c7a-b2dd-0b08dfd37712",
        "chunk_name": "",
        "score": 0.0,
        "summary": "",
        "separator": "\n",
        "retriever": null
    },...
  ]

----------------------------------------

TITLE: Creating Agent Context and Memory for DB-GPT
DESCRIPTION: Python code to create an agent context and memory for DB-GPT. It sets up the conversation ID, language, temperature, and token limit.

LANGUAGE: python
CODE:
from dbgpt.agent import AgentContext, AgentMemory

# language="zh" for Chinese
context: AgentContext = AgentContext(
    conv_id="test123", language="en", temperature=0.5, max_new_tokens=2048
) 
# Create an agent memory, default memory is ShortTermMemory
agent_memory: AgentMemory = AgentMemory()

----------------------------------------

TITLE: Rendering ServiceConfig Configuration Details in JSX
DESCRIPTION: This code snippet uses a custom React component 'ConfigDetail' to render the configuration details of the ServiceConfig class. It includes parameters for web and model services, along with their types, descriptions, and default values.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "ServiceConfig",
  "description": "ServiceConfig(web: dbgpt_app.config.ServiceWebParameters = <factory>, model: dbgpt.model.parameter.ModelServiceConfig = <factory>)",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "model",
      "type": "ModelServiceConfig",
      "required": false,
      "description": "Model service configuration",
      "nestedTypes": [
        {
          "type": "link",
          "text": "modelserviceconfig configuration",
          "url": "././parameter_modelserviceconfig_20d67d"
        }
      ],
      "defaultValue": "ModelServiceConfig"
    },
    {
      "name": "web",
      "type": "ServiceWebParameters",
      "required": false,
      "description": "Web service configuration",
      "nestedTypes": [
        {
          "type": "link",
          "text": "servicewebparameters configuration",
          "url": "././config_servicewebparameters_3ab7fd"
        }
      ],
      "defaultValue": "ServiceWebParameters"
    }
  ]
}} />

----------------------------------------

TITLE: Agent API Endpoints List
DESCRIPTION: API endpoints for agent management including installation, querying, and personal agent operations.

LANGUAGE: python
CODE:
api/v1/agent/hub/update
api/v1/agent/query
api/v1/agent/my
api/v1/agent/install
api/v1/agent/uninstall
api/v1/personal/agent/upload

----------------------------------------

TITLE: Implementing Custom Post-SQL Processing
DESCRIPTION: Definition of custom operators for processing SQL query results, including sum calculation and odd/even number handling.

LANGUAGE: python
CODE:
import pandas as pd

from dbgpt.core.awel import MapOperator, BranchOperator, JoinOperator, is_empty_data


class TwoSumOperator(MapOperator[pd.DataFrame, int]):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
    async def map(self, df: pd.DataFrame) -> int:
        return await self.blocking_func_to_async(self._two_sum, df)
    
    def _two_sum(self, df: pd.DataFrame) -> int:
        return df['age'].sum()

def branch_even(x: int) -> bool:
    return x % 2 == 0

def branch_odd(x: int) -> bool:
    return not branch_even(x)

class DataDecisionOperator(BranchOperator[int, int]):
    def __init__(self, odd_task_name: str, even_task_name: str, **kwargs):
        super().__init__(**kwargs)
        self.odd_task_name = odd_task_name
        self.even_task_name = even_task_name
        
    async def branches(self):
        return {
            branch_even: self.even_task_name,
            branch_odd: self.odd_task_name
        }

class OddOperator(MapOperator[int, str]):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
    async def map(self, x: int) -> str:
        print(f"{x} is odd")
        return f"{x} is odd"

class EvenOperator(MapOperator[int, str]):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
    async def map(self, x: int) -> str:
        print(f"{x} is even")
        return f"{x} is even"

class MergeOperator(JoinOperator[str]):
    def __init__(self, **kwargs):
        super().__init__(combine_function=self.merge_func, **kwargs)
        
    async def merge_func(self, odd: str, even: str) -> str:
        return odd if not is_empty_data(odd) else even

----------------------------------------

TITLE: Making Curl Request to DB-GPT API
DESCRIPTION: Example of making an API request to DB-GPT's chat completions endpoint using curl with proper authentication.

LANGUAGE: bash
CODE:
curl "http://localhost:5670/api/v2/chat/completions" \
-H "Authorization: Bearer $DBGPT_API_KEY" \

----------------------------------------

TITLE: Installing DB-GPT Agent and OpenAI Packages
DESCRIPTION: Commands to install the required packages for using DB-GPT agents and OpenAI integration.

LANGUAGE: bash
CODE:
pip install "dbgpt[agent]>=0.5.9rc0"

LANGUAGE: bash
CODE:
pip install openai

----------------------------------------

TITLE: Rendering RagParameters Configuration Component in JSX
DESCRIPTION: This code snippet renders a ConfigDetail component with RagParameters configuration. It includes various parameters for controlling RAG behavior, such as chunk overlap, similarity search settings, and storage configuration.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "RagParameters",
  "description": "Rag configuration.",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "chunk_overlap",
      "type": "integer",
      "required": false,
      "description": "The default thread pool size, If None, use default config of python thread pool",
      "defaultValue": "50"
    },
    {
      "name": "similarity_top_k",
      "type": "integer",
      "required": false,
      "description": "knowledge search top k",
      "defaultValue": "10"
    },
    {
      "name": "similarity_score_threshold",
      "type": "integer",
      "required": false,
      "description": "knowledge search top similarity score",
      "defaultValue": "0.0"
    },
    {
      "name": "query_rewrite",
      "type": "boolean",
      "required": false,
      "description": "knowledge search rewrite",
      "defaultValue": "False"
    },
    {
      "name": "max_chunks_once_load",
      "type": "integer",
      "required": false,
      "description": "knowledge max chunks once load",
      "defaultValue": "10"
    },
    {
      "name": "max_threads",
      "type": "integer",
      "required": false,
      "description": "knowledge max load thread",
      "defaultValue": "1"
    },
    {
      "name": "rerank_top_k",
      "type": "integer",
      "required": false,
      "description": "knowledge rerank top k",
      "defaultValue": "3"
    },
    {
      "name": "storage",
      "type": "StorageConfig",
      "required": false,
      "description": "Storage configuration",
      "nestedTypes": [
        {
          "type": "link",
          "text": "storageconfig configuration",
          "url": "././config_storageconfig_028579"
        }
      ],
      "defaultValue": "StorageConfig"
    },
    {
      "name": "graph_search_top_k",
      "type": "integer",
      "required": false,
      "description": "knowledge graph search top k",
      "defaultValue": "3"
    },
    {
      "name": "graph_community_summary_enabled",
      "type": "boolean",
      "required": false,
      "description": "graph community summary enabled",
      "defaultValue": "False"
    },
    {
      "name": "chunk_size",
      "type": "integer",
      "required": false,
      "description": "Whether to verify the SSL certificate of the database",
      "defaultValue": "500"
    }
  ]
}} />

----------------------------------------

TITLE: Editor API Endpoints List
DESCRIPTION: Available endpoints for editor-related functionality including SQL operations and chart management.

LANGUAGE: python
CODE:
api/v1/editor/db/tables
api/v1/editor/sql/rounds
api/v1/editor/sql
api/v1/editor/sql/run
api/v1/sql/editor/submit
api/v1/editor/chart/list
api/v1/editor/chart/info
api/v1/editor/chart/run
api/v1/chart/editor/submit

----------------------------------------

TITLE: Chat Completion Stream Response Format
DESCRIPTION: This snippet shows the format of the streaming response from the DB-GPT API for chat completions. It demonstrates how the response is broken down into multiple data chunks, each containing a part of the assistant's reply.

LANGUAGE: commandline
CODE:
data: {"id": "chatcmpl-ba6fb52e-e5b2-11ee-b031-acde48001122", "model": "gpt-4o", "choices": [{"index": 0, "delta": {"role": "assistant", "content": "Hello"}}]}

data: {"id": "chatcmpl-ba6fb52e-e5b2-11ee-b031-acde48001122", "model": "gpt-4o", "choices": [{"index": 0, "delta": {"role": "assistant", "content": "!"}}]}

data: {"id": "chatcmpl-ba6fb52e-e5b2-11ee-b031-acde48001122", "model": "gpt-4o", "choices": [{"index": 0, "delta": {"role": "assistant", "content": " How"}}]}

data: {"id": "chatcmpl-ba6fb52e-e5b2-11ee-b031-acde48001122", "model": "gpt-4o", "choices": [{"index": 0, "delta": {"role": "assistant", "content": " can"}}]}

data: {"id": "chatcmpl-ba6fb52e-e5b2-11ee-b031-acde48001122", "model": "gpt-4o", "choices": [{"index": 0, "delta": {"role": "assistant", "content": " I"}}]}

data: {"id": "chatcmpl-ba6fb52e-e5b2-11ee-b031-acde48001122", "model": "gpt-4o", "choices": [{"index": 0, "delta": {"role": "assistant", "content": " assist"}}]}

data: {"id": "chatcmpl-ba6fb52e-e5b2-11ee-b031-acde48001122", "model": "gpt-4o", "choices": [{"index": 0, "delta": {"role": "assistant", "content": " you"}}]}

data: {"id": "chatcmpl-ba6fb52e-e5b2-11ee-b031-acde48001122", "model": "gpt-4o", "choices": [{"index": 0, "delta": {"role": "assistant", "content": " today"}}]}

data: {"id": "chatcmpl-ba6fb52e-e5b2-11ee-b031-acde48001122", "model": "gpt-4o", "choices": [{"index": 0, "delta": {"role": "assistant", "content": "?"}}]}

data: [DONE]

----------------------------------------

TITLE: Starting GLM Model Worker
DESCRIPTION: Command to start a GLM-4-9B-Chat model worker instance that connects to multiple controller instances for high availability.

LANGUAGE: shell
CODE:
dbgpt start worker --model_name glm-4-9b-chat \
--model_path /app/models/glm-4-9b-chat \
--port 8001 \
--controller_addr "http://server1:8000,http://server2:8000"

----------------------------------------

TITLE: Managing Model Services
DESCRIPTION: Commands for listing and verifying model services and interactive model testing.

LANGUAGE: bash
CODE:
dbgpt model list 

dbgpt model chat --model_name glm-4-9b-chat

----------------------------------------

TITLE: Configuring OceanBase Connection Parameters in JSON
DESCRIPTION: Comprehensive configuration object defining connection parameters for OceanBase database integration. Includes essential connection details like host, port, credentials, and connection pool settings with their default values and requirements.

LANGUAGE: json
CODE:
{
  "name": "OceanBaseParameters",
  "description": "An Ultra-Fast & Cost-Effective Distributed SQL Database.",
  "documentationUrl": null,
  "parameters": [
    {
      "name": "port",
      "type": "integer",
      "required": true,
      "description": "Database port, e.g., 3306"
    },
    {
      "name": "user",
      "type": "string",
      "required": true,
      "description": "Database user to connect"
    },
    {
      "name": "database",
      "type": "string",
      "required": true,
      "description": "Database name"
    },
    {
      "name": "driver",
      "type": "string",
      "required": false,
      "description": "Driver name for oceanbase, default is mysql+ob.",
      "defaultValue": "mysql+ob"
    },
    {
      "name": "password",
      "type": "string",
      "required": false,
      "description": "Database password, you can write your password directly, of course, you can also use environment variables, such as ${env:DBGPT_DB_PASSWORD}",
      "defaultValue": "${env:DBGPT_DB_PASSWORD}"
    },
    {
      "name": "pool_size",
      "type": "integer",
      "required": false,
      "description": "Connection pool size, default 5",
      "defaultValue": "5"
    },
    {
      "name": "max_overflow",
      "type": "integer",
      "required": false,
      "description": "Max overflow connections, default 10",
      "defaultValue": "10"
    },
    {
      "name": "pool_timeout",
      "type": "integer",
      "required": false,
      "description": "Connection pool timeout, default 30",
      "defaultValue": "30"
    },
    {
      "name": "pool_recycle",
      "type": "integer",
      "required": false,
      "description": "Connection pool recycle, default 3600",
      "defaultValue": "3600"
    },
    {
      "name": "pool_pre_ping",
      "type": "boolean",
      "required": false,
      "description": "Connection pool pre ping, default True",
      "defaultValue": "True"
    },
    {
      "name": "host",
      "type": "string",
      "required": true,
      "description": "Database host, e.g., localhost"
    }
  ]
}

----------------------------------------

TITLE: Chat Completion Request via cURL
DESCRIPTION: cURL command to send a chat completion request using the GLM model

LANGUAGE: bash
CODE:
curl http://127.0.0.1:8100/api/v1/chat/completions \\n-H "Authorization: Bearer EMPTY" \\n-H "Content-Type: application/json" \\n-d '{"model": "glm-4-9b-chat", "messages": [{"role": "user", "content": "hello"}]}'

----------------------------------------

TITLE: Creating Hybrid Memory from Vector Store in Python
DESCRIPTION: Demonstrates how to create a Hybrid Memory instance from an existing vector store, using default values for sensory and short-term memory.

LANGUAGE: python
CODE:
from dbgpt.agent import HybridMemory, AgentMemory

hybrid_memory = HybridMemory.from_vstore(
    vector_store=vector_store, embeddings=embeddings
)

agent_memory: AgentMemory = AgentMemory(memory=hybrid_memory)

----------------------------------------

TITLE: Listing DB-GPT Start Server Options in Python
DESCRIPTION: Displays the help information for the 'start' subcommand, showing options for starting various server components like API server, controller, and web server.

LANGUAGE: python
CODE:
~ dbgpt start --help
Already connect 'dbgpt'
Usage: dbgpt start [OPTIONS] COMMAND [ARGS]...

  Start specific server.

Options:
  --help  Show this message and exit.

Commands:
  apiserver   Start apiserver
  controller  Start model controller
  webserver   Start webserver(dbgpt_server.py)
  worker      Start model worker

----------------------------------------

TITLE: Starting HA Cluster with Docker Compose
DESCRIPTION: Command to start the DB-GPT high availability cluster using Docker Compose with OpenAI configuration.

LANGUAGE: bash
CODE:
OPENAI_API_KEY="{your api key}" OPENAI_API_BASE="https://api.openai.com/v1" \
docker compose -f ha-cluster-docker-compose.yml up -d

----------------------------------------

TITLE: Stream Chat App POST Endpoint
DESCRIPTION: POST endpoint for streaming chat completions with app integration. Requires API key authentication and accepts message content with model specifications.

LANGUAGE: python
CODE:
POST /api/v2/chat/completions

----------------------------------------

TITLE: Starting DB-GPT API Server
DESCRIPTION: Command to start the API server on port 8100 with a specified controller address and API keys

LANGUAGE: bash
CODE:
dbgpt start apiserver --controller_addr http://127.0.0.1:8000 --api_keys EMPTY

----------------------------------------

TITLE: Searching Trace Information in DB-GPT
DESCRIPTION: This command searches for trace information in DB-GPT containing a specific keyword (in this case, 'Hello').

LANGUAGE: python
CODE:
dbgpt trace list --search Hello

----------------------------------------

TITLE: Viewing Trace Information Based on Span Type in DB-GPT
DESCRIPTION: This command displays trace information for a specific span type (in this case, 'chat') in DB-GPT.

LANGUAGE: python
CODE:
dbgpt trace list --span_type chat

----------------------------------------

TITLE: Creating Knowledge Space API Endpoint
DESCRIPTION: POST endpoint for creating a new knowledge space with configurable parameters like name, vector type, description and owner. Returns a Space Object.

LANGUAGE: python
CODE:
POST /api/v2/serve/knowledge/spaces

----------------------------------------

TITLE: Downloading DB-GPT Source Code
DESCRIPTION: Initial steps to clone the DB-GPT repository from GitHub and navigate to the project directory.

LANGUAGE: bash
CODE:
# download source code
git clone https://github.com/eosphoros-ai/DB-GPT.git

cd DB-GPT

----------------------------------------

TITLE: Starting Web Server
DESCRIPTION: Commands to start the DB-GPT web server in light mode with different model configurations

LANGUAGE: shell
CODE:
dbgpt start webserver --light

LANGUAGE: shell
CODE:
LLM_MODEL=glm-4-9b-chat dbgpt start webserver --light --remote_embedding

----------------------------------------

TITLE: Creating Evaluation using cURL in DB-GPT
DESCRIPTION: This snippet demonstrates how to create an evaluation using cURL. It sends a POST request to the evaluation endpoint with necessary parameters including scene key, context, and evaluation metrics.

LANGUAGE: shell
CODE:
DBGPT_API_KEY=dbgpt
SPACE_ID={YOUR_SPACE_ID}

curl -X POST "http://localhost:5670/api/v2/serve/evaluate/evaluation" 
-H "Authorization: Bearer $DBGPT_API_KEY" \
-H "accept: application/json" \
-H "Content-Type: application/json" \
-d '{
  "scene_key": "recall",
  "scene_value":147,
  "context":{"top_k":5},
  "sys_code":"xx",
  "evaluate_metrics":["RetrieverHitRateMetric","RetrieverMRRMetric","RetrieverSimilarityMetric"],
  "datasets": [{
            "query": "what awel talked about",
            "doc_name":"awel.md"
        }]
}'

----------------------------------------

TITLE: LoRA Fine-tuning Script for Text2SQL
DESCRIPTION: Bash script for running LoRA fine-tuning on the CodeLlama-13b-Instruct-hf model for Text2SQL tasks.

LANGUAGE: bash
CODE:
CUDA_VISIBLE_DEVICES=0 python dbgpt_hub/train/sft_train.py \
    --model_name_or_path Your_download_CodeLlama-13b-Instruct-hf_path \
    --do_train \
    --dataset example_text2sql_train \
    --max_source_length 2048 \
    --max_target_length 512 \
    --finetuning_type lora \
    --lora_target q_proj,v_proj \
    --template llama2 \
    --lora_rank 64 \
    --lora_alpha 32 \
    --output_dir dbgpt_hub/output/adapter/code_llama-13b-2048_epoch8_lora \
    --overwrite_cache \
    --overwrite_output_dir \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 16 \
    --lr_scheduler_type cosine_with_restarts \
    --logging_steps 50 \
    --save_steps 2000 \
    --learning_rate 2e-4 \
    --num_train_epochs 8 \
    --plot_loss \
    --bf16

----------------------------------------

TITLE: Configuring OpenTelemetry in DB-GPT Environment
DESCRIPTION: Environment variables to be set in the .env file for enabling OpenTelemetry tracing in DB-GPT, including the OTLP exporter endpoint.

LANGUAGE: bash
CODE:
## Whether to enable DB-GPT send trace to OpenTelemetry
TRACER_TO_OPEN_TELEMETRY=True
## More details see https://opentelemetry-python.readthedocs.io/en/latest/exporter/otlp/otlp.html
OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://localhost:4317

----------------------------------------

TITLE: Viewing Chat Details and Call Chain in DB-GPT
DESCRIPTION: This command displays chat details and the call chain for a conversation in DB-GPT, showing the sequence of operations and their timings.

LANGUAGE: python
CODE:
dbgpt trace chat --hide_run_params --tree

----------------------------------------

TITLE: Setting Up Python Environment
DESCRIPTION: Creating and activating a Conda virtual environment for DB-GPT with Python 3.10.

LANGUAGE: bash
CODE:
# create a virtual environment
conda create -n dbgpt_env python=3.10

# activate virtual environment
conda activate dbgpt_env

----------------------------------------

TITLE: Chatting with Datasource using Python
DESCRIPTION: Demonstrates how to use the DB-GPT client to chat with a datasource, specifying the database name and chat parameters.

LANGUAGE: python
CODE:
from dbgpt_client import Client

DBGPT_API_KEY = "dbgpt"
DB_NAME="{your_db_name}"

client = Client(api_key=DBGPT_API_KEY)
res = client.chat(
    messages="show space datas limit 5", 
    model="gpt-4o", 
    chat_mode="chat_data", 
    chat_param=DB_NAME
)

----------------------------------------

TITLE: Initializing OpenAI LLM Client for DB-GPT Agent
DESCRIPTION: Python code to initialize the OpenAI LLM client for use with DB-GPT agents. It sets up the model, API base, and API key.

LANGUAGE: python
CODE:
import os
from dbgpt.model.proxy import OpenAILLMClient

llm_client = OpenAILLMClient(
    model_alias="gpt-3.5-turbo", # or other models, eg. "gpt-4o"
    api_base=os.getenv("OPENAI_API_BASE"),
    api_key=os.getenv("OPENAI_API_KEY"),
)

----------------------------------------

TITLE: Viewing Trace Logs with DB-GPT Command
DESCRIPTION: This command displays help information for the 'dbgpt trace' tool, which is used to analyze trace logs in DB-GPT.

LANGUAGE: python
CODE:
dbgpt trace --help

----------------------------------------

TITLE: Configuring Azure OpenAI Proxy in DB-GPT Environment
DESCRIPTION: This code snippet illustrates the configuration settings for using Azure OpenAI as a proxy LLM in DB-GPT, including API details and deployment information.

LANGUAGE: python
CODE:
# .env
LLM_MODEL=proxyllm
PROXY_API_KEY=xxxx
PROXY_API_BASE=https://xxxxxx.openai.azure.com/
PROXY_API_TYPE=azure
PROXY_SERVER_URL=xxxx
PROXY_API_VERSION=2023-05-15
PROXYLLM_BACKEND=gpt-35-turbo
API_AZURE_DEPLOYMENT=xxxx[deployment_name]

----------------------------------------

TITLE: Configuring File Server Parameters in DB-GPT using JSX Component
DESCRIPTION: React/JSX component that defines the configuration schema for DB-GPT's file server module. Includes parameters for network settings, chunk sizes, timeouts, storage paths, and API authentication.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "ServeConfig",
  "description": "This configuration is for the file serve module. In DB-GPT, you can store yourfiles in the file server.",
  "documentationUrl": null,
  "parameters": [
    {
      "name": "check_hash",
      "type": "boolean",
      "required": false,
      "description": "Check the hash of the file when downloading",
      "defaultValue": "True"
    },
    {
      "name": "host",
      "type": "string",
      "required": false,
      "description": "The host of the file server"
    },
    {
      "name": "port",
      "type": "integer",
      "required": false,
      "description": "The port of the file server, default is 5670",
      "defaultValue": "5670"
    },
    {
      "name": "download_chunk_size",
      "type": "integer",
      "required": false,
      "description": "The chunk size when downloading the file",
      "defaultValue": "1048576"
    },
    {
      "name": "save_chunk_size",
      "type": "integer",
      "required": false,
      "description": "The chunk size when saving the file",
      "defaultValue": "1048576"
    },
    {
      "name": "transfer_chunk_size",
      "type": "integer",
      "required": false,
      "description": "The chunk size when transferring the file",
      "defaultValue": "1048576"
    },
    {
      "name": "transfer_timeout",
      "type": "integer",
      "required": false,
      "description": "The timeout when transferring the file",
      "defaultValue": "360"
    },
    {
      "name": "local_storage_path",
      "type": "string",
      "required": false,
      "description": "The local storage path"
    },
    {
      "name": "api_keys",
      "type": "string",
      "required": false,
      "description": "API keys for the endpoint, if None, allow all"
    }
  ]
}} />

----------------------------------------

TITLE: Installing DB-GPT Dependencies
DESCRIPTION: Installing the required dependencies using pip with the default configuration.

LANGUAGE: bash
CODE:
pip install -e ".[default]"

----------------------------------------

TITLE: Configuring Model Service Components using JSX ConfigDetail
DESCRIPTION: A JSX component that renders detailed configuration documentation for ModelServiceConfig. Defines parameters for model API server, controller, and worker configurations with their respective types and default values.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "ModelServiceConfig",
  "description": "Model service configuration.",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "api",
      "type": "ModelAPIServerParameters",
      "required": false,
      "description": "Model API",
      "nestedTypes": [
        {
          "type": "link",
          "text": "modelapiserverparameters configuration",
          "url": "././parameter_modelapiserverparameters_763bec"
        }
      ],
      "defaultValue": "ModelControllerParameters"
    },
    {
      "name": "controller",
      "type": "ModelControllerParameters",
      "required": false,
      "description": "Model controller",
      "nestedTypes": [
        {
          "type": "link",
          "text": "modelcontrollerparameters configuration",
          "url": "././parameter_modelcontrollerparameters_689309"
        }
      ],
      "defaultValue": "ModelControllerParameters"
    },
    {
      "name": "worker",
      "type": "ModelWorkerParameters",
      "required": false,
      "description": "Model worker configuration",
      "nestedTypes": [
        {
          "type": "link",
          "text": "modelworkerparameters configuration",
          "url": "././parameter_modelworkerparameters_3fd00b"
        }
      ],
      "defaultValue": "ModelWorkerParameters"
    }
  ]
}} />

----------------------------------------

TITLE: Configuring Claude AI API Key and Model Parameters in Python
DESCRIPTION: Sets up the API key for Claude AI and defines model parameters including temperature and max tokens. It also includes a function to generate a system message.

LANGUAGE: Python
CODE:
import os

from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT

# Set up the API key
api_key = os.environ.get("ANTHROPIC_API_KEY")
if api_key is None:
    raise ValueError("ANTHROPIC_API_KEY environment variable not set")

# Initialize the Anthropic client
client = Anthropic(api_key=api_key)

# Model parameters
model = "claude-2"
max_tokens_to_sample = 100000
temperature = 0

def get_system_message():
    return "You are Claude, an AI assistant created by Anthropic to be helpful, harmless, and honest."

----------------------------------------

TITLE: Running Jaeger All-in-One for DB-GPT Tracing
DESCRIPTION: Docker command to run the Jaeger all-in-one image, setting up ports and environment variables for tracing DB-GPT.

LANGUAGE: bash
CODE:
docker run --rm --name jaeger \
  -e COLLECTOR_ZIPKIN_HOST_PORT=:9411 \
  -p 6831:6831/udp \
  -p 6832:6832/udp \
  -p 5778:5778 \
  -p 16686:16686 \
  -p 4317:4317 \
  -p 4318:4318 \
  -p 14250:14250 \
  -p 14268:14268 \
  -p 14269:14269 \
  -p 9411:9411 \
  jaegertracing/all-in-one:1.58

----------------------------------------

TITLE: Configuring vLLM Inference in DB-GPT
DESCRIPTION: Example of .env configuration file settings to enable vLLM inference in DB-GPT. It includes setting the LLM model, inference type, and an optional GPU memory utilization parameter.

LANGUAGE: bash
CODE:
LLM_MODEL=glm-4-9b-chat
MODEL_TYPE=vllm
# modify the following configuration if you possess GPU resources
# gpu_memory_utilization=0.8

----------------------------------------

TITLE: Starting DB-GPT Webserver
DESCRIPTION: Command to start the DB-GPT webserver with specified model and port configurations.

LANGUAGE: bash
CODE:
LLM_MODEL=glm-4-9b-chat 
dbgpt start webserver --port 6006

----------------------------------------

TITLE: Rendering Prompt Serve Configuration Component in JSX
DESCRIPTION: This snippet renders a ConfigDetail component with configuration details for the ServeConfig. It specifies parameters such as default user, default system code, and API keys for the prompt serve module.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "ServeConfig",
  "description": "This configuration is for the prompt serve module.",
  "documentationUrl": null,
  "parameters": [
    {
      "name": "default_user",
      "type": "string",
      "required": false,
      "description": "Default user name for prompt"
    },
    {
      "name": "default_sys_code",
      "type": "string",
      "required": false,
      "description": "Default system code for prompt"
    },
    {
      "name": "api_keys",
      "type": "string",
      "required": false,
      "description": "API keys for the endpoint, if None, allow all"
    }
  ]
}} />

----------------------------------------

TITLE: Initializing StorageGraphConfig in Python
DESCRIPTION: This code snippet defines the configuration structure for StorageGraphConfig. It specifies a single parameter 'type' with a default value of 'TuGraph'.

LANGUAGE: python
CODE:
StorageGraphConfig(type: str = 'TuGraph')

----------------------------------------

TITLE: Defining ServeConfig Component in React/JSX
DESCRIPTION: React/JSX component that renders configuration documentation for the evaluate serve module. It defines parameters for embedding models, similarity search settings, and API key authentication.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "ServeConfig",
  "description": "This configuration is for the evaluate serve module.",
  "documentationUrl": null,
  "parameters": [
    {
      "name": "embedding_model",
      "type": "string",
      "required": false,
      "description": "Embedding Model",
      "defaultValue": "None"
    },
    {
      "name": "similarity_top_k",
      "type": "integer",
      "required": false,
      "description": "knowledge search top k",
      "defaultValue": "10"
    },
    {
      "name": "api_keys",
      "type": "string",
      "required": false,
      "description": "API keys for the endpoint, if None, allow all"
    }
  ]
}} />

----------------------------------------

TITLE: Rerank API Request via cURL
DESCRIPTION: Experimental cURL command to use the rerank API for document relevance scoring

LANGUAGE: bash
CODE:
curl http://127.0.0.1:8100/api/v1/beta/relevance \\n-H "Authorization: Bearer EMPTY" \\n-H "Content-Type: application/json" \\n-d '{\\n    "model": "bge-reranker-base",\\n    "query": "what is awel talk about?",\\n    "documents": [\\n      "Agentic Workflow Expression Language(AWEL) is a set of intelligent agent workflow expression language specially designed for large model application development.",\\n      "Autonomous agents have long been a research focus in academic and industry communities",\\n      "AWEL is divided into three levels in deign, namely the operator layer, AgentFream layer and DSL layer.",\\n      "Elon musk is a famous entrepreneur and inventor, he is the founder of SpaceX and Tesla."\\n    ]\\n}'

----------------------------------------

TITLE: Rendering Gemini Proxy LLM Configuration Component in JSX
DESCRIPTION: This code snippet renders a ConfigDetail component with configuration parameters for the Google Gemini proxy LLM. It includes various settings such as model name, backend, provider, API details, and deployment options.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "GeminiDeployModelParameters",
  "description": "Google Gemini proxy LLM configuration.",
  "documentationUrl": "https://ai.google.dev/gemini-api/docs",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name."
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "proxy/gemini"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "100"
    },
    {
      "name": "prompt_template",
      "type": "string",
      "required": false,
      "description": "Prompt template. If None, the prompt template is automatically determined from model. Just for local deployment."
    },
    {
      "name": "context_length",
      "type": "integer",
      "required": false,
      "description": "The context length of the OpenAI API. If None, it is determined by the model."
    },
    {
      "name": "api_base",
      "type": "string",
      "required": false,
      "description": "The base url of the gemini API.",
      "defaultValue": "${env:GEMINI_PROXY_API_BASE}"
    },
    {
      "name": "api_key",
      "type": "string",
      "required": false,
      "description": "The API key of the gemini API.",
      "defaultValue": "${env:GEMINI_PROXY_API_KEY}"
    },
    {
      "name": "api_type",
      "type": "string",
      "required": false,
      "description": "The type of the OpenAI API, if you use Azure, it can be: azure"
    },
    {
      "name": "api_version",
      "type": "string",
      "required": false,
      "description": "The version of the OpenAI API."
    },
    {
      "name": "http_proxy",
      "type": "string",
      "required": false,
      "description": "The http or https proxy to use openai"
    }
  ]
}} />

----------------------------------------

TITLE: Installing uv Package Manager (macOS and Linux)
DESCRIPTION: Command to install the uv package manager using a shell script.

LANGUAGE: bash
CODE:
curl -LsSf https://astral.sh/uv/install.sh | sh

----------------------------------------

TITLE: Embedding Generation via cURL
DESCRIPTION: cURL command to generate embeddings using the text2vec model

LANGUAGE: bash
CODE:
curl http://127.0.0.1:8100/api/v1/embeddings \\n-H "Authorization: Bearer EMPTY" \\n-H "Content-Type: application/json" \\n-d '{\\n    "model": "text2vec",\\n    "input": "Hello world!"\\n}'

----------------------------------------

TITLE: Call Data Processing with SimpleCallDataInputSource
DESCRIPTION: Shows how to use SimpleCallDataInputSource to handle call data passed directly to the operator's call method.

LANGUAGE: python
CODE:
import asyncio
from dbgpt.core.awel import DAG, MapOperator, InputOperator, SimpleCallDataInputSource

with DAG("awel_input_operator") as dag:
    input_source = SimpleCallDataInputSource()
    input_task = InputOperator(input_source=input_source)
    print_task = MapOperator(map_function=lambda x: print(x))
    input_task >> print_task

asyncio.run(print_task.call(call_data="Hello, World!"))
asyncio.run(print_task.call(call_data="AWEL is cool!"))

----------------------------------------

TITLE: Initializing ModelWorkerParameters in Python
DESCRIPTION: This code snippet shows the constructor signature for ModelWorkerParameters. It includes optional parameters for configuring the host, port, worker type, logging, tracing, and various operational settings for the model worker.

LANGUAGE: python
CODE:
ModelWorkerParameters(host: Optional[str] = '0.0.0.0', port: Optional[int] = 8001, daemon: Optional[bool] = False, log: dbgpt.util.utils.LoggingParameters = <factory>, trace: Optional[dbgpt.util.tracer.tracer_impl.TracerParameters] = None, worker_type: Optional[str] = None, worker_class: Optional[str] = None, standalone: Optional[bool] = False, register: Optional[bool] = True, worker_register_host: Optional[str] = None, controller_addr: Optional[str] = None, send_heartbeat: Optional[bool] = True, heartbeat_interval: Optional[int] = 20)

----------------------------------------

TITLE: Stream Chat Flow API Endpoint
DESCRIPTION: POST endpoint for streaming chat flow interactions. Requires API key authentication and accepts messages with model and chat parameters.

LANGUAGE: python
CODE:
POST /api/v2/chat/completions

----------------------------------------

TITLE: Data Persistence Directory Setup
DESCRIPTION: Commands for creating directories to persist DB-GPT data and messages.

LANGUAGE: bash
CODE:
mkdir -p ./pilot/data
mkdir -p ./pilot/message
mkdir -p ./pilot/alembic_versions

----------------------------------------

TITLE: Configuring Local GLM4 Model
DESCRIPTION: TOML configuration snippet for setting up the local GLM4 model and embedding model in the DB-GPT configuration file.

LANGUAGE: toml
CODE:
# Model Configurations
[models]
[[models.llms]]
name = "THUDM/glm-4-9b-chat-hf"
provider = "hf"
# If not provided, the model will be downloaded from the Hugging Face model hub
# uncomment the following line to specify the model path in the local file system
# path = "the-model-path-in-the-local-file-system"

[[models.embeddings]]
name = "BAAI/bge-large-zh-v1.5"
provider = "hf"
# If not provided, the model will be downloaded from the Hugging Face model hub
# uncomment the following line to specify the model path in the local file system
# path = "the-model-path-in-the-local-file-system"

----------------------------------------

TITLE: Custom Input Source Implementation
DESCRIPTION: Demonstrates how to create a custom input source by implementing the BaseInputSource class.

LANGUAGE: python
CODE:
import asyncio
from dbgpt.core.awel import DAG, InputOperator, MapOperator, BaseInputSource, TaskContext

class MyInputSource(BaseInputSource):
    """Create an input source with a single data"""
    def _read_data(self, ctx: TaskContext) -> str:
        return "Hello, World!"

with DAG("awel_input_operator") as dag:
    input_source = MyInputSource()
    input_task = InputOperator(input_source=input_source)
    print_task = MapOperator(map_function=lambda x: print(x))

    input_task >> print_task

asyncio.run(print_task.call())

----------------------------------------

TITLE: Defining ServiceWebParameters Class in Python
DESCRIPTION: This code snippet defines the ServiceWebParameters class with various configuration options for the DB-GPT web service. It includes parameters for host, port, database connections, model storage, logging, and other features.

LANGUAGE: python
CODE:
ServiceWebParameters(host: str = '0.0.0.0', port: int = 5670, light: Optional[bool] = False, controller_addr: Optional[str] = None, database: dbgpt.datasource.parameter.BaseDatasourceParameters = <factory>, model_storage: Optional[str] = None, trace: Optional[dbgpt.util.tracer.tracer_impl.TracerParameters] = None, log: Optional[dbgpt.util.utils.LoggingParameters] = None, disable_alembic_upgrade: Optional[bool] = False, db_ssl_verify: Optional[bool] = False, default_thread_pool_size: Optional[int] = None, remote_embedding: Optional[bool] = False, remote_rerank: Optional[bool] = False, awel_dirs: Optional[str] = None, new_web_ui: bool = True, model_cache: dbgpt.storage.cache.manager.ModelCacheParameters = <factory>, embedding_model_max_seq_len: Optional[int] = 512)

----------------------------------------

TITLE: Rendering DuckDB Configuration Component in JSX
DESCRIPTION: This code snippet renders a ConfigDetail component with DuckDB connector parameters. It specifies the driver name and file path required for connecting to a DuckDB database.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "DuckDbConnectorParameters",
  "description": "In-memory analytical database with efficient query processing.",
  "documentationUrl": null,
  "parameters": [
    {
      "name": "driver",
      "type": "string",
      "required": false,
      "description": "Driver name for DuckDB, default is duckdb.",
      "defaultValue": "duckdb"
    },
    {
      "name": "path",
      "type": "string",
      "required": true,
      "description": "Path to the DuckDB file."
    }
  ]
}} />

----------------------------------------

TITLE: DB-GPT Local GPU Configuration
DESCRIPTION: TOML configuration file for setting up local GPU deployment with specific model paths.

LANGUAGE: toml
CODE:
[models]
[[models.llms]]
name = "Qwen2.5-Coder-0.5B-Instruct"
provider = "hf"
path = "/app/models/Qwen2.5-Coder-0.5B-Instruct"

[[models.embeddings]]
name = "BAAI/bge-large-zh-v1.5"
provider = "hf"
path = "/app/models/bge-large-zh-v1.5"

----------------------------------------

TITLE: Basic Input Operator Usage with SimpleInputSource in Python
DESCRIPTION: Demonstrates how to create a basic Input Operator using SimpleInputSource to handle string input data.

LANGUAGE: python
CODE:
from dbgpt.core.awel import DAG, InputOperator, SimpleInputSource

with DAG("awel_input_operator") as dag:
    input_source = SimpleInputSource(data="Hello, World!")
    input_task = InputOperator(input_source=input_source)

----------------------------------------

TITLE: Running DB-GPT Webserver with Local GLM4 Model
DESCRIPTION: Command to start the DB-GPT webserver using the local GLM4 model configuration.

LANGUAGE: bash
CODE:
uv run dbgpt start webserver --config configs/dbgpt-local-glm.toml

----------------------------------------

TITLE: Displaying DB-GPT Model Command Options in Python
DESCRIPTION: Shows the help information for the 'model' subcommand, listing options for managing model instances and interacting with models.

LANGUAGE: python
CODE:
~ dbgpt model --help
Already connect 'dbgpt'
Usage: dbgpt model [OPTIONS] COMMAND [ARGS]...

  Clients that manage model serving

Options:
  --address TEXT  Address of the Model Controller to connect to. Just support
                  light deploy model, If the environment variable
                  CONTROLLER_ADDRESS is configured, read from the environment
                  variable
  --help          Show this message and exit.

Commands:
  chat     Interact with your bot from the command line
  list     List model instances
  restart  Restart model instances
  start    Start model instances
  stop     Stop model instances

----------------------------------------

TITLE: Rendering TracerParameters Configuration Component in JSX
DESCRIPTION: JSX component implementation that displays TracerParameters configuration documentation using a ConfigDetail component. Includes comprehensive parameter definitions for OpenTelemetry tracing setup.

LANGUAGE: jsx
CODE:
import { ConfigDetail } from "@site/src/components/mdx/ConfigDetail";

<ConfigDetail config={{
  "name": "TracerParameters",
  "description": "TracerParameters(file: Optional[str] = None, root_operation_name: Optional[str] = None, exporter: Optional[str] = None, otlp_endpoint: Optional[str] = None, otlp_insecure: Optional[bool] = None, otlp_timeout: Optional[int] = None, tracer_storage_cls: Optional[str] = None)",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "root_operation_name",
      "type": "string",
      "required": false,
      "description": "The root operation name of the tracer"
    },
    {
      "name": "exporter",
      "type": "string",
      "required": false,
      "description": "The exporter of the tracer, e.g. telemetry"
    },
    {
      "name": "otlp_endpoint",
      "type": "string",
      "required": false,
      "description": "The endpoint of the OpenTelemetry Protocol, you can set '${env:OTEL_EXPORTER_OTLP_TRACES_ENDPOINT}' to use the environment variable"
    },
    {
      "name": "otlp_insecure",
      "type": "boolean",
      "required": false,
      "description": "Whether to use insecure connection, you can set '${env:OTEL_EXPORTER_OTLP_TRACES_INSECURE}' to use the environment "
    },
    {
      "name": "otlp_timeout",
      "type": "integer",
      "required": false,
      "description": "The timeout of the connection, in seconds, you can set '${env:OTEL_EXPORTER_OTLP_TRACES_TIMEOUT}' to use the environment "
    },
    {
      "name": "tracer_storage_cls",
      "type": "string",
      "required": false,
      "description": "The class of the tracer storage"
    },
    {
      "name": "file",
      "type": "string",
      "required": false,
      "description": "The file to store the tracer, e.g. dbgpt_webserver_tracer.jsonl"
    }
  ]
}} />

----------------------------------------

TITLE: Configuring BitsandbytesQuantization Component in React/MDX
DESCRIPTION: React/MDX component implementation for displaying BitsandbytesQuantization configuration details. The component accepts configuration parameters for 4-bit and 8-bit model quantization options.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "BitsandbytesQuantization",
  "description": "Bits and bytes quantization parameters.",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "load_in_4bits",
      "type": "boolean",
      "required": false,
      "description": "Whether to load the model in 4 bits, default is False.",
      "defaultValue": "False"
    },
    {
      "name": "load_in_8bits",
      "type": "boolean",
      "required": false,
      "description": "Whether to load the model in 8 bits(LLM.int8() algorithm), default is False.",
      "defaultValue": "False"
    }
  ]
}} />

----------------------------------------

TITLE: Model Download Setup Commands
DESCRIPTION: Commands for setting up the environment and downloading models from ModelScope or Hugging Face.

LANGUAGE: bash
CODE:
sudo apt-get install git git-lfs
mkdir -p ./models
cd ./models
git lfs install
git clone https://www.modelscope.cn/Qwen/Qwen2.5-Coder-0.5B-Instruct.git
git clone https://www.modelscope.cn/BAAI/bge-large-zh-v1.5.git
cd ..

----------------------------------------

TITLE: Installing DB-GPT Dependencies
DESCRIPTION: Commands for installing DB-GPT dependencies with different model configurations using UV package manager

LANGUAGE: bash
CODE:
uv sync --all-packages \
--extra "base" \
--extra "proxy_openai" \
--extra "rag" \
--extra "storage_chromadb" \
--extra "dbgpts"

----------------------------------------

TITLE: Configuring OpenAI Compatible LLM Parameters using React Component
DESCRIPTION: JSX component rendering that displays OpenAI LLM configuration parameters. Includes all model settings, API configurations, and deployment options with their types, requirements and default values.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "OpenAICompatibleDeployModelParameters",
  "description": "OpenAI Compatible Proxy LLM",
  "documentationUrl": "https://platform.openai.com/docs/api-reference/chat",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name."
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "proxy/openai"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "100"
    },
    {
      "name": "prompt_template",
      "type": "string",
      "required": false,
      "description": "Prompt template. If None, the prompt template is automatically determined from model. Just for local deployment."
    },
    {
      "name": "context_length",
      "type": "integer",
      "required": false,
      "description": "The context length of the OpenAI API. If None, it is determined by the model."
    },
    {
      "name": "api_base",
      "type": "string",
      "required": false,
      "description": "The base url of the OpenAI API.",
      "defaultValue": "${env:OPENAI_API_BASE:-https://api.openai.com/v1}"
    },
    {
      "name": "api_key",
      "type": "string",
      "required": false,
      "description": "The API key of the OpenAI API.",
      "defaultValue": "${env:OPENAI_API_KEY}"
    },
    {
      "name": "api_type",
      "type": "string",
      "required": false,
      "description": "The type of the OpenAI API, if you use Azure, it can be: azure"
    },
    {
      "name": "api_version",
      "type": "string",
      "required": false,
      "description": "The version of the OpenAI API."
    },
    {
      "name": "http_proxy",
      "type": "string",
      "required": false,
      "description": "The http or https proxy to use openai"
    }
  ]
}} />

----------------------------------------

TITLE: Chatting with Datasource using cURL
DESCRIPTION: Shows how to use cURL to send a POST request to the chat completions endpoint, including authentication and chat parameters.

LANGUAGE: shell
CODE:
DBGPT_API_KEY=dbgpt
DB_NAME="{your_db_name}"

curl -X POST "http://localhost:5670/api/v2/chat/completions" \
    -H "Authorization: Bearer $DBGPT_API_KEY" \
    -H "accept: application/json" \
    -H "Content-Type: application/json" \
    -d "{\"messages\":\"show space datas limit 5\",\"model\":\"gpt-4o\", \"chat_mode\": \"chat_data\", \"chat_param\": \"$DB_NAME\"}"

----------------------------------------

TITLE: Pulling DB-GPT Docker Image
DESCRIPTION: Command to pull the official DB-GPT Docker image from Eosphoros AI Docker Hub repository.

LANGUAGE: bash
CODE:
docker pull eosphorosai/dbgpt-openai:latest

----------------------------------------

TITLE: Rendering SiliconFlowRerankEmbeddingsParameters Configuration in JSX
DESCRIPTION: This code snippet renders a ConfigDetail component with the configuration details for SiliconFlowRerankEmbeddingsParameters. It includes various parameters such as name, provider, verbose, concurrency, API URL, API key, backend, and timeout, along with their types, descriptions, and default values.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "SiliconFlowRerankEmbeddingsParameters",
  "description": "SiliconFlow Rerank Embeddings Parameters.",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "proxy/siliconflow"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "50"
    },
    {
      "name": "api_url",
      "type": "string",
      "required": false,
      "description": "The URL of the rerank API.",
      "defaultValue": "https://api.siliconflow.cn/v1/rerank"
    },
    {
      "name": "api_key",
      "type": "string",
      "required": false,
      "description": "The API key for the rerank API.",
      "defaultValue": "${env:SILICONFLOW_API_KEY}"
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name."
    },
    {
      "name": "timeout",
      "type": "integer",
      "required": false,
      "description": "The timeout for the request in seconds.",
      "defaultValue": "60"
    }
  ]
}} />

----------------------------------------

TITLE: Sample Conversation Output Between Agents
DESCRIPTION: Shows the output of a conversation between Bob and Alice, demonstrating the exchange of jokes and responses between the agents.

LANGUAGE: bash
CODE:
--------------------------------------------------------------------------------
Bob (to Alice)-[]:

"Tell me a joke."

--------------------------------------------------------------------------------
un_stream ai response: Why don't scientists trust atoms?

Because they make up everything!

--------------------------------------------------------------------------------
Alice (to Bob)-[gpt-4o]:

"Why don't scientists trust atoms?\n\nBecause they make up everything!"
>>>>>>>>Alice Review info: 
Pass(None)
>>>>>>>>Alice Action report: 
execution succeeded,
Why don't scientists trust atoms?

Because they make up everything!

--------------------------------------------------------------------------------

----------------------------------------

TITLE: Rendering BitsandbytesQuantization8bits Configuration in JSX
DESCRIPTION: This code snippet uses a custom React component called ConfigDetail to render the configuration details for BitsandbytesQuantization8bits. It includes parameters for controlling 4-bit and 8-bit model loading, CPU offloading, quantization thresholds, and module skipping.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "BitsandbytesQuantization8bits",
  "description": "Bits and bytes quantization 8 bits parameters.",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "load_in_4bits",
      "type": "boolean",
      "required": false,
      "description": "Whether to load the model in 4 bits, default is False.",
      "defaultValue": "False"
    },
    {
      "name": "llm_int8_enable_fp32_cpu_offload",
      "type": "boolean",
      "required": false,
      "description": "8-bit models can offload weights between the CPU and GPU to support fitting very large models into memory. The weights dispatched to the CPU are actually stored in float32, and aren't converted to 8-bit. ",
      "defaultValue": "False"
    },
    {
      "name": "llm_int8_threshold",
      "type": "number",
      "required": false,
      "description": "An \"outlier\" is a hidden state value greater than a certain threshold, and these values are computed in fp16. While the values are usually normally distributed ([-3.5, 3.5]), this distribution can be very different for large models ([-60, 6] or [6, 60]). 8-bit quantization works well for values ~5, but beyond that, there is a significant performance penalty. A good default threshold value is 6, but a lower threshold may be needed for more unstable models (small models or finetuning).",
      "defaultValue": "6.0"
    },
    {
      "name": "llm_int8_skip_modules",
      "type": "string",
      "required": false,
      "description": "An explicit list of the modules that we do not want to convert in 8-bit. This is useful for models such as Jukebox that has several heads in different places and not necessarily at the last position. For example for `CausalLM` models, the last `lm_head` is kept in its original `dtype`",
      "defaultValue": "[]"
    },
    {
      "name": "load_in_8bits",
      "type": "boolean",
      "required": false,
      "description": "Whether to load the model in 8 bits(LLM.int8() algorithm).",
      "defaultValue": "True"
    }
  ]
}} />

----------------------------------------

TITLE: Configuring Proxy API Settings
DESCRIPTION: Environment configuration for using proxy LLM models like OpenAI.

LANGUAGE: bash
CODE:
#set LLM_MODEL TYPE
LLM_MODEL=proxyllm
#set your Proxy Api key and Proxy Server url
PROXY_API_KEY={your-openai-sk}
PROXY_SERVER_URL=https://api.openai.com/v1/chat/completions

----------------------------------------

TITLE: Rendering MySQL Configuration Parameters with React Component in JSX
DESCRIPTION: This code snippet uses a custom React component called ConfigDetail to render the configuration parameters for a MySQL database connection. It includes details such as port, user, database name, driver, password, and various connection pool settings.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "MySQLParameters",
  "description": "Fast, reliable, scalable open-source relational database management system.",
  "documentationUrl": null,
  "parameters": [
    {
      "name": "port",
      "type": "integer",
      "required": true,
      "description": "Database port, e.g., 3306"
    },
    {
      "name": "user",
      "type": "string",
      "required": true,
      "description": "Database user to connect"
    },
    {
      "name": "database",
      "type": "string",
      "required": true,
      "description": "Database name"
    },
    {
      "name": "driver",
      "type": "string",
      "required": false,
      "description": "Driver name for MySQL, default is mysql+pymysql.",
      "defaultValue": "mysql+pymysql"
    },
    {
      "name": "password",
      "type": "string",
      "required": false,
      "description": "Database password, you can write your password directly, of course, you can also use environment variables, such as ${env:DBGPT_DB_PASSWORD}",
      "defaultValue": "${env:DBGPT_DB_PASSWORD}"
    },
    {
      "name": "pool_size",
      "type": "integer",
      "required": false,
      "description": "Connection pool size, default 5",
      "defaultValue": "5"
    },
    {
      "name": "max_overflow",
      "type": "integer",
      "required": false,
      "description": "Max overflow connections, default 10",
      "defaultValue": "10"
    },
    {
      "name": "pool_timeout",
      "type": "integer",
      "required": false,
      "description": "Connection pool timeout, default 30",
      "defaultValue": "30"
    },
    {
      "name": "pool_recycle",
      "type": "integer",
      "required": false,
      "description": "Connection pool recycle, default 3600",
      "defaultValue": "3600"
    },
    {
      "name": "pool_pre_ping",
      "type": "boolean",
      "required": false,
      "description": "Connection pool pre ping, default True",
      "defaultValue": "True"
    },
    {
      "name": "host",
      "type": "string",
      "required": true,
      "description": "Database host, e.g., localhost"
    }
  ]
}} />

----------------------------------------

TITLE: Stream Chat Flow CURL Implementation
DESCRIPTION: CURL example for making a streaming chat flow request with authentication and required parameters.

LANGUAGE: shell
CODE:
DBGPT_API_KEY=dbgpt
FLOW_ID={YOUR_FLOW_ID}

curl -X POST "http://localhost:5670/api/v2/chat/completions" \
    -H "Authorization: Bearer $DBGPT_API_KEY" \
    -H "accept: application/json" \
    -H "Content-Type: application/json" \
    -d "{\"messages\":\"Hello\",\"model\":\"chatgpt_proxyllm\", \"chat_mode\": \"chat_flow\", \"chat_param\": \"$FLOW_ID\"}"

----------------------------------------

TITLE: Configuring MySQL Database for DB-GPT
DESCRIPTION: TOML configuration snippet for setting up MySQL as the database for DB-GPT application.

LANGUAGE: toml
CODE:
[service.web.database]
type = "mysql"
host = "127.0.0.1"
port = 3306
user = "root"
database = "dbgpt"
password = "aa123456"

----------------------------------------

TITLE: Running Profile Script with Environment Variables
DESCRIPTION: Examples of running the profile script with and without environment variables to demonstrate dynamic value changes.

LANGUAGE: bash
CODE:
python profile_dynamic.py

LANGUAGE: bash
CODE:
summary_profile_name="Plato" python profile_dynamic.py

----------------------------------------

TITLE: Downloading Local GPU Models
DESCRIPTION: Steps for downloading models when GPU resources are available for local deployment.

LANGUAGE: bash
CODE:
mkdir models && cd models

# # download embedding model, eg: glm-4-9b-chat or  
git clone https://huggingface.co/THUDM/glm-4-9b-chat

# download embedding model, eg: text2vec-large-chinese
git clone https://huggingface.co/GanymedeNil/text2vec-large-chinese

popd

----------------------------------------

TITLE: Rendering ServeConfig Configuration Details in JSX
DESCRIPTION: This code snippet uses a custom ConfigDetail component to render the configuration details for the ServeConfig object. It includes parameters for loading intervals, encryption keys, and API keys.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "ServeConfig",
  "description": "This configuration is for the flow serve module.",
  "documentationUrl": null,
  "parameters": [
    {
      "name": "load_dbgpts_interval",
      "type": "integer",
      "required": false,
      "description": "Interval to load dbgpts from installed packages",
      "defaultValue": "5"
    },
    {
      "name": "encrypt_key",
      "type": "string",
      "required": false,
      "description": "The key to encrypt the data"
    },
    {
      "name": "api_keys",
      "type": "string",
      "required": false,
      "description": "API keys for the endpoint, if None, allow all"
    }
  ]
}} />

----------------------------------------

TITLE: Creating Custom ProfileFactory in Python
DESCRIPTION: Demonstrates how to create a custom ProfileFactory class for more flexible profile creation.

LANGUAGE: python
CODE:
from typing import Optional
from dbgpt.agent import ProfileFactory, Profile, DefaultProfile

class MyProfileFactory(ProfileFactory):
    def create_profile(
        self,
        profile_id: int,
        name: Optional[str] = None,
        role: Optional[str] = None,
        goal: Optional[str] = None,
        prefer_prompt_language: Optional[str] = None,
        prefer_model: Optional[str] = None,
    ) -> Optional[Profile]:
        return DefaultProfile(
            name="Aristotle",
            role="Summarizer",
            goal=(
                "Summarize answer summaries based on user questions from provided "
                "resource information or from historical conversation memories."
            ),
            desc=(
                "You can summarize provided text content according to user's questions"
                " and output the summarization."
            ),
            expand_prompt="Keep your answer concise",
            examples=""
        )

----------------------------------------

TITLE: Creating MySQL Database for DB-GPT
DESCRIPTION: Command to create the MySQL database and tables for DB-GPT using a provided SQL script.

LANGUAGE: bash
CODE:
$ mysql -h127.0.0.1 -uroot -p{your_password} < ./assets/schema/dbgpt.sql

----------------------------------------

TITLE: Downloading Model Files
DESCRIPTION: Instructions for downloading the required LLM and embedding models using Git.

LANGUAGE: bash
CODE:
mkdir models && cd models

# download embedding model, eg: text2vec-large-chinese
git clone https://huggingface.co/GanymedeNil/text2vec-large-chinese

----------------------------------------

TITLE: Chat API Endpoints List
DESCRIPTION: List of available chat-related API endpoints for DB-GPT including database management, dialogue handling, and chat completions.

LANGUAGE: python
CODE:
api/v1/chat/db/list
api/v1/chat/db/add
api/v1/chat/db/edit
api/v1/chat/db/delete
api/v1/chat/db/test/connect
api/v1/chat/db/summary
api/v1/chat/db/support/type
api/v1/chat/dialogue/list
api/v1/chat/dialogue/scenes
api/v1/chat/dialogue/new
api/v1/chat/mode/params/list
api/v1/chat/mode/params/file/load
api/v1/chat/dialogue/delete
api/v1/chat/dialogue/messages
api/v1/chat/prepare
api/v1/chat/completions

----------------------------------------

TITLE: Rendering Zhipu Proxy LLM Configuration in JSX
DESCRIPTION: This code snippet uses a custom React component called ConfigDetail to render the configuration parameters for the Zhipu proxy LLM. It includes various settings such as model name, backend, provider, API details, and other operational parameters.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "ZhipuDeployModelParameters",
  "description": "Zhipu proxy LLM configuration.",
  "documentationUrl": "https://open.bigmodel.cn/dev/api/normal-model/glm-4#overview",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name."
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "proxy/zhipu"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "100"
    },
    {
      "name": "prompt_template",
      "type": "string",
      "required": false,
      "description": "Prompt template. If None, the prompt template is automatically determined from model. Just for local deployment."
    },
    {
      "name": "context_length",
      "type": "integer",
      "required": false,
      "description": "The context length of the OpenAI API. If None, it is determined by the model."
    },
    {
      "name": "api_base",
      "type": "string",
      "required": false,
      "description": "The base url of the Zhipu API.",
      "defaultValue": "${env:ZHIPUAI_BASE_URL:-https://open.bigmodel.cn/api/paas/v4}"
    },
    {
      "name": "api_key",
      "type": "string",
      "required": false,
      "description": "The API key of the Zhipu API.",
      "defaultValue": "${env:ZHIPUAI_API_KEY}"
    },
    {
      "name": "api_type",
      "type": "string",
      "required": false,
      "description": "The type of the OpenAI API, if you use Azure, it can be: azure"
    },
    {
      "name": "api_version",
      "type": "string",
      "required": false,
      "description": "The version of the OpenAI API."
    },
    {
      "name": "http_proxy",
      "type": "string",
      "required": false,
      "description": "The http or https proxy to use openai"
    }
  ]
}} />

----------------------------------------

TITLE: Configuring Hive Connection Parameters using React Component
DESCRIPTION: React/JSX component implementation for displaying Hive connection configuration parameters. The component uses ConfigDetail to render a structured view of all required and optional Hive connection settings including authentication, network, and driver configurations.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "HiveParameters",
  "description": "A distributed fault-tolerant data warehouse system.",
  "documentationUrl": null,
  "parameters": [
    {
      "name": "port",
      "type": "integer",
      "required": false,
      "description": "Hive server port, default 10000",
      "defaultValue": "10000"
    },
    {
      "name": "database",
      "type": "string",
      "required": false,
      "description": "Database name, default 'default'",
      "defaultValue": "default"
    },
    {
      "name": "auth",
      "type": "string",
      "required": false,
      "description": "Authentication mode: NONE, NOSASL, LDAP, KERBEROS, CUSTOM",
      "defaultValue": "NONE",
      "validValues": [
        "NONE",
        "NOSASL",
        "LDAP",
        "KERBEROS",
        "CUSTOM"
      ]
    },
    {
      "name": "username",
      "type": "string",
      "required": false,
      "description": "Username for authentication",
      "defaultValue": ""
    },
    {
      "name": "password",
      "type": "string",
      "required": false,
      "description": "Password for LDAP or CUSTOM auth",
      "defaultValue": ""
    },
    {
      "name": "kerberos_service_name",
      "type": "string",
      "required": false,
      "description": "Kerberos service name",
      "defaultValue": "hive"
    },
    {
      "name": "transport_mode",
      "type": "string",
      "required": false,
      "description": "Transport mode: binary or http",
      "defaultValue": "binary"
    },
    {
      "name": "driver",
      "type": "string",
      "required": false,
      "description": "Driver name for Hive, default is hive.",
      "defaultValue": "hive"
    },
    {
      "name": "host",
      "type": "string",
      "required": true,
      "description": "Hive server host"
    }
  ]
}} />

----------------------------------------

TITLE: Get App Endpoint
DESCRIPTION: GET endpoint for retrieving specific app details by app_id. Requires API key authentication.

LANGUAGE: python
CODE:
GET /api/v2/serve/apps/{app_id}

----------------------------------------

TITLE: Installing Dependencies for GLM4 Local Model
DESCRIPTION: Command to install dependencies required for running DB-GPT with the local GLM4 model using uv.

LANGUAGE: bash
CODE:
uv sync --all-packages \
--extra "base" \
--extra "cuda121" \
--extra "hf" \
--extra "rag" \
--extra "storage_chromadb" \
--extra "quant_bnb" \
--extra "dbgpts"

----------------------------------------

TITLE: Listing DB-GPT Knowledge Base Commands in Python
DESCRIPTION: Displays the help information for the 'knowledge' subcommand, showing options for managing knowledge spaces and documents.

LANGUAGE: python
CODE:
~ dbgpt knowledge --help
Already connect 'dbgpt'
Usage: dbgpt knowledge [OPTIONS] COMMAND [ARGS]...

  Knowledge command line tool

Options:
  --address TEXT  Address of the Api server(If not set, try to read from
                  environment variable: API_ADDRESS).  [default:
                  http://127.0.0.1:5670]
  --help          Show this message and exit.

Commands:
  delete  Delete your knowledge space or document in space
  list    List knowledge space
  load    Load your local documents to DB-GPT

----------------------------------------

TITLE: Rendering QianfanEmbeddingDeployModelParameters Configuration in JSX
DESCRIPTION: This code snippet uses a custom React component called ConfigDetail to render the configuration schema for QianfanEmbeddingDeployModelParameters. It includes detailed information about each parameter, such as name, type, required status, description, and default values.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "QianfanEmbeddingDeployModelParameters",
  "description": "Qianfan Embeddings deploy model parameters.",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "proxy/qianfan"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "100"
    },
    {
      "name": "api_key",
      "type": "string",
      "required": false,
      "description": "The API key for the embeddings API."
    },
    {
      "name": "api_secret",
      "type": "string",
      "required": false,
      "description": "The Secret key for the embeddings API. It's the sk for qianfan."
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name."
    }
  ]
}} />

----------------------------------------

TITLE: Rendering Tongyi Proxy LLM Configuration using React Component in JSX
DESCRIPTION: This code snippet uses a custom React component called ConfigDetail to render the configuration parameters for the Tongyi proxy LLM. It includes details such as model name, backend, provider, API settings, and other deployment options.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "TongyiDeployModelParameters",
  "description": "Tongyi proxy LLM configuration.",
  "documentationUrl": "https://help.aliyun.com/zh/model-studio/getting-started/first-api-call-to-qwen",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name."
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "proxy/tongyi"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "100"
    },
    {
      "name": "prompt_template",
      "type": "string",
      "required": false,
      "description": "Prompt template. If None, the prompt template is automatically determined from model. Just for local deployment."
    },
    {
      "name": "context_length",
      "type": "integer",
      "required": false,
      "description": "The context length of the OpenAI API. If None, it is determined by the model."
    },
    {
      "name": "api_base",
      "type": "string",
      "required": false,
      "description": "The base url of the tongyi API.",
      "defaultValue": "https://dashscope.aliyuncs.com/compatible-mode/v1"
    },
    {
      "name": "api_key",
      "type": "string",
      "required": false,
      "description": "The API key of the tongyi API.",
      "defaultValue": "${env:DASHSCOPE_API_KEY}"
    },
    {
      "name": "api_type",
      "type": "string",
      "required": false,
      "description": "The type of the OpenAI API, if you use Azure, it can be: azure"
    },
    {
      "name": "api_version",
      "type": "string",
      "required": false,
      "description": "The version of the OpenAI API."
    },
    {
      "name": "http_proxy",
      "type": "string",
      "required": false,
      "description": "The http or https proxy to use openai"
    }
  ]
}} />

----------------------------------------

TITLE: Configuring OpenAI API Key
DESCRIPTION: TOML configuration snippet for setting up the OpenAI API key in the DB-GPT configuration file.

LANGUAGE: toml
CODE:
# Model Configurations
[models]
[[models.llms]]
...
api_key = "your-openai-api-key"
[[models.embeddings]]
...
api_key = "your-openai-api-key"

----------------------------------------

TITLE: Deleting a Datasource using Python
DESCRIPTION: Demonstrates how to delete a datasource using the DB-GPT client in Python, specifying the datasource ID.

LANGUAGE: python
CODE:
from dbgpt_client import Client
from dbgpt_client.datasource import delete_datasource

DBGPT_API_KEY = "dbgpt"
datasource_id = "{your_datasource_id}"

client = Client(api_key=DBGPT_API_KEY)
res = await delete_datasource(client=client, datasource_id=datasource_id)

----------------------------------------

TITLE: Importing ConfigDiagram Component in JavaScript/JSX
DESCRIPTION: This snippet imports the ConfigDiagram component from a local file path. The component is likely used to render the configuration diagram based on the provided relationships data.

LANGUAGE: JavaScript
CODE:
import { ConfigDiagram } from "@site/src/components/mdx/ConfigDiagram";

----------------------------------------

TITLE: Installing DB-GPT Python Client
DESCRIPTION: Command to install the official DB-GPT client package from PyPI with version specification.

LANGUAGE: bash
CODE:
pip install "dbgpt[client]>=0.5.2"

----------------------------------------

TITLE: Configuring Datasource Serve Module with React Component
DESCRIPTION: React/MDX component implementation for displaying datasource serve configuration details, including API key settings. The configuration uses ConfigDetail component to render structured documentation.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "ServeConfig",
  "description": "This configuration is for the datasource serve module.",
  "documentationUrl": null,
  "parameters": [
    {
      "name": "api_keys",
      "type": "string",
      "required": false,
      "description": "API keys for the endpoint, if None, allow all"
    }
  ]
}} />

----------------------------------------

TITLE: Installing Chroma Dependencies
DESCRIPTION: Installs the required chromadb package for vector storage functionality.

LANGUAGE: bash
CODE:
pip install chromadb

----------------------------------------

TITLE: Installing Dependencies for OpenAI Proxy
DESCRIPTION: Command to install dependencies required for running DB-GPT with OpenAI proxy using uv.

LANGUAGE: bash
CODE:
uv sync --all-packages \
--extra "base" \
--extra "proxy_openai" \
--extra "rag" \
--extra "storage_chromadb" \
--extra "dbgpts"

----------------------------------------

TITLE: Deleting a Datasource using cURL
DESCRIPTION: Shows how to delete a datasource using a cURL DELETE request, including authentication and datasource ID.

LANGUAGE: shell
CODE:
DBGPT_API_KEY=dbgpt
DATASOURCE_ID={YOUR_DATASOURCE_ID}
 
curl -X DELETE "http://localhost:5670/api/v2/serve/datasources/$DATASOURCE_ID" \
    -H "Authorization: Bearer $DBGPT_API_KEY"

----------------------------------------

TITLE: Rendering ServeConfig Documentation with React Component
DESCRIPTION: This code snippet uses a custom React component called ConfigDetail to render the configuration details for the DB-GPT serve module. It includes information about the API keys parameter.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "ServeConfig",
  "description": "This configuration is for the my dbgpts serve module.",
  "documentationUrl": null,
  "parameters": [
    {
      "name": "api_keys",
      "type": "string",
      "required": false,
      "description": "API keys for the endpoint, if None, allow all"
    }
  ]
}} />

----------------------------------------

TITLE: Configuring Qwen (Tongyi) Proxy in DB-GPT Environment
DESCRIPTION: This code snippet demonstrates the configuration settings for using Qwen (Tongyi) as a proxy LLM in DB-GPT, including the API key and service URL.

LANGUAGE: python
CODE:
# .env
# Aliyun tongyiqianwen
LLM_MODEL=tongyi_proxyllm
TONGYI_PROXY_API_KEY={your-tongyi-sk}
PROXY_SERVER_URL={your_service_url}

----------------------------------------

TITLE: OpenAI Configuration Example
DESCRIPTION: TOML configuration example for setting up OpenAI API integration

LANGUAGE: toml
CODE:
# Model Configurations
[models]
[[models.llms]]
...
api_key = "your-openai-api-key"
[[models.embeddings]]
...
api_key = "your-openai-api-key"

----------------------------------------

TITLE: Creating Evaluation using Python in DB-GPT
DESCRIPTION: This snippet shows how to create an evaluation using Python. It uses the dbgpt_client library to create a client and send an evaluation request with specified parameters.

LANGUAGE: python
CODE:
from dbgpt_client import Client
from dbgpt_client.evaluation import run_evaluation
from dbgpt.serve.evaluate.api.schemas import EvaluateServeRequest

DBGPT_API_KEY = "dbgpt"
client = Client(api_key=DBGPT_API_KEY)
request = EvaluateServeRequest(
    # The scene type of the evaluation, e.g. support app, recall
    scene_key="recall",
    # e.g. app id(when scene_key is app), space id(when scene_key is recall)
    scene_value="147",
    context={"top_k": 5},
    evaluate_metrics=[
        "RetrieverHitRateMetric",
        "RetrieverMRRMetric",
        "RetrieverSimilarityMetric",
    ],
    datasets=[
        {
            "query": "what awel talked about",
            "doc_name": "awel.md",
        }
    ],
)
data = await run_evaluation(client, request=request)

----------------------------------------

TITLE: Installing uv Package Manager (PyPI)
DESCRIPTION: Commands to install uv using pip and pipx.

LANGUAGE: bash
CODE:
python -m pip install --upgrade pip
python -m pip install --upgrade pipx
python -m pipx ensurepath
pipx install uv --global

----------------------------------------

TITLE: Checking uv Installation
DESCRIPTION: Command to verify the successful installation of uv by checking its version.

LANGUAGE: bash
CODE:
uv --version

----------------------------------------

TITLE: Rendering Deepseek LLM Configuration Component in JSX
DESCRIPTION: This code snippet renders a ConfigDetail component with Deepseek proxy LLM configuration parameters. It includes settings for model name, backend, provider, API connections, and various runtime options.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "DeepSeekDeployModelParameters",
  "description": "Deepseek proxy LLM configuration.",
  "documentationUrl": "https://api-docs.deepseek.com/",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name."
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "proxy/deepseek"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "100"
    },
    {
      "name": "prompt_template",
      "type": "string",
      "required": false,
      "description": "Prompt template. If None, the prompt template is automatically determined from model. Just for local deployment."
    },
    {
      "name": "context_length",
      "type": "integer",
      "required": false,
      "description": "The context length of the OpenAI API. If None, it is determined by the model."
    },
    {
      "name": "api_base",
      "type": "string",
      "required": false,
      "description": "The base url of the DeepSeek API.",
      "defaultValue": "${env:DEEPSEEK_API_BASE:-https://api.deepseek.com/v1}"
    },
    {
      "name": "api_key",
      "type": "string",
      "required": false,
      "description": "The API key of the DeepSeek API.",
      "defaultValue": "${env:DEEPSEEK_API_KEY}"
    },
    {
      "name": "api_type",
      "type": "string",
      "required": false,
      "description": "The type of the OpenAI API, if you use Azure, it can be: azure"
    },
    {
      "name": "api_version",
      "type": "string",
      "required": false,
      "description": "The version of the OpenAI API."
    },
    {
      "name": "http_proxy",
      "type": "string",
      "required": false,
      "description": "The http or https proxy to use openai"
    }
  ]
}} />

----------------------------------------

TITLE: Downloading DB-GPT Source Code
DESCRIPTION: Command to clone the DB-GPT repository from GitHub

LANGUAGE: bash
CODE:
git clone https://github.com/eosphoros-ai/DB-GPT.git

----------------------------------------

TITLE: Installing UV Package Manager
DESCRIPTION: Different methods for installing the UV package manager, including direct shell installation and PyPI installation

LANGUAGE: bash
CODE:
curl -LsSf https://astral.sh/uv/install.sh | sh

LANGUAGE: bash
CODE:
python -m pip install --upgrade pip
python -m pip install --upgrade pipx
python -m pipx ensurepath
pipx install uv --global

----------------------------------------

TITLE: Stream Chat Flow Python Client Implementation
DESCRIPTION: Python client example for streaming chat interactions using the DB-GPT client library.

LANGUAGE: python
CODE:
from dbgpt_client import Client

DBGPT_API_KEY = "dbgpt"
FLOW_ID="{YOUR_FLOW_ID}"

client = Client(api_key=DBGPT_API_KEY)
async for data in client.chat_stream(
    messages="Introduce AWEL", 
    model="chatgpt_proxyllm", 
    chat_mode="chat_flow", 
    chat_param=FLOW_ID
):
    print(data)

----------------------------------------

TITLE: Downloading DB-GPT Source Code
DESCRIPTION: Command to clone the DB-GPT repository from GitHub.

LANGUAGE: bash
CODE:
git clone https://github.com/eosphoros-ai/DB-GPT.git

----------------------------------------

TITLE: Configuring BitsandbytesQuantization4bits Component in React/MDX
DESCRIPTION: React/MDX component implementation for displaying quantization configuration parameters. Includes settings for 4-bit and 8-bit model loading, computation datatypes, quantization types, and nested quantization options.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "BitsandbytesQuantization4bits",
  "description": "Bits and bytes quantization 4 bits parameters.",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "load_in_4bits",
      "type": "boolean",
      "required": false,
      "description": "Whether to load the model in 4 bits.",
      "defaultValue": "True"
    },
    {
      "name": "bnb_4bit_compute_dtype",
      "type": "string",
      "required": false,
      "description": "To speedup computation, you can change the data type from float32 (the default value) to bfloat16",
      "validValues": [
        "bfloat16",
        "float16",
        "float32"
      ]
    },
    {
      "name": "bnb_4bit_quant_type",
      "type": "string",
      "required": false,
      "description": "Quantization datatypes, `fp4` (four bit float) and `nf4` (normal four bit float), only valid when load_4bit=True",
      "defaultValue": "nf4",
      "validValues": [
        "nf4",
        "fp4"
      ]
    },
    {
      "name": "bnb_4bit_use_double_quant",
      "type": "boolean",
      "required": false,
      "description": "Nested quantization is a technique that can save additional memory at no additional performance cost. This feature performs a second quantization of the already quantized weights to save an additional 0.4 bits/parameter. ",
      "defaultValue": "True"
    },
    {
      "name": "load_in_8bits",
      "type": "boolean",
      "required": false,
      "description": "Whether to load the model in 8 bits(LLM.int8() algorithm), default is False.",
      "defaultValue": "False"
    }
  ]
}} />

----------------------------------------

TITLE: Configuring DeepSeek API and Embedding Model
DESCRIPTION: TOML configuration snippet for setting up the DeepSeek API key and specifying the embedding model in the DB-GPT configuration file.

LANGUAGE: toml
CODE:
# Model Configurations
[models]
[[models.llms]]
# name = "deepseek-chat"
name = "deepseek-reasoner"
provider = "proxy/deepseek"
api_key = "your-deepseek-api-key"
[[models.embeddings]]
name = "BAAI/bge-large-zh-v1.5"
provider = "hf"
# If not provided, the model will be downloaded from the Hugging Face model hub
# uncomment the following line to specify the model path in the local file system
# path = "the-model-path-in-the-local-file-system"
path = "/data/models/bge-large-zh-v1.5"

----------------------------------------

TITLE: Displaying Baseline Scores
DESCRIPTION: Imports and calls the show_scores function to display baseline performance metrics.

LANGUAGE: python
CODE:
from dbgpt_hub.baseline import show_scores
show_scores()

----------------------------------------

TITLE: Installing ChatGLM Dependencies for DB-GPT
DESCRIPTION: This snippet shows how to install the zhipuai library, which is required for using the ChatGLM model in DB-GPT.

LANGUAGE: python
CODE:
pip install zhipuai

----------------------------------------

TITLE: Setting HTTP Authorization Header for DB-GPT API
DESCRIPTION: Demonstrates how to set the Authorization header with a Bearer token for DB-GPT API requests.

LANGUAGE: http
CODE:
Authorization: Bearer DBGPT_API_KEY

----------------------------------------

TITLE: Configuring OpenAI Proxy in DB-GPT Environment
DESCRIPTION: This snippet shows the configuration settings for using OpenAI as a proxy LLM in DB-GPT, including model selection, API key, and server URL.

LANGUAGE: python
CODE:
# .env
LLM_MODEL=chatgpt_proxyllm
PROXY_API_KEY={your-openai-sk}
PROXY_SERVER_URL=https://api.openai.com/v1/chat/completions
# If you use gpt-4
# PROXYLLM_BACKEND=gpt-4

----------------------------------------

TITLE: Configuring Wenxin Model Parameters in React/JSX
DESCRIPTION: React/JSX component configuration for Baidu Wenxin model parameters including API credentials, model settings, and deployment options. The configuration includes essential parameters like API keys, model name, backend settings, and performance configurations.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "WenxinDeployModelParameters",
  "description": "Baidu Wenxin proxy LLM configuration.",
  "documentationUrl": "https://cloud.baidu.com/doc/WENXINWORKSHOP/s/clntwmv7t",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name."
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "proxy/wenxin"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "100"
    },
    {
      "name": "prompt_template",
      "type": "string",
      "required": false,
      "description": "Prompt template. If None, the prompt template is automatically determined from model. Just for local deployment."
    },
    {
      "name": "context_length",
      "type": "integer",
      "required": false,
      "description": "The context length of the OpenAI API. If None, it is determined by the model."
    },
    {
      "name": "api_key",
      "type": "string",
      "required": false,
      "description": "The API key of the Wenxin API.",
      "defaultValue": "${env:WEN_XIN_API_KEY}"
    },
    {
      "name": "api_secret",
      "type": "string",
      "required": false,
      "description": "The API secret key of the Wenxin API.",
      "defaultValue": "${env:WEN_XIN_API_SECRET}"
    }
  ]
}} />

----------------------------------------

TITLE: Running DB-GPT Webserver with DeepSeek Proxy
DESCRIPTION: Commands to start the DB-GPT webserver using the DeepSeek proxy configuration.

LANGUAGE: bash
CODE:
uv run dbgpt start webserver --config configs/dbgpt-proxy-deepseek.toml

LANGUAGE: bash
CODE:
uv run python packages/dbgpt-app/src/dbgpt_app/dbgpt_server.py --config configs/dbgpt-proxy-deepseek.toml

----------------------------------------

TITLE: Installing dbgpt_hub Package
DESCRIPTION: Installs the dbgpt_hub package using pip.

LANGUAGE: bash
CODE:
pip install dbgpt_hub

----------------------------------------

TITLE: Setting API Keys in Environment File
DESCRIPTION: Configuration example for setting allowed API keys in the environment file.

LANGUAGE: python
CODE:
API_KEYS=dbgpt

----------------------------------------

TITLE: Installing OpenAI Dependencies for DB-GPT
DESCRIPTION: This snippet shows how to install the OpenAI-specific dependencies for DB-GPT using pip.

LANGUAGE: python
CODE:
pip install  -e ".[openai]"

----------------------------------------

TITLE: Configuring CrossEncoder Rerank Component in JSX
DESCRIPTION: JSX component implementation showing the configuration structure for CrossEncoder Rerank Embeddings Parameters. The configuration includes model name, path, device settings, provider details, and various operational parameters.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "CrossEncoderRerankEmbeddingsParameters",
  "description": "CrossEncoder Rerank Embeddings Parameters.",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "path",
      "type": "string",
      "required": false,
      "description": "The path of the model, if you want to deploy a local model."
    },
    {
      "name": "device",
      "type": "string",
      "required": false,
      "description": "Device to run model. If None, the device is automatically determined"
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "hf"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "50"
    },
    {
      "name": "max_length",
      "type": "integer",
      "required": false,
      "description": "Max length for input sequences. Longer sequences will be truncated."
    },
    {
      "name": "model_kwargs",
      "type": "object",
      "required": false,
      "description": "Keyword arguments to pass to the model.",
      "defaultValue": "{}"
    }
  ]
}} />

----------------------------------------

TITLE: Running DB-GPT with Docker-Compose
DESCRIPTION: Command to start DB-GPT containers using Docker-Compose. Requires a valid SiliconFlow API key to be set as an environment variable.

LANGUAGE: bash
CODE:
SILICONFLOW_API_KEY=${SILICONFLOW_API_KEY} docker compose up -d

----------------------------------------

TITLE: Evaluating Text2SQL Model on Spider Dataset
DESCRIPTION: Command for evaluating the Text2SQL model's performance on the Spider dataset.

LANGUAGE: python
CODE:
python dbgpt_hub/eval/evaluation.py --plug_value --input  Your_model_pred.sql

----------------------------------------

TITLE: Installing LLama.cpp Server Dependencies for DB-GPT
DESCRIPTION: Command to install the necessary dependencies for using LLama.cpp server with DB-GPT. This includes an optional command for GPU acceleration.

LANGUAGE: bash
CODE:
pip install -e ".[llama_cpp_server]"

LANGUAGE: bash
CODE:
CMAKE_ARGS="-DGGML_CUDA=ON" pip install -e ".[llama_cpp_server]"

----------------------------------------

TITLE: Creating Knowledge Space with cURL
DESCRIPTION: Example of creating a knowledge space using cURL with required authorization and JSON payload.

LANGUAGE: shell
CODE:
DBGPT_API_KEY="dbgpt"

curl --location --request POST 'http://localhost:5670/api/v2/serve/knowledge/spaces' \
--header 'Authorization: Bearer $DBGPT_API_KEY' \
--header 'Content-Type: application/json' \
--data-raw '{"desc": "for client space desc", "name": "test_space_2", "owner": "dbgpt", "vector_type": "Chroma"
}'

----------------------------------------

TITLE: Installing vLLM Dependencies for DB-GPT
DESCRIPTION: Command to install vLLM as an optional dependency for DB-GPT using pip. This step is necessary to enable vLLM inference.

LANGUAGE: bash
CODE:
pip install -e ".[vllm]"

----------------------------------------

TITLE: Configuring OllamaEmbeddingDeployModelParameters Component in JSX
DESCRIPTION: React/MDX component that defines the configuration schema for Ollama embedding model deployment. Includes parameters for model name, provider, verbosity, concurrency limits, API URL, and backend model specification.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "OllamaEmbeddingDeployModelParameters",
  "description": "Ollama Embeddings deploy model parameters.",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "proxy/ollama"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "100"
    },
    {
      "name": "api_url",
      "type": "string",
      "required": false,
      "description": "The URL of the embeddings API.",
      "defaultValue": "http://localhost:11434"
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name."
    }
  ]
}} />

----------------------------------------

TITLE: Initializing LlamaCppModelParameters in Python
DESCRIPTION: This code snippet shows the constructor signature for the LlamaCppModelParameters class. It includes all possible parameters that can be used to configure a Llama.cpp model, such as model name, path, device, concurrency, and various performance-related settings.

LANGUAGE: python
CODE:
LlamaCppModelParameters(name: str, provider: str = 'llama.cpp', verbose: Optional[bool] = False, concurrency: Optional[int] = 5, backend: Optional[str] = None, prompt_template: Optional[str] = None, context_length: Optional[int] = None, path: Optional[str] = None, device: Optional[str] = None, seed: Optional[int] = -1, n_threads: Optional[int] = None, n_batch: Optional[int] = 512, n_gpu_layers: Optional[int] = 1000000000, n_gqa: Optional[int] = None, rms_norm_eps: Optional[float] = 5e-06, cache_capacity: Optional[str] = None, prefer_cpu: Optional[bool] = False)

----------------------------------------

TITLE: Asynchronous Hello World Implementation
DESCRIPTION: Implementation of the hello world DAG using asyncio for asynchronous execution

LANGUAGE: python
CODE:
import asyncio

from dbgpt.core.awel import DAG, MapOperator

with DAG("awel_hello_world") as dag:
    task = MapOperator(map_function=lambda x: print(f"Hello, {x}!"))

asyncio.run(task.call(call_data="world"))

----------------------------------------

TITLE: Installing DB-GPT Postgres Datasource Dependencies
DESCRIPTION: This command installs the necessary dependencies for using Postgres as a datasource in DB-GPT, including base packages, datasource_postgres, rag, storage_chromadb, and dbgpts extras.

LANGUAGE: bash
CODE:
uv sync --all-packages \
--extra "base" \
--extra "datasource_postgres" \
--extra "rag" \
--extra "storage_chromadb" \
--extra "dbgpts"

----------------------------------------

TITLE: Defining Volcengine LLM Configuration Parameters
DESCRIPTION: Comprehensive configuration object that defines all available parameters for setting up a Volcengine proxy LLM integration. Includes API settings, model specifications, and runtime parameters.

LANGUAGE: json
CODE:
{
  "name": "VolcengineDeployModelParameters",
  "description": "Volcengine proxy LLM configuration.",
  "documentationUrl": "https://www.volcengine.com/docs/82379/1298454",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name."
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "proxy/volcengine"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "100"
    },
    {
      "name": "prompt_template",
      "type": "string",
      "required": false,
      "description": "Prompt template. If None, the prompt template is automatically determined from model. Just for local deployment."
    },
    {
      "name": "context_length",
      "type": "integer",
      "required": false,
      "description": "The context length of the OpenAI API. If None, it is determined by the model."
    },
    {
      "name": "api_base",
      "type": "string",
      "required": false,
      "description": "The base url of the Volcengine API.",
      "defaultValue": "${env:ARK_API_BASE:-https://ark.cn-beijing.volces.com/api/v3}"
    },
    {
      "name": "api_key",
      "type": "string",
      "required": false,
      "description": "The API key of the Volcengine API.",
      "defaultValue": "${env:ARK_API_KEY}"
    },
    {
      "name": "api_type",
      "type": "string",
      "required": false,
      "description": "The type of the OpenAI API, if you use Azure, it can be: azure"
    },
    {
      "name": "api_version",
      "type": "string",
      "required": false,
      "description": "The version of the OpenAI API."
    },
    {
      "name": "http_proxy",
      "type": "string",
      "required": false,
      "description": "The http or https proxy to use openai"
    }
  ]
}

----------------------------------------

TITLE: Rendering Claude Proxy LLM Configuration Component in JSX
DESCRIPTION: This code snippet renders a ConfigDetail component with a detailed configuration object for Claude Proxy LLM. It includes parameters for model name, backend, provider, API settings, and other deployment options.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "ClaudeDeployModelParameters",
  "description": "Claude Proxy LLM",
  "documentationUrl": "https://docs.anthropic.com/en/api/getting-started",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name."
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "proxy/claude"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "100"
    },
    {
      "name": "prompt_template",
      "type": "string",
      "required": false,
      "description": "Prompt template. If None, the prompt template is automatically determined from model. Just for local deployment."
    },
    {
      "name": "context_length",
      "type": "integer",
      "required": false,
      "description": "The context length of the OpenAI API. If None, it is determined by the model."
    },
    {
      "name": "api_base",
      "type": "string",
      "required": false,
      "description": "The base url of the claude API.",
      "defaultValue": "${env:ANTHROPIC_BASE_URL:-https://api.anthropic.com}"
    },
    {
      "name": "api_key",
      "type": "string",
      "required": false,
      "description": "The API key of the claude API.",
      "defaultValue": "${env:ANTHROPIC_API_KEY}"
    },
    {
      "name": "api_type",
      "type": "string",
      "required": false,
      "description": "The type of the OpenAI API, if you use Azure, it can be: azure"
    },
    {
      "name": "api_version",
      "type": "string",
      "required": false,
      "description": "The version of the OpenAI API."
    },
    {
      "name": "http_proxy",
      "type": "string",
      "required": false,
      "description": "The http or https proxy to use openai"
    }
  ]
}} />

----------------------------------------

TITLE: Installing DB-GPT Dependencies for Database Integration
DESCRIPTION: This command installs the necessary dependencies to use databases with DB-GPT agents, specifically the 'simple_framework' package version 0.5.9rc0 or higher.

LANGUAGE: bash
CODE:
pip install "dbgpt[simple_framework]>=0.5.9rc0"

----------------------------------------

TITLE: Starting DB-GPT Webserver with Postgres Configuration
DESCRIPTION: This command starts the DB-GPT webserver using a specific configuration file for OpenAI proxy settings. It runs the dbgpt_server.py script with the specified configuration.

LANGUAGE: bash
CODE:
uv run python packages/dbgpt-app/src/dbgpt_app/dbgpt_server.py --config configs/dbgpt-proxy-openai.toml

----------------------------------------

TITLE: Executing Hello World Custom Operator in Bash
DESCRIPTION: This command runs the Python script containing the HelloWorldOperator, demonstrating how to execute custom operators from the command line.

LANGUAGE: bash
CODE:
poetry run python awel_tutorial/hello_world_custom_operator.py

----------------------------------------

TITLE: Starting DB-GPT Server with DuckDB
DESCRIPTION: Commands to start the DB-GPT webserver using a proxy configuration for OpenAI. Two identical commands are provided as alternative methods.

LANGUAGE: bash
CODE:
uv run python packages/dbgpt-app/src/dbgpt_app/dbgpt_server.py --config configs/dbgpt-proxy-openai.toml

----------------------------------------

TITLE: Loading Test Data in Linux
DESCRIPTION: Command to load built-in test data into the local database for DB-GPT in Linux environments.

LANGUAGE: bash
CODE:
bash ./scripts/examples/load_examples.sh

----------------------------------------

TITLE: Rendering ServeConfig Configuration Details using ConfigDetail Component in JSX
DESCRIPTION: This code snippet uses a custom ConfigDetail component to render the configuration details for ServeConfig. It includes parameters for the default model and API keys used in the conversation serve module.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "ServeConfig",
  "description": "This configuration is for the conversation serve module.",
  "documentationUrl": null,
  "parameters": [
    {
      "name": "default_model",
      "type": "string",
      "required": false,
      "description": "Default model for the conversation"
    },
    {
      "name": "api_keys",
      "type": "string",
      "required": false,
      "description": "API keys for the endpoint, if None, allow all"
    }
  ]
}} />

----------------------------------------

TITLE: Testing AWEL DAG in Development Environment
DESCRIPTION: Shows how to test the AWEL DAG using curl in the development environment set up for local debugging.

LANGUAGE: bash
CODE:
curl -X GET http://127.0.0.1:5555/api/v1/awel/trigger/examples/hello\?name\=zhangsan
"Hello, zhangsan, your age is 18"

----------------------------------------

TITLE: Executing Custom Streaming Operators in Bash
DESCRIPTION: This command runs the Python script containing the custom streaming operators, showing how to execute and test streaming operators from the command line.

LANGUAGE: bash
CODE:
poetry run python awel_tutorial/custom_streaming_operator.py

----------------------------------------

TITLE: Configuring Elasticsearch Storage in DB-GPT
DESCRIPTION: TOML configuration for setting up Elasticsearch connection parameters including host URI and port.

LANGUAGE: toml
CODE:
[rag.storage]
[rag.storage.full_text]
type = "ElasticSearch"
uri = "127.0.0.1"
port = "9200"

----------------------------------------

TITLE: Linking to GitHub Release Notes in Markdown
DESCRIPTION: This snippet creates a markdown link to the GitHub releases page for the DB-GPT project. It provides users with easy access to detailed version release information.

LANGUAGE: markdown
CODE:
[ReleaseNotes](https://github.com/eosphoros-ai/DB-GPT/releases)

----------------------------------------

TITLE: Running Web Front-end Separately
DESCRIPTION: Commands to set up and run the DB-GPT web front-end separately from the main application.

LANGUAGE: bash
CODE:
cd web && npm install
cp .env.template .env
// Set API_BASE_URL to your DB-GPT server address, usually http://localhost:5670
npm run dev

----------------------------------------

TITLE: Setting Up Development Environment for AWEL Testing
DESCRIPTION: Provides a script to set up a development environment for testing the AWEL DAG without starting the full DB-GPT server.

LANGUAGE: python
CODE:
if __name__ == "__main__":
    if dag.leaf_nodes[0].dev_mode:
        # Development mode, you can run the dag locally for debugging.
        from dbgpt.core.awel import setup_dev_environment
        setup_dev_environment([dag], port=5555)
    else:
        # Production mode, DB-GPT will automatically load and execute the current file after startup.
        pass

----------------------------------------

TITLE: Installing DB-GPT Oceanbase Dependencies
DESCRIPTION: Command to install required DB-GPT packages including base, proxy_openai, rag, storage_obvector, and dbgpts using the uv package manager.

LANGUAGE: bash
CODE:
uv sync --all-packages \
--extra "base" \
--extra "proxy_openai" \
--extra "rag" \
--extra "storage_obvector" \
--extra "dbgpts"

----------------------------------------

TITLE: Testing HTTP Trigger with cURL
DESCRIPTION: Command to test the HTTP trigger endpoint using cURL, sending a GET request to the configured endpoint.

LANGUAGE: bash
CODE:
curl -X GET http://127.0.0.1:5555/api/v1/awel/trigger/awel_tutorial/hello_world

----------------------------------------

TITLE: Configuring Oceanbase Vector Storage Settings
DESCRIPTION: TOML configuration for setting up Oceanbase Vector storage connection parameters including host, port, and optional authentication credentials.

LANGUAGE: toml
CODE:
[rag.storage]
[rag.storage.vector]
type = "Oceanbase"
uri = "127.0.0.1"
port = "19530"
#username="dbgpt"
#password=19530

----------------------------------------

TITLE: Creating GPTS App Tables in SQL
DESCRIPTION: SQL scripts to create tables for GPTS applications, including gpts_app for storing application details, gpts_app_collection for user collections, and gpts_app_detail for agent-specific information.

LANGUAGE: SQL
CODE:
CREATE TABLE `gpts_app` (
  `id` int NOT NULL AUTO_INCREMENT COMMENT 'autoincrement id',
  `app_code` varchar(255) NOT NULL COMMENT 'Current AI assistant code',
  `app_name` varchar(255) NOT NULL COMMENT 'Current AI assistant name',
  `app_describe` varchar(2255) NOT NULL COMMENT 'Current AI assistant describe',
  `language` varchar(100) NOT NULL COMMENT 'gpts language',
  `team_mode` varchar(255) NOT NULL COMMENT 'Team work mode',
  `team_context` text COMMENT 'The execution logic and team member content that teams with different working modes rely on',
  `user_code` varchar(255) DEFAULT NULL COMMENT 'user code',
  `sys_code` varchar(255) DEFAULT NULL COMMENT 'system app code',
  `created_at` datetime DEFAULT NULL COMMENT 'create time',
  `updated_at` datetime DEFAULT NULL COMMENT 'last update time',
  `icon` varchar(1024) DEFAULT NULL COMMENT 'app icon, url',
  PRIMARY KEY (`id`),
  UNIQUE KEY `uk_gpts_app` (`app_name`)
) ENGINE=InnoDB AUTO_INCREMENT=39 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;

CREATE TABLE `gpts_app_collection` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT 'autoincrement id',
  `app_code` varchar(255) NOT NULL COMMENT 'Current AI assistant code',
  `user_code` int(11) NOT NULL COMMENT 'user code',
  `sys_code` varchar(255) NOT NULL COMMENT 'system app code',
  `created_at` datetime DEFAULT NULL COMMENT 'create time',
  `updated_at` datetime DEFAULT NULL COMMENT 'last update time',
  PRIMARY KEY (`id`),
  KEY `idx_app_code` (`app_code`),
  KEY `idx_user_code` (`user_code`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COMMENT="gpt collections";

CREATE TABLE `gpts_app_detail` (
  `id` int NOT NULL AUTO_INCREMENT COMMENT 'autoincrement id',
  `app_code` varchar(255) NOT NULL COMMENT 'Current AI assistant code',
  `app_name` varchar(255) NOT NULL COMMENT 'Current AI assistant name',
  `agent_name` varchar(255) NOT NULL COMMENT ' Agent name',
  `node_id` varchar(255) NOT NULL COMMENT 'Current AI assistant Agent Node id',
  `resources` text COMMENT 'Agent bind  resource',
  `prompt_template` text COMMENT 'Agent bind  template',
  `llm_strategy` varchar(25) DEFAULT NULL COMMENT 'Agent use llm strategy',
  `llm_strategy_value` text COMMENT 'Agent use llm strategy value',
  `created_at` datetime DEFAULT NULL COMMENT 'create time',
  `updated_at` datetime DEFAULT NULL COMMENT 'last update time',
  PRIMARY KEY (`id`),
  UNIQUE KEY `uk_gpts_app_agent_node` (`app_name`,`agent_name`,`node_id`)
) ENGINE=InnoDB AUTO_INCREMENT=23 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;

----------------------------------------

TITLE: Starting DB-GPT Webserver with Tracing
DESCRIPTION: Command to start the DB-GPT webserver after configuring OpenTelemetry tracing.

LANGUAGE: bash
CODE:
dbgpt start webserver

----------------------------------------

TITLE: Importing AWEL Dependencies in Python
DESCRIPTION: Imports necessary classes from dbgpt for creating AWEL workflows, including BaseModel, Field, DAG, HttpTrigger, and MapOperator.

LANGUAGE: python
CODE:
from dbgpt._private.pydantic import BaseModel, Field
from dbgpt.core.awel import DAG, HttpTrigger, MapOperator

----------------------------------------

TITLE: Implementing HTTP Trigger for String Response in Python
DESCRIPTION: Python code to create an HTTP trigger that returns a string response based on query parameters. It uses AWEL's DAG structure and includes a custom TriggerReqBody class for request parsing.

LANGUAGE: python
CODE:
from dbgpt._private.pydantic import BaseModel, Field
from dbgpt.core.awel import DAG, HttpTrigger, MapOperator, setup_dev_environment

class TriggerReqBody(BaseModel):
    name: str = Field(..., description="User name")
    age: int = Field(18, description="User age")

with DAG("awel_say_hello") as dag:
    trigger_task = HttpTrigger(
        endpoint="/awel_tutorial/say_hello", 
        methods="GET", 
        request_body=TriggerReqBody,
        status_code=200
    )
    task = MapOperator(
        map_function=lambda x: f"Hello, {x.name}! You are {x.age} years old."
    )
    trigger_task >> task

setup_dev_environment([dag], port=5555)

----------------------------------------

TITLE: Starting DB-GPT Server with Oceanbase Config
DESCRIPTION: Command to launch the DB-GPT server using the configured Oceanbase Vector settings from the TOML file.

LANGUAGE: bash
CODE:
uv run python packages/dbgpt-app/src/dbgpt_app/dbgpt_server.py --config configs/dbgpt-proxy-openai.toml

----------------------------------------

TITLE: Creating Elasticsearch Connector
DESCRIPTION: Python function to create and configure an Elasticsearch connection for document storage

LANGUAGE: python
CODE:
from dbgpt_ext.storage.full_text.elasticsearch import ElasticDocumentConfig, \
    ElasticDocumentStore
def _create_es_connector():
    """Create es connector."""
    config = ElasticDocumentConfig(
        name="keyword_rag_test",
        uri="localhost",
        port="9200",
        user="elastic",
        password="dbgpt",
    )

    return ElasticDocumentStore(config)

----------------------------------------

TITLE: Installing DB-GPT Elasticsearch Dependencies
DESCRIPTION: Command to install required DB-GPT packages including elasticsearch storage and RAG components using the uv package manager.

LANGUAGE: bash
CODE:
uv sync --all-packages --frozen \
--extra "base" \
--extra "proxy_openai" \
--extra "rag" \
--extra "storage_elasticsearch" \
--extra "dbgpts"

----------------------------------------

TITLE: Visualizing AWEL DAGs
DESCRIPTION: Code to generate visual representations of the AWEL DAGs for both schema loading and chat data processing.

LANGUAGE: python
CODE:
load_schema_dag.visualize_dag()
chat_data_dag.visualize_dag()

# For Jupyter Notebook display
display(load_schema_dag)
display(chat_data_dag)

----------------------------------------

TITLE: Processing Ollama API Response in Python
DESCRIPTION: Handles the response from the Ollama API, extracting the generated text and processing it line by line. It uses json.loads to parse the response data.

LANGUAGE: python
CODE:
import json

for line in response.iter_lines():
    if line:
        response_data = json.loads(line)
        response_text = response_data['response']
        yield response_text

----------------------------------------

TITLE: Running the AWEL HTTP Server
DESCRIPTION: Command to start the HTTP server that hosts the POST endpoint on port 5555.

LANGUAGE: bash
CODE:
poetry run python awel_tutorial/http_trigger_say_hello_post.py

----------------------------------------

TITLE: RAG Storage Provider Configuration
DESCRIPTION: Table listing supported RAG storage providers and their installation requirements. Includes vector databases and search engines like Chroma, Milvus, and Elasticsearch.

LANGUAGE: markdown
CODE:
| Provider    | Supported | Install Packages               |
|-------------|-----------|--------------------------------|
| Chroma      | ✅         | --extra storage_chroma         |       
| Milvus      | ✅         | --extra storage_milvus         |       
| Elasticsearch | ✅         | --extra storage_elasticsearch   |        
| OceanBase   | ✅         | --extra storage_obvector      |

----------------------------------------

TITLE: Configuring OpenAI ChatGPT Integration
DESCRIPTION: Environment variables setup for using OpenAI's ChatGPT service with DB-GPT via proxy configuration.

LANGUAGE: shell
CODE:
LLM_MODEL=proxyllm

LANGUAGE: shell
CODE:
PROXY_API_KEY={your-openai-sk}
PROXY_SERVER_URL=https://api.openai.com/v1/chat/completions

----------------------------------------

TITLE: Installing DB-GPT DuckDB Dependencies
DESCRIPTION: Command to install required packages for DuckDB integration with DB-GPT using uv package manager. Installs base, datasource_duckdb, rag, and storage_chromadb extras.

LANGUAGE: bash
CODE:
uv sync --all-packages \
--extra "base" \
--extra "datasource_duckdb" \
--extra "rag" \
--extra "storage_chromadb" \


----------------------------------------

TITLE: Running DB-GPT Streaming HTTP Trigger Script
DESCRIPTION: This bash command runs the Python script that sets up the streaming HTTP trigger in DB-GPT.

LANGUAGE: bash
CODE:
poetry run python awel_tutorial/http_trigger_stream_numbers.py

----------------------------------------

TITLE: Expected JSON Response
DESCRIPTION: Example response showing the formatted greeting message returned by the endpoint.

LANGUAGE: plaintext
CODE:
{"message":"Hello, John! You are 20 years old."}

----------------------------------------

TITLE: Graph RAG Storage Provider Configuration
DESCRIPTION: Table showing supported graph database providers for RAG storage and their installation packages. Currently supports TuGraph with Neo4j planned for future integration.

LANGUAGE: markdown
CODE:
| Provider | Supported | Install Packages |
|----------|-----------|------------------|
| TuGraph  | ✅         | --extra graph_rag|
| Neo4j    | ❌         |                  |

----------------------------------------

TITLE: Installing Git LFS for Text2Vec Model
DESCRIPTION: Commands for installing Git Large File Storage (LFS) on different operating systems to download the text2vec-large-chinese embedding model.

LANGUAGE: bash
CODE:
cd models
git lfs clone https://huggingface.co/GanymedeNil/text2vec-large-chinese

----------------------------------------

TITLE: Displaying Data Source Support Table in Markdown
DESCRIPTION: This snippet presents a markdown table listing various data sources, their support status in DB-GPT, and brief descriptions. It covers a wide range of databases and data warehouses, including SQL and NoSQL options.

LANGUAGE: markdown
CODE:
| DataSource                                                                      | support | Notes                                       |
| ------------------------------------------------------------------------------  |---------| ------------------------------------------- |
| [MySQL](https://www.mysql.com/)                                                 | Yes     |  MySQL is the world's most popular open source database.                             |
| [PostgresSQL](https://www.postgresql.org/)                                      | Yes     |  The World's Most Advanced Open Source Relational Database                                   |
| [Vertica](https://www.vertica.com/)                                             | Yes     |  Vertica is a strongly consistent, ACID-compliant, SQL data warehouse, built for the scale and complexity of today's data-driven world.                                   |
| [Spark](https://github.com/apache/spark)                                        | Yes     |  Unified Engine for large-scale data analytics                                |
| [DuckDB](https://github.com/duckdb/duckdb)                                      | Yes     |  DuckDB is an in-process SQL OLAP database management system                                          |
| [Sqlite](https://github.com/sqlite/sqlite)                                      | Yes     |                                             |
| [MSSQL](https://github.com/microsoft/mssql-jdbc)                                | Yes     |                                             |
| [ClickHouse](https://github.com/ClickHouse/ClickHouse)                          | Yes     |  ClickHouse is the fastest and most resource efficient open-source database for real-time apps and analytics.                                      |
| [Oracle](https://github.com/oracle)                                             | No      |           TODO                              |
| [Redis](https://github.com/redis/redis)                                         | No      |  The Multi-model NoSQL Database                              |
| [MongoDB](https://github.com/mongodb/mongo)                                     | No      |  MongoDB is a source-available cross-platform document-oriented database program                              |
| [HBase](https://github.com/apache/hbase)                                        | No      |  Open-source, distributed, versioned, column-oriented store modeled                              |
| [Doris](https://github.com/apache/doris)                                        | Yes     |  Apache Doris is an easy-to-use, high performance and unified analytics database.                              |
| [DB2](https://github.com/IBM/Db2)                                               | No      |           TODO                              |
| [Couchbase](https://github.com/couchbase)                                       | No      |           TODO                              |
| [Elasticsearch](https://github.com/elastic/elasticsearch)                       | No      |  Free and Open, Distributed, RESTful Search Engine                              |
| [OceanBase](https://github.com/OceanBase)                                       | No      |  OceanBase is a distributed relational database.                               |
| [TiDB](https://github.com/pingcap/tidb)                                         | No      |           TODO                              |
| [StarRocks](https://github.com/StarRocks/starrocks)                             | Yes     | StarRocks is a next-gen, high-performance analytical data warehouse                               |

----------------------------------------

TITLE: Testing POST Endpoint with CURL
DESCRIPTION: CURL command to test the HTTP endpoint by sending a POST request with JSON payload containing name and age values.

LANGUAGE: bash
CODE:
curl -X POST \
"http://127.0.0.1:5555/api/v1/awel/trigger/awel_tutorial/say_hello_post" \
-H "Content-Type: application/json" \
-d '{"name": "John", "age": 25}'

----------------------------------------

TITLE: Starting Model Controller with Storage Registry
DESCRIPTION: Command to start a DB-GPT model controller instance using MySQL as the storage backend for the model registry. Configures database connection and port settings.

LANGUAGE: bash
CODE:
dbgpt start controller \
--port 8000 \
--registry_type database \
--registry_db_type mysql \
--registry_db_name dbgpt \
--registry_db_host 127.0.0.1 \
--registry_db_port 3306 \
--registry_db_user root \
--registry_db_password aa123456

----------------------------------------

TITLE: HTTP POST Request for RAG Knowledge Embedding
DESCRIPTION: cURL command to trigger the knowledge embedding process via HTTP POST request to the AWEL API endpoint.

LANGUAGE: bash
CODE:
curl --location --request POST 'http://localhost:5670/api/v1/awel/trigger/rag/knowledge/embedding/process' \
--header 'Content-Type: application/json' \
--data-raw '{}'

----------------------------------------

TITLE: Building Docker Image for HA Deployment
DESCRIPTION: Command to build a Docker image for DB-GPT with OpenAI dependencies.

LANGUAGE: bash
CODE:
bash ./docker/base/build_proxy_image.sh --pip-index-url https://pypi.tuna.tsinghua.edu.cn/simple

----------------------------------------

TITLE: Predicting with Fine-tuned Text2SQL Model
DESCRIPTION: Bash script for running predictions using the fine-tuned Text2SQL model.

LANGUAGE: python
CODE:
sh ./dbgpt_hub/scripts/predict_sft.sh

----------------------------------------

TITLE: Configuring Chroma Vector Database in DB-GPT
DESCRIPTION: Sets the VECTOR_STORE_TYPE to Chroma in the .env file and optionally specifies the CHROMA_PERSIST_PATH for data persistence.

LANGUAGE: shell
CODE:
### Chroma vector db config
VECTOR_STORE_TYPE=Chroma
#CHROMA_PERSIST_PATH=/root/DB-GPT/pilot/data

----------------------------------------

TITLE: Testing HTTP Trigger with cURL for JSON Response
DESCRIPTION: cURL command to test the HTTP trigger by sending a GET request with query parameters, expecting a JSON response.

LANGUAGE: bash
CODE:
curl -X GET \
"http://127.0.0.1:5555/api/v1/awel/trigger/awel_tutorial/say_hello_json?name=John&age=25"

----------------------------------------

TITLE: Making HTTP Request to Trigger Hybrid Knowledge Process
DESCRIPTION: Example of how to trigger the hybrid knowledge processing workflow via HTTP POST request to the API endpoint. The request demonstrates the basic structure needed to initiate the process, with an empty JSON body.

LANGUAGE: bash
CODE:
curl --location --request POST 'http://localhost:5670/api/v1/awel/trigger/rag/knowledge/hybrid/process' \
--header 'Content-Type: application/json' \
--data-raw '{}'

LANGUAGE: bash
CODE:
[
    "async persist vector store success 1 chunks.",
    "async persist graph store success 1 chunks."
]

----------------------------------------

TITLE: Defining Intent Knowledge Base for Application Routing
DESCRIPTION: This snippet shows the structure for defining intents, app codes, descriptions, and slots for each sub-application. This information is used for intent recognition and routing user questions to the appropriate intelligent application.

LANGUAGE: plaintext
CODE:
#######################
Intent:DB答疑 App Code:a41d0274-8ac4-11ef-8735-3ea07eeef889 Describe: 所有DB领域相关知识的咨询答疑，包含了日常DBA的FAQ问题数据、OceanBase(OB)的官方文档手册，操作手册、问题排查手册、日常疑难问题的知识总结、可以进行专业的DBA领域知识答疑。 只要和DB相关的不属于其他应用负责范畴的都可以使用我来回答 问题范例: 1.怎么查看OB抖动？ 2.DMS权限如何申请 3.如何确认xxxxx 类型:知识库咨询
#######################
Intent:数据对话 App Code:516963c4-8ac9-11ef-8735-3ea07eeef889 Describe: 通过SQL查询分析当前数据库(dbgpt-test:包含用户和用户销售订单数据的数据库） 类型:数据查询
#######################
Intent:天气检索助手 App Code:f93610cc-8acc-11ef-8735-3ea07eeef889 Describe: 可以进行天气查询 Slots:
位置: 要获取天气信息的具体位置
时间: 要获取的天气信息的时间，如果没有明确提到，使用当前时间

----------------------------------------

TITLE: Displaying Help Information for dbgpt trace Command
DESCRIPTION: Command to display help information for the dbgpt trace functionality.

LANGUAGE: python
CODE:
dbgpt trace --help

----------------------------------------

TITLE: Configuring Milvus Vector Database in DB-GPT
DESCRIPTION: Sets the VECTOR_STORE_TYPE to Milvus in the .env file and configures Milvus connection details including URL, port, and optional authentication parameters.

LANGUAGE: shell
CODE:
### Milvus vector db config
VECTOR_STORE_TYPE=Milvus
MILVUS_URL=127.0.0.1
MILVUS_PORT=19530
#MILVUS_USERNAME
#MILVUS_PASSWORD
#MILVUS_SECURE=

----------------------------------------

TITLE: Testing HTTP Trigger with cURL for String Response
DESCRIPTION: cURL command to test the HTTP trigger by sending a GET request with query parameters.

LANGUAGE: bash
CODE:
curl -X GET \
"http://127.0.0.1:5555/api/v1/awel/trigger/awel_tutorial/say_hello?name=John&age=25"

----------------------------------------

TITLE: DAG Visualization Implementation
DESCRIPTION: Enhanced version of the two-task DAG with visualization capabilities using graphviz

LANGUAGE: python
CODE:
import asyncio

from dbgpt.core.awel import DAG, MapOperator, InputOperator, SimpleCallDataInputSource

with DAG("awel_hello_world") as dag:
    input_task = InputOperator(
        input_source=SimpleCallDataInputSource()
    )
    task = MapOperator(map_function=lambda x: print(f"Hello, {x}!"))
    input_task >> task

dag.visualize_dag()
asyncio.run(task.call(call_data="world"))

----------------------------------------

TITLE: Triggering Knowledge Graph Processing via HTTP POST
DESCRIPTION: This bash script demonstrates how to trigger the knowledge graph processing workflow using an HTTP POST request to the AWEL API endpoint.

LANGUAGE: bash
CODE:
curl --location --request POST 'http://localhost:5670/api/v1/awel/trigger/rag/knowledge/kg/process' \
--header 'Content-Type: application/json' \
--data-raw '{}'

----------------------------------------

TITLE: Running HTTP Trigger Script for String Response
DESCRIPTION: Command to execute the Python script that sets up the HTTP trigger for string response.

LANGUAGE: bash
CODE:
poetry run python awel_tutorial/http_trigger_say_hello.py

----------------------------------------

TITLE: Preprocessing Data for Fine-tuning
DESCRIPTION: Preprocesses Spider dataset into the required format for fine-tuning. Specifies data sources, file paths, and output locations.

LANGUAGE: python
CODE:
data_folder = "dbgpt_hub/data"
data_info = [
     {
        "data_source": "spider",
        "train_file": ["train_spider.json", "train_others.json"],
        "dev_file": ["dev.json"],
        "tables_file": "tables.json",
        "db_id_name": "db_id",
        "is_multiple_turn": False,
        "train_output": "spider_train.json",
        "dev_output": "spider_dev.json",
    }
]

preprocess_sft_data(
      data_folder = data_folder,
      data_info = data_info
)

----------------------------------------

TITLE: Downloading CodeLlama-13b-Instruct-hf Model
DESCRIPTION: Commands for downloading the CodeLlama-13b-Instruct-hf model from HuggingFace using git lfs.

LANGUAGE: python
CODE:
cd Your_model_dir
git lfs install
git clone git@hf.co:codellama/CodeLlama-13b-Instruct-hf

----------------------------------------

TITLE: Installing Pydantic Package for DB-GPT AWEL
DESCRIPTION: Command to install the pydantic package, which is required for the HTTP trigger examples.

LANGUAGE: bash
CODE:
poetry add "pydantic>=2.6.0"

----------------------------------------

TITLE: Initializing OpenAI Embedding Model in Python
DESCRIPTION: Creates an embedding model using the OpenAI API through the DefaultEmbeddingFactory.

LANGUAGE: python
CODE:
from dbgpt.rag.embedding import DefaultEmbeddingFactory

embeddings = DefaultEmbeddingFactory.openai()

----------------------------------------

TITLE: Running HTTP Trigger Script for JSON Response
DESCRIPTION: Command to execute the Python script that sets up the HTTP trigger for JSON response.

LANGUAGE: bash
CODE:
poetry run python awel_tutorial/http_trigger_say_hello_json.py

----------------------------------------

TITLE: Evaluating Model Predictions
DESCRIPTION: Configures and initiates evaluation of the model's predictions against gold standard data. Specifies input files, database information, and evaluation type.

LANGUAGE: python
CODE:
evaluate_args =  {
            "input": "./dbgpt_hub/output/pred/pred_sql_dev_skeleton.sql",
            "gold": "./dbgpt_hub/data/eval_data/gold.txt",
            "gold_natsql": "./dbgpt_hub/data/eval_data/gold_natsql2sql.txt",
            "db": "./dbgpt_hub/data/spider/database",
            "table": "./dbgpt_hub/data/eval_data/tables.json",
            "table_natsql": "./dbgpt_hub/data/eval_data/tables_for_natsql2sql.json",
            "etype": "exec",
            "plug_value": True,
            "keep_distict": False,
            "progress_bar_for_each_datapoint": False,
            "natsql": False,
}
start_evaluate(evaluate_args)

----------------------------------------

TITLE: Example of Generated Training Data Format
DESCRIPTION: JSON representation of the generated training data format for Text2SQL tasks.

LANGUAGE: json
CODE:
{
  "db_id": "department_management",
  "instruction": "I want you to act as a SQL terminal in front of an example database, you need only to return the sql command to me.Below is an instruction that describes a task, Write a response that appropriately completes the request.\n\"\n##Instruction:\ndepartment_management contains tables such as department, head, management. Table department has columns such as Department_ID, Name, Creation, Ranking, Budget_in_Billions, Num_Employees. Department_ID is the primary key.\nTable head has columns such as head_ID, name, born_state, age. head_ID is the primary key.\nTable management has columns such as department_ID, head_ID, temporary_acting. department_ID is the primary key.\nThe head_ID of management is the foreign key of head_ID of head.\nThe department_ID of management is the foreign key of Department_ID of department.\n\n",
  "input": "###Input:\nHow many heads of the departments are older than 56 ?\n\n###Response:",
  "output": "SELECT count(*) FROM head WHERE age  >  56",
  "history": []
}

----------------------------------------

TITLE: Installing HTTP Dependencies with Poetry
DESCRIPTION: Installing required FastAPI and Uvicorn packages using Poetry package manager.

LANGUAGE: bash
CODE:
poetry add fastapi uvicorn

----------------------------------------

TITLE: Testing DB-GPT Streaming HTTP Trigger with cURL
DESCRIPTION: This cURL command sends a POST request to test the streaming HTTP trigger implemented in DB-GPT.

LANGUAGE: bash
CODE:
curl -X POST \
"http://127.0.0.1:5555/api/v1/awel/trigger/awel_tutorial/stream_numbers" \
-H "Content-Type: application/json" \
-d '{"n": 5}'

----------------------------------------

TITLE: Installing DB-GPT and Dependencies
DESCRIPTION: Commands to install DB-GPT and OpenAI packages using pip

LANGUAGE: shell
CODE:
pip install dbgpt --upgrade
pip install openai

----------------------------------------

TITLE: Expected Output from DB-GPT Streaming HTTP Trigger
DESCRIPTION: This plaintext snippet shows the expected output from the streaming HTTP trigger when requested to stream 5 numbers.

LANGUAGE: plaintext
CODE:
0
1
2
3
4

----------------------------------------

TITLE: Setting up DB-GPT-Hub Environment with Conda
DESCRIPTION: Instructions for cloning the DB-GPT-Hub repository, creating a conda environment, and installing dependencies using poetry.

LANGUAGE: python
CODE:
git clone https://github.com/eosphoros-ai/DB-GPT-Hub.git
cd DB-GPT-Hub
conda create -n dbgpt_hub python=3.10 
conda activate dbgpt_hub
conda install -c conda-forge poetry>=1.4.0
poetry install

----------------------------------------

TITLE: Model Prediction Script Configuration
DESCRIPTION: Detailed configuration for the prediction script, including model path, template, and output settings.

LANGUAGE: python
CODE:
echo " predict Start time: $(date)"
## predict
CUDA_VISIBLE_DEVICES=0 python dbgpt_hub/predict/predict.py \
    --model_name_or_path Your_download_CodeLlama-13b-Instruct-hf_path \
    --template llama2 \
    --finetuning_type lora \
    --checkpoint_dir Your_last_peft_checkpoint-4000  \
    --predicted_out_filename Your_model_pred.sql

echo "predict End time: $(date)"

----------------------------------------

TITLE: Two-Task DAG Implementation
DESCRIPTION: Implementation of a DAG with two connected tasks using InputOperator and MapOperator

LANGUAGE: python
CODE:
import asyncio

from dbgpt.core.awel import DAG, MapOperator, InputOperator, SimpleCallDataInputSource

with DAG("awel_hello_world") as dag:
    input_task = InputOperator(
        input_source=SimpleCallDataInputSource()
    )
    task = MapOperator(map_function=lambda x: print(f"Hello, {x}!"))
    input_task >> task
    

asyncio.run(task.call(call_data="world"))

----------------------------------------

TITLE: Installing DB-GPT Dependencies
DESCRIPTION: Command to install the required DB-GPT library with RAG support.

LANGUAGE: bash
CODE:
pip install "dbgpt[rag]>=0.5.3rc0" -U

----------------------------------------

TITLE: Installing DB-GPT with RAG Support in Python
DESCRIPTION: Installs the dbgpt library with RAG support using pip. This is a prerequisite for the RAG implementation.

LANGUAGE: bash
CODE:
pip install "dbgpt[rag]>=0.5.2"

----------------------------------------

TITLE: Installing OpenTelemetry Dependencies for DB-GPT
DESCRIPTION: Command to install the required OpenTelemetry packages for enabling distributed tracing in DB-GPT.

LANGUAGE: bash
CODE:
pip install opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp

----------------------------------------

TITLE: Implementing AWEL Task Lifecycle Hooks in Python
DESCRIPTION: This code snippet demonstrates how to create a custom task class with lifecycle hooks in AWEL. It defines a MyLifecycleTask class that implements 'before_dag_run' and 'after_dag_end' hooks, as well as a 'map' method for processing input.

LANGUAGE: python
CODE:
import asyncio
from dbgpt.core.awel import DAG, MapOperator

class MyLifecycleTask(MapOperator[str, str]):
    async def before_dag_run(self):
        print("Before DAG run")

    async def after_dag_end(self):
        print("After DAG end")

    async def map(self, x: str) -> str:
        return f"Hello, {x}!"

with DAG("awel_lifecycle_hooks") as dag:
    task = MyLifecycleTask()

print(asyncio.run(task.call("world")))

----------------------------------------

TITLE: Adding DB-GPT Dependency
DESCRIPTION: Command to add DB-GPT package as a project dependency

LANGUAGE: bash
CODE:
poetry add "dbgpt>=0.5.1"

----------------------------------------

TITLE: Executing Stream Transformation Script
DESCRIPTION: Command to run the stream transformation example using Poetry package manager.

LANGUAGE: bash
CODE:
poetry run python awel_tutorial/transform_stream_operator_double_numbers.py

----------------------------------------

TITLE: Installing AWEL Workflow for Web Info Search
DESCRIPTION: This command installs the 'awel-flow-web-info-search' workflow locally, which provides internet search capabilities for the search assistant.

LANGUAGE: bash
CODE:
dbgpt app install awel-flow-web-info-search

----------------------------------------

TITLE: Launching TuGraph Database Container
DESCRIPTION: Docker commands to pull and run TuGraph database container with required port configurations

LANGUAGE: bash
CODE:
docker pull tugraph/tugraph-runtime-centos7:4.5.1
docker run -d -p 7070:7070  -p 7687:7687 -p 9090:9090 --name tugraph_demo tugraph/tugraph-runtime-centos7:latest lgraph_server -d run --enable_plugin true

----------------------------------------

TITLE: Enabling Knowledge Search Rewrite in DB-GPT
DESCRIPTION: Sets the KNOWLEDGE_SEARCH_REWRITE environment variable to True in the .env file to enable Chat Knowledge Search Rewrite Mode.

LANGUAGE: shell
CODE:
# Whether to enable Chat Knowledge Search Rewrite Mode
KNOWLEDGE_SEARCH_REWRITE=True

----------------------------------------

TITLE: Executing AWEL Lifecycle Hooks Example in Bash
DESCRIPTION: This command runs the Python script containing the AWEL lifecycle hooks example using Poetry, a Python dependency management tool.

LANGUAGE: bash
CODE:
poetry run python awel_tutorial/lifecycle_hooks.py

----------------------------------------

TITLE: Creating Project Directory Structure
DESCRIPTION: Commands to create the initial project directory structure using poetry

LANGUAGE: bash
CODE:
mkdir -p ~/projects
cd ~/projects

LANGUAGE: bash
CODE:
poetry new awel-tutorial
cd awel-tutorial

----------------------------------------

TITLE: Installing DB Expert Assistant AWEL Workflow
DESCRIPTION: This command installs the 'db-expert-assisant' AWEL workflow, which provides intent recognition and routing capabilities for the unified intelligent application.

LANGUAGE: bash
CODE:
dbgpt app install db-expert-assisant

----------------------------------------

TITLE: Sample Output from Tool-Enabled Agent Execution
DESCRIPTION: Shows example output from running the tool-enabled agent, including thought process and tool execution results.

LANGUAGE: bash
CODE:
--------------------------------------------------------------------------------
User (to LuBan)-[]:

"Calculate the product of 10 and 99"

--------------------------------------------------------------------------------
un_stream ai response: {
  "thought": "To calculate the product of 10 and 99, we need to use a tool that can perform multiplication operation.",
  "tool_name": "simple_calculator",
  "args": {
    "first_number": 10,
    "second_number": 99,
    "operator": "*"
  }
}

--------------------------------------------------------------------------------
LuBan (to User)-[gpt-3.5-turbo]:

"{\n  \"thought\": \"To calculate the product of 10 and 99, we need to use a tool that can perform multiplication operation.\",\n  \"tool_name\": \"simple_calculator\",\n  \"args\": {\n    \"first_number\": 10,\n    \"second_number\": 99,\n    \"operator\": \"*\"\n  }\n}"
>>>>>>>>LuBan Review info: 
Pass(None)
>>>>>>>>LuBan Action report: 
execution succeeded,
990

--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
User (to LuBan)-[]:

"Count the number of files in /tmp"

--------------------------------------------------------------------------------
un_stream ai response: {
  "thought": "To count the number of files in /tmp directory, we should use a tool that can perform this operation.",
  "tool_name": "count_directory_files",
  "args": {
    "path": "/tmp"
  }
}

--------------------------------------------------------------------------------
LuBan (to User)-[gpt-3.5-turbo]:

"{\n  \"thought\": \"To count the number of files in /tmp directory, we should use a tool that can perform this operation.\",\n  \"tool_name\": \"count_directory_files\",\n  \"args\": {\n    \"path\": \"/tmp\"\n  }\n}"
>>>>>>>>LuBan Review info: 
Pass(None)
>>>>>>>>LuBan Action report: 
execution succeeded,
19

--------------------------------------------------------------------------------

----------------------------------------

TITLE: Configuring Weaviate Vector Database in DB-GPT
DESCRIPTION: Sets the VECTOR_STORE_TYPE to Weaviate in the .env file and optionally specifies the WEAVIATE_URL for connection.

LANGUAGE: shell
CODE:
### Weaviate vector db config
VECTOR_STORE_TYPE=Weaviate
#WEAVIATE_URL=https://kt-region-m8hcy0wc.weaviate.network

----------------------------------------

TITLE: Creating Basic MapOperator with Lambda Function in Python
DESCRIPTION: Demonstrates how to create a MapOperator using a simple lambda function to print greetings. This approach is useful for simple transformations that can be expressed in a single line.

LANGUAGE: python
CODE:
from dbgpt.core.awel import DAG, MapOperator

with DAG("awel_hello_world") as dag:
    task = MapOperator(map_function=lambda x: print(f"Hello, {x}!"))

----------------------------------------

TITLE: Summing Numbers with UnstreamifyAbsOperator in Python
DESCRIPTION: This example demonstrates how to use UnstreamifyAbsOperator to sum a stream of numbers. It includes a NumberProducerOperator to generate a stream of integers and a SumOperator to sum them.

LANGUAGE: python
CODE:
import asyncio
from typing import AsyncIterator
from dbgpt.core.awel import DAG, UnstreamifyAbsOperator, StreamifyAbsOperator

class NumberProducerOperator(StreamifyAbsOperator[int, int]):
    """Create a stream of numbers from 0 to `n-1`"""
    async def streamify(self, n: int) -> AsyncIterator[int]:
        for i in range(n):
            yield i

class SumOperator(UnstreamifyAbsOperator[int, int]):
    """Unstreamify the stream of numbers"""
    async def unstreamify(self, it: AsyncIterator[int]) -> int:
        return sum([i async for i in it])

with DAG("sum_dag") as dag:
    task = NumberProducerOperator()
    sum_task = SumOperator()
    task >> sum_task

print(asyncio.run(sum_task.call(call_data=5)))
print(asyncio.run(sum_task.call(call_data=10)))

----------------------------------------

TITLE: Sample Chat Output
DESCRIPTION: Example output showing a two-round conversation about Elon Musk, demonstrating the context-aware responses from the LLM.

LANGUAGE: plaintext
CODE:
First round
User: Who is elon musk?
AI: Elon Musk is a well-known entrepreneur and business magnate. He is the CEO and founder of SpaceX, Tesla Inc., Neuralink, and The Boring Company. Musk is known for his work in the technology and space industries, and he is also involved in the development of electric vehicles, renewable energy, and artificial intelligence.

Second round
User: Is he rich?
AI: Yes, Elon Musk is one of the richest people in the world. As the CEO and founder of multiple successful companies, including SpaceX and Tesla, his net worth fluctuates but is consistently in the billions of dollars.

----------------------------------------

TITLE: Installing DB-GPT Dependencies with UV
DESCRIPTION: Command to install required DB-GPT packages including proxy_openai, rag, storage_chromadb, and graph_rag extensions

LANGUAGE: bash
CODE:
uv sync --all-packages --frozen \
--extra "proxy_openai" \
--extra "rag" \
--extra "storage_chromadb" \
--extra "dbgpts"
--extra "graph_rag"

----------------------------------------

TITLE: Executing Multi-Round Chat Script
DESCRIPTION: Command to run the Python script that implements the multi-round chat system.

LANGUAGE: bash
CODE:
python multi_round_chat_with_llm.py

----------------------------------------

TITLE: Viewing Chat Details Based on Trace ID in DB-GPT
DESCRIPTION: This command displays chat details for a specific conversation in DB-GPT, identified by its trace ID.

LANGUAGE: python
CODE:
dbgpt trace chat --hide_run_params --trace_id ec30d733-7b35-4d61-b02e-2832fd2e29ff

----------------------------------------

TITLE: Double Number MapOperator Example in Python
DESCRIPTION: A complete example showing how to implement a MapOperator that doubles input numbers. Includes DAG setup, operator implementation, and testing code with async execution.

LANGUAGE: python
CODE:
import asyncio
from dbgpt.core.awel import DAG, MapOperator

class DoubleNumberOperator(MapOperator[int, int]):
    async def map(self, x: int) -> int:
        print(f"Received {x}, returning {x * 2}")
        return x * 2

with DAG("awel_double_number") as dag:
    task = DoubleNumberOperator()  
assert asyncio.run(task.call(2)) == 4

----------------------------------------

TITLE: Print Input Data Example with MapOperator
DESCRIPTION: Shows how to combine InputOperator with MapOperator to print input data using a simple string source.

LANGUAGE: python
CODE:
import asyncio
from dbgpt.core.awel import DAG, MapOperator, InputOperator, SimpleInputSource

with DAG("awel_input_operator") as dag:
    input_source = SimpleInputSource(data="Hello, World!")
    input_task = InputOperator(input_source=input_source)
    print_task = MapOperator(map_function=lambda x: print(x))
    input_task >> print_task

asyncio.run(print_task.call())

----------------------------------------

TITLE: Elasticsearch Configuration Settings
DESCRIPTION: Environment variables for configuring Elasticsearch connection in DB-GPT

LANGUAGE: plaintext
CODE:
ELASTICSEARCH_URL=localhost
ELASTICSEARCH_PORT=9200
ELASTICSEARCH_USERNAME=elastic
ELASTICSEARCH_PASSWORD=dbgpt

----------------------------------------

TITLE: Example Output Format
DESCRIPTION: Sample output showing the LLM-generated SQL query and associated metadata

LANGUAGE: plaintext
CODE:
ModelOutput(text='SELECT * FROM users;', error_code=0, model_context=None, finish_reason=None, usage={'completion_tokens': 5, 'prompt_tokens': 19, 'total_tokens': 24}, metrics=None)

----------------------------------------

TITLE: Downloading Pre-trained Embedding Model for Financial Analysis
DESCRIPTION: Clones a pre-trained embedding model from ModelScope, which is used for processing financial text in the analysis system.

LANGUAGE: bash
CODE:
git clone https://www.modelscope.cn/models/AI-ModelScope/bge-large-zh-v1.5

----------------------------------------

TITLE: Executing MapOperator Example via Poetry
DESCRIPTION: Command to run the double number operator example using Poetry package manager.

LANGUAGE: bash
CODE:
poetry run python awel_tutorial/map_operator_double_number.py

----------------------------------------

TITLE: Expected Output of AWEL Lifecycle Hooks Example
DESCRIPTION: This snippet shows the expected console output when running the AWEL lifecycle hooks example. It demonstrates the execution order of the hooks and the task's main functionality.

LANGUAGE: plaintext
CODE:
Before DAG run
After DAG end
Hello, world!

----------------------------------------

TITLE: Running the AWEL Workflow
DESCRIPTION: Command to execute the Python script containing the AWEL workflow

LANGUAGE: bash
CODE:
python simple_sdk_llm_example_dag.py

----------------------------------------

TITLE: Installing dbgpts Package
DESCRIPTION: Command to install the awel-flow-simple-streaming-chat package using the dbgpt CLI

LANGUAGE: bash
CODE:
dbgpt app install awel-flow-simple-streaming-chat -U

----------------------------------------

TITLE: Running DB-GPT-Web in Development Mode
DESCRIPTION: Commands to start DB-GPT-Web in development mode using npm or yarn.

LANGUAGE: bash
CODE:
# development model
npm run dev
yarn dev

----------------------------------------

TITLE: Installing Dependencies for Financial Report Analysis in Python
DESCRIPTION: Installs required Python packages for processing financial reports, including pdfplumber for PDF handling and fuzzywuzzy for string matching.

LANGUAGE: bash
CODE:
pip install pdfplumber
pip install fuzzywuzzy

----------------------------------------

TITLE: Executing Two Sum Example with Python and Poetry
DESCRIPTION: Shows the command to run the Two Sum example using Poetry. This command executes the Python script containing the DAG implementation.

LANGUAGE: bash
CODE:
poetry run python awel_tutorial/join_operator_sum_numbers.py

----------------------------------------

TITLE: Compiling and Deploying DB-GPT-Web for DB-GPT
DESCRIPTION: Steps to compile DB-GPT-Web and copy the compiled files to the DB-GPT static file directory.

LANGUAGE: bash
CODE:
npm run compile
yarn compile

# copy compile file to DB-GPT static file dictory
cp -rf out/* ../dbgpt/app/static

----------------------------------------

TITLE: Setting Up OceanBase Vector Database
DESCRIPTION: Steps to initialize and configure OceanBase vector database, including docker setup and Python client configuration.

LANGUAGE: shell
CODE:
docker run --name=ob433 -e MODE=slim -p 2881:2881 -d quay.io/oceanbase/oceanbase-ce:4.3.3.0-100000142024101215

LANGUAGE: shell
CODE:
pip install --upgrade --quiet pyobvector

----------------------------------------

TITLE: Updating POT File for DB-GPT
DESCRIPTION: Command to update the POT file with the latest translatable strings from the source code.

LANGUAGE: bash
CODE:
make pot

----------------------------------------

TITLE: Configuring Financial Chat Model Path in DB-GPT
DESCRIPTION: Sets the path for the financial report model in the DB-GPT configuration, pointing to the downloaded pre-trained model.

LANGUAGE: bash
CODE:
#*******************************************************************#
#**                     FINANCIAL CHAT Config                     **#
#*******************************************************************#
FIN_REPORT_MODEL=/app/DB-GPT/models/bge-large-zh-v1.5

----------------------------------------

TITLE: Creating a Custom BranchOperator in Python
DESCRIPTION: This example shows how to implement a custom BranchOperator by overriding the branches method. It includes a DAG setup with even and odd tasks.

LANGUAGE: python
CODE:
from dbgpt.core.awel import DAG, BranchOperator, MapOperator

def branch_even(x: int) -> bool:
    return x % 2 == 0

def branch_odd(x: int) -> bool:
    return not branch_even(x)

class MyBranchOperator(BranchOperator[int]):
    def __init__(self, even_task_name: str, odd_task_name: str, **kwargs):
        self.even_task_name = even_task_name
        self.odd_task_name = odd_task_name
        super().__init__(**kwargs)
        
    async def branches(self):
        return {
            branch_even: self.even_task_name,
            branch_odd: self.odd_task_name
        }

with DAG("awel_branch_operator") as dag:
    task = MyBranchOperator(even_task_name="even_task", odd_task_name="odd_task")
    even_task = MapOperator(
        task_name="even_task", 
        map_function=lambda x: print(f"{x} is even")
    )
    odd_task = MapOperator(
        task_name="odd_task", 
        map_function=lambda x: print(f"{x} is odd")
    )

----------------------------------------

TITLE: Parsing Expressions and Operators
DESCRIPTION: These rules handle parsing of complex expressions involving operators, functions, subqueries, etc.

LANGUAGE: Bison
CODE:
expr : expr AND expr
     | expr OR expr
     | NOT expr
     | expr comparison_operator expr
     | expr '+' expr
     | expr '-' expr 
     | expr '*' expr
     | expr '/' expr
     | '(' expr ')'
     | function_call
     | column_ref
     | literal
     | subquery

----------------------------------------

TITLE: Installing gettext on CentOS/RHEL
DESCRIPTION: Commands to install the gettext package on CentOS and RHEL, differentiating between older and newer versions using yum and dnf package managers.

LANGUAGE: bash
CODE:
# CentOS/RHEL 7 And Older
sudo yum install gettext
# CentOS/RHEL 8 And Newer
sudo dnf install gettext

----------------------------------------

TITLE: Initializing MySQL KBQA Schema
DESCRIPTION: Command to import the knowledge management database schema for MySQL KBQA system.

LANGUAGE: bash
CODE:
$ mysql -h127.0.0.1 -uroot -p{your_password} < ./assets/schema/knowledge_management.sql

----------------------------------------

TITLE: Setting up Python Environment
DESCRIPTION: Commands to create and activate a Python virtual environment using conda. Requires Python 3.10 or higher.

LANGUAGE: bash
CODE:
# Make sure python >= 3.10
conda create -n dbgpt_env python=3.10
conda activate dbgpt_env

----------------------------------------

TITLE: Initializing Test Data for DB-GPT Chat Dashboard using Python
DESCRIPTION: This Python command runs a script that initializes test data in the previously created 'dbgpt_test' database. The script is located in the docker/examples/dashboard directory and is used to populate the database with sample data for testing the Chat Dashboard functionality.

LANGUAGE: Python
CODE:
python docker/examples/dashboard/test_case_mysql_data.py

----------------------------------------

TITLE: Implementing Custom StreamifyAbsOperator for Number Stream in Python
DESCRIPTION: This snippet demonstrates how to create a custom StreamifyAbsOperator that generates a stream of numbers from 0 to n-1. It uses the AWEL framework from DB-GPT to define a DAG and a custom operator.

LANGUAGE: python
CODE:
from typing import AsyncIterator
from dbgpt.core.awel import DAG, StreamifyAbsOperator

class NumberProducerOperator(StreamifyAbsOperator[int, int]):
    """Create a stream of numbers from 0 to `n-1`"""
    async def streamify(self, n: int) -> AsyncIterator[int]:
        for i in range(n):
            yield i

with DAG("numbers_dag") as dag:
    task = NumberProducerOperator()

----------------------------------------

TITLE: SQL Grammar Production Rule - SELECT Expression
DESCRIPTION: Production rule for SELECT statement syntax including projections and clauses

LANGUAGE: BNF
CODE:
select_expr_list : projection | select_expr_list "," projection

----------------------------------------

TITLE: Creating Test Database in SQL for DB-GPT Chat Dashboard
DESCRIPTION: This SQL command creates a new database named 'dbgpt_test' with UTF-8 character set if it doesn't already exist. This database is used for testing the report analysis capabilities in DB-GPT.

LANGUAGE: SQL
CODE:
CREATE DATABASE IF NOT EXISTS dbgpt_test CHARACTER SET utf8;

----------------------------------------

TITLE: Creating and Running a Stream of Numbers using StreamifyAbsOperator in Python
DESCRIPTION: This snippet shows a complete example of creating a stream of numbers using a custom StreamifyAbsOperator. It includes the DAG setup, the operator implementation, and an asynchronous function to print the stream output.

LANGUAGE: python
CODE:
import asyncio
from typing import AsyncIterator
from dbgpt.core.awel import DAG, StreamifyAbsOperator

class NumberProducerOperator(StreamifyAbsOperator[int, int]):
    """Create a stream of numbers from 0 to `n-1`"""
    async def streamify(self, n: int) -> AsyncIterator[int]:
        for i in range(n):
            yield i

with DAG("numbers_dag") as dag:
    task = NumberProducerOperator()

async def print_stream(t, n: int):
    # Call the streaming operator by `call_stream` method
    async for i in await t.call_stream(call_data=n):
        print(i)

asyncio.run(print_stream(task, 10))

----------------------------------------

TITLE: Generating Translations for DB-GPT Modules
DESCRIPTION: Python command to automatically generate the latest translations for specified modules and languages.

LANGUAGE: bash
CODE:
python ./translate_util.py --lang zh_CN --modules app,core,model,rag,serve,storage,util

----------------------------------------

TITLE: GraphRAG Markdown Link
DESCRIPTION: Markdown link reference pointing to the GraphRAG application development documentation.

LANGUAGE: markdown
CODE:
[GraphRAG](../cookbook/rag/graph_rag_app_develop.md)

----------------------------------------

TITLE: Defining HTTP Request Body Model in Python
DESCRIPTION: Creates a Pydantic model 'TriggerReqBody' to define the structure of the HTTP request body, including 'name' and 'age' fields.

LANGUAGE: python
CODE:
class TriggerReqBody(BaseModel):
    name: str = Field(..., description="User name")
    age: int = Field(18, description="User age")

----------------------------------------

TITLE: Installing gettext on MacOS
DESCRIPTION: Command to install the gettext package on MacOS using Homebrew package manager.

LANGUAGE: bash
CODE:
brew install gettext

----------------------------------------

TITLE: DB-GPT Datasource Provider Configuration
DESCRIPTION: Table showing supported database providers and their corresponding installation packages. Includes popular databases like MySQL, PostgreSQL, ClickHouse, and others.

LANGUAGE: markdown
CODE:
| Provider    | Supported | Install Packages     |
|-------------|-----------|----------------------|
| MySQL       | ✅        | --extra datasource_mysql |
| OceanBase   | ✅        |  |
| ClickHouse  | ✅        | --extra datasource_clickhouse |
| Hive        | ✅        | --extra datasource_hive |
| MSSQL       | ✅        | --extra datasource_mssql |
| PostgreSQL  | ✅        | --extra datasource_postgres |
| ApacheDoris | ✅        |                      |
| StarRocks   | ✅        | --extra datasource_starroks |
| Spark       | ✅        | --extra datasource_spark |
| Oracle      | ❌        |                      |

----------------------------------------

TITLE: Installing DB-GPT Agent Dependencies
DESCRIPTION: Commands to install the required Python packages for DB-GPT agent functionality and OpenAI integration.

LANGUAGE: bash
CODE:
pip install "dbgpt[agent]>=0.5.6rc1" -U
pip install openai

----------------------------------------

TITLE: Testing AWEL DAG with curl Command
DESCRIPTION: Demonstrates how to test the AWEL DAG using a curl command to send an HTTP GET request to the defined endpoint.

LANGUAGE: bash
CODE:
% curl -X GET http://127.0.0.1:5670/api/v1/awel/trigger/examples/hello\?name\=zhangsan
"Hello, zhangsan, your age is 18"

----------------------------------------

TITLE: Installing gettext on Arch Linux
DESCRIPTION: Command to install the gettext package on Arch Linux using pacman package manager.

LANGUAGE: bash
CODE:
sudo pacman -Sy gettext

----------------------------------------

TITLE: Sending API Request to Ollama in Python
DESCRIPTION: Constructs and sends an API request to the Ollama service, including the prompt and model parameters. It uses the requests library to make a POST request.

LANGUAGE: python
CODE:
import requests

data = {
    "model": model_name,
    "prompt": prompt,
    **model_params
}

response = requests.post(f"{api_base}/api/generate", json=data)

----------------------------------------

TITLE: Implementing Two Sum DAG with JoinOperator in Python
DESCRIPTION: Creates a DAG that sums two numbers using JoinOperator. It uses InputOperator to receive input, MapOperators to extract values, and a JoinOperator to sum them.

LANGUAGE: python
CODE:
import asyncio
from dbgpt.core.awel import (
    DAG, JoinOperator, MapOperator, InputOperator, SimpleCallDataInputSource
)

with DAG("sum_numbers_dag") as dag:
    # Create a input task to receive data from call_data
    input_task = InputOperator(input_source=SimpleCallDataInputSource())
    task1 = MapOperator(map_function=lambda x: x["t1"])
    task2 = MapOperator(map_function=lambda x: x["t2"])
    sum_task = JoinOperator(combine_function=lambda x, y: x + y)
    input_task >> task1 >> sum_task
    input_task >> task2 >> sum_task

if asyncio.run(sum_task.call(call_data={"t1": 5, "t2": 8})) == 13:
    print("Success!")
else:
    print("Failed")

----------------------------------------

TITLE: Downloading Financial Report Dataset from ModelScope
DESCRIPTION: Clones a financial report dataset from ModelScope, which can be used for testing and training the financial analysis system.

LANGUAGE: bash
CODE:
git clone http://www.modelscope.cn/datasets/modelscope/chatglm_llm_fintech_raw_dataset.git

----------------------------------------

TITLE: Installing Dependencies with Pip
DESCRIPTION: Commands for installing various optional dependencies of DB-GPT framework based on specific needs like core functionality, model inference, quantization, and knowledge base features.

LANGUAGE: bash
CODE:
pip install -e .
pip install -e ".[core]"
pip install -e ".[framework]"
pip install -e ".[openai]"
pip install -e ".[default]"
pip install -e ".[vllm]"
pip install -e ".[quantization]"
pip install -e ".[knowledge]"
pip install -e ".[torch]"
pip install -e ".[llama_cpp]"
pip install -e ".[vstore]"
pip install -e ".[datasource]"

----------------------------------------

TITLE: Listing Model Instances
DESCRIPTION: Command to list all registered model instances in the DB-GPT cluster.

LANGUAGE: bash
CODE:
CONTROLLER_ADDRESS="http://server1:8000,http://server2:8000" dbgpt model list

----------------------------------------

TITLE: Executing the Number Summation Example - Bash
DESCRIPTION: Command to run the number summation example script using Poetry package manager.

LANGUAGE: bash
CODE:
poetry run python awel_tutorial/reduce_operator_sum_numbers.py

----------------------------------------

TITLE: Altering GPTS Conversations Table in SQL
DESCRIPTION: SQL commands to alter the gpts_conversations table, adding columns for team_mode and current_goal to support new features in DB-GPT 0.5.0.

LANGUAGE: SQL
CODE:
ALTER TABLE `gpts_conversations`
ADD COLUMN `team_mode` varchar(255) NULL COMMENT 'agent team work mode';

ALTER TABLE `gpts_conversations`
ADD COLUMN  `current_goal` text COMMENT 'The target corresponding to the current message';

----------------------------------------

TITLE: Installing DB-GPT MSSQL Dependencies
DESCRIPTION: Command to synchronize and install required packages including MSSQL datasource, RAG, ChromaDB storage, and other base dependencies using the UV package manager.

LANGUAGE: bash
CODE:
uv sync --all-packages \
--extra "base" \
--extra "datasource_mssql" \
--extra "rag" \
--extra "storage_chromadb" \
--extra "dbgpts"

----------------------------------------

TITLE: Defining DB-GPT Terminology Table in Markdown
DESCRIPTION: A markdown table defining key terminology used throughout the DB-GPT framework, including descriptions of core concepts like Text2SQL, KBQA, GBI, LLMOps, and AWEL.

LANGUAGE: markdown
CODE:
| terminology          | Description                                                   |
|----------------------|---------------------------------------------------------------|
| <center> `DB-GPT`       </center>| DataBase Generative Pre-trained Transformer, an open source framework around databases and large language models |
|<center> `Data App` </center> | an intelligent Data application built on DB-GPT. |
| <center> `Text2SQL/NL2SQL`  </center>  | Text to SQL uses large language model capabilities to generate SQL statements based on natural language, or provide explanations based on SQL statements |
| <center>`KBQA`   </center>  | Knowledge-Based Q&A system |
| <center>`GBI`            </center>  | Generative Business Intelligence, based on large language models and data analysis, provides business intelligence analysis and decision-making through dialogue |
| <center>`LLMOps`   </center>  | A large language model operation framework that provides a standard end-to-end workflow for training, tuning, deploying, and monitoring LLM to accelerate application deployment of generated AI models |
|<center> `Embedding`  </center>   | Methods to convert text, audio, video and other materials into vectors |
|<center> `RAG`   </center>| Retrieval Augmented Generation |
|<center> `AWEL` </center> | Agentic Workflow Expression Language, intelligent Workflow Expression Language |
|<center> `AWEL Flow` </center> | workflow orchestration using the intelligent workflow Expression Language |
|<center> `SMMF` </center> | a service-oriented multi-model management framework.

----------------------------------------

TITLE: Installing DB-GPT Milvus Dependencies
DESCRIPTION: Command to install required packages for DB-GPT Milvus storage integration using the uv package manager.

LANGUAGE: bash
CODE:
uv sync --all-packages \
--extra "base" \
--extra "proxy_openai" \
--extra "rag" \
--extra "storage_milvus" \
--extra "dbgpts"

----------------------------------------

TITLE: Listing Remote AWEL Workflows in DB-GPT
DESCRIPTION: This command lists all available AWEL sample processes in the remote repository, including the 'awel-flow-web-info-search' workflow for internet search capabilities.

LANGUAGE: bash
CODE:
dbgpt app list-remote

----------------------------------------

TITLE: Launching TuGraph Database Container
DESCRIPTION: Docker commands to pull and run the TuGraph database container with necessary port configurations for graph storage

LANGUAGE: bash
CODE:
docker pull tugraph/tugraph-runtime-centos7:4.5.1
docker run -d -p 7070:7070  -p 7687:7687 -p 9090:9090 --name tugraph_demo tugraph/tugraph-runtime-centos7:latest lgraph_server -d run --enable_plugin true

----------------------------------------

TITLE: Installing Docusaurus Dependencies for DB-GPT Documentation
DESCRIPTION: Installs the necessary dependencies for the DB-GPT documentation using Yarn. This command should be run after cloning the project repository.

LANGUAGE: bash
CODE:
yarn install

----------------------------------------

TITLE: Configuring Milvus Storage Settings
DESCRIPTION: TOML configuration for setting up Milvus connection parameters in DB-GPT, including host URI and port settings.

LANGUAGE: toml
CODE:
[rag.storage]
[rag.storage.vector]
type = "Milvus"
uri = "127.0.0.1"
port = "19530"
#username="dbgpt"
#password=19530

----------------------------------------

TITLE: Database Migration Management Commands
DESCRIPTION: Series of commands for managing and migrating databases using DB-GPT's migration tools, including initialization, upgrades, and schema changes.

LANGUAGE: commandline
CODE:
dbgpt db migration --help

LANGUAGE: commandline
CODE:
dbgpt db migration init

LANGUAGE: commandline
CODE:
dbgpt db migration upgrade

LANGUAGE: commandline
CODE:
dbgpt db migration migrate -m "your message"

----------------------------------------

TITLE: Starting DB-GPT Server with Milvus Configuration
DESCRIPTION: Commands to start the DB-GPT web server using the configured Milvus settings.

LANGUAGE: bash
CODE:
uv run python packages/dbgpt-app/src/dbgpt_app/dbgpt_server.py --config configs/dbgpt-proxy-openai.toml

----------------------------------------

TITLE: Updating PO Files for DB-GPT
DESCRIPTION: Command to update PO files for all languages with new or changed strings.

LANGUAGE: bash
CODE:
make po

----------------------------------------

TITLE: Installing DB-GPT ClickHouse Datasource Dependencies
DESCRIPTION: This command installs the necessary dependencies for using ClickHouse as a Datasource in DB-GPT, including the base package, ClickHouse datasource, RAG, ChromaDB storage, and dbgpts extras.

LANGUAGE: bash
CODE:
uv sync --all-packages \
--extra "base" \
--extra "datasource_clickhouse" \
--extra "rag" \
--extra "storage_chromadb" \
--extra "dbgpts"

----------------------------------------

TITLE: Adding Column to Knowledge Space Table
DESCRIPTION: MySQL commands to add a context column to the knowledge_space table for fixing the OperationalError.

LANGUAGE: commandline
CODE:
mysql> use knowledge_management;
mysql> ALTER TABLE knowledge_space ADD COLUMN context TEXT COMMENT "arguments context";

----------------------------------------

TITLE: Building Default CUDA Docker Image
DESCRIPTION: Creates a CUDA-based Docker image with standard DB-GPT features including proxy integrations, RAG capabilities, and quantization support.

LANGUAGE: bash
CODE:
bash docker/base/build_image.sh

----------------------------------------

TITLE: Compiling MO Files for DB-GPT
DESCRIPTION: Command to compile PO files into MO files after translation.

LANGUAGE: bash
CODE:
make mo

----------------------------------------

TITLE: Installing DB-GPT Dependencies with UV
DESCRIPTION: Command to install required DB-GPT packages including graph_rag and related dependencies using the UV package manager

LANGUAGE: bash
CODE:
uv sync --all-packages \
--extra "base" \
--extra "proxy_openai" \
--extra "rag" \
--extra "storage_chromadb" \
--extra "dbgpts" \
--extra "graph_rag"

----------------------------------------

TITLE: Downloading Qwen2.5-0.5B-Instruct Model for LLama.cpp Server
DESCRIPTION: Command to download the qwen2.5-0.5b-instruct model from Huggingface for use with LLama.cpp server in DB-GPT.

LANGUAGE: bash
CODE:
wget https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q4_k_m.gguf?download=true -O /tmp/qwen2.5-0.5b-instruct-q4_k_m.gguf

----------------------------------------

TITLE: Configuring Multi-GPU Usage
DESCRIPTION: Commands to specify GPU devices for DB-GPT server deployment, allowing selection of specific GPU IDs.

LANGUAGE: shell
CODE:
# Specify 1 gpu
CUDA_VISIBLE_DEVICES=0 python3 dbgpt/app/dbgpt_server.py

# Specify 4 gpus
CUDA_VISIBLE_DEVICES=3,4,5,6 python3 dbgpt/app/dbgpt_server.py

----------------------------------------

TITLE: Submitting Translation Changes to Git
DESCRIPTION: Git commands to add and commit changes to translation files in the locales directory.

LANGUAGE: bash
CODE:
git add locales/
git commit -m "Update translations"

----------------------------------------

TITLE: Configuring DeepSeek Model Parameters in YAML
DESCRIPTION: Defines the configuration for the DeepSeek language model, specifying model paths, quantization settings, and other parameters such as temperature and top_p.

LANGUAGE: yaml
CODE:
# DeepSeek
model_name: deepseek-ai/deepseek-coder-33b-instruct
model_path: "/data/models/deepseek-coder-33b-instruct-hf"
model_format: "hf"
quantize: 4
temperature: 0.0
top_p: 0.7
top_k: 50
max_new_tokens: 1024
num_beams: 1
use_fast_tokenizer: true
left_padding: false
prompt_template: deepseek_coder
do_sample: false
repetition_penalty: 1

----------------------------------------

TITLE: Starting DB-GPT Server with LLama.cpp Integration
DESCRIPTION: Command to start the DB-GPT server after configuring it to use LLama.cpp server for inference.

LANGUAGE: bash
CODE:
python dbgpt/app/dbgpt_server.py

----------------------------------------

TITLE: Database Migration to SQLite
DESCRIPTION: Command to migrate meta tables (chat_history and connect_config) from DuckDB to SQLite.

LANGUAGE: commandline
CODE:
python docker/examples/metadata/duckdb2sqlite.py

----------------------------------------

TITLE: Defining SQL Statement Production Rules
DESCRIPTION: This snippet shows some of the top-level production rules for parsing SQL statements like SELECT, INSERT, UPDATE, etc.

LANGUAGE: Bison
CODE:
stmt : select_stmt
     | insert_stmt 
     | update_stmt
     | delete_stmt
     | create_table_stmt
     | alter_table_stmt
     | drop_table_stmt
     | create_index_stmt
     | drop_index_stmt
     | create_view_stmt
     | drop_view_stmt
     | grant_stmt
     | revoke_stmt

----------------------------------------

TITLE: Starting DB-GPT Web Server
DESCRIPTION: Commands to start the DB-GPT web server using a proxy configuration for OpenAI. The server can be started using the dbgpt_server.py script with specific configuration files.

LANGUAGE: bash
CODE:
uv run python packages/dbgpt-app/src/dbgpt_app/dbgpt_server.py --config configs/dbgpt-proxy-openai.toml

----------------------------------------

TITLE: Rerank API Python Implementation
DESCRIPTION: Python code example for using the rerank API with the OpenAPIRerankEmbeddings class

LANGUAGE: python
CODE:
from dbgpt.rag.embedding import OpenAPIRerankEmbeddings\\n\\nrerank = OpenAPIRerankEmbeddings(api_key="EMPTY", model_name="bge-reranker-base")\\nrerank.predict(\\n    query="what is awel talk about?", \\n    candidates=[\\n        "Agentic Workflow Expression Language(AWEL) is a set of intelligent agent workflow expression language specially designed for large model application development.",\\n        "Autonomous agents have long been a research focus in academic and industry communities",\\n        "AWEL is divided into three levels in deign, namely the operator layer, AgentFream layer and DSL layer.",\\n        "Elon musk is a famous entrepreneur and inventor, he is the founder of SpaceX and Tesla."\\n    ]\\n)

----------------------------------------

TITLE: Setting up Public Network Access with Gradio
DESCRIPTION: Creates a public URL for accessing the website using Gradio's networking functionality. Generates a secure token and sets up a tunnel on a specified local port.

LANGUAGE: python
CODE:
import secrets
from gradio import networking
token=secrets.token_urlsafe(32)
local_port=5670
url = networking.setup_tunnel('0.0.0.0', local_port, token)
print(f'Public url: {url}')
time.sleep(60 * 60 * 24)

----------------------------------------

TITLE: Parsing SELECT Statement Components
DESCRIPTION: These rules define the structure of a SELECT statement, including optional clauses like WHERE, GROUP BY, HAVING, etc.

LANGUAGE: Bison
CODE:
select_stmt : SELECT opt_distinct select_expr_list
             FROM table_references
             opt_where 
             opt_groupby
             opt_having
             opt_orderby
             opt_limit

opt_where : /* empty */
           | WHERE expr

opt_groupby : /* empty */
             | GROUP BY groupby_list

opt_having : /* empty */
            | HAVING expr

----------------------------------------

TITLE: Starting DB-GPT Server with OpenAI Proxy Configuration
DESCRIPTION: This command starts the DB-GPT server using a specific configuration file for OpenAI proxy settings. It utilizes the uv run command to execute the Python script.

LANGUAGE: bash
CODE:
uv run python packages/dbgpt-app/src/dbgpt_app/dbgpt_server.py --config configs/dbgpt-proxy-openai.toml

----------------------------------------

TITLE: Building VLLM-enabled Docker Image
DESCRIPTION: Creates a CUDA-based Docker image with VLLM support for optimized inference along with default features.

LANGUAGE: bash
CODE:
bash docker/base/build_image.sh --install-mode vllm

----------------------------------------

TITLE: Building VLLM-enabled Docker Image
DESCRIPTION: Creates a CUDA-based Docker image with VLLM support for optimized inference along with default features.

LANGUAGE: bash
CODE:
bash docker/base/build_image.sh --install-mode vllm

----------------------------------------

TITLE: Configuring Logging Parameters with React Component
DESCRIPTION: React component configuration for LoggingParameters that specifies logging file location and log level settings. Supports multiple log levels (FATAL, ERROR, WARNING, INFO, DEBUG, NOTSET) with INFO as the default value.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "LoggingParameters",
  "description": "Logging parameters.",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "file",
      "type": "string",
      "required": false,
      "description": "The filename to store logs"
    },
    {
      "name": "level",
      "type": "string",
      "required": false,
      "description": "Logging level, just support FATAL, ERROR, WARNING, INFO, DEBUG, NOTSET",
      "defaultValue": "${env:DBGPT_LOG_LEVEL:-INFO}",
      "validValues": [
        "FATAL",
        "ERROR",
        "WARNING",
        "WARNING",
        "INFO",
        "DEBUG",
        "NOTSET"
      ]
    }
  ]
}} />

----------------------------------------

TITLE: SQL Grammar Production Rule - WITH Clause
DESCRIPTION: Production rule defining the WITH clause syntax for CTEs (Common Table Expressions)

LANGUAGE: BNF
CODE:
with_clause : WITH RECURSIVE common_table_expr | WITH RECURSIVE with_list | WITH with_list

----------------------------------------

TITLE: Installing DB-GPT Hive Datasource Dependencies
DESCRIPTION: This command installs the necessary dependencies for using Hive as a datasource in DB-GPT, including base packages, Hive datasource, RAG, ChromaDB storage, and DBGPTs.

LANGUAGE: bash
CODE:
uv sync --all-packages \
--extra "base" \
--extra "datasource_hive" \
--extra "rag" \
--extra "storage_chromadb" \
--extra "dbgpts"

----------------------------------------

TITLE: Starting DB-GPT Server with Elasticsearch Configuration
DESCRIPTION: Command to launch the DB-GPT server using the Elasticsearch configuration file.

LANGUAGE: bash
CODE:
uv run python packages/dbgpt-app/src/dbgpt_app/dbgpt_server.py --config configs/dbgpt-bm25-rag.toml

----------------------------------------

TITLE: Configuring OpenAPI Reranker Model Parameters in React/MDX
DESCRIPTION: React/MDX component configuration that defines the schema for OpenAPI reranker model deployment parameters, including model name, provider settings, API configuration, and runtime parameters.

LANGUAGE: javascript
CODE:
<ConfigDetail config={{
  "name": "OpenAPIRerankerDeployModelParameters",
  "description": "OpenAPI Reranker Deploy Model Parameters.",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "proxy/openapi"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "50"
    },
    {
      "name": "api_url",
      "type": "string",
      "required": false,
      "description": "The URL of the rerank API.",
      "defaultValue": "http://localhost:8100/v1/beta/relevance"
    },
    {
      "name": "api_key",
      "type": "string",
      "required": false,
      "description": "The API key for the rerank API."
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name."
    },
    {
      "name": "timeout",
      "type": "integer",
      "required": false,
      "description": "The timeout for the request in seconds.",
      "defaultValue": "60"
    }
  ]
}} />

----------------------------------------

TITLE: SQL Grammar Production Rule - Function Declaration
DESCRIPTION: Production rule for function definition syntax including parameters and return type

LANGUAGE: BNF
CODE:
func_decl : FUNCTION func_name opt_sp_param_list RETURN pl_outer_data_type opt_sf_clause opt_pipelined

----------------------------------------

TITLE: Building Enterprise DB-GPT Docker Image
DESCRIPTION: Creates a full-featured enterprise version with PostgreSQL and Elasticsearch support, custom Python version, and no example data.

LANGUAGE: bash
CODE:
bash docker/base/build_image.sh --install-mode full \
  --add-extras "storage_elasticsearch,datasource_postgres" \
  --image-name-suffix enterprise \
  --python-version 3.10 \
  --load-examples false

----------------------------------------

TITLE: Starting DB-GPT Webserver with ClickHouse Configuration
DESCRIPTION: This command starts the DB-GPT webserver using a configuration file that includes ClickHouse settings. It specifies the Python script to run and the configuration file to use.

LANGUAGE: bash
CODE:
uv run python packages/dbgpt-app/src/dbgpt_app/dbgpt_server.py --config configs/dbgpt-proxy-openai.toml

----------------------------------------

TITLE: Initializing ModelControllerParameters in Python
DESCRIPTION: This code snippet defines the ModelControllerParameters class with various configuration options. It includes parameters for network settings, logging, tracing, model registry, and heartbeat management.

LANGUAGE: python
CODE:
ModelControllerParameters(host: Optional[str] = '0.0.0.0', port: Optional[int] = 8000, daemon: Optional[bool] = False, log: dbgpt.util.utils.LoggingParameters = <factory>, trace: Optional[dbgpt.util.tracer.tracer_impl.TracerParameters] = None, registry: Optional[dbgpt.model.parameter.BaseModelRegistryParameters] = None, heartbeat_interval_secs: Optional[int] = 20, heartbeat_timeout_secs: Optional[int] = 60)

----------------------------------------

TITLE: SQL Grammar Production Rules
DESCRIPTION: Defines comprehensive SQL grammar production rules and patterns for parsing SQL syntax including tokens, expressions, statements, and clauses.

LANGUAGE: Grammar
CODE:
/* Select statement production */

select_stmt : select_into
          | select_no_parens opt_when
          | select_with_parens
          | subquery fetch_next for_update 
          | subquery for_update
          | subquery for_update order_by
          | subquery opt_fetch_next
          | subquery order_by opt_fetch_next
          | subquery order_by opt_fetch_next for_update
          | with_select

----------------------------------------

TITLE: TuGraph Configuration Settings
DESCRIPTION: TOML configuration settings for connecting DB-GPT to TuGraph database, including host, port, credentials, and feature flags

LANGUAGE: toml
CODE:
[rag.storage.graph]
type = "TuGraph"
host="127.0.0.1"
port=7687
username="admin"
password="73@TuGraph"
enable_summary="True"
enable_similarity_search="True"

----------------------------------------

TITLE: Rendering Ollama Proxy LLM Configuration in React
DESCRIPTION: This code snippet uses a custom React component called ConfigDetail to render the configuration parameters for the Ollama proxy LLM. It includes details such as model name, backend, provider, and various API-related settings.

LANGUAGE: javascript
CODE:
<ConfigDetail config={{
  "name": "OllamaDeployModelParameters",
  "description": "Ollama proxy LLM configuration.",
  "documentationUrl": "https://ollama.com/library",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name."
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "proxy/ollama"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "100"
    },
    {
      "name": "prompt_template",
      "type": "string",
      "required": false,
      "description": "Prompt template. If None, the prompt template is automatically determined from model. Just for local deployment."
    },
    {
      "name": "context_length",
      "type": "integer",
      "required": false,
      "description": "The context length of the OpenAI API. If None, it is determined by the model."
    },
    {
      "name": "api_base",
      "type": "string",
      "required": false,
      "description": "The base url of the Ollama API.",
      "defaultValue": "${env:OLLAMA_API_BASE:-http://localhost:11434}"
    },
    {
      "name": "api_key",
      "type": "string",
      "required": false,
      "description": "The API key of the OpenAI API.",
      "defaultValue": "${env:OPENAI_API_KEY}"
    },
    {
      "name": "api_type",
      "type": "string",
      "required": false,
      "description": "The type of the OpenAI API, if you use Azure, it can be: azure"
    },
    {
      "name": "api_version",
      "type": "string",
      "required": false,
      "description": "The version of the OpenAI API."
    },
    {
      "name": "http_proxy",
      "type": "string",
      "required": false,
      "description": "The http or https proxy to use openai"
    }
  ]
}} />

----------------------------------------

TITLE: Specifying Python Development Dependencies
DESCRIPTION: This snippet lists the required Python packages for code formatting, linting, and type checking in the db-gpt project. It includes tools like Black for formatting, Flake8 for linting, and various type stubs for improved type checking.

LANGUAGE: Text
CODE:
# python code format, usage `black .`
black==22.8.0
blackdoc==0.3.7
flake8==5.0.4
flake8-bugbear==22.10.25
flake8-comprehensions==3.10.0
flake8-docstrings==1.6.0
flake8-simplify==0.19.3
flake8-tidy-imports==4.8.0
isort==5.10.1
pyupgrade==3.1.0
types-requests
types-beautifulsoup4
types-Markdown
types-tqdm
pandas-stubs
types-psutil

----------------------------------------

TITLE: Viewing DB-GPT Webserver Logs
DESCRIPTION: Command to view the logs of the DB-GPT webserver container in follow mode, which streams new log entries as they are generated.

LANGUAGE: bash
CODE:
docker logs db-gpt-webserver-1 -f

----------------------------------------

TITLE: Starting DB-GPT Webserver
DESCRIPTION: Command to start the DB-GPT webserver with the Graph RAG configuration file

LANGUAGE: bash
CODE:
uv run python packages/dbgpt-app/src/dbgpt_app/dbgpt_server.py --config configs/dbgpt-graphrag.toml

----------------------------------------

TITLE: Rendering HFLLMDeployModelParameters Configuration in JSX
DESCRIPTION: This code snippet uses a custom React component called ConfigDetail to render the configuration details for HFLLMDeployModelParameters. It includes a comprehensive list of parameters with their types, descriptions, and default values where applicable.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "HFLLMDeployModelParameters",
  "description": "Local deploy model parameters.",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "path",
      "type": "string",
      "required": false,
      "description": "The path of the model, if you want to deploy a local model."
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name."
    },
    {
      "name": "device",
      "type": "string",
      "required": false,
      "description": "Device to run model. If None, the device is automatically determined"
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "hf"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "5"
    },
    {
      "name": "prompt_template",
      "type": "string",
      "required": false,
      "description": "Prompt template. If None, the prompt template is automatically determined from model. Just for local deployment."
    },
    {
      "name": "context_length",
      "type": "integer",
      "required": false,
      "description": "The context length of the model. If None, it is automatically determined from model."
    },
    {
      "name": "trust_remote_code",
      "type": "boolean",
      "required": false,
      "description": "Trust remote code or not.",
      "defaultValue": "True"
    },
    {
      "name": "quantization",
      "type": "BaseHFQuantization",
      "required": false,
      "description": "The quantization parameters.",
      "nestedTypes": [
        {
          "type": "link",
          "text": "bitsandbytes configuration",
          "url": "././parameter_bitsandbytesquantization_d40e3b"
        },
        {
          "type": "link",
          "text": "bitsandbytes_8bits configuration",
          "url": "././parameter_bitsandbytesquantization8bits_909aed"
        },
        {
          "type": "link",
          "text": "bitsandbytes_4bits configuration",
          "url": "././parameter_bitsandbytesquantization4bits_52b778"
        }
      ]
    },
    {
      "name": "low_cpu_mem_usage",
      "type": "boolean",
      "required": false,
      "description": "Whether to use low CPU memory usage mode. It can reduce the memory when loading the model, if you load your model with quantization, it will be True by default. You must install `accelerate` to make it work."
    },
    {
      "name": "num_gpus",
      "type": "integer",
      "required": false,
      "description": "The number of gpus you expect to use, if it is empty, use all of them as much as possible"
    },
    {
      "name": "max_gpu_memory",
      "type": "string",
      "required": false,
      "description": "The maximum memory limit of each GPU, only valid in multi-GPU configuration, eg: 10GiB, 24GiB"
    },
    {
      "name": "torch_dtype",
      "type": "string",
      "required": false,
      "description": "The dtype of the model, default is None.",
      "validValues": [
        "auto",
        "float16",
        "bfloat16",
        "float",
        "float32"
      ]
    }
  ]
}} />

----------------------------------------

TITLE: Installing Dependencies for DB-GPT-Web
DESCRIPTION: Commands to install the necessary dependencies for DB-GPT-Web using npm or yarn package managers.

LANGUAGE: bash
CODE:
# Install dependencies
npm install
yarn install

----------------------------------------

TITLE: Configuring SQLite Database for DB-GPT
DESCRIPTION: TOML configuration snippet for setting up SQLite as the database for DB-GPT application.

LANGUAGE: toml
CODE:
[service.web.database]
type = "sqlite"
path = "pilot/meta_data/dbgpt.db"

----------------------------------------

TITLE: Loading Test Data in Windows
DESCRIPTION: Command to load built-in test data into the local database for DB-GPT in Windows environments.

LANGUAGE: bash
CODE:
.\scripts\examples\load_examples.bat

----------------------------------------

TITLE: Configuring Gitee LLM Parameters Using React Component
DESCRIPTION: React component configuration for Gitee proxy LLM parameters, defining model settings, API connections, and deployment options. The configuration includes essential parameters like model name, backend settings, API credentials, and connection properties.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "GiteeDeployModelParameters",
  "description": "Gitee proxy LLM configuration.",
  "documentationUrl": "https://ai.gitee.com/docs/getting-started/intro",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name."
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "proxy/gitee"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "100"
    },
    {
      "name": "prompt_template",
      "type": "string",
      "required": false,
      "description": "Prompt template. If None, the prompt template is automatically determined from model. Just for local deployment."
    },
    {
      "name": "context_length",
      "type": "integer",
      "required": false,
      "description": "The context length of the OpenAI API. If None, it is determined by the model."
    },
    {
      "name": "api_base",
      "type": "string",
      "required": false,
      "description": "The base url of the Gitee API.",
      "defaultValue": "${env:GITEE_API_BASE:-https://ai.gitee.com/v1}"
    },
    {
      "name": "api_key",
      "type": "string",
      "required": false,
      "description": "The API key of the Gitee API.",
      "defaultValue": "${env:GITEE_API_KEY}"
    },
    {
      "name": "api_type",
      "type": "string",
      "required": false,
      "description": "The type of the OpenAI API, if you use Azure, it can be: azure"
    },
    {
      "name": "api_version",
      "type": "string",
      "required": false,
      "description": "The version of the OpenAI API."
    },
    {
      "name": "http_proxy",
      "type": "string",
      "required": false,
      "description": "The http or https proxy to use openai"
    }
  ]
}} />

----------------------------------------

TITLE: Configuring Environment Variables for DB-GPT-Web
DESCRIPTION: Steps to set up the environment variables by copying the template file and editing the API base URL.

LANGUAGE: bash
CODE:
cp .env.template .env

----------------------------------------

TITLE: Running DB-GPT Webserver with OpenAI Proxy
DESCRIPTION: Commands to start the DB-GPT webserver using the OpenAI proxy configuration.

LANGUAGE: bash
CODE:
uv run dbgpt start webserver --config configs/dbgpt-proxy-openai.toml

LANGUAGE: bash
CODE:
uv run python packages/dbgpt-app/src/dbgpt_app/dbgpt_server.py --config configs/dbgpt-proxy-openai.toml

----------------------------------------

TITLE: Adding New Columns to Existing Tables for DB-GPT v0.5.0 Upgrade in MySQL
DESCRIPTION: SQL statements to add new columns to the gpts_conversations table for the DB-GPT v0.5.0 upgrade. This includes adding team_mode and current_goal columns.

LANGUAGE: sql
CODE:
ALTER TABLE `gpts_conversations`
ADD COLUMN `team_mode` varchar(255) NULL COMMENT 'agent team work mode';

ALTER TABLE `gpts_conversations`
ADD COLUMN  `current_goal` text COMMENT 'The target corresponding to the current message';

----------------------------------------

TITLE: Configuring StorageVectorConfig Component in MDX
DESCRIPTION: MDX component configuration for StorageVectorConfig that specifies the vector storage type parameter. The component uses ConfigDetail to render the configuration documentation with default vector type set to 'Chroma'.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "StorageVectorConfig",
  "description": "StorageVectorConfig(type: str = 'Chroma')",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "type",
      "type": "string",
      "required": false,
      "description": "default vector type",
      "defaultValue": "Chroma"
    }
  ]
}} />

----------------------------------------

TITLE: Downloading Text2Vec Embedding Model for DB-GPT
DESCRIPTION: This code snippet demonstrates how to clone the text2vec-large-chinese embedding model from Hugging Face for use in DB-GPT.

LANGUAGE: python
CODE:
cd DB-GPT
mkdir models and cd models
git clone https://huggingface.co/GanymedeNil/text2vec-large-chinese

----------------------------------------

TITLE: DB-GPT v0.6.0 MySQL Upgrade Script
DESCRIPTION: Comprehensive SQL script for upgrading DB-GPT database schema from v0.5.10 to v0.6.0. Includes table alterations to add new columns and creation of new tables for enhanced functionality.

LANGUAGE: sql
CODE:
USE dbgpt;
-- chat_history
ALTER TABLE  chat_history ADD COLUMN `app_code` varchar(255) DEFAULT NULL COMMENT 'App unique code' after `message_ids`;

-- gpts_app
ALTER TABLE  gpts_app ADD COLUMN `published` varchar(64) DEFAULT 'false' COMMENT 'Has it been published?';
ALTER TABLE  gpts_app ADD COLUMN `param_need` text DEFAULT NULL COMMENT 'Parameter information supported by the application';
ALTER TABLE  gpts_app ADD COLUMN `admins` text DEFAULT NULL COMMENT 'administrator';

-- connect_config
ALTER TABLE  connect_config ADD COLUMN `user_name` varchar(255) DEFAULT NULL COMMENT 'user name';
ALTER TABLE  connect_config ADD COLUMN `user_id` varchar(255) DEFAULT NULL COMMENT 'user id';

-- document_chunk
ALTER TABLE  document_chunk ADD COLUMN `questions` text DEFAULT NULL COMMENT 'chunk related questions';

-- knowledge_document
ALTER TABLE  knowledge_document ADD COLUMN `doc_token` varchar(100) DEFAULT NULL COMMENT 'doc token';
ALTER TABLE  knowledge_document ADD COLUMN `questions` text DEFAULT NULL COMMENT 'document related questions';

-- gpts_messages
ALTER TABLE  gpts_messages ADD COLUMN `is_success` int(4)  NULL DEFAULT 0 COMMENT 'agent message is success';
ALTER TABLE  gpts_messages ADD COLUMN `app_code` varchar(255) NOT NULL COMMENT 'Current AI assistant code';
ALTER TABLE  gpts_messages ADD COLUMN `app_name` varchar(255) NOT NULL COMMENT 'Current AI assistant name';
ALTER TABLE  gpts_messages ADD COLUMN `resource_info` text DEFAULT NULL  COMMENT 'Current conversation resource info';

-- prompt_manage
ALTER TABLE  prompt_manage ADD COLUMN `prompt_code` varchar(255) NULL COMMENT 'Prompt code';
ALTER TABLE  prompt_manage ADD COLUMN `response_schema` text  NULL COMMENT 'Prompt response schema';
ALTER TABLE  prompt_manage ADD COLUMN `user_code` varchar(128)  NULL COMMENT 'User code';

-- chat_feed_back
ALTER TABLE  chat_feed_back ADD COLUMN `message_id` varchar(255)  NULL COMMENT 'Message id';
ALTER TABLE  chat_feed_back ADD COLUMN `feedback_type` varchar(50)  NULL COMMENT 'Feedback type like or unlike';
ALTER TABLE  chat_feed_back ADD COLUMN `reason_types` varchar(255)  NULL COMMENT 'Feedback reason categories';
ALTER TABLE  chat_feed_back ADD COLUMN `user_code` varchar(128)  NULL COMMENT 'User code';
ALTER TABLE  chat_feed_back ADD COLUMN `remark` text NULL COMMENT 'Feedback remark';

-- dbgpt_serve_flow
ALTER TABLE dbgpt_serve_flow ADD COLUMN `variables` text DEFAULT NULL COMMENT 'Flow variables, JSON format';

-- dbgpt.recommend_question definition
CREATE TABLE `recommend_question` (
  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT 'autoincrement id',
  `gmt_create` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'create time',
  `gmt_modified` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'last update time',
  `app_code` varchar(255) DEFAULT NULL COMMENT 'Current AI assistant code',
  `question` text DEFAULT NULL COMMENT 'question',
  `user_code` int(11) DEFAULT NULL COMMENT 'user code',
  `sys_code` varchar(255) DEFAULT NULL COMMENT 'system app code',
  `valid` varchar(10) DEFAULT 'true' COMMENT 'is it effective，true/false',
  `chat_mode` varchar(255) DEFAULT NULL COMMENT 'Conversation scene mode，chat_knowledge...',
  `params` text DEFAULT NULL COMMENT 'question param',
  `is_hot_question` varchar(10) DEFAULT 'false' COMMENT 'Is it a popular recommendation question?',
  PRIMARY KEY (`id`),
  KEY `idx_rec_q_app_code` (`app_code`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT="AI application related recommendation issues";

-- dbgpt.user_recent_apps definition
CREATE TABLE `user_recent_apps` (
  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT 'autoincrement id',
  `gmt_create` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'create time',
  `gmt_modified` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'last update time',
  `app_code` varchar(255) DEFAULT NULL COMMENT 'AI assistant code',
  `last_accessed` timestamp NULL DEFAULT NULL COMMENT 'User recent usage time',
  `user_code` varchar(255) DEFAULT NULL COMMENT 'user code',
  `sys_code` varchar(255) DEFAULT NULL COMMENT 'system app code',
  PRIMARY KEY (`id`),
  KEY `idx_user_r_app_code` (`app_code`),
  KEY `idx_last_accessed` (`last_accessed`),
  KEY `idx_user_code` (`user_code`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT='User recently used apps';

-- dbgpt.dbgpt_serve_file definition
CREATE TABLE `dbgpt_serve_file` (
  `id` int NOT NULL AUTO_INCREMENT COMMENT 'Auto increment id',
  `bucket` varchar(255) NOT NULL COMMENT 'Bucket name',
  `file_id` varchar(255) NOT NULL COMMENT 'File id',
  `file_name` varchar(256) NOT NULL COMMENT 'File name',
  `file_size` int DEFAULT NULL COMMENT 'File size',
  `storage_type` varchar(32) NOT NULL COMMENT 'Storage type',
  `storage_path` varchar(512) NOT NULL COMMENT 'Storage path',
  `uri` varchar(512) NOT NULL COMMENT 'File URI',
  `custom_metadata` text DEFAULT NULL COMMENT 'Custom metadata, JSON format',
  `file_hash` varchar(128) DEFAULT NULL COMMENT 'File hash',
  `user_name` varchar(128) DEFAULT NULL COMMENT 'User name',
  `sys_code` varchar(128) DEFAULT NULL COMMENT 'System code',
  `gmt_created` datetime DEFAULT CURRENT_TIMESTAMP COMMENT 'Record creation time',
  `gmt_modified` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'Record update time',
  PRIMARY KEY (`id`),
  UNIQUE KEY `uk_bucket_file_id` (`bucket`, `file_id`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- dbgpt.dbgpt_serve_variables definition
CREATE TABLE `dbgpt_serve_variables` (
  `id` int NOT NULL AUTO_INCREMENT COMMENT 'Auto increment id',
  `key` varchar(128) NOT NULL COMMENT 'Variable key',
  `name` varchar(128) DEFAULT NULL COMMENT 'Variable name',
  `label` varchar(128) DEFAULT NULL COMMENT 'Variable label',
  `value` text DEFAULT NULL COMMENT 'Variable value, JSON format',
  `value_type` varchar(32) DEFAULT NULL COMMENT 'Variable value type(string, int, float, bool)',
  `category` varchar(32) DEFAULT 'common' COMMENT 'Variable category(common or secret)',
  `encryption_method` varchar(32) DEFAULT NULL COMMENT 'Variable encryption method(fernet, simple, rsa, aes)',
  `salt` varchar(128) DEFAULT NULL COMMENT 'Variable salt',
  `scope` varchar(32) DEFAULT 'global' COMMENT 'Variable scope(global,flow,app,agent,datasource,flow_priv,agent_priv, ""etc)',
  `scope_key` varchar(256) DEFAULT NULL COMMENT 'Variable scope key, default is empty, for scope is "flow_priv", the scope_key is dag id of flow',
  `enabled` int DEFAULT 1 COMMENT 'Variable enabled, 0: disabled, 1: enabled',
  `description` text DEFAULT NULL COMMENT 'Variable description',
  `user_name` varchar(128) DEFAULT NULL COMMENT 'User name',
  `sys_code` varchar(128) DEFAULT NULL COMMENT 'System code',
  `gmt_created` datetime DEFAULT CURRENT_TIMESTAMP COMMENT 'Record creation time',
  `gmt_modified` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'Record update time',
  PRIMARY KEY (`id`),
  KEY `ix_your_table_name_key` (`key`),
  KEY `ix_your_table_name_name` (`name`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- dbgpt.dbgpt_serve_dbgpts_my definition
CREATE TABLE `dbgpt_serve_dbgpts_my` (
  `id` int NOT NULL AUTO_INCREMENT COMMENT 'autoincrement id',
  `name` varchar(255)  NOT NULL COMMENT 'plugin name',
  `user_name` varchar(255)  DEFAULT NULL COMMENT 'user name',
  `file_name` varchar(255)  NOT NULL COMMENT 'plugin package file name',
  `type` varchar(255)  DEFAULT NULL COMMENT 'plugin type',
  `version` varchar(255)  DEFAULT NULL COMMENT 'plugin version',
  `use_count` int DEFAULT NULL COMMENT 'plugin total use count',
  `succ_count` int DEFAULT NULL COMMENT 'plugin total success count',
  `sys_code` varchar(128) DEFAULT NULL COMMENT 'System code',
  `gmt_created` TIMESTAMP DEFAULT CURRENT_TIMESTAMP COMMENT 'plugin install time',
  `gmt_modified` TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'update time',
  PRIMARY KEY (`id`),
  UNIQUE KEY `name` (`name`, `user_name`),
  KEY `ix_my_plugin_sys_code` (`sys_code`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- dbgpt.dbgpt_serve_dbgpts_hub definition
CREATE TABLE `dbgpt_serve_dbgpts_hub` (
  `id` int NOT NULL AUTO_INCREMENT COMMENT 'autoincrement id',
  `name` varchar(255) NOT NULL COMMENT 'plugin name',
  `description` varchar(255)  NULL COMMENT 'plugin description',
  `author` varchar(255) DEFAULT NULL COMMENT 'plugin author',
  `email` varchar(255) DEFAULT NULL COMMENT 'plugin author email',
  `type` varchar(255) DEFAULT NULL COMMENT 'plugin type',
  `version` varchar(255) DEFAULT NULL COMMENT 'plugin version',
  `storage_channel` varchar(255) DEFAULT NULL COMMENT 'plugin storage channel',
  `storage_url` varchar(255) DEFAULT NULL COMMENT 'plugin download url',
  `download_param` varchar(255) DEFAULT NULL COMMENT 'plugin download param',
  `gmt_created` TIMESTAMP DEFAULT CURRENT_TIMESTAMP COMMENT 'plugin upload time',
  `gmt_modified` TIMESTAMP    DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'update time',
  `installed` int DEFAULT NULL COMMENT 'plugin already installed count',
  PRIMARY KEY (`id`),
  UNIQUE KEY `name` (`name`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

----------------------------------------

TITLE: Configuring ServeConfig Component in React/JSX
DESCRIPTION: React/JSX component implementation for displaying serve configuration documentation. The component uses ConfigDetail to render configuration settings for API keys and related parameters.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "ServeConfig",
  "description": "This configuration is for the hub dbgpts serve module.",
  "documentationUrl": null,
  "parameters": [
    {
      "name": "api_keys",
      "type": "string",
      "required": false,
      "description": "API keys for the endpoint, if None, allow all"
    }
  ]
}} />

----------------------------------------

TITLE: Configuring System Parameters using ConfigDetail Component
DESCRIPTION: Detailed configuration object for system-wide parameters including logging level, API keys, encryption key, and language settings. The configuration is passed to a custom ConfigDetail component for rendering the documentation interface.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "SystemParameters",
  "description": "System parameters.",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "log_level",
      "type": "string",
      "required": false,
      "description": "Logging level",
      "defaultValue": "INFO",
      "validValues": [
        "DEBUG",
        "INFO",
        "WARNING",
        "ERROR",
        "CRITICAL"
      ]
    },
    {
      "name": "api_keys",
      "type": "string",
      "required": false,
      "description": "API keys",
      "defaultValue": "[]"
    },
    {
      "name": "encrypt_key",
      "type": "string",
      "required": false,
      "description": "The key to encrypt the data",
      "defaultValue": "your_secret_key"
    },
    {
      "name": "language",
      "type": "string",
      "required": false,
      "description": "Language setting",
      "defaultValue": "en",
      "validValues": [
        "en",
        "zh",
        "fr",
        "ja",
        "ko",
        "ru"
      ]
    }
  ]
}} />

----------------------------------------

TITLE: Installing Qwen (Tongyi) Dependencies for DB-GPT
DESCRIPTION: This snippet shows how to install the dashscope library, which is required for using the Qwen (Tongyi) model in DB-GPT.

LANGUAGE: python
CODE:
pip install dashscope

----------------------------------------

TITLE: Embedding Video Tutorial Links in Markdown
DESCRIPTION: HTML iframe elements embedded in markdown to display video tutorials from Bilibili and YouTube (commented out) explaining DB-GPT functionality.

LANGUAGE: markdown
CODE:
<iframe width="800" height="480" src="https://player.bilibili.com/player.html?bvid=BV1bp2GYzEpi" frameborder="0" allow="encrypted-media" allowfullscreen></iframe>

<!-- <iframe width="800" height="480" src="https://www.youtube.com/embed/f5_g0OObZBQ" frameborder="0" allow="encrypted-media" allowfullscreen></iframe> -->

----------------------------------------

TITLE: Rendering ServeConfig Configuration Details in JSX
DESCRIPTION: This code snippet uses a custom ConfigDetail component to display the configuration details for ServeConfig. It includes parameters such as api_keys for the serve command.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "ServeConfig",
  "description": "Parameters for the serve command",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "api_keys",
      "type": "string",
      "required": false,
      "description": "API keys for the endpoint, if None, allow all"
    }
  ]
}} />

----------------------------------------

TITLE: Configuring Apache Doris Parameters using JSX Component
DESCRIPTION: JSX component configuration that defines all required and optional parameters for connecting to an Apache Doris database. Includes connection pool settings, authentication credentials, and basic connection properties.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "DorisParameters",
  "description": "A new-generation open-source real-time data warehouse.",
  "documentationUrl": null,
  "parameters": [
    {
      "name": "port",
      "type": "integer",
      "required": true,
      "description": "Database port, e.g., 3306"
    },
    {
      "name": "user",
      "type": "string",
      "required": true,
      "description": "Database user to connect"
    },
    {
      "name": "database",
      "type": "string",
      "required": true,
      "description": "Database name"
    },
    {
      "name": "driver",
      "type": "string",
      "required": false,
      "description": "Driver name for Doris, default is doris.",
      "defaultValue": "doris"
    },
    {
      "name": "password",
      "type": "string",
      "required": false,
      "description": "Database password, you can write your password directly, of course, you can also use environment variables, such as ${env:DBGPT_DB_PASSWORD}",
      "defaultValue": "${env:DBGPT_DB_PASSWORD}"
    },
    {
      "name": "pool_size",
      "type": "integer",
      "required": false,
      "description": "Connection pool size, default 5",
      "defaultValue": "5"
    },
    {
      "name": "max_overflow",
      "type": "integer",
      "required": false,
      "description": "Max overflow connections, default 10",
      "defaultValue": "10"
    },
    {
      "name": "pool_timeout",
      "type": "integer",
      "required": false,
      "description": "Connection pool timeout, default 30",
      "defaultValue": "30"
    },
    {
      "name": "pool_recycle",
      "type": "integer",
      "required": false,
      "description": "Connection pool recycle, default 3600",
      "defaultValue": "3600"
    },
    {
      "name": "pool_pre_ping",
      "type": "boolean",
      "required": false,
      "description": "Connection pool pre ping, default True",
      "defaultValue": "True"
    },
    {
      "name": "host",
      "type": "string",
      "required": true,
      "description": "Database host, e.g., localhost"
    }
  ]
}} />

----------------------------------------

TITLE: Configuring WenXin (Ernie Bot) Proxy in DB-GPT Environment
DESCRIPTION: This snippet shows the configuration settings for using WenXin (Ernie Bot) as a proxy LLM in DB-GPT, including the model version, API key, and API secret.

LANGUAGE: python
CODE:
# .env
LLM_MODEL=wenxin_proxyllm
WEN_XIN_MODEL_VERSION={version} # ERNIE-Bot or ERNIE-Bot-turbo
WEN_XIN_API_KEY={your-wenxin-sk}
WEN_XIN_API_SECRET={your-wenxin-sct}

----------------------------------------

TITLE: Installing Required Transformers Package for Llama 3.1
DESCRIPTION: Command to upgrade the transformers package to version 4.43.0 or higher, which is required for Llama 3.1 support.

LANGUAGE: bash
CODE:
pip install --upgrade "transformers>=4.43.0"

----------------------------------------

TITLE: Configuring TuGraph Database Parameters using React Component
DESCRIPTION: React component implementation showing the configuration schema for TuGraph database connection parameters. Includes essential fields like user authentication, server details, and database selection with their types and requirements.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "TuGraphParameters",
  "description": "TuGraph is a high-performance graph database jointly developed by Ant Group and Tsinghua University.",
  "documentationUrl": null,
  "parameters": [
    {
      "name": "user",
      "type": "string",
      "required": true,
      "description": "TuGraph server user"
    },
    {
      "name": "password",
      "type": "string",
      "required": false,
      "description": "Database password, you can write your password directly, of course, you can also use environment variables, such as ${env:DBGPT_DB_PASSWORD}",
      "defaultValue": "${env:DBGPT_DB_PASSWORD}"
    },
    {
      "name": "port",
      "type": "integer",
      "required": false,
      "description": "TuGraph server port, default 7687",
      "defaultValue": "7687"
    },
    {
      "name": "database",
      "type": "string",
      "required": false,
      "description": "Database name, default 'default'",
      "defaultValue": "default"
    },
    {
      "name": "host",
      "type": "string",
      "required": true,
      "description": "TuGraph server host"
    }
  ]
}} />

----------------------------------------

TITLE: Rendering SparkParameters Configuration Component in JSX
DESCRIPTION: This code snippet renders a ConfigDetail component with SparkParameters configuration. It includes the configuration name, description, and a parameter for the file path of the data source.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "SparkParameters",
  "description": "Unified engine for large-scale data analytics.",
  "documentationUrl": null,
  "parameters": [
    {
      "name": "path",
      "type": "string",
      "required": true,
      "description": "The file path of the data source."
    }
  ]
}} />

----------------------------------------

TITLE: Configuring DB-GPT for LLama.cpp Server
DESCRIPTION: Example configuration settings for the .env file to use LLama.cpp server with DB-GPT, specifying the model and inference type.

LANGUAGE: bash
CODE:
LLM_MODEL=qwen2.5-0.5b-instruct
LLM_MODEL_PATH=/tmp/qwen2.5-0.5b-instruct-q4_k_m.gguf
MODEL_TYPE=llama_cpp_server

----------------------------------------

TITLE: Launching DB-GPT Documentation Development Server
DESCRIPTION: Starts the local development server for the DB-GPT documentation. The server runs on port 3000 by default and can be accessed at localhost:3000.

LANGUAGE: bash
CODE:
yarn start

----------------------------------------

TITLE: Creating DB-GPT Serve Flow Table in SQL
DESCRIPTION: SQL script to create the dbgpt_serve_flow table for storing AWEL workflow data. This table includes fields for unique identifiers, flow data, metadata, and versioning information.

LANGUAGE: SQL
CODE:
CREATE TABLE `dbgpt_serve_flow` (
  `id` int NOT NULL AUTO_INCREMENT COMMENT 'Auto increment id',
  `uid` varchar(128) NOT NULL COMMENT 'Unique id',
  `dag_id` varchar(128) DEFAULT NULL COMMENT 'DAG id',
  `name` varchar(128) DEFAULT NULL COMMENT 'Flow name',
  `flow_data` text COMMENT 'Flow data, JSON format',
  `user_name` varchar(128) DEFAULT NULL COMMENT 'User name',
  `sys_code` varchar(128) DEFAULT NULL COMMENT 'System code',
  `gmt_created` datetime DEFAULT NULL COMMENT 'Record creation time',
  `gmt_modified` datetime DEFAULT NULL COMMENT 'Record update time',
  `flow_category` varchar(64) DEFAULT NULL COMMENT 'Flow category',
  `description` varchar(512) DEFAULT NULL COMMENT 'Flow description',
  `state` varchar(32) DEFAULT NULL COMMENT 'Flow state',
  `source` varchar(64) DEFAULT NULL COMMENT 'Flow source',
  `source_url` varchar(512) DEFAULT NULL COMMENT 'Flow source url',
  `version` varchar(32) DEFAULT NULL COMMENT 'Flow version',
  `label` varchar(128) DEFAULT NULL COMMENT 'Flow label',
  `editable` int DEFAULT NULL COMMENT 'Editable, 0: editable, 1: not editable',
  PRIMARY KEY (`id`),
  UNIQUE KEY `uk_uid` (`uid`),
  KEY `ix_dbgpt_serve_flow_sys_code` (`sys_code`),
  KEY `ix_dbgpt_serve_flow_uid` (`uid`),
  KEY `ix_dbgpt_serve_flow_dag_id` (`dag_id`),
  KEY `ix_dbgpt_serve_flow_user_name` (`user_name`),
  KEY `ix_dbgpt_serve_flow_name` (`name`)
) ENGINE=InnoDB AUTO_INCREMENT=15 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;

----------------------------------------

TITLE: Rendering StarRocks Configuration Component in JSX
DESCRIPTION: This code snippet renders a ConfigDetail component with StarRocks database connection parameters. It includes both required and optional settings for establishing and managing the database connection.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "StarRocksParameters",
  "description": "An Open-Source, High-Performance Analytical Database.",
  "documentationUrl": null,
  "parameters": [
    {
      "name": "port",
      "type": "integer",
      "required": true,
      "description": "Database port, e.g., 3306"
    },
    {
      "name": "user",
      "type": "string",
      "required": true,
      "description": "Database user to connect"
    },
    {
      "name": "database",
      "type": "string",
      "required": true,
      "description": "Database name"
    },
    {
      "name": "driver",
      "type": "string",
      "required": false,
      "description": "Driver name for starrocks, default is starrocks.",
      "defaultValue": "starrocks"
    },
    {
      "name": "password",
      "type": "string",
      "required": false,
      "description": "Database password, you can write your password directly, of course, you can also use environment variables, such as ${env:DBGPT_DB_PASSWORD}",
      "defaultValue": "${env:DBGPT_DB_PASSWORD}"
    },
    {
      "name": "pool_size",
      "type": "integer",
      "required": false,
      "description": "Connection pool size, default 5",
      "defaultValue": "5"
    },
    {
      "name": "max_overflow",
      "type": "integer",
      "required": false,
      "description": "Max overflow connections, default 10",
      "defaultValue": "10"
    },
    {
      "name": "pool_timeout",
      "type": "integer",
      "required": false,
      "description": "Connection pool timeout, default 30",
      "defaultValue": "30"
    },
    {
      "name": "pool_recycle",
      "type": "integer",
      "required": false,
      "description": "Connection pool recycle, default 3600",
      "defaultValue": "3600"
    },
    {
      "name": "pool_pre_ping",
      "type": "boolean",
      "required": false,
      "description": "Connection pool pre ping, default True",
      "defaultValue": "True"
    },
    {
      "name": "host",
      "type": "string",
      "required": true,
      "description": "Database host, e.g., localhost"
    }
  ]
}} />

----------------------------------------

TITLE: Configuring OceanBase Vector Memory
DESCRIPTION: Python code to connect to OceanBase and set vector memory usage ratio.

LANGUAGE: python
CODE:
from pyobvector import ObVecClient

tmp_client = ObVecClient()
tmp_client.perform_raw_text_sql(
    "ALTER SYSTEM ob_vector_memory_limit_percentage = 30"
)

----------------------------------------

TITLE: Installing Project Dependencies
DESCRIPTION: Series of commands to install project dependencies including development and linting requirements, and pre-commit hooks.

LANGUAGE: bash
CODE:
pip install -e ".[default]"

LANGUAGE: bash
CODE:
pip install -r requirements/dev-requirements.txt
pip install -r requirements/lint-requirements.txt

LANGUAGE: bash
CODE:
pre-commit install

----------------------------------------

TITLE: Generating Train and Evaluation Data for Text2SQL
DESCRIPTION: Shell command to generate training and evaluation datasets for Text2SQL tasks.

LANGUAGE: python
CODE:
sh dbgpt_hub/scripts/gen_train_eval_data.sh

----------------------------------------

TITLE: Generating Train and Evaluation Data for Text2SQL
DESCRIPTION: Shell command to generate training and evaluation datasets for Text2SQL tasks.

LANGUAGE: python
CODE:
sh dbgpt_hub/scripts/gen_train_eval_data.sh

----------------------------------------

TITLE: Configuring Vertica Database Parameters using JSX Component
DESCRIPTION: Component configuration for Vertica database connection parameters, including authentication, connection pooling, and basic connection details. Utilizes a ConfigDetail component to render the parameter documentation.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "VerticaParameters",
  "description": "Vertica is a strongly consistent, ACID-compliant, SQL data warehouse, built for the scale and complexity of today`s data-driven world.",
  "documentationUrl": null,
  "parameters": [
    {
      "name": "port",
      "type": "integer",
      "required": true,
      "description": "Database port, e.g., 3306"
    },
    {
      "name": "user",
      "type": "string",
      "required": true,
      "description": "Database user to connect"
    },
    {
      "name": "database",
      "type": "string",
      "required": true,
      "description": "Database name"
    },
    {
      "name": "driver",
      "type": "string",
      "required": false,
      "description": "Driver name for vertica, default is vertica+vertica_python",
      "defaultValue": "vertica+vertica_python"
    },
    {
      "name": "password",
      "type": "string",
      "required": false,
      "description": "Database password, you can write your password directly, of course, you can also use environment variables, such as ${env:DBGPT_DB_PASSWORD}",
      "defaultValue": "${env:DBGPT_DB_PASSWORD}"
    },
    {
      "name": "pool_size",
      "type": "integer",
      "required": false,
      "description": "Connection pool size, default 5",
      "defaultValue": "5"
    },
    {
      "name": "max_overflow",
      "type": "integer",
      "required": false,
      "description": "Max overflow connections, default 10",
      "defaultValue": "10"
    },
    {
      "name": "pool_timeout",
      "type": "integer",
      "required": false,
      "description": "Connection pool timeout, default 30",
      "defaultValue": "30"
    },
    {
      "name": "pool_recycle",
      "type": "integer",
      "required": false,
      "description": "Connection pool recycle, default 3600",
      "defaultValue": "3600"
    },
    {
      "name": "pool_pre_ping",
      "type": "boolean",
      "required": false,
      "description": "Connection pool pre ping, default True",
      "defaultValue": "True"
    },
    {
      "name": "host",
      "type": "string",
      "required": true,
      "description": "Database host, e.g., localhost"
    }
  ]
}} />

----------------------------------------

TITLE: Implementing Simple Chat App
DESCRIPTION: Python implementation of a simple chat application using DB-GPT's AWEL framework

LANGUAGE: python
CODE:
import os
from dbgpt._private.pydantic import BaseModel, Field
from dbgpt.core import ModelMessage, ModelRequest
from dbgpt.core.awel import DAG, HttpTrigger, MapOperator
from dbgpt.model.proxy import OpenAILLMClient
from dbgpt.model.operators import LLMOperator


class TriggerReqBody(BaseModel):
    model: str = Field(..., description="Model name")
    messages: str = Field(..., description="User input")


class RequestHandleOperator(MapOperator[TriggerReqBody, ModelRequest]):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    async def map(self, input_value: TriggerReqBody) -> ModelRequest:
        messages = [ModelMessage.build_human_message(input_value.messages)]
        return ModelRequest.build_request(input_value.model, messages)


with DAG("dbgpts_simple_chat_app") as dag:
    # Receive http request and trigger dag to run.
    trigger = HttpTrigger(
        "/dbgpts/simple_chat_app", methods="POST", request_body=TriggerReqBody
    )
    llm_client = OpenAILLMClient(
        model_alias="gpt-3.5-turbo",  # or other models, eg. "gpt-4o"
        api_base=os.getenv("OPENAI_API_BASE"),
        api_key=os.getenv("OPENAI_API_KEY"),
    )
    request_handle_task = RequestHandleOperator()
    llm_task = LLMOperator(llm_client=llm_client)
    model_parse_task = MapOperator(lambda out: out.text)
    trigger >> request_handle_task >> llm_task >> model_parse_task

----------------------------------------

TITLE: Git Branch and Development Commands
DESCRIPTION: Essential git commands for creating branches, committing changes, and maintaining code quality through formatting and testing.

LANGUAGE: bash
CODE:
git checkout -b <branch-name>

LANGUAGE: bash
CODE:
make fmt

LANGUAGE: bash
CODE:
make test

LANGUAGE: bash
CODE:
make mypy

LANGUAGE: bash
CODE:
make fmt-check

LANGUAGE: bash
CODE:
git add xxxx
git commit -m "your commit message"
git push origin <branch-name>

----------------------------------------

TITLE: Installing Financial Report Analysis Apps with DB-GPT
DESCRIPTION: Installs Poetry package manager and then uses it to install the financial report knowledge processing pipeline and financial robot app workflows for DB-GPT.

LANGUAGE: bash
CODE:
# install poetry
pip install poetry

# install financial report knowledge process pipeline workflow and financial-robot-app workflow
dbgpt app install financial-robot-app financial-report-knowledge-factory

----------------------------------------

TITLE: Initializing HFEmbeddingDeployModelParameters in Python
DESCRIPTION: This code snippet shows the initialization of HFEmbeddingDeployModelParameters with various configuration options. It includes parameters for model name, provider, verbosity, concurrency, path, device, cache folder, embedding normalization, multi-process execution, and custom model and encoding arguments.

LANGUAGE: python
CODE:
HFEmbeddingDeployModelParameters(name: str, provider: str = 'hf', verbose: Optional[bool] = False, concurrency: Optional[int] = 100, path: Optional[str] = None, device: Optional[str] = None, cache_folder: Optional[str] = None, normalize_embeddings: bool = False, multi_process: bool = False, model_kwargs: Dict[str, Any] = <factory>, encode_kwargs: Dict[str, Any] = <factory>, embed_instruction: Optional[str] = None, query_instruction: Optional[str] = None)

----------------------------------------

TITLE: Configuring Environment Variables for Llama 3.1
DESCRIPTION: Environment configuration settings for using Llama 3.1 models, including model selection, paths, quantization settings, and GPU memory allocation.

LANGUAGE: env
CODE:
LLM_MODEL=meta-llama-3.1-8b-instruct
# LLM_MODEL=meta-llama-3.1-70b-instruct
# LLM_MODEL=meta-llama-3.1-405b-instruct
## you can also specify the model path
# LLM_MODEL_PATH=models/Meta-Llama-3.1-8B-Instruct
## Quantization settings
# QUANTIZE_8bit=False
# QUANTIZE_4bit=True
## You can configure the maximum memory used by each GPU.
# MAX_GPU_MEMORY=16Gib

----------------------------------------

TITLE: Importing Fine-tuning Functions
DESCRIPTION: Imports necessary functions for data preprocessing, training, prediction, and evaluation from dbgpt_hub.

LANGUAGE: python
CODE:
from dbgpt_hub.data_process import preprocess_sft_data
from dbgpt_hub.train import train_sft
from dbgpt_hub.predict import start_predict
from dbgpt_hub.eval import start_evaluate

----------------------------------------

TITLE: Starting DB-GPT Cluster with Docker Compose
DESCRIPTION: Command to start the DB-GPT cluster using Docker Compose, including environment variables for OpenAI API integration.

LANGUAGE: bash
CODE:
OPENAI_API_KEY="{your api key}" OPENAI_API_BASE="https://api.openai.com/v1" docker compose up -d

----------------------------------------

TITLE: Rendering JinaEmbeddingsDeployModelParameters Configuration in JSX
DESCRIPTION: This code snippet uses a custom React component 'ConfigDetail' to render the configuration details for JinaEmbeddingsDeployModelParameters. It includes a comprehensive list of parameters with their types, descriptions, and default values.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "JinaEmbeddingsDeployModelParameters",
  "description": "Jina AI Embeddings deploy model parameters.",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "proxy/jina"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "100"
    },
    {
      "name": "api_url",
      "type": "string",
      "required": false,
      "description": "The URL of the embeddings API.",
      "defaultValue": "https://api.jina.ai/v1/embeddings"
    },
    {
      "name": "api_key",
      "type": "string",
      "required": false,
      "description": "The API key for the embeddings API."
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name.",
      "defaultValue": "jina-embeddings-v2-base-en"
    },
    {
      "name": "timeout",
      "type": "integer",
      "required": false,
      "description": "The timeout for the request in seconds.",
      "defaultValue": "60"
    }
  ]
}} />

----------------------------------------

TITLE: Starting DB-GPT Webserver
DESCRIPTION: Command to launch the DB-GPT webserver with Llama 3.1 integration.

LANGUAGE: bash
CODE:
dbgpt start webserver

----------------------------------------

TITLE: Configuring OpenAPIEmbeddingDeployModelParameters in JSX
DESCRIPTION: JSX component configuration that defines the parameters for deploying OpenAPI embedding models. Includes essential settings like model name, provider, API endpoints, timeouts, and other deployment configurations.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "OpenAPIEmbeddingDeployModelParameters",
  "description": "OpenAPI embedding deploy model parameters.",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "proxy/openai"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "100"
    },
    {
      "name": "api_url",
      "type": "string",
      "required": false,
      "description": "The URL of the embeddings API.",
      "defaultValue": "http://localhost:8100/api/v1/embeddings"
    },
    {
      "name": "api_key",
      "type": "string",
      "required": false,
      "description": "The API key for the embeddings API."
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name."
    },
    {
      "name": "timeout",
      "type": "integer",
      "required": false,
      "description": "The timeout for the request in seconds.",
      "defaultValue": "60"
    }
  ]
}} />

----------------------------------------

TITLE: Running Local Chat Workflow
DESCRIPTION: Example of running a chat workflow locally with stream output using the dbgpt CLI

LANGUAGE: bash
CODE:
dbgpt run flow --local chat \
--name awel-flow-simple-streaming-chat \
--model "gpt-3.5-turbo" \
--messages "hello" \
--stream

----------------------------------------

TITLE: Configuring DBModelRegistryParameters Component in JSX
DESCRIPTION: React/JSX component that renders configuration documentation for DBModelRegistryParameters. The component displays database configuration options for model registry with links to specific database connector configurations.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "DBModelRegistryParameters",
  "description": "Database model registry parameters.",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "database",
      "type": "BaseDatasourceParameters",
      "required": false,
      "description": "Database configuration for model registry",
      "nestedTypes": [
        {
          "type": "link",
          "text": "rdbmsdatasourceparameters configuration",
          "url": "././base_rdbmsdatasourceparameters_4f774f"
        },
        {
          "type": "link",
          "text": "sqlite configuration",
          "url": "././conn_sqlite_sqliteconnectorparameters_82c8b5"
        },
        {
          "type": "link",
          "text": "tugraph configuration",
          "url": "././conn_tugraph_tugraphparameters_0c844e"
        },
        {
          "type": "link",
          "text": "spark configuration",
          "url": "././conn_spark_sparkparameters_174bbc"
        },
        {
          "type": "link",
          "text": "clickhouse configuration",
          "url": "././conn_clickhouse_clickhouseparameters_4a1237"
        },
        {
          "type": "link",
          "text": "doris configuration",
          "url": "././conn_doris_dorisparameters_e33c53"
        },
        {
          "type": "link",
          "text": "duckdb configuration",
          "url": "././conn_duckdb_duckdbconnectorparameters_c672c7"
        },
        {
          "type": "link",
          "text": "hive configuration",
          "url": "././conn_hive_hiveparameters_ec3601"
        },
        {
          "type": "link",
          "text": "mssql configuration",
          "url": "././conn_mssql_mssqlparameters_d79d1c"
        },
        {
          "type": "link",
          "text": "mysql configuration",
          "url": "././conn_mysql_mysqlparameters_4393c4"
        },
        {
          "type": "link",
          "text": "oceanbase configuration",
          "url": "././conn_oceanbase_oceanbaseparameters_260d2d"
        },
        {
          "type": "link",
          "text": "postgresql configuration",
          "url": "././conn_postgresql_postgresqlparameters_22efa5"
        },
        {
          "type": "link",
          "text": "starrocks configuration",
          "url": "././conn_starrocks_starrocksparameters_e511f7"
        },
        {
          "type": "link",
          "text": "vertica configuration",
          "url": "././conn_vertica_verticaparameters_c712b8"
        }
      ]
    }
  ]
}} />

----------------------------------------

TITLE: Navigating to DB-GPT Directory
DESCRIPTION: Command to change directory to DB-GPT root directory for configuration and deployment.

LANGUAGE: bash
CODE:
cd DB-GPT

----------------------------------------

TITLE: Initializing LlamaServerParameters in Python
DESCRIPTION: This code snippet shows the constructor for the LlamaServerParameters class with all available parameters. It includes options for model configuration, server settings, and various performance and functionality toggles.

LANGUAGE: python
CODE:
LlamaServerParameters(name: str, provider: str = 'llama.cpp.server', verbose: Optional[bool] = False, concurrency: Optional[int] = 20, backend: Optional[str] = None, prompt_template: Optional[str] = None, context_length: Optional[int] = None, path: Optional[str] = None, model_hf_repo: Optional[str] = None, model_hf_file: Optional[str] = None, device: Optional[str] = None, server_bin_path: Optional[str] = None, server_host: str = '127.0.0.1', server_port: int = 0, temperature: float = 0.8, seed: int = 42, debug: bool = False, model_url: Optional[str] = None, model_draft: Optional[str] = None, threads: Optional[int] = None, n_gpu_layers: Optional[int] = None, batch_size: Optional[int] = None, ubatch_size: Optional[int] = None, ctx_size: Optional[int] = None, grp_attn_n: Optional[int] = None, grp_attn_w: Optional[int] = None, n_predict: Optional[int] = None, slot_save_path: Optional[str] = None, n_slots: Optional[int] = None, cont_batching: bool = False, embedding: bool = False, reranking: bool = False, metrics: bool = False, slots: bool = False, draft: Optional[int] = None, draft_max: Optional[int] = None, draft_min: Optional[int] = None, api_key: Optional[str] = None, lora_files: List[str] = <factory>, no_context_shift: bool = False, no_webui: Optional[bool] = None, startup_timeout: Optional[int] = None)

----------------------------------------

TITLE: Installing DB-GPT RAG Dependencies
DESCRIPTION: Command to install the required DB-GPT library with RAG support version 0.5.8 or higher

LANGUAGE: bash
CODE:
pip install "dbgpt[rag]>=0.5.8"

----------------------------------------

TITLE: Rendering Xunfei Spark Proxy LLM Configuration in JSX
DESCRIPTION: This code snippet uses a custom React component called ConfigDetail to render the configuration details for the Xunfei Spark proxy LLM. It includes a comprehensive list of parameters with their types, descriptions, and default values.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "SparkDeployModelParameters",
  "description": "Xunfei Spark proxy LLM configuration.",
  "documentationUrl": "https://www.xfyun.cn/doc/spark/HTTP%E8%B0%83%E7%94%A8%E6%96%87%E6%A1%A3.html#_1-%E6%8E%A5%E5%8F%A3%E8%AF%B4%E6%98%8E",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name."
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "proxy/spark"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "100"
    },
    {
      "name": "prompt_template",
      "type": "string",
      "required": false,
      "description": "Prompt template. If None, the prompt template is automatically determined from model. Just for local deployment."
    },
    {
      "name": "context_length",
      "type": "integer",
      "required": false,
      "description": "The context length of the OpenAI API. If None, it is determined by the model."
    },
    {
      "name": "api_base",
      "type": "string",
      "required": false,
      "description": "The base url of the Spark API.",
      "defaultValue": "${env:XUNFEI_SPARK_API_BASE:-https://spark-api-open.xf-yun.com/v1}"
    },
    {
      "name": "api_key",
      "type": "string",
      "required": false,
      "description": "The API key of the Spark API.",
      "defaultValue": "${env:XUNFEI_SPARK_API_KEY}"
    },
    {
      "name": "api_type",
      "type": "string",
      "required": false,
      "description": "The type of the OpenAI API, if you use Azure, it can be: azure"
    },
    {
      "name": "api_version",
      "type": "string",
      "required": false,
      "description": "The version of the OpenAI API."
    },
    {
      "name": "http_proxy",
      "type": "string",
      "required": false,
      "description": "The http or https proxy to use openai"
    }
  ]
}} />

----------------------------------------

TITLE: Installing Dependencies for DB-GPT v0.5.0 Upgrade
DESCRIPTION: Bash command to install dependencies for the DB-GPT v0.5.0 upgrade when installed from source code using the default method.

LANGUAGE: bash
CODE:
pip install -e ".[default]"

----------------------------------------

TITLE: Using DB-GPT Command Line Tool
DESCRIPTION: Example of using the dbgpt command line tool to access help documentation for available commands and subcommands.

LANGUAGE: bash
CODE:
dbgpt --help
dbgpt start --help

----------------------------------------

TITLE: Configuring OceanBase Vector Database in DB-GPT
DESCRIPTION: Configures OceanBase connection details including host, port, user, database, and optional parameters for password and vector normalization.

LANGUAGE: shell
CODE:
OB_HOST=127.0.0.1
OB_PORT=2881
OB_USER=root@test
OB_DATABASE=test
## Optional
# OB_PASSWORD=
## Optional: If {OB_ENABLE_NORMALIZE_VECTOR} is set, the vector stored in OceanBase is normalized.
# OB_ENABLE_NORMALIZE_VECTOR=True

----------------------------------------

TITLE: Configuring TongyiEmbedding Model Parameters using React JSX
DESCRIPTION: React component that displays configuration details for TongyiEmbedding model deployment parameters. This includes settings for model name, provider, verbosity, concurrency limits, API key and backend model name.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "TongyiEmbeddingDeployModelParameters",
  "description": "Qianfan Embeddings deploy model parameters.",
  "documentationUrl": "",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "proxy/tongyi"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "100"
    },
    {
      "name": "api_key",
      "type": "string",
      "required": false,
      "description": "The API key for the embeddings API."
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name.",
      "defaultValue": "text-embedding-v1"
    }
  ]
}} />

----------------------------------------

TITLE: Installing gettext on Ubuntu/Debian
DESCRIPTION: Commands to install the gettext package on Ubuntu, Debian, and their derivatives using apt package manager.

LANGUAGE: bash
CODE:
sudo apt update
sudo apt install gettext

----------------------------------------

TITLE: Installing gettext on Fedora
DESCRIPTION: Command to install the gettext package on Fedora using dnf package manager.

LANGUAGE: bash
CODE:
sudo dnf install gettext

----------------------------------------

TITLE: Rendering Yi Proxy LLM Configuration Component in JSX
DESCRIPTION: This code snippet renders a ConfigDetail component with a detailed configuration object for Yi proxy LLM. It includes parameters for model settings, API configuration, and deployment options.

LANGUAGE: jsx
CODE:
<ConfigDetail config={{
  "name": "YiDeployModelParameters",
  "description": "Yi proxy LLM configuration.",
  "documentationUrl": "https://platform.lingyiwanwu.com/docs",
  "parameters": [
    {
      "name": "name",
      "type": "string",
      "required": true,
      "description": "The name of the model."
    },
    {
      "name": "backend",
      "type": "string",
      "required": false,
      "description": "The real model name to pass to the provider, default is None. If backend is None, use name as the real model name."
    },
    {
      "name": "provider",
      "type": "string",
      "required": false,
      "description": "The provider of the model. If model is deployed in local, this is the inference type. If model is deployed in third-party service, this is platform name('proxy/<platform>')",
      "defaultValue": "proxy/yi"
    },
    {
      "name": "verbose",
      "type": "boolean",
      "required": false,
      "description": "Show verbose output.",
      "defaultValue": "False"
    },
    {
      "name": "concurrency",
      "type": "integer",
      "required": false,
      "description": "Model concurrency limit",
      "defaultValue": "100"
    },
    {
      "name": "prompt_template",
      "type": "string",
      "required": false,
      "description": "Prompt template. If None, the prompt template is automatically determined from model. Just for local deployment."
    },
    {
      "name": "context_length",
      "type": "integer",
      "required": false,
      "description": "The context length of the OpenAI API. If None, it is determined by the model."
    },
    {
      "name": "api_base",
      "type": "string",
      "required": false,
      "description": "The base url of the Yi API.",
      "defaultValue": "${env:YI_API_BASE:-https://api.lingyiwanwu.com/v1}"
    },
    {
      "name": "api_key",
      "type": "string",
      "required": false,
      "description": "The API key of the Yi API.",
      "defaultValue": "${env:YI_API_KEY}"
    },
    {
      "name": "api_type",
      "type": "string",
      "required": false,
      "description": "The type of the OpenAI API, if you use Azure, it can be: azure"
    },
    {
      "name": "api_version",
      "type": "string",
      "required": false,
      "description": "The version of the OpenAI API."
    },
    {
      "name": "http_proxy",
      "type": "string",
      "required": false,
      "description": "The http or https proxy to use openai"
    }
  ]
}} />

----------------------------------------

TITLE: Cloning DB-GPT Repository
DESCRIPTION: Command to clone the forked DB-GPT repository to local machine. Requires replacing placeholder with actual GitHub username.

LANGUAGE: bash
CODE:
git clone https://github.com/<YOUR-GITHUB-USERNAME>/DB-GPT

----------------------------------------

TITLE: Markdown Heading
DESCRIPTION: Simple markdown heading for the dbgpt-app project documentation

LANGUAGE: markdown
CODE:
# dbgpt-app

----------------------------------------

TITLE: Stream Transformation Output Example
DESCRIPTION: Example output showing doubled numbers from 0 to 9 after transformation.

LANGUAGE: bash
CODE:
0
2
4
6
8
10
12
14
16
18

----------------------------------------

TITLE: Displaying Project Badges in Markdown
DESCRIPTION: Markdown code for displaying various project badges and shields, including GitHub stars, forks, license, and community links.

LANGUAGE: markdown
CODE:
<p align="left">
  <img src="./assets/LOGO.png" width="100%" />
</p>

<div align="center">
  <p>
    <a href="https://github.com/eosphoros-ai/DB-GPT">
        <img alt="stars" src="https://img.shields.io/github/stars/eosphoros-ai/db-gpt?style=social" />
    </a>
    <a href="https://github.com/eosphoros-ai/DB-GPT">
        <img alt="forks" src="https://img.shields.io/github/forks/eosphoros-ai/db-gpt?style=social" />
    </a>
    <a href="https://opensource.org/licenses/MIT">
      <img alt="License: MIT" src="https://img.shields.io/badge/License-MIT-yellow.svg" />
    </a>
     <a href="https://github.com/eosphoros-ai/DB-GPT/releases">
      <img alt="Release Notes" src="https://img.shields.io/github/release/eosphoros-ai/DB-GPT" />
    </a>
    <a href="https://github.com/eosphoros-ai/DB-GPT/issues">
      <img alt="Open Issues" src="https://img.shields.io/github/issues-raw/eosphoros-ai/DB-GPT" />
    </a>
    <a href="https://discord.gg/7uQnPuveTY">
      <img alt="Discord" src="https://dcbadge.vercel.app/api/server/7uQnPuveTY?compact=true&style=flat" />
    </a>
    <a href="https://join.slack.com/t/slack-inu2564/shared_invite/zt-29rcnyw2b-N~ubOD9kFc7b7MDOAM1otA">
      <img alt="Slack" src="https://badgen.net/badge/Slack/Join%20DB-GPT/0abd59?icon=slack" />
    </a>
    <a href="https://codespaces.new/eosphoros-ai/DB-GPT">
      <img alt="Open in GitHub Codespaces" src="https://github.com/codespaces/badge.svg" />
    </a>
  </p>
</div>

----------------------------------------

TITLE: Python Testing and Development Dependencies List
DESCRIPTION: Comprehensive list of Python packages required for development, including testing frameworks like pytest and its plugins, type checking with mypy, and development tools like pre-commit for git hooks. Specific version constraints are defined for certain packages.

LANGUAGE: plaintext
CODE:
# Testing and dev dependencies
pytest
pytest-cov
asynctest
pytest-asyncio
pytest-benchmark
pytest-cov
pytest-integration
pytest-mock
pytest-recording
pytesseract==0.3.10
aioresponses
# for git hooks
pre-commit
# Type checking
mypy==1.7.0
httpx
vcrpy<6.0.0

----------------------------------------

TITLE: SQL Grammar Production Rules
DESCRIPTION: Defines grammar production rules for parsing SQL statements and database operations. Includes rules for DDL, DML, DCL and other SQL commands.

LANGUAGE: SQL
CODE:
/*! Production::    sql_stmt : select_stmt */
/*! Production::    sql_stmt : insert_stmt */
/*! Production::    sql_stmt : update_stmt */
/*! Production::    sql_stmt : delete_stmt */
/*! Production::    sql_stmt : create_table_stmt */
/*! Production::    sql_stmt : alter_table_stmt */
/*! Production::    sql_stmt : drop_table_stmt */