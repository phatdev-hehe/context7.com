TITLE: Advanced ML Pipeline Implementation in Python using Kubeflow
DESCRIPTION: Complete implementation of an ML pipeline that processes the Iris dataset, performs feature normalization, and trains KNN models with different hyperparameters. The pipeline showcases KFP features including package installation, artifact handling, and parallel execution.

LANGUAGE: python
CODE:
from typing import List

from kfp import client
from kfp import dsl
from kfp.dsl import Dataset
from kfp.dsl import Input
from kfp.dsl import Model
from kfp.dsl import Output


@dsl.component(packages_to_install=['pandas==1.3.5'])
def create_dataset(iris_dataset: Output[Dataset]):
    import pandas as pd

    csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
    col_names = [
        'Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Labels'
    ]
    df = pd.read_csv(csv_url, names=col_names)

    with open(iris_dataset.path, 'w') as f:
        df.to_csv(f)


@dsl.component(packages_to_install=['pandas==1.3.5', 'scikit-learn==1.0.2'])
def normalize_dataset(
    input_iris_dataset: Input[Dataset],
    normalized_iris_dataset: Output[Dataset],
    standard_scaler: bool,
    min_max_scaler: bool,
):
    if standard_scaler is min_max_scaler:
        raise ValueError(
            'Exactly one of standard_scaler or min_max_scaler must be True.')

    import pandas as pd
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.preprocessing import StandardScaler

    with open(input_iris_dataset.path) as f:
        df = pd.read_csv(f)
    labels = df.pop('Labels')

    if standard_scaler:
        scaler = StandardScaler()
    if min_max_scaler:
        scaler = MinMaxScaler()

    df = pd.DataFrame(scaler.fit_transform(df))
    df['Labels'] = labels
    with open(normalized_iris_dataset.path, 'w') as f:
        df.to_csv(f)


@dsl.component(packages_to_install=['pandas==1.3.5', 'scikit-learn==1.0.2'])
def train_model(
    normalized_iris_dataset: Input[Dataset],
    model: Output[Model],
    n_neighbors: int,
):
    import pickle

    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.neighbors import KNeighborsClassifier

    with open(normalized_iris_dataset.path) as f:
        df = pd.read_csv(f)

    y = df.pop('Labels')
    X = df

    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

    clf = KNeighborsClassifier(n_neighbors=n_neighbors)
    clf.fit(X_train, y_train)
    with open(model.path, 'wb') as f:
        pickle.dump(clf, f)


@dsl.pipeline(name='iris-training-pipeline')
def my_pipeline(
    standard_scaler: bool,
    min_max_scaler: bool,
    neighbors: List[int],
):
    create_dataset_task = create_dataset()

    normalize_dataset_task = normalize_dataset(
        input_iris_dataset=create_dataset_task.outputs['iris_dataset'],
        standard_scaler=True,
        min_max_scaler=False)

    with dsl.ParallelFor(neighbors) as n_neighbors:
        train_model(
            normalized_iris_dataset=normalize_dataset_task
            .outputs['normalized_iris_dataset'],
            n_neighbors=n_neighbors)


endpoint = '<KFP_UI_URL>'
kfp_client = client.Client(host=endpoint)
run = kfp_client.create_run_from_pipeline_func(
    my_pipeline,
    arguments={
        'min_max_scaler': True,
        'standard_scaler': False,
        'neighbors': [3, 6, 9]
    },
)
url = f'{endpoint}/#/runs/details/{run.run_id}'
print(url)

----------------------------------------

TITLE: Running KFP Components and Pipelines Locally in Python
DESCRIPTION: This snippet demonstrates how to initialize a local session, define components and pipelines, and execute them locally using the DockerRunner. It shows both single component execution and pipeline execution.

LANGUAGE: python
CODE:
from kfp import local
from kfp import dsl

local.init(runner=local.DockerRunner())

@dsl.component
def add(a: int, b: int) -> int:
    return a + b

# run a single component
task = add(a=1, b=2)
assert task.output == 3

# or run it in a pipeline
@dsl.pipeline
def math_pipeline(x: int, y: int, z: int) -> int:
    t1 = add(a=x, b=y)
    t2 = add(a=t1.output, b=z)
    return t2.output

pipeline_task = math_pipeline(x=1, y=2, z=3)
assert pipeline_task.output == 6

----------------------------------------

TITLE: Katib Hyperparameter Tuning with Python SDK
DESCRIPTION: Example showing how to use Katib Python SDK to optimize a mathematical function F(a,b) = 4a - b^2. The code creates an experiment that runs 12 trials with 2 CPUs each to find optimal parameters.

LANGUAGE: python
CODE:
# [1] Create an objective function.
def objective(parameters):
    # Import required packages.
    import time
    time.sleep(5)
    # Calculate objective function.
    result = 4 * int(parameters["a"]) - float(parameters["b"]) ** 2
    # Katib parses metrics in this format: <metric-name>=<metric-value>.
    print(f"result={result}")

import kubeflow.katib as katib

# [2] Create hyperparameter search space.
parameters = {
    "a": katib.search.int(min=10, max=20),
    "b": katib.search.double(min=0.1, max=0.2)
}

# [3] Create Katib Experiment with 12 Trials and 2 CPUs per Trial.
katib_client = katib.KatibClient(namespace="kubeflow")

name = "tune-experiment"
katib_client.tune(
    name=name,
    objective=objective,
    parameters=parameters,
    objective_metric_name="result",
    max_trial_count=12,
    resources_per_trial={"cpu": "2"},
)

# [4] Wait until Katib Experiment is complete
katib_client.wait_for_experiment_condition(name=name)

# [5] Get the best hyperparameters.
print(katib_client.get_optimal_hyperparameters(name))

----------------------------------------

TITLE: Creating Hello World Pipeline Components
DESCRIPTION: Defines a simple Kubeflow pipeline with a single component that prints and returns a greeting. Uses type annotations and KFP decorators to define the component and pipeline structure.

LANGUAGE: python
CODE:
from kfp import dsl

@dsl.component
def say_hello(name: str) -> str:
    hello_text = f'Hello, {name}!'
    print(hello_text)
    return hello_text

@dsl.pipeline
def hello_pipeline(recipient: str) -> str:
    hello_task = say_hello(name=recipient)
    return hello_task.output

----------------------------------------

TITLE: Defining a Training Component with Input and Output Artifacts in Python
DESCRIPTION: This code snippet demonstrates how to create a KFP component that accepts an input Dataset artifact and produces an output Model artifact. It shows how to read from the input artifact's path and write to the output artifact's path, as well as set metadata on the output artifact.

LANGUAGE: python
CODE:
from kfp.dsl import Input, Output, Dataset, Model

@dsl.component
def training_component(dataset: Input[Dataset], model: Output[Model]):
    """Trains an output Model on an input Dataset."""
    with open(dataset.path) as f:
        contents = f.read()

    # ... train tf_model model on contents of dataset ...

    tf_model.save(model.path)
    model.metadata['framework'] = 'tensorflow'

----------------------------------------

TITLE: Fine-tuning BERT model using Kubeflow Training API
DESCRIPTION: This code snippet demonstrates how to use the Kubeflow Training Client's 'train' API to fine-tune a BERT model using the Yelp Review dataset from HuggingFace Hub. It configures model parameters, dataset parameters, trainer settings, and distributed training resources.

LANGUAGE: python
CODE:
import transformers
from peft import LoraConfig

from kubeflow.training import TrainingClient
from kubeflow.storage_initializer.hugging_face import (
    HuggingFaceModelParams,
    HuggingFaceTrainerParams,
    HuggingFaceDatasetParams,
)

TrainingClient().train(
    name="fine-tune-bert",
    # BERT model URI and type of Transformer to train it.
    model_provider_parameters=HuggingFaceModelParams(
        model_uri="hf://google-bert/bert-base-cased",
        transformer_type=transformers.AutoModelForSequenceClassification,
    ),
    # Use 3000 samples from Yelp dataset.
    dataset_provider_parameters=HuggingFaceDatasetParams(
        repo_id="yelp_review_full",
        split="train[:3000]",
    ),
    # Specify HuggingFace Trainer parameters. In this example, we will skip evaluation and model checkpoints.
    trainer_parameters=HuggingFaceTrainerParams(
        training_parameters=transformers.TrainingArguments(
            output_dir="test_trainer",
            save_strategy="no",
            evaluation_strategy="no",
            do_eval=False,
            disable_tqdm=True,
            log_level="info",
        ),
        # Set LoRA config to reduce number of trainable model parameters.
        lora_config=LoraConfig(
            r=8,
            lora_alpha=8,
            lora_dropout=0.1,
            bias="none",
        ),
    ),
    num_workers=4, # nnodes parameter for torchrun command.
    num_procs_per_worker=2, # nproc-per-node parameter for torchrun command.
    resources_per_worker={
        "gpu": 2,
        "cpu": 5,
        "memory": "10G",
    },
)

----------------------------------------

TITLE: Defining a Kubeflow Pipeline with Component Tasks
DESCRIPTION: Example of defining a Kubeflow pipeline that implements the Pythagorean theorem using simple arithmetic components. The pipeline decorator, component definitions, and data passing between tasks are demonstrated.

LANGUAGE: python
CODE:
from kfp import dsl

@dsl.component
def square(x: float) -> float:
    return x ** 2

@dsl.component
def add(x: float, y: float) -> float:
    return x + y

@dsl.component
def square_root(x: float) -> float:
    return x ** .5

@dsl.pipeline
def pythagorean(a: float, b: float) -> float:
    a_sq_task = square(x=a)
    b_sq_task = square(x=b)
    sum_task = add(x=a_sq_task.output, y=b_sq_task.output)
    return square_root(x=sum_task.output).output

----------------------------------------

TITLE: Training Model Component with Pythonic Artifact Syntax
DESCRIPTION: This component demonstrates the new Pythonic artifact syntax for KFP. It takes a Dataset as input and returns a Model as output, showing how to construct and return the output artifact.

LANGUAGE: python
CODE:
from kfp import dsl
from kfp.dsl import Dataset, Model

@dsl.component
def train_model(dataset: Dataset) -> Model:
    with open(dataset.path) as f:
        dataset_lines = f.readlines()

    # train a model
    trained_model = ...

    model_artifact = Model(uri=dsl.get_uri(), metadata={'samples': len(dataset_lines)})
    trained_model.save(model_artifact.path)
    
    return model_artifact

----------------------------------------

TITLE: Accessing Pipeline and Task Status Metadata in Exit Handlers in Kubeflow Pipelines
DESCRIPTION: This snippet demonstrates how to use dsl.PipelineTaskFinalStatus in an exit task to access pipeline and task status metadata, including pipeline failure or success status. The exit_op task prints pipeline run status information even after fail_op fails.

LANGUAGE: python
CODE:
from kfp import dsl
from kfp.dsl import PipelineTaskFinalStatus

@dsl.component
def print_op(message: str):
    print(message)

@dsl.component
def exit_op(user_input: str, status: PipelineTaskFinalStatus):
    """Prints pipeline run status."""
    print(user_input)
    print('Pipeline status: ', status.state)
    print('Job resource name: ', status.pipeline_job_resource_name)
    print('Pipeline task name: ', status.pipeline_task_name)
    print('Error code: ', status.error_code)
    print('Error message: ', status.error_message)

@dsl.component
def fail_op():
    import sys
    sys.exit(1)

@dsl.pipeline
def my_pipeline():
    print_op(message='Starting pipeline...')
    print_status_task = exit_op(user_input='Task execution status:')
    with dsl.ExitHandler(exit_task=print_status_task):
        fail_op()

----------------------------------------

TITLE: Implementing Custom Katib Algorithm Service in Python
DESCRIPTION: Example implementation of a custom hyperparameter tuning algorithm service for Katib using gRPC. Shows how to implement the SuggestionServicer interface and handle hyperparameter suggestions.

LANGUAGE: python
CODE:
from pkg.apis.manager.v1beta1.python import api_pb2
from pkg.apis.manager.v1beta1.python import api_pb2_grpc
from pkg.suggestion.v1beta1.internal.search_space import HyperParameter, HyperParameterSearchSpace
from pkg.suggestion.v1beta1.internal.trial import Trial, Assignment
from pkg.suggestion.v1beta1.hyperopt.base_service import BaseHyperoptService
from pkg.suggestion.v1beta1.internal.base_health_service import HealthServicer

class HyperoptService(
        api_pb2_grpc.SuggestionServicer, HealthServicer):
    def ValidateAlgorithmSettings(self, request, context):
        # Optional, it is used to validate algorithm settings defined by users.
        pass
    def GetSuggestions(self, request, context):
        search_space = HyperParameterSearchSpace.convert(request.experiment)
        trials = Trial.convert(request.trials)
        list_of_assignments = your_logic(search_space, trials, request.current_request_number)
        return api_pb2.GetSuggestionsReply(
            trials=Assignment.generate(list_of_assignments)
        )

----------------------------------------

TITLE: Implementing Conditional Execution with dsl.If in Kubeflow Pipelines
DESCRIPTION: This snippet demonstrates how to use dsl.If to conditionally execute tasks based on the output of an upstream task. The conditional_task only executes if the coin_flip_task outputs 'heads'.

LANGUAGE: python
CODE:
from kfp import dsl

@dsl.component
def flip_coin() -> str:
    import random
    return random.choice(['heads', 'tails'])

#@dsl.component
#def my_comp():
#    print('Conditional task executed!')

@dsl.pipeline
def my_pipeline():
    coin_flip_task = flip_coin()
    with dsl.If(coin_flip_task.output == 'heads'):
        conditional_task = my_comp()

----------------------------------------

TITLE: Basic Container Component Definition
DESCRIPTION: Demonstrates how to create a simple Container Component that prints 'Hello' using the alpine image.

LANGUAGE: python
CODE:
from kfp import dsl

@dsl.container_component
def say_hello():
    return dsl.ContainerSpec(image='alpine', command=['echo'], args=['Hello'])

----------------------------------------

TITLE: Defining XGBoost Classifier Component in YAML
DESCRIPTION: Example YAML specification for an XGBoost classifier training component, showing metadata, inputs, outputs and container implementation details.

LANGUAGE: yaml
CODE:
name: xgboost4j - Train classifier
description: Trains a boosted tree ensemble classifier using xgboost4j

inputs:
- {name: Training data}
- {name: Rounds, type: Integer, default: '30', description: 'Number of training rounds'}

outputs:
- {name: Trained model, type: XGBoost model, description: 'Trained XGBoost model'}

implementation:
  container:
    image: gcr.io/ml-pipeline/xgboost-classifier-train@sha256:b3a64d57
    command: [
      /ml/train.py,
      --train-set, {inputPath: Training data},
      --rounds,    {inputValue: Rounds},
      --out-model, {outputPath: Trained model},
    ]

----------------------------------------

TITLE: Creating a Kubeflow TrainJob for Distributed PyTorch Training
DESCRIPTION: Creates a TrainJob using the 'torch-distributed' Runtime, scaling the training function across 4 PyTorch nodes, each with 1 GPU.

LANGUAGE: python
CODE:
job_id = TrainerClient().train(
    trainer=CustomTrainer(
        func=train_pytorch,
        num_nodes=4,
        resources_per_node={
            "cpu": 5,
            "memory": "16Gi",
            "gpu": 1, # Comment this line if you don't have GPUs.
        },
    ),
    runtime_ref="torch-distributed",
)

----------------------------------------

TITLE: Implementing Distributed PyTorch Training with Training Operator
DESCRIPTION: Example of implementing distributed PyTorch training using the Training Operator Python SDK. The code demonstrates setting up a CNN model for FashionMNIST classification using PyTorch DDP (Distributed Data Parallel) across multiple workers.

LANGUAGE: python
CODE:
def train_func():
    import torch
    import torch.nn.functional as F
    from torch.utils.data import DistributedSampler
    from torchvision import datasets, transforms
    import torch.distributed as dist

    # [1] Setup PyTorch DDP. Distributed environment will be set automatically by Training Operator.
    dist.init_process_group(backend="nccl")
    Distributor = torch.nn.parallel.DistributedDataParallel
    local_rank = int(os.getenv("LOCAL_RANK", 0))
    print(
        "Distributed Training for WORLD_SIZE: {}, RANK: {}, LOCAL_RANK: {}".format(
            dist.get_world_size(),
            dist.get_rank(),
            local_rank,
        )
    )

    # [2] Create PyTorch CNN Model.
    class Net(torch.nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.conv1 = torch.nn.Conv2d(1, 20, 5, 1)
            self.conv2 = torch.nn.Conv2d(20, 50, 5, 1)
            self.fc1 = torch.nn.Linear(4 * 4 * 50, 500)
            self.fc2 = torch.nn.Linear(500, 10)

        def forward(self, x):
            x = F.relu(self.conv1(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv2(x))
            x = F.max_pool2d(x, 2, 2)
            x = x.view(-1, 4 * 4 * 50)
            x = F.relu(self.fc1(x))
            x = self.fc2(x)
            return F.log_softmax(x, dim=1)

    # [3] Attach model to the correct GPU device and distributor.
    device = torch.device(f"cuda:{local_rank}")
    model = Net().to(device)
    model = Distributor(model)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)

    # [4] Setup FashionMNIST dataloader and distribute data across PyTorchJob workers.
    dataset = datasets.FashionMNIST(
        "./data",
        download=True,
        train=True,
        transform=transforms.Compose([transforms.ToTensor()]),
    )
    train_loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=128,
        sampler=DistributedSampler(dataset),
    )

    # [5] Start model Training.
    for epoch in range(3):
        for batch_idx, (data, target) in enumerate(train_loader):
            # Attach Tensors to the device.
            data = data.to(device)
            target = target.to(device)

            optimizer.zero_grad()
            output = model(data)
            loss = F.nll_loss(output, target)
            loss.backward()
            optimizer.step()
            if batch_idx % 10 == 0 and dist.get_rank() == 0:
                print(
                    "Train Epoch: {} [{}/{} ({:.0f}%)]\tloss={:.4f}".format(
                        epoch,
                        batch_idx * len(data),
                        len(train_loader.dataset),
                        100.0 * batch_idx / len(train_loader),
                        loss.item(),
                    )
                )


from kubeflow.training import TrainingClient

# Start PyTorchJob with 3 Workers and 1 GPU per Worker (e.g. multi-node, multi-worker job).
TrainingClient().create_job(
    name="pytorch-ddp",
    train_func=train_func,
    num_procs_per_worker="auto",
    num_workers=3,
    resources_per_worker={"gpu": "1"},
)

----------------------------------------

TITLE: Using dsl.If, dsl.Elif, and dsl.Else for Multiple Conditional Branches in Kubeflow Pipelines
DESCRIPTION: This example shows how to use dsl.If, dsl.Elif, and dsl.Else together to create multiple conditional branches based on the output of a three-sided coin flip task.

LANGUAGE: python
CODE:
from kfp import dsl

@dsl.component
def flip_three_sided_coin() -> str:
    import random
    return random.choice(['heads', 'tails', 'draw'])

@dsl.component
def print_comp(text: str):
    print(text)

@dsl.pipeline
def my_pipeline():
    coin_flip_task = flip_three_sided_coin()
    with dsl.If(coin_flip_task.output == 'heads'):
        print_comp(text='Got heads!')
    with dsl.Elif(coin_flip_task.output == 'tails'):
        print_comp(text='Got tails!')
    with dsl.Else():
        print_comp(text='Draw!')

----------------------------------------

TITLE: Training Model Component with Traditional Artifact Syntax
DESCRIPTION: This component uses the traditional artifact syntax to train a model. It reads from an input Dataset artifact and writes to an output Model artifact, demonstrating how to set metadata on the output artifact.

LANGUAGE: python
CODE:
from kfp import dsl
from kfp.dsl import Dataset, Input, Model, Output

@dsl.component
def train_model(dataset: Input[Dataset], model: Output[Model]):
    with open(dataset.path) as f:
        dataset_lines = f.readlines()

    # train a model
    trained_model = ...
    
    trained_model.save(model.path)
    model.metadata['samples'] = len(dataset_lines)

----------------------------------------

TITLE: Kubeflow Pipeline Component Structure
DESCRIPTION: Components in Kubeflow Pipelines consist of client code (for endpoint communication) and runtime code (for actual task execution). Each component requires a YAML specification defining its metadata, interface, and implementation details. Components must be containerized as Docker images and handle data serialization for inter-component communication.

LANGUAGE: yaml
CODE:
metadata:
  name: component_name
  description: component_description
interface:
  inputs:
    - name: input_name
      type: input_type
      description: input_description
      default: default_value
  outputs:
    - name: output_name
      type: output_type
      description: output_description
implementation:
  container:
    image: docker_image
    command: execution_command

----------------------------------------

TITLE: Initializing KFP Client in Python
DESCRIPTION: Creates a Kubeflow Pipelines client instance for programmatic pipeline submission. Authentication may be required depending on the KFP instance configuration.

LANGUAGE: python
CODE:
import kfp

# TIP: you may need to authenticate with the KFP instance
kfp_client = kfp.Client()

----------------------------------------

TITLE: Using dsl.importer in Kubeflow Pipeline
DESCRIPTION: Demonstrates how to use the dsl.importer component to import external artifacts into a Kubeflow pipeline. The example shows importing a dataset from Google Cloud Storage, setting metadata with dynamic values from upstream tasks, and configuring reimport behavior.

LANGUAGE: python
CODE:
from kfp import dsl

@dsl.pipeline
def my_pipeline():
    task = get_date_string()
    importer_task = dsl.importer(
        artifact_uri='gs://ml-pipeline-playground/shakespeare1.txt',
        artifact_class=dsl.Dataset,
        reimport=True,
        metadata={'date': task.output})
    other_component(dataset=importer_task.output)

----------------------------------------

TITLE: Python Component Implementation Example
DESCRIPTION: Sample Python program that reads lines from an input file and writes them to an output file, demonstrating command-line argument handling and file I/O for a Kubeflow pipeline component

LANGUAGE: python
CODE:
#!/usr/bin/env python3
import argparse
from pathlib import Path

# Function doing the actual work (Outputs first N lines from a text file)
def do_work(input1_file, output1_file, param1):
  for x, line in enumerate(input1_file):
    if x >= param1:
      break
    _ = output1_file.write(line)
  
# Defining and parsing the command-line arguments
parser = argparse.ArgumentParser(description='My program description')
# Paths must be passed in, not hardcoded
parser.add_argument('--input1-path', type=str,
  help='Path of the local file containing the Input 1 data.')
parser.add_argument('--output1-path', type=str,
  help='Path of the local file where the Output 1 data should be written.')
parser.add_argument('--param1', type=int, default=100,
  help='The number of lines to read from the input and write to the output.')
args = parser.parse_args()

# Creating the directory where the output file is created (the directory
# may or may not exist).
Path(args.output1_path).parent.mkdir(parents=True, exist_ok=True)

with open(args.input1_path, 'r') as input1_file:
    with open(args.output1_path, 'w') as output1_file:
        do_work(input1_file, output1_file, args.param1)

----------------------------------------

TITLE: Complete Parameter Pipeline Example in Kubeflow
DESCRIPTION: Comprehensive example showing parameter usage across Python components, container components, and pipeline definitions. Demonstrates all supported parameter types including strings, integers, floats, booleans, dictionaries, and lists.

LANGUAGE: python
CODE:
from typing import Dict, List
from kfp import dsl

@dsl.component
def python_comp(
    string: str = 'hello',
    integer: int = 1,
    floating_pt: float = 0.1,
    boolean: bool = True,
    dictionary: Dict = {'key': 'value'},
    array: List = [1, 2, 3],
):
    print(string)
    print(integer)
    print(floating_pt)
    print(boolean)
    print(dictionary)
    print(array)


@dsl.container_component
def container_comp(
    string: str = 'hello',
    integer: int = 1,
    floating_pt: float = 0.1,
    boolean: bool = True,
    dictionary: Dict = {'key': 'value'},
    array: List = [1, 2, 3],
):
    return dsl.ContainerSpec(
        image='alpine',
        command=['sh', '-c', """echo $0 $1 $2 $3 $4 $5 $6"""],
        args=[
            string,
            integer,
            floating_pt,
            boolean,
            dictionary,
            array,
        ])

@dsl.pipeline
def my_pipeline(
    string: str = 'Hey!',
    integer: int = 100,
    floating_pt: float = 0.1,
    boolean: bool = False,
    dictionary: Dict = {'key': 'value'},
    array: List = [1, 2, 3],
):
    python_comp(
        string='howdy',
        integer=integer,
        array=[4, 5, 6],
    )
    container_comp(
        string=string,
        integer=20,
        dictionary={'other key': 'other val'},
        boolean=boolean,
    )

----------------------------------------

TITLE: Defining a TFJob Resource in YAML
DESCRIPTION: Example YAML configuration for a TFJob resource, specifying replicas, container images, and commands for parameter servers and workers.

LANGUAGE: YAML
CODE:
apiVersion: kubeflow.org/v1
kind: TFJob
metadata:
  generateName: tfjob
  namespace: your-user-namespace
spec:
  tfReplicaSpecs:
    PS:
      replicas: 1
      restartPolicy: OnFailure
      template:
        metadata:
          annotations:
            sidecar.istio.io/inject: "false"
        spec:
          containers:
            - name: tensorflow
              image: gcr.io/your-project/your-image
              command:
                - python
                - -m
                - trainer.task
                - --batch_size=32
                - --training_steps=1000
    Worker:
      replicas: 3
      restartPolicy: OnFailure
      template:
        metadata:
          annotations:
            sidecar.istio.io/inject: "false"
        spec:
          containers:
            - name: tensorflow
              image: gcr.io/your-project/your-image
              command:
                - python
                - -m
                - trainer.task
                - --batch_size=32
                - --training_steps=1000

----------------------------------------

TITLE: Compiling a Basic Pipeline in Python with KFP SDK
DESCRIPTION: This snippet demonstrates how to define a simple pipeline with a component, and compile it to YAML using the KFP SDK compiler. It creates a file called 'pipeline.yaml' containing the pipeline's Intermediate Representation (IR).

LANGUAGE: python
CODE:
from kfp import compiler, dsl

@dsl.component
def comp(message: str) -> str:
    print(message)
    return message

@dsl.pipeline
def my_pipeline(message: str) -> str:
    """My ML pipeline."""
    return comp(message=message).output

compiler.Compiler().compile(my_pipeline, package_path='pipeline.yaml')

----------------------------------------

TITLE: Pipeline Definition and Compilation
DESCRIPTION: Define and compile the complete pipeline with web download and CSV merge steps

LANGUAGE: python
CODE:
def my_pipeline(url):
  web_downloader_task = web_downloader_op(url=url)
  merge_csv_task = create_step_merge_csv(file=web_downloader_task.outputs['data'])

kfp.compiler.Compiler().compile(
    pipeline_func=my_pipeline,
    package_path='pipeline.yaml')

----------------------------------------

TITLE: Implementing Parallel Loops with dsl.ParallelFor in Kubeflow Pipelines
DESCRIPTION: This example shows how to use dsl.ParallelFor to execute tasks in parallel over a static set of items. The train_model task will run for 1, 5, 10, and 25 epochs, with a maximum of two training tasks running concurrently.

LANGUAGE: python
CODE:
from kfp import dsl

#@dsl.component
#def train_model(epochs: int) -> Model:
#    ...

@dsl.pipeline
def my_pipeline():
    with dsl.ParallelFor(
        items=[1, 5, 10, 25],
        parallelism=2
    ) as epochs:
        train_model(epochs=epochs)

----------------------------------------

TITLE: Using Special Input Types in Kubeflow Pipeline Components
DESCRIPTION: Example of using special input types like dsl.PIPELINE_JOB_NAME_PLACEHOLDER to pass metadata to a component within a Kubeflow pipeline definition.

LANGUAGE: python
CODE:
from kfp import dsl

@dsl.pipeline
def my_pipeline():
    print_op(text=dsl.PIPELINE_JOB_NAME_PLACEHOLDER)

----------------------------------------

TITLE: Implementing Exit Handling with dsl.ExitHandler in Kubeflow Pipelines
DESCRIPTION: This example shows how to use dsl.ExitHandler to specify a cleanup task that runs after the main pipeline tasks finish execution, even if one of those tasks fails. The clean_up_task will execute after both create_dataset and train_and_save_models finish.

LANGUAGE: python
CODE:
from kfp import dsl
from kfp.dsl import Dataset

#@dsl.component
#def clean_up_resources():
#    ...

#@dsl.component
#def create_datasets():
#    ...

#@dsl.component
#def train_and_save_models(dataset: Dataset):
#    ...

@dsl.pipeline
def my_pipeline():
    clean_up_task = clean_up_resources()
    with dsl.ExitHandler(exit_task=clean_up_task):
        dataset_task = create_datasets()
        train_task = train_and_save_models(dataset=dataset_task.output)

----------------------------------------

TITLE: Accessing Artifact Properties in a KFP Component
DESCRIPTION: This component demonstrates how to access various properties of an input artifact, including its name, URI, path, and metadata. It reads the contents of the input Dataset and returns the number of lines.

LANGUAGE: python
CODE:
from kfp import dsl
from kfp.dsl import Dataset
from kfp.dsl import Input

@dsl.component
def print_artifact_properties(dataset: Input[Dataset]):
    with open(dataset.path) as f:
        lines = f.readlines()
    
    print('Information about the artifact')
    print('Name:', dataset.name)
    print('URI:', dataset.uri)
    print('Path:', dataset.path)
    print('Metadata:', dataset.metadata)
    
    return len(lines)

----------------------------------------

TITLE: Defining a Basic Lightweight Python Component
DESCRIPTION: Shows the initial structure of a lightweight Python component for addition.

LANGUAGE: python
CODE:
from kfp import dsl

@dsl.component
def add(a: int, b: int) -> int:
    return a + b

----------------------------------------

TITLE: Documenting KFP Component with Docstrings
DESCRIPTION: Example of a KFP component using the KFP docstring style to document inputs and outputs. The component concatenates two datasets and demonstrates proper documentation of input datasets and output artifacts.

LANGUAGE: python
CODE:
@dsl.component
def join_datasets(
    dataset_a: Input[Dataset],
    dataset_b: Input[Dataset],
    out_dataset: Output[Dataset],
) -> str:
    """Concatenates two datasets.

    Args:
        dataset_a: First dataset.
        dataset_b: Second dataset.

    Returns:
        out_dataset: The concatenated dataset.
        Output: The concatenated string.
    """
    ...

----------------------------------------

TITLE: Implementing Basic DSL Recursion in Kubeflow Pipeline
DESCRIPTION: Demonstrates a basic recursive function implementation using the kfp.dsl.graph_component decorator. Shows how to create conditional logic, component dependencies, and integrate with pipeline functions.

LANGUAGE: python
CODE:
import kfp.dsl as dsl
@dsl.graph_component
def graph_component_a(input_x):
  with dsl.Condition(input_x == 'value_x'):
    op_a = task_factory_a(input_x)
    op_b = task_factory_b().after(op_a)
    graph_component_a(op_b.output)
    
@dsl.pipeline(
  name='pipeline',
  description='shows how to use the recursion.'
)
def pipeline():
  op_a = task_factory_a()
  op_b = task_factory_b()
  graph_op_a = graph_component_a(op_a.output)
  graph_op_a.after(op_b)
  task_factory_c(op_a.output).after(graph_op_a)

----------------------------------------

TITLE: Compiling KFP Pipeline from Python File in Shell
DESCRIPTION: This command compiles a Kubeflow Pipeline defined in a Python file to IR YAML format using the KFP CLI.

LANGUAGE: shell
CODE:
kfp dsl compile --py [PATH_TO_INPUT_PYTHON] --output [PATH_TO_OUTPUT_YAML] --function [PIPELINE_NAME]

----------------------------------------

TITLE: Deploying Kubeflow Pipelines with Kustomize
DESCRIPTION: Commands to deploy Kubeflow Pipelines using kustomize manifests. Includes setting up cluster-scoped resources, waiting for CRD establishment, and deploying development environment configuration.

LANGUAGE: bash
CODE:
export PIPELINE_VERSION={{% pipelines/latest-version %}}
kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION"
kubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io
kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref=$PIPELINE_VERSION"

----------------------------------------

TITLE: Command Line Usage Pattern for Pipeline Components
DESCRIPTION: Demonstrates the recommended command line pattern for passing input and output file paths to pipeline components. Shows the proper way to handle file paths rather than hardcoding them.

LANGUAGE: bash
CODE:
program.py --input-data <input path> --output-data <output path> --param 42

----------------------------------------

TITLE: SparkApplication Configuration for GCS and BigQuery Integration
DESCRIPTION: Complete YAML configuration for a SparkApplication that uses GCS and BigQuery connectors, including Hadoop configurations, secrets mounting, and container specifications.

LANGUAGE: yaml
CODE:
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: foo-gcs-bg
spec:
  type: Java
  mode: cluster
  image: gcr.io/ynli-k8s/spark:v2.3.0-gcs
  imagePullPolicy: Always
  hadoopConf:
    "fs.gs.project.id": "foo"
    "fs.gs.system.bucket": "foo-bucket"
    "google.cloud.auth.service.account.enable": "true"
    "google.cloud.auth.service.account.json.keyfile": "/mnt/secrets/key.json"
  driver:
    cores: 1
    secrets:
    - name: "gcs-bq"
      path: "/mnt/secrets"
      secretType: GCPServiceAccount
    envVars:
      GCS_PROJECT_ID: foo
    serviceAccount: spark
  executor:
    instances: 2
    cores: 1
    memory: "512m"
    secrets:
    - name: "gcs-bq"
      path: "/mnt/secrets"
      secretType: GCPServiceAccount
    envVars:
      GCS_PROJECT_ID: foo

----------------------------------------

TITLE: Retrieving Model Metadata
DESCRIPTION: Code for retrieving registered model metadata, including model versions and artifacts.

LANGUAGE: python
CODE:
model = registry.get_registered_model("iris")
print("Registered Model:", model, "with ID", model.id)

version = registry.get_model_version("iris", "v1")
print("Model Version:", version, "with ID", version.id)

art = registry.get_model_artifact("iris", "v1")
print("Model Artifact:", art, "with ID", art.id)

----------------------------------------

TITLE: Executing Pipeline with KFP Client
DESCRIPTION: Shows how to submit and execute the compiled pipeline using the KFP Client, specifying the endpoint and runtime arguments

LANGUAGE: python
CODE:
from kfp.client import Client

client = Client(host='<MY-KFP-ENDPOINT>')
run = client.create_run_from_pipeline_package(
    'pipeline.yaml',
    arguments={
        'recipient': 'World',
    },
)

----------------------------------------

TITLE: Configuring AWS Secret for S3 Pipeline Root in Python
DESCRIPTION: This code snippet demonstrates how to configure AWS credentials for using S3 as a pipeline root in Kubeflow Pipelines. It uses the add_op_transformer method to apply AWS secret configuration to all pipeline operations.

LANGUAGE: python
CODE:
dsl.get_pipeline_conf().add_op_transformer(aws.use_aws_secret('xxx', 'xxx', 'xxx'))

----------------------------------------

TITLE: Deploying PyTorchJob Resource for Training
DESCRIPTION: This command deploys a PyTorchJob resource to start a PyTorch training job on Kubernetes using a predefined configuration file.

LANGUAGE: bash
CODE:
kubectl create -f https://raw.githubusercontent.com/kubeflow/training-operator/refs/heads/release-1.9/examples/pytorch/simple.yaml

----------------------------------------

TITLE: Configuring KFP Launcher with Provider Overrides
DESCRIPTION: This YAML snippet provides a comprehensive example of configuring the KFP Launcher ConfigMap with provider overrides for both GCS and S3-compatible storage. It demonstrates how to specify different credentials and settings for various paths within a given PipelineRoot.

LANGUAGE: yaml
CODE:
gs:
  default:
    credentials:
      fromEnv: false
      secretRef:
        secretName: gs-secret-1
        tokenKey: gs-tokenKey
  overrides:
    # Matches pipeline root: gs://your-bucket/some/subfolder
    - bucketName: your-bucket
      keyPrefix: some/subfolder
      credentials:
        fromEnv: false
        secretRef:
          secretName: gcs-secret-2
          tokenKey: gs-tokenKey-2
    # Matches pipeline root: gs://your-bucket/some/othersubfolder
    - bucketName: your-bucket
      keyPrefix: some/othersubfolder
      credentials:
        fromEnv: true
s3:
  default:
    endpoint: http://some-s3-compliant-store-endpoint.com
    disableSSL: true
    region: minio
    credentials:
      fromEnv: false
      secretRef:
        secretName: your-secret
        accessKeyKey: accesskey
        secretKeyKey: secretkey
  overrides:
    # Matches pipeline root: s3://your-bucket/subfolder
    # aws-s3-creds secret is used for static credentials
    - bucketName: your-bucket
      keyPrefix: subfolder
      endpoint: s3.amazonaws.com
      region: us-east-2
      disableSSL: false
      credentials:
        fromEnv: false
        secretRef:
          secretName: aws-s3-creds
          accessKeyKey: AWS_ACCESS_KEY_ID
          secretKeyKey: AWS_SECRET_ACCESS_KEY
    # Matches pipeline root: s3://your-bucket/some/s3/path/a/b
    - bucketName: your-bucket
      keyPrefix: some/s3/path/a/b
      endpoint: s3.amazonaws.com
      region: us-east-2
      credentials:
        fromEnv: true
    # Matches pipeline root: s3://your-bucket/some/s3/path/a/c
    - bucketName: your-bucket
      keyPrefix: some/s3/path/a/c
      endpoint: s3.amazonaws.com
      region: us-east-2
      credentials:
        fromEnv: false
        secretRef:
          secretName: aws-s3-creds
          accessKeyKey: AWS_ACCESS_KEY_ID
          secretKeyKey: AWS_SECRET_ACCESS_KEY
    # Matches pipeline root: s3://your-bucket/some/s3/path/b/a
    - bucketName: your-bucket
      keyPrefix: some/s3/path/b/a
      endpoint: https://s3.amazonaws.com
      region: us-east-2
      credentials:
        fromEnv: false
        secretRef:
          secretName: aws-s3-creds
          accessKeyKey: AWS_ACCESS_KEY_ID
          secretKeyKey: AWS_SECRET_ACCESS_KEY

----------------------------------------

TITLE: Creating Distributed TensorFlow Training Job
DESCRIPTION: Example of creating a distributed TensorFlow training job using the Training Operator Python SDK. Uses a pre-built Docker image containing distributed TensorFlow MNIST training code.

LANGUAGE: python
CODE:
from kubeflow.training import TrainingClient

TrainingClient().create_job(
    name="tensorflow-dist",
    job_kind="TFJob",
    base_image="docker.io/kubeflow/tf-mnist-with-summaries:latest",
    num_workers=3,
)

----------------------------------------

TITLE: Deploying KServe Inference Service with Model Registry Metadata
DESCRIPTION: Example of creating a KServe inference endpoint using model metadata from the Model Registry.

LANGUAGE: python
CODE:
from kubernetes import client
import kserve

isvc = kserve.V1beta1InferenceService(
    api_version=kserve.constants.KSERVE_GROUP + "/v1beta1",
    kind=kserve.constants.KSERVE_KIND,
    metadata=client.V1ObjectMeta(
        name="iris-model",
        namespace=kserve.utils.get_default_target_namespace(),
        labels={
            "modelregistry/registered-model-id": model.id,
            "modelregistry/model-version-id": version.id,
        },
    ),
    spec=kserve.V1beta1InferenceServiceSpec(
        predictor=kserve.V1beta1PredictorSpec(
            model=kserve.V1beta1ModelSpec(
                storage_uri=art.uri,
                model_format=kserve.V1beta1ModelFormat(
                    name=art.model_format_name, version=art.model_format_version
                ),
            )
        )
    ),
)
ks_client = kserve.KServeClient()
ks_client.create(isvc)

----------------------------------------

TITLE: Monitoring JAXJob Logs
DESCRIPTION: Commands to fetch and follow logs from the main worker pod of the JAX training job.

LANGUAGE: bash
CODE:
PODNAME=$(kubectl get pods -l training.kubeflow.org/job-name=jaxjob-simple,training.kubeflow.org/replica-type=worker,training.kubeflow.org/replica-index=0 -o name -n kubeflow)
kubectl logs -f ${PODNAME} -n kubeflow

----------------------------------------

TITLE: Gathering Outputs from Mutually Exclusive Branches with dsl.OneOf in Kubeflow Pipelines
DESCRIPTION: This snippet demonstrates how to use dsl.OneOf to gather outputs from mutually exclusive conditional branches into a single task output, which can then be consumed by downstream tasks or returned from the pipeline.

LANGUAGE: python
CODE:
from kfp import dsl

@dsl.component
def flip_three_sided_coin() -> str:
    import random
    return random.choice(['heads', 'tails', 'draw'])

@dsl.component
def print_and_return(text: str) -> str:
    print(text)
    return text

@dsl.component
def announce_result(result: str):
    print(f'The result is: {result}')

@dsl.pipeline
def my_pipeline() -> str:
    coin_flip_task = flip_three_sided_coin()
    with dsl.If(coin_flip_task.output == 'heads'):
        t1 = print_and_return(text='Got heads!')
    with dsl.Elif(coin_flip_task.output == 'tails'):
        t2 = print_and_return(text='Got tails!')
    with dsl.Else():
        t3 = print_and_return(text='Draw!')
    
    oneof = dsl.OneOf(t1.output, t2.output, t3.output)
    announce_result(oneof)
    return oneof

----------------------------------------

TITLE: Valid Constant Definition in KFP Component
DESCRIPTION: Example of correctly defining and using a constant within a hermetic Lightweight Python Component function.

LANGUAGE: python
CODE:
@dsl.component
def double(a: int) -> int:
    """Succeeds at runtime."""
    VALID_CONSTANT = 2
    return VALID_CONSTANT * a

----------------------------------------

TITLE: Complete TFJob Gang Scheduling Example
DESCRIPTION: Full example of a TFJob configuration using gang-scheduling with worker and parameter server replicas

LANGUAGE: yaml
CODE:
apiVersion: "kubeflow.org/v1beta1"
kind: "TFJob"
metadata:
  name: "tfjob-gang-scheduling"
spec:
  tfReplicaSpecs:
    Worker:
      replicas: 1
      template:
        spec:
          containers:
            - args:
                - python
                - tf_cnn_benchmarks.py
                - --batch_size=32
                - --model=resnet50
                - --variable_update=parameter_server
                - --flush_stdout=true
                - --num_gpus=1
                - --local_parameter_device=cpu
                - --device=gpu
                - --data_format=NHWC
              image: gcr.io/kubeflow/tf-benchmarks-gpu:v20171202-bdab599-dirty-284af3
              name: tensorflow
              resources:
                limits:
                  nvidia.com/gpu: 1
              workingDir: /opt/tf-benchmarks/scripts/tf_cnn_benchmarks
          restartPolicy: OnFailure
    PS:
      replicas: 1
      template:
        spec:
          containers:
            - args:
                - python
                - tf_cnn_benchmarks.py
                - --batch_size=32
                - --model=resnet50
                - --variable_update=parameter_server
                - --flush_stdout=true
                - --num_gpus=1
                - --local_parameter_device=cpu
                - --device=cpu
                - --data_format=NHWC
              image: gcr.io/kubeflow/tf-benchmarks-cpu:v20171202-bdab599-dirty-284af3
              name: tensorflow
              resources:
                limits:
                  cpu: "1"
              workingDir: /opt/tf-benchmarks/scripts/tf_cnn_benchmarks
          restartPolicy: OnFailure

----------------------------------------

TITLE: Deploying PaddleJob Training Job
DESCRIPTION: Command to create a PaddleJob resource in Kubernetes using a predefined configuration file from the training-operator repository.

LANGUAGE: bash
CODE:
kubectl create -f https://raw.githubusercontent.com/kubeflow/training-operator/refs/heads/release-1.9/examples/paddlepaddle/simple-cpu.yaml

----------------------------------------

TITLE: Using dsl.Collected to Gather Outputs from Parallel Loops in Kubeflow Pipelines
DESCRIPTION: This snippet demonstrates how to use dsl.Collected with dsl.ParallelFor to gather outputs from a parallel loop of tasks. The max_accuracy task receives a list of models trained in the parallel loop and finds the one with the highest accuracy.

LANGUAGE: python
CODE:
from kfp import dsl
from kfp.dsl import Model, Input

#def score_model(model: Model) -> float:
#    return ...

#@dsl.component
#def train_model(epochs: int) -> Model:
#    ...

@dsl.component
def max_accuracy(models: Input[List[Model]]) -> float:
    return max(score_model(model) for model in models)

@dsl.pipeline
def my_pipeline():
    
    # Train a model for 1, 5, 10, and 25 epochs
    with dsl.ParallelFor(
        items=[1, 5, 10, 25],
    ) as epochs:
        train_model_task = train_model(epochs=epochs)
        
    # Find the model with the highest accuracy
    max_accuracy(
        models=dsl.Collected(train_model_task.outputs['model'])
    )

----------------------------------------

TITLE: Creating a Kubeflow Profile YAML Definition
DESCRIPTION: YAML definition for creating a new Kubeflow Profile. It specifies the profile name, owner, plugins, and optional resource quota.

LANGUAGE: yaml
CODE:
apiVersion: kubeflow.org/v1
kind: Profile
metadata:
  name: my-profile
spec:
  owner:
    kind: User
    name: admin@example.com
  plugins: []
  resourceQuotaSpec: {}

----------------------------------------

TITLE: Configuring PodDefault for GCP Credentials in Kubeflow
DESCRIPTION: YAML configuration for setting up a PodDefault resource that injects GCP credentials into notebook pods. This configuration mounts a GCP secret volume at /secret/gcp path when the pod label 'add-gcp-secret: true' is present.

LANGUAGE: yaml
CODE:
apiVersion: kubeflow.org/v1alpha1
kind: PodDefault
metadata:
  name: add-gcp-secret
  namespace: MY_PROFILE_NAMESPACE
spec:
 selector:
  matchLabels:
    add-gcp-secret: "true"
 desc: "add gcp credential"
 volumeMounts:
 - name: secret-volume
   mountPath: /secret/gcp
 volumes:
 - name: secret-volume
   secret:
    secretName: gcp-secret

----------------------------------------

TITLE: Defining a SparkApplication in YAML for Kubernetes
DESCRIPTION: This YAML snippet demonstrates the basic structure of a SparkApplication custom resource. It includes essential fields such as apiVersion, kind, metadata, and spec. The spec section defines the application type, deployment mode, container image, main class, and the path to the main application file.

LANGUAGE: yaml
CODE:
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: spark-pi
  namespace: default
spec:
  type: Scala
  mode: cluster
  image: spark:3.5.1
  mainClass: org.apache.spark.examples.SparkPi
  mainApplicationFile: local:///opt/spark/examples/jars/spark-examples_2.12-3.5.1.jar

----------------------------------------

TITLE: Python Component Type Definition
DESCRIPTION: Example of defining component types using Python decorators and type annotations, demonstrating both core and custom type usage.

LANGUAGE: python
CODE:
from kfp.dsl import component
from kfp.dsl.types import Integer, GCRPath


@component
def task_factory_a(field_l: Integer()) -> {
    'field_m': {
        'GCSPath': {
            'openapi_schema_validator':
                '{"type": "string", "pattern": "^gs://.*$"}'
        }
    },
    'field_n': 'customized_type',
    'field_o': GCRPath()
}:
  return ContainerOp(
      name='operator a',
      image='gcr.io/ml-pipeline/component-a',
      command=['python3', '/pipelines/component/src/train.py'],
      arguments=[
          '--field-l',
          field_l,
      ],
      file_outputs={
          'field_m': '/schema.txt',
          'field_n': '/feature.txt',
          'field_o': '/output.txt'
      })

----------------------------------------

TITLE: Katib Runtime Configuration
DESCRIPTION: YAML configuration for Katib runtime parameters including metrics collectors, suggestions and early stopping settings.

LANGUAGE: yaml
CODE:
apiVersion: config.kubeflow.org/v1beta1
kind: KatibConfig
runtime:
  metricsCollectors:
    - kind: StdOut
      image: docker.io/kubeflowkatib/file-metrics-collector:latest
    ...
  suggestions:
    - algorithmName: random
      image: docker.io/kubeflowkatib/suggestion-hyperopt:latest
    ...
  earlyStoppings:
    - algorithmName: medianstop
      image: docker.io/kubeflowkatib/earlystopping-medianstop:latest
    ...

----------------------------------------

TITLE: Deploying Katib Experiment using kubectl
DESCRIPTION: Shell command to deploy a Katib Experiment using kubectl. This command creates an experiment from a YAML file hosted on GitHub.

LANGUAGE: shell
CODE:
kubectl create -f https://raw.githubusercontent.com/kubeflow/katib/master/examples/v1beta1/hp-tuning/random.yaml

----------------------------------------

TITLE: KFP Component with Multiple Artifact Outputs using Pythonic Syntax
DESCRIPTION: This component demonstrates how to return multiple artifact outputs using the Pythonic artifact syntax in KFP. It trains two models and returns them as named outputs using a NamedTuple.

LANGUAGE: python
CODE:
from kfp import dsl
from kfp.dsl import Dataset, Model
from typing import NamedTuple

@dsl.component
def train_multiple_models(
    dataset: Dataset,
) -> NamedTuple('outputs', model1=Model, model2=Model):
    with open(dataset.path) as f:
        dataset_lines = f.readlines()

    # train a model
    trained_model1 = ...
    trained_model2 = ...
    
    model_artifact1 = Model(uri=dsl.get_uri(suffix='model1'), metadata={'samples': len(dataset_lines)})
    trained_model1.save(model_artifact1.path)
    
    model_artifact2 = Model(uri=dsl.get_uri(suffix='model2'), metadata={'samples': len(dataset_lines)})
    trained_model2.save(model_artifact2.path)
    
    outputs = NamedTuple('outputs', model1=Model, model2=Model)
    return outputs(model1=model_artifact1, model2=model_artifact2)

----------------------------------------

TITLE: Generating Confusion Matrix Visualization with v2 SDK
DESCRIPTION: Demonstrates how to output a confusion matrix visualization using the v2 SDK ClassificationMetrics output type.

LANGUAGE: Python
CODE:
@component(
    packages_to_install=['sklearn'],
    base_image='python:3.9'
)
def iris_sgdclassifier(test_samples_fraction: float, metrics: Output[ClassificationMetrics]):
    from sklearn import datasets, model_selection
    from sklearn.linear_model import SGDClassifier
    from sklearn.metrics import confusion_matrix

    iris_dataset = datasets.load_iris()
    train_x, test_x, train_y, test_y = model_selection.train_test_split(
        iris_dataset['data'], iris_dataset['target'], test_size=test_samples_fraction)


    classifier = SGDClassifier()
    classifier.fit(train_x, train_y)
    predictions = model_selection.cross_val_predict(classifier, train_x, train_y, cv=3)
    metrics.log_confusion_matrix(
        ['Setosa', 'Versicolour', 'Virginica'],
        confusion_matrix(train_y, predictions).tolist() # .tolist() to convert np array to list.
    )

@dsl.pipeline(
    name='metrics-visualization-pipeline')
def metrics_visualization_pipeline():
    iris_sgdclassifier_op = iris_sgdclassifier(test_samples_fraction=0.3)

----------------------------------------

TITLE: Basic SparkApplication Example
DESCRIPTION: Example showing basic SparkApplication configuration with essential fields like API version, type, mode, image and main class.

LANGUAGE: yaml
CODE:
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: spark-pi
  namespace: default
spec:
  type: Scala
  mode: cluster
  image: spark:3.5.1
  mainClass: org.apache.spark.examples.SparkPi
  mainApplicationFile: local:///opt/spark/examples/jars/spark-examples_2.12-3.5.1.jar

----------------------------------------

TITLE: Installing Katib with Cert Manager
DESCRIPTION: This command deploys Katib with Cert Manager integration for provisioning webhook certificates. Cert Manager must be deployed on the Kubernetes cluster before using this installation option.

LANGUAGE: shell
CODE:
kubectl apply -k "github.com/kubeflow/katib.git/manifests/v1beta1/installs/katib-cert-manager?ref=master"

----------------------------------------

TITLE: Driver and Executor Specifications
DESCRIPTION: Examples showing how to configure driver and executor pods with resource requests, labels and service accounts.

LANGUAGE: yaml
CODE:
spec:
  driver:
    cores: 1
    coreLimit: 200m
    memory: 512m
    labels:
      version: 3.1.1
    serviceAccount: spark
  executor:
    cores: 1
    instances: 1
    memory: 512m
    labels:
      version: 3.1.1
    serviceAccount: spark

----------------------------------------

TITLE: Installing Katib with External Database
DESCRIPTION: This command deploys Katib with support for a custom database backend. It allows using a custom instance of MySQL DB instead of the default katib-mysql. Users need to modify environment variables for katib-db-manager in the secrets.env file with their MySQL DB values.

LANGUAGE: shell
CODE:
kubectl apply -k "github.com/kubeflow/katib.git/manifests/v1beta1/installs/katib-external-db?ref=master"

----------------------------------------

TITLE: Output Path Handling in Pipeline Component
DESCRIPTION: Demonstrates output path specification and handling in component definitions and pipeline execution.

LANGUAGE: yaml
CODE:
command: [program.py, --out-model, {outputPath: trained_model}]

LANGUAGE: python
CODE:
task1 = component1()
# You can now pass `task1.outputs['trained_model']` to other components as argument.

LANGUAGE: shell
CODE:
program.py --out-model /outputs/trained_model/data

----------------------------------------

TITLE: Running Kubeflow Pipelines Benchmark Script in Jupyter Notebook
DESCRIPTION: This snippet demonstrates how to set up and run a benchmark script for Kubeflow Pipelines in a Jupyter notebook environment. It includes creating a pipeline, running multiple instances, and recording performance metrics.

LANGUAGE: Python
CODE:
# Set parameters
host = "<your_kfp_api_server_url>"
pipeline_file_url = "https://storage.googleapis.com/ml-pipeline/sample-benchmark/taxi_updated_pool.yaml"
num_runs = 50
run_status_polling_interval_sec = 10

# Run benchmark script
# (Actual benchmark code not provided in the original text)
# This would typically include:
# 1. Creating a new pipeline
# 2. Creating multiple runs of the pipeline
# 3. Recording success rates, run durations, and API latencies
# 4. Cleaning up created resources

----------------------------------------

TITLE: Configuring Spark Operator for Batch Scheduling with YuniKorn
DESCRIPTION: YAML configuration for enabling batch scheduling in the Spark operator controller. It allows setting YuniKorn as the default batch scheduler for all SparkApplication definitions.

LANGUAGE: yaml
CODE:
controller:
  batchScheduler:
    enable: true
    # Setting the default batch scheduler is optional. The default only
    # applies if the batchScheduler field on the SparkApplication spec is not set
    default: yunikorn

----------------------------------------

TITLE: Installing Katib with PostgreSQL Database
DESCRIPTION: This command deploys Katib using PostgreSQL as the database backend instead of MySQL. It provides an alternative database option for Katib installations.

LANGUAGE: shell
CODE:
kubectl apply -k "github.com/kubeflow/katib.git/manifests/v1beta1/installs/katib-standalone-postgres?ref=master"

----------------------------------------

TITLE: Configuring Kubeflow Pipeline with Decorator Arguments
DESCRIPTION: Example of using the @dsl.pipeline decorator arguments to provide metadata for a Kubeflow pipeline, including name, description, pipeline root, and display name.

LANGUAGE: python
CODE:
@dsl.pipeline(name='pythagorean-theorem-pipeline',
              description='Solve for the length of a hypotenuse of a triangle with sides length `a` and `b`.',
              pipeline_root='gs://my-pipelines-bucket',
              display_name='Pythagorean pipeline.')
def pythagorean(a: float, b: float) -> float:
    ...

----------------------------------------

TITLE: Disabling Component Caching in Python Pipeline
DESCRIPTION: Example showing how to create a simple pipeline with a component where caching is explicitly disabled using set_caching_options()

LANGUAGE: python
CODE:
from kfp import dsl

@dsl.component
def say_hello(name: str) -> str:
    hello_text = f'Hello, {name}!'
    print(hello_text)
    return hello_text

@dsl.pipeline
def hello_pipeline(recipient: str = 'World!') -> str:
    hello_task = say_hello(name=recipient)
    hello_task.set_caching_options(False)
    return hello_task.output

----------------------------------------

TITLE: Migrating from create_component_from_func to @dsl.component decorator
DESCRIPTION: Shows how to migrate from using create_component_from_func and func_to_container_op in v1 to the @dsl.component decorator in v2 for creating lightweight Python components.

LANGUAGE: Python
CODE:
from kfp import dsl

@dsl.component
def component1(...):
    ...

@dsl.component
def component2(...):
    ...

@dsl.component
def component3(...):
    ...

@dsl.pipeline(name='my-pipeline')
def pipeline():
    component1(...)
    component2(...)
    component3(...)

----------------------------------------

TITLE: Installing kind on Windows via PowerShell
DESCRIPTION: Downloads and installs kind binary on Windows systems using PowerShell commands.

LANGUAGE: powershell
CODE:
curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/{KIND_VERSION}/kind-windows-amd64
Move-Item .\kind-windows-amd64.exe c:\{YOUR_KIND_DIRECTORY}\kind.exe

----------------------------------------

TITLE: Defining a KFP Pipeline with Artifact Input and Output
DESCRIPTION: This code snippet shows how to define a Kubeflow Pipeline that takes a Dataset artifact as input and returns a Model artifact as output. It demonstrates chaining components that use artifacts.

LANGUAGE: python
CODE:
from kfp import dsl
from kfp.dsl import Dataset, Model

@dsl.pipeline
def augment_and_train(dataset: Dataset) -> Model:
    augment_task = augment_dataset(dataset=dataset)
    return train_model(dataset=augment_task.output).output

----------------------------------------

TITLE: Creating an AuthorizationPolicy for Profile Contributor Access
DESCRIPTION: YAML definition for creating an AuthorizationPolicy to grant a user access to a Kubeflow Profile. It specifies the user's email, role, profile name, and necessary service account principals.

LANGUAGE: yaml
CODE:
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: user-<SAFE_USER_EMAIL>-clusterrole-<USER_ROLE>
  namespace: <PROFILE_NAME>
  annotations:
    role: <USER_ROLE>
    user: <RAW_USER_EMAIL>
spec:
  rules:
    - from:
        - source:
            principals:
              - "cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account"
              - "cluster.local/ns/kubeflow/sa/ml-pipeline-ui"
      when:
        - key: request.headers[kubeflow-userid]
          values:
            - <RAW_USER_EMAIL>

----------------------------------------

TITLE: Uninstalling Kubeflow Pipelines
DESCRIPTION: These commands uninstall Kubeflow Pipelines by deleting the deployed kustomize manifests.

LANGUAGE: bash
CODE:
export PIPELINE_VERSION={{% pipelines/latest-version %}}
kubectl delete -k "github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref=$PIPELINE_VERSION"
kubectl delete -k "github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION"

----------------------------------------

TITLE: Component YAML Type Definition
DESCRIPTION: Example of defining component types in YAML format, showing input/output type specifications including core and custom types.

LANGUAGE: yaml
CODE:
name: component a
description: component desc
inputs:
  - {name: field_l, type: Integer}
outputs:
  - {name: field_m, type: {GCSPath: {openapi_schema_validator: {type: string, pattern: "^gs://.*$" } }}}
  - {name: field_n, type: customized_type}
  - {name: field_o, type: GCRPath} 
implementation:
  container:
    image: gcr.io/ml-pipeline/component-a
    command: [python3, /pipelines/component/src/train.py]
    args: [
      --field-l, {inputValue: field_l},
    ]
    fileOutputs: 
      field_m: /schema.txt
      field_n: /feature.txt
      field_o: /output.txt

----------------------------------------

TITLE: Compiling KFP Pipeline with Parameters in Shell
DESCRIPTION: This command compiles a Kubeflow Pipeline with specified parameters using the KFP CLI, demonstrating how to provide pipeline parameters as JSON.

LANGUAGE: shell
CODE:
kfp dsl compile [PATH_TO_INPUT_PYTHON] --output [PATH_TO_OUTPUT_YAML] --pipeline-parameters [PIPELINE_PARAMETERS_JSON]

----------------------------------------

TITLE: DARTS Best Genotype Representation
DESCRIPTION: Example of the Best Genotype representation in DARTS, showing the structure of normal and reduction cells in the neural network. Each cell contains nodes with operations and connections.

LANGUAGE: python
CODE:
Genotype(
  normal=[
      [('max_pooling_3x3',0),('max_pooling_3x3',1)],
      [('max_pooling_3x3',0),('max_pooling_3x3',1)],
      [('max_pooling_3x3',0),('dilated_convolution_3x3',3)],
      [('max_pooling_3x3',0),('max_pooling_3x3',1)]
    ],
    normal_concat=range(2,6),
  reduce=[
      [('dilated_convolution_5x5',1),('separable_convolution_3x3',0)],
      [('max_pooling_3x3',2),('dilated_convolution_5x5',1)],
      [('dilated_convolution_5x5',3),('dilated_convolution_5x5',2)],
      [('dilated_convolution_5x5',3),('dilated_convolution_5x5',4)]
    ],
    reduce_concat=range(2,6)
)

----------------------------------------

TITLE: Configuring SparkApplication with Volcano Scheduler
DESCRIPTION: YAML configuration for a Spark application using Volcano as batch scheduler. Demonstrates essential configurations including batch scheduler specification, resource allocations, and volume mounts.

LANGUAGE: yaml
CODE:
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: spark-pi
  namespace: default
spec:
  type: Scala
  mode: cluster
  image: spark:3.5.1
  imagePullPolicy: Always
  mainClass: org.apache.spark.examples.SparkPi
  mainApplicationFile: local:///opt/spark/examples/jars/spark-examples_2.12-v3.5.1.jar
  sparkVersion: 3.5.1
  batchScheduler: volcano
  restartPolicy:
    type: Never
  volumes:
    - name: test-volume
      hostPath:
        path: /tmp
        type: Directory
  driver:
    cores: 1
    coreLimit: 1200m
    memory: 512m
    labels:
      version: 3.5.1
    serviceAccount: spark
    volumeMounts:
      - name: test-volume
        mountPath: /tmp
  executor:
    cores: 1
    instances: 1
    memory: 512m
    labels:
      version: 3.5.1
    volumeMounts:
      - name: test-volume
        mountPath: "/tmp"

----------------------------------------

TITLE: Creating a Local Kubernetes Cluster with Kind
DESCRIPTION: Command to quickly create a local Kubernetes cluster using Kind. This step is optional and can be used if you don't already have a Kubernetes cluster.

LANGUAGE: bash
CODE:
kind create cluster # or minikube start

----------------------------------------

TITLE: Triggering Pipeline Run
DESCRIPTION: API call to create a new pipeline run using the uploaded pipeline ID

LANGUAGE: bash
CODE:
RUN_ID=$((curl -H "Content-Type: application/json" -X POST ${SVC}/apis/v1beta1/runs \
-d @- << EOF
{
   "name":"${PIPELINE_NAME}_run",
   "pipeline_spec":{
      "pipeline_id":"${PIPELINE_ID}"
   }
}
EOF
) | jq -r .run.id)

----------------------------------------

TITLE: Submitting Pipeline from YAML using KFP Client
DESCRIPTION: Demonstrates how to submit a pipeline run from a compiled IR YAML file using the KFP client's create_run_from_pipeline_package method. Includes example pipeline arguments.

LANGUAGE: python
CODE:
kfp_client.create_run_from_pipeline_package(
    "./add-pipeline.yaml", 
    arguments={
        "a": 1,
        "b": 2,
    }
)

----------------------------------------

TITLE: Running E2E Tests
DESCRIPTION: Set up a kind cluster, build and load the Docker image, run E2E tests, and clean up.

LANGUAGE: shell
CODE:
# Create a kind cluster
make kind-create-cluster

# Build docker image
make docker-build IMAGE_TAG=local

# Load docker image to kind cluster
make kind-load-image

# Run e2e tests
make e2e-test

# Delete the kind cluster
make kind-delete-cluster

----------------------------------------

TITLE: Adding Documentation Links to Kubeflow Dashboard ConfigMap
DESCRIPTION: This YAML snippet shows how to add a documentation link to the Kubeflow website in the Central Dashboard ConfigMap. It includes the required fields such as text, desc, and link.

LANGUAGE: yaml
CODE:
apiVersion: v1
kind: ConfigMap
metadata:
  name: centraldashboard-config
  namespace: kubeflow
data:
  settings: |-
    ...
  links: |-
    {
      "menuLinks": [
        ...
      ],
      "externalLinks": [
        ...
      ],
      "quickLinks": [
        ...
      ],
      "documentationItems": [
        {
          "text": "Kubeflow Website",
          "desc": "Kubeflow website documentation",
          "link": "https://www.kubeflow.org/docs/"
        }
      ]
    }

----------------------------------------

TITLE: Generating API Documentation
DESCRIPTION: Build and update the API specification documentation.

LANGUAGE: shell
CODE:
make build-api-docs

----------------------------------------

TITLE: Configuring AWS S3 with Static Credentials for KFP Launcher
DESCRIPTION: This YAML snippet shows how to configure the KFP Launcher ConfigMap to use an AWS S3 bucket with static credentials. It includes settings for the S3 endpoint, region, and references to a Kubernetes secret for access keys.

LANGUAGE: yaml
CODE:
apiVersion: v1
data:
  defaultPipelineRoot: s3://mlpipeline
  providers: |-
    s3:
      default:
        endpoint: s3.amazonaws.com
        disableSSL: false
        region: us-east-2
        credentials:
          fromEnv: false
          secretRef:
            secretName: your-k8s-secret
            accessKeyKey: some-key-1
            secretKeyKey: some-key-2
kind: ConfigMap
metadata:
  name: kfp-launcher
  namespace: user-namespace

----------------------------------------

TITLE: Migrating from ContainerOp to @dsl.container_component
DESCRIPTION: Shows how to migrate from using ContainerOp in v1 to the @dsl.container_component decorator in v2 for creating container components.

LANGUAGE: Python
CODE:
from kfp import dsl

@dsl.container_component
def flip_coin(rand: int, result: dsl.OutputPath(str)):
  return ContainerSpec(
    image='gcr.io/flip-image'
    command=['flip'],
    arguments=['--seed', rand, '--result-file', result])

----------------------------------------

TITLE: Visualizing Kubeflow Notebook Image Relationships with Mermaid
DESCRIPTION: This Mermaid graph shows the hierarchical relationships between different Kubeflow Notebook container images, including base images and specialized variants for different frameworks and hardware.

LANGUAGE: mermaid
CODE:
%%{init: {'theme':'forest'}}%%
graph TD
  Base[Base] --> Jupyter[Jupyter]
  Base --> Code-Server[code-server]
  Base --> RStudio[RStudio]
  
  Jupyter --> PyTorch[PyTorch]
  Jupyter --> SciPy[SciPy]
  Jupyter --> TensorFlow[TensorFlow]
  
  Code-Server --> Code-Server-Conda-Python[Conda Python]
  RStudio --> Tidyverse[Tidyverse]

  PyTorch --> PyTorchFull[PyTorch Full]
  TensorFlow --> TensorFlowFull[TensorFlow Full]

  Jupyter --> PyTorchCuda[PyTorch CUDA]
  Jupyter --> TensorFlowCuda[TensorFlow CUDA]
  Jupyter --> PyTorchGaudi[PyTorch Gaudi]

  PyTorchCuda --> PyTorchCudaFull[PyTorch CUDA Full]
  TensorFlowCuda --> TensorFlowCudaFull[TensorFlow CUDA Full]
  PyTorchGaudi --> PyTorchGaudiFull[PyTorch Gaudi Full]

  click Base "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/base"
  click Jupyter "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter"
  click Code-Server "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/codeserver"
  click RStudio "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/rstudio"
  click PyTorch "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch"
  click SciPy "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-scipy"
  click TensorFlow "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-tensorflow"
  click Code-Server-Conda-Python "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/codeserver-python"
  click Tidyverse "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/rstudio-tidyverse"
  click PyTorchFull "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-full"
  click TensorFlowFull "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-tensorflow-full"
  click PyTorchCuda "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-cuda"
  click TensorFlowCuda "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-tensorflow-cuda"
  click PyTorchCudaFull "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-cuda-full"
  click TensorFlowCudaFull "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-tensorflow-cuda-full"
  click PyTorchGaudi "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-gaudi"
  click PyTorchGaudiFull "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-gaudi-full"

----------------------------------------

TITLE: PostgreSQL Connection String Format
DESCRIPTION: Data source name format used by DB Manager to connect to PostgreSQL database when DB_NAME=postgres.

LANGUAGE: text
CODE:
postgresql://[DB_USER[:DB_PASSWORD]@][KATIB_POSTGRESQL_DB_HOST][:KATIB_POSTGRESQL_DB_PORT][/KATIB_POSTGRESQL_DB_DATABASE]

----------------------------------------

TITLE: Creating Kubernetes Job with ResourceOp
DESCRIPTION: Example of using ResourceOp to create a Kubernetes Job and extract its metadata name as an output parameter.

LANGUAGE: python
CODE:
job = kubernetes_client.V1Job(...)

rop = kfp.dsl.ResourceOp(
    name="create-job",
    k8s_resource=job,
    action="create",
    attribute_outputs={"name": "{.metadata.name}"}
)

----------------------------------------

TITLE: Configuring GPU Usage in TFJob
DESCRIPTION: YAML snippet showing how to specify GPU resources for a TFJob worker.

LANGUAGE: YAML
CODE:
apiVersion: "kubeflow.org/v1"
kind: "TFJob"
metadata:
  name: "tf-smoke-gpu"
spec:
  tfReplicaSpecs:
    Worker:
      replicas: 1
      template:
        spec:
          containers:
            - name: tensorflow
              resources:
                limits:
                  nvidia.com/gpu: 1

----------------------------------------

TITLE: Installing Kubeflow Python SDK
DESCRIPTION: Installs the latest version of the Kubeflow Python SDK directly from the source repository.

LANGUAGE: bash
CODE:
pip install git+https://github.com/kubeflow/trainer.git@master#subdirectory=sdk

----------------------------------------

TITLE: Building Containerized Python Component in KFP CLI
DESCRIPTION: This command builds a containerized Python component for Kubeflow Pipelines using the KFP CLI, specifying the component directory and options.

LANGUAGE: shell
CODE:
kfp component build [OPTIONS] [COMPONENTS_DIRECTORY] [ARGS]...

----------------------------------------

TITLE: Deploying KServe Inference Service with Custom Storage Initializer
DESCRIPTION: Example of creating a KServe inference endpoint using the Model Registry Custom Storage Initializer.

LANGUAGE: python
CODE:
from kubernetes import client
import kserve

isvc = kserve.V1beta1InferenceService(
    api_version=kserve.constants.KSERVE_GROUP + "/v1beta1",
    kind=kserve.constants.KSERVE_KIND,
    metadata=client.V1ObjectMeta(
        name="iris-model",
        namespace=kserve.utils.get_default_target_namespace(),
        labels={
            "modelregistry/registered-model-id": model.id,
            "modelregistry/model-version-id": version.id,
        },
    ),
    spec=kserve.V1beta1InferenceServiceSpec(
        predictor=kserve.V1beta1PredictorSpec(
            model=kserve.V1beta1ModelSpec(
                storage_uri="model-registry://iris/v1",
                model_format=kserve.V1beta1ModelFormat(
                    name=art.model_format_name, version=art.model_format_version
                ),
            )
        )
    ),
)
ks_client = kserve.KServeClient()
ks_client.create(isvc)

----------------------------------------

TITLE: Installing Stable Release of Training Operator Control Plane
DESCRIPTION: Installs the stable release (v1.8.1) of the Training Operator control plane using kubectl. This command applies the Kubernetes manifests from the specified GitHub repository.

LANGUAGE: shell
CODE:
kubectl apply --server-side -k "github.com/kubeflow/training-operator.git/manifests/overlays/standalone?ref=v1.8.1"

----------------------------------------

TITLE: Manually Deleting a Notebook in Kubeflow using kubectl
DESCRIPTION: This command manually deletes a Notebook resource from the Kubeflow cluster, useful when the notebook cannot be deleted through the usual interface.

LANGUAGE: shell
CODE:
kubectl delete notebook "${MY_NOTEBOOK_NAME}" --namespace "${MY_PROFILE_NAMESPACE}"

----------------------------------------

TITLE: Generating Scalar Metrics Visualization with v2 SDK
DESCRIPTION: Demonstrates how to output scalar metrics using the v2 SDK Metrics output type.

LANGUAGE: Python
CODE:
@component(
    packages_to_install=['sklearn'],
    base_image='python:3.9',
)
def digit_classification(metrics: Output[Metrics]):
    from sklearn import model_selection
    from sklearn.linear_model import LogisticRegression
    from sklearn import datasets
    from sklearn.metrics import accuracy_score

    # Load digits dataset
    iris = datasets.load_iris()

    # # Create feature matrix
    X = iris.data

    # Create target vector
    y = iris.target

    #test size
    test_size = 0.33

    seed = 7
    #cross-validation settings
    kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)

    #Model instance
    model = LogisticRegression()
    scoring = 'accuracy'
    results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)

    #split data
    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size, random_state=seed)
    #fit model
    model.fit(X_train, y_train)

    #accuracy on test set
    result = model.score(X_test, y_test)
    metrics.log_metric('accuracy', (result*100.0))

@dsl.pipeline(
    name='metrics-visualization-pipeline')
def metrics_visualization_pipeline():
    digit_classification_op = digit_classification()

----------------------------------------

TITLE: MPIJob Scheduling Configuration
DESCRIPTION: YAML configuration showing scheduling policy options for MPIJob including gang-scheduling parameters.

LANGUAGE: yaml
CODE:
apiVersion: kubeflow.org/v2beta1
kind: MPIJob
metadata:
  name: tensorflow-benchmarks
spec:
  slotsPerWorker: 1
  runPolicy:
    cleanPodPolicy: Running
+   schedulingPolicy:
+     minAvailable: 10
+     queue: test-queue
+     minResources:
+       cpu: 3000m
+     priorityClass: high
+     scheduleTimeoutSeconds: 180
  mpiReplicaSpecs:
...

----------------------------------------

TITLE: Installing Latest Katib Control Plane
DESCRIPTION: Command to install the latest master branch version of Katib control plane

LANGUAGE: shell
CODE:
kubectl apply -k "github.com/kubeflow/katib.git/manifests/v1beta1/installs/katib-standalone?ref=master"

----------------------------------------

TITLE: Listing XGBoost Training Pods
DESCRIPTION: Command to view the pods created for the XGBoost training job.

LANGUAGE: bash
CODE:
kubectl get pods -l job-name=xgboost-dist-iris-test-train

----------------------------------------

TITLE: Configuring Google Cloud Storage with Static Credentials for KFP Launcher
DESCRIPTION: This YAML snippet demonstrates how to configure the KFP Launcher ConfigMap to use Google Cloud Storage (GCS) with static credentials. It references a Kubernetes secret for the GCS App Credentials file.

LANGUAGE: yaml
CODE:
apiVersion: v1
data:
  defaultPipelineRoot: gs://mlpipeline
  providers: |-
    gs:
      default:
        credentials:
          fromEnv: false
          secretRef:
            secretName: your-k8s-secret
            tokenKey: some-key-1
kind: ConfigMap
metadata:
  name: kfp-launcher
  namespace: user-namespace

----------------------------------------

TITLE: TFJob Configuration with Custom Scheduler
DESCRIPTION: Example of a TFJob configuration with scheduler-plugins specified as the scheduler

LANGUAGE: diff
CODE:
apiVersion: "kubeflow.org/v1"
kind: TFJob
metadata:
  name: tfjob-simple
  namespace: kubeflow
spec:
  tfReplicaSpecs:
    Worker:
      replicas: 2
      restartPolicy: OnFailure
      template:
        spec:
+         schedulerName: scheduler-plugins-scheduler
          containers:
            - name: tensorflow
              image: kubeflow/tf-mnist-with-summaries:latest
              command:
                - "python"
                - "/var/tf_mnist/mnist_with_summaries.py"

----------------------------------------

TITLE: Installing Standalone Model Registry
DESCRIPTION: Commands to install a specific version of Model Registry standalone using kubectl and Kustomize.

LANGUAGE: shell
CODE:
MODEL_REGISTRY_VERSION={{% model-registry/latest-version %}}
kubectl apply -k "https://github.com/kubeflow/model-registry/manifests/kustomize/overlays/db?ref=v${MODEL_REGISTRY_VERSION}"

----------------------------------------

TITLE: Getting JAXJob Status
DESCRIPTION: Command to retrieve the full YAML configuration and status of a JAX training job.

LANGUAGE: bash
CODE:
kubectl get -o yaml jaxjobs jaxjob-simple -n kubeflow

----------------------------------------

TITLE: Integrating Containerized Component in Pipeline
DESCRIPTION: Demonstrates how to use the containerized component in a Kubeflow pipeline definition.

LANGUAGE: python
CODE:
# pipeline.py
from kfp import compiler, dsl
from src.my_component import add

@dsl.pipeline
def addition_pipeline(x: int, y: int) -> int:
    task1 = add(a=x, b=y)
    task2 = add(a=task1.output, b=x)
    return task2.output

compiler.Compiler().compile(addition_pipeline, 'pipeline.yaml')

----------------------------------------

TITLE: Accessing Kubeflow Pipelines UI URL
DESCRIPTION: This command retrieves the public URL for the Kubeflow Pipelines UI from the inverse-proxy-config ConfigMap.

LANGUAGE: bash
CODE:
kubectl describe configmap inverse-proxy-config -n kubeflow | grep googleusercontent.com

----------------------------------------

TITLE: Port Forwarding Katib UI Service
DESCRIPTION: Command to set up port forwarding for accessing the Katib UI service on localhost port 8080.

LANGUAGE: shell
CODE:
kubectl port-forward svc/katib-ui -n kubeflow 8080:80

----------------------------------------

TITLE: Listing Available Kubeflow Training Runtimes
DESCRIPTION: Lists the available Kubeflow Training Runtimes using the TrainerClient.

LANGUAGE: python
CODE:
from kubeflow.trainer import TrainerClient, CustomTrainer

for r in TrainerClient().list_runtimes():
    print(f"Runtime: {r.name}")

----------------------------------------

TITLE: Checking TrainJob Components and Device Usage
DESCRIPTION: Retrieves information about the components of the TrainJob and the number of devices each PyTorch node is using.

LANGUAGE: python
CODE:
for c in TrainerClient().get_job(name=job_id).components:
    print(f"Component: {c.name}, Status: {c.status}, Devices: {c.device} x {c.device_count}")

----------------------------------------

TITLE: Installing Kubeflow Pipelines
DESCRIPTION: Deploys Kubeflow Pipelines on a Kubernetes cluster using kubectl and kustomize manifests.

LANGUAGE: shell
CODE:
export PIPELINE_VERSION={{% pipelines/latest-version %}}
kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION"
kubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io
kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic?ref=$PIPELINE_VERSION"

----------------------------------------

TITLE: Adding In-Cluster Links to Kubeflow Dashboard ConfigMap
DESCRIPTION: This YAML snippet shows how to add an in-cluster link to a custom application in the Kubeflow Central Dashboard ConfigMap. It includes the necessary fields such as type, link, text, and icon.

LANGUAGE: yaml
CODE:
apiVersion: v1
kind: ConfigMap
metadata:
  name: centraldashboard-config
  namespace: kubeflow
data:
  settings: |-
    ...
  links: |-
    {
      "menuLinks": [
        ...
        {
          "type": "item",
          "link": "/my-app/",
          "text": "My App",
          "icon": "social:mood"
        },
        ...
      ],
      "externalLinks": [
        ...
      ],
      "quickLinks": [
        ...
      ],
      "documentationItems": [
        ...
      ]
    }

----------------------------------------

TITLE: Simple Output Parameters in Kubeflow Components
DESCRIPTION: Shows how to define output parameters in both Python components and pipelines using return annotations.

LANGUAGE: python
CODE:
from kfp import dsl

@dsl.component
def my_comp() -> int:
    return 1

@dsl.pipeline
def my_pipeline() -> int:
    task = my_comp()
    return task.output

----------------------------------------

TITLE: Volume and Secret Mounting
DESCRIPTION: Example showing how to mount volumes and secrets into driver and executor pods.

LANGUAGE: yaml
CODE:
spec:
  volumes:
    - name: spark-data
      persistentVolumeClaim:
        claimName: my-pvc
  driver:
    secrets:
      - name: gcp-svc-account
        path: /mnt/secrets
        secretType: GCPServiceAccount

----------------------------------------

TITLE: Installing Latest Stable Release of Training SDK
DESCRIPTION: Installs the latest stable release of the Kubeflow Training Python SDK using pip.

LANGUAGE: shell
CODE:
pip install -U kubeflow-training

----------------------------------------

TITLE: Deploying Kubeflow Training Runtimes
DESCRIPTION: Command to deploy the Kubeflow Training Runtimes using kubectl. This applies the necessary Kubernetes manifests from the Kubeflow Trainer GitHub repository.

LANGUAGE: bash
CODE:
kubectl apply --server-side -k "https://github.com/kubeflow/trainer.git/manifests/overlays/runtimes?ref=master"

----------------------------------------

TITLE: Connecting KFP SDK Inside Kubernetes Cluster
DESCRIPTION: Creates a KFP client using a ServiceAccount token for authentication when running inside a Kubernetes cluster with Kubeflow Platform.

LANGUAGE: python
CODE:
import kfp

# by default, when run from inside a Kubernetes cluster:
#  - the token is read from the `KF_PIPELINES_SA_TOKEN_PATH` path
#  - the host is set to `http://ml-pipeline-ui.kubeflow.svc.cluster.local`
kfp_client = kfp.Client()

# test the client by listing experiments
experiments = kfp_client.list_experiments(namespace="my-profile")
print(experiments)

----------------------------------------

TITLE: Installing Latest Development Katib SDK
DESCRIPTION: Command to install the latest development version of Katib SDK from the master branch

LANGUAGE: shell
CODE:
pip install git+https://github.com/kubeflow/katib.git@master#subdirectory=sdk/python/v1beta1

----------------------------------------

TITLE: Deploying Kubeflow Trainer Controller Manager
DESCRIPTION: Command to deploy the Kubeflow Trainer controller manager using kubectl. This applies the necessary Kubernetes manifests from the Kubeflow Trainer GitHub repository.

LANGUAGE: bash
CODE:
kubectl apply --server-side -k "https://github.com/kubeflow/trainer.git/manifests/overlays/manager?ref=master"

----------------------------------------

TITLE: Generating HTML Visualization with v2 SDK
DESCRIPTION: Demonstrates how to output HTML content using the v2 SDK HTML output type.

LANGUAGE: Python
CODE:
@component
def html_visualization(html_artifact: Output[HTML]):
    html_content = '<!DOCTYPE html><html><body><h1>Hello world</h1></body></html>'
    with open(html_artifact.path, 'w') as f:
        f.write(html_content)

----------------------------------------

TITLE: Configuring Artifact Domain Regex in Python for Multi-User Kubeflow
DESCRIPTION: Example of adding an entry for ALLOWED_ARTIFACT_DOMAIN_REGEX in the sync.py file for full-fledged Kubeflow deployment. This configuration is applied to user namespaces in a multi-user environment.

LANGUAGE: python
CODE:
sync.py#L304-L310

----------------------------------------

TITLE: Viewing PyTorchJob Logs
DESCRIPTION: These commands retrieve the name of the master pod and then stream its logs to monitor training progress.

LANGUAGE: bash
CODE:
PODNAME=$(kubectl get pods -l training.kubeflow.org/job-name=pytorch-simple,training.kubeflow.org/replica-type=master,training.kubeflow.org/replica-index=0 -o name -n kubeflow)
kubectl logs -f ${PODNAME} -n kubeflow

----------------------------------------

TITLE: Setting Time-To-Live for SparkApplication in YAML
DESCRIPTION: This YAML snippet shows how to set the Time-To-Live (TTL) duration for a SparkApplication using the timeToLiveSeconds field. The SparkApplication object will be garbage collected after the specified duration since its termination.

LANGUAGE: yaml
CODE:
spec:
  timeToLiveSeconds: 3600

----------------------------------------

TITLE: Container Component Output Parameters in Kubeflow
DESCRIPTION: Demonstrates how to handle output parameters in container components using dsl.OutputPath annotation.

LANGUAGE: python
CODE:
from kfp import dsl

@dsl.container_component
def my_comp(int_path: dsl.OutputPath(int)):
    return dsl.ContainerSpec(
        image='alpine',
        command=[
            'sh', '-c', f"""mkdir -p $(dirname {int_path})\
                            && echo 1 > {int_path}"""
        ])

@dsl.pipeline
def my_pipeline() -> int:
    task = my_comp()
    return task.outputs['int_path']

----------------------------------------

TITLE: Component Dockerfile Example
DESCRIPTION: Example Dockerfile for building a Kubeflow pipeline component container image

LANGUAGE: dockerfile
CODE:
FROM python:3.7
RUN python3 -m pip install keras
COPY ./src /pipelines/component/src

----------------------------------------

TITLE: Retrieving Pod YAML in Kubeflow using kubectl
DESCRIPTION: This command fetches the YAML configuration of the Pod associated with the Notebook, allowing for detailed inspection of its setup.

LANGUAGE: shell
CODE:
kubectl get pod "${MY_NOTEBOOK_NAME}-0" --namespace "${MY_PROFILE_NAMESPACE}" -o yaml

----------------------------------------

TITLE: Configuring RestartPolicy for SparkApplication in YAML
DESCRIPTION: This YAML snippet demonstrates how to configure the RestartPolicy for a SparkApplication. It specifies the restart type, retry counts, and retry intervals for both failure and submission failure scenarios.

LANGUAGE: yaml
CODE:
  restartPolicy:
     type: OnFailure
     onFailureRetries: 3
     onFailureRetryInterval: 10
     onSubmissionFailureRetries: 5
     onSubmissionFailureRetryInterval: 20

----------------------------------------

TITLE: Viewing Pod Logs in Kubeflow using kubectl
DESCRIPTION: This command retrieves the logs from the Pod associated with the Notebook, which can provide insights into any runtime errors or issues.

LANGUAGE: shell
CODE:
kubectl logs "${MY_NOTEBOOK_NAME}-0" --namespace "${MY_PROFILE_NAMESPACE}"

----------------------------------------

TITLE: YuniKorn Driver Pod Annotations and Labels
DESCRIPTION: YAML representation of the annotations and labels added to a Spark driver pod when using YuniKorn for batch scheduling. It includes task group information and queue settings.

LANGUAGE: yaml
CODE:
apiVersion: v1
kind: Pod
metadata:
  annotations:
    yunikorn.apache.org/allow-preemption: "true"
    yunikorn.apache.org/task-group-name: spark-driver
    yunikorn.apache.org/task-groups: '[{"name":"spark-driver","minMember":1,"minResource":{"cpu":"1","memory":"896Mi"},"labels":{"queue":"root.default","version":"3.5.2"}},{"name":"spark-executor","minMember":2,"minResource":{"cpu":"1","memory":"896Mi"},"labels":{"queue":"root.default","version":"3.5.2"}}]'
    yunikorn.apache.org/user.info: '{"user":"system:serviceaccount:spark-operator:spark-operator-controller","groups":["system:serviceaccounts","system:serviceaccounts:spark-operator","system:authenticated"]}'
  creationTimestamp: "2024-09-10T04:40:37Z"
  labels:
    queue: root.default
    spark-app-name: spark-pi-yunikorn
    spark-app-selector: spark-1bfe85bb77df4d5594337249b38c9648
    spark-role: driver
    spark-version: 3.5.2
    sparkoperator.k8s.io/app-name: spark-pi-yunikorn
    sparkoperator.k8s.io/launched-by-spark-operator: "true"
    sparkoperator.k8s.io/submission-id: 1a71de55-cdc7-4e62-b997-197883dc4cbe
    version: 3.5.2
  name: spark-pi-yunikorn-driver
  ...

----------------------------------------

TITLE: Generating Confusion Matrix Visualization with v1 SDK
DESCRIPTION: Shows how to output a confusion matrix visualization using the v1 SDK by writing metadata to a JSON file.

LANGUAGE: Python
CODE:
def confusion_matrix_viz(mlpipeline_ui_metadata_path: kfp.components.OutputPath()):
  import json
    
  metadata = {
    'outputs' : [{
      'type': 'confusion_matrix',
      'format': 'csv',
      'schema': [
        {'name': 'target', 'type': 'CATEGORY'},
        {'name': 'predicted', 'type': 'CATEGORY'},
        {'name': 'count', 'type': 'NUMBER'},
      ],
      'source': <CONFUSION_MATRIX_CSV_FILE>,
      # Convert vocab to string because for bealean values we want "True|False" to match csv data.
      'labels': list(map(str, vocab)),
    }]
  }

  with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:
    json.dump(metadata, metadata_file)

----------------------------------------

TITLE: Component Build Script Example
DESCRIPTION: Bash script to build and push the component's container image to a registry

LANGUAGE: bash
CODE:
#!/bin/bash -e
image_name=gcr.io/my-org/my-image
image_tag=latest
full_image_name=${image_name}:${image_tag}

cd "$(dirname "$0")" 
docker build -t "${full_image_name}" .
docker push "$full_image_name"

# Output the strict image name, which contains the sha256 image digest
docker inspect --format="{{index .RepoDigests 0}}" "${full_image_name}"

----------------------------------------

TITLE: Release Information Table Structure in HTML
DESCRIPTION: HTML table structure defining the release information including release date, media links, manifests, and release team details.

LANGUAGE: html
CODE:
<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2023-03-29
      </td>
    </tr>
    <!-- Additional table rows... -->
  </tbody>
</table>
</div>

----------------------------------------

TITLE: Sample SparkApplication Configuration
DESCRIPTION: Example YAML configuration for a Spark Pi application showing driver and executor specifications

LANGUAGE: yaml
CODE:
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  ...
spec:
  deps: {}
  driver:
    coreLimit: 1200m
    cores: 1
    labels:
      version: 2.3.0
    memory: 512m
    serviceAccount: spark
  executor:
    cores: 1
    instances: 1
    labels:
      version: 2.3.0
    memory: 512m
  image: gcr.io/ynli-k8s/spark:v3.1.1
  mainApplicationFile: local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar
  mainClass: org.apache.spark.examples.SparkPi
  mode: cluster
  restartPolicy:
      type: OnFailure
      onFailureRetries: 3
      onFailureRetryInterval: 10
      onSubmissionFailureRetries: 5
      onSubmissionFailureRetryInterval: 20
  type: Scala

----------------------------------------

TITLE: Verifying Training Operator Installation
DESCRIPTION: Checks the status of the Training Operator pods in the kubeflow namespace to verify successful installation.

LANGUAGE: shell
CODE:
kubectl get pods -n kubeflow

----------------------------------------

TITLE: Building Spark Operator Binary
DESCRIPTION: Use the Makefile to build the Spark operator binary, which will be placed in the 'bin' directory.

LANGUAGE: bash
CODE:
make build-operator

----------------------------------------

TITLE: Component Versions Table Structure in HTML
DESCRIPTION: HTML table structure listing all Kubeflow component versions organized by working groups including AutoML, Notebooks, Pipelines, Serving, and Training components.

LANGUAGE: html
CODE:
<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Maintainers</th>
        <th>Component Name</th>
        <th>Version</th>
      </tr>
    </thead>
  <tbody>
      <!-- Component rows... -->
  </tbody>
</table>
</div>

----------------------------------------

TITLE: Installing Spark Operator with Helm
DESCRIPTION: Commands for installing the Spark Operator using Helm, including options for namespace creation and webhook enablement

LANGUAGE: shell
CODE:
helm install spark-operator spark-operator/spark-operator \
    --namespace spark-operator \
    --create-namespace

----------------------------------------

TITLE: Dependency Versions Table Structure in HTML
DESCRIPTION: HTML table structure documenting the validated dependency versions for the Kubeflow manifests including Kubernetes, Istio, cert-manager, and other core dependencies.

LANGUAGE: html
CODE:
<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Dependency</th>
        <th>Validated or Included Version(s)</th>
        <th>Notes</th>
      </tr>
    </thead>
  <tbody>
      <!-- Dependency rows... -->
  </tbody>
</table>
</div>

----------------------------------------

TITLE: Installing Model Registry Python Dependencies
DESCRIPTION: Installation commands for the Model Registry Python client and KServe dependencies using pip.

LANGUAGE: raw
CODE:
!pip install model-registry=="{{% model-registry/latest-version %}}"
!pip install kserve=="0.13"

----------------------------------------

TITLE: Generating Markdown Visualization with v1 SDK
DESCRIPTION: Demonstrates how to output markdown content using the v1 SDK by writing metadata to a JSON file.

LANGUAGE: Python
CODE:
def markdown_vis(mlpipeline_ui_metadata_path: kfp.components.OutputPath()):
  import json
    
  metadata = {
    'outputs' : [
    # Markdown that is hardcoded inline
    {
      'storage': 'inline',
      'source': '# Inline Markdown\n[A link](https://www.kubeflow.org/)',
      'type': 'markdown',
    },
    # Markdown that is read from a file
    {
      'source': 'gs://your_project/your_bucket/your_markdown_file',
      'type': 'markdown',
    }]
  }

  with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:
    json.dump(metadata, metadata_file)

----------------------------------------

TITLE: Configuring Metrics Port in Training Operator Deployment YAML
DESCRIPTION: This YAML snippet shows how to modify the Training Operator deployment to change the metrics port and restrict IP access. It demonstrates setting the metrics-bind-address argument.

LANGUAGE: yaml
CODE:
# deployment.yaml for the Training Operator
spec:
    containers:
    - command:
        - /manager
        image: kubeflow/training-operator
        name: training-operator
        ports:
        - containerPort: 8080
        - containerPort: 9443
            name: webhook-server
            protocol: TCP
        args:
        - "--metrics-bind-address=192.168.1.100:8082"

----------------------------------------

TITLE: MySQL Connection String Format
DESCRIPTION: Data source name format used by DB Manager to connect to MySQL database when DB_NAME=mysql.

LANGUAGE: text
CODE:
DB_USER:DB_PASSWORD@tcp(KATIB_MYSQL_DB_HOST:KATIB_MYSQL_DB_PORT)/KATIB_MYSQL_DB_DATABASE?timeout=5s

----------------------------------------

TITLE: Katib Config for Tekton Pipeline Integration
DESCRIPTION: YAML configuration to add Tekton PipelineRun support in Katib controller parameters.

LANGUAGE: yaml
CODE:
trialResources:
  - PipelineRun.v1beta1.tekton.dev

----------------------------------------

TITLE: Creating a Kubeflow Pipelines component from a Python function
DESCRIPTION: Using create_component_from_func to generate a component specification and factory function from the add function.

LANGUAGE: python
CODE:
add_op = create_component_from_func(
    add, output_component_file='add_component.yaml')

----------------------------------------

TITLE: Configuring KFP Launcher ConfigMap for Default Minio Installation
DESCRIPTION: This YAML snippet demonstrates how to configure the KFP Launcher ConfigMap to use a custom path within the default Minio installation for storing artifacts.

LANGUAGE: yaml
CODE:
apiVersion: v1
kind: ConfigMap
metadata:
  name: kfp-launcher
  namespace: user-namespace
data:
  defaultPipelineRoot: "minio://mlpipeline/some/other/path"

----------------------------------------

TITLE: Extracting Current Optimal Trial Information from Katib Experiment
DESCRIPTION: Shell command using kubectl to extract and display information about the current optimal trial from a Katib Experiment named 'random' in JSON format.

LANGUAGE: shell
CODE:
kubectl get experiment random -n kubeflow -o=jsonpath='{.status.currentOptimalTrial}'

----------------------------------------

TITLE: Installing Specific Version of Katib SDK
DESCRIPTION: Command to install Katib SDK from a specific GitHub commit

LANGUAGE: shell
CODE:
pip install git+https://github.com/kubeflow/katib.git@ea46a7f2b73b2d316b6b7619f99eb440ede1909b#subdirectory=sdk/python/v1beta1

----------------------------------------

TITLE: Creating Basic Addition Component with KFP
DESCRIPTION: Simple example of a Lightweight Python Component that adds two integers using the @dsl.component decorator.

LANGUAGE: python
CODE:
from kfp import dsl

@dsl.component
def add(a: int, b: int) -> int:
    return a + b

----------------------------------------

TITLE: Katib Optimization Results Output
DESCRIPTION: Example JSON output showing the optimal trial results from the Katib experiment, including the best parameters found and their corresponding metrics.

LANGUAGE: json
CODE:
{
  "best_trial_name": "tune-experiment-nmggpxx2",
  "parameter_assignments": [
    {
      "name": "a",
      "value": "19"
    },
    {
      "name": "b",
      "value": "0.13546396192975868"
    }
  ],
  "observation": {
    "metrics": [
      {
        "latest": "75.98164951501829",
        "max": "75.98164951501829",
        "min": "75.98164951501829",
        "name": "result"
      }
    ]
  }
}

----------------------------------------

TITLE: Representing Neural Network Architecture in ENAS
DESCRIPTION: Example of how Katib represents a neural network architecture for ENAS. Each row represents a layer, with the first integer indicating the operation and subsequent integers indicating skip connections.

LANGUAGE: json
CODE:
[
[2],
[0 0],
[1 1 0],
[5 1 0 1],
[1 1 1 0 1],
[5 0 0 1 0 1],
[1 1 1 0 0 1 0],
[2 0 0 0 1 1 0 1],
[0 0 0 1 1 1 1 1 0],
[2 0 1 0 1 1 1 0 0 0],
[3 1 1 1 1 1 1 0 0 1 1],
[0 1 1 1 1 0 0 1 1 1 1 0]
]

----------------------------------------

TITLE: Creating a Kubeflow Pipelines component with a custom base image
DESCRIPTION: Using create_component_from_func to generate a component from the my_divmod function, specifying a custom TensorFlow base image.

LANGUAGE: python
CODE:
divmod_op = comp.create_component_from_func(
    my_divmod, base_image='tensorflow/tensorflow:1.11.0-py3')

----------------------------------------

TITLE: Installing Miniconda on Debian/Ubuntu/Cloud Shell
DESCRIPTION: Commands to install Miniconda on Debian-based systems or Cloud Shell. This installs the necessary Python environment for the Kubeflow Pipelines SDK.

LANGUAGE: bash
CODE:
apt-get update; apt-get install -y wget bzip2
wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh

----------------------------------------

TITLE: Describing Kubeflow Edit ClusterRole in Kubernetes
DESCRIPTION: This command retrieves the full list of RBAC permissions for the 'kubeflow-edit' ClusterRole in Kubernetes. It's used to understand the extent of permissions available to Notebook Pods.

LANGUAGE: shell
CODE:
kubectl describe clusterrole kubeflow-edit

----------------------------------------

TITLE: Installing Pre-Commit Package Manager
DESCRIPTION: Install the pre-commit package manager using pip, conda, or Homebrew to set up Git pre-commit hooks.

LANGUAGE: shell
CODE:
# Using pip
pip install pre-commit

# Using conda
conda install -c conda-forge pre-commit

# Using Homebrew
brew install pre-commit

----------------------------------------

TITLE: Building Spark Operator Docker Image
DESCRIPTION: Build a Docker image for the Spark operator with a specified image tag.

LANGUAGE: shell
CODE:
make docker-build IMAGE_TAG=<image-tag>

----------------------------------------

TITLE: Modifying Dockerfile for Custom Spark Operator
DESCRIPTION: Example of modifying the FROM directive in Dockerfile to use a custom Spark image as the base for the operator.

LANGUAGE: dockerfile
CODE:
FROM <your-custom-spark-image>

----------------------------------------

TITLE: Labeling Existing Namespace for Katib Metrics Collector
DESCRIPTION: Bash command to add the required label to an existing Kubernetes namespace for enabling metrics collector injection.

LANGUAGE: bash
CODE:
kubectl label namespace <your-namespace> katib.kubeflow.org/metrics-collector-injection=enabled

----------------------------------------

TITLE: Getting TFJob Logs
DESCRIPTION: Example of retrieving logs from a running TensorFlow training job using the Training Operator Python SDK.

LANGUAGE: python
CODE:
TrainingClient().get_job_logs(
    name="tensorflow-dist",
    job_kind="TFJob",
    follow=True,
)

----------------------------------------

TITLE: Building the Containerized Component
DESCRIPTION: Uses the KFP CLI to build the component image without pushing it to a registry.

LANGUAGE: sh
CODE:
kfp component build src/ --component-filepattern my_component.py --no-push-image

----------------------------------------

TITLE: Accessing HuggingFace Trainer Implementation
DESCRIPTION: Reference to the HuggingFace trainer implementation that handles the training loop for fine-tuning LLMs.

LANGUAGE: Python
CODE:
github.com/kubeflow/training-operator/blob/6ce4d57d699a76c3d043917bd0902c931f14080f/sdk/python/kubeflow/trainer/hf_llm_training.py#L118-L139

----------------------------------------

TITLE: Defining Hugo Front Matter in TOML
DESCRIPTION: Sets up Hugo page metadata including title, description, and weight for the Kubeflow Notebooks documentation page.

LANGUAGE: toml
CODE:
+++
title = "Kubeflow Notebooks"
description = "Documentation for Kubeflow Notebooks"
weight = 10
+++

----------------------------------------

TITLE: Defining a simple addition function in Python
DESCRIPTION: Creating a standalone Python function to add two float numbers, which will be used as a component.

LANGUAGE: python
CODE:
def add(a: float, b: float) -> float:
  '''Calculates sum of two arguments'''
  return a + b

----------------------------------------

TITLE: Generating Web App Visualization with v1 SDK
DESCRIPTION: Demonstrates how to output a custom web app visualization using the v1 SDK by writing metadata to a JSON file.

LANGUAGE: Python
CODE:
def tensorboard_vis(mlpipeline_ui_metadata_path: kfp.components.OutputPath()):
  import json

  static_html_path = os.path.join(output_dir, _OUTPUT_HTML_FILE)
  file_io.write_string_to_file(static_html_path, rendered_template)

  metadata = {
    'outputs' : [{
      'type': 'web-app',
      'storage': 'gcs',
      'source': static_html_path,
    }, {
      'type': 'web-app',
      'storage': 'inline',
      'source': '<h1>Hello, World!</h1>',
    }]
  }

  with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:
    json.dump(metadata, metadata_file)

----------------------------------------

TITLE: Configuring Namespace for Katib Metrics Collector in YAML
DESCRIPTION: YAML configuration to add the required label for enabling sidecar container injection for pull-based metrics collectors in a Kubernetes namespace.

LANGUAGE: yaml
CODE:
apiVersion: v1
kind: Namespace
metadata:
  name: <your-namespace>
  labels:
    katib.kubeflow.org/metrics-collector-injection: enabled

----------------------------------------

TITLE: Defining TOML Frontmatter for Kubeflow Pipelines Documentation
DESCRIPTION: TOML frontmatter block that specifies metadata for the Kubeflow Pipelines documentation page. Sets the page title, description, and weight for navigation ordering.

LANGUAGE: toml
CODE:
+++
title = "Kubeflow Pipelines"
description = "Documentation for Kubeflow Pipelines."
weight = 15
+++

----------------------------------------

TITLE: Pipeline Component Definition
DESCRIPTION: Define a pipeline component for merging CSV files using KFP InputPath and OutputPath annotations

LANGUAGE: python
CODE:
def merge_csv(file_path: comp.InputPath('Tarball'),
              output_csv: comp.OutputPath('CSV')):
  import glob
  import pandas as pd
  import tarfile

  tarfile.open(name=file_path, mode="r|gz").extractall('data')
  df = pd.concat(
      [pd.read_csv(csv_file, header=None) 
       for csv_file in glob.glob('data/*.csv')])
  df.to_csv(output_csv, index=False, header=False)

----------------------------------------

TITLE: Generating TensorBoard Visualization with v1 SDK
DESCRIPTION: Shows how to add a TensorBoard visualization option using the v1 SDK by writing metadata to a JSON file.

LANGUAGE: Python
CODE:
def tensorboard_vis(mlpipeline_ui_metadata_path: kfp.components.OutputPath()):
  import json

  metadata = {
    'outputs' : [{
      'type': 'tensorboard',
      'source': args.job_dir,
    }]
  }

  with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:
    json.dump(metadata, metadata_file)

----------------------------------------

TITLE: Editing Katib ConfigMap
DESCRIPTION: Command to edit the Katib configuration ConfigMap in the kubeflow namespace.

LANGUAGE: shell
CODE:
kubectl edit configMap katib-config -n kubeflow

----------------------------------------

TITLE: Defining Hugo Front Matter for Kubeflow Documentation
DESCRIPTION: TOML-formatted Hugo front matter that defines metadata for the main documentation page, including the page title and description.

LANGUAGE: toml
CODE:
+++
title = "Documentation"
description = "All of Kubeflow documentation"
+++

----------------------------------------

TITLE: Implementing Push-based Metrics Collection in Katib with Python
DESCRIPTION: Python code example demonstrating how to use the Katib SDK to implement push-based metrics collection in a training script. It includes the objective function and Katib client configuration.

LANGUAGE: python
CODE:
import kubeflow.katib as katib

def objective(parameters):
  import time
  import kubeflow.katib as katib
  time.sleep(5)
  result = 4 * int(parameters["a"])
  # Push metrics to Katib DB.
  katib.report_metrics({"result": result})

katib.KatibClient(namespace="kubeflow").tune(
  name="push-metrics-exp",
  objective=objective,
  parameters= {"a": katib.search.int(min=10, max=20)},
  objective_metric_name="result",
  max_trial_count=2,
  metrics_collector_config={"kind": "Push"},
  # When SDK is released, replace it with packages_to_install=["kubeflow-katib==0.18.0"].
  # Currently, the training container should have `git` package to install this SDK.
  packages_to_install=["git+https://github.com/kubeflow/katib.git@master#subdirectory=sdk/python/v1beta1"],
)

----------------------------------------

TITLE: Disabling Istio Sidecar Injection in Katib Trial Template
DESCRIPTION: YAML snippet showing how to disable Istio sidecar injection for a Katib Experiment Trial by adding an annotation to the trial specification.

LANGUAGE: yaml
CODE:
trialSpec:
  apiVersion: batch/v1
  kind: Job
  spec:
    template:
      metadata:
        annotations:
          "sidecar.istio.io/inject": "false"

----------------------------------------

TITLE: Hugo Front Matter Configuration
DESCRIPTION: TOML format front matter configuration for the Kubeflow 1.9 documentation page

LANGUAGE: toml
CODE:
+++
title = "Kubeflow 1.9"
description = "Information about the Kubeflow 1.9 release"
weight = 95
+++

----------------------------------------

TITLE: Setting Environment Variables for Kubeflow Pipeline Tasks
DESCRIPTION: Demonstration of how to set environment variables for tasks in a Kubeflow pipeline using the set_env_variable method on PipelineTask objects.

LANGUAGE: python
CODE:
from kfp import dsl

@dsl.component
def print_env_var():
    import os
    print(os.environ.get('MY_ENV_VAR'))

@dsl.pipeline()
def my_pipeline():
    task = print_env_var()
    task.set_env_variable('MY_ENV_VAR', 'hello')

----------------------------------------

TITLE: Generating ROC Curve Visualization with v1 SDK
DESCRIPTION: Shows how to output a ROC curve visualization using the v1 SDK by writing metadata to a JSON file.

LANGUAGE: Python
CODE:
def roc_vis(roc_csv_file_path: str, mlpipeline_ui_metadata_path: kfp.components.OutputPath()):
  import json

  df_roc = pd.DataFrame({'fpr': fpr, 'tpr': tpr, 'thresholds': thresholds})
  roc_file = os.path.join(roc_csv_file_path, 'roc.csv')
  with file_io.FileIO(roc_file, 'w') as f:
    df_roc.to_csv(f, columns=['fpr', 'tpr', 'thresholds'], header=False, index=False)

  metadata = {
    'outputs': [{
      'type': 'roc',
      'format': 'csv',
      'schema': [
        {'name': 'fpr', 'type': 'NUMBER'},
        {'name': 'tpr', 'type': 'NUMBER'},
        {'name': 'thresholds', 'type': 'NUMBER'},
      ],
      'source': roc_file
    }]
  }

  with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:
    json.dump(metadata, metadata_file)

----------------------------------------

TITLE: Modifying Running Experiment with kubectl
DESCRIPTION: Shell command to edit a running Katib experiment using kubectl. This allows modification of parallelTrialCount, maxTrialCount, and maxFailedTrialCount parameters.

LANGUAGE: shell
CODE:
kubectl edit experiment <experiment-name> -n <experiment-namespace>

----------------------------------------

TITLE: Component Version Table Structure in HTML
DESCRIPTION: HTML table structure for displaying Kubeflow component versions organized by working groups.

LANGUAGE: HTML
CODE:
<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Maintainers</th>
        <th>Component Name</th>
        <th>Version</th>
      </tr>
    </thead>
  <tbody>
      <tr>
        <td rowspan="1" class="align-middle">AutoML Working Group</td>
        <td>Katib</td>
        <td>
          <a href="https://github.com/kubeflow/katib/releases/tag/v0.16.0">v0.16.0</a>
        </td>
      </tr>
  </tbody>
</table>
</div>

----------------------------------------

TITLE: Defining PyTorch Distributed Training Function
DESCRIPTION: Defines a training function for distributed PyTorch training using Kubeflow Trainer. It sets up the distributed environment, defines a CNN model, prepares the dataset, and implements the training loop.

LANGUAGE: python
CODE:
def train_pytorch():
    import os

    import torch
    from torch import nn
    import torch.nn.functional as F

    from torchvision import datasets, transforms
    import torch.distributed as dist
    from torch.utils.data import DataLoader, DistributedSampler

    # [1] Configure CPU/GPU device and distributed backend.
    # Kubeflow Trainer will automatically configure the distributed environment.
    device, backend = ("cuda", "nccl") if torch.cuda.is_available() else ("cpu", "gloo")
    dist.init_process_group(backend=backend)

    local_rank = int(os.getenv("LOCAL_RANK", 0))
    print(
        "Distributed Training with WORLD_SIZE: {}, RANK: {}, LOCAL_RANK: {}.".format(
            dist.get_world_size(),
            dist.get_rank(),
            local_rank,
        )
    )

    # [2] Define PyTorch CNN Model to be trained.
    class Net(nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.conv1 = nn.Conv2d(1, 20, 5, 1)
            self.conv2 = nn.Conv2d(20, 50, 5, 1)
            self.fc1 = nn.Linear(4 * 4 * 50, 500)
            self.fc2 = nn.Linear(500, 10)

        def forward(self, x):
            x = F.relu(self.conv1(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv2(x))
            x = F.max_pool2d(x, 2, 2)
            x = x.view(-1, 4 * 4 * 50)
            x = F.relu(self.fc1(x))
            x = self.fc2(x)
            return F.log_softmax(x, dim=1)

    # [3] Attach model to the correct device.
    device = torch.device(f"{device}:{local_rank}")
    model = nn.parallel.DistributedDataParallel(Net().to(device))
    model.train()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)

    # [4] Get the Fashion-MNIST dataset and distributed it across all available devices.
    dataset = datasets.FashionMNIST(
        "./data",
        train=True,
        download=True,
        transform=transforms.Compose([transforms.ToTensor()]),
    )
    train_loader = DataLoader(
        dataset,
        batch_size=100,
        sampler=DistributedSampler(dataset),
    )

    # [5] Define the training loop.
    for epoch in range(3):
        for batch_idx, (inputs, labels) in enumerate(train_loader):
            # Attach tensors to the device.
            inputs, labels = inputs.to(device), labels.to(device)

            # Forward pass
            outputs = model(inputs)
            loss = F.nll_loss(outputs, labels)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            if batch_idx % 10 == 0 and dist.get_rank() == 0:
                print(
                    "Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}".format(
                        epoch,
                        batch_idx * len(inputs),
                        len(train_loader.dataset),
                        100.0 * batch_idx / len(train_loader),
                        loss.item(),
                    )
                )

    # Wait for the training to complete and destroy to PyTorch distributed process group.
    dist.barrier()
    if dist.get_rank() == 0:
        print("Training is finished")
    dist.destroy_process_group()

----------------------------------------

TITLE: PaddleJob Status Output Example
DESCRIPTION: Sample YAML output showing the structure and fields of a successfully completed PaddleJob resource status.

LANGUAGE: yaml
CODE:
apiVersion: kubeflow.org/v1
kind: PaddleJob
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"kubeflow.org/v1","kind":"PaddleJob","metadata":{"annotations":{},"name":"paddle-simple-cpu","namespace":"kubeflow"},"spec":{"paddleReplicaSpecs":{"Worker":{"replicas":2,"restartPolicy":"OnFailure","template":{"spec":{"containers":[{"args":["-m","paddle.distributed.launch","run_check"],"command":["python"],"image":"registry.baidubce.com/paddlepaddle/paddle:2.4.0rc0-cpu","imagePullPolicy":"Always","name":"paddle","ports":[{"containerPort":37777,"name":"master"}]}]}}}}}}
  creationTimestamp: "2022-10-24T03:47:45Z"
  generation: 3
  name: paddle-simple-cpu
  namespace: kubeflow
  resourceVersion: "266235056"
  selfLink: /apis/kubeflow.org/v1/namespaces/kubeflow/paddlejobs/paddle-simple-cpu
  uid: 7ef4f92f-0ed4-4a35-b10a-562b79538cc6
spec:
  paddleReplicaSpecs:
    Worker:
      replicas: 2
      restartPolicy: OnFailure
      template:
        spec:
          containers:
          - args:
            - -m
            - paddle.distributed.launch
            - run_check
            command:
            - python
            image: registry.baidubce.com/paddlepaddle/paddle:2.4.0rc0-cpu
            imagePullPolicy: Always
            name: paddle
            ports:
            - containerPort: 37777
              name: master
              protocol: TCP
status:
  completionTime: "2022-10-24T04:04:43Z"
  conditions:
  - lastTransitionTime: "2022-10-24T03:47:45Z"
    lastUpdateTime: "2022-10-24T03:47:45Z"
    message: PaddleJob paddle-simple-cpu is created.
    reason: PaddleJobCreated
    status: "True"
    type: Created
  - lastTransitionTime: "2022-10-24T04:04:28Z"
    lastUpdateTime: "2022-10-24T04:04:28Z"
    message: PaddleJob kubeflow/paddle-simple-cpu is running.
    reason: JobRunning
    status: "False"
    type: Running
  - lastTransitionTime: "2022-10-24T04:04:43Z"
    lastUpdateTime: "2022-10-24T04:04:43Z"
    message: PaddleJob kubeflow/paddle-simple-cpu successfully completed.
    reason: JobSucceeded
    status: "True"
    type: Succeeded
  replicaStatuses:
    Worker:
      labelSelector:
        matchLabels:
          group-name: kubeflow.org
          job-name: paddle-simple-cpu
          training.kubeflow.org/job-name: paddle-simple-cpu
          training.kubeflow.org/operator-name: paddlejob-controller
          training.kubeflow.org/replica-type: Worker
      succeeded: 2
  startTime: "2022-10-24T03:47:45Z"

----------------------------------------

TITLE: Compiling Pipeline to YAML
DESCRIPTION: Demonstrates how to compile the defined pipeline into a YAML file using the KFP Compiler

LANGUAGE: python
CODE:
from kfp import compiler

compiler.Compiler().compile(hello_pipeline, 'pipeline.yaml')

----------------------------------------

TITLE: Defining Hugo Frontmatter for Kubeflow Release Documentation
DESCRIPTION: Hugo frontmatter configuration in TOML format that sets up the metadata for a documentation page about Kubeflow releases. Includes title, description, and navigation weight settings.

LANGUAGE: toml
CODE:
+++
title = "Releases"
description = "Information about past and future Kubeflow releases"
weight = 100
+++

----------------------------------------

TITLE: Training MNIST Classifier with TensorFlow in Jupyter Notebook
DESCRIPTION: This code snippet demonstrates how to train a simple softmax classifier on the MNIST dataset using TensorFlow. It includes data loading, model definition, training loop, and accuracy evaluation. The example is adapted from the official TensorFlow MNIST tutorial.

LANGUAGE: python
CODE:
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

import tensorflow as tf

x = tf.placeholder(tf.float32, [None, 784])

W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))

y = tf.nn.softmax(tf.matmul(x, W) + b)

y_ = tf.placeholder(tf.float32, [None, 10])
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))

train_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)

sess = tf.InteractiveSession()
tf.global_variables_initializer().run()

for _ in range(1000):
  batch_xs, batch_ys = mnist.train.next_batch(100)
  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})

correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
print("Accuracy: ", sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))

----------------------------------------

TITLE: Basic Parameter Component in Kubeflow Pipelines
DESCRIPTION: Demonstrates how to create a simple component that joins words using string and integer parameters. Shows basic parameter type annotation usage.

LANGUAGE: python
CODE:
from kfp import dsl

@dsl.component
def join_words(word: str, count: int = 10) -> str:
    return ' '.join(word for _ in range(count))

----------------------------------------

TITLE: Configuring TOML Frontmatter for Kubeflow User Guide
DESCRIPTION: TOML configuration block defining metadata for a Kubeflow Trainer user documentation page, including title, description and navigation weight.

LANGUAGE: toml
CODE:
+++
title = "User Guides"
description = "Documentation for ML users of Kubeflow Trainer"
weight = 40
+++

----------------------------------------

TITLE: Ignoring Upstream Task Failures in Kubeflow Pipelines
DESCRIPTION: This example shows how to use the .ignore_upstream_failure() method to make a task ignore failures of specified upstream tasks. The clean_up_task is executed after fail_task, regardless of whether fail_op succeeds or fails.

LANGUAGE: python
CODE:
from kfp import dsl

@dsl.component
def cleanup_op(message: str = 'Cleaning up...'):
    print(message)

@dsl.component
def fail_op(message: str):
    print(message)
    raise ValueError('Task failed!')

@dsl.pipeline()
def my_pipeline(text: str = 'message'):
    fail_task = fail_op(message=text)
    clean_up_task = cleanup_op(
        message=fail_task.output
    ).ignore_upstream_failure()

----------------------------------------

TITLE: Checking PaddleJob Pod Status
DESCRIPTION: Command to list pods created by the PaddleJob with specific label selectors in the kubeflow namespace.

LANGUAGE: bash
CODE:
kubectl get pods -l job-name=paddle-simple-cpu -n kubeflow

----------------------------------------

TITLE: Compiling a Component to YAML in Python with KFP SDK
DESCRIPTION: This snippet shows how to compile an individual component to IR YAML using the KFP SDK compiler. It creates a file called 'component.yaml' containing the component's Intermediate Representation.

LANGUAGE: python
CODE:
@dsl.component
def comp(message: str) -> str:
    print(message)
    return message

compiler.Compiler().compile(comp, package_path='component.yaml')

----------------------------------------

TITLE: Configuring Hugo Frontmatter for KServe Documentation Link
DESCRIPTION: TOML frontmatter configuration for a Hugo page that creates an external link to the KServe documentation website. Includes title, description, weight for sorting, manual link URL, and icon specification.

LANGUAGE: toml
CODE:
+++
title = "KServe Website"
description = "LINK | KServe Documentation Website"
weight = 998
manualLink = "https://kserve.github.io/website/latest/"
icon = "fa-solid fa-arrow-up-right-from-square"
+++

----------------------------------------

TITLE: Data Download and Merge Function
DESCRIPTION: Python function to download and merge CSV files from a tar.gz archive

LANGUAGE: python
CODE:
import glob
import pandas as pd
import tarfile
import urllib.request
    
def download_and_merge_csv(url: str, output_csv: str):
  with urllib.request.urlopen(url) as res:
    tarfile.open(fileobj=res, mode="r|gz").extractall('data')
  df = pd.concat(
      [pd.read_csv(csv_file, header=None) 
       for csv_file in glob.glob('data/*.csv')])
  df.to_csv(output_csv, index=False, header=False)

----------------------------------------

TITLE: Installing MPI Operator from Source
DESCRIPTION: Commands to clone and deploy the MPI operator using kubectl from the source repository.

LANGUAGE: shell
CODE:
git clone https://github.com/kubeflow/mpi-operator
cd mpi-operator
kubectl apply -f deploy/v2beta1/mpi-operator.yaml

----------------------------------------

TITLE: Running Helm Chart Unit Tests
DESCRIPTION: Execute unit tests for the Spark operator Helm chart using helm-unittest.

LANGUAGE: shell
CODE:
make helm-unittest

----------------------------------------

TITLE: Models Web App AuthorizationPolicy in YAML
DESCRIPTION: YAML configuration for an Istio AuthorizationPolicy to allow access to the Models Web App from the Istio ingress gateway.

LANGUAGE: yaml
CODE:
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: models-web-app
  namespace: kubeflow
spec:
  action: ALLOW
  rules:
  - from:
    - source:
        principals:
        - cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account
  selector:
    matchLabels:
      kustomize.component: kserve-models-web-app
      app.kubernetes.io/component: kserve-models-web-app

----------------------------------------

TITLE: ENAS Configuration Parameters for Neural Architecture Search
DESCRIPTION: Configuration table showing the supported algorithm settings for Efficient Neural Architecture Search (ENAS) in Katib. Parameters include controller settings like hidden size, temperature, learning rate, and other hyperparameters.

LANGUAGE: yaml
CODE:
algorithm: enas
parameters:
  controller_hidden_size: 64
  controller_temperature: 5.0
  controller_tanh_const: 2.25
  controller_entropy_weight: 1e-5
  controller_baseline_decay: 0.999
  controller_learning_rate: 5e-5
  controller_skip_target: 0.4
  controller_skip_weight: 0.8
  controller_train_steps: 50
  controller_log_every_steps: 10

----------------------------------------

TITLE: Signing Off Commits
DESCRIPTION: Sign off commits to pass the DCO check CI using the -s or --signoff flag.

LANGUAGE: bash
CODE:
git commit -s -m "Your commit message"

----------------------------------------

TITLE: OWNERS_ALIASES File Structure in YAML
DESCRIPTION: Example of an OWNERS_ALIASES file showing how to define user groups/aliases

LANGUAGE: yaml
CODE:
aliases:
  sig-foo:
    - david
    - erin
  sig-bar:
    - bob
    - frank

----------------------------------------

TITLE: Setting Cache Staleness in Python Pipeline
DESCRIPTION: Python code examples showing how to set cache staleness for pipeline steps, including setting 30-day staleness and disabling caching.

LANGUAGE: python
CODE:
def some_pipeline():
      # task is a target step in a pipeline
      task = some_op()
      task.execution_options.caching_strategy.max_cache_staleness = "P30D"

LANGUAGE: python
CODE:
def some_pipeline():
      # task is a target step in a pipeline
      task_never_use_cache = some_op()
      task_never_use_cache.execution_options.caching_strategy.max_cache_staleness = "P0D"

----------------------------------------

TITLE: Checking JAXJob Pods
DESCRIPTION: Command to list pods created by the JAX training job in the kubeflow namespace.

LANGUAGE: bash
CODE:
kubectl get pods -n kubeflow -l training.kubeflow.org/job-name=jaxjob-simple

----------------------------------------

TITLE: Running Helm Chart Lint Tests
DESCRIPTION: Execute lint tests on the Helm chart for the Spark operator.

LANGUAGE: shell
CODE:
make helm-lint

----------------------------------------

TITLE: Committing Documentation Changes
DESCRIPTION: Examples of commands for committing changes to the local Git repository.

LANGUAGE: bash
CODE:
git commit -a -m "Fixed some doc errors."

# Or:

git add add-this-doc.md
git commit -a -m "Added a shiny new doc."

----------------------------------------

TITLE: Querying Pipeline Runs in Kubeflow API
DESCRIPTION: This snippet demonstrates how to make a GET request to list pipeline runs in a specific namespace using the Kubeflow Pipelines API. It requires the base URL of your Kubeflow installation and assumes proper authentication is set up.

LANGUAGE: bash
CODE:
https://kubeflow.example.com/pipeline/apis/v1beta1/runs?resource_reference_key.type=NAMESPACE&resource_reference_key.id=team-1

----------------------------------------

TITLE: Deploying JAXJob Resource
DESCRIPTION: Command to deploy a JAX training job using kubectl by applying a configuration file from the Kubeflow training operator repository.

LANGUAGE: bash
CODE:
kubectl create -f https://raw.githubusercontent.com/kubeflow/training-operator/refs/heads/release-1.9/examples/jax/cpu-demo/demo.yaml

----------------------------------------

TITLE: Updating CRD and RBAC Manifests
DESCRIPTION: Generate updated CustomResourceDefinition, RBAC, and WebhookConfiguration manifests.

LANGUAGE: shell
CODE:
make manifests

----------------------------------------

TITLE: Embedding Google Calendar in HTML
DESCRIPTION: This snippet embeds a Google Calendar showing Kubeflow community meetings into the page. It uses JavaScript to dynamically set the timezone and calendar sources.

LANGUAGE: html
CODE:
<style>
#calendar-container {
   overflow: auto;
}
</style>
<div id="calendar-container"></div>
<script type="text/javascript">
const timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
const calender_src_list = [
  // Kubeflow Community
  "kubeflow.org_7l5vnbn8suj2se10sen81d9428%40group.calendar.google.com",
  // KServe Community
  "4fqdmu5fp4l0bgdlf4lm1atnsl2j4612%40import.calendar.google.com",
];
let calender_src = calender_src_list.map(src => `&src=${src}&color=%23A79B8E`).join('');
const html = `<iframe src="https://calendar.google.com/calendar/embed?ctz=${timezone}&height=600&wkst=1&bgcolor=%23ffffff&showPrint=0&showDate=1&mode=AGENDA&showTitle=0${calender_src}" style="border:solid 1px #777" width="800" height="600" frameborder="0" scrolling="no"></iframe>`;
document.getElementById('calendar-container').innerHTML = html;
</script>

----------------------------------------

TITLE: Specifying Task Execution Order in Kubeflow Pipeline
DESCRIPTION: Demonstration of how to specify task execution order in a Kubeflow pipeline using the .after() method, even when tasks don't exchange data directly.

LANGUAGE: python
CODE:
@dsl.pipeline
def pythagorean(a: float, b: float) -> float:
    a_sq_task = square(x=a)
    b_sq_task = square(x=b)
    b_sq_task.after(a_sq_task)
    ...

----------------------------------------

TITLE: Updating Auto-Generated Code
DESCRIPTION: Generate DeepCopy, DeepCopyInto, and DeepCopyObject method implementations after updating API definitions.

LANGUAGE: shell
CODE:
make generate

----------------------------------------

TITLE: Setting AWS Environment Variables in Python Pipeline
DESCRIPTION: This Python snippet shows how to set AWS environment variables directly in a Kubeflow Pipeline using the Kubernetes API. It demonstrates setting AWS_SECRET_ACCESS_KEY, AWS_ACCESS_KEY_ID, and AWS_REGION from a Kubernetes secret.

LANGUAGE: python
CODE:
kubernetes.use_secret_as_env(
    your_task,
    secret_name='aws-s3-creds',
    secret_key_to_env={'AWS_SECRET_ACCESS_KEY': 'AWS_SECRET_ACCESS_KEY'})
kubernetes.use_secret_as_env(
    your_task,
    secret_name='aws-s3-creds',
    secret_key_to_env={'AWS_ACCESS_KEY_ID': 'AWS_ACCESS_KEY_ID'})
kubernetes.use_secret_as_env(
    your_task,
    secret_name='aws-s3-creds',
    secret_key_to_env={'AWS_REGION': 'AWS_REGION'})
...

----------------------------------------

TITLE: Installing Kubeflow Pipelines SDK
DESCRIPTION: Command to install the Kubeflow Pipelines SDK using pip. This installs version 1.8 of the SDK.

LANGUAGE: bash
CODE:
pip install kfp==1.8

----------------------------------------

TITLE: Setting Up Git Pre-Commit Hooks
DESCRIPTION: Install and set up Git pre-commit hooks for the Spark operator project.

LANGUAGE: shell
CODE:
pre-commit install

pre-commit install-hooks

----------------------------------------

TITLE: Configuring AWS S3 with IRSA for KFP Launcher
DESCRIPTION: This YAML snippet demonstrates how to configure the KFP Launcher ConfigMap to use AWS S3 with IAM Roles for Service Accounts (IRSA) or environment-based credentials.

LANGUAGE: yaml
CODE:
apiVersion: v1
data:
  defaultPipelineRoot: s3://mlpipeline
  providers: |-
    s3:
      default:
        endpoint: s3.amazonaws.com
        disableSSL: false
        region: us-east-2
        credentials:
          fromEnv: true
kind: ConfigMap
metadata:
  name: kfp-launcher
  namespace: user-namespace

----------------------------------------

TITLE: Adding Miniconda to PATH
DESCRIPTION: Command to add Miniconda to the system PATH if the conda command is not found.

LANGUAGE: bash
CODE:
export PATH=<YOUR_MINICONDA_PATH>/bin:$PATH

----------------------------------------

TITLE: HTML Tables for Kubeflow Release Details
DESCRIPTION: HTML tables containing release information, team members, component versions and dependencies for Kubeflow 1.9

LANGUAGE: html
CODE:
<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2024-07-22
      </td>
    </tr>
    <!-- ... additional table content ... -->
  </tbody>
</table>
</div>

----------------------------------------

TITLE: Configuring ScheduledSparkApplication in Kubernetes
DESCRIPTION: Example configuration for a scheduled Spark Pi application that runs every 5 minutes. Demonstrates setting up cron scheduling, concurrency policies, history limits, and Spark execution parameters using the sparkoperator.k8s.io/v1beta2 API.

LANGUAGE: yaml
CODE:
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: ScheduledSparkApplication
metadata:
  name: spark-pi-scheduled
  namespace: default
spec:
  schedule: "@every 5m"
  concurrencyPolicy: Allow
  successfulRunHistoryLimit: 1
  failedRunHistoryLimit: 3
  template:
    type: Scala
    mode: cluster
    image: gcr.io/spark/spark:v3.1.1
    mainClass: org.apache.spark.examples.SparkPi
    mainApplicationFile: local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar
    driver:
      cores: 1
      memory: 512m
    executor:
      cores: 1
      instances: 1
      memory: 512m
    restartPolicy:
      type: Never

----------------------------------------

TITLE: Event Details Table Structure in HTML
DESCRIPTION: HTML table structure displaying key event information including date, time, location, cost and registration buttons.

LANGUAGE: html
CODE:
<div class="table-responsive">
  <table class="table table-bordered">
    <tr class="thead-light">
      <th>Date</th>
      <td>October 6th, 2023</td>
    </tr>
    <tr class="thead-light">
      <th>Time</th>
      <td>7:30 AM - 5:30 PM CDT</td>
    </tr>
    <tr class="thead-light">
      <th>Location</th>
      <td><a href="https://maps.app.goo.gl/Xnf4Y1ffVLRiPNGR9">Irving Convention Center at Las Colinas, Irving, TX, USA</a></td>
    </tr>
    <tr class="thead-light">
      <th>Cost</th>
      <td><strong>FREE</strong>, but registration is required.</td>
    </tr>
    <tr class="thead-light">
      <th>Registration</th>
      <td>
        <a href="https://www.eventbrite.com/e/kubeflow-summit-2023-virtual-registration-tickets-726298186427">
          <button class="btn btn-warning py-2 px-3 mx-3 my-3">Register to Attend<br>(VIRTUAL)</button>
        </a>
        <a href="https://www.eventbrite.com/e/kubeflow-summit-2023-in-person-registration-tickets-726236511957">
          <button class="btn btn-warning py-2 px-3 mx-3 my-3">Register to Attend<br>(IN-PERSON)</button>
        </a>
      </td>
    </tr>
  </table>
</div>

----------------------------------------

TITLE: Installing Latest Changes of Training Operator Control Plane
DESCRIPTION: Installs the latest changes of the Training Operator control plane from the master branch. This command applies the Kubernetes manifests from the specified GitHub repository.

LANGUAGE: shell
CODE:
kubectl apply --server-side -k "github.com/kubeflow/training-operator.git/manifests/overlays/standalone?ref=master"

----------------------------------------

TITLE: Disabling Cache Webhook
DESCRIPTION: Command to disable caching by modifying the webhook configuration rules.

LANGUAGE: bash
CODE:
kubectl patch mutatingwebhookconfiguration cache-webhook-${NAMESPACE} --type='json' -p='[{"op":"replace", "path": "/webhooks/0/rules/0/operations/0", "value": "DELETE"}]'

----------------------------------------

TITLE: Project Structure Definition in Markdown
DESCRIPTION: Frontmatter configuration for the GSoC 2025 project page defining title, description, icon and URL settings

LANGUAGE: markdown
CODE:
+++
title = "Google Summer of Code 2025"
description = "Google Summer of Code 2025"
icon = "fa-regular fa-calendar-day"

#
# NOTE: to avoid 404 when we move events to the "/past-events/", 
#       we explicitly set the URL here so it doesn't change
#
url = "/events/gsoc-2025/"
+++

----------------------------------------

TITLE: Installing Training SDK with HuggingFace Support
DESCRIPTION: Installs the Kubeflow Training Python SDK with additional packages for HuggingFace support, enabling LLM fine-tuning capabilities.

LANGUAGE: shell
CODE:
pip install -U "kubeflow-training[huggingface]"

----------------------------------------

TITLE: Defining Metadata for Kubeflow Summit 2025 Event Page
DESCRIPTION: This snippet defines metadata for the Kubeflow Summit 2025 event page. It includes the event title, a manual link to the event's webpage, and an icon specification. This format is commonly used in static site generators or content management systems to provide structured data for page rendering.

LANGUAGE: Markup
CODE:
+++
title = "Kubeflow Summit 2025"

manualLink = "https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/co-located-events/kubeflow-summit/"
icon = "fa-solid fa-arrow-up-right-from-square"
+++

----------------------------------------

TITLE: ENAS GetSuggestion() Output - Neural Network Configuration
DESCRIPTION: Example of the 'nn_config' part of the GetSuggestion() output from the ENAS algorithm service. It provides a detailed description of the neural network, including layer count, input/output sizes, and operation definitions.

LANGUAGE: json
CODE:
{
    "num_layers": 8,
    "input_sizes": [32, 32, 3],
    "output_sizes": [10],
    "embedding": {
        "27": {
            "opt_id": 27,
            "opt_type": "convolution",
            "opt_params": {
                "filter_size": "7",
                "num_filter": "96",
                "stride": "2"
            }
        },
        "29": {
            "opt_id": 29,
            "opt_type": "convolution",
            "opt_params": {
                "filter_size": "7",
                "num_filter": "128",
                "stride": "2"
            }
        },
        "22": {
            "opt_id": 22,
            "opt_type": "convolution",
            "opt_params": {
                "filter_size": "7",
                "num_filter": "48",
                "stride": "1"
            }
        },
        "13": {
            "opt_id": 13,
            "opt_type": "convolution",
            "opt_params": {
                "filter_size": "5",
                "num_filter": "48",
                "stride": "2"
            }
        },
        "26": {
            "opt_id": 26,
            "opt_type": "convolution",
            "opt_params": {
                "filter_size": "7",
                "num_filter": "96",
                "stride": "1"
            }
        },
        "30": {
            "opt_id": 30,
            "opt_type": "reduction",
            "opt_params": {
                "reduction_type": "max_pooling",
                "pool_size": 2
            }
        },
        "11": {
            "opt_id": 11,
            "opt_type": "convolution",
            "opt_params": {
                "filter_size": "5",
                "num_filter": "32",
                "stride": "2"
            }
        },
        "9": {
            "opt_id": 9,
            "opt_type": "convolution",
            "opt_params": {
                "filter_size": "3",
                "num_filter": "128",
                "stride": "2"
            }
        }
    }
}

----------------------------------------

TITLE: Versioned GitHub Link in Kubeflow Documentation
DESCRIPTION: Example of using a version shortcode to ensure links to GitHub source code always point to the correct branch in versioned documentation.

LANGUAGE: markdown
CODE:
https://github.com/kubeflow/kubeflow/blob/{{< params "githubbranch" >}}/scripts/gke/deploy.sh

----------------------------------------

TITLE: Getting Help for KFP CLI Run Command
DESCRIPTION: This command displays help information for the 'run' command in the KFP CLI.

LANGUAGE: shell
CODE:
kfp run --help

----------------------------------------

TITLE: Example Markdown Front Matter for Kubeflow Documentation
DESCRIPTION: Sample front matter configuration for a Kubeflow documentation page, including title, description, and weight for ordering.

LANGUAGE: markdown
CODE:
+++
title = "Getting Started with Kubeflow"
description = "Overview"
weight = 1
+++

----------------------------------------

TITLE: Deploying Spark Operator Instance 2 in Kubernetes
DESCRIPTION: This snippet demonstrates how to create a namespace 'spark-2' and install the second Spark operator instance using Helm. The operator is configured to watch the 'spark-2' namespace.

LANGUAGE: bash
CODE:
# Create the spark-2 namespace if it does not exist
kubectl create ns spark-2

# Install the Spark operator with release name spark-operator-2
helm install spark-operator-2 spark-operator/spark-operator \
    --namespace spark-operator \
    --create-namespace \
    --set 'spark.jobNamespaces={spark-2}'

----------------------------------------

TITLE: Running compiled Kubeflow Pipelines DSL script
DESCRIPTION: Command to run a compiled Kubeflow Pipelines DSL script directly using Python, which will produce a Tekton YAML file.

LANGUAGE: bash
CODE:
python3 pipeline.py

----------------------------------------

TITLE: Adding Upstream Remote Repository
DESCRIPTION: Command to add the original Kubeflow repository as an upstream remote for synchronization purposes.

LANGUAGE: bash
CODE:
git remote add upstream https://github.com/kubeflow/website.git

----------------------------------------

TITLE: Configuring Early Stopping Settings in Katib YAML
DESCRIPTION: Example structure showing how to configure early stopping parameters in a Katib Experiment YAML file. The configuration includes algorithm name and settings under the .spec.earlyStopping parameter.

LANGUAGE: yaml
CODE:
.spec.earlyStopping:
  algorithmName: medianstop
  algorithmSettings:
    - name: min_trials_required
      value: "3"
    - name: start_step
      value: "4"

----------------------------------------

TITLE: Adding Kubeflow Calendar ID in Google Calendar
DESCRIPTION: Command showing the calendar ID needed to manually add the Kubeflow community calendar to Google Calendar.

LANGUAGE: shell
CODE:
kubeflow.org_7l5vnbn8suj2se10sen81d9428@group.calendar.google.com

----------------------------------------

TITLE: Installing Kubeflow Pipelines SDK in Python
DESCRIPTION: Command to install the Kubeflow Pipelines SDK version 1.8 using pip.

LANGUAGE: bash
CODE:
$ pip install kfp==1.8

----------------------------------------

TITLE: Increasing gRPC Message Size Limit for Visualizations
DESCRIPTION: Go code snippet to increase the maximum gRPC message size for the visualization service in Kubeflow Pipelines, allowing for larger visualizations.

LANGUAGE: go
CODE:
var maxCallRecvMsgSize = 4 * 1024 * 1024
if serviceName == "Visualization" {
      // Only change the maxCallRecvMesSize if it is for visualizations
      maxCallRecvMsgSize = 50 * 1024 * 1024
}
opts := []grpc.DialOption{
      grpc.WithDefaultCallOptions(grpc.MaxCallRecvMsgSize(maxCallRecvMsgSize)),
      grpc.WithInsecure(),
}

----------------------------------------

TITLE: Basic OWNERS File Structure in YAML
DESCRIPTION: Example of a typical OWNERS file structure showing approvers and reviewers configuration

LANGUAGE: yaml
CODE:
approvers:
  - alice
  - bob # this is a comment
reviewers:
  - alice
  - carol # this is another comment
  - sig-foo # this is an alias

----------------------------------------

TITLE: Updating CRD Files in Helm Chart
DESCRIPTION: Copy the updated CRD files to the Helm chart directory.

LANGUAGE: shell
CODE:
make update-crd

----------------------------------------

TITLE: Using IfPresentPlaceholder in Container Component
DESCRIPTION: Demonstrates conditional argument passing using IfPresentPlaceholder.

LANGUAGE: python
CODE:
@dsl.container_component
def hello_someone(optional_name: str = None):
    return dsl.ContainerSpec(
        image='python:3.7',
        command=[
            'say_hello',
            dsl.IfPresentPlaceholder(
                input_name='optional_name',
                then=['--name', optional_name],
                else_=['--name', 'friend'])
        ])

----------------------------------------

TITLE: TOML Front Matter Configuration for Kubeflow Components Documentation
DESCRIPTION: TOML-formatted front matter metadata for a documentation page about Kubeflow components. Defines the title, description, and navigation weight for the page.

LANGUAGE: toml
CODE:
+++
title = "Components"
description = "Logical components that make up Kubeflow"
weight = 30
+++

----------------------------------------

TITLE: Creating VirtualService for In-Cluster App in Kubeflow
DESCRIPTION: This YAML snippet demonstrates how to create a VirtualService to expose a non-Kubeflow application running on the cluster through the Kubeflow Central Dashboard. It includes gateway, host, and routing configurations.

LANGUAGE: yaml
CODE:
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: my-custom-app
  namespace: <MY_APP_NAMESPACE>
spec:
  gateways:
    - kubeflow/kubeflow-gateway
  hosts:
    - '*'
  http:
    - headers:
        request:
          add:
            x-forwarded-prefix: /my-app
      match:
        - uri:
            prefix: /my-app/
      rewrite:
        uri: /
      route:
        - destination:
            host: <MY_APP_SERVICE_NAME>.<MY_APP_NAMESPACE>.svc.cluster.local
            port:
              number: 80

----------------------------------------

TITLE: Adding External Links to Kubeflow Dashboard ConfigMap
DESCRIPTION: This YAML snippet demonstrates how to add an external link to the Kubeflow website in the Central Dashboard ConfigMap. It includes the necessary fields such as type, iframe, text, link, and icon.

LANGUAGE: yaml
CODE:
apiVersion: v1
kind: ConfigMap
metadata:
  name: centraldashboard-config
  namespace: kubeflow
data:
  settings: |-
    ...
  links: |-
    {
      "menuLinks": [
        ...
      ],
      "externalLinks": [
        {
          "type": "item",
          "iframe": false,
          "text": "Kubeflow Website",
          "link": "https://www.kubeflow.org/",
          "icon": "launch"
        }
      ],
      "quickLinks": [
        ...
      ],
      "documentationItems": [
        ...
      ]
    }

----------------------------------------

TITLE: Monitoring PaddleJob Status
DESCRIPTION: Command to get detailed YAML output of the PaddleJob resource status.

LANGUAGE: bash
CODE:
kubectl get -o yaml paddlejobs paddle-simple-cpu -n kubeflow

----------------------------------------

TITLE: Using ConcatPlaceholder in Container Component
DESCRIPTION: Shows how to use ConcatPlaceholder for string concatenation without spaces.

LANGUAGE: python
CODE:
from kfp import dsl

@dsl.container_component
def concatenator(prefix: str, suffix: str):
    return dsl.ContainerSpec(
        image='alpine',
        command=[
            'my_program.sh'
        ],
        args=['--input', dsl.ConcatPlaceholder([prefix, suffix, '.txt'])])

----------------------------------------

TITLE: Defining Kubeflow Documentation Frontmatter in Markdown
DESCRIPTION: Frontmatter metadata block that defines the documentation page properties including title, description, and weight for the Kubeflow Model Registry reference documentation.

LANGUAGE: markdown
CODE:
+++
title = "Reference"
description = "Reference docs for Kubeflow Model Registry"
weight = 100
+++

----------------------------------------

TITLE: Executing Pipeline with Kubeflow SDK
DESCRIPTION: Submits the pipeline for execution using the Kubeflow Pipelines SDK with empty arguments.

LANGUAGE: python
CODE:
#Specify pipeline argument values
arguments = {}

#Submit a pipeline run
kfp.Client().create_run_from_pipeline_func(environment_pipeline,
                                           arguments=arguments)

----------------------------------------

TITLE: Invalid Constant Definition in KFP Component
DESCRIPTION: Example showing incorrect constant definition outside the component function that will fail at runtime.

LANGUAGE: python
CODE:
# non-example!
INVALID_CONSTANT = 2

@dsl.component
def errored_double(a: int) -> int:
    """Fails at runtime."""
    return INVALID_CONSTANT * a

----------------------------------------

TITLE: Hugo Frontmatter Configuration for Kubeflow Concepts Page
DESCRIPTION: Hugo content frontmatter that defines metadata for a documentation page about Kubeflow Pipelines concepts, including title, description and navigation weight.

LANGUAGE: markdown
CODE:
+++
title = "Concepts"
description = "Concepts used in Kubeflow Pipelines"
weight = 4
+++

----------------------------------------

TITLE: Configuring AWS S3 with Static Credentials for KFP API Server
DESCRIPTION: This YAML snippet shows how to configure the KFP API Server deployment to use an AWS S3 bucket with static credentials. It includes environment variables for bucket configuration and references to a Kubernetes secret for access keys.

LANGUAGE: yaml
CODE:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-pipeline
  namespace: kubeflow
spec:
  ...
  template:
    ...
    spec:
      containers:
      - name: ml-pipeline-api-server
        serviceAccountName: "ml-pipeline"
        env:
          ...
          - name: OBJECTSTORECONFIG_HOST
            value: "your-bucket" # e.g. s3.amazonaws.com
          - name: OBJECTSTORECONFIG_PORT
            value: "port" # e.g. 443   
          - name: OBJECTSTORECONFIG_REGION
            value: "region" # e.g. us-east-1
            # true if object store is on a secure connection 
          - name: OBJECTSTORECONFIG_SECURE
            value: "true"        
            # These env vars reference the values from a Kubernetes secret
            # this requires deploying the secret ahead of time, and filling out the
            # following values accordingly.
          - name: OBJECTSTORECONFIG_ACCESSKEY
            valueFrom:
              secretKeyRef:
                key: "some-key-1"
                name: "secret-name"
          - name: OBJECTSTORECONFIG_SECRETACCESSKEY
            valueFrom:
              secretKeyRef:
                key: "some-key-2"
                name: "secret-name"

----------------------------------------

TITLE: Deploying Spark Operator Instance 1 in Kubernetes
DESCRIPTION: This snippet shows how to create a namespace 'spark-1' and install the first Spark operator instance using Helm. The operator is configured to watch the 'spark-1' namespace.

LANGUAGE: bash
CODE:
# Create the spark-1 namespace if it does not exist
kubectl create ns spark-1

# Install the Spark operator with release name spark-operator-1
helm install spark-operator-1 spark-operator/spark-operator \
    --namespace spark-operator \
    --create-namespace \
    --set 'spark.jobNamespaces={spark-1}'

----------------------------------------

TITLE: Using Pipelines as Components in Kubeflow Pipelines (Python)
DESCRIPTION: Demonstrates how to load an entire pipeline as a component and use it within another pipeline. This showcases the composability of pipelines in Kubeflow, allowing for complex nested structures.

LANGUAGE: python
CODE:
from kfp import components

loaded_pipeline = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/2.0.0/sdk/python/test_data/pipelines/pipeline_in_pipeline_complex.yaml')

@dsl.pipeline
def my_pipeline():
    loaded_pipeline(msg='Hello KFP v2!')

----------------------------------------

TITLE: Configuring Hugo Frontmatter for Past Events Page
DESCRIPTION: TOML frontmatter configuration block that defines metadata for a Past Events page, including title, description and weight properties. This metadata is used by Hugo static site generator to organize and display the page content.

LANGUAGE: toml
CODE:
+++
title = "Past Events"
description = "Past Kubeflow events"
weight = 200
+++

----------------------------------------

TITLE: Viewing PaddleJob Training Logs
DESCRIPTION: Commands to fetch and follow logs from the worker pod of the PaddleJob training job.

LANGUAGE: bash
CODE:
PODNAME=$(kubectl get pods -l job-name=paddle-simple-cpu,replica-type=worker,replica-index=0 -o name -n kubeflow)
kubectl logs -f ${PODNAME} -n kubeflow

----------------------------------------

TITLE: Loading Components from Text in Kubeflow Pipelines (Python)
DESCRIPTION: Illustrates how to load a component from a string containing YAML content. This method is useful when the component definition is stored in a variable or needs to be dynamically generated.

LANGUAGE: python
CODE:
with open('component.yaml') as f:
    component_str = f.read()

loaded_comp = components.load_component_from_text(component_str)

----------------------------------------

TITLE: TOML Frontmatter Configuration for 2024 Events Page
DESCRIPTION: Hugo page frontmatter defining metadata for a 2024 events listing page, including title and description fields.

LANGUAGE: toml
CODE:
+++
title = "2024"
description = "Events from 2024"
+++

----------------------------------------

TITLE: Setting GCS Environment Variables in Python Pipeline
DESCRIPTION: This Python snippet shows how to set Google Cloud Storage (GCS) environment variables and mount credentials directly in a Kubeflow Pipeline task using the Kubernetes API.

LANGUAGE: python
CODE:
# Specify the default APP Credential path
your_task.set_env_variable(name='GOOGLE_APPLICATION_CREDENTIALS', value='/gcloud/credentials.json')
# Mount the GCS Credentials JSON
kubernetes.use_secret_as_volume(your_task, secret_name='gcs-secret', mount_path='/gcloud')

----------------------------------------

TITLE: Defining Hugo Page Front Matter for Kubeflow Summit Link
DESCRIPTION: Hugo front matter configuration that defines a manual link to the Kubeflow Summit 2024 YouTube playlist with associated metadata and FontAwesome icon.

LANGUAGE: toml
CODE:
+++
title = "Watch: Kubeflow Summit 2024"

manualLink = "https://www.youtube.com/playlist?list=PLj6h78yzYM2Nk-8Zyjaefz9yFJ-NxC-qn"
icon = "fa-brands fa-youtube"
+++

----------------------------------------

TITLE: Installing Kubeflow Pipelines SDK
DESCRIPTION: Command to install the Kubeflow Pipelines SDK package

LANGUAGE: bash
CODE:
!pip install kfp --upgrade

----------------------------------------

TITLE: Standalone KFP In-Cluster Connection
DESCRIPTION: Example of connecting to standalone Kubeflow Pipelines from inside the cluster using direct service access.

LANGUAGE: python
CODE:
import kfp

# the namespace in which you deployed Kubeflow Pipelines
namespace = "kubeflow" 

client = kfp.Client(host=f"http://ml-pipeline-ui.{namespace}")

print(client.list_experiments())

----------------------------------------

TITLE: Defining Hugo Front Matter for Kubeflow Events Page in Markdown
DESCRIPTION: This snippet defines the Hugo front matter for a webpage listing future Kubeflow events. It sets the page title, description, and weight for organizing content within the Hugo-based website structure.

LANGUAGE: markdown
CODE:
+++
title = "Future Events"
description = "Future Kubeflow events"
weight = 100
+++

----------------------------------------

TITLE: Downloading and Compiling Pipeline
DESCRIPTION: Commands to download the sequential.py sample pipeline and compile it into a tar.gz file using dsl-compile

LANGUAGE: bash
CODE:
PIPELINE_URL=https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/core/sequential/sequential.py
PIPELINE_FILE=${PIPELINE_URL##*/}
PIPELINE_NAME=${PIPELINE_FILE%.*}

wget -O ${PIPELINE_FILE} ${PIPELINE_URL}
dsl-compile --py ${PIPELINE_FILE} --output ${PIPELINE_NAME}.tar.gz

----------------------------------------

TITLE: Installing Latest Changes of Training SDK
DESCRIPTION: Installs the latest changes of the Kubeflow Training Python SDK directly from the GitHub repository's master branch.

LANGUAGE: shell
CODE:
pip install git+https://github.com/kubeflow/training-operator.git@master#subdirectory=sdk/python

----------------------------------------

TITLE: Defining YAML Front Matter for Search Results Page in Kubeflow
DESCRIPTION: This YAML front matter block sets up the configuration for a search results page in the Kubeflow project documentation. It specifies the page title as 'Search results' and uses a custom 'search' layout for rendering the results.

LANGUAGE: yaml
CODE:
---
title: Search results
layout: search
---

----------------------------------------

TITLE: Kubeflow 1.4 Release Tables Structure
DESCRIPTION: HTML tables displaying release information and component versions for Kubeflow 1.4. The tables include release dates, media links, manifest locations, team members, and detailed component version information organized by working groups.

LANGUAGE: html
CODE:
<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2021-12-23
      </td>
    </tr>
    <!-- Additional table content -->
  </tbody>
</table>
</div>

----------------------------------------

TITLE: Verifying Kubeflow Pipelines SDK Installation
DESCRIPTION: Command to verify the installation of the Kubeflow Pipelines SDK by checking the location of the dsl-compile binary.

LANGUAGE: bash
CODE:
which dsl-compile

----------------------------------------

TITLE: Pushing Changes to Remote Repository
DESCRIPTION: Commands to push local changes to the feature branch on the forked repository.

LANGUAGE: bash
CODE:
git checkout doc-updates
git push origin doc-updates

----------------------------------------

TITLE: Checking MPI Job CRD Installation
DESCRIPTION: Command to verify if the MPI Job custom resource definition is installed in the cluster.

LANGUAGE: shell
CODE:
kubectl get crd

----------------------------------------

TITLE: Release Information Table in HTML
DESCRIPTION: HTML table structure displaying key information about Kubeflow 1.1 release, including release date, media links, manifest locations, and team details.

LANGUAGE: html
CODE:
<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2020-07-31
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://blog.kubeflow.org/release/official/2020/07/31/kubeflow-1.1-blog-post.html">Kubeflow 1.1 Release Announcement</a>
        <br>
        <b>Video:</b> 
          <a href="https://www.youtube.com/watch?v=kd-mWl1cq48">Kubeflow 1.1 Community Release Update</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-11-features-release-date-late-june-2020">Kubeflow 1.1 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v1.1.0">v1.1.0</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v1.1-branch">v1.1-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Jeremy Lewi (<a href="https://github.com/jlewi">@jlewi</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>

----------------------------------------

TITLE: Creating a RoleBinding for Profile Contributor Access
DESCRIPTION: YAML definition for creating a RoleBinding to grant a user access to a Kubeflow Profile. It specifies the user's email, role, and profile name.

LANGUAGE: yaml
CODE:
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: user-<SAFE_USER_EMAIL>-clusterrole-<USER_ROLE>
  namespace: <PROFILE_NAME>
  annotations:
    role: <USER_ROLE>
    user: <RAW_USER_EMAIL>
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kubeflow-<USER_ROLE>
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: <RAW_USER_EMAIL>

----------------------------------------

TITLE: Creating Python Environment for Kubeflow Pipelines
DESCRIPTION: Commands to create a clean Python 3.7 environment named 'mlpipeline' for Kubeflow Pipelines SDK installation.

LANGUAGE: bash
CODE:
conda create --name mlpipeline python=3.7
conda activate mlpipeline

----------------------------------------

TITLE: Front Matter Configuration for Elyra Doc Link
DESCRIPTION: Hugo front matter configuration block that defines metadata for the Elyra documentation link page, including title, description, weight, manual link URL and icon specification.

LANGUAGE: markdown
CODE:
+++
title = "Elyra Website"
description = "LINK | Elyra Documentation Website"
weight = 998
manualLink = "https://elyra.readthedocs.io/"
icon = "fa-solid fa-arrow-up-right-from-square"
+++

----------------------------------------

TITLE: Deploying XGBoost Training Job
DESCRIPTION: Command to create and deploy an XGBoostJob resource in Kubernetes.

LANGUAGE: bash
CODE:
kubectl create -f xgboostjob.yaml

----------------------------------------

TITLE: Kubeflow Release Info Table Structure
DESCRIPTION: HTML table structure used to display release information including release date, media links, manifests, and release team details. The table uses Bootstrap classes for responsive layout and styling.

LANGUAGE: html
CODE:
<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2020-04-18
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://blog.kubeflow.org/releases/2020/03/02/kubeflow-1-0-cloud-native-ml-for-everyone.html">Kubeflow 1.0 Release Announcement</a>
      </td>
    </tr>
  </tbody>
</table>
</div>

----------------------------------------

TITLE: Checking kubectl Installation
DESCRIPTION: Commands to verify if kubectl is installed on the system and get its location.

LANGUAGE: bash
CODE:
which kubectl

----------------------------------------

TITLE: Enabling Cache Webhook
DESCRIPTION: Command to enable caching by modifying the webhook configuration rules.

LANGUAGE: bash
CODE:
kubectl patch mutatingwebhookconfiguration cache-webhook-${NAMESPACE} --type='json' -p='[{"op":"replace", "path": "/webhooks/0/rules/0/operations/0", "value": "CREATE"}]'

----------------------------------------

TITLE: Rendering Kubeflow 0.7 Release Information Table in HTML
DESCRIPTION: This HTML snippet creates a responsive table displaying key information about the Kubeflow 0.7 release. It includes details such as release date, media links, manifest locations, and release team lead.

LANGUAGE: html
CODE:
<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2019-10-17
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://medium.com/kubeflow/kubeflow-v0-7-delivers-beta-functionality-in-the-leadup-to-v1-0-1e63036c07b8">Kubeflow 0.7 Release Announcement</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-07">Kubeflow 0.7 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v0.7.0-rc.2">v0.7.0-rc.2</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v0.7-branch">v0.7-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Richard Liu (<a href="https://github.com/richardsliu">@richardsliu</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>

----------------------------------------

TITLE: Checking Webhook Configuration
DESCRIPTION: Commands to verify the existence of cache webhook configuration in the cluster.

LANGUAGE: bash
CODE:
export NAMESPACE=<Namespace where KFP is installed>
kubectl get mutatingwebhookconfiguration cache-webhook-${NAMESPACE}

----------------------------------------

TITLE: Checking Kubeflow Pipelines SDK Version
DESCRIPTION: Command to verify the installed version of Kubeflow Pipelines SDK using pip.

LANGUAGE: bash
CODE:
pip list | grep kfp

----------------------------------------

TITLE: HTTP GET Request for Kubeflow Model Registry Artifacts
DESCRIPTION: Example HTTP GET request to list all Artifact entities from the Model Registry API. The request includes pagination and sorting parameters.

LANGUAGE: http
CODE:
https://kubeflow.example.com/api/model_registry/v1alpha3/artifacts?pageSize=100&orderBy=ID

----------------------------------------

TITLE: Pipeline Details API Response
DESCRIPTION: Example JSON response showing pipeline details after uploading

LANGUAGE: json
CODE:
{
  "id": "d30d28d7-0bfc-4f0c-8a57-6844a8ec9742",
  "created_at": "2020-02-20T16:15:02Z",
  "name": "sequential.tar.gz",
  "parameters": [
    {
      "name": "url",
      "value": "gs://ml-pipeline-playground/shakespeare1.txt"
    }
  ],
  "default_version": {
    "id": "d30d28d7-0bfc-4f0c-8a57-6844a8ec9742",
    "name": "sequential.tar.gz",
    "created_at": "2020-02-20T16:15:02Z",
    "parameters": [
      {
        "name": "url",
        "value": "gs://ml-pipeline-playground/shakespeare1.txt"
      }
    ],
    "resource_references": [
      {
        "key": {
          "type": "PIPELINE",
          "id": "d30d28d7-0bfc-4f0c-8a57-6844a8ec9742"
        },
        "relationship": "OWNER"
      }
    ]
  }
}

----------------------------------------

TITLE: Activating Python Environment for Kubeflow Pipelines
DESCRIPTION: Commands to activate a Python virtual environment for working with Kubeflow Pipelines.

LANGUAGE: bash
CODE:
source activate <YOUR-PYTHON-ENVIRONMENT-NAME>

LANGUAGE: bash
CODE:
source activate mlpipeline

----------------------------------------

TITLE: Grafana Custom Configuration in YAML
DESCRIPTION: YAML configuration for a ConfigMap to customize Grafana settings, enabling anonymous access and allowing embedding.

LANGUAGE: yaml
CODE:
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-custom-config
  namespace: knative-monitoring
  labels:
    serving.knative.dev/release: "v0.11.0"
data:
  custom.ini: |
    # You can customize Grafana via changing the context of this field.
    [auth.anonymous]
    # enable anonymous access
    enabled = true
    [security]
    allow_embedding = true
    [server]
    root_url = "/grafana"
    serve_from_sub_path = true

----------------------------------------

TITLE: Accessing Metrics via Port Forwarding in Kubernetes
DESCRIPTION: This bash command demonstrates how to use kubectl port-forward to access the Training Operator's metrics endpoint in a local environment.

LANGUAGE: bash
CODE:
kubectl port-forward -n kubeflow deployment/training-operator 8080:8080

----------------------------------------

TITLE: Compiling Kubeflow Pipeline Using DSL Compiler
DESCRIPTION: Commands to compile a Python pipeline definition into a deployable tar.gz format using the dsl-compile command.

LANGUAGE: bash
CODE:
dsl-compile --py [path/to/python/file] --output [path/to/output/tar.gz]

LANGUAGE: bash
CODE:
export DIR=[YOUR PIPELINES REPO DIRECTORY]/samples/core/sequential
dsl-compile --py ${DIR}/sequential.py --output ${DIR}/sequential.tar.gz

----------------------------------------

TITLE: Grafana VirtualService Configuration in YAML
DESCRIPTION: YAML configuration for an Istio VirtualService to expose Grafana through the Kubeflow gateway.

LANGUAGE: yaml
CODE:
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: grafana
  namespace: knative-monitoring
spec:
  gateways:
  - kubeflow/kubeflow-gateway
  hosts:
  - '*'
  http:
  - match:
    - uri:
        prefix: /grafana/
    route:
    - destination:
        host: grafana.knative-monitoring.svc.cluster.local
        port:
          number: 30802

----------------------------------------

TITLE: Listing Pipelines with Filter in KFP
DESCRIPTION: Example demonstrating how to list pipelines with a specific name filter using the Kubeflow Pipelines SDK. Uses JSON-formatted predicates for filtering.

LANGUAGE: python
CODE:
import kfp
import json

host = <host>
pipeline_name = <pipeline name>

client = kfp.Client(host)
filter = json.dumps({'predicates': [{'key': 'name', 'op': 1, 'string_value': '{}'.format(pipeline_name)}]})
pipelines = client.pipelines.list_pipelines(filter=filter)

----------------------------------------

TITLE: Adding Spark Operator Helm Repository
DESCRIPTION: Commands to add the Spark Operator Helm repository and update local repositories

LANGUAGE: shell
CODE:
helm repo add spark-operator https://kubeflow.github.io/spark-operator

helm repo update

----------------------------------------

TITLE: Embedded Video Configuration
DESCRIPTION: HTML configuration for embedding the GSoC introductory video with responsive layout settings

LANGUAGE: html
CODE:
<div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><iframe src="https://www.youtube.com/embed/93oj6b7d3VI?rel=0" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute; border: 0;" allowfullscreen scrolling="no" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share;"></iframe></div>

----------------------------------------

TITLE: Configuring SparkApplication for YuniKorn Batch Scheduling
DESCRIPTION: YAML snippet showing how to specify YuniKorn as the batch scheduler in a SparkApplication resource. It includes setting the scheduler name and queue options.

LANGUAGE: yaml
CODE:
spec:
  ...
  batchScheduler: yunikorn
  batchSchedulerOptions:
    queue: root.default

----------------------------------------

TITLE: Defining Markdown Frontmatter for Spark Operator Documentation
DESCRIPTION: YAML frontmatter block defining metadata for a documentation page about the Spark Operator reference documentation, including title, description and page weight.

LANGUAGE: markdown
CODE:
---
title: Reference
description: Reference documentation for Spark Operator
weight: 50
---

----------------------------------------

TITLE: Environment Variable Configuration
DESCRIPTION: Example showing various ways to configure environment variables for driver and executor containers.

LANGUAGE: yaml
CODE:
spec:
  driver:
    env:
      - name: ENV1
        value: VAL1
      - name: ENV3
        valueFrom:
          configMapKeyRef:
            name: some-config-map
            key: env3-key
    envFrom:
      - configMapRef:
          name: env-config-map

----------------------------------------

TITLE: Deploying Custom Spark Operator with YAML
DESCRIPTION: Configuration file path for deploying the custom Spark operator, which needs to be modified with the new operator image reference.

LANGUAGE: yaml
CODE:
/manifest/spark-operator-install/spark-operator.yaml

----------------------------------------

TITLE: Installing Locust and Dependencies
DESCRIPTION: Commands to set up a virtual environment and install Locust along with its dependencies for running the benchmark tests.

LANGUAGE: sh
CODE:
python3.12 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

----------------------------------------

TITLE: Deploying Kubeflow Pipelines using kubectl and kustomize
DESCRIPTION: These commands deploy Kubeflow Pipelines using kubectl and kustomize. It applies cluster-scoped resources and environment-specific configurations.

LANGUAGE: bash
CODE:
export PIPELINE_VERSION={{% pipelines/latest-version %}}
kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION"
kubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io
kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref=$PIPELINE_VERSION"

----------------------------------------

TITLE: Kubeflow Release Information Table
DESCRIPTION: HTML table structure containing release information including dates, media links, manifest locations and team leads for Kubeflow 0.6.x releases.

LANGUAGE: html
CODE:
<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2019-07-31
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://medium.com/kubeflow/kubeflow-v0-6-a-robust-foundation-for-artifact-tracking-data-versioning-multi-user-support-9896d329412c">Kubeflow 0.6 Release Announcement</a>
        <br>
        <b>Video:</b> 
          <a href="https://www.youtube.com/watch?v=fiFk5FB7il8">Kubeflow 0.6 Release Feature Review</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-06">Kubeflow 0.6 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v0.6.1">v0.6.1</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v0.6-branch">v0.6-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Zhenghui Wang (<a href="https://github.com/zhenghuiwang">@zhenghuiwang</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>

----------------------------------------

TITLE: Dependency Version Table Structure in HTML
DESCRIPTION: HTML table structure for displaying Kubeflow dependency versions and validation information.

LANGUAGE: HTML
CODE:
<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Dependency</th>
        <th>Validated or Included Version(s)</th>
        <th>Notes</th>
      </tr>
    </thead>
  <tbody>
      <tr>
        <td>
          <a href="https://kubernetes.io/">Kubernetes</a>
        </td>
        <td>1.25/1.26</td>
        <td>
          <i>Other versions may work...</i>
        </td>
      </tr>
  </tbody>
</table>
</div>

----------------------------------------

TITLE: Creating a GKE cluster for Kubeflow Pipelines
DESCRIPTION: This command creates a Google Kubernetes Engine cluster suitable for running Kubeflow Pipelines samples. It specifies the cluster name, zone, machine type, and necessary scopes.

LANGUAGE: bash
CODE:
CLUSTER_NAME="kubeflow-pipelines-standalone"
ZONE="us-central1-a"
MACHINE_TYPE="e2-standard-2"
SCOPES="cloud-platform"

gcloud container clusters create $CLUSTER_NAME \
     --zone $ZONE \
     --machine-type $MACHINE_TYPE \
     --scopes $SCOPES

----------------------------------------

TITLE: Configuring Kubeflow Events Page Metadata in TOML
DESCRIPTION: This TOML snippet defines metadata for a Kubeflow Events page. It sets the title and description for the page, and specifies that all child pages should have the type 'docs'.

LANGUAGE: toml
CODE:
+++
title = "Kubeflow Events"
description = "Kubeflow Community Events"

[[cascade]]
type = "docs"
+++

----------------------------------------

TITLE: Defining Hugo Front Matter in TOML Format
DESCRIPTION: TOML-formatted front matter block that defines metadata for a Hugo documentation page, including title, description and weight parameters for the reference documentation section.

LANGUAGE: toml
CODE:
+++
title = "Reference"
description = "Reference docs for Kubeflow Pipelines Version 2"
weight = 8
+++

----------------------------------------

TITLE: Creating Pipeline and Version with KFP SDK
DESCRIPTION: Example showing how to create a pipeline and add a pipeline version using the Kubeflow Pipelines SDK. Uses kfp.Client to upload pipeline files and create versions.

LANGUAGE: python
CODE:
import kfp
import os

host = <host>
pipeline_file_path = <path to pipeline file>
pipeline_name = <pipeline name>
pipeline_version_file_path = <path to pipeline version file>
pipeline_version_name = <pipeline version name>

client = kfp.Client(host)
pipeline_file = os.path.join(pipeline_file_path)
pipeline = client.pipeline_uploads.upload_pipeline(pipeline_file, name=pipeline_name)
pipeline_version_file = os.path.join(pipeline_version_file_path)
pipeline_version = client.pipeline_uploads.upload_pipeline_version(pipeline_version_file,
                                                                   name=pipeline_version_name,
                                                                   pipelineid=pipeline.id)

----------------------------------------

TITLE: Cloning Kubeflow Documentation Repository
DESCRIPTION: Commands for creating a directory, cloning a forked repository, and setting up the workspace for Kubeflow documentation.

LANGUAGE: bash
CODE:
mkdir kubeflow
cd kubeflow/
git clone git@github.com:<your-github-username>/website.git
cd website/

----------------------------------------

TITLE: Using KFP Client Manager
DESCRIPTION: Example usage of the KFPClientManager class to create an authenticated client for external access.

LANGUAGE: python
CODE:
# initialize a KFPClientManager
kfp_client_manager = KFPClientManager(
    api_url="http://localhost:8080/pipeline",
    skip_tls_verify=True,

    dex_username="user@example.com",
    dex_password="12341234",

    # can be 'ldap' or 'local' depending on your Dex configuration
    dex_auth_type="local",
)

# get a newly authenticated KFP client
# TIP: long-lived sessions might need to get a new client when their session expires
kfp_client = kfp_client_manager.create_kfp_client()

# test the client by listing experiments
experiments = kfp_client.list_experiments(namespace="my-profile")
print(experiments)

----------------------------------------

TITLE: Installing KFP SDK with Docker Dependency in Shell
DESCRIPTION: This command installs the KFP SDK with additional Docker dependency, which is required for building containerized Python components.

LANGUAGE: shell
CODE:
pip install kfp[all]

----------------------------------------

TITLE: Listing Runs for a Specific Endpoint in KFP CLI
DESCRIPTION: This command lists all runs for a specific Kubeflow Pipelines endpoint using the KFP CLI.

LANGUAGE: shell
CODE:
kfp --endpoint http://my_kfp_endpoint.com run list

----------------------------------------

TITLE: KFP Component with Custom Package Index
DESCRIPTION: Example showing how to use pip_index_urls to install packages from custom package repositories.

LANGUAGE: python
CODE:
@dsl.component(packages_to_install=['custom-ml-package==0.0.1', 'numpy==1.21.6'],
               pip_index_urls=['http://myprivaterepo.com/simple', 'http://pypi.org/simple'],
)
def comp():
    from custom_ml_package import model_trainer
    import numpy as np
    ...

LANGUAGE: bash
CODE:
pip install custom-ml-package==0.0.1 numpy==1.21.6 kfp==2 --index-url http://myprivaterepo.com/simple --trusted-host http://myprivaterepo.com/simple --extra-index-url http://pypi.org/simple --trusted-host http://pypi.org/simple

----------------------------------------

TITLE: Deploying Kubeflow Pipelines with Emissary Executor
DESCRIPTION: Command to deploy a new Kubeflow Pipelines cluster using the Emissary executor in a platform-agnostic environment.

LANGUAGE: bash
CODE:
kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-emissary?ref=$PIPELINE_VERSION"

----------------------------------------

TITLE: Configuring Emissary Executor in Kubeflow Pipelines
DESCRIPTION: A series of kubectl commands to check the current executor, switch to the Emissary executor, and verify the change in a Kubeflow Pipelines cluster.

LANGUAGE: bash
CODE:
kubectl config set-context --current --namespace <your-kfp-namespace>

kubectl describe configmap workflow-controller-configmap | grep -A 2 containerRuntimeExecutor

kubectl patch configmap workflow-controller-configmap --patch '{"data":{"containerRuntimeExecutor":"emissary"}}'

kubectl describe configmap workflow-controller-configmap | grep -A 2 containerRuntimeExecutor

----------------------------------------

TITLE: Inspecting Docker Image Entrypoint and CMD
DESCRIPTION: Docker command to inspect the default ENTRYPOINT and CMD of an image, useful for determining the correct 'command' field when migrating to Emissary executor.

LANGUAGE: bash
CODE:
docker image inspect -f '{{.Config.Entrypoint}} {{.Config.Cmd}}' hello-world

----------------------------------------

TITLE: Disabling Caching via CLI Command
DESCRIPTION: Shows how to disable caching by default during pipeline compilation using the KFP CLI

LANGUAGE: bash
CODE:
kfp dsl compile --py my_pipeline.py --output my_pipeline.yaml --disable-execution-caching-by-default

----------------------------------------

TITLE: Installing kind on Linux
DESCRIPTION: Downloads and installs kind binary on Linux systems by downloading the executable and moving it to PATH directory.

LANGUAGE: shell
CODE:
curl -Lo ./kind https://kind.sigs.k8s.io/dl/{KIND_VERSION}/kind-linux-amd64 && \
chmod +x ./kind && \
mv ./kind /{YOUR_KIND_DIRECTORY}/kind

----------------------------------------

TITLE: Installing KFP SDK
DESCRIPTION: Command to install the Kubeflow Pipelines SDK version 2 using pip

LANGUAGE: sh
CODE:
pip install kfp

----------------------------------------

TITLE: Installing Model Registry on Kubeflow Profile
DESCRIPTION: Commands to install Model Registry on a specific Kubeflow Profile namespace using Kustomize.

LANGUAGE: shell
CODE:
kustomize set namespace <your-profile>
kubectl apply -k .

----------------------------------------

TITLE: Installing KServe Support for Model Registry
DESCRIPTION: Command to apply the CustomStorageContainer CR for KServe to support model-registry:// URI formats.

LANGUAGE: shell
CODE:
MODEL_REGISTRY_VERSION={{% model-registry/latest-version %}}
kubectl apply -k "https://github.com/kubeflow/model-registry/manifests/kustomize/options/csi?ref=v${MODEL_REGISTRY_VERSION}"

----------------------------------------

TITLE: Checking Model Registry Deployment Status
DESCRIPTION: Commands to check the status of the Model Registry deployment and view its logs using kubectl.

LANGUAGE: shell
CODE:
kubectl wait --for=condition=available -n kubeflow deployment/model-registry-deployment --timeout=1m
kubectl logs -n kubeflow deployment/model-registry-deployment

----------------------------------------

TITLE: Accessing Model Registry REST API
DESCRIPTION: Commands to port-forward the Model Registry service and make a sample API request using curl.

LANGUAGE: shell
CODE:
kubectl port-forward svc/model-registry-service -n kubeflow 8081:8080
# in another terminal:
curl -X 'GET' \
  'http://localhost:8081/api/model_registry/v1alpha3/registered_models?pageSize=100&orderBy=ID&sortOrder=DESC' \
  -H 'accept: application/json' | jq

----------------------------------------

TITLE: Registering Model Metadata
DESCRIPTION: Example of registering a scikit-learn model with metadata including model format, version, and custom attributes.

LANGUAGE: python
CODE:
rm = registry.register_model(
    "iris",
    "gs://kfserving-examples/models/sklearn/1.0/model",
    model_format_name="sklearn",
    model_format_version="1",
    version="v1",
    description="Iris scikit-learn model",
    metadata={
        "accuracy": 3.14,
        "license": "BSD 3-Clause License",
    }
)

----------------------------------------

TITLE: Installing Katib Control Plane Stable Release
DESCRIPTION: Command to install the stable v0.17.0 release of Katib control plane using kubectl apply with kustomize

LANGUAGE: shell
CODE:
kubectl apply -k "github.com/kubeflow/katib.git/manifests/v1beta1/installs/katib-standalone?ref=v0.17.0"

----------------------------------------

TITLE: Verifying Katib Installation
DESCRIPTION: Command to verify that all Katib control plane components are running in the kubeflow namespace

LANGUAGE: shell
CODE:
kubectl get pods -n kubeflow

----------------------------------------

TITLE: Installing Katib on OpenShift
DESCRIPTION: This command deploys Katib on OpenShift v4.4+. It uses the OpenShift service controller instead of Katib certificate generator to provision Katib webhook certificates.

LANGUAGE: shell
CODE:
kubectl apply -k "github.com/kubeflow/katib.git/manifests/v1beta1/installs/katib-openshift?ref=master"

----------------------------------------

TITLE: JSON Format for File-based Metrics Collection in Katib
DESCRIPTION: Example of JSON format for metrics logged to a file, which can be collected by Katib's File metrics collector. Metrics are line-separated by epoch or step.

LANGUAGE: json
CODE:
{"epoch": 0, "foo": "bar", "fizz": "buzz", "timestamp": "2021-12-02T14:27:51"}
{"epoch": 1, "foo": "bar", "fizz": "buzz", "timestamp": "2021-12-02T14:27:52"}
{"epoch": 2, "foo": "bar", "fizz": "buzz", "timestamp": "2021-12-02T14:27:53"}
{"epoch": 3, "foo": "bar", "fizz": "buzz", "timestamp": "2021-12-02T14:27:54"}

----------------------------------------

TITLE: Katib Controller Connection String Format
DESCRIPTION: Format string used by Katib Controller to connect to DB Manager service using environment variables.

LANGUAGE: text
CODE:
KATIB_DB_MANAGER_SERVICE_IP.KATIB_DB_MANAGER_SERVICE_NAMESPACE:KATIB_DB_MANAGER_SERVICE_PORT

----------------------------------------

TITLE: Default Success Condition for Kubernetes Jobs
DESCRIPTION: GJSON format condition that determines when a Kubernetes Job Trial has succeeded.

LANGUAGE: text
CODE:
status.conditions.#(type=="Complete")#|#(status=="True")#

----------------------------------------

TITLE: Default Success Condition for Kubeflow Jobs
DESCRIPTION: GJSON format condition that determines when a Kubeflow Job Trial (TFJob, PyTorchJob, MXJob, XGBoostJob) has succeeded.

LANGUAGE: text
CODE:
status.conditions.#(type=="Succeeded")#|#(status=="True")#

----------------------------------------

TITLE: Katib Controller ClusterRole for Tekton Integration
DESCRIPTION: YAML configuration for adding Tekton Pipeline resources to Katib controller's ClusterRole rules.

LANGUAGE: yaml
CODE:
- apiGroups:
    - tekton.dev
  resources:
    - pipelineruns
    - taskruns
  verbs:
    - "get"
    - "list"
    - "watch"
    - "create"
    - "delete"

----------------------------------------

TITLE: Katib UI Access URL
DESCRIPTION: URL to access the Katib UI after setting up port forwarding.

LANGUAGE: shell
CODE:
http://localhost:8080/katib/

----------------------------------------

TITLE: Configuring Hugo Frontmatter for Katib Reference Docs
DESCRIPTION: TOML-formatted Hugo frontmatter that defines the metadata for the Katib reference documentation page. Sets the page title, description and weight for navigation ordering.

LANGUAGE: toml
CODE:
+++
title = "Reference"
description = "Reference docs for Katib"
weight = 50
+++

----------------------------------------

TITLE: Listing Available Makefile Targets
DESCRIPTION: Display the list of available Makefile targets for the Spark operator project.

LANGUAGE: bash
CODE:
make help

----------------------------------------

TITLE: Building Spark Operator with Custom Spark Image
DESCRIPTION: Build the Spark operator Docker image using a custom Spark base image.

LANGUAGE: shell
CODE:
docker build --build-arg SPARK_IMAGE=<your Spark image> -t <image-tag> .

----------------------------------------

TITLE: Running Unit Tests
DESCRIPTION: Execute the unit tests for the Spark operator project.

LANGUAGE: shell
CODE:
make unit-test

----------------------------------------

TITLE: Generating Helm Chart Documentation
DESCRIPTION: Generate the README.md file for the Helm chart using helm-docs.

LANGUAGE: shell
CODE:
make helm-docs

----------------------------------------

TITLE: Running and Monitoring a TFJob
DESCRIPTION: Bash commands for deploying, monitoring, and deleting a TFJob using kubectl.

LANGUAGE: Bash
CODE:
kubectl create -f https://raw.githubusercontent.com/kubeflow/training-operator/refs/heads/release-1.9/examples/tensorflow/simple.yaml

kubectl -n kubeflow get tfjob tfjob-simple -o yaml

kubectl -n kubeflow delete tfjob tfjob-simple

----------------------------------------

TITLE: Querying Stackdriver Logs for TFJob
DESCRIPTION: Bash command to query Stackdriver logs for a specific TFJob using gcloud.

LANGUAGE: Bash
CODE:
QUERY="resource.type=\"k8s_container\" "
QUERY="${QUERY} resource.labels.cluster_name=\"${CLUSTER}\" "
QUERY="${QUERY} metadata.userLabels.job-name=\"${JOB_NAME}\" "
QUERY="${QUERY} metadata.userLabels.replica-type=\"${TYPE}\" "
QUERY="${QUERY} metadata.userLabels.replica-index=\"${INDEX}\" "
gcloud --project=${PROJECT} logging read  \
     --freshness=24h \
     --order asc  ${QUERY}

----------------------------------------

TITLE: Listing PyTorchJob Pods
DESCRIPTION: This command lists the pods created by the PyTorchJob, filtered by the job name label.

LANGUAGE: bash
CODE:
kubectl get pods -l training.kubeflow.org/job-name=pytorch-simple -n kubeflow

----------------------------------------

TITLE: Viewing XGBoost Training Logs
DESCRIPTION: Commands to get the master pod name and view its training logs.

LANGUAGE: bash
CODE:
PODNAME=$(kubectl get pods -l job-name=xgboost-dist-iris-test-train,replica-type=master,replica-index=0 -o name)
kubectl logs -f ${PODNAME}

----------------------------------------

TITLE: Monitoring XGBoost Job Status
DESCRIPTION: Command to get detailed status of the XGBoost training job.

LANGUAGE: bash
CODE:
kubectl get -o yaml xgboostjobs xgboost-dist-iris-test-train

----------------------------------------

TITLE: Verifying Controller Manager Deployment
DESCRIPTION: Command to check the status of the JobSet and Trainer controller manager pods in the kubeflow-system namespace. This helps ensure that the deployment was successful.

LANGUAGE: bash
CODE:
$ kubectl get pods -n kubeflow-system

NAME                                                  READY   STATUS    RESTARTS   AGE
jobset-controller-manager-54968bd57b-88dk4            2/2     Running   0          65s
kubeflow-trainer-controller-manager-cc6468559-dblnw   1/1     Running   0          65s

----------------------------------------

TITLE: Checking Notebook Events in Kubeflow using kubectl
DESCRIPTION: This command describes the Notebook resource and displays its events, helping to identify any errors preventing the notebook from starting.

LANGUAGE: shell
CODE:
kubectl describe notebooks "${MY_NOTEBOOK_NAME}" --namespace "${MY_PROFILE_NAMESPACE}"

----------------------------------------

TITLE: Inspecting Pod Events in Kubeflow using kubectl
DESCRIPTION: This command describes the Pod associated with the Notebook and displays its events, useful for troubleshooting startup issues.

LANGUAGE: shell
CODE:
kubectl describe pod "${MY_NOTEBOOK_NAME}-0" --namespace "${MY_PROFILE_NAMESPACE}"

----------------------------------------

TITLE: Installing Hugo 0.124.1 with Homebrew
DESCRIPTION: Commands to install a specific version of Hugo (0.124.1) using Homebrew on macOS or Linux. This ensures compatibility with the website's deployment.

LANGUAGE: bash
CODE:
HOMEBREW_COMMIT="9d025105a8be086b2eeb3b1b2697974f848dbaac" # 0.124.1
curl -fL -o "hugo.rb" "https://raw.githubusercontent.com/Homebrew/homebrew-core/${HOMEBREW_COMMIT}/Formula/h/hugo.rb"
brew install ./hugo.rb
brew pin hugo

----------------------------------------

TITLE: Setting Up Local Kubeflow Website Development Environment
DESCRIPTION: Steps to clone the Kubeflow website repository, initialize submodules, install dependencies, and start a local Hugo server for development.

LANGUAGE: bash
CODE:
git clone git@github.com:<your-github-username>/website.git
cd website/
git submodule update --init --recursive
(cd themes/docsy/ && npm install)
hugo server -D

----------------------------------------

TITLE: Updating Docsy Theme in Kubeflow Website
DESCRIPTION: Command to update the Docsy theme submodule to a specific version (v0.6.0) in the Kubeflow website repository.

LANGUAGE: bash
CODE:
git -C themes/docsy fetch --tags
git -C themes/docsy checkout tags/v0.6.0

----------------------------------------

TITLE: Using Hugo Shortcodes in Kubeflow Documentation
DESCRIPTION: Example of defining and using a Hugo shortcode to maintain consistent versioning information across the Kubeflow documentation.

LANGUAGE: markdown
CODE:
You need Kubernetes version {{% kubernetes-min-version %}} or later.

----------------------------------------

TITLE: Checking Git Remote Repositories
DESCRIPTION: Command to verify the configured remote repositories in the local Git setup.

LANGUAGE: bash
CODE:
git remote -vv

----------------------------------------

TITLE: Creating and Switching to Feature Branch
DESCRIPTION: Command to create and checkout a new branch for documentation updates.

LANGUAGE: bash
CODE:
git checkout -b doc-updates

----------------------------------------

TITLE: OWNERS File with Emeritus Approvers in YAML
DESCRIPTION: Example showing how to configure emeritus approvers in an OWNERS file

LANGUAGE: yaml
CODE:
emeritus_approvers:
  - david
  - emily