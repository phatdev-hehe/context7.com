TITLE: Extracting Tool Calls from Steps in TypeScript
DESCRIPTION: Demonstrates how to extract all tool calls from the steps returned by generateText. This is useful for analyzing the sequence of tool calls made during multi-step generations.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';

const { steps } = await generateText({
  model: openai('gpt-4-turbo'),
  maxSteps: 10,
  // ...
});

// extract all tool calls from the steps:
const allToolCalls = steps.flatMap(step => step.toolCalls);

----------------------------------------

TITLE: Creating React Component for Multi-Step Tool Calling Chat Interface
DESCRIPTION: This code snippet shows how to create a React component using the useChat hook from the AI SDK. It sets up a chat interface that can handle multiple tool calls in sequence, with a maximum of 5 steps.

LANGUAGE: tsx
CODE:
'use client';

import { useChat } from '@ai-sdk/react';

export default function Page() {
  const { messages, input, setInput, append } = useChat({
    api: '/api/chat',
    maxSteps: 5,
  });

  return (
    <div>
      <input
        value={input}
        onChange={event => {
          setInput(event.target.value);
        }}
        onKeyDown={async event => {
          if (event.key === 'Enter') {
            append({ content: input, role: 'user' });
          }
        }}
      />

      {messages.map((message, index) => (
        <div key={index}>{message.content}</div>
      ))}
    </div>
  );
}

----------------------------------------

TITLE: Generating Structured Recipe Data with AI SDK and Zod Schema
DESCRIPTION: Demonstrates how to use generateObject to create a structured recipe object with TypeScript and Zod schema validation. The example shows generating a lasagna recipe with defined structure for name, ingredients, and steps using the GPT-4 Turbo model.

LANGUAGE: typescript
CODE:
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const result = await generateObject({
  model: openai('gpt-4-turbo'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(
        z.object({
          name: z.string(),
          amount: z.string(),
        }),
      ),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a lasagna recipe.',
});

console.log(JSON.stringify(result.object.recipe, null, 2));

----------------------------------------

TITLE: Generating Structured City Predictions using DeepSeek and OpenAI Models
DESCRIPTION: This code demonstrates how to generate structured predictions about future cities by combining a reasoning model (DeepSeek) with a structured output model (OpenAI). It uses Zod for schema validation and handles the two-step process of generating raw text and then converting it to structured data.

LANGUAGE: typescript
CODE:
import { deepseek } from '@ai-sdk/deepseek';
import { openai } from '@ai-sdk/openai';
import { generateObject, generateText } from 'ai';
import 'dotenv/config';
import { z } from 'zod';

async function main() {
  const { text: rawOutput } = await generateText({
    model: deepseek('deepseek-reasoner'),
    prompt:
      'Predict the top 3 largest city by 2050. For each, return the name, the country, the reason why it will on the list, and the estimated population in millions.',
  });

  const { object } = await generateObject({
    model: openai('gpt-4o-mini'),
    prompt: 'Extract the desired information from this text: \n' + rawOutput,
    schema: z.object({
      name: z.string().describe('the name of the city'),
      country: z.string().describe('the name of the country'),
      reason: z
        .string()
        .describe(
          'the reason why the city will be one of the largest cities by 2050',
        ),
      estimatedPopulation: z.number(),
    }),
    output: 'array',
  });

  console.log(object);
}

main().catch(console.error);

----------------------------------------

TITLE: Implementing Chat API Route with OpenAI and Streaming in Next.js
DESCRIPTION: This snippet demonstrates how to create an API route for the chat functionality using OpenAI and streaming responses. It sets up a POST handler that streams text responses.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai('gpt-4-turbo'),
    system: 'You are a helpful assistant.',
    messages,
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Advanced Text Generation with System Prompt
DESCRIPTION: Shows how to use generateText with a system prompt to provide additional context and instructions to the model.

LANGUAGE: tsx
CODE:
import { generateText } from 'ai';

const { text } = await generateText({
  model: yourModel,
  system: 'You are a professional writer. ' +
    'You write simple, clear, and concise content.',
  prompt: `Summarize the following article in 3-5 sentences: ${article}`,
});

----------------------------------------

TITLE: Wrapping Language Model with Middleware in TypeScript
DESCRIPTION: Demonstrates how to use the wrapLanguageModel function to apply middleware to a language model. This allows for enhancing the model's behavior with features like guardrails, RAG, caching, and logging.

LANGUAGE: typescript
CODE:
import { wrapLanguageModel } from 'ai';

const wrappedLanguageModel = wrapLanguageModel({
  model: yourModel,
  middleware: yourLanguageModelMiddleware,
});

----------------------------------------

TITLE: Implementing Parallel Tool Calls in TypeScript with AI SDK
DESCRIPTION: Example showing how to configure and execute multiple tools (weather and city attractions) in parallel using the AI SDK. The code demonstrates tool definition with Zod schemas for parameter validation, async execution functions, and parallel invocation through generateText.

LANGUAGE: typescript
CODE:
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const result = await generateText({
  model: openai('gpt-4-turbo'),
  tools: {
    weather: tool({
      description: 'Get the weather in a location',
      parameters: z.object({
        location: z.string().describe('The location to get the weather for'),
      }),
      execute: async ({ location }: { location: string }) => ({
        location,
        temperature: 72 + Math.floor(Math.random() * 21) - 10,
      }),
    }),
    cityAttractions: tool({
      parameters: z.object({ city: z.string() }),
      execute: async ({ city }: { city: string }) => {
        if (city === 'San Francisco') {
          return {
            attractions: [
              'Golden Gate Bridge',
              'Alcatraz Island',
              "Fisherman's Wharf",
            ],
          };
        } else {
          return { attractions: [] };
        }
      },
    }),
  },
  prompt:
    'What is the weather in San Francisco and what attractions should I visit?',
});

console.log(result);

----------------------------------------

TITLE: Configuring Environment Variables for AI Providers
DESCRIPTION: Environment configuration for OpenAI and Anthropic API keys used by the chatbot.

LANGUAGE: env
CODE:
OPENAI_API_KEY=xxxxxxxxx
ANTHROPIC_API_KEY=xxxxxxxxx

----------------------------------------

TITLE: Implementing Orchestrator-Worker Pattern for Feature Implementation in TypeScript
DESCRIPTION: This code demonstrates the orchestrator-worker pattern for implementing software features. It uses the AI SDK to create an implementation plan (orchestrator) and then executes specific file changes (workers). This pattern is effective for complex tasks requiring different types of expertise or processing.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateObject } from 'ai';
import { z } from 'zod';

async function implementFeature(featureRequest: string) {
  // Orchestrator: Plan the implementation
  const { object: implementationPlan } = await generateObject({
    model: openai('o3-mini'),
    schema: z.object({
      files: z.array(
        z.object({
          purpose: z.string(),
          filePath: z.string(),
          changeType: z.enum(['create', 'modify', 'delete']),
        }),
      ),
      estimatedComplexity: z.enum(['low', 'medium', 'high']),
    }),
    system:
      'You are a senior software architect planning feature implementations.',
    prompt: `Analyze this feature request and create an implementation plan:
    ${featureRequest}`,
  });

  // Workers: Execute the planned changes
  const fileChanges = await Promise.all(
    implementationPlan.files.map(async file => {
      // Each worker is specialized for the type of change
      const workerSystemPrompt = {
        create:
          'You are an expert at implementing new files following best practices and project patterns.',
        modify:
          'You are an expert at modifying existing code while maintaining consistency and avoiding regressions.',
        delete:
          'You are an expert at safely removing code while ensuring no breaking changes.',
      }[file.changeType];

      const { object: change } = await generateObject({
        model: openai('gpt-4o'),
        schema: z.object({
          explanation: z.string(),
          code: z.string(),
        }),
        system: workerSystemPrompt,
        prompt: `Implement the changes for ${file.filePath} to support:
        ${file.purpose}

        Consider the overall feature context:
        ${featureRequest}`,
      });

      return {
        file,
        implementation: change,
      };
    }),
  );

  return {
    plan: implementationPlan,
    changes: fileChanges,
  };
}

----------------------------------------

TITLE: Building a Chatbot API with Next.js and Llama 3.1
DESCRIPTION: This code creates an API route for a chatbot using Next.js, the AI SDK, and Llama 3.1 via DeepInfra. It streams the model's responses for up to 30 seconds.

LANGUAGE: tsx
CODE:
import { streamText } from 'ai';
import { deepinfra } from '@ai-sdk/deepinfra';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),
    system: 'You are a helpful assistant.',
    messages,
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Implementing Multi-Step Tool Usage for Math Problem Solving in TypeScript
DESCRIPTION: This code demonstrates how to create an agent that solves math problems using multi-step tool usage. It utilizes the AI SDK's `maxSteps` parameter to allow the model to make multiple tool calls and solve complex problems iteratively. The example includes a calculator tool and structured answer output.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateText, tool } from 'ai';
import * as mathjs from 'mathjs';
import { z } from 'zod';

const { toolCalls } = await generateText({
  model: openai('gpt-4o-2024-08-06', { structuredOutputs: true }),
  tools: {
    calculate: tool({
      description:
        'A tool for evaluating mathematical expressions. Example expressions: ' +
        "'1.2 * (2 + 4.5)', '12.7 cm to inch', 'sin(45 deg) ^ 2'.",
      parameters: z.object({ expression: z.string() }),
      execute: async ({ expression }) => mathjs.evaluate(expression),
    }),
    // answer tool: the LLM will provide a structured answer
    answer: tool({
      description: 'A tool for providing the final answer.',
      parameters: z.object({
        steps: z.array(
          z.object({
            calculation: z.string(),
            reasoning: z.string(),
          }),
        ),
        answer: z.string(),
      }),
      // no execute function - invoking it will terminate the agent
    }),
  },
  toolChoice: 'required',
  maxSteps: 10,
  system:
    'You are solving math problems. ' +
    'Reason step by step. ' +
    'Use the calculator when necessary. ' +
    'The calculator can only do simple additions, subtractions, multiplications, and divisions. ' +
    'When you give the final answer, provide an explanation for how you got it.',
  prompt:
    'A taxi driver earns $9461 per 1-hour work. ' +
    'If he works 12 hours a day and in 1 hour he uses 14-liters petrol with price $134 for 1-liter. ' +
    'How much money does he earn in one day?',
});

console.log(`FINAL TOOL CALLS: ${JSON.stringify(toolCalls, null, 2)}`);

----------------------------------------

TITLE: Building a Chat Interface with Next.js and AI SDK UI
DESCRIPTION: This React component uses the useChat hook from AI SDK UI to create an interactive chat interface. It handles user input, message submission, and displays the chat history, integrating seamlessly with the OpenAI GPT-4.5 model on the backend.

LANGUAGE: tsx
CODE:
'use client';

import { useChat } from '@ai-sdk/react';

export default function Page() {
  const { messages, input, handleInputChange, handleSubmit, error } = useChat();

  return (
    <>
      {messages.map(message => (
        <div key={message.id}>
          {message.role === 'user' ? 'User: ' : 'AI: '}
          {message.content}
        </div>
      ))}
      <form onSubmit={handleSubmit}>
        <input name="prompt" value={input} onChange={handleInputChange} />
        <button type="submit">Submit</button>
      </form>
    </>
  );
}

----------------------------------------

TITLE: Enhancing Flight Booking with Contact Lookup in AI Multistep Interface
DESCRIPTION: Shows how adding a lookupContacts tool can streamline the booking process by automatically retrieving passenger information, reducing the number of steps required from the user.

LANGUAGE: txt
CODE:
User: I want to book a flight from New York to London.
Tool: searchFlights("New York", "London")
Model: Here are the available flights from New York to London.
User: I want to book flight number BA123 on 12th December for myself an my wife.
Tool: lookupContacts() -> ["John Doe", "Jane Doe"]
Tool: bookFlight("BA123", "12th December", ["John Doe", "Jane Doe"])
Model: Your flight has been booked!

----------------------------------------

TITLE: Applying Multiple Middlewares to Language Model
DESCRIPTION: Illustrates how to apply multiple middlewares to a language model using the wrapLanguageModel function. The middlewares are applied in the order they are provided.

LANGUAGE: typescript
CODE:
const wrappedLanguageModel = wrapLanguageModel({
  model: yourModel,
  middleware: [firstMiddleware, secondMiddleware],
});

// applied as: firstMiddleware(secondMiddleware(yourModel))

----------------------------------------

TITLE: Creating a Chat Route Handler with Next.js and OpenAI GPT-4.5
DESCRIPTION: This code snippet sets up a route handler for a chat endpoint in a Next.js application. It uses the AI SDK and OpenAI's GPT-4.5 model to stream text responses based on the incoming chat messages.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';

// Allow responses up to 30 seconds
export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai('gpt-4.5-preview'),
    messages,
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Processing Messages and PDFs with Anthropic's Claude Model in Next.js API Route
DESCRIPTION: This code creates a route handler that uses Anthropic's Claude model to process messages and PDFs. It streams the text response from the AI model.

LANGUAGE: typescript
CODE:
import { anthropic } from '@ai-sdk/anthropic';
import { streamText } from 'ai';

export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: anthropic('claude-3-5-sonnet-latest'),
    messages,
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Implementing AI Logic with OpenAI and Custom Tools
DESCRIPTION: This function generates responses using the AI SDK with OpenAI's GPT-4 model. It includes custom tools for getting weather data and searching the web, enabling multi-step conversations and enhanced functionality.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { CoreMessage, generateText, tool } from 'ai';
import { z } from 'zod';
import { exa } from './utils';

export const generateResponse = async (
  messages: CoreMessage[],
  updateStatus?: (status: string) => void,
) => {
  const { text } = await generateText({
    model: openai('gpt-4o'),
    system: `You are a Slack bot assistant. Keep your responses concise and to the point.
    - Do not tag users.
    - Current date is: ${new Date().toISOString().split('T')[0]}
    - Always include sources in your final response if you use web search.`,
    messages,
    maxSteps: 10,
    tools: {
      getWeather: tool({
        description: 'Get the current weather at a location',
        parameters: z.object({
          latitude: z.number(),
          longitude: z.number(),
          city: z.string(),
        }),
        execute: async ({ latitude, longitude, city }) => {
          updateStatus?.(`is getting weather for ${city}...`);

          const response = await fetch(
            `https://api.open-meteo.com/v1/forecast?latitude=${latitude}&longitude=${longitude}&current=temperature_2m,weathercode,relativehumidity_2m&timezone=auto`,
          );

          const weatherData = await response.json();
          return {
            temperature: weatherData.current.temperature_2m,
            weatherCode: weatherData.current.weathercode,
            humidity: weatherData.current.relativehumidity_2m,
            city,
          };
        },
      }),
      searchWeb: tool({
        description: 'Use this to search the web for information',
        parameters: z.object({
          query: z.string(),
          specificDomain: z
            .string()
            .nullable()
            .describe(
              'a domain to search if the user specifies e.g. bbc.com. Should be only the domain name without the protocol',
            ),
        }),
        execute: async ({ query, specificDomain }) => {
          updateStatus?.(`is searching the web for ${query}...`);
          const { results } = await exa.searchAndContents(query, {
            livecrawl: 'always',
            numResults: 3,
            includeDomains: specificDomain ? [specificDomain] : undefined,
          });

          return {
            results: results.map(result => ({
              title: result.title,
              url: result.url,
              snippet: result.text.slice(0, 1000),
            })),
          };
        },
      }),
    },
  });

  // Convert markdown to Slack mrkdwn format
  return text.replace(/\[(.*?)\]\((.*?)\)/g, '<$2|$1>').replace(/\*\*/g, '*');
};

----------------------------------------

TITLE: Implementing Routing for Customer Query Handling in TypeScript
DESCRIPTION: This code showcases a routing pattern for handling customer queries. It uses the AI SDK to classify the query type and complexity, then routes the query to an appropriate model and system prompt based on the classification. This demonstrates how to adapt the processing based on the input context.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateObject, generateText } from 'ai';
import { z } from 'zod';

async function handleCustomerQuery(query: string) {
  const model = openai('gpt-4o');

  // First step: Classify the query type
  const { object: classification } = await generateObject({
    model,
    schema: z.object({
      reasoning: z.string(),
      type: z.enum(['general', 'refund', 'technical']),
      complexity: z.enum(['simple', 'complex']),
    }),
    prompt: `Classify this customer query:
    ${query}

    Determine:
    1. Query type (general, refund, or technical)
    2. Complexity (simple or complex)
    3. Brief reasoning for classification`,
  });

  // Route based on classification
  // Set model and system prompt based on query type and complexity
  const { text: response } = await generateText({
    model:
      classification.complexity === 'simple'
        ? openai('gpt-4o-mini')
        : openai('o3-mini'),
    system: {
      general:
        'You are an expert customer service agent handling general inquiries.',
      refund:
        'You are a customer service agent specializing in refund requests. Follow company policy and collect necessary information.',
      technical:
        'You are a technical support specialist with deep product knowledge. Focus on clear step-by-step troubleshooting.',
    }[classification.type],
    prompt: query,
  });

  return { response, classification };
}

----------------------------------------

TITLE: Using Wrapped Language Model in StreamText Function
DESCRIPTION: Shows how to use a wrapped language model with middleware in the streamText function, demonstrating that it can be used like any other language model.

LANGUAGE: typescript
CODE:
const result = streamText({
  model: wrappedLanguageModel,
  prompt: 'What cities are in the United States?',
});

----------------------------------------

TITLE: Implementing Client-Side Object Streaming with Next.js and React
DESCRIPTION: This React component uses the useObject hook from the AI SDK to stream object generation. It displays a button to trigger generation and renders the notifications as they are received.

LANGUAGE: tsx
CODE:
'use client';

import { experimental_useObject as useObject } from '@ai-sdk/react';
import { notificationSchema } from './api/use-object/schema';

export default function Page() {
  const { object, submit } = useObject({
    api: '/api/use-object',
    schema: notificationSchema,
  });

  return (
    <div>
      <button onClick={() => submit('Messages during finals week.')}>
        Generate notifications
      </button>

      {object?.notifications?.map((notification, index) => (
        <div key={index}>
          <p>{notification?.name}</p>
          <p>{notification?.message}</p>
        </div>
      ))}
    </div>
  );
}

----------------------------------------

TITLE: Implementing Client-Side Chat Interface with React and AI SDK
DESCRIPTION: A React component that implements a chat interface with support for tool calls, weather display, and user confirmation dialogs. Uses the useChat hook from @ai-sdk/react to manage chat state and handle tool invocations.

LANGUAGE: tsx
CODE:
'use client';

import { ToolInvocation } from 'ai';
import { Message, useChat } from '@ai-sdk/react';

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit, addToolResult } =
    useChat({
      api: '/api/use-chat',
      maxSteps: 5,

      // run client-side tools that are automatically executed:
      async onToolCall({ toolCall }) {
        if (toolCall.toolName === 'getLocation') {
          const cities = [
            'New York',
            'Los Angeles',
            'Chicago',
            'San Francisco',
          ];
          return cities[Math.floor(Math.random() * cities.length)];
        }
      },
    });

  return (
    <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch gap-4">
      {messages?.map((m: Message) => (
        <div key={m.id} className="whitespace-pre-wrap flex flex-col gap-1">
          <strong>{`${m.role}: `}</strong>
          {m.content}
          {m.toolInvocations?.map((toolInvocation: ToolInvocation) => {
            const toolCallId = toolInvocation.toolCallId;

            // render confirmation tool (client-side tool with user interaction)
            if (toolInvocation.toolName === 'askForConfirmation') {
              return (
                <div
                  key={toolCallId}
                  className="text-gray-500 flex flex-col gap-2"
                >
                  {toolInvocation.args.message}
                  <div className="flex gap-2">
                    {'result' in toolInvocation ? (
                      <b>{toolInvocation.result}</b>
                    ) : (
                      <>
                        <button
                          className="px-4 py-2 font-bold text-white bg-blue-500 rounded hover:bg-blue-700"
                          onClick={() =>
                            addToolResult({
                              toolCallId,
                              result: 'Yes, confirmed.',
                            })
                          }
                        >
                          Yes
                        </button>
                        <button
                          className="px-4 py-2 font-bold text-white bg-red-500 rounded hover:bg-red-700"
                          onClick={() =>
                            addToolResult({
                              toolCallId,
                              result: 'No, denied',
                            })
                          }
                        >
                          No
                        </button>
                      </>
                    )}
                  </div>
                </div>
              );
            }

            // other tools:
            return 'result' in toolInvocation ? (
              toolInvocation.toolName === 'getWeatherInformation' ? (
                <div
                  key={toolCallId}
                  className="flex flex-col gap-2 p-4 bg-blue-400 rounded-lg"
                >
                  <div className="flex flex-row justify-between items-center">
                    <div className="text-4xl text-blue-50 font-medium">
                      {toolInvocation.result.value}°
                      {toolInvocation.result.unit === 'celsius' ? 'C' : 'F'}
                    </div>

                    <div className="h-9 w-9 bg-amber-400 rounded-full flex-shrink-0" />
                  </div>
                  <div className="flex flex-row gap-2 text-blue-50 justify-between">
                    {toolInvocation.result.weeklyForecast.map(
                      (forecast: any) => (
                        <div
                          key={forecast.day}
                          className="flex flex-col items-center"
                        >
                          <div className="text-xs">{forecast.day}</div>
                          <div>{forecast.value}°</div>
                        </div>
                      ),
                    )}
                  </div>
                </div>
              ) : toolInvocation.toolName === 'getLocation' ? (
                <div
                  key={toolCallId}
                  className="text-gray-500 bg-gray-100 rounded-lg p-4"
                >
                  User is in {toolInvocation.result}.
                </div>
              ) : (
                <div key={toolCallId} className="text-gray-500">
                  Tool call {`${toolInvocation.toolName}: `}
                  {toolInvocation.result}
                </div>
              )
            ) : (
              <div key={toolCallId} className="text-gray-500">
                Calling {toolInvocation.toolName}...
              </div>
            );
          })}
        </div>
      ))}

      <form onSubmit={handleSubmit}>
        <input
          className="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl"
          value={input}
          placeholder="Say something..."
          onChange={handleInputChange}
        />
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Implementing Rate Limiting with Vercel KV and Upstash Ratelimit in TypeScript
DESCRIPTION: This code snippet demonstrates how to implement rate limiting for an API endpoint using Vercel KV and Upstash Ratelimit. It sets a limit of 5 requests per 30 seconds for each IP address and handles streaming responses from OpenAI.

LANGUAGE: typescript
CODE:
import kv from '@vercel/kv';
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import { Ratelimit } from '@upstash/ratelimit';
import { NextRequest } from 'next/server';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

// Create Rate limit
const ratelimit = new Ratelimit({
  redis: kv,
  limiter: Ratelimit.fixedWindow(5, '30s'),
});

export async function POST(req: NextRequest) {
  // call ratelimit with request ip
  const ip = req.ip ?? 'ip';
  const { success, remaining } = await ratelimit.limit(ip);

  // block the request if unsuccessfull
  if (!success) {
    return new Response('Ratelimited!', { status: 429 });
  }

  const { messages } = await req.json();

  const result = streamText({
    model: openai('gpt-3.5-turbo'),
    messages,
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Implementing Evaluator-Optimizer Pattern for Translation in TypeScript
DESCRIPTION: This code showcases the evaluator-optimizer pattern for improving translations. It uses the AI SDK to generate an initial translation, evaluate its quality, and iteratively improve it based on feedback. This pattern creates more robust workflows capable of self-improvement and error recovery.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateText, generateObject } from 'ai';
import { z } from 'zod';

async function translateWithFeedback(text: string, targetLanguage: string) {
  let currentTranslation = '';
  let iterations = 0;
  const MAX_ITERATIONS = 3;

  // Initial translation
  const { text: translation } = await generateText({
    model: openai('gpt-4o-mini'), // use small model for first attempt
    system: 'You are an expert literary translator.',
    prompt: `Translate this text to ${targetLanguage}, preserving tone and cultural nuances:
    ${text}`,
  });

  currentTranslation = translation;

  // Evaluation-optimization loop
  while (iterations < MAX_ITERATIONS) {
    // Evaluate current translation
    const { object: evaluation } = await generateObject({
      model: openai('gpt-4o'), // use a larger model to evaluate
      schema: z.object({
        qualityScore: z.number().min(1).max(10),
        preservesTone: z.boolean(),
        preservesNuance: z.boolean(),
        culturallyAccurate: z.boolean(),
        specificIssues: z.array(z.string()),
        improvementSuggestions: z.array(z.string()),
      }),
      system: 'You are an expert in evaluating literary translations.',
      prompt: `Evaluate this translation:

      Original: ${text}
      Translation: ${currentTranslation}

      Consider:
      1. Overall quality
      2. Preservation of tone
      3. Preservation of nuance
      4. Cultural accuracy`,
    });

    // Check if quality meets threshold
    if (
      evaluation.qualityScore >= 8 &&
      evaluation.preservesTone &&
      evaluation.preservesNuance &&
      evaluation.culturallyAccurate
    ) {
      break;
    }

    // Generate improved translation based on feedback
    const { text: improvedTranslation } = await generateText({
      model: openai('gpt-4o'), // use a larger model
      system: 'You are an expert literary translator.',
      prompt: `Improve this translation based on the following feedback:
      ${evaluation.specificIssues.join('\n')}
      ${evaluation.improvementSuggestions.join('\n')}

      Original: ${text}
      Current Translation: ${currentTranslation}`,
    });

    currentTranslation = improvedTranslation;
    iterations++;
  }

  return {
    finalTranslation: currentTranslation,
    iterationsRequired: iterations,
  };
}

----------------------------------------

TITLE: Handling PDF Analysis with AI SDK in Next.js API Route
DESCRIPTION: This server-side code snippet demonstrates how to create an API route in Next.js that receives a PDF file, sends it to an LLM for analysis using the AI SDK, and returns a structured summary. It uses the generateObject function and Zod for schema validation.

LANGUAGE: typescript
CODE:
import { generateObject } from 'ai';
import { anthropic } from '@ai-sdk/anthropic';
import { z } from 'zod';

export async function POST(request: Request) {
  const formData = await request.formData();
  const file = formData.get('pdf') as File;

  const result = await generateObject({
    model: anthropic('claude-3-5-sonnet-latest'),
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'text',
            text: 'Analyze the following PDF and generate a summary.',
          },
          {
            type: 'file',
            data: await file.arrayBuffer(),
            mimeType: 'application/pdf',
          },
        ],
      },
    ],
    schema: z.object({
      summary: z.string().describe('A 50 word sumamry of the PDF.'),
    }),
  });

  return new Response(result.object.summary);
}

----------------------------------------

TITLE: Generating Text with OpenAI Integration
DESCRIPTION: Example of using AI SDK Core to generate text using OpenAI's GPT-4 model. Demonstrates basic text generation with system prompt and user input.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai'; // Ensure OPENAI_API_KEY environment variable is set

const { text } = await generateText({
  model: openai('gpt-4o'),
  system: 'You are a friendly assistant!',
  prompt: 'Why is the sky blue?',
});

console.log(text);

----------------------------------------

TITLE: Implementing Sequential Processing for Marketing Copy Generation in TypeScript
DESCRIPTION: This code demonstrates a sequential processing pattern for generating and improving marketing copy. It uses the AI SDK to generate initial copy, perform a quality check, and regenerate if necessary. The process involves multiple steps with each output feeding into the next step.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateText, generateObject } from 'ai';
import { z } from 'zod';

async function generateMarketingCopy(input: string) {
  const model = openai('gpt-4o');

  // First step: Generate marketing copy
  const { text: copy } = await generateText({
    model,
    prompt: `Write persuasive marketing copy for: ${input}. Focus on benefits and emotional appeal.`,
  });

  // Perform quality check on copy
  const { object: qualityMetrics } = await generateObject({
    model,
    schema: z.object({
      hasCallToAction: z.boolean(),
      emotionalAppeal: z.number().min(1).max(10),
      clarity: z.number().min(1).max(10),
    }),
    prompt: `Evaluate this marketing copy for:
    1. Presence of call to action (true/false)
    2. Emotional appeal (1-10)
    3. Clarity (1-10)

    Copy to evaluate: ${copy}`,
  });

  // If quality check fails, regenerate with more specific instructions
  if (
    !qualityMetrics.hasCallToAction ||
    qualityMetrics.emotionalAppeal < 7 ||
    qualityMetrics.clarity < 7
  ) {
    const { text: improvedCopy } = await generateText({
      model,
      prompt: `Rewrite this marketing copy with:
      ${!qualityMetrics.hasCallToAction ? '- A clear call to action' : ''}
      ${qualityMetrics.emotionalAppeal < 7 ? '- Stronger emotional appeal' : ''}
      ${qualityMetrics.clarity < 7 ? '- Improved clarity and directness' : ''}

      Original copy: ${copy}`,
    });
    return { copy: improvedCopy, qualityMetrics };
  }

  return { copy, qualityMetrics };
}

----------------------------------------

TITLE: Embedding a Single Value with OpenAI in TypeScript
DESCRIPTION: Demonstrates how to use the 'embed' function from the AI SDK to embed a single text value using OpenAI's embedding model. The function returns an embedding object containing a vector representation of the input text.

LANGUAGE: tsx
CODE:
import { embed } from 'ai';
import { openai } from '@ai-sdk/openai';

// 'embedding' is a single embedding object (number[])
const { embedding } = await embed({
  model: openai.embedding('text-embedding-3-small'),
  value: 'sunny day at the beach',
});

----------------------------------------

TITLE: Implementing Parallel Processing for Code Review in TypeScript
DESCRIPTION: This code demonstrates a parallel processing pattern for conducting code reviews. It uses the AI SDK to run multiple specialized reviews concurrently (security, performance, maintainability), then aggregates the results. This approach improves efficiency by executing independent tasks simultaneously.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateText, generateObject } from 'ai';
import { z } from 'zod';

// Example: Parallel code review with multiple specialized reviewers
async function parallelCodeReview(code: string) {
  const model = openai('gpt-4o');

  // Run parallel reviews
  const [securityReview, performanceReview, maintainabilityReview] =
    await Promise.all([
      generateObject({
        model,
        system:
          'You are an expert in code security. Focus on identifying security vulnerabilities, injection risks, and authentication issues.',
        schema: z.object({
          vulnerabilities: z.array(z.string()),
          riskLevel: z.enum(['low', 'medium', 'high']),
          suggestions: z.array(z.string()),
        }),
        prompt: `Review this code:
      ${code}`,
      }),

      generateObject({
        model,
        system:
          'You are an expert in code performance. Focus on identifying performance bottlenecks, memory leaks, and optimization opportunities.',
        schema: z.object({
          issues: z.array(z.string()),
          impact: z.enum(['low', 'medium', 'high']),
          optimizations: z.array(z.string()),
        }),
        prompt: `Review this code:
      ${code}`,
      }),

      generateObject({
        model,
        system:
          'You are an expert in code quality. Focus on code structure, readability, and adherence to best practices.',
        schema: z.object({
          concerns: z.array(z.string()),
          qualityScore: z.number().min(1).max(10),
          recommendations: z.array(z.string()),
        }),
        prompt: `Review this code:
      ${code}`,
      }),
    ]);

  const reviews = [
    { ...securityReview.object, type: 'security' },
    { ...performanceReview.object, type: 'performance' },
    { ...maintainabilityReview.object, type: 'maintainability' },
  ];

  // Aggregate results using another model instance
  const { text: summary } = await generateText({
    model,
    system: 'You are a technical lead summarizing multiple code reviews.',
    prompt: `Synthesize these code review results into a concise summary with key actions:
    ${JSON.stringify(reviews, null, 2)}`,
  });

  return { reviews, summary };
}

----------------------------------------

TITLE: Implementing Chat API Route with Parallel Tool Calling in Next.js
DESCRIPTION: This server-side code creates an API route for handling chat requests. It uses the streamText function from the AI SDK to generate responses and demonstrates how to implement a custom getWeather tool that can be called in parallel with other tools.

LANGUAGE: ts
CODE:
import { ToolInvocation, streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

interface Message {
  role: 'user' | 'assistant';
  content: string;
  toolInvocations?: ToolInvocation[];
}

function getWeather({ city, unit }) {
  return { value: 25, description: 'Sunny' };
}

export async function POST(req: Request) {
  const { messages }: { messages: Message[] } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    system: 'You are a helpful assistant.',
    messages,
    tools: {
      getWeather: {
        description: 'Get the weather for a location',
        parameters: z.object({
          city: z.string().describe('The city to get the weather for'),
          unit: z
            .enum(['C', 'F'])
            .describe('The unit to display the temperature in'),
        }),
        execute: async ({ city, unit }) => {
          const { value, description } = getWeather({ city, unit });
          return `It is currently ${value}°${unit} and ${description} in ${city}!`;
        },
      },
    },
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Exa Web Search Tool Implementation
DESCRIPTION: Custom web search tool implementation using the Exa API, demonstrating how to create a reusable search tool that can be used with any model supporting tool calling.

LANGUAGE: typescript
CODE:
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';
import Exa from 'exa-js';

export const exa = new Exa(process.env.EXA_API_KEY);

export const webSearch = tool({
  description: 'Search the web for up-to-date information',
  parameters: z.object({
    query: z.string().min(1).max(100).describe('The search query'),
  }),
  execute: async ({ query }) => {
    const { results } = await exa.searchAndContents(query, {
      livecrawl: 'always',
      numResults: 3,
    });
    return results.map(result => ({
      title: result.title,
      url: result.url,
      content: result.text.slice(0, 1000),
      publishedDate: result.publishedDate,
    }));
  },
});

const { text } = await generateText({
  model: openai('gpt-4o-mini'),
  prompt: 'What happened in San Francisco last week?',
  tools: {
    webSearch,
  },
  maxSteps: 2,
});

----------------------------------------

TITLE: Implementing Chat Interface with Tool Support in Next.js
DESCRIPTION: A client-side React component using the useChat hook to handle chat interactions and tool calls. The component manages chat messages, user input, and renders the conversation history.

LANGUAGE: tsx
CODE:
'use client';

import { useChat } from '@ai-sdk/react';

export default function Page() {
  const { messages, input, setInput, append } = useChat({
    api: '/api/chat',
    maxSteps: 2,
  });

  return (
    <div>
      <input
        value={input}
        onChange={event => {
          setInput(event.target.value);
        }}
        onKeyDown={async event => {
          if (event.key === 'Enter') {
            append({ content: input, role: 'user' });
          }
        }}
      />

      {messages.map((message, index) => (
        <div key={index}>{message.content}</div>
      ))}
    </div>
  );
}

----------------------------------------

TITLE: Implementing Server-Side Actions for AI-Powered Chat
DESCRIPTION: This server-side code defines the actions for continuing the conversation, including AI text generation and custom tool functions for displaying stock and flight information.

LANGUAGE: tsx
CODE:
'use server';

import { getMutableAIState, streamUI } from 'ai/rsc';
import { openai } from '@ai-sdk/openai';
import { ReactNode } from 'react';
import { z } from 'zod';
import { generateId } from 'ai';
import { Stock } from '@/components/stock';
import { Flight } from '@/components/flight';

export interface ServerMessage {
  role: 'user' | 'assistant';
  content: string;
}

export interface ClientMessage {
  id: string;
  role: 'user' | 'assistant';
  display: ReactNode;
}

export async function continueConversation(
  input: string,
): Promise<ClientMessage> {
  'use server';

  const history = getMutableAIState();

  const result = await streamUI({
    model: openai('gpt-3.5-turbo'),
    messages: [...history.get(), { role: 'user', content: input }],
    text: ({ content, done }) => {
      if (done) {
        history.done((messages: ServerMessage[]) => [
          ...messages,
          { role: 'assistant', content },
        ]);
      }

      return <div>{content}</div>;
    },
    tools: {
      showStockInformation: {
        description:
          'Get stock information for symbol for the last numOfMonths months',
        parameters: z.object({
          symbol: z
            .string()
            .describe('The stock symbol to get information for'),
          numOfMonths: z
            .number()
            .describe('The number of months to get historical information for'),
        }),
        generate: async ({ symbol, numOfMonths }) => {
          history.done((messages: ServerMessage[]) => [
            ...messages,
            {
              role: 'assistant',
              content: `Showing stock information for ${symbol}`,
            },
          ]);

          return <Stock symbol={symbol} numOfMonths={numOfMonths} />;
        },
      },
      showFlightStatus: {
        description: 'Get the status of a flight',
        parameters: z.object({
          flightNumber: z
            .string()
            .describe('The flight number to get status for'),
        }),
        generate: async ({ flightNumber }) => {
          history.done((messages: ServerMessage[]) => [
            ...messages,
            {
              role: 'assistant',
              content: `Showing flight status for ${flightNumber}`,
            },
          ]);

          return <Flight flightNumber={flightNumber} />;
        },
      },
    },
  });

  return {
    id: generateId(),
    role: 'assistant',
    display: result.value,
  };
}

----------------------------------------

TITLE: Basic Object Generation with Zod Schema
DESCRIPTION: Demonstrates how to use generateObject with a Zod schema to generate a structured recipe object.

LANGUAGE: typescript
CODE:
import { generateObject } from 'ai';
import { z } from 'zod';

const { object } = await generateObject({
  model: yourModel,
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a lasagna recipe.',
});

----------------------------------------

TITLE: Implementing Server-Side Text Generation with TypeScript and AI SDK
DESCRIPTION: This server-side function uses the AI SDK's generateText function to generate text based on a given prompt. It utilizes the OpenAI model and returns the generated text along with finish reason and usage information.

LANGUAGE: typescript
CODE:
'use server';

import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

export async function getAnswer(question: string) {
  const { text, finishReason, usage } = await generateText({
    model: openai('gpt-3.5-turbo'),
    prompt: question,
  });

  return { text, finishReason, usage };
}

----------------------------------------

TITLE: Specifying Image Size with OpenAI's DALL-E 3
DESCRIPTION: Demonstrates how to set a specific image size when generating an image with DALL-E 3.

LANGUAGE: tsx
CODE:
import { experimental_generateImage as generateImage } from 'ai';
import { openai } from '@ai-sdk/openai';

const { image } = await generateImage({
  model: openai.image('dall-e-3'),
  prompt: 'Santa Claus driving a Cadillac',
  size: '1024x1024',
});

----------------------------------------

TITLE: Implementing OpenAI Web Search with AI SDK
DESCRIPTION: Example of using OpenAI's Responses API with web_search_preview tool to perform web searches. Uses the @ai-sdk/openai package to generate text with web search capabilities.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

const { text, sources } = await generateText({
  model: openai.responses('gpt-4o-mini'),
  prompt: 'What happened in San Francisco last week?',
  tools: {
    web_search_preview: openai.tools.webSearchPreview(),
  },
});

console.log(text);
console.log(sources);

----------------------------------------

TITLE: Creating an API Route for Chat Functionality
DESCRIPTION: Implements an API route that handles chat requests, uses the OpenAI provider, and streams text responses. It also includes tool definitions for weather and temperature conversion.

LANGUAGE: typescript
CODE:
import { streamText, tool } from 'ai';
import { createOpenAI } from '@ai-sdk/openai';
import { z } from 'zod';

export default defineLazyEventHandler(async () => {
  const apiKey = useRuntimeConfig().openaiApiKey;
  if (!apiKey) throw new Error('Missing OpenAI API key');
  const openai = createOpenAI({
    apiKey: apiKey,
  });

  return defineEventHandler(async (event: any) => {
    const { messages } = await readBody(event);

    const result = streamText({
      model: openai('gpt-4o-preview'),
      messages,
      tools: {
        weather: tool({
          description: 'Get the weather in a location (fahrenheit)',
          parameters: z.object({
            location: z
              .string()
              .describe('The location to get the weather for'),
          }),
          execute: async ({ location }) => {
            const temperature = Math.round(Math.random() * (90 - 32) + 32);
            return {
              location,
              temperature,
            };
          },
        }),
        convertFahrenheitToCelsius: tool({
          description: 'Convert a temperature in fahrenheit to celsius',
          parameters: z.object({
            temperature: z
              .number()
              .describe('The temperature in fahrenheit to convert'),
          }),
          execute: async ({ temperature }) => {
            const celsius = Math.round((temperature - 32) * (5 / 9));
            return {
              celsius,
            };
          },
        }),
      },
    });

    return result.toDataStreamResponse();
  });
});

----------------------------------------

TITLE: Initializing Basic Chatbot UI with useChat Hook in React
DESCRIPTION: This snippet shows how to set up a basic chatbot UI using the useChat hook. It renders messages, handles input changes, and submits new messages.

LANGUAGE: tsx
CODE:
'use client';

import { useChat } from '@ai-sdk/react';

export default function Page() {
  const { messages, input, handleInputChange, handleSubmit } = useChat({});

  return (
    <>
      {messages.map(message => (
        <div key={message.id}>
          {message.role === 'user' ? 'User: ' : 'AI: '}
          {message.content}
        </div>
      ))}

      <form onSubmit={handleSubmit}>
        <input name="prompt" value={input} onChange={handleInputChange} />
        <button type="submit">Submit</button>
      </form>
    </>
  );
}

----------------------------------------

TITLE: Basic AI Text Generation Settings in TypeScript
DESCRIPTION: Demonstrates basic usage of common AI SDK settings including maxTokens, temperature, and maxRetries for text generation.

LANGUAGE: typescript
CODE:
const result = await generateText({
  model: yourModel,
  maxTokens: 512,
  temperature: 0.3,
  maxRetries: 5,
  prompt: 'Invent a new holiday and describe its traditions.'
});

----------------------------------------

TITLE: Creating Custom Provider with Model Aliases in TypeScript
DESCRIPTION: This example shows how to create a custom provider that uses model name aliases, allowing for easier updates to model versions in the future.

LANGUAGE: typescript
CODE:
import { anthropic as originalAnthropic } from '@ai-sdk/anthropic';
import { customProvider } from 'ai';

// custom provider with alias names:
export const anthropic = customProvider({
  languageModels: {
    opus: originalAnthropic('claude-3-opus-20240229'),
    sonnet: originalAnthropic('claude-3-5-sonnet-20240620'),
    haiku: originalAnthropic('claude-3-haiku-20240307'),
  },
  fallbackProvider: originalAnthropic,
});

----------------------------------------

TITLE: Implementing Server-Side AI Tool Integration
DESCRIPTION: A server action that uses the AI SDK to generate responses and handle tool calls. Implements a temperature conversion tool using Zod for parameter validation.

LANGUAGE: tsx
CODE:
'use server';

import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

export interface Message {
  role: 'user' | 'assistant';
  content: string;
}

export async function continueConversation(history: Message[]) {
  'use server';

  const { text, toolResults } = await generateText({
    model: openai('gpt-3.5-turbo'),
    system: 'You are a friendly assistant!',
    messages: history,
    tools: {
      celsiusToFahrenheit: {
        description: 'Converts celsius to fahrenheit',
        parameters: z.object({
          value: z.string().describe('The value in celsius'),
        }),
        execute: async ({ value }) => {
          const celsius = parseFloat(value);
          const fahrenheit = celsius * (9 / 5) + 32;
          return `${celsius}°C is ${fahrenheit.toFixed(2)}°F`;
        },
      },
    },
  });

  return {
    messages: [
      ...history,
      {
        role: 'assistant' as const,
        content:
          text || toolResults.map(toolResult => toolResult.result).join('\n'),
      },
    ],
  };
}

----------------------------------------

TITLE: Creating Streamable Message Component with AI SDK
DESCRIPTION: This snippet defines a Message component that uses the useStreamableValue hook from the AI SDK to render streaming text content.

LANGUAGE: tsx
CODE:
'use client';

import { StreamableValue, useStreamableValue } from 'ai/rsc';

export function Message({ textStream }: { textStream: StreamableValue }) {
  const [text] = useStreamableValue(textStream);

  return <div>{text}</div>;
}

----------------------------------------

TITLE: Handling Errors in Chatbot UI with React
DESCRIPTION: This snippet demonstrates how to handle and display errors in the chatbot UI, including showing an error message and a retry button when an error occurs.

LANGUAGE: tsx
CODE:
'use client';

import { useChat } from '@ai-sdk/react';

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit, error, reload } =
    useChat({});

  return (
    <div>
      {messages.map(m => (
        <div key={m.id}>
          {m.role}: {m.content}
        </div>
      ))}

      {error && (
        <>
          <div>An error occurred.</div>
          <button type="button" onClick={() => reload()}>
            Retry
          </button>
        </>
      )}

      <form onSubmit={handleSubmit}>
        <input
          value={input}
          onChange={handleInputChange}
          disabled={error != null}
        />
      </form>
    </div>
  );
}

----------------------------------------

TITLE: StreamUI with Weather Tool Implementation
DESCRIPTION: Extended example showing streamUI implementation with a weather tool that includes loading states and async data fetching. Demonstrates generator function usage for streaming components.

LANGUAGE: tsx
CODE:
const result = await streamUI({
  model: openai('gpt-4o'),
  prompt: 'Get the weather for San Francisco',
  text: ({ content }) => <div>{content}</div>,
  tools: {
    getWeather: {
      description: 'Get the weather for a location',
      parameters: z.object({ location: z.string() }),
      generate: async function* ({ location }) {
        yield <LoadingComponent />;
        const weather = await getWeather(location);
        return <WeatherComponent weather={weather} location={location} />;
      },
    },
  },
});

----------------------------------------

TITLE: Implementing Chat UI with React
DESCRIPTION: Creates the frontend chat interface using the AI SDK's useChat hook to handle message streaming and tool interactions

LANGUAGE: tsx
CODE:
'use client';

import { useChat } from '@ai-sdk/react';

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat({
    maxSteps: 3,
  });
  return (
    <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch">
      <div className="space-y-4">
        {messages.map(m => (
          <div key={m.id} className="whitespace-pre-wrap">
            <div>
              <div className="font-bold">{m.role}</div>
              <p>
                {m.content.length > 0 ? (
                  m.content
                ) : (
                  <span className="italic font-light">
                    {'calling tool: ' + m?.toolInvocations?.[0].toolName}
                  </span>
                )}
              </p>
            </div>
          </div>
        ))}
      </div>

      <form onSubmit={handleSubmit}>
        <input
          className="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl"
          value={input}
          placeholder="Say something..."
          onChange={handleInputChange}
        />
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Implementing Server-Side Text Generation Streaming with OpenAI in Next.js
DESCRIPTION: This code snippet demonstrates how to create a Next.js API route handler for text generation streaming. It uses the streamText function from the ai module and the openai model to generate and stream text based on the input prompt.

LANGUAGE: typescript
CODE:
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';

export async function POST(req: Request) {
  const { prompt }: { prompt: string } = await req.json();

  const result = streamText({
    model: openai('gpt-4'),
    system: 'You are a helpful assistant.',
    prompt,
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Generating Text with OpenAI o1-mini using AI SDK Core
DESCRIPTION: Demonstrates how to use the AI SDK Core to generate text using the OpenAI o1-mini model. This snippet shows the basic setup required to make an API call to the model.

LANGUAGE: tsx
CODE:
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const { text } = await generateText({
  model: openai('o1-mini'),
  prompt: 'Explain the concept of quantum entanglement.',
});

----------------------------------------

TITLE: Configuring Retries for Embedding in TypeScript
DESCRIPTION: Demonstrates how to set the maximum number of retries for the embedding process using the 'maxRetries' parameter. This can be used to control error handling and reliability of the embedding operation.

LANGUAGE: ts
CODE:
import { openai } from '@ai-sdk/openai';
import { embed } from 'ai';

const { embedding } = await embed({
  model: openai.embedding('text-embedding-3-small'),
  value: 'sunny day at the beach',
  maxRetries: 0, // Disable retries
});

----------------------------------------

TITLE: Implementing Sequential AI Generations in TypeScript
DESCRIPTION: This code snippet demonstrates how to create a sequence of dependent AI generations using the AI SDK. It generates blog post ideas about making spaghetti, selects the best idea, and then creates an outline based on that idea. The example uses the OpenAI GPT-4 model for text generation.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

async function sequentialActions() {
  // Generate blog post ideas
  const ideasGeneration = await generateText({
    model: openai('gpt-4o'),
    prompt: 'Generate 10 ideas for a blog post about making spaghetti.',
  });

  console.log('Generated Ideas:\n', ideasGeneration);

  // Pick the best idea
  const bestIdeaGeneration = await generateText({
    model: openai('gpt-4o'),
    prompt: `Here are some blog post ideas about making spaghetti:
${ideasGeneration}

Pick the best idea from the list above and explain why it's the best.`,
  });

  console.log('\nBest Idea:\n', bestIdeaGeneration);

  // Generate an outline
  const outlineGeneration = await generateText({
    model: openai('gpt-4o'),
    prompt: `We've chosen the following blog post idea about making spaghetti:
${bestIdeaGeneration}

Create a detailed outline for a blog post based on this idea.`,
  });

  console.log('\nBlog Post Outline:\n', outlineGeneration);
}

sequentialActions().catch(console.error);

----------------------------------------

TITLE: Implementing Server-Side Object Streaming with Next.js API Route
DESCRIPTION: This Next.js API route uses the streamObject function from the AI SDK to generate and stream object data. It uses OpenAI's GPT-4 model to generate notifications based on the provided context.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamObject } from 'ai';
import { notificationSchema } from './schema';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export async function POST(req: Request) {
  const context = await req.json();

  const result = streamObject({
    model: openai('gpt-4-turbo'),
    schema: notificationSchema,
    prompt:
      `Generate 3 notifications for a messages app in this context:` + context,
  });

  return result.toTextStreamResponse();
}

----------------------------------------

TITLE: Implementing Multi-Modal Chat Interface
DESCRIPTION: React component implementing the chat interface with support for image and PDF uploads using the useChat hook.

LANGUAGE: tsx
CODE:
'use client';

import { useChat } from '@ai-sdk/react';
import { useRef, useState } from 'react';
import Image from 'next/image';

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat();

  const [files, setFiles] = useState<FileList | undefined>(undefined);
  const fileInputRef = useRef<HTMLInputElement>(null);

  return (
    <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch">
      {messages.map(m => (
        <div key={m.id} className="whitespace-pre-wrap">
          {m.role === 'user' ? 'User: ' : 'AI: '}
          {m.content}
          <div>
            {m?.experimental_attachments
              ?.filter(
                attachment =>
                  attachment?.contentType?.startsWith('image/') ||
                  attachment?.contentType?.startsWith('application/pdf'),
              )
              .map((attachment, index) =>
                attachment.contentType?.startsWith('image/') ? (
                  <Image
                    key={`${m.id}-${index}`}
                    src={attachment.url}
                    width={500}
                    height={500}
                    alt={attachment.name ?? `attachment-${index}`}
                  />
                ) : attachment.contentType?.startsWith('application/pdf') ? (
                  <iframe
                    key={`${m.id}-${index}`}
                    src={attachment.url}
                    width={500}
                    height={600}
                    title={attachment.name ?? `attachment-${index}`}
                  />
                ) : null,
              )}
          </div>
        </div>
      ))}

      <form
        className="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl space-y-2"
        onSubmit={event => {
          handleSubmit(event, {
            experimental_attachments: files,
          });

          setFiles(undefined);

          if (fileInputRef.current) {
            fileInputRef.current.value = '';
          }
        }}
      >
        <input
          type="file"
          className=""
          onChange={event => {
            if (event.target.files) {
              setFiles(event.target.files);
            }
          }}
          multiple
          ref={fileInputRef}
        />
        <input
          className="w-full p-2"
          value={input}
          placeholder="Say something..."
          onChange={handleInputChange}
        />
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Implementing Server Action for Multistep AI Interface in TypeScript
DESCRIPTION: This code defines a server action that uses streamUI to create a multistep interface for a flight booking assistant. It includes tools for searching flights and looking up flight details.

LANGUAGE: typescript
CODE:
import { streamUI } from 'ai/rsc';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const searchFlights = async (
  source: string,
  destination: string,
  date: string,
) => {
  return [
    {
      id: '1',
      flightNumber: 'AA123',
    },
    {
      id: '2',
      flightNumber: 'AA456',
    },
  ];
};

const lookupFlight = async (flightNumber: string) => {
  return {
    flightNumber: flightNumber,
    departureTime: '10:00 AM',
    arrivalTime: '12:00 PM',
  };
};

export async function submitUserMessage(input: string) {
  'use server';

  const ui = await streamUI({
    model: openai('gpt-4o'),
    system: 'you are a flight booking assistant',
    prompt: input,
    text: async ({ content }) => <div>{content}</div>,
    tools: {
      searchFlights: {
        description: 'search for flights',
        parameters: z.object({
          source: z.string().describe('The origin of the flight'),
          destination: z.string().describe('The destination of the flight'),
          date: z.string().describe('The date of the flight'),
        }),
        generate: async function* ({ source, destination, date }) {
          yield `Searching for flights from ${source} to ${destination} on ${date}...`;
          const results = await searchFlights(source, destination, date);

          return (
            <div>
              {results.map(result => (
                <div key={result.id}>
                  <div>{result.flightNumber}</div>
                </div>
              ))}
            </div>
          );
        },
      },
      lookupFlight: {
        description: 'lookup details for a flight',
        parameters: z.object({
          flightNumber: z.string().describe('The flight number'),
        }),
        generate: async function* ({ flightNumber }) {
          yield `Looking up details for flight ${flightNumber}...`;
          const details = await lookupFlight(flightNumber);

          return (
            <div>
              <div>Flight Number: {details.flightNumber}</div>
              <div>Departure Time: {details.departureTime}</div>
              <div>Arrival Time: {details.arrivalTime}</div>
            </div>
          );
        },
      },
    },
  });

  return ui.value;
}

----------------------------------------

TITLE: Defining and Using a Weather Tool in TypeScript
DESCRIPTION: Demonstrates how to define a weather tool using the AI SDK Core and use it in a generateText call. The tool takes a location parameter and returns simulated weather data.

LANGUAGE: typescript
CODE:
import { z } from 'zod';
import { generateText, tool } from 'ai';

const result = await generateText({
  model: yourModel,
  tools: {
    weather: tool({
      description: 'Get the weather in a location',
      parameters: z.object({
        location: z.string().describe('The location to get the weather for'),
      }),
      execute: async ({ location }) => ({
        location,
        temperature: 72 + Math.floor(Math.random() * 21) - 10,
      }),
    }),
  },
  prompt: 'What is the weather in San Francisco?',
});

----------------------------------------

TITLE: Implementing Client-Side Chat Interface with useAssistant Hook
DESCRIPTION: A React component implementing a chat interface using the useAssistant hook. Handles message display, input management, and status updates.

LANGUAGE: tsx
CODE:
'use client';

import { Message, useAssistant } from '@ai-sdk/react';

export default function Chat() {
  const { status, messages, input, submitMessage, handleInputChange } =
    useAssistant({ api: '/api/assistant' });

  return (
    <div>
      {messages.map((m: Message) => (
        <div key={m.id}>
          <strong>{`${m.role}: `}</strong>
          {m.role !== 'data' && m.content}
          {m.role === 'data' && (
            <>
              {(m.data as any).description}
              <br />
              <pre className={'bg-gray-200'}>
                {JSON.stringify(m.data, null, 2)}
              </pre>
            </>
          )}
        </div>
      ))}

      {status === 'in_progress' && <div />}

      <form onSubmit={submitMessage}>
        <input
          disabled={status !== 'awaiting_message'}
          value={input}
          placeholder="What is the temperature in the living room?"
          onChange={handleInputChange}
        />
      </form>
    </div>
  );

----------------------------------------

TITLE: Conditional Rendering of Multiple UI Components in React TypeScript
DESCRIPTION: This code snippet shows how to conditionally render multiple UI components based on the tool name returned by the language model. It demonstrates the complexity that can arise when handling multiple tools and interfaces.

LANGUAGE: tsx
CODE:
{
  message.role === 'tool' ? (
    message.name === 'api-search-course' ? (
      <Courses courses={message.content} />
    ) : message.name === 'api-search-profile' ? (
      <People people={message.content} />
    ) : message.name === 'api-meetings' ? (
      <Meetings meetings={message.content} />
    ) : message.name === 'api-search-building' ? (
      <Buildings buildings={message.content} />
    ) : message.name === 'api-events' ? (
      <Events events={message.content} />
    ) : message.name === 'api-meals' ? (
      <Meals meals={message.content} />
    ) : null
  ) : (
    <div>{message.content}</div>
  );
}

----------------------------------------

TITLE: Rendering Streamed UI Components in Next.js
DESCRIPTION: This client-side component demonstrates how to render the server-streamed UI components generated by Llama 3.1 in a Next.js application.

LANGUAGE: tsx
CODE:
'use client';

import { useState } from 'react';
import { streamComponent } from './actions';

export default function Page() {
  const [component, setComponent] = useState<React.ReactNode>();

  return (
    <div>
      <form
        onSubmit={async e => {
          e.preventDefault();
          setComponent(await streamComponent());
        }}
      >
        <button>Stream Component</button>
      </form>
      <div>{component}</div>
    </div>
  );
}

----------------------------------------

TITLE: Implementing Text Generation API Route in Next.js
DESCRIPTION: This snippet demonstrates how to create a route handler for '/api/completion' that generates text using the AI SDK. It uses the 'generateText' function from the 'ai' module and the OpenAI model to process the input prompt and return generated text.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

export async function POST(req: Request) {
  const { prompt }: { prompt: string } = await req.json();

  const { text } = await generateText({
    model: openai('gpt-4'),
    system: 'You are a helpful assistant.',
    prompt,
  });

  return Response.json({ text });
}

----------------------------------------

TITLE: Creating OpenAI Language Model
DESCRIPTION: Create an OpenAI language model instance with optional settings.

LANGUAGE: typescript
CODE:
const model = openai('gpt-4-turbo');

const modelWithSettings = openai('gpt-4-turbo', {
  // additional settings
});

----------------------------------------

TITLE: Implementing Redis Caching Middleware for AI SDK
DESCRIPTION: A middleware implementation that enables caching of AI responses using Redis. Handles both streaming and non-streaming responses with support for timestamp formatting and simulated stream playback.

LANGUAGE: typescript
CODE:
import { Redis } from '@upstash/redis';
import {
  type LanguageModelV1,
  type LanguageModelV1Middleware,
  type LanguageModelV1StreamPart,
  simulateReadableStream,
} from 'ai';

const redis = new Redis({
  url: process.env.KV_URL,
  token: process.env.KV_TOKEN,
});

export const cacheMiddleware: LanguageModelV1Middleware = {
  wrapGenerate: async ({ doGenerate, params }) => {
    const cacheKey = JSON.stringify(params);

    const cached = (await redis.get(cacheKey)) as Awaited<
      ReturnType<LanguageModelV1['doGenerate']>
    > | null;

    if (cached !== null) {
      return {
        ...cached,
        response: {
          ...cached.response,
          timestamp: cached?.response?.timestamp
            ? new Date(cached?.response?.timestamp)
            : undefined,
        },
      };
    }

    const result = await doGenerate();

    redis.set(cacheKey, result);

    return result;
  },
  wrapStream: async ({ doStream, params }) => {
    const cacheKey = JSON.stringify(params);

    const cached = await redis.get(cacheKey);

    if (cached !== null) {
      const formattedChunks = (cached as LanguageModelV1StreamPart[]).map(p => {
        if (p.type === 'response-metadata' && p.timestamp) {
          return { ...p, timestamp: new Date(p.timestamp) };
        } else return p;
      });
      return {
        stream: simulateReadableStream({
          initialDelayInMs: 0,
          chunkDelayInMs: 10,
          chunks: formattedChunks,
        }),
        rawCall: { rawPrompt: null, rawSettings: {} },
      };
    }

    const { stream, ...rest } = await doStream();

    const fullResponse: LanguageModelV1StreamPart[] = [];

    const transformStream = new TransformStream<
      LanguageModelV1StreamPart,
      LanguageModelV1StreamPart
    >({
      transform(chunk, controller) {
        fullResponse.push(chunk);
        controller.enqueue(chunk);
      },
      flush() {
        redis.set(cacheKey, fullResponse);
      },
    });

    return {
      stream: stream.pipeThrough(transformStream),
      ...rest,
    };
  },
};

----------------------------------------

TITLE: Implementing RAG Middleware for Language Models
DESCRIPTION: Demonstrates how to implement Retrieval Augmented Generation (RAG) as middleware for language models, modifying the input parameters with relevant information.

LANGUAGE: typescript
CODE:
import type { LanguageModelV1Middleware } from 'ai';

export const yourRagMiddleware: LanguageModelV1Middleware = {
  transformParams: async ({ params }) => {
    const lastUserMessageText = getLastUserMessageText({
      prompt: params.prompt,
    });

    if (lastUserMessageText == null) {
      return params; // do not use RAG (send unmodified parameters)
    }

    const instruction =
      'Use the following information to answer the question:\n' +
      findSources({ text: lastUserMessageText })
        .map(chunk => JSON.stringify(chunk))
        .join('\n');

    return addToLastUserMessage({ params, text: instruction });
  },
};

----------------------------------------

TITLE: Implementing Client-Side UI for AI Chat with React
DESCRIPTION: This code snippet shows the client-side implementation of a chat interface using React. It handles user input, displays conversation history, and manages state updates for streaming AI responses.

LANGUAGE: tsx
CODE:
'use client';

import { useState } from 'react';
import { ClientMessage } from './actions';
import { useActions, useUIState } from 'ai/rsc';
import { generateId } from 'ai';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export default function Home() {
  const [input, setInput] = useState<string>('');
  const [conversation, setConversation] = useUIState();
  const { continueConversation } = useActions();

  return (
    <div>
      <div>
        {conversation.map((message: ClientMessage) => (
          <div key={message.id}>
            {message.role}: {message.display}
          </div>
        ))}
      </div>

      <div>
        <input
          type="text"
          value={input}
          onChange={event => {
            setInput(event.target.value);
          }}
        />
        <button
          onClick={async () => {
            setConversation((currentConversation: ClientMessage[]) => [
              ...currentConversation,
              { id: generateId(), role: 'user', display: input },
            ]);

            const message = await continueConversation(input);

            setConversation((currentConversation: ClientMessage[]) => [
              ...currentConversation,
              message,
            ]);
          }}
        >
          Send Message
        </button>
      </div>
    </div>
  );
}

----------------------------------------

TITLE: Multi-Step Tool Calls with maxSteps in TypeScript
DESCRIPTION: Shows how to enable multi-step tool calls by setting the maxSteps parameter. This allows the model to make multiple tool calls and generate responses based on the results.

LANGUAGE: typescript
CODE:
import { z } from 'zod';
import { generateText, tool } from 'ai';

const { text, steps } = await generateText({
  model: yourModel,
  tools: {
    weather: tool({
      description: 'Get the weather in a location',
      parameters: z.object({
        location: z.string().describe('The location to get the weather for'),
      }),
      execute: async ({ location }) => ({
        location,
        temperature: 72 + Math.floor(Math.random() * 21) - 10,
      }),
    }),
  },
  maxSteps: 5, // allow up to 5 steps
  prompt: 'What is the weather in San Francisco?',
});

----------------------------------------

TITLE: Creating OpenAI Embedding Model
DESCRIPTION: Example of creating an OpenAI embedding model with specific settings.

LANGUAGE: typescript
CODE:
const model = openai.embedding('text-embedding-3-large', {
  dimensions: 512 // optional, number of dimensions for the embedding
  user: 'test-user' // optional unique user identifier
})

----------------------------------------

TITLE: Implementing Guardrails Middleware for Language Models
DESCRIPTION: Provides an example of implementing guardrails as middleware for language models, filtering out sensitive information or inappropriate content from the generated text.

LANGUAGE: typescript
CODE:
import type { LanguageModelV1Middleware } from 'ai';

export const yourGuardrailMiddleware: LanguageModelV1Middleware = {
  wrapGenerate: async ({ doGenerate }) => {
    const { text, ...rest } = await doGenerate();

    // filtering approach, e.g. for PII or other sensitive information:
    const cleanedText = text?.replace(/badword/g, '<REDACTED>');

    return { text: cleanedText, ...rest };
  },

  // here you would implement the guardrail logic for streaming
  // Note: streaming guardrails are difficult to implement, because
  // you do not know the full content of the stream until it's finished.
};

----------------------------------------

TITLE: Creating Embeddings Schema with Drizzle ORM
DESCRIPTION: Defines database schema for storing embeddings with Postgres and pgvector, including vector dimensions and HNSW index configuration

LANGUAGE: tsx
CODE:
import { nanoid } from '@/lib/utils';
import { index, pgTable, text, varchar, vector } from 'drizzle-orm/pg-core';
import { resources } from './resources';

export const embeddings = pgTable(
  'embeddings',
  {
    id: varchar('id', { length: 191 })
      .primaryKey()
      .$defaultFn(() => nanoid()),
    resourceId: varchar('resource_id', { length: 191 }).references(
      () => resources.id,
      { onDelete: 'cascade' },
    ),
    content: text('content').notNull(),
    embedding: vector('embedding', { dimensions: 1536 }).notNull(),
  },
  table => ({
    embeddingIndex: index('embeddingIndex').using(
      'hnsw',
      table.embedding.op('vector_cosine_ops'),
    ),
  }),
);

----------------------------------------

TITLE: Implementing Client-side Chat Interface
DESCRIPTION: Creates a chat interface that handles tool calls, displays messages with tool invocations, and manages user interactions. Includes support for automatic tool execution and handling of user confirmation dialogs.

LANGUAGE: tsx
CODE:
'use client';

import { ToolInvocation } from 'ai';
import { useChat } from '@ai-sdk/react';

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit, addToolResult } =
    useChat({
      maxSteps: 5,
      async onToolCall({ toolCall }) {
        if (toolCall.toolName === 'getLocation') {
          const cities = ['New York', 'Los Angeles', 'Chicago', 'San Francisco'];
          return cities[Math.floor(Math.random() * cities.length)];
        }
      },
    });

  return (
    <>
      {messages?.map(message => (
        <div key={message.id}>
          <strong>{`${message.role}: `}</strong>
          {message.parts.map(part => {
            switch (part.type) {
              case 'text':
                return part.text;
              case 'tool-invocation': {
                const callId = part.toolInvocation.toolCallId;
                switch (part.toolInvocation.toolName) {
                  // ... tool invocation rendering logic
                }
              }
            }
          })}
          <br />
        </div>
      ))}

      <form onSubmit={handleSubmit}>
        <input value={input} onChange={handleInputChange} />
      </form>
    </>
  );
}

----------------------------------------

TITLE: Implementing RAG Chat Route Handler
DESCRIPTION: Implements the API route handler for the RAG chatbot, configuring the model, system prompt, and tool definitions for adding and retrieving information

LANGUAGE: tsx
CODE:
import { createResource } from '@/lib/actions/resources';
import { openai } from '@ai-sdk/openai';
import { streamText, tool } from 'ai';
import { z } from 'zod';
import { findRelevantContent } from '@/lib/ai/embedding';

export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    messages,
    system: `You are a helpful assistant. Check your knowledge base before answering any questions.
    Only respond to questions using information from tool calls.
    if no relevant information is found in the tool calls, respond, "Sorry, I don't know."`,
    tools: {
      addResource: tool({
        description: `add a resource to your knowledge base.
          If the user provides a random piece of knowledge unprompted, use this tool without asking for confirmation.`,
        parameters: z.object({
          content: z
            .string()
            .describe('the content or resource to add to the knowledge base'),
        }),
        execute: async ({ content }) => createResource({ content }),
      }),
      getInformation: tool({
        description: `get information from your knowledge base to answer questions.`,
        parameters: z.object({
          question: z.string().describe('the users question'),
        }),
        execute: async ({ question }) => findRelevantContent(question),
      }),
    },
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Configuring OpenAI Assistant API Route
DESCRIPTION: Server-side API route implementation for handling OpenAI assistant interactions. Manages thread creation, message handling, and streaming responses.

LANGUAGE: tsx
CODE:
import { AssistantResponse } from 'ai';
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY || '',
});

export const maxDuration = 30;

export async function POST(req: Request) {
  const input: {
    threadId: string | null;
    message: string;
  } = await req.json();

  const threadId = input.threadId ?? (await openai.beta.threads.create({})).id;

  const createdMessage = await openai.beta.threads.messages.create(threadId, {
    role: 'user',
    content: input.message,
  });

  return AssistantResponse(
    { threadId, messageId: createdMessage.id },
    async ({ forwardStream, sendDataMessage }) => {
      const runStream = openai.beta.threads.runs.stream(threadId, {
        assistant_id:
          process.env.ASSISTANT_ID ??
          (() => {
            throw new Error('ASSISTANT_ID is not set');
          })(),
      });

      let runResult = await forwardStream(runStream);

      while (
        runResult?.status === 'requires_action' &&
        runResult.required_action?.type === 'submit_tool_outputs'
      ) {
        const tool_outputs =
          runResult.required_action.submit_tool_outputs.tool_calls.map(
            (toolCall: any) => {
              const parameters = JSON.parse(toolCall.function.arguments);

              switch (toolCall.function.name) {
                default:
                  throw new Error(
                    `Unknown tool call function: ${toolCall.function.name}`,
                  );
              }
            },
          );

        runResult = await forwardStream(
          openai.beta.threads.runs.submitToolOutputsStream(
            threadId,
            runResult.id,
            { tool_outputs },
          ),
        );
      }
    },
  );
}

----------------------------------------

TITLE: Defining LanguageModel Interface in TypeScript for AI SDK
DESCRIPTION: This code snippet defines the LanguageModel interface, which specifies the required methods and properties for language model providers in the AI SDK. It includes methods for text generation, embeddings, and tokenization.

LANGUAGE: typescript
CODE:
export interface LanguageModel {
  id: string;
  name: string;
  /**
   * The provider of this language model, e.g. "openai", "anthropic", etc.
   */
  provider: string;
  /**
   * A short description of this language model's capabilities
   */
  description?: string;
  /**
   * The maximum number of tokens this model can process in a single request
   */
  maxTokens: number;
  /**
   * Generate text based on a prompt
   */
  complete(options: CompletionOptions): Promise<CompletionResponse>;
  /**
   * Generate text based on a chat history
   */
  chat(options: ChatOptions): Promise<ChatResponse>;
  /**
   * Generate embeddings for a given text
   */
  embed(options: EmbedOptions): Promise<EmbedResponse>;
  /**
   * Tokenize a given text
   */
  tokenize(text: string): Promise<string[]>;
  /**
   * Count the number of tokens in a given text
   */
  countTokens(text: string): Promise<number>;
}

----------------------------------------

TITLE: Generating Text with Image Input and Tool Execution in TypeScript
DESCRIPTION: This code snippet demonstrates how to use the AI SDK to generate text using an image prompt and execute a custom tool. It utilizes the OpenAI GPT-4 Turbo model, accepts an image URL as input, and defines a 'logFood' tool to store food information in a database.

LANGUAGE: typescript
CODE:
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const result = await generateText({
  model: openai('gpt-4-turbo'),
  messages: [
    {
      role: 'user',
      content: [
        { type: 'text', text: 'can you log this meal for me?' },
        {
          type: 'image',
          image: new URL(
            'https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Cheeseburger_%2817237580619%29.jpg/640px-Cheeseburger_%2817237580619%29.jpg',
          ),
        },
      ],
    },
  ],
  tools: {
    logFood: tool({
      description: 'Log a food item',
      parameters: z.object({
        name: z.string(),
        calories: z.number(),
      }),
      execute({ name, calories }) {
        storeInDatabase({ name, calories });
      },
    }),
  },
});

----------------------------------------

TITLE: Implementing Client-side UI with useObject Hook in React
DESCRIPTION: This React component uses the useObject hook to stream and display notifications. It includes a button to trigger notification generation and renders the notifications as they are received.

LANGUAGE: tsx
CODE:
'use client';

import { experimental_useObject as useObject } from '@ai-sdk/react';
import { notificationSchema } from './api/notifications/schema';

export default function Page() {
  const { object, submit } = useObject({
    api: '/api/notifications',
    schema: notificationSchema,
  });

  return (
    <>
      <button onClick={() => submit('Messages during finals week.')}>
        Generate notifications
      </button>

      {object?.notifications?.map((notification, index) => (
        <div key={index}>
          <p>{notification?.name}</p>
          <p>{notification?.message}</p>
        </div>
      ))}
    </>
  );
}

----------------------------------------

TITLE: Basic Text Generation with OpenAI
DESCRIPTION: Simple example showing how to use generateText with OpenAI's GPT-4 model to generate text from a prompt.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

const { text } = await generateText({
  model: openai('gpt-4o'),
  prompt: 'Invent a new holiday and describe its traditions.',
});

console.log(text);

----------------------------------------

TITLE: Integrating Dynamic UI Components in Chat Interface
DESCRIPTION: This code updates the chat interface to render dynamic UI components based on tool invocations. It checks for tool results and renders appropriate components.

LANGUAGE: tsx
CODE:
'use client';

import { useChat } from '@ai-sdk/react';
import { Weather } from '@/components/weather';

export default function Page() {
  const { messages, input, handleInputChange, handleSubmit } = useChat();

  return (
    <div>
      {messages.map(message => (
        <div key={message.id}>
          <div>{message.role === 'user' ? 'User: ' : 'AI: '}</div>
          <div>{message.content}</div>

          <div>
            {message.toolInvocations?.map(toolInvocation => {
              const { toolName, toolCallId, state } = toolInvocation;

              if (state === 'result') {
                if (toolName === 'displayWeather') {
                  const { result } = toolInvocation;
                  return (
                    <div key={toolCallId}>
                      <Weather {...result} />
                    </div>
                  );
                }
              } else {
                return (
                  <div key={toolCallId}>
                    {toolName === 'displayWeather' ? (
                      <div>Loading weather...</div>
                    ) : null}
                  </div>
                );
              }
            })}
          </div>
        </div>
      ))}

      <form onSubmit={handleSubmit}>
        <input
          value={input}
          onChange={handleInputChange}
          placeholder="Type a message..."
        />
        <button type="submit">Send</button>
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Implementing Streaming Text Generation with OpenAI GPT-4 Turbo
DESCRIPTION: This snippet demonstrates how to use the AI SDK to stream text generation from OpenAI's gpt-4-turbo model. It utilizes the streamText function to create a text stream and then iterates over the stream to log each part of the generated text.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';

const { textStream } = streamText({
  model: openai('gpt-4-turbo'),
  prompt: 'Write a poem about embedding models.',
});

for await (const textPart of textStream) {
  console.log(textPart);
}

----------------------------------------

TITLE: Handling App Mentions in Slack
DESCRIPTION: This function processes app mentions in Slack channels. It checks if the message is from a bot, creates a status updater, retrieves thread history if applicable, calls the LLM, and updates the initial message with the AI response.

LANGUAGE: typescript
CODE:
import { AppMentionEvent } from '@slack/web-api';
import { client, getThread } from './slack-utils';
import { generateResponse } from './ai';

const updateStatusUtil = async (
  initialStatus: string,
  event: AppMentionEvent,
) => {
  const initialMessage = await client.chat.postMessage({
    channel: event.channel,
    thread_ts: event.thread_ts ?? event.ts,
    text: initialStatus,
  });

  if (!initialMessage || !initialMessage.ts)
    throw new Error('Failed to post initial message');

  const updateMessage = async (status: string) => {
    await client.chat.update({
      channel: event.channel,
      ts: initialMessage.ts as string,
      text: status,
    });
  };
  return updateMessage;
};

export async function handleNewAppMention(
  event: AppMentionEvent,
  botUserId: string,
) {
  console.log('Handling app mention');
  if (event.bot_id || event.bot_id === botUserId || event.bot_profile) {
    console.log('Skipping app mention');
    return;
  }

  const { thread_ts, channel } = event;
  const updateMessage = await updateStatusUtil('is thinking...', event);

  if (thread_ts) {
    const messages = await getThread(channel, thread_ts, botUserId);
    const result = await generateResponse(messages, updateMessage);
    updateMessage(result);
  } else {
    const result = await generateResponse(
      [{ role: 'user', content: event.text }],
      updateMessage,
    );
    updateMessage(result);
  }
}

----------------------------------------

TITLE: Using Provider-Specific Settings for Image Generation
DESCRIPTION: Shows how to pass provider-specific settings to the generateImage function.

LANGUAGE: tsx
CODE:
import { experimental_generateImage as generateImage } from 'ai';
import { openai } from '@ai-sdk/openai';

const { image } = await generateImage({
  model: openai.image('dall-e-3'),
  prompt: 'Santa Claus driving a Cadillac',
  size: '1024x1024',
  providerOptions: {
    openai: { style: 'vivid', quality: 'hd' },
  },
});

----------------------------------------

TITLE: Implementing Logging Middleware for Language Models
DESCRIPTION: Provides an example of implementing a logging middleware that logs parameters and generated text for both doGenerate and doStream methods of a language model.

LANGUAGE: typescript
CODE:
import type { LanguageModelV1Middleware, LanguageModelV1StreamPart } from 'ai';

export const yourLogMiddleware: LanguageModelV1Middleware = {
  wrapGenerate: async ({ doGenerate, params }) => {
    console.log('doGenerate called');
    console.log(`params: ${JSON.stringify(params, null, 2)}`);

    const result = await doGenerate();

    console.log('doGenerate finished');
    console.log(`generated text: ${result.text}`);

    return result;
  },

  wrapStream: async ({ doStream, params }) => {
    console.log('doStream called');
    console.log(`params: ${JSON.stringify(params, null, 2)}`);

    const { stream, ...rest } = await doStream();

    let generatedText = '';

    const transformStream = new TransformStream<
      LanguageModelV1StreamPart,
      LanguageModelV1StreamPart
    >({
      transform(chunk, controller) {
        if (chunk.type === 'text-delta') {
          generatedText += chunk.textDelta;
        }

        controller.enqueue(chunk);
      },

      flush() {
        console.log('doStream finished');
        console.log(`generated text: ${generatedText}`);
      },
    });

    return {
      stream: stream.pipeThrough(transformStream),
      ...rest,
    };
  },
};

----------------------------------------

TITLE: Creating Azure OpenAI Completion Model
DESCRIPTION: Demonstrates how to create a completion model using the Azure OpenAI provider.

LANGUAGE: typescript
CODE:
const model = azure.completion('your-gpt-35-turbo-instruct-deployment');

----------------------------------------

TITLE: Combining Custom Providers, Provider Registry, and Middleware in TypeScript
DESCRIPTION: This comprehensive example demonstrates how to combine custom providers, provider registry, and middleware to create a flexible and powerful setup for managing multiple AI providers and models.

LANGUAGE: typescript
CODE:
import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
import { xai } from '@ai-sdk/xai';
import { groq } from '@ai-sdk/groq';
import {
  createProviderRegistry,
  customProvider,
  defaultSettingsMiddleware,
  wrapLanguageModel,
} from 'ai';

export const registry = createProviderRegistry(
  {
    // pass through a full provider with a namespace prefix
    xai,

    // access an OpenAI-compatible provider with custom setup
    custom: createOpenAICompatible({
      name: 'provider-name',
      apiKey: process.env.CUSTOM_API_KEY,
      baseURL: 'https://api.custom.com/v1',
    }),

    // setup model name aliases
    anthropic: customProvider({
      languageModels: {
        fast: anthropic('claude-3-haiku-20240307'),

        // simple model
        writing: anthropic('claude-3-7-sonnet-20250219'),

        // extended reasoning model configuration:
        reasoning: wrapLanguageModel({
          model: anthropic('claude-3-7-sonnet-20250219'),
          middleware: defaultSettingsMiddleware({
            settings: {
              maxTokens: 100000, // example default setting
              providerMetadata: {
                anthropic: {
                  thinking: {
                    type: 'enabled',
                    budgetTokens: 32000,
                  },
                } satisfies AnthropicProviderOptions,
              },
            },
          }),
        }),
      },
      fallbackProvider: anthropic,
    }),

    // limit a provider to certain models without a fallback
    groq: customProvider({
      languageModels: {
        'gemma2-9b-it': groq('gemma2-9b-it'),
        'qwen-qwq-32b': groq('qwen-qwq-32b'),
      },
    }),
  },
  { separator: ' > ' },
);

// usage:
const model = registry.languageModel('anthropic > reasoning');

----------------------------------------

TITLE: Creating Chat Interface with React
DESCRIPTION: Implementation of a chat interface using AI SDK UI hooks in a Next.js client component. Shows message handling and input management.

LANGUAGE: typescript
CODE:
'use client';

import { useChat } from '@ai-sdk/react';

export default function Page() {
  const { messages, input, handleSubmit, handleInputChange, status } =
    useChat();

  return (
    <div>
      {messages.map(message => (
        <div key={message.id}>
          <strong>{`${message.role}: `}</strong>
          {message.parts.map((part, index) => {
            switch (part.type) {
              case 'text':
                return <span key={index}>{part.text}</span>;

              // other cases can handle images, tool calls, etc
            }
          })}
        </div>
      ))}

      <form onSubmit={handleSubmit}>
        <input
          value={input}
          placeholder="Send a message..."
          onChange={handleInputChange}
          disabled={status !== 'ready'}
        />
      </form>
    </div>
  );

----------------------------------------

TITLE: Streaming Structured Object Data with AI SDK and OpenAI in TypeScript
DESCRIPTION: This code snippet shows how to use the streamObject function to generate a partial object stream for a recipe. It utilizes OpenAI's GPT-4 model, Zod for schema validation, and asynchronous iteration to display the generated data in real-time.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamObject } from 'ai';
import { z } from 'zod';

const { partialObjectStream } = streamObject({
  model: openai('gpt-4-turbo'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.string()),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a lasagna recipe.',
});

for await (const partialObject of partialObjectStream) {
  console.clear();
  console.log(partialObject);
}

----------------------------------------

TITLE: Implementing Server-Side Message Submission with OpenAI Assistant API
DESCRIPTION: This server action handles message submission, manages thread creation, and streams assistant responses using the OpenAI API and AI SDK utilities.

LANGUAGE: tsx
CODE:
'use server';

import { generateId } from 'ai';
import { createStreamableUI, createStreamableValue } from 'ai/rsc';
import { OpenAI } from 'openai';
import { ReactNode } from 'react';
import { Message } from './message';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

export interface ClientMessage {
  id: string;
  status: ReactNode;
  text: ReactNode;
}

const ASSISTANT_ID = 'asst_xxxx';
let THREAD_ID = '';
let RUN_ID = '';

export async function submitMessage(question: string): Promise<ClientMessage> {
  const statusUIStream = createStreamableUI('thread.init');

  const textStream = createStreamableValue('');
  const textUIStream = createStreamableUI(
    <Message textStream={textStream.value} />,
  );

  const runQueue = [];

  (async () => {
    if (THREAD_ID) {
      await openai.beta.threads.messages.create(THREAD_ID, {
        role: 'user',
        content: question,
      });

      const run = await openai.beta.threads.runs.create(THREAD_ID, {
        assistant_id: ASSISTANT_ID,
        stream: true,
      });

      runQueue.push({ id: generateId(), run });
    } else {
      const run = await openai.beta.threads.createAndRun({
        assistant_id: ASSISTANT_ID,
        stream: true,
        thread: {
          messages: [{ role: 'user', content: question }],
        },
      });

      runQueue.push({ id: generateId(), run });
    }

    while (runQueue.length > 0) {
      const latestRun = runQueue.shift();

      if (latestRun) {
        for await (const delta of latestRun.run) {
          const { data, event } = delta;

          statusUIStream.update(event);

          if (event === 'thread.created') {
            THREAD_ID = data.id;
          } else if (event === 'thread.run.created') {
            RUN_ID = data.id;
          } else if (event === 'thread.message.delta') {
            data.delta.content?.map(part => {
              if (part.type === 'text') {
                if (part.text) {
                  textStream.append(part.text.value as string);
                }
              }
            });
          } else if (event === 'thread.run.failed') {
            console.error(data);
          }
        }
      }
    }

    statusUIStream.done();
    textStream.done();
  })();

  return {
    id: generateId(),
    status: statusUIStream.value,
    text: textUIStream.value,
  };
}

----------------------------------------

TITLE: Implementing Client-Side Chat Interface with Streaming in React
DESCRIPTION: This code snippet shows how to create a client-side React component that handles user input, displays conversation history, and streams chat completions in real-time. It uses the useState hook for state management and the readStreamableValue function from the AI SDK.

LANGUAGE: tsx
CODE:
'use client';

import { useState } from 'react';
import { Message, continueConversation } from './actions';
import { readStreamableValue } from 'ai/rsc';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export default function Home() {
  const [conversation, setConversation] = useState<Message[]>([]);
  const [input, setInput] = useState<string>('');

  return (
    <div>
      <div>
        {conversation.map((message, index) => (
          <div key={index}>
            {message.role}: {message.content}
          </div>
        ))}
      </div>

      <div>
        <input
          type="text"
          value={input}
          onChange={event => {
            setInput(event.target.value);
          }}
        />
        <button
          onClick={async () => {
            const { messages, newMessage } = await continueConversation([
              ...conversation,
              { role: 'user', content: input },
            ]);

            let textContent = '';

            for await (const delta of readStreamableValue(newMessage)) {
              textContent = `${textContent}${delta}`;

              setConversation([
                ...messages,
                { role: 'assistant', content: textContent },
              ]);
            }
          }}
        >
          Send Message
        </button>
      </div>
    </div>
  );
}

----------------------------------------

TITLE: Handling Full Stream Errors with Error Support in TypeScript
DESCRIPTION: Illustrates advanced error handling for streams that support error parts. Combines both try/catch blocks and error part handling within the stream processing.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';

try {
  const { fullStream } = streamText({
    model: yourModel,
    prompt: 'Write a vegetarian lasagna recipe for 4 people.',
  });

  for await (const part of fullStream) {
    switch (part.type) {
      // ... handle other part types

      case 'error': {
        const error = part.error;
        // handle error
        break;
      }
    }
  }
} catch (error) {
  // handle error
}

----------------------------------------

TITLE: Creating Cohere Embedding Model
DESCRIPTION: Initialize a Cohere embedding model with optional configuration settings.

LANGUAGE: typescript
CODE:
const model = cohere.embedding('embed-english-v3.0');

LANGUAGE: typescript
CODE:
const model = cohere.embedding('embed-english-v3.0', {
  inputType: 'search_document',
});

----------------------------------------

TITLE: Implementing Event Callbacks with useObject in React
DESCRIPTION: This React component shows how to use the onFinish and onError callbacks provided by useObject. These callbacks can be used for logging, analytics, or custom UI updates based on the object generation lifecycle.

LANGUAGE: tsx
CODE:
'use client';

import { experimental_useObject as useObject } from '@ai-sdk/react';
import { notificationSchema } from './api/notifications/schema';

export default function Page() {
  const { object, submit } = useObject({
    api: '/api/notifications',
    schema: notificationSchema,
    onFinish({ object, error }) {
      // typed object, undefined if schema validation fails:
      console.log('Object generation completed:', object);

      // error, undefined if schema validation succeeds:
      console.log('Schema validation error:', error);
    },
    onError(error) {
      // error during fetch request:
      console.error('An error occurred:', error);
    },
  });

  return (
    <div>
      <button onClick={() => submit('Messages during finals week.')}>
        Generate notifications
      </button>

      {object?.notifications?.map((notification, index) => (
        <div key={index}>
          <p>{notification?.name}</p>
          <p>{notification?.message}</p>
        </div>
      ))}
    </div>
  );
}

----------------------------------------

TITLE: Defining Schema Using Zod in TypeScript
DESCRIPTION: Demonstrates how to create a recipe schema using Zod for parameter validation in AI SDK tools. The schema defines a structure for recipes including name, ingredients array, and steps array.

LANGUAGE: typescript
CODE:
import z from 'zod';

const recipeSchema = z.object({
  recipe: z.object({
    name: z.string(),
    ingredients: z.array(
      z.object({
        name: z.string(),
        amount: z.string(),
      }),
    ),
    steps: z.array(z.string()),
  }),
});

----------------------------------------

TITLE: Extracting Reasoning with Built-in Middleware
DESCRIPTION: Demonstrates the use of the extractReasoningMiddleware to extract reasoning information from generated text and expose it as a 'reasoning' property on the result.

LANGUAGE: typescript
CODE:
import { wrapLanguageModel, extractReasoningMiddleware } from 'ai';

const model = wrapLanguageModel({
  model: yourModel,
  middleware: extractReasoningMiddleware({ tagName: 'think' }),
});

----------------------------------------

TITLE: Using Tools with OpenAI o1 in AI SDK
DESCRIPTION: Demonstrates how to use tools with the OpenAI o1 model in the AI SDK. This example creates a 'getWeather' tool that simulates fetching weather data, extending the model's capabilities to interact with external systems.

LANGUAGE: tsx
CODE:
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';

const { text } = await generateText({
  model: openai('o1'),
  prompt: 'What is the weather like today?',
  tools: {
    getWeather: tool({
      description: 'Get the weather in a location',
      parameters: z.object({
        location: z.string().describe('The location to get the weather for'),
      }),
      execute: async ({ location }) => ({
        location,
        temperature: 72 + Math.floor(Math.random() * 21) - 10,
      }),
    }),
  },
});

----------------------------------------

TITLE: Implementing Basic Error Handling with useChat Hook in React
DESCRIPTION: Demonstrates basic error handling in a React chat component using the useChat hook. Shows how to display error messages, implement retry functionality, and disable input during errors.

LANGUAGE: tsx
CODE:
'use client';

import { useChat } from '@ai-sdk/react';

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit, error, reload } =
    useChat({});

  return (
    <div>
      {messages.map(m => (
        <div key={m.id}>
          {m.role}: {m.content}
        </div>
      ))}

      {error && (
        <>
          <div>An error occurred.</div>
          <button type="button" onClick={() => reload()}>
            Retry
          </button>
        </>
      )}

      <form onSubmit={handleSubmit}>
        <input
          value={input}
          onChange={handleInputChange}
          disabled={error != null}
        />
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Creating React Component for Object Generation in Next.js
DESCRIPTION: This code snippet shows a React component that sends a POST request to generate an object based on a prompt and displays the result. It uses useState for state management and handles loading states.

LANGUAGE: tsx
CODE:
'use client';

import { useState } from 'react';

export default function Page() {
  const [generation, setGeneration] = useState();
  const [isLoading, setIsLoading] = useState(false);

  return (
    <div>
      <div
        onClick={async () => {
          setIsLoading(true);

          await fetch('/api/completion', {
            method: 'POST',
            body: JSON.stringify({
              prompt: 'Messages during finals week.',
            }),
          }).then(response => {
            response.json().then(json => {
              setGeneration(json.notifications);
              setIsLoading(false);
            });
          });
        }}
      >
        Generate
      </div>

      {isLoading ? 'Loading...' : <pre>{JSON.stringify(generation)}</pre>}
    </div>
  );
}

----------------------------------------

TITLE: Creating a Chat API Route with OpenAI o1-mini in Next.js
DESCRIPTION: Shows how to set up a chat API route in Next.js using the AI SDK and OpenAI o1-mini model. This endpoint handles incoming chat messages and streams responses back to the client.

LANGUAGE: tsx
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';

// Allow responses up to 5 minutes
export const maxDuration = 300;

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai('o1-mini'),
    messages,
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Server-Side Assistant API Implementation
DESCRIPTION: API route handler that processes assistant messages, manages threads, and handles tool interactions including celsius to fahrenheit conversion. Uses OpenAI's API and implements streaming responses.

LANGUAGE: typescript
CODE:
import { AssistantResponse } from 'ai';
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY || '',
});

export async function POST(req: Request) {
  const input: {
    threadId: string | null;
    message: string;
  } = await req.json();

  const threadId = input.threadId ?? (await openai.beta.threads.create({})).id;

  const createdMessage = await openai.beta.threads.messages.create(threadId, {
    role: 'user',
    content: input.message,
  });

  return AssistantResponse(
    { threadId, messageId: createdMessage.id },
    async ({ forwardStream }) => {
      const runStream = openai.beta.threads.runs.stream(threadId, {
        assistant_id:
          process.env.ASSISTANT_ID ??
          (() => {
            throw new Error('ASSISTANT_ID is not set');
          })(),
      });

      let runResult = await forwardStream(runStream);

      while (
        runResult?.status === 'requires_action' &&
        runResult.required_action?.type === 'submit_tool_outputs'
      ) {
        const tool_outputs =
          runResult.required_action.submit_tool_outputs.tool_calls.map(
            (toolCall: any) => {
              const parameters = JSON.parse(toolCall.function.arguments);

              switch (toolCall.function.name) {
                case 'celsiusToFahrenheit':
                  const celsius = parseFloat(parameters.value);
                  const fahrenheit = celsius * (9 / 5) + 32;

                  return {
                    tool_call_id: toolCall.id,
                    output: `${celsius}°C is ${fahrenheit.toFixed(2)}°F`,
                  };

                default:
                  throw new Error(
                    `Unknown tool call function: ${toolCall.function.name}`,
                  );
              }
            },
          );

        runResult = await forwardStream(
          openai.beta.threads.runs.submitToolOutputsStream(
            threadId,
            runResult.id,
            { tool_outputs },
          ),
        );
      }
    },
  );
}

----------------------------------------

TITLE: Creating Custom Cohere Provider
DESCRIPTION: Create a customized Cohere provider instance with specific settings using createCohere function.

LANGUAGE: typescript
CODE:
import { createCohere } from '@ai-sdk/cohere';

const cohere = createCohere({
  // custom settings
});

----------------------------------------

TITLE: Implementing Chat UI with useAssistant Hook in Next.js
DESCRIPTION: A React client component that creates a chat interface using the useAssistant hook from @ai-sdk/react. It handles message streaming, user input, and displays chat messages with their respective roles.

LANGUAGE: tsx
CODE:
'use client';

import { Message, useAssistant } from '@ai-sdk/react';

export default function Page() {
  const { status, messages, input, submitMessage, handleInputChange } =
    useAssistant({ api: '/api/assistant' });

  return (
    <div className="flex flex-col gap-2">
      <div className="p-2">status: {status}</div>

      <div className="flex flex-col p-2 gap-2">
        {messages.map((message: Message) => (
          <div key={message.id} className="flex flex-row gap-2">
            <div className="w-24 text-zinc-500">{`${message.role}: `}</div>
            <div className="w-full">{message.content}</div>
          </div>
        ))}
      </div>

      <form onSubmit={submitMessage} className="fixed bottom-0 p-2 w-full">
        <input
          disabled={status !== 'awaiting_message'}
          value={input}
          onChange={handleInputChange}
          className="bg-zinc-100 w-full p-2"
        />
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Basic Text Streaming with streamText
DESCRIPTION: Demonstrates how to use streamText for real-time text generation and process the stream using an async iterator.

LANGUAGE: ts
CODE:
import { streamText } from 'ai';

const result = streamText({
  model: yourModel,
  prompt: 'Invent a new holiday and describe its traditions.',
});

// example: use textStream as an async iterable
for await (const textPart of result.textStream) {
  console.log(textPart);
}

----------------------------------------

TITLE: Booking Flights Using Composed AI Tools in a Multistep Interface
DESCRIPTION: Illustrates the composition of multiple tools (searchFlights, lookupFlight, bookFlight) to create a flight booking assistant, demonstrating how tools interact to form a coherent user experience.

LANGUAGE: txt
CODE:
User: I want to book a flight from New York to London.
Tool: searchFlights("New York", "London")
Model: Here are the available flights from New York to London.
User: I want to book flight number BA123 on 12th December for myself and my wife.
Tool: lookupFlight("BA123") -> "4 seats available"
Model: Sure, there are seats available! Can you provide the names of the passengers?
User: John Doe and Jane Doe.
Tool: bookFlight("BA123", "12th December", ["John Doe", "Jane Doe"])
Model: Your flight has been booked!

----------------------------------------

TITLE: Configuring Request Options for useObject in React
DESCRIPTION: This code snippet demonstrates how to configure the API endpoint, custom headers, and credentials for the useObject hook. It allows for customization of the request sent to the server.

LANGUAGE: tsx
CODE:
const { submit, object } = useObject({
  api: '/api/use-object',
  headers: {
    'X-Custom-Header': 'CustomValue',
  },
  credentials: 'include',
  schema: yourSchema,
});

----------------------------------------

TITLE: Using Tools with OpenAI GPT-4.5 and AI SDK
DESCRIPTION: This example demonstrates how to use tool calling with OpenAI's GPT-4.5 model using the AI SDK. It defines a getWeather tool that simulates fetching weather data for a given location, allowing the model to incorporate external data into its responses.

LANGUAGE: typescript
CODE:
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';

const { text } = await generateText({
  model: openai('gpt-4.5-preview'),
  prompt: 'What is the weather like today in San Francisco?',
  tools: {
    getWeather: tool({
      description: 'Get the weather in a location',
      parameters: z.object({
        location: z.string().describe('The location to get the weather for'),
      }),
      execute: async ({ location }) => ({
        location,
        temperature: 72 + Math.floor(Math.random() * 21) - 10,
      }),
    }),
  },
});

----------------------------------------

TITLE: Implementing Chat UI in Expo with AI SDK
DESCRIPTION: This code creates the user interface for the chat application using Expo components and the useChat hook from the AI SDK. It handles displaying messages, user input, and submission of chat messages.

LANGUAGE: typescript
CODE:
import { generateAPIUrl } from '@/utils';
import { useChat } from '@ai-sdk/react';
import { fetch as expoFetch } from 'expo/fetch';
import { View, TextInput, ScrollView, Text, SafeAreaView } from 'react-native';

export default function App() {
  const { messages, error, handleInputChange, input, handleSubmit } = useChat({
    fetch: expoFetch as unknown as typeof globalThis.fetch,
    api: generateAPIUrl('/api/chat'),
    onError: error => console.error(error, 'ERROR'),
  });

  if (error) return <Text>{error.message}</Text>;

  return (
    <SafeAreaView style={{ height: '100%' }}>
      <View
        style={{
          height: '95%',
          display: 'flex',
          flexDirection: 'column',
          paddingHorizontal: 8,
        }}
      >
        <ScrollView style={{ flex: 1 }}>
          {messages.map(m => (
            <View key={m.id} style={{ marginVertical: 8 }}>
              <View>
                <Text style={{ fontWeight: 700 }}>{m.role}</Text>
                <Text>{m.content}</Text>
              </View>
            </View>
          ))}
        </ScrollView>

        <View style={{ marginTop: 8 }}>
          <TextInput
            style={{ backgroundColor: 'white', padding: 8 }}
            placeholder="Say something..."
            value={input}
            onChange={e =>
              handleInputChange({
                ...e,
                target: {
                  ...e.target,
                  value: e.nativeEvent.text,
                },
              } as unknown as React.ChangeEvent<HTMLInputElement>)
            }
            onSubmitEditing={e => {
              handleSubmit(e);
              e.preventDefault();
            }}
            autoFocus={true}
          />
        </View>
      </View>
    </SafeAreaView>
  );
}

----------------------------------------

TITLE: Implementing Agents with Llama 3.1 in AI SDK
DESCRIPTION: This snippet shows how to implement an agent that can solve math problems using Llama 3.1 and the AI SDK. It uses a calculator tool and allows multiple steps of reasoning.

LANGUAGE: tsx
CODE:
import { generateText, tool } from 'ai';
import { deepinfra } from '@ai-sdk/deepinfra';
import * as mathjs from 'mathjs';
import { z } from 'zod';

const problem =
  'Calculate the profit for a day if revenue is $5000 and expenses are $3500.';

const { text: answer } = await generateText({
  model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),
  system:
    'You are solving math problems. Reason step by step. Use the calculator when necessary.',
  prompt: problem,
  tools: {
    calculate: tool({
      description: 'A tool for evaluating mathematical expressions.',
      parameters: z.object({ expression: z.string() }),
      execute: async ({ expression }) => mathjs.evaluate(expression),
    }),
  },
  maxSteps: 5,
});

----------------------------------------

TITLE: Applying Default Settings with Built-in Middleware
DESCRIPTION: Demonstrates the use of defaultSettingsMiddleware to apply default settings to a language model, including temperature, maxTokens, and provider-specific metadata.

LANGUAGE: typescript
CODE:
import { wrapLanguageModel, defaultSettingsMiddleware } from 'ai';

const model = wrapLanguageModel({
  model: yourModel,
  middleware: defaultSettingsMiddleware({
    settings: {
      temperature: 0.5,
      maxTokens: 800,
      // note: use providerMetadata instead of providerOptions here:
      providerMetadata: { openai: { store: false } },
    },
  }),
});

----------------------------------------

TITLE: Implementing Server-Side Streaming Chat Completion with AI SDK
DESCRIPTION: This server-side code snippet demonstrates how to use the AI SDK to create a streamable chat completion. It defines a continueConversation function that takes the conversation history, generates a response using the OpenAI model, and streams the response back to the client.

LANGUAGE: typescript
CODE:
'use server';

import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { createStreamableValue } from 'ai/rsc';

export interface Message {
  role: 'user' | 'assistant';
  content: string;
}

export async function continueConversation(history: Message[]) {
  'use server';

  const stream = createStreamableValue();

  (async () => {
    const { textStream } = streamText({
      model: openai('gpt-3.5-turbo'),
      system:
        "You are a dude that doesn't drop character until the DVD commentary.",
      messages: history,
    });

    for await (const text of textStream) {
      stream.update(text);
    }

    stream.done();
  })();

  return {
    messages: history,
    newMessage: stream.value,
  };
}

----------------------------------------

TITLE: Generating Text with Anthropic Model
DESCRIPTION: Demonstrates how to use Anthropic language models to generate text using the generateText function.

LANGUAGE: typescript
CODE:
import { anthropic } from '@ai-sdk/anthropic';
import { generateText } from 'ai';

const { text } = await generateText({
  model: anthropic('claude-3-haiku-20240307'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Checking Flight Status Using Composed AI Tools in Multistep Interface
DESCRIPTION: Demonstrates the composition of multiple tools (lookupContacts, lookupBooking, lookupFlight) to provide a comprehensive flight status check, showcasing how tool composition can create more powerful and context-aware interactions.

LANGUAGE: txt
CODE:
User: What's the status of my wife's upcoming flight?
Tool: lookupContacts() -> ["John Doe", "Jane Doe"]
Tool: lookupBooking("Jane Doe") -> "BA123 confirmed"
Tool: lookupFlight("BA123") -> "Flight BA123 is scheduled to depart on 12th December."
Model: Your wife's flight BA123 is confirmed and scheduled to depart on 12th December.

----------------------------------------

TITLE: Implementing Server-Side AI Logic with React Server Components
DESCRIPTION: This server-side code handles the AI conversation logic, including streaming UI updates, managing conversation history, and integrating with OpenAI's GPT model. It also includes a custom 'deploy' tool for simulating repository deployment.

LANGUAGE: tsx
CODE:
'use server';

import { getMutableAIState, streamUI } from 'ai/rsc';
import { openai } from '@ai-sdk/openai';
import { ReactNode } from 'react';
import { z } from 'zod';
import { generateId } from 'ai';

export interface ServerMessage {
  role: 'user' | 'assistant';
  content: string;
}

export interface ClientMessage {
  id: string;
  role: 'user' | 'assistant';
  display: ReactNode;
}

export async function continueConversation(
  input: string,
): Promise<ClientMessage> {
  'use server';

  const history = getMutableAIState();

  const result = await streamUI({
    model: openai('gpt-3.5-turbo'),
    messages: [...history.get(), { role: 'user', content: input }],
    text: ({ content, done }) => {
      if (done) {
        history.done((messages: ServerMessage[]) => [
          ...messages,
          { role: 'assistant', content },
        ]);
      }

      return <div>{content}</div>;
    },
    tools: {
      deploy: {
        description: 'Deploy repository to vercel',
        parameters: z.object({
          repositoryName: z
            .string()
            .describe('The name of the repository, example: vercel/ai-chatbot'),
        }),
        generate: async function* ({ repositoryName }) {
          yield <div>Cloning repository {repositoryName}...</div>;
          await new Promise(resolve => setTimeout(resolve, 3000));
          yield <div>Building repository {repositoryName}...</div>;
          await new Promise(resolve => setTimeout(resolve, 2000));
          return <div>{repositoryName} deployed!</div>;
        },
      },
    },
  });

  return {
    id: generateId(),
    role: 'assistant',
    display: result.value,
  };
}

----------------------------------------

TITLE: Setting Up API Route for Chat Requests with OpenAI
DESCRIPTION: This code sets up an API route to handle chat requests using the OpenAI model. It uses the streamText function to process messages and stream responses.

LANGUAGE: ts
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';

export async function POST(request: Request) {
  const { messages } = await request.json();

  const result = streamText({
    model: openai('gpt-4o'),
    system: 'You are a friendly assistant!',
    messages,
    maxSteps: 5,
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Creating Custom Mistral Provider - TypeScript
DESCRIPTION: Creates a customized Mistral provider instance with specific settings like baseURL, apiKey, headers, and fetch implementation.

LANGUAGE: typescript
CODE:
import { createMistral } from '@ai-sdk/mistral';

const mistral = createMistral({
  // custom settings
});

----------------------------------------

TITLE: Implementing Basic Chat Interface with useChat Hook in React
DESCRIPTION: This snippet shows how to create a basic chat interface using the useChat hook from the AI SDK. It renders messages and handles user input.

LANGUAGE: tsx
CODE:
'use client';

import { useChat } from '@ai-sdk/react';

export default function Page() {
  const { messages, input, handleInputChange, handleSubmit } = useChat();

  return (
    <div>
      {messages.map(message => (
        <div key={message.id}>
          <div>{message.role === 'user' ? 'User: ' : 'AI: '}</div>
          <div>{message.content}</div>
        </div>
      ))}

      <form onSubmit={handleSubmit}>
        <input
          value={input}
          onChange={handleInputChange}
          placeholder="Type a message..."
        />
        <button type="submit">Send</button>
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Creating Custom Anthropic Provider
DESCRIPTION: Example of creating a customized Anthropic provider instance with specific settings.

LANGUAGE: typescript
CODE:
import { createAnthropic } from '@ai-sdk/anthropic';

const anthropic = createAnthropic({
  // custom settings
});

----------------------------------------

TITLE: Creating a Streaming Chat API Endpoint in Next.js
DESCRIPTION: This code snippet demonstrates how to create a /api/chat endpoint in Next.js that generates and streams the assistant's response based on the conversation history using the AI SDK and OpenAI.

LANGUAGE: typescript
CODE:
import { streamText, UIMessage } from 'ai';
import { openai } from '@ai-sdk/openai';

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    system: 'You are a helpful assistant.',
    messages,
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Server-Side Rendering of UI with AI SDK RSC in TypeScript
DESCRIPTION: This code snippet illustrates how to use the AI SDK RSC to render React components on the server and stream them to the client. It uses createStreamableUI to create a stream for sending React components and demonstrates how to render a WeatherCard component.

LANGUAGE: tsx
CODE:
import { createStreamableUI } from 'ai/rsc'

const uiStream = createStreamableUI();

const text = generateText({
  model: openai('gpt-3.5-turbo'),
  system: 'you are a friendly assistant'
  prompt: 'what is the weather in SF?'
  tools: {
    getWeather: {
      description: 'Get the weather for a location',
      parameters: z.object({
        city: z.string().describe('The city to get the weather for'),
        unit: z
          .enum(['C', 'F'])
          .describe('The unit to display the temperature in')
      }),
      execute: async ({ city, unit }) => {
        const weather = getWeather({ city, unit })
        const { temperature, unit, description, forecast } = weather

        uiStream.done(
          <WeatherCard
            weather={{
              temperature: 47,
              unit: 'F',
              description: 'sunny'
              forecast,
            }}
          />
        )
      }
    }
  }
})

return {
  display: uiStream.value
}

----------------------------------------

TITLE: Basic Text Generation with OpenAI Responses API
DESCRIPTION: Demonstrates how to generate text using GPT-4o with the OpenAI Responses API through AI SDK Core. Shows the minimal setup required for text generation.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const { text } = await generateText({
  model: openai.responses('gpt-4o'),
  prompt: 'Explain the concept of quantum entanglement.',
});

----------------------------------------

TITLE: Restoring UI State with onGetUIState Callback in TypeScript
DESCRIPTION: This snippet illustrates how to restore the UI state using the onGetUIState callback. It compares the chat history from the database with the app's state and returns the UI state based on the database if they're out of sync.

LANGUAGE: tsx
CODE:
export const AI = createAI<ServerMessage[], ClientMessage[]>({
  actions: {
    continueConversation,
  },
  onGetUIState: async () => {
    'use server';

    const historyFromDB: ServerMessage[] = await loadChatFromDB();
    const historyFromApp: ServerMessage[] = getAIState();

    // If the history from the database is different from the
    // history in the app, they're not in sync so return the UIState
    // based on the history from the database

    if (historyFromDB.length !== historyFromApp.length) {
      return historyFromDB.map(({ role, content }) => ({
        id: generateId(),
        role,
        display:
          role === 'function' ? (
            <Component {...JSON.parse(content)} />
          ) : (
            content
          ),
      }));
    }
  },
});

----------------------------------------

TITLE: Handling UI Stream Errors with createStreamableUI in TypeScript/React
DESCRIPTION: Demonstrates how to handle errors while streaming UI components using the createStreamableUI method. The code shows updating UI states during loading, success, and error conditions in a server action.

LANGUAGE: tsx
CODE:
'use server';

import { createStreamableUI } from 'ai/rsc';

export async function getStreamedUI() {
  const ui = createStreamableUI();

  (async () => {
    ui.update(<div>loading</div>);
    const data = await fetchData();
    ui.done(<div>{data}</div>);
  })().catch(e => {
    ui.error(<div>Error: {e.message}</div>);
  });

  return ui.value;
}

----------------------------------------

TITLE: Implementing Chat API Route with AI SDK and OpenAI
DESCRIPTION: Next.js API route handler using AI SDK Core and OpenAI provider to stream text responses for a chat application.

LANGUAGE: typescript
CODE:
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    system: 'You are a helpful assistant.',
    messages,
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Creating a JSON Schema with jsonSchema() in TypeScript
DESCRIPTION: Demonstrates how to use the jsonSchema() function to create a typed JSON schema for a recipe object. The schema defines the structure for a recipe with name, ingredients, and steps.

LANGUAGE: typescript
CODE:
import { jsonSchema } from 'ai';

const mySchema = jsonSchema<{
  recipe: {
    name: string;
    ingredients: { name: string; amount: string }[];
    steps: string[];
  };
}>({
  type: 'object',
  properties: {
    recipe: {
      type: 'object',
      properties: {
        name: { type: 'string' },
        ingredients: {
          type: 'array',
          items: {
            type: 'object',
            properties: {
              name: { type: 'string' },
              amount: { type: 'string' },
            },
            required: ['name', 'amount'],
          },
        },
        steps: {
          type: 'array',
          items: { type: 'string' },
        },
      },
      required: ['name', 'ingredients', 'steps'],
    },
  },
  required: ['recipe'],
});

----------------------------------------

TITLE: Using pipeDataStreamToResponse Function in TypeScript/TSX
DESCRIPTION: This example demonstrates how to use the pipeDataStreamToResponse function to pipe streaming data to a ServerResponse object. It shows setting response status, headers, writing data, annotations, and merging streams.

LANGUAGE: tsx
CODE:
pipeDataStreamToResponse(serverResponse, {
  status: 200,
  statusText: 'OK',
  headers: {
    'Custom-Header': 'value',
  },
  async execute(dataStream) {
    // Write data
    dataStream.writeData({ value: 'Hello' });

    // Write annotation
    dataStream.writeMessageAnnotation({ type: 'status', value: 'processing' });

    // Merge another stream
    const otherStream = getAnotherStream();
    dataStream.merge(otherStream);
  },
  onError: error => `Custom error: ${error.message}`,
});

----------------------------------------

TITLE: Creating Custom Cerebras Provider
DESCRIPTION: Example of creating a custom Cerebras provider instance with specific configuration options including API key.

LANGUAGE: typescript
CODE:
import { createCerebras } from '@ai-sdk/cerebras';

const cerebras = createCerebras({
  apiKey: process.env.CEREBRAS_API_KEY ?? '',
});

----------------------------------------

TITLE: Creating API Route for Chat with Caching Middleware
DESCRIPTION: This snippet shows how to create an API route for handling chat messages and responses. It uses the custom caching middleware and implements a weather tool for the language model.

LANGUAGE: typescript
CODE:
import { cacheMiddleware } from '@/ai/middleware';
import { openai } from '@ai-sdk/openai';
import { wrapLanguageModel, streamText, tool } from 'ai';
import { z } from 'zod';

const wrappedModel = wrapLanguageModel({
  model: openai('gpt-4o-mini'),
  middleware: cacheMiddleware,
});

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: wrappedModel,
    messages,
    tools: {
      weather: tool({
        description: 'Get the weather in a location',
        parameters: z.object({
          location: z.string().describe('The location to get the weather for'),
        }),
        execute: async ({ location }) => ({
          location,
          temperature: 72 + Math.floor(Math.random() * 21) - 10,
        }),
      }),
    },
  });
  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Streaming Text with Express and AI SDK
DESCRIPTION: An Express server setup that demonstrates how to stream text directly to the client using pipeTextStreamToResponse. It uses the AI SDK to generate text with OpenAI's GPT-4 model.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import express, { Request, Response } from 'express';

const app = express();

app.post('/', async (req: Request, res: Response) => {
  const result = streamText({
    model: openai('gpt-4o'),
    prompt: 'Invent a new holiday and describe its traditions.',
  });

  result.pipeTextStreamToResponse(res);
});

app.listen(8080, () => {
  console.log(`Example app listening on port ${8080}`);
});

----------------------------------------

TITLE: Creating Chat Route Handler in Next.js
DESCRIPTION: Implements a POST route handler for chat functionality using the AI SDK and OpenAI integration. Handles streaming responses and message processing.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';

export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    messages,
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Streaming Text Generation with Reader in TypeScript
DESCRIPTION: This snippet shows how to stream text generation using a reader. It uses the same AI SDK and OpenAI model setup as the previous example, but utilizes a reader to process the text stream. This method provides more control over the streaming process.

LANGUAGE: typescript
CODE:
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';

const result = streamText({
  model: openai('gpt-3.5-turbo'),
  maxTokens: 512,
  temperature: 0.3,
  maxRetries: 5,
  prompt: 'Invent a new holiday and describe its traditions.',
});

const reader = result.textStream.getReader();

while (true) {
  const { done, value } = await reader.read();
  if (done) {
    break;
  }
  console.log(value);
}

----------------------------------------

TITLE: Customizing Chatbot UI with Status Indicators in React
DESCRIPTION: This example shows how to enhance the chatbot UI by adding status indicators, a stop button, and disabling the input when appropriate based on the chat status.

LANGUAGE: tsx
CODE:
'use client';

import { useChat } from '@ai-sdk/react';

export default function Page() {
  const { messages, input, handleInputChange, handleSubmit, status, stop } =
    useChat({});

  return (
    <>
      {messages.map(message => (
        <div key={message.id}>
          {message.role === 'user' ? 'User: ' : 'AI: '}
          {message.content}
        </div>
      ))}

      {(status === 'submitted' || status === 'streaming') && (
        <div>
          {status === 'submitted' && <Spinner />}
          <button type="button" onClick={() => stop()}>
            Stop
          </button>
        </div>
      )}

      <form onSubmit={handleSubmit}>
        <input
          name="prompt"
          value={input}
          onChange={handleInputChange}
          disabled={status !== 'ready'}
        />
        <button type="submit">Submit</button>
      </form>
    </>
  );
}

----------------------------------------

TITLE: Tool Call Repair with Structured Outputs in TypeScript
DESCRIPTION: Shows how to implement tool call repair using a model with structured outputs. This approach attempts to fix invalid tool calls by generating corrected arguments based on the tool's schema.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateObject, generateText, NoSuchToolError, tool } from 'ai';

const result = await generateText({
  model,
  tools,
  prompt,

  experimental_repairToolCall: async ({
    toolCall,
    tools,
    parameterSchema,
    error,
  }) => {
    if (NoSuchToolError.isInstance(error)) {
      return null; // do not attempt to fix invalid tool names
    }

    const tool = tools[toolCall.toolName as keyof typeof tools];

    const { object: repairedArgs } = await generateObject({
      model: openai('gpt-4o', { structuredOutputs: true }),
      schema: tool.parameters,
      prompt: [
        `The model tried to call the tool "${toolCall.toolName}"` +
          ` with the following arguments:`,
        JSON.stringify(toolCall.args),
        `The tool accepts the following schema:`,
        JSON.stringify(parameterSchema(toolCall)),
        'Please fix the arguments.',
      ].join('\n'),
    });

    return { ...toolCall, args: JSON.stringify(repairedArgs) };
  },
});

----------------------------------------

TITLE: Handling Generic Stream Errors with createStreamableValue in TypeScript
DESCRIPTION: Demonstrates error handling for non-UI streaming data using createStreamableValue. The code shows how to handle multiple sequential updates and return either successful data or error messages.

LANGUAGE: tsx
CODE:
'use server';

import { createStreamableValue } from 'ai/rsc';
import { fetchData, emptyData } from '../utils/data';

export const getStreamedData = async () => {
  const streamableData = createStreamableValue<string>(emptyData);

  try {
    (() => {
      const data1 = await fetchData();
      streamableData.update(data1);

      const data2 = await fetchData();
      streamableData.update(data2);

      const data3 = await fetchData();
      streamableData.done(data3);
    })();

    return { data: streamableData.value };
  } catch (e) {
    return { error: e.message };
  }
};

----------------------------------------

TITLE: Implementing Batch Text Embedding with AI SDK in TypeScript
DESCRIPTION: Demonstrates how to use the AI SDK to convert multiple text inputs into embeddings in a single operation. The code utilizes OpenAI's text-embedding-3-small model and returns both the embeddings and usage statistics.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { embedMany } from 'ai';
import 'dotenv/config';

async function main() {
  const { embeddings, usage } = await embedMany({
    model: openai.embedding('text-embedding-3-small'),
    values: [
      'sunny day at the beach',
      'rainy afternoon in the city',
      'snowy night in the mountains',
    ],
  });

  console.log(embeddings);
  console.log(usage);
}

main().catch(console.error);

----------------------------------------

TITLE: Generating Text with Anthropic Claude Model
DESCRIPTION: Complete example showing how to generate text using the Anthropic provider with Claude model. Demonstrates proper model selection and prompt formatting.

LANGUAGE: typescript
CODE:
import { anthropic } from '@ai-sdk/anthropic';
import { generateText } from 'ai';

const { text } = await generateText({
  model: anthropic('claude-3-haiku-20240307'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Generating Responses with Tools Call in TypeScript
DESCRIPTION: Demonstrates how to use the generateText function with tools call functionality, incorporating memory context and custom tool definitions.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';
import { createMem0 } from '@mem0/vercel-ai-provider';
import { z } from 'zod';

const mem0 = createMem0({
  provider: 'anthropic',
  apiKey: 'anthropic-api-key',
  mem0Config: {
    // Global User ID
    user_id: 'borat',
  },
});

const prompt = 'What the temperature in the city that I live in?';

const result = await generateText({
  model: mem0('claude-3-5-sonnet-20240620'),
  tools: {
    weather: tool({
      description: 'Get the weather in a location',
      parameters: z.object({
        location: z.string().describe('The location to get the weather for'),
      }),
      execute: async ({ location }) => ({
        location,
        temperature: 72 + Math.floor(Math.random() * 21) - 10,
      }),
    }),
  },
  prompt: prompt,
});

console.log(result);

----------------------------------------

TITLE: Generating Text with Perplexity Model
DESCRIPTION: Example of using Perplexity's Sonar model to generate text responses for a given prompt

LANGUAGE: typescript
CODE:
import { perplexity } from '@ai-sdk/perplexity';
import { generateText } from 'ai';

const { text } = await generateText({
  model: perplexity('sonar-pro'),
  prompt: 'What are the latest developments in quantum computing?',
});

----------------------------------------

TITLE: Handling Client-Side Loading State in Next.js with AI SDK RSC
DESCRIPTION: This snippet demonstrates how to manage loading state on the client side in a Next.js application using AI SDK RSC. It includes a form that submits user input, handles the loading state, and displays the generated response.

LANGUAGE: tsx
CODE:
'use client';

import { useState } from 'react';
import { generateResponse } from './actions';
import { readStreamableValue } from 'ai/rsc';

// Force the page to be dynamic and allow streaming responses up to 30 seconds
export const maxDuration = 30;

export default function Home() {
  const [input, setInput] = useState<string>('');
  const [generation, setGeneration] = useState<string>('');
  const [loading, setLoading] = useState<boolean>(false);

  return (
    <div>
      <div>{generation}</div>
      <form
        onSubmit={async e => {
          e.preventDefault();
          setLoading(true);
          const response = await generateResponse(input);

          let textContent = '';

          for await (const delta of readStreamableValue(response)) {
            textContent = `${textContent}${delta}`;
            setGeneration(textContent);
          }
          setInput('');
          setLoading(false);
        }}
      >
        <input
          type="text"
          value={input}
          disabled={loading}
          className="disabled:opacity-50"
          onChange={event => {
            setInput(event.target.value);
          }}
        />
        <button>Send Message</button>
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Implementing Chat Interface with Streaming in React
DESCRIPTION: This code snippet shows how to create a React component using the useChat hook from @ai-sdk/react to implement a chat interface with real-time streaming of responses.

LANGUAGE: tsx
CODE:
'use client';

import { useChat } from '@ai-sdk/react';

export default function Page() {
  const { messages, input, setInput, append } = useChat();

  return (
    <div>
      <input
        value={input}
        onChange={event => {
          setInput(event.target.value);
        }}
        onKeyDown={async event => {
          if (event.key === 'Enter') {
            append({ content: input, role: 'user' });
          }
        }}
      />

      {messages.map((message, index) => (
        <div key={index}>{message.content}</div>
      ))}
    </div>
  );
}

----------------------------------------

TITLE: Handling Streaming Errors in Simple Streams with TypeScript
DESCRIPTION: Shows how to handle errors in simple streaming scenarios where error chunks are not supported. Uses try/catch block around a streaming text generation process.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';

try {
  const { textStream } = streamText({
    model: yourModel,
    prompt: 'Write a vegetarian lasagna recipe for 4 people.',
  });

  for await (const textPart of textStream) {
    process.stdout.write(textPart);
  }
} catch (error) {
  // handle error
}

----------------------------------------

TITLE: Using onStepFinish Callback in TypeScript
DESCRIPTION: Shows how to use the onStepFinish callback to perform custom logic after each step in a multi-step generation. This can be useful for logging, analytics, or updating UI in real-time.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';

const result = await generateText({
  // ...
  onStepFinish({ text, toolCalls, toolResults, finishReason, usage }) {
    // your own logic, e.g. for saving the chat history or recording usage
  },
});

----------------------------------------

TITLE: Processing Tool Results in AI SDK
DESCRIPTION: Shows how to access and process tool execution results. Includes type-safe handling of tool results and demonstrates accessing specific properties from each tool's response.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateText, tool } from 'ai';
import dotenv from 'dotenv';
import { z } from 'zod';

dotenv.config();

async function main() {
  const result = await generateText({
    model: openai('gpt-3.5-turbo'),
    maxTokens: 512,
    tools: {
      weather: tool({
        description: 'Get the weather in a location',
        parameters: z.object({
          location: z.string().describe('The location to get the weather for'),
        }),
        execute: async ({ location }) => ({
          location,
          temperature: 72 + Math.floor(Math.random() * 21) - 10,
        }),
      }),
      cityAttractions: tool({
        parameters: z.object({ city: z.string() }),
      }),
    },
    prompt:
      'What is the weather in San Francisco and what attractions should I visit?',
  });

  // typed tool results for tools with execute method:
  for (const toolResult of result.toolResults) {
    switch (toolResult.toolName) {
      case 'weather': {
        toolResult.args.location; // string
        toolResult.result.location; // string
        toolResult.result.temperature; // number
        break;
      }
    }
  }

  console.log(JSON.stringify(result, null, 2));
}

main().catch(console.error);

----------------------------------------

TITLE: Installing Google Generative AI Provider
DESCRIPTION: Package installation commands for different package managers (pnpm, npm, yarn)

LANGUAGE: bash
CODE:
pnpm add @ai-sdk/google

LANGUAGE: bash
CODE:
npm install @ai-sdk/google

LANGUAGE: bash
CODE:
yarn add @ai-sdk/google

----------------------------------------

TITLE: SQL Query Generation Action in Next.js
DESCRIPTION: Server action that generates SQL queries from natural language input using OpenAI.

LANGUAGE: typescript
CODE:
export const generateQuery = async (input: string) => {
  'use server';
  try {
    const result = await generateObject({
      model: openai('gpt-4o'),
      system: `You are a SQL (postgres) ...`,
      prompt: `Generate the query necessary to retrieve the data the user wants: ${input}`,
      schema: z.object({
        query: z.string(),
      }),
    });
    return result.object.query;
  } catch (e) {
    console.error(e);
    throw new Error('Failed to generate query');
  }
};

----------------------------------------

TITLE: Adding Weather Tool Integration
DESCRIPTION: Enhanced route handler with weather tool implementation using Zod schema validation

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText, tool } from 'ai';
import { z } from 'zod';

export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    messages,
    tools: {
      weather: tool({
        description: 'Get the weather in a location (fahrenheit)',
        parameters: z.object({
          location: z.string().describe('The location to get the weather for'),
        }),
        execute: async ({ location }) => {
          const temperature = Math.round(Math.random() * (90 - 32) + 32);
          return {
            location,
            temperature,
          };
        },
      }),
    },
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Using valibotSchema with Valibot in TypeScript
DESCRIPTION: This example demonstrates how to use the valibotSchema function to create a JSON schema object from a Valibot schema definition. It defines a recipe schema with name, ingredients, and steps.

LANGUAGE: typescript
CODE:
import { valibotSchema } from 'ai';
import { object, string, array } from 'valibot';

const recipeSchema = valibotSchema(
  object({
    name: string(),
    ingredients: array(
      object({
        name: string(),
        amount: string(),
      }),
    ),
    steps: array(string()),
  }),
);

----------------------------------------

TITLE: Generating Structured Data with Zod Schema
DESCRIPTION: Shows how to generate structured JSON data using the generateObject function with a Zod schema to constrain model outputs. Example demonstrates creating a recipe structure.

LANGUAGE: typescript
CODE:
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const { object } = await generateObject({
  model: openai.responses('gpt-4o'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a lasagna recipe.',
});

----------------------------------------

TITLE: Using PDF Files with OpenAI Chat API
DESCRIPTION: Example of passing PDF files to the OpenAI Chat API.

LANGUAGE: typescript
CODE:
const result = await generateText({
  model: openai('gpt-4o'),
  messages: [
    {
      role: 'user',
      content: [
        {
          type: 'text',
          text: 'What is an embedding model?',
        },
        {
          type: 'file',
          data: fs.readFileSync('./data/ai.pdf'),
          mimeType: 'application/pdf',
          filename: 'ai.pdf', // optional
        },
      ],
    },
  ],
});

----------------------------------------

TITLE: Initializing and Using MCP Clients with AI SDK in TypeScript
DESCRIPTION: This snippet shows how to initialize MCP clients for different server types (stdio, SSE, and custom), retrieve tools, and use them with the generateText function. It also demonstrates proper resource management by closing the clients after use.

LANGUAGE: typescript
CODE:
import { experimental_createMCPClient, generateText } from 'ai';
import { Experimental_StdioMCPTransport } from 'ai/mcp-stdio';
import { openai } from '@ai-sdk/openai';

let clientOne;
let clientTwo;
let clientThree;

try {
  // Initialize an MCP client to connect to a `stdio` MCP server:
  const transport = new Experimental_StdioMCPTransport({
    command: 'node',
    args: ['src/stdio/dist/server.js'],
  });
  clientOne = await experimental_createMCPClient({
    transport,
  });

  // Alternatively, you can connect to a Server-Sent Events (SSE) MCP server:
  clientTwo = await experimental_createMCPClient({
    transport: {
      type: 'sse',
      url: 'http://localhost:3000/sse',
    },
  });

  // Similarly to the stdio example, you can pass in your own custom transport as long as it implements the `MCPTransport` interface:
  const transport = new MyCustomTransport({
    // ...
  });
  clientThree = await experimental_createMCPClient({
    transport,
  });

  const toolSetOne = await clientOne.tools();
  const toolSetTwo = await clientTwo.tools();
  const toolSetThree = await clientThree.tools();
  const tools = {
    ...toolSetOne,
    ...toolSetTwo,
    ...toolSetThree, // note: this approach causes subsequent tool sets to override tools with the same name
  };

  const response = await generateText({
    model: openai('gpt-4o'),
    tools,
    messages: [
      {
        role: 'user',
        content: 'Find products under $100',
      },
    ],
  });

  console.log(response.text);
} catch (error) {
  console.error(error);
} finally {
  await Promise.all([clientOne.close(), clientTwo.close()]);
}

----------------------------------------

TITLE: Installing Google Vertex AI Provider
DESCRIPTION: Installing the Google Vertex AI provider package using different package managers

LANGUAGE: bash
CODE:
pnpm add @ai-sdk/google-vertex

LANGUAGE: bash
CODE:
npm install @ai-sdk/google-vertex

LANGUAGE: bash
CODE:
yarn add @ai-sdk/google-vertex @google-cloud/vertexai

----------------------------------------

TITLE: Defining and Using a Tool Set with Type Inference in TypeScript
DESCRIPTION: Demonstrates how to define a set of tools and use TypeScript type inference to strongly type tool calls and results. This approach enhances type safety and enables better IDE support.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { ToolCallUnion, ToolResultUnion, generateText, tool } from 'ai';
import { z } from 'zod';

const myToolSet = {
  firstTool: tool({
    description: 'Greets the user',
    parameters: z.object({ name: z.string() }),
    execute: async ({ name }) => `Hello, ${name}!`,
  }),
  secondTool: tool({
    description: 'Tells the user their age',
    parameters: z.object({ age: z.number() }),
    execute: async ({ age }) => `You are ${age} years old!`,
  }),
};

type MyToolCall = ToolCallUnion<typeof myToolSet>;
type MyToolResult = ToolResultUnion<typeof myToolSet>;

async function generateSomething(prompt: string): Promise<{
  text: string;
  toolCalls: Array<MyToolCall>; // typed tool calls
  toolResults: Array<MyToolResult>; // typed tool results
}> {
  return generateText({
    model: openai('gpt-4o'),
    tools: myToolSet,
    prompt,
  });
}

----------------------------------------

TITLE: Complete Chat Implementation with Data Clearing
DESCRIPTION: Full example of a chat implementation that includes data management, message display, and form handling with data clearing on submit.

LANGUAGE: tsx
CODE:
'use client';

import { Message, useChat } from '@ai-sdk/react';

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit, data, setData } =
    useChat();

  return (
    <>
      {data && <pre>{JSON.stringify(data, null, 2)}</pre>}

      {messages?.map((m: Message) => (
        <div key={m.id}>{`${m.role}: ${m.content}`}</div>
      ))}

      <form
        onSubmit={e => {
          setData(undefined);
          handleSubmit(e);
        }}
      >
        <input value={input} onChange={handleInputChange} />
      </form>
    </>
  );
}

----------------------------------------

TITLE: Adding Custom Headers to Image Generation Request
DESCRIPTION: Shows how to add custom headers to the image generation request.

LANGUAGE: ts
CODE:
import { openai } from '@ai-sdk/openai';
import { experimental_generateImage as generateImage } from 'ai';

const { image } = await generateImage({
  model: openai.image('dall-e-3'),
  value: 'sunny day at the beach',
  headers: { 'X-Custom-Header': 'custom-value' },
});

----------------------------------------

TITLE: Implementing No-Schema Mode for Object Streaming in Next.js
DESCRIPTION: This set of snippets demonstrates how to use the 'no-schema' output mode for streaming objects without a predefined schema. It includes client-side and server-side implementations.

LANGUAGE: tsx
CODE:
// Client
'use client';

import { experimental_useObject as useObject } from '@ai-sdk/react';
import { z } from 'zod';

export default function Page() {
  const { object, submit, isLoading, stop } = useObject({
    api: '/api/use-object',
    schema: z.unknown(),
  });

  return (
    <div>
      <button
        onClick={() => submit('Messages during finals week.')}
        disabled={isLoading}
      >
        Generate notifications
      </button>

      {isLoading && (
        <div>
          <div>Loading...</div>
          <button type="button" onClick={() => stop()}>
            Stop
          </button>
        </div>
      )}

      {object?.map((notification, index) => (
        <div key={index}>
          <p>{notification.name}</p>
          <p>{notification.message}</p>
        </div>
      ))}
    </div>
  );
}

LANGUAGE: typescript
CODE:
// Server
import { openai } from '@ai-sdk/openai';
import { streamObject } from 'ai';
import { notificationSchema } from './schema';

export const maxDuration = 30;

export async function POST(req: Request) {
  const context = await req.json();

  const result = streamObject({
    model: openai('gpt-4-turbo'),
    output: 'no-schema',
    prompt:
      `Generate 3 notifications for a messages app in this context:` + context,
  });

  return result.toTextStreamResponse();
}

----------------------------------------

TITLE: Implementing Server-Side Logic for Multi-Step Tool Calling in Next.js
DESCRIPTION: This code sets up a server-side route in Next.js to handle chat requests. It defines two tools (getLocation and getWeather) and uses the streamText function from the AI SDK to generate responses that can include multiple tool calls.

LANGUAGE: ts
CODE:
import { ToolInvocation, streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

interface Message {
  role: 'user' | 'assistant';
  content: string;
  toolInvocations?: ToolInvocation[];
}

function getLocation({ lat, lon }) {
  return { lat: 37.7749, lon: -122.4194 };
}

function getWeather({ lat, lon, unit }) {
  return { value: 25, description: 'Sunny' };
}

export async function POST(req: Request) {
  const { messages }: { messages: Message[] } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    system: 'You are a helpful assistant.',
    messages,
    tools: {
      getLocation: {
        description: 'Get the location of the user',
        parameters: z.object({}),
        execute: async () => {
          const { lat, lon } = getLocation();
          return `Your location is at latitude ${lat} and longitude ${lon}`;
        },
      },
      getWeather: {
        description: 'Get the weather for a location',
        parameters: z.object({
          lat: z.number().describe('The latitude of the location'),
          lon: z.number().describe('The longitude of the location'),
          unit: z
            .enum(['C', 'F'])
            .describe('The unit to display the temperature in'),
        }),
        execute: async ({ lat, lon, unit }) => {
          const { value, description } = getWeather({ lat, lon, unit });
          return `It is currently ${value}°${unit} and ${description}!`;
        },
      },
    },
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Implementing Text Stream with Fastify and AI SDK
DESCRIPTION: Demonstrates how to use the textStream property to get a text stream from the AI result and send it as a response in a Fastify server. It uses OpenAI's GPT-4 model to generate text based on a prompt.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import Fastify from 'fastify';

const fastify = Fastify({ logger: true });

fastify.post('/', async function (request, reply) {
  const result = streamText({
    model: openai('gpt-4o'),
    prompt: 'Invent a new holiday and describe its traditions.',
  });

  reply.header('Content-Type', 'text/plain; charset=utf-8');

  return reply.send(result.textStream);
});

fastify.listen({ port: 8080 });

----------------------------------------

TITLE: Using Tools with Llama 3.1 in AI SDK
DESCRIPTION: This example demonstrates how to use tools with the AI SDK and Llama 3.1. It includes a weather tool that allows the model to fetch simulated weather data for a given location.

LANGUAGE: tsx
CODE:
import { generateText, tool } from 'ai';
import { deepinfra } from '@ai-sdk/deepinfra';
import { getWeather } from './weatherTool';

const { text } = await generateText({
  model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),
  prompt: 'What is the weather like today?',
  tools: {
    weather: tool({
      description: 'Get the weather in a location',
      parameters: z.object({
        location: z.string().describe('The location to get the weather for'),
      }),
      execute: async ({ location }) => ({
        location,
        temperature: 72 + Math.floor(Math.random() * 21) - 10,
      }),
    }),
  },
});

----------------------------------------

TITLE: Creating Expo API Route for OpenAI Chat
DESCRIPTION: This code snippet sets up an API route in Expo to handle chat requests using OpenAI's API. It uses the streamText function from the AI SDK to process messages and stream the response.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    messages,
  });

  return result.toDataStreamResponse({
    headers: {
      'Content-Type': 'application/octet-stream',
      'Content-Encoding': 'none',
    },
  });
}

----------------------------------------

TITLE: LanguageModelV1Middleware Interface Definition
DESCRIPTION: Defines the core middleware interface with three main methods: transformParams for parameter transformation, wrapGenerate for wrapping generation operations, and wrapStream for wrapping streaming operations.

LANGUAGE: typescript
CODE:
interface LanguageModelV1Middleware {
  transformParams: ({ type: "generate" | "stream", params: LanguageModelV1CallOptions }) => Promise<LanguageModelV1CallOptions>;
  wrapGenerate: ({ doGenerate: DoGenerateFunction, params: LanguageModelV1CallOptions, model: LanguageModelV1 }) => Promise<DoGenerateResult>;
  wrapStream: ({ doStream: DoStreamFunction, params: LanguageModelV1CallOptions, model: LanguageModelV1 }) => Promise<DoStreamResult>;
}

----------------------------------------

TITLE: Generating Text with Chat Prompt using AI SDK and OpenAI GPT-3.5-turbo in TypeScript
DESCRIPTION: This code snippet demonstrates how to use the generateText function from the AI SDK to generate text based on a series of chat messages. It utilizes the OpenAI GPT-3.5-turbo model and includes a system prompt and multiple user-assistant interactions.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const result = await generateText({
  model: openai('gpt-3.5-turbo'),
  maxTokens: 1024,
  system: 'You are a helpful chatbot.',
  messages: [
    {
      role: 'user',
      content: 'Hello!',
    },
    {
      role: 'assistant',
      content: 'Hello! How can I help you today?',
    },
    {
      role: 'user',
      content: 'I need help with my computer.',
    },
  ],
});

console.log(result.text);

----------------------------------------

TITLE: Implementing Data Stream Protocol in Next.js Frontend
DESCRIPTION: Example of implementing data stream protocol in a Next.js frontend component. Uses the default data stream protocol setting with useCompletion hook.

LANGUAGE: tsx
CODE:
'use client';

import { useCompletion } from '@ai-sdk/react';

export default function Page() {
  const { completion, input, handleInputChange, handleSubmit } = useCompletion({
    streamProtocol: 'data', // optional, this is the default
  });

  return (
    <form onSubmit={handleSubmit}>
      <input name="prompt" value={input} onChange={handleInputChange} />
      <button type="submit">Submit</button>
      <div>{completion}</div>
    </form>
  );
}

----------------------------------------

TITLE: Initializing Google Vertex Provider
DESCRIPTION: Creating and configuring a Google Vertex provider instance with custom settings

LANGUAGE: typescript
CODE:
import { createVertex } from '@ai-sdk/google-vertex';

const vertex = createVertex({
  project: 'my-project', // optional
  location: 'us-central1', // optional
});

----------------------------------------

TITLE: Customizing Chat API Route with Error Handling in Next.js
DESCRIPTION: This example shows how to customize the chat API route to handle errors and control the response stream, including custom error messages and usage information.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    messages,
  });

  return result.toDataStreamResponse({
    getErrorMessage: error => {
      if (error == null) {
        return 'unknown error';
      }

      if (typeof error === 'string') {
        return error;
      }

      if (error instanceof Error) {
        return error.message;
      }

      return JSON.stringify(error);
    },
  });
}

----------------------------------------

TITLE: Implementing Array Mode for Object Streaming in Next.js
DESCRIPTION: This set of snippets demonstrates how to use the 'array' output mode for streaming an array of objects. It includes updates to the schema, client-side code, and server-side implementation.

LANGUAGE: typescript
CODE:
// Schema
import { z } from 'zod';

// define a schema for a single notification
export const notificationSchema = z.object({
  name: z.string().describe('Name of a fictional person.'),
  message: z.string().describe('Message. Do not use emojis or links.'),
});

LANGUAGE: tsx
CODE:
// Client
'use client';

import { experimental_useObject as useObject } from '@ai-sdk/react';
import { notificationSchema } from './api/use-object/schema';

export default function Page() {
  const { object, submit, isLoading, stop } = useObject({
    api: '/api/use-object',
    schema: z.array(notificationSchema),
  });

  return (
    <div>
      <button
        onClick={() => submit('Messages during finals week.')}
        disabled={isLoading}
      >
        Generate notifications
      </button>

      {isLoading && (
        <div>
          <div>Loading...</div>
          <button type="button" onClick={() => stop()}>
            Stop
          </button>
        </div>
      )}

      {object?.map((notification, index) => (
        <div key={index}>
          <p>{notification.name}</p>
          <p>{notification.message}</p>
        </div>
      ))}
    </div>
  );
}

LANGUAGE: typescript
CODE:
// Server
import { openai } from '@ai-sdk/openai';
import { streamObject } from 'ai';
import { notificationSchema } from './schema';

export const maxDuration = 30;

export async function POST(req: Request) {
  const context = await req.json();

  const result = streamObject({
    model: openai('gpt-4-turbo'),
    output: 'array',
    schema: notificationSchema,
    prompt:
      `Generate 3 notifications for a messages app in this context:` + context,
  });

  return result.toTextStreamResponse();
}

----------------------------------------

TITLE: Streaming UI Components with AI SDK RSC
DESCRIPTION: This server-side code demonstrates how to use the streamUI function to stream React components as loading state to the client. It uses a generator function to yield a loading component while the main content is being generated.

LANGUAGE: tsx
CODE:
'use server';

import { openai } from '@ai-sdk/openai';
import { streamUI } from 'ai/rsc';

export async function generateResponse(prompt: string) {
  const result = await streamUI({
    model: openai('gpt-4o'),
    prompt,
    text: async function* ({ content }) {
      yield <div>loading...</div>;
      return <div>{content}</div>;
    },
  });

  return result.value;
}

----------------------------------------

TITLE: Generating Text with DeepSeek Model - TypeScript
DESCRIPTION: Example of using DeepSeek's chat model to generate text responses using the AI SDK's generateText function.

LANGUAGE: typescript
CODE:
import { deepseek } from '@ai-sdk/deepseek';
import { generateText } from 'ai';

const { text } = await generateText({
  model: deepseek('deepseek-chat'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Implementing Authenticated Server Action in Next.js with AI SDK
DESCRIPTION: Example of a Server Action that validates authentication tokens before allowing access to weather data. Uses Next.js cookies API for token management and AI SDK's createStreamableUI for streaming UI components.

LANGUAGE: tsx
CODE:
'use server';

import { cookies } from 'next/headers';
import { createStremableUI } from 'ai/rsc';
import { validateToken } from '../utils/auth';

export const getWeather = async () => {
  const token = cookies().get('token');

  if (!token || !validateToken(token)) {
    return {
      error: 'This action requires authentication',
    };
  }
  const streamableDisplay = createStreamableUI(null);

  streamableDisplay.update(<Skeleton />);
  streamableDisplay.done(<Weather />);

  return {
    display: streamableDisplay.value,
  };
};

----------------------------------------

TITLE: Streaming Object Generation with Schema in TypeScript
DESCRIPTION: Example of using streamObject to generate a structured recipe object based on a Zod schema.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamObject } from 'ai';
import { z } from 'zod';

const { partialObjectStream } = streamObject({
  model: openai('gpt-4-turbo'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.string()),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a lasagna recipe.',
});

for await (const partialObject of partialObjectStream) {
  console.clear();
  console.log(partialObject);
}

----------------------------------------

TITLE: Implementing Tool Calling with GPT-3.5 Turbo in TypeScript
DESCRIPTION: This code snippet demonstrates how to use GPT-3.5 Turbo to implement tool calling for a weather assistant. It defines a sendMessage function that can call a getWeather tool based on user queries, showcasing deterministic outputs with probabilistic routing.

LANGUAGE: tsx
CODE:
const sendMessage = (prompt: string) =>
  generateText({
    model: 'gpt-3.5-turbo',
    system: 'you are a friendly weather assistant!',
    prompt,
    tools: {
      getWeather: {
        description: 'Get the weather in a location',
        parameters: z.object({
          location: z.string().describe('The location to get the weather for'),
        }),
        execute: async ({ location }: { location: string }) => ({
          location,
          temperature: 72 + Math.floor(Math.random() * 21) - 10,
        }),
      },
    },
  });

sendMessage('What is the weather in San Francisco?'); // getWeather is called
sendMessage('What is the weather in New York?'); // getWeather is called
sendMessage('What events are happening in London?'); // No function is called

----------------------------------------

TITLE: Creating AI Context Provider Configuration
DESCRIPTION: Sets up the AI context provider with initial states and action definitions using createAI.

LANGUAGE: typescript
CODE:
import { createAI } from 'ai/rsc';
import { ClientMessage, ServerMessage, sendMessage } from './actions';

export type AIState = ServerMessage[];
export type UIState = ClientMessage[];

export const AI = createAI<AIState, UIState>({
  initialAIState: [],
  initialUIState: [],
  actions: {
    sendMessage,
  },
});

----------------------------------------

TITLE: Implementing API Route with Tool Definitions
DESCRIPTION: Sets up an API route that defines three different types of tools: a server-side weather tool, a client-side confirmation tool, and an automatic location tool. Includes tool parameter validation using zod.

LANGUAGE: tsx
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import { z } from 'zod';

export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    messages,
    tools: {
      getWeatherInformation: {
        description: 'show the weather in a given city to the user',
        parameters: z.object({ city: z.string() }),
        execute: async ({}: { city: string }) => {
          const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy', 'windy'];
          return weatherOptions[
            Math.floor(Math.random() * weatherOptions.length)
          ];
        },
      },
      askForConfirmation: {
        description: 'Ask the user for confirmation.',
        parameters: z.object({
          message: z.string().describe('The message to ask for confirmation.')
        }),
      },
      getLocation: {
        description: 'Get the user location. Always ask for confirmation before using this tool.',
        parameters: z.object({}),
      },
    },
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Importing and Basic Usage of createIdGenerator in TypeScript
DESCRIPTION: Demonstrates how to import the createIdGenerator function and create a basic custom ID generator with a prefix and separator.

LANGUAGE: typescript
CODE:
import { createIdGenerator } from 'ai';

const generateCustomId = createIdGenerator({
  prefix: 'user',
  separator: '_',
});

const id = generateCustomId(); // Example: "user_1a2b3c4d5e6f7g8h"

----------------------------------------

TITLE: Generating Structured Data with OpenAI o1 and Zod Schema
DESCRIPTION: Illustrates how to use the AI SDK Core to generate structured JSON data using the OpenAI o1 model. It uses a Zod schema to define the structure of the output, ensuring type-safe data generation.

LANGUAGE: tsx
CODE:
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const { object } = await generateObject({
  model: openai('o1'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a lasagna recipe.',
});

----------------------------------------

TITLE: Using OpenAI Structured Outputs
DESCRIPTION: Example of using OpenAI structured outputs with a specific schema.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateObject } from 'ai';
import { z } from 'zod';

const result = await generateObject({
  model: openai('gpt-4o-2024-08-06', {
    structuredOutputs: true,
  }),
  schemaName: 'recipe',
  schemaDescription: 'A recipe for lasagna.',
  schema: z.object({
    name: z.string(),
    ingredients: z.array(
      z.object({
        name: z.string(),
        amount: z.string(),
      }),
    ),
    steps: z.array(z.string()),
  }),
  prompt: 'Generate a lasagna recipe.',
});

console.log(JSON.stringify(result.object, null, 2));

----------------------------------------

TITLE: Implementing Chat Interface in Vue.js
DESCRIPTION: Creates a Vue component that uses the useChat hook from the AI SDK to handle chat functionality, including displaying messages, tool invocations, and user input.

LANGUAGE: vue
CODE:
<script setup lang="ts">
import { useChat } from '@ai-sdk/vue';

const { messages, input, handleSubmit } = useChat({ maxSteps: 5 });
</script>

<template>
  <div class="flex flex-col w-full max-w-md py-24 mx-auto stretch">
    <div v-for="m in messages" :key="m.id" class="whitespace-pre-wrap">
      {{ m.role === 'user' ? 'User: ' : 'AI: ' }}
      <template v-if="m.toolInvocations">
        <pre>{{ JSON.stringify(m.toolInvocations, null, 2) }}</pre>
      </template>
      <template v-else>
        <p>{{ m.content }}</p>
      </template>
    </div>

    <form @submit="handleSubmit">
      <input
        class="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl"
        v-model="input"
        placeholder="Say something..."
      />
    </form>
  </div>
</template>

----------------------------------------

TITLE: Basic Text Prompt with GenerateText
DESCRIPTION: Example of using a simple text prompt with the generateText function. Shows how to provide a basic string prompt to generate content.

LANGUAGE: typescript
CODE:
const result = await generateText({
  model: yourModel,
  prompt: 'Invent a new holiday and describe its traditions.',
});

----------------------------------------

TITLE: Importing Default Voyage Provider
DESCRIPTION: Basic import of the default Voyage provider instance.

LANGUAGE: typescript
CODE:
import { voyage } from 'voyage-ai-provider';

----------------------------------------

TITLE: Configuring Multi-Step Tool Calls with AI SDK in TypeScript
DESCRIPTION: This code snippet demonstrates how to set up multi-step tool calls using the AI SDK. It configures a weather tool and sets the maxSteps option to 5, allowing up to 5 LLM calls to prevent infinite loops. The example uses the OpenAI GPT-4 Turbo model and includes a prompt to get the weather in San Francisco.

LANGUAGE: typescript
CODE:
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const { text } = await generateText({
  model: openai('gpt-4-turbo'),
  maxSteps: 5,
  tools: {
    weather: tool({
      description: 'Get the weather in a location',
      parameters: z.object({
        location: z.string().describe('The location to get the weather for'),
      }),
      execute: async ({ location }: { location: string }) => ({
        location,
        temperature: 72 + Math.floor(Math.random() * 21) - 10,
      }),
    }),
  },
  prompt: 'What is the weather in San Francisco?',
});

----------------------------------------

TITLE: Creating Chat Interface with React in Next.js
DESCRIPTION: This code snippet shows how to create a client-side chat interface using React in a Next.js application. It manages the chat state, handles user input, and communicates with the server to generate responses.

LANGUAGE: tsx
CODE:
'use client';

import { CoreMessage } from 'ai';
import { useState } from 'react';

export default function Page() {
  const [input, setInput] = useState('');
  const [messages, setMessages] = useState<CoreMessage[]>([]);

  return (
    <div>
      <input
        value={input}
        onChange={event => {
          setInput(event.target.value);
        }}
        onKeyDown={async event => {
          if (event.key === 'Enter') {
            setMessages(currentMessages => [
              ...currentMessages,
              { role: 'user', content: input },
            ]);

            const response = await fetch('/api/chat', {
              method: 'POST',
              body: JSON.stringify({
                messages: [...messages, { role: 'user', content: input }],
              }),
            });

            const { messages: newMessages } = await response.json();

            setMessages(currentMessages => [
              ...currentMessages,
              ...newMessages,
            ]);
          }
        }}
      />

      {messages.map((message, index) => (
        <div key={`${message.role}-${index}`}>
          {typeof message.content === 'string'
            ? message.content
            : message.content
                .filter(part => part.type === 'text')
                .map((part, partIndex) => (
                  <div key={partIndex}>{part.text}</div>
                ))}
        </div>
      ))}
    </div>
  );
}

----------------------------------------

TITLE: Generating Text with Together.ai Model
DESCRIPTION: Example of using a Together.ai language model to generate text with the generateText function.

LANGUAGE: typescript
CODE:
import { togetherai } from '@ai-sdk/togetherai';
import { generateText } from 'ai';

const { text } = await generateText({
  model: togetherai('meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Implementing Error Callback Handler in useChat Hook
DESCRIPTION: Shows how to implement an error callback function with the useChat hook to handle errors programmatically.

LANGUAGE: tsx
CODE:
import { useChat } from '@ai-sdk/react';

export default function Page() {
  const {
    /* ... */
  } = useChat({
    // handle error:
    onError: error => {
      console.error(error);
    },
  });
}

----------------------------------------

TITLE: Implementing Text Stream Protocol in Next.js Frontend
DESCRIPTION: Example of implementing text stream protocol in a Next.js frontend component using useCompletion hook from AI SDK. Shows form setup with input handling and completion display.

LANGUAGE: tsx
CODE:
'use client';

import { useCompletion } from '@ai-sdk/react';

export default function Page() {
  const { completion, input, handleInputChange, handleSubmit } = useCompletion({
    streamProtocol: 'text',
  });

  return (
    <form onSubmit={handleSubmit}>
      <input name="prompt" value={input} onChange={handleInputChange} />
      <button type="submit">Submit</button>
      <div>{completion}</div>
    </form>
  );
}

----------------------------------------

TITLE: Web Search Tool Integration
DESCRIPTION: Shows how to use the built-in webSearch tool for grounding responses with internet data. Includes basic setup and advanced configuration with metadata.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

const result = await generateText({
  model: openai.responses('gpt-4o-mini'),
  prompt: 'What happened in San Francisco last week?',
  tools: {
    web_search_preview: openai.tools.webSearchPreview(),
  },
});

console.log(result.text);
console.log(result.sources);

----------------------------------------

TITLE: Importing and Using streamText in TypeScript
DESCRIPTION: This example demonstrates how to import and use the streamText function to generate text from a language model and stream the results.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';

const { textStream } = streamText({
  model: openai('gpt-4o'),
  prompt: 'Invent a new holiday and describe its traditions.',
});

for await (const textPart of textStream) {
  process.stdout.write(textPart);
}

----------------------------------------

TITLE: Creating Streamable UI with Server Action
DESCRIPTION: Server-side implementation showing how to create and update a streamable UI component that displays weather information.

LANGUAGE: tsx
CODE:
'use server';

import { createStreamableUI } from 'ai/rsc';

export async function getWeather() {
  const weatherUI = createStreamableUI();

  weatherUI.update(<div style={{ color: 'gray' }}>Loading...</div>);

  setTimeout(() => {
    weatherUI.done(<div>It&apos;s a sunny day!</div>);
  }, 1000);

  return weatherUI.value;
}

----------------------------------------

TITLE: Rendering Chat Interface with React Server Components
DESCRIPTION: This code snippet shows the client-side implementation of a chat interface using React Server Components. It manages the conversation state, handles user input, and renders messages.

LANGUAGE: tsx
CODE:
'use client';

import { useState } from 'react';
import { ClientMessage } from './actions';
import { useActions, useUIState } from 'ai/rsc';
import { generateId } from 'ai';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export default function Home() {
  const [input, setInput] = useState<string>('');
  const [conversation, setConversation] = useUIState();
  const { continueConversation } = useActions();

  return (
    <div>
      <div>
        {conversation.map((message: ClientMessage) => (
          <div key={message.id}>
            {message.role}: {message.display}
          </div>
        ))}
      </div>

      <div>
        <input
          type="text"
          value={input}
          onChange={event => {
            setInput(event.target.value);
          }}
        />
        <button
          onClick={async () => {
            setConversation((currentConversation: ClientMessage[]) => [
              ...currentConversation,
              { id: generateId(), role: 'user', display: input },
            ]);

            const message = await continueConversation(input);

            setConversation((currentConversation: ClientMessage[]) => [
              ...currentConversation,
              message,
            ]);
          }}
        >
          Send Message
        </button>
      </div>
    </div>
  );
}

----------------------------------------

TITLE: Initializing Google Language Model
DESCRIPTION: Creating a language model instance with optional safety settings

LANGUAGE: typescript
CODE:
const model = google('gemini-1.5-pro-latest', {
  safetySettings: [
    { category: 'HARM_CATEGORY_UNSPECIFIED', threshold: 'BLOCK_LOW_AND_ABOVE' },
  ],
});

----------------------------------------

TITLE: Streaming Responses with Memory Context in TypeScript
DESCRIPTION: Shows how to use the streamText function to stream responses with memory context using the Mem0 provider.

LANGUAGE: typescript
CODE:
import { streamText } from 'ai';
import { createMem0 } from '@mem0/vercel-ai-provider';

const mem0 = createMem0();

const { textStream } = await streamText({
  model: mem0('gpt-4-turbo', {
    user_id: 'borat',
  }),
  prompt:
    'Suggest me a good car to buy! Why is it better than the other cars for me? Give options for every price range.',
});

for await (const textPart of textStream) {
  process.stdout.write(textPart);
}

----------------------------------------

TITLE: Implementing Server-Side Object Generation with AI SDK
DESCRIPTION: This server-side code demonstrates how to use the AI SDK to generate a stream of notifications based on a given input. It uses the 'streamObject' function and defines a schema for the generated objects.

LANGUAGE: typescript
CODE:
'use server';

import { streamObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { createStreamableValue } from 'ai/rsc';
import { z } from 'zod';

export async function generate(input: string) {
  'use server';

  const stream = createStreamableValue();

  (async () => {
    const { partialObjectStream } = streamObject({
      model: openai('gpt-4-turbo'),
      system: 'You generate three notifications for a messages app.',
      prompt: input,
      schema: z.object({
        notifications: z.array(
          z.object({
            name: z.string().describe('Name of a fictional person.'),
            message: z.string().describe('Do not use emojis or links.'),
            minutesAgo: z.number(),
          }),
        ),
      }),
    });

    for await (const partialObject of partialObjectStream) {
      stream.update(partialObject);
    }

    stream.done();
  })();

  return { object: stream.value };
}

----------------------------------------

TITLE: Implementing Client-side Message Handling
DESCRIPTION: Demonstrates how to handle message submission and update UI state in a client component using useActions and useUIState hooks.

LANGUAGE: typescript
CODE:
'use client';

import { useActions, useUIState } from 'ai/rsc';
import { AI } from './ai';

export default function Page() {
  const { sendMessage } = useActions<typeof AI>();
  const [messages, setMessages] = useUIState();

  const handleSubmit = async event => {
    event.preventDefault();

    setMessages([
      ...messages,
      { id: Date.now(), role: 'user', display: event.target.message.value },
    ]);

    const response = await sendMessage(event.target.message.value);

    setMessages([
      ...messages,
      { id: Date.now(), role: 'assistant', display: response },
    ]);
  };

  return (
    <>
      <ul>
        {messages.map(message => (
          <li key={message.id}>{message.display}</li>
        ))}
      </ul>
      <form onSubmit={handleSubmit}>
        <input type="text" name="message" />
        <button type="submit">Send</button>
      </form>
    </>
  );
}

----------------------------------------

TITLE: Generating Text with OpenAI GPT-3.5-turbo in TypeScript
DESCRIPTION: This code snippet demonstrates how to use the AI SDK to generate text using OpenAI's GPT-3.5-turbo model. It imports the necessary functions, sets up the model, provides a prompt, and logs the result.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const result = await generateText({
  model: openai('gpt-3.5-turbo'),
  prompt: 'Why is the sky blue?',
});

console.log(result);

----------------------------------------

TITLE: Tool Integration with Weather Example
DESCRIPTION: Demonstrates how to use tool calling with the AI SDK by implementing a weather checking function. Shows tool definition and execution setup.

LANGUAGE: typescript
CODE:
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';

const { text } = await generateText({
  model: openai.responses('gpt-4o'),
  prompt: 'What is the weather like today in San Francisco?',
  tools: {
    getWeather: tool({
      description: 'Get the weather in a location',
      parameters: z.object({
        location: z.string().describe('The location to get the weather for'),
      }),
      execute: async ({ location }) => ({
        location,
        temperature: 72 + Math.floor(Math.random() * 21) - 10,
      }),
    }),
  },
});

----------------------------------------

TITLE: Generating Text with NIM Language Model
DESCRIPTION: Example of using a NIM language model to generate text with the generateText function. It demonstrates setting up the provider, generating text, and logging the results.

LANGUAGE: typescript
CODE:
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
import { generateText } from 'ai';

const nim = createOpenAICompatible({
  name: 'nim',
  baseURL: 'https://integrate.api.nvidia.com/v1',
  headers: {
    Authorization: `Bearer ${process.env.NIM_API_KEY}`,
  },
});

const { text, usage, finishReason } = await generateText({
  model: nim.chatModel('deepseek-ai/deepseek-r1'),
  prompt: 'Tell me the history of the San Francisco Mission-style burrito.',
});

console.log(text);
console.log('Token usage:', usage);
console.log('Finish reason:', finishReason);

----------------------------------------

TITLE: Generating Text with OpenAI Provider
DESCRIPTION: Example of using the OpenAI provider to generate text using the GPT-4 Turbo model. Demonstrates how to set up the model and prompt for text generation.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

const { text } = await generateText({
  model: openai('gpt-4-turbo'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Creating Custom AnthropicVertex Provider in TypeScript
DESCRIPTION: Create a customized AnthropicVertex provider instance with specific settings.

LANGUAGE: typescript
CODE:
import { createAnthropicVertex } from 'anthropic-vertex-ai';

const anthropicVertex = createAnthropicVertex({
  region: 'us-central1',
  projectId: 'your-project-id',
  // other options
});

----------------------------------------

TITLE: Limiting Available Models with Custom Provider in TypeScript
DESCRIPTION: This snippet demonstrates how to create a custom provider that limits the available models in the system, even when using multiple providers.

LANGUAGE: typescript
CODE:
import { anthropic } from '@ai-sdk/anthropic';
import { openai } from '@ai-sdk/openai';
import { customProvider } from 'ai';

export const myProvider = customProvider({
  languageModels: {
    'text-medium': anthropic('claude-3-5-sonnet-20240620'),
    'text-small': openai('gpt-4o-mini'),
    'structure-medium': openai('gpt-4o', { structuredOutputs: true }),
    'structure-fast': openai('gpt-4o-mini', { structuredOutputs: true }),
  },
  embeddingModels: {
    emdedding: openai.textEmbeddingModel('text-embedding-3-small'),
  },
  // no fallback provider
});

----------------------------------------

TITLE: Initializing MCP Client with SSE Transport in TypeScript
DESCRIPTION: Demonstrates how to initialize an MCP (Model Context Protocol) client using Server-Sent Events (SSE) transport. This setup allows for real-time communication with remote MCP servers.

LANGUAGE: typescript
CODE:
import { experimental_createMCPClient as createMCPClient } from 'ai';

const mcpClient = await createMCPClient({
  transport: {
    type: 'sse',
    url: 'https://my-server.com/sse',

    // optional: configure HTTP headers, e.g. for authentication
    headers: {
      Authorization: 'Bearer my-api-key',
    },
  },
});

----------------------------------------

TITLE: Streaming Loading State from Server to Client with AI SDK RSC
DESCRIPTION: This server-side code demonstrates how to create and update a separate streamable value for tracking loading state, which can be read on the frontend for more granular loading feedback.

LANGUAGE: typescript
CODE:
'use server';

import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { createStreamableValue } from 'ai/rsc';

export async function generateResponse(prompt: string) {
  const stream = createStreamableValue();
  const loadingState = createStreamableValue({ loading: true });

  (async () => {
    const { textStream } = streamText({
      model: openai('gpt-4o'),
      prompt,
    });

    for await (const text of textStream) {
      stream.update(text);
    }

    stream.done();
    loadingState.done({ loading: false });
  })();

  return { response: stream.value, loadingState: loadingState.value };
}

----------------------------------------

TITLE: Generating Weather Text with OpenAI Model in TypeScript
DESCRIPTION: This code snippet demonstrates how to use an OpenAI language model to generate weather information as text using a custom tool. It includes parameter validation with Zod and executes a weather lookup function.

LANGUAGE: tsx
CODE:
const text = generateText({
  model: openai('gpt-3.5-turbo'),
  system: 'You are a friendly assistant',
  prompt: 'What is the weather in SF?',
  tools: {
    getWeather: {
      description: 'Get the weather for a location',
      parameters: z.object({
        city: z.string().describe('The city to get the weather for'),
        unit: z
          .enum(['C', 'F'])
          .describe('The unit to display the temperature in'),
      }),
      execute: async ({ city, unit }) => {
        const weather = getWeather({ city, unit });
        return `It is currently ${weather.value}°${unit} and ${weather.description} in ${city}!`;
      },
    },
  },
});

----------------------------------------

TITLE: Implementing Streaming API Route with Next.js App Router
DESCRIPTION: Demonstrates how to create a streaming-compatible API route using Next.js App Router and AI SDK. The route handles chat messages and streams responses using OpenAI's GPT-4 model with a 30-second duration limit.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { StreamingTextResponse, streamText } from 'ai';

// Force the route to be dynamic and allow streaming responses up to 30 seconds
export const dynamic = 'force-dynamic';
export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai('gpt-4-turbo'),
    messages,
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Setting Up Provider Registry in TypeScript
DESCRIPTION: This code snippet shows how to create a provider registry with multiple providers and models using the createProviderRegistry function from the AI SDK.

LANGUAGE: typescript
CODE:
import { anthropic } from '@ai-sdk/anthropic';
import { createOpenAI } from '@ai-sdk/openai';
import { createProviderRegistry } from 'ai';

export const registry = createProviderRegistry({
  // register provider with prefix and default setup:
  anthropic,

  // register provider with prefix and custom setup:
  openai: createOpenAI({
    apiKey: process.env.OPENAI_API_KEY,
  }),
});

----------------------------------------

TITLE: Generating Text with Memory Context in TypeScript
DESCRIPTION: Shows how to use the generateText function with Mem0 provider to generate text responses incorporating memory context.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';
import { createMem0 } from '@mem0/vercel-ai-provider';

const mem0 = createMem0();

const { text } = await generateText({
  model: mem0('gpt-4-turbo', { user_id: 'borat' }),
  prompt: 'Suggest me a good car to buy!',
});

----------------------------------------

TITLE: Implementing API Route for Object Generation in Next.js
DESCRIPTION: This server-side code implements a route handler for '/api/completion' that uses the generateObject function from the AI SDK. It defines a schema using zod and generates an object based on the input prompt.

LANGUAGE: typescript
CODE:
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

export async function POST(req: Request) {
  const { prompt }: { prompt: string } = await req.json();

  const result = await generateObject({
    model: openai('gpt-4'),
    system: 'You generate three notifications for a messages app.',
    prompt,
    schema: z.object({
      notifications: z.array(
        z.object({
          name: z.string().describe('Name of a fictional person.'),
          message: z.string().describe('Do not use emojis or links.'),
          minutesAgo: z.number(),
        }),
      ),
    }),
  });

  return result.toJsonResponse();
}

----------------------------------------

TITLE: Generating Text with Inflection AI in TypeScript
DESCRIPTION: Example of using the Inflection AI provider to generate text with the 'inflection_3_with_tools' model. This demonstrates how to integrate the provider with the AI SDK's generateText function.

LANGUAGE: typescript
CODE:
import { inflection } from 'inflection-ai-sdk-provider';
import { generateText } from 'ai';

const { text } = await generateText({
  model: inflection('inflection_3_with_tools'),
  prompt: 'how can I make quick chicken pho?',
});

----------------------------------------

TITLE: Using Language Models with Provider Registry in TypeScript
DESCRIPTION: This snippet shows how to access language models using the languageModel method on the provider registry, with the provider ID as a prefix for the model ID.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';
import { registry } from './registry';

const { text } = await generateText({
  model: registry.languageModel('openai:gpt-4-turbo'), // default separator
  // or with custom separator:
  // model: customSeparatorRegistry.languageModel('openai > gpt-4-turbo'),
  prompt: 'Invent a new holiday and describe its traditions.',
});

----------------------------------------

TITLE: Configuring Azure OpenAI Image Model with Options
DESCRIPTION: Illustrates how to create an Azure OpenAI image model with custom settings like user identifier and response format.

LANGUAGE: typescript
CODE:
const model = azure.imageModel('your-dalle-deployment-name', {
  user: 'test-user', // optional unique user identifier
  responseFormat: 'url', // 'url' or 'b64_json', defaults to 'url'
});

----------------------------------------

TITLE: Generating Text with Bedrock Provider
DESCRIPTION: Complete example showing how to use the Bedrock provider with the AI SDK to generate text using the Meta Llama model.

LANGUAGE: typescript
CODE:
import { bedrock } from '@ai-sdk/amazon-bedrock';
import { generateText } from 'ai';

const { text } = await generateText({
  model: bedrock('meta.llama3-8b-instruct-v1:0'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Streaming Text with File Prompt using Anthropic's Claude Model in Node.js
DESCRIPTION: This code snippet demonstrates how to stream text responses from Anthropic's Claude model using a file prompt. It reads a PDF file, sends it along with a text question to the model, and streams the response to stdout. The code uses the AI SDK and requires the '@ai-sdk/anthropic' package and a .env file for configuration.

LANGUAGE: typescript
CODE:
import { anthropic } from '@ai-sdk/anthropic';
import { streamText } from 'ai';
import 'dotenv/config';
import fs from 'node:fs';

async function main() {
  const result = streamText({
    model: anthropic('claude-3-5-sonnet-20241022'),
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'text',
            text: 'What is an embedding model according to this document?',
          },
          {
            type: 'file',
            data: fs.readFileSync('./data/ai.pdf'),
            mimeType: 'application/pdf',
          },
        ],
      },
    ],
  });

  for await (const textPart of result.textStream) {
    process.stdout.write(textPart);
  }
}

main().catch(console.error);

----------------------------------------

TITLE: Importing useObject Hook
DESCRIPTION: Shows how to import the experimental useObject hook from the AI SDK React package.

LANGUAGE: typescript
CODE:
import { experimental_useObject as useObject } from '@ai-sdk/react'

----------------------------------------

TITLE: Next.js Server Action Implementation
DESCRIPTION: Complete server action implementation for Next.js, including weather components, loading states, and streamUI integration. Shows full setup for server-side component streaming.

LANGUAGE: tsx
CODE:
'use server';

import { streamUI } from 'ai/rsc';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const LoadingComponent = () => (
  <div className="animate-pulse p-4">getting weather...</div>
);

const getWeather = async (location: string) => {
  await new Promise(resolve => setTimeout(resolve, 2000));
  return '82°F️ ☀️';
};

interface WeatherProps {
  location: string;
  weather: string;
}

const WeatherComponent = (props: WeatherProps) => (
  <div className="border border-neutral-200 p-4 rounded-lg max-w-fit">
    The weather in {props.location} is {props.weather}
  </div>
);

export async function streamComponent() {
  const result = await streamUI({
    model: openai('gpt-4o'),
    prompt: 'Get the weather for San Francisco',
    text: ({ content }) => <div>{content}</div>,
    tools: {
      getWeather: {
        description: 'Get the weather for a location',
        parameters: z.object({
          location: z.string(),
        }),
        generate: async function* ({ location }) {
          yield <LoadingComponent />;
          const weather = await getWeather(location);
          return <WeatherComponent weather={weather} location={location} />;
        },
      },
    },
  });

  return result.value;
}

----------------------------------------

TITLE: Implementing Basic Text Completion UI in React
DESCRIPTION: Basic implementation of text completion functionality using the useCompletion hook. Creates a form with input field and displays streamed completion results.

LANGUAGE: tsx
CODE:
'use client';

import { useCompletion } from '@ai-sdk/react';

export default function Page() {
  const { completion, input, handleInputChange, handleSubmit } = useCompletion({
    api: '/api/completion',
  });

  return (
    <form onSubmit={handleSubmit}>
      <input
        name="prompt"
        value={input}
        onChange={handleInputChange}
        id="input"
      />
      <button type="submit">Submit</button>
      <div>{completion}</div>
    </form>
  );
}

----------------------------------------

TITLE: Rendering Flight Status Component
DESCRIPTION: This component fetches and displays flight status information for a given flight number. It's used as a custom component in the chat interface.

LANGUAGE: tsx
CODE:
export async function Flight({ flightNumber }) {
  const data = await fetch(`https://api.example.com/flight/${flightNumber}`);

  return (
    <div>
      <div>{flightNumber}</div>
      <div>{data.status}</div>
      <div>{data.source}</div>
      <div>{data.destination}</div>
    </div>
  );
}

----------------------------------------

TITLE: Implementing Error Handling for streamText in TypeScript/React
DESCRIPTION: Example showing how to properly implement error handling in the streamText function using an onError callback. This allows capturing and logging errors that would otherwise be silently handled by the stream.

LANGUAGE: tsx
CODE:
import { streamText } from 'ai';

const result = streamText({
  model: yourModel,
  prompt: 'Invent a new holiday and describe its traditions.',
  onError({ error }) {
    console.error(error); // your error logging logic here
  },
});

----------------------------------------

TITLE: Creating a Chat Interface with PDF Upload in Next.js
DESCRIPTION: This React component creates a chat interface that allows uploading PDFs alongside messages. It uses the useChat hook to handle file uploads and message streaming, and displays both text messages and uploaded PDFs or images.

LANGUAGE: tsx
CODE:
'use client';

import { useChat } from '@ai-sdk/react';
import { useRef, useState } from 'react';
import Image from 'next/image';

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat();

  const [files, setFiles] = useState<FileList | undefined>(undefined);
  const fileInputRef = useRef<HTMLInputElement>(null);

  return (
    <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch">
      {messages.map(m => (
        <div key={m.id} className="whitespace-pre-wrap">
          {m.role === 'user' ? 'User: ' : 'AI: '}
          {m.content}
          <div>
            {m?.experimental_attachments
              ?.filter(
                attachment =>
                  attachment?.contentType?.startsWith('image/') ||
                  attachment?.contentType?.startsWith('application/pdf'),
              )
              .map((attachment, index) =>
                attachment.contentType?.startsWith('image/') ? (
                  <Image
                    key={`${m.id}-${index}`}
                    src={attachment.url}
                    width={500}
                    height={500}
                    alt={attachment.name ?? `attachment-${index}`}
                  />
                ) : attachment.contentType?.startsWith('application/pdf') ? (
                  <iframe
                    key={`${m.id}-${index}`}
                    src={attachment.url}
                    width="500"
                    height="600"
                    title={attachment.name ?? `attachment-${index}`}
                  />
                ) : null,
              )}
          </div>
        </div>
      ))}

      <form
        className="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl space-y-2"
        onSubmit={event => {
          handleSubmit(event, {
            experimental_attachments: files,
          });

          setFiles(undefined);

          if (fileInputRef.current) {
            fileInputRef.current.value = '';
          }
        }}
      >
        <input
          type="file"
          className=""
          onChange={event => {
            if (event.target.files) {
              setFiles(event.target.files);
            }
          }}
          multiple
          ref={fileInputRef}
        />
        <input
          className="w-full p-2"
          value={input}
          placeholder="Say something..."
          onChange={handleInputChange}
        />
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Importing FriendliAI Provider
DESCRIPTION: Basic provider import statement for TypeScript integration.

LANGUAGE: typescript
CODE:
import { friendli } from '@friendliai/ai-provider';

----------------------------------------

TITLE: Generating Embeddings with OpenAI Model in TypeScript
DESCRIPTION: This snippet demonstrates how to use the `embed()` function to generate an embedding for a single text value using an OpenAI embedding model. It shows the import statements and a basic usage example.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { embed } from 'ai';

const { embedding } = await embed({
  model: openai.embedding('text-embedding-3-small'),
  value: 'sunny day at the beach',
});

----------------------------------------

TITLE: Implementing Smooth Streaming for Azure OpenAI in TypeScript
DESCRIPTION: This code snippet demonstrates how to use the smoothStream transformation from the 'ai' package to improve streaming performance with Azure OpenAI. It applies the transformation to the streamText function, which helps to stream each word individually.

LANGUAGE: tsx
CODE:
import { smoothStream, streamText } from 'ai';

const result = streamText({
  model,
  prompt,
  experimental_transform: smoothStream(),
});

----------------------------------------

TITLE: Implementing Multiple Streamable UIs with AI SDK in TypeScript React
DESCRIPTION: This code snippet demonstrates how to create and return multiple streamable UIs using the AI SDK's createStreamableUI function. It creates separate streamables for weather and forecast data, updates them asynchronously, and returns them along with other data.

LANGUAGE: tsx
CODE:
'use server';

import { createStreamableUI } from 'ai/rsc';

export async function getWeather() {
  const weatherUI = createStreamableUI();
  const forecastUI = createStreamableUI();

  weatherUI.update(<div>Loading weather...</div>);
  forecastUI.update(<div>Loading forecast...</div>);

  getWeatherData().then(weatherData => {
    weatherUI.done(<div>{weatherData}</div>);
  });

  getForecastData().then(forecastData => {
    forecastUI.done(<div>{forecastData}</div>);
  });

  // Return both streamable UIs and other data fields.
  return {
    requestedAt: Date.now(),
    weather: weatherUI.value,
    forecast: forecastUI.value,
  };
}

----------------------------------------

TITLE: Interactive Chat Interface Implementation
DESCRIPTION: React component implementation showing how to create an interactive chat interface using the AI SDK's useChat hook with Claude 3.7 Sonnet.

LANGUAGE: tsx
CODE:
'use client';

import { useChat } from '@ai-sdk/react';

export default function Page() {
  const { messages, input, handleInputChange, handleSubmit, error } = useChat();

  return (
    <>
      {messages.map(message => (
        <div key={message.id}>
          {message.role === 'user' ? 'User: ' : 'AI: '}
          {message.parts.map((part, index) => {
            // text parts:
            if (part.type === 'text') {
              return <div key={index}>{part.text}</div>;
            }
            // reasoning parts:
            if (part.type === 'reasoning') {
              return (
                <pre key={index}>
                  {part.details.map(detail =>
                    detail.type === 'text' ? detail.text : '<redacted>',
                  )}
                </pre>
              );
            }
          })}
        </div>
      ))}
      <form onSubmit={handleSubmit}>
        <input name="prompt" value={input} onChange={handleInputChange} />
        <button type="submit">Submit</button>
      </form>
    </>
  );
}

----------------------------------------

TITLE: Implementing Client-Side Object Streaming with React
DESCRIPTION: This code snippet shows how to create a React component that triggers object generation and displays the streamed results. It uses the 'readStreamableValue' function to handle the streaming process.

LANGUAGE: tsx
CODE:
'use client';

import { useState } from 'react';
import { generate } from './actions';
import { readStreamableValue } from 'ai/rsc';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export default function Home() {
  const [generation, setGeneration] = useState<string>('');

  return (
    <div>
      <button
        onClick={async () => {
          const { object } = await generate('Messages during finals week.');

          for await (const partialObject of readStreamableValue(object)) {
            if (partialObject) {
              setGeneration(
                JSON.stringify(partialObject.notifications, null, 2),
              );
            }
          }
        }}
      >
        Ask
      </button>

      <pre>{generation}</pre>
    </div>
  );
}

----------------------------------------

TITLE: Implementing Server-Side Custom Data Streaming in TypeScript
DESCRIPTION: Shows how to use createDataStreamResponse and streamText to send custom data and model responses. Demonstrates writing data annotations, message annotations, and handling errors.

LANGUAGE: tsx
CODE:
import { openai } from '@ai-sdk/openai';
import { generateId, createDataStreamResponse, streamText } from 'ai';

export async function POST(req: Request) {
  const { messages } = await req.json();

  return createDataStreamResponse({
    execute: dataStream => {
      dataStream.writeData('initialized call');

      const result = streamText({
        model: openai('gpt-4o'),
        messages,
        onChunk() {
          dataStream.writeMessageAnnotation({ chunk: '123' });
        },
        onFinish() {
          dataStream.writeMessageAnnotation({
            id: generateId(),
            other: 'information',
          });

          dataStream.writeData('call completed');
        },
      });

      result.mergeIntoDataStream(dataStream);
    },
    onError: error => {
      return error instanceof Error ? error.message : String(error);
    },
  });
}

----------------------------------------

TITLE: Defining a Weather Tool with TypeScript and Zod
DESCRIPTION: This snippet demonstrates how to use the 'tool' helper function to define a weather tool. It uses Zod for parameter validation and includes an async execute function that returns simulated weather data.

LANGUAGE: typescript
CODE:
import { tool } from 'ai';
import { z } from 'zod';

export const weatherTool = tool({
  description: 'Get the weather in a location',
  parameters: z.object({
    location: z.string().describe('The location to get the weather for'),
  }),
  // location below is inferred to be a string:
  execute: async ({ location }) => ({
    location,
    temperature: 72 + Math.floor(Math.random() * 21) - 10,
  }),
});

----------------------------------------

TITLE: Configuring Multi-Tool Computer Use in TypeScript
DESCRIPTION: Example of combining multiple Computer Use tools (computer, bash, and text editor) in a single request for complex workflows.

LANGUAGE: typescript
CODE:
const computerTool = anthropic.tools.computer_20241022({
  ...
});

const bashTool = anthropic.tools.bash_20241022({
  execute: async ({ command, restart }) => execSync(command).toString()
});

const textEditorTool = anthropic.tools.textEditor_20241022({
  execute: async ({
    command,
    path,
    file_text,
    insert_line,
    new_str,
    old_str,
    view_range
  }) => {
    // Handle file operations based on command
    switch(command) {
      return executeTextEditorFunction({
        command,
        path,
        fileText: file_text,
        insertLine: insert_line,
        newStr: new_str,
        oldStr: old_str,
        viewRange: view_range
      });
    }
  }
});


const response = await generateText({
  model: anthropic("claude-3-5-sonnet-20241022"),
  prompt: "Create a new file called example.txt, write 'Hello World' to it, and run 'cat example.txt' in the terminal",
  tools: {
    computer: computerTool,
    bash: bashTool
    str_replace_editor: textEditorTool,
  },
});

----------------------------------------

TITLE: Using useObject Hook in React Component
DESCRIPTION: Example of implementing the useObject hook in a React client component. The hook is used to generate and consume streamed JSON objects based on a schema, with a simple button interface to trigger generation.

LANGUAGE: tsx
CODE:
'use client';

import { experimental_useObject as useObject } from '@ai-sdk/react';

export default function Page() {
  const { object, submit } = useObject({
    api: '/api/use-object',
    schema: z.object({ content: z.string() }),
  });

  return (
    <div>
      <button onClick={() => submit('example input')}>Generate</button>
      {object?.content && <p>{object.content}</p>}
    </div>
  );
}

----------------------------------------

TITLE: Handling Slack Events in TypeScript
DESCRIPTION: This code defines a POST function to handle incoming Slack events, including URL verification, app mentions, assistant thread starts, and direct messages. It uses the waitUntil function to handle long-running AI operations.

LANGUAGE: typescript
CODE:
import type { SlackEvent } from '@slack/web-api';
import {
  assistantThreadMessage,
  handleNewAssistantMessage,
} from '../lib/handle-messages';
import { waitUntil } from '@vercel/functions';
import { handleNewAppMention } from '../lib/handle-app-mention';
import { verifyRequest, getBotId } from '../lib/slack-utils';

export async function POST(request: Request) {
  const rawBody = await request.text();
  const payload = JSON.parse(rawBody);
  const requestType = payload.type as 'url_verification' | 'event_callback';

  // See https://api.slack.com/events/url_verification
  if (requestType === 'url_verification') {
    return new Response(payload.challenge, { status: 200 });
  }

  await verifyRequest({ requestType, request, rawBody });

  try {
    const botUserId = await getBotId();

    const event = payload.event as SlackEvent;

    if (event.type === 'app_mention') {
      waitUntil(handleNewAppMention(event, botUserId));
    }

    if (event.type === 'assistant_thread_started') {
      waitUntil(assistantThreadMessage(event));
    }

    if (
      event.type === 'message' &&
      !event.subtype &&
      event.channel_type === 'im' &&
      !event.bot_id &&
      !event.bot_profile &&
      event.bot_id !== botUserId
    ) {
      waitUntil(handleNewAssistantMessage(event, botUserId));
    }

    return new Response('Success!', { status: 200 });
  } catch (error) {
    console.error('Error generating response', error);
    return new Response('Error generating response', { status: 500 });
  }
}

----------------------------------------

TITLE: Configuring AI Instance for Chat Application
DESCRIPTION: This code creates an AI instance with initial state and actions for the chat application using the AI SDK.

LANGUAGE: typescript
CODE:
import { createAI } from 'ai/rsc';
import { ServerMessage, ClientMessage, continueConversation } from './actions';

export const AI = createAI<ServerMessage[], ClientMessage[]>({
  actions: {
    continueConversation,
  },
  initialAIState: [],
  initialUIState: [],
});

----------------------------------------

TITLE: Implementing Error Handler Function for useChat in TSX
DESCRIPTION: A utility function that processes different types of errors and converts them into string messages. Handles null values, string errors, Error objects, and other types by stringifying them.

LANGUAGE: tsx
CODE:
export function errorHandler(error: unknown) {
  if (error == null) {
    return 'unknown error';
  }

  if (typeof error === 'string') {
    return error;
  }

  if (error instanceof Error) {
    return error.message;
  }

  return JSON.stringify(error);
}

----------------------------------------

TITLE: Implementing Server-Side Weather Assistant with Parallel Tool Calls
DESCRIPTION: This server-side code demonstrates how to use the AI SDK to create a weather assistant that can call the getWeather tool for multiple cities in parallel. It defines the tool structure, executes the weather lookup, and formats the response.

LANGUAGE: ts
CODE:
'use server';

import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

export interface Message {
  role: 'user' | 'assistant';
  content: string;
}

function getWeather({ city, unit }) {
  // This function would normally make an
  // API request to get the weather.

  return { value: 25, description: 'Sunny' };
}

export async function continueConversation(history: Message[]) {
  'use server';

  const { text, toolResults } = await generateText({
    model: openai('gpt-3.5-turbo'),
    system: 'You are a friendly weather assistant!',
    messages: history,
    tools: {
      getWeather: {
        description: 'Get the weather for a location',
        parameters: z.object({
          city: z.string().describe('The city to get the weather for'),
          unit: z
            .enum(['C', 'F'])
            .describe('The unit to display the temperature in'),
        }),
        execute: async ({ city, unit }) => {
          const weather = getWeather({ city, unit });
          return `It is currently ${weather.value}°${unit} and ${weather.description} in ${city}!`;
        },
      },
    },
  });

  return {
    messages: [
      ...history,
      {
        role: 'assistant' as const,
        content:
          text || toolResults.map(toolResult => toolResult.result).join('\n'),
      },
    ],
  };
}

----------------------------------------

TITLE: Generating Text with Perplexity Sonar Pro
DESCRIPTION: Example demonstrating how to use the Perplexity provider to generate text using the Sonar Pro model. Shows complete implementation including imports and async/await pattern.

LANGUAGE: typescript
CODE:
import { perplexity } from '@ai-sdk/perplexity';
import { generateText } from 'ai';

const { text } = await generateText({
  model: perplexity('sonar-pro'),
  prompt: 'What are the latest developments in quantum computing?',
});

----------------------------------------

TITLE: Creating Test Errors in API Route Handler
DESCRIPTION: Demonstrates how to inject errors for testing purposes by throwing an error in the API route handler.

LANGUAGE: ts
CODE:
export async function POST(req: Request) {
  throw new Error('This is a test error');
}

----------------------------------------

TITLE: Configuring AI Instance with React Server Components
DESCRIPTION: This TypeScript file sets up the AI instance using the createAI function from the AI SDK. It defines the initial state and actions for the AI, connecting the server-side logic with the client-side UI.

LANGUAGE: typescript
CODE:
import { createAI } from 'ai/rsc';
import { ServerMessage, ClientMessage, continueConversation } from './actions';

export const AI = createAI<ServerMessage[], ClientMessage[]>({
  actions: {
    continueConversation,
  },
  initialAIState: [],
  initialUIState: [],
});

----------------------------------------

TITLE: Handling Regular Errors with try/catch in TypeScript
DESCRIPTION: Demonstrates basic error handling for text generation using try/catch blocks. Shows how to handle errors when using the generateText function from the AI SDK.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';

try {
  const { text } = await generateText({
    model: yourModel,
    prompt: 'Write a vegetarian lasagna recipe for 4 people.',
  });
} catch (error) {
  // handle error
}

----------------------------------------

TITLE: Converting LangChain Expression Language Stream to Data Stream Response
DESCRIPTION: Demonstrates how to use LangChainAdapter to convert a LangChain ChatOpenAI stream into a data stream response. It uses the 'gpt-3.5-turbo-0125' model with zero temperature.

LANGUAGE: tsx
CODE:
import { ChatOpenAI } from '@langchain/openai';
import { LangChainAdapter } from 'ai';

export async function POST(req: Request) {
  const { prompt } = await req.json();

  const model = new ChatOpenAI({
    model: 'gpt-3.5-turbo-0125',
    temperature: 0,
  });

  const stream = await model.stream(prompt);

  return LangChainAdapter.toDataStreamResponse(stream);
}

----------------------------------------

TITLE: Generating Text Embeddings with OpenAI and AI SDK in TypeScript
DESCRIPTION: This code snippet demonstrates how to use the OpenAI API and AI SDK to generate text embeddings. It imports necessary modules, sets up an async function, and uses the 'embed' function to convert a text string into an embedding vector. The result includes both the embedding and usage information.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { embed } from 'ai';
import 'dotenv/config';

async function main() {
  const { embedding, usage } = await embed({
    model: openai.embedding('text-embedding-3-small'),
    value: 'sunny day at the beach',
  });

  console.log(embedding);
  console.log(usage);
}

main().catch(console.error);

----------------------------------------

TITLE: Importing smoothStream Function
DESCRIPTION: Shows how to import the smoothStream function from the ai package.

LANGUAGE: typescript
CODE:
import { smoothStream } from "ai"

----------------------------------------

TITLE: Implementing Chat UI Component
DESCRIPTION: React component for chat interface using the AI SDK's useChat hook

LANGUAGE: typescript
CODE:
'use client';

import { useChat } from '@ai-sdk/react';

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat();
  return (
    <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch">
      {messages.map(message => (
        <div key={message.id} className="whitespace-pre-wrap">
          {message.role === 'user' ? 'User: ' : 'AI: '}
          {message.parts.map((part, i) => {
            switch (part.type) {
              case 'text':
                return <div key={`${message.id}-${i}`}>{part.text}</div>;
            }
          })}
        </div>
      ))}

      <form onSubmit={handleSubmit}>
        <input
          className="fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl"
          value={input}
          placeholder="Say something..."
          onChange={handleInputChange}
        />
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Creating AI Instance with Message Restoration in TypeScript
DESCRIPTION: This snippet shows how to create an AI instance using the AI SDK, including the implementation of onGetUIState for restoring and transforming messages from server to client format.

LANGUAGE: tsx
CODE:
import { createAI } from 'ai/rsc';
import { ServerMessage, ClientMessage, continueConversation } from './actions';
import { Stock } from '@ai-studio/components/stock';
import { generateId } from 'ai';

export const AI = createAI<ServerMessage[], ClientMessage[]>({
  actions: {
    continueConversation,
  },
  onGetUIState: async () => {
    'use server';

    // Get the current AI state (stored messages)
    const history: ServerMessage[] = getAIState();

    // Transform server messages into client messages
    return history.map(({ role, content }) => ({
      id: generateId(),
      role,
      display:
        role === 'function' ? <Stock {...JSON.parse(content)} /> : content,
    }));
  },
});

----------------------------------------

TITLE: Calculating Cosine Similarity between Embeddings in TypeScript
DESCRIPTION: This snippet demonstrates how to use the cosineSimilarity function to compare embeddings of two text strings. It uses the openai and ai libraries to generate embeddings and then calculate their similarity.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { cosineSimilarity, embedMany } from 'ai';

const { embeddings } = await embedMany({
  model: openai.embedding('text-embedding-3-small'),
  values: ['sunny day at the beach', 'rainy afternoon in the city'],
});

console.log(
  `cosine similarity: ${cosineSimilarity(embeddings[0], embeddings[1])}`,
);

----------------------------------------

TITLE: Implementing Text Streaming with OpenAI Chat in TypeScript
DESCRIPTION: Demonstrates how to set up and use text streaming with OpenAI's GPT model using the AI SDK. The code configures a chat conversation with system and user messages, then streams the response using async iteration.

LANGUAGE: typescript
CODE:
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';

const result = streamText({
  model: openai('gpt-3.5-turbo'),
  maxTokens: 1024,
  system: 'You are a helpful chatbot.',
  messages: [
    {
      role: 'user',
      content: 'Hello!',
    },
    {
      role: 'assistant',
      content: 'Hello! How can I help you today?',
    },
    {
      role: 'user',
      content: 'I need help with my computer.',
    },
  ],
});

for await (const textPart of result.textStream) {
  console.log(textPart);
}

----------------------------------------

TITLE: Implementing Loading State and Stream Control in React Component
DESCRIPTION: This enhanced React component adds loading state and the ability to stop the stream. It disables the generation button during loading and provides a stop button.

LANGUAGE: tsx
CODE:
'use client';

import { experimental_useObject as useObject } from '@ai-sdk/react';
import { notificationSchema } from './api/use-object/schema';

export default function Page() {
  const { object, submit, isLoading, stop } = useObject({
    api: '/api/use-object',
    schema: notificationSchema,
  });

  return (
    <div>
      <button
        onClick={() => submit('Messages during finals week.')}
        disabled={isLoading}
      >
        Generate notifications
      </button>

      {isLoading && (
        <div>
          <div>Loading...</div>
          <button type="button" onClick={() => stop()}>
            Stop
          </button>
        </div>
      )}

      {object?.notifications?.map((notification, index) => (
        <div key={index}>
          <p>{notification?.name}</p>
          <p>{notification?.message}</p>
        </div>
      ))}
    </div>
  );
}

----------------------------------------

TITLE: Simulating Streaming with Built-in Middleware
DESCRIPTION: Shows how to use the simulateStreamingMiddleware to simulate streaming behavior with responses from non-streaming language models, maintaining a consistent streaming interface.

LANGUAGE: typescript
CODE:
import { wrapLanguageModel, simulateStreamingMiddleware } from 'ai';

const model = wrapLanguageModel({
  model: yourModel,
  middleware: simulateStreamingMiddleware(),
});

----------------------------------------

TITLE: Custom Error Handling with Message Replacement in React Chat
DESCRIPTION: Shows how to implement custom error handling by replacing the last message when an error occurs. Uses setMessages to manage the message history during error states.

LANGUAGE: tsx
CODE:
'use client';

import { useChat } from '@ai-sdk/react';

export default function Chat() {
  const {
    handleInputChange,
    handleSubmit,
    error,
    input,
    messages,
    setMessages,
  } = useChat({});

  function customSubmit(event: React.FormEvent<HTMLFormElement>) {
    if (error != null) {
      setMessages(messages.slice(0, -1)); // remove last message
    }

    handleSubmit(event);
  }

  return (
    <div>
      {messages.map(m => (
        <div key={m.id}>
          {m.role}: {m.content}
        </div>
      ))}

      {error && <div>An error occurred.</div>}

      <form onSubmit={customSubmit}>
        <input value={input} onChange={handleInputChange} />
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Generating Text with LangDB and AI SDK
DESCRIPTION: Example of using LangDB with the generateText function from the AI SDK to generate text using a specified model.

LANGUAGE: tsx
CODE:
import { createLangDB } from '@langdb/vercel-provider';
import { generateText } from 'ai';

const langdb = createLangDB({
  apiKey: process.env.LANGDB_API_KEY,
  projectId: 'your-project-id',
});

export async function generateTextExample() {
  const { text } = await generateText({
    model: langdb('openai/gpt-4o-mini'),
    prompt: 'Write a Python function that sorts a list:',
  });

  console.log(text);
}

----------------------------------------

TITLE: Streaming Object Generation
DESCRIPTION: Example of using streamObject to handle streaming responses from the model.

LANGUAGE: typescript
CODE:
import { streamObject } from 'ai';

const { partialObjectStream } = streamObject({
  // ...
});

// use partialObjectStream as an async iterable
for await (const partialObject of partialObjectStream) {
  console.log(partialObject);
}

----------------------------------------

TITLE: Document OCR with Mistral - TypeScript
DESCRIPTION: Example of using Mistral's OCR capabilities to process PDF documents with configurable image and page limits.

LANGUAGE: typescript
CODE:
const result = await generateText({
  model: mistral('mistral-small-latest'),
  messages: [
    {
      role: 'user',
      content: [
        {
          type: 'text',
          text: 'What is an embedding model according to this document?',
        },
        {
          type: 'file',
          data: new URL(
            'https://github.com/vercel/ai/blob/main/examples/ai-core/data/ai.pdf?raw=true',
          ),
          mimeType: 'application/pdf',
        },
      ],
    },
  ],
  // optional settings:
  providerOptions: {
    mistral: {
      documentImageLimit: 8,
      documentPageLimit: 64,
    },
  },
});

----------------------------------------

TITLE: Creating Weather Component in React
DESCRIPTION: This snippet defines a React component to display weather information. It takes temperature, weather condition, and location as props.

LANGUAGE: tsx
CODE:
type WeatherProps = {
  temperature: number;
  weather: string;
  location: string;
};

export const Weather = ({ temperature, weather, location }: WeatherProps) => {
  return (
    <div>
      <h2>Current Weather for {location}</h2>
      <p>Condition: {weather}</p>
      <p>Temperature: {temperature}°C</p>
    </div>
  );
};

----------------------------------------

TITLE: Wrapping Application with AI Context in TypeScript React
DESCRIPTION: This code wraps the application with the AI context created earlier, enabling the use of AI-related hooks and actions throughout the app.

LANGUAGE: tsx
CODE:
import { type ReactNode } from 'react';
import { AI } from './ai';

export default function RootLayout({
  children,
}: Readonly<{ children: ReactNode }>) {
  return (
    <AI>
      <html lang="en">
        <body>{children}</body>
      </html>
    </AI>
  );
}

----------------------------------------

TITLE: Implementing Streamable Value with Server Action in TypeScript
DESCRIPTION: Server-side implementation of a streamable value using createStreamableValue to stream thread status updates.

LANGUAGE: typescript
CODE:
'use server';

import { createStreamableValue } from 'ai/rsc';

export const runThread = async () => {
  const streamableStatus = createStreamableValue('thread.init');

  setTimeout(() => {
    streamableStatus.update('thread.run.create');
    streamableStatus.update('thread.run.update');
    streamableStatus.update('thread.run.end');
    streamableStatus.done('thread.end');
  }, 1000);

  return {
    status: streamableStatus.value,
  };
};

----------------------------------------

TITLE: Accessing Token Usage Information in TypeScript
DESCRIPTION: Shows how to access token usage information from the embedding process, which is useful for tracking costs when using providers that charge based on token usage. The 'usage' property in the result object contains this information.

LANGUAGE: ts
CODE:
import { openai } from '@ai-sdk/openai';
import { embed } from 'ai';

const { embedding, usage } = await embed({
  model: openai.embedding('text-embedding-3-small'),
  value: 'sunny day at the beach',
});

console.log(usage); // { tokens: 10 }

----------------------------------------

TITLE: Error Handling in Text Streaming
DESCRIPTION: Shows how to implement error handling in streamText using the onError callback.

LANGUAGE: tsx
CODE:
import { streamText } from 'ai';

const result = streamText({
  model: yourModel,
  prompt: 'Invent a new holiday and describe its traditions.',
  onError({ error }) {
    console.error(error); // your error logging logic here
  },
});

----------------------------------------

TITLE: Calculating Embedding Similarity in TypeScript
DESCRIPTION: Demonstrates how to calculate the cosine similarity between two embeddings using the 'cosineSimilarity' function from the AI SDK. This is useful for finding similar words or phrases in a dataset or ranking related items.

LANGUAGE: ts
CODE:
import { openai } from '@ai-sdk/openai';
import { cosineSimilarity, embedMany } from 'ai';

const { embeddings } = await embedMany({
  model: openai.embedding('text-embedding-3-small'),
  values: ['sunny day at the beach', 'rainy afternoon in the city'],
});

console.log(
  `cosine similarity: ${cosineSimilarity(embeddings[0], embeddings[1])}`,
);

----------------------------------------

TITLE: Generating Structured Data with OpenAI GPT-4.5 and Zod Schema
DESCRIPTION: This code snippet shows how to use the AI SDK Core to generate structured JSON data using OpenAI's GPT-4.5 model. It uses the generateObject function along with a Zod schema to constrain the model's output to a specific structure, in this case, a recipe format.

LANGUAGE: typescript
CODE:
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const { object } = await generateObject({
  model: openai('gpt-4.5-preview'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a lasagna recipe.',
});

----------------------------------------

TITLE: Amazon Bedrock Integration with Claude 3.7
DESCRIPTION: Shows how to use Claude 3.7 Sonnet through Amazon Bedrock instead of direct Anthropic integration, demonstrating the SDK's provider flexibility.

LANGUAGE: typescript
CODE:
import { bedrock } from '@ai-sdk/amazon-bedrock';
import { generateText } from 'ai';

const { reasoning, text } = await generateText({
  model: bedrock('anthropic.claude-3-7-sonnet-20250219-v1:0'),
  prompt: 'How many people will live in the world in 2040?',
});

----------------------------------------

TITLE: Embedding Multiple Values with OpenAI in TypeScript
DESCRIPTION: Shows how to use the 'embedMany' function to embed multiple text values in a single batch using OpenAI's embedding model. This is useful for preparing data for retrieval-augmented generation (RAG) or other batch processing tasks.

LANGUAGE: tsx
CODE:
import { openai } from '@ai-sdk/openai';
import { embedMany } from 'ai';

// 'embeddings' is an array of embedding objects (number[][]).
// It is sorted in the same order as the input values.
const { embeddings } = await embedMany({
  model: openai.embedding('text-embedding-3-small'),
  values: [
    'sunny day at the beach',
    'rainy afternoon in the city',
    'snowy night in the mountains',
  ],
});

----------------------------------------

TITLE: Generating Images with LangDB and AI SDK
DESCRIPTION: Demonstrates how to use LangDB with the experimental_generateImage function from the AI SDK to generate and save an image based on a prompt.

LANGUAGE: tsx
CODE:
import { createLangDB } from '@langdb/vercel-provider';
import { experimental_generateImage as generateImage } from 'ai';
import fs from 'fs';
import path from 'path';

const langdb = createLangDB({
  apiKey: process.env.LANGDB_API_KEY,
  projectId: 'your-project-id',
});

export async function generateImageExample() {
  const { images } = await generateImage({
    model: langdb.image('openai/dall-e-3'),
    prompt: 'A delighted resplendent quetzal mid-flight amidst raindrops',
  });

  const imagePath = path.join(__dirname, 'generated-image.png');
  fs.writeFileSync(imagePath, images[0].uint8Array);
  console.log(`Image saved to: ${imagePath}`);
}

----------------------------------------

TITLE: Generating Images with Multi-Modal Language Models
DESCRIPTION: Demonstrates how to generate images using a language model that supports multi-modal outputs.

LANGUAGE: ts
CODE:
import { google } from '@ai-sdk/google';
import { generateText } from 'ai';

const result = await generateText({
  model: google('gemini-2.0-flash-exp'),
  providerOptions: {
    google: { responseModalities: ['TEXT', 'IMAGE'] },
  },
  prompt: 'Generate an image of a comic cat',
});

for (const file of result.files) {
  if (file.mimeType.startsWith('image/')) {
    // The file object provides multiple data formats:
    // Access images as base64 string, Uint8Array binary data, or check type
    // - file.base64: string (data URL format)
    // - file.uint8Array: Uint8Array (binary data)
    // - file.mimeType: string (e.g. "image/png")
  }
}

----------------------------------------

TITLE: Sending Custom Data with Fastify and AI SDK
DESCRIPTION: Shows how to use createDataStream to send custom data to the client along with the AI-generated content. It demonstrates writing initial data and merging the AI result into the data stream.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { createDataStream, streamText } from 'ai';
import Fastify from 'fastify';

const fastify = Fastify({ logger: true });

fastify.post('/stream-data', async function (request, reply) {
  // immediately start streaming the response
  const dataStream = createDataStream({
    execute: async dataStreamWriter => {
      dataStreamWriter.writeData('initialized call');

      const result = streamText({
        model: openai('gpt-4o'),
        prompt: 'Invent a new holiday and describe its traditions.',
      });

      result.mergeIntoDataStream(dataStreamWriter);
    },
    onError: error => {
      // Error messages are masked by default for security reasons.
      // If you want to expose the error message to the client, you can do so here:
      return error instanceof Error ? error.message : String(error);
    },
  });

  // Mark the response as a v1 data stream:
  reply.header('X-Vercel-AI-Data-Stream', 'v1');
  reply.header('Content-Type', 'text/plain; charset=utf-8');

  return reply.send(dataStream);
});

fastify.listen({ port: 8080 });

----------------------------------------

TITLE: Implementing Raw Text Stream with AI SDK in TypeScript
DESCRIPTION: This code snippet demonstrates how to use the streamText function from the AI Core to send a raw text stream, avoiding the '0:...' output issue. It sets up a POST request handler that processes a prompt and returns a text stream response.

LANGUAGE: tsx
CODE:
export async function POST(req: Request) {
  const { prompt } = await req.json();

  const result = streamText({
    model: openai.completion('gpt-3.5-turbo-instruct'),
    maxTokens: 2000,
    prompt,
  });

  return result.toTextStreamResponse();
}

----------------------------------------

TITLE: Validating AI_InvalidDataContentError in TypeScript
DESCRIPTION: Example showing how to check if an error is an instance of AI_InvalidDataContentError using the isInstance static method from the ai package.

LANGUAGE: typescript
CODE:
import { InvalidDataContentError } from 'ai';

if (InvalidDataContentError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Reading UI State in Client Components
DESCRIPTION: Demonstrates how to access and render UI state in a client-side component using useUIState hook.

LANGUAGE: typescript
CODE:
'use client';

import { useUIState } from 'ai/rsc';

export default function Page() {
  const [messages, setMessages] = useUIState();

  return (
    <ul>
      {messages.map(message => (
        <li key={message.id}>{message.display}</li>
      ))}
    </ul>
  );
}

----------------------------------------

TITLE: Implementing Chat API Endpoint in Next.js
DESCRIPTION: This code snippet demonstrates how to create a server-side API endpoint for chat completions in Next.js. It uses the AI SDK to generate text based on the conversation history and returns the response.

LANGUAGE: typescript
CODE:
import { CoreMessage, generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

export async function POST(req: Request) {
  const { messages }: { messages: CoreMessage[] } = await req.json();

  const { response } = await generateText({
    model: openai('gpt-4'),
    system: 'You are a helpful assistant.',
    messages,
  });

  return Response.json({ messages: response.messages });
}

----------------------------------------

TITLE: Adding Custom Headers to Embedding Request in TypeScript
DESCRIPTION: Demonstrates how to add custom headers to the embedding request using the 'headers' parameter. This can be useful for adding authentication or other custom information to the API request.

LANGUAGE: ts
CODE:
import { openai } from '@ai-sdk/openai';
import { embed } from 'ai';

const { embedding } = await embed({
  model: openai.embedding('text-embedding-3-small'),
  value: 'sunny day at the beach',
  headers: { 'X-Custom-Header': 'custom-value' },
});

----------------------------------------

TITLE: Adding Weather Tool Integration
DESCRIPTION: Extends the chat route handler with a weather tool implementation using the AI SDK's tool functionality and Zod for parameter validation.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText, tool } from 'ai';
import { z } from 'zod';

export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    messages,
    tools: {
      weather: tool({
        description: 'Get the weather in a location (fahrenheit)',
        parameters: z.object({
          location: z.string().describe('The location to get the weather for'),
        }),
        execute: async ({ location }) => {
          const temperature = Math.round(Math.random() * (90 - 32) + 32);
          return {
            location,
            temperature,
          };
        },
      }),
    },
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Adding Weather Tool to Expo API Route
DESCRIPTION: This code enhances the API route by adding a custom weather tool. It demonstrates how to define tool parameters, execute function, and integrate it with the streamText function.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText, tool } from 'ai';
import { z } from 'zod';

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    messages,
    tools: {
      weather: tool({
        description: 'Get the weather in a location (fahrenheit)',
        parameters: z.object({
          location: z.string().describe('The location to get the weather for'),
        }),
        execute: async ({ location }) => {
          const temperature = Math.round(Math.random() * (90 - 32) + 32);
          return {
            location,
            temperature,
          };
        },
      }),
    },
  });

  return result.toDataStreamResponse({
    headers: {
      'Content-Type': 'application/octet-stream',
      'Content-Encoding': 'none',
    },
  });
}

----------------------------------------

TITLE: Generating Text with Mistral Model in TypeScript
DESCRIPTION: Example of using the Mistral provider to generate text with a specific model and prompt. This snippet demonstrates how to use the generateText function from the AI SDK with the Mistral provider.

LANGUAGE: typescript
CODE:
import { mistral } from '@ai-sdk/mistral';
import { generateText } from 'ai';

const { text } = await generateText({
  model: mistral('mistral-large-latest'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Generating Embeddings with LangDB and AI SDK
DESCRIPTION: Shows how to use LangDB with the embed function from the AI SDK to generate embeddings for a given text input.

LANGUAGE: tsx
CODE:
import { createLangDB } from '@langdb/vercel-provider';
import { embed } from 'ai';

const langdb = createLangDB({
  apiKey: process.env.LANGDB_API_KEY,
  projectId: 'your-project-id',
});

export async function generateEmbeddings() {
  const { embedding } = await embed({
    model: langdb.textEmbeddingModel('text-embedding-3-small'),
    value: 'sunny day at the beach',
  });

  console.log('Embedding:', embedding);
}

----------------------------------------

TITLE: Using File Inputs with Amazon Bedrock
DESCRIPTION: Example of using file inputs with an Amazon Bedrock model.

LANGUAGE: typescript
CODE:
import { bedrock } from '@ai-sdk/amazon-bedrock';
import { generateText } from 'ai';

const result = await generateText({
  model: bedrock('anthropic.claude-3-haiku-20240307-v1:0'),
  messages: [
    {
      role: 'user',
      content: [
        { type: 'text', text: 'Describe the pdf in detail.' },
        {
          type: 'file',
          data: fs.readFileSync('./data/ai.pdf'),
          mimeType: 'application/pdf',
        },
      ],
    },
  ],
});

----------------------------------------

TITLE: Implementing Data Stream with Fastify and AI SDK
DESCRIPTION: Demonstrates how to use the toDataStream method to create a data stream from the AI result and send it as a response in a Fastify server. It uses OpenAI's GPT-4 model to generate text based on a prompt.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import Fastify from 'fastify';

const fastify = Fastify({ logger: true });

fastify.post('/', async function (request, reply) {
  const result = streamText({
    model: openai('gpt-4o'),
    prompt: 'Invent a new holiday and describe its traditions.',
  });

  // Mark the response as a v1 data stream:
  reply.header('X-Vercel-AI-Data-Stream', 'v1');
  reply.header('Content-Type', 'text/plain; charset=utf-8');

  return reply.send(result.toDataStream({ data }));
});

fastify.listen({ port: 8080 });

----------------------------------------

TITLE: Custom Data Streaming with Hono and OpenAI
DESCRIPTION: Implementation showing how to send custom data streams using createDataStream with error handling and OpenAI integration.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { serve } from '@hono/node-server';
import { createDataStream, streamText } from 'ai';
import { Hono } from 'hono';
import { stream } from 'hono/streaming';

const app = new Hono();

app.post('/stream-data', async c => {
  // immediately start streaming the response
  const dataStream = createDataStream({
    execute: async dataStreamWriter => {
      dataStreamWriter.writeData('initialized call');

      const result = streamText({
        model: openai('gpt-4o'),
        prompt: 'Invent a new holiday and describe its traditions.',
      });

      result.mergeIntoDataStream(dataStreamWriter);
    },
    onError: error => {
      // Error messages are masked by default for security reasons.
      // If you want to expose the error message to the client, you can do so here:
      return error instanceof Error ? error.message : String(error);
    },
  });

  // Mark the response as a v1 data stream:
  c.header('X-Vercel-AI-Data-Stream', 'v1');
  c.header('Content-Type', 'text/plain; charset=utf-8');

  return stream(c, stream =>
    stream.pipe(dataStream.pipeThrough(new TextEncoderStream())),
  );
});

serve({ fetch: app.fetch, port: 8080 });

----------------------------------------

TITLE: Handling Date Objects with Zod Schema in TypeScript
DESCRIPTION: Demonstrates how to handle date conversions using Zod schema with AI SDK Core. The code shows the implementation of date string validation and transformation into JavaScript Date objects within a generateObject function.

LANGUAGE: typescript
CODE:
const result = await generateObject({
  model: openai('gpt-4-turbo'),
  schema: z.object({
    events: z.array(
      z.object({
        event: z.string(),
        date: z
          .string()
          .date()
          .transform(value => new Date(value)),
      }),
    ),
  }),
  prompt: 'List 5 important events from the year 2000.'
});

----------------------------------------

TITLE: Checking AI_InvalidPromptError Instance in TypeScript
DESCRIPTION: Demonstrates how to check if an error is an instance of AI_InvalidPromptError using the isInstance static method. This code snippet shows the type-safe way to handle invalid prompt errors in an AI application.

LANGUAGE: typescript
CODE:
import { InvalidPromptError } from 'ai';

if (InvalidPromptError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Using Spark Provider for Text Generation in TypeScript
DESCRIPTION: Example of using the Spark provider to generate text with the 'lite' model. This snippet demonstrates creating a provider instance, setting up the model, and generating text based on a prompt.

LANGUAGE: typescript
CODE:
import { createSparkProvider } from './index.mjs';
import { generateText } from 'ai';
const spark = createSparkProvider({
  apiKey: '',
});
const { text } = await generateText({
  model: spark('lite'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Using Object Promise for Stream Completion in TypeScript
DESCRIPTION: Shows how to use the object Promise to handle the final streamed object. Includes error handling for type validation failures and demonstrates proper stream consumption with backpressure handling.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamObject } from 'ai';
import { z } from 'zod';

const result = streamObject({
  model: openai('gpt-4-turbo'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.string()),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a lasagna recipe.',
});

result.object
  .then(({ recipe }) => {
    // do something with the fully typed, final object:
    console.log('Recipe:', JSON.stringify(recipe, null, 2));
  })
  .catch(error => {
    // handle type validation failure
    // (when the object does not match the schema):
    console.error(error);
  });

// note: the stream needs to be consumed because of backpressure
for await (const partialObject of result.partialObjectStream) {
}

----------------------------------------

TITLE: Generating Text from Image File Buffer with OpenAI GPT-4
DESCRIPTION: Example demonstrating how to generate text responses from a local image file using the AI SDK with OpenAI's GPT-4 Turbo model. The code shows reading an image file as a buffer and sending it along with a text question for analysis.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';
import fs from 'fs';

const result = await generateText({
  model: openai('gpt-4-turbo'),
  maxTokens: 512,
  messages: [
    {
      role: 'user',
      content: [
        {
          type: 'text',
          text: 'what are the red things in this image?',
        },
        {
          type: 'image',
          image: fs.readFileSync('./node/attachments/eclipse.jpg'),
        },
      ],
    },
  ],
});

console.log(result);

----------------------------------------

TITLE: Testing Fastify Server with cURL
DESCRIPTION: Shows how to test the Fastify server using cURL by sending a POST request to localhost:8080.

LANGUAGE: bash
CODE:
curl -X POST http://localhost:8080

----------------------------------------

TITLE: Import Statement
DESCRIPTION: Shows how to import the generateText function from the ai package.

LANGUAGE: typescript
CODE:
import { generateText } from "ai"

----------------------------------------

TITLE: Implementing Chat UI Component
DESCRIPTION: Creates a React component for the chat interface using the useChat hook from AI SDK, displaying messages and handling user input.

LANGUAGE: typescript
CODE:
import { useChat } from '@ai-sdk/react';

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat();
  return (
    <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch">
      {messages.map(message => (
        <div key={message.id} className="whitespace-pre-wrap">
          {message.role === 'user' ? 'User: ' : 'AI: '}
          {message.parts.map((part, i) => {
            switch (part.type) {
              case 'text':
                return <div key={`${message.id}-${i}`}>{part.text}</div>;
            }
          })}
        </div>
      ))}

      <form onSubmit={handleSubmit}>
        <input
          className="fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl"
          value={input}
          placeholder="Say something..."
          onChange={handleInputChange}
        />
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Implementing Chat UI Component in Svelte
DESCRIPTION: Basic chat interface implementation using the AI SDK's Chat class in Svelte

LANGUAGE: svelte
CODE:
<script>
  import { Chat } from '@ai-sdk/svelte';

  const chat = new Chat();
</script>

<main>
  <ul>
    {#each chat.messages as message}
      <li>{message.role}: {message.content}</li>
    {/each}
  </ul>
  <form onsubmit={chat.handleSubmit}>
    <input bind:value={chat.input} />
    <button type="submit">Send</button>
  </form>
</main>

----------------------------------------

TITLE: Accessing Language Models from Provider Registry in TypeScript
DESCRIPTION: This snippet demonstrates how to access language models from the registry using the languageModel method. It shows the format for specifying the provider and model ID.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';
import { registry } from './registry';

const { text } = await generateText({
  model: registry.languageModel('openai:gpt-4-turbo'),
  prompt: 'Invent a new holiday and describe its traditions.',
});

----------------------------------------

TITLE: Setting Image Detail for OpenAI Vision Models
DESCRIPTION: Example of setting the image detail level for OpenAI vision models.

LANGUAGE: typescript
CODE:
const result = await generateText({
  model: openai('gpt-4o'),
  messages: [
    {
      role: 'user',
      content: [
        { type: 'text', text: 'Describe the image in detail.' },
        {
          type: 'image',
          image:
            'https://github.com/vercel/ai/blob/main/examples/ai-core/data/comic-cat.png?raw=true',

          // OpenAI specific options - image detail:
          providerOptions: {
            openai: { imageDetail: 'low' },
          },
        },
      ],
    },
  ],
});

----------------------------------------

TITLE: Structured Data Generation with o3-mini
DESCRIPTION: Demonstrates how to generate structured JSON data using o3-mini with schema validation using Zod.

LANGUAGE: tsx
CODE:
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

const { object } = await generateObject({
  model: openai('o3-mini'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a lasagna recipe.',
});

----------------------------------------

TITLE: Generating Text with Chrome AI - JavaScript
DESCRIPTION: Examples of using Chrome AI for text generation using both generateText and streamText functions. Shows how to get both immediate and streamed responses.

LANGUAGE: javascript
CODE:
import { generateText } from 'ai';
import { chromeai } from 'chrome-ai';

const { text } = await generateText({
  model: chromeai(),
  prompt: 'Who are you?',
});

console.log(text); //  I am a large language model, trained by Google.

LANGUAGE: javascript
CODE:
import { streamText } from 'ai';
import { chromeai } from 'chrome-ai';

const { textStream } = streamText({
  model: chromeai(),
  prompt: 'Who are you?',
});

let result = '';
for await (const textPart of textStream) {
  result = textPart;
}

console.log(result);
//  I am a large language model, trained by Google.

----------------------------------------

TITLE: Implementing Data Stream in NestJS Controller
DESCRIPTION: Shows how to use pipeDataStreamToResponse to stream AI-generated content to the client. Uses OpenAI's GPT-4 model to generate holiday descriptions.

LANGUAGE: typescript
CODE:
import { Controller, Post, Res } from '@nestjs/common';
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import { Response } from 'express';

@Controller()
export class AppController {
  @Post()
  async example(@Res() res: Response) {
    const result = streamText({
      model: openai('gpt-4o'),
      prompt: 'Invent a new holiday and describe its traditions.',
    });

    result.pipeDataStreamToResponse(res);
  }
}

----------------------------------------

TITLE: Generating Weather JSON with OpenAI Model in TypeScript
DESCRIPTION: This code snippet shows how to modify the previous example to return a JSON object instead of text. The weather information is structured as an object, which can be used to render a React component.

LANGUAGE: tsx
CODE:
const text = generateText({
  model: openai('gpt-3.5-turbo'),
  system: 'You are a friendly assistant',
  prompt: 'What is the weather in SF?',
  tools: {
    getWeather: {
      description: 'Get the weather for a location',
      parameters: z.object({
        city: z.string().describe('The city to get the weather for'),
        unit: z
          .enum(['C', 'F'])
          .describe('The unit to display the temperature in'),
      }),
      execute: async ({ city, unit }) => {
        const weather = getWeather({ city, unit });
        const { temperature, unit, description, forecast } = weather;

        return {
          temperature,
          unit,
          description,
          forecast,
        };
      },
    },
  },
});

----------------------------------------

TITLE: Using Model-Specific Options for Image Generation
DESCRIPTION: Shows how to pass model-specific parameters for image generation using the DeepInfra provider.

LANGUAGE: typescript
CODE:
import { deepinfra } from '@ai-sdk/deepinfra';
import { experimental_generateImage as generateImage } from 'ai';

const { image } = await generateImage({
  model: deepinfra.image('stabilityai/sd3.5'),
  prompt: 'A futuristic cityscape at sunset',
  aspectRatio: '16:9',
  providerOptions: {
    deepinfra: {
      num_inference_steps: 30, // Control the number of denoising steps (1-50)
    },
  },
});

----------------------------------------

TITLE: Using Amazon Bedrock Language Model
DESCRIPTION: Example of using an Amazon Bedrock language model to generate text.

LANGUAGE: typescript
CODE:
import { bedrock } from '@ai-sdk/amazon-bedrock';
import { generateText } from 'ai';

const { text } = await generateText({
  model: bedrock('meta.llama3-70b-instruct-v1:0'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Installing OpenAI Provider Package
DESCRIPTION: Installation command for the OpenAI provider package using npm.

LANGUAGE: shell
CODE:
npm install @ai-sdk/openai

----------------------------------------

TITLE: Basic Usage of simulateReadableStream in TypeScript
DESCRIPTION: Shows a basic example of creating a ReadableStream with simulateReadableStream without specifying delays.

LANGUAGE: typescript
CODE:
const stream = simulateReadableStream({
  chunks: ['Hello', ' ', 'World'],
});

----------------------------------------

TITLE: Importing useChat Hook for Solid
DESCRIPTION: Demonstrates the import statement for utilizing the useChat hook in a Solid application.

LANGUAGE: javascript
CODE:
import { useChat } from '@ai-sdk/solid'

----------------------------------------

TITLE: Implementing Custom Metadata Extraction for OpenAI Compatible Provider
DESCRIPTION: Shows how to implement a metadata extractor to capture provider-specific information from both streaming and non-streaming responses.

LANGUAGE: typescript
CODE:
const myMetadataExtractor: MetadataExtractor = {
  extractMetadata: ({ parsedBody }) => {
    return {
      myProvider: {
        standardUsage: parsedBody.usage,
        experimentalFeatures: parsedBody.beta_features,
        customMetrics: {
          processingTime: parsedBody.server_timing?.total_ms,
          modelVersion: parsedBody.model_version,
        },
      },
    };
  },

  createStreamExtractor: () => {
    let accumulatedData = {
      timing: [],
      customFields: {},
    };

    return {
      processChunk: parsedChunk => {
        if (parsedChunk.server_timing) {
          accumulatedData.timing.push(parsedChunk.server_timing);
        }
        if (parsedChunk.custom_data) {
          Object.assign(accumulatedData.customFields, parsedChunk.custom_data);
        }
      },
      buildMetadata: () => ({
        myProvider: {
          streamTiming: accumulatedData.timing,
          customData: accumulatedData.customFields,
        },
      }),
    };
  },
};

----------------------------------------

TITLE: Reinitializing Git Repository
DESCRIPTION: Commands for removing existing git configuration and creating a fresh repository when encountering 'remote origin already exists' error.

LANGUAGE: bash
CODE:
rm -rf .git
git init
git add .
git commit -m "init"

----------------------------------------

TITLE: Generating Multiple Images with OpenAI's DALL-E 2
DESCRIPTION: Demonstrates how to generate multiple images in a single request using DALL-E 2.

LANGUAGE: tsx
CODE:
import { experimental_generateImage as generateImage } from 'ai';
import { openai } from '@ai-sdk/openai';

const { images } = await generateImage({
  model: openai.image('dall-e-2'),
  prompt: 'Santa Claus driving a Cadillac',
  n: 4, // number of images to generate
});

----------------------------------------

TITLE: Creating Baseten Language Model
DESCRIPTION: Shows how to create a Baseten language model using the provider instance. The model name is passed as an argument to the provider function.

LANGUAGE: typescript
CODE:
const model = baseten('ultravox');

----------------------------------------

TITLE: Text Stream Implementation in NestJS Controller
DESCRIPTION: Shows how to use pipeTextStreamToResponse for direct text streaming from the AI model to the client response.

LANGUAGE: typescript
CODE:
import { Controller, Post, Res } from '@nestjs/common';
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import { Response } from 'express';

@Controller()
export class AppController {
  @Post()
  async example(@Res() res: Response) {
    const result = streamText({
      model: openai('gpt-4o'),
      prompt: 'Invent a new holiday and describe its traditions.',
    });

    result.pipeTextStreamToResponse(res);
  }
}

----------------------------------------

TITLE: Using Reasoning with Amazon Bedrock
DESCRIPTION: Example of using reasoning capabilities with an Amazon Bedrock model.

LANGUAGE: typescript
CODE:
import { bedrock } from '@ai-sdk/amazon-bedrock';
import { generateText } from 'ai';

const { text, reasoning, reasoningDetails } = await generateText({
  model: bedrock('us.anthropic.claude-3-7-sonnet-20250219-v1:0'),
  prompt: 'How many people will live in the world in 2040?',
  providerOptions: {
    bedrock: {
      reasoningConfig: { type: 'enabled', budgetTokens: 1024 },
    },
  },
});

console.log(reasoning); // reasoning text
console.log(reasoningDetails); // reasoning details including redacted reasoning
console.log(text); // text response

----------------------------------------

TITLE: Implementing Client-Side Text Generation Streaming with useCompletion Hook in React
DESCRIPTION: This code snippet shows how to create a React component that uses the useCompletion hook from the @ai-sdk/react module to stream text generation. It sets up a button to trigger the text generation process and displays the streamed completion.

LANGUAGE: tsx
CODE:
'use client';

import { useCompletion } from '@ai-sdk/react';

export default function Page() {
  const { completion, complete } = useCompletion({
    api: '/api/completion',
  });

  return (
    <div>
      <div
        onClick={async () => {
          await complete('Why is the sky blue?');
        }}
      >
        Generate
      </div>

      {completion}
    </div>
  );
}

----------------------------------------

TITLE: Wrapping AI SDK Models with Braintrust Logger
DESCRIPTION: Shows how to wrap AI SDK models using Braintrust to automatically log requests, responses, and metrics. This example uses the openai chat model.

LANGUAGE: typescript
CODE:
import { initLogger, wrapAISDKModel } from 'braintrust';
import { openai } from '@ai-sdk/openai';

const logger = initLogger({
  projectName: 'My Project',
  apiKey: process.env.BRAINTRUST_API_KEY,
});

const model = wrapAISDKModel(openai.chat('gpt-3.5-turbo'));

async function main() {
  // This will automatically log the request, response, and metrics to Braintrust
  const response = await model.doGenerate({
    inputFormat: 'messages',
    mode: {
      type: 'regular',
    },
    prompt: [
      {
        role: 'user',
        content: [{ type: 'text', text: 'What is the capital of France?' }],
      },
    ],
  });
  console.log(response);
}

main();

----------------------------------------

TITLE: Defining FilePart Interface in TypeScript
DESCRIPTION: Defines the structure of a file part in a user message. It includes fields for the file data (which can be a base64 string, Uint8Array, ArrayBuffer, Buffer, or URL) and the MIME type of the file.

LANGUAGE: typescript
CODE:
export interface FilePart {
  type: 'file';

  /**
   * File data. Can either be:
   * - data: a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer
   * - URL: a URL that points to the file
   */
  data: DataContent | URL;

  /**
   * Mime type of the file.
   */
  mimeType: string;
}

----------------------------------------

TITLE: Generating and Saving Images with Fireworks Image Model in TypeScript
DESCRIPTION: Example of using the Fireworks provider to generate an image with a specific model and prompt, then saving it to a file. It uses the experimental_generateImage function and Node.js fs module.

LANGUAGE: typescript
CODE:
import { fireworks } from '@ai-sdk/fireworks';
import { experimental_generateImage as generateImage } from 'ai';
import fs from 'fs';

const { image } = await generateImage({
  model: fireworks.image('accounts/fireworks/models/flux-1-dev-fp8'),
  prompt: 'A serene mountain landscape at sunset',
});
const filename = `image-${Date.now()}.png`;
fs.writeFileSync(filename, image.uint8Array);
console.log(`Image saved to ${filename}`);

----------------------------------------

TITLE: Implementing Client-Side Text Streaming with React and AI SDK
DESCRIPTION: This React component demonstrates how to trigger text generation and consume the streamed response on the client side. It uses the `readStreamableValue` function from the `ai/rsc` module to handle the stream.

LANGUAGE: tsx
CODE:
'use client';

import { useState } from 'react';
import { generate } from './actions';
import { readStreamableValue } from 'ai/rsc';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export default function Home() {
  const [generation, setGeneration] = useState<string>('');

  return (
    <div>
      <button
        onClick={async () => {
          const { output } = await generate('Why is the sky blue?');

          for await (const delta of readStreamableValue(output)) {
            setGeneration(currentGeneration => `${currentGeneration}${delta}`);
          }
        }}
      >
        Ask
      </button>

      <div>{generation}</div>
    </div>
  );
}

----------------------------------------

TITLE: Defining CoreUserMessage and UserContent Types in TypeScript
DESCRIPTION: Defines the structure of a user message that can contain text or a combination of text, images, and files. It includes a 'role' field set to 'user' and a 'content' field of type UserContent.

LANGUAGE: typescript
CODE:
type CoreUserMessage = {
  role: 'user';
  content: UserContent;
};

type UserContent = string | Array<TextPart | ImagePart | FilePart>;

----------------------------------------

TITLE: Importing useCompletion in Solid
DESCRIPTION: How to import the useCompletion hook in a Solid application from the AI SDK

LANGUAGE: javascript
CODE:
import { useCompletion } from '@ai-sdk/solid'

----------------------------------------

TITLE: Adding Weather Tool to Chat API
DESCRIPTION: Enhanced API endpoint with weather tool integration for providing weather information

LANGUAGE: typescript
CODE:
import { createOpenAI } from '@ai-sdk/openai';
import { streamText, tool } from 'ai';
import { z } from 'zod';

import { OPENAI_API_KEY } from '$env/static/private';

const openai = createOpenAI({
  apiKey: OPENAI_API_KEY,
});

export async function POST({ request }) {
  const { messages } = await request.json();

  const result = streamText({
    model: openai('gpt-4o'),
    messages,
    tools: {
      weather: tool({
        description: 'Get the weather in a location (fahrenheit)',
        parameters: z.object({
          location: z.string().describe('The location to get the weather for'),
        }),
        execute: async ({ location }) => {
          const temperature = Math.round(Math.random() * (90 - 32) + 32);
          return {
            location,
            temperature,
          };
        },
      }),
      convertFahrenheitToCelsius: tool({
        description: 'Convert a temperature in fahrenheit to celsius',
        parameters: z.object({
          temperature: z.number().describe('The temperature in fahrenheit to convert'),
        }),
        execute: async ({ temperature }) => {
          const celsius = Math.round((temperature - 32) * (5 / 9));
          return {
            celsius,
          };
        },
      }),
    },
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Setting Aspect Ratio with Google Vertex AI
DESCRIPTION: Shows how to specify an aspect ratio when generating an image using Google Vertex AI's Imagen model.

LANGUAGE: tsx
CODE:
import { experimental_generateImage as generateImage } from 'ai';
import { vertex } from '@ai-sdk/google-vertex';

const { image } = await generateImage({
  model: vertex.image('imagen-3.0-generate-001'),
  prompt: 'Santa Claus driving a Cadillac',
  aspectRatio: '16:9',
});

----------------------------------------

TITLE: Implementing Server Actions with Token Usage Tracking
DESCRIPTION: Server-side implementation that handles conversation processing and token usage tracking. Uses streamUI from the AI SDK with onFinish callback to record token usage metrics.

LANGUAGE: tsx
CODE:
'use server';

import { createAI, getMutableAIState, streamUI } from 'ai/rsc';
import { openai } from '@ai-sdk/openai';
import { ReactNode } from 'react';
import { z } from 'zod';
import { generateId } from 'ai';

export interface ServerMessage {
  role: 'user' | 'assistant';
  content: string;
}

export interface ClientMessage {
  id: string;
  role: 'user' | 'assistant';
  display: ReactNode;
}

export async function continueConversation(
  input: string,
): Promise<ClientMessage> {
  'use server';

  const history = getMutableAIState();

  const result = await streamUI({
    model: openai('gpt-3.5-turbo'),
    messages: [...history.get(), { role: 'user', content: input }],
    text: ({ content, done }) => {
      if (done) {
        history.done((messages: ServerMessage[]) => [
          ...messages,
          { role: 'assistant', content },
        ]);
      }

      return <div>{content}</div>;
    },
    tools: {
      deploy: {
        description: 'Deploy repository to vercel',
        parameters: z.object({
          repositoryName: z
            .string()
            .describe('The name of the repository, example: vercel/ai-chatbot'),
        }),
        generate: async function* ({ repositoryName }) {
          yield <div>Cloning repository {repositoryName}...</div>;
          await new Promise(resolve => setTimeout(resolve, 3000));
          yield <div>Building repository {repositoryName}...</div>;
          await new Promise(resolve => setTimeout(resolve, 2000));
          return <div>{repositoryName} deployed!</div>;
        },
      },
    },
    onFinish: ({ usage }) => {
      const { promptTokens, completionTokens, totalTokens } = usage;
      // your own logic, e.g. for saving the chat history or recording usage
      console.log('Prompt tokens:', promptTokens);
      console.log('Completion tokens:', completionTokens);
      console.log('Total tokens:', totalTokens);
    },
  });

  return {
    id: generateId(),
    role: 'assistant',
    display: result.value,
  };
}

----------------------------------------

TITLE: Basic Chat Setup with RSC - Server Action
DESCRIPTION: Server action implementation using streamUI for chat completion in RSC

LANGUAGE: tsx
CODE:
import { openai } from '@ai-sdk/openai';
import { getMutableAIState, streamUI } from 'ai/rsc';

export async function sendMessage(message: string) {
  'use server';

  const messages = getMutableAIState('messages');

  messages.update([...messages.get(), { role: 'user', content: message }]);

  const { value: stream } = await streamUI({
    model: openai('gpt-4o'),
    system: 'you are a friendly assistant!',
    messages: messages.get(),
    text: async function* ({ content, done }) {
      // process text
    },
    tools: {
      // tool definitions
    },
  });

  return stream;
}

----------------------------------------

TITLE: Generating Text with Groq Models
DESCRIPTION: Example of generating text using a Groq language model.

LANGUAGE: typescript
CODE:
import { groq } from '@ai-sdk/groq';
import { generateText } from 'ai';

const { text } = await generateText({
  model: groq('gemma2-9b-it'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Creating Recursive Zod Schema with zodSchema in TypeScript
DESCRIPTION: This example demonstrates how to create a recursive Zod schema for a category structure with subcategories, using the zodSchema helper function with the useReferences option enabled for recursive support.

LANGUAGE: typescript
CODE:
import { zodSchema } from 'ai';
import { z } from 'zod';

// Define a base category schema
const baseCategorySchema = z.object({
  name: z.string(),
});

// Define the recursive Category type
type Category = z.infer<typeof baseCategorySchema> & {
  subcategories: Category[];
};

// Create the recursive schema using z.lazy
const categorySchema: z.ZodType<Category> = baseCategorySchema.extend({
  subcategories: z.lazy(() => categorySchema.array()),
});

// Create the final schema with useReferences enabled for recursive support
const mySchema = zodSchema(
  z.object({
    category: categorySchema,
  }),
  { useReferences: true },
);

----------------------------------------

TITLE: Defining Tool Schema in JSON
DESCRIPTION: JSON schema definition for a celsius to fahrenheit conversion tool that can be used with OpenAI Assistant.

LANGUAGE: json
CODE:
{
  "name": "celsiusToFahrenheit",
  "description": "convert celsius to fahrenheit.",
  "parameters": {
    "type": "object",
    "properties": {
      "value": {
        "type": "number",
        "description": "the value in celsius."
      }
    },
    "required": ["value"]
  }
}

----------------------------------------

TITLE: Converting Messages in API Route Handler (TypeScript)
DESCRIPTION: Example showing how to use convertToCoreMessages in an API route to transform useChat messages for streamText function usage.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { convertToCoreMessages, streamText } from 'ai';

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    messages: convertToCoreMessages(messages),
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Generating Text with Cloudflare Workers AI
DESCRIPTION: Example of using the generateText function with a Cloudflare Workers AI language model to generate a short essay.

LANGUAGE: typescript
CODE:
import { createWorkersAI } from 'workers-ai-provider';
import { generateText } from 'ai';

type Env = {
  AI: Ai;
};

export default {
  async fetch(_: Request, env: Env) {
    const workersai = createWorkersAI({ binding: env.AI });
    const result = await generateText({
      model: workersai('@cf/meta/llama-2-7b-chat-int8'),
      prompt: 'Write a 50-word essay about hello world.',
    });

    return new Response(result.text);
  },
};

----------------------------------------

TITLE: Streaming Text with Portkey and AI SDK in JavaScript
DESCRIPTION: Shows how to use the Portkey completion model with the streamText function from the AI SDK to stream generated text based on a prompt.

LANGUAGE: javascript
CODE:
import { createPortkey } from '@portkey-ai/vercel-provider';
import { streamText } from 'ai';

const portkey = createPortkey({
  apiKey: 'YOUR_PORTKEY_API_KEY',
  config: portkeyConfig,
});

const result = streamText({
  model: portkey.completionModel(''),
  prompt: 'Invent a new holiday and describe its traditions.',
});

for await (const chunk of result) {
  console.log(chunk);
}

----------------------------------------

TITLE: Generating Images with fal Provider
DESCRIPTION: Complete example of generating an image using the fal provider and saving it to a file. Uses the flux/schnell model.

LANGUAGE: typescript
CODE:
import { fal } from '@ai-sdk/fal';
import { experimental_generateImage as generateImage } from 'ai';
import fs from 'fs';
const { image } = await generateImage({
  model: fal.image('fal-ai/flux/schnell'),
  prompt: 'A cat wearing a intricate robe',
});

const filename = `image-${Date.now()}.png`;
fs.writeFileSync(filename, image.uint8Array);
console.log(`Image saved to ${filename}`);

----------------------------------------

TITLE: Importing GoogleGenerativeAIStream in React
DESCRIPTION: This snippet shows how to import the GoogleGenerativeAIStream function from the 'ai' package in a React application.

LANGUAGE: javascript
CODE:
import { GoogleGenerativeAIStream } from "ai"

----------------------------------------

TITLE: Importing wrapLanguageModel Function
DESCRIPTION: Simple import statement showing how to import the wrapLanguageModel function from the ai package.

LANGUAGE: typescript
CODE:
import { wrapLanguageModel } from "ai"

----------------------------------------

TITLE: Creating Chat API Endpoint with OpenAI Integration
DESCRIPTION: SvelteKit server endpoint implementation for handling chat requests using OpenAI and the AI SDK

LANGUAGE: typescript
CODE:
import { createOpenAI } from '@ai-sdk/openai';
import { streamText } from 'ai';

import { OPENAI_API_KEY } from '$env/static/private';

const openai = createOpenAI({
  apiKey: OPENAI_API_KEY,
});

export async function POST({ request }) {
  const { messages } = await request.json();

  const result = streamText({
    model: openai('gpt-4o'),
    messages,
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Enhanced Chatbot with Weather Tool
DESCRIPTION: Advanced implementation adding a weather tool to demonstrate tool integration capabilities.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { CoreMessage, streamText, tool } from 'ai';
import dotenv from 'dotenv';
import { z } from 'zod';
import * as readline from 'node:readline/promises';

dotenv.config();

const terminal = readline.createInterface({
  input: process.stdin,
  output: process.stdout,
});

const messages: CoreMessage[] = [];

async function main() {
  while (true) {
    const userInput = await terminal.question('You: ');

    messages.push({ role: 'user', content: userInput });

    const result = streamText({
      model: openai('gpt-4o'),
      messages,
      tools: {
        weather: tool({
          description: 'Get the weather in a location (in Celsius)',
          parameters: z.object({
            location: z
              .string()
              .describe('The location to get the weather for'),
          }),
          execute: async ({ location }) => ({
            location,
            temperature: Math.round((Math.random() * 30 + 5) * 10) / 10,
          }),
        }),
        convertCelsiusToFahrenheit: tool({
          description: 'Convert a temperature from Celsius to Fahrenheit',
          parameters: z.object({
            celsius: z
              .number()
              .describe('The temperature in Celsius to convert'),
          }),
          execute: async ({ celsius }) => {
            const fahrenheit = (celsius * 9) / 5 + 32;
            return { fahrenheit: Math.round(fahrenheit * 100) / 100 };
          },
        }),
      },
      maxSteps: 5,
      onStepFinish: step => {
        console.log(JSON.stringify(step, null, 2));
      },
    });

    let fullResponse = '';
    process.stdout.write('\nAssistant: ');
    for await (const delta of result.textStream) {
      fullResponse += delta;
      process.stdout.write(delta);
    }
    process.stdout.write('\n\n');

    messages.push({ role: 'assistant', content: fullResponse });
  }
}

main().catch(console.error);

----------------------------------------

TITLE: Enhancing Azure OpenAI Model with Reasoning Middleware
DESCRIPTION: Shows how to wrap an Azure OpenAI model with reasoning extraction middleware for DeepSeek-R1.

LANGUAGE: typescript
CODE:
import { azure } from '@ai-sdk/azure';
import { wrapLanguageModel, extractReasoningMiddleware } from 'ai';

const enhancedModel = wrapLanguageModel({
  model: azure('your-deepseek-r1-deployment-name'),
  middleware: extractReasoningMiddleware({ tagName: 'think' }),
});

----------------------------------------

TITLE: Configuring useAssistant Request Options
DESCRIPTION: Example showing how to customize API requests with additional headers, body content, and credentials.

LANGUAGE: tsx
CODE:
const { messages, input, handleInputChange, handleSubmit } = useAssistant({
  api: '/api/custom-completion',
  headers: {
    Authorization: 'your_token',
  },
  body: {
    user_id: '123',
  },
  credentials: 'same-origin',
});

----------------------------------------

TITLE: Initializing Language Model with Cloudflare Workers AI
DESCRIPTION: Create a language model instance with specific settings using the Cloudflare Workers AI provider.

LANGUAGE: typescript
CODE:
import { createWorkersAI } from 'workers-ai-provider';

const workersai = createWorkersAI({ binding: env.AI });
const model = workersai('@cf/meta/llama-3.1-8b-instruct', {
  // additional settings
  safePrompt: true,
});

----------------------------------------

TITLE: Importing LangChainStream in JavaScript
DESCRIPTION: This snippet demonstrates how to import the LangChainStream component from the 'ai' package. Note that this is part of the legacy LangChain integration and is no longer recommended for use.

LANGUAGE: javascript
CODE:
import { LangChainStream } from "ai"

----------------------------------------

TITLE: Creating React Component for Text Generation in Next.js
DESCRIPTION: This snippet shows how to create a React component that sends a POST request to the '/api/completion' endpoint to generate text based on a predefined prompt. It handles loading states and displays the generated text.

LANGUAGE: tsx
CODE:
'use client';

import { useState } from 'react';

export default function Page() {
  const [generation, setGeneration] = useState('');
  const [isLoading, setIsLoading] = useState(false);

  return (
    <div>
      <div
        onClick={async () => {
          setIsLoading(true);

          await fetch('/api/completion', {
            method: 'POST',
            body: JSON.stringify({
              prompt: 'Why is the sky blue?',
            }),
          }).then(response => {
            response.json().then(json => {
              setGeneration(json.text);
              setIsLoading(false);
            });
          });
        }}
      >
        Generate
      </div>

      {isLoading ? 'Loading...' : generation}
    </div>
  );
}

----------------------------------------

TITLE: Next.js OpenTelemetry Integration
DESCRIPTION: Setting up Langfuse exporter with Next.js OpenTelemetry instrumentation

LANGUAGE: typescript
CODE:
import { registerOTel } from '@vercel/otel';
import { LangfuseExporter } from 'langfuse-vercel';

export function register() {
  registerOTel({
    serviceName: 'langfuse-vercel-ai-nextjs-example',
    traceExporter: new LangfuseExporter(),
  });
}

----------------------------------------

TITLE: Generating Images with Azure OpenAI Image Model
DESCRIPTION: Demonstrates how to use the Azure OpenAI image model to generate images using the generateImage function.

LANGUAGE: typescript
CODE:
import { azure } from '@ai-sdk/azure';
import { experimental_generateImage as generateImage } from 'ai';

const { image } = await generateImage({
  model: azure.imageModel('your-dalle-deployment-name'),
  prompt: 'A photorealistic image of a cat astronaut floating in space',
  size: '1024x1024', // '1024x1024', '1792x1024', or '1024x1792' for DALL-E 3
});

// image contains the URL or base64 data of the generated image
console.log(image);

----------------------------------------

TITLE: Creating Embedding Model - TypeScript
DESCRIPTION: Initializes a Mistral embedding model for generating vector embeddings.

LANGUAGE: typescript
CODE:
const model = mistral.embedding('mistral-embed');

----------------------------------------

TITLE: Importing cosineSimilarity Function in TypeScript
DESCRIPTION: This snippet shows how to import the cosineSimilarity function from the 'ai' library. This import is necessary to use the function in your TypeScript code.

LANGUAGE: typescript
CODE:
import { cosineSimilarity } from "ai"

----------------------------------------

TITLE: Chat API Route Handler Setup
DESCRIPTION: Implementation of a Next.js API route handler for chat functionality using Claude 3.7 Sonnet, including streaming response handling.

LANGUAGE: typescript
CODE:
import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';
import { streamText } from 'ai';

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: anthropic('claude-3-7-sonnet-20250219'),
    messages,
    providerOptions: {
      anthropic: {
        thinking: { type: 'enabled', budgetTokens: 12000 },
      } satisfies AnthropicProviderOptions,
    },
  });

  return result.toDataStreamResponse({
    sendReasoning: true,
  });
}

----------------------------------------

TITLE: Converting LlamaIndex ChatEngine Stream in TypeScript
DESCRIPTION: Example demonstrating how to use LlamaIndexAdapter to convert a LlamaIndex ChatEngine stream to a data stream response. Shows integration with OpenAI and SimpleChatEngine for handling chat completions with streaming.

LANGUAGE: tsx
CODE:
import { OpenAI, SimpleChatEngine } from 'llamaindex';
import { LlamaIndexAdapter } from 'ai';

export async function POST(req: Request) {
  const { prompt } = await req.json();

  const llm = new OpenAI({ model: 'gpt-4o' });
  const chatEngine = new SimpleChatEngine({ llm });

  const stream = await chatEngine.chat({
    message: prompt,
    stream: true,
  });

  return LlamaIndexAdapter.toDataStreamResponse(stream);
}

----------------------------------------

TITLE: Importing LangChainAdapter in TypeScript
DESCRIPTION: Shows how to import the LangChainAdapter module from the 'ai' package.

LANGUAGE: typescript
CODE:
import { LangChainAdapter } from "ai"

----------------------------------------

TITLE: Generating Structured Data with Llama 3.1
DESCRIPTION: This snippet shows how to generate structured JSON data using the generateObject function from the AI SDK with Llama 3.1. It uses a Zod schema to define the structure of the output.

LANGUAGE: tsx
CODE:
import { generateObject } from 'ai';
import { deepinfra } from '@ai-sdk/deepinfra';
import { z } from 'zod';

const { object } = await generateObject({
  model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a lasagna recipe.',
});

----------------------------------------

TITLE: Generating Array with Schema in TypeScript
DESCRIPTION: Example demonstrating how to generate an array of objects using a schema to define hero characters

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateObject } from 'ai';
import { z } from 'zod';

const { object } = await generateObject({
  model: openai('gpt-4-turbo'),
  output: 'array',
  schema: z.object({
    name: z.string(),
    class: z
      .string()
      .describe('Character class, e.g. warrior, mage, or thief.'),
    description: z.string(),
  }),
  prompt: 'Generate 3 hero descriptions for a fantasy role playing game.',
});

----------------------------------------

TITLE: Streaming Text with Baseten Model
DESCRIPTION: Shows how to use a Baseten language model to generate text in a streaming fashion using the streamText function from the AI SDK. It includes setting up the provider and model, and streaming text based on a prompt.

LANGUAGE: typescript
CODE:
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
import { streamText } from 'ai';

const BASETEN_MODEL_ID = '<deployment-id>';
const BASETEN_DEPLOYMENT_ID = null;

// see https://docs.baseten.co/api-reference/openai for more information
const basetenExtraPayload = {
  model_id: BASETEN_MODEL_ID,
  deployment_id: BASETEN_DEPLOYMENT_ID,
};

const baseten = createOpenAICompatible({
  name: 'baseten',
  apiKey: process.env.BASETEN_API_KEY,
  baseURL: 'https://bridge.baseten.co/v1/direct',
  fetch: async (url, request) => {
    const bodyWithBasetenPayload = JSON.stringify({
      ...JSON.parse(request.body),
      baseten: basetenExtraPayload,
    });
    return await fetch(url, { ...request, body: bodyWithBasetenPayload });
  },
});

const result = streamText({
  model: baseten('ultravox'),
  prompt: 'Tell me about yourself in one sentence',
});

for await (const message of result.textStream) {
  console.log(message);
}

----------------------------------------

TITLE: Basic Chat Setup with UI SDK - Route Handler
DESCRIPTION: Route handler implementation using streamText for chat completion in UI SDK

LANGUAGE: typescript
CODE:
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';

export async function POST(request) {
  const { messages } = await request.json();

  const result = streamText({
    model: openai('gpt-4o'),
    system: 'you are a friendly assistant!',
    messages,
    tools: {
      // tool definitions
    },
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Implementing Chat UI with React Client Component
DESCRIPTION: This code snippet shows how to create a client-side React component for managing a chat conversation. It includes state management for the conversation history and user input, as well as a function to continue the conversation using a server action.

LANGUAGE: tsx
CODE:
'use client';

import { useState } from 'react';
import { Message, continueConversation } from './actions';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export default function Home() {
  const [conversation, setConversation] = useState<Message[]>([]);
  const [input, setInput] = useState<string>('');

  return (
    <div>
      <div>
        {conversation.map((message, index) => (
          <div key={index}>
            {message.role}: {message.content}
          </div>
        ))}
      </div>

      <div>
        <input
          type="text"
          value={input}
          onChange={event => {
            setInput(event.target.value);
          }}
        />
        <button
          onClick={async () => {
            const { messages } = await continueConversation([
              ...conversation,
              { role: 'user', content: input },
            ]);

            setConversation(messages);
          }}
        >
          Send Message
        </button>
      </div>
    </div>
  );
}

----------------------------------------

TITLE: Grouping Multiple Executions in One Trace
DESCRIPTION: Example of grouping multiple AI SDK executions under a single Langfuse trace with custom configuration

LANGUAGE: typescript
CODE:
import { randomUUID } from 'crypto';
import { Langfuse } from 'langfuse';

const langfuse = new Langfuse();
const parentTraceId = randomUUID();

langfuse.trace({
  id: parentTraceId,
  name: 'holiday-traditions',
});

for (let i = 0; i < 3; i++) {
  const result = await generateText({
    model: openai('gpt-3.5-turbo'),
    maxTokens: 50,
    prompt: 'Invent a new holiday and describe its traditions.',
    experimental_telemetry: {
      isEnabled: true,
      functionId: `holiday-tradition-${i}`,
      metadata: {
        langfuseTraceId: parentTraceId,
        langfuseUpdateParent: false,
      },
    },
  });

  console.log(result.text);
}

await langfuse.flushAsync();
await sdk.shutdown();

----------------------------------------

TITLE: Generating an Image with OpenAI's DALL-E 3
DESCRIPTION: Demonstrates basic usage of the generateImage function to create an image using OpenAI's DALL-E 3 model.

LANGUAGE: tsx
CODE:
import { experimental_generateImage as generateImage } from 'ai';
import { openai } from '@ai-sdk/openai';

const { image } = await generateImage({
  model: openai.image('dall-e-3'),
  prompt: 'Santa Claus driving a Cadillac',
});

----------------------------------------

TITLE: Importing streamToResponse from AI SDK
DESCRIPTION: Shows how to import the streamToResponse function from the AI SDK.

LANGUAGE: typescript
CODE:
import { streamToResponse } from "ai"

----------------------------------------

TITLE: Generating Text with Azure OpenAI
DESCRIPTION: Complete example demonstrating text generation using Azure OpenAI provider with GPT-4 model. Shows how to configure the model and provide a prompt for text generation.

LANGUAGE: typescript
CODE:
import { azure } from '@ai-sdk/azure';
import { generateText } from 'ai';

const { text } = await generateText({
  model: azure('gpt-4o'), // your deployment name
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Using Structured Message Format with Memory in TypeScript
DESCRIPTION: Demonstrates how to use structured message format with memory context in the generateText function.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';
import { createMem0 } from '@mem0/vercel-ai-provider';

const mem0 = createMem0();

const { text } = await generateText({
  model: mem0('gpt-4-turbo', { user_id: 'borat' }),
  messages: [
    {
      role: 'user',
      content: [
        { type: 'text', text: 'Suggest me a good car to buy.' },
        { type: 'text', text: 'Why is it better than the other cars for me?' },
      ],
    },
  ],
});

----------------------------------------

TITLE: Handling Error State in React with useObject Hook
DESCRIPTION: This React component demonstrates how to handle and display error states using the error object returned by useObject. It shows a generic error message when an error occurs during the fetch request.

LANGUAGE: tsx
CODE:
'use client';

import { useObject } from '@ai-sdk/react';

export default function Page() {
  const { error, object, submit } = useObject({
    api: '/api/notifications',
    schema: notificationSchema,
  });

  return (
    <>
      {error && <div>An error occurred.</div>}

      <button onClick={() => submit('Messages during finals week.')}>
        Generate notifications
      </button>

      {object?.notifications?.map((notification, index) => (
        <div key={index}>
          <p>{notification?.name}</p>
          <p>{notification?.message}</p>
        </div>
      ))}
    </>
  );
}

----------------------------------------

TITLE: Importing OpenAIStream in React
DESCRIPTION: Shows how to import the OpenAIStream helper function from the 'ai' package in a React application.

LANGUAGE: javascript
CODE:
import { OpenAIStream } from "ai"

----------------------------------------

TITLE: Accessing Response Headers and Body
DESCRIPTION: Example of how to access the raw response headers and body from the model provider.

LANGUAGE: ts
CODE:
import { generateText } from 'ai';

const result = await generateText({
  // ...
});

console.log(JSON.stringify(result.response.headers, null, 2));
console.log(JSON.stringify(result.response.body, null, 2));

----------------------------------------

TITLE: Sending Custom Sources in Server Route
DESCRIPTION: Demonstrates how to send custom source information to the client using the writeSource method of DataStreamWriter.

LANGUAGE: tsx
CODE:
import { openai } from '@ai-sdk/openai';
import { createDataStreamResponse, streamText } from 'ai';

export async function POST(req: Request) {
  const { messages } = await req.json();

  return createDataStreamResponse({
    execute: dataStream => {
      dataStream.writeSource({
        sourceType: 'url',
        id: 'source-1',
        url: 'https://example.com',
        title: 'Example Source',
      });

      const result = streamText({
        model: openai('gpt-4o'),
        messages,
      });

      result.mergeIntoDataStream(dataStream);
    },
  });
}

----------------------------------------

TITLE: Generating Text with Baseten Model
DESCRIPTION: Demonstrates how to use a Baseten language model to generate text using the generateText function from the AI SDK. It includes setting up the provider and model, and generating text based on a prompt.

LANGUAGE: typescript
CODE:
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
import { generateText } from 'ai';

const BASETEN_MODEL_ID = '<deployment-id>';
const BASETEN_DEPLOYMENT_ID = null;

// see https://docs.baseten.co/api-reference/openai for more information
const basetenExtraPayload = {
  model_id: BASETEN_MODEL_ID,
  deployment_id: BASETEN_DEPLOYMENT_ID,
};

const baseten = createOpenAICompatible({
  name: 'baseten',
  apiKey: process.env.BASETEN_API_KEY,
  baseURL: 'https://bridge.baseten.co/v1/direct',
  fetch: async (url, request) => {
    const bodyWithBasetenPayload = JSON.stringify({
      ...JSON.parse(request.body),
      baseten: basetenExtraPayload,
    });
    return await fetch(url, { ...request, body: bodyWithBasetenPayload });
  },
});

const { text } = await generateText({
  model: baseten('ultravox'),
  prompt: 'Tell me about yourself in one sentence',
});

console.log(text);

----------------------------------------

TITLE: Configuring OpenAI API Key in Environment File
DESCRIPTION: Sets up the OpenAI API key in a .env.local file for authentication with OpenAI services.

LANGUAGE: env
CODE:
OPENAI_API_KEY=xxxxxxxxx

----------------------------------------

TITLE: Importing Cohere Provider in TypeScript
DESCRIPTION: This snippet shows how to import the default Cohere provider instance in a TypeScript file.

LANGUAGE: typescript
CODE:
import { cohere } from '@ai-sdk/cohere';

----------------------------------------

TITLE: Rendering Weather Component in React TypeScript
DESCRIPTION: This code snippet demonstrates how to use the weather object returned by the getWeather function to render a WeatherCard component. It maps through messages and conditionally renders the component based on the message role and content.

LANGUAGE: tsx
CODE:
return (
  <div>
    {messages.map(message => {
      if (message.role === 'function') {
        const { name, content } = message
        const { temperature, unit, description, forecast } = content;

        return (
          <WeatherCard
            weather={{
              temperature: 47,
              unit: 'F',
              description: 'sunny'
              forecast,
            }}
          />
        )
      }
    })}
  </div>
)

----------------------------------------

TITLE: Using streamToResponse with Node.js HTTP Server
DESCRIPTION: Demonstrates how to use streamToResponse to pipe a data stream to a Node.js HTTP server response. It includes setting up a server, streaming text using OpenAI's GPT-4 model, and handling stream data.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { StreamData, streamText, streamToResponse } from 'ai';
import { createServer } from 'http';

createServer(async (req, res) => {
  const result = streamText({
    model: openai('gpt-4-turbo'),
    prompt: 'What is the weather in San Francisco?',
  });

  // use stream data
  const data = new StreamData();

  data.append('initialized call');

  streamToResponse(
    result.toAIStream({
      onFinal() {
        data.append('call completed');
        data.close();
      },
    }),
    res,
    {},
    data,
  );
}).listen(8080);

----------------------------------------

TITLE: Example Fetch Request Output
DESCRIPTION: Sample output showing the structure of intercepted fetch requests.

LANGUAGE: bash
CODE:
URL https://api.sambanova.ai/v1/chat/completions
Headers {
  "Content-Type": "application/json",
  "Authorization": "Bearer YOUR_API_KEY"
}
Body {
  "model": "Meta-Llama-3.1-70B-Instruct",
  "temperature": 0,
  "messages": [
    {
      "role": "user",
      "content": "Hello, nice to meet you."
    }
  ]
}

----------------------------------------

TITLE: Extended Thinking Implementation
DESCRIPTION: Demonstrates how to enable Claude 3.7's extended thinking capability with a specified token budget for complex reasoning tasks.

LANGUAGE: typescript
CODE:
import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';
import { generateText } from 'ai';

const { text, reasoning, reasoningDetails } = await generateText({
  model: anthropic('claude-3-7-sonnet-20250219'),
  prompt: 'How many people will live in the world in 2040?',
  providerOptions: {
    anthropic: {
      thinking: { type: 'enabled', budgetTokens: 12000 },
    } satisfies AnthropicProviderOptions,
  },
});

console.log(reasoning); // reasoning text
console.log(reasoningDetails); // reasoning details including redacted reasoning
console.log(text); // text response

----------------------------------------

TITLE: Generating Images with Provider-Specific Options
DESCRIPTION: Example of generating images with Together.ai model, including provider-specific options.

LANGUAGE: typescript
CODE:
import { togetherai } from '@ai-sdk/togetherai';
import { experimental_generateImage as generateImage } from 'ai';

const { images } = await generateImage({
  model: togetherai.image('black-forest-labs/FLUX.1-dev'),
  prompt: 'A delighted resplendent quetzal mid flight amidst raindrops',
  size: '512x512',
  // Optional additional provider-specific request parameters
  providerOptions: {
    togetherai: {
      steps: 40,
    },
  },
});

----------------------------------------

TITLE: Creating Azure OpenAI Language Model Instance
DESCRIPTION: Demonstrates how to create a language model instance using the Azure OpenAI provider.

LANGUAGE: typescript
CODE:
const model = azure('your-deployment-name');

----------------------------------------

TITLE: Using Memory Functions in TypeScript
DESCRIPTION: Examples of using addMemories, retrieveMemories, and getMemories functions with Mem0 provider, including optional configuration parameters.

LANGUAGE: typescript
CODE:
await addMemories(messages, {
  user_id: 'borat',
  mem0ApiKey: 'm0-xxx',
  org_id: 'org_xx',
  project_id: 'proj_xx',
});
await retrieveMemories(prompt, {
  user_id: 'borat',
  mem0ApiKey: 'm0-xxx',
  org_id: 'org_xx',
  project_id: 'proj_xx',
});
await getMemories(prompt, {
  user_id: 'borat',
  mem0ApiKey: 'm0-xxx',
  org_id: 'org_xx',
  project_id: 'proj_xx',
});

----------------------------------------

TITLE: Importing jsonSchema from AI SDK in TypeScript
DESCRIPTION: Shows how to import the jsonSchema function from the 'ai' package.

LANGUAGE: typescript
CODE:
import { jsonSchema } from "ai"

----------------------------------------

TITLE: New Message Parts Format - JavaScript
DESCRIPTION: Demonstrates the new message parts array structure introduced in AI SDK 4.2 for handling multi-modal responses.

LANGUAGE: javascript
CODE:
message.parts = [
  { type: "text", text: "Final answer: 42" },
  { type: "reasoning", reasoning: "First I'll calculate X, then Y..." },
  { type: "tool-invocation", toolInvocation: { toolName: "calculator", args: {...} } },
];

----------------------------------------

TITLE: Basic Text Generation with generateText
DESCRIPTION: Demonstrates how to use the generateText function to generate text from a simple prompt using an LLM model.

LANGUAGE: tsx
CODE:
import { generateText } from 'ai';

const { text } = await generateText({
  model: yourModel,
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Generating JSON without Schema in TypeScript
DESCRIPTION: Example of using streamObject to generate unstructured JSON data without a schema.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamObject } from 'ai';

const { partialObjectStream } = streamObject({
  model: openai('gpt-4-turbo'),
  output: 'no-schema',
  prompt: 'Generate a lasagna recipe.',
});

for await (const partialObject of partialObjectStream) {
  console.clear();
  console.log(partialObject);
}

----------------------------------------

TITLE: Provider Usage Example in TypeScript
DESCRIPTION: Demonstrates how to use the custom provider in a TypeScript application.

LANGUAGE: typescript
CODE:
import { example } from '@company-name/example';
import { generateText } from 'ai';

const { text } = await generateText({
  model: example('example/chat-model-1'),
  prompt: 'Hello, how are you?',
});

----------------------------------------

TITLE: Accessing Tool Calls in AI SDK
DESCRIPTION: Demonstrates how to access and handle tool calls from the model response. Includes type-safe handling of different tool calls using TypeScript switch statements.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateText, tool } from 'ai';
import dotenv from 'dotenv';
import { z } from 'zod';

dotenv.config();

async function main() {
  const result = await generateText({
    model: openai('gpt-3.5-turbo'),
    maxTokens: 512,
    tools: {
      weather: tool({
        description: 'Get the weather in a location',
        parameters: z.object({
          location: z.string().describe('The location to get the weather for'),
        }),
        execute: async ({ location }) => ({
          location,
          temperature: 72 + Math.floor(Math.random() * 21) - 10,
        }),
      }),
      cityAttractions: tool({
        parameters: z.object({ city: z.string() }),
      }),
    },
    prompt:
      'What is the weather in San Francisco and what attractions should I visit?',
  });

  // typed tool calls:
  for (const toolCall of result.toolCalls) {
    switch (toolCall.toolName) {
      case 'cityAttractions': {
        toolCall.args.city; // string
        break;
      }

      case 'weather': {
        toolCall.args.location; // string
        break;
      }
    }
  }

  console.log(JSON.stringify(result, null, 2));
}

main().catch(console.error);

----------------------------------------

TITLE: Testing streamObject with MockLanguageModelV1
DESCRIPTION: Demonstrates testing object streaming with MockLanguageModelV1 and simulated JSON chunks.

LANGUAGE: typescript
CODE:
import { streamObject, simulateReadableStream } from 'ai';
import { MockLanguageModelV1 } from 'ai/test';
import { z } from 'zod';

const result = streamObject({
  model: new MockLanguageModelV1({
    defaultObjectGenerationMode: 'json',
    doStream: async () => ({
      stream: simulateReadableStream({
        chunks: [
          { type: 'text-delta', textDelta: '{ ' },
          { type: 'text-delta', textDelta: '"content": ' },
          { type: 'text-delta', textDelta: `"Hello, ` },
          { type: 'text-delta', textDelta: `world` },
          { type: 'text-delta', textDelta: `!"` },
          { type: 'text-delta', textDelta: ' }' },
          {
            type: 'finish',
            finishReason: 'stop',
            logprobs: undefined,
            usage: { completionTokens: 10, promptTokens: 3 },
          },
        ],
      }),
      rawCall: { rawPrompt: null, rawSettings: {} },
    }),
  }),
  schema: z.object({ content: z.string() }),
  prompt: 'Hello, test!',
});

----------------------------------------

TITLE: Configuring Jest Module Resolution for AI SDK RSC
DESCRIPTION: Jest configuration update to properly map the ai/rsc module to its correct path in node_modules. This resolves the 'Cannot find module 'ai/rsc'' error when testing React Server Components.

LANGUAGE: json
CODE:
"moduleNameMapper": {
  "^ai/rsc$": "<rootDir>/node_modules/ai/rsc/dist"
}

----------------------------------------

TITLE: Importing useUIState Hook from AI SDK RSC
DESCRIPTION: Shows how to import the useUIState hook from the AI SDK RSC package. This hook is used to manage UI state in AI applications.

LANGUAGE: javascript
CODE:
import { useUIState } from "ai/rsc"

----------------------------------------

TITLE: Next.js OpenTelemetry Instrumentation Setup
DESCRIPTION: Configuration of OpenTelemetry instrumentation in Next.js using AISDKExporter.

LANGUAGE: typescript
CODE:
import { registerOTel } from '@vercel/otel';
import { AISDKExporter } from 'langsmith/vercel';

export function register() {
  registerOTel({
    serviceName: 'langsmith-vercel-ai-sdk-example',
    traceExporter: new AISDKExporter(),
  });
}

----------------------------------------

TITLE: Generating Text with Cohere Model
DESCRIPTION: Example of using Cohere language model to generate text using the generateText function.

LANGUAGE: typescript
CODE:
import { cohere } from '@ai-sdk/cohere';
import { generateText } from 'ai';

const { text } = await generateText({
  model: cohere('command-r-plus'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Creating xAI Chat Model with Custom Options
DESCRIPTION: Initialize an xAI chat model with specific settings like user identifier.

LANGUAGE: typescript
CODE:
const model = xai('grok-2-1212', {
  user: 'test-user', // optional unique user identifier
});

----------------------------------------

TITLE: Creating Custom Groq Provider
DESCRIPTION: Creating a customized Groq provider instance with specific settings.

LANGUAGE: typescript
CODE:
import { createGroq } from '@ai-sdk/groq';

const groq = createGroq({
  // custom settings
});

----------------------------------------

TITLE: Migrating from Mistral Facade
DESCRIPTION: Example of updating from the Mistral facade to the createMistral function.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
const mistral = new Mistral({
  // ...
});

// AI SDK 4.0
const mistral = createMistral({
  // ...
});

----------------------------------------

TITLE: Streaming Text Response with Llama 3.1
DESCRIPTION: This code demonstrates how to stream the model's response as it's being generated using the streamText function from the AI SDK with Llama 3.1 via DeepInfra.

LANGUAGE: tsx
CODE:
import { streamText } from 'ai';
import { deepinfra } from '@ai-sdk/deepinfra';

const { textStream } = streamText({
  model: deepinfra('meta-llama/Meta-Llama-3.1-405B-Instruct'),
  prompt: 'What is love?',
});

----------------------------------------

TITLE: Updating AI SDK Package Versions
DESCRIPTION: Required updates to AI SDK package versions in package.json for the 4.0 upgrade.

LANGUAGE: json
CODE:
{
  "dependencies": {
    "ai": "4.0.*",
    "ai-sdk@provider-utils": "2.0.*",
    "ai-sdk/*": "1.0.*"
  }
}

----------------------------------------

TITLE: Creating Custom Azure OpenAI Provider Instance
DESCRIPTION: Shows how to create a customized Azure OpenAI provider instance with specific settings.

LANGUAGE: typescript
CODE:
import { createAzure } from '@ai-sdk/azure';

const azure = createAzure({
  resourceName: 'your-resource-name', // Azure resource name
  apiKey: 'your-api-key',
});

----------------------------------------

TITLE: Implementing Server-Side Stream Cancellation with AI SDK Core
DESCRIPTION: Demonstrates how to implement server-side stream cancellation using the AI SDK's core functionality. Uses the abortSignal parameter to forward cancellation requests from the client to the LLM API.

LANGUAGE: tsx
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';

export async function POST(req: Request) {
  const { prompt } = await req.json();

  const result = streamText({
    model: openai('gpt-4-turbo'),
    prompt,
    // forward the abort signal:
    abortSignal: req.signal,
  });

  return result.toTextStreamResponse();
}

----------------------------------------

TITLE: Creating Cloudflare Workers AI Provider Instance
DESCRIPTION: Initialize a Cloudflare Workers AI provider instance using the createWorkersAI function and AI binding.

LANGUAGE: typescript
CODE:
import { createWorkersAI } from 'workers-ai-provider';

const workersai = createWorkersAI({ binding: env.AI });

----------------------------------------

TITLE: Importing useAssistant in React
DESCRIPTION: Shows how to import the useAssistant hook in a React application.

LANGUAGE: javascript
CODE:
import { useAssistant } from '@ai-sdk/react'

----------------------------------------

TITLE: Initializing Laminar SDK
DESCRIPTION: Basic initialization of Laminar SDK with project API key for tracing AI SDK applications.

LANGUAGE: javascript
CODE:
import { Laminar } from '@lmnr-ai/lmnr';

Laminar.initialize({
  projectApiKey: '...',
});

----------------------------------------

TITLE: Next.js API Route with Telemetry
DESCRIPTION: Example of AI SDK usage in Next.js API route with experimental telemetry enabled for tracing.

LANGUAGE: javascript
CODE:
const { text } = await generateText({
  model: openai('gpt-4o-mini'),
  prompt: 'What is Laminar flow?',
  experimental_telemetry: {
    isEnabled: true,
  },
});

----------------------------------------

TITLE: Using Embedding Models with Amazon Bedrock
DESCRIPTION: Example of creating and using an embedding model with Amazon Bedrock.

LANGUAGE: typescript
CODE:
const model = bedrock.embedding('amazon.titan-embed-text-v2:0', {
  dimensions: 512, // optional, number of dimensions for the embedding
  normalize: true // optional  normalize the output embeddings
});

----------------------------------------

TITLE: Migrating Experimental Continuation Steps
DESCRIPTION: Example of updating from experimental_continuationSteps to experimental_continueSteps.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
const result = await generateText({
  experimental_continuationSteps: true,
  // ...
});

// AI SDK 4.0
const result = await generateText({
  experimental_continueSteps: true,
  // ...
});

----------------------------------------

TITLE: Creating a Provider Registry with Custom Separator in TypeScript
DESCRIPTION: This snippet shows how to create a provider registry with a custom separator between provider and model IDs. It also demonstrates how to use the custom separator when accessing a model.

LANGUAGE: typescript
CODE:
const registry = createProviderRegistry(
  {
    anthropic,
    openai,
  },
  { separator: ' > ' },
);

// Now you can use the custom separator
const model = registry.languageModel('anthropic > claude-3-opus-20240229');

----------------------------------------

TITLE: Using simulateReadableStream with Delays in TypeScript
DESCRIPTION: Demonstrates creating a ReadableStream with simulateReadableStream, specifying both initial and chunk delays.

LANGUAGE: typescript
CODE:
const stream = simulateReadableStream({
  chunks: ['Hello', ' ', 'World'],
  initialDelayInMs: 1000, // Wait 1 second before first chunk
  chunkDelayInMs: 500, // Wait 0.5 seconds between chunks
});

----------------------------------------

TITLE: Importing pipeDataStreamToResponse Function in TypeScript
DESCRIPTION: This snippet shows how to import the pipeDataStreamToResponse function from the 'ai' package.

LANGUAGE: typescript
CODE:
import { pipeDataStreamToResponse } from "ai"

----------------------------------------

TITLE: Migrating streamObject Usage
DESCRIPTION: Example of updating streamObject usage to remove await.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
const result = await streamObject({
  // ...
});

// AI SDK 4.0
const result = streamObject({
  // ...
});

----------------------------------------

TITLE: Basic Claude 3.7 Sonnet Text Generation
DESCRIPTION: Demonstrates how to generate text using Claude 3.7 Sonnet with the AI SDK core functionality. Shows the basic setup for text generation with model response handling.

LANGUAGE: typescript
CODE:
import { anthropic } from '@ai-sdk/anthropic';
import { generateText } from 'ai';

const { text, reasoning, reasoningDetails } = await generateText({
  model: anthropic('claude-3-7-sonnet-20250219'),
  prompt: 'How many people will live in the world in 2040?',
});
console.log(text); // text response

----------------------------------------

TITLE: Implementing AI Conversation UI in React
DESCRIPTION: This snippet demonstrates how to create a React component for displaying and continuing an AI conversation. It uses the useUIState and useActions hooks from the AI SDK to manage conversation state and actions.

LANGUAGE: tsx
CODE:
'use client';

import { useState, useEffect } from 'react';
import { ClientMessage } from './actions';
import { useActions, useUIState } from 'ai/rsc';
import { generateId } from 'ai';

export default function Home() {
  const [conversation, setConversation] = useUIState();
  const [input, setInput] = useState<string>('');
  const { continueConversation } = useActions();

  return (
    <div>
      <div className="conversation-history">
        {conversation.map((message: ClientMessage) => (
          <div key={message.id} className={`message ${message.role}`}>
            {message.role}: {message.display}
          </div>
        ))}
      </div>

      <div className="input-area">
        <input
          type="text"
          value={input}
          onChange={e => setInput(e.target.value)}
          placeholder="Type your message..."
        />
        <button
          onClick={async () => {
            // Add user message to UI
            setConversation((currentConversation: ClientMessage[]) => [
              ...currentConversation,
              { id: generateId(), role: 'user', display: input },
            ]);

            // Get AI response
            const message = await continueConversation(input);

            // Add AI response to UI
            setConversation((currentConversation: ClientMessage[]) => [
              ...currentConversation,
              message,
            ]);

            setInput('');
          }}
        >
          Send
        </button>
      </div>
    </div>
  );
}

----------------------------------------

TITLE: Using OpenAI Responses API
DESCRIPTION: Example of using the OpenAI responses API with provider options.

LANGUAGE: typescript
CODE:
import { openai, OpenAIResponsesProviderOptions } from '@ai-sdk/openai';
import { generateText } from 'ai';

const result = await generateText({
  model: openai.responses('gpt-4o-mini'),
  providerOptions: {
    openai: {
      parallelToolCalls: false,
      store: false,
      user: 'user_123',
      // ...
    } satisfies OpenAIResponsesProviderOptions,
  },
  // ...
});

----------------------------------------

TITLE: User Message with Binary Image
DESCRIPTION: Example of including a binary image in a user message using Buffer.

LANGUAGE: typescript
CODE:
const result = await generateText({
  model,
  messages: [
    {
      role: 'user',
      content: [
        { type: 'text', text: 'Describe the image in detail.' },
        {
          type: 'image',
          image: fs.readFileSync('./data/comic-cat.png'),
        },
      ],
    },
  ],
});

----------------------------------------

TITLE: Setting Environment Variables
DESCRIPTION: Configuration of required environment variables for LangSmith and OpenAI API access.

LANGUAGE: bash
CODE:
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=<your-api-key>

export OPENAI_API_KEY=<your-openai-api-key>

----------------------------------------

TITLE: Common TypeScript Component Errors in Streamable UI
DESCRIPTION: Lists common TypeScript compilation errors that occur when using streamable UI components with incorrect file extensions. The main issue stems from using .ts instead of .tsx file extensions for files containing JSX/TSX syntax.

LANGUAGE: typescript
CODE:
// Common errors:
// - Variable Not Found
// - Cannot find `div`
// - `Component` refers to a value, but is being used as a type

----------------------------------------

TITLE: Running Individual AI SDK 4.0 Codemods
DESCRIPTION: Command to run a specific codemod by specifying its name and the path to apply it to.

LANGUAGE: sh
CODE:
npx @ai-sdk/codemod <codemod-name> <path>

----------------------------------------

TITLE: Migrating topK Setting for Google
DESCRIPTION: Example of updating the topK setting from model-specific to standard for Google.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
const result = await generateText({
  model: google('gemini-1.5-flash', {
    topK: 0.5,
  }),
});

// AI SDK 4.0
const result = await generateText({
  model: google('gemini-1.5-flash'),
  topK: 0.5,
});

----------------------------------------

TITLE: Text Generation with Mistral - TypeScript
DESCRIPTION: Example of generating text using a Mistral language model with the generateText function.

LANGUAGE: typescript
CODE:
import { mistral } from '@ai-sdk/mistral';
import { generateText } from 'ai';

const { text } = await generateText({
  model: mistral('mistral-large-latest'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Image Generation with Reference Images
DESCRIPTION: Example of generating images using reference images to guide the generation process.

LANGUAGE: typescript
CODE:
await generateImage({
  model: luma.image('photon-1'),
  prompt: 'A salamander at dusk in a forest pond, in the style of ukiyo-e',
  providerOptions: {
    luma: {
      image_ref: [
        {
          url: 'https://example.com/reference.jpg',
          weight: 0.85,
        },
      ],
    },
  },
});

----------------------------------------

TITLE: Using Google Vertex for Text Generation
DESCRIPTION: Example of using Google Vertex AI for text generation with the Gemini model

LANGUAGE: typescript
CODE:
import { vertex } from '@ai-sdk/google-vertex';
import { generateText } from 'ai';

const { text } = await generateText({
  model: vertex('gemini-1.5-pro'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Testing generateText with MockLanguageModelV1
DESCRIPTION: Demonstrates how to use MockLanguageModelV1 to test the generateText function with a predefined response.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';
import { MockLanguageModelV1 } from 'ai/test';

const result = await generateText({
  model: new MockLanguageModelV1({
    doGenerate: async () => ({
      rawCall: { rawPrompt: null, rawSettings: {} },
      finishReason: 'stop',
      usage: { promptTokens: 10, completionTokens: 20 },
      text: `Hello, world!`,
    }),
  }),
  prompt: 'Hello, test!',
});

----------------------------------------

TITLE: Updating AI SDK Package in Node.js
DESCRIPTION: Command to update the AI SDK package to the latest version using pnpm package manager.

LANGUAGE: bash
CODE:
pnpm add ai@latest

----------------------------------------

TITLE: Creating Chat API Route Handler
DESCRIPTION: Implementation of the API route handler for processing chat messages with OpenAI GPT-4 vision model.

LANGUAGE: tsx
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText, Message } from 'ai';

export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages }: { messages: Message[] } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    messages,
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: OpenAI API Configuration
DESCRIPTION: Environment variable configuration for OpenAI API authentication.

LANGUAGE: env
CODE:
OPENAI_API_KEY=xxxxxxxxx

----------------------------------------

TITLE: Using OpenAI Prompt Caching
DESCRIPTION: Example of using OpenAI prompt caching and accessing cache hit information.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

const { text, usage, providerMetadata } = await generateText({
  model: openai('gpt-4o-mini'),
  prompt: `A 1024-token or longer prompt...`,
});

console.log(`usage:`, {
  ...usage,
  cachedPromptTokens: providerMetadata?.openai?.cachedPromptTokens,
});

----------------------------------------

TITLE: Migrating Token Usage Types
DESCRIPTION: Example of updating token usage types to language model usage types.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
import { TokenUsage, CompletionTokenUsage, EmbeddingTokenUsage } from 'ai';

// AI SDK 4.0
import { LanguageModelUsage, EmbeddingModelUsage } from 'ai';

----------------------------------------

TITLE: Importing useChat Hook for React
DESCRIPTION: Demonstrates how to import the useChat hook for use in a React application.

LANGUAGE: javascript
CODE:
import { useChat } from '@ai-sdk/react'

----------------------------------------

TITLE: Migrating from OpenAI Facade
DESCRIPTION: Example of updating from the OpenAI facade to the createOpenAI function.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
const openai = new OpenAI({
  // ...
});

// AI SDK 4.0
const openai = createOpenAI({
  // ...
});

----------------------------------------

TITLE: Using smoothStream with streamText in TypeScript
DESCRIPTION: Example showing how to use smoothStream as a transform option with streamText function. Demonstrates configuring delay and chunking options for text stream transformation.

LANGUAGE: typescript
CODE:
import { smoothStream, streamText } from 'ai';

const result = streamText({
  model,
  prompt,
  experimental_transform: smoothStream({
    delayInMs: 20, // optional: defaults to 10ms
    chunking: 'line', // optional: defaults to 'word'
  }),
});

----------------------------------------

TITLE: Using Custom Tracer for Telemetry in generateText Function
DESCRIPTION: This snippet demonstrates how to use a custom tracer with the generateText function for telemetry purposes.

LANGUAGE: typescript
CODE:
const tracerProvider = new NodeTracerProvider();
const result = await generateText({
  model: openai('gpt-4-turbo'),
  prompt: 'Write a short story about a cat.',
  experimental_telemetry: {
    isEnabled: true,
    tracer: tracerProvider.getTracer('ai'),
  },
});

----------------------------------------

TITLE: Basic Text Generation Example
DESCRIPTION: Complete example of text generation using the SambaNova provider including initialization and API usage.

LANGUAGE: typescript
CODE:
import { createSambaNova } from 'sambanova-ai-provider';
import { generateText } from 'ai';

const sambanova = createSambaNova({
  apiKey: 'YOUR_API_KEY',
});

const model = sambanova('Meta-Llama-3.1-70B-Instruct');

const { text } = await generateText({
  model,
  prompt: 'Hello, nice to meet you.',
});

console.log(text);

----------------------------------------

TITLE: Implementing Data Stream with AI SDK
DESCRIPTION: Example showing how to use pipeDataStreamToResponse to stream AI-generated text to the client response. Uses OpenAI's GPT-4 model to generate text based on a prompt.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import { createServer } from 'http';

createServer(async (req, res) => {
  const result = streamText({
    model: openai('gpt-4o'),
    prompt: 'Invent a new holiday and describe its traditions.',
  });

  result.pipeDataStreamToResponse(res);
}).listen(8080);

----------------------------------------

TITLE: Sentry Integration Setup
DESCRIPTION: Integration of LangSmith trace exporter with Sentry's OpenTelemetry instrumentation.

LANGUAGE: typescript
CODE:
import * as Sentry from '@sentry/node';
import { BatchSpanProcessor } from '@opentelemetry/sdk-trace-base';
import { AISDKExporter } from 'langsmith/vercel';

const client = Sentry.init({
  dsn: '[Sentry DSN]',
  tracesSampleRate: 1.0,
});

client?.traceProvider?.addSpanProcessor(
  new BatchSpanProcessor(new AISDKExporter()),
);

----------------------------------------

TITLE: Starting Development Server
DESCRIPTION: Command to start the development server using PNPM

LANGUAGE: sh
CODE:
pnpm dev

----------------------------------------

TITLE: Migrating Experimental AI Function Exports
DESCRIPTION: Example of updating from experimental AI function exports to standard exports.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
import {
  experimental_generateText,
  experimental_streamText,
  experimental_generateObject,
  experimental_streamObject,
} from 'ai';

// AI SDK 4.0
import { generateText, streamText, generateObject, streamObject } from 'ai';

----------------------------------------

TITLE: Implementing Server Actions with OpenAI Integration
DESCRIPTION: Demonstrates how to create a server action that generates text using OpenAI's GPT-3.5-turbo model. The function accepts a question string and returns the AI-generated answer.

LANGUAGE: typescript
CODE:
'use server';

import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

export async function getAnswer(question: string) {
  'use server';

  const { text } = await generateText({
    model: openai.chat('gpt-3.5-turbo'),
    prompt: question,
  });

  return { answer: text };
}

----------------------------------------

TITLE: Basic StreamUI Implementation in TypeScript/React
DESCRIPTION: Basic example of using streamUI function to handle streaming UI components with OpenAI GPT-4 model. Shows minimal setup with text rendering but no tools.

LANGUAGE: tsx
CODE:
const result = await streamUI({
  model: openai('gpt-4o'),
  prompt: 'Get the weather for San Francisco',
  text: ({ content }) => <div>{content}</div>,
  tools: {},
});

----------------------------------------

TITLE: Importing StreamingTextResponse
DESCRIPTION: Shows how to import the StreamingTextResponse utility from the AI package.

LANGUAGE: javascript
CODE:
import { StreamingTextResponse } from "ai"

----------------------------------------

TITLE: Using OpenAI Predicted Outputs
DESCRIPTION: Example of using OpenAI predicted outputs to reduce latency.

LANGUAGE: typescript
CODE:
const result = streamText({
  model: openai('gpt-4o'),
  messages: [
    {
      role: 'user',
      content: 'Replace the Username property with an Email property.',
    },
    {
      role: 'user',
      content: existingCode,
    },
  ],
  providerOptions: {
    openai: {
      prediction: {
        type: 'content',
        content: existingCode,
      },
    },
  },
});

----------------------------------------

TITLE: Implementing Error Handling for Tool Calls
DESCRIPTION: Demonstrates how to implement custom error handling for tool calls using errorHandler function and toDataStreamResponse configuration.

LANGUAGE: tsx
CODE:
export function errorHandler(error: unknown) {
  if (error == null) {
    return 'unknown error';
  }

  if (typeof error === 'string') {
    return error;
  }

  if (error instanceof Error) {
    return error.message;
  }

  return JSON.stringify(error);
}

const result = streamText({
  // ...
});

return result.toDataStreamResponse({
  getErrorMessage: errorHandler,
});

----------------------------------------

TITLE: Starting LLM Span in LangWatch Trace
DESCRIPTION: TypeScript example of starting an LLM span within a LangWatch trace, capturing input data and model information.

LANGUAGE: typescript
CODE:
const span = trace.startLLMSpan({
  name: 'llm',
  model: model,
  input: {
    type: 'chat_messages',
    value: messages,
  },
});

----------------------------------------

TITLE: Configuring LangWatch Exporter in Node.js OpenTelemetry SDK
DESCRIPTION: TypeScript code snippet for configuring LangWatch exporter in a Node.js OpenTelemetry SDK setup.

LANGUAGE: typescript
CODE:
import { LangWatchExporter } from 'langwatch';

const sdk = new NodeSDK({
  traceExporter: new LangWatchExporter({
    apiKey: process.env.LANGWATCH_API_KEY,
  }),
  // ...
});

----------------------------------------

TITLE: Migrating Svelte, Vue, and SolidJS Exports
DESCRIPTION: Example of updating imports for Svelte, Vue, and SolidJS from 'ai' to their respective packages.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
import { useChat } from 'ai/svelte';

// AI SDK 4.0
import { useChat } from '@ai-sdk/svelte';

----------------------------------------

TITLE: Implementing Custom Provider Entry Point in TypeScript
DESCRIPTION: This code snippet demonstrates how to create an entry point for a custom provider in the AI SDK. It includes the provider factory function, type definitions, and a default provider instance.

LANGUAGE: typescript
CODE:
import {
  generateId,
  loadApiKey,
  withoutTrailingSlash,
} from '@ai-sdk/provider-utils';
import { CustomChatLanguageModel } from './custom-chat-language-model';
import { CustomChatModelId, CustomChatSettings } from './custom-chat-settings';

export interface CustomProvider {
  (
    modelId: CustomChatModelId,
    settings?: CustomChatSettings,
  ): CustomChatLanguageModel;

  chat(
    modelId: CustomChatModelId,
    settings?: CustomChatSettings,
  ): CustomChatLanguageModel;
}

export interface CustomProviderSettings {
  baseURL?: string;
  apiKey?: string;
  headers?: Record<string, string>;
}

export function createCustomProvider(
  options: CustomProviderSettings = {},
): CustomProvider {
  const createModel = (
    modelId: CustomChatModelId,
    settings: CustomChatSettings = {},
  ) =>
    new CustomChatLanguageModel(modelId, settings, {
      provider: 'custom.chat',
      baseURL:
        withoutTrailingSlash(options.baseURL) ?? 'https://custom.ai/api/v1',
      headers: () => ({
        Authorization: `Bearer ${loadApiKey({
          apiKey: options.apiKey,
          environmentVariableName: 'CUSTOM_API_KEY',
          description: 'Custom Provider',
        })}`,
        ...options.headers,
      }),
      generateId: options.generateId ?? generateId,
    });

  const provider = function (
    modelId: CustomChatModelId,
    settings?: CustomChatSettings,
  ) {
    if (new.target) {
      throw new Error(
        'The model factory function cannot be called with the new keyword.',
      );
    }

    return createModel(modelId, settings);
  };

  provider.chat = createModel;

  return provider;
}

export const customProvider = createCustomProvider();

----------------------------------------

TITLE: File Input Processing
DESCRIPTION: Processing file inputs (e.g., PDF) with Google AI model

LANGUAGE: typescript
CODE:
import { google } from '@ai-sdk/google';
import { generateText } from 'ai';

const result = await generateText({
  model: google('gemini-1.5-flash'),
  messages: [
    {
      role: 'user',
      content: [
        {
          type: 'text',
          text: 'What is an embedding model according to this document?',
        },
        {
          type: 'file',
          data: fs.readFileSync('./data/ai.pdf'),
          mimeType: 'application/pdf',
        },
      ],
    },
  ],
});

----------------------------------------

TITLE: Generating Text with Context using Crosshatch
DESCRIPTION: Example of using Crosshatch to generate text based on user context and a specified language model.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';
import createCrosshatch from '@crosshatch/ai-provider':
const crosshatch = createCrosshatch();

const { text } = await generateText({
  model: crosshatch.languageModel("gpt-4o-mini", {
    token: 'YOUR_ACCESS_TOKEN',
    replace: {
      restaurants: {
        select: ["entity_name", "entity_city", "entity_region"],
        from: "personalTimeline",
        where: [
          { field: "event", op: "=", value: "confirmed" },
          { field: "entity_subtype2", op: "=", value: "RESTAURANTS" }
        ],
        groupby: ["entity_name", "entity_city", "entity_region"],
        orderby: "count DESC",
        limit: 5
      }
    }
  }),
  system: `The user recently ate at these restaurants: {restaurants}`,
  messages: [{role: "user", content: "Where should I stay in Paris?"}]
});

----------------------------------------

TITLE: Importing streamUI Function in JavaScript/TypeScript
DESCRIPTION: This snippet shows how to import the streamUI function from the AI SDK RSC package.

LANGUAGE: javascript
CODE:
import { streamUI } from "ai/rsc"

----------------------------------------

TITLE: Implementing Throttling with useCompletion Hook in React
DESCRIPTION: Shows how to add throttling to the useCompletion hook to prevent render overload. The implementation limits completion updates to occur every 50ms.

LANGUAGE: tsx
CODE:
const { completion, ... } = useCompletion({
  // Throttle the completion and data updates to 50ms:
  experimental_throttle: 50
})

----------------------------------------

TITLE: Provider Capabilities Comparison Table in Markdown
DESCRIPTION: A detailed markdown table comparing different AI model providers and their capabilities across features like image input, object generation, tool usage, and tool streaming. Includes models from major providers like xAI Grok, OpenAI, Anthropic, Groq, DeepInfra, Mistral, Google, DeepSeek, and Cerebras.

LANGUAGE: markdown
CODE:
| Provider                                                                 | Model                                         | Image Input         | Object Generation   | Tool Usage          | Tool Streaming      |
| ------------------------------------------------------------------------ | --------------------------------------------- | ------------------- | ------------------- | ------------------- | ------------------- |
| [xAI Grok](/providers/ai-sdk-providers/xai)                              | `grok-2-1212`                                 | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |
...

----------------------------------------

TITLE: Accessing Response Headers and Body
DESCRIPTION: Shows how to access raw response headers and body from the model provider.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';

const result = await generateText({
  // ...
});

console.log(JSON.stringify(result.response.headers, null, 2));
console.log(JSON.stringify(result.response.body, null, 2));

----------------------------------------

TITLE: Generating Text with OpenAI Model
DESCRIPTION: Use an OpenAI language model to generate text using the generateText function.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

const { text } = await generateText({
  model: openai('gpt-4-turbo'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Configuring Next.js for OpenTelemetry
DESCRIPTION: JavaScript configuration to enable instrumentationHook in Next.js config file.

LANGUAGE: javascript
CODE:
/** @type {import('next').NextConfig} */
const nextConfig = {
  experimental: {
    instrumentationHook: true,
  },
};

module.exports = nextConfig;

----------------------------------------

TITLE: Handling Client-Side Tool Results with onToolCall in React
DESCRIPTION: Shows how to handle tool results on the client side using the useChat hook's onToolCall callback for processing location data.

LANGUAGE: tsx
CODE:
const { messages } = useChat({
  // Option 1: Handle using onToolCall
  onToolCall: async ({ toolCall }) => {
    if (toolCall.toolName === 'getLocation') {
      const result = await getLocationData();
      return result; // This becomes the tool result
    }
  },
});

----------------------------------------

TITLE: Implementing Server-Side Tool Execution in TypeScript/React
DESCRIPTION: Demonstrates how to create a tool with an execute function for server-side weather data fetching. The tool includes a description and typed parameters using Zod schema validation.

LANGUAGE: tsx
CODE:
const tools = {
  weather: tool({
    description: 'Get the weather in a location',
    parameters: z.object({
      location: z
        .string()
        .describe('The city and state, e.g. "San Francisco, CA"'),
    }),
    execute: async ({ location }) => {
      // Fetch and return weather data
      return { temperature: 72, conditions: 'sunny', location };
    },
  }),
};

----------------------------------------

TITLE: Recording Token Usage with onFinish Callback in TypeScript
DESCRIPTION: Demonstrates how to use the onFinish callback to record token usage when streaming structured data. Uses Zod for schema validation and OpenAI's GPT-4 Turbo model to generate a recipe.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamObject } from 'ai';
import { z } from 'zod';

const result = streamObject({
  model: openai('gpt-4-turbo'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.string()),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a lasagna recipe.',
  onFinish({ usage }) {
    console.log('Token usage:', usage);
  },
});

----------------------------------------

TITLE: Running AI Core Examples
DESCRIPTION: Executes individual AI core examples using the tsx command from the examples/ai-core directory.

LANGUAGE: sh
CODE:
pnpm tsx src/path/to/example.ts

----------------------------------------

TITLE: Enabling Telemetry in AI SDK Request
DESCRIPTION: Basic example of enabling telemetry for an AI SDK text generation request

LANGUAGE: typescript
CODE:
const result = await generateText({
  model: openai('gpt-4o'),
  prompt: 'Write a short story about a cat.',
  experimental_telemetry: { isEnabled: true },
});

----------------------------------------

TITLE: Installing Groq Provider Package
DESCRIPTION: Command to install the Groq provider package using npm package manager.

LANGUAGE: bash
CODE:
npm i @ai-sdk/groq

----------------------------------------

TITLE: Importing AWSBedrockStream in React
DESCRIPTION: This snippet shows how to import the AWSBedrockStream function from the 'ai' package in a React application.

LANGUAGE: javascript
CODE:
import { AWSBedrockStream } from "ai"

----------------------------------------

TITLE: Installing Cohere Provider for AI SDK
DESCRIPTION: This command installs the Cohere provider module for the AI SDK using npm.

LANGUAGE: bash
CODE:
npm i @ai-sdk/cohere

----------------------------------------

TITLE: Increasing Function Duration in Next.js App Router
DESCRIPTION: This snippet shows how to increase the maximum duration of a function to 30 seconds in Next.js with App Router. Add this code to your route file or the page calling your Server Action.

LANGUAGE: tsx
CODE:
export const maxDuration = 30;

----------------------------------------

TITLE: Closing Streams with createStreamableUI in TypeScript React
DESCRIPTION: Demonstrates how to properly close a stream using the createStreamableUI method from the ai/rsc package. The example shows the correct implementation of stream management including updates, appends, and proper closure using the .done() method.

LANGUAGE: tsx
CODE:
import { createStreamableUI } from 'ai/rsc';

const submitMessage = async () => {
  'use server';

  const stream = createStreamableUI('1');

  stream.update('2');
  stream.append('3');
  stream.done('4'); // [!code ++]

  return stream.value;
};

----------------------------------------

TITLE: Converting Messages Format for useChat in TypeScript
DESCRIPTION: Example showing how to convert incoming messages to CoreMessage format using convertToCoreMessages function to resolve the no-response issue with useChat and maxSteps.

LANGUAGE: tsx
CODE:
import { openai } from '@ai-sdk/openai';
import { convertToCoreMessages, streamText } from 'ai';

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    messages: convertToCoreMessages(messages),
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Using createDataStreamResponse Function
DESCRIPTION: Demonstrates creating a streaming response with custom headers, data writing, annotations, and stream merging. Shows error handling and multiple stream operations.

LANGUAGE: tsx
CODE:
const response = createDataStreamResponse({
  status: 200,
  statusText: 'OK',
  headers: {
    'Custom-Header': 'value',
  },
  async execute(dataStream) {
    // Write data
    dataStream.writeData({ value: 'Hello' });

    // Write annotation
    dataStream.writeMessageAnnotation({ type: 'status', value: 'processing' });

    // Merge another stream
    const otherStream = getAnotherStream();
    dataStream.merge(otherStream);
  },
  onError: error => `Custom error: ${error.message}`,
});

----------------------------------------

TITLE: Configuring Azure OpenAI Embedding Model with Options
DESCRIPTION: Demonstrates how to create an Azure OpenAI embedding model with custom settings like dimensions and user identifier.

LANGUAGE: typescript
CODE:
const model = azure.embedding('your-embedding-deployment', {
  dimensions: 512 // optional, number of dimensions for the embedding
  user: 'test-user' // optional unique user identifier
})

----------------------------------------

TITLE: Server-side Stream Generation with createStreamableValue
DESCRIPTION: Demonstrates creating and updating a streamable value on the server side using a server action.

LANGUAGE: typescript
CODE:
async function generate() {
  'use server';
  const streamable = createStreamableValue();

  streamable.update(1);
  streamable.update(2);
  streamable.done(3);

  return streamable.value;
}

----------------------------------------

TITLE: Running Single End-to-End Test File
DESCRIPTION: Executes a specific end-to-end test file for a single provider.

LANGUAGE: sh
CODE:
pnpm run test:file src/e2e/google.test.ts

----------------------------------------

TITLE: Importing Bedrock Provider
DESCRIPTION: Shows how to import the default bedrock provider instance from the Amazon Bedrock package.

LANGUAGE: typescript
CODE:
import { bedrock } from '@ai-sdk/amazon-bedrock';

----------------------------------------

TITLE: Defining CoreToolMessage and ToolContent Types in TypeScript
DESCRIPTION: Defines the structure of a tool message that contains the result of one or more tool calls. It includes a 'role' field set to 'tool' and a 'content' field of type ToolContent.

LANGUAGE: typescript
CODE:
type CoreToolMessage = {
  role: 'tool';
  content: ToolContent;
};

type ToolContent = Array<ToolResultPart>;

----------------------------------------

TITLE: API Route Handler with Tool Integration
DESCRIPTION: Backend implementation of a chat API route that provides weather information through a tool interface.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { createDataStreamResponse, streamText, tool } from 'ai';
import { z } from 'zod';

export async function POST(req: Request) {
  const { messages } = await req.json();

  return createDataStreamResponse({
    execute: async dataStream => {
      const result = streamText({
        model: openai('gpt-4o'),
        messages,
        tools: {
          getWeatherInformation: tool({
            description: 'show the weather in a given city to the user',
            parameters: z.object({ city: z.string() }),
            execute: async ({}: { city: string }) => {
              const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy'];
              return weatherOptions[
                Math.floor(Math.random() * weatherOptions.length)
              ];
            },
          }),
        },
      });

      result.mergeIntoDataStream(dataStream);
    },
  });
}

----------------------------------------

TITLE: Recommending Items based on Context with Crosshatch
DESCRIPTION: Example of using Crosshatch to re-rank items based on recent user purchases and stream the results.

LANGUAGE: typescript
CODE:
import { streamObject } from 'ai';
import createCrosshatch from `@crosshatch/ai-provider`
const crosshatch = createCrosshatch();

const itemSummaries = [...]; // list of items
const ids = (itemSummaries?.map(({ itemId }) => itemId) ?? []) as string[];

const { elementStream } = streamObject({
  output: "array",
  mode: "json",
  model: crosshatch.languageModel("gpt-4o-mini", {
    token,
    replace: {
      "orders": {
        select: ["originalTimestamp", "entity_name", "order_total", "order_summary"],
        from: "personalTimeline",
        where: [{ field: "event", op: "=", value: "purchased" }],
        orderBy: [{ field: "originalTimestamp", dir: "desc" }],
        limit: 5,
      },
    },
  }),
  system: `Rerank the following items based on alignment with users recent purchases {orders}`,
  messages: [{role: "user", content: "Heres a list of item: ${JSON.stringify(itemSummaries)}"},],
  schema: jsonSchema<{ id: string; reason: string }>({
    type: "object",
    properties: {
      id: { type: "string", enum: ids },
      reason: { type: "string", description: "Explain your ranking." },
    },
  }),
})

----------------------------------------

TITLE: Creating Amazon Bedrock Provider Instance
DESCRIPTION: Example of creating a customized Amazon Bedrock provider instance with specific settings.

LANGUAGE: typescript
CODE:
import { createAmazonBedrock } from '@ai-sdk/amazon-bedrock';

const bedrock = createAmazonBedrock({
  region: 'us-east-1',
  accessKeyId: 'xxxxxxxxx',
  secretAccessKey: 'xxxxxxxxx',
  sessionToken: 'xxxxxxxxx',
});

----------------------------------------

TITLE: Initializing LM Studio Provider with OpenAI Compatible API
DESCRIPTION: Creates a custom provider instance for LM Studio using the createOpenAICompatible function. Configures the connection to the local LM Studio server running on the default port 1234.

LANGUAGE: typescript
CODE:
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

const lmstudio = createOpenAICompatible({
  name: 'lmstudio',
  baseURL: 'http://localhost:1234/v1',
});

----------------------------------------

TITLE: Creating Custom Fireworks Provider Instance
DESCRIPTION: Create a customized Fireworks provider instance with specific settings.

LANGUAGE: typescript
CODE:
import { createFireworks } from '@ai-sdk/fireworks';

const fireworks = createFireworks({
  apiKey: process.env.FIREWORKS_API_KEY ?? '',
});

----------------------------------------

TITLE: Installing xAI Grok Provider Package
DESCRIPTION: Commands to install the xAI Grok provider package using different package managers.

LANGUAGE: bash
CODE:
pnpm add @ai-sdk/xai

LANGUAGE: bash
CODE:
npm install @ai-sdk/xai

LANGUAGE: bash
CODE:
yarn add @ai-sdk/xai

----------------------------------------

TITLE: Adding Tool Results via UI Interaction in React
DESCRIPTION: Illustrates how to add tool results through UI elements using the addToolResult function from useChat hook, suitable for interactive confirmation flows.

LANGUAGE: tsx
CODE:
const { messages, addToolResult } = useChat();

// Inside your JSX, when rendering tool calls:
<button
  onClick={() =>
    addToolResult({
      toolCallId, // must provide tool call ID
      result: {
        /* your tool result */
      },
    })
  }
>
  Confirm
</button>;

----------------------------------------

TITLE: Markdown Changelog Entry
DESCRIPTION: Initial release entry for the valibot schema package

LANGUAGE: markdown
CODE:
### Patch Changes
- 704e17f: chore (schema/valibot): release valibot schema

----------------------------------------

TITLE: Configuring Azure OpenAI Completion Model with Options
DESCRIPTION: Shows how to create an Azure OpenAI completion model with custom settings like echo, logit bias, and suffix.

LANGUAGE: typescript
CODE:
const model = azure.completion('your-gpt-35-turbo-instruct-deployment', {
  echo: true, // optional, echo the prompt in addition to the completion
  logitBias: {
    // optional likelihood for specific tokens
    '50256': -100,
  },
  suffix: 'some text', // optional suffix that comes after a completion of inserted text
  user: 'test-user', // optional unique user identifier
});

----------------------------------------

TITLE: Importing useCompletion in Svelte
DESCRIPTION: How to import the useCompletion hook in a Svelte application from the AI SDK

LANGUAGE: javascript
CODE:
import { useCompletion } from '@ai-sdk/svelte'

----------------------------------------

TITLE: Defining CoreSystemMessage Type in TypeScript
DESCRIPTION: Defines the structure of a system message containing system information. It includes a 'role' field set to 'system' and a 'content' field of type string.

LANGUAGE: typescript
CODE:
type CoreSystemMessage = {
  role: 'system';
  content: string;
};

----------------------------------------

TITLE: Implementing Chat Interface with useChat Hook in Next.js
DESCRIPTION: Frontend implementation using the useChat hook to manage message state and user interactions for a basic chatbot interface.

LANGUAGE: tsx
CODE:
'use client';

import { useChat } from '@ai-sdk/react';

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat();

  return (
    <div>
      <div>
        {messages?.map(m => (
          <div key={m.id}>
            <strong>{`${m.role}: `}</strong>
            {m.parts?.map((part, i) => {
              switch (part.type) {
                case 'text':
                  return <div key={i}>{part.text}</div>;
              }
            })}
            <br />
          </div>
        ))}
      </div>

      <form onSubmit={handleSubmit}>
        <input
          value={input}
          placeholder="Say something..."
          onChange={handleInputChange}
        />
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Creating Crosshatch Provider Instance
DESCRIPTION: Example of creating a Crosshatch provider instance in TypeScript.

LANGUAGE: typescript
CODE:
import { createCrosshatch } from '@crosshatch/ai-provider';
const crosshatch = createCrosshatch();

----------------------------------------

TITLE: Using Groq Reasoning Models
DESCRIPTION: Example of using Groq reasoning models with specific formatting options.

LANGUAGE: typescript
CODE:
import { groq } from '@ai-sdk/groq';
import { generateText } from 'ai';

const result = await generateText({
  model: groq('qwen-qwq-32b'),
  providerOptions: {
    groq: { reasoningFormat: 'parsed' },
  },
  prompt: 'How many "r"s are in the word "strawberry"?',
});

----------------------------------------

TITLE: Checking for ToolCallRepairError in TypeScript
DESCRIPTION: This code snippet demonstrates how to check if an error is an instance of ToolCallRepairError using the isInstance method provided by the AI SDK.

LANGUAGE: typescript
CODE:
import { ToolCallRepairError } from 'ai';

if (ToolCallRepairError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Checking AI Tool Execution Error in TypeScript
DESCRIPTION: Demonstrates how to check if an error is an instance of AI_ToolExecutionError using the isInstance static method. This is useful for error handling when working with AI tools.

LANGUAGE: typescript
CODE:
import { ToolExecutionError } from 'ai';

if (ToolExecutionError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Installing Fireworks Provider Package
DESCRIPTION: Commands to install the Fireworks provider package using different package managers.

LANGUAGE: bash
CODE:
pnpm add @ai-sdk/fireworks

LANGUAGE: bash
CODE:
npm install @ai-sdk/fireworks

LANGUAGE: bash
CODE:
yarn add @ai-sdk/fireworks

----------------------------------------

TITLE: Implementing Custom Request Body in Client-Side Chat Component
DESCRIPTION: Client-side implementation showing how to customize the request body using experimental_prepareRequestBody option in useChat hook. This example demonstrates sending only the last message's content instead of the full message history.

LANGUAGE: typescript
CODE:
'use client';

import { useChat } from '@ai-sdk/react';

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat({
    experimental_prepareRequestBody: ({ messages }) => {
      // e.g. only the text of the last message:
      return messages[messages.length - 1].content;
    },
  });

  return (
    <div>
      {messages.map(m => (
        <div key={m.id}>
          {m.role === 'user' ? 'User: ' : 'AI: '}
          {m.content}
        </div>
      ))}

      <form onSubmit={handleSubmit}>
        <input value={input} onChange={handleInputChange} />
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Importing Default xAI Provider Instance
DESCRIPTION: Import the default xAI provider instance from the package.

LANGUAGE: typescript
CODE:
import { xai } from '@ai-sdk/xai';

----------------------------------------

TITLE: Implementation of Chat Component with Message Parts - JavaScript
DESCRIPTION: Shows how to implement a Chat component using the useChat hook with the new message parts system, including rendering different types of content.

LANGUAGE: javascript
CODE:
function Chat() {
  const { messages } = useChat();
  return (
    <div>
      {messages.map(message =>
        message.parts.map((part, i) => {
          switch (part.type) {
            case 'text':
              return <p key={i}>{part.text}</p>;
            case 'source':
              return <p key={i}>{part.source.url}</p>;
            case 'reasoning':
              return <div key={i}>{part.reasoning}</div>;
            case 'tool-invocation':
              return <div key={i}>{part.toolInvocation.toolName}</div>;
            case 'file':
              return (
                <img
                  key={i}
                  src={`data:${part.mimeType};base64,${part.data}`}
                />
              );
          }
        }),
      )}
    </div>
  );
}

----------------------------------------

TITLE: Basic AI Chatbot Implementation
DESCRIPTION: Initial implementation of the AI chatbot with streaming capabilities using the AI SDK.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { CoreMessage, streamText } from 'ai';
import dotenv from 'dotenv';
import * as readline from 'node:readline/promises';

dotenv.config();

const terminal = readline.createInterface({
  input: process.stdin,
  output: process.stdout,
});

const messages: CoreMessage[] = [];

async function main() {
  while (true) {
    const userInput = await terminal.question('You: ');

    messages.push({ role: 'user', content: userInput });

    const result = streamText({
      model: openai('gpt-4o'),
      messages,
    });

    let fullResponse = '';
    process.stdout.write('\nAssistant: ');
    for await (const delta of result.textStream) {
      fullResponse += delta;
      process.stdout.write(delta);
    }
    process.stdout.write('\n\n');

    messages.push({ role: 'assistant', content: fullResponse });
  }
}

main().catch(console.error);

----------------------------------------

TITLE: Public Exports Configuration in TypeScript
DESCRIPTION: Exports the provider creation function and types for public use.

LANGUAGE: typescript
CODE:
export { createExample, example } from './example-provider';
export type {
  ExampleProvider,
  ExampleProviderSettings,
} from './example-provider';

----------------------------------------

TITLE: Importing AIStream in React
DESCRIPTION: This snippet demonstrates how to import the AIStream function from the 'ai' package in a React application.

LANGUAGE: javascript
CODE:
import { AIStream } from "ai"

----------------------------------------

TITLE: Configuring AI Binding in Cloudflare Workers
DESCRIPTION: Setup an AI binding in the Cloudflare Workers project wrangler.toml file.

LANGUAGE: bash
CODE:
[ai]
binding = "AI"

----------------------------------------

TITLE: Basic Slogan Generator Prompt Example
DESCRIPTION: Simple prompt demonstration for generating a coffee shop slogan with minimal instructions

LANGUAGE: markdown
CODE:
Create a slogan for a coffee shop.

----------------------------------------

TITLE: Testing Server Endpoint with Curl
DESCRIPTION: Command to test the HTTP server endpoint using Curl with a POST request

LANGUAGE: sh
CODE:
curl -X POST http://localhost:8080

----------------------------------------

TITLE: Implementing Server-Side Chat API Route Handler
DESCRIPTION: Server-side implementation for handling custom request body in a Next.js API route. Shows how to process streaming responses with OpenAI, load message history, and handle the modified request format that only contains the last message.

LANGUAGE: tsx
CODE:
import { openai } from '@ai-sdk/openai'
import { streamText } from 'ai'

// Allow streaming responses up to 30 seconds
export const maxDuration = 30

export async function POST(req: Request) {
  // we receive only the text from the last message
  const text = await req.json()

  // e.g. load message history from storage
  const history = await loadHistory()

  // Call the language model
  const result = streamText({
    model: openai('gpt-4-turbo'),
    messages: [...history, { role: 'user', content: text }]
    onFinish({ text }) {
      // e.g. save the message and the response to storage
    }
  })

  // Respond with the stream
  return result.toDataStreamResponse()
}

----------------------------------------

TITLE: Nested Span Tracing with Laminar
DESCRIPTION: Implementation of nested span tracing using Laminar's observe wrapper for detailed function monitoring.

LANGUAGE: javascript
CODE:
const result = await observe(
  { name: 'poem writer' },
  async (topic: string, mood: string) => {
    const { text } = await generateText({
      model: openai('gpt-4o-mini'),
      prompt: `Write a poem about ${topic} in ${mood} mood.`,
    });
    return text;
  },
  'Laminar flow',
  'happy',
);

----------------------------------------

TITLE: Creating Embedding Model
DESCRIPTION: Creating a text embedding model using the Voyage provider with the voyage-3 model.

LANGUAGE: typescript
CODE:
import { voyage } from 'voyage-ai-provider';

const embeddingModel = voyage.textEmbeddingModel('voyage-3');

----------------------------------------

TITLE: Checking for AI_NoImageGeneratedError in TypeScript
DESCRIPTION: This snippet demonstrates how to use try-catch to handle the AI_NoImageGeneratedError when generating an image. It shows how to check if the caught error is an instance of NoImageGeneratedError and how to access its properties.

LANGUAGE: typescript
CODE:
import { generateImage, NoImageGeneratedError } from 'ai';

try {
  await generateImage({ model, prompt });
} catch (error) {
  if (NoImageGeneratedError.isInstance(error)) {
    console.log('NoImageGeneratedError');
    console.log('Cause:', error.cause);
    console.log('Responses:', error.responses);
  }
}

----------------------------------------

TITLE: Installing FriendliAI Provider
DESCRIPTION: Package installation commands for different package managers to add the FriendliAI provider to your project.

LANGUAGE: bash
CODE:
pnpm add @friendliai/ai-provider

----------------------------------------

TITLE: Installing OpenAI Provider Package
DESCRIPTION: Commands to install the OpenAI provider package using different package managers.

LANGUAGE: bash
CODE:
pnpm add @ai-sdk/openai

LANGUAGE: bash
CODE:
npm install @ai-sdk/openai

LANGUAGE: bash
CODE:
yarn add @ai-sdk/openai

----------------------------------------

TITLE: Importing and Using generateImage Function in TypeScript
DESCRIPTION: This snippet demonstrates how to import the generateImage function from the ai library and use it to generate multiple images based on a text prompt. It specifies the model, prompt, number of images, and image size.

LANGUAGE: typescript
CODE:
import { experimental_generateImage as generateImage } from 'ai';

const { images } = await generateImage({
  model: openai.image('dall-e-3'),
  prompt: 'A futuristic cityscape at sunset',
  n: 3,
  size: '1024x1024',
});

console.log(images);

----------------------------------------

TITLE: Generating Text with Together.ai Provider in TypeScript
DESCRIPTION: Example of using the Together.ai provider with the AI SDK to generate text using a specified model and prompt.

LANGUAGE: typescript
CODE:
import { togetherai } from '@ai-sdk/togetherai';
import { generateText } from 'ai';

const { text } = await generateText({
  model: togetherai('meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'),
  prompt: 'Write a Python function that sorts a list:',
});

----------------------------------------

TITLE: Main Provider Implementation in TypeScript
DESCRIPTION: Implements the core provider functionality including chat, completion, embedding, and image models with OpenAI compatibility.

LANGUAGE: typescript
CODE:
import { LanguageModelV1, EmbeddingModelV1 } from '@ai-sdk/provider';
import {
  OpenAICompatibleChatLanguageModel,
  OpenAICompatibleCompletionLanguageModel,
  OpenAICompatibleEmbeddingModel,
  OpenAICompatibleImageModel,
} from '@ai-sdk/openai-compatible';
import {
  FetchFunction,
  loadApiKey,
  withoutTrailingSlash,
} from '@ai-sdk/provider-utils';

export interface ExampleProviderSettings {
  apiKey?: string;
  baseURL?: string;
  headers?: Record<string, string>;
  queryParams?: Record<string, string>;
  fetch?: FetchFunction;
}

export interface ExampleProvider {
  (modelId: ExampleChatModelId, settings?: ExampleChatSettings): LanguageModelV1;
  chatModel(modelId: ExampleChatModelId, settings?: ExampleChatSettings): LanguageModelV1;
  completionModel(modelId: ExampleCompletionModelId, settings?: ExampleCompletionSettings): LanguageModelV1;
  textEmbeddingModel(modelId: ExampleEmbeddingModelId, settings?: ExampleEmbeddingSettings): EmbeddingModelV1<string>;
  imageModel(modelId: ExampleImageModelId, settings?: ExampleImageSettings): ImageModelV1;
}

export function createExample(options: ExampleProviderSettings = {}): ExampleProvider {
  const baseURL = withoutTrailingSlash(options.baseURL ?? 'https://api.example.com/v1');
  const getHeaders = () => ({
    Authorization: `Bearer ${loadApiKey({
      apiKey: options.apiKey,
      environmentVariableName: 'EXAMPLE_API_KEY',
      description: 'Example API key',
    })}`,
    ...options.headers,
  });

  // ... rest of implementation

  return provider;
}

----------------------------------------

TITLE: Enhanced Slogan Generator with Examples
DESCRIPTION: Advanced prompt template that includes example inputs and outputs to guide the model in generating context-appropriate slogans for different business types

LANGUAGE: markdown
CODE:
Create three slogans for a business with unique features.

Business: Bookstore with cats
Slogans: "Purr-fect Pages", "Books and Whiskers", "Novels and Nuzzles"
Business: Gym with rock climbing
Slogans: "Peak Performance", "Reach New Heights", "Climb Your Way Fit"
Business: Coffee shop with live music
Slogans:

----------------------------------------

TITLE: Creating Custom Voyage Provider
DESCRIPTION: Creating a customized Voyage provider instance with specific configuration options including base URL and API key.

LANGUAGE: typescript
CODE:
import { createVoyage } from 'voyage-ai-provider';

const voyage = createVoyage({
  baseURL: 'https://api.voyageai.com/v1',
  apiKey: process.env.VOYAGE_API_KEY,
});

----------------------------------------

TITLE: Checking AI_TooManyEmbeddingValuesForCallError in TypeScript
DESCRIPTION: Demonstrates how to check if an error is an instance of TooManyEmbeddingValuesForCallError using the isInstance method. This code helps identify when too many embedding values are provided in a single call.

LANGUAGE: typescript
CODE:
import { TooManyEmbeddingValuesForCallError } from 'ai';

if (TooManyEmbeddingValuesForCallError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Implementing Server-Side Chat API with OpenAI Integration
DESCRIPTION: Server-side implementation that handles chat requests, defines tools for weather information, user confirmation, and location services. Uses OpenAI's GPT-4 model and implements custom tool definitions with Zod schema validation.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import { z } from 'zod';

export default async function POST(request: Request) {
  const { messages } = await request.json();

  const result = streamText({
    model: openai('gpt-4-turbo'),
    messages,
    tools: {
      // server-side tool with execute function:
      getWeatherInformation: {
        description: 'show the weather in a given city to the user',
        parameters: z.object({ city: z.string() }),
        execute: async ({}: { city: string }) => {
          return {
            value: 24,
            unit: 'celsius',
            weeklyForecast: [
              { day: 'Monday', value: 24 },
              { day: 'Tuesday', value: 25 },
              { day: 'Wednesday', value: 26 },
              { day: 'Thursday', value: 27 },
              { day: 'Friday', value: 28 },
              { day: 'Saturday', value: 29 },
              { day: 'Sunday', value: 30 },
            ],
          };
        },
      },
      // client-side tool that starts user interaction:
      askForConfirmation: {
        description: 'Ask the user for confirmation.',
        parameters: z.object({
          message: z.string().describe('The message to ask for confirmation.'),
        }),
      },
      // client-side tool that is automatically executed on the client:
      getLocation: {
        description:
          'Get the user location. Always ask for confirmation before using this tool.',
        parameters: z.object({}),
      },
    },
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Installing Voyage AI Provider
DESCRIPTION: Package installation commands for different package managers (pnpm, npm, yarn).

LANGUAGE: bash
CODE:
pnpm add voyage-ai-provider

LANGUAGE: bash
CODE:
npm install voyage-ai-provider

LANGUAGE: bash
CODE:
yarn add voyage-ai-provider

----------------------------------------

TITLE: Creating AnthropicVertex Language Model in TypeScript
DESCRIPTION: Create a language model instance using the AnthropicVertex provider.

LANGUAGE: typescript
CODE:
const model = anthropicVertex('claude-3-sonnet@20240229');

----------------------------------------

TITLE: Creating Baseten Provider Instance
DESCRIPTION: Demonstrates how to create a custom provider instance for Baseten using the createOpenAICompatible function. It includes setting up the API key, base URL, and custom fetch function to include Baseten-specific payload.

LANGUAGE: typescript
CODE:
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

const BASETEN_MODEL_ID = '<deployment-id>';
const BASETEN_DEPLOYMENT_ID = null;

// see https://docs.baseten.co/api-reference/openai for more information
const basetenExtraPayload = {
  model_id: BASETEN_MODEL_ID,
  deployment_id: BASETEN_DEPLOYMENT_ID,
};

const baseten = createOpenAICompatible({
  name: 'baseten',
  apiKey: process.env.BASETEN_API_KEY,
  baseURL: 'https://bridge.baseten.co/v1/direct',
  fetch: async (url, request) => {
    const bodyWithBasetenPayload = JSON.stringify({
      ...JSON.parse(request.body),
      baseten: basetenExtraPayload,
    });
    return await fetch(url, { ...request, body: bodyWithBasetenPayload });
  },
});

----------------------------------------

TITLE: Installing Perplexity Provider via NPM
DESCRIPTION: Command to install the Perplexity provider package using npm package manager.

LANGUAGE: bash
CODE:
npm i @ai-sdk/perplexity

----------------------------------------

TITLE: Creating Qwen Text Embedding Model
DESCRIPTION: Create a Qwen text embedding model using the textEmbeddingModel factory method.

LANGUAGE: typescript
CODE:
const model = qwen.textEmbeddingModel('text-embedding-v3');

----------------------------------------

TITLE: Implementing Caching Middleware for Language Model
DESCRIPTION: This snippet demonstrates the implementation of a LanguageModelMiddleware that caches assistant responses in KV storage. It includes methods for both generate and stream operations, using Redis for caching.

LANGUAGE: typescript
CODE:
import { Redis } from '@upstash/redis';
import {
  type LanguageModelV1,
  type LanguageModelV1Middleware,
  type LanguageModelV1StreamPart,
  simulateReadableStream,
} from 'ai';

const redis = new Redis({
  url: process.env.KV_URL,
  token: process.env.KV_TOKEN,
});

export const cacheMiddleware: LanguageModelV1Middleware = {
  wrapGenerate: async ({ doGenerate, params }) => {
    const cacheKey = JSON.stringify(params);

    const cached = (await redis.get(cacheKey)) as Awaited<
      ReturnType<LanguageModelV1['doGenerate']>
    > | null;

    if (cached !== null) {
      return {
        ...cached,
        response: {
          ...cached.response,
          timestamp: cached?.response?.timestamp
            ? new Date(cached?.response?.timestamp)
            : undefined,
        },
      };
    }

    const result = await doGenerate();

    redis.set(cacheKey, result);

    return result;
  },
  wrapStream: async ({ doStream, params }) => {
    const cacheKey = JSON.stringify(params);

    // Check if the result is in the cache
    const cached = await redis.get(cacheKey);

    // If cached, return a simulated ReadableStream that yields the cached result
    if (cached !== null) {
      // Format the timestamps in the cached response
      const formattedChunks = (cached as LanguageModelV1StreamPart[]).map(p => {
        if (p.type === 'response-metadata' && p.timestamp) {
          return { ...p, timestamp: new Date(p.timestamp) };
        } else return p;
      });
      return {
        stream: simulateReadableStream({
          initialDelayInMs: 0,
          chunkDelayInMs: 10,
          chunks: formattedChunks,
        }),
        rawCall: { rawPrompt: null, rawSettings: {} },
      };
    }

    // If not cached, proceed with streaming
    const { stream, ...rest } = await doStream();

    const fullResponse: LanguageModelV1StreamPart[] = [];

    const transformStream = new TransformStream<
      LanguageModelV1StreamPart,
      LanguageModelV1StreamPart
    >({
      transform(chunk, controller) {
        fullResponse.push(chunk);
        controller.enqueue(chunk);
      },
      flush() {
        // Store the full response in the cache after streaming is complete
        redis.set(cacheKey, fullResponse);
      },
    });

    return {
      stream: stream.pipeThrough(transformStream),
      ...rest,
    };
  },
};

----------------------------------------

TITLE: Configuring Provider-Specific Options for Azure OpenAI
DESCRIPTION: Demonstrates how to set provider-specific options when using Azure OpenAI language models.

LANGUAGE: typescript
CODE:
const messages = [
  {
    role: 'user',
    content: [
      {
        type: 'text',
        text: 'What is the capital of the moon?',
      },
      {
        type: 'image',
        image: 'https://example.com/image.png',
        providerOptions: {
          openai: { imageDetail: 'low' },
        },
      },
    ],
  },
];

const { text } = await generateText({
  model: azure('your-deployment-name'),
  providerOptions: {
    openai: {
      reasoningEffort: 'low',
    },
  },
});

----------------------------------------

TITLE: Implementing Eager Stream Generation in JavaScript
DESCRIPTION: This snippet demonstrates an eager approach to stream generation using an async generator and ReadableStream. It illustrates potential issues with back-pressure and cancellation.

LANGUAGE: jsx
CODE:
// A generator that will yield positive integers
async function* integers() {
  let i = 1;
  while (true) {
    console.log(`yielding ${i}`);
    yield i++;

    await sleep(100);
  }
}
function sleep(ms) {
  return new Promise(resolve => setTimeout(resolve, ms));
}

// Wraps a generator into a ReadableStream
function createStream(iterator) {
  return new ReadableStream({
    async start(controller) {
      for await (const v of iterator) {
        controller.enqueue(v);
      }
      controller.close();
    },
  });
}

// Collect data from stream
async function run() {
  // Set up a stream of integers
  const stream = createStream(integers());

  // Read values from our stream
  const reader = stream.getReader();
  for (let i = 0; i < 10_000; i++) {
    // we know our stream is infinite, so there's no need to check `done`.
    const { value } = await reader.read();
    console.log(`read ${value}`);

    await sleep(1_000);
  }
}
run();

----------------------------------------

TITLE: Defining CoreAssistantMessage and AssistantContent Types in TypeScript
DESCRIPTION: Defines the structure of an assistant message that can contain text, tool calls, or a combination of both. It includes a 'role' field set to 'assistant' and a 'content' field of type AssistantContent.

LANGUAGE: typescript
CODE:
type CoreAssistantMessage = {
  role: 'assistant';
  content: AssistantContent;
};

type AssistantContent = string | Array<TextPart | ToolCallPart>;

----------------------------------------

TITLE: Chat Interface Implementation - Client Side
DESCRIPTION: Client-side chat interface implementation using useChat hook

LANGUAGE: tsx
CODE:
'use client';

import { useChat } from '@ai-sdk/react';

export default function Page() {
  const { messages, input, setInput, handleSubmit } = useChat();

  return (
    <div>
      {messages.map(message => (
        <div key={message.id}>
          <div>{message.role}</div>
          <div>{message.content}</div>
        </div>
      ))}

      <form onSubmit={handleSubmit}>
        <input
          type="text"
          value={input}
          onChange={event => {
            setInput(event.target.value);
          }}
        />
        <button type="submit">Send</button>
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Message Prompts with Chat Interface
DESCRIPTION: Example of using message prompts for a chat interface with user and assistant roles.

LANGUAGE: typescript
CODE:
const result = await streamUI({
  model: yourModel,
  messages: [
    { role: 'user', content: 'Hi!' },
    { role: 'assistant', content: 'Hello, how can I help?' },
    { role: 'user', content: 'Where can I buy the best Currywurst in Berlin?' },
  ],
});

----------------------------------------

TITLE: Generating Text with Google AI
DESCRIPTION: Example of using generateText function with Google AI model

LANGUAGE: typescript
CODE:
import { google } from '@ai-sdk/google';
import { generateText } from 'ai';

const { text } = await generateText({
  model: google('gemini-1.5-pro-latest'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Creating Embedding Model with Custom Settings in TypeScript
DESCRIPTION: This snippet demonstrates how to create an embedding model with custom settings using the Mixedbread provider. It allows specifying a prompt and custom dimensions for the embeddings.

LANGUAGE: typescript
CODE:
import { mixedbread } from 'mixedbread-ai-provider';

const embeddingModel = mixedbread.textEmbeddingModel(
  'mixedbread-ai/mxbai-embed-large-v1',
  {
    prompt: 'Generate embeddings for text', // Max 256 characters
    dimensions: 512, // Max 1024 for embed-large-v1
  },
);

----------------------------------------

TITLE: Rendering Index Cards Component with AI SDK Guides
DESCRIPTION: JSX component that renders a grid of documentation guide cards, each linking to specific AI SDK tutorials. The component takes an array of cards with title, description, and href properties.

LANGUAGE: jsx
CODE:
<IndexCards
  cards={[
    {
      title: 'RAG Chatbot',
      description:
        'Learn how to build a retrieval-augmented generation chatbot with the AI SDK.',
      href: '/docs/guides/rag-chatbot',
    },
    {
      title: 'Multimodal Chatbot',
      description: 'Learn how to build a multimodal chatbot with the AI SDK.',
      href: '/docs/guides/multi-modal-chatbot',
    },
    {
      title: 'Get started with Llama 3.1',
      description: 'Get started with Llama 3.1 using the AI SDK.',
      href: '/docs/guides/llama-3_1',
    },
    {
      title: 'Get started with OpenAI o1',
      description: 'Get started with OpenAI o1 using the AI SDK.',
      href: '/docs/guides/o1',
    },
  ]}
/>

----------------------------------------

TITLE: Implementing MCP Tools Client-Side with React Component
DESCRIPTION: This code snippet demonstrates how to create a React component that uses the useCompletion hook from the AI SDK to interact with the MCP tools API endpoint. It allows users to trigger text generation with a button click.

LANGUAGE: tsx
CODE:
'use client';

import { useCompletion } from '@ai-sdk/react';

export default function Page() {
  const { completion, complete } = useCompletion({
    api: '/api/completion',
  });

  return (
    <div>
      <div
        onClick={async () => {
          await complete(
            'Please schedule a call with Sonny and Robby for tomorrow at 10am ET for me!',
          );
        }}
      >
        Schedule a call
      </div>

      {completion}
    </div>
  );
}

----------------------------------------

TITLE: Using Amazon Bedrock Guardrails
DESCRIPTION: Example of using Amazon Bedrock Guardrails with a language model.

LANGUAGE: typescript
CODE:
const result = await generateText({
  bedrock('anthropic.claude-3-sonnet-20240229-v1:0'),
  providerOptions: {
    bedrock: {
      guardrailConfig: {
        guardrailIdentifier: '1abcd2ef34gh',
        guardrailVersion: '1',
        trace: 'enabled' as const,
        streamProcessingMode: 'async',
      },
    },
  },
});

----------------------------------------

TITLE: Creating OpenAI Completion Model
DESCRIPTION: Example of creating an OpenAI completion model with specific settings.

LANGUAGE: typescript
CODE:
const model = openai.completion('gpt-3.5-turbo-instruct', {
  echo: true, // optional, echo the prompt in addition to the completion
  logitBias: {
    // optional likelihood for specific tokens
    '50256': -100,
  },
  suffix: 'some text', // optional suffix that comes after a completion of inserted text
  user: 'test-user', // optional unique user identifier
});

----------------------------------------

TITLE: Migrating from ExperimentalTool
DESCRIPTION: Example of updating from ExperimentalTool to CoreTool.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
import { ExperimentalTool } from 'ai';

// AI SDK 4.0
import { CoreTool } from 'ai';

----------------------------------------

TITLE: Initializing Mistral Language Model - TypeScript
DESCRIPTION: Creates a Mistral language model instance with optional safety settings.

LANGUAGE: typescript
CODE:
const model = mistral('mistral-large-latest', {
  safePrompt: true, // optional safety prompt injection
});

----------------------------------------

TITLE: Creating Azure OpenAI Embedding Model
DESCRIPTION: Illustrates how to create an embedding model using the Azure OpenAI provider.

LANGUAGE: typescript
CODE:
const model = azure.embedding('your-embedding-deployment');

----------------------------------------

TITLE: Handling Streamed Loading State in Next.js Client Component
DESCRIPTION: This client-side code snippet shows how to handle both the streamed response and loading state in a Next.js client component. It updates the UI based on the received data and loading state.

LANGUAGE: tsx
CODE:
'use client';

import { useState } from 'react';
import { generateResponse } from './actions';
import { readStreamableValue } from 'ai/rsc';

// Force the page to be dynamic and allow streaming responses up to 30 seconds
export const maxDuration = 30;

export default function Home() {
  const [input, setInput] = useState<string>('');
  const [generation, setGeneration] = useState<string>('');
  const [loading, setLoading] = useState<boolean>(false);

  return (
    <div>
      <div>{generation}</div>
      <form
        onSubmit={async e => {
          e.preventDefault();
          setLoading(true);
          const { response, loadingState } = await generateResponse(input);

          let textContent = '';

          for await (const responseDelta of readStreamableValue(response)) {
            textContent = `${textContent}${responseDelta}`;
            setGeneration(textContent);
          }
          for await (const loadingDelta of readStreamableValue(loadingState)) {
            if (loadingDelta) {
              setLoading(loadingDelta.loading);
            }
          }
          setInput('');
          setLoading(false);
        }}
      >
        <input
          type="text"
          value={input}
          disabled={loading}
          className="disabled:opacity-50"
          onChange={event => {
            setInput(event.target.value);
          }}
        />
        <button>Send Message</button>
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Generating Text with OpenAI GPT-4.5 using AI SDK Core
DESCRIPTION: This snippet demonstrates how to use the AI SDK Core to generate text using OpenAI's GPT-4.5 model. It imports the necessary functions and uses the generateText function to create a response based on a given prompt.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const { text } = await generateText({
  model: openai('gpt-4.5-preview'),
  prompt: 'Explain the concept of quantum entanglement.',
});

----------------------------------------

TITLE: Error Handling in Image Generation
DESCRIPTION: Shows how to handle errors that may occur during the image generation process.

LANGUAGE: ts
CODE:
import { generateImage, NoImageGeneratedError } from 'ai';

try {
  await generateImage({ model, prompt });
} catch (error) {
  if (NoImageGeneratedError.isInstance(error)) {
    console.log('NoImageGeneratedError');
    console.log('Cause:', error.cause);
    console.log('Responses:', error.responses);
  }
}

----------------------------------------

TITLE: Installing and Building AI SDK for Express.js Integration
DESCRIPTION: Runs commands to install dependencies and build the AI SDK project. These steps should be executed from the root directory of the AI SDK repository.

LANGUAGE: sh
CODE:
pnpm install
pnpm build

----------------------------------------

TITLE: Setting up Environment Variables for AI SDK
DESCRIPTION: Configuration of environment variables required for AI SDK, specifically the OpenAI API key.

LANGUAGE: sh
CODE:
OPENAI_API_KEY="YOUR_OPENAI_API_KEY"

----------------------------------------

TITLE: Implementing Server-Side Text Streaming with AI SDK and OpenAI
DESCRIPTION: This server-side function handles text generation using the AI SDK and OpenAI. It creates a streamable value that can be updated and sent to the client in real-time using the `createStreamableValue` function.

LANGUAGE: typescript
CODE:
'use server';

import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { createStreamableValue } from 'ai/rsc';

export async function generate(input: string) {
  const stream = createStreamableValue('');

  (async () => {
    const { textStream } = streamText({
      model: openai('gpt-3.5-turbo'),
      prompt: input,
    });

    for await (const delta of textStream) {
      stream.update(delta);
    }

    stream.done();
  })();

  return { output: stream.value };
}

----------------------------------------

TITLE: Installing Inflection AI SDK Provider with npm
DESCRIPTION: Command to install the unofficial Inflection AI provider package using npm.

LANGUAGE: bash
CODE:
npm i inflection-ai-sdk-provider

----------------------------------------

TITLE: Generating Text with Portkey and AI SDK in JavaScript
DESCRIPTION: Demonstrates how to use the Portkey chat model with the generateText function from the AI SDK to generate text based on a prompt.

LANGUAGE: javascript
CODE:
import { createPortkey } from '@portkey-ai/vercel-provider';
import { generateText } from 'ai';

const portkey = createPortkey({
  apiKey: 'YOUR_PORTKEY_API_KEY',
  config: portkeyConfig,
});

const { text } = await generateText({
  model: portkey.chatModel(''),
  prompt: 'What is Portkey?',
});

console.log(text);

----------------------------------------

TITLE: Migrating Experimental useAssistant
DESCRIPTION: Example of updating from experimental_useAssistant to useAssistant.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
import { experimental_useAssistant } from '@ai-sdk/react';

// AI SDK 4.0
import { useAssistant } from '@ai-sdk/react';

----------------------------------------

TITLE: Setting OpenAI API Key Configuration
DESCRIPTION: Environment configuration file to store the OpenAI API key securely

LANGUAGE: env
CODE:
OPENAI_API_KEY=xxxxxxxxx

----------------------------------------

TITLE: Initializing Language Model Wrapper in TypeScript
DESCRIPTION: Example showing how to import and use the wrapLanguageModel function to wrap a language model with middleware. This creates an enhanced model instance with additional middleware functionality.

LANGUAGE: typescript
CODE:
import { wrapLanguageModel } from 'ai';

const wrappedLanguageModel = wrapLanguageModel({
  model: yourModel,
  middleware: yourLanguageModelMiddleware,
});

----------------------------------------

TITLE: Generating Text with OpenRouter
DESCRIPTION: Example of using the generateText function with OpenRouter to generate AI responses.

LANGUAGE: javascript
CODE:
import { createOpenRouter } from '@openrouter/ai-sdk-provider';
import { generateText } from 'ai';

const openrouter = createOpenRouter({
  apiKey: 'YOUR_OPENROUTER_API_KEY',
});

const { text } = await generateText({
  model: openrouter.chatModel('anthropic/claude-3.5-sonnet'),
  prompt: 'What is OpenRouter?',
});

console.log(text);

----------------------------------------

TITLE: Creating React Component for Generating Notifications (TypeScript/React)
DESCRIPTION: This client-side component uses React hooks to manage state and calls a server action to generate notifications. It displays the generated notifications as a JSON string.

LANGUAGE: tsx
CODE:
'use client';

import { useState } from 'react';
import { getNotifications } from './actions';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export default function Home() {
  const [generation, setGeneration] = useState<string>('');

  return (
    <div>
      <button
        onClick={async () => {
          const { notifications } = await getNotifications(
            'Messages during finals week.'
          );

          setGeneration(JSON.stringify(notifications, null, 2));
        }}
      >
        View Notifications
      </button>

      <pre>{generation}</pre>
    </div>
  );
}

----------------------------------------

TITLE: Importing Anthropic Provider
DESCRIPTION: Basic usage example showing how to import the default Anthropic provider instance.

LANGUAGE: typescript
CODE:
import { anthropic } from '@ai-sdk/anthropic';

----------------------------------------

TITLE: Implementing Error Boundary for Streamed UI Components in React
DESCRIPTION: Shows how to implement client-side error handling for streamed UI components using React Error Boundary and useState hook.

LANGUAGE: tsx
CODE:
import { getStreamedUI } from '@/actions';
import { useState } from 'react';
import { ErrorBoundary } from './ErrorBoundary';

export default function Page() {
  const [streamedUI, setStreamedUI] = useState(null);

  return (
    <div>
      <button
        onClick={async () => {
          const newUI = await getStreamedUI();
          setStreamedUI(newUI);
        }}
      >
        What does the new UI look like?
      </button>
      <ErrorBoundary>{streamedUI}</ErrorBoundary>
    </div>
  );
}

----------------------------------------

TITLE: Installing Azure Provider Package
DESCRIPTION: Command to install the Azure provider package for AI SDK using npm

LANGUAGE: bash
CODE:
npm i @ai-sdk/azure

----------------------------------------

TITLE: Overriding Default Batch Size for Image Generation
DESCRIPTION: Shows how to override the default batch size when generating multiple images.

LANGUAGE: tsx
CODE:
const model = openai.image('dall-e-2', {
  maxImagesPerCall: 5, // Override the default batch size
});

const { images } = await generateImage({
  model,
  prompt: 'Santa Claus driving a Cadillac',
  n: 10, // Will make 2 calls of 5 images each
});

----------------------------------------

TITLE: User Message with Text Parts
DESCRIPTION: Shows how to structure a user message with explicit text content parts.

LANGUAGE: typescript
CODE:
const result = await generateText({
  model: yourModel,
  messages: [
    {
      role: 'user',
      content: [
        {
          type: 'text',
          text: 'Where can I buy the best Currywurst in Berlin?',
        },
      ],
    },
  ],
});

----------------------------------------

TITLE: Installing Portkey Provider for Vercel AI SDK
DESCRIPTION: Instructions for installing the Portkey provider using different package managers.

LANGUAGE: shell
CODE:
pnpm add @portkey-ai/vercel-provider

LANGUAGE: shell
CODE:
npm install @portkey-ai/vercel-provider

LANGUAGE: shell
CODE:
yarn add @portkey-ai/vercel-provider

----------------------------------------

TITLE: Installing DeepInfra Provider Package
DESCRIPTION: Command to install the DeepInfra provider package using npm package manager.

LANGUAGE: bash
CODE:
npm i @ai-sdk/deepinfra

----------------------------------------

TITLE: Configuring AI Actions for React Server Components
DESCRIPTION: This snippet creates an AI configuration object using the createAI function from the AI SDK, defining available actions and initial state.

LANGUAGE: typescript
CODE:
import { createAI } from 'ai/rsc';
import { submitMessage } from './actions';

export const AI = createAI({
  actions: {
    submitMessage,
  },
  initialAIState: [],
  initialUIState: [],
});

----------------------------------------

TITLE: Using DeepSeek Reasoning Model - TypeScript
DESCRIPTION: Demonstrates how to use DeepSeek's reasoning model to get both text output and reasoning information.

LANGUAGE: typescript
CODE:
import { deepseek } from '@ai-sdk/deepseek';
import { generateText } from 'ai';

const { text, reasoning } = await generateText({
  model: deepseek('deepseek-reasoner'),
  prompt: 'How many people will live in the world in 2040?',
});

console.log(reasoning);
console.log(text);

----------------------------------------

TITLE: Installing Together.ai Provider Package
DESCRIPTION: Instructions for installing the Together.ai provider package using different package managers.

LANGUAGE: bash
CODE:
pnpm add @ai-sdk/togetherai

LANGUAGE: bash
CODE:
npm install @ai-sdk/togetherai

LANGUAGE: bash
CODE:
yarn add @ai-sdk/togetherai

----------------------------------------

TITLE: Importing and Using extractReasoningMiddleware in TypeScript
DESCRIPTION: Example showing how to import and initialize the extractReasoningMiddleware with custom configuration options. The middleware extracts reasoning sections marked with XML tags from generated text.

LANGUAGE: typescript
CODE:
import { extractReasoningMiddleware } from 'ai';

const middleware = extractReasoningMiddleware({
  tagName: 'reasoning',
  separator: '\n',
});

----------------------------------------

TITLE: Migrating isXXXError Static Methods
DESCRIPTION: Example of updating from isXXXError static methods to isInstance method.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
import { APICallError } from 'ai';

APICallError.isAPICallError(error);

// AI SDK 4.0
import { APICallError } from 'ai';

APICallError.isInstance(error);

----------------------------------------

TITLE: Updating API Route to Include Custom Tools
DESCRIPTION: This code updates the API route to include the custom tools defined earlier, allowing the model to use them in responses.

LANGUAGE: ts
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import { tools } from '@/ai/tools';

export async function POST(request: Request) {
  const { messages } = await request.json();

  const result = streamText({
    model: openai('gpt-4o'),
    system: 'You are a friendly assistant!',
    messages,
    maxSteps: 5,
    tools,
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Generating Image with Fal Model in TypeScript
DESCRIPTION: Illustrates how to use the Fal image model to generate an image based on a prompt and save it to a file. This example uses the experimental_generateImage function from the 'ai' package.

LANGUAGE: typescript
CODE:
import { fal } from '@ai-sdk/fal';
import { experimental_generateImage as generateImage } from 'ai';
import fs from 'fs';

const { image } = await generateImage({
  model: fal.image('fal-ai/fast-sdxl'),
  prompt: 'A serene mountain landscape at sunset',
});

const filename = `image-${Date.now()}.png`;
fs.writeFileSync(filename, image.uint8Array);
console.log(`Image saved to ${filename}`);

----------------------------------------

TITLE: Customizing UI with Loading State in React
DESCRIPTION: This React component demonstrates how to use the isLoading state from useObject to show a loading spinner and disable the submit button while notifications are being generated.

LANGUAGE: tsx
CODE:
'use client';

import { useObject } from '@ai-sdk/react';

export default function Page() {
  const { isLoading, object, submit } = useObject({
    api: '/api/notifications',
    schema: notificationSchema,
  });

  return (
    <>
      {isLoading && <Spinner />}

      <button
        onClick={() => submit('Messages during finals week.')}
        disabled={isLoading}
      >
        Generate notifications
      </button>

      {object?.notifications?.map((notification, index) => (
        <div key={index}>
          <p>{notification?.name}</p>
          <p>{notification?.message}</p>
        </div>
      ))}
    </>
  );
}

----------------------------------------

TITLE: Importing useAIState from AI SDK RSC in JavaScript
DESCRIPTION: This code snippet demonstrates how to import the useAIState hook from the AI SDK RSC package. It's used to access and manipulate the global AI state within a React component.

LANGUAGE: javascript
CODE:
import { useAIState } from "ai/rsc"

----------------------------------------

TITLE: Inspecting AI SDK Warning Messages
DESCRIPTION: Shows how to access and inspect warning messages from provider responses to check for feature support and handling issues.

LANGUAGE: typescript
CODE:
const result = await generateText({
  model: openai('gpt-4o'),
  prompt: 'Hello, world!'
});

console.log(result.warnings);

----------------------------------------

TITLE: Migrating from nanoid to generateId
DESCRIPTION: Example of updating from nanoid to generateId.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
import { nanoid } from 'ai';

// AI SDK 4.0
import { generateId } from 'ai';

----------------------------------------

TITLE: Wrapping Layout with AI Component for React Server Components
DESCRIPTION: This layout component wraps its children with the AI component to provide AI context throughout the application.

LANGUAGE: tsx
CODE:
import { ReactNode } from 'react';
import { AI } from './ai';

export default function Layout({ children }: { children: ReactNode }) {
  return <AI>{children}</AI>;
}

----------------------------------------

TITLE: Configuring Reasoning Effort for o3-mini
DESCRIPTION: Shows how to adjust the reasoning effort parameter for o3-mini to control the trade-off between response speed and reasoning depth.

LANGUAGE: tsx
CODE:
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

// Reduce reasoning effort for faster responses
const { text } = await generateText({
  model: openai('o3-mini'),
  prompt: 'Explain quantum entanglement briefly.',
  providerOptions: {
    openai: { reasoningEffort: 'low' },
  },
});

----------------------------------------

TITLE: Creating xAI Language Model Instance
DESCRIPTION: Initialize an xAI language model using a provider instance.

LANGUAGE: typescript
CODE:
const model = xai('grok-beta');

----------------------------------------

TITLE: Basic Image Generation with Replicate
DESCRIPTION: Example of generating an image using the Replicate provider with basic prompt configuration. Shows how to save the generated image to a file.

LANGUAGE: typescript
CODE:
import { replicate } from '@ai-sdk/replicate';
import { experimental_generateImage as generateImage } from 'ai';

const { image } = await generateImage({
  model: replicate.image('black-forest-labs/flux-schnell'),
  prompt: 'The Loch Ness Monster getting a manicure',
});

const filename = `image-${Date.now()}.png`;
fs.writeFileSync(filename, image.uint8Array);
console.log(`Image saved to ${filename}`);

----------------------------------------

TITLE: Migrating from Anthropic Facade
DESCRIPTION: Example of updating from the Anthropic facade to the createAnthropic function.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
const anthropic = new Anthropic({
  // ...
});

// AI SDK 4.0
const anthropic = createAnthropic({
  // ...
});

----------------------------------------

TITLE: Migrating Experimental Message Types
DESCRIPTION: Example of updating from experimental message types to core message types.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
import {
  ExperimentalMessage,
  ExperimentalUserMessage,
  ExperimentalAssistantMessage,
  ExperimentalToolMessage,
} from 'ai';

// AI SDK 4.0
import {
  CoreMessage,
  CoreUserMessage,
  CoreAssistantMessage,
  CoreToolMessage,
} from 'ai';

----------------------------------------

TITLE: Migrating Experimental StreamData
DESCRIPTION: Example of updating from experimental_StreamData to StreamData.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
import { experimental_StreamData } from 'ai';

// AI SDK 4.0
import { StreamData } from 'ai';

----------------------------------------

TITLE: Server-Side Message Handler with OpenAI Integration
DESCRIPTION: Server action that manages message submission, thread creation, and handles OpenAI Assistant API interactions including tool calls for email searching functionality.

LANGUAGE: tsx
CODE:
'use server';

import { generateId } from 'ai';
import { createStreamableUI, createStreamableValue } from 'ai/rsc';
import { OpenAI } from 'openai';
import { ReactNode } from 'react';
import { searchEmails } from './function';
import { Message } from './message';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

export interface ClientMessage {
  id: string;
  status: ReactNode;
  text: ReactNode;
  gui: ReactNode;
}

const ASSISTANT_ID = 'asst_xxxx';
let THREAD_ID = '';
let RUN_ID = '';

export async function submitMessage(question: string): Promise<ClientMessage> {
  // ... [rest of the implementation]

----------------------------------------

TITLE: Importing Default DeepInfra Provider Instance
DESCRIPTION: Demonstrates how to import the default DeepInfra provider instance.

LANGUAGE: typescript
CODE:
import { deepinfra } from '@ai-sdk/deepinfra';

----------------------------------------

TITLE: Chat Store Implementation
DESCRIPTION: File-based chat store implementation that handles chat creation and file management. Uses the filesystem to store chat messages in JSON format.

LANGUAGE: tsx
CODE:
import { generateId } from 'ai';
import { existsSync, mkdirSync } from 'fs';
import { writeFile } from 'fs/promises';
import path from 'path';

export async function createChat(): Promise<string> {
  const id = generateId(); // generate a unique chat ID
  await writeFile(getChatFile(id), '[]'); // create an empty chat file
  return id;
}

function getChatFile(id: string): string {
  const chatDir = path.join(process.cwd(), '.chats');
  if (!existsSync(chatDir)) mkdirSync(chatDir, { recursive: true });
  return path.join(chatDir, `${id}.json`);
}

----------------------------------------

TITLE: Importing Render Function from AI SDK RSC
DESCRIPTION: Example of how to import the (now removed) render function from the AI SDK RSC package.

LANGUAGE: javascript
CODE:
import { render } from "ai/rsc"

----------------------------------------

TITLE: System Prompt with Text Prompt
DESCRIPTION: Shows how to combine system and text prompts to guide the model's behavior for travel planning.

LANGUAGE: typescript
CODE:
const result = await generateText({
  model: yourModel,
  system:
    `You help planning travel itineraries. ` +
    `Respond to the users' request with a list ` +
    `of the best stops to make in their destination.`,
  prompt:
    `I am planning a trip to ${destination} for ${lengthOfStay} days. ` +
    `Please suggest the best tourist activities for me to do.`,
});

----------------------------------------

TITLE: Customizing Headers for OpenAI Compatible Provider in TypeScript
DESCRIPTION: This snippet illustrates how to customize headers when using the OpenAI-compatible provider, specifically for API key authentication.

LANGUAGE: typescript
CODE:
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
import { generateText } from 'ai';

const { text } = await generateText({
  model: createOpenAICompatible({
    baseURL: 'https://api.example.com/v1',
    name: 'example',
    headers: {
      Authorization: `Bearer ${process.env.MY_API_KEY}`,
    },
  }).chatModel('meta-llama/Llama-3-70b-chat-hf'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Creating Azure OpenAI Chat Model with Custom Options
DESCRIPTION: Shows how to create an Azure OpenAI chat model with custom settings like logit bias and user identifier.

LANGUAGE: typescript
CODE:
const model = azure('your-deployment-name', {
  logitBias: {
    // optional likelihood for specific tokens
    '50256': -100,
  },
  user: 'test-user', // optional unique user identifier
});

----------------------------------------

TITLE: Installing AI SDK via npm
DESCRIPTION: Command to install the AI SDK using npm package manager.

LANGUAGE: shell
CODE:
npm install ai

----------------------------------------

TITLE: Accessing Response Sources
DESCRIPTION: Shows how to retrieve the source websites used to generate the response

LANGUAGE: typescript
CODE:
import { perplexity } from '@ai-sdk/perplexity';
import { generateText } from 'ai';

const { text, sources } = await generateText({
  model: perplexity('sonar-pro'),
  prompt: 'What are the latest developments in quantum computing?',
});

console.log(sources);

----------------------------------------

TITLE: Root Layout Component with AI Provider
DESCRIPTION: Root layout component that wraps the application with the AI provider component for context access.

LANGUAGE: tsx
CODE:
import { ReactNode } from 'react';
import { AI } from './ai';

export default function Layout({ children }: { children: ReactNode }) {
  return <AI>{children}</AI>;
}

----------------------------------------

TITLE: Creating Custom Google Provider Instance
DESCRIPTION: Creating a customized Google provider instance with specific settings

LANGUAGE: typescript
CODE:
import { createGoogleGenerativeAI } from '@ai-sdk/google';

const google = createGoogleGenerativeAI({
  // custom settings
});

----------------------------------------

TITLE: Adding Metadata to Telemetry in generateText Function
DESCRIPTION: This example shows how to add custom metadata and a functionId to the telemetry data for the generateText function.

LANGUAGE: typescript
CODE:
const result = await generateText({
  model: openai('gpt-4-turbo'),
  prompt: 'Write a short story about a cat.',
  experimental_telemetry: {
    isEnabled: true,
    functionId: 'my-awesome-function',
    metadata: {
      something: 'custom',
      someOtherThing: 'other-value',
    },
  },
});

----------------------------------------

TITLE: Migrating Provider Registry Exports
DESCRIPTION: Example of updating from experimental provider registry exports to standard exports.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
import { experimental_Provider, experimental_ProviderRegistry } from 'ai';

// AI SDK 4.0
import { Provider } from 'ai';

----------------------------------------

TITLE: Calling DeepSeek R1 with AI SDK Core
DESCRIPTION: This snippet demonstrates how to use AI SDK Core to generate text using the DeepSeek R1 model. It imports the necessary functions and makes a simple API call to generate text based on a prompt.

LANGUAGE: typescript
CODE:
import { deepseek } from '@ai-sdk/deepseek';
import { generateText } from 'ai';

const { reasoning, text } = await generateText({
  model: deepseek('deepseek-reasoner'),
  prompt: 'Explain quantum entanglement.',
});

----------------------------------------

TITLE: Creating Custom Together.ai Provider Instance
DESCRIPTION: Example of creating a customized Together.ai provider instance with specific settings.

LANGUAGE: typescript
CODE:
import { createTogetherAI } from '@ai-sdk/togetherai';

const togetherai = createTogetherAI({
  apiKey: process.env.TOGETHER_AI_API_KEY ?? '',
});

----------------------------------------

TITLE: Implementing Text Stream Protocol Backend Route
DESCRIPTION: Backend implementation for text stream protocol using Next.js API route. Handles POST requests and streams text responses using the openai model.

LANGUAGE: typescript
CODE:
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export async function POST(req: Request) {
  const { prompt }: { prompt: string } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    prompt,
  });

  return result.toTextStreamResponse();
}

----------------------------------------

TITLE: Checking for AI_APICallError Instance in TypeScript
DESCRIPTION: This snippet demonstrates how to check if an error is an instance of AI_APICallError using the isInstance method. It's useful for specifically handling API call errors in your error management logic.

LANGUAGE: typescript
CODE:
import { APICallError } from 'ai';

if (APICallError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Dynamic Text Prompt with Template Literals
DESCRIPTION: Demonstrates using template literals to create dynamic prompts with variable interpolation.

LANGUAGE: typescript
CODE:
const result = await generateText({
  model: yourModel,
  prompt:
    `I am planning a trip to ${destination} for ${lengthOfStay} days. ` +
    `Please suggest the best tourist activities for me to do.`,
});

----------------------------------------

TITLE: Configuring Language Models
DESCRIPTION: Examples of instantiating both chat and completion models using OpenRouter.

LANGUAGE: typescript
CODE:
// Chat models (recommended)
const chatModel = openrouter.chatModel('anthropic/claude-3.5-sonnet');

// Completion models
const completionModel = openrouter.completionModel(
  'meta-llama/llama-3.1-405b-instruct',
);

----------------------------------------

TITLE: Package Installation Commands
DESCRIPTION: Commands for installing the Zod package using different package managers (pnpm, npm, yarn).

LANGUAGE: shell
CODE:
pnpm add zod

LANGUAGE: shell
CODE:
npm install zod

LANGUAGE: shell
CODE:
yarn add zod

----------------------------------------

TITLE: Implementing Retrieval Augmented Generation with AI SDK in TypeScript
DESCRIPTION: This code snippet demonstrates the implementation of Retrieval Augmented Generation using the AI SDK and OpenAI's models. It reads an essay, chunks it, creates embeddings, stores them in a simple vector database, and then uses this to answer a question by finding relevant context and generating a response.

LANGUAGE: typescript
CODE:
import fs from 'fs';
import path from 'path';
import dotenv from 'dotenv';
import { openai } from '@ai-sdk/openai';
import { cosineSimilarity, embed, embedMany, generateText } from 'ai';

dotenv.config();

async function main() {
  const db: { embedding: number[]; value: string }[] = [];

  const essay = fs.readFileSync(path.join(__dirname, 'essay.txt'), 'utf8');
  const chunks = essay
    .split('.')
    .map(chunk => chunk.trim())
    .filter(chunk => chunk.length > 0 && chunk !== '\n');

  const { embeddings } = await embedMany({
    model: openai.embedding('text-embedding-3-small'),
    values: chunks,
  });
  embeddings.forEach((e, i) => {
    db.push({
      embedding: e,
      value: chunks[i],
    });
  });

  const input =
    'What were the two main things the author worked on before college?';

  const { embedding } = await embed({
    model: openai.embedding('text-embedding-3-small'),
    value: input,
  });
  const context = db
    .map(item => ({
      document: item,
      similarity: cosineSimilarity(embedding, item.embedding),
    }))
    .sort((a, b) => b.similarity - a.similarity)
    .slice(0, 3)
    .map(r => r.document.value)
    .join('\n');

  const { text } = await generateText({
    model: openai('gpt-4o'),
    prompt: `Answer the following question based only on the provided context:
             ${context}

             Question: ${input}`,
  });
  console.log(text);
}

main().catch(console.error);

----------------------------------------

TITLE: Creating Custom Mixedbread Provider in TypeScript
DESCRIPTION: This snippet demonstrates how to create a customized Mixedbread provider instance using createMixedbread function. It allows setting the base URL and API key.

LANGUAGE: typescript
CODE:
import { createMixedbread } from 'mixedbread-ai-provider';

const mixedbread = createMixedbread({
  baseURL: 'https://api.mixedbread.ai/v1',
  apiKey: process.env.MIXEDBREAD_API_KEY,
});

----------------------------------------

TITLE: Setting AWS Credentials in .env File
DESCRIPTION: Example of setting AWS credentials as environment variables in a .env file.

LANGUAGE: makefile
CODE:
AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY
AWS_REGION=YOUR_REGION

----------------------------------------

TITLE: Configuring Langfuse Environment Variables
DESCRIPTION: Environment variable configuration for Langfuse integration

LANGUAGE: bash
CODE:
LANGFUSE_SECRET_KEY="sk-lf-..."
LANGFUSE_PUBLIC_KEY="pk-lf-..."
LANGFUSE_BASEURL="https://cloud.langfuse.com"

----------------------------------------

TITLE: Advanced Configuration Example of createIdGenerator
DESCRIPTION: Shows how to create a more specific ID generator with custom size parameter for generating shorter IDs.

LANGUAGE: typescript
CODE:
// Create a custom ID generator for user IDs
const generateUserId = createIdGenerator({
  prefix: 'user',
  separator: '_',
  size: 8,
});

// Generate IDs
const id1 = generateUserId(); // e.g., "user_1a2b3c4d"

----------------------------------------

TITLE: Installing DeepInfra Provider for AI SDK
DESCRIPTION: Shows how to install the DeepInfra provider using different package managers.

LANGUAGE: bash
CODE:
pnpm add @ai-sdk/deepinfra

LANGUAGE: bash
CODE:
npm install @ai-sdk/deepinfra

LANGUAGE: bash
CODE:
yarn add @ai-sdk/deepinfra

----------------------------------------

TITLE: Streaming Chatbot Response Handler in Next.js
DESCRIPTION: Implements a server-side route handler that streams responses from a language model. It uses the AI SDK to handle the streaming and sets up the system message for Markdown formatting.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';

export const maxDuration = 60;

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    system:
      'You are a helpful assistant. Respond to the user in Markdown format.',
    model: openai('gpt-4o'),
    messages,
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Generating Text from Image URL with OpenAI GPT-4
DESCRIPTION: Example showing how to generate text responses from an image URL using the AI SDK with OpenAI's GPT-4 Turbo model. The code demonstrates sending a text question along with an image URL to analyze image content.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const result = await generateText({
  model: openai('gpt-4-turbo'),
  maxTokens: 512,
  messages: [
    {
      role: 'user',
      content: [
        {
          type: 'text',
          text: 'what are the red things in this image?',
        },
        {
          type: 'image',
          image: new URL(
            'https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/2024_Solar_Eclipse_Prominences.jpg/720px-2024_Solar_Eclipse_Prominences.jpg',
          ),
        },
      ],
    },
  ],
});

console.log(result);

----------------------------------------

TITLE: Initializing Cohere Language Model
DESCRIPTION: Create a Cohere language model instance by specifying the model ID.

LANGUAGE: typescript
CODE:
const model = cohere('command-r-plus');

----------------------------------------

TITLE: Initializing LangWatch Client with API Key
DESCRIPTION: TypeScript code snippet for initializing the LangWatch client with an API key.

LANGUAGE: typescript
CODE:
import { LangWatch } from 'langwatch';

const langwatch = new LangWatch({
  apiKey: 'your_api_key_here',
});

----------------------------------------

TITLE: Initial Project Setup Commands
DESCRIPTION: Commands to create a new directory, initialize a Node.js project, and set up the basic project structure.

LANGUAGE: bash
CODE:
mkdir my-ai-app
cd my-ai-app
pnpm init

----------------------------------------

TITLE: Setting OpenAI API Key Configuration
DESCRIPTION: Environment variable configuration for OpenAI API key authentication

LANGUAGE: env
CODE:
OPENAI_API_KEY=xxxxxxxxx

----------------------------------------

TITLE: Defining Message Types and Database Fetching in TypeScript
DESCRIPTION: This snippet defines the types for server and client messages, and includes a function for fetching saved messages from a database. It uses server-side TypeScript with React Server Components.

LANGUAGE: tsx
CODE:
'use server';

import { getAIState } from 'ai/rsc';

export interface ServerMessage {
  role: 'user' | 'assistant' | 'function';
  content: string;
}

export interface ClientMessage {
  id: string;
  role: 'user' | 'assistant' | 'function';
  display: ReactNode;
}

// Function to get saved messages from database
export async function getSavedMessages(): Promise<ServerMessage[]> {
  'use server';

  // Implement your database fetching logic here
  return await fetchMessagesFromDatabase();
}

----------------------------------------

TITLE: Initializing Mem0 Client in TypeScript
DESCRIPTION: Creates a Mem0 client instance with OpenAI as the provider, configuring API keys and optional global settings.

LANGUAGE: typescript
CODE:
import { createMem0 } from '@mem0/vercel-ai-provider';

const mem0 = createMem0({
  provider: 'openai',
  mem0ApiKey: 'm0-xxx',
  apiKey: 'provider-api-key',
  config: {
    compatibility: 'strict',
  },
  // Optional Mem0 Global Config
  mem0Config: {
    user_id: 'mem0-user-id',
    org_id: 'mem0-org-id',
    project_id: 'mem0-project-id',
  },
});

----------------------------------------

TITLE: Importing the Tool Helper Function
DESCRIPTION: This snippet shows how to import the 'tool' helper function from the 'ai' package.

LANGUAGE: typescript
CODE:
import { tool } from "ai"

----------------------------------------

TITLE: Installing Google Generative AI Provider
DESCRIPTION: Command to install the Google Generative AI provider package using npm.

LANGUAGE: bash
CODE:
npm i @ai-sdk/google

----------------------------------------

TITLE: Creating a Chatbot UI with Next.js and AI SDK
DESCRIPTION: This snippet demonstrates how to create a simple chatbot UI using Next.js and the AI SDK's useChat hook. It renders messages and handles user input.

LANGUAGE: tsx
CODE:
'use client';

import { useChat } from '@ai-sdk/react';

export default function Page() {
  const { messages, input, handleInputChange, handleSubmit } = useChat();

  return (
    <>
      {messages.map(message => (
        <div key={message.id}>
          {message.role === 'user' ? 'User: ' : 'AI: '}
          {message.content}
        </div>
      ))}
      <form onSubmit={handleSubmit}>
        <input name="prompt" value={input} onChange={handleInputChange} />
        <button type="submit">Submit</button>
      </form>
    </>
  );
}

----------------------------------------

TITLE: Enabling Telemetry in AI SDK Calls
DESCRIPTION: TypeScript example of enabling experimental telemetry in AI SDK generateText function calls.

LANGUAGE: typescript
CODE:
const result = await generateText({
  model: openai('gpt-4o-mini'),
  prompt:
    'Explain why a chicken would make a terrible astronaut, be creative and humorous about it.',
  experimental_telemetry: {
    isEnabled: true,
    // optional metadata
    metadata: {
      userId: 'myuser-123',
      threadId: 'mythread-123',
    },
  },
});

----------------------------------------

TITLE: Importing Default Settings Middleware
DESCRIPTION: Simple import statement for the defaultSettingsMiddleware function from the ai package.

LANGUAGE: typescript
CODE:
import { defaultSettingsMiddleware } from "ai"

----------------------------------------

TITLE: Importing and Using simulateReadableStream in TypeScript
DESCRIPTION: Demonstrates how to import and use the simulateReadableStream function to create a stream that emits chunks with delays.

LANGUAGE: typescript
CODE:
import { simulateReadableStream } from 'ai';

const stream = simulateReadableStream({
  chunks: ['Hello', ' ', 'World'],
  initialDelayInMs: 100,
  chunkDelayInMs: 50,
});

----------------------------------------

TITLE: Enhancing Model with Reasoning Middleware
DESCRIPTION: Wrap a Fireworks model with reasoning extraction middleware for enhanced capabilities.

LANGUAGE: typescript
CODE:
import { fireworks } from '@ai-sdk/fireworks';
import { wrapLanguageModel, extractReasoningMiddleware } from 'ai';

const enhancedModel = wrapLanguageModel({
  model: fireworks('accounts/fireworks/models/deepseek-r1'),
  middleware: extractReasoningMiddleware({ tagName: 'think' }),
});

----------------------------------------

TITLE: Creating a LangChain Completion API Route in Next.js
DESCRIPTION: This snippet shows how to create an API route in Next.js that uses LangChain with the OpenAI ChatGPT model. It utilizes the AI SDK's LangChainAdapter to stream the response to the client.

LANGUAGE: typescript
CODE:
import { ChatOpenAI } from '@langchain/openai';
import { LangChainAdapter } from 'ai';

export const maxDuration = 60;

export async function POST(req: Request) {
  const { prompt } = await req.json();

  const model = new ChatOpenAI({
    model: 'gpt-3.5-turbo-0125',
    temperature: 0,
  });

  const stream = await model.stream(prompt);

  return LangChainAdapter.toDataStreamResponse(stream);
}

----------------------------------------

TITLE: Previewing Text Generation with Various Models in JSX
DESCRIPTION: This snippet demonstrates how to use a component called PreviewSwitchProviders to showcase text generation using different models with AI SDK.

LANGUAGE: jsx
CODE:
<PreviewSwitchProviders />

----------------------------------------

TITLE: Checking AI_LoadAPIKeyError Type in TypeScript
DESCRIPTION: Example showing how to check if an error is an instance of AI_LoadAPIKeyError using the isInstance method. This is useful for error handling and type checking in TypeScript applications.

LANGUAGE: typescript
CODE:
import { LoadAPIKeyError } from 'ai';

if (LoadAPIKeyError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Importing MCP Client - TypeScript
DESCRIPTION: Shows how to import the experimental_createMCPClient function from the ai package.

LANGUAGE: typescript
CODE:
import { experimental_createMCPClient } from "ai"

----------------------------------------

TITLE: Installing Google Vertex Provider Package
DESCRIPTION: Command to install the Google Vertex provider module using npm

LANGUAGE: bash
CODE:
npm i @ai-sdk/google-vertex

----------------------------------------

TITLE: Next.js Laminar Registration
DESCRIPTION: Implementation of Laminar initialization in Next.js instrumentation file, specifically for Node.js runtime.

LANGUAGE: javascript
CODE:
export async function register() {
  // prevent this from running in the edge runtime
  if (process.env.NEXT_RUNTIME === 'nodejs') {
    const { Laminar } = await import('@lmnr-ai/lmnr');
    Laminar.initialize({
      projectApiKey: process.env.LMNR_API_KEY,
    });
  }
}

----------------------------------------

TITLE: Chat UI Component Implementation
DESCRIPTION: React client component that implements the chat interface using useChat hook. Handles message display and user input.

LANGUAGE: tsx
CODE:
'use client';

import { Message, useChat } from '@ai-sdk/react';

export default function Chat({
  id,
  initialMessages,
}: { id?: string | undefined; initialMessages?: Message[] } = {}) {
  const { input, handleInputChange, handleSubmit, messages } = useChat({
    id, // use the provided chat ID
    initialMessages, // initial messages if provided
    sendExtraMessageFields: true, // send id and createdAt for each message
  });

  return (
    <div>
      {messages.map(m => (
        <div key={m.id}>
          {m.role === 'user' ? 'User: ' : 'AI: '}
          {m.content}
        </div>
      ))}

      <form onSubmit={handleSubmit}>
        <input value={input} onChange={handleInputChange} />
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Importing StdioMCPTransport Module
DESCRIPTION: Shows how to import the Experimental_StdioMCPTransport module from the ai/mcp-stdio package.

LANGUAGE: typescript
CODE:
import { Experimental_StdioMCPTransport } from "ai/mcp-stdio"

----------------------------------------

TITLE: Adding Metadata to Laminar Traces
DESCRIPTION: Example of adding metadata key-value pairs to Laminar traces for filtering purposes.

LANGUAGE: javascript
CODE:
import { Laminar } from '@lmnr-ai/lmnr';
const { text } = await generateText({
  model: openai('gpt-4o-mini'),
  prompt: `Write a poem about Laminar flow.`,
  experimental_telemetry: {
    isEnabled: true,
    metadata: {
      'my-key': 'my-value',
      'another-key': 'another-value',
    },
  },
});

----------------------------------------

TITLE: Importing useCompletion in React
DESCRIPTION: How to import the useCompletion hook in a React application from the AI SDK

LANGUAGE: javascript
CODE:
import { useCompletion } from '@ai-sdk/react'

----------------------------------------

TITLE: Installing Mistral Provider for AI SDK
DESCRIPTION: Command to install the Mistral provider module for the AI SDK using npm.

LANGUAGE: bash
CODE:
npm i @ai-sdk/mistral

----------------------------------------

TITLE: Migrating formatStreamPart
DESCRIPTION: Example of updating from formatStreamPart to formatDataStreamPart.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
formatStreamPart('text', 'Hello, world!');

// AI SDK 4.0
formatDataStreamPart('text', 'Hello, world!');

----------------------------------------

TITLE: Markdown Documentation Structure
DESCRIPTION: Structured documentation outlining the AI SDK RSC package functionality, including warnings about experimental status, compatibility notes, and detailed function descriptions.

LANGUAGE: markdown
CODE:
---
title: Overview
description: An overview of AI SDK RSC.
---

# AI SDK RSC

<Note type="warning">
  AI SDK RSC is currently experimental. We recommend using [AI SDK
  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from
  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).
</Note>

<Note>
  The `ai/rsc` package is compatible with frameworks that support React Server
  Components.
</Note>

----------------------------------------

TITLE: Text Generation with LM Studio Language Model
DESCRIPTION: Demonstrates how to generate text using a local LM Studio language model. Includes error handling for server connectivity through maxRetries parameter.

LANGUAGE: typescript
CODE:
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
import { generateText } from 'ai';

const lmstudio = createOpenAICompatible({
  name: 'lmstudio',
  baseURL: 'https://localhost:1234/v1',
});

const { text } = await generateText({
  model: lmstudio('llama-3.2-1b'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
  maxRetries: 1, // immediately error if the server is not running
});

----------------------------------------

TITLE: Fetch Request Interception Example
DESCRIPTION: Example showing how to intercept and log fetch requests made by the SambaNova provider.

LANGUAGE: typescript
CODE:
import { createSambaNova } from 'sambanova-ai-provider';
import { generateText } from 'ai';

const sambanovaProvider = createSambaNova({
  apiKey: 'YOUR_API_KEY',
  fetch: async (url, options) => {
    console.log('URL', url);
    console.log('Headers', JSON.stringify(options.headers, null, 2));
    console.log(`Body ${JSON.stringify(JSON.parse(options.body), null, 2)}`);
    return await fetch(url, options);
  },
});

const model = sambanovaProvider('Meta-Llama-3.1-70B-Instruct');

const { text } = await generateText({
  model,
  prompt: 'Hello, nice to meet you.',
});

----------------------------------------

TITLE: Importing Fireworks Provider in TypeScript
DESCRIPTION: Example of importing the default Fireworks provider instance in a TypeScript file.

LANGUAGE: typescript
CODE:
import { fireworks } from '@ai-sdk/fireworks';

----------------------------------------

TITLE: Checking AI_DownloadError Instance in TypeScript
DESCRIPTION: Shows how to check if an error object is an instance of AI_DownloadError. This code demonstrates the type checking pattern using the isInstance static method from the AI package.

LANGUAGE: typescript
CODE:
import { DownloadError } from 'ai';

if (DownloadError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Using OpenAI Model Distillation
DESCRIPTION: Example of using OpenAI model distillation to store generations for later use.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';
import 'dotenv/config';

async function main() {
  const { text, usage } = await generateText({
    model: openai('gpt-4o-mini'),
    prompt: 'Who worked on the original macintosh?',
    providerOptions: {
      openai: {
        store: true,
        metadata: {
          custom: 'value',
        },
      },
    },
  });

  console.log(text);
  console.log();
  console.log('Usage:', usage);
}

main().catch(console.error);

----------------------------------------

TITLE: Registering LangWatch Exporter with OpenTelemetry in Next.js
DESCRIPTION: TypeScript code to register LangWatch exporter with OpenTelemetry in a Next.js application.

LANGUAGE: typescript
CODE:
import { registerOTel } from '@vercel/otel';
import { LangWatchExporter } from 'langwatch';

export function register() {
  registerOTel({
    serviceName: 'next-app',
    traceExporter: new LangWatchExporter(),
  });
}

----------------------------------------

TITLE: Importing zodSchema from AI SDK in TypeScript
DESCRIPTION: This snippet shows how to import the zodSchema function from the 'ai' package.

LANGUAGE: typescript
CODE:
import { zodSchema } from "ai"

----------------------------------------

TITLE: Importing fal Provider
DESCRIPTION: How to import the default fal provider instance

LANGUAGE: typescript
CODE:
import { fal } from '@ai-sdk/fal';

----------------------------------------

TITLE: Implementing Root Layout with AI Provider
DESCRIPTION: Wraps the application with the AI context provider in the root layout component.

LANGUAGE: typescript
CODE:
import { type ReactNode } from 'react';
import { AI } from './ai';

export default function RootLayout({
  children,
}: Readonly<{ children: ReactNode }>) {
  return (
    <AI>
      <html lang="en">
        <body>{children}</body>
      </html>
    </AI>
  );
}

----------------------------------------

TITLE: Defining TextPart Interface in TypeScript
DESCRIPTION: Defines the structure of a text content part in a prompt. It includes a 'type' field set to 'text' and a 'text' field containing the actual text content.

LANGUAGE: typescript
CODE:
export interface TextPart {
  type: 'text';
  /**
   * The text content.
   */
  text: string;
}

----------------------------------------

TITLE: Creating Custom Fal Provider Instance in TypeScript
DESCRIPTION: Demonstrates how to create a customized Fal provider instance using createFal function with optional settings like API key, base URL, and custom headers.

LANGUAGE: typescript
CODE:
import { createFal } from '@ai-sdk/fal';

const fal = createFal({
  apiKey: 'your-api-key', // optional, defaults to FAL_API_KEY environment variable, falling back to FAL_KEY
  baseURL: 'custom-url', // optional
  headers: {
    /* custom headers */
  }, // optional
});

----------------------------------------

TITLE: Importing AssistantResponse from ai package
DESCRIPTION: Shows how to import the AssistantResponse class from the ai package

LANGUAGE: javascript
CODE:
import { AssistantResponse } from "ai"

----------------------------------------

TITLE: Configuring OpenAI API Key in Nuxt.js
DESCRIPTION: Sets up the OpenAI API key as an environment variable and configures it in the Nuxt.js runtime config.

LANGUAGE: env
CODE:
OPENAI_API_KEY=xxxxxxxxx

LANGUAGE: typescript
CODE:
export default defineNuxtConfig({
  // rest of your nuxt config
  runtimeConfig: {
    openaiApiKey: process.env.OPENAI_API_KEY,
  },
});

----------------------------------------

TITLE: Creating Basic OpenAI Compatible Provider Instance in TypeScript
DESCRIPTION: Demonstrates how to create a basic provider instance using createOpenAICompatible function with essential configuration options including name, API key, and base URL.

LANGUAGE: typescript
CODE:
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

const provider = createOpenAICompatible({
  name: 'provider-name',
  apiKey: process.env.PROVIDER_API_KEY,
  baseURL: 'https://api.provider.com/v1',
});

----------------------------------------

TITLE: Initializing SambaNova Model
DESCRIPTION: Example of initializing a SambaNova model instance.

LANGUAGE: typescript
CODE:
const model = sambanova('Meta-Llama-3.1-70B-Instruct');

----------------------------------------

TITLE: Initializing Chrome AI Model - TypeScript
DESCRIPTION: Basic initialization of the Chrome AI model using the chromeai function. Shows both default initialization and initialization with custom settings.

LANGUAGE: typescript
CODE:
import { chromeai } from 'chrome-ai';

const model = chromeai();

LANGUAGE: typescript
CODE:
import { chromeai } from 'chrome-ai';

const model = chromeai('generic', {
  // additional settings
  temperature: 0.5,
  topK: 5,
});

----------------------------------------

TITLE: Importing ReplicateStream in React
DESCRIPTION: Shows how to import the ReplicateStream function from the ai package in a React application.

LANGUAGE: javascript
CODE:
import { ReplicateStream } from "ai"

----------------------------------------

TITLE: Importing readStreamableValue from AI SDK RSC
DESCRIPTION: Shows how to import the readStreamableValue function from the AI SDK RSC package.

LANGUAGE: typescript
CODE:
import { readStreamableValue } from "ai/rsc"

----------------------------------------

TITLE: Installing fal Provider Package
DESCRIPTION: Command to install the fal provider package using npm

LANGUAGE: bash
CODE:
npm i @ai-sdk/fal

----------------------------------------

TITLE: Creating AI Context for Multistep Interface in TypeScript
DESCRIPTION: This code creates an AI context using createAI function to hold the UI State and AI State for the multistep interface.

LANGUAGE: typescript
CODE:
import { createAI } from 'ai/rsc';
import { submitUserMessage } from './actions';

export const AI = createAI<any[], React.ReactNode[]>({
  initialUIState: [],
  initialAIState: [],
  actions: {
    submitUserMessage,
  },
});

----------------------------------------

TITLE: Defining ToolResultPart Interface in TypeScript
DESCRIPTION: Defines the structure of a tool result part in a tool message. It includes fields for the tool call ID, tool name, result of the tool call, optional multi-part content, and an optional error flag.

LANGUAGE: typescript
CODE:
export interface ToolResultPart {
  type: 'tool-result';

  /**
   * ID of the tool call that this result is associated with.
   */
  toolCallId: string;

  /**
   * Name of the tool that generated this result.
   */
  toolName: string;

  /**
   * Result of the tool call. This is a JSON-serializable object.
   */
  result: unknown;

  /**
   * Multi-part content of the tool result. Only for tools that support multipart results.
   */
  experimental_content?: ToolResultContent;

  /**
   * Optional flag if the result is an error or an error message.
   */
  isError?: boolean;
}

----------------------------------------

TITLE: Creating Fireworks Language Model
DESCRIPTION: Initialize a Fireworks language model using a provider instance.

LANGUAGE: typescript
CODE:
const model = fireworks('accounts/fireworks/models/firefunction-v1');

----------------------------------------

TITLE: Importing appendResponseMessages from AI Package
DESCRIPTION: Shows how to import the appendResponseMessages function from the AI package.

LANGUAGE: typescript
CODE:
import { appendResponseMessages } from "ai"

----------------------------------------

TITLE: Installing Replicate Provider Package
DESCRIPTION: Command to install the Replicate provider package using npm

LANGUAGE: bash
CODE:
npm i @ai-sdk/replicate

----------------------------------------

TITLE: Style-Based Image Generation
DESCRIPTION: Demonstrates image generation with style reference images for applying specific visual styles.

LANGUAGE: typescript
CODE:
await generateImage({
  model: luma.image('photon-1'),
  prompt: 'A blue cream Persian cat launching its website on Vercel',
  providerOptions: {
    luma: {
      style_ref: [
        {
          url: 'https://example.com/style.jpg',
          weight: 0.8,
        },
      ],
    },
  },
});

----------------------------------------

TITLE: Importing createDataStream Function
DESCRIPTION: Shows how to import the createDataStream function from the ai package.

LANGUAGE: typescript
CODE:
import { createDataStream } from "ai"

----------------------------------------

TITLE: Implementing a LangChain Completion React Component
DESCRIPTION: This snippet demonstrates how to create a React component that uses the AI SDK's useCompletion hook to handle completions. It includes an input form and displays the completion result.

LANGUAGE: tsx
CODE:
'use client';

import { useCompletion } from '@ai-sdk/react';

export default function Chat() {
  const { completion, input, handleInputChange, handleSubmit } =
    useCompletion();

  return (
    <div>
      {completion}
      <form onSubmit={handleSubmit}>
        <input value={input} onChange={handleInputChange} />
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Creating Qwen Language Model Instance
DESCRIPTION: Create a Qwen language model instance using the provider and specifying the model ID.

LANGUAGE: typescript
CODE:
const model = qwen('qwen-plus');

----------------------------------------

TITLE: Importing Default AnthropicVertex Provider in TypeScript
DESCRIPTION: Import the default provider instance from the anthropic-vertex-ai package.

LANGUAGE: typescript
CODE:
import { anthropicVertex } from 'anthropic-vertex-ai';

----------------------------------------

TITLE: Testing the API Endpoint
DESCRIPTION: Curl command to test the HTTP endpoint with a POST request

LANGUAGE: sh
CODE:
curl -i -X POST http://localhost:8080

----------------------------------------

TITLE: Importing AWSBedrockAnthropicMessagesStream in React
DESCRIPTION: Shows how to import the AWSBedrockAnthropicMessagesStream function from the AI package in a React application.

LANGUAGE: javascript
CODE:
import { AWSBedrockAnthropicMessagesStream } from "ai"

----------------------------------------

TITLE: Checking for AI_InvalidMessageRoleError in TypeScript
DESCRIPTION: This code snippet demonstrates how to check if an error is an instance of AI_InvalidMessageRoleError using the isInstance method. It's useful for error handling in applications using the 'ai' library.

LANGUAGE: typescript
CODE:
import { InvalidMessageRoleError } from 'ai';

if (InvalidMessageRoleError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Defining ToolResultContent Type in TypeScript
DESCRIPTION: Defines the structure of multi-part content for tool results. It can include text parts and image parts with base64 encoded data and optional MIME type.

LANGUAGE: typescript
CODE:
export type ToolResultContent = Array<
  | {
      type: 'text';
      text: string;
    }
  | {
      type: 'image';
      data: string; // base64 encoded png image, e.g. screenshot
      mimeType?: string; // e.g. 'image/png';
    }
>;

----------------------------------------

TITLE: Migrating LanguageModelResponseMetadataWithHeaders
DESCRIPTION: Example of updating from LanguageModelResponseMetadataWithHeaders to LanguageModelResponseMetadata.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
import { LanguageModelResponseMetadataWithHeaders } from 'ai';

// AI SDK 4.0
import { LanguageModelResponseMetadata } from 'ai';

----------------------------------------

TITLE: Importing Default Groq Provider
DESCRIPTION: Basic import of the default Groq provider instance.

LANGUAGE: typescript
CODE:
import { groq } from '@ai-sdk/groq';

----------------------------------------

TITLE: Importing LanguageModelV1Middleware
DESCRIPTION: Shows how to import the LanguageModelV1Middleware class from the ai package.

LANGUAGE: javascript
CODE:
import { LanguageModelV1Middleware } from "ai"

----------------------------------------

TITLE: Using Image Models with Amazon Bedrock
DESCRIPTION: Example of creating and using an image model with Amazon Bedrock.

LANGUAGE: typescript
CODE:
import { bedrock } from '@ai-sdk/amazon-bedrock';
import { experimental_generateImage as generateImage } from 'ai';

const { image } = await generateImage({
  model: bedrock.imageModel('amazon.nova-canvas-v1:0'),
  prompt: 'A beautiful sunset over a calm ocean',
  size: '512x512',
  seed: 42,
  providerOptions: { bedrock: { quality: 'premium' } },
});

----------------------------------------

TITLE: Importing appendClientMessage Function from AI Library
DESCRIPTION: Shows how to import the appendClientMessage function from the AI library for use in a chat application.

LANGUAGE: javascript
CODE:
import { appendClientMessage } from "ai"

----------------------------------------

TITLE: Creating FriendliAI Language Model
DESCRIPTION: Instantiating a language model using the FriendliAI provider with a specific model ID.

LANGUAGE: typescript
CODE:
const model = friendli('meta-llama-3.1-8b-instruct');

----------------------------------------

TITLE: Importing useChat Hook for Svelte
DESCRIPTION: Shows the import statement for using the useChat hook in a Svelte application.

LANGUAGE: javascript
CODE:
import { useChat } from '@ai-sdk/svelte'

----------------------------------------

TITLE: Initializing Ollama Language Model
DESCRIPTION: Create a language model instance using the Ollama provider with a specific model ID.

LANGUAGE: typescript
CODE:
const model = ollama('phi3');

----------------------------------------

TITLE: Importing Default Mixedbread Provider in TypeScript
DESCRIPTION: This snippet shows how to import the default Mixedbread provider instance from the mixedbread-ai-provider package.

LANGUAGE: typescript
CODE:
import { mixedbread } from 'mixedbread-ai-provider';

----------------------------------------

TITLE: Importing AnthropicStream in React
DESCRIPTION: This snippet shows how to import the AnthropicStream function from the 'ai' package in a React application.

LANGUAGE: javascript
CODE:
import { AnthropicStream } from "ai"

----------------------------------------

TITLE: Importing AWSBedrockAnthropicStream in React
DESCRIPTION: Shows how to import the AWSBedrockAnthropicStream function from the AI package in a React application.

LANGUAGE: javascript
CODE:
import { AWSBedrockAnthropicStream } from "ai"

----------------------------------------

TITLE: Initializing StreamData
DESCRIPTION: Demonstrates how to create a new instance of the StreamData class.

LANGUAGE: typescript
CODE:
const data = new StreamData();

----------------------------------------

TITLE: Initializing StreamData
DESCRIPTION: Demonstrates how to create a new instance of the StreamData class.

LANGUAGE: typescript
CODE:
const data = new StreamData();

----------------------------------------

TITLE: Creating OpenAI Chat Model
DESCRIPTION: Create an OpenAI chat model with specific settings.

LANGUAGE: typescript
CODE:
const model = openai.chat('gpt-3.5-turbo', {
  logitBias: {
    // optional likelihood for specific tokens
    '50256': -100,
  },
  user: 'test-user', // optional unique user identifier
});

----------------------------------------

TITLE: Importing Default Fal Provider in TypeScript
DESCRIPTION: Shows how to import the default Fal provider instance from the @ai-sdk/fal module.

LANGUAGE: typescript
CODE:
import { fal } from '@ai-sdk/fal';

----------------------------------------

TITLE: Importing useCompletion in Vue
DESCRIPTION: How to import the useCompletion hook in a Vue application from the AI SDK

LANGUAGE: javascript
CODE:
import { useCompletion } from '@ai-sdk/vue'

----------------------------------------

TITLE: Checking for AI_JSONParseError Instance in TypeScript
DESCRIPTION: This snippet demonstrates how to check if an error is an instance of AI_JSONParseError using the isInstance method. It's useful for error handling in applications using the 'ai' library.

LANGUAGE: typescript
CODE:
import { JSONParseError } from 'ai';

if (JSONParseError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Installing Azure AI Provider via npm
DESCRIPTION: Install the @quail-ai/azure-ai-provider package using npm to integrate Azure AI capabilities into your project.

LANGUAGE: bash
CODE:
npm i @quail-ai/azure-ai-provider

----------------------------------------

TITLE: Importing LlamaIndexAdapter in TypeScript
DESCRIPTION: Example showing how to import the LlamaIndexAdapter module

LANGUAGE: typescript
CODE:
import { LlamaIndexAdapter } from "ai"

----------------------------------------

TITLE: Creating Custom OpenAI Provider Instance
DESCRIPTION: Create a customized OpenAI provider instance with specific settings.

LANGUAGE: typescript
CODE:
import { createOpenAI } from '@ai-sdk/openai';

const openai = createOpenAI({
  // custom settings, e.g.
  compatibility: 'strict', // strict mode, enable when using the OpenAI API
});

----------------------------------------

TITLE: Using DeepInfra Image Model for Image Generation
DESCRIPTION: Demonstrates how to use a DeepInfra image model to generate images using the AI SDK.

LANGUAGE: typescript
CODE:
import { deepinfra } from '@ai-sdk/deepinfra';
import { experimental_generateImage as generateImage } from 'ai';

const { image } = await generateImage({
  model: deepinfra.image('stabilityai/sd3.5'),
  prompt: 'A futuristic cityscape at sunset',
  aspectRatio: '16:9',
});

----------------------------------------

TITLE: Generating Images with Together.ai Model
DESCRIPTION: Example of using a Together.ai image model to generate images with the generateImage function.

LANGUAGE: typescript
CODE:
import { togetherai } from '@ai-sdk/togetherai';
import { experimental_generateImage as generateImage } from 'ai';

const { images } = await generateImage({
  model: togetherai.image('black-forest-labs/FLUX.1-dev'),
  prompt: 'A delighted resplendent quetzal mid flight amidst raindrops',
});

----------------------------------------

TITLE: Setting up Azure AI environment variables
DESCRIPTION: Configure the Azure AI endpoint URL and API key in the .env file for authentication and access to Azure AI services.

LANGUAGE: bash
CODE:
AZURE_API_ENDPOINT=https://<your-resource>.services.ai.azure.com/models
AZURE_API_KEY=<your-api-key>

----------------------------------------

TITLE: Generating Text with Azure AI Provider in TypeScript
DESCRIPTION: Use the generateText function from the AI SDK to generate text using the Azure AI provider, specifying the model deployment and prompt.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';

const { text } = await generateText({
  model: azure('your-deployment-name'),
  prompt: 'Write a story about a robot.',
});

----------------------------------------

TITLE: Importing CohereStream in React
DESCRIPTION: This snippet shows how to import the CohereStream function from the 'ai' package in a React application.

LANGUAGE: javascript
CODE:
import { CohereStream } from "ai"

----------------------------------------

TITLE: Using useStreamableValue in a React Component
DESCRIPTION: Shows an example of using the useStreamableValue hook in a React component to handle streamable values, including loading and error states.

LANGUAGE: tsx
CODE:
function MyComponent({ streamableValue }) {
  const [data, error, pending] = useStreamableValue(streamableValue);

  if (pending) return <div>Loading...</div>;
  if (error) return <div>Error: {error.message}</div>;

  return <div>Data: {data}</div>;
}

----------------------------------------

TITLE: Markdown Documentation - AI SDK Migration Guide
DESCRIPTION: Comprehensive documentation detailing the changes and improvements between AI SDK versions 3.2 and 3.3, including new features like OpenTelemetry support, UI improvements, core enhancements, and expanded framework support.

LANGUAGE: markdown
CODE:
---
title: Migrate AI SDK 3.2 to 3.3
description: Learn how to upgrade AI SDK 3.2 to 3.3.
---

# Migrate AI SDK 3.2 to 3.3

<Note>
  Check out the [AI SDK 3.3 release blog
  post](https://vercel.com/blog/vercel-ai-sdk-3-3) for more information about
  the release.
</Note>

No breaking changes in this release.

The following changelog encompasses all changes made in the 3.2.x series,
introducing significant improvements and new features across the AI SDK and its associated libraries:

----------------------------------------

TITLE: Checking AI_NoSuchProviderError Instance in TypeScript
DESCRIPTION: Demonstrates how to check if an error is an instance of AI_NoSuchProviderError using the isInstance static method. This is useful for error handling when working with AI providers.

LANGUAGE: typescript
CODE:
import { NoSuchProviderError } from 'ai';

if (NoSuchProviderError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Creating Next.js App with AI SDK and OpenAI Rate Limits Example using npx
DESCRIPTION: This command uses create-next-app to bootstrap a new Next.js application with the AI SDK and OpenAI rate limits example. It clones the example from the Vercel AI repository.

LANGUAGE: bash
CODE:
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-rate-limits next-openai-rate-limits-app

----------------------------------------

TITLE: Basic Image Generation with Luma
DESCRIPTION: Demonstrates basic image generation using Luma's Photon-1 model, including saving the generated image to a file.

LANGUAGE: typescript
CODE:
import { luma } from '@ai-sdk/luma';
import { experimental_generateImage as generateImage } from 'ai';
import fs from 'fs';

const { image } = await generateImage({
  model: luma.image('photon-1'),
  prompt: 'A serene mountain landscape at sunset',
  aspectRatio: '16:9',
});

const filename = `image-${Date.now()}.png`;
fs.writeFileSync(filename, image.uint8Array);
console.log(`Image saved to ${filename}`);

----------------------------------------

TITLE: Importing useAssistant in Svelte
DESCRIPTION: Shows how to import the useAssistant hook in a Svelte application.

LANGUAGE: javascript
CODE:
import { useAssistant } from '@ai-sdk/svelte'

----------------------------------------

TITLE: Importing Google Provider Instance
DESCRIPTION: Basic import of the default Google provider instance

LANGUAGE: typescript
CODE:
import { google } from '@ai-sdk/google';

----------------------------------------

TITLE: Configuring Vercel Function Duration
DESCRIPTION: TypeScript configuration to set maximum duration for Vercel serverless functions using route segment config. Allows extending function timeout up to 60 seconds on Hobby tier.

LANGUAGE: typescript
CODE:
export const maxDuration = 30;

----------------------------------------

TITLE: Installing Dependencies with Package Managers
DESCRIPTION: Commands to install the required AI SDK OpenAI provider and LangSmith client SDK using different package managers.

LANGUAGE: bash
CODE:
pnpm add @ai-sdk/openai langsmith

----------------------------------------

TITLE: Retrieving Memory Sources in TypeScript
DESCRIPTION: Shows how to retrieve and log memory sources when generating text using the Mem0 provider.

LANGUAGE: typescript
CODE:
const { text, sources } = await generateText({
  model: mem0('gpt-4-turbo'),
  prompt: 'Suggest me a good car to buy!',
});

console.log(sources);

----------------------------------------

TITLE: Streaming Text with OpenRouter
DESCRIPTION: Demonstrates how to use text streaming with OpenRouter for real-time AI responses.

LANGUAGE: javascript
CODE:
import { createOpenRouter } from '@openrouter/ai-sdk-provider';
import { streamText } from 'ai';

const openrouter = createOpenRouter({
  apiKey: 'YOUR_OPENROUTER_API_KEY',
});

const result = streamText({
  model: openrouter.chatModel('meta-llama/llama-3.1-405b-instruct'),
  prompt: 'Write a short story about AI.',
});

for await (const chunk of result) {
  console.log(chunk);
}

----------------------------------------

TITLE: Importing AWSBedrockCohereStream in React
DESCRIPTION: Shows how to import the AWSBedrockCohereStream function from the 'ai' package for use in React applications.

LANGUAGE: javascript
CODE:
import { AWSBedrockCohereStream } from "ai"

----------------------------------------

TITLE: Importing useStreamableValue from AI SDK RSC
DESCRIPTION: Demonstrates how to import the useStreamableValue function from the AI SDK RSC package.

LANGUAGE: typescript
CODE:
import { useStreamableValue } from "ai/rsc"

----------------------------------------

TITLE: Checking for AI_NoSuchToolError in TypeScript
DESCRIPTION: Shows how to check if an error is an instance of AI_NoSuchToolError using the isInstance static method from the ai package. This is useful for error handling when working with AI tools and models.

LANGUAGE: typescript
CODE:
import { NoSuchToolError } from 'ai';

if (NoSuchToolError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Installing Next.js Project with Yarn
DESCRIPTION: Command to create a new Next.js project using Yarn, specifically for the AI SDK example with OpenAI and Sentry telemetry.

LANGUAGE: bash
CODE:
yarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry-sentry next-openai-telemetry-sentry-app

----------------------------------------

TITLE: Creating Azure AI Provider Instance in TypeScript
DESCRIPTION: Import and initialize the Azure AI provider using the createAzure function, configuring it with the endpoint and API key from environment variables.

LANGUAGE: typescript
CODE:
import { createAzure } from '@quail-ai/azure-ai-provider';

const azure = createAzure({
  endpoint: process.env.AZURE_API_ENDPOINT,
  apiKey: process.env.AZURE_API_KEY,
});

----------------------------------------

TITLE: Multi-modal Tool Response Configuration (TypeScript)
DESCRIPTION: Demonstrates configuring convertToCoreMessages with tools that support multi-modal content responses, including image handling.

LANGUAGE: typescript
CODE:
import { tool } from 'ai';
import { z } from 'zod';

const screenshotTool = tool({
  parameters: z.object({}),
  execute: async () => 'imgbase64',
  experimental_toToolResultContent: result => [{ type: 'image', data: result }],
});

const result = streamText({
  model: openai('gpt-4'),
  messages: convertToCoreMessages(messages, {
    tools: {
      screenshot: screenshotTool,
    },
  }),
});

----------------------------------------

TITLE: Checking for AI_TypeValidationError Instances in TypeScript
DESCRIPTION: This code snippet demonstrates how to check if an error is an instance of AI_TypeValidationError using the isInstance static method. It imports the TypeValidationError class from the 'ai' package and uses a conditional statement to handle the error if it matches the type.

LANGUAGE: typescript
CODE:
import { TypeValidationError } from 'ai';

if (TypeValidationError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Importing Default Fireworks Provider
DESCRIPTION: Import the default Fireworks provider instance from the package.

LANGUAGE: typescript
CODE:
import { fireworks } from '@ai-sdk/fireworks';

----------------------------------------

TITLE: Initializing Git Repository and Committing Changes
DESCRIPTION: Commands for initializing a new git repository and committing local changes before deployment to Vercel.

LANGUAGE: bash
CODE:
git add .
git commit -m "init"

----------------------------------------

TITLE: Listing AI SDK React Hooks
DESCRIPTION: This snippet lists the available React hooks in the AI SDK, including useChat, useCompletion, and useAssistant. Each hook is linked to its corresponding documentation page.

LANGUAGE: markdown
CODE:
- [`useChat`](https://sdk.vercel.ai/docs/reference/ai-sdk-ui/use-chat) hook
- [`useCompletion`](https://sdk.vercel.ai/docs/reference/ai-sdk-ui/use-completion) hook
- [`useAssistant`](https://sdk.vercel.ai/docs/reference/ai-sdk-ui/use-assistant) hook

----------------------------------------

TITLE: Project File Structure Organization with Bash
DESCRIPTION: Shows the recommended file structure for organizing a custom provider package implementation.

LANGUAGE: bash
CODE:
packages/example/
├── src/
│   ├── example-chat-settings.ts       # Chat model types and settings
│   ├── example-completion-settings.ts # Completion model types and settings
│   ├── example-embedding-settings.ts  # Embedding model types and settings
│   ├── example-image-settings.ts      # Image model types and settings
│   ├── example-provider.ts            # Main provider implementation
│   ├── example-provider.test.ts       # Provider tests
│   └── index.ts                       # Public exports
├── package.json
├── tsconfig.json
├── tsup.config.ts                     # Build configuration
└── README.md

----------------------------------------

TITLE: Package Configuration in JSON
DESCRIPTION: Configures the npm package with necessary dependencies and settings.

LANGUAGE: javascript
CODE:
{
  "name": "@company-name/example",
  "version": "0.0.1",
  "dependencies": {
    "@ai-sdk/openai-compatible": "^0.0.7",
    "@ai-sdk/provider": "^1.0.2",
    "@ai-sdk/provider-utils": "^2.0.4"
  }
}

----------------------------------------

TITLE: Installing AnthropicVertex Provider with Package Managers
DESCRIPTION: Commands to install the anthropic-vertex-ai package using different package managers.

LANGUAGE: shell
CODE:
pnpm add anthropic-vertex-ai

LANGUAGE: shell
CODE:
npm install anthropic-vertex-ai

LANGUAGE: shell
CODE:
yarn add anthropic-vertex-ai

----------------------------------------

TITLE: Generating Structured Objects with Cloudflare Workers AI
DESCRIPTION: Example of using the generateObject function with a Cloudflare Workers AI language model to generate a structured recipe object.

LANGUAGE: typescript
CODE:
import { createWorkersAI } from 'workers-ai-provider';
import { generateObject } from 'ai';
import { z } from 'zod';

type Env = {
  AI: Ai;
};

export default {
  async fetch(_: Request, env: Env) {
    const workersai = createWorkersAI({ binding: env.AI });
    const result = await generateObject({
      model: workersai('@cf/meta/llama-3.1-8b-instruct'),
      prompt: 'Generate a Lasagna recipe',
      schema: z.object({
        recipe: z.object({
          ingredients: z.array(z.string()),
          description: z.string(),
        }),
      }),
    });

    return Response.json(result.object);
  },
};

----------------------------------------

TITLE: Custom Headers Configuration in AI SDK
DESCRIPTION: Example of setting custom HTTP headers for AI SDK requests, including provider-specific headers like Prompt-Id.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const result = await generateText({
  model: openai('gpt-4o'),
  prompt: 'Invent a new holiday and describe its traditions.',
  headers: {
    'Prompt-Id': 'my-prompt-id',
  },
});

----------------------------------------

TITLE: Importing useActions Hook from AI SDK RSC
DESCRIPTION: Shows how to import the useActions hook from the AI SDK RSC package. This hook returns a dictionary of server actions that can be used in client components.

LANGUAGE: javascript
CODE:
import { useActions } from "ai/rsc"

----------------------------------------

TITLE: Importing useChat Hook for Vue
DESCRIPTION: Illustrates how to import the useChat hook for use in a Vue application.

LANGUAGE: javascript
CODE:
import { useChat } from '@ai-sdk/vue'

----------------------------------------

TITLE: Checking AI_UnsupportedFunctionalityError in TypeScript
DESCRIPTION: Demonstrates how to check if an error is an instance of AI_UnsupportedFunctionalityError using the isInstance static method.

LANGUAGE: typescript
CODE:
import { UnsupportedFunctionalityError } from 'ai';

if (UnsupportedFunctionalityError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Setting Timeouts in Vercel Configuration for Other Frameworks
DESCRIPTION: This JSON configuration demonstrates how to set timeouts for specific functions in the vercel.json file. It's useful for frameworks like Svelte when deploying to Vercel.

LANGUAGE: json
CODE:
{
  "functions": {
    "api/chat/route.ts": {
      "maxDuration": 30
    }
  }
}

----------------------------------------

TITLE: Appending Value to StreamData
DESCRIPTION: Shows how to use the append method to add a value to the stream data. The value must be of type JSONValue.

LANGUAGE: typescript
CODE:
data.append(value: JSONValue)

----------------------------------------

TITLE: Importing OpenAI Compatible Provider in TypeScript
DESCRIPTION: This snippet demonstrates how to import the createOpenAICompatible function from the AI SDK package.

LANGUAGE: typescript
CODE:
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

----------------------------------------

TITLE: Adding Stub for experimental_onFunctionCall in OpenAIStream (TSX)
DESCRIPTION: This code snippet shows how to add a stub for experimental_onFunctionCall to OpenAIStream. This enables the correct forwarding of function calls to the client, resolving the issue of client-side function calls not being invoked after upgrading the AI SDK to v3.0.20 or newer.

LANGUAGE: tsx
CODE:
const stream = OpenAIStream(response, {
  async experimental_onFunctionCall() {
    return;
  },
});

----------------------------------------

TITLE: Importing Spark Provider in TypeScript
DESCRIPTION: Import the createSparkProvider function from the spark-ai-provider package to create a provider instance in TypeScript.

LANGUAGE: typescript
CODE:
import { createSparkProvider } from 'spark-ai-provider';

----------------------------------------

TITLE: Reading Streamable UI in Next.js Client Component
DESCRIPTION: Client-side implementation demonstrating how to render and update a streamable UI component in a Next.js page.

LANGUAGE: tsx
CODE:
'use client';

import { useState } from 'react';
import { readStreamableValue } from 'ai/rsc';
import { getWeather } from '@/actions';

export default function Page() {
  const [weather, setWeather] = useState<React.ReactNode | null>(null);

  return (
    <div>
      <button
        onClick={async () => {
          const weatherUI = await getWeather();
          setWeather(weatherUI);
        }}
      >
        What&apos;s the weather?
      </button>

      {weather}
    </div>
  );
}

----------------------------------------

TITLE: Migrating LangChain Adapter toAIStream
DESCRIPTION: Example of updating from toAIStream to toDataStream in the LangChain adapter.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
LangChainAdapter.toAIStream(stream);

// AI SDK 4.0
LangChainAdapter.toDataStream(stream);

----------------------------------------

TITLE: Creating LangDB Provider Instance
DESCRIPTION: Demonstrates how to create a LangDB provider instance using the createLangDB function. It shows required and optional parameters for configuration.

LANGUAGE: tsx
CODE:
import { createLangDB } from '@langdb/vercel-provider';

const langdb = createLangDB({
  apiKey: process.env.LANGDB_API_KEY, // Required
  projectId: 'your-project-id', // Required
  threadId: uuidv4(), // Optional
  runId: uuidv4(), // Optional
  label: 'code-agent', // Optional
  headers: { 'Custom-Header': 'value' }, // Optional
});

----------------------------------------

TITLE: Installing Cloudflare Workers AI Provider
DESCRIPTION: Instructions for installing the Cloudflare Workers AI provider using different package managers.

LANGUAGE: bash
CODE:
pnpm add workers-ai-provider

LANGUAGE: bash
CODE:
npm install workers-ai-provider

LANGUAGE: bash
CODE:
yarn add workers-ai-provider

----------------------------------------

TITLE: Generating Text with AI SDK
DESCRIPTION: This code snippet demonstrates how to generate text using various models with the AI SDK. It uses a custom component called PreviewSwitchProviders to showcase different provider implementations.

LANGUAGE: jsx
CODE:
<PreviewSwitchProviders />

----------------------------------------

TITLE: Importing getAIState Function from AI SDK RSC in JavaScript
DESCRIPTION: This snippet demonstrates how to import the getAIState function from the AI SDK RSC library. It's a crucial step for using the function in your project.

LANGUAGE: javascript
CODE:
import { getAIState } from "ai/rsc"

----------------------------------------

TITLE: Accessing Raw HTTP Request Bodies
DESCRIPTION: Demonstrates how to inspect raw HTTP request bodies sent to model providers for debugging purposes.

LANGUAGE: typescript
CODE:
const result = await generateText({
  model: openai('gpt-4o'),
  prompt: 'Hello, world!'
});

console.log(result.request.body);

----------------------------------------

TITLE: Importing HuggingFaceStream in React
DESCRIPTION: This snippet shows how to import the HuggingFaceStream function from the 'ai' package in a React application.

LANGUAGE: javascript
CODE:
import { HuggingFaceStream } from "ai"

----------------------------------------

TITLE: Importing Perplexity Provider in TypeScript
DESCRIPTION: Shows how to import the default Perplexity provider instance from the package.

LANGUAGE: typescript
CODE:
import { perplexity } from '@ai-sdk/perplexity';

----------------------------------------

TITLE: Configuring Response Headers for AI SDK Streaming
DESCRIPTION: Demonstrates how to add necessary HTTP headers to enable proper streaming functionality in deployed environments. Adds chunked transfer encoding and keep-alive connection headers to maintain the streaming connection.

LANGUAGE: tsx
CODE:
return result.toDataStreamResponse({
  headers: {
    'Transfer-Encoding': 'chunked',
    Connection: 'keep-alive',
  },
});

----------------------------------------

TITLE: Importing Default Together.ai Provider Instance
DESCRIPTION: How to import the default Together.ai provider instance from the package.

LANGUAGE: typescript
CODE:
import { togetherai } from '@ai-sdk/togetherai';

----------------------------------------

TITLE: Accessing Message Annotations in Client Component
DESCRIPTION: Shows how to access and display message annotations in a React component using the useChat hook.

LANGUAGE: tsx
CODE:
import { Message, useChat } from '@ai-sdk/react';

const { messages } = useChat();

const result = (
  <>
    {messages?.map((m: Message) => (
      <div key={m.id}>
        {m.annotations && <>{JSON.stringify(m.annotations)}</>}
      </div>
    ))}
  </>
);

----------------------------------------

TITLE: Structured Output with Regex Pattern
DESCRIPTION: Example of using regex patterns to control LLM output format for specific character sets or formats.

LANGUAGE: typescript
CODE:
import { friendli } from '@friendliai/ai-provider';
import { generateText } from 'ai';

const { text } = await generateText({
  model: friendli('meta-llama-3.1-8b-instruct', {
    regex: new RegExp('[\n ,.?!0-9\uac00-\ud7af]*'),
  }),
  prompt: 'Who is the first king of the Joseon Dynasty?',
});

console.log(text);

----------------------------------------

TITLE: Using a Seed for Reproducible Image Generation
DESCRIPTION: Demonstrates how to use a seed value to control the output of the image generation process.

LANGUAGE: tsx
CODE:
import { experimental_generateImage as generateImage } from 'ai';
import { openai } from '@ai-sdk/openai';

const { image } = await generateImage({
  model: openai.image('dall-e-3'),
  prompt: 'Santa Claus driving a Cadillac',
  seed: 1234567890,
});

----------------------------------------

TITLE: Anthropic Provider Node.js Implementation
DESCRIPTION: Implementation of the Google Vertex Anthropic provider in Node.js runtime

LANGUAGE: typescript
CODE:
import { vertexAnthropic } from '@ai-sdk/google-vertex/anthropic';
import { generateText } from 'ai';

const { text } = await generateText({
  model: vertexAnthropic('claude-3-5-sonnet@20240620'),
  prompt: 'Write a vegetarian lasagna recipe.',
});

----------------------------------------

TITLE: Initializing Langfuse Exporter
DESCRIPTION: Constructor-based configuration of the Langfuse exporter with API keys and base URL

LANGUAGE: typescript
CODE:
import { LangfuseExporter } from 'langfuse-vercel';

new LangfuseExporter({
  secretKey: 'sk-lf-...',
  publicKey: 'pk-lf-...',
  baseUrl: 'https://cloud.langfuse.com',
});

----------------------------------------

TITLE: Initializing Portkey Language Models in TypeScript
DESCRIPTION: Shows how to initialize chat and completion models using the Portkey provider.

LANGUAGE: typescript
CODE:
const chatModel = portkey.chatModel('');
const completionModel = portkey.completionModel('');

----------------------------------------

TITLE: Configuring Luma Image Model Settings
DESCRIPTION: Shows how to configure image model settings including max images per call, polling interval, and max poll attempts.

LANGUAGE: typescript
CODE:
const model = luma.image('photon-1', {
  maxImagesPerCall: 1, // Maximum number of images to generate per API call
  pollIntervalMillis: 5000, // How often to check for completed images (in ms)
  maxPollAttempts: 10, // Maximum number of polling attempts before timeout
});

----------------------------------------

TITLE: Importing createStreamableValue from AI SDK RSC
DESCRIPTION: Shows how to import the createStreamableValue function from the AI SDK RSC package.

LANGUAGE: javascript
CODE:
import { createStreamableValue } from "ai/rsc"

----------------------------------------

TITLE: Importing SambaNova Provider
DESCRIPTION: Basic import of the default SambaNova provider instance.

LANGUAGE: typescript
CODE:
import { sambanova } from 'sambanova-ai-provider';

----------------------------------------

TITLE: Detailed Error Logging for Anthropic API Call in TypeScript
DESCRIPTION: This snippet demonstrates comprehensive error logging for a failed Anthropic API call. It includes the full error stack trace, request details, and response information, providing valuable debugging information for API integration issues.

LANGUAGE: typescript
CODE:
APICallError [AI_APICallError]: Failed to process error response
    at postToApi (/Users/larsgrammel/repositories/ai/packages/provider-utils/dist/index.js:382:15)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    ... 4 lines matching cause stack trace ...
    at async fn (/Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:2723:36)
    at async /Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:339:22
    at async main (/Users/larsgrammel/repositories/ai/examples/ai-core/src/generate-text/anthropic-cache-control.ts:54:361) {
  cause: TypeError: Body is unusable
      at consumeBody (node:internal/deps/undici/undici:4281:15)
      at _Response.text (node:internal/deps/undici/undici:4236:18)
      at /Users/larsgrammel/repositories/ai/packages/provider-utils/dist/index.js:443:39
      at postToApi (/Users/larsgrammel/repositories/ai/packages/provider-utils/dist/index.js:373:34)
      at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
      at async AnthropicMessagesLanguageModel.doGenerate (/Users/larsgrammel/repositories/ai/packages/anthropic/dist/index.js:316:50)
      at async fn (/Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:2748:34)
      at async /Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:339:22
      at async _retryWithExponentialBackoff (/Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:170:12)
      at async fn (/Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:2723:36),
  url: 'https://api.anthropic.com/v1/messages',
  requestBodyValues: {
    model: 'claude-3-5-sonnet-20240620',
    top_k: undefined,
    max_tokens: 4096,
    temperature: 0,
    top_p: undefined,
    stop_sequences: undefined,
    system: undefined,
    messages: [ [Object] ],
    tools: undefined,
    tool_choice: undefined
  },
  statusCode: 400,
  responseHeaders: {
    'cf-cache-status': 'DYNAMIC',
    'cf-ray': '8b39b87a8f684541-TXL',
    connection: 'keep-alive',
    'content-length': '173',
    'content-type': 'application/json',
    date: 'Thu, 15 Aug 2024 14:02:08 GMT',
    'request-id': 'req_01YZqjpifTdvLZqfwBieLs44',
    server: 'cloudflare',
    via: '1.1 google',
    'x-cloud-trace-context': '00f2b1629d0dc8c6a4714db1dbdb4c2c',
    'x-robots-tag': 'none',
    'x-should-retry': 'false'
  },
  responseBody: undefined,
  isRetryable: false,
  data: undefined,
  [Symbol(vercel.ai.error)]: true,
  [Symbol(vercel.ai.error.AI_APICallError)]: true
}

----------------------------------------

TITLE: Checking for AI_InvalidResponseDataError Instance in TypeScript
DESCRIPTION: This code snippet demonstrates how to check if an error is an instance of AI_InvalidResponseDataError using the isInstance method. It's useful for error handling in applications using the 'ai' library.

LANGUAGE: typescript
CODE:
import { InvalidResponseDataError } from 'ai';

if (InvalidResponseDataError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Streaming UI Components with Llama 3.1 and Next.js
DESCRIPTION: This example shows how to use the streamUI function from AI SDK RSC to create dynamic, server-streamed React components with Llama 3.1 and Next.js App Router.

LANGUAGE: tsx
CODE:
'use server';

import { streamUI } from 'ai/rsc';
import { deepinfra } from '@ai-sdk/deepinfra';
import { z } from 'zod';

export async function streamComponent() {
  const result = await streamUI({
    model: deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),
    prompt: 'Get the weather for San Francisco',
    text: ({ content }) => <div>{content}</div>,
    tools: {
      getWeather: {
        description: 'Get the weather for a location',
        parameters: z.object({ location: z.string() }),
        generate: async function* ({ location }) {
          yield <div>loading...</div>;
          const weather = '25c'; // await getWeather(location);
          return (
            <div>
              the weather in {location} is {weather}.
            </div>
          );
        },
      },
    },
  });
  return result.value;
}

----------------------------------------

TITLE: Importing Anthropic Provider
DESCRIPTION: How to import the default Anthropic provider instance from the package.

LANGUAGE: typescript
CODE:
import { anthropic } from '@ai-sdk/anthropic';

----------------------------------------

TITLE: Starting the Development Server
DESCRIPTION: Command to start the development server

LANGUAGE: sh
CODE:
pnpm dev

----------------------------------------

TITLE: Checking AI_LoadSettingError Instance in TypeScript
DESCRIPTION: Demonstrates how to check if an error is an instance of AI_LoadSettingError using the isInstance static method. This pattern is useful for error handling and type checking in TypeScript applications.

LANGUAGE: typescript
CODE:
import { LoadSettingError } from 'ai';

if (LoadSettingError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Defining Schema for Notifications using Zod in TypeScript
DESCRIPTION: This snippet defines a schema for notifications using the Zod library. It creates a structure for an array of notifications, each containing a name and a message.

LANGUAGE: typescript
CODE:
import { z } from 'zod';

// define a schema for the notifications
export const notificationSchema = z.object({
  notifications: z.array(
    z.object({
      name: z.string().describe('Name of a fictional person.'),
      message: z.string().describe('Message. Do not use emojis or links.'),
    }),
  ),
});

----------------------------------------

TITLE: Implementing Text Completion API Route
DESCRIPTION: Server-side implementation of the completion API endpoint using OpenAI. Handles streaming text responses with a 30-second duration limit.

LANGUAGE: ts
CODE:
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export async function POST(req: Request) {
  const { prompt }: { prompt: string } = await req.json();

  const result = streamText({
    model: openai('gpt-3.5-turbo'),
    prompt,
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Installing LangWatch with Package Managers
DESCRIPTION: Commands for installing the LangWatch package using different package managers (pnpm, npm, yarn).

LANGUAGE: bash
CODE:
pnpm add langwatch

LANGUAGE: bash
CODE:
npm install langwatch

LANGUAGE: bash
CODE:
yarn add langwatch

----------------------------------------

TITLE: Creating Portkey Provider Instance in TypeScript
DESCRIPTION: Demonstrates how to create a Portkey provider instance using the createPortkey function, including configuration for the provider, API key, and model parameters.

LANGUAGE: typescript
CODE:
import { createPortkey } from '@portkey-ai/vercel-provider';

const portkeyConfig = {
  provider: 'openai', //enter provider of choice
  api_key: 'OPENAI_API_KEY', //enter the respective provider's api key
  override_params: {
    model: 'gpt-4', //choose from 250+ LLMs
  },
};

const portkey = createPortkey({
  apiKey: 'YOUR_PORTKEY_API_KEY',
  config: portkeyConfig,
});

----------------------------------------

TITLE: Generating Text with Azure OpenAI Language Model
DESCRIPTION: Illustrates how to use the Azure OpenAI language model to generate text using the generateText function.

LANGUAGE: typescript
CODE:
import { azure } from '@ai-sdk/azure';
import { generateText } from 'ai';

const { text } = await generateText({
  model: azure('your-deployment-name'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Closing StreamData
DESCRIPTION: Shows how to use the close method to finalize and close the stream data.

LANGUAGE: typescript
CODE:
data.close();

----------------------------------------

TITLE: Checking AI_InvalidToolArgumentsError Type in TypeScript
DESCRIPTION: Demonstrates how to check if an error is an instance of AI_InvalidToolArgumentsError using the isInstance static method. This is useful for error handling and type narrowing in TypeScript applications.

LANGUAGE: typescript
CODE:
import { InvalidToolArgumentsError } from 'ai';

if (InvalidToolArgumentsError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Creating Next.js App with Yarn for AI SDK and OpenAI Example
DESCRIPTION: Command to create a new Next.js application using Yarn, bootstrapping it with the AI SDK and OpenAI example from the Vercel AI repository.

LANGUAGE: bash
CODE:
yarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app

----------------------------------------

TITLE: Configuring Environment Variables for AI SDK in Nest.js
DESCRIPTION: Creates a .env file with the necessary API key for OpenAI. Additional settings may be required depending on the providers being used.

LANGUAGE: sh
CODE:
OPENAI_API_KEY="YOUR_OPENAI_API_KEY"

----------------------------------------

TITLE: Configuring Additional fal Provider Options
DESCRIPTION: Example showing how to pass additional provider-specific options when generating images. Uses the recraft-v3 model with custom style and size settings.

LANGUAGE: typescript
CODE:
const { image } = await generateImage({
  model: fal.image('fal-ai/recraft-v3'),
  prompt: 'A cat wearing a intricate robe',
  size: '1920x1080',
  providerOptions: {
    fal: {
      style: 'digital_illustration',
    },
  },
});

----------------------------------------

TITLE: Installing Dependencies with PNPM
DESCRIPTION: Commands for installing global PNPM package manager using NPM or Homebrew.

LANGUAGE: bash
CODE:
npm install -g pnpm@9

----------------------------------------

TITLE: Checking for AI_NoContentGeneratedError in TypeScript
DESCRIPTION: This snippet demonstrates how to check if an error is an instance of AI_NoContentGeneratedError using the isInstance method. It's useful for handling specific AI content generation errors in your application.

LANGUAGE: typescript
CODE:
import { NoContentGeneratedError } from 'ai';

if (NoContentGeneratedError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Creating Provider Registry with Custom Separator in TypeScript
DESCRIPTION: This example demonstrates how to create a provider registry with a custom separator between provider and model IDs.

LANGUAGE: typescript
CODE:
import { anthropic } from '@ai-sdk/anthropic';
import { openai } from '@ai-sdk/openai';

export const customSeparatorRegistry = createProviderRegistry(
  {
    anthropic,
    openai,
  },
  { separator: ' > ' },
);

----------------------------------------

TITLE: Implementing Client-Side Chat Interface with React
DESCRIPTION: A React client component that creates a chat interface with message input and display functionality. Uses useState for message management and connects to server actions for message submission.

LANGUAGE: tsx
CODE:
'use client';

import { useState } from 'react';
import { ClientMessage, submitMessage } from './actions';
import { useActions } from 'ai/rsc';

export default function Home() {
  const [input, setInput] = useState('');
  const [messages, setMessages] = useState<ClientMessage[]>([]);
  const { submitMessage } = useActions();

  const handleSubmission = async () => {
    setMessages(currentMessages => [
      ...currentMessages,
      {
        id: '123',
        status: 'user.message.created',
        text: input,
        gui: null,
      },
    ]);

    const response = await submitMessage(input);
    setMessages(currentMessages => [...currentMessages, response]);
    setInput('');
  };

  return (
    <div className="flex flex-col-reverse">
      <div className="flex flex-row gap-2 p-2 bg-zinc-100 w-full">
        <input
          className="bg-zinc-100 w-full p-2 outline-none"
          value={input}
          onChange={event => setInput(event.target.value)}
          placeholder="Ask a question"
          onKeyDown={event => {
            if (event.key === 'Enter') {
              handleSubmission();
            }
          }}
        />
        <button
          className="p-2 bg-zinc-900 text-zinc-100 rounded-md"
          onClick={handleSubmission}
        >
          Send
        </button>
      </div>

      <div className="flex flex-col h-[calc(100dvh-56px)] overflow-y-scroll">
        <div>
          {messages.map(message => (
            <div key={message.id} className="flex flex-col gap-1 border-b p-2">
              <div className="flex flex-row justify-between">
                <div className="text-sm text-zinc-500">{message.status}</div>
              </div>
              <div className="flex flex-col gap-2">{message.gui}</div>
              <div>{message.text}</div>
            </div>
          ))}
        </div>
      </div>
    </div>
  );

----------------------------------------

TITLE: Generating Text with AI SDK Core and OpenAI
DESCRIPTION: TypeScript example demonstrating how to use the AI SDK Core with OpenAI provider to generate text based on a prompt.

LANGUAGE: typescript
CODE:
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai'; // Ensure OPENAI_API_KEY environment variable is set

const { text } = await generateText({
  model: openai('gpt-4o'),
  system: 'You are a friendly assistant!',
  prompt: 'Why is the sky blue?',
});

console.log(text);

----------------------------------------

TITLE: Ending LLM Span and Capturing Metrics in LangWatch
DESCRIPTION: TypeScript code for ending an LLM span in LangWatch, capturing output and token metrics for cost calculation.

LANGUAGE: typescript
CODE:
span.end({
  output: {
    type: 'chat_messages',
    value: [chatCompletion.choices[0]!.message],
  },
  metrics: {
    promptTokens: chatCompletion.usage?.prompt_tokens,
    completionTokens: chatCompletion.usage?.completion_tokens,
  },
});

----------------------------------------

TITLE: Generating Text with FriendliAI
DESCRIPTION: Example of using FriendliAI to generate text responses using the generateText function.

LANGUAGE: typescript
CODE:
import { friendli } from '@friendliai/ai-provider';
import { generateText } from 'ai';

const { text } = await generateText({
  model: friendli('meta-llama-3.1-8b-instruct'),
  prompt: 'What is the meaning of life?',
});

console.log(text);

----------------------------------------

TITLE: Using OpenAI Reasoning Models
DESCRIPTION: Example of using OpenAI reasoning models with specific settings and accessing metadata.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

const { text, usage, providerMetadata } = await generateText({
  model: openai('o3-mini'),
  prompt: 'Invent a new holiday and describe its traditions.',
  providerOptions: {
    openai: {
      reasoningEffort: 'low',
    },
  },
});

console.log(text);
console.log('Usage:', {
  ...usage,
  reasoningTokens: providerMetadata?.openai?.reasoningTokens,
});

----------------------------------------

TITLE: Importing Custom Provider Module
DESCRIPTION: Code snippet showing how to import the customProvider function from the AI package.

LANGUAGE: typescript
CODE:
import {  customProvider } from "ai"

----------------------------------------

TITLE: Migrating from baseUrl to baseURL
DESCRIPTION: Example of updating the baseUrl option to baseURL in provider configuration.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
const perplexity = createOpenAI({
  // ...
  baseUrl: 'https://api.perplexity.ai/',
});

// AI SDK 4.0
const perplexity = createOpenAI({
  // ...
  baseURL: 'https://api.perplexity.ai/',
});

----------------------------------------

TITLE: Migrating topK Setting for Google Vertex
DESCRIPTION: Example of updating the topK setting from model-specific to standard for Google Vertex.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
const result = await generateText({
  model: vertex('gemini-1.5-flash', {
    topK: 0.5,
  }),
});

// AI SDK 4.0
const result = await generateText({
  model: vertex('gemini-1.5-flash'),
  topK: 0.5,
});

----------------------------------------

TITLE: Importing DeepInfra Provider
DESCRIPTION: Code snippet showing how to import the default DeepInfra provider instance.

LANGUAGE: typescript
CODE:
import { deepinfra } from '@ai-sdk/deepinfra';

----------------------------------------

TITLE: Installing Project with NPX
DESCRIPTION: Command to create a new Next.js application using npx create-next-app with the OpenAI telemetry template

LANGUAGE: bash
CODE:
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry next-openai-telemetry-app

----------------------------------------

TITLE: MCP Client Usage Example - TypeScript
DESCRIPTION: Demonstrates how to create and use an MCP client with tool integration and error handling. Shows connection setup, tool retrieval, text generation, and proper cleanup.

LANGUAGE: typescript
CODE:
import { experimental_createMCPClient, generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

try {
  const client = await experimental_createMCPClient({
    transport: {
      type: 'stdio',
      command: 'node server.js',
    },
  });

  const tools = await client.tools();

  const response = await generateText({
    model: openai('gpt-4o-mini'),
    tools,
    messages: [{ role: 'user', content: 'Query the data' }],
  });

  console.log(response);
} finally {
  await client.close();
}

----------------------------------------

TITLE: Creating Custom Provider with Model Settings in TypeScript
DESCRIPTION: This snippet demonstrates how to create a custom provider using the AI SDK to override default model settings and provide model name aliases with pre-configured settings.

LANGUAGE: typescript
CODE:
import { openai as originalOpenAI } from '@ai-sdk/openai';
import { customProvider } from 'ai';

// custom provider with different model settings:
export const openai = customProvider({
  languageModels: {
    // replacement model with custom settings:
    'gpt-4o': originalOpenAI('gpt-4o', { structuredOutputs: true }),
    // alias model with custom settings:
    'gpt-4o-mini-structured': originalOpenAI('gpt-4o-mini', {
      structuredOutputs: true,
    }),
  },
  fallbackProvider: originalOpenAI,
});

----------------------------------------

TITLE: Logging and Deleting Meals Using AI Tools in a Multistep Interface
DESCRIPTION: Demonstrates how to use log_meal and delete_meal tools in a meal tracking application, showcasing the importance of maintaining application context across multiple steps.

LANGUAGE: txt
CODE:
User: Log a chicken shawarma for lunch.
Tool: log_meal("chicken shawarma", "250g", "12:00 PM")
Model: Chicken shawarma has been logged for lunch.
...
...
User: I skipped lunch today, can you update my log?
Tool: delete_meal("chicken shawarma")
Model: Chicken shawarma has been deleted from your log.

----------------------------------------

TITLE: Migrating parseStreamPart
DESCRIPTION: Example of updating from parseStreamPart to parseDataStreamPart.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
const part = parseStreamPart(line);

// AI SDK 4.0
const part = parseDataStreamPart(line);

----------------------------------------

TITLE: Configuring Environment Variables for AI Core Examples
DESCRIPTION: Sets up the necessary environment variables for running AI core examples, including API keys for different providers.

LANGUAGE: sh
CODE:
OPENAI_API_KEY="YOUR_OPENAI_API_KEY"
...

----------------------------------------

TITLE: Setting FriendliAI Credentials
DESCRIPTION: Environment variable configuration for FriendliAI authentication token.

LANGUAGE: bash
CODE:
export FRIENDLI_TOKEN="YOUR_FRIENDLI_TOKEN"

----------------------------------------

TITLE: Importing OpenAI Provider
DESCRIPTION: Import the default OpenAI provider instance.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';

----------------------------------------

TITLE: Importing valibotSchema from AI SDK in TypeScript
DESCRIPTION: This snippet shows how to import the valibotSchema function from the 'ai' package.

LANGUAGE: typescript
CODE:
import { valibotSchema } from "ai"

----------------------------------------

TITLE: Text Stream Implementation
DESCRIPTION: Example demonstrating how to implement text streaming using pipeTextStreamToResponse. Streams AI-generated text directly to the client response.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import { createServer } from 'http';

createServer(async (req, res) => {
  const result = streamText({
    model: openai('gpt-4o'),
    prompt: 'Invent a new holiday and describe its traditions.',
  });

  result.pipeTextStreamToResponse(res);
}).listen(8080);

----------------------------------------

TITLE: Installing Next.js App with PNPM
DESCRIPTION: Command to create a new Next.js application using PNPM and the AI SDK template repository.

LANGUAGE: bash
CODE:
pnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-fastapi next-fastapi-app

----------------------------------------

TITLE: Streaming Structured Data with File Buffer Image Prompt in Node.js
DESCRIPTION: This code snippet shows how to use the AI SDK to stream structured data responses from a language model using a file buffer as an image prompt. It uses the OpenAI GPT-4 Turbo model and defines a schema for passport stamp data, reading the image from a local file.

LANGUAGE: typescript
CODE:
import { streamObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import dotenv from 'dotenv';
import { z } from 'zod';

dotenv.config();

async function main() {
  const { partialObjectStream } = streamObject({
    model: openai('gpt-4-turbo'),
    maxTokens: 512,
    schema: z.object({
      stamps: z.array(
        z.object({
          country: z.string(),
          date: z.string(),
        }),
      ),
    }),
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'text',
            text: 'list all the stamps in these passport pages?',
          },
          {
            type: 'image',
            image: fs.readFileSync('./node/attachments/eclipse.jpg'),
          },
        ],
      },
    ],
  });

  for await (const partialObject of partialObjectStream) {
    console.clear();
    console.log(partialObject);
  }
}

main();

----------------------------------------

TITLE: Defining ToolCallPart Interface in TypeScript
DESCRIPTION: Defines the structure of a tool call content part in a prompt, typically generated by the AI model. It includes fields for the tool call ID, tool name, and arguments for the tool call.

LANGUAGE: typescript
CODE:
export interface ToolCallPart {
  type: 'tool-call';

  /**
   * ID of the tool call. This ID is used to match the tool call with the tool result.
   */
  toolCallId: string;

  /**
   * Name of the tool that is being called.
   */
  toolName: string;

  /**
   * Arguments of the tool call. This is a JSON-serializable object that matches the tool's input schema.
   */
  args: unknown;
}

----------------------------------------

TITLE: Using Groq's DeepSeek R1 Model with Middleware
DESCRIPTION: This example demonstrates how to use Groq's DeepSeek R1 model with the AI SDK. It includes middleware configuration to extract reasoning tokens from the model's output.

LANGUAGE: typescript
CODE:
import { groq } from '@ai-sdk/groq';
import {
  generateText,
  wrapLanguageModel,
  extractReasoningMiddleware,
} from 'ai';

// middleware to extract reasoning tokens
const enhancedModel = wrapLanguageModel({
  model: groq('deepseek-r1-distill-llama-70b'),
  middleware: extractReasoningMiddleware({ tagName: 'think' }),
});

const { reasoning, text } = await generateText({
  model: enhancedModel,
  prompt: 'Explain quantum entanglement.',
});

----------------------------------------

TITLE: Character-Based Image Generation
DESCRIPTION: Shows how to generate images with consistent character representation using reference images.

LANGUAGE: typescript
CODE:
await generateImage({
  model: luma.image('photon-1'),
  prompt: 'A woman with a cat riding a broomstick in a forest',
  providerOptions: {
    luma: {
      character_ref: {
        identity0: {
          images: ['https://example.com/character.jpg'],
        },
      },
    },
  },
});

----------------------------------------

TITLE: Importing Azure Provider
DESCRIPTION: TypeScript code showing how to import the default Azure provider instance

LANGUAGE: typescript
CODE:
import { azure } from '@ai-sdk/azure';

----------------------------------------

TITLE: Installing Next.js Project with pnpm
DESCRIPTION: Command to create a new Next.js application using pnpm create next-app with the LangChain template

LANGUAGE: bash
CODE:
pnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-langchain next-langchain-app

----------------------------------------

TITLE: Setting up OpenAI API Environment Variables
DESCRIPTION: Configuration of environment variables required for OpenAI API integration

LANGUAGE: sh
CODE:
OPENAI_API_KEY="YOUR_OPENAI_API_KEY"

----------------------------------------

TITLE: Generating Structured Objects with Chrome AI - JavaScript
DESCRIPTION: Example of using Chrome AI with generateObject function to create structured data using Zod schema validation.

LANGUAGE: javascript
CODE:
import { generateObject } from 'ai';
import { chromeai } from 'chrome-ai';
import { z } from 'zod';

const { object } = await generateObject({
  model: chromeai('text'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(
        z.object({
          name: z.string(),
          amount: z.string(),
        }),
      ),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a lasagna recipe.',
});

console.log(object);
// { recipe: {...} }

----------------------------------------

TITLE: Importing Default Azure OpenAI Provider Instance
DESCRIPTION: Demonstrates how to import the default Azure OpenAI provider instance.

LANGUAGE: typescript
CODE:
import { azure } from '@ai-sdk/azure';

----------------------------------------

TITLE: Importing embedMany Function
DESCRIPTION: Shows how to import the embedMany function from the ai package.

LANGUAGE: typescript
CODE:
import { embedMany } from "ai"

----------------------------------------

TITLE: Filtering End-to-End Tests
DESCRIPTION: Runs a subset of end-to-end tests by applying a filter to the test cases.

LANGUAGE: sh
CODE:
pnpm run test:file src/e2e/google.test.ts -t stream

----------------------------------------

TITLE: Accessing Token Usage via Promise in TypeScript
DESCRIPTION: Shows how to access token usage through the usage promise property of the streamObject result. Includes examples of both Promise-based and async/await approaches, along with proper stream consumption.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamObject, TokenUsage } from 'ai';
import { z } from 'zod';

const result = streamObject({
  model: openai('gpt-4-turbo'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.string()),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a lasagna recipe.',
});

// your custom function to record token usage:
function recordTokenUsage({
  promptTokens,
  completionTokens,
  totalTokens,
}: TokenUsage) {
  console.log('Prompt tokens:', promptTokens);
  console.log('Completion tokens:', completionTokens);
  console.log('Total tokens:', totalTokens);
}

// use as promise:
result.usage.then(recordTokenUsage);

// use with async/await:
recordTokenUsage(await result.usage);

// note: the stream needs to be consumed because of backpressure
for await (const partialObject of result.partialObjectStream) {
}

----------------------------------------

TITLE: Running All End-to-End Provider Integration Tests
DESCRIPTION: Executes all end-to-end provider integration tests located in the src/e2e directory.

LANGUAGE: sh
CODE:
pnpm run test:e2e:all

----------------------------------------

TITLE: Changelog Entry for Version 1.2.4
DESCRIPTION: Changelog entry documenting dependency update for provider-utils package

LANGUAGE: markdown
CODE:
## 1.2.4

### Patch Changes

- Updated dependencies [28be004]
  - @ai-sdk/provider-utils@2.2.3

----------------------------------------

TITLE: Gemini Search Grounding Implementation
DESCRIPTION: Example of implementing web search with Gemini models using Google search grounding, including metadata and safety ratings access.

LANGUAGE: typescript
CODE:
import { google } from '@ai-sdk/google';
import { generateText } from 'ai';

const { text, sources, providerMetadata } = await generateText({
  model: google('gemini-1.5-pro', {
    useSearchGrounding: true,
  }),
  prompt:
    'List the top 5 San Francisco news from the past week.' +
    'You must include the date of each article.',
});

console.log(text);
console.log(sources);

// access the grounding metadata.
const metadata = providerMetadata?.google;
const groundingMetadata = metadata?.groundingMetadata;
const safetyRatings = metadata?.safetyRatings;

----------------------------------------

TITLE: Importing generateId Module
DESCRIPTION: Shows the basic import statement for the generateId function from the AI package.

LANGUAGE: typescript
CODE:
import { generateId } from "ai"

----------------------------------------

TITLE: Tool Calling Implementation with o3-mini
DESCRIPTION: Shows how to implement tool calling functionality with o3-mini, including tool definition and execution.

LANGUAGE: tsx
CODE:
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';

const { text } = await generateText({
  model: openai('o3-mini'),
  prompt: 'What is the weather like today in San Francisco?',
  tools: {
    getWeather: tool({
      description: 'Get the weather in a location',
      parameters: z.object({
        location: z.string().describe('The location to get the weather for'),
      }),
      execute: async ({ location }) => ({
        location,
        temperature: 72 + Math.floor(Math.random() * 21) - 10,
      }),
    }),
  },
});

----------------------------------------

TITLE: Installing OpenAI Provider for AI SDK
DESCRIPTION: Command to install the OpenAI provider package for use with the AI SDK Core.

LANGUAGE: shell
CODE:
npm install @ai-sdk/openai

----------------------------------------

TITLE: Building the AI SDK Project
DESCRIPTION: These commands are used to install dependencies and build the AI SDK project from the root directory of the repository.

LANGUAGE: sh
CODE:
pnpm install
pnpm build

----------------------------------------

TITLE: Creating Custom Ollama Provider Instance
DESCRIPTION: Create a customized Ollama provider instance with specific settings like baseURL and headers.

LANGUAGE: typescript
CODE:
import { createOllama } from 'ollama-ai-provider';

const ollama = createOllama({
  // optional settings, e.g.
  baseURL: 'https://api.ollama.com',
});

----------------------------------------

TITLE: Customizing xAI Image Generation
DESCRIPTION: Generate multiple images with custom model-specific settings.

LANGUAGE: typescript
CODE:
import { xai } from '@ai-sdk/xai';
import { experimental_generateImage as generateImage } from 'ai';

const { images } = await generateImage({
  model: xai.image('grok-2-image', {
    maxImagesPerCall: 5, // Default is 10
  }),
  prompt: 'A futuristic cityscape at sunset',
  n: 2, // Generate 2 images
});

----------------------------------------

TITLE: Implementing Lazy Stream Generation in JavaScript
DESCRIPTION: This snippet shows a lazy approach to stream generation using ReadableStream's pull handler. It addresses back-pressure and cancellation issues by producing values on-demand.

LANGUAGE: jsx
CODE:
function createStream(iterator) {
  return new ReadableStream({
    async pull(controller) {
      const { value, done } = await iterator.next();

      if (done) {
        controller.close();
      } else {
        controller.enqueue(value);
      }
    },
  });
}

----------------------------------------

TITLE: Checking AI Empty Response Body Error in TypeScript
DESCRIPTION: Demonstrates how to check if an error is an instance of AI_EmptyResponseBodyError using the isInstance static method. This code snippet shows the proper error type checking pattern for handling empty response body errors in AI operations.

LANGUAGE: typescript
CODE:
import { EmptyResponseBodyError } from 'ai';

if (EmptyResponseBodyError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Defining Zod Schema for Notifications in TypeScript
DESCRIPTION: This snippet defines a Zod schema for notifications, which is used to structure the generated object data. It includes fields for name and message.

LANGUAGE: typescript
CODE:
import { z } from 'zod';

// define a schema for the notifications
export const notificationSchema = z.object({
  notifications: z.array(
    z.object({
      name: z.string().describe('Name of a fictional person.'),
      message: z.string().describe('Message. Do not use emojis or links.'),
    }),
  ),
});

----------------------------------------

TITLE: Generating Text with DeepInfra Model
DESCRIPTION: Example demonstrating how to use the DeepInfra provider to generate text using the Llama-3 model. Shows complete implementation including imports and model configuration.

LANGUAGE: typescript
CODE:
import { deepinfra } from '@ai-sdk/deepinfra';
import { generateText } from 'ai';

const { text } = await generateText({
  model: deepinfra('meta-llama/Llama-3.3-70B-Instruct'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Accessing Text Embedding Models from Provider Registry in TypeScript
DESCRIPTION: This snippet shows how to access text embedding models from the registry using the textEmbeddingModel method. It demonstrates the format for specifying the provider and model ID.

LANGUAGE: typescript
CODE:
import { embed } from 'ai';
import { registry } from './registry';

const { embedding } = await embed({
  model: registry.textEmbeddingModel('openai:text-embedding-3-small'),
  value: 'sunny day at the beach',
});

----------------------------------------

TITLE: Building a Chat Interface with AI SDK UI in Next.js
DESCRIPTION: Demonstrates how to create a chat interface using the AI SDK UI in a Next.js application. This component uses the 'useChat' hook to manage chat state and handle user interactions.

LANGUAGE: tsx
CODE:
'use client';

import { useChat } from '@ai-sdk/react';

export default function Page() {
  const { messages, input, handleInputChange, handleSubmit, error } = useChat();

  return (
    <>
      {messages.map(message => (
        <div key={message.id}>
          {message.role === 'user' ? 'User: ' : 'AI: '}
          {message.content}
        </div>
      ))}
      <form onSubmit={handleSubmit}>
        <input name="prompt" value={input} onChange={handleInputChange} />
        <button type="submit">Submit</button>
      </form>
    </>
  );
}

----------------------------------------

TITLE: Handling Assistant Thread Messages in Slack
DESCRIPTION: This function handles the 'assistant_thread_started' event in Slack. It posts a welcome message to the thread and sets up suggested prompts to help users get started with the AI assistant.

LANGUAGE: typescript
CODE:
import type { AssistantThreadStartedEvent } from '@slack/web-api';
import { client } from './slack-utils';

export async function assistantThreadMessage(
  event: AssistantThreadStartedEvent,
) {
  const { channel_id, thread_ts } = event.assistant_thread;
  console.log(`Thread started: ${channel_id} ${thread_ts}`);
  console.log(JSON.stringify(event));

  await client.chat.postMessage({
    channel: channel_id,
    thread_ts: thread_ts,
    text: "Hello, I'm an AI assistant built with the AI SDK by Vercel!",
  });

  await client.assistant.threads.setSuggestedPrompts({
    channel_id: channel_id,
    thread_ts: thread_ts,
    prompts: [
      {
        title: 'Get the weather',
        message: 'What is the current weather in London?',
      },
      {
        title: 'Get the news',
        message: 'What is the latest Premier League news from the BBC?',
      },
    ],
  });
}

----------------------------------------

TITLE: Checking for AI_MessageConversionError Instance in TypeScript
DESCRIPTION: This code snippet demonstrates how to check if an error is an instance of AI_MessageConversionError using the isInstance method. It imports the MessageConversionError from the 'ai' package and provides a conditional check for error handling.

LANGUAGE: typescript
CODE:
import { MessageConversionError } from 'ai';

if (MessageConversionError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Installing Next.js App with Yarn
DESCRIPTION: Command to create a new Next.js application using Yarn and the AI SDK template repository.

LANGUAGE: bash
CODE:
yarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-fastapi next-fastapi-app

----------------------------------------

TITLE: Creating Custom Qwen Provider Instance
DESCRIPTION: Import createQwen function and create a custom Qwen provider instance with optional settings.

LANGUAGE: typescript
CODE:
import { createQwen } from 'qwen-ai-provider';

const qwen = createQwen({
  // optional settings, e.g.
  // baseURL: 'https://qwen/api/v1',
});

----------------------------------------

TITLE: Configuring Provider Options and Metadata
DESCRIPTION: Demonstrates how to set provider-specific options and access response metadata including usage statistics and image information

LANGUAGE: typescript
CODE:
const result = await generateText({
  model: perplexity('sonar-pro'),
  prompt: 'What are the latest developments in quantum computing?',
  providerOptions: {
    perplexity: {
      return_images: true, // Enable image responses (Tier-2 Perplexity users only)
    },
  },
});

console.log(result.providerMetadata);

----------------------------------------

TITLE: Generating Text with Computer Tools in TypeScript
DESCRIPTION: Demonstrates how to use the computer tool with generateText function for one-shot text generation.

LANGUAGE: typescript
CODE:
const result = await generateText({
  model: anthropic('claude-3-5-sonnet-20241022'),
  prompt: 'Move the cursor to the center of the screen and take a screenshot',
  tools: { computer: computerTool },
});

console.log(response.text);

----------------------------------------

TITLE: Building and Running Stdio Transport Example
DESCRIPTION: These commands build and run the Stdio Transport example for the MCP and AI SDK integration.

LANGUAGE: sh
CODE:
pnpm stdio:build

LANGUAGE: sh
CODE:
pnpm stdio:client

----------------------------------------

TITLE: Managing Server-side AI State
DESCRIPTION: Shows how to access and update AI state within server actions using getAIState and getMutableAIState.

LANGUAGE: typescript
CODE:
import { getMutableAIState } from 'ai/rsc';

export async function sendMessage(message: string) {
  'use server';

  const history = getMutableAIState();

  history.update([...history.get(), { role: 'user', content: message }]);

  const response = await generateText({
    model: openai('gpt-3.5-turbo'),
    messages: history.get(),
  });

  history.done([...history.get(), { role: 'assistant', content: response }]);

  return response;
}

----------------------------------------

TITLE: Generating Text with DeepSeek Provider
DESCRIPTION: Example of using the DeepSeek provider to generate text using the deepseek-chat model

LANGUAGE: typescript
CODE:
import { deepseek } from '@ai-sdk/deepseek';
import { generateText } from 'ai';

const { text } = await generateText({
  model: deepseek('deepseek-chat'),
  prompt: 'Write a JavaScript function that sorts a list:',
});

----------------------------------------

TITLE: Checking AI NoSuchModelError in TypeScript
DESCRIPTION: Demonstrates how to check if an error is an instance of NoSuchModelError using the isInstance static method from the ai package.

LANGUAGE: typescript
CODE:
import { NoSuchModelError } from 'ai';

if (NoSuchModelError.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Generating Streaming Responses with AI SDK RSC on Server
DESCRIPTION: This server-side code snippet shows how to use the AI SDK RSC to generate streaming responses. It uses the streamText function to create a streamable value that can be updated and returned to the client.

LANGUAGE: typescript
CODE:
'use server';

import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { createStreamableValue } from 'ai/rsc';

export async function generateResponse(prompt: string) {
  const stream = createStreamableValue();

  (async () => {
    const { textStream } = streamText({
      model: openai('gpt-4o'),
      prompt,
    });

    for await (const text of textStream) {
      stream.update(text);
    }

    stream.done();
  })();

  return stream.value;
}

----------------------------------------

TITLE: Appending Message Annotation to StreamData
DESCRIPTION: Demonstrates how to use the appendMessageAnnotation method to add a message annotation to the stream data. The annotation must be of type JSONValue.

LANGUAGE: typescript
CODE:
data.appendMessageAnnotation(annotation: JSONValue)

----------------------------------------

TITLE: Streaming Text with Computer Tools in TypeScript
DESCRIPTION: Shows how to implement streaming responses with computer tools using streamText function.

LANGUAGE: typescript
CODE:
const result = streamText({
  model: anthropic('claude-3-5-sonnet-20241022'),
  prompt: 'Open the browser and navigate to vercel.com',
  tools: { computer: computerTool },
});

for await (const chunk of result.textStream) {
  console.log(chunk);
}

----------------------------------------

TITLE: Installing Dependencies and Building SvelteKit OpenAI Project
DESCRIPTION: Commands to install project dependencies and build the SvelteKit OpenAI integration project. These steps should be executed at the root of the cloned repository.

LANGUAGE: bash
CODE:
pnpm install
pnpm build

----------------------------------------

TITLE: Creating Next.js App with AI SDK and OpenAI using npx
DESCRIPTION: Command to create a new Next.js application with AI SDK and OpenAI integration using npx and create-next-app.

LANGUAGE: bash
CODE:
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-pages next-openai-app

----------------------------------------

TITLE: Creating NVIDIA NIM Provider Instance
DESCRIPTION: Example of creating a custom provider instance for NVIDIA NIM using the createOpenAICompatible function from @ai-sdk/openai-compatible.

LANGUAGE: typescript
CODE:
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

const nim = createOpenAICompatible({
  name: 'nim',
  baseURL: 'https://integrate.api.nvidia.com/v1',
  headers: {
    Authorization: `Bearer ${process.env.NIM_API_KEY}`,
  },
});

----------------------------------------

TITLE: Creating Custom Perplexity Provider
DESCRIPTION: Demonstrates how to create a custom Perplexity provider instance with specific configuration settings like API key

LANGUAGE: typescript
CODE:
import { createPerplexity } from '@ai-sdk/perplexity';

const perplexity = createPerplexity({
  apiKey: process.env.PERPLEXITY_API_KEY ?? '',
});

----------------------------------------

TITLE: Initializing Computer Tool with AI SDK in TypeScript
DESCRIPTION: Example showing how to set up the Computer Tool with configuration for screen dimensions and execute function implementation.

LANGUAGE: typescript
CODE:
import { anthropic } from '@ai-sdk/anthropic';
import { getScreenshot, executeComputerAction } from '@/utils/computer-use';

const computerTool = anthropic.tools.computer_20241022({
  displayWidthPx: 1920,
  displayHeightPx: 1080,
  execute: async ({ action, coordinate, text }) => {
    switch (action) {
      case 'screenshot': {
        return {
          type: 'image',
          data: getScreenshot(),
        };
      }
      default: {
        return executeComputerAction(action, coordinate, text);
      }
    }
  },
  experimental_toToolResultContent(result) {
    return typeof result === 'string'
      ? [{ type: 'text', text: result }]
      : [{ type: 'image', data: result.data, mimeType: 'image/png' }];
  },
});

----------------------------------------

TITLE: Demonstrating Cancellation Issues with Eager Streaming in JavaScript
DESCRIPTION: This code exemplifies cancellation problems in eager streaming approaches. It shows how the stream continues to produce values even after the consumer has stopped reading.

LANGUAGE: jsx
CODE:
// A generator that will yield positive integers
async function* integers() {
  let i = 1;
  while (true) {
    console.log(`yielding ${i}`);
    yield i++;

    await sleep(100);
  }
}
function sleep(ms) {
  return new Promise(resolve => setTimeout(resolve, ms));
}

// Wraps a generator into a ReadableStream
function createStream(iterator) {
  return new ReadableStream({
    async start(controller) {
      for await (const v of iterator) {
        controller.enqueue(v);
      }
      controller.close();
    },
  });
}
// Collect data from stream
async function run() {
  // Set up a stream that of integers
  const stream = createStream(integers());

  // Read values from our stream
  const reader = stream.getReader();
  // We're only reading 3 items this time:
  for (let i = 0; i < 3; i++) {
    // we know our stream is infinite, so there's no need to check `done`.
    const { value } = await reader.read();
    console.log(`read ${value}`);

    await sleep(1000);
  }
}
run();

----------------------------------------

TITLE: Custom Provider Configuration for Node.js
DESCRIPTION: Example of creating a custom provider instance with specific configuration options for Node.js runtime

LANGUAGE: typescript
CODE:
import { createVertex } from '@ai-sdk/google-vertex';
import { generateText } from 'ai';

const customProvider = createVertex({
  project: 'your-project-id',
  location: 'us-central1',
  googleAuthOptions: {
    credentials: {
      client_email: 'your-client-email',
      private_key: 'your-private-key',
    },
  },
});

const { text } = await generateText({
  model: customProvider('gemini-1.5-flash'),
  prompt: 'Write a vegetarian lasagna recipe.',
});

----------------------------------------

TITLE: Server-Side Message Processing with OpenAI Integration
DESCRIPTION: Implements server-side message handling with OpenAI integration, including stream processing and tool usage for stock information.

LANGUAGE: tsx
CODE:
'use server';

import { getAIState, getMutableAIState, streamUI } from 'ai/rsc';
import { openai } from '@ai-sdk/openai';
import { ReactNode } from 'react';
import { z } from 'zod';
import { generateId } from 'ai';
import { Stock } from '@ai-studio/components/stock';

export interface ServerMessage {
  role: 'user' | 'assistant' | 'function';
  content: string;
}

export interface ClientMessage {
  id: string;
  role: 'user' | 'assistant' | 'function';
  display: ReactNode;
}

export async function continueConversation(
  input: string,
): Promise<ClientMessage> {
  'use server';

  const history = getMutableAIState();

  const result = await streamUI({
    model: openai('gpt-3.5-turbo'),
    messages: [...history.get(), { role: 'user', content: input }],
    text: ({ content, done }) => {
      if (done) {
        history.done([
          ...history.get(),
          { role: 'user', content: input },
          { role: 'assistant', content },
        ]);
      }

      return <div>{content}</div>;
    },
    tools: {
      showStockInformation: {
        description:
          'Get stock information for symbol for the last numOfMonths months',
        parameters: z.object({
          symbol: z
            .string()
            .describe('The stock symbol to get information for'),
          numOfMonths: z
            .number()
            .describe('The number of months to get historical information for'),
        }),
        generate: async ({ symbol, numOfMonths }) => {
          history.done([
            ...history.get(),
            {
              role: 'function',
              name: 'showStockInformation',
              content: JSON.stringify({ symbol, numOfMonths }),
            },
          ]);

          return <Stock symbol={symbol} numOfMonths={numOfMonths} />;
        },
      },
    },
  });

  return {
    id: generateId(),
    role: 'assistant',
    display: result.value,
  };
}

----------------------------------------

TITLE: Importing StreamData in React
DESCRIPTION: Shows how to import the StreamData class from the 'ai' package in a React application.

LANGUAGE: typescript
CODE:
import { StreamData } from "ai"

----------------------------------------

TITLE: Environment Variables Configuration in Next.js
DESCRIPTION: Configuration of required environment variables for OpenAI and Postgres connections.

LANGUAGE: bash
CODE:
OPENAI_API_KEY="your_api_key_here"
POSTGRES_URL="..."
POSTGRES_PRISMA_URL="..."
POSTGRES_URL_NO_SSL="..."
POSTGRES_URL_NON_POOLING="..."
POSTGRES_USER="..."
POSTGRES_HOST="..."
POSTGRES_PASSWORD="..."
POSTGRES_DATABASE="..."

----------------------------------------

TITLE: Importing Groq Provider
DESCRIPTION: Code snippet showing how to import the default Groq provider instance.

LANGUAGE: typescript
CODE:
import { groq } from '@ai-sdk/groq';

----------------------------------------

TITLE: Initializing Nuxt Project with AI SDK and OpenAI Example
DESCRIPTION: Command to bootstrap the example project using nuxi. This creates a new Nuxt project with the AI SDK and OpenAI integration.

LANGUAGE: bash
CODE:
npx nuxi@latest init -t github:vercel/ai/examples/nuxt-openai nuxt-openai

----------------------------------------

TITLE: Image Modification with Luma
DESCRIPTION: Example of modifying existing images using text prompts and weight parameters.

LANGUAGE: typescript
CODE:
await generateImage({
  model: luma.image('photon-1'),
  prompt: 'transform the bike to a boat',
  providerOptions: {
    luma: {
      modify_image_ref: {
        url: 'https://example.com/image.jpg',
        weight: 1.0,
      },
    },
  },
});

----------------------------------------

TITLE: Using Versioned Models in Replicate
DESCRIPTION: Demonstrates how to use specific versioned models with Replicate for image generation

LANGUAGE: typescript
CODE:
import { replicate } from '@ai-sdk/replicate';
import { experimental_generateImage as generateImage } from 'ai';

const { image } = await generateImage({
  model: replicate.image(
    'bytedance/sdxl-lightning-4step:5599ed30703defd1d160a25a63321b4dec97101d98b4674bcc56e41f62f35637',
  ),
  prompt: 'The Loch Ness Monster getting a manicure',
});

----------------------------------------

TITLE: Chart Configuration Schema Definition
DESCRIPTION: Zod schema definition for chart configuration including visualization options and data mappings.

LANGUAGE: typescript
CODE:
export const configSchema = z.object({
  description: z.string().describe('Describe the chart. What is it showing? What is interesting about the way the data is displayed?'),
  takeaway: z.string().describe('What is the main takeaway from the chart?'),
  type: z.enum(['bar', 'line', 'area', 'pie']).describe('Type of chart'),
  title: z.string(),
  xKey: z.string().describe('Key for x-axis or category'),
  yKeys: z.array(z.string()).describe('Key(s) for y-axis values this is typically the quantitative column'),
  multipleLines: z.boolean().describe('For line charts only: whether the chart is comparing groups of data.').optional(),
  measurementColumn: z.string().describe('For line charts only: key for quantitative y-axis column to measure against (eg. values, counts etc.)').optional(),
  lineCategories: z.array(z.string()).describe('For line charts only: Categories used to compare different lines or data series. Each category represents a distinct line in the chart.').optional(),
  colors: z.record(z.string(), z.string()).describe('Mapping of data keys to color values for chart elements').optional(),
  legend: z.boolean().describe('Whether to show legend')
});

----------------------------------------

TITLE: Importing Google Provider Instance
DESCRIPTION: How to import the default Google provider instance from the AI SDK package.

LANGUAGE: typescript
CODE:
import { google } from '@ai-sdk/google';

----------------------------------------

TITLE: Legacy 'render' Function Usage in Server Action (TypeScript)
DESCRIPTION: This example demonstrates the use of the 'render' function with the OpenAI provider in a Server Action, including a custom tool for fetching weather information.

LANGUAGE: tsx
CODE:
import { render } from 'ai/rsc';
import OpenAI from 'openai';
import { z } from 'zod';
import { Spinner, Weather } from '@/components';
import { getWeather } from '@/utils';

const openai = new OpenAI();

async function submitMessage(userInput = 'What is the weather in SF?') {
  'use server';

  return render({
    provider: openai,
    model: 'gpt-4-turbo',
    messages: [
      { role: 'system', content: 'You are a helpful assistant' },
      { role: 'user', content: userInput },
    ],
    text: ({ content }) => <p>{content}</p>,
    tools: {
      get_city_weather: {
        description: 'Get the current weather for a city',
        parameters: z
          .object({
            city: z.string().describe('the city'),
          })
          .required(),
        render: async function* ({ city }) {
          yield <Spinner />;
          const weather = await getWeather(city);
          return <Weather info={weather} />;
        },
      },
    },
  });
}

----------------------------------------

TITLE: Importing extractReasoningMiddleware Module
DESCRIPTION: Simple import statement for the extractReasoningMiddleware function from the ai package.

LANGUAGE: typescript
CODE:
import { extractReasoningMiddleware } from "ai"

----------------------------------------

TITLE: Rendering Index Cards for AI Foundations in JSX
DESCRIPTION: This code snippet creates an IndexCards component with multiple cards, each representing a foundational topic in AI and LLMs. Each card contains a title, description, and a link to more detailed documentation.

LANGUAGE: jsx
CODE:
<IndexCards
  cards={[
    {
      title: 'Overview',
      description: 'Learn about foundational concepts around AI and LLMs.',
      href: '/docs/foundations/overview',
    },
    {
      title: 'Providers and Models',
      description:
        'Learn about the providers and models that you can use with the AI SDK.',
      href: '/docs/foundations/providers-and-models',
    },
    {
      title: 'Prompts',
      description:
        'Learn about how Prompts are used and defined in the AI SDK.',
      href: '/docs/foundations/prompts',
    },
    {
      title: 'Tools',
      description: 'Learn about tools in the AI SDK.',
      href: '/docs/foundations/tools',
    },
    {
      title: 'Streaming',
      description: 'Learn why streaming is used for AI applications.',
      href: '/docs/foundations/streaming',
    },
    {
      title: 'Agents',
      description: 'Learn how to build agents with the AI SDK.',
      href: '/docs/foundations/agents',
    },
  ]}
/>

----------------------------------------

TITLE: Implementing No-Store Cache Strategy for Vercel Deployment
DESCRIPTION: Workaround for Vercel deployments using the unstable_noStore directive to prevent caching and enable proper streaming functionality.

LANGUAGE: tsx
CODE:
import { unstable_noStore as noStore } from 'next/cache';

export default async function Component() {
  noStore();
  const result = await generateText({...})
  ...
}

----------------------------------------

TITLE: Starting Development Server
DESCRIPTION: Commands for starting the development server, with an option to automatically open in a browser

LANGUAGE: bash
CODE:
npm run dev

# or start the server and open the app in a new browser tab
npm run dev -- --open

----------------------------------------

TITLE: Creating Custom Luma Provider Instance
DESCRIPTION: Creates a customized Luma provider instance with specific settings including API key, base URL, and custom headers.

LANGUAGE: typescript
CODE:
import { createLuma } from '@ai-sdk/luma';

const luma = createLuma({
  apiKey: 'your-api-key', // optional, defaults to LUMA_API_KEY environment variable
  baseURL: 'custom-url', // optional
  headers: {
    /* custom headers */
  }, // optional
});

----------------------------------------

TITLE: Accessing Cache Token Usage Metrics - TypeScript
DESCRIPTION: Shows how to access DeepSeek's cache hit/miss metrics through providerMetadata for monitoring token usage efficiency.

LANGUAGE: typescript
CODE:
import { deepseek } from '@ai-sdk/deepseek';
import { generateText } from 'ai';

const result = await generateText({
  model: deepseek('deepseek-chat'),
  prompt: 'Your prompt here',
});

console.log(result.providerMetadata);
// Example output: { deepseek: { promptCacheHitTokens: 1856, promptCacheMissTokens: 5 } }

----------------------------------------

TITLE: Importing OpenAI Provider
DESCRIPTION: How to import the default OpenAI provider instance

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';

----------------------------------------

TITLE: Implementing LlamaIndex Completion API Route in Next.js
DESCRIPTION: Sets up an API route using LlamaIndex's ChatEngine with OpenAI integration. Uses the LlamaIndexAdapter to stream completion responses to the client. Requires OpenAI and LlamaIndex dependencies.

LANGUAGE: typescript
CODE:
import { OpenAI, SimpleChatEngine } from 'llamaindex';
import { LlamaIndexAdapter } from 'ai';

export const maxDuration = 60;

export async function POST(req: Request) {
  const { prompt } = await req.json();

  const llm = new OpenAI({ model: 'gpt-4o' });
  const chatEngine = new SimpleChatEngine({ llm });

  const stream = await chatEngine.chat({
    message: prompt,
    stream: true,
  });

  return LlamaIndexAdapter.toDataStreamResponse(stream);
}

----------------------------------------

TITLE: Changelog Entries in Markdown
DESCRIPTION: Complete changelog detailing version updates and changes for the @ai-sdk/mistral package, including dependency updates, new features like PDF support, raw response access, and image support.

LANGUAGE: markdown
CODE:
# @ai-sdk/mistral\n\n## 1.2.3\n\n### Patch Changes\n\n- Updated dependencies [28be004]\n  - @ai-sdk/provider-utils@2.2.3\n\n## 1.2.2\n\n### Patch Changes\n\n- Updated dependencies [b01120e]\n  - @ai-sdk/provider-utils@2.2.2\n\n[...additional changelog entries...]

----------------------------------------

TITLE: Using Image Models with Provider Registry in TypeScript
DESCRIPTION: This snippet shows how to access image models using the imageModel method on the provider registry.

LANGUAGE: typescript
CODE:
import { generateImage } from 'ai';
import { registry } from './registry';

const { image } = await generateImage({
  model: registry.imageModel('openai:dall-e-3'),
  prompt: 'A beautiful sunset over a calm ocean',
});

----------------------------------------

TITLE: Importing simulateStreamingMiddleware in TypeScript
DESCRIPTION: Shows how to import the simulateStreamingMiddleware function from the 'ai' library.

LANGUAGE: typescript
CODE:
import { simulateStreamingMiddleware } from 'ai';

const middleware = simulateStreamingMiddleware();

----------------------------------------

TITLE: Updating Expo Chat UI to Display Tool Invocations
DESCRIPTION: This code modifies the chat UI to display tool invocations when present in the message object. It checks for toolInvocations and renders them as JSON if available.

LANGUAGE: typescript
CODE:
import { generateAPIUrl } from '@/utils';
import { useChat } from '@ai-sdk/react';
import { fetch as expoFetch } from 'expo/fetch';
import { View, TextInput, ScrollView, Text, SafeAreaView } from 'react-native';

export default function App() {
  const { messages, error, handleInputChange, input, handleSubmit } = useChat({
    fetch: expoFetch as unknown as typeof globalThis.fetch,
    api: generateAPIUrl('/api/chat'),
    onError: error => console.error(error, 'ERROR'),
  });

  if (error) return <Text>{error.message}</Text>;

  return (
    <SafeAreaView style={{ height: '100vh' }}>
      <View
        style={{
          height: '95%',
          display: 'flex',
          flexDirection: 'column',
          paddingHorizontal: 8,
        }}
      >
        <ScrollView style={{ flex: 1 }}>
          {messages.map(m => (
            <View key={m.id} style={{ marginVertical: 8 }}>
              <View>
                <Text style={{ fontWeight: 700 }}>{m.role}</Text>
                {m.toolInvocations ? (
                  <Text>{JSON.stringify(m.toolInvocations, null, 2)}</Text>
                ) : (
                  <Text>{m.content}</Text>
                )}
              </View>
            </View>
          ))}
        </ScrollView>

        <View style={{ marginTop: 8 }}>
          <TextInput
            style={{ backgroundColor: 'white', padding: 8 }}
            placeholder="Say something..."
            value={input}
            onChange={e =>
              handleInputChange({
                ...e,
                target: {
                  ...e.target,
                  value: e.nativeEvent.text,
                },
              } as unknown as React.ChangeEvent<HTMLInputElement>)
            }
            onSubmitEditing={e => {
              handleSubmit(e);
              e.preventDefault();
            }}
            autoFocus={true}
          />
        </View>
      </View>
    </SafeAreaView>
  );
}

----------------------------------------

TITLE: Creating Custom SambaNova Provider
DESCRIPTION: Creating a customized SambaNova provider instance with optional settings.

LANGUAGE: typescript
CODE:
import { createSambaNova } from 'sambanova-ai-provider';

const sambanova = createSambaNova({
  // Optional settings
});

----------------------------------------

TITLE: Updating Cerebras Provider Version in Markdown
DESCRIPTION: This snippet shows the format used to document version updates for the @ai-sdk/cerebras package. It includes the version number, change type, and any dependency updates.

LANGUAGE: markdown
CODE:
## 0.2.5

### Patch Changes

- Updated dependencies [d186cca]
  - @ai-sdk/openai-compatible@0.2.5

----------------------------------------

TITLE: Basic Image Generation with Replicate
DESCRIPTION: Demonstrates basic image generation using the Replicate provider with the flux-schnell model

LANGUAGE: typescript
CODE:
import { replicate } from '@ai-sdk/replicate';
import { experimental_generateImage as generateImage } from 'ai';
import { writeFile } from 'node:fs/promises';

const { image } = await generateImage({
  model: replicate.image('black-forest-labs/flux-schnell'),
  prompt: 'The Loch Ness Monster getting a manicure',
  aspectRatio: '16:9',
});

await writeFile('image.webp', image.uint8Array);

console.log('Image saved as image.webp');

----------------------------------------

TITLE: Formatting Changelog Entries in Markdown
DESCRIPTION: Standard changelog format showing version numbers, change types (Minor/Patch), and dependency updates

LANGUAGE: markdown
CODE:
# @ai-sdk/deepinfra\n\n## 0.2.5\n\n### Patch Changes\n\n- Updated dependencies [d186cca]\n  - @ai-sdk/openai-compatible@0.2.5

----------------------------------------

TITLE: Installing OpenAI Provider Package
DESCRIPTION: Command to install the OpenAI provider package using npm

LANGUAGE: bash
CODE:
npm i @ai-sdk/openai

----------------------------------------

TITLE: Importing Mistral Provider in TypeScript
DESCRIPTION: Code snippet showing how to import the default Mistral provider instance in a TypeScript file.

LANGUAGE: typescript
CODE:
import { mistral } from '@ai-sdk/mistral';

----------------------------------------

TITLE: Importing createStreamableUI from AI SDK RSC in JavaScript
DESCRIPTION: This snippet shows how to import the createStreamableUI function from the AI SDK RSC package.

LANGUAGE: javascript
CODE:
import { createStreamableUI } from "ai/rsc"

----------------------------------------

TITLE: Implementing Client-Side Conversation Interface in React
DESCRIPTION: A React client component that manages conversation state and handles user input. Uses useState for state management and calls a server action to continue the conversation.

LANGUAGE: tsx
CODE:
'use client';

import { useState } from 'react';
import { Message, continueConversation } from './actions';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export default function Home() {
  const [conversation, setConversation] = useState<Message[]>([]);
  const [input, setInput] = useState<string>('');

  return (
    <div>
      <div>
        {conversation.map((message, index) => (
          <div key={index}>
            {message.role}: {message.content}
          </div>
        ))}
      </div>

      <div>
        <input
          type="text"
          value={input}
          onChange={event => {
            setInput(event.target.value);
          }}
        />
        <button
          onClick={async () => {
            const { messages } = await continueConversation([
              ...conversation,
              { role: 'user', content: input },
            ]);

            setConversation(messages);
          }}
        >
          Send Message
        </button>
      </div>
    </div>
  );
}

----------------------------------------

TITLE: Creating Fireworks Text Embedding Model
DESCRIPTION: Initialize a Fireworks text embedding model using the textEmbeddingModel factory method.

LANGUAGE: typescript
CODE:
const model = fireworks.textEmbeddingModel(
  'accounts/fireworks/models/nomic-embed-text-v1',
);

----------------------------------------

TITLE: Generating Text with Cerebras Model
DESCRIPTION: Example demonstrating how to use the Cerebras provider with the generateText function to perform text generation using the Llama 3.1 8B model.

LANGUAGE: typescript
CODE:
import { cerebras } from '@ai-sdk/cerebras';
import { generateText } from 'ai';

const { text } = await generateText({
  model: cerebras('llama3.1-8b'),
  prompt: 'Write a JavaScript function that sorts a list:',
});

----------------------------------------

TITLE: Initializing Basic Replicate Provider in TypeScript
DESCRIPTION: Basic import and usage of the default Replicate provider instance

LANGUAGE: typescript
CODE:
import { replicate } from '@ai-sdk/replicate';

----------------------------------------

TITLE: Installing AI SDK OpenAI Compatible Module
DESCRIPTION: Instructions for installing the @ai-sdk/openai-compatible module using different package managers.

LANGUAGE: shell
CODE:
pnpm add @ai-sdk/openai-compatible

LANGUAGE: shell
CODE:
npm install @ai-sdk/openai-compatible

LANGUAGE: shell
CODE:
yarn add @ai-sdk/openai-compatible

----------------------------------------

TITLE: Importing Cerebras Provider
DESCRIPTION: TypeScript code showing how to import the default Cerebras provider instance.

LANGUAGE: typescript
CODE:
import { cerebras } from '@ai-sdk/cerebras';

----------------------------------------

TITLE: AI SDK Perplexity Provider Changelog
DESCRIPTION: Complete changelog documenting version updates, dependency changes, and new features for the Perplexity AI provider package.

LANGUAGE: markdown
CODE:
# @ai-sdk/perplexity\n\n## 1.1.3\n\n### Patch Changes\n\n- Updated dependencies [28be004]\n  - @ai-sdk/provider-utils@2.2.3\n\n## 1.1.2\n\n### Patch Changes\n\n- Updated dependencies [b01120e]\n  - @ai-sdk/provider-utils@2.2.2\n\n## 1.1.1\n\n### Patch Changes\n\n- Updated dependencies [f10f0fa]\n  - @ai-sdk/provider-utils@2.2.1\n\n## 1.1.0\n\n### Minor Changes\n\n- 5bc638d: AI SDK 4.2\n\n### Patch Changes\n\n- Updated dependencies [5bc638d]\n  - @ai-sdk/provider@1.1.0\n  - @ai-sdk/provider-utils@2.2.0

----------------------------------------

TITLE: Converting StringOutputParser Stream to Data Stream Response
DESCRIPTION: Shows how to use LangChainAdapter with a StringOutputParser to convert a LangChain ChatOpenAI stream into a data stream response. It uses the 'gpt-3.5-turbo-0125' model with zero temperature and pipes the output through a StringOutputParser.

LANGUAGE: tsx
CODE:
import { ChatOpenAI } from '@langchain/openai';
import { LangChainAdapter } from 'ai';
import { StringOutputParser } from '@langchain/core/output_parsers';

export async function POST(req: Request) {
  const { prompt } = await req.json();

  const model = new ChatOpenAI({
    model: 'gpt-3.5-turbo-0125',
    temperature: 0,
  });

  const parser = new StringOutputParser();
  const stream = await model.pipe(parser).stream(prompt);

  return LangChainAdapter.toDataStreamResponse(stream);
}

----------------------------------------

TITLE: Streaming Structured Data with URL Image Prompt in Node.js
DESCRIPTION: This code snippet demonstrates how to use the AI SDK to stream structured data responses from a language model using an image URL as part of the prompt. It uses the OpenAI GPT-4 Turbo model and defines a schema for passport stamp data.

LANGUAGE: typescript
CODE:
import { streamObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import dotenv from 'dotenv';
import { z } from 'zod';

dotenv.config();

async function main() {
  const { partialObjectStream } = streamObject({
    model: openai('gpt-4-turbo'),
    maxTokens: 512,
    schema: z.object({
      stamps: z.array(
        z.object({
          country: z.string(),
          date: z.string(),
        }),
      ),
    }),
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'text',
            text: 'list all the stamps in these passport pages?',
          },
          {
            type: 'image',
            image: new URL(
              'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c5/WW2_Spanish_official_passport.jpg/1498px-WW2_Spanish_official_passport.jpg',
            ),
          },
        ],
      },
    ],
  });

  for await (const partialObject of partialObjectStream) {
    console.clear();
    console.log(partialObject);
  }
}

main();

----------------------------------------

TITLE: Importing InkeepStream in React
DESCRIPTION: Example showing how to import the InkeepStream function from the ai package in a React application.

LANGUAGE: javascript
CODE:
import { InkeepStream } from "ai"

----------------------------------------

TITLE: Building AI SDK Project
DESCRIPTION: Installs dependencies and builds the AI SDK project from the root directory.

LANGUAGE: sh
CODE:
pnpm install
pnpm build

----------------------------------------

TITLE: Installing DeepSeek Provider Package
DESCRIPTION: Command to install the DeepSeek provider package via npm

LANGUAGE: bash
CODE:
npm i @ai-sdk/deepseek

----------------------------------------

TITLE: Importing DeepSeek Provider - TypeScript
DESCRIPTION: Basic import of the default DeepSeek provider instance for use with the AI SDK.

LANGUAGE: typescript
CODE:
import { deepseek } from '@ai-sdk/deepseek';

----------------------------------------

TITLE: Chat Settings Type Definitions in TypeScript
DESCRIPTION: Defines TypeScript types and interfaces for chat model settings, extending the OpenAI Compatible package's base settings.

LANGUAGE: typescript
CODE:
import { OpenAICompatibleChatSettings } from '@ai-sdk/openai-compatible';

export type ExampleChatModelId =
  | 'example/chat-model-1'
  | 'example/chat-model-2'
  | (string & {});

export interface ExampleChatSettings extends OpenAICompatibleChatSettings {
  // Add any custom settings here
}

----------------------------------------

TITLE: Running All AI SDK Codemods
DESCRIPTION: This command runs all available codemods to upgrade your project. It should be executed from the root of your project directory.

LANGUAGE: sh
CODE:
npx @ai-sdk/codemod upgrade

----------------------------------------

TITLE: Using simulateReadableStream Without Delays in TypeScript
DESCRIPTION: Shows how to create a ReadableStream with simulateReadableStream that emits chunks immediately without any delays.

LANGUAGE: typescript
CODE:
const stream = simulateReadableStream({
  chunks: ['Hello', ' ', 'World'],
  initialDelayInMs: null, // No initial delay
  chunkDelayInMs: null, // No delay between chunks
});

----------------------------------------

TITLE: Streaming Text Generation Output Example
DESCRIPTION: This snippet shows an example output of a text generation task, inventing a new holiday called "Joyful Hearts Day" and describing its traditions.

LANGUAGE: plaintext
CODE:
Introducing "Joyful Hearts Day" - a holiday dedicated to spreading love, joy, and kindness to others.

On Joyful Hearts Day, people exchange handmade cards, gifts, and acts of kindness to show appreciation and love for their friends, family, and community members. It is a day to focus on positivity and gratitude, spreading happiness and warmth to those around us.

Traditions include decorating homes and public spaces with hearts and bright colors, hosting community events such as charity drives, volunteer projects, and festive gatherings. People also participate in random acts of kindness, such as paying for someone's coffee, leaving encouraging notes for strangers, or simply offering a helping hand to those in need.

One of the main traditions of Joyful Hearts Day is the "Heart Exchange" where people write heartfelt messages to loved ones and exchange them in person or through mail. These messages can be words of encouragement, expressions of gratitude, or simply a reminder of how much they are loved.

Overall, Joyful Hearts Day is a day to celebrate love, kindness, and positivity, and to spread joy and happiness to all those around us. It is a reminder to appreciate the people in our lives and to make the world a brighter and more loving place.

----------------------------------------

TITLE: Implementing Multi-Step Text Streaming API Route
DESCRIPTION: Server-side implementation of a multi-step text streaming API route using OpenAI. Shows how to chain multiple streamText operations with different configurations and control the stream flow using createDataStreamResponse.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { createDataStreamResponse, streamText, tool } from 'ai';
import { z } from 'zod';

export async function POST(req: Request) {
  const { messages } = await req.json();

  return createDataStreamResponse({
    execute: async dataStream => {
      // step 1 example: forced tool call
      const result1 = streamText({
        model: openai('gpt-4o-mini', { structuredOutputs: true }),
        system: 'Extract the user goal from the conversation.',
        messages,
        toolChoice: 'required', // force the model to call a tool
        tools: {
          extractGoal: tool({
            parameters: z.object({ goal: z.string() }),
            execute: async ({ goal }) => goal, // no-op extract tool
          }),
        },
      });

      // forward the initial result to the client without the finish event:
      result1.mergeIntoDataStream(dataStream, {
        experimental_sendFinish: false, // omit the finish event
      });

      // note: you can use any programming construct here, e.g. if-else, loops, etc.
      // workflow programming is normal programming with this approach.

      // example: continue stream with forced tool call from previous step
      const result2 = streamText({
        // different system prompt, different model, no tools:
        model: openai('gpt-4o'),
        system:
          'You are a helpful assistant with a different system prompt. Repeat the extract user goal in your answer.',
        // continue the workflow stream with the messages from the previous step:
        messages: [...messages, ...(await result1.response).messages],
      });

      // forward the 2nd result to the client (incl. the finish event):
      result2.mergeIntoDataStream(dataStream, {
        experimental_sendStart: false, // omit the start event
      });
    },
  });
}

----------------------------------------

TITLE: Installing Next.js Project with npx
DESCRIPTION: Command to create a new Next.js application using npx create-next-app with the LangChain template

LANGUAGE: bash
CODE:
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-langchain next-langchain-app

----------------------------------------

TITLE: Installing Fireworks Provider for AI SDK
DESCRIPTION: Command to install the Fireworks provider module for the AI SDK using npm.

LANGUAGE: bash
CODE:
npm i @ai-sdk/fireworks

----------------------------------------

TITLE: Generating Image with Fireworks Image Model
DESCRIPTION: Use a Fireworks image model to generate an image using the generateImage function.

LANGUAGE: typescript
CODE:
import { fireworks } from '@ai-sdk/fireworks';
import { experimental_generateImage as generateImage } from 'ai';

const { image } = await generateImage({
  model: fireworks.image('accounts/fireworks/models/flux-1-dev-fp8'),
  prompt: 'A futuristic cityscape at sunset',
  aspectRatio: '16:9',
});

----------------------------------------

TITLE: Implementing Type-Safe Model IDs with OpenAI Compatible Provider
DESCRIPTION: Demonstrates how to implement type-safe model IDs for auto-completion support across different model types including chat, completion, and embedding models.

LANGUAGE: typescript
CODE:
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
import { generateText } from 'ai';

type ExampleChatModelIds =
  | 'meta-llama/Llama-3-70b-chat-hf'
  | 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'
  | (string & {});

type ExampleCompletionModelIds =
  | 'codellama/CodeLlama-34b-Instruct-hf'
  | 'Qwen/Qwen2.5-Coder-32B-Instruct'
  | (string & {});

type ExampleEmbeddingModelIds =
  | 'BAAI/bge-large-en-v1.5'
  | 'bert-base-uncased'
  | (string & {});

const model = createOpenAICompatible<
  ExampleChatModelIds,
  ExampleCompletionModelIds,
  ExampleEmbeddingModelIds
>({
  name: 'example',
  apiKey: process.env.PROVIDER_API_KEY,
  baseURL: 'https://api.example.com/v1',
});

const { text } = await generateText({
  model: model.chatModel('meta-llama/Llama-3-70b-chat-hf'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Configuring OpenAI API Key in Environment File
DESCRIPTION: Instructions for creating a .env.local file in the sveltekit-openai example directory and adding the OpenAI API key. This step is crucial for authenticating with the OpenAI service.

LANGUAGE: bash
CODE:
OPENAI_API_KEY=<your key>

----------------------------------------

TITLE: Kasada API URL Configuration Example
DESCRIPTION: Example URL structure for Kasada API endpoint configuration, showing the format with placeholders for API hostname, version, and unique identifiers.

LANGUAGE: typescript
CODE:
https://${kasadaAPIHostname}/149e9513-01fa-4fb0-aad4-566afd725d1b/2d206a39-8ed7-437e-a3be-862e0f06eea3/api/${kasadaAPIVersion}/classification

----------------------------------------

TITLE: Enforcing Dynamic Route Behavior in Next.js
DESCRIPTION: Configuration setting to force dynamic behavior for routes that require streaming functionality.

LANGUAGE: tsx
CODE:
export const dynamic = 'force-dynamic';

----------------------------------------

TITLE: Text Stream Implementation with Hono and OpenAI
DESCRIPTION: Implementation of a text streaming endpoint using Hono and OpenAI, utilizing the textStream property for direct text streaming.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { serve } from '@hono/node-server';
import { streamText } from 'ai';
import { Hono } from 'hono';
import { stream } from 'hono/streaming';

const app = new Hono();

app.post('/', async c => {
  const result = streamText({
    model: openai('gpt-4o'),
    prompt: 'Invent a new holiday and describe its traditions.',
  });

  c.header('Content-Type', 'text/plain; charset=utf-8');

  return stream(c, stream => stream.pipe(result.textStream));
});

serve({ fetch: app.fetch, port: 8080 });

----------------------------------------

TITLE: Generating Text with Groq Provider
DESCRIPTION: Complete example demonstrating how to use the Groq provider to generate text using the gemma2-9b-it model.

LANGUAGE: typescript
CODE:
import { groq } from '@ai-sdk/groq';
import { generateText } from 'ai';

const { text } = await generateText({
  model: groq('gemma2-9b-it'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Node.js Runtime Text Generation with Gemini
DESCRIPTION: Example of generating text using the Gemini model in Node.js runtime with default provider configuration

LANGUAGE: typescript
CODE:
import { vertex } from '@ai-sdk/google-vertex';
import { generateText } from 'ai';

const { text } = await generateText({
  model: vertex('gemini-1.5-flash'),
  prompt: 'Write a vegetarian lasagna recipe.',
});

----------------------------------------

TITLE: Configuring Edge Runtime Authentication
DESCRIPTION: Setting up authentication for Edge runtime environments using Google credentials

LANGUAGE: typescript
CODE:
import { createVertex } from '@ai-sdk/google-vertex/edge';

const vertex = createVertex({
  googleCredentials: {
    clientEmail: process.env.GOOGLE_CLIENT_EMAIL,
    privateKey: process.env.GOOGLE_PRIVATE_KEY,
    privateKeyId: process.env.GOOGLE_PRIVATE_KEY_ID
  }
});

----------------------------------------

TITLE: Generating Text with OpenAI Compatible Provider in TypeScript
DESCRIPTION: Shows how to use a provider instance to generate text using the generateText function with a specific model and prompt.

LANGUAGE: typescript
CODE:
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
import { generateText } from 'ai';

const provider = createOpenAICompatible({
  name: 'provider-name',
  apiKey: process.env.PROVIDER_API_KEY,
  baseURL: 'https://api.provider.com/v1',
});

const { text } = await generateText({
  model: provider('model-id'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Installing Next.js Project with pnpm
DESCRIPTION: Command to create a new Next.js project using pnpm, specifically for the AI SDK example with OpenAI and Sentry telemetry.

LANGUAGE: bash
CODE:
pnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry-sentry next-openai-telemetry-sentry-app

----------------------------------------

TITLE: Running SvelteKit OpenAI Project in Development Mode
DESCRIPTION: Command to start the SvelteKit OpenAI project in development mode using pnpm. This command should be run after setting up the environment and installing dependencies.

LANGUAGE: bash
CODE:
pnpm -F sveltekit-openai dev

----------------------------------------

TITLE: Adding Headers to Disable Content Encoding for AI SDK Streaming in TSX
DESCRIPTION: This code snippet demonstrates how to add a 'Content-Encoding: none' header to the response when using the AI SDK's toDataStreamResponse method. This solution prevents proxy middleware from compressing the response, allowing streaming to function correctly.

LANGUAGE: tsx
CODE:
return result.toDataStreamResponse({
  headers: {
    'Content-Encoding': 'none',
  },
});

----------------------------------------

TITLE: Creating File Upload Form in Next.js
DESCRIPTION: This code snippet shows how to create a client-side form in Next.js that allows users to upload a PDF file and submit it for analysis. It handles form submission, displays loading state, and shows the analysis result.

LANGUAGE: tsx
CODE:
'use client';

import { useState } from 'react';

export default function Page() {
  const [description, setDescription] = useState<string>();
  const [loading, setLoading] = useState(false);

  return (
    <div>
      <form
        action={async formData => {
          try {
            setLoading(true);
            const response = await fetch('/api/analyze', {
              method: 'POST',
              body: formData,
            });
            setLoading(false);

            if (response.ok) {
              setDescription(await response.text());
            }
          } catch (error) {
            console.error('Analysis failed:', error);
          }
        }}
      >
        <div>
          <label>Upload Image</label>
          <input name="pdf" type="file" accept="application/pdf" />
        </div>
        <button type="submit" disabled={loading}>
          Submit{loading && 'ing...'}
        </button>
      </form>
      {description && <pre>{description}</pre>}
    </div>
  );
}

----------------------------------------

TITLE: Generating Text with Fireworks Language Model in TypeScript
DESCRIPTION: Example of using the Fireworks provider to generate text with a language model. It demonstrates how to use the generateText function with a specific model and prompt.

LANGUAGE: typescript
CODE:
import { fireworks } from '@ai-sdk/fireworks';
import { generateText } from 'ai';

const { text } = await generateText({
  model: fireworks('accounts/fireworks/models/deepseek-v3'),
  prompt: 'Write a JavaScript function that sorts a list:',
});

----------------------------------------

TITLE: Prompt Caching with Anthropic Claude Models
DESCRIPTION: Example demonstrating prompt caching implementation with Anthropic Claude models including error message handling

LANGUAGE: typescript
CODE:
import { vertexAnthropic } from '@ai-sdk/google-vertex/anthropic';
import { generateText } from 'ai';
import fs from 'node:fs';

const errorMessage = fs.readFileSync('data/error-message.txt', 'utf8');

async function main() {
  const result = await generateText({
    model: vertexAnthropic('claude-3-5-sonnet-v2@20241022', {
      cacheControl: true,
    }),
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'text',
            text: 'You are a JavaScript expert.',
          },
          {
            type: 'text',
            text: `Error message: ${errorMessage}`,
            providerOptions: {
              anthropic: {
                cacheControl: { type: 'ephemeral' },
              },
            },
          },
          {
            type: 'text',
            text: 'Explain the error message.',
          },
        ],
      },
    ],
  });

  console.log(result.text);
  console.log(result.experimental_providerMetadata?.anthropic);
}

main().catch(console.error);

----------------------------------------

TITLE: Creating Custom DeepInfra Provider Instance
DESCRIPTION: Shows how to create a customized DeepInfra provider instance with specific settings.

LANGUAGE: typescript
CODE:
import { createDeepInfra } from '@ai-sdk/deepinfra';

const deepinfra = createDeepInfra({
  apiKey: process.env.DEEPINFRA_API_KEY ?? '',
});

----------------------------------------

TITLE: Installing AI SDK OpenAI Compatible Provider with npm
DESCRIPTION: This snippet shows how to install the OpenAI-compatible provider package using npm.

LANGUAGE: bash
CODE:
npm i @ai-sdk/openai-compatible

----------------------------------------

TITLE: Creating Next.js App with pnpm for AI SDK and OpenAI Example
DESCRIPTION: Command to create a new Next.js application using pnpm, bootstrapping it with the AI SDK and OpenAI example from the Vercel AI repository.

LANGUAGE: bash
CODE:
pnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app

----------------------------------------

TITLE: Firecrawl Web Scraping Tool Implementation
DESCRIPTION: Implementation of a web scraping tool using Firecrawl API, showing how to create a tool for crawling specific URLs and processing their content.

LANGUAGE: typescript
CODE:
import { generateText, tool } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';
import FirecrawlApp from '@mendable/firecrawl-js';
import 'dotenv/config';

const app = new FirecrawlApp({ apiKey: process.env.FIRECRAWL_API_KEY });

export const webSearch = tool({
  description: 'Search the web for up-to-date information',
  parameters: z.object({
    urlToCrawl: z
      .string()
      .url()
      .min(1)
      .max(100)
      .describe('The URL to crawl (including http:// or https://)'),
  }),
  execute: async ({ urlToCrawl }) => {
    const crawlResponse = await app.crawlUrl(urlToCrawl, {
      limit: 1,
      scrapeOptions: {
        formats: ['markdown', 'html'],
      },
    });
    if (!crawlResponse.success) {
      throw new Error(`Failed to crawl: ${crawlResponse.error}`);
    }
    return crawlResponse.data;
  },
});

const main = async () => {
  const { text } = await generateText({
    model: openai('gpt-4o-mini'),
    prompt: 'Get the latest blog post from vercel.com/blog',
    tools: {
      webSearch,
    },
    maxSteps: 2,
  });
  console.log(text);
};

main();

----------------------------------------

TITLE: Implementing Throttling with useChat Hook in React
DESCRIPTION: Demonstrates how to implement throttling in the useChat hook to prevent maximum update depth errors. The throttle option limits UI updates to every 50ms during message streaming.

LANGUAGE: tsx
CODE:
const { messages, ... } = useChat({
  // Throttle the messages and data updates to 50ms:
  experimental_throttle: 50
})

----------------------------------------

TITLE: Chatbot Client Implementation with Next.js and AI SDK
DESCRIPTION: Implements the client-side chat interface using the 'useChat' hook from the AI SDK. It renders messages using the MemoizedMarkdown component and includes a separate MessageInput component to manage input. The implementation uses throttling to manage rendering performance.

LANGUAGE: typescript
CODE:
'use client';

import { useChat } from '@ai-sdk/react';
import { MemoizedMarkdown } from '@/components/memoized-markdown';

export default function Page() {
  const { messages } = useChat({
    id: 'chat',
    // Throttle the messages and data updates to 50ms:
    experimental_throttle: 50,
  });

  return (
    <div className="flex flex-col w-full max-w-xl py-24 mx-auto stretch">
      <div className="space-y-8 mb-4">
        {messages.map(message => (
          <div key={message.id}>
            <div className="font-bold mb-2">
              {message.role === 'user' ? 'You' : 'Assistant'}
            </div>
            <div className="prose space-y-2">
              <MemoizedMarkdown id={message.id} content={message.content} />
            </div>
          </div>
        ))}
      </div>
      <MessageInput />
    </div>
  );
}

const MessageInput = () => {
  const { input, handleSubmit, handleInputChange } = useChat({ id: 'chat' });
  return (
    <form onSubmit={handleSubmit}>
      <input
        className="fixed bottom-0 w-full max-w-xl p-2 mb-8 dark:bg-zinc-900 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl"
        placeholder="Say something..."
        value={input}
        onChange={handleInputChange}
      />
    </form>
  );
};

----------------------------------------

TITLE: Creating OpenAI Assistant API Route in Next.js
DESCRIPTION: A Next.js API route that handles assistant interactions using the OpenAI API. It creates threads, manages messages, and streams responses back to the client using AssistantResponse from the AI SDK.

LANGUAGE: typescript
CODE:
import OpenAI from 'openai';
import { AssistantResponse } from 'ai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY || '',
});

export async function POST(req: Request) {
  const input: {
    threadId: string | null;
    message: string;
  } = await req.json();

  const threadId = input.threadId ?? (await openai.beta.threads.create({})).id;

  const createdMessage = await openai.beta.threads.messages.create(threadId, {
    role: 'user',
    content: input.message,
  });

  return AssistantResponse(
    { threadId, messageId: createdMessage.id },
    async ({ forwardStream }) => {
      const runStream = openai.beta.threads.runs.stream(threadId, {
        assistant_id:
          process.env.ASSISTANT_ID ??
          (() => {
            throw new Error('ASSISTANT_ID environment is not set');
          })(),
      });

      await forwardStream(runStream);
    },
  );
}

----------------------------------------

TITLE: Importing Luma Provider in TypeScript
DESCRIPTION: Code snippet showing how to import the default Luma provider instance in TypeScript.

LANGUAGE: typescript
CODE:
import { luma } from '@ai-sdk/luma';

----------------------------------------

TITLE: Creating Azure OpenAI Image Model
DESCRIPTION: Shows how to create an image model (DALL-E) using the Azure OpenAI provider.

LANGUAGE: typescript
CODE:
const model = azure.imageModel('your-dalle-deployment-name');

----------------------------------------

TITLE: Importing Inflection AI Provider in TypeScript
DESCRIPTION: Code snippet showing how to import the default provider instance 'inflection' from the Inflection AI SDK provider package.

LANGUAGE: typescript
CODE:
import { inflection } from 'inflection-ai-sdk-provider';

----------------------------------------

TITLE: Installing Next.js Project with Yarn
DESCRIPTION: Command to create a new Next.js application using yarn create next-app with the LangChain template

LANGUAGE: bash
CODE:
yarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-langchain next-langchain-app

----------------------------------------

TITLE: Perplexity Web Search Integration
DESCRIPTION: Implementation of web search using Perplexity's Sonar models, which combines real-time web search with natural language processing and includes citations.

LANGUAGE: typescript
CODE:
import { perplexity } from '@ai-sdk/perplexity';
import { generateText } from 'ai';

const { text, sources } = await generateText({
  model: perplexity('sonar-pro'),
  prompt: 'What are the latest developments in quantum computing?',
});

console.log(text);
console.log(sources);

----------------------------------------

TITLE: Migrating Roundtrips to MaxSteps
DESCRIPTION: Example of updating from maxToolRoundtrips to maxSteps.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
const { text, roundtrips } = await generateText({
  maxToolRoundtrips: 1, // or maxAutomaticRoundtrips
  // ...
});

// AI SDK 4.0
const { text, steps } = await generateText({
  maxSteps: 2,
  // ...
});

----------------------------------------

TITLE: Defining getRoomTemperature Function for OpenAI Assistant
DESCRIPTION: This JSON schema defines the getRoomTemperature function for the OpenAI assistant. It specifies the function name, description, and required parameters for retrieving room temperature.

LANGUAGE: json
CODE:
{\n  "name": "getRoomTemperature",\n  "description": "Get the temperature in a room",\n  "parameters": {\n    "type": "object",\n    "properties": {\n      "room": {\n        "type": "string",\n        "enum": ["bedroom", "home office", "living room", "kitchen", "bathroom"]\n      }\n    },\n    "required": ["room"]\n  }\n}

----------------------------------------

TITLE: Setting up Environment Variables for Anthropic API
DESCRIPTION: This code snippet shows how to set up the environment variable for the Anthropic API key in a .env.local file. This is necessary for authenticating requests to the Anthropic API.

LANGUAGE: env
CODE:
ANTHROPIC_API_KEY=xxxxxxxxx

----------------------------------------

TITLE: Advanced Image Generation with Provider Options
DESCRIPTION: Example demonstrating how to use additional provider-specific options when generating images with Replicate, including style and size parameters.

LANGUAGE: typescript
CODE:
const { image } = await generateImage({
  model: replicate.image('recraft-ai/recraft-v3'),
  prompt: 'The Loch Ness Monster getting a manicure',
  size: '1365x1024',
  providerOptions: {
    replicate: {
      style: 'realistic_image',
    },
  },
});

----------------------------------------

TITLE: Installing Azure OpenAI Provider with Package Managers
DESCRIPTION: Shows how to install the Azure OpenAI provider using different package managers (pnpm, npm, yarn).

LANGUAGE: bash
CODE:
pnpm add @ai-sdk/azure

LANGUAGE: bash
CODE:
npm install @ai-sdk/azure

LANGUAGE: bash
CODE:
yarn add @ai-sdk/azure

----------------------------------------

TITLE: Generating Text with AnthropicVertex Model in TypeScript
DESCRIPTION: Use the AnthropicVertex language model to generate text using the generateText function from the AI SDK.

LANGUAGE: typescript
CODE:
import { anthropicVertex } from 'anthropic-vertex-ai';
import { generateText } from 'ai';

const { text } = await generateText({
  model: anthropicVertex('claude-3-sonnet@20240229'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Deploying Nuxt AI Project to Vercel
DESCRIPTION: Commands to build the Nuxt project and deploy it to Vercel. This process prepares the project for deployment and pushes it to Vercel's hosting platform.

LANGUAGE: bash
CODE:
pnpm run build
vercel deploy

----------------------------------------

TITLE: Testing HTTP Server with cURL
DESCRIPTION: Command to test the HTTP server running on port 8080 using cURL

LANGUAGE: bash
CODE:
curl -X POST http://localhost:8080

----------------------------------------

TITLE: New 'streamUI' Function Usage in Server Action (TypeScript)
DESCRIPTION: This example shows how to use the new 'streamUI' function with the AI SDK OpenAI provider in a Server Action, including a custom tool for fetching weather information.

LANGUAGE: tsx
CODE:
import { streamUI } from 'ai/rsc';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';
import { Spinner, Weather } from '@/components';
import { getWeather } from '@/utils';

async function submitMessage(userInput = 'What is the weather in SF?') {
  'use server';

  const result = await streamUI({
    model: openai('gpt-4-turbo'),
    system: 'You are a helpful assistant',
    messages: [{ role: 'user', content: userInput }],
    text: ({ content }) => <p>{content}</p>,
    tools: {
      get_city_weather: {
        description: 'Get the current weather for a city',
        parameters: z
          .object({
            city: z.string().describe('Name of the city'),
          })
          .required(),
        generate: async function* ({ city }) {
          yield <Spinner />;
          const weather = await getWeather(city);
          return <Weather info={weather} />;
        },
      },
    },
  });

  return result.value;
}

----------------------------------------

TITLE: Installing Next.js App with NPX
DESCRIPTION: Command to create a new Next.js application using npx and the AI SDK template repository.

LANGUAGE: bash
CODE:
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-fastapi next-fastapi-app

----------------------------------------

TITLE: Installing Project with Yarn
DESCRIPTION: Command to create a new Next.js application using yarn create next-app with the OpenAI telemetry template

LANGUAGE: bash
CODE:
yarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry next-openai-telemetry-app

----------------------------------------

TITLE: Importing xAI Provider Instance in TypeScript
DESCRIPTION: This snippet shows how to import the default xAI provider instance named 'xai' from the @ai-sdk/xai module in a TypeScript file.

LANGUAGE: typescript
CODE:
import { xai } from '@ai-sdk/xai';

----------------------------------------

TITLE: Creating OpenAI Image Generation Model
DESCRIPTION: Example of creating an OpenAI image generation model.

LANGUAGE: typescript
CODE:
const model = openai.image('dall-e-3');

----------------------------------------

TITLE: Installing Mem0 Provider with Package Managers
DESCRIPTION: Commands for installing the Mem0 provider using different package managers (pnpm, npm, yarn).

LANGUAGE: bash
CODE:
pnpm add @mem0/vercel-ai-provider

LANGUAGE: bash
CODE:
npm install @mem0/vercel-ai-provider

LANGUAGE: bash
CODE:
yarn add @mem0/vercel-ai-provider

----------------------------------------

TITLE: Testing Nest.js AI SDK Endpoint with cURL
DESCRIPTION: Sends a POST request to the local Nest.js server endpoint to test the AI SDK integration using cURL.

LANGUAGE: sh
CODE:
curl -X POST http://localhost:8080

----------------------------------------

TITLE: Installing Groq Provider
DESCRIPTION: Commands for installing the Groq provider package using different package managers.

LANGUAGE: bash
CODE:
pnpm add @ai-sdk/groq

LANGUAGE: bash
CODE:
npm install @ai-sdk/groq

LANGUAGE: bash
CODE:
yarn add @ai-sdk/groq

----------------------------------------

TITLE: Legacy OpenAI Provider Usage in Route Handler (TypeScript)
DESCRIPTION: This snippet demonstrates how to use the OpenAI SDK to query their model and stream the response using the OpenAIStream function in a Route Handler.

LANGUAGE: tsx
CODE:
import OpenAI from 'openai';
import { OpenAIStream, StreamingTextResponse } from 'ai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
});

export async function POST(req: Request) {
  const { messages } = await req.json();

  const response = await openai.chat.completions.create({
    model: 'gpt-4-turbo',
    stream: true,
    messages,
  });

  const stream = OpenAIStream(response);

  return new StreamingTextResponse(stream);
}

----------------------------------------

TITLE: Starting the Development Server
DESCRIPTION: Command to start the development server using pnpm.

LANGUAGE: sh
CODE:
pnpm dev

----------------------------------------

TITLE: Setting Timeout for Embedding in TypeScript
DESCRIPTION: Shows how to set a timeout for the embedding process using the 'abortSignal' parameter with AbortSignal.timeout(). This allows controlling the maximum time allowed for the embedding operation to complete.

LANGUAGE: ts
CODE:
import { openai } from '@ai-sdk/openai';
import { embed } from 'ai';

const { embedding } = await embed({
  model: openai.embedding('text-embedding-3-small'),
  value: 'sunny day at the beach',
  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second
});

----------------------------------------

TITLE: Using OpenAI Web Search Tool
DESCRIPTION: Example of using the OpenAI web search tool with the responses API.

LANGUAGE: typescript
CODE:
const result = await generateText({
  model: openai.responses('gpt-4o-mini'),
  prompt: 'What happened in San Francisco last week?',
  tools: {
    web_search_preview: openai.tools.webSearchPreview({
      // optional configuration:
      searchContextSize: 'high',
      userLocation: {
        type: 'approximate',
        city: 'San Francisco',
        region: 'California',
      },
    }),
  },
  // Force web search tool:
  toolChoice: { type: 'tool', toolName: 'web_search_preview' },
});

// URL sources
const sources = result.sources;

----------------------------------------

TITLE: Creating Embedding Model with Mixedbread in TypeScript
DESCRIPTION: This snippet shows how to create an embedding model using the Mixedbread provider. It uses the textEmbeddingModel method to create a model with a specific model name.

LANGUAGE: typescript
CODE:
import { mixedbread } from 'mixedbread-ai-provider';

const embeddingModel = mixedbread.textEmbeddingModel(
  'mixedbread-ai/mxbai-embed-large-v1',
);

----------------------------------------

TITLE: Installing Dependencies for Nest.js and AI SDK Project
DESCRIPTION: Installs the required dependencies for the Nest.js and AI SDK project using pnpm package manager.

LANGUAGE: sh
CODE:
pnpm install

----------------------------------------

TITLE: Content Caching Implementation
DESCRIPTION: Using Google AI cache manager for content caching with models

LANGUAGE: typescript
CODE:
import { google } from '@ai-sdk/google';
import { GoogleAICacheManager } from '@google/generative-ai/server';
import { generateText } from 'ai';

const cacheManager = new GoogleAICacheManager(
  process.env.GOOGLE_GENERATIVE_AI_API_KEY,
);

type GoogleModelCacheableId =
  | 'models/gemini-1.5-flash-001'
  | 'models/gemini-1.5-pro-001';

const model: GoogleModelCacheableId = 'models/gemini-1.5-pro-001';

const { name: cachedContent } = await cacheManager.create({
  model,
  contents: [
    {
      role: 'user',
      parts: [{ text: '1000 Lasanga Recipes...' }],
    },
  ],
  ttlSeconds: 60 * 5,
});

const { text: veggieLasangaRecipe } = await generateText({
  model: google(model, { cachedContent }),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

const { text: meatLasangaRecipe } = await generateText({
  model: google(model, { cachedContent }),
  prompt: 'Write a meat lasagna recipe for 12 people.',
});

----------------------------------------

TITLE: Using Text Embedding Models with Provider Registry in TypeScript
DESCRIPTION: This example demonstrates how to access text embedding models using the textEmbeddingModel method on the provider registry.

LANGUAGE: typescript
CODE:
import { embed } from 'ai';
import { registry } from './registry';

const { embedding } = await embed({
  model: registry.textEmbeddingModel('openai:text-embedding-3-small'),
  value: 'sunny day at the beach',
});

----------------------------------------

TITLE: Adding Labels to Laminar Spans
DESCRIPTION: Implementation of adding pre-defined labels to Laminar spans for filtering and organization.

LANGUAGE: javascript
CODE:
import { withLabels } from '@lmnr-ai/lmnr';

withLabels({ myLabel: 'someValue' }, async () => {
  // ...
});

----------------------------------------

TITLE: Implementing MCP Tools Server-Side with Next.js API Route
DESCRIPTION: This code snippet shows how to create a Next.js API route that uses MCP tools for text generation. It demonstrates initializing MCP clients with different transports, retrieving tools, and streaming text using the AI SDK.

LANGUAGE: typescript
CODE:
import { experimental_createMCPClient, streamText } from 'ai';
import { Experimental_StdioMCPTransport } from 'ai/mcp-stdio';
import { openai } from '@ai-sdk/openai';

export async function POST(req: Request) {
  const { prompt }: { prompt: string } = await req.json();

  try {
    // Initialize an MCP client to connect to a `stdio` MCP server:
    const transport = new Experimental_StdioMCPTransport({
      command: 'node',
      args: ['src/stdio/dist/server.js'],
    });
    const stdioClient = await experimental_createMCPClient({
      transport,
    });

    // Alternatively, you can connect to a Server-Sent Events (SSE) MCP server:
    const sseClient = await experimental_createMCPClient({
      transport: {
        type: 'sse',
        url: 'https://actions.zapier.com/mcp/[YOUR_KEY]/sse',
      },
    });

    // Similarly to the stdio example, you can pass in your own custom transport as long as it implements the `MCPTransport` interface:
    const transport = new MyCustomTransport({
      // ...
    });
    const customTransportClient = await experimental_createMCPClient({
      transport,
    });

    const toolSetOne = await stdioClient.tools();
    const toolSetTwo = await sseClient.tools();
    const toolSetThree = await customTransportClient.tools();
    const tools = {
      ...toolSetOne,
      ...toolSetTwo,
      ...toolSetThree, // note: this approach causes subsequent tool sets to override tools with the same name
    };

    const response = await streamText({
      model: openai('gpt-4o'),
      tools,
      prompt,
      // When streaming, the client should be closed after the response is finished:
      onFinish: async () => {
        await stdioClient.close();
        await sseClient.close();
        await customTransportClient.close();
      },
    });

    return response.toDataStreamResponse();
  } catch (error) {
    return new Response('Internal Server Error', { status: 500 });
  }
}

----------------------------------------

TITLE: Using OpenAI Audio Input
DESCRIPTION: Example of passing audio files to the OpenAI audio-capable model.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

const result = await generateText({
  model: openai('gpt-4o-audio-preview'),
  messages: [
    {
      role: 'user',
      content: [
        { type: 'text', text: 'What is the audio saying?' },
        {
          type: 'file',
          mimeType: 'audio/mpeg',
          data: fs.readFileSync('./data/galileo.mp3'),
        },
      ],
    },
  ],
});

----------------------------------------

TITLE: Extracting Reasoning from FriendliAI Output
DESCRIPTION: Implementation of reasoning extraction middleware for the deepseek-r1 model using think tags.

LANGUAGE: typescript
CODE:
import { friendli } from '@friendliai/ai-provider';
import { wrapLanguageModel, extractReasoningMiddleware } from 'ai';

const enhancedModel = wrapLanguageModel({
  model: friendli('deepseek-r1'),
  middleware: extractReasoningMiddleware({ tagName: 'think' }),
});

const { text, reasoning } = await generateText({
  model: enhancedModel,
  prompt: 'Explain quantum entanglement.',
});

----------------------------------------

TITLE: Handling Anthropic API Error Response in TypeScript
DESCRIPTION: This snippet shows the error response received from the Anthropic API when attempting to generate text. It includes details about the error type, message, and the specific issue with token count in relation to cache control.

LANGUAGE: typescript
CODE:
Fetched {"type":"error","error":{"type":"invalid_request_error","message":"The message up to and including the first cache-control block must be at least 1024 tokens. Found: 939."}}

----------------------------------------

TITLE: Creating Fireworks Completion Model
DESCRIPTION: Initialize a Fireworks completion model using the completion factory method.

LANGUAGE: typescript
CODE:
const model = fireworks.completion('accounts/fireworks/models/firefunction-v1');

----------------------------------------

TITLE: Implementing Data Stream Protocol Backend Route
DESCRIPTION: Backend implementation for data stream protocol using Next.js API route. Handles POST requests and returns data stream responses.

LANGUAGE: typescript
CODE:
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export async function POST(req: Request) {
  const { prompt }: { prompt: string } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    prompt,
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Next.js Configuration for Instrumentation
DESCRIPTION: Next.js configuration to enable experimental instrumentation hook for versions 13.4 to 15.

LANGUAGE: javascript
CODE:
module.exports = {
  experimental: {
    instrumentationHook: true,
  },
};

----------------------------------------

TITLE: Configuring useChat with Text Stream Protocol
DESCRIPTION: Demonstrates how to configure the useChat hook to use raw text stream processing by setting the streamProtocol parameter to 'text'. This solution addresses stream parsing errors that occur when using older SDK versions or custom providers.

LANGUAGE: tsx
CODE:
const { messages, append } = useChat({ streamProtocol: 'text' });

----------------------------------------

TITLE: Single Value Embedding with LM Studio
DESCRIPTION: Shows how to create embeddings for a single text value using LM Studio's embedding API. Uses the nomic-embed-text model for text embedding generation.

LANGUAGE: typescript
CODE:
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
import { embed } from 'ai';

const lmstudio = createOpenAICompatible({
  name: 'lmstudio',
  baseURL: 'https://localhost:1234/v1',
});

// 'embedding' is a single embedding object (number[])
const { embedding } = await embed({
  model: lmstudio.textEmbeddingModel('text-embedding-nomic-embed-text-v1.5'),
  value: 'sunny day at the beach',
});

----------------------------------------

TITLE: Creating Custom xAI Provider Instance
DESCRIPTION: Create a customized xAI provider instance with specific settings.

LANGUAGE: typescript
CODE:
import { createXai } from '@ai-sdk/xai';

const xai = createXai({
  apiKey: 'your-api-key',
});

----------------------------------------

TITLE: Generating Text with Qwen Language Model
DESCRIPTION: Use the Qwen language model to generate text using the generateText function from the AI SDK.

LANGUAGE: typescript
CODE:
import { qwen } from 'qwen-ai-provider';
import { generateText } from 'ai';

const { text } = await generateText({
  model: qwen('qwen-plus'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Installing OpenTelemetry Dependencies for LangWatch
DESCRIPTION: Command to install necessary dependencies for using LangWatch with OpenTelemetry.

LANGUAGE: bash
CODE:
npm install @vercel/otel langwatch @opentelemetry/api-logs @opentelemetry/instrumentation @opentelemetry/sdk-logs

----------------------------------------

TITLE: Reading Streamable Values in Next.js Client Component
DESCRIPTION: Client-side implementation showing how to read and consume streamable values using readStreamableValue in a Next.js page component.

LANGUAGE: tsx
CODE:
import { readStreamableValue } from 'ai/rsc';
import { runThread } from '@/actions';

export default function Page() {
  return (
    <button
      onClick={async () => {
        const { status } = await runThread();

        for await (const value of readStreamableValue(status)) {
          console.log(value);
        }
      }}
    >
      Ask
    </button>
  );
}

----------------------------------------

TITLE: Enabling Telemetry in AI SDK Function Calls
DESCRIPTION: Demonstrates how to use the experimental_telemetry option to enable telemetry on supported AI SDK function calls, including metadata for the query.

LANGUAGE: typescript
CODE:
import { createOpenAI } from '@ai-sdk/openai';
import { generateText } from 'ai';

const openai = createOpenAI();

async function main() {
  const result = await generateText({
    model: openai('gpt-4o-mini'),
    prompt: 'What is 2 + 2?',
    experimental_telemetry: {
      isEnabled: true,
      metadata: {
        query: 'weather',
        location: 'San Francisco',
      },
    },
  });
  console.log(result);
}

main();

----------------------------------------

TITLE: Implementing Server-Side Image Generation API in Next.js
DESCRIPTION: Creates a POST endpoint that handles chat messages and image generation using OpenAI's GPT-4 and DALL-E 3. The implementation includes message formatting, stream handling, and a custom generateImage tool.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { experimental_generateImage, Message, streamText, tool } from 'ai';
import { z } from 'zod';

export async function POST(request: Request) {
  const { messages }: { messages: Message[] } = await request.json();

  const formattedMessages = messages.map(m => {
    if (m.role === 'assistant' && m.toolInvocations) {
      m.toolInvocations.forEach(ti => {
        if (ti.toolName === 'generateImage' && ti.state === 'result') {
          ti.result.image = `redacted-for-length`;
        }
      });
    }
    return m;
  });

  const result = streamText({
    model: openai('gpt-4o'),
    messages: formattedMessages,
    tools: {
      generateImage: tool({
        description: 'Generate an image',
        parameters: z.object({
          prompt: z.string().describe('The prompt to generate the image from'),
        }),
        execute: async ({ prompt }) => {
          const { image } = await experimental_generateImage({
            model: openai.image('dall-e-3'),
            prompt,
          });
          return { image: image.base64, prompt };
        },
      }),
    },
  });
  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Streaming Text with NIM Language Model
DESCRIPTION: Example of using a NIM language model to generate text in a streaming fashion with the streamText function. It demonstrates setting up the provider, streaming text, and logging the results.

LANGUAGE: typescript
CODE:
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
import { streamText } from 'ai';

const nim = createOpenAICompatible({
  name: 'nim',
  baseURL: 'https://integrate.api.nvidia.com/v1',
  headers: {
    Authorization: `Bearer ${process.env.NIM_API_KEY}`,
  },
});

const result = streamText({
  model: nim.chatModel('deepseek-ai/deepseek-r1'),
  prompt: 'Tell me the history of the Northern White Rhino.',
});

for await (const textPart of result.textStream) {
  process.stdout.write(textPart);
}

console.log();
console.log('Token usage:', await result.usage);
console.log('Finish reason:', await result.finishReason);

----------------------------------------

TITLE: Installing AI SDK Valibot Package via NPM
DESCRIPTION: Command to install the Valibot schema support package for AI SDK using npm package manager

LANGUAGE: bash
CODE:
npm i @ai-sdk/valibot

----------------------------------------

TITLE: Installing LangDB Provider for AI SDK
DESCRIPTION: Commands to install the LangDB provider module using different package managers.

LANGUAGE: bash
CODE:
pnpm add @langdb/vercel-provider

LANGUAGE: bash
CODE:
npm install @langdb/vercel-provider

LANGUAGE: bash
CODE:
yarn add @langdb/vercel-provider

----------------------------------------

TITLE: Setting LangWatch API Key in Environment Variables
DESCRIPTION: Example of setting the LANGWATCH_API_KEY in a .env file.

LANGUAGE: bash
CODE:
LANGWATCH_API_KEY='your_api_key_here'

----------------------------------------

TITLE: Importing Default Ollama Provider
DESCRIPTION: Import the default ollama provider instance for basic usage.

LANGUAGE: typescript
CODE:
import { ollama } from 'ollama-ai-provider';

----------------------------------------

TITLE: Testing Express Server with cURL
DESCRIPTION: A bash command to test the Express server using cURL, sending a POST request to localhost on port 8080.

LANGUAGE: bash
CODE:
curl -X POST http://localhost:8080

----------------------------------------

TITLE: Importing Crosshatch AI Provider
DESCRIPTION: How to import the Crosshatch AI provider in TypeScript.

LANGUAGE: typescript
CODE:
import createCrosshatch from '@crosshatch/ai-provider';

----------------------------------------

TITLE: Configuring Embedding Model Settings
DESCRIPTION: Creating a text embedding model with custom settings including input type, truncation, output dimension, and output data type.

LANGUAGE: typescript
CODE:
import { voyage } from 'voyage-ai-provider';

const embeddingModel = voyage.textEmbeddingModel('voyage-3', {
  inputType: 'document', // 'document' or 'query'
  truncation: false,
  outputDimension: 1024, // the new model voyage-code-3, voyage-3-large has 4 different output dimensions: 256, 512, 1024 (default), 2048
  outputDtype: 'float', // output data types - int8, uint8, binary, ubinary are supported by the new model voyage-code-3, voyage-3-large
});

----------------------------------------

TITLE: Installing xAI Grok Provider Package
DESCRIPTION: This command installs the xAI Grok provider package for the AI SDK using npm. It adds the @ai-sdk/xai module to your project dependencies.

LANGUAGE: bash
CODE:
npm i @ai-sdk/xai

----------------------------------------

TITLE: Importing Together.ai Provider in TypeScript
DESCRIPTION: Code snippet demonstrating how to import the default Together.ai provider instance in TypeScript.

LANGUAGE: typescript
CODE:
import { togetherai } from '@ai-sdk/togetherai';

----------------------------------------

TITLE: Creating LlamaIndex Completion UI Component in Next.js
DESCRIPTION: Implements a React client component using the AI SDK's useCompletion hook to handle completion interactions. Provides a simple form interface for user input and displays completion results.

LANGUAGE: tsx
CODE:
'use client';

import { useCompletion } from '@ai-sdk/react';

export default function Chat() {
  const { completion, input, handleInputChange, handleSubmit } =
    useCompletion();

  return (
    <div>
      {completion}
      <form onSubmit={handleSubmit}>
        <input value={input} onChange={handleInputChange} />
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Streaming Text with Cloudflare Workers AI
DESCRIPTION: Demonstrates how to use the streamText function with a Cloudflare Workers AI language model to stream generated text.

LANGUAGE: typescript
CODE:
import { createWorkersAI } from 'workers-ai-provider';
import { streamText } from 'ai';

type Env = {
  AI: Ai;
};

export default {
  async fetch(_: Request, env: Env) {
    const workersai = createWorkersAI({ binding: env.AI });
    const result = streamText({
      model: workersai('@cf/meta/llama-2-7b-chat-int8'),
      prompt: 'Write a 50-word essay about hello world.',
    });

    return result.toTextStreamResponse({
      headers: {
        // add these headers to ensure that the
        // response is chunked and streamed
        'Content-Type': 'text/x-unknown',
        'content-encoding': 'identity',
        'transfer-encoding': 'chunked',
      },
    });
  },
};

----------------------------------------

TITLE: Implementing Data Stream with Hono and OpenAI
DESCRIPTION: Implementation of a Hono server endpoint that uses OpenAI's GPT-4 model to stream text responses. Uses toDataStream method to convert AI response to a data stream.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { serve } from '@hono/node-server';
import { streamText } from 'ai';
import { Hono } from 'hono';
import { stream } from 'hono/streaming';

const app = new Hono();

app.post('/', async c => {
  const result = streamText({
    model: openai('gpt-4o'),
    prompt: 'Invent a new holiday and describe its traditions.',
  });

  // Mark the response as a v1 data stream:
  c.header('X-Vercel-AI-Data-Stream', 'v1');
  c.header('Content-Type', 'text/plain; charset=utf-8');

  return stream(c, stream => stream.pipe(result.toDataStream()));
});

serve({ fetch: app.fetch, port: 8080 });

----------------------------------------

TITLE: Installing Qwen Provider with Package Managers
DESCRIPTION: Commands to install the qwen-ai-provider package using different package managers (pnpm, npm, yarn).

LANGUAGE: bash
CODE:
pnpm add qwen-ai-provider

LANGUAGE: bash
CODE:
npm install qwen-ai-provider

LANGUAGE: bash
CODE:
yarn add qwen-ai-provider

----------------------------------------

TITLE: Handling Stream Completion with onFinish Callback in TypeScript
DESCRIPTION: Demonstrates how to use the onFinish callback to handle the final object after streaming. The callback receives the final object and any potential errors from schema validation.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { streamObject } from 'ai';
import { z } from 'zod';

const result = streamObject({
  model: openai('gpt-4-turbo'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.string()),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a lasagna recipe.',
  onFinish({ object, error }) {
    // handle type validation failure (when the object does not match the schema):
    if (object === undefined) {
      console.error('Error:', error);
      return;
    }

    console.log('Final object:', JSON.stringify(object, null, 2));
  },
});

----------------------------------------

TITLE: Markdown Changelog
DESCRIPTION: Version history documenting changes to the @ai-sdk/togetherai package and its dependencies, organized by semantic versioning.

LANGUAGE: markdown
CODE:
# @ai-sdk/togetherai\n\n## 0.2.5\n\n### Patch Changes\n\n- Updated dependencies [d186cca]\n  - @ai-sdk/openai-compatible@0.2.5\n\n## 0.2.4\n\n### Patch Changes\n\n- Updated dependencies [28be004]\n  - @ai-sdk/provider-utils@2.2.3\n  - @ai-sdk/openai-compatible@0.2.4

----------------------------------------

TITLE: Installing Together.ai Provider for AI SDK
DESCRIPTION: Command to install the Together.ai provider module for the AI SDK using npm.

LANGUAGE: bash
CODE:
npm i @ai-sdk/togetherai

----------------------------------------

TITLE: Installing Spark Provider Package
DESCRIPTION: Install the Spark provider package using npm. This package is required to use the Spark provider with the AI SDK.

LANGUAGE: bash
CODE:
npm i spark-ai-provider

----------------------------------------

TITLE: Installing OpenRouter Provider Package
DESCRIPTION: Package installation commands for different package managers to add the OpenRouter provider to your project.

LANGUAGE: bash
CODE:
pnpm add @openrouter/ai-sdk-provider

LANGUAGE: bash
CODE:
npm install @openrouter/ai-sdk-provider

LANGUAGE: bash
CODE:
yarn add @openrouter/ai-sdk-provider

----------------------------------------

TITLE: Creating Client-Side Chat Interface with Image Display in Next.js
DESCRIPTION: Implements a React client component that provides a chat interface with image generation capabilities. Uses the useChat hook for message handling and displays generated images using Next.js Image component.

LANGUAGE: tsx
CODE:
'use client';

import { useChat } from '@ai-sdk/react';
import Image from 'next/image';

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat();

  return (
    <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch">
      <div className="space-y-4">
        {messages.map(m => (
          <div key={m.id} className="whitespace-pre-wrap">
            <div key={m.id}>
              <div className="font-bold">{m.role}</div>
              {m.toolInvocations ? (
                m.toolInvocations.map(ti =>
                  ti.toolName === 'generateImage' ? (
                    ti.state === 'result' ? (
                      <Image
                        key={ti.toolCallId}
                        src={`data:image/png;base64,${ti.result.image}`}
                        alt={ti.result.prompt}
                        height={400}
                        width={400}
                      />
                    ) : (
                      <div key={ti.toolCallId} className="animate-pulse">
                        Generating image...
                      </div>
                    )
                  ) : null,
                )
              ) : (
                <p>{m.content}</p>
              )}
            </div>
          </div>
        ))}
      </div>

      <form onSubmit={handleSubmit}>
        <input
          className="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl"
          value={input}
          placeholder="Say something..."
          onChange={handleInputChange}
        />
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Importing Cohere Provider
DESCRIPTION: Import the default Cohere provider instance from the AI SDK package.

LANGUAGE: typescript
CODE:
import { cohere } from '@ai-sdk/cohere';

----------------------------------------

TITLE: Enabling Telemetry in AI SDK Function Calls with Traceloop
DESCRIPTION: Demonstrates how to use the experimental_telemetry option to enable telemetry on supported AI SDK function calls. This example shows how to generate text using OpenAI's GPT-4 model with telemetry enabled and custom metadata.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

const result = await generateText({
  model: openai('gpt-4o-mini'),
  prompt: 'What is 2 + 2?',
  experimental_telemetry: {
    isEnabled: true,
    metadata: {
      query: 'weather',
      location: 'San Francisco',
    },
  },
});

----------------------------------------

TITLE: Listing AI SDK Svelte Components
DESCRIPTION: This snippet lists the main Svelte UI components provided by the AI SDK, including Chat, Completion, and StructuredObject. It also provides links to their respective documentation.

LANGUAGE: markdown
CODE:
- [`Chat`](https://sdk.vercel.ai/docs/reference/ai-sdk-ui/use-chat)
- [`Completion`](https://sdk.vercel.ai/docs/reference/ai-sdk-ui/use-completion)
- [`StructuredObject`](https://sdk.vercel.ai/docs/reference/ai-sdk-ui/use-object)

----------------------------------------

TITLE: Testing the API Endpoint
DESCRIPTION: Curl command to test the server endpoint with a POST request.

LANGUAGE: sh
CODE:
curl -X POST http://localhost:8080

----------------------------------------

TITLE: Installing Crosshatch AI Provider
DESCRIPTION: Instructions for installing the Crosshatch AI provider using different package managers.

LANGUAGE: bash
CODE:
pnpm add @crosshatch/ai-provider

LANGUAGE: bash
CODE:
npm install @crosshatch/ai-provider

LANGUAGE: bash
CODE:
yarn add @crosshatch/ai-provider

----------------------------------------

TITLE: Manual Trace Creation with LangWatch
DESCRIPTION: TypeScript code for manually creating a trace using LangWatch, including metadata for thread and user identification.

LANGUAGE: typescript
CODE:
import { LangWatch } from 'langwatch';

const langwatch = new LangWatch();

const trace = langwatch.getTrace({
  metadata: { threadId: 'mythread-123', userId: 'myuser-123' },
});

----------------------------------------

TITLE: Implementing Redis Caching with Lifecycle Callbacks
DESCRIPTION: Example of implementing caching using Upstash Redis and Next.js API route, utilizing the onFinish lifecycle callback to cache responses for 1 hour.

LANGUAGE: typescript
CODE:
import { openai } from '@ai-sdk/openai';
import { formatDataStreamPart, streamText } from 'ai';
import { Redis } from '@upstash/redis';

export const maxDuration = 30;

const redis = new Redis({
  url: process.env.KV_URL,
  token: process.env.KV_TOKEN,
});

export async function POST(req: Request) {
  const { messages } = await req.json();

  const key = JSON.stringify(messages);

  const cached = await redis.get(key);
  if (cached != null) {
    return new Response(formatDataStreamPart('text', cached), {
      status: 200,
      headers: { 'Content-Type': 'text/plain' },
    });
  }

  const result = streamText({
    model: openai('gpt-4o'),
    messages,
    async onFinish({ text }) {
      await redis.set(key, text);
      await redis.expire(key, 60 * 60);
    },
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Creating Together.ai Language Model
DESCRIPTION: How to create a Together.ai language model using the provider instance.

LANGUAGE: typescript
CODE:
const model = togetherai('google/gemma-2-9b-it');

----------------------------------------

TITLE: Installing Luma Provider for AI SDK
DESCRIPTION: Command to install the Luma provider module for the AI SDK using npm.

LANGUAGE: bash
CODE:
npm i @ai-sdk/luma

----------------------------------------

TITLE: Installing Amazon Bedrock Provider Package
DESCRIPTION: Command to install the Amazon Bedrock provider package using npm package manager.

LANGUAGE: bash
CODE:
npm i @ai-sdk/amazon-bedrock

----------------------------------------

TITLE: Creating Next.js App with AI SDK and OpenAI Rate Limits Example using Yarn
DESCRIPTION: This command uses Yarn to create a new Next.js application with the AI SDK and OpenAI rate limits example. It clones the example from the Vercel AI repository.

LANGUAGE: bash
CODE:
yarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-rate-limits next-openai-rate-limits-app

----------------------------------------

TITLE: Initializing OpenRouter Provider
DESCRIPTION: Creates an OpenRouter provider instance using an API key for authentication.

LANGUAGE: typescript
CODE:
import { createOpenRouter } from '@openrouter/ai-sdk-provider';

const openrouter = createOpenRouter({
  apiKey: 'YOUR_OPENROUTER_API_KEY',
});

----------------------------------------

TITLE: Generating Text with Cohere Model in TypeScript
DESCRIPTION: This example demonstrates how to use the Cohere provider to generate text using the 'command-r-plus' model. It includes importing necessary functions and specifying the prompt for text generation.

LANGUAGE: typescript
CODE:
import { cohere } from '@ai-sdk/cohere';
import { generateText } from 'ai';

const { text } = await generateText({
  model: cohere('command-r-plus'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Using Cache Points with Amazon Bedrock
DESCRIPTION: Example of using cache points with Amazon Bedrock for prompt caching.

LANGUAGE: typescript
CODE:
import { bedrock } from '@ai-sdk/amazon-bedrock';
import { generateText } from 'ai';

const cyberpunkAnalysis = '... literary analysis of cyberpunk themes and concepts ...';

const result = await generateText({
  model: bedrock('anthropic.claude-3-5-sonnet-20241022-v2:0'),
  messages: [
    {
      role: 'system',
      content: `You are an expert on William Gibson's cyberpunk literature and themes. You have access to the following academic analysis: ${cyberpunkAnalysis}`,
      providerOptions: {
        bedrock: { cachePoint: { type: 'default' } },
      },
    },
    {
      role: 'user',
      content: 'What are the key cyberpunk themes that Gibson explores in Neuromancer?',
    },
  ],
});

console.log(result.text);
console.log(result.providerMetadata?.bedrock?.usage);

----------------------------------------

TITLE: Streaming Text with Image Prompt using AI SDK and Anthropic's Claude Model in TypeScript
DESCRIPTION: This code snippet demonstrates how to use the AI SDK to stream text responses from Anthropic's Claude model based on an image input. It reads an image file, sends it along with a text prompt to the model, and streams the generated response to stdout.

LANGUAGE: typescript
CODE:
import { anthropic } from '@ai-sdk/anthropic';
import { streamText } from 'ai';
import 'dotenv/config';
import fs from 'node:fs';

async function main() {
  const result = streamText({
    model: anthropic('claude-3-5-sonnet-20240620'),
    messages: [
      {
        role: 'user',
        content: [
          { type: 'text', text: 'Describe the image in detail.' },
          { type: 'image', image: fs.readFileSync('./data/comic-cat.png') },
        ],
      },
    ],
  });

  for await (const textPart of result.textStream) {
    process.stdout.write(textPart);
  }
}

main().catch(console.error);

----------------------------------------

TITLE: Creating Solid.js Project
DESCRIPTION: Commands for initializing a new Solid.js project either in the current directory or in a new directory

LANGUAGE: bash
CODE:
# create a new project in the current directory
npm init solid@latest

# create a new project in my-app
npm init solid@latest my-app

----------------------------------------

TITLE: Installing Next.js AI Example with npm
DESCRIPTION: This command uses create-next-app to bootstrap a Next.js application with the AI SDK and OpenAI integration example.

LANGUAGE: bash
CODE:
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app

----------------------------------------

TITLE: Creating Ollama Embedding Model
DESCRIPTION: Initialize an embedding model using the Ollama embeddings API.

LANGUAGE: typescript
CODE:
const model = ollama.embedding('nomic-embed-text');

----------------------------------------

TITLE: Testing AI SDK Express.js Endpoint with cURL
DESCRIPTION: Demonstrates how to test the Express.js server endpoint that utilizes the AI SDK. This cURL command sends a POST request to the local server.

LANGUAGE: sh
CODE:
curl -X POST http://localhost:8080

----------------------------------------

TITLE: Importing DeepSeek Provider
DESCRIPTION: How to import the default DeepSeek provider instance from the package

LANGUAGE: typescript
CODE:
import { deepseek } from '@ai-sdk/deepseek';

----------------------------------------

TITLE: Batch Embedding with LM Studio
DESCRIPTION: Demonstrates batch embedding of multiple text values using LM Studio. Useful for preparing data for retrieval-augmented generation (RAG) applications.

LANGUAGE: typescript
CODE:
import { createOpenAICompatible } from '@ai-sdk/openai';
import { embedMany } from 'ai';

const lmstudio = createOpenAICompatible({
  name: 'lmstudio',
  baseURL: 'https://localhost:1234/v1',
});

// 'embeddings' is an array of embedding objects (number[][]).
// It is sorted in the same order as the input values.
const { embeddings } = await embedMany({
  model: lmstudio.textEmbeddingModel('text-embedding-nomic-embed-text-v1.5'),
  values: [
    'sunny day at the beach',
    'rainy afternoon in the city',
    'snowy night in the mountains',
  ],
});

----------------------------------------

TITLE: Installing Project with PNPM
DESCRIPTION: Command to create a new Next.js application using pnpm create next-app with the OpenAI telemetry template

LANGUAGE: bash
CODE:
pnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry next-openai-telemetry-app

----------------------------------------

TITLE: Creating Next.js App with npx for AI SDK and OpenAI Example
DESCRIPTION: Command to create a new Next.js application using npx, bootstrapping it with the AI SDK and OpenAI example from the Vercel AI repository.

LANGUAGE: bash
CODE:
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai next-openai-app

----------------------------------------

TITLE: Installing Ollama Provider Package
DESCRIPTION: Commands to install the ollama-ai-provider package using different package managers.

LANGUAGE: bash
CODE:
pnpm add ollama-ai-provider

LANGUAGE: bash
CODE:
npm install ollama-ai-provider

LANGUAGE: bash
CODE:
yarn add ollama-ai-provider

----------------------------------------

TITLE: Changelog Entry - FAL Provider Updates
DESCRIPTION: Markdown changelog documenting version changes and updates for the @ai-sdk/fal package, including dependency updates and feature additions.

LANGUAGE: markdown
CODE:
# @ai-sdk/fal

## 0.1.4

### Patch Changes

- 56c6d8b: feat (providers/fal): fall back to FAL_KEY for api key environment variable name

## 0.1.3

### Patch Changes

- Updated dependencies [28be004]
  - @ai-sdk/provider-utils@2.2.3

## 0.1.2

### Patch Changes

- Updated dependencies [b01120e]
  - @ai-sdk/provider-utils@2.2.2

----------------------------------------

TITLE: Recent Feature Updates Changelog Entry
DESCRIPTION: Changelog entry documenting multiple feature additions including model version updates, tool call support, and object generation capabilities

LANGUAGE: markdown
CODE:
## 1.2.1

### Patch Changes

- 724c0a1: chore (provider/cohere): update model versions
- f1c34e0: feat (provider/cohere): support tool calls finish reason
- 724c0a1: feat (provider/cohere): support all tool choice options
- 724c0a1: feat (provider/cohere): support object generation (json mode)
- 724c0a1: feat (provider/cohere): support object generation (tool mode)

----------------------------------------

TITLE: Testing Hono Server Endpoint with cURL
DESCRIPTION: Command to test the Hono server endpoint using cURL on localhost port 8080.

LANGUAGE: bash
CODE:
curl -X POST http://localhost:8080

----------------------------------------

TITLE: Creating Next.js App with AI SDK and OpenAI using pnpm
DESCRIPTION: Command to create a new Next.js application with AI SDK and OpenAI integration using pnpm and create-next-app.

LANGUAGE: bash
CODE:
pnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-pages next-openai-app

----------------------------------------

TITLE: Creating Next.js App with AI SDK and Google Vertex AI Example
DESCRIPTION: This command uses create-next-app to bootstrap a new Next.js project with the AI SDK and Google Vertex AI example. It clones the example from the Vercel AI repository.

LANGUAGE: bash
CODE:
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-google-vertex-edge next-vertex-edge-app

----------------------------------------

TITLE: Importing Default Qwen Provider Instance
DESCRIPTION: Import the default Qwen provider instance from the qwen-ai-provider package.

LANGUAGE: typescript
CODE:
import { qwen } from 'qwen-ai-provider';

----------------------------------------

TITLE: Configuring Environment Variables for AI SDK in Express.js
DESCRIPTION: Creates a .env file with necessary API keys for the AI SDK, focusing on OpenAI integration. Additional settings may be required depending on the providers used.

LANGUAGE: sh
CODE:
OPENAI_API_KEY="YOUR_OPENAI_API_KEY"

----------------------------------------

TITLE: Installing Cerebras Provider Package
DESCRIPTION: Command to install the Cerebras provider package using npm package manager.

LANGUAGE: bash
CODE:
npm i @ai-sdk/cerebras

----------------------------------------

TITLE: Using simulateStreamingMiddleware with Non-Streaming Model in TypeScript
DESCRIPTION: Demonstrates how to use simulateStreamingMiddleware with a non-streaming language model to create a streaming interface. It includes wrapping the model and processing the resulting stream.

LANGUAGE: typescript
CODE:
import { streamText } from 'ai';
import { wrapLanguageModel } from 'ai';
import { simulateStreamingMiddleware } from 'ai';

// Example with a non-streaming model
const result = streamText({
  model: wrapLanguageModel({
    model: nonStreamingModel,
    middleware: simulateStreamingMiddleware(),
  }),
  prompt: 'Your prompt here',
});

// Now you can use the streaming interface
for await (const chunk of result.fullStream) {
  // Process streaming chunks
}

----------------------------------------

TITLE: Creating Next.js App with AI SDK and OpenAI using Yarn
DESCRIPTION: Command to create a new Next.js application with AI SDK and OpenAI integration using Yarn and create-next-app.

LANGUAGE: bash
CODE:
yarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-pages next-openai-app

----------------------------------------

TITLE: Installing Dependencies with PNPM
DESCRIPTION: Command to install project dependencies using PNPM package manager

LANGUAGE: sh
CODE:
pnpm install

----------------------------------------

TITLE: Importing Luma Provider in TypeScript
DESCRIPTION: Basic import of the default Luma provider instance.

LANGUAGE: typescript
CODE:
import { luma } from '@ai-sdk/luma';

----------------------------------------

TITLE: Configuring OpenTelemetry for Braintrust in Next.js
DESCRIPTION: Sets up environment variables in a Next.js app's .env file to send telemetry data to Braintrust using OpenTelemetry Protocol Exporter.

LANGUAGE: bash
CODE:
OTEL_EXPORTER_OTLP_ENDPOINT=https://api.braintrust.dev/otel
OTEL_EXPORTER_OTLP_HEADERS="Authorization=Bearer <Your API Key>, x-bt-parent=project_id:<Your Project ID>"

----------------------------------------

TITLE: Installing Anthropic Provider Package
DESCRIPTION: Command to install the Anthropic provider package via npm.

LANGUAGE: bash
CODE:
npm i @ai-sdk/anthropic

----------------------------------------

TITLE: Installing and Building AI SDK
DESCRIPTION: Commands for installing dependencies and building the AI SDK from the root directory

LANGUAGE: sh
CODE:
pnpm install
pnpm build

----------------------------------------

TITLE: Configuring Environment Variables for AI SDK
DESCRIPTION: Setting up the required environment variables in .env file for OpenAI API integration

LANGUAGE: sh
CODE:
OPENAI_API_KEY="YOUR_OPENAI_API_KEY"

----------------------------------------

TITLE: Importing createAI Function from AI SDK RSC
DESCRIPTION: This snippet shows how to import the createAI function from the AI SDK RSC module.

LANGUAGE: JavaScript
CODE:
import { createAI } from "ai/rsc"

----------------------------------------

TITLE: Importing Default Perplexity Provider
DESCRIPTION: Shows how to import the default Perplexity provider instance from the SDK

LANGUAGE: typescript
CODE:
import { perplexity } from '@ai-sdk/perplexity';

----------------------------------------

TITLE: Installing React UI Package
DESCRIPTION: Installation command for the React-specific UI components package using npm.

LANGUAGE: shell
CODE:
npm install @ai-sdk/react

----------------------------------------

TITLE: Installing AI SDK Base Package
DESCRIPTION: Basic installation command for the AI SDK using npm package manager. Requires Node.js 18+ and pnpm.

LANGUAGE: shell
CODE:
npm install ai

----------------------------------------

TITLE: Creating a Chat Route Handler with DeepSeek R1
DESCRIPTION: This code creates a route handler for a chat endpoint using Next.js and the AI SDK. It sets up streaming text generation with the DeepSeek R1 model and includes reasoning tokens in the response.

LANGUAGE: typescript
CODE:
import { deepseek } from '@ai-sdk/deepseek';
import { streamText } from 'ai';

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: deepseek('deepseek-reasoner'),
    messages,
  });

  return result.toDataStreamResponse({
    sendReasoning: true,
  });
}

----------------------------------------

TITLE: Creating a Chat Route Handler with DeepSeek R1
DESCRIPTION: This code creates a route handler for a chat endpoint using Next.js and the AI SDK. It sets up streaming text generation with the DeepSeek R1 model and includes reasoning tokens in the response.

LANGUAGE: typescript
CODE:
import { deepseek } from '@ai-sdk/deepseek';
import { streamText } from 'ai';

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: deepseek('deepseek-reasoner'),
    messages,
  });

  return result.toDataStreamResponse({
    sendReasoning: true,
  });
}

----------------------------------------

TITLE: Starting the Express.js Server with AI SDK Integration
DESCRIPTION: Launches the Express.js server that incorporates the AI SDK. This command starts the development server.

LANGUAGE: sh
CODE:
pnpm dev

----------------------------------------

TITLE: Importing embed Function in TypeScript
DESCRIPTION: This snippet shows how to import the `embed` function from the 'ai' package.

LANGUAGE: typescript
CODE:
import { embed } from "ai"

----------------------------------------

TITLE: Image Generation with Model-Specific Options
DESCRIPTION: Shows how to use model-specific options when generating images with the recraft-v3 model

LANGUAGE: typescript
CODE:
import { replicate } from '@ai-sdk/replicate';
import { experimental_generateImage as generateImage } from 'ai';

const { image } = await generateImage({
  model: replicate.image('recraft-ai/recraft-v3'),
  prompt: 'The Loch Ness Monster getting a manicure',
  size: '1365x1024',
  providerOptions: {
    replicate: {
      style: 'realistic_image',
    },
  },
});

----------------------------------------

TITLE: Creating Next.js App with AI SDK and OpenAI Rate Limits Example using pnpm
DESCRIPTION: This command uses pnpm to bootstrap a new Next.js application with the AI SDK and OpenAI rate limits example. It clones the example from the Vercel AI repository.

LANGUAGE: bash
CODE:
pnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-rate-limits next-openai-rate-limits-app

----------------------------------------

TITLE: Installing AI SDK React Package
DESCRIPTION: Command to install the AI SDK React package for building UI components.

LANGUAGE: shell
CODE:
npm install @ai-sdk/react

----------------------------------------

TITLE: Starting SSE Server and Running SSE Transport Example
DESCRIPTION: These commands start the SSE server and run the SSE Transport example for the MCP and AI SDK integration.

LANGUAGE: sh
CODE:
pnpm sse:server

LANGUAGE: sh
CODE:
pnpm sse:client

----------------------------------------

TITLE: Configuring Anthropic API Request in TypeScript
DESCRIPTION: This snippet shows the configuration of a request body for the Anthropic API, including model selection, token limits, and message structure. It demonstrates how to set up a prompt for the AI model.

LANGUAGE: typescript
CODE:
Body {
  "model": "claude-3-5-sonnet-20240620",
  "max_tokens": 4096,
  "temperature": 0,
  "messages": [
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "You are a JavaScript expert."
        },
        {
          "type": "text",
          "text": "Error messages: \nAPICallError [AI_APICallError]: Failed to process error response\n    at postToApi (/Users/larsgrammel/repositories/ai/packages/provider-utils/dist/index.js:382:15)\n    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n    ... 4 lines matching cause stack trace ...\n    at async fn (/Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:2723:36)\n    at async /Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:339:22\n    at async main (/Users/larsgrammel/repositories/ai/examples/ai-core/src/generate-text/anthropic-cache-control.ts:2:1351) {\n  cause: TypeError: Body is unusable\n      at consumeBody (node:internal/deps/undici/undici:4281:15)\n      at _Response.text (node:internal/deps/undici/undici:4236:18)\n      at /Users/larsgrammel/repositories/ai/packages/provider-utils/dist/index.js:443:39\n      at postToApi (/Users/larsgrammel/repositories/ai/packages/provider-utils/dist/index.js:373:34)\n      at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n      at async AnthropicMessagesLanguageModel.doGenerate (/Users/larsgrammel/repositories/ai/packages/anthropic/dist/index.js:316:50)\n      at async fn (/Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:2748:34)\n      at async /Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:339:22\n      at async _retryWithExponentialBackoff (/Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:170:12)\n      at async fn (/Users/larsgrammel/repositories/ai/packages/ai/dist/index.js:2723:36),\n  url: 'https://api.anthropic.com/v1/messages',\n  requestBodyValues: {\n    model: 'claude-3-5-sonnet-20240620',\n    top_k: undefined,\n    max_tokens: 4096,\n    temperature: 0,\n    top_p: undefined,\n    stop_sequences: undefined,\n    system: undefined,\n    messages: [ [Object] ],\n    tools: undefined,\n    tool_choice: undefined\n  },\n  statusCode: 400,\n  responseHeaders: {\n    'cf-cache-status': 'DYNAMIC',\n    'cf-ray': '8b39b60ab8734516-TXL',\n    connection: 'keep-alive',\n    'content-length': '171',\n    'content-type': 'application/json',\n    date: 'Thu, 15 Aug 2024 14:00:28 GMT',\n    'request-id': 'req_01PLrS159iiihG7kS9PFQiqx',\n    server: 'cloudflare',\n    via: '1.1 google',\n    'x-cloud-trace-context': '1371f8e6d358102b79d109db3829d62e',\n    'x-robots-tag': 'none',\n    'x-should-retry': 'false'\n  },\n  responseBody: undefined,\n  isRetryable: false,\n  data: undefined,\n  [Symbol(vercel.ai.error)]: true,\n  [Symbol(vercel.ai.error.AI_APICallError)]: true\n}",
          "cache_control": {
            "type": "ephemeral"
          }
        },
        {
          "type": "text",
          "text": "Explain the error message."
        }
      ]
    }
  ]
}

----------------------------------------

TITLE: Defining setRoomTemperature Function for OpenAI Assistant
DESCRIPTION: This JSON schema defines the setRoomTemperature function for the OpenAI assistant. It specifies the function name, description, and required parameters for setting room temperature.

LANGUAGE: json
CODE:
{\n  "name": "setRoomTemperature",\n  "description": "Set the temperature in a room",\n  "parameters": {\n    "type": "object",\n    "properties": {\n      "room": {\n        "type": "string",\n        "enum": ["bedroom", "home office", "living room", "kitchen", "bathroom"]\n      },\n      "temperature": { "type": "number" }\n    },\n    "required": ["room", "temperature"]\n  }\n}

----------------------------------------

TITLE: Creating Custom Replicate Provider Instance
DESCRIPTION: Creates a customized Replicate provider instance with specific API token and settings

LANGUAGE: typescript
CODE:
import { createReplicate } from '@ai-sdk/replicate';

const replicate = createReplicate({
  apiToken: process.env.REPLICATE_API_TOKEN ?? '',
});

----------------------------------------

TITLE: Installing Dependencies and Environment Setup
DESCRIPTION: Commands for installing project dependencies using pnpm package manager and creating the environment configuration file.

LANGUAGE: shell
CODE:
pnpm i
cp .env.local.example .env.local

----------------------------------------

TITLE: Using OpenAI Compatible Provider for Text Generation in TypeScript
DESCRIPTION: This example shows how to use the OpenAI-compatible provider to generate text using a specified model and prompt.

LANGUAGE: typescript
CODE:
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
import { generateText } from 'ai';

const { text } = await generateText({
  model: createOpenAICompatible({
    baseURL: 'https://api.example.com/v1',
    name: 'example',
    apiKey: process.env.MY_API_KEY,
  }).chatModel('meta-llama/Llama-3-70b-chat-hf'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Migrating topK Setting for Anthropic
DESCRIPTION: Example of updating the topK setting from model-specific to standard for Anthropic.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
const result = await generateText({
  model: anthropic('claude-3-5-sonnet-latest', {
    topK: 0.5,
  }),
});

// AI SDK 4.0
const result = await generateText({
  model: anthropic('claude-3-5-sonnet-latest'),
  topK: 0.5,
});

----------------------------------------

TITLE: Prompting LLMs with AI SDK Documentation
DESCRIPTION: Example of how to format a prompt for querying an LLM about the AI SDK using the documentation content.

LANGUAGE: markdown
CODE:
```prompt
Documentation:
{paste documentation here}
---
Based on the above documentation, answer the following:
{your question}
```

----------------------------------------

TITLE: Generating Text with Cerebras Model
DESCRIPTION: Example of using a Cerebras language model to generate text using the generateText function from the AI SDK.

LANGUAGE: typescript
CODE:
import { cerebras } from '@ai-sdk/cerebras';
import { generateText } from 'ai';

const { text } = await generateText({
  model: cerebras('llama3.1-8b'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Starting Nest.js Development Server with AI SDK
DESCRIPTION: Launches the Nest.js development server with the AI SDK integration using the pnpm run command.

LANGUAGE: sh
CODE:
pnpm run start:dev

----------------------------------------

TITLE: Node.js OpenTelemetry Setup
DESCRIPTION: Configuration of OpenTelemetry in Node.js environment with AISDKExporter.

LANGUAGE: typescript
CODE:
import { AISDKExporter } from 'langsmith/vercel';
import { NodeSDK } from '@opentelemetry/sdk-node';
import { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';

const sdk = new NodeSDK({
  traceExporter: new AISDKExporter(),
  instrumentations: [getNodeAutoInstrumentations()]
});

sdk.start();

----------------------------------------

TITLE: Checking for AI_InvalidDataContent Error in TypeScript
DESCRIPTION: This snippet demonstrates how to check if an error is an instance of AI_InvalidDataContent using the isInstance method from the 'ai' package. This is useful for error handling and implementing specific logic for this type of error.

LANGUAGE: typescript
CODE:
import { InvalidDataContent } from 'ai';

if (InvalidDataContent.isInstance(error)) {
  // Handle the error
}

----------------------------------------

TITLE: Generating Text with xAI Grok Model in TypeScript
DESCRIPTION: This example demonstrates how to use the xAI Grok provider to generate text. It imports the necessary functions, specifies the Grok model, and provides a prompt for generating a vegetarian lasagna recipe.

LANGUAGE: typescript
CODE:
import { xai } from '@ai-sdk/xai';
import { generateText } from 'ai';

const { text } = await generateText({
  model: xai('grok-2-1212'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Installing Cerebras SDK Package
DESCRIPTION: Command line instructions for installing the Cerebras provider package using different package managers.

LANGUAGE: shell
CODE:
pnpm add @ai-sdk/cerebras

LANGUAGE: shell
CODE:
npm install @ai-sdk/cerebras

LANGUAGE: shell
CODE:
yarn add @ai-sdk/cerebras

----------------------------------------

TITLE: Installing and Building AI SDK
DESCRIPTION: Commands for installing dependencies and building the AI SDK project using pnpm package manager.

LANGUAGE: sh
CODE:
pnpm install
pnpm build

----------------------------------------

TITLE: Running Individual AI SDK Codemods
DESCRIPTION: This command allows you to run a specific codemod by specifying its name and the path to apply it to. It provides more granular control over the transformation process.

LANGUAGE: sh
CODE:
npx @ai-sdk/codemod <codemod-name> <path>

----------------------------------------

TITLE: Importing MistralStream in React
DESCRIPTION: This snippet demonstrates how to import the MistralStream function from the 'ai' package in a React application.

LANGUAGE: javascript
CODE:
import { MistralStream } from "ai"

----------------------------------------

TITLE: Package Version History in Markdown
DESCRIPTION: Markdown formatted changelog entries documenting version updates, dependency changes, and feature additions for the @ai-sdk/xai package.

LANGUAGE: markdown
CODE:
# @ai-sdk/xai

## 1.2.6

### Patch Changes

- Updated dependencies [d186cca]
  - @ai-sdk/openai-compatible@0.2.5

## 1.2.5

### Patch Changes

- Updated dependencies [28be004]
  - @ai-sdk/provider-utils@2.2.3
  - @ai-sdk/openai-compatible@0.2.4

----------------------------------------

TITLE: Migrating AI-stream Related Methods
DESCRIPTION: Example of updating AI-stream related methods to data stream methods.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
const result = await streamText({
  // ...
});

result.toAIStream();
result.pipeAIStreamToResponse(response);
result.toAIStreamResponse();

// AI SDK 4.0
const result = streamText({
  // ...
});

result.toDataStream();
result.pipeDataStreamToResponse(response);
result.toDataStreamResponse();

----------------------------------------

TITLE: Basic OpenAI o3-mini Text Generation with AI SDK
DESCRIPTION: Demonstrates the basic usage of o3-mini model for text generation using AI SDK Core. Shows how to import dependencies and make a simple text generation call.

LANGUAGE: tsx
CODE:
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const { text } = await generateText({
  model: openai('o3-mini'),
  prompt: 'Explain the concept of quantum entanglement.',
});

----------------------------------------

TITLE: Running Example Projects
DESCRIPTION: Commands for navigating to example projects and running them using tsx or dev server.

LANGUAGE: bash
CODE:
cd examples/ai-core
pnpm tsx src/stream-text/openai.ts
pnpm dev

----------------------------------------

TITLE: Importing AWSBedrockLlama2Stream in React
DESCRIPTION: Shows how to import the AWSBedrockLlama2Stream function from the ai package in a React application.

LANGUAGE: javascript
CODE:
import { AWSBedrockLlama2Stream } from "ai"

----------------------------------------

TITLE: Edge Runtime Text Generation with Gemini
DESCRIPTION: Example of generating text using the Gemini model in Edge runtime environment

LANGUAGE: typescript
CODE:
import { vertex } from '@ai-sdk/google-vertex/edge';
import { generateText } from 'ai';

const { text } = await generateText({
  model: vertex('gemini-1.5-flash'),
  prompt: 'Write a vegetarian lasagna recipe.',
});

----------------------------------------

TITLE: Migrating from Google Facade
DESCRIPTION: Example of updating from the Google facade to the createGoogleGenerativeAI function.

LANGUAGE: typescript
CODE:
// AI SDK 3.4
const google = new Google({
  // ...
});

// AI SDK 4.0
const google = createGoogleGenerativeAI({
  // ...
});

----------------------------------------

TITLE: Setup and Build Commands
DESCRIPTION: Basic commands for setting up the project locally and building packages.

LANGUAGE: bash
CODE:
pnpm install
pnpm build

----------------------------------------

TITLE: Development Commands
DESCRIPTION: Commands for building packages in watch mode and running tests during development.

LANGUAGE: bash
CODE:
pnpm build
pnpm build:watch
pnpm test
pnpm prettier-fix

----------------------------------------

TITLE: Importing getMutableAIState from AI SDK RSC in JavaScript
DESCRIPTION: This snippet demonstrates how to import the getMutableAIState function from the AI SDK RSC package.

LANGUAGE: javascript
CODE:
import { getMutableAIState } from "ai/rsc"

----------------------------------------

TITLE: Rendering Stock Information Component
DESCRIPTION: This component fetches and displays stock information for a given symbol and number of months. It's used as a custom component in the chat interface.

LANGUAGE: tsx
CODE:
export async function Stock({ symbol, numOfMonths }) {
  const data = await fetch(
    `https://api.example.com/stock/${symbol}/${numOfMonths}`,
  );

  return (
    <div>
      <div>{symbol}</div>

      <div>
        {data.timeline.map(data => (
          <div>
            <div>{data.date}</div>
            <div>{data.value}</div>
          </div>
        ))}
      </div>
    </div>
  );
}

----------------------------------------

TITLE: Handling Streamed UI Components in Next.js Client
DESCRIPTION: This client-side code snippet shows how to handle streamed UI components in a Next.js application. It updates the UI with the received React components, which can include loading states and final content.

LANGUAGE: tsx
CODE:
'use client';

import { useState } from 'react';
import { generateResponse } from './actions';
import { readStreamableValue } from 'ai/rsc';

// Force the page to be dynamic and allow streaming responses up to 30 seconds
export const maxDuration = 30;

export default function Home() {
  const [input, setInput] = useState<string>('');
  const [generation, setGeneration] = useState<React.ReactNode>();

  return (
    <div>
      <div>{generation}</div>
      <form
        onSubmit={async e => {
          e.preventDefault();
          const result = await generateResponse(input);
          setGeneration(result);
          setInput('');
        }}
      >
        <input
          type="text"
          value={input}
          onChange={event => {
            setInput(event.target.value);
          }}
        />
        <button>Send Message</button>
      </form>
    </div>
  );
}

----------------------------------------

TITLE: Installing Next.js Project with npx
DESCRIPTION: Command to create a new Next.js project using npx, specifically for the AI SDK example with OpenAI and Sentry telemetry.

LANGUAGE: bash
CODE:
npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-openai-telemetry-sentry next-openai-telemetry-sentry-app

----------------------------------------

TITLE: Importing createDataStreamResponse Function
DESCRIPTION: Shows how to import the createDataStreamResponse function from the ai package.

LANGUAGE: typescript
CODE:
import { createDataStreamResponse } from "ai"

----------------------------------------

TITLE: Implementing Chat API Route
DESCRIPTION: Server-side implementation of a chat API endpoint using AI SDK streaming capabilities with OpenAI integration in Next.js.

LANGUAGE: typescript
CODE:
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai('gpt-4o'),
    system: 'You are a helpful assistant.',
    messages,
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Generating Text with Google Generative AI
DESCRIPTION: Example of using the Google Generative AI provider to generate text using the Gemini model. Shows how to set up the model and provide a prompt for text generation.

LANGUAGE: typescript
CODE:
import { google } from '@ai-sdk/google';
import { generateText } from 'ai';

const { text } = await generateText({
  model: google('gemini-1.5-pro-latest'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Testing streamText with MockLanguageModelV1
DESCRIPTION: Shows how to use MockLanguageModelV1 and simulateReadableStream to test streaming text responses.

LANGUAGE: typescript
CODE:
import { streamText, simulateReadableStream } from 'ai';
import { MockLanguageModelV1 } from 'ai/test';

const result = streamText({
  model: new MockLanguageModelV1({
    doStream: async () => ({
      stream: simulateReadableStream({
        chunks: [
          { type: 'text-delta', textDelta: 'Hello' },
          { type: 'text-delta', textDelta: ', ' },
          { type: 'text-delta', textDelta: `world!` },
          {
            type: 'finish',
            finishReason: 'stop',
            logprobs: undefined,
            usage: { completionTokens: 10, promptTokens: 3 },
          },
        ],
      }),
      rawCall: { rawPrompt: null, rawSettings: {} },
    }),
  }),
  prompt: 'Hello, test!',
});

----------------------------------------

TITLE: Generating Text with Llama 3.1 using Amazon Bedrock
DESCRIPTION: This snippet shows how to switch from DeepInfra to Amazon Bedrock for generating text with Llama 3.1. It demonstrates the AI SDK's flexibility in changing providers with minimal code changes.

LANGUAGE: tsx
CODE:
import { generateText } from 'ai';
import { bedrock } from '@ai-sdk/amazon-bedrock';

const { text } = await generateText({
  model: bedrock('meta.llama3-1-405b-instruct-v1'),
  prompt: 'What is love?',
});

----------------------------------------

TITLE: Using DeepSeek R1 via Fireworks with Middleware
DESCRIPTION: This code snippet shows how to use DeepSeek R1 through the Fireworks provider. It includes the use of middleware to extract reasoning tokens from the model's output.

LANGUAGE: typescript
CODE:
import { fireworks } from '@ai-sdk/fireworks';
import {
  generateText,
  wrapLanguageModel,
  extractReasoningMiddleware,
} from 'ai';

// middleware to extract reasoning tokens
const enhancedModel = wrapLanguageModel({
  model: fireworks('accounts/fireworks/models/deepseek-r1'),
  middleware: extractReasoningMiddleware({ tagName: 'think' }),
});

const { reasoning, text } = await generateText({
  model: enhancedModel,
  prompt: 'Explain quantum entanglement.',
});

----------------------------------------

TITLE: Generating Text with Fireworks Model
DESCRIPTION: Use a Fireworks language model to generate text using the generateText function.

LANGUAGE: typescript
CODE:
import { fireworks } from '@ai-sdk/fireworks';
import { generateText } from 'ai';

const { text } = await generateText({
  model: fireworks('accounts/fireworks/models/firefunction-v1'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});

----------------------------------------

TITLE: Chat API Route Handler Setup
DESCRIPTION: Implementation of a Next.js API route handler for chat functionality using o3-mini.

LANGUAGE: tsx
CODE:
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';

// Allow responses up to 5 minutes
export const maxDuration = 300;

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai('o3-mini'),
    messages,
  });

  return result.toDataStreamResponse();
}

----------------------------------------

TITLE: Adjusting Reasoning Effort for OpenAI o1 Model
DESCRIPTION: Shows how to control the reasoning effort of the o1 model using the 'reasoningEffort' parameter. This allows developers to adjust the model's computation time based on the task complexity.

LANGUAGE: tsx
CODE:
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

// Reduce reasoning effort for faster responses
const { text } = await generateText({
  model: openai('o1'),
  prompt: 'Explain quantum entanglement briefly.',
  providerOptions: {
    openai: { reasoningEffort: 'low' },
  },
});

----------------------------------------

TITLE: Installing Project Dependencies
DESCRIPTION: Command to install required dependencies including AI SDK, OpenAI provider, and development tools.

LANGUAGE: bash
CODE:
pnpm add ai @ai-sdk/openai zod dotenv
pnpm add -D @types/node tsx typescript