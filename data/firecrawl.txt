TITLE: Basic usage of Firecrawl Python SDK
DESCRIPTION: Example demonstrating how to initialize the FirecrawlApp and use its scrape_url and crawl_url methods. Shows setting up the API key and basic parameters for scraping and crawling.

LANGUAGE: python
CODE:
from firecrawl.firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# Scrape a website:
scrape_status = app.scrape_url(
  'https://firecrawl.dev', 
  params={'formats': ['markdown', 'html']}
)
print(scrape_status)

# Crawl a website:
crawl_status = app.crawl_url(
  'https://firecrawl.dev', 
  params={
    'limit': 100, 
    'scrapeOptions': {'formats': ['markdown', 'html']}
  }, 
  poll_interval=30
)
print(crawl_status)

----------------------------------------

TITLE: Using Firecrawl Python SDK
DESCRIPTION: Example of using the Firecrawl Python SDK to scrape and crawl websites with schema validation using pydantic models.

LANGUAGE: python
CODE:
from firecrawl.firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# Scrape a website:
scrape_status = app.scrape_url(
  'https://firecrawl.dev', 
  params={'formats': ['markdown', 'html']}
)
print(scrape_status)

# Crawl a website:
crawl_status = app.crawl_url(
  'https://firecrawl.dev', 
  params={
    'limit': 100, 
    'scrapeOptions': {'formats': ['markdown', 'html']}
  },
  poll_interval=30
)
print(crawl_status)

----------------------------------------

TITLE: Scraping Single URLs with Firecrawl API
DESCRIPTION: Shows how to scrape a single URL and get its content in markdown and HTML formats using the Firecrawl API.

LANGUAGE: bash
CODE:
curl -X POST https://api.firecrawl.dev/v1/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "formats" : ["markdown", "html"]
    }'

----------------------------------------

TITLE: Scraping a single URL with Firecrawl SDK
DESCRIPTION: Example of using the scrape_url method to scrape data from a single URL. Returns scraped data as a dictionary.

LANGUAGE: python
CODE:
url = 'https://example.com'
scraped_data = app.scrape_url(url)

----------------------------------------

TITLE: Crawling a website with Firecrawl SDK
DESCRIPTION: Example of using the crawl_url method to crawl a website. Demonstrates setting crawl parameters and using an idempotency key.

LANGUAGE: python
CODE:
idempotency_key = str(uuid.uuid4()) # optional idempotency key
crawl_result = app.crawl_url('firecrawl.dev', {'excludePaths': ['blog/*']}, 2, idempotency_key)
print(crawl_result)

----------------------------------------

TITLE: Basic URL Scraping with Firecrawl in Python
DESCRIPTION: Demonstrates how to scrape a single URL using Firecrawl's scrape_url method and access the scraped content.

LANGUAGE: python
CODE:
url = "https://arxiv.org"
data = app.scrape_url(url)

data['metadata']

from IPython.display import Markdown
Markdown(data['markdown'][:500])

----------------------------------------

TITLE: Initializing and Using Firecrawl SDK in JavaScript
DESCRIPTION: Example of importing the FirecrawlApp class, initializing it with an API key, and using methods for scraping and crawling URLs.

LANGUAGE: javascript
CODE:
import FirecrawlApp, { CrawlParams, CrawlStatusResponse } from '@mendable/firecrawl-js';

const app = new FirecrawlApp({apiKey: "fc-YOUR_API_KEY"});

// Scrape a website
const scrapeResponse = await app.scrapeUrl('https://firecrawl.dev', {
  formats: ['markdown', 'html'],
});

if (scrapeResponse) {
  console.log(scrapeResponse)
}

// Crawl a website
const crawlResponse = await app.crawlUrl('https://firecrawl.dev', {
  limit: 100,
  scrapeOptions: {
    formats: ['markdown', 'html'],
  }
})

console.log(crawlResponse)

----------------------------------------

TITLE: Scraping a Single URL in JavaScript
DESCRIPTION: Example of using the scrapeUrl method to scrape data from a single URL.

LANGUAGE: javascript
CODE:
const url = "https://example.com";
const scrapedData = await app.scrapeUrl(url);

----------------------------------------

TITLE: Scraping Dynamic JavaScript Websites with Firecrawl
DESCRIPTION: Shows how to scrape dynamic JavaScript-based websites by defining actions to interact with the page before scraping.

LANGUAGE: python
CODE:
actions = [
    {"type": "wait", "milliseconds": 3000},
    {"type": "click", "selector": 'input[id="LocationSearch_input"]'},
    {"type": "write", "text": "London"},
    {"type": "screenshot"},
    {"type": "wait", "milliseconds": 1000},
    {"type": "click", "selector": "button[data-testid='ctaButton']"},
    {"type": "wait", "milliseconds": 3000},
]

class WeatherData(BaseModel):
    location: str = Field(description="The name of the city")
    temperature: str = Field(description="The current temperature in degrees Fahrenheit")
    temperature_high: str = Field(description="The high temperature for the day in degrees Fahrenheit")
    temperature_low: str = Field(description="The low temperature for the day in degrees Fahrenheit")
    humidity: str = Field(description="The current humidity as a percentage")
    pressure: str = Field(description="The current air pressure in inches of mercury")
    visibility: str = Field(description="The current visibility in miles")
    wind_speed: str = Field(description="The current wind speed in miles per hour")
    dew_point: str = Field(description="The current dew point in degrees Fahrenheit")
    uv_index: str = Field(description="The current UV index")
    moon_phase: str = Field(description="The current moon phase")

data = app.scrape_url(
    "https://weather.com",
    params={
        "formats": ["screenshot", "markdown", "extract"],
        "actions": actions,
        "extract": {
            "schema": WeatherData.model_json_schema(),
            "prompt": "Extract the following weather data from the weather.com page: temperature, temperature high, temperature low, humidity, pressure, visibility, wind speed, dew point, UV index, and moon phase",
        },
    },
)

----------------------------------------

TITLE: Crawling Website URLs with Firecrawl API
DESCRIPTION: Demonstrates how to crawl a website and get content from all accessible subpages using the Firecrawl API. Returns a job ID to check crawl status.

LANGUAGE: bash
CODE:
curl -X POST https://api.firecrawl.dev/v1/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer fc-YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "limit": 10,
      "scrapeOptions": {
        "formats": ["markdown", "html"]
      }
    }'

----------------------------------------

TITLE: Extracting structured data using LLM with Firecrawl SDK
DESCRIPTION: Demonstrates how to use LLM extraction to get structured data from a URL using Pydantic schemas. The example extracts top articles from Hacker News.

LANGUAGE: python
CODE:
class ArticleSchema(BaseModel):
    title: str
    points: int
    by: str
    commentsURL: str

class TopArticlesSchema(BaseModel):
    top: List[ArticleSchema] = Field(..., max_items=5, description="Top 5 stories")

data = app.scrape_url('https://news.ycombinator.com', {
    'extractorOptions': {
        'extractionSchema': TopArticlesSchema.model_json_schema(),
        'mode': 'llm-extraction'
    },
    'pageOptions':{
        'onlyMainContent': True
    }
})
print(data["llm_extraction"])

----------------------------------------

TITLE: Batch Scraping with Firecrawl in Python
DESCRIPTION: Demonstrates how to perform batch scraping of multiple URLs using Firecrawl's batch_scrape_urls method.

LANGUAGE: python
CODE:
articles = structured_data['extract']['news_articles']
article_links = [article['url'] for article in articles]

class ArticleSummary(BaseModel):
    title: str = Field(description="The title of the news article")
    summary: str = Field(description="A short summary of the news article")

batch_data = app.batch_scrape_urls(article_links, params={
    "formats": ["extract"],
    "extract": {
        "schema": ArticleSummary.model_json_schema(),
        "prompt": "Extract the title of the news article and generate its brief summary",
    }
})

----------------------------------------

TITLE: Configuring Environment Variables for Firecrawl
DESCRIPTION: This snippet shows the required and optional environment variables for setting up the Firecrawl project. It includes settings for workers, Redis, authentication, and various optional services.

LANGUAGE: env
CODE:
# ===== Required ENVS ======
NUM_WORKERS_PER_QUEUE=8
PORT=3002
HOST=0.0.0.0
REDIS_URL=redis://localhost:6379
REDIS_RATE_LIMIT_URL=redis://localhost:6379

## To turn on DB authentication, you need to set up supabase.
USE_DB_AUTHENTICATION=false

# ===== Optional ENVS ======

# Supabase Setup (used to support DB authentication, advanced logging, etc.)
SUPABASE_ANON_TOKEN=
SUPABASE_URL=
SUPABASE_SERVICE_TOKEN=

# Other Optionals
TEST_API_KEY= # use if you've set up authentication and want to test with a real API key
SCRAPING_BEE_API_KEY= #Set if you'd like to use scraping Bee to handle JS blocking
OPENAI_API_KEY= # add for LLM dependednt features (image alt generation, etc.)
BULL_AUTH_KEY= @
PLAYWRIGHT_MICROSERVICE_URL=  # set if you'd like to run a playwright fallback
LLAMAPARSE_API_KEY= #Set if you have a llamaparse key you'd like to use to parse pdfs
SLACK_WEBHOOK_URL= # set if you'd like to send slack server health status messages
POSTHOG_API_KEY= # set if you'd like to send posthog events like job logs
POSTHOG_HOST= # set if you'd like to send posthog events like job logs

----------------------------------------

TITLE: Configuring Environment Variables for Firecrawl
DESCRIPTION: This snippet shows the structure and content of the .env file required for setting up Firecrawl. It includes required and optional environment variables for various features and integrations.

LANGUAGE: env
CODE:
# ===== Required ENVS ======
PORT=3002
HOST=0.0.0.0

# To turn on DB authentication, you need to set up Supabase.
USE_DB_AUTHENTICATION=false

# ===== Optional ENVS ======

## === AI features (JSON format on scrape, /extract API) ===
# Provide your OpenAI API key here to enable AI features
# OPENAI_API_KEY=

# Experimental: Use Ollama
# OLLAMA_BASE_URL=http://localhost:11434/api
# MODEL_NAME=deepseek-r1:7b
# MODEL_EMBEDDING_NAME=nomic-embed-text

# Experimental: Use any OpenAI-compatible API
# OPENAI_BASE_URL=https://example.com/v1
# OPENAI_API_KEY=

## === Proxy ===
# PROXY_SERVER can be a full URL (e.g. http://0.1.2.3:1234) or just an IP and port combo (e.g. 0.1.2.3:1234)
# Do not uncomment PROXY_USERNAME and PROXY_PASSWORD if your proxy is unauthenticated
# PROXY_SERVER=
# PROXY_USERNAME=
# PROXY_PASSWORD=

## === /search API ===
# By default, the /search API will use Google search.

# You can specify a SearXNG server with the JSON format enabled, if you'd like to use that instead of direct Google.
# You can also customize the engines and categories parameters, but the defaults should also work just fine.
# SEARXNG_ENDPOINT=http://your.searxng.server
# SEARXNG_ENGINES=
# SEARXNG_CATEGORIES=

## === Other ===

# Supabase Setup (used to support DB authentication, advanced logging, etc.)
# SUPABASE_ANON_TOKEN=
# SUPABASE_URL=
# SUPABASE_SERVICE_TOKEN=

# Use if you've set up authentication and want to test with a real API key
# TEST_API_KEY=

# You can add this to enable ScrapingBee as a fallback scraping engine.
# SCRAPING_BEE_API_KEY=

# This key lets you access the queue admin panel. Change this if your deployment is publicly accessible.
BULL_AUTH_KEY=CHANGEME

# This is now autoconfigured by the docker-compose.yaml. You shouldn't need to set it.
# PLAYWRIGHT_MICROSERVICE_URL=http://playwright-service:3000/scrape
# REDIS_URL=redis://redis:6379
# REDIS_RATE_LIMIT_URL=redis://redis:6379

# Set if you have a llamaparse key you'd like to use to parse pdfs
# LLAMAPARSE_API_KEY=

# Set if you'd like to send server health status messages to Slack
# SLACK_WEBHOOK_URL=

# Set if you'd like to send posthog events like job logs
# POSTHOG_API_KEY=
# POSTHOG_HOST=

----------------------------------------

TITLE: Crawling a website with WebSockets using Firecrawl SDK
DESCRIPTION: Demonstrates how to use WebSockets for real-time updates during website crawling. Includes setting up event handlers and connecting to the WebSocket.

LANGUAGE: python
CODE:
# inside an async function...
nest_asyncio.apply()

# Define event handlers
def on_document(detail):
    print("DOC", detail)

def on_error(detail):
    print("ERR", detail['error'])

def on_done(detail):
    print("DONE", detail['status'])

    # Function to start the crawl and watch process
async def start_crawl_and_watch():
    # Initiate the crawl job and get the watcher
    watcher = app.crawl_url_and_watch('firecrawl.dev', { 'excludePaths': ['blog/*'], 'limit': 5 })

    # Add event listeners
    watcher.add_event_listener("document", on_document)
    watcher.add_event_listener("error", on_error)
    watcher.add_event_listener("done", on_done)

    # Start the watcher
    await watcher.connect()

# Run the event loop
await start_crawl_and_watch()

----------------------------------------

TITLE: Asynchronous website crawling with Firecrawl SDK
DESCRIPTION: Shows how to use the async_crawl_url method for asynchronous website crawling. Allows specifying crawl parameters.

LANGUAGE: python
CODE:
crawl_result = app.async_crawl_url('firecrawl.dev', {'excludePaths': ['blog/*']}, "")
print(crawl_result)

----------------------------------------

TITLE: Crawling a Website in JavaScript
DESCRIPTION: Example of using the crawlUrl method to crawl a website with specific options.

LANGUAGE: javascript
CODE:
const crawlResponse = await app.crawlUrl('https://firecrawl.dev', {
  limit: 100,
  scrapeOptions: {
    formats: ['markdown', 'html'],
  }
})

----------------------------------------

TITLE: Setting up Vectorstore with FAISS
DESCRIPTION: Configure text splitting and vectorstore setup using Ollama embeddings and FAISS. Splits documents into 1000-character chunks with 200-character overlap.

LANGUAGE: python
CODE:
from langchain_community.embeddings import OllamaEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)
vectorstore = FAISS.from_documents(documents=splits, embedding=OllamaEmbeddings())

----------------------------------------

TITLE: Extracting Structured Data with LLM in JavaScript
DESCRIPTION: Example of using LLM extraction to extract structured data from a URL using a Zod schema.

LANGUAGE: javascript
CODE:
import FirecrawlApp from "@mendable/firecrawl-js";
import { z } from "zod";

const app = new FirecrawlApp({
  apiKey: "fc-YOUR_API_KEY",
});

// Define schema to extract contents into
const schema = z.object({
  top: z
    .array(
      z.object({
        title: z.string(),
        points: z.number(),
        by: z.string(),
        commentsURL: z.string(),
      })
    )
    .length(5)
    .describe("Top 5 stories on Hacker News"),
});

const scrapeResult = await app.scrapeUrl("https://firecrawl.dev", {
  extractorOptions: { extractionSchema: schema },
});

console.log(scrapeResult.data["llm_extraction"]);

----------------------------------------

TITLE: Batch scraping multiple URLs with Firecrawl SDK
DESCRIPTION: Shows how to use the batch_scrape_urls method to scrape multiple URLs in a single request. Includes setting scrape parameters and using an idempotency key.

LANGUAGE: python
CODE:
idempotency_key = str(uuid.uuid4()) # optional idempotency key
batch_scrape_result = app.batch_scrape_urls(['firecrawl.dev', 'mendable.ai'], {'formats': ['markdown', 'html']}, 2, idempotency_key)
print(batch_scrape_result)

----------------------------------------

TITLE: Generating Internal Link Suggestions Using AI in Python
DESCRIPTION: This code snippet uses the Anthropic API to analyze blog content and suggest internal linking opportunities. It processes each crawled page, generates link suggestions, and writes the results to a CSV file.

LANGUAGE: python
CODE:
import json
import csv

# Convert potential_links to a JSON string
potential_links_json = json.dumps(potential_links, indent=2)

# Prepare CSV file
csv_filename = "link_suggestions.csv"
csv_headers = ["Source Blog Title", "Source Blog URL", "Target Phrase", "Suggested Link URL"]

# Write headers to the CSV file
with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
    csvwriter = csv.writer(csvfile)
    csvwriter.writerow(csv_headers)

# Loop through each blog post content
for item in crawl_result:
    current_blog_url = item["metadata"].get("ogUrl", "")
    if current_blog_url == blog_url:
        continue
    current_blog_content = item["content"]
    current_blog_title = item["metadata"].get("title", "")

    prompt_instructions = f"""Given this blog post from {current_blog_url} called '{current_blog_title}', analyze the following blog content. Identify 0 to 3 of phrases (5 words max) from the <blog_content> inside of the middle of the article that could be linked to other blog posts from the list of potential links provided inside of <potential_links>. Return a JSON object structured as follows:

    {{
      "link_suggestions": [
        {{
          "target_phrase": "the EXACT phrase from the <blog_content> to be linked to one of the links in <potential_links> (5 words max)",
          "suggested_link_url": "url of the suggested internal link from <potential_links>",
        }}
      ],
      "metadata": {{
        "source_blog_url": "{current_blog_url}",
        "source_blog_title": "{current_blog_title}",
      }}
    }}

    Ensure that you provide the EXACT phrase from <blog_content> in target_phrase (5 words max) to locate each suggestion in the blog content without using character positions. Your target phrases must NOT be a title!

    Blog Content:
    <blog_content>
    {current_blog_content}
    </blog_content>

    Potential Links:
    <potential_links>
    {potential_links_json}
    </potential_links>

    GO AND ONLY RETURN THE JSON NOTHING ELSE:"""

    try:
        message = client.messages.create(
            model="claude-3-5-sonnet-20240620",
            max_tokens=1024,
            messages=[
                {"role": "user", "content": prompt_instructions}
            ]
        )
        
        # Extract the JSON string from the TextBlock
        json_string = message.content[0].text
        
        # Parse the JSON response
        response_json = json.loads(json_string)
        
        # Write suggestions to CSV
        for suggestion in response_json['link_suggestions']:
            print("Suggestion for: " + current_blog_title )
            print("Blog phrase: " + suggestion['target_phrase']) 
            print("Internal Link: " + suggestion['suggested_link_url'])
            print("---\n\n")

            # Open the CSV file in append mode and write the new row
            with open(csv_filename, 'a', newline='', encoding='utf-8') as csvfile:
                csvwriter = csv.writer(csvfile)
                csvwriter.writerow([
                    response_json['metadata']['source_blog_title'],
                    response_json['metadata']['source_blog_url'],
                    suggestion['target_phrase'],
                    suggestion['suggested_link_url'],
                ])
      
    except json.JSONDecodeError:
        print(f"Error parsing JSON response for blog {current_blog_title}")
        print("Raw response:", message.content)
    except Exception as e:
        print(f"Error processing blog {current_blog_title}: {str(e)}")
    

print(f"Finished processing all blog posts. Results saved to {csv_filename}")

----------------------------------------

TITLE: Initializing Firecrawl Scraper for ProductHunt
DESCRIPTION: Sets up a basic web scraper using Firecrawl to extract top products from ProductHunt. Defines Pydantic models for data validation and implements the main scraping function.

LANGUAGE: python
CODE:
import json

from firecrawl import FirecrawlApp
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from datetime import datetime

load_dotenv()

class Product(BaseModel):
    name: str = Field(description="The name of the product")
    description: str = Field(description="A short description of the product")
    url: str = Field(description="The URL of the product")
    
    topics: list[str] = Field(
        description="A list of topics the product belongs to. Can be found below the product description."
    )
    
    n_upvotes: int = Field(description="The number of upvotes the product has")
    n_comments: int = Field(description="The number of comments the product has")
    
    rank: int = Field(
        description="The rank of the product on Product Hunt's Yesterday's Top Products section."
    )
    logo_url: str = Field(description="The URL of the product's logo.")

class YesterdayTopProducts(BaseModel):
    products: list[Product] = Field(
        description="A list of top products from yesterday on Product Hunt."
    )

BASE_URL = "https://www.producthunt.com"


def get_yesterday_top_products():
    app = FirecrawlApp()

    data = app.scrape_url(
        BASE_URL,
        params={
            "formats": ["extract"],
            "extract": {
                "schema": YesterdayTopProducts.model_json_schema(),
                
                "prompt": "Extract the top products listed under the 'Yesterday's Top Products' section. There will be exactly 5 products.",
            },
        },
    )

    return data["extract"]["products"]

def save_yesterday_top_products():
    products = get_yesterday_top_products()
    
    date_str = datetime.now().strftime("%Y_%m_%d")
    filename = f"ph_top_products_{date_str}.json"
    
    with open(filename, "w") as f:
        json.dump(products, f)
        
if __name__ == "__main__":
    save_yesterday_top_products()

----------------------------------------

TITLE: Basic Firecrawl Web Scraper Implementation in Python
DESCRIPTION: Python script implementing a web scraper for Hacker News using Firecrawl with Pydantic models for data validation and extraction.

LANGUAGE: python
CODE:
# firecrawl_scraper.py
import json
from firecrawl import FirecrawlApp
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from typing import List
from datetime import datetime

load_dotenv()
BASE_URL = "https://news.ycombinator.com/"

----------------------------------------

TITLE: Asynchronous batch scraping with Firecrawl SDK
DESCRIPTION: Demonstrates using the async_batch_scrape_urls method for asynchronous batch scraping of multiple URLs.

LANGUAGE: python
CODE:
batch_scrape_result = app.async_batch_scrape_urls(['firecrawl.dev', 'mendable.ai'], {'formats': ['markdown', 'html']})
print(batch_scrape_result)

----------------------------------------

TITLE: Crawling with WebSockets in JavaScript
DESCRIPTION: Example of using the crawlUrlAndWatch method to crawl a website with WebSocket updates.

LANGUAGE: javascript
CODE:
const watch = await app.crawlUrlAndWatch('mendable.ai', { excludePaths: ['blog/*'], limit: 5});

watch.addEventListener("document", doc => {
 console.log("DOC", doc.detail);
});

watch.addEventListener("error", err => {
 console.error("ERR", err.detail.error);
});

watch.addEventListener("done", state => {
 console.log("DONE", state.detail.status);
});

----------------------------------------

TITLE: Batch Scraping Multiple URLs in JavaScript
DESCRIPTION: Example of using the batchScrapeUrls method to scrape multiple URLs in a batch.

LANGUAGE: javascript
CODE:
const batchScrapeResponse = await app.batchScrapeUrls(['https://firecrawl.dev', 'https://mendable.ai'], {
  formats: ['markdown', 'html'],
})

----------------------------------------

TITLE: Querying Google Generative AI Model with Cached Content in Python
DESCRIPTION: This snippet demonstrates how to query the Google Generative AI model using the cached content created from the crawled website data. It processes the response and prints the generated text.

LANGUAGE: python
CODE:
# Query the model
response = model.generate_content(["What powers website scraping with Dify?"])
response_dict = response.to_dict()
response_text = response_dict['candidates'][0]['content']['parts'][0]['text']
print(response_text)

----------------------------------------

TITLE: Mapping a Website in JavaScript
DESCRIPTION: Example of using the mapUrl method to generate a list of URLs from a website.

LANGUAGE: javascript
CODE:
const mapResult = await app.mapUrl('https://example.com') as MapResponse;
console.log(mapResult)

----------------------------------------

TITLE: Starting Redis Server for Firecrawl
DESCRIPTION: This command starts the Redis server, which is a required component for running the Firecrawl project.

LANGUAGE: bash
CODE:
redis-server

----------------------------------------

TITLE: Initializing Firecrawl App in Python
DESCRIPTION: Sets up the Firecrawl application by loading environment variables and creating an instance of FirecrawlApp.

LANGUAGE: python
CODE:
from firecrawl import FirecrawlApp
from dotenv import load_dotenv

load_dotenv()

app = FirecrawlApp()

----------------------------------------

TITLE: Starting Main Server for Firecrawl
DESCRIPTION: This command starts the main server for the Firecrawl project. It should be run in the apps/api directory after installing dependencies.

LANGUAGE: bash
CODE:
pnpm run start

----------------------------------------

TITLE: Starting Worker Processes for Firecrawl
DESCRIPTION: This command starts the worker processes responsible for processing crawl jobs in Firecrawl. It should be run in the apps/api directory.

LANGUAGE: bash
CODE:
pnpm run workers
# if you are going to use the [llm-extract feature](https://github.com/mendableai/firecrawl/pull/586/), you should also export OPENAI_API_KEY=sk-______

----------------------------------------

TITLE: Scraping a URL with Firecrawl Rust SDK
DESCRIPTION: Use the scrape_url method to fetch and parse content from a single URL. This snippet shows how to perform a basic scrape operation and handle the result.

LANGUAGE: rust
CODE:
let scrape_result = app.scrape_url("https://firecrawl.dev", None).await;
match scrape_result {
    Ok(data) => println!("Scrape result:\n{}", data.markdown),
    Err(e) => eprintln!("Scrape failed: {}", e),
}

----------------------------------------

TITLE: Batch scraping with WebSockets using Firecrawl SDK
DESCRIPTION: Demonstrates how to use WebSockets for real-time updates during batch scraping. Includes setting up event handlers and connecting to the WebSocket.

LANGUAGE: python
CODE:
# inside an async function...
nest_asyncio.apply()

# Define event handlers
def on_document(detail):
    print("DOC", detail)

def on_error(detail):
    print("ERR", detail['error'])

def on_done(detail):
    print("DONE", detail['status'])

# Function to start the crawl and watch process
async def start_crawl_and_watch():
    # Initiate the crawl job and get the watcher
    watcher = app.batch_scrape_urls_and_watch(['firecrawl.dev', 'mendable.ai'], {'formats': ['markdown', 'html']})

    # Add event listeners
    watcher.add_event_listener("document", on_document)
    watcher.add_event_listener("error", on_error)
    watcher.add_event_listener("done", on_done)

    # Start the watcher
    await watcher.connect()

# Run the event loop
await start_crawl_and_watch()

----------------------------------------

TITLE: Implementing Error Handling and Monitoring for Web Scraper
DESCRIPTION: Adds robust error handling with retries, comprehensive logging, and monitoring alerts to the web scraper for improved reliability.

LANGUAGE: python
CODE:
from tenacity import retry, stop_after_attempt, wait_exponential
import logging
from datetime import datetime
import requests
import os

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    reraise=True
)
def get_yesterday_top_products():
    try:
        app = FirecrawlApp()
        data = app.scrape_url(
            BASE_URL,
            params={
                "formats": ["extract"],
                "extract": {
                    "schema": YesterdayTopProducts.model_json_schema(),
                    "prompt": "Extract the top products listed under the 'Yesterday's Top Products' section."
                },
            },
        )
        return data["extract"]["products"]
    except Exception as e:
        logger.error(f"Scraping failed: {str(e)}")
        raise

def setup_logging():
    """Configure logging with both file and console handlers."""
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)

    # Create formatters
    detailed_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    simple_formatter = logging.Formatter('%(levelname)s: %(message)s')

    # File handler
    file_handler = logging.FileHandler(
        f'logs/scraper_{datetime.now().strftime("%Y%m%d")}.log'
    )
    file_handler.setFormatter(detailed_formatter)
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(simple_formatter)

    # Add handlers
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    return logger

logger = setup_logging()

def send_alert(message, webhook_url):
    """Send alerts to Slack/Discord/etc."""
    payload = {"text": message}
    try:
        requests.post(webhook_url, json=payload)
    except Exception as e:
        logger.error(f"Failed to send alert: {str(e)}")

def monitor_scraping_health(products):
    """Monitor scraping health and send alerts if needed."""
    if not products:
        send_alert(
            "⚠️ Warning: No products scraped from ProductHunt",
            os.getenv("WEBHOOK_URL")
        )
        return False
        
    if len(products) < 5:
        send_alert(
            f"⚠️ Warning: Only {len(products)} products scraped (expected 5)",
            os.getenv("WEBHOOK_URL")
        )
        return False
        
    return True

----------------------------------------

TITLE: Scraping Web Content with Firecrawl
DESCRIPTION: Performs web scraping using Firecrawl with specific page options to extract main content only.

LANGUAGE: python
CODE:
# Scrape the URL using Firecrawl
page_content = firecrawl_app.scrape_url(url, params={"pageOptions": {"onlyMainContent": True}})

print(f"Page content scraped. Length: {len(page_content['content'])} characters")

----------------------------------------

TITLE: Initiating Asynchronous Crawl in JavaScript
DESCRIPTION: Example of using the asyncCrawlUrl method to start an asynchronous crawl of a website.

LANGUAGE: javascript
CODE:
const asyncCrawlResult = await app.asyncCrawlUrl('mendable.ai', { excludePaths: ['blog/*'], limit: 5});

----------------------------------------

TITLE: Checking Crawl Status in JavaScript
DESCRIPTION: Example of using the checkCrawlStatus method to check the status of a crawl job.

LANGUAGE: javascript
CODE:
const status = await app.checkCrawlStatus(id);

----------------------------------------

TITLE: Mapping a website with Firecrawl SDK
DESCRIPTION: Example of using the map_url method to generate a list of URLs from a website.

LANGUAGE: python
CODE:
# Map a website:
map_result = app.map_url('https://example.com')
print(map_result)

----------------------------------------

TITLE: Installing Dependencies for Firecrawl
DESCRIPTION: This command installs the project dependencies using pnpm. It should be run in the apps/api directory of the project.

LANGUAGE: bash
CODE:
# cd apps/api # to make sure you're in the right folder
pnpm install # make sure you have pnpm version 9+!

----------------------------------------

TITLE: Testing Firecrawl Server with cURL
DESCRIPTION: These cURL commands test the Firecrawl server by sending a GET request to the test endpoint and a POST request to the crawl endpoint.

LANGUAGE: bash
CODE:
curl -X GET http://localhost:3002/test

LANGUAGE: bash
CODE:
curl -X POST http://localhost:3002/v1/crawl \
    -H 'Content-Type: application/json' \
    -d '{
      "url": "https://mendable.ai"
    }'

----------------------------------------

TITLE: Extracting Structured Data with Firecrawl Rust SDK
DESCRIPTION: Use the Extract feature to scrape structured data based on a JSON schema. This snippet demonstrates defining a schema and extracting data from Hacker News.

LANGUAGE: rust
CODE:
let json_schema = json!({
    "type": "object",
    "properties": {
        "top": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "title": {"type": "string"},
                    "points": {"type": "number"},
                    "by": {"type": "string"},
                    "commentsURL": {"type": "string"}
                },
                "required": ["title", "points", "by", "commentsURL"]
            },
            "minItems": 5,
            "maxItems": 5,
            "description": "Top 5 stories on Hacker News"
        }
    },
    "required": ["top"]
});

let llm_extraction_options = ScrapeOptions {
    formats: vec![ ScrapeFormats::Extract ].into(),
    extract: ExtractOptions {
        schema: json_schema.into(),
        ..Default::default()
    }.into(),
    ..Default::default()
};

let llm_extraction_result = app
    .scrape_url("https://news.ycombinator.com", llm_extraction_options)
    .await;

match llm_extraction_result {
    Ok(data) => println!("LLM Extraction Result:\n{:#?}", data.extract.unwrap()),
    Err(e) => eprintln!("LLM Extraction failed: {}", e),
}

----------------------------------------

TITLE: Scraping Website Content using Firecrawl in Python
DESCRIPTION: This code demonstrates how to use the FirecrawlApp to scrape a specific URL, extracting only the main content while ignoring navigation and footer elements.

LANGUAGE: python
CODE:
from firecrawl import FirecrawlApp  # Importing the FireCrawlLoader

url = "https://about.fb.com/news/2024/04/introducing-our-open-mixed-reality-ecosystem/"

firecrawl = FirecrawlApp(
    api_key="fc-YOUR_FIRECRAWL_API_KEY",
)
page_content = firecrawl.scrape_url(url=url,  # Target URL to crawl
    params={
        "pageOptions":{
            "onlyMainContent": True # Ignore navs, footers, etc.
        }
    })
print(page_content)

----------------------------------------

TITLE: Extracting Structured Data from Website Content using Groq LLM in Python
DESCRIPTION: This snippet illustrates how to use Groq's Llama 3 model to extract specific fields from the scraped website content. It sets up the Groq client, defines extraction fields, and generates a JSON response with the extracted data.

LANGUAGE: python
CODE:
import json
from groq import Groq

client = Groq(
    api_key="gsk_YOUR_GROQ_API_KEY",  # Note: Replace 'API_KEY' with your actual Groq API key
)

# Here we define the fields we want to extract from the page content
extract = ["summary","date","companies_building_with_quest","title_of_the_article","people_testimonials"]

completion = client.chat.completions.create(
    model="llama3-8b-8192",
    messages=[
        {
            "role": "system",
            "content": "You are a legal advisor who extracts information from documents in JSON."
        },
        {
            "role": "user",
            # Here we pass the page content and the fields we want to extract
            "content": f"Extract the following information from the provided documentation:\Page content:\n\n{page_content}\n\nInformation to extract: {extract}"
        }
    ],
    temperature=0,
    max_tokens=1024,
    top_p=1,
    stream=False,
    stop=None,
    # We set the response format to JSON object
    response_format={"type": "json_object"}
)


# Pretty print the JSON response
dataExtracted = json.dumps(str(completion.choices[0].message.content), indent=4)

print(dataExtracted)

----------------------------------------

TITLE: Initializing FirecrawlApp in Rust
DESCRIPTION: Initialize the FirecrawlApp with an API key. This snippet demonstrates the basic setup required to start using the Firecrawl SDK in a Rust application.

LANGUAGE: rust
CODE:
use firecrawl::FirecrawlApp;

#[tokio::main]
async fn main() {
    // Initialize the FirecrawlApp with the API key
    let app = FirecrawlApp::new("fc-YOUR-API-KEY").expect("Failed to initialize FirecrawlApp");

    // ...
}

----------------------------------------

TITLE: Analyzing Website Pages for Contradictions with Claude Opus
DESCRIPTION: This snippet shows how to combine scraped website pages, create prompts for each combination, and use Claude Opus to analyze them for contradictions or inconsistencies.

LANGUAGE: python
CODE:
from itertools import combinations

page_combinations = []

for first_page, second_page in combinations(crawl_result, 2):
    combined_string = "First Page:\n" + first_page['markdown'] + "\n\nSecond Page:\n" + second_page['markdown']
    page_combinations.append(combined_string)

import anthropic

client = anthropic.Anthropic(
    # defaults to os.environ.get("ANTHROPIC_API_KEY")
    api_key="YOUR-KEY",
)

final_output = []

for page_combination in page_combinations:

    prompt = "Here are two pages from a companies website, your job is to find any contradictions or differences in opinion between the two pages, this could be caused by outdated information or other. If you find any contradictions, list them out and provide a brief explanation of why they are contradictory or differing. Make sure the explanation is specific and concise. It is okay if you don't find any contradictions, just say 'No contradictions found' and nothing else. Here are the pages: " + "\n\n".join(page_combination)

    message = client.messages.create(
        model="claude-3-opus-20240229",
        max_tokens=1000,
        temperature=0.0,
        system="You are an assistant that helps find contradictions or differences in opinion between pages in a company website and knowledge base. This could be caused by outdated information in the knowledge base.",
        messages=[
            {"role": "user", "content": prompt}
        ]
    )
    final_output.append(message.content)

----------------------------------------

TITLE: Asynchronous Batch Scraping in JavaScript
DESCRIPTION: Example of using the asyncBatchScrapeUrls method to initiate an asynchronous batch scrape of multiple URLs.

LANGUAGE: javascript
CODE:
const asyncBatchScrapeResult = await app.asyncBatchScrapeUrls(['https://firecrawl.dev', 'https://mendable.ai'], { formats: ['markdown', 'html'] });

----------------------------------------

TITLE: Initializing Firecrawl and Google Generative AI in Python
DESCRIPTION: This snippet sets up the environment by loading API keys, configuring Google Generative AI, and initializing the FirecrawlApp. It requires the firecrawl, google.generativeai, and python-dotenv libraries.

LANGUAGE: python
CODE:
import os
import datetime
import time
import google.generativeai as genai
from google.generativeai import caching
from dotenv import load_dotenv
from firecrawl import FirecrawlApp
import json

# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
google_api_key = os.getenv("GOOGLE_API_KEY")
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")

# Configure the Google Generative AI module with the API key
genai.configure(api_key=google_api_key)

# Initialize the FirecrawlApp with your API key
app = FirecrawlApp(api_key=firecrawl_api_key)

----------------------------------------

TITLE: Batch Scraping with WebSockets in JavaScript
DESCRIPTION: Example of using the batchScrapeUrlsAndWatch method to perform batch scraping with WebSocket updates.

LANGUAGE: javascript
CODE:
const watch = await app.batchScrapeUrlsAndWatch(['https://firecrawl.dev', 'https://mendable.ai'], { formats: ['markdown', 'html'] });

watch.addEventListener("document", doc => {
 console.log("DOC", doc.detail);
});

watch.addEventListener("error", err => {
 console.error("ERR", err.detail.error);
});

watch.addEventListener("done", state => {
 console.log("DONE", state.detail.status);
});

----------------------------------------

TITLE: Crawling a Website with Firecrawl Rust SDK
DESCRIPTION: Use the crawl_url method to crawl an entire website. This snippet shows how to set crawl options and handle the crawl result.

LANGUAGE: rust
CODE:
let crawl_options = CrawlOptions {
    exclude_paths: vec![ "blog/*".into() ].into(),
    ..Default::default()
};

let crawl_result = app
    .crawl_url("https://mendable.ai", crawl_options)
    .await;

match crawl_result {
    Ok(data) => println!("Crawl Result (used {} credits):\n{:#?}", data.credits_used, data.data),
    Err(e) => eprintln!("Crawl failed: {}", e),
}

----------------------------------------

TITLE: Initializing Libraries and API Clients for Web Crawling in Python
DESCRIPTION: This code snippet sets up the necessary libraries and API clients for web crawling and AI analysis. It imports required modules, loads environment variables, and initializes the Firecrawl and Anthropic API clients.

LANGUAGE: python
CODE:
import os
import datetime
import time
from firecrawl import FirecrawlApp
import json
import anthropic
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
anthropic_api_key = os.getenv("ANTHROPIC_API_KEY") or ""
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY") or ""
# Set variables
blog_url="https://mendable.ai/blog"

# Set up anthropic client
client = anthropic.Anthropic(
    api_key=anthropic_api_key,
)

# Initialize the FirecrawlApp with your API key
app = FirecrawlApp(api_key=firecrawl_api_key)

----------------------------------------

TITLE: Asynchronous Crawling with Firecrawl Rust SDK
DESCRIPTION: Use crawl_url_async for non-blocking crawls and check_crawl_status to monitor progress. This snippet demonstrates starting an async crawl and checking its status.

LANGUAGE: rust
CODE:
let crawl_id = app.crawl_url_async("https://mendable.ai", None).await?.id;

// ... later ...

let status = app.check_crawl_status(crawl_id).await?;

if status.status == CrawlStatusTypes::Completed {
    println!("Crawl is done: {:#?}", status.data);
} else {
    // ... wait some more ...
}

----------------------------------------

TITLE: Building and Running Docker Containers for Firecrawl
DESCRIPTION: This snippet demonstrates the commands to build and run Docker containers for Firecrawl using docker-compose.

LANGUAGE: bash
CODE:
docker compose build
docker compose up

----------------------------------------

TITLE: Checking crawl status with Firecrawl SDK
DESCRIPTION: Demonstrates how to check the status of a crawl job using the check_crawl_status method.

LANGUAGE: python
CODE:
id = crawl_result['id']
status = app.check_crawl_status(id)

----------------------------------------

TITLE: Testing Firecrawl API with cURL
DESCRIPTION: This snippet shows how to test the Firecrawl crawl endpoint using a cURL command, providing a sample API request to crawl a specific URL.

LANGUAGE: bash
CODE:
curl -X POST http://localhost:3002/v1/crawl \
    -H 'Content-Type: application/json' \
    -d '{
      "url": "https://firecrawl.dev"
    }'

----------------------------------------

TITLE: Configuring a Comprehensive GitHub Actions Workflow
DESCRIPTION: Shows a more complex workflow configuration with multiple triggers including scheduled runs, manual triggers, and specific branch pushes.

LANGUAGE: yaml
CODE:
name: Comprehensive Workflow
on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 0 * * *'  # Daily at midnight
  workflow_dispatch:  # Manual trigger

jobs:
  process:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run daily tasks
        run: python daily_tasks.py
        env:
          API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}

----------------------------------------

TITLE: Checking batch scrape status with Firecrawl SDK
DESCRIPTION: Shows how to check the status of an asynchronous batch scrape job using the check_batch_scrape_status method.

LANGUAGE: python
CODE:
id = batch_scrape_result['id']
status = app.check_batch_scrape_status(id)

----------------------------------------

TITLE: Automating Python Package Publishing with GitHub Actions
DESCRIPTION: Shows how to create a workflow that automatically builds and publishes a Python package to PyPI when a new release is created.

LANGUAGE: yaml
CODE:
name: Publish Python Package
on:
  release:
    types: [created]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install build twine
          
      - name: Build package
        run: python -m build
        
      - name: Publish to PyPI
        env:
          TWINE_USERNAME: __token__
          TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}
        run: |
          python -m twine upload dist/*

----------------------------------------

TITLE: Configuring Redis URL in YAML ConfigMap
DESCRIPTION: Example of how to configure Redis URL with password in the ConfigMap YAML file. This is required if REDIS_PASSWORD is set in the secret.

LANGUAGE: yaml
CODE:
REDIS_URL: "redis://:password@host:port"
REDIS_RATE_LIMIT_URL: "redis://:password@host:port"

----------------------------------------

TITLE: Product Scraping Model
DESCRIPTION: Pydantic model defining the structure for scraped product data including URL, name, price, currency and image

LANGUAGE: python
CODE:
class Product(BaseModel):
    """Schema for creating a new product"""
    url: str = Field(description="The URL of the product")
    name: str = Field(description="The product name/title")
    price: float = Field(description="The current price of the product")
    currency: str = Field(description="Currency code (USD, EUR, etc)")
    main_image_url: str = Field(description="The URL of the main image of the product")

----------------------------------------

TITLE: Applying Kubernetes Resources for Firecrawl Installation
DESCRIPTION: Kubectl commands to apply the necessary Kubernetes resources for installing Firecrawl, including ConfigMap, Secret, services, and deployments.

LANGUAGE: bash
CODE:
kubectl apply -f configmap.yaml
kubectl apply -f secret.yaml
kubectl apply -f playwright-service.yaml
kubectl apply -f api.yaml
kubectl apply -f worker.yaml
kubectl apply -f redis.yaml

----------------------------------------

TITLE: Uninstalling Firecrawl from Kubernetes Cluster
DESCRIPTION: Kubectl commands to delete all Firecrawl-related resources from the Kubernetes cluster.

LANGUAGE: bash
CODE:
kubectl delete -f configmap.yaml
kubectl delete -f secret.yaml
kubectl delete -f playwright-service.yaml
kubectl delete -f api.yaml
kubectl delete -f worker.yaml
kubectl delete -f redis.yaml

----------------------------------------

TITLE: URL Validation Utility
DESCRIPTION: Function to validate URLs using regex and urllib.parse, checking for proper format and protocols

LANGUAGE: python
CODE:
from urllib.parse import urlparse
import re

def is_valid_url(url: str) -> bool:
    try:
        result = urlparse(url)
        if not all([result.scheme, result.netloc]):
            return False
        if result.scheme not in ["http", "https"]:
            return False
        domain_pattern = r"^[a-zA-Z0-9]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(\.[a-zA-Z]{2,})+$"
        if not re.match(domain_pattern, result.netloc):
            return False
        return True
    except Exception:
        return False

----------------------------------------

TITLE: Creating a Basic GitHub Actions Workflow for Python
DESCRIPTION: Demonstrates how to create a simple GitHub Actions workflow file that sets up a Python environment and runs tests on push or pull request events.

LANGUAGE: yaml
CODE:
name: Python Tests
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

----------------------------------------

TITLE: Implementing Environment Variable Validation in Python
DESCRIPTION: Demonstrates how to use Pydantic for validating and managing environment variables in a Python application.

LANGUAGE: python
CODE:
from pydantic import BaseSettings, SecretStr

class Settings(BaseSettings):
    api_key: SecretStr
    database_url: str
    debug_mode: bool = False

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"

# Initialize settings
settings = Settings()

----------------------------------------

TITLE: Performing Similarity Search
DESCRIPTION: Execute similarity search on the vectorstore to retrieve relevant documents based on user questions.

LANGUAGE: python
CODE:
question = "What is firecrawl?"
docs = vectorstore.similarity_search(query=question)

----------------------------------------

TITLE: Making Scraping API Request
DESCRIPTION: Example cURL command demonstrating how to make a POST request to the scraping endpoint with configuration options including custom headers, timeout, and selector checking

LANGUAGE: bash
CODE:
curl -X POST http://localhost:3000/scrape \
-H "Content-Type: application/json" \
-d '{
  "url": "https://example.com",
  "wait_after_load": 1000,
  "timeout": 15000,
  "headers": {
    "Custom-Header": "value"
  },
  "check_selector": "#content"
}'

----------------------------------------

TITLE: Installing Playwright Dependencies
DESCRIPTION: Commands to install npm dependencies and Playwright browser binaries

LANGUAGE: bash
CODE:
npm install
npx playwright install

----------------------------------------

TITLE: Mapping URLs with Firecrawl Rust SDK
DESCRIPTION: Use the map_url method to discover and list all associated links from a starting URL. This snippet shows how to perform a URL mapping operation.

LANGUAGE: rust
CODE:
let map_result = app
    .map_url("https://firecrawl.dev", None)
    .await;

match map_result {
    Ok(data) => println!("Mapped URLs: {:#?}", data),
    Err(e) => eprintln!("Map failed: {}", e),
}

----------------------------------------

TITLE: Running the Scraping Service
DESCRIPTION: Commands to build and start the service in either production or development mode

LANGUAGE: bash
CODE:
npm run build
npm start

LANGUAGE: bash
CODE:
npm run dev

----------------------------------------

TITLE: Querying Cached Generative Model in Python
DESCRIPTION: This code snippet demonstrates how to query the cached generative model created from the crawled website data. It generates a response to a specific question and prints the result.

LANGUAGE: python
CODE:
# Query the model
response = model.generate_content(["What powers website scraping with Dify?"])
response_dict = response.to_dict()
response_text = response_dict['candidates'][0]['content']['parts'][0]['text']
print(response_text)

----------------------------------------

TITLE: Preparing Claude AI Prompt
DESCRIPTION: Creates a structured prompt for Claude AI to extract specific information from the scraped content.

LANGUAGE: python
CODE:
# Prepare the prompt for Claude
prompt = f"""Analyze the following webpage content and extract the following information:
1. The title of the page
2. Whether the company is part of Y Combinator (YC)
3. Whether the company/product is open source

Return the information in JSON format with the following schema:
{{
    \"main_header_title\": string,
    \"is_yc_company\": boolean,
    \"is_open_source\": boolean
}}

Webpage content:
{page_content['content']}

Return only the JSON, nothing else."""

print("Prompt prepared for Claude.")

----------------------------------------

TITLE: Creating Cached Content with Google Generative AI in Python
DESCRIPTION: This code snippet creates a cached content model using Google Generative AI. It sets up a cache with a specific TTL and system instruction, and then constructs a GenerativeModel using the cached content.

LANGUAGE: python
CODE:
# Create a cache with a 5 minute TTL
cache = caching.CachedContent.create(
    model="models/gemini-1.5-flash-002",
    display_name="website crawl testing again", # used to identify the cache
    system_instruction="You are an expert at this website, and your job is to answer user's query based on the website you have access to.",
    contents=[text_file],
    ttl=datetime.timedelta(minutes=15),
)
# Construct a GenerativeModel which uses the created cache.
model = genai.GenerativeModel.from_cached_content(cached_content=cache)

----------------------------------------

TITLE: Crawling a Website with Firecrawl in Python
DESCRIPTION: This code snippet demonstrates how to crawl a website using Firecrawl, process the results, and save them to a file. It uses the FirecrawlApp to perform the crawl and the json library to save the results.

LANGUAGE: python
CODE:
# Crawl a website
crawl_url = 'https://dify.ai/'
params = {
   
    'crawlOptions': {
        'limit': 100
    }
}
crawl_result = app.crawl_url(crawl_url, params=params)

if crawl_result is not None:
    # Convert crawl results to JSON format, excluding 'content' field from each entry
    cleaned_crawl_result = [{k: v for k, v in entry.items() if k != 'content'} for entry in crawl_result]

    # Save the modified results as a text file containing JSON data
    with open('crawl_result.txt', 'w') as file:
        file.write(json.dumps(cleaned_crawl_result, indent=4))
else:
    print("No data returned from crawl.")

----------------------------------------

TITLE: Parsing and Displaying Results
DESCRIPTION: Processes the Claude AI response and displays the extracted information in formatted JSON.

LANGUAGE: python
CODE:
# Parse and print the result
result = json.loads(response.content[0].text)
print(json.dumps(result, indent=2))

----------------------------------------

TITLE: Crawling a Website with Firecrawl in Python
DESCRIPTION: This snippet demonstrates how to use Firecrawl to crawl a website, process the results, and save them to a file. It excludes the 'content' field from each entry in the crawl results.

LANGUAGE: python
CODE:
# Crawl a website
crawl_url = 'https://dify.ai/'
params = {
   
    'crawlOptions': {
        'limit': 100
    }
}
crawl_result = app.crawl_url(crawl_url, params=params)

if crawl_result is not None:
    # Convert crawl results to JSON format, excluding 'content' field from each entry
    cleaned_crawl_result = [{k: v for k, v in entry.items() if k != 'content'} for entry in crawl_result]

    # Save the modified results as a text file containing JSON data
    with open('crawl_result.txt', 'w') as file:
        file.write(json.dumps(cleaned_crawl_result, indent=4))
else:
    print("No data returned from crawl.")

----------------------------------------

TITLE: Natural Language Extraction with Firecrawl
DESCRIPTION: Demonstrates how to use natural language prompts to extract specific information from scraped content.

LANGUAGE: python
CODE:
data = app.scrape_url(
    url,
    params={
        'formats': ['markdown', 'extract', 'screenshot'],
        'extract': {
            'prompt': "Return a list of links of news articles that may be about the 2024 US presidential election"
        }
    }
)

----------------------------------------

TITLE: Creating a Web Scraping Workflow with Firecrawl
DESCRIPTION: Illustrates how to set up a GitHub Actions workflow for scheduled web scraping using Firecrawl, including environment setup and data saving.

LANGUAGE: yaml
CODE:
name: Run Hacker News Scraper

permissions:
  contents: write

on:
  schedule:
    - cron: "0 */6 * * *"
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r hacker-news-scraper/requirements.txt
          
      - name: Run scraper
        run: python hacker-news-scraper/scraper.py
        
      - name: Commit and push if changes
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add .
          git commit -m "Update scraped data" -a || exit 0
          git push

----------------------------------------

TITLE: Configuring Scrape Parameters in Firecrawl
DESCRIPTION: Shows how to customize the scraping process by specifying formats and including/excluding specific HTML tags.

LANGUAGE: python
CODE:
data = app.scrape_url(
    url, 
    params={
        'formats': [
            'html', 
            'rawHtml', 
            'links', 
            'screenshot',
        ]
    }
)

data = app.scrape_url(url, params={"includeTags": ["p"], "excludeTags": ["span"]})

----------------------------------------

TITLE: Structured Data Extraction with Pydantic in Firecrawl
DESCRIPTION: Shows how to use Pydantic models to define schemas for structured data extraction from web pages.

LANGUAGE: python
CODE:
from pydantic import BaseModel, Field

class IndividualArticle(BaseModel):
    title: str = Field(description="The title of the news article")
    subtitle: str = Field(description="The subtitle of the news article")
    url: str = Field(description="The URL of the news article")
    author: str = Field(description="The author of the news article")
    date: str = Field(description="The date the news article was published")
    read_duration: int = Field(description="The estimated time it takes to read the news article")
    topics: list[str] = Field(description="A list of topics the news article is about")

class NewsArticlesSchema(BaseModel):
    news_articles: list[IndividualArticle] = Field(
        description="A list of news articles extracted from the page"
    )

structured_data = app.scrape_url(
    url,
    params={
        "formats": ["extract", "screenshot"],
        "extract": {
            "schema": NewsArticlesSchema.model_json_schema(),
            "prompt": "Extract the following data from the NY Times homepage: news article title, url, author, date, read_duration for all news articles",
            "systemPrompt": "You are a helpful assistant that extracts news article data from NY Times.",
        },
    },
)

----------------------------------------

TITLE: Building and Pushing Docker Containers with GitHub Actions
DESCRIPTION: Demonstrates a workflow for automatically building a Docker container and pushing it to Docker Hub when changes are made to specific files.

LANGUAGE: yaml
CODE:
name: Build and Push Container
on:
  push:
    branches: [main]
    paths:
      - 'Dockerfile'
      - 'src/**'
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
        
      - name: Login to Docker Hub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
          
      - name: Build and push
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: |
            user/app:latest
            user/app:${{ github.sha }}

----------------------------------------

TITLE: Loading Website Content with FireCrawl
DESCRIPTION: Initialize FireCrawlLoader to crawl and load website content for processing. Requires a FireCrawl API key and target URL.

LANGUAGE: python
CODE:
from langchain_community.document_loaders import FireCrawlLoader  # Importing the FireCrawlLoader

url = "https://firecrawl.dev"
loader = FireCrawlLoader(
    api_key="fc-YOUR_API_KEY", # Note: Replace 'YOUR_API_KEY' with your actual FireCrawl API key
    url=url,  # Target URL to crawl
    mode="crawl"  # Mode set to 'crawl' to crawl all accessible subpages
)
docs = loader.load()

----------------------------------------

TITLE: Generating Responses with Groq
DESCRIPTION: Use Groq's Llama 3 model to generate responses based on retrieved documents. Requires Groq API key and handles chat completion with specific parameters.

LANGUAGE: python
CODE:
from groq import Groq

client = Groq(
    api_key="YOUR_GROQ_API_KEY",
)

completion = client.chat.completions.create(
    model="llama3-8b-8192",
    messages=[
        {
            "role": "user",
            "content": f"You are a friendly assistant. Your job is to answer the users question based on the documentation provided below:\nDocs:\n\n{docs}\n\nQuestion: {question}"
        }
    ],
    temperature=1,
    max_tokens=1024,
    top_p=1,
    stream=False,
    stop=None,
)

print(completion.choices[0].message)

----------------------------------------

TITLE: Python Package Dependencies
DESCRIPTION: List of Python package dependencies required for the firecrawl project. Includes packages for HTTP requests, testing, environment variables, websockets, async operations, and data validation.

LANGUAGE: text
CODE:
requests
pytest
python-dotenv
websockets
nest-asyncio
pydantic

----------------------------------------

TITLE: Crawling Website URL with Firecrawl in Python
DESCRIPTION: This code demonstrates how to use Firecrawl to scrape a website URL, excluding specific paths, and obtain the crawl result in a clean format for LLM processing.

LANGUAGE: python
CODE:
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="YOUR-KEY")

crawl_result = app.crawl_url('mendable.ai', {'crawlerOptions': {'excludes': ['blog/*','usecases/*']}})

print(crawl_result)

----------------------------------------

TITLE: Installing Dependencies for Website Contradiction Checker
DESCRIPTION: This snippet shows how to install the required Python dependencies, including firecrawl-py and anthropic, using pip.

LANGUAGE: bash
CODE:
pip install firecrawl-py anthropic

----------------------------------------

TITLE: Configuring Fly.io Service Concurrency Limits
DESCRIPTION: Fly.io service configuration defining request concurrency limits with hard limit set to 100 and soft limit to 75 requests

LANGUAGE: toml
CODE:
# fly.staging.toml
[http_service.concurrency]
  type = "requests"
  hard_limit = 100
  soft_limit = 75

----------------------------------------

TITLE: Crawling a Website Using Firecrawl API in Python
DESCRIPTION: This code snippet demonstrates how to crawl a website using the Firecrawl API. It sets crawl parameters and executes the crawl operation for the specified blog URL.

LANGUAGE: python
CODE:
# Crawl a website
params = {
    'crawlOptions': {
        'limit': 100
    },
    "pageOptions": {
        "onlyMainContent": True
    }
}
crawl_result = app.crawl_url(blog_url, params=params)

----------------------------------------

TITLE: Extracting Potential Links from Crawl Results in Python
DESCRIPTION: This code snippet processes the crawl results to extract potential internal links. It collects unique URLs and titles from the crawled pages, excluding the main blog URL.

LANGUAGE: python
CODE:
potential_links = []

if crawl_result:
    print("Collecting potential links from crawl_result:")
    
    for item in crawl_result:
        metadata = item["metadata"]
        og_url = metadata.get("ogUrl")
        title = metadata.get("title")
        if og_url and title and og_url != blog_url:
            potential_links.append({"url": og_url, "title": title})
    
    print(f"Collected {len(potential_links)} potential links:")
    for link in potential_links:
        print(f"URL: {link['url']}, Title: {link['title']}")
        
else:
    print("crawl_result is empty or None")

----------------------------------------

TITLE: Installing Python Dependencies
DESCRIPTION: Install required Python packages including langchain, groq, faiss, ollama, and firecrawl-py using pip.

LANGUAGE: bash
CODE:
pip install --upgrade --quiet langchain langchain-community groq faiss-cpu ollama firecrawl-py

----------------------------------------

TITLE: Setting Up API Keys and Target URL
DESCRIPTION: Retrieves API keys from environment variables and configures the target URL for web scraping.

LANGUAGE: python
CODE:
# Retrieve API keys from environment variables
anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")

# Set the URL to scrape
url = "https://mendable.ai"  # Replace with the actual URL you want to scrape

print(f"URL to scrape: {url}")

----------------------------------------

TITLE: Initializing Firecrawl and Anthropic Clients
DESCRIPTION: Creates instances of FirecrawlApp and Anthropic clients using the configured API keys.

LANGUAGE: python
CODE:
# Initialize FirecrawlApp and Anthropic client
firecrawl_app = FirecrawlApp(api_key=firecrawl_api_key)
anthropic_client = Anthropic(api_key=anthropic_api_key)

print("Firecrawl and Anthropic clients initialized.")

----------------------------------------

TITLE: Building and Pushing Firecrawl Playwright Docker Image
DESCRIPTION: Commands to build the Firecrawl Playwright Docker image for linux/amd64 platform and push it to a Docker registry.

LANGUAGE: bash
CODE:
docker build --no-cache --platform linux/amd64 -t ghcr.io/winkk-dev/firecrawl-playwright:latest ../../../apps/playwright-service
docker push ghcr.io/winkk-dev/firecrawl-playwright:latest

----------------------------------------

TITLE: Querying Claude AI
DESCRIPTION: Sends the prepared prompt to Claude AI using the Anthropic API and specific model parameters.

LANGUAGE: python
CODE:
# Query Claude
response = anthropic_client.messages.create(
    model="claude-3-opus-20240229",
    max_tokens=1000,
    messages=[
        {"role": "user", "content": prompt}
    ]
)

print("Claude response received.")

----------------------------------------

TITLE: XML Sitemap Generation Function
DESCRIPTION: Function to create an XML sitemap from a list of URLs including lastmod, changefreq and priority tags

LANGUAGE: python
CODE:
def create_xml_sitemap(urls, base_url):
    # Create the root element
    urlset = ET.Element("urlset")
    urlset.set("xmlns", "http://www.sitemaps.org/schemas/sitemap/0.9")

    # Get current date for lastmod
    today = datetime.now().strftime("%Y-%m-%d")

    # Add each URL to the sitemap
    for url in urls:
        # Only include URLs from the same domain
        if urlparse(url).netloc == urlparse(base_url).netloc:
            url_element = ET.SubElement(urlset, "url")
            loc = ET.SubElement(url_element, "loc")
            loc.text = url

            # Add optional elements
            lastmod = ET.SubElement(url_element, "lastmod")
            lastmod.text = today

            changefreq = ET.SubElement(url_element, "changefreq")
            changefreq.text = "monthly"

            priority = ET.SubElement(url_element, "priority")
            priority.text = "0.5"

    # Create the XML string
    return ET.tostring(urlset, encoding="unicode", method="xml")

----------------------------------------

TITLE: Installing and Running Playwright Tests
DESCRIPTION: Commands for setting up and executing the main test suite using Playwright. Requires npm and includes installation of dependencies and test runner execution.

LANGUAGE: bash
CODE:
npm install
npx playwright install
npm run test

----------------------------------------

TITLE: Basic URL Mapping with Firecrawl
DESCRIPTION: Basic example of using Firecrawl's map_url() method to discover website URLs

LANGUAGE: python
CODE:
from firecrawl import FirecrawlApp
from dotenv import load_dotenv; load_dotenv()

app = FirecrawlApp()

response = app.map_url(url="https://firecrawl.dev")

----------------------------------------

TITLE: Implementing Data Validation and Storage for Web Scraper
DESCRIPTION: Adds data validation and efficient storage mechanisms to the web scraper, including compression options and date-based file organization.

LANGUAGE: python
CODE:
from typing import Optional
from datetime import datetime
import json
import gzip
from pathlib import Path

class ProductValidator:
    @staticmethod
    def validate_product(product: dict) -> Optional[str]:
        """Validate product data and return error message if invalid."""
        required_fields = ['name', 'description', 'url', 'topics']
        
        for field in required_fields:
            if not product.get(field):
                return f"Missing required field: {field}"
                
        if not isinstance(product.get('n_upvotes'), int):
            return "Invalid upvote count"
            
        if not product.get('url').startswith('http'):
            return "Invalid URL format"
            
        return None

def validate_products(products: list) -> list:
    """Validate and filter products."""
    valid_products = []
    
    for product in products:
        error = ProductValidator.validate_product(product)
        if error:
            logger.warning(f"Invalid product data: {error}")
            continue
        valid_products.append(product)
    
    return valid_products

class DataManager:
    def __init__(self, base_dir: str = "data"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)

    def save_products(self, products: list, compress: bool = True):
        """Save products with optional compression."""
        date_str = datetime.now().strftime("%Y_%m_%d")
        
        if compress:
            filename = self.base_dir / f"ph_products_{date_str}.json.gz"
            with gzip.open(filename, 'wt', encoding='utf-8') as f:
                json.dump(products, f)
        else:
            filename = self.base_dir / f"ph_products_{date_str}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(products, f)

    def load_products(self, date_str: str) -> list:
        """Load products for a specific date."""
        gz_file = self.base_dir / f"ph_products_{date_str}.json.gz"
        json_file = self.base_dir / f"ph_products_{date_str}.json"
        
        if gz_file.exists():
            with gzip.open(gz_file, 'rt', encoding='utf-8') as f:
                return json.load(f)
        elif json_file.exists():
            with open(json_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []

----------------------------------------

TITLE: Configuring Firecrawl API Key in TypeScript
DESCRIPTION: TypeScript code snippet showing how to set up the Firecrawl API key in the ingestion component

LANGUAGE: typescript
CODE:
const FIRECRAWL_API_KEY = "your-api-key-here";

----------------------------------------

TITLE: Configuring Heroku for Web Scraper Deployment
DESCRIPTION: Commands for setting up Heroku CLI, creating a new Heroku app, and configuring environment variables for the web scraper.

LANGUAGE: bash
CODE:
$ brew install heroku/brew/heroku  # macOS
$ curl https://cli-assets.heroku.com/install.sh | sh  # Linux
$ heroku login  # Opens your web browser
$ heroku create ph-scraper-your-name  # Make the app name unique
$ heroku git:remote -a ph-scraper-your-name
$ touch Procfile
$ echo "worker: python scraper.py" > Procfile
$ touch runtime.txt
$ echo "python-3.10.12" > runtime.txt
$ heroku config:set FIRECRAWL_API_KEY='your-api-key-here'
$ heroku addons:create scheduler:standard
$ heroku addons:open scheduler

----------------------------------------

TITLE: Configuring GitHub Actions Workflow for Web Scraper
DESCRIPTION: YAML configuration for a GitHub Actions workflow that runs the web scraper daily and commits the results to the repository.

LANGUAGE: yaml
CODE:
name: Product Hunt Scraper

on:
  schedule:
    - cron: '0 1 * * *'  # Runs at 1 AM UTC daily
  workflow_dispatch:  # Allows manual trigger

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      with:
          persist-credentials: true
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run scraper
      env:
        FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}
      run: python scraper.py
        
    - name: Commit and push if changes
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        git add *.json
        git diff --quiet && git diff --staged --quiet || git commit -m "Update ProductHunt data [skip ci]"
        git push

----------------------------------------

TITLE: Load Testing Phase Configuration
DESCRIPTION: Artillery load test configuration defining four phases with varying arrival rates over different durations, including ramp-up, peak load, and cool-down periods

LANGUAGE: yaml
CODE:
# load-test.yml
- duration: 60
arrivalRate: 10  # Initial load
- duration: 120
arrivalRate: 20  # Increased load
- duration: 180
arrivalRate: 30  # Peak load
- duration: 60
arrivalRate: 10  # Cool down

----------------------------------------

TITLE: Running Firecrawl with Docker Compose
DESCRIPTION: This command uses Docker Compose to run all services for Firecrawl, including Redis, the API server, and workers in the correct configuration.

LANGUAGE: bash
CODE:
docker compose up

----------------------------------------

TITLE: Running Tests for Firecrawl
DESCRIPTION: These commands run the test suite for Firecrawl, with options for running tests with or without authentication.

LANGUAGE: bash
CODE:
npm run test:local-no-auth

LANGUAGE: bash
CODE:
npm run test:prod

----------------------------------------

TITLE: Defining Load Test Phases for Web Scraping using Artillery in YAML
DESCRIPTION: This YAML configuration defines four phases for the load test, including initial load, increased load, peak load, and cool down. Each phase specifies a duration and arrival rate for requests.

LANGUAGE: yaml
CODE:
# load-test.yml
- duration: 60
arrivalRate: 10  # Initial load
- duration: 120
arrivalRate: 20  # Increased load
- duration: 180
arrivalRate: 30  # Peak load
- duration: 60
arrivalRate: 10  # Cool down

----------------------------------------

TITLE: Installing Firecrawl Python SDK using pip
DESCRIPTION: Command to install the Firecrawl Python SDK using pip package manager.

LANGUAGE: bash
CODE:
pip install firecrawl-py

----------------------------------------

TITLE: Installing pytest for Firecrawl SDK testing
DESCRIPTION: Command to install pytest for running the Firecrawl SDK tests.

LANGUAGE: bash
CODE:
pip install pytest

----------------------------------------

TITLE: Running Firecrawl Rust SDK Tests with Cargo
DESCRIPTION: Execute end-to-end tests for the Firecrawl Rust SDK using Cargo. This snippet shows the commands to set environment variables and run the tests.

LANGUAGE: bash
CODE:
$ export $(xargs < ./tests/.env)
$ cargo test --test e2e_with_auth

----------------------------------------

TITLE: Installing Firecrawl Node SDK via npm
DESCRIPTION: Command to install the Firecrawl Node SDK using npm package manager.

LANGUAGE: bash
CODE:
npm install @mendable/firecrawl-js

----------------------------------------

TITLE: Installing Firecrawl Rust SDK Dependencies in Cargo.toml
DESCRIPTION: Add the Firecrawl SDK and Tokio dependencies to your Rust project's Cargo.toml file. This snippet specifies the required versions and features.

LANGUAGE: toml
CODE:
[dependencies]
firecrawl = "^0.1"
tokio = { version = "^1", features = ["full"] }

----------------------------------------

TITLE: Running Firecrawl SDK tests with pytest
DESCRIPTION: Command to run the end-to-end tests for the Firecrawl SDK using pytest.

LANGUAGE: bash
CODE:
pytest firecrawl/__tests__/e2e_withAuth/test.py

----------------------------------------

TITLE: Load Test Configuration in YAML
DESCRIPTION: YAML configuration file specifying the load test parameters with a 10-minute duration and 10 requests per second arrival rate.

LANGUAGE: yaml
CODE:
# load-test.yml
  - duration: 10
    arrivalRate: 10

----------------------------------------

TITLE: Setting Number of Workers per Queue in YAML
DESCRIPTION: This YAML configuration sets the number of workers per queue to 8, which is a key parameter for the fire-engine scraping strategy.

LANGUAGE: yaml
CODE:
NUM_WORKERS_PER_QUEUE=8

----------------------------------------

TITLE: Port Forwarding for Firecrawl API Testing
DESCRIPTION: Kubectl command to set up port forwarding for testing the Firecrawl API service.

LANGUAGE: bash
CODE:
kubectl port-forward svc/api 3002:3002 -n dev

----------------------------------------

TITLE: Configuring Load Test Phases in YAML
DESCRIPTION: Defines four test phases with different arrival rates and durations to simulate varying load conditions. Includes initial load, increased load, peak load, and cool down periods.

LANGUAGE: yaml
CODE:
phases:
    - duration: 60
      arrivalRate: 1  # Initial load
    - duration: 120
      arrivalRate: 2  # Increased load
    - duration: 180
      arrivalRate: 3  # Peak load
    - duration: 60
      arrivalRate: 1  # Cool down

----------------------------------------

TITLE: Load Test Configuration in YAML
DESCRIPTION: Artillery load test configuration specifying a 60-second duration with an arrival rate of 10 requests per second, totaling 600 requests.

LANGUAGE: yaml
CODE:
- duration: 60
  arrivalRate: 10  # Initial load

----------------------------------------

TITLE: Configuring Volume Mount in Fly.toml
DESCRIPTION: This TOML configuration snippet shows how to mount the persistent volume to the Redis app in Fly.toml. It specifies the source volume and the destination path in the container.

LANGUAGE: toml
CODE:
[mounts]
source      = "redis_server"
destination = "/data"

----------------------------------------

TITLE: Configuring Load Test Phases in Artillery
DESCRIPTION: Artillery load test configuration defining 4 phases of traffic: initial load (10 req/s), increased load (20 req/s), peak load (30 req/s), and cool down (10 req/s) over a total duration of 420 seconds

LANGUAGE: yaml
CODE:
- duration: 60
  arrivalRate: 10  # Initial load
- duration: 120
  arrivalRate: 20  # Increased load
- duration: 180
  arrivalRate: 30  # Peak load
- duration: 60
  arrivalRate: 10  # Cool down

----------------------------------------

TITLE: Configuring Fly.io Concurrency Limits for Web Scraping in TOML
DESCRIPTION: This snippet sets the concurrency limits for the HTTP service in Fly.io. It defines a hard limit of 100 requests and a soft limit of 50 requests.

LANGUAGE: toml
CODE:
# fly.staging.toml
[http_service.concurrency]
  type = "requests"
  hard_limit = 100
  soft_limit = 50

----------------------------------------

TITLE: Releasing Redis on Fly.io
DESCRIPTION: These bash commands are used to cut a new release or prerelease of the Redis on Fly.io project. They use a custom script to bump the version number.

LANGUAGE: bash
CODE:
scripts/bump_version.sh

LANGUAGE: bash
CODE:
scripts/bump_version.sh prerel

----------------------------------------

TITLE: HTTP Timeout Configuration in YAML
DESCRIPTION: YAML configuration setting for HTTP timeout value of 30 seconds used in the load test

LANGUAGE: yaml
CODE:
  http:
    timeout: 30

----------------------------------------

TITLE: Setting Up and Running Artillery Load Tests
DESCRIPTION: Instructions for installing Artillery globally and running load tests using a configuration file. Used for performance testing and load simulation.

LANGUAGE: bash
CODE:
npm install -g artillery
artillery run load-test.yml

----------------------------------------

TITLE: Configuring Load Test Phases in YAML
DESCRIPTION: This YAML configuration defines the load test phases, specifying the duration and arrival rate for each phase. It outlines an initial load, increased load, peak load, and cool-down period.

LANGUAGE: yaml
CODE:
phases:
  - duration: 60
    arrivalRate: 1  # Initial load
  - duration: 120
    arrivalRate: 2  # Increased load
  - duration: 180
    arrivalRate: 3  # Peak load
  - duration: 60
    arrivalRate: 1  # Cool down

----------------------------------------

TITLE: Production-Ready Logging Configuration for Web Scrapers
DESCRIPTION: Python code implementing comprehensive logging setup for web scrapers with file and console handlers.

LANGUAGE: python
CODE:
import logging
from pathlib import Path

def setup_logging():
    log_dir = Path('logs')
    log_dir.mkdir(exist_ok=True)
    
    # File handler for detailed logs
    file_handler = logging.FileHandler(
        log_dir / f'scraper_{datetime.now().strftime("%Y%m%d")}.log'
    )
    file_handler.setLevel(logging.DEBUG)
    
    # Console handler for important messages
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    # Configure logging
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[file_handler, console_handler]
    )

----------------------------------------

TITLE: GitHub Actions Workflow Configuration for Scheduled Scraping
DESCRIPTION: YAML configuration file for setting up automated web scraping using GitHub Actions with scheduled execution and environment variables.

LANGUAGE: yaml
CODE:
name: Run Firecrawl Scraper

on:
  schedule:
    - cron: "0/5 * * * *" # Runs every five minute
  workflow_dispatch: # Allows manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.9"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pydantic firecrawl-py

      - name: Run scraper
        run: python firecrawl_scraper.py
        env:
          FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}

      - name: Commit and push if changes
        run: |
          git config --global user.name 'GitHub Actions Bot'
          git config --global user.email 'actions@github.com'
          git add .
          git commit -m "Update scraped data" || exit 0
          git push

----------------------------------------

TITLE: Worker Configuration Setting in YAML
DESCRIPTION: Specifies the number of workers per queue for the system configuration.

LANGUAGE: yaml
CODE:
NUM_WORKERS_PER_QUEUE=12

----------------------------------------

TITLE: Setting up Python Virtual Environment
DESCRIPTION: Commands to create and activate a virtual environment for the project, including initial file creation and Git setup

LANGUAGE: bash
CODE:
mkdir automated-price-tracker
cd automated-price-tracker
python -m venv .venv
source .venv/bin/activate
touch ui.py
pip install streamlit
touch requirements.txt
echo "streamlit\n" >> requirements.txt
git init
touch .gitignore
echo ".venv" >> .gitignore
git commit -m "Initial commit"

----------------------------------------

TITLE: Creating Basic Streamlit UI
DESCRIPTION: Initial Streamlit interface code that creates a sidebar for product input and main dashboard area

LANGUAGE: python
CODE:
import streamlit as st

# Set up sidebar
with st.sidebar:
    st.title("Add New Product")
    product_url = st.text_input("Product URL")
    add_button = st.button("Add Product")

# Main content
st.title("Price Tracker Dashboard")
st.markdown("## Tracked Products")

----------------------------------------

TITLE: Installing Dependencies for Firecrawl UI Template
DESCRIPTION: Command to install the required npm dependencies for the project

LANGUAGE: bash
CODE:
npm install

----------------------------------------

TITLE: Database Models Definition
DESCRIPTION: SQLAlchemy models defining the database schema for products and price histories

LANGUAGE: python
CODE:
class Product(Base):
    __tablename__ = "products"
    url = Column(String, primary_key=True)
    prices = relationship("PriceHistory", back_populates="product", cascade="all, delete-orphan")

class PriceHistory(Base):
    __tablename__ = "price_histories"
    id = Column(String, primary_key=True)
    product_url = Column(String, ForeignKey("products.url"))
    name = Column(String, nullable=False)
    price = Column(Float, nullable=False)
    currency = Column(String, nullable=False)
    main_image_url = Column(String)
    timestamp = Column(DateTime, nullable=False)
    product = relationship("Product", back_populates="prices")

----------------------------------------

TITLE: GitHub Actions Workflow Configuration
DESCRIPTION: YAML configuration for automated price checking using GitHub Actions, running on a schedule

LANGUAGE: yaml
CODE:
name: Price Check

on:
  schedule:
    - cron: "0 0,6,12,18 * * *"
  workflow_dispatch:

jobs:
  check-prices:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: "pip"
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Run price checker
        env:
          FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}
          POSTGRES_URL: ${{ secrets.POSTGRES_URL }}
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
        run: python check_prices.py

----------------------------------------

TITLE: Starting Development Server for Firecrawl UI
DESCRIPTION: Command to start the development server for local testing

LANGUAGE: bash
CODE:
npm run dev

----------------------------------------

TITLE: Building and Pushing Firecrawl API Docker Image
DESCRIPTION: Commands to build the Firecrawl API Docker image for linux/amd64 platform and push it to a Docker registry.

LANGUAGE: bash
CODE:
docker build --no-cache --platform linux/amd64 -t ghcr.io/winkk-dev/firecrawl:latest ../../../apps/api
docker push ghcr.io/winkk-dev/firecrawl:latest

----------------------------------------

TITLE: Environment Variables Configuration
DESCRIPTION: Configuration of API keys in .env file for OpenAI and Firecrawl services

LANGUAGE: plaintext
CODE:
OPENAI_API_KEY=your_openai_api_key
FIRECRAWL_API_KEY=your_firecrawl_api_key

----------------------------------------

TITLE: Installing Dependencies with pip
DESCRIPTION: Command to install required Python packages from requirements.txt file

LANGUAGE: bash
CODE:
pip install -r requirements.txt

----------------------------------------

TITLE: Running the Main Application
DESCRIPTION: Command to execute the main Python script for the interactive demo

LANGUAGE: bash
CODE:
python main.py

----------------------------------------

TITLE: Initializing Firecrawl and Google Generative AI in Python
DESCRIPTION: This code snippet sets up the environment by loading API keys, configuring Google Generative AI, and initializing the FirecrawlApp. It requires the dotenv, google.generativeai, and firecrawl libraries.

LANGUAGE: python
CODE:
import os
import datetime
import time
import google.generativeai as genai
from google.generativeai import caching
from dotenv import load_dotenv
from firecrawl import FirecrawlApp
import json

# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
google_api_key = os.getenv("GOOGLE_API_KEY")
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")

# Configure the Google Generative AI module with the API key
genai.configure(api_key=google_api_key)

# Initialize the FirecrawlApp with your API key
app = FirecrawlApp(api_key=firecrawl_api_key)

----------------------------------------

TITLE: Installing Firecrawl Python Dependencies
DESCRIPTION: Commands to install the required Python packages including firecrawl-py and python-dotenv

LANGUAGE: bash
CODE:
pip install firecrawl-py
touch .env
echo "FIRECRAWL_API_KEY='YOUR-API-KEY'" >> .env
pip install python-dotenv

----------------------------------------

TITLE: Listing Required Python Packages
DESCRIPTION: Lists the Python packages required for the firecrawl project. Includes requests for HTTP requests, beautifulsoup4 for web scraping, pydantic for data validation, python-dotenv for environment management, and firecrawl-py package.

LANGUAGE: plaintext
CODE:
requests
beautifulsoup4
firecrawl
pydantic
python-dotenv
firecrawl-py

----------------------------------------

TITLE: Creating Persistent Volume for Redis on Fly.io
DESCRIPTION: This command creates a persistent volume for Redis data storage on Fly.io. It specifies the volume name, region, and size. This ensures data persistence across deploys and restarts.

LANGUAGE: cmd
CODE:
flyctl volumes create redis_server --region ord

LANGUAGE: out
CODE:
      Name: redis_server
    Region: ord
   Size GB: 10
Created at: 02 Nov 20 19:55 UTC

----------------------------------------

TITLE: Listing Python Package Dependencies
DESCRIPTION: Requirements file specifying Python packages needed for the firecrawl project. Includes core dependencies for OpenAI integration, Google search functionality, and a custom swarm package from GitHub.

LANGUAGE: text
CODE:
firecrawl-py
openai
google-search-results
git+https://github.com/openai/swarm.git

----------------------------------------

TITLE: Compiling go-html-to-md Library to Shared Object in Bash
DESCRIPTION: This bash script navigates to the go-html-to-md library directory, compiles the Go code into a shared object file named 'html-to-markdown.so', and sets executable permissions on the resulting file. The -buildmode=c-shared flag is used to create a C-compatible shared library.

LANGUAGE: bash
CODE:
cd apps/api/src/lib/go-html-to-md
go build -o html-to-markdown.so -buildmode=c-shared html-to-markdown.go
chmod +x html-to-markdown.so

----------------------------------------

TITLE: Listing Python Package Dependencies for firecrawl
DESCRIPTION: This snippet lists the required Python packages for the firecrawl project. It includes standard PyPI packages like openai and google-search-results, as well as a Git repository for the swarm package from OpenAI.

LANGUAGE: plaintext
CODE:
firecrawl-py
openai
google-search-results
git+https://github.com/openai/swarm.git

----------------------------------------

TITLE: Installing Dependencies for Airbnb Data Scraping Project
DESCRIPTION: This command installs the necessary Node.js dependencies for the project. It should be run in the project's root directory after cloning the repository.

LANGUAGE: bash
CODE:
npm install

----------------------------------------

TITLE: Importing Required Libraries for Web Scraping
DESCRIPTION: Imports necessary Python libraries for web scraping and API interactions, including Firecrawl, Anthropic Claude, and environment variable management.

LANGUAGE: python
CODE:
import os
import json
from firecrawl import FirecrawlApp
from anthropic import Anthropic
from dotenv import load_dotenv

# Load environment variables
load_dotenv()