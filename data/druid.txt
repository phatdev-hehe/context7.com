TITLE: Defining Basic DataSchema for Druid Ingestion
DESCRIPTION: Initial setup of the dataSchema object, which is the core element of a Druid ingestion spec. It defines how to parse input data into columns for storage in Druid.

LANGUAGE: json
CODE:
"dataSchema" : {}

----------------------------------------

TITLE: Example Segment Identifier Without Partition
DESCRIPTION: Shows the format of a Druid segment identifier for partition 0, which omits the partition number suffix.

LANGUAGE: text
CODE:
clarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z

----------------------------------------

TITLE: Example Segment Identifier Without Partition
DESCRIPTION: Shows the format of a Druid segment identifier for partition 0, which omits the partition number suffix.

LANGUAGE: text
CODE:
clarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z

----------------------------------------

TITLE: Configuring JVM Flags for Apache Druid
DESCRIPTION: Essential JVM configuration flags for Druid deployments, including timezone, encoding, garbage collection, memory management, and logging settings. These flags help optimize performance and provide better debugging capabilities.

LANGUAGE: java
CODE:
-Duser.timezone=UTC
-Dfile.encoding=UTF-8
-Djava.io.tmpdir=<something other than /tmp which might be mounted to volatile tmpfs file system>
-Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
-Dorg.jboss.logging.provider=slf4j
-Dnet.spy.log.LoggerImpl=net.spy.memcached.compat.log.SLF4JLogger
-Dlog4j.shutdownCallbackRegistry=org.apache.druid.common.config.Log4jShutdown
-Dlog4j.shutdownHookEnabled=true
-XX:+PrintGCDetails
-XX:+PrintGCDateStamps
-XX:+PrintGCTimeStamps
-XX:+PrintGCApplicationStoppedTime
-XX:+PrintGCApplicationConcurrentTime
-Xloggc:/var/logs/druid/historical.gc.log
-XX:+UseGCLogFileRotation
-XX:NumberOfGCLogFiles=50
-XX:GCLogFileSize=10m
-XX:+ExitOnOutOfMemoryError
-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/var/logs/druid/historical.hprof
-XX:MaxDirectMemorySize=10240g

----------------------------------------

TITLE: Configuring DataSchema in JSON for Apache Druid Ingestion
DESCRIPTION: This snippet demonstrates a complete dataSchema configuration for Apache Druid ingestion, including dataSource, parser, metricsSpec, and granularitySpec settings.

LANGUAGE: json
CODE:
"dataSchema" : {
  "dataSource" : "wikipedia",
  "parser" : {
    "type" : "string",
    "parseSpec" : {
      "format" : "json",
      "timestampSpec" : {
        "column" : "timestamp",
        "format" : "auto"
      },
      "dimensionsSpec" : {
        "dimensions": [
          "page",
          "language",
          "user",
          "unpatrolled",
          "newPage",
          "robot",
          "anonymous",
          "namespace",
          "continent",
          "country",
          "region",
          "city",
          {
            "type": "long",
            "name": "countryNum"
          },
          {
            "type": "float",
            "name": "userLatitude"
          },
          {
            "type": "float",
            "name": "userLongitude"
          }
        ],
        "dimensionExclusions" : [],
        "spatialDimensions" : []
      }
    }
  },
  "metricsSpec" : [{
    "type" : "count",
    "name" : "count"
  }, {
    "type" : "doubleSum",
    "name" : "added",
    "fieldName" : "added"
  }, {
    "type" : "doubleSum",
    "name" : "deleted",
    "fieldName" : "deleted"
  }, {
    "type" : "doubleSum",
    "name" : "delta",
    "fieldName" : "delta"
  }],
  "granularitySpec" : {
    "segmentGranularity" : "DAY",
    "queryGranularity" : "NONE",
    "intervals" : [ "2013-08-31/2013-09-01" ]
  },
  "transformSpec" : null
}

----------------------------------------

TITLE: Configuring JVM Flags for Apache Druid
DESCRIPTION: Essential JVM configuration flags for Druid deployments, including timezone, encoding, garbage collection, memory management, and logging settings. These flags help optimize performance and provide better debugging capabilities.

LANGUAGE: java
CODE:
-Duser.timezone=UTC
-Dfile.encoding=UTF-8
-Djava.io.tmpdir=<something other than /tmp which might be mounted to volatile tmpfs file system>
-Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
-Dorg.jboss.logging.provider=slf4j
-Dnet.spy.log.LoggerImpl=net.spy.memcached.compat.log.SLF4JLogger
-Dlog4j.shutdownCallbackRegistry=org.apache.druid.common.config.Log4jShutdown
-Dlog4j.shutdownHookEnabled=true
-XX:+PrintGCDetails
-XX:+PrintGCDateStamps
-XX:+PrintGCTimeStamps
-XX:+PrintGCApplicationStoppedTime
-XX:+PrintGCApplicationConcurrentTime
-Xloggc:/var/logs/druid/historical.gc.log
-XX:+UseGCLogFileRotation
-XX:NumberOfGCLogFiles=50
-XX:GCLogFileSize=10m
-XX:+ExitOnOutOfMemoryError
-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/var/logs/druid/historical.hprof
-XX:MaxDirectMemorySize=10240g

----------------------------------------

TITLE: Executing SQL Query with JDBC
DESCRIPTION: Example Java code for executing a SQL query using JDBC connection to Druid

LANGUAGE: java
CODE:
String url = "jdbc:avatica:remote:url=http://localhost:8082/druid/v2/sql/avatica/";

Properties connectionProperties = new Properties();

try (Connection connection = DriverManager.getConnection(url, connectionProperties)) {
  try (
      final Statement statement = connection.createStatement();
      final ResultSet resultSet = statement.executeQuery(query)
  ) {
    while (resultSet.next()) {
      // Do something
    }
  }
}

----------------------------------------

TITLE: Constructing a Timeseries Query in Apache Druid (JSON)
DESCRIPTION: This snippet demonstrates the structure of a timeseries query object in Apache Druid. It includes various query parameters such as dataSource, granularity, filter, aggregations, and postAggregations. This example query calculates the sum of two fields and their ratio over a specific time interval.

LANGUAGE: json
CODE:
{
  "queryType": "timeseries",
  "dataSource": "sample_datasource",
  "granularity": "day",
  "descending": "true",
  "filter": {
    "type": "and",
    "fields": [
      { "type": "selector", "dimension": "sample_dimension1", "value": "sample_value1" },
      { "type": "or",
        "fields": [
          { "type": "selector", "dimension": "sample_dimension2", "value": "sample_value2" },
          { "type": "selector", "dimension": "sample_dimension3", "value": "sample_value3" }
        ]
      }
    ]
  },
  "aggregations": [
    { "type": "longSum", "name": "sample_name1", "fieldName": "sample_fieldName1" },
    { "type": "doubleSum", "name": "sample_name2", "fieldName": "sample_fieldName2" }
  ],
  "postAggregations": [
    { "type": "arithmetic",
      "name": "sample_divide",
      "fn": "/",
      "fields": [
        { "type": "fieldAccess", "name": "postAgg__sample_name1", "fieldName": "sample_name1" },
        { "type": "fieldAccess", "name": "postAgg__sample_name2", "fieldName": "sample_name2" }
      ]
    }
  ],
  "intervals": [ "2012-01-01T00:00:00.000/2012-01-03T00:00:00.000" ]
}

----------------------------------------

TITLE: SQL Query for Retrieving Column Metadata in Druid
DESCRIPTION: Example SQL query to retrieve metadata for columns in a specific Druid datasource using the INFORMATION_SCHEMA.

LANGUAGE: SQL
CODE:
SELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = 'druid' AND TABLE_NAME = 'foo'

----------------------------------------

TITLE: Executing SQL Query with JDBC
DESCRIPTION: Example Java code for executing a SQL query using JDBC connection to Druid

LANGUAGE: java
CODE:
String url = "jdbc:avatica:remote:url=http://localhost:8082/druid/v2/sql/avatica/";

Properties connectionProperties = new Properties();

try (Connection connection = DriverManager.getConnection(url, connectionProperties)) {
  try (
      final Statement statement = connection.createStatement();
      final ResultSet resultSet = statement.executeQuery(query)
  ) {
    while (resultSet.next()) {
      // Do something
    }
  }
}

----------------------------------------

TITLE: Executing SQL Query with JDBC
DESCRIPTION: Example Java code for executing a SQL query using JDBC connection to Druid

LANGUAGE: java
CODE:
String url = "jdbc:avatica:remote:url=http://localhost:8082/druid/v2/sql/avatica/";

Properties connectionProperties = new Properties();

try (Connection connection = DriverManager.getConnection(url, connectionProperties)) {
  try (
      final Statement statement = connection.createStatement();
      final ResultSet resultSet = statement.executeQuery(query)
  ) {
    while (resultSet.next()) {
      // Do something
    }
  }
}

----------------------------------------

TITLE: Basic Druid Ingestion Specification Structure
DESCRIPTION: Shows the basic structure of a Druid ingestion specification with its three main components.

LANGUAGE: json
CODE:
{
  "dataSchema" : {...},
  "ioConfig" : {...},
  "tuningConfig" : {...}
}

----------------------------------------

TITLE: Configuring Batch Ingestion for Wikipedia Data in Apache Druid
DESCRIPTION: This JSON configuration specifies the ingestion task for loading Wikipedia page edit data into Druid. It defines the data source, parser settings, dimensions, timestamp column, and various ingestion parameters.

LANGUAGE: json
CODE:
{
  "type" : "index",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "wikipedia",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "dimensionsSpec" : {
            "dimensions" : [
              "channel",
              "cityName",
              "comment",
              "countryIsoCode",
              "countryName",
              "isAnonymous",
              "isMinor",
              "isNew",
              "isRobot",
              "isUnpatrolled",
              "metroCode",
              "namespace",
              "page",
              "regionIsoCode",
              "regionName",
              "user",
              { "name": "added", "type": "long" },
              { "name": "deleted", "type": "long" },
              { "name": "delta", "type": "long" }
            ]
          },
          "timestampSpec": {
            "column": "time",
            "format": "iso"
          }
        }
      },
      "metricsSpec" : [],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "day",
        "queryGranularity" : "none",
        "intervals" : ["2015-09-12/2015-09-13"],
        "rollup" : false
      }
    },
    "ioConfig" : {
      "type" : "index",
      "firehose" : {
        "type" : "local",
        "baseDir" : "quickstart/tutorial/",
        "filter" : "wikiticker-2015-09-12-sampled.json.gz"
      },
      "appendToExisting" : false
    },
    "tuningConfig" : {
      "type" : "index",
      "maxRowsPerSegment" : 5000000,
      "maxRowsInMemory" : 25000
    }
  }
}

----------------------------------------

TITLE: Basic Druid Ingestion Spec Structure
DESCRIPTION: The base structure of a Druid ingestion specification showing the three main components: dataSchema, ioConfig, and tuningConfig.

LANGUAGE: json
CODE:
{
  "dataSchema" : {...},
  "ioConfig" : {...},
  "tuningConfig" : {...}
}

----------------------------------------

TITLE: Configuring Parallel Index Task in Druid
DESCRIPTION: Example configuration for a parallel index task that processes Wikipedia data across multiple worker nodes. Demonstrates dataSchema, ioConfig, and tuningConfig settings.

LANGUAGE: json
CODE:
{
  "type": "index_parallel",
  "spec": {
    "dataSchema": {
      "dataSource": "wikipedia_parallel_index_test",
      "metricsSpec": [
        {
          "type": "count",
              "name": "count"
            },
            {
              "type": "doubleSum",
              "name": "added",
              "fieldName": "added"
            },
            {
              "type": "doubleSum",
              "name": "deleted",
              "fieldName": "deleted"
            },
            {
              "type": "doubleSum",
              "name": "delta",
              "fieldName": "delta"
            }
        ],
        "granularitySpec": {
          "segmentGranularity": "DAY",
          "queryGranularity": "second",
          "intervals" : [ "2013-08-31/2013-09-02" ]
        },
        "parser": {
          "parseSpec": {
            "format" : "json",
            "timestampSpec": {
              "column": "timestamp"
            },
            "dimensionsSpec": {
              "dimensions": [
                "page",
                "language",
                "user",
                "unpatrolled",
                "newPage",
                "robot",
                "anonymous",
                "namespace",
                "continent",
                "country",
                "region",
                "city"
              ]
            }
          }
        }
    },
    "ioConfig": {
        "type": "index_parallel",
        "firehose": {
          "type": "local",
          "baseDir": "examples/indexing/",
          "filter": "wikipedia_index_data*"
        }
    },
    "tuningconfig": {
        "type": "index_parallel",
        "maxNumSubTasks": 2
    }
  }
}

----------------------------------------

TITLE: Creating Authenticator Configuration
DESCRIPTION: Configuration properties for setting up a Basic authenticator in Druid with initial admin and internal client passwords.

LANGUAGE: properties
CODE:
druid.auth.authenticatorChain=["MyBasicAuthenticator"]

druid.auth.authenticator.MyBasicAuthenticator.type=basic
druid.auth.authenticator.MyBasicAuthenticator.initialAdminPassword=password1
druid.auth.authenticator.MyBasicAuthenticator.initialInternalClientPassword=password2
druid.auth.authenticator.MyBasicAuthenticator.authorizerName=MyBasicAuthorizer

----------------------------------------

TITLE: Constructing a Timeseries Query in Apache Druid (JSON)
DESCRIPTION: This snippet demonstrates the structure of a timeseries query object in Apache Druid. It includes various query parameters such as dataSource, granularity, filter, aggregations, and postAggregations. This example query calculates the sum of two fields and their ratio over a specific time interval.

LANGUAGE: json
CODE:
{
  "queryType": "timeseries",
  "dataSource": "sample_datasource",
  "granularity": "day",
  "descending": "true",
  "filter": {
    "type": "and",
    "fields": [
      { "type": "selector", "dimension": "sample_dimension1", "value": "sample_value1" },
      { "type": "or",
        "fields": [
          { "type": "selector", "dimension": "sample_dimension2", "value": "sample_value2" },
          { "type": "selector", "dimension": "sample_dimension3", "value": "sample_value3" }
        ]
      }
    ]
  },
  "aggregations": [
    { "type": "longSum", "name": "sample_name1", "fieldName": "sample_fieldName1" },
    { "type": "doubleSum", "name": "sample_name2", "fieldName": "sample_fieldName2" }
  ],
  "postAggregations": [
    { "type": "arithmetic",
      "name": "sample_divide",
      "fn": "/",
      "fields": [
        { "type": "fieldAccess", "name": "postAgg__sample_name1", "fieldName": "sample_name1" },
        { "type": "fieldAccess", "name": "postAgg__sample_name2", "fieldName": "sample_name2" }
      ]
    }
  ],
  "intervals": [ "2012-01-01T00:00:00.000/2012-01-03T00:00:00.000" ]
}

----------------------------------------

TITLE: Basic Hadoop Indexing Task Configuration in Druid
DESCRIPTION: Sample configuration for a Hadoop-based batch ingestion task in Druid showing dataSchema, ioConfig, and tuningConfig settings for ingesting Wikipedia data.

LANGUAGE: json
CODE:
{
  "type" : "index_hadoop",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "wikipedia",
      "parser" : {
        "type" : "hadoopyString",
        "parseSpec" : {
          "format" : "json",
          "timestampSpec" : {
            "column" : "timestamp",
            "format" : "auto"
          },
          "dimensionsSpec" : {
            "dimensions": ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"],
            "dimensionExclusions" : [],
            "spatialDimensions" : []
          }
        }
      },
      "metricsSpec" : [
        {
          "type" : "count",
          "name" : "count"
        },
        {
          "type" : "doubleSum",
          "name" : "added",
          "fieldName" : "added"
        },
        {
          "type" : "doubleSum",
          "name" : "deleted",
          "fieldName" : "deleted"
        },
        {
          "type" : "doubleSum",
          "name" : "delta",
          "fieldName" : "delta"
        }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "DAY",
        "queryGranularity" : "NONE",
        "intervals" : [ "2013-08-31/2013-09-01" ]
      }
    },
    "ioConfig" : {
      "type" : "hadoop",
      "inputSpec" : {
        "type" : "static",
        "paths" : "/MyDirectory/example/wikipedia_data.json"
      }
    },
    "tuningConfig" : {
      "type": "hadoop"
    }
  },
  "hadoopDependencyCoordinates": <my_hadoop_version>
}

----------------------------------------

TITLE: Defining Table Data Source in Apache Druid JSON
DESCRIPTION: Demonstrates how to define a table data source in Apache Druid using JSON. Table data sources are the most common type and can be represented by a string or a full structure with 'type' and 'name' fields.

LANGUAGE: json
CODE:
{
	"type": "table",
	"name": "<string_value>"
}

----------------------------------------

TITLE: Defining TransformSpec Structure in JSON for Apache Druid
DESCRIPTION: This snippet shows the basic structure of a transformSpec in Apache Druid. It includes a list of transforms and an optional filter to be applied to input rows during ingestion.

LANGUAGE: json
CODE:
"transformSpec": {
  "transforms: <List of transforms>,
  "filter": <filter>
}

----------------------------------------

TITLE: Constructing a Timeseries Query in Apache Druid (JSON)
DESCRIPTION: This snippet demonstrates the structure of a timeseries query object in Apache Druid. It includes various query parameters such as dataSource, granularity, filter, aggregations, and postAggregations. This example query calculates the sum of two fields and their ratio over a specific time interval.

LANGUAGE: json
CODE:
{
  "queryType": "timeseries",
  "dataSource": "sample_datasource",
  "granularity": "day",
  "descending": "true",
  "filter": {
    "type": "and",
    "fields": [
      { "type": "selector", "dimension": "sample_dimension1", "value": "sample_value1" },
      { "type": "or",
        "fields": [
          { "type": "selector", "dimension": "sample_dimension2", "value": "sample_value2" },
          { "type": "selector", "dimension": "sample_dimension3", "value": "sample_value3" }
        ]
      }
    ]
  },
  "aggregations": [
    { "type": "longSum", "name": "sample_name1", "fieldName": "sample_fieldName1" },
    { "type": "doubleSum", "name": "sample_name2", "fieldName": "sample_fieldName2" }
  ],
  "postAggregations": [
    { "type": "arithmetic",
      "name": "sample_divide",
      "fn": "/",
      "fields": [
        { "type": "fieldAccess", "name": "postAgg__sample_name1", "fieldName": "sample_name1" },
        { "type": "fieldAccess", "name": "postAgg__sample_name2", "fieldName": "sample_name2" }
      ]
    }
  ],
  "intervals": [ "2012-01-01T00:00:00.000/2012-01-03T00:00:00.000" ]
}

----------------------------------------

TITLE: Segment Identifier Examples in Druid
DESCRIPTION: Examples showing the format of Druid segment identifiers including datasource name, time interval, version number and partition number.

LANGUAGE: text
CODE:
clarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z_1

clarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z

----------------------------------------

TITLE: Configuring Hadoop Index Task in Druid
DESCRIPTION: Example configuration for a Hadoop-based ingestion task that loads Wikipedia data into Druid. Includes dataSchema specification, input configuration, and tuning parameters.

LANGUAGE: json
CODE:
{
  "type" : "index_hadoop",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "wikipedia",
      "parser" : {
        "type" : "hadoopyString",
        "parseSpec" : {
          "format" : "json",
          "timestampSpec" : {
            "column" : "timestamp",
            "format" : "auto"
          },
          "dimensionsSpec" : {
            "dimensions": ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"],
            "dimensionExclusions" : [],
            "spatialDimensions" : []
          }
        }
      },
      "metricsSpec" : [
        {
          "type" : "count",
          "name" : "count"
        },
        {
          "type" : "doubleSum",
          "name" : "added",
          "fieldName" : "added"
        },
        {
          "type" : "doubleSum",
          "name" : "deleted",
          "fieldName" : "deleted"
        },
        {
          "type" : "doubleSum",
          "name" : "delta",
          "fieldName" : "delta"
        }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "DAY",
        "queryGranularity" : "NONE",
        "intervals" : [ "2013-08-31/2013-09-01" ]
      }
    },
    "ioConfig" : {
      "type" : "hadoop",
      "inputSpec" : {
        "type" : "static",
        "paths" : "/MyDirectory/example/wikipedia_data.json"
      }
    },
    "tuningConfig" : {
      "type": "hadoop"
    }
  },
  "hadoopDependencyCoordinates": <my_hadoop_version>
}

----------------------------------------

TITLE: Basic Druid Ingestion Spec Structure
DESCRIPTION: The base structure of a Druid ingestion specification showing the three main components: dataSchema, ioConfig, and tuningConfig.

LANGUAGE: json
CODE:
{
  "dataSchema" : {...},
  "ioConfig" : {...},
  "tuningConfig" : {...}
}

----------------------------------------

TITLE: Basic Hadoop Indexing Task Configuration in Druid
DESCRIPTION: Sample configuration for a Hadoop-based batch ingestion task in Druid showing dataSchema, ioConfig, and tuningConfig settings for ingesting Wikipedia data.

LANGUAGE: json
CODE:
{
  "type" : "index_hadoop",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "wikipedia",
      "parser" : {
        "type" : "hadoopyString",
        "parseSpec" : {
          "format" : "json",
          "timestampSpec" : {
            "column" : "timestamp",
            "format" : "auto"
          },
          "dimensionsSpec" : {
            "dimensions": ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"],
            "dimensionExclusions" : [],
            "spatialDimensions" : []
          }
        }
      },
      "metricsSpec" : [
        {
          "type" : "count",
          "name" : "count"
        },
        {
          "type" : "doubleSum",
          "name" : "added",
          "fieldName" : "added"
        },
        {
          "type" : "doubleSum",
          "name" : "deleted",
          "fieldName" : "deleted"
        },
        {
          "type" : "doubleSum",
          "name" : "delta",
          "fieldName" : "delta"
        }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "DAY",
        "queryGranularity" : "NONE",
        "intervals" : [ "2013-08-31/2013-09-01" ]
      }
    },
    "ioConfig" : {
      "type" : "hadoop",
      "inputSpec" : {
        "type" : "static",
        "paths" : "/MyDirectory/example/wikipedia_data.json"
      }
    },
    "tuningConfig" : {
      "type": "hadoop"
    }
  },
  "hadoopDependencyCoordinates": <my_hadoop_version>
}

----------------------------------------

TITLE: Configuring Hadoop Index Task in Druid
DESCRIPTION: Example configuration for a Hadoop-based ingestion task that loads Wikipedia data into Druid. Includes dataSchema specification, input configuration, and tuning parameters.

LANGUAGE: json
CODE:
{
  "type" : "index_hadoop",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "wikipedia",
      "parser" : {
        "type" : "hadoopyString",
        "parseSpec" : {
          "format" : "json",
          "timestampSpec" : {
            "column" : "timestamp",
            "format" : "auto"
          },
          "dimensionsSpec" : {
            "dimensions": ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"],
            "dimensionExclusions" : [],
            "spatialDimensions" : []
          }
        }
      },
      "metricsSpec" : [
        {
          "type" : "count",
          "name" : "count"
        },
        {
          "type" : "doubleSum",
          "name" : "added",
          "fieldName" : "added"
        },
        {
          "type" : "doubleSum",
          "name" : "deleted",
          "fieldName" : "deleted"
        },
        {
          "type" : "doubleSum",
          "name" : "delta",
          "fieldName" : "delta"
        }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "DAY",
        "queryGranularity" : "NONE",
        "intervals" : [ "2013-08-31/2013-09-01" ]
      }
    },
    "ioConfig" : {
      "type" : "hadoop",
      "inputSpec" : {
        "type" : "static",
        "paths" : "/MyDirectory/example/wikipedia_data.json"
      }
    },
    "tuningConfig" : {
      "type": "hadoop"
    }
  },
  "hadoopDependencyCoordinates": <my_hadoop_version>
}

----------------------------------------

TITLE: Executing SQL Query with curl
DESCRIPTION: Example of how to execute a SQL query using curl by posting to the Druid SQL HTTP endpoint.

LANGUAGE: bash
CODE:
$ cat query.json
{"query":"SELECT COUNT(*) AS TheCount FROM data_source"}

$ curl -XPOST -H'Content-Type: application/json' http://BROKER:8082/druid/v2/sql/ -d @query.json
[{"TheCount":24433}]

----------------------------------------

TITLE: Querying Top Wikipedia Pages with Druid SQL
DESCRIPTION: This SQL query retrieves the 10 Wikipedia pages with the most edits on 2015-09-12. It demonstrates filtering by time, grouping, ordering, and limiting results.

LANGUAGE: sql
CODE:
SELECT page, COUNT(*) AS Edits
FROM wikipedia
WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00'
GROUP BY page ORDER BY Edits DESC
LIMIT 10

----------------------------------------

TITLE: Configuring Druid Batch Ingestion Task for Wikipedia Data
DESCRIPTION: JSON specification for a Druid index task that loads Wikipedia page edit data. Defines the data schema, input source, and ingestion parameters including dimensions, timestamp format, and granularity settings.

LANGUAGE: json
CODE:
{
  "type" : "index",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "wikipedia",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "dimensionsSpec" : {
            "dimensions" : [
              "channel",
              "cityName",
              "comment",
              "countryIsoCode",
              "countryName",
              "isAnonymous",
              "isMinor",
              "isNew",
              "isRobot",
              "isUnpatrolled",
              "metroCode",
              "namespace",
              "page",
              "regionIsoCode",
              "regionName",
              "user",
              { "name": "added", "type": "long" },
              { "name": "deleted", "type": "long" },
              { "name": "delta", "type": "long" }
            ]
          },
          "timestampSpec": {
            "column": "time",
            "format": "iso"
          }
        }
      },
      "metricsSpec" : [],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "day",
        "queryGranularity" : "none",
        "intervals" : ["2015-09-12/2015-09-13"],
        "rollup" : false
      }
    },
    "ioConfig" : {
      "type" : "index",
      "firehose" : {
        "type" : "local",
        "baseDir" : "quickstart/tutorial/",
        "filter" : "wikiticker-2015-09-12-sampled.json.gz"
      },
      "appendToExisting" : false
    },
    "tuningConfig" : {
      "type" : "index",
      "maxRowsPerSegment" : 5000000,
      "maxRowsInMemory" : 25000,
      "forceExtendableShardSpecs" : true
    }
  }
}

----------------------------------------

TITLE: Executing SQL Query with curl
DESCRIPTION: Example of sending a SQL query to Druid using curl and JSON over HTTP

LANGUAGE: bash
CODE:
$ cat query.json
{"query":"SELECT COUNT(*) AS TheCount FROM data_source"}

$ curl -XPOST -H'Content-Type: application/json' http://BROKER:8082/druid/v2/sql/ -d @query.json
[{"TheCount":24433}]

----------------------------------------

TITLE: Configuring HyperUnique Metric for Unique Counts
DESCRIPTION: This JSON snippet shows how to configure a hyperUnique metric in the metricsSpec of a Druid ingestion specification for fast unique counts.

LANGUAGE: json
CODE:
{ "type" : "hyperUnique", "name" : "devices", "fieldName" : "device_id_met" }

----------------------------------------

TITLE: JSON Query Request Format
DESCRIPTION: Example of how to format a SQL query request as JSON when using the HTTP API, including optional context parameters.

LANGUAGE: json
CODE:
{
  "query" : "SELECT COUNT(*) FROM data_source WHERE foo = 'bar' AND __time > TIMESTAMP '2000-01-01 00:00:00'",
  "context" : {
    "sqlTimeZone" : "America/Los_Angeles"
  }
}

----------------------------------------

TITLE: Configuring HyperUnique Metric for Unique Counts
DESCRIPTION: This JSON snippet shows how to configure a hyperUnique metric in the metricsSpec of a Druid ingestion specification for fast unique counts.

LANGUAGE: json
CODE:
{ "type" : "hyperUnique", "name" : "devices", "fieldName" : "device_id_met" }

----------------------------------------

TITLE: Configuring DataSchema for Apache Druid Ingestion
DESCRIPTION: This example demonstrates how to configure the dataSchema component of an Apache Druid ingestion specification, including parser settings, metrics, and granularity specifications.

LANGUAGE: json
CODE:
"dataSchema" : {
  "dataSource" : "wikipedia",
  "parser" : {
    "type" : "string",
    "parseSpec" : {
      "format" : "json",
      "timestampSpec" : {
        "column" : "timestamp",
        "format" : "auto"
      },
      "dimensionsSpec" : {
        "dimensions": [
          "page",
          "language",
          "user",
          "unpatrolled",
          "newPage",
          "robot",
          "anonymous",
          "namespace",
          "continent",
          "country",
          "region",
          "city",
          {
            "type": "long",
            "name": "countryNum"
          },
          {
            "type": "float",
            "name": "userLatitude"
          },
          {
            "type": "float",
            "name": "userLongitude"
          }
        ],
        "dimensionExclusions" : [],
        "spatialDimensions" : []
      }
    }
  },
  "metricsSpec" : [{
    "type" : "count",
    "name" : "count"
  }, {
    "type" : "doubleSum",
    "name" : "added",
    "fieldName" : "added"
  }, {
    "type" : "doubleSum",
    "name" : "deleted",
    "fieldName" : "deleted"
  }, {
    "type" : "doubleSum",
    "name" : "delta",
    "fieldName" : "delta"
  }],
  "granularitySpec" : {
    "segmentGranularity" : "DAY",
    "queryGranularity" : "NONE",
    "intervals" : [ "2013-08-31/2013-09-01" ]
  },
  "transformSpec" : null
}

----------------------------------------

TITLE: Enabling Grand Totals in Apache Druid Timeseries Query (JSON)
DESCRIPTION: This snippet demonstrates how to enable the grand totals feature in a timeseries query by adding the 'grandTotal' flag to the query context. This will include an additional row with the totals across all time buckets.

LANGUAGE: json
CODE:
{
  "queryType": "timeseries",
  "dataSource": "sample_datasource",
  "intervals": [ "2012-01-01T00:00:00.000/2012-01-03T00:00:00.000" ],
  "granularity": "day",
  "aggregations": [
    { "type": "longSum", "name": "sample_name1", "fieldName": "sample_fieldName1" },
    { "type": "doubleSum", "name": "sample_name2", "fieldName": "sample_fieldName2" }
  ],
  "context": {
    "grandTotal": true
  }
}

----------------------------------------

TITLE: Constructing a TopN Query in Apache Druid
DESCRIPTION: This snippet demonstrates the structure of a TopN query in Druid. It includes various components such as queryType, dataSource, dimension, threshold, metric, granularity, filter, aggregations, postAggregations, and intervals.

LANGUAGE: json
CODE:
{
  "queryType": "topN",
  "dataSource": "sample_data",
  "dimension": "sample_dim",
  "threshold": 5,
  "metric": "count",
  "granularity": "all",
  "filter": {
    "type": "and",
    "fields": [
      {
        "type": "selector",
        "dimension": "dim1",
        "value": "some_value"
      },
      {
        "type": "selector",
        "dimension": "dim2",
        "value": "some_other_val"
      }
    ]
  },
  "aggregations": [
    {
      "type": "longSum",
      "name": "count",
      "fieldName": "count"
    },
    {
      "type": "doubleSum",
      "name": "some_metric",
      "fieldName": "some_metric"
    }
  ],
  "postAggregations": [
    {
      "type": "arithmetic",
      "name": "average",
      "fn": "/",
      "fields": [
        {
          "type": "fieldAccess",
          "name": "some_metric",
          "fieldName": "some_metric"
        },
        {
          "type": "fieldAccess",
          "name": "count",
          "fieldName": "count"
        }
      ]
    }
  ],
  "intervals": [
    "2013-08-31T00:00:00.000/2013-09-03T00:00:00.000"
  ]
}

----------------------------------------

TITLE: Configuring Compaction Task in Apache Druid
DESCRIPTION: This JSON schema defines the structure for a compaction task in Apache Druid. It includes fields for specifying the data source, interval, dimension specifications, and various configuration options for controlling the compaction process.

LANGUAGE: json
CODE:
{
    "type": "compact",
    "id": <task_id>,
    "dataSource": <task_datasource>,
    "interval": <interval to specify segments to be merged>,
    "dimensions" <custom dimensionsSpec>,
    "keepSegmentGranularity": <true or false>,
    "segmentGranularity": <segment granularity after compaction>,
    "targetCompactionSizeBytes": <target size of compacted segments>
    "tuningConfig" <index task tuningConfig>,
    "context": <task context>
}

----------------------------------------

TITLE: Illustrating Druid's Dimension Column Data Structures
DESCRIPTION: This code snippet demonstrates the three main data structures used to represent a dimension column in Druid: a dictionary mapping values to integer IDs, a list of encoded column values, and bitmaps for each unique value indicating which rows contain that value.

LANGUAGE: json
CODE:
1: Dictionary that encodes column values
  {
    "Justin Bieber": 0,
    "Ke$ha":         1
  }

2: Column data
  [0,
   0,
   1,
   1]

3: Bitmaps - one for each unique value of the column
  value="Justin Bieber": [1,1,0,0]
  value="Ke$ha":         [0,0,1,1]

----------------------------------------

TITLE: Configuring Multi-InputSpec for Delta Ingestion in Druid
DESCRIPTION: This JSON configuration shows how to set up a multi-inputSpec for delta ingestion in Druid. It combines data from an existing Druid dataSource and a static file path, allowing for updates to existing data alongside new data ingestion.

LANGUAGE: json
CODE:
"ioConfig" : {
  "type" : "hadoop",
  "inputSpec" : {
    "type" : "multi",
    "children": [
      {
        "type" : "dataSource",
        "ingestionSpec" : {
          "dataSource": "wikipedia",
          "intervals": ["2012-01-01T00:00:00.000/2012-01-03T00:00:00.000", "2012-01-05T00:00:00.000/2012-01-07T00:00:00.000"],
          "segments": [
            {
              "dataSource": "test1",
              "interval": "2012-01-01T00:00:00.000/2012-01-03T00:00:00.000",
              "version": "v2",
              "loadSpec": {
                "type": "local",
                "path": "/tmp/index1.zip"
              },
              "dimensions": "host",
              "metrics": "visited_sum,unique_hosts",
              "shardSpec": {
                "type": "none"
              },
              "binaryVersion": 9,
              "size": 2,
              "identifier": "test1_2000-01-01T00:00:00.000Z_3000-01-01T00:00:00.000Z_v2"
            }
          ]
        }
      },
      {
        "type" : "static",
        "paths": "/path/to/more/wikipedia/data/"
      }
    ]  
  },
  ...
}

----------------------------------------

TITLE: Configuring Druid for PostgreSQL Metadata Storage
DESCRIPTION: Properties to add to Druid configuration for using PostgreSQL as metadata storage. Includes extension loading, storage type, connection URI, and credentials.

LANGUAGE: properties
CODE:
druid.extensions.loadList=["postgresql-metadata-storage"]
druid.metadata.storage.type=postgresql
druid.metadata.storage.connector.connectURI=jdbc:postgresql://<host>/druid
druid.metadata.storage.connector.user=druid
druid.metadata.storage.connector.password=diurd

----------------------------------------

TITLE: Defining a Selector Filter in Druid
DESCRIPTION: Demonstrates how to create a selector filter to match a specific dimension with a specific value. This is equivalent to a WHERE clause in SQL.

LANGUAGE: JSON
CODE:
"filter": { "type": "selector", "dimension": <dimension_string>, "value": <dimension_value_string> }

----------------------------------------

TITLE: Metrics Specification for Event Counting
DESCRIPTION: JSON configuration showing how to set up a count metric in the metricsSpec section of an ingestion specification.

LANGUAGE: json
CODE:
"metricsSpec" : [
      {
        "type" : "count",
        "name" : "count"
      }

----------------------------------------

TITLE: Example groupBy Query Structure in Druid
DESCRIPTION: Demonstrates a complete groupBy query example showing filtering, aggregations, post-aggregations and other key components.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "sample_datasource",
  "granularity": "day",
  "dimensions": ["country", "device"],
  "limitSpec": { "type": "default", "limit": 5000, "columns": ["country", "data_transfer"] },
  "filter": {
    "type": "and",
    "fields": [
      { "type": "selector", "dimension": "carrier", "value": "AT&T" },
      { "type": "or", 
        "fields": [
          { "type": "selector", "dimension": "make", "value": "Apple" },
          { "type": "selector", "dimension": "make", "value": "Samsung" }
        ]
      }
    ]
  },
  "aggregations": [
    { "type": "longSum", "name": "total_usage", "fieldName": "user_count" },
    { "type": "doubleSum", "name": "data_transfer", "fieldName": "data_transfer" }
  ],
  "postAggregations": [
    { "type": "arithmetic",
      "name": "avg_usage",
      "fn": "/",
      "fields": [
        { "type": "fieldAccess", "fieldName": "data_transfer" },
        { "type": "fieldAccess", "fieldName": "total_usage" }
      ]
    }
  ],
  "intervals": [ "2012-01-01T00:00:00.000/2012-01-03T00:00:00.000" ],
  "having": {
    "type": "greaterThan",
    "aggregation": "total_usage",
    "value": 100
  }
}

----------------------------------------

TITLE: Configuring Filtered Aggregator in Druid
DESCRIPTION: The filtered aggregator wraps any given aggregator and only aggregates values that match the specified dimension filter. This allows computing filtered and unfiltered aggregations simultaneously.

LANGUAGE: json
CODE:
{
  "type" : "filtered",
  "filter" : {
    "type" : "selector",
    "dimension" : <dimension>,
    "value" : <dimension value>
  }
  "aggregator" : <aggregation>
}

----------------------------------------

TITLE: Illustrating Druid's Dimension Column Data Structures
DESCRIPTION: This code snippet demonstrates the three main data structures used to represent a dimension column in Druid: a dictionary mapping values to integer IDs, a list of encoded column values, and bitmaps for each unique value indicating which rows contain that value.

LANGUAGE: json
CODE:
1: Dictionary that encodes column values
  {
    "Justin Bieber": 0,
    "Ke$ha":         1
  }

2: Column data
  [0,
   0,
   1,
   1]

3: Bitmaps - one for each unique value of the column
  value="Justin Bieber": [1,1,0,0]
  value="Ke$ha":         [0,0,1,1]

----------------------------------------

TITLE: Querying Segment Statistics in Apache Druid SQL
DESCRIPTION: This SQL query retrieves statistics about published segments in a Druid datasource, including average number of rows, average size, and total counts. It helps identify if segment compaction is necessary.

LANGUAGE: sql
CODE:
SELECT
  "start",
  "end",
  version,
  COUNT(*) AS num_segments,
  AVG("num_rows") AS avg_num_rows,
  SUM("num_rows") AS total_num_rows,
  AVG("size") AS avg_size,
  SUM("size") AS total_size
FROM
  sys.segments A
WHERE
  datasource = 'your_dataSource' AND
  is_published = 1
GROUP BY 1, 2, 3
ORDER BY 1, 2, 3 DESC;

----------------------------------------

TITLE: Metrics Specification for Event Counting
DESCRIPTION: JSON configuration showing how to set up a count metric in the metricsSpec section of an ingestion specification.

LANGUAGE: json
CODE:
"metricsSpec" : [
      {
        "type" : "count",
        "name" : "count"
      }

----------------------------------------

TITLE: Defining a Selector Filter in Druid
DESCRIPTION: Demonstrates how to create a selector filter to match a specific dimension with a specific value. This is equivalent to a WHERE clause in SQL.

LANGUAGE: JSON
CODE:
"filter": { "type": "selector", "dimension": <dimension_string>, "value": <dimension_value_string> }

----------------------------------------

TITLE: Defining TransformSpec Structure in JSON for Apache Druid
DESCRIPTION: This snippet demonstrates the basic structure of a transformSpec in Apache Druid, which includes a list of transforms and an optional filter.

LANGUAGE: json
CODE:
"transformSpec": {
  "transforms: <List of transforms>,
  "filter": <filter>
}

----------------------------------------

TITLE: Example groupBy Query Structure in Druid
DESCRIPTION: Demonstrates a complete groupBy query example showing filtering, aggregations, post-aggregations and other key components.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "sample_datasource",
  "granularity": "day",
  "dimensions": ["country", "device"],
  "limitSpec": { "type": "default", "limit": 5000, "columns": ["country", "data_transfer"] },
  "filter": {
    "type": "and",
    "fields": [
      { "type": "selector", "dimension": "carrier", "value": "AT&T" },
      { "type": "or", 
        "fields": [
          { "type": "selector", "dimension": "make", "value": "Apple" },
          { "type": "selector", "dimension": "make", "value": "Samsung" }
        ]
      }
    ]
  },
  "aggregations": [
    { "type": "longSum", "name": "total_usage", "fieldName": "user_count" },
    { "type": "doubleSum", "name": "data_transfer", "fieldName": "data_transfer" }
  ],
  "postAggregations": [
    { "type": "arithmetic",
      "name": "avg_usage",
      "fn": "/",
      "fields": [
        { "type": "fieldAccess", "fieldName": "data_transfer" },
        { "type": "fieldAccess", "fieldName": "total_usage" }
      ]
    }
  ],
  "intervals": [ "2012-01-01T00:00:00.000/2012-01-03T00:00:00.000" ],
  "having": {
    "type": "greaterThan",
    "aggregation": "total_usage",
    "value": 100
  }
}

----------------------------------------

TITLE: Executing Native TopN Query in Druid
DESCRIPTION: This snippet shows a native JSON TopN query to retrieve the 10 Wikipedia pages with the most edits on a specific date. It demonstrates the structure of a Druid native query and how to submit it using curl.

LANGUAGE: json
CODE:
{
  "queryType" : "topN",
  "dataSource" : "wikipedia",
  "intervals" : ["2015-09-12/2015-09-13"],
  "granularity" : "all",
  "dimension" : "page",
  "metric" : "count",
  "threshold" : 10,
  "aggregations" : [
    {
      "type" : "count",
      "name" : "count"
    }
  ]
}

LANGUAGE: bash
CODE:
curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-top-pages.json http://localhost:8082/druid/v2?pretty

----------------------------------------

TITLE: Configuring JSON Flatten Spec in Druid ParseSpec
DESCRIPTION: This snippet demonstrates how to configure the JSON Flatten Spec within a Druid parseSpec. It includes field discovery settings, custom field definitions using root, path, and jq types, and dimension exclusions.

LANGUAGE: json
CODE:
"parseSpec": {
  "format": "json",
  "flattenSpec": {
    "useFieldDiscovery": true,
    "fields": [
      {
        "type": "root",
        "name": "dim1"
      },
      "dim2",
      {
        "type": "path",
        "name": "foo.bar",
        "expr": "$.foo.bar"
      },
      {
        "type": "root",
        "name": "foo.bar"
      },
      {
        "type": "path",
        "name": "path-metric",
        "expr": "$.nestmet.val"
      },
      {
        "type": "path",
        "name": "hello-0",
        "expr": "$.hello[0]"
      },
      {
        "type": "path",
        "name": "hello-4",
        "expr": "$.hello[4]"
      },
      {
        "type": "path",
        "name": "world-hey",
        "expr": "$.world[0].hey"
      },
      {
        "type": "path",
        "name": "worldtree",
        "expr": "$.world[1].tree"
      },
      {
        "type": "path",
        "name": "first-food",
        "expr": "$.thing.food[0]"
      },
      {
        "type": "path",
        "name": "second-food",
        "expr": "$.thing.food[1]"
      },
      {
        "type": "jq",
        "name": "first-food-by-jq",
        "expr": ".thing.food[1]"
      },
      {
        "type": "jq",
        "name": "hello-total",
        "expr": ".hello | sum"
      }
    ]
  },
  "dimensionsSpec" : {
   "dimensions" : [],
   "dimensionsExclusions": ["ignore_me"]
  },
  "timestampSpec" : {
   "format" : "auto",
   "column" : "timestamp"
  }
}

----------------------------------------

TITLE: Executing Search Query in Druid JSON
DESCRIPTION: Example of a search query that returns dimension values matching a search specification. The query searches for values containing 'Ke' in dimensions 'dim1' and 'dim2' over a specific time interval with daily granularity.

LANGUAGE: json
CODE:
{
  "queryType": "search",
  "dataSource": "sample_datasource",
  "granularity": "day",
  "searchDimensions": [
    "dim1",
    "dim2"
  ],
  "query": {
    "type": "insensitive_contains",
    "value": "Ke"
  },
  "sort" : {
    "type": "lexicographic"
  },
  "intervals": [
    "2013-01-01T00:00:00.000/2013-01-03T00:00:00.000"
  ]
}

----------------------------------------

TITLE: Configuring CSV parseSpec for Druid Ingestion
DESCRIPTION: JSON configuration for the parseSpec section of a Druid ingestion task for CSV data. Specifies the format, timestamp, columns, and dimensions.

LANGUAGE: json
CODE:
{
  "parseSpec": {
    "format" : "csv",
    "timestampSpec" : {
      "column" : "timestamp"
    },
    "columns" : ["timestamp","page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city","added","deleted","delta"],
    "dimensionsSpec" : {
      "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"]
    }
  }
}

----------------------------------------

TITLE: Executing Basic Scan Query in Druid
DESCRIPTION: Example of a basic scan query configuration to retrieve raw data from the Wikipedia dataset. Demonstrates core parameters like queryType, dataSource, resultFormat, and intervals.

LANGUAGE: json
CODE:
{
   "queryType": "scan",
   "dataSource": "wikipedia",
   "resultFormat": "list",
   "columns":[],
   "intervals": [
     "2013-01-01/2013-01-02"
   ],
   "batchSize":20480,
   "limit":5
 }

----------------------------------------

TITLE: Executing HTTP POST Query in Apache Druid using Curl
DESCRIPTION: This snippet demonstrates how to execute a query in Apache Druid using a curl command. It sends a POST request to the queryable host with the query JSON file as the request body.

LANGUAGE: bash
CODE:
curl -X POST '<queryable_host>:<port>/druid/v2/?pretty' -H 'Content-Type:application/json' -H 'Accept:application/json' -d @<query_json_file>

----------------------------------------

TITLE: Configuring CSV parseSpec for Druid Ingestion
DESCRIPTION: JSON configuration for the parseSpec section of a Druid ingestion task for CSV data. Specifies the format, timestamp, columns, and dimensions.

LANGUAGE: json
CODE:
{
  "parseSpec": {
    "format" : "csv",
    "timestampSpec" : {
      "column" : "timestamp"
    },
    "columns" : ["timestamp","page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city","added","deleted","delta"],
    "dimensionsSpec" : {
      "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"]
    }
  }
}

----------------------------------------

TITLE: Configuring CSV parseSpec for Druid Ingestion
DESCRIPTION: JSON configuration for the parseSpec section of a Druid ingestion task for CSV data. Specifies the format, timestamp, columns, and dimensions.

LANGUAGE: json
CODE:
{
  "parseSpec": {
    "format" : "csv",
    "timestampSpec" : {
      "column" : "timestamp"
    },
    "columns" : ["timestamp","page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city","added","deleted","delta"],
    "dimensionsSpec" : {
      "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"]
    }
  }
}

----------------------------------------

TITLE: Configuring CSV parseSpec for Druid Ingestion
DESCRIPTION: JSON configuration for the parseSpec section of a Druid ingestion task for CSV data. Specifies the format, timestamp, columns, and dimensions.

LANGUAGE: json
CODE:
{
  "parseSpec": {
    "format" : "csv",
    "timestampSpec" : {
      "column" : "timestamp"
    },
    "columns" : ["timestamp","page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city","added","deleted","delta"],
    "dimensionsSpec" : {
      "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"]
    }
  }
}

----------------------------------------

TITLE: Executing Basic Scan Query in Druid
DESCRIPTION: Example of a basic scan query configuration to retrieve raw data from the Wikipedia dataset. Demonstrates core parameters like queryType, dataSource, resultFormat, and intervals.

LANGUAGE: json
CODE:
{
   "queryType": "scan",
   "dataSource": "wikipedia",
   "resultFormat": "list",
   "columns":[],
   "intervals": [
     "2013-01-01/2013-01-02"
   ],
   "batchSize":20480,
   "limit":5
 }

----------------------------------------

TITLE: Configuring subtotalsSpec in a groupBy Query
DESCRIPTION: Example of using subtotalsSpec in a groupBy query to compute multiple sub-groupings. This query demonstrates how to specify dimensions and subtotal combinations.

LANGUAGE: json
CODE:
{
"type": "groupBy",
 ...
 ...
"dimensions": [
  {
  "type" : "default",
  "dimension" : "d1col",
  "outputName": "D1"
  },
  {
  "type" : "extraction",
  "dimension" : "d2col",
  "outputName" :  "D2",
  "extractionFn" : extraction_func
  },
  {
  "type":"lookup",
  "dimension":"d3col",
  "outputName":"D3",
  "name":"my_lookup"
  }
],
...
...
"subtotalsSpec":[ ["D1", "D2", "D3"], ["D1", "D3"], ["D3"]],
..

}

----------------------------------------

TITLE: Executing Search Query in Apache Druid
DESCRIPTION: This snippet demonstrates the structure of a search query in Apache Druid. It includes essential parameters such as queryType, dataSource, granularity, searchDimensions, query specifications, sorting, and time intervals.

LANGUAGE: json
CODE:
{
  "queryType": "search",
  "dataSource": "sample_datasource",
  "granularity": "day",
  "searchDimensions": [
    "dim1",
    "dim2"
  ],
  "query": {
    "type": "insensitive_contains",
    "value": "Ke"
  },
  "sort" : {
    "type": "lexicographic"
  },
  "intervals": [
    "2013-01-01T00:00:00.000/2013-01-03T00:00:00.000"
  ]
}

----------------------------------------

TITLE: Configuring General TLS Settings in Apache Druid
DESCRIPTION: Configuration properties for enabling/disabling HTTP and HTTPS connectors in Apache Druid.

LANGUAGE: markdown
CODE:
|Property|Description|Default|
|--------|-----------|-------|
|`druid.enablePlaintextPort`|Enable/Disable HTTP connector.|`true`|
|`druid.enableTlsPort`|Enable/Disable HTTPS connector.|`false`|

----------------------------------------

TITLE: Configuring JSON Data Ingestion in Apache Druid
DESCRIPTION: This snippet shows the parseSpec configuration for ingesting JSON formatted data into Druid. It specifies the timestamp column and dimensions to be extracted.

LANGUAGE: json
CODE:
{
  "parseSpec":{
    "format" : "json",
    "timestampSpec" : {
      "column" : "timestamp"
    },
    "dimensionSpec" : {
      "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"]
    }
  }
}

----------------------------------------

TITLE: Configuring General TLS Settings in Apache Druid
DESCRIPTION: Configuration properties for enabling/disabling HTTP and HTTPS connectors in Apache Druid.

LANGUAGE: markdown
CODE:
|Property|Description|Default|
|--------|-----------|-------|
|`druid.enablePlaintextPort`|Enable/Disable HTTP connector.|`true`|
|`druid.enableTlsPort`|Enable/Disable HTTPS connector.|`false`|

----------------------------------------

TITLE: Druid Rolled-up Data Example
DESCRIPTION: Sample data after roll-up aggregation at minute granularity, showing how records are combined while maintaining dimension values.

LANGUAGE: text
CODE:
timestamp                 srcIP         dstIP          packets     bytes
2018-01-01T01:01:00Z      1.1.1.1       2.2.2.2            600      6000
2018-01-01T01:02:00Z      1.1.1.1       2.2.2.2            900      9000
2018-01-01T01:03:00Z      1.1.1.1       2.2.2.2            600      6000
2018-01-02T21:33:00Z      7.7.7.7       8.8.8.8            300      3000
2018-01-02T21:35:00Z      7.7.7.7       8.8.8.8            300      3000

----------------------------------------

TITLE: Executing a groupBy Query in Druid
DESCRIPTION: Example of a groupBy query structure in Druid, including dimensions, filters, aggregations, and post-aggregations. This query groups data by country and device, filters for specific carriers and device makers, and calculates usage metrics.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "sample_datasource",
  "granularity": "day",
  "dimensions": ["country", "device"],
  "limitSpec": { "type": "default", "limit": 5000, "columns": ["country", "data_transfer"] },
  "filter": {
    "type": "and",
    "fields": [
      { "type": "selector", "dimension": "carrier", "value": "AT&T" },
      { "type": "or", 
        "fields": [
          { "type": "selector", "dimension": "make", "value": "Apple" },
          { "type": "selector", "dimension": "make", "value": "Samsung" }
        ]
      }
    ]
  },
  "aggregations": [
    { "type": "longSum", "name": "total_usage", "fieldName": "user_count" },
    { "type": "doubleSum", "name": "data_transfer", "fieldName": "data_transfer" }
  ],
  "postAggregations": [
    { "type": "arithmetic",
      "name": "avg_usage",
      "fn": "/",
      "fields": [
        { "type": "fieldAccess", "fieldName": "data_transfer" },
        { "type": "fieldAccess", "fieldName": "total_usage" }
      ]
    }
  ],
  "intervals": [ "2012-01-01T00:00:00.000/2012-01-03T00:00:00.000" ],
  "having": {
    "type": "greaterThan",
    "aggregation": "total_usage",
    "value": 100
  }
}

----------------------------------------

TITLE: Starting Druid Services
DESCRIPTION: Command to start Druid services using the supervisor script with the tutorial configuration.

LANGUAGE: bash
CODE:
bin/supervise -c quickstart/tutorial/conf/tutorial-cluster.conf

----------------------------------------

TITLE: Executing Scan Query in Apache Druid
DESCRIPTION: This JSON snippet demonstrates how to structure a Scan query in Apache Druid. It includes essential properties such as queryType, dataSource, resultFormat, columns, intervals, batchSize, and limit.

LANGUAGE: json
CODE:
{
   "queryType": "scan",
   "dataSource": "wikipedia",
   "resultFormat": "list",
   "columns":[],
   "intervals": [
     "2013-01-01/2013-01-02"
   ],
   "batchSize":20480,
   "limit":5
 }

----------------------------------------

TITLE: Configuring Count Aggregator in Druid JSON
DESCRIPTION: Defines a count aggregator to compute the number of Druid rows matching filters. The output is stored in the specified name field.

LANGUAGE: JSON
CODE:
{ "type" : "count", "name" : <output_name> }

----------------------------------------

TITLE: Querying Segment Statistics using SQL in Apache Druid
DESCRIPTION: This SQL query retrieves statistics about published segments in a Druid datasource, including average number of rows, average size, and total counts. It uses the sys.segments table from the System Schema to gather this information.

LANGUAGE: sql
CODE:
SELECT
  "start",
  "end",
  version,
  COUNT(*) AS num_segments,
  AVG("num_rows") AS avg_num_rows,
  SUM("num_rows") AS total_num_rows,
  AVG("size") AS avg_size,
  SUM("size") AS total_size
FROM
  sys.segments A
WHERE
  datasource = 'your_dataSource' AND
  is_published = 1
GROUP BY 1, 2, 3
ORDER BY 1, 2, 3 DESC;

----------------------------------------

TITLE: Basic SQL SELECT Query in Druid
DESCRIPTION: Example of a simple SQL SELECT query structure supported by Druid SQL.

LANGUAGE: SQL
CODE:
[ EXPLAIN PLAN FOR ]
[ WITH tableName [ ( column1, column2, ... ) ] AS ( query ) ]
SELECT [ ALL | DISTINCT ] { * | exprs }
FROM table
[ WHERE expr ]
[ GROUP BY exprs ]
[ HAVING expr ]
[ ORDER BY expr [ ASC | DESC ], expr [ ASC | DESC ], ... ]
[ LIMIT limit ]
[ UNION ALL <another query> ]

----------------------------------------

TITLE: Configuring Kerberos and Basic Authentication Chain in Druid
DESCRIPTION: Example configuration showing how to enable Kerberos and HTTP Basic authenticators in the authentication chain using extensions.

LANGUAGE: json
CODE:
druid.auth.authenticatorChain=["kerberos", "basic"]

----------------------------------------

TITLE: Configuring CSV Ingestion in Apache Druid
DESCRIPTION: This snippet demonstrates the parseSpec configuration for ingesting CSV data into Druid. It defines the format, timestamp column, input columns, and dimensions to be extracted.

LANGUAGE: json
CODE:
{
  "parseSpec": {
    "format" : "csv",
    "timestampSpec" : {
      "column" : "timestamp"
    },
    "columns" : ["timestamp","page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city","added","deleted","delta"],
    "dimensionsSpec" : {
      "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"]
    }
  }
}

----------------------------------------

TITLE: Submitting a Kafka Supervisor Spec in Apache Druid
DESCRIPTION: Example of submitting a Kafka supervisor specification via HTTP POST to configure Kafka ingestion in Druid.

LANGUAGE: json
CODE:
{
  "type": "kafka",
  "dataSchema": {
    "dataSource": "metrics-kafka",
    "parser": {
      "type": "string",
      "parseSpec": {
        "format": "json",
        "timestampSpec": {
          "column": "timestamp",
          "format": "auto"
        },
        "dimensionsSpec": {
          "dimensions": [],
          "dimensionExclusions": [
            "timestamp",
            "value"
          ]
        }
      }
    },
    "metricsSpec": [
      {
        "name": "count",
        "type": "count"
      },
      {
        "name": "value_sum",
        "fieldName": "value",
        "type": "doubleSum"
      },
      {
        "name": "value_min",
        "fieldName": "value",
        "type": "doubleMin"
      },
      {
        "name": "value_max",
        "fieldName": "value",
        "type": "doubleMax"
      }
    ],
    "granularitySpec": {
      "type": "uniform",
      "segmentGranularity": "HOUR",
      "queryGranularity": "NONE"
    }
  },
  "tuningConfig": {
    "type": "kafka",
    "maxRowsPerSegment": 5000000
  },
  "ioConfig": {
    "topic": "metrics",
    "consumerProperties": {
      "bootstrap.servers": "localhost:9092"
    },
    "taskCount": 1,
    "replicas": 1,
    "taskDuration": "PT1H"
  }
}

----------------------------------------

TITLE: Configuring CSV Ingestion in Apache Druid
DESCRIPTION: This snippet demonstrates the parseSpec configuration for ingesting CSV data into Druid. It defines the format, timestamp column, input columns, and dimensions to be extracted.

LANGUAGE: json
CODE:
{
  "parseSpec": {
    "format" : "csv",
    "timestampSpec" : {
      "column" : "timestamp"
    },
    "columns" : ["timestamp","page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city","added","deleted","delta"],
    "dimensionsSpec" : {
      "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"]
    }
  }
}

----------------------------------------

TITLE: Druid Ingestion Spec with Transforms
DESCRIPTION: Ingestion specification that defines data schema, transforms, and filtering rules. Includes expression transforms to modify animal names and multiply numbers, plus filtering logic.

LANGUAGE: json
CODE:
{
  "type" : "index",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "transform-tutorial",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "dimensionsSpec" : {
            "dimensions" : [
              "animal",
              { "name": "location", "type": "long" }
            ]
          },
          "timestampSpec": {
            "column": "timestamp",
            "format": "iso"
          }
        }
      },
      "metricsSpec" : [
        { "type" : "count", "name" : "count" },
        { "type" : "longSum", "name" : "number", "fieldName" : "number" },
        { "type" : "longSum", "name" : "triple-number", "fieldName" : "triple-number" }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "week",
        "queryGranularity" : "minute",
        "intervals" : ["2018-01-01/2018-01-03"],
        "rollup" : true
      },
      "transformSpec": {
        "transforms": [
          {
            "type": "expression",
            "name": "animal",
            "expression": "concat('super-', animal)"
          },
          {
            "type": "expression",
            "name": "triple-number",
            "expression": "number * 3"
          }
        ],
        "filter": {
          "type":"or",
          "fields": [
            { "type": "selector", "dimension": "animal", "value": "super-mongoose" },
            { "type": "selector", "dimension": "triple-number", "value": "300" },
            { "type": "selector", "dimension": "location", "value": "3" }
          ]
        }
      }
    },
    "ioConfig" : {
      "type" : "index",
      "firehose" : {
        "type" : "local",
        "baseDir" : "quickstart/tutorial",
        "filter" : "transform-data.json"
      },
      "appendToExisting" : false
    },
    "tuningConfig" : {
      "type" : "index",
      "maxRowsPerSegment" : 5000000,
      "maxRowsInMemory" : 25000
    }
  }
}

----------------------------------------

TITLE: Sample Druid Directory Structure
DESCRIPTION: Shows the recommended directory structure for organizing Druid configuration files

LANGUAGE: text
CODE:
$ ls -R conf
druid       tranquility

conf/druid:
_common       broker        coordinator   historical    middleManager overlord

conf/druid/_common:
common.runtime.properties log4j2.xml

conf/druid/broker:
jvm.config         runtime.properties

conf/druid/coordinator:
jvm.config         runtime.properties

conf/druid/historical:
jvm.config         runtime.properties

conf/druid/middleManager:
jvm.config         runtime.properties

conf/druid/overlord:
jvm.config         runtime.properties

conf/tranquility:
kafka.json  server.json

----------------------------------------

TITLE: Executing SQL TopN Query in Apache Druid
DESCRIPTION: This snippet demonstrates how to execute a SQL query equivalent to the native JSON TopN query. It shows the SQL query syntax and how to submit it to the Druid Broker using curl.

LANGUAGE: sql
CODE:
SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;

LANGUAGE: bash
CODE:
curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-top-pages-sql.json http://localhost:8082/druid/v2/sql

----------------------------------------

TITLE: Druid DataSchema Configuration Example
DESCRIPTION: Detailed example of a dataSchema configuration showing parser settings, metrics specification, and granularity settings for Wikipedia data ingestion.

LANGUAGE: json
CODE:
"dataSchema" : {
  "dataSource" : "wikipedia",
  "parser" : {
    "type" : "string",
    "parseSpec" : {
      "format" : "json",
      "timestampSpec" : {
        "column" : "timestamp",
        "format" : "auto"
      },
      "dimensionsSpec" : {
        "dimensions": [
          "page",
          "language",
          "user",
          "unpatrolled",
          "newPage",
          "robot",
          "anonymous",
          "namespace",
          "continent",
          "country",
          "region",
          "city",
          {
            "type": "long",
            "name": "countryNum"
          },
          {
            "type": "float",
            "name": "userLatitude"
          },
          {
            "type": "float",
            "name": "userLongitude"
          }
        ],
        "dimensionExclusions" : [],
        "spatialDimensions" : []
      }
    }
  },
  "metricsSpec" : [{
    "type" : "count",
    "name" : "count"
  }, {
    "type" : "doubleSum",
    "name" : "added",
    "fieldName" : "added"
  }, {
    "type" : "doubleSum",
    "name" : "deleted",
    "fieldName" : "deleted"
  }, {
    "type" : "doubleSum",
    "name" : "delta",
    "fieldName" : "delta"
  }],
  "granularitySpec" : {
    "segmentGranularity" : "DAY",
    "queryGranularity" : "NONE",
    "intervals" : [ "2013-08-31/2013-09-01" ]
  },
  "transformSpec" : null
}

----------------------------------------

TITLE: Executing Native Query with Jackson Smile Format
DESCRIPTION: This snippet shows how to execute a native query using the Jackson Smile format for Content-Type and Accept headers. This alternative format may be used for more efficient data transfer.

LANGUAGE: bash
CODE:
curl -X POST '<queryable_host>:<port>/druid/v2/?pretty' -H 'Content-Type:application/json' -H 'Accept:application/x-jackson-smile' -d @<query_json_file>

----------------------------------------

TITLE: Configuring Hadoop Batch Ingestion Task
DESCRIPTION: Example configuration for a Hadoop-based batch ingestion task in Druid, showing data schema, input specification, and tuning parameters for ingesting Wikipedia data.

LANGUAGE: json
CODE:
{
  "type" : "index_hadoop",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "wikipedia",
      "parser" : {
        "type" : "hadoopyString",
        "parseSpec" : {
          "format" : "json",
          "timestampSpec" : {
            "column" : "timestamp",
            "format" : "auto"
          },
          "dimensionsSpec" : {
            "dimensions": ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"],
            "dimensionExclusions" : [],
            "spatialDimensions" : []
          }
        }
      },
      "metricsSpec" : [
        {
          "type" : "count",
          "name" : "count"
        },
        {
          "type" : "doubleSum",
          "name" : "added",
          "fieldName" : "added"
        },
        {
          "type" : "doubleSum",
          "name" : "deleted",
          "fieldName" : "deleted"
        },
        {
          "type" : "doubleSum",
          "name" : "delta",
          "fieldName" : "delta"
        }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "DAY",
        "queryGranularity" : "NONE",
        "intervals" : [ "2013-08-31/2013-09-01" ]
      }
    },
    "ioConfig" : {
      "type" : "hadoop",
      "inputSpec" : {
        "type" : "static",
        "paths" : "/MyDirectory/example/wikipedia_data.json"
      }
    },
    "tuningConfig" : {
      "type": "hadoop"
    }
  },
  "hadoopDependencyCoordinates": <my_hadoop_version>
}

----------------------------------------

TITLE: Configuring Authorizers in Apache Druid
DESCRIPTION: This JSON snippet shows how to enable the 'basic' authorizer implementation from the druid-basic-security extension.

LANGUAGE: json
CODE:
"druid.auth.authorizers":["basic"]

----------------------------------------

TITLE: Retrieving Scan Query Results in List Format
DESCRIPTION: Example of Scan query results when resultFormat is set to 'list'. It shows the segmentId, columns, and events with their respective attributes.

LANGUAGE: json
CODE:
[{
    "segmentId" : "wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9",
    "columns" : [
      "timestamp",
      "robot",
      "namespace",
      "anonymous",
      "unpatrolled",
      "page",
      "language",
      "newpage",
      "user",
      "count",
      "added",
      "delta",
      "variation",
      "deleted"
    ],
    "events" : [ {
        "timestamp" : "2013-01-01T00:00:00.000Z",
        "robot" : "1",
        "namespace" : "article",
        "anonymous" : "0",
        "unpatrolled" : "0",
        "page" : "11._korpus_(NOVJ)",
        "language" : "sl",
        "newpage" : "0",
        "user" : "EmausBot",
        "count" : 1.0,
        "added" : 39.0,
        "delta" : 39.0,
        "variation" : 39.0,
        "deleted" : 0.0
    }, {
        "timestamp" : "2013-01-01T00:00:00.000Z",
        "robot" : "0",
        "namespace" : "article",
        "anonymous" : "0",
        "unpatrolled" : "0",
        "page" : "112_U.S._580",
        "language" : "en",
        "newpage" : "1",
        "user" : "MZMcBride",
        "count" : 1.0,
        "added" : 70.0,
        "delta" : 70.0,
        "variation" : 70.0,
        "deleted" : 0.0
    }, {
        "timestamp" : "2013-01-01T00:00:00.000Z",
        "robot" : "0",
        "namespace" : "article",
        "anonymous" : "0",
        "unpatrolled" : "0",
        "page" : "113_U.S._243",
        "language" : "en",
        "newpage" : "1",
        "user" : "MZMcBride",
        "count" : 1.0,
        "added" : 77.0,
        "delta" : 77.0,
        "variation" : 77.0,
        "deleted" : 0.0
    } ]
} ]

----------------------------------------

TITLE: Configuring Parallel Index Task in Apache Druid
DESCRIPTION: Example JSON configuration for a parallel index task in Apache Druid. This task type allows for parallel batch indexing across multiple MiddleManager processes.

LANGUAGE: json
CODE:
{
  "type": "index_parallel",
  "spec": {
    "dataSchema": {
      "dataSource": "wikipedia_parallel_index_test",
      "metricsSpec": [
        {
          "type": "count",
              "name": "count"
            },
            {
              "type": "doubleSum",
              "name": "added",
              "fieldName": "added"
            },
            {
              "type": "doubleSum",
              "name": "deleted",
              "fieldName": "deleted"
            },
            {
              "type": "doubleSum",
              "name": "delta",
              "fieldName": "delta"
            }
        ],
        "granularitySpec": {
          "segmentGranularity": "DAY",
          "queryGranularity": "second",
          "intervals" : [ "2013-08-31/2013-09-02" ]
        },
        "parser": {
          "parseSpec": {
            "format" : "json",
            "timestampSpec": {
              "column": "timestamp"
            },
            "dimensionsSpec": {
              "dimensions": [
                "page",
                "language",
                "user",
                "unpatrolled",
                "newPage",
                "robot",
                "anonymous",
                "namespace",
                "continent",
                "country",
                "region",
                "city"
              ]
            }
          }
        }
    },
    "ioConfig": {
        "type": "index_parallel",
        "firehose": {
          "type": "local",
          "baseDir": "examples/indexing/",
          "filter": "wikipedia_index_data*"
        }
    },
    "tuningconfig": {
        "type": "index_parallel",
        "maxNumSubTasks": 2
    }
  }
}

----------------------------------------

TITLE: Druid DataSchema Configuration Example
DESCRIPTION: Detailed example of a dataSchema configuration showing parser settings, metrics specification, and granularity settings for Wikipedia data ingestion.

LANGUAGE: json
CODE:
"dataSchema" : {
  "dataSource" : "wikipedia",
  "parser" : {
    "type" : "string",
    "parseSpec" : {
      "format" : "json",
      "timestampSpec" : {
        "column" : "timestamp",
        "format" : "auto"
      },
      "dimensionsSpec" : {
        "dimensions": [
          "page",
          "language",
          "user",
          "unpatrolled",
          "newPage",
          "robot",
          "anonymous",
          "namespace",
          "continent",
          "country",
          "region",
          "city",
          {
            "type": "long",
            "name": "countryNum"
          },
          {
            "type": "float",
            "name": "userLatitude"
          },
          {
            "type": "float",
            "name": "userLongitude"
          }
        ],
        "dimensionExclusions" : [],
        "spatialDimensions" : []
      }
    }
  },
  "metricsSpec" : [{
    "type" : "count",
    "name" : "count"
  }, {
    "type" : "doubleSum",
    "name" : "added",
    "fieldName" : "added"
  }, {
    "type" : "doubleSum",
    "name" : "deleted",
    "fieldName" : "deleted"
  }, {
    "type" : "doubleSum",
    "name" : "delta",
    "fieldName" : "delta"
  }],
  "granularitySpec" : {
    "segmentGranularity" : "DAY",
    "queryGranularity" : "NONE",
    "intervals" : [ "2013-08-31/2013-09-01" ]
  },
  "transformSpec" : null
}

----------------------------------------

TITLE: Complete Druid Ingestion Specification
DESCRIPTION: Full ingestion specification for batch indexing task including dataSchema, parser configuration, metrics specification, granularity settings, and IO configuration.

LANGUAGE: json
CODE:
{
  "type" : "index",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "ingestion-tutorial",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "timestampSpec" : {
            "format" : "iso",
            "column" : "ts"
          },
          "dimensionsSpec" : {
            "dimensions": [
              "srcIP",
              { "name" : "srcPort", "type" : "long" },
              { "name" : "dstIP", "type" : "string" },
              { "name" : "dstPort", "type" : "long" },
              { "name" : "protocol", "type" : "string" }
            ]
          }      
        }
      },
      "metricsSpec" : [
        { "type" : "count", "name" : "count" },
        { "type" : "longSum", "name" : "packets", "fieldName" : "packets" },
        { "type" : "longSum", "name" : "bytes", "fieldName" : "bytes" },
        { "type" : "doubleSum", "name" : "cost", "fieldName" : "cost" }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "HOUR",
        "queryGranularity" : "MINUTE",
        "intervals" : ["2018-01-01/2018-01-02"],
        "rollup" : true
      }
    },
    "ioConfig" : {
      "type" : "index",
      "firehose" : {
        "type" : "local",
        "baseDir" : "quickstart/",
        "filter" : "ingestion-tutorial-data.json"
      }
    },
    "tuningConfig" : {
      "type" : "index",
      "maxRowsPerSegment" : 5000000
    }
  }
}

----------------------------------------

TITLE: Configuring Basic Authenticator in Druid
DESCRIPTION: Configuration properties for setting up basic HTTP authentication with initial admin and internal client passwords.

LANGUAGE: properties
CODE:
druid.auth.authenticatorChain=["MyBasicAuthenticator"]

druid.auth.authenticator.MyBasicAuthenticator.type=basic
druid.auth.authenticator.MyBasicAuthenticator.initialAdminPassword=password1
druid.auth.authenticator.MyBasicAuthenticator.initialInternalClientPassword=password2
druid.auth.authenticator.MyBasicAuthenticator.authorizerName=MyBasicAuthorizer

----------------------------------------

TITLE: JSON ParseSpec Configuration for Druid
DESCRIPTION: Configuration specification for parsing JSON data in Druid, including timestamp and dimension specifications.

LANGUAGE: json
CODE:
"parseSpec":{ "format" : "json", "timestampSpec" : { "column" : "timestamp" }, "dimensionSpec" : { "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"] } }

----------------------------------------

TITLE: Executing Native JSON TopN Query in Apache Druid
DESCRIPTION: This snippet shows a native JSON TopN query to retrieve the 10 Wikipedia pages with the most edits on 2015-09-12. It demonstrates how to structure the query and submit it to the Druid Broker using curl.

LANGUAGE: json
CODE:
{
  "queryType" : "topN",
  "dataSource" : "wikipedia",
  "intervals" : ["2015-09-12/2015-09-13"],
  "granularity" : "all",
  "dimension" : "page",
  "metric" : "count",
  "threshold" : 10,
  "aggregations" : [
    {
      "type" : "count",
      "name" : "count"
    }
  ]
}

LANGUAGE: bash
CODE:
curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-top-pages.json http://localhost:8082/druid/v2?pretty

----------------------------------------

TITLE: Configuring JVM Flags for Apache Druid
DESCRIPTION: A set of recommended JVM flags for optimizing Druid performance, including timezone settings, memory management, garbage collection logging, and error handling.

LANGUAGE: java
CODE:
-Duser.timezone=UTC
-Dfile.encoding=UTF-8
-Djava.io.tmpdir=<something other than /tmp which might be mounted to volatile tmpfs file system>
-Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
-Dorg.jboss.logging.provider=slf4j
-Dnet.spy.log.LoggerImpl=net.spy.memcached.compat.log.SLF4JLogger
-Dlog4j.shutdownCallbackRegistry=org.apache.druid.common.config.Log4jShutdown
-Dlog4j.shutdownHookEnabled=true
-XX:+PrintGCDetails
-XX:+PrintGCDateStamps
-XX:+PrintGCTimeStamps
-XX:+PrintGCApplicationStoppedTime
-XX:+PrintGCApplicationConcurrentTime
-Xloggc:/var/logs/druid/historical.gc.log
-XX:+UseGCLogFileRotation
-XX:NumberOfGCLogFiles=50
-XX:GCLogFileSize=10m
-XX:+ExitOnOutOfMemoryError
-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/var/logs/druid/historical.hprof
-XX:MaxDirectMemorySize=10240g

----------------------------------------

TITLE: Druid MySQL Metadata Storage Configuration
DESCRIPTION: Properties configuration for connecting Druid to MySQL metadata storage, including extension loading and database connection parameters.

LANGUAGE: properties
CODE:
druid.extensions.loadList=["mysql-metadata-storage"]
druid.metadata.storage.type=mysql
druid.metadata.storage.connector.connectURI=jdbc:mysql://<host>/druid
druid.metadata.storage.connector.user=druid
druid.metadata.storage.connector.password=druid

----------------------------------------

TITLE: Basic SQL SELECT Query Syntax
DESCRIPTION: Shows the supported syntax structure for SELECT queries in Druid SQL, including WITH clauses, WHERE filters, GROUP BY, HAVING, ORDER BY, and LIMIT clauses.

LANGUAGE: sql
CODE:
[ EXPLAIN PLAN FOR ]
[ WITH tableName [ ( column1, column2, ... ) ] AS ( query ) ]
SELECT [ ALL | DISTINCT ] { * | exprs }
FROM table
[ WHERE expr ]
[ GROUP BY exprs ]
[ HAVING expr ]
[ ORDER BY expr [ ASC | DESC ], expr [ ASC | DESC ], ... ]
[ LIMIT limit ]
[ UNION ALL <another query> ]

----------------------------------------

TITLE: Enabling Rollup in Druid Ingestion Spec
DESCRIPTION: Adds a granularitySpec to the dataSchema to enable rollup, which allows pre-aggregation of data during ingestion.

LANGUAGE: json
CODE:
"dataSchema" : {
  "dataSource" : "ingestion-tutorial",
  "parser" : {
    "type" : "string",
    "parseSpec" : {
      "format" : "json",
      "timestampSpec" : {
        "format" : "iso",
        "column" : "ts"
      }
    }
  },
  "granularitySpec" : {
    "rollup" : true
  }
}

----------------------------------------

TITLE: Core Java Classes Overview - Druid Architecture
DESCRIPTION: Key Java classes that implement Druid's core functionality across different components like storage, querying, coordination and ingestion.

LANGUAGE: java
CODE:
Column.java                               // Core storage format implementation
IncrementalIndex.java                      // Raw data ingestion
IndexMerger.java                          // Segment creation
IndexIO.java                              // Segment memory mapping
QueryResource.java                        // Query execution entry point
DruidCoordinator.java                    // Historical process coordination
OverlordResource.java                     // Ingestion coordination
FirehoseFactory.java                     // Data loading interface
RealtimeManager.java                     // Real-time ingestion management
RealtimePlumber.java                     // Persist and hand-off logic
HadoopDruidDetermineConfigurationJob.java // Hadoop segment planning
HadoopDruidIndexerJob.java               // Hadoop segment creation

----------------------------------------

TITLE: Druid DataSchema Configuration Example
DESCRIPTION: Detailed example of a dataSchema configuration showing parser settings, metrics specification, and granularity settings for Wikipedia data ingestion.

LANGUAGE: json
CODE:
"dataSchema" : {
  "dataSource" : "wikipedia",
  "parser" : {
    "type" : "string",
    "parseSpec" : {
      "format" : "json",
      "timestampSpec" : {
        "column" : "timestamp",
        "format" : "auto"
      },
      "dimensionsSpec" : {
        "dimensions": [
          "page",
          "language",
          "user",
          "unpatrolled",
          "newPage",
          "robot",
          "anonymous",
          "namespace",
          "continent",
          "country",
          "region",
          "city",
          {
            "type": "long",
            "name": "countryNum"
          },
          {
            "type": "float",
            "name": "userLatitude"
          },
          {
            "type": "float",
            "name": "userLongitude"
          }
        ],
        "dimensionExclusions" : [],
        "spatialDimensions" : []
      }
    }
  },
  "metricsSpec" : [{
    "type" : "count",
    "name" : "count"
  }, {
    "type" : "doubleSum",
    "name" : "added",
    "fieldName" : "added"
  }, {
    "type" : "doubleSum",
    "name" : "deleted",
    "fieldName" : "deleted"
  }, {
    "type" : "doubleSum",
    "name" : "delta",
    "fieldName" : "delta"
  }],
  "granularitySpec" : {
    "segmentGranularity" : "DAY",
    "queryGranularity" : "NONE",
    "intervals" : [ "2013-08-31/2013-09-01" ]
  },
  "transformSpec" : null
}

----------------------------------------

TITLE: Querying Lookups with Druid SQL
DESCRIPTION: Example of using the LOOKUP function in Druid SQL queries to perform dimension value lookups.

LANGUAGE: sql
CODE:
SELECT LOOKUP(column_name, 'lookup-name'), COUNT(*) FROM datasource GROUP BY 1

----------------------------------------

TITLE: Submitting a Kafka Supervisor Spec in JSON
DESCRIPTION: Example JSON configuration for submitting a Kafka supervisor spec to Druid's indexing service. Includes dataSchema, tuningConfig, and ioConfig settings.

LANGUAGE: json
CODE:
{
  "type": "kafka",
  "dataSchema": {
    "dataSource": "metrics-kafka",
    "parser": {
      "type": "string",
      "parseSpec": {
        "format": "json",
        "timestampSpec": {
          "column": "timestamp",
          "format": "auto"
        },
        "dimensionsSpec": {
          "dimensions": [],
          "dimensionExclusions": [
            "timestamp",
            "value"
          ]
        }
      }
    },
    "metricsSpec": [
      {
        "name": "count",
        "type": "count"
      },
      {
        "name": "value_sum",
        "fieldName": "value",
        "type": "doubleSum"
      },
      {
        "name": "value_min",
        "fieldName": "value",
        "type": "doubleMin"
      },
      {
        "name": "value_max",
        "fieldName": "value",
        "type": "doubleMax"
      }
    ],
    "granularitySpec": {
      "type": "uniform",
      "segmentGranularity": "HOUR",
      "queryGranularity": "NONE"
    }
  },
  "tuningConfig": {
    "type": "kafka",
    "maxRowsPerSegment": 5000000
  },
  "ioConfig": {
    "topic": "metrics",
    "consumerProperties": {
      "bootstrap.servers": "localhost:9092"
    },
    "taskCount": 1,
    "replicas": 1,
    "taskDuration": "PT1H"
  }
}

----------------------------------------

TITLE: Configuring Parallel Index Task in Apache Druid
DESCRIPTION: JSON configuration for a Parallel Index Task in Apache Druid. Specifies dataSchema, ioConfig, and tuningConfig for parallel batch ingestion.

LANGUAGE: json
CODE:
{
  "type": "index_parallel",
  "spec": {
    "dataSchema": {
      "dataSource": "wikipedia_parallel_index_test",
      "metricsSpec": [
        {
          "type": "count",
              "name": "count"
            },
            {
              "type": "doubleSum",
              "name": "added",
              "fieldName": "added"
            },
            {
              "type": "doubleSum",
              "name": "deleted",
              "fieldName": "deleted"
            },
            {
              "type": "doubleSum",
              "name": "delta",
              "fieldName": "delta"
            }
        ],
        "granularitySpec": {
          "segmentGranularity": "DAY",
          "queryGranularity": "second",
          "intervals" : [ "2013-08-31/2013-09-02" ]
        },
        "parser": {
          "parseSpec": {
            "format" : "json",
            "timestampSpec": {
              "column": "timestamp"
            },
            "dimensionsSpec": {
              "dimensions": [
                "page",
                "language",
                "user",
                "unpatrolled",
                "newPage",
                "robot",
                "anonymous",
                "namespace",
                "continent",
                "country",
                "region",
                "city"
              ]
            }
          }
        }
    },
    "ioConfig": {
        "type": "index_parallel",
        "firehose": {
          "type": "local",
          "baseDir": "examples/indexing/",
          "filter": "wikipedia_index_data*"
        }
    },
    "tuningconfig": {
        "type": "index_parallel",
        "maxNumSubTasks": 2
    }
  }
}

----------------------------------------

TITLE: Druid DataSchema Configuration Example
DESCRIPTION: Detailed example of a dataSchema configuration showing parser settings, metrics specification, and granularity settings for Wikipedia data ingestion.

LANGUAGE: json
CODE:
"dataSchema" : {
  "dataSource" : "wikipedia",
  "parser" : {
    "type" : "string",
    "parseSpec" : {
      "format" : "json",
      "timestampSpec" : {
        "column" : "timestamp",
        "format" : "auto"
      },
      "dimensionsSpec" : {
        "dimensions": [
          "page",
          "language",
          "user",
          "unpatrolled",
          "newPage",
          "robot",
          "anonymous",
          "namespace",
          "continent",
          "country",
          "region",
          "city",
          {
            "type": "long",
            "name": "countryNum"
          },
          {
            "type": "float",
            "name": "userLatitude"
          },
          {
            "type": "float",
            "name": "userLongitude"
          }
        ],
        "dimensionExclusions" : [],
        "spatialDimensions" : []
      }
    }
  },
  "metricsSpec" : [{
    "type" : "count",
    "name" : "count"
  }, {
    "type" : "doubleSum",
    "name" : "added",
    "fieldName" : "added"
  }, {
    "type" : "doubleSum",
    "name" : "deleted",
    "fieldName" : "deleted"
  }, {
    "type" : "doubleSum",
    "name" : "delta",
    "fieldName" : "delta"
  }],
  "granularitySpec" : {
    "segmentGranularity" : "DAY",
    "queryGranularity" : "NONE",
    "intervals" : [ "2013-08-31/2013-09-01" ]
  },
  "transformSpec" : null
}

----------------------------------------

TITLE: Configuring Local Index Task in Apache Druid
DESCRIPTION: Example JSON configuration for a local index task in Apache Druid. This task type is designed for smaller datasets and executes within the indexing service.

LANGUAGE: json
CODE:
{
  "type" : "index",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "wikipedia",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "timestampSpec" : {
            "column" : "timestamp",
            "format" : "auto"
          },
          "dimensionsSpec" : {
            "dimensions": ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"],
            "dimensionExclusions" : [],
            "spatialDimensions" : []
          }
        }
      },
      "metricsSpec" : [
        {
          "type" : "count",
          "name" : "count"
        },
        {
          "type" : "doubleSum",
          "name" : "added",
          "fieldName" : "added"
        },
        {
          "type" : "doubleSum",
          "name" : "deleted",
          "fieldName" : "deleted"
        },
        {
          "type" : "doubleSum",
          "name" : "delta",
          "fieldName" : "delta"
        }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "DAY",
        "queryGranularity" : "NONE",
        "intervals" : [ "2013-08-31/2013-09-01" ]
      }
    },
    "ioConfig" : {
      "type" : "index",
      "firehose" : {
        "type" : "local",
        "baseDir" : "examples/indexing/",
        "filter" : "wikipedia_data.json"
       }
    },
    "tuningConfig" : {
      "type" : "index",
      "maxRowsPerSegment" : 5000000,
      "maxRowsInMemory" : 1000000
    }
  }
}

----------------------------------------

TITLE: Configuring Historical Heap Size in Java
DESCRIPTION: Sets the heap size for a Historical node based on the number of CPU cores, with an upper limit of 24GB. This configuration helps balance memory usage and garbage collection efficiency.

LANGUAGE: java
CODE:
-Xmx$(( $(nproc) * 512 ))M -Xmx24G

----------------------------------------

TITLE: Example Segment Naming - Initial Version
DESCRIPTION: Demonstrates the naming convention for multiple segments within the same time interval using version v1. Shows how partition numbers are used when data exceeds single segment capacity.

LANGUAGE: plaintext
CODE:
foo_2015-01-01/2015-01-02_v1_0
foo_2015-01-01/2015-01-02_v1_1
foo_2015-01-01/2015-01-02_v1_2

----------------------------------------

TITLE: Executing Druid Query with Jackson Smile Format
DESCRIPTION: Example of sending a Druid query using curl with Jackson Smile content type for alternative data format support.

LANGUAGE: bash
CODE:
curl -X POST '<queryable_host>:<port>/druid/v2/?pretty' -H 'Content-Type:application/json' -H 'Accept:application/x-jackson-smile' -d @<query_json_file>

----------------------------------------

TITLE: Configuring JVM Flags for Apache Druid
DESCRIPTION: A set of recommended JVM flags for optimizing Druid performance, including timezone settings, memory management, garbage collection logging, and error handling.

LANGUAGE: java
CODE:
-Duser.timezone=UTC
-Dfile.encoding=UTF-8
-Djava.io.tmpdir=<something other than /tmp which might be mounted to volatile tmpfs file system>
-Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
-Dorg.jboss.logging.provider=slf4j
-Dnet.spy.log.LoggerImpl=net.spy.memcached.compat.log.SLF4JLogger
-Dlog4j.shutdownCallbackRegistry=org.apache.druid.common.config.Log4jShutdown
-Dlog4j.shutdownHookEnabled=true
-XX:+PrintGCDetails
-XX:+PrintGCDateStamps
-XX:+PrintGCTimeStamps
-XX:+PrintGCApplicationStoppedTime
-XX:+PrintGCApplicationConcurrentTime
-Xloggc:/var/logs/druid/historical.gc.log
-XX:+UseGCLogFileRotation
-XX:NumberOfGCLogFiles=50
-XX:GCLogFileSize=10m
-XX:+ExitOnOutOfMemoryError
-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/var/logs/druid/historical.hprof
-XX:MaxDirectMemorySize=10240g

----------------------------------------

TITLE: Configuring JVM Flags for Apache Druid
DESCRIPTION: Essential JVM configuration flags for running Apache Druid, including timezone settings, memory management, garbage collection logging, and error handling parameters. These settings help optimize performance and provide better debugging capabilities.

LANGUAGE: java
CODE:
-Duser.timezone=UTC
-Dfile.encoding=UTF-8
-Djava.io.tmpdir=<something other than /tmp which might be mounted to volatile tmpfs file system>
-Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
-Dorg.jboss.logging.provider=slf4j
-Dnet.spy.log.LoggerImpl=net.spy.memcached.compat.log.SLF4JLogger
-Dlog4j.shutdownCallbackRegistry=org.apache.druid.common.config.Log4jShutdown
-Dlog4j.shutdownHookEnabled=true
-XX:+PrintGCDetails
-XX:+PrintGCDateStamps
-XX:+PrintGCTimeStamps
-XX:+PrintGCApplicationStoppedTime
-XX:+PrintGCApplicationConcurrentTime
-Xloggc:/var/logs/druid/historical.gc.log
-XX:+UseGCLogFileRotation
-XX:NumberOfGCLogFiles=50
-XX:GCLogFileSize=10m
-XX:+ExitOnOutOfMemoryError
-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/var/logs/druid/historical.hprof
-XX:MaxDirectMemorySize=10240g

----------------------------------------

TITLE: Configuring LocalFirehose in Apache Druid
DESCRIPTION: Demonstrates how to configure a LocalFirehose to read data from files on local disk. This firehose is splittable and can be used by native parallel index tasks.

LANGUAGE: json
CODE:
{
    "type"    : "local",
    "filter"   : "*.csv",
    "baseDir"  : "/data/directory"
}

----------------------------------------

TITLE: Configuring Core Extensions in Druid Properties File
DESCRIPTION: This snippet shows how to configure the common.runtime.properties file to load core extensions in Druid. It demonstrates loading the postgresql-metadata-storage and druid-hdfs-storage extensions.

LANGUAGE: properties
CODE:
druid.extensions.loadList=["postgresql-metadata-storage", "druid-hdfs-storage"]

----------------------------------------

TITLE: Formatting JSON Data for Druid Ingestion
DESCRIPTION: Example of JSON-formatted data that can be ingested into Druid. Each line represents a separate event with timestamp and various dimensions.

LANGUAGE: json
CODE:
{
  "timestamp": "2013-08-31T01:02:33Z",
  "page": "Gypsy Danger",
  "language" : "en",
  "user" : "nuclear",
  "unpatrolled" : "true",
  "newPage" : "true",
  "robot": "false",
  "anonymous": "false",
  "namespace":"article",
  "continent":"North America",
  "country":"United States",
  "region":"Bay Area",
  "city":"San Francisco",
  "added": 57,
  "deleted": 200,
  "delta": -143
}

----------------------------------------

TITLE: Druid Ingestion Task Specification
DESCRIPTION: Configuration for ingesting data with roll-up enabled, defining data schema, metrics, and granularity specifications. Includes dimension definitions for srcIP and dstIP with minute-level query granularity.

LANGUAGE: json
CODE:
{
  "type" : "index",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "rollup-tutorial",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "dimensionsSpec" : {
            "dimensions" : [
              "srcIP",
              "dstIP"
            ]
          },
          "timestampSpec": {
            "column": "timestamp",
            "format": "iso"
          }
        }
      },
      "metricsSpec" : [
        { "type" : "count", "name" : "count" },
        { "type" : "longSum", "name" : "packets", "fieldName" : "packets" },
        { "type" : "longSum", "name" : "bytes", "fieldName" : "bytes" }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "week",
        "queryGranularity" : "minute",
        "intervals" : ["2018-01-01/2018-01-03"],
        "rollup" : true
      }
    },
    "ioConfig" : {
      "type" : "index",
      "firehose" : {
        "type" : "local",
        "baseDir" : "quickstart/tutorial",
        "filter" : "rollup-data.json"
      },
      "appendToExisting" : false
    },
    "tuningConfig" : {
      "type" : "index",
      "maxRowsPerSegment" : 5000000,
      "maxRowsInMemory" : 25000
    }
  }
}

----------------------------------------

TITLE: Querying sys.tasks Table
DESCRIPTION: SQL query to retrieve failed tasks from sys.tasks

LANGUAGE: sql
CODE:
SELECT * FROM sys.tasks WHERE status='FAILED';

----------------------------------------

TITLE: Querying sys.tasks Table
DESCRIPTION: SQL query to retrieve failed tasks from sys.tasks

LANGUAGE: sql
CODE:
SELECT * FROM sys.tasks WHERE status='FAILED';

----------------------------------------

TITLE: Parallel Index Task Configuration Example
DESCRIPTION: Example configuration for a parallel index task that processes Wikipedia data across multiple nodes

LANGUAGE: json
CODE:
{
  "type": "index_parallel",
  "spec": {
    "dataSchema": {
      "dataSource": "wikipedia_parallel_index_test",
      "metricsSpec": [
        {
          "type": "count",
              "name": "count"
            },
            {
              "type": "doubleSum",
              "name": "added",
              "fieldName": "added"
            },
            {
              "type": "doubleSum",
              "name": "deleted",
              "fieldName": "deleted"
            },
            {
              "type": "doubleSum",
              "name": "delta",
              "fieldName": "delta"
            }
        ],
        "granularitySpec": {
          "segmentGranularity": "DAY",
          "queryGranularity": "second",
          "intervals" : [ "2013-08-31/2013-09-02" ]
        },
        "parser": {
          "parseSpec": {
            "format" : "json",
            "timestampSpec": {
              "column": "timestamp"
            },
            "dimensionsSpec": {
              "dimensions": [
                "page",
                "language",
                "user",
                "unpatrolled",
                "newPage",
                "robot",
                "anonymous",
                "namespace",
                "continent",
                "country",
                "region",
                "city"
              ]
            }
          }
        }
    },
    "ioConfig": {
        "type": "index_parallel",
        "firehose": {
          "type": "local",
          "baseDir": "examples/indexing/",
          "filter": "wikipedia_index_data*"
        }
    },
    "tuningconfig": {
        "type": "index_parallel",
        "maxNumSubTasks": 2
    }
  }
}

----------------------------------------

TITLE: Bulk Lookup Configuration JSON
DESCRIPTION: Example JSON configuration for bulk updating multiple lookups across different tiers including map-based and JDBC-based lookups.

LANGUAGE: json
CODE:
{\n  "__default": {\n    "country_code": {\n      "version": "v0",\n      "lookupExtractorFactory": {\n        "type": "map",\n        "map": {\n          "77483": "United States"\n        }\n      }\n    }\n  }\n}

----------------------------------------

TITLE: Starting Druid Services
DESCRIPTION: Command to start the Druid micro-quickstart configuration which launches all required services on a single machine.

LANGUAGE: bash
CODE:
./bin/start-micro-quickstart

----------------------------------------

TITLE: Configuring Hash-based Partitioning in Apache Druid
DESCRIPTION: JSON configuration for hash-based partitioning in Apache Druid's Hadoop-based batch ingestion. Specifies target partition size or number of shards.

LANGUAGE: json
CODE:
"partitionsSpec": {
   "type": "hashed",
   "targetPartitionSize": 5000000
 }

----------------------------------------

TITLE: Sample Result of Segment Metadata Query in Apache Druid
DESCRIPTION: This JSON snippet shows the structure of a typical result from a segment metadata query. It includes segment ID, time intervals, column information, aggregators, query granularity, size, and row count.

LANGUAGE: json
CODE:
[ {
  "id" : "some_id",
  "intervals" : [ "2013-05-13T00:00:00.000Z/2013-05-14T00:00:00.000Z" ],
  "columns" : {
    "__time" : { "type" : "LONG", "hasMultipleValues" : false, "size" : 407240380, "cardinality" : null, "errorMessage" : null },
    "dim1" : { "type" : "STRING", "hasMultipleValues" : false, "size" : 100000, "cardinality" : 1944, "errorMessage" : null },
    "dim2" : { "type" : "STRING", "hasMultipleValues" : true, "size" : 100000, "cardinality" : 1504, "errorMessage" : null },
    "metric1" : { "type" : "FLOAT", "hasMultipleValues" : false, "size" : 100000, "cardinality" : null, "errorMessage" : null }
  },
  "aggregators" : {
    "metric1" : { "type" : "longSum", "name" : "metric1", "fieldName" : "metric1" }
  },
  "queryGranularity" : {
    "type": "none"
  },
  "size" : 300000,
  "numRows" : 5000000
} ]

----------------------------------------

TITLE: Submitting Kafka Supervisor Configuration
DESCRIPTION: Example supervisor specification for configuring Kafka ingestion. Includes data schema, tuning config, and Kafka connection settings.

LANGUAGE: json
CODE:
{
  "type": "kafka",
  "dataSchema": {
    "dataSource": "metrics-kafka",
    "parser": {
      "type": "string",
      "parseSpec": {
        "format": "json",
        "timestampSpec": {
          "column": "timestamp",
          "format": "auto"
        },
        "dimensionsSpec": {
          "dimensions": [],
          "dimensionExclusions": [
            "timestamp",
            "value"
          ]
        }
      }
    },
    "metricsSpec": [
      {
        "name": "count",
        "type": "count"
      },
      {
        "name": "value_sum",
        "fieldName": "value",
        "type": "doubleSum"
      },
      {
        "name": "value_min",
        "fieldName": "value",
        "type": "doubleMin"
      },
      {
        "name": "value_max",
        "fieldName": "value",
        "type": "doubleMax"
      }
    ],
    "granularitySpec": {
      "type": "uniform",
      "segmentGranularity": "HOUR",
      "queryGranularity": "NONE"
    }
  },
  "tuningConfig": {
    "type": "kafka",
    "maxRowsPerSegment": 5000000
  },
  "ioConfig": {
    "topic": "metrics",
    "consumerProperties": {
      "bootstrap.servers": "localhost:9092"
    },
    "taskCount": 1,
    "replicas": 1,
    "taskDuration": "PT1H"
  }
}

----------------------------------------

TITLE: Submit Ingestion Task - Bash
DESCRIPTION: Command to submit the transform index task to Druid using the post-index-task script.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/transform-index.json

----------------------------------------

TITLE: Configuring DBCP Properties for Druid Metadata Storage
DESCRIPTION: Example of custom DBCP connection pool properties for metadata storage. Shows how to configure connection lifetime and query timeout settings.

LANGUAGE: properties
CODE:
druid.metadata.storage.connector.dbcp.maxConnLifetimeMillis=1200000
druid.metadata.storage.connector.dbcp.defaultQueryTimeout=30000

----------------------------------------

TITLE: Configuring Basic Authorizer in JSON
DESCRIPTION: This JSON snippet shows how to enable the 'basic' authorizer implementation from the 'druid-basic-security' extension in Apache Druid.

LANGUAGE: json
CODE:
"druid.auth.authorizers":["basic"]

----------------------------------------

TITLE: Creating MySQL Database and User for Druid
DESCRIPTION: SQL commands to create a dedicated database and user for Druid with appropriate permissions. Creates a UTF8MB4-encoded database and grants necessary privileges.

LANGUAGE: sql
CODE:
-- create a druid database, make sure to use utf8mb4 as encoding
CREATE DATABASE druid DEFAULT CHARACTER SET utf8mb4;

-- create a druid user
CREATE USER 'druid'@'localhost' IDENTIFIED BY 'diurd';

-- grant the user all the permissions on the database we just created
GRANT ALL PRIVILEGES ON druid.* TO 'druid'@'localhost';

----------------------------------------

TITLE: Configuring JavaScript Extraction Function in Druid Query
DESCRIPTION: The JavaScript Extraction Function transforms dimension values using a specified JavaScript function. It includes an option to specify if the function preserves uniqueness.

LANGUAGE: JSON
CODE:
{
  "type" : "javascript",
  "function" : "function(str) { return str.substr(0, 3); }"
}

----------------------------------------

TITLE: Submit Ingestion Task - Bash
DESCRIPTION: Command to submit the transform index task to Druid using the post-index-task script.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/transform-index.json

----------------------------------------

TITLE: Configuring Derivative DataSource Supervisor in Druid
DESCRIPTION: JSON configuration for creating a derivative dataSource supervisor that maintains derived views. Specifies the base dataSource, dimensions, metrics and Hadoop tuning configuration.

LANGUAGE: json
CODE:
{
    "type": "derivativeDataSource",
    "baseDataSource": "wikiticker",
    "dimensionsSpec": {
        "dimensions": [
            "isUnpatrolled",
            "metroCode",
            "namespace",
            "page",
            "regionIsoCode",
            "regionName",
            "user"
        ]
    },
    "metricsSpec": [
        {
            "name": "count",
            "type": "count"
        },
        {
            "name": "added",
            "type": "longSum",
            "fieldName": "added"
        }
    ],
    "tuningConfig": {
        "type": "hadoop"
    }
}

----------------------------------------

TITLE: Executing Timeseries SQL Query in Druid
DESCRIPTION: This snippet demonstrates a Timeseries SQL query in Druid, which groups data by hour and calculates the sum of deleted lines for each hour.

LANGUAGE: sql
CODE:
SELECT FLOOR(__time to HOUR) AS HourTime, SUM(deleted) AS LinesDeleted FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY FLOOR(__time to HOUR);

----------------------------------------

TITLE: Demonstrating Druid Multi-Value Column Structure
DESCRIPTION: Example illustrating how Druid handles multi-value columns through modified column data arrays and bitmap entries to support multiple values per row.

LANGUAGE: plaintext
CODE:
1: Dictionary that encodes column values
  {
    "Justin Bieber": 0,
    "Ke$ha":         1
  }

2: Column data
  [0,
   [0,1],  <--Row value of multi-value column can have array of values
   1,
   1]

3: Bitmaps - one for each unique value
  value="Justin Bieber": [1,1,0,0]
  value="Ke$ha":         [0,1,1,1]

----------------------------------------

TITLE: Retrieving Task Status in JSON Format
DESCRIPTION: Example JSON response from the /druid/indexer/v1/task/{taskId}/status endpoint to get the status of a task.

LANGUAGE: json
CODE:
{
  "task": "index_kafka_wikiticker_f7011f8ffba384b_fpeclode"
}

----------------------------------------

TITLE: Configuring Parquet Parser with TimeAndDims ParseSpec in Druid
DESCRIPTION: This snippet shows how to configure the Parquet parser with a timeAndDims parseSpec in a Druid ingestion spec. It includes settings for input format, timestampSpec, and dimensionsSpec.

LANGUAGE: json
CODE:
{
  "type": "index_hadoop",
  "spec": {
    "ioConfig": {
      "type": "hadoop",
      "inputSpec": {
        "type": "static",
        "inputFormat": "org.apache.druid.data.input.parquet.DruidParquetInputFormat",
        "paths": "path/to/file.parquet"
      },
      ...
    },
    "dataSchema": {
      "dataSource": "example",
      "parser": {
        "type": "parquet",
        "parseSpec": {
          "format": "timeAndDims",
          "timestampSpec": {
            "column": "timestamp",
            "format": "auto"
          },
          "dimensionsSpec": {
            "dimensions": [
              "dim1",
              "dim2",
              "dim3",
              "listDim"
            ],
            "dimensionExclusions": [],
            "spatialDimensions": []
          }
        }
      },
      ...
    },
    "tuningConfig": <hadoop-tuning-config>
  }
}

----------------------------------------

TITLE: Druid Configuration File Organization
DESCRIPTION: Example of recommended file organization structure for Druid configuration files

LANGUAGE: bash
CODE:
$ ls -R conf
druid       tranquility

conf/druid:
_common       broker        coordinator   historical    middleManager overlord

conf/druid/_common:
common.runtime.properties log4j2.xml

conf/druid/broker:
jvm.config         runtime.properties

conf/druid/coordinator:
jvm.config         runtime.properties

conf/druid/historical:
jvm.config         runtime.properties

conf/druid/middleManager:
jvm.config         runtime.properties

conf/druid/overlord:
jvm.config         runtime.properties

conf/tranquility:
kafka.json  server.json

----------------------------------------

TITLE: Calculating Rollup Ratio with Druid SQL
DESCRIPTION: SQL query to measure the rollup ratio of a Druid datasource by comparing the number of rows to the number of ingested events.

LANGUAGE: sql
CODE:
SELECT SUM("event_count") / COUNT(*) * 1.0 FROM datasource

----------------------------------------

TITLE: Querying Data Source Metadata in Druid
DESCRIPTION: Query structure for retrieving metadata information about a datasource, including the timestamp of the latest ingested event. Requires specifying the queryType as 'dataSourceMetadata' and the target datasource name.

LANGUAGE: json
CODE:
{
    "queryType" : "dataSourceMetadata",
    "dataSource": "sample_datasource"
}

----------------------------------------

TITLE: Druid Configuration File Organization
DESCRIPTION: Example of recommended file organization structure for Druid configuration files

LANGUAGE: bash
CODE:
$ ls -R conf
druid       tranquility

conf/druid:
_common       broker        coordinator   historical    middleManager overlord

conf/druid/_common:
common.runtime.properties log4j2.xml

conf/druid/broker:
jvm.config         runtime.properties

conf/druid/coordinator:
jvm.config         runtime.properties

conf/druid/historical:
jvm.config         runtime.properties

conf/druid/middleManager:
jvm.config         runtime.properties

conf/druid/overlord:
jvm.config         runtime.properties

conf/tranquility:
kafka.json  server.json

----------------------------------------

TITLE: Displaying Druid Segment Naming Convention
DESCRIPTION: This snippet shows examples of Druid segment names, illustrating how segments are uniquely identified using datasource, interval, version, and partition number.

LANGUAGE: markdown
CODE:
```
foo_2015-01-01/2015-01-02_v1_0
foo_2015-01-01/2015-01-02_v1_1
foo_2015-01-01/2015-01-02_v1_2
```

----------------------------------------

TITLE: Configuring Numbered Sharding in Druid Realtime Spec
DESCRIPTION: This JSON snippet demonstrates the configuration for numbered sharding in the Druid Realtime spec. Numbered sharding requires sequential partition numbering and explicit setting of the total number of partitions.

LANGUAGE: json
CODE:
"shardSpec": {
    "type": "numbered",
    "partitionNum": 0,
    "partitions": 2
}

----------------------------------------

TITLE: Displaying Druid Segment Naming Convention
DESCRIPTION: This snippet shows examples of Druid segment names, illustrating how segments are uniquely identified using datasource, interval, version, and partition number.

LANGUAGE: markdown
CODE:
```
foo_2015-01-01/2015-01-02_v1_0
foo_2015-01-01/2015-01-02_v1_1
foo_2015-01-01/2015-01-02_v1_2
```

----------------------------------------

TITLE: Configuring Parallel Index Task in Apache Druid
DESCRIPTION: Example JSON configuration for a Parallel Index Task, which runs tasks in parallel on multiple MiddleManager processes. This task is suitable for larger datasets and supports splittable firehoses.

LANGUAGE: json
CODE:
{
  "type": "index_parallel",
  "spec": {
    "dataSchema": {
      "dataSource": "wikipedia_parallel_index_test",
      "metricsSpec": [
        {
          "type": "count",
              "name": "count"
            },
            {
              "type": "doubleSum",
              "name": "added",
              "fieldName": "added"
            },
            {
              "type": "doubleSum",
              "name": "deleted",
              "fieldName": "deleted"
            },
            {
              "type": "doubleSum",
              "name": "delta",
              "fieldName": "delta"
            }
        ],
        "granularitySpec": {
          "segmentGranularity": "DAY",
          "queryGranularity": "second",
          "intervals" : [ "2013-08-31/2013-09-02" ]
        },
        "parser": {
          "parseSpec": {
            "format" : "json",
            "timestampSpec": {
              "column": "timestamp"
            },
            "dimensionsSpec": {
              "dimensions": [
                "page",
                "language",
                "user",
                "unpatrolled",
                "newPage",
                "robot",
                "anonymous",
                "namespace",
                "continent",
                "country",
                "region",
                "city"
              ]
            }
          }
        }
    },
    "ioConfig": {
        "type": "index_parallel",
        "firehose": {
          "type": "local",
          "baseDir": "examples/indexing/",
          "filter": "wikipedia_index_data*"
        }
    },
    "tuningconfig": {
        "type": "index_parallel",
        "maxNumSubTasks": 2
    }
  }
}

----------------------------------------

TITLE: Druid Ingestion Spec with Transform and Filter
DESCRIPTION: This ingestion specification includes transform specs to modify the 'animal' column, create a new 'triple-number' column, and apply a filter to exclude certain rows during ingestion.

LANGUAGE: json
CODE:
{
  "type": "index",
  "spec": {
    "dataSchema": {
      "dataSource": "transform-tutorial",
      "parser": {
        "type": "string",
        "parseSpec": {
          "format": "json",
          "dimensionsSpec": {
            "dimensions": [
              "animal",
              { "name": "location", "type": "long" }
            ]
          },
          "timestampSpec": {
            "column": "timestamp",
            "format": "iso"
          }
        }
      },
      "metricsSpec": [
        { "type": "count", "name": "count" },
        { "type": "longSum", "name": "number", "fieldName": "number" },
        { "type": "longSum", "name": "triple-number", "fieldName": "triple-number" }
      ],
      "granularitySpec": {
        "type": "uniform",
        "segmentGranularity": "week",
        "queryGranularity": "minute",
        "intervals": ["2018-01-01/2018-01-03"],
        "rollup": true
      },
      "transformSpec": {
        "transforms": [
          {
            "type": "expression",
            "name": "animal",
            "expression": "concat('super-', animal)"
          },
          {
            "type": "expression",
            "name": "triple-number",
            "expression": "number * 3"
          }
        ],
        "filter": {
          "type":"or",
          "fields": [
            { "type": "selector", "dimension": "animal", "value": "super-mongoose" },
            { "type": "selector", "dimension": "triple-number", "value": "300" },
            { "type": "selector", "dimension": "location", "value": "3" }
          ]
        }
      }
    },
    "ioConfig": {
      "type": "index",
      "firehose": {
        "type": "local",
        "baseDir": "quickstart/tutorial",
        "filter": "transform-data.json"
      },
      "appendToExisting": false
    },
    "tuningConfig": {
      "type": "index",
      "maxRowsPerSegment": 5000000,
      "maxRowsInMemory": 25000,
      "forceExtendableShardSpecs": true
    }
  }
}

----------------------------------------

TITLE: Configuring Batch Ingestion for Wikipedia Data in Apache Druid
DESCRIPTION: This JSON specification defines a batch ingestion task for loading Wikipedia page edit data into Apache Druid. It specifies the data source, parser configuration, granularity settings, and input file location.

LANGUAGE: json
CODE:
{
  "type" : "index",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "wikipedia",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "dimensionsSpec" : {
            "dimensions" : [
              "channel",
              "cityName",
              "comment",
              "countryIsoCode",
              "countryName",
              "isAnonymous",
              "isMinor",
              "isNew",
              "isRobot",
              "isUnpatrolled",
              "metroCode",
              "namespace",
              "page",
              "regionIsoCode",
              "regionName",
              "user",
              { "name": "added", "type": "long" },
              { "name": "deleted", "type": "long" },
              { "name": "delta", "type": "long" }
            ]
          },
          "timestampSpec": {
            "column": "time",
            "format": "iso"
          }
        }
      },
      "metricsSpec" : [],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "day",
        "queryGranularity" : "none",
        "intervals" : ["2015-09-12/2015-09-13"],
        "rollup" : false
      }
    },
    "ioConfig" : {
      "type" : "index",
      "firehose" : {
        "type" : "local",
        "baseDir" : "quickstart/tutorial/",
        "filter" : "wikiticker-2015-09-12-sampled.json.gz"
      },
      "appendToExisting" : false
    },
    "tuningConfig" : {
      "type" : "index",
      "maxRowsPerSegment" : 5000000,
      "maxRowsInMemory" : 25000
    }
  }
}

----------------------------------------

TITLE: Configuring Protobuf Parser in Druid JSON
DESCRIPTION: This JSON snippet shows the configuration options for the Protobuf parser in Druid. It specifies required fields like the descriptor file location and Protobuf message type.

LANGUAGE: json
CODE:
{
  "type": "protobuf",
  "descriptor": "String",
  "protoMessageType": "String",
  "parseSpec": {
    "format": "json",
    "timestampSpec": { },
    "dimensionsSpec": { }
  }
}

----------------------------------------

TITLE: Starting Druid Services
DESCRIPTION: Command to start all Druid services using the supervision script with the tutorial configuration.

LANGUAGE: bash
CODE:
bin/supervise -c quickstart/tutorial/conf/tutorial-cluster.conf

----------------------------------------

TITLE: Configuring Realtime Process SpecFile in JSON for Apache Druid
DESCRIPTION: Example JSON configuration for a Realtime process specFile in Apache Druid. This configuration defines the dataSchema, ioConfig, and tuningConfig for ingesting Wikipedia data from a Kafka stream.

LANGUAGE: json
CODE:
[
  {
    "dataSchema" : {
      "dataSource" : "wikipedia",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "timestampSpec" : {
            "column" : "timestamp",
            "format" : "auto"
          },
          "dimensionsSpec" : {
            "dimensions": ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"],
            "dimensionExclusions" : [],
            "spatialDimensions" : []
          }
        }
      },
      "metricsSpec" : [{
        "type" : "count",
        "name" : "count"
      }, {
        "type" : "doubleSum",
        "name" : "added",
        "fieldName" : "added"
      }, {
        "type" : "doubleSum",
        "name" : "deleted",
        "fieldName" : "deleted"
      }, {
        "type" : "doubleSum",
        "name" : "delta",
        "fieldName" : "delta"
      }],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "DAY",
        "queryGranularity" : "NONE"
      }
    },
    "ioConfig" : {
      "type" : "realtime",
      "firehose": {
        "type": "kafka-0.8",
        "consumerProps": {
          "zookeeper.connect": "localhost:2181",
          "zookeeper.connection.timeout.ms" : "15000",
          "zookeeper.session.timeout.ms" : "15000",
          "zookeeper.sync.time.ms" : "5000",
          "group.id": "druid-example",
          "fetch.message.max.bytes" : "1048586",
          "auto.offset.reset": "largest",
          "auto.commit.enable": "false"
        },
        "feed": "wikipedia"
      },
      "plumber": {
        "type": "realtime"
      }
    },
    "tuningConfig": {
      "type" : "realtime",
      "maxRowsInMemory": 1000000,
      "intermediatePersistPeriod": "PT10M",
      "windowPeriod": "PT10M",
      "basePersistDirectory": "\/tmp\/realtime\/basePersist",
      "rejectionPolicy": {
        "type": "serverTime"
      }
    }
  }
]

----------------------------------------

TITLE: Configuring JSON Flatten Spec in Druid ParseSpec
DESCRIPTION: This snippet demonstrates how to configure the JSON Flatten Spec within a Druid parseSpec. It includes examples of different field types (root, path, jq) and shows how to use JsonPath and jackson-jq expressions to access nested fields.

LANGUAGE: json
CODE:
"parseSpec": {
  "format": "json",
  "flattenSpec": {
    "useFieldDiscovery": true,
    "fields": [
      {
        "type": "root",
        "name": "dim1"
      },
      "dim2",
      {
        "type": "path",
        "name": "foo.bar",
        "expr": "$.foo.bar"
      },
      {
        "type": "root",
        "name": "foo.bar"
      },
      {
        "type": "path",
        "name": "path-metric",
        "expr": "$.nestmet.val"
      },
      {
        "type": "path",
        "name": "hello-0",
        "expr": "$.hello[0]"
      },
      {
        "type": "path",
        "name": "hello-4",
        "expr": "$.hello[4]"
      },
      {
        "type": "path",
        "name": "world-hey",
        "expr": "$.world[0].hey"
      },
      {
        "type": "path",
        "name": "worldtree",
        "expr": "$.world[1].tree"
      },
      {
        "type": "path",
        "name": "first-food",
        "expr": "$.thing.food[0]"
      },
      {
        "type": "path",
        "name": "second-food",
        "expr": "$.thing.food[1]"
      },
      {
        "type": "jq",
        "name": "first-food-by-jq",
        "expr": ".thing.food[1]"
      },
      {
        "type": "jq",
        "name": "hello-total",
        "expr": ".hello | sum"
      }
    ]
  },
  "dimensionsSpec" : {
   "dimensions" : [],
   "dimensionsExclusions": ["ignore_me"]
  },
  "timestampSpec" : {
   "format" : "auto",
   "column" : "timestamp"
  }
}

----------------------------------------

TITLE: Example Query: Unique Users for Product A
DESCRIPTION: Druid query to calculate the number of unique users who visited product A using Theta Sketch aggregator.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "test_datasource",
  "granularity": "ALL",
  "dimensions": [],
  "aggregations": [
    { "type": "thetaSketch", "name": "unique_users", "fieldName": "user_id_sketch" }
  ],
  "filter": { "type": "selector", "dimension": "product", "value": "A" },
  "intervals": [ "2014-10-19T00:00:00.000Z/2014-10-22T00:00:00.000Z" ]
}

----------------------------------------

TITLE: Implementing a Regular Expression Filter in Druid JSON
DESCRIPTION: Example of a regular expression filter that matches a dimension against a pattern using Java regular expressions.

LANGUAGE: json
CODE:
"filter": { "type": "regex", "dimension": <dimension_string>, "pattern": <pattern_string> }

----------------------------------------

TITLE: Configuring JVM Garbage Collection for Druid
DESCRIPTION: Sets the JVM to use the G1 Garbage Collector and enables termination on out-of-memory errors. These are typically set as JVM arguments.

LANGUAGE: java
CODE:
-XX:+UseG1GC -XX:+ExitOnOutOfMemoryError

----------------------------------------

TITLE: Basic Druid Ingestion Spec Structure
DESCRIPTION: The core structure of a Druid ingestion specification showing the three main components: dataSchema, ioConfig, and tuningConfig.

LANGUAGE: json
CODE:
{
  "dataSchema" : {...},
  "ioConfig" : {...},
  "tuningConfig" : {...}
}

----------------------------------------

TITLE: Configuring Environment Variable Password Provider in Apache Druid
DESCRIPTION: This snippet demonstrates how to configure an environment variable-based password provider in Apache Druid. It specifies the provider type and the environment variable name to retrieve the password from.

LANGUAGE: json
CODE:
{ "type": "environment", "variable": "METADATA_STORAGE_PASSWORD" }

----------------------------------------

TITLE: Configuring DimensionsSpec in Druid Ingestion Spec
DESCRIPTION: This example shows how to configure the dimensionsSpec within a dataSchema, including specifying different column types and disabling bitmap indexing for a specific column.

LANGUAGE: json
CODE:
"dimensionsSpec" : {
  "dimensions": [
    "page",
    "language",
    "user",
    "unpatrolled",
    "newPage",
    "robot",
    "anonymous",
    "namespace",
    "continent",
    "country",
    "region",
    "city",
    {
      "type": "string",
      "name": "comment",
      "createBitmapIndex": false
    },
    {
      "type": "long",
      "name": "countryNum"
    },
    {
      "type": "float",
      "name": "userLatitude"
    },
    {
      "type": "float",
      "name": "userLongitude"
    }
  ],
  "dimensionExclusions" : [],
  "spatialDimensions" : []
}

----------------------------------------

TITLE: Applying Query Filter in groupBy Query (JSON)
DESCRIPTION: Demonstrates how to use a query filter HavingSpec in a groupBy query. This allows any Druid query filter to be used in the Having part of the query.

LANGUAGE: json
CODE:
{
    "queryType": "groupBy",
    "dataSource": "sample_datasource",
    ...
    "having": 
        {
            "type" : "filter",
            "filter" : <any Druid query filter>
        }
}

----------------------------------------

TITLE: Logical OR Having Specification in Druid
DESCRIPTION: Shows how to combine multiple having conditions using the OR logical operator.

LANGUAGE: json
CODE:
{
    "queryType": "groupBy",
    "dataSource": "sample_datasource",
    ...
    "having": 
        {
            "type": "or",
            "havingSpecs": [        
                {
                    "type": "greaterThan",
                    "aggregation": "<aggregate_metric>",
                    "value": <numeric_value>
                },
                {
                    "type": "equalTo",
                    "aggregation": "<aggregate_metric>",
                    "value": <numeric_value>
                }
            ]
        }
}

----------------------------------------

TITLE: Configuring Interval Load Rule in Apache Druid
DESCRIPTION: This JSON snippet defines an Interval Load Rule, which specifies how many replicas of a segment should exist in different server tiers for a specific time interval.

LANGUAGE: json
CODE:
{
  "type" : "loadByInterval",
  "interval": "2012-01-01/2013-01-01",
  "tieredReplicants": {
    "hot": 1,
    "_default_tier" : 1
  }
}

----------------------------------------

TITLE: Filtering for Null Values in Multi-value Dimensions in Druid
DESCRIPTION: This JSON snippet demonstrates a selector filter that matches rows where the 'tags' dimension is null or empty. It would match row 4 in the example dataset.

LANGUAGE: json
CODE:
{
  "type": "selector",
  "dimension": "tags",
  "value": null
}

----------------------------------------

TITLE: Running Druid Broker Process in Java
DESCRIPTION: Command to start the Druid Broker process using the Main class. This initializes the Broker server, which is responsible for routing queries in a distributed Druid cluster.

LANGUAGE: java
CODE:
org.apache.druid.cli.Main server broker

----------------------------------------

TITLE: Defining Table Datasource in Apache Druid JSON
DESCRIPTION: This snippet shows the JSON structure for defining a table datasource in Apache Druid. Table datasources are the most common type and can be represented by a simple string or a full structure specifying the type and name.

LANGUAGE: json
CODE:
{
	"type": "table",
	"name": "<string_value>"
}

----------------------------------------

TITLE: Starting Druid Services
DESCRIPTION: Command to start all Druid services using the supervision script with the tutorial configuration.

LANGUAGE: bash
CODE:
bin/supervise -c quickstart/tutorial/conf/tutorial-cluster.conf

----------------------------------------

TITLE: Complete Druid Ingestion Specification
DESCRIPTION: Full ingestion specification defining the schema, parser, metrics, granularity settings and input source configuration for batch ingestion of network flow data

LANGUAGE: json
CODE:
{
  "type" : "index",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "ingestion-tutorial",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "timestampSpec" : {
            "format" : "iso",
            "column" : "ts"
          },
          "dimensionsSpec" : {
            "dimensions": [
              "srcIP",
              { "name" : "srcPort", "type" : "long" },
              { "name" : "dstIP", "type" : "string" },
              { "name" : "dstPort", "type" : "long" },
              { "name" : "protocol", "type" : "string" }
            ]
          }      
        }
      },
      "metricsSpec" : [
        { "type" : "count", "name" : "count" },
        { "type" : "longSum", "name" : "packets", "fieldName" : "packets" },
        { "type" : "longSum", "name" : "bytes", "fieldName" : "bytes" },
        { "type" : "doubleSum", "name" : "cost", "fieldName" : "cost" }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "HOUR",
        "queryGranularity" : "MINUTE",
        "intervals" : ["2018-01-01/2018-01-02"],
        "rollup" : true
      }
    },
    "ioConfig" : {
      "type" : "index",
      "firehose" : {
        "type" : "local",
        "baseDir" : "quickstart/",
        "filter" : "ingestion-tutorial-data.json"
      }
    },
    "tuningConfig" : {
      "type" : "index",
      "maxRowsPerSegment" : 5000000
    }
  }
}

----------------------------------------

TITLE: Building Hadoop Docker Image for Druid
DESCRIPTION: Commands to build a Docker image for a Hadoop 2.8.3 cluster, used for running batch indexing tasks in Druid.

LANGUAGE: bash
CODE:
cd quickstart/tutorial/hadoop/docker
docker build -t druid-hadoop-demo:2.8.3 .

----------------------------------------

TITLE: Configuring Basic Authenticator in Druid
DESCRIPTION: Configuration properties for setting up the basic authenticator with initial admin and internal client passwords.

LANGUAGE: properties
CODE:
druid.auth.authenticatorChain=["MyBasicAuthenticator"]

druid.auth.authenticator.MyBasicAuthenticator.type=basic
druid.auth.authenticator.MyBasicAuthenticator.initialAdminPassword=password1
druid.auth.authenticator.MyBasicAuthenticator.initialInternalClientPassword=password2
druid.auth.authenticator.MyBasicAuthenticator.authorizerName=MyBasicAuthorizer

----------------------------------------

TITLE: Configuring Local Index Task in Apache Druid
DESCRIPTION: Example JSON configuration for a Local Index Task, which is designed for smaller datasets and executes within the indexing service.

LANGUAGE: json
CODE:
{
  "type" : "index",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "wikipedia",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "timestampSpec" : {
            "column" : "timestamp",
            "format" : "auto"
          },
          "dimensionsSpec" : {
            "dimensions": ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"],
            "dimensionExclusions" : [],
            "spatialDimensions" : []
          }
        }
      },
      "metricsSpec" : [
        {
          "type" : "count",
          "name" : "count"
        },
        {
          "type" : "doubleSum",
          "name" : "added",
          "fieldName" : "added"
        },
        {
          "type" : "doubleSum",
          "name" : "deleted",
          "fieldName" : "deleted"
        },
        {
          "type" : "doubleSum",
          "name" : "delta",
          "fieldName" : "delta"
        }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "DAY",
        "queryGranularity" : "NONE",
        "intervals" : [ "2013-08-31/2013-09-01" ]
      }
    },
    "ioConfig" : {
      "type" : "index",
      "firehose" : {
        "type" : "local",
        "baseDir" : "examples/indexing/",
        "filter" : "wikipedia_data.json"
       }
    },
    "tuningConfig" : {
      "type" : "index",
      "maxRowsPerSegment" : 5000000,
      "maxRowsInMemory" : 1000000
    }
  }
}

----------------------------------------

TITLE: Configuring Kafka Emitter in Apache Druid YAML
DESCRIPTION: Example YAML configuration for setting up the Kafka Emitter in Apache Druid. This snippet demonstrates how to specify Kafka bootstrap servers, topic names for metrics and alerts, additional producer configuration, and an optional cluster name.

LANGUAGE: yaml
CODE:
druid.emitter.kafka.bootstrap.servers=hostname1:9092,hostname2:9092
druid.emitter.kafka.metric.topic=druid-metric
druid.emitter.kafka.alert.topic=druid-alert
druid.emitter.kafka.producer.config={"max.block.ms":10000}

----------------------------------------

TITLE: Configuring Kerberos Authenticator in Apache Druid
DESCRIPTION: YAML configuration for adding a Kerberos authenticator to the Druid authenticator chain. This snippet shows how to declare a Kerberos authenticator named 'MyKerberosAuthenticator'.

LANGUAGE: yaml
CODE:
druid.auth.authenticatorChain=["MyKerberosAuthenticator"]

druid.auth.authenticator.MyKerberosAuthenticator.type=kerberos

----------------------------------------

TITLE: Configuring Avro Stream Parser with Schema Repo Decoder in Druid
DESCRIPTION: This JSON snippet demonstrates how to configure the Avro stream parser with a schema repo Avro bytes decoder. It includes settings for the parser type, bytes decoder, and parseSpec with timestamp and dimensions specifications.

LANGUAGE: json
CODE:
{
  "type" : "avro_stream",
  "avroBytesDecoder" : {
    "type" : "schema_repo",
    "subjectAndIdConverter" : {
      "type" : "avro_1124",
      "topic" : "${YOUR_TOPIC}"
    },
    "schemaRepository" : {
      "type" : "avro_1124_rest_client",
      "url" : "${YOUR_SCHEMA_REPO_END_POINT}",
    }
  },
  "parseSpec" : {
    "format": "avro",
    "timestampSpec": <standard timestampSpec>,
    "dimensionsSpec": <standard dimensionsSpec>,
    "flattenSpec": <optional>
  }
}

----------------------------------------

TITLE: Timeseries Query Result Format in Apache Druid (JSON)
DESCRIPTION: This snippet shows the expected format of the results returned by a timeseries query in Apache Druid. Each result object contains a timestamp and a result object with the calculated aggregations and post-aggregations.

LANGUAGE: json
CODE:
[
  {
    "timestamp": "2012-01-01T00:00:00.000Z",
    "result": { "sample_name1": <some_value>, "sample_name2": <some_value>, "sample_divide": <some_value> } 
  },
  {
    "timestamp": "2012-01-02T00:00:00.000Z",
    "result": { "sample_name1": <some_value>, "sample_name2": <some_value>, "sample_divide": <some_value> }
  }
]

----------------------------------------

TITLE: Timestamp Processing Expression Examples
DESCRIPTION: Examples of timestamp manipulation functions showing parsing, formatting and arithmetic operations on datetime values

LANGUAGE: expression
CODE:
timestamp(expr[,format-string])
timestamp_ceil(expr, period, [origin, [timezone]])
timestamp_floor(expr, period, [origin, [timezone]])

----------------------------------------

TITLE: Druid Router Runtime Properties Configuration
DESCRIPTION: Runtime configuration properties for Druid Router including service settings, connection parameters, and thread configurations.

LANGUAGE: plaintext
CODE:
druid.host=#{IP_ADDR}:8080
druid.plaintextPort=8080
druid.service=druid/router

druid.router.defaultBrokerServiceName=druid:broker-cold
druid.router.coordinatorServiceName=druid:coordinator
druid.router.tierToBrokerMap={"hot":"druid:broker-hot","_default_tier":"druid:broker-cold"}
druid.router.http.numConnections=50
druid.router.http.readTimeout=PT5M

# Number of threads used by the Router proxy http client
druid.router.http.numMaxThreads=100

druid.server.http.numThreads=100

----------------------------------------

TITLE: Implementing StringFirst Aggregator in Druid
DESCRIPTION: JSON configuration for the stringFirst aggregator in Druid, which computes the metric value with the minimum timestamp or null if no row exists.

LANGUAGE: json
CODE:
{
  "type" : "stringFirst",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "maxStringBytes" : <integer> # (optional, defaults to 1024),
  "filterNullValues" : <boolean> # (optional, defaults to false)
}

----------------------------------------

TITLE: Configuring Avro Stream Parser with Schema Repo Decoder in Druid
DESCRIPTION: This JSON snippet demonstrates how to configure the Avro stream parser with a schema repo Avro bytes decoder. It includes settings for the parser type, bytes decoder, and parseSpec with timestamp and dimensions specifications.

LANGUAGE: json
CODE:
{
  "type" : "avro_stream",
  "avroBytesDecoder" : {
    "type" : "schema_repo",
    "subjectAndIdConverter" : {
      "type" : "avro_1124",
      "topic" : "${YOUR_TOPIC}"
    },
    "schemaRepository" : {
      "type" : "avro_1124_rest_client",
      "url" : "${YOUR_SCHEMA_REPO_END_POINT}",
    }
  },
  "parseSpec" : {
    "format": "avro",
    "timestampSpec": <standard timestampSpec>,
    "dimensionsSpec": <standard dimensionsSpec>,
    "flattenSpec": <optional>
  }
}

----------------------------------------

TITLE: Configuring Kill Task for Data Deletion in Apache Druid
DESCRIPTION: This JSON configuration defines a Kill Task in Apache Druid. Kill Tasks are used to delete all information about a segment and remove it from deep storage. The task specifies the data source, interval for deletion, and optional context.

LANGUAGE: json
CODE:
{
    "type": "kill",
    "id": <task_id>,
    "dataSource": <task_datasource>,
    "interval" : <all_segments_in_this_interval_will_die!>,
    "context": <task context>
}

----------------------------------------

TITLE: Configuring Authentication Chain in Apache Druid
DESCRIPTION: This JSON snippet shows how to configure the authentication chain in Druid to enable Kerberos and HTTP Basic authenticators from the druid-kerberos and druid-basic-security core extensions.

LANGUAGE: json
CODE:
"druid.auth.authenticatorChain":["kerberos", "basic"]

----------------------------------------

TITLE: Configuring DataSchema in Druid Ingestion Spec
DESCRIPTION: This example demonstrates how to configure the dataSchema component of a Druid ingestion spec, including parser, metricsSpec, and granularitySpec settings for a Wikipedia dataset.

LANGUAGE: json
CODE:
"dataSchema" : {
  "dataSource" : "wikipedia",
  "parser" : {
    "type" : "string",
    "parseSpec" : {
      "format" : "json",
      "timestampSpec" : {
        "column" : "timestamp",
        "format" : "auto"
      },
      "dimensionsSpec" : {
        "dimensions": [
          "page",
          "language",
          "user",
          "unpatrolled",
          "newPage",
          "robot",
          "anonymous",
          "namespace",
          "continent",
          "country",
          "region",
          "city",
          {
            "type": "long",
            "name": "countryNum"
          },
          {
            "type": "float",
            "name": "userLatitude"
          },
          {
            "type": "float",
            "name": "userLongitude"
          }
        ],
        "dimensionExclusions" : [],
        "spatialDimensions" : []
      }
    }
  },
  "metricsSpec" : [{
    "type" : "count",
    "name" : "count"
  }, {
    "type" : "doubleSum",
    "name" : "added",
    "fieldName" : "added"
  }, {
    "type" : "doubleSum",
    "name" : "deleted",
    "fieldName" : "deleted"
  }, {
    "type" : "doubleSum",
    "name" : "delta",
    "fieldName" : "delta"
  }],
  "granularitySpec" : {
    "segmentGranularity" : "DAY",
    "queryGranularity" : "NONE",
    "intervals" : [ "2013-08-31/2013-09-01" ]
  },
  "transformSpec" : null
}

----------------------------------------

TITLE: Configuring Avro Stream Parser with Schema Repo in Druid
DESCRIPTION: This snippet demonstrates how to set up an Avro stream parser using a schema repository for decoding Avro bytes. It includes configuration for the parser type, bytes decoder, and parse specification.

LANGUAGE: json
CODE:
"parser" : {
  "type" : "avro_stream",
  "avroBytesDecoder" : {
    "type" : "schema_repo",
    "subjectAndIdConverter" : {
      "type" : "avro_1124",
      "topic" : "${YOUR_TOPIC}"
    },
    "schemaRepository" : {
      "type" : "avro_1124_rest_client",
      "url" : "${YOUR_SCHEMA_REPO_END_POINT}",
    }
  },
  "parseSpec" : {
    "format": "avro",
    "timestampSpec": <standard timestampSpec>,
    "dimensionsSpec": <standard dimensionsSpec>,
    "flattenSpec": <optional>
  }
}

----------------------------------------

TITLE: Defining HyperUnique Cardinality Post-Aggregator in Druid JSON Query
DESCRIPTION: Shows the syntax for a HyperUnique Cardinality post-aggregator in a Druid query. This post-aggregator wraps a hyperUnique object for use in post-aggregations.

LANGUAGE: json
CODE:
{
  "type"  : "hyperUniqueCardinality",
  "name": <output name>,
  "fieldName"  : <the name field value of the hyperUnique aggregator>
}

----------------------------------------

TITLE: Configuring Basic Log4j2 Setup for Druid
DESCRIPTION: Basic log4j2 configuration for Druid that sets up console logging with timestamp patterns and info-level logging. Includes optional HTTP request logging configuration that is commented out by default.

LANGUAGE: xml
CODE:
<?xml version="1.0" encoding="UTF-8" ?>
<Configuration status="WARN">
  <Appenders>
    <Console name="Console" target="SYSTEM_OUT">
      <PatternLayout pattern="%d{ISO8601} %p [%t] %c - %m%n"/>
    </Console>
  </Appenders>
  <Loggers>
    <Root level="info">
      <AppenderRef ref="Console"/>
    </Root>

    <!-- Uncomment to enable logging of all HTTP requests
    <Logger name="org.apache.druid.jetty.RequestLog" additivity="false" level="DEBUG">
        <AppenderRef ref="Console"/>
    </Logger>
    -->
  </Loggers>
</Configuration>

----------------------------------------

TITLE: Install Tranquility Server
DESCRIPTION: Commands to download and install Tranquility Server for stream ingestion

LANGUAGE: bash
CODE:
curl http://static.druid.io/tranquility/releases/tranquility-distribution-0.8.3.tgz -o tranquility-distribution-0.8.3.tgz
tar -xzf tranquility-distribution-0.8.3.tgz
mv tranquility-distribution-0.8.3 tranquility

----------------------------------------

TITLE: Example TopN Query with Variance and StdDev
DESCRIPTION: Complete example of a TopN query using both variance aggregator and standard deviation post-aggregator.

LANGUAGE: json
CODE:
{
  "queryType": "topN",
  "dataSource": "testing",
  "dimensions": ["alias"],
  "threshold": 5,
  "granularity": "all",
  "aggregations": [
    {
      "type": "variance",
      "name": "index_var",
      "fieldName": "index"
    }
  ],
  "postAggregations": [
    {
      "type": "stddev",
      "name": "index_stddev",
      "fieldName": "index_var"
    }
  ],
  "intervals": [
    "2016-03-06T00:00:00/2016-03-06T23:59:59"
  ]
}

----------------------------------------

TITLE: Ingesting Network Flow Data with Roll-up in Apache Druid
DESCRIPTION: This JSON configuration specifies an ingestion task for network flow data with roll-up enabled. It defines dimensions, metrics, and granularity settings for the roll-up process.

LANGUAGE: json
CODE:
{
  "type" : "index",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "rollup-tutorial",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "dimensionsSpec" : {
            "dimensions" : [
              "srcIP",
              "dstIP"
            ]
          },
          "timestampSpec": {
            "column": "timestamp",
            "format": "iso"
          }
        }
      },
      "metricsSpec" : [
        { "type" : "count", "name" : "count" },
        { "type" : "longSum", "name" : "packets", "fieldName" : "packets" },
        { "type" : "longSum", "name" : "bytes", "fieldName" : "bytes" }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "week",
        "queryGranularity" : "minute",
        "intervals" : ["2018-01-01/2018-01-03"],
        "rollup" : true
      }
    },
    "ioConfig" : {
      "type" : "index",
      "firehose" : {
        "type" : "local",
        "baseDir" : "quickstart/tutorial",
        "filter" : "rollup-data.json"
      },
      "appendToExisting" : false
    },
    "tuningConfig" : {
      "type" : "index",
      "maxRowsPerSegment" : 5000000,
      "maxRowsInMemory" : 25000,
      "forceExtendableShardSpecs" : true
    }
  }
}

----------------------------------------

TITLE: Demonstrating Multi-Interval Segment Updates in Druid
DESCRIPTION: This snippet illustrates how segments spanning multiple intervals are named and how updates across intervals are not atomic.

LANGUAGE: plaintext
CODE:
foo_2015-01-01/2015-01-02_v1_0
foo_2015-01-02/2015-01-03_v1_1
foo_2015-01-03/2015-01-04_v1_2

----------------------------------------

TITLE: Implementing Kafka Producer for Protobuf Messages in Python
DESCRIPTION: This Python script reads JSON data from stdin, converts it to Protobuf format, and publishes it to a Kafka topic. It uses the kafka-python and protobuf libraries.

LANGUAGE: python
CODE:
#!/usr/bin/env python

import sys
import json

from kafka import KafkaProducer
from metrics_pb2 import Metrics

producer = KafkaProducer(bootstrap_servers='localhost:9092')
topic = 'metrics_pb'
metrics = Metrics()

for row in iter(sys.stdin):
    d = json.loads(row)
    for k, v in d.items():
        setattr(metrics, k, v)
    pb = metrics.SerializeToString()
    producer.send(topic, pb)

----------------------------------------

TITLE: Configuring TwitterSpritzerFirehose in Druid
DESCRIPTION: JSON configuration specification for setting up a Twitter Spritzer firehose connection in Druid. Includes parameters for controlling event count limits and runtime duration to prevent throttling and manage resource consumption.

LANGUAGE: json
CODE:
"firehose" : {
    "type" : "twitzer",
    "maxEventCount": -1,
    "maxRunMinutes": 0
}

----------------------------------------

TITLE: Configuring TwitterSpritzerFirehose in Druid
DESCRIPTION: JSON configuration specification for setting up a Twitter Spritzer firehose connection in Druid. Includes parameters for controlling event count limits and runtime duration to prevent throttling and manage resource consumption.

LANGUAGE: json
CODE:
"firehose" : {
    "type" : "twitzer",
    "maxEventCount": -1,
    "maxRunMinutes": 0
}

----------------------------------------

TITLE: Configuring TwitterSpritzerFirehose in Druid
DESCRIPTION: JSON configuration specification for setting up a Twitter Spritzer firehose connection in Druid. Includes parameters for controlling event count limits and runtime duration to prevent throttling and manage resource consumption.

LANGUAGE: json
CODE:
"firehose" : {
    "type" : "twitzer",
    "maxEventCount": -1,
    "maxRunMinutes": 0
}

----------------------------------------

TITLE: Configuring JVM Flags for Apache Druid
DESCRIPTION: Recommended JVM flags for optimal performance and logging in Apache Druid. Includes settings for timezone, file encoding, garbage collection, memory management, and error handling.

LANGUAGE: java
CODE:
-Duser.timezone=UTC
-Dfile.encoding=UTF-8
-Djava.io.tmpdir=<something other than /tmp which might be mounted to volatile tmpfs file system>
-Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
-Dorg.jboss.logging.provider=slf4j
-Dnet.spy.log.LoggerImpl=net.spy.memcached.compat.log.SLF4JLogger
-Dlog4j.shutdownCallbackRegistry=org.apache.druid.common.config.Log4jShutdown
-Dlog4j.shutdownHookEnabled=true
-XX:+PrintGCDetails
-XX:+PrintGCDateStamps
-XX:+PrintGCTimeStamps
-XX:+PrintGCApplicationStoppedTime
-XX:+PrintGCApplicationConcurrentTime
-Xloggc:/var/logs/druid/historical.gc.log
-XX:+UseGCLogFileRotation
-XX:NumberOfGCLogFiles=50
-XX:GCLogFileSize=10m
-XX:+ExitOnOutOfMemoryError
-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/var/logs/druid/historical.hprof
-XX:MaxDirectMemorySize=10240g

----------------------------------------

TITLE: Basic TopN Query Structure in Druid
DESCRIPTION: Demonstrates the basic structure of a TopN query including essential components like queryType, dataSource, dimension, threshold, metric, and aggregations. Shows how to query top 5 results from a sample dataset with filters and aggregations.

LANGUAGE: json
CODE:
{
  "queryType": "topN",
  "dataSource": "sample_data",
  "dimension": "sample_dim",
  "threshold": 5,
  "metric": "count",
  "granularity": "all",
  "filter": {
    "type": "and",
    "fields": [
      {
        "type": "selector",
        "dimension": "dim1",
        "value": "some_value"
      },
      {
        "type": "selector",
        "dimension": "dim2",
        "value": "some_other_val"
      }
    ]
  },
  "aggregations": [
    {
      "type": "longSum",
      "name": "count",
      "fieldName": "count"
    },
    {
      "type": "doubleSum",
      "name": "some_metric",
      "fieldName": "some_metric"
    }
  ],
  "postAggregations": [
    {
      "type": "arithmetic",
      "name": "average",
      "fn": "/",
      "fields": [
        {
          "type": "fieldAccess",
          "name": "some_metric",
          "fieldName": "some_metric"
        },
        {
          "type": "fieldAccess",
          "name": "count",
          "fieldName": "count"
        }
      ]
    }
  ],
  "intervals": [
    "2013-08-31T00:00:00.000/2013-09-03T00:00:00.000"
  ]
}

----------------------------------------

TITLE: Configuring TSV Ingestion in Apache Druid
DESCRIPTION: This snippet illustrates the parseSpec configuration for ingesting TSV (tab-separated values) data into Druid. It specifies the format, timestamp column, delimiter, input columns, and dimensions to be extracted.

LANGUAGE: json
CODE:
{
  "parseSpec": {
    "format" : "tsv",
    "timestampSpec" : {
      "column" : "timestamp"
    },
    "columns" : ["timestamp","page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city","added","deleted","delta"],
    "delimiter":"|",
    "dimensionsSpec" : {
      "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"]
    }
  }
}

----------------------------------------

TITLE: Defining HyperUnique Cardinality Post-Aggregator in Druid Query
DESCRIPTION: Shows how to wrap a hyperUnique object for use in post-aggregations, inheriting the rounding behavior of the referenced aggregator.

LANGUAGE: JSON
CODE:
{
  "type"  : "hyperUniqueCardinality",
  "name": <output name>,
  "fieldName"  : <the name field value of the hyperUnique aggregator>
}

----------------------------------------

TITLE: Querying with Period Granularity in Apache Druid
DESCRIPTION: This example demonstrates a groupBy query with a period granularity of 1 day in the Pacific timezone.

LANGUAGE: json
CODE:
{
   "queryType":"groupBy",
   "dataSource":"my_dataSource",
   "granularity":{"type": "period", "period": "P1D", "timeZone": "America/Los_Angeles"},
   "dimensions":[
      "language"
   ],
   "aggregations":[
      {
         "type":"count",
         "name":"count"
      }
   ],
   "intervals":[
      "1999-12-31T16:00:00.000-08:00/2999-12-31T16:00:00.000-08:00"
   ]
}

----------------------------------------

TITLE: Constructing Data Source Metadata Query in JSON for Apache Druid
DESCRIPTION: This snippet shows the JSON structure for a Data Source Metadata query in Apache Druid. It includes the required 'queryType' and 'dataSource' fields.

LANGUAGE: json
CODE:
{
    "queryType" : "dataSourceMetadata",
    "dataSource": "sample_datasource"
}

----------------------------------------

TITLE: Showing Mixed Version Segments During Updates in Druid
DESCRIPTION: This snippet demonstrates how a cluster may contain a mixture of v1 and v2 segments during the update process.

LANGUAGE: plaintext
CODE:
foo_2015-01-01/2015-01-02_v1_0
foo_2015-01-02/2015-01-03_v2_1
foo_2015-01-03/2015-01-04_v1_2

----------------------------------------

TITLE: Configuring StaticS3Firehose in Druid
DESCRIPTION: JSON configuration for StaticS3Firehose to ingest events from predefined S3 objects. This firehose supports splitting for parallel processing and includes caching and prefetching capabilities.

LANGUAGE: json
CODE:
{
    "firehose" : {
        "type" : "static-s3",
        "uris": ["s3://foo/bar/file.gz", "s3://bar/foo/file2.gz"]
    }
}

----------------------------------------

TITLE: HDFS Storage Configuration Parameters
DESCRIPTION: Configuration properties for setting up HDFS as deep storage in Druid. Includes paths, authentication, and Kerberos settings.

LANGUAGE: yaml
CODE:
druid.storage.type: hdfs
druid.storage.storageDirectory: <storage-directory>
druid.hadoop.security.kerberos.principal: druid@EXAMPLE.COM
druid.hadoop.security.kerberos.keytab: /etc/security/keytabs/druid.headlessUser.keytab

----------------------------------------

TITLE: Defining Basic Ingestion Spec Structure in Apache Druid
DESCRIPTION: This snippet shows the high-level structure of an Apache Druid ingestion specification, which consists of dataSchema, ioConfig, and tuningConfig components.

LANGUAGE: json
CODE:
{
  "dataSchema" : {...},
  "ioConfig" : {...},
  "tuningConfig" : {...}
}

----------------------------------------

TITLE: SQL Query for Joining Servers and Segments Tables in Druid
DESCRIPTION: Example SQL query demonstrating a join between servers and segments tables to count segments per server for a specific datasource.

LANGUAGE: SQL
CODE:
SELECT count(segments.segment_id) as num_segments from sys.segments as segments 
INNER JOIN sys.server_segments as server_segments 
ON segments.segment_id  = server_segments.segment_id 
INNER JOIN sys.servers as servers 
ON servers.server = server_segments.server
WHERE segments.datasource = 'wikipedia' 
GROUP BY servers.server;

----------------------------------------

TITLE: Configuring Regex-based Data Ingestion in Apache Druid
DESCRIPTION: This snippet illustrates the parseSpec configuration for ingesting data using a regular expression pattern in Druid. It allows for custom parsing of data that doesn't fit standard formats.

LANGUAGE: json
CODE:
{
  "parseSpec":{
    "format" : "regex",
    "timestampSpec" : {
      "column" : "timestamp"
    },        
    "dimensionsSpec" : {
      "dimensions" : [<your_list_of_dimensions>]
    },
    "columns" : [<your_columns_here>],
    "pattern" : <regex pattern for partitioning data>
  }
}

----------------------------------------

TITLE: Configuring Historical Processing Threads in Druid
DESCRIPTION: Sets the number of processing threads for a Historical node to one less than the number of CPU cores. This is typically set in the Druid configuration file.

LANGUAGE: java
CODE:
druid.processing.numThreads=<number of cores - 1>

----------------------------------------

TITLE: Kinesis Supervisor Configuration
DESCRIPTION: Sample JSON configuration for setting up a Kinesis supervisor, including dataSchema, tuningConfig, and ioConfig settings.

LANGUAGE: json
CODE:
{
  "type": "kinesis",
  "dataSchema": {
    "dataSource": "metrics-kinesis",
    "parser": {
      "type": "string",
      "parseSpec": {
        "format": "json",
        "timestampSpec": {
          "column": "timestamp",
          "format": "auto"
        },
        "dimensionsSpec": {
          "dimensions": [],
          "dimensionExclusions": [
            "timestamp",
            "value"
          ]
        }
      }
    },
    "metricsSpec": [
      {
        "name": "count",
        "type": "count"
      },
      {
        "name": "value_sum",
        "fieldName": "value",
        "type": "doubleSum"
      },
      {
        "name": "value_min",
        "fieldName": "value",
        "type": "doubleMin"
      },
      {
        "name": "value_max",
        "fieldName": "value",
        "type": "doubleMax"
      }
    ],
    "granularitySpec": {
      "type": "uniform",
      "segmentGranularity": "HOUR",
      "queryGranularity": "NONE"
    }
  },
  "tuningConfig": {
    "type": "kinesis",
    "maxRowsPerSegment": 5000000
  },
  "ioConfig": {
    "stream": "metrics",
    "endpoint": "kinesis.us-east-1.amazonaws.com",
    "taskCount": 1,
    "replicas": 1,
    "taskDuration": "PT1H",
    "recordsPerFetch": 2000,
    "fetchDelayMillis": 1000
  }
}

----------------------------------------

TITLE: Querying Druid SYS Schema Servers
DESCRIPTION: SQL query example to retrieve information about all Druid servers from the SYS schema.

LANGUAGE: sql
CODE:
SELECT * FROM sys.servers;

----------------------------------------

TITLE: CSV ParseSpec Configuration for Druid
DESCRIPTION: Configuration specification for parsing CSV data in Druid, including column definitions and dimension specifications.

LANGUAGE: json
CODE:
"parseSpec": { "format" : "csv", "timestampSpec" : { "column" : "timestamp" }, "columns" : ["timestamp","page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city","added","deleted","delta"], "dimensionsSpec" : { "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"] } }

----------------------------------------

TITLE: Registering Custom Jackson Subtypes for S3 Firehose
DESCRIPTION: Example showing how to register a custom FirehoseFactory implementation with Jackson's polymorphic serialization system.

LANGUAGE: java
CODE:
@Override
public List<? extends Module> getJacksonModules()
{
  return ImmutableList.of(
          new SimpleModule().registerSubtypes(new NamedType(StaticS3FirehoseFactory.class, "static-s3"))
  );
}

----------------------------------------

TITLE: Timeseries Query with Grand Totals
DESCRIPTION: Example showing how to enable grand totals in a timeseries query by adding the grandTotal context parameter. This returns an additional row with totals across all time buckets.

LANGUAGE: json
CODE:
{
  "queryType": "timeseries",
  "dataSource": "sample_datasource",
  "intervals": [ "2012-01-01T00:00:00.000/2012-01-03T00:00:00.000" ],
  "granularity": "day",
  "aggregations": [
    { "type": "longSum", "name": "sample_name1", "fieldName": "sample_fieldName1" },
    { "type": "doubleSum", "name": "sample_name2", "fieldName": "sample_fieldName2" }
  ],
  "context": {
    "grandTotal": true
  }
}

----------------------------------------

TITLE: Configuring Quantiles Doubles Sketch to Quantile Post-Aggregator in Druid
DESCRIPTION: JSON configuration for the quantilesDoublesSketchToQuantile post-aggregator. This returns an approximation of a value at a given fraction in the sketch.

LANGUAGE: json
CODE:
{
  "type"  : "quantilesDoublesSketchToQuantile",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,
  "fraction" : <fractional position in the hypothetical sorted stream, number from 0 to 1 inclusive>
}

----------------------------------------

TITLE: Configuring DefaultLimitSpec in Druid groupBy Queries
DESCRIPTION: This JSON structure defines the DefaultLimitSpec used to limit and order the results of a groupBy query. It specifies the limit and the columns to order by.

LANGUAGE: json
CODE:
{
    "type"    : "default",
    "limit"   : <integer_value>,
    "columns" : [list of OrderByColumnSpec],
}

----------------------------------------

TITLE: Basic ResetCluster Command Usage
DESCRIPTION: Shows the basic command syntax for running the ResetCluster tool with optional parameters to selectively reset different components of the Druid cluster.

LANGUAGE: bash
CODE:
java org.apache.druid.cli.Main tools reset-cluster [--metadataStore] [--segmentFiles] [--taskLogs] [--hadoopWorkingPath]

----------------------------------------

TITLE: Basic Hadoop Index Task Configuration
DESCRIPTION: Example configuration for a Hadoop-based batch ingestion task in Druid, showing the core components including dataSchema, ioConfig, and tuningConfig. The task ingests Wikipedia data with specified dimensions and metrics.

LANGUAGE: json
CODE:
{
  "type" : "index_hadoop",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "wikipedia",
      "parser" : {
        "type" : "hadoopyString",
        "parseSpec" : {
          "format" : "json",
          "timestampSpec" : {
            "column" : "timestamp",
            "format" : "auto"
          },
          "dimensionsSpec" : {
            "dimensions": ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"],
            "dimensionExclusions" : [],
            "spatialDimensions" : []
          }
        }
      },
      "metricsSpec" : [
        {
          "type" : "count",
          "name" : "count"
        },
        {
          "type" : "doubleSum",
          "name" : "added",
          "fieldName" : "added"
        },
        {
          "type" : "doubleSum",
          "name" : "deleted",
          "fieldName" : "deleted"
        },
        {
          "type" : "doubleSum",
          "name" : "delta",
          "fieldName" : "delta"
        }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "DAY",
        "queryGranularity" : "NONE",
        "intervals" : [ "2013-08-31/2013-09-01" ]
      }
    },
    "ioConfig" : {
      "type" : "hadoop",
      "inputSpec" : {
        "type" : "static",
        "paths" : "/MyDirectory/example/wikipedia_data.json"
      }
    },
    "tuningConfig" : {
      "type": "hadoop"
    }
  },
  "hadoopDependencyCoordinates": <my_hadoop_version>
}

----------------------------------------

TITLE: Registering Custom Jackson Subtypes for S3 Firehose
DESCRIPTION: Example showing how to register a custom FirehoseFactory implementation with Jackson's polymorphic serialization system.

LANGUAGE: java
CODE:
@Override
public List<? extends Module> getJacksonModules()
{
  return ImmutableList.of(
          new SimpleModule().registerSubtypes(new NamedType(StaticS3FirehoseFactory.class, "static-s3"))
  );
}

----------------------------------------

TITLE: Calculating Complex Percentage in Druid Query
DESCRIPTION: Shows a more complex percentage calculation using nested arithmetic post-aggregators, including multiplication and division operations.

LANGUAGE: JSON
CODE:
"aggregations" : [
    { "type" : "doubleSum", "name" : "tot", "fieldName" : "total" },
    { "type" : "doubleSum", "name" : "part", "fieldName" : "part" }
  ],
  "postAggregations" : [{
    "type"   : "arithmetic",
    "name"   : "part_percentage",
    "fn"     : "*",
    "fields" : [
       { "type"   : "arithmetic",
         "name"   : "ratio",
         "fn"     : "/",
         "fields" : [
           { "type" : "fieldAccess", "name" : "part", "fieldName" : "part" },
           { "type" : "fieldAccess", "name" : "tot", "fieldName" : "tot" }
         ]
       },
       { "type" : "constant", "name": "const", "value" : 100 }
    ]
  }]

----------------------------------------

TITLE: Configuring DoubleMax Aggregator in Druid
DESCRIPTION: The doubleMax aggregator computes the maximum of all metric values and Double.NEGATIVE_INFINITY. It requires specifying an output name and the metric field to analyze.

LANGUAGE: json
CODE:
{ "type" : "doubleMax", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: Implementing LIKE Filter in Druid
DESCRIPTION: Example of a LIKE filter for wildcard searches. This is equivalent to the SQL LIKE operator and supports '%' and '_' wildcards.

LANGUAGE: JSON
CODE:
{
    "type": "like",
    "dimension": "last_name",
    "pattern": "D%"
}

----------------------------------------

TITLE: Querying with Virtual Columns in Apache Druid
DESCRIPTION: This example demonstrates a scan query in Apache Druid that utilizes virtual columns. It creates two virtual columns: 'fooPage' concatenating 'foo' with the 'page' column, and 'tripleWordCount' multiplying the 'wordCount' column by 3.

LANGUAGE: json
CODE:
{
 "queryType": "scan",
 "dataSource": "page_data",
 "columns":[],
 "virtualColumns": [
    {
      "type": "expression",
      "name": "fooPage",
      "expression": "concat('foo' + page)",
      "outputType": "STRING"
    },
    {
      "type": "expression",
      "name": "tripleWordCount",
      "expression": "wordCount * 3",
      "outputType": "LONG"
    }
  ],
 "intervals": [
   "2013-01-01/2019-01-02"
 ] 
}

----------------------------------------

TITLE: CSV ParseSpec Configuration for Druid
DESCRIPTION: Configuration specification for parsing CSV data in Druid, including column definitions and dimension specifications.

LANGUAGE: json
CODE:
"parseSpec": { "format" : "csv", "timestampSpec" : { "column" : "timestamp" }, "columns" : ["timestamp","page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city","added","deleted","delta"], "dimensionsSpec" : { "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"] } }

----------------------------------------

TITLE: Defining Basic Ingestion Spec Structure in Apache Druid
DESCRIPTION: This snippet shows the high-level structure of an Apache Druid ingestion specification, which consists of dataSchema, ioConfig, and tuningConfig components.

LANGUAGE: json
CODE:
{
  "dataSchema" : {...},
  "ioConfig" : {...},
  "tuningConfig" : {...}
}

----------------------------------------

TITLE: Configuring Append Task in Apache Druid (Deprecated)
DESCRIPTION: JSON configuration for an Append task in Druid. This deprecated task appends a list of segments into a single segment, specifying the task ID, data source, segments to append, optional aggregators, and task context.

LANGUAGE: json
CODE:
{
    "type": "append",
    "id": <task_id>,
    "dataSource": <task_datasource>,
    "segments": <JSON list of DataSegment objects to append>,
    "aggregations": <optional list of aggregators>,
    "context": <task context>
}

----------------------------------------

TITLE: Configuring Authentication Chain in Apache Druid
DESCRIPTION: This JSON snippet shows how to enable Kerberos and HTTP Basic authenticators in the authentication chain using the druid-kerberos and druid-basic-security core extensions.

LANGUAGE: json
CODE:
"druid.auth.authenticatorChain":["kerberos", "basic"]

----------------------------------------

TITLE: Starting Druid Services
DESCRIPTION: Command to start all Druid services using the supervision script with tutorial configuration.

LANGUAGE: bash
CODE:
bin/supervise -c quickstart/tutorial/conf/tutorial-cluster.conf

----------------------------------------

TITLE: Example Druid Query for Unique Users of Both Product A and B
DESCRIPTION: Complex Druid query using thetaSketch aggregators and post-aggregators to count unique users who visited both product A and B.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "test_datasource",
  "granularity": "ALL",
  "dimensions": [],
  "filter": {
    "type": "or",
    "fields": [
      {"type": "selector", "dimension": "product", "value": "A"},
      {"type": "selector", "dimension": "product", "value": "B"}
    ]
  },
  "aggregations": [
    {
      "type" : "filtered",
      "filter" : {
        "type" : "selector",
        "dimension" : "product",
        "value" : "A"
      },
      "aggregator" :     {
        "type": "thetaSketch", "name": "A_unique_users", "fieldName": "user_id_sketch"
      }
    },
    {
      "type" : "filtered",
      "filter" : {
        "type" : "selector",
        "dimension" : "product",
        "value" : "B"
      },
      "aggregator" :     {
        "type": "thetaSketch", "name": "B_unique_users", "fieldName": "user_id_sketch"
      }
    }
  ],
  "postAggregations": [
    {
      "type": "thetaSketchEstimate",
      "name": "final_unique_users",
      "field":
      {
        "type": "thetaSketchSetOp",
        "name": "final_unique_users_sketch",
        "func": "INTERSECT",
        "fields": [
          {
            "type": "fieldAccess",
            "fieldName": "A_unique_users"
          },
          {
            "type": "fieldAccess",
            "fieldName": "B_unique_users"
          }
        ]
      }
    }
  ],
  "intervals": [
    "2014-10-19T00:00:00.000Z/2014-10-22T00:00:00.000Z"
  ]
}

----------------------------------------

TITLE: Defining JSON Event Structure in Druid
DESCRIPTION: This snippet shows an example JSON event structure that can be ingested into Druid. It includes various types of fields such as timestamps, dimensions, metrics, nested objects, and arrays.

LANGUAGE: json
CODE:
{
 "timestamp": "2015-09-12T12:10:53.155Z",
 "dim1": "qwerty",
 "dim2": "asdf",
 "dim3": "zxcv",
 "ignore_me": "ignore this",
 "metrica": 9999,
 "foo": {"bar": "abc"},
 "foo.bar": "def",
 "nestmet": {"val": 42},
 "hello": [1.0, 2.0, 3.0, 4.0, 5.0],
 "mixarray": [1.0, 2.0, 3.0, 4.0, {"last": 5}],
 "world": [{"hey": "there"}, {"tree": "apple"}],
 "thing": {"food": ["sandwich", "pizza"]}
}

----------------------------------------

TITLE: Configuring Append Task in Druid (Deprecated)
DESCRIPTION: The Append task combines multiple segments sequentially into a single segment. It requires specification of task ID, datasource, list of segments to append, and optional aggregations and context parameters.

LANGUAGE: json
CODE:
{
    "type": "append",
    "id": <task_id>,
    "dataSource": <task_datasource>,
    "segments": <JSON list of DataSegment objects to append>,
    "aggregations": <optional list of aggregators>,
    "context": <task context>
}

----------------------------------------

TITLE: Configuring Kafka Lookup Extractor in Apache Druid
DESCRIPTION: JSON configuration for setting up a Kafka-based lookup extractor in Apache Druid. Specifies the Kafka topic and connection properties for dimension value renaming.

LANGUAGE: json
CODE:
{
  "type":"kafka",
  "kafkaTopic":"testTopic",
  "kafkaProperties":{"zookeeper.connect":"somehost:2181/kafka"}
}

----------------------------------------

TITLE: Adding Custom DBCP Properties for Metadata Storage in Apache Druid
DESCRIPTION: This snippet demonstrates how to add custom DBCP properties for metadata storage configuration in Druid. It sets the maximum connection lifetime and default query timeout.

LANGUAGE: properties
CODE:
druid.metadata.storage.connector.dbcp.maxConnLifetimeMillis=1200000
druid.metadata.storage.connector.dbcp.defaultQueryTimeout=30000

----------------------------------------

TITLE: Creating IN Filter in Druid
DESCRIPTION: Example of an IN filter that matches a dimension against a list of values. This is equivalent to the SQL IN operator.

LANGUAGE: JSON
CODE:
{
    "type": "in",
    "dimension": "outlaw",
    "values": ["Good", "Bad", "Ugly"]
}

----------------------------------------

TITLE: Setting Up DoubleLast Aggregator in Druid
DESCRIPTION: JSON configuration for the doubleLast aggregator in Druid, which computes the metric value with the maximum timestamp or 0 if no row exists.

LANGUAGE: json
CODE:
{
  "type" : "doubleLast",
  "name" : <output_name>,
  "fieldName" : <metric_name>
}

----------------------------------------

TITLE: Basic GroupBy Query for Multi-value Dimension
DESCRIPTION: Example GroupBy query that groups results by the multi-value 'tags' dimension with no filtering.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "test",
  "intervals": [
    "1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"
  ],
  "granularity": {
    "type": "all"
  },
  "dimensions": [
    {
      "type": "default",
      "dimension": "tags",
      "outputName": "tags"
    }
  ],
  "aggregations": [
    {
      "type": "count",
      "name": "count"
    }
  ]
}

----------------------------------------

TITLE: TopN Query Result Format in Apache Druid
DESCRIPTION: This snippet shows the format of results returned by a TopN query in Druid. It includes a timestamp and an array of result objects, each containing dimension values and associated metrics.

LANGUAGE: json
CODE:
[
  {
    "timestamp": "2013-08-31T00:00:00.000Z",
    "result": [
      {
        "dim1": "dim1_val",
        "count": 111,
        "some_metrics": 10669,
        "average": 96.11711711711712
      },
      {
        "dim1": "another_dim1_val",
        "count": 88,
        "some_metrics": 28344,
        "average": 322.09090909090907
      },
      {
        "dim1": "dim1_val3",
        "count": 70,
        "some_metrics": 871,
        "average": 12.442857142857143
      },
      {
        "dim1": "dim1_val4",
        "count": 62,
        "some_metrics": 815,
        "average": 13.14516129032258
      },
      {
        "dim1": "dim1_val5",
        "count": 60,
        "some_metrics": 2787,
        "average": 46.45
      }
    ]
  }
]

----------------------------------------

TITLE: Defining TransformSpec Structure in JSON for Apache Druid
DESCRIPTION: This snippet shows the basic structure of a transformSpec in Apache Druid. It includes a list of transforms to be applied to input rows and an optional filter to determine which rows will be ingested.

LANGUAGE: json
CODE:
"transformSpec": {
  "transforms: <List of transforms>,
  "filter": <filter>
}

----------------------------------------

TITLE: Configuring Redis Cache Properties in Druid
DESCRIPTION: Core configuration properties for setting up Redis cache in Druid's common.runtime.properties file. Includes essential settings like host, port, connection pooling, and cache entry expiration.

LANGUAGE: properties
CODE:
druid.cache.host=<redis-host>
druid.cache.port=<redis-port>
druid.cache.expiration=86400000
druid.cache.timeout=2000
druid.cache.maxTotalConnections=8
druid.cache.maxIdleConnections=8
druid.cache.minIdleConnections=0

----------------------------------------

TITLE: Kinesis Supervisor Configuration JSON
DESCRIPTION: Example supervisor specification for configuring Kinesis ingestion, including data schema, tuning parameters and IO configuration.

LANGUAGE: json
CODE:
{
  "type": "kinesis",
  "dataSchema": {
    "dataSource": "metrics-kinesis",
    "parser": {
      "type": "string",
      "parseSpec": {
        "format": "json",
        "timestampSpec": {
          "column": "timestamp",
          "format": "auto"
        },
        "dimensionsSpec": {
          "dimensions": [],
          "dimensionExclusions": [
            "timestamp",
            "value"
          ]
        }
      }
    },
    "metricsSpec": [
      {
        "name": "count",
        "type": "count"
      },
      {
        "name": "value_sum",
        "fieldName": "value",
        "type": "doubleSum"
      },
      {
        "name": "value_min",
        "fieldName": "value",
        "type": "doubleMin"
      },
      {
        "name": "value_max",
        "fieldName": "value",
        "type": "doubleMax"
      }
    ],
    "granularitySpec": {
      "type": "uniform",
      "segmentGranularity": "HOUR",
      "queryGranularity": "NONE"
    }
  },
  "tuningConfig": {
    "type": "kinesis",
    "maxRowsPerSegment": 5000000
  },
  "ioConfig": {
    "stream": "metrics",
    "endpoint": "kinesis.us-east-1.amazonaws.com",
    "taskCount": 1,
    "replicas": 1,
    "taskDuration": "PT1H",
    "recordsPerFetch": 2000,
    "fetchDelayMillis": 1000
  }
}

----------------------------------------

TITLE: Complete Lookup Configuration Example
DESCRIPTION: A comprehensive example showing the full configuration for a lookup tier with a single cached namespace lookup. This includes the tier name, lookup name, and all required parameters for a JDBC-based lookup.

LANGUAGE: json
CODE:
{
  "realtime_customer2": {
    "country_code": {
      "version": "v0",
      "lookupExtractorFactory": {
        "type": "cachedNamespace",
        "extractionNamespace": {
          "type": "jdbc",
          "connectorConfig": {
            "createTables": true,
            "connectURI": "jdbc:mysql:\/\/localhost:3306\/druid",
            "user": "druid",
            "password": "diurd"
          },
          "table": "lookupValues",
          "keyColumn": "value_id",
          "valueColumn": "value_text",
          "filter": "value_type='country'",
          "tsColumn": "timeColumn"
        },
        "firstCacheTimeout": 120000,
        "injective": true
      }
    }
  }
}

----------------------------------------

TITLE: Configuring Asynchronous Logging for Druid
DESCRIPTION: Advanced log4j2 configuration that implements asynchronous logging for specific high-volume Druid components to improve performance. Sets different log levels for various system components and uses AsyncLogger for better throughput.

LANGUAGE: xml
CODE:
<?xml version="1.0" encoding="UTF-8" ?>
<Configuration status="WARN">
  <Appenders>
    <Console name="Console" target="SYSTEM_OUT">
      <PatternLayout pattern="%d{ISO8601} %p [%t] %c - %m%n"/>
    </Console>
  </Appenders>
  <Loggers>
    <AsyncLogger name="org.apache.druid.curator.inventory.CuratorInventoryManager" level="debug" additivity="false">
      <AppenderRef ref="Console"/>
    </AsyncLogger>
    <AsyncLogger name="org.apache.druid.client.BatchServerInventoryView" level="debug" additivity="false">
      <AppenderRef ref="Console"/>
    </AsyncLogger>
    <!-- Make extra sure nobody adds logs in a bad way that can hurt performance -->
    <AsyncLogger name="org.apache.druid.client.ServerInventoryView" level="debug" additivity="false">
      <AppenderRef ref="Console"/>
    </AsyncLogger>
    <AsyncLogger name ="org.apache.druid.java.util.http.client.pool.ChannelResourceFactory" level="info" additivity="false">
      <AppenderRef ref="Console"/>
    </AsyncLogger>
    <Root level="info">
      <AppenderRef ref="Console"/>
    </Root>
  </Loggers>
</Configuration>

----------------------------------------

TITLE: Configuring Druid Ingestion Task with Roll-up in JSON
DESCRIPTION: Druid ingestion task specification in JSON format, defining the data schema, input source, and enabling roll-up functionality.

LANGUAGE: json
CODE:
{
  "type" : "index",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "rollup-tutorial",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "dimensionsSpec" : {
            "dimensions" : [
              "srcIP",
              "dstIP"
            ]
          },
          "timestampSpec": {
            "column": "timestamp",
            "format": "iso"
          }
        }
      },
      "metricsSpec" : [
        { "type" : "count", "name" : "count" },
        { "type" : "longSum", "name" : "packets", "fieldName" : "packets" },
        { "type" : "longSum", "name" : "bytes", "fieldName" : "bytes" }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "week",
        "queryGranularity" : "minute",
        "intervals" : ["2018-01-01/2018-01-03"],
        "rollup" : true
      }
    },
    "ioConfig" : {
      "type" : "index",
      "firehose" : {
        "type" : "local",
        "baseDir" : "quickstart/tutorial",
        "filter" : "rollup-data.json"
      },
      "appendToExisting" : false
    },
    "tuningConfig" : {
      "type" : "index",
      "maxRowsPerSegment" : 5000000,
      "maxRowsInMemory" : 25000
    }
  }
}

----------------------------------------

TITLE: Rolled-up Data in Druid
DESCRIPTION: Shows the result of Druid's rollup operation on the sample raw data, with timestamps floored to minutes and metrics aggregated.

LANGUAGE: plaintext
CODE:
timestamp                 srcIP         dstIP          packets     bytes
2018-01-01T01:01:00Z      1.1.1.1       2.2.2.2            600      6000
2018-01-01T01:02:00Z      1.1.1.1       2.2.2.2            900      9000
2018-01-01T01:03:00Z      1.1.1.1       2.2.2.2            600      6000
2018-01-02T21:33:00Z      7.7.7.7       8.8.8.8            300      3000
2018-01-02T21:35:00Z      7.7.7.7       8.8.8.8            300      3000

----------------------------------------

TITLE: Executing Scan Query in Apache Druid
DESCRIPTION: Example of a Scan query object in Apache Druid. It specifies the query type, data source, result format, columns, intervals, batch size, and limit for the query.

LANGUAGE: json
CODE:
{
   "queryType": "scan",
   "dataSource": "wikipedia",
   "resultFormat": "list",
   "columns":[],
   "intervals": [
     "2013-01-01/2013-01-02"
   ],
   "batchSize":20480,
   "limit":3
 }

----------------------------------------

TITLE: Starting Druid Historical Server
DESCRIPTION: Command to start a Druid Historical process server. This command initializes the Historical node which is responsible for loading and serving segments for querying.

LANGUAGE: bash
CODE:
org.apache.druid.cli.Main server historical

----------------------------------------

TITLE: Specifying Period Granularity in Apache Druid
DESCRIPTION: This example shows how to specify a period granularity of 2 days in the Pacific timezone for aggregation.

LANGUAGE: json
CODE:
{"type": "period", "period": "P2D", "timeZone": "America/Los_Angeles"}

----------------------------------------

TITLE: Executing Search Query in Apache Druid
DESCRIPTION: Sample search query that looks for dimension values containing 'Ke' in a specified time range. The query searches across dim1 and dim2 dimensions with daily granularity and lexicographic sorting.

LANGUAGE: json
CODE:
{
  "queryType": "search",
  "dataSource": "sample_datasource",
  "granularity": "day",
  "searchDimensions": [
    "dim1",
    "dim2"
  ],
  "query": {
    "type": "insensitive_contains",
    "value": "Ke"
  },
  "sort" : {
    "type": "lexicographic"
  },
  "intervals": [
    "2013-01-01T00:00:00.000/2013-01-03T00:00:00.000"
  ]
}

----------------------------------------

TITLE: Configuring StaticCloudFilesFirehose in JSON
DESCRIPTION: JSON configuration for setting up a static Cloud Files firehose in Druid. This configuration enables ingestion of data from multiple Rackspace Cloud Files blobs, with support for caching and prefetching features.

LANGUAGE: json
CODE:
{
    "type" : "static-cloudfiles",
    "blobs": [
        {
          "region": "DFW"
          "container": "container",
          "path": "/path/to/your/file.json"
        },
        {
          "region": "ORD"
          "container": "anothercontainer",
          "path": "/another/path.json"
        }
    ]
}

----------------------------------------

TITLE: Configuring Azure Blob Store Firehose in Druid
DESCRIPTION: Configuration specification for the StaticAzureBlobStoreFirehose to ingest data from Azure Blob Storage. This firehose supports reading JSON objects from multiple blob containers and paths with caching and prefetching capabilities.

LANGUAGE: json
CODE:
{
    "type" : "static-azure-blobstore",
    "blobs": [
        {
          "container": "container",
          "path": "/path/to/your/file.json"
        },
        {
          "container": "anothercontainer",
          "path": "/another/path.json"
        }
    ]
}

----------------------------------------

TITLE: Configuring Globally Cached Lookup with JDBC Namespace
DESCRIPTION: Example configuration for a globally cached lookup using a JDBC namespace. This setup connects to a MySQL database and specifies lookup parameters including the table, key column, and value column.

LANGUAGE: json
CODE:
{
   "type": "cachedNamespace",
   "extractionNamespace": {
      "type": "jdbc",
      "connectorConfig": {
        "createTables": true,
        "connectURI": "jdbc:mysql:\/\/localhost:3306\/druid",
        "user": "druid",
        "password": "diurd"
      },
      "table": "lookupTable",
      "keyColumn": "mykeyColumn",
      "valueColumn": "myValueColumn",
      "filter" : "myFilterSQL (Where clause statement  e.g LOOKUPTYPE=1)",
      "tsColumn": "timeColumn"
   },
   "firstCacheTimeout": 120000,
   "injective":true
}

----------------------------------------

TITLE: Configuring Kerberos Authenticator in Druid
DESCRIPTION: Basic configuration to enable Kerberos authenticator in Druid's authentication chain. Specifies the authenticator type as 'kerberos'.

LANGUAGE: properties
CODE:
druid.auth.authenticatorChain=["MyKerberosAuthenticator"]

druid.auth.authenticator.MyKerberosAuthenticator.type=kerberos

----------------------------------------

TITLE: Configuring JavaScript Ingestion in Apache Druid
DESCRIPTION: This snippet demonstrates the parseSpec configuration for ingesting data using a JavaScript parser in Druid. It specifies the format, timestamp column, dimensions, and a JavaScript function to parse the input data.

LANGUAGE: json
CODE:
{
  "parseSpec":{
    "format" : "javascript",
    "timestampSpec" : {
      "column" : "timestamp"
    },        
    "dimensionsSpec" : {
      "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"]
    },
    "function" : "function(str) { var parts = str.split(\"-\"); return { one: parts[0], two: parts[1] } }"
  }
}

----------------------------------------

TITLE: Druid Search Query Response Format
DESCRIPTION: Example response format from a Druid search query showing results grouped by timestamp. Each result contains matching dimension values and their occurrence counts.

LANGUAGE: json
CODE:
[
  {
    "timestamp": "2012-01-01T00:00:00.000Z",
    "result": [
      {
        "dimension": "dim1",
        "value": "Ke$ha",
        "count": 3
      },
      {
        "dimension": "dim2",
        "value": "Ke$haForPresident",
        "count": 1
      }
    ]
  },
  {
    "timestamp": "2012-01-02T00:00:00.000Z",
    "result": [
      {
        "dimension": "dim1",
        "value": "SomethingThatContainsKe",
        "count": 1
      },
      {
        "dimension": "dim2",
        "value": "SomethingElseThatContainsKe",
        "count": 2
      }
    ]
  }
]

----------------------------------------

TITLE: Configuring Period Load Rule in Druid
DESCRIPTION: JSON configuration for a Period Load Rule that defines segment replication based on a time period and includes future data handling.

LANGUAGE: json
CODE:
{
  "type" : "loadByPeriod",
  "period" : "P1M",
  "includeFuture" : true,
  "tieredReplicants": {
      "hot": 1,
      "_default_tier" : 1
  }
}

----------------------------------------

TITLE: Configuring FloatSum Aggregator in Druid
DESCRIPTION: JSON configuration for the floatSum aggregator in Druid, which computes and stores the sum of values as 32-bit floating point values.

LANGUAGE: json
CODE:
{ "type" : "floatSum", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: Setting Processing Threads for Historical Nodes in YAML
DESCRIPTION: Configures the number of processing threads for a Historical node to be one less than the number of CPU cores. This setting helps maximize CPU utilization without oversubscription.

LANGUAGE: yaml
CODE:
druid.processing.numThreads: $(($(nproc) - 1))

----------------------------------------

TITLE: Example Query: Retention Analysis with Theta Sketch
DESCRIPTION: Advanced Druid query for retention analysis, counting unique users who visited Product A in two different time periods using Theta Sketch aggregators and post-aggregators.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "test_datasource",
  "granularity": "ALL",
  "dimensions": [],
  "filter": {
    "type": "or",
    "fields": [
      {"type": "selector", "dimension": "product", "value": "A"}
    ]
  },
  "aggregations": [
    {
      "type" : "filtered",
      "filter" : {
        "type" : "and",
        "fields" : [
          {
            "type" : "selector",
            "dimension" : "product",
            "value" : "A"
          },
          {
            "type" : "interval",
            "dimension" : "__time",
            "intervals" :  ["2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z"]
          }
        ]
      },
      "aggregator" :     {
        "type": "thetaSketch", "name": "A_unique_users_week_1", "fieldName": "user_id_sketch"
      }
    },
    {
      "type" : "filtered",
      "filter" : {
        "type" : "and",
        "fields" : [
          {
            "type" : "selector",
            "dimension" : "product",
            "value" : "A"
          },
          {
            "type" : "interval",
            "dimension" : "__time",
            "intervals" :  ["2014-10-08T00:00:00.000Z/2014-10-14T00:00:00.000Z"]
          }
        ]
      },
      "aggregator" : {
        "type": "thetaSketch", "name": "A_unique_users_week_2", "fieldName": "user_id_sketch"
      }
    },
  ],
  "postAggregations": [
    {
      "type": "thetaSketchEstimate",
      "name": "final_unique_users",
      "field":
      {
        "type": "thetaSketchSetOp",
        "name": "final_unique_users_sketch",
        "func": "INTERSECT",
        "fields": [
          {
            "type": "fieldAccess",
            "fieldName": "A_unique_users_week_1"
          },
          {
            "type": "fieldAccess",
            "fieldName": "A_unique_users_week_2"
          }
        ]
      }
    }
  ],
  "intervals": ["2014-10-01T00:00:00.000Z/2014-10-14T00:00:00.000Z"]
}

----------------------------------------

TITLE: Configuring Azure Blob Store Firehose in Druid
DESCRIPTION: JSON configuration for setting up StaticAzureBlobStoreFirehose to ingest data from Azure Blob Storage. Supports multiple blob configurations with caching and prefetching capabilities.

LANGUAGE: json
CODE:
{
    "type" : "static-azure-blobstore",
    "blobs": [
        {
          "container": "container",
          "path": "/path/to/your/file.json"
        },
        {
          "container": "anothercontainer",
          "path": "/another/path.json"
        }
    ]
}

----------------------------------------

TITLE: GroupBy Query with Filtered DimensionSpec on Multi-value Dimensions in Apache Druid
DESCRIPTION: Example of a groupBy query that uses a filtered dimensionSpec to filter the 'tags' dimension for 't3' after dimension explosion, resulting in more precise grouping.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "test",
  "intervals": [
    "1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"
  ],
  "filter": {
    "type": "selector",
    "dimension": "tags",
    "value": "t3"
  },
  "granularity": {
    "type": "all"
  },
  "dimensions": [
    {
      "type": "listFiltered",
      "delegate": {
        "type": "default",
        "dimension": "tags",
        "outputName": "tags"
      },
      "values": ["t3"]
    }
  ],
  "aggregations": [
    {
      "type": "count",
      "name": "count"
    }
  ]
}

----------------------------------------

TITLE: Configuring Avro Stream Parser with Schema Repo Decoder in Druid
DESCRIPTION: This JSON configuration sets up an Avro stream parser using a schema repo Avro bytes decoder. It specifies how to decode Avro bytes and parse the data, including timestamp and dimension specifications.

LANGUAGE: json
CODE:
{
  "parser" : {
    "type" : "avro_stream",
    "avroBytesDecoder" : {
      "type" : "schema_repo",
      "subjectAndIdConverter" : {
        "type" : "avro_1124",
        "topic" : "${YOUR_TOPIC}"
      },
      "schemaRepository" : {
        "type" : "avro_1124_rest_client",
        "url" : "${YOUR_SCHEMA_REPO_END_POINT}",
      }
    },
    "parseSpec" : {
      "format": "avro",
      "timestampSpec": <standard timestampSpec>,
      "dimensionsSpec": <standard dimensionsSpec>,
      "flattenSpec": <optional>
    }
  }
}

----------------------------------------

TITLE: Configuring Parquet Parser with Parquet ParseSpec
DESCRIPTION: Example configuration for ingesting Parquet files using the direct parquet parser with field flattening and discovery. Shows how to set up input format, parser type, and specify timestamp and dimension specifications.

LANGUAGE: JSON
CODE:
{
  "type": "index_hadoop",
  "spec": {
    "ioConfig": {
      "type": "hadoop",
      "inputSpec": {
        "type": "static",
        "inputFormat": "org.apache.druid.data.input.parquet.DruidParquetInputFormat",
        "paths": "path/to/file.parquet"
      }
    },
    "dataSchema": {
      "dataSource": "example",
      "parser": {
        "type": "parquet",
        "parseSpec": {
          "format": "parquet",
          "flattenSpec": {
            "useFieldDiscovery": true,
            "fields": [
              {
                "type": "path",
                "name": "nestedDim",
                "expr": "$.nestedData.dim1"
              },
              {
                "type": "path",
                "name": "listDimFirstItem",
                "expr": "$.listDim[1]"
              }
            ]
          },
          "timestampSpec": {
            "column": "timestamp",
            "format": "auto"
          },
          "dimensionsSpec": {
            "dimensions": [],
            "dimensionExclusions": [],
            "spatialDimensions": []
          }
        }
      }
    }
  }
}

----------------------------------------

TITLE: Configuring Basic Authorizer in Druid
DESCRIPTION: Properties for setting up the basic role-based authorizer.

LANGUAGE: properties
CODE:
druid.auth.authorizers=["MyBasicAuthorizer"]

druid.auth.authorizer.MyBasicAuthorizer.type=basic

----------------------------------------

TITLE: Cancelling a Query in Apache Druid using Curl
DESCRIPTION: This snippet demonstrates how to cancel a query in Apache Druid using its unique identifier. It sends a DELETE request to the Broker or Router endpoint.

LANGUAGE: bash
CODE:
curl -X DELETE "http://host:port/druid/v2/abc123"

----------------------------------------

TITLE: Demonstrating Druid Expression Syntax
DESCRIPTION: Examples of Druid expression syntax, including operators, data types, and function calls. This snippet showcases various aspects of the expression language without providing executable code.

LANGUAGE: SQL
CODE:
cast(expr, 'LONG')
if(x > 0, 'positive', 'non-positive')
nvl(null_column, 'default_value')
like(customer_name, 'A%')
case_searched(x > 0, 'positive', x < 0, 'negative', 'zero')
bloom_filter_test(user_id, base64_encoded_filter)
concat(first_name, ' ', last_name)
format('Hello, %s', name)
regexp_extract(url, '(https?://.*?)/')
timestamp(date_string, 'yyyy-MM-dd')
timestamp_ceil(time_column, 'P1D')
abs(x)
log(y)
round(z, 2)

----------------------------------------

TITLE: Configuring Druid Extension for DataSketches Theta Sketch
DESCRIPTION: Configuration to include the DataSketches Theta Sketch extension in Druid.

LANGUAGE: json
CODE:
"druid.extensions.loadList":["druid-datasketches"]

----------------------------------------

TITLE: Configuring HttpFirehose in Apache Druid
DESCRIPTION: This snippet demonstrates the configuration of an HttpFirehose in Druid to read data from remote sites via HTTP. It includes options for specifying URIs and authentication.

LANGUAGE: json
CODE:
{
    "type": "http",
    "uris": ["http://example.com/uri1", "http://example2.com/uri2"],
    "httpAuthenticationUsername": "username",
    "httpAuthenticationPassword": "password123"
}

----------------------------------------

TITLE: SQL Query Using Bloom Filter in Druid
DESCRIPTION: This SQL snippet demonstrates how to use a bloom filter in a Druid SQL query's WHERE clause using the bloom_filter_test operator.

LANGUAGE: sql
CODE:
SELECT COUNT(*) FROM druid.foo WHERE bloom_filter_test(<expr>, '<serialized_bytes_for_BloomKFilter>')

----------------------------------------

TITLE: Configuring Extraction DimensionSpec in Apache Druid
DESCRIPTION: JSON configuration for an Extraction DimensionSpec, which transforms dimension values using a specified extraction function.

LANGUAGE: JSON
CODE:
{
  "type" : "extraction",
  "dimension" : <dimension>,
  "outputName" :  <output_name>,
  "outputType": <"STRING"|"LONG"|"FLOAT">,
  "extractionFn" : <extraction_function>
}

----------------------------------------

TITLE: Defining Arithmetic Post-Aggregator in Druid Query
DESCRIPTION: Demonstrates the structure of an arithmetic post-aggregator in a Druid query. It applies a specified function to given fields and can include an optional ordering parameter.

LANGUAGE: JSON
CODE:
{
  "type"  : "arithmetic",
  "name"  : <output_name>,
  "fn"    : <arithmetic_function>,
  "fields": [<post_aggregator>, <post_aggregator>, ...],
  "ordering" : <null (default), or "numericFirst">
}

----------------------------------------

TITLE: Configuring Authentication Chain in Apache Druid
DESCRIPTION: This JSON snippet shows how to configure the authentication chain in Apache Druid to enable Kerberos and HTTP Basic authenticators from the druid-kerberos and druid-basic-security core extensions.

LANGUAGE: json
CODE:
"druid.auth.authenticatorChain":["kerberos", "basic"]

----------------------------------------

TITLE: Defining Compaction Task Configuration in JSON for Apache Druid
DESCRIPTION: This JSON structure defines the configuration for a Compaction Task in Apache Druid. It specifies the task type, data source, interval, and optional parameters for customizing the compaction process.

LANGUAGE: json
CODE:
{
    "type": "compact",
    "id": <task_id>,
    "dataSource": <task_datasource>,
    "interval": <interval to specify segments to be merged>,
    "dimensions" <custom dimensionsSpec>,
    "keepSegmentGranularity": <true or false>,
    "segmentGranularity": <segment granularity after compaction>,
    "targetCompactionSizeBytes": <target size of compacted segments>
    "tuningConfig" <index task tuningConfig>,
    "context": <task context>
}

----------------------------------------

TITLE: Configuring Polling Off-heap Lookup Cache in Druid
DESCRIPTION: JSON configuration for a polling off-heap lookup cache that is loaded once and never swapped. It uses a JDBC data fetcher to retrieve data from a MySQL database.

LANGUAGE: json
CODE:
{
    "type":"pollingLookup",
   "dataFetcher":{ "type":"jdbcDataFetcher", "connectorConfig":"jdbc://mysql://localhost:3306/my_data_base", "table":"lookup_table_name", "keyColumn":"key_column_name", "valueColumn": "value_column_name"},
   "cacheFactory":{"type":"offHeapPolling"}
}

----------------------------------------

TITLE: SQL Query Using Bloom Filter in Druid
DESCRIPTION: This SQL snippet demonstrates how to use a bloom filter in a Druid SQL query's WHERE clause using the bloom_filter_test operator.

LANGUAGE: sql
CODE:
SELECT COUNT(*) FROM druid.foo WHERE bloom_filter_test(<expr>, '<serialized_bytes_for_BloomKFilter>')

----------------------------------------

TITLE: Druid Query Context Properties
DESCRIPTION: Table showing core query context properties with default values and descriptions. Includes parameters for timeout, priority, caching, result handling, and performance tuning.

LANGUAGE: json
CODE:
{
  "timeout": "druid.server.http.defaultQueryTimeout",
  "priority": 0,
  "queryId": "auto-generated",
  "useCache": true,
  "populateCache": true,
  "useResultLevelCache": false,
  "populateResultLevelCache": false,
  "bySegment": false,
  "finalize": true,
  "chunkPeriod": "P0D",
  "maxScatterGatherBytes": "druid.server.http.maxScatterGatherBytes",
  "maxQueuedBytes": "druid.broker.http.maxQueuedBytes",
  "serializeDateTimeAsLong": false,
  "serializeDateTimeAsLongInner": false
}

----------------------------------------

TITLE: Configuring Parquet Parser with Parquet ParseSpec
DESCRIPTION: Example configuration for ingesting Parquet data using the simple parquet parser with parquet parseSpec. Includes flatten spec for nested data handling and timestamp configuration.

LANGUAGE: json
CODE:
{
  "type": "index_hadoop",
  "spec": {
    "ioConfig": {
      "type": "hadoop",
      "inputSpec": {
        "type": "static",
        "inputFormat": "org.apache.druid.data.input.parquet.DruidParquetInputFormat",
        "paths": "path/to/file.parquet"
      }
    },
    "dataSchema": {
      "dataSource": "example",
      "parser": {
        "type": "parquet",
        "parseSpec": {
          "format": "parquet",
          "flattenSpec": {
            "useFieldDiscovery": true,
            "fields": [
              {
                "type": "path",
                "name": "nestedDim",
                "expr": "$.nestedData.dim1"
              },
              {
                "type": "path",
                "name": "listDimFirstItem",
                "expr": "$.listDim[1]"
              }
            ]
          },
          "timestampSpec": {
            "column": "timestamp",
            "format": "auto"
          },
          "dimensionsSpec": {
            "dimensions": [],
            "dimensionExclusions": [],
            "spatialDimensions": []
          }
        }
      }
    },
    "tuningConfig": "<hadoop-tuning-config>"
    }
  }
}

----------------------------------------

TITLE: Metrics Specification
DESCRIPTION: Example of metrics configuration using the flattened field names, showing how to define aggregators for both path-based and auto-discovered metrics.

LANGUAGE: json
CODE:
"metricsSpec" : [ 
{
  "type" : "longSum",
  "name" : "path-metric-sum",
  "fieldName" : "path-metric"
}, 
{
  "type" : "doubleSum",
  "name" : "hello-0-sum",
  "fieldName" : "hello-0"
},
{
  "type" : "longSum",
  "name" : "metrica-sum",
  "fieldName" : "metrica"
}
]

----------------------------------------

TITLE: Configuring Regex Ingestion in Apache Druid
DESCRIPTION: This snippet shows the parseSpec configuration for ingesting data using a regex parser in Druid. It defines the format, timestamp column, dimensions, columns, and the regex pattern for parsing the data.

LANGUAGE: json
CODE:
{
  "parseSpec":{
    "format" : "regex",
    "timestampSpec" : {
      "column" : "timestamp"
    },        
    "dimensionsSpec" : {
      "dimensions" : [<your_list_of_dimensions>]
    },
    "columns" : [<your_columns_here>],
    "pattern" : <regex pattern for partitioning data>
  }
}

----------------------------------------

TITLE: Configuring Bulk Lookup Updates in Druid
DESCRIPTION: Example JSON configuration for bulk updating lookups across multiple tiers in Druid. Demonstrates how to define lookups with different extractor factory types and settings.

LANGUAGE: json
CODE:
{
  "__default": {
    "country_code": {
      "version": "v0",
      "lookupExtractorFactory": {
        "type": "map",
        "map": {
          "77483": "United States"
        }
      }
    },
    "site_id": {
      "version": "v0",
      "lookupExtractorFactory": {
        "type": "cachedNamespace",
        "extractionNamespace": {
          "type": "jdbc",
          "connectorConfig": {
            "createTables": true,
            "connectURI": "jdbc:mysql://localhost:3306/druid",
            "user": "druid",
            "password": "diurd"
          },
          "table": "lookupTable",
          "keyColumn": "country_id",
          "valueColumn": "country_name",
          "tsColumn": "timeColumn"
        },
        "firstCacheTimeout": 120000,
        "injective": true
      }
    },
    "site_id_customer1": {
      "version": "v0",
      "lookupExtractorFactory": {
        "type": "map",
        "map": {
          "847632": "Internal Use Only"
        }
      }
    },
    "site_id_customer2": {
      "version": "v0",
      "lookupExtractorFactory": {
        "type": "map",
        "map": {
          "AHF77": "Home"
        }
      }
    }
  },
  "realtime_customer1": {
    "country_code": {
      "version": "v0",
      "lookupExtractorFactory": {
        "type": "map",
        "map": {
          "77483": "United States"
        }
      }
    },
    "site_id_customer1": {
      "version": "v0",
      "lookupExtractorFactory": {
        "type": "map",
        "map": {
          "847632": "Internal Use Only"
        }
      }
    }
  },
  "realtime_customer2": {
    "country_code": {
      "version": "v0",
      "lookupExtractorFactory": {
        "type": "map",
        "map": {
          "77483": "United States"
        }
      }
    },
    "site_id_customer2": {
      "version": "v0",
      "lookupExtractorFactory": {
        "type": "map",
        "map": {
          "AHF77": "Home"
        }
      }
    }
  }
}

----------------------------------------

TITLE: Configuring DefaultLimitSpec in Druid groupBy Queries
DESCRIPTION: This JSON snippet shows the structure of the DefaultLimitSpec used to limit and sort groupBy query results. It includes a limit value and a list of columns for ordering.

LANGUAGE: json
CODE:
{
    "type"    : "default",
    "limit"   : <integer_value>,
    "columns" : [list of OrderByColumnSpec],
}

----------------------------------------

TITLE: Configuring Granularities in Druid Ingestion Spec
DESCRIPTION: Sets the segment granularity and query granularity in the granularitySpec, defining the time interval for segments and the bucketing granularity for timestamps.

LANGUAGE: json
CODE:
"dataSchema" : {
  "dataSource" : "ingestion-tutorial",
  "parser" : {
    "type" : "string",
    "parseSpec" : {
      "format" : "json",
      "timestampSpec" : {
        "format" : "iso",
        "column" : "ts"
      },
      "dimensionsSpec" : {
        "dimensions": [
          "srcIP",
          { "name" : "srcPort", "type" : "long" },
          { "name" : "dstIP", "type" : "string" },
          { "name" : "dstPort", "type" : "long" },
          { "name" : "protocol", "type" : "string" }
        ]
      }      
    }
  },
  "metricsSpec" : [
    { "type" : "count", "name" : "count" },
    { "type" : "longSum", "name" : "packets", "fieldName" : "packets" },
    { "type" : "longSum", "name" : "bytes", "fieldName" : "bytes" },
    { "type" : "doubleSum", "name" : "cost", "fieldName" : "cost" }
  ],
  "granularitySpec" : {
    "type" : "uniform",
    "segmentGranularity" : "HOUR",
    "queryGranularity" : "MINUTE",
    "intervals" : ["2018-01-01/2018-01-02"],
    "rollup" : true
  }
}

----------------------------------------

TITLE: Configuring Realtime Spec File in Druid
DESCRIPTION: Example configuration for a realtime ingestion spec file in Druid. Shows how to configure dataSchema, ioConfig, and tuningConfig for ingesting streaming data from Kafka.

LANGUAGE: json
CODE:
[
  {
    "dataSchema" : {
      "dataSource" : "wikipedia",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "timestampSpec" : {
            "column" : "timestamp",
            "format" : "auto"
          },
          "dimensionsSpec" : {
            "dimensions": ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"],
            "dimensionExclusions" : [],
            "spatialDimensions" : []
          }
        }
      },
      "metricsSpec" : [{
        "type" : "count",
        "name" : "count"
      }, {
        "type" : "doubleSum",
        "name" : "added",
        "fieldName" : "added"
      }, {
        "type" : "doubleSum",
        "name" : "deleted",
        "fieldName" : "deleted"
      }, {
        "type" : "doubleSum",
        "name" : "delta",
        "fieldName" : "delta"
      }],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "DAY",
        "queryGranularity" : "NONE"
      }
    },
    "ioConfig" : {
      "type" : "realtime",
      "firehose": {
        "type": "kafka-0.8",
        "consumerProps": {
          "zookeeper.connect": "localhost:2181",
          "zookeeper.connection.timeout.ms" : "15000",
          "zookeeper.session.timeout.ms" : "15000",
          "zookeeper.sync.time.ms" : "5000",
          "group.id": "druid-example",
          "fetch.message.max.bytes" : "1048586",
          "auto.offset.reset": "largest",
          "auto.commit.enable": "false"
        },
        "feed": "wikipedia"
      },
      "plumber": {
        "type": "realtime"
      }
    },
    "tuningConfig": {
      "type" : "realtime",
      "maxRowsInMemory": 1000000,
      "intermediatePersistPeriod": "PT10M",
      "windowPeriod": "PT10M",
      "basePersistDirectory": "\/tmp\/realtime\/basePersist",
      "rejectionPolicy": {
        "type": "serverTime"
      }
    }
  }
]

----------------------------------------

TITLE: Configuring Realtime Spec File in Druid
DESCRIPTION: Example configuration for a realtime ingestion spec file in Druid. Shows how to configure dataSchema, ioConfig, and tuningConfig for ingesting streaming data from Kafka.

LANGUAGE: json
CODE:
[
  {
    "dataSchema" : {
      "dataSource" : "wikipedia",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "timestampSpec" : {
            "column" : "timestamp",
            "format" : "auto"
          },
          "dimensionsSpec" : {
            "dimensions": ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"],
            "dimensionExclusions" : [],
            "spatialDimensions" : []
          }
        }
      },
      "metricsSpec" : [{
        "type" : "count",
        "name" : "count"
      }, {
        "type" : "doubleSum",
        "name" : "added",
        "fieldName" : "added"
      }, {
        "type" : "doubleSum",
        "name" : "deleted",
        "fieldName" : "deleted"
      }, {
        "type" : "doubleSum",
        "name" : "delta",
        "fieldName" : "delta"
      }],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "DAY",
        "queryGranularity" : "NONE"
      }
    },
    "ioConfig" : {
      "type" : "realtime",
      "firehose": {
        "type": "kafka-0.8",
        "consumerProps": {
          "zookeeper.connect": "localhost:2181",
          "zookeeper.connection.timeout.ms" : "15000",
          "zookeeper.session.timeout.ms" : "15000",
          "zookeeper.sync.time.ms" : "5000",
          "group.id": "druid-example",
          "fetch.message.max.bytes" : "1048586",
          "auto.offset.reset": "largest",
          "auto.commit.enable": "false"
        },
        "feed": "wikipedia"
      },
      "plumber": {
        "type": "realtime"
      }
    },
    "tuningConfig": {
      "type" : "realtime",
      "maxRowsInMemory": 1000000,
      "intermediatePersistPeriod": "PT10M",
      "windowPeriod": "PT10M",
      "basePersistDirectory": "\/tmp\/realtime\/basePersist",
      "rejectionPolicy": {
        "type": "serverTime"
      }
    }
  }
]

----------------------------------------

TITLE: Configuring Kafka Lookup in Druid
DESCRIPTION: JSON configuration for setting up a Kafka-based lookup in Druid. Specifies the Kafka topic and connection properties for dimension value mapping.

LANGUAGE: json
CODE:
{
  "type":"kafka",
  "kafkaTopic":"testTopic",
  "kafkaProperties":{"zookeeper.connect":"somehost:2181/kafka"}
}

----------------------------------------

TITLE: Configuring Basic Authenticator in Druid
DESCRIPTION: Configuration properties for setting up basic authentication in Druid with initial admin and internal client passwords.

LANGUAGE: properties
CODE:
druid.auth.authenticatorChain=["MyBasicAuthenticator"]

druid.auth.authenticator.MyBasicAuthenticator.type=basic
druid.auth.authenticator.MyBasicAuthenticator.initialAdminPassword=password1
druid.auth.authenticator.MyBasicAuthenticator.initialInternalClientPassword=password2
druid.auth.authenticator.MyBasicAuthenticator.authorizerName=MyBasicAuthorizer

----------------------------------------

TITLE: Installing Community Extensions using pull-deps Tool
DESCRIPTION: Command line example demonstrating how to download and install a community extension using Druid's pull-deps tool. The example shows configuration for extension directories and Maven coordinates.

LANGUAGE: bash
CODE:
java \
  -cp "lib/*" \
  -Ddruid.extensions.directory="extensions" \
  -Ddruid.extensions.hadoopDependenciesDir="hadoop-dependencies" \
  org.apache.druid.cli.Main tools pull-deps \
  --no-default-hadoop \
  -c "com.example:druid-example-extension:1.0.0"

----------------------------------------

TITLE: Configuring LongMin Aggregator in Druid
DESCRIPTION: The longMin aggregator computes the minimum of all metric values and Long.MAX_VALUE. It requires specifying an output name and the metric field to analyze.

LANGUAGE: json
CODE:
{ "type" : "longMin", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: Configuring Kafka Lookup in Druid
DESCRIPTION: JSON configuration for setting up a Kafka-based lookup in Druid. Specifies the Kafka topic and connection properties for dimension value mapping.

LANGUAGE: json
CODE:
{
  "type":"kafka",
  "kafkaTopic":"testTopic",
  "kafkaProperties":{"zookeeper.connect":"somehost:2181/kafka"}
}

----------------------------------------

TITLE: Creating an Interval Filter in Druid
DESCRIPTION: Demonstrates how to define an interval filter for range filtering on time-based columns. This example filters for specific date ranges in October and November 2014.

LANGUAGE: JSON
CODE:
{
    "type" : "interval",
    "dimension" : "__time",
    "intervals" : [
      "2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z",
      "2014-11-15T00:00:00.000Z/2014-11-16T00:00:00.000Z"
    ]
}

----------------------------------------

TITLE: Defining an IN Filter in Druid
DESCRIPTION: This snippet shows how to create an IN filter in Druid, which is equivalent to the SQL IN clause. It matches a dimension against a list of specified values.

LANGUAGE: json
CODE:
{
    "type": "in",
    "dimension": "outlaw",
    "values": ["Good", "Bad", "Ugly"]
}

----------------------------------------

TITLE: Configuring Protobuf Parser in Apache Druid
DESCRIPTION: JSON configuration for the Protobuf parser in Apache Druid. It specifies the parser type, descriptor file, message type, and parse specification.

LANGUAGE: json
CODE:
{
  "type": "protobuf",
  "descriptor": "file:///tmp/metrics.desc",
  "protoMessageType": "Metrics",
  "parseSpec": {
    "format": "json",
    "timestampSpec": {
      "column": "timestamp",
      "format": "auto"
    },
    "dimensionsSpec": {
      "dimensions": [
        "unit",
        "http_method",
        "http_code",
        "page",
        "metricType",
        "server"
      ],
      "dimensionExclusions": [
        "timestamp",
        "value"
      ]
    }
  }
}

----------------------------------------

TITLE: Defining Protobuf Message Structure
DESCRIPTION: This Protobuf schema defines the structure of the metrics message. It includes fields for various metrics data points.

LANGUAGE: protobuf
CODE:
syntax = "proto3";
message Metrics {
  string unit = 1;
  string http_method = 2;
  int32 value = 3;
  string timestamp = 4;
  string http_code = 5;
  string page = 6;
  string metricType = 7;
  string server = 8;
}

----------------------------------------

TITLE: Theta Sketch Estimate Post-Aggregator
DESCRIPTION: Configuration for the thetaSketchEstimate post-aggregator that estimates unique counts from sketch data.

LANGUAGE: json
CODE:
{
  "type"  : "thetaSketchEstimate",
  "name": <output name>,
  "field"  : <post aggregator of type fieldAccess that refers to a thetaSketch aggregator or that of type thetaSketchSetOp>
}

----------------------------------------

TITLE: Example Compaction Task Configuration in Apache Druid
DESCRIPTION: This example demonstrates a simple compaction task configuration for Apache Druid. It specifies the task type as 'compact', targets the 'wikipedia' data source, and defines an interval for the segments to be compacted.

LANGUAGE: json
CODE:
{
  "type" : "compact",
  "dataSource" : "wikipedia",
  "interval" : "2017-01-01/2018-01-01"
}

----------------------------------------

TITLE: Configuring Event Count Metric in Druid Ingestion Spec
DESCRIPTION: Example of how to configure a count metric in the metricsSpec section of a Druid ingestion specification.

LANGUAGE: json
CODE:
"metricsSpec" : [
      {
        "type" : "count",
        "name" : "count"
      }
]

----------------------------------------

TITLE: Basic TopN Query Structure in Druid
DESCRIPTION: Example of a basic TopN query structure showing core components including dataSource, dimension, threshold, metric, granularity, filters, aggregations, and post-aggregations. The query returns top 5 results based on count metric.

LANGUAGE: json
CODE:
{
  "queryType": "topN",
  "dataSource": "sample_data",
  "dimension": "sample_dim",
  "threshold": 5,
  "metric": "count",
  "granularity": "all",
  "filter": {
    "type": "and",
    "fields": [
      {
        "type": "selector",
        "dimension": "dim1",
        "value": "some_value"
      },
      {
        "type": "selector",
        "dimension": "dim2",
        "value": "some_other_val"
      }
    ]
  },
  "aggregations": [
    {
      "type": "longSum",
      "name": "count",
      "fieldName": "count"
    },
    {
      "type": "doubleSum",
      "name": "some_metric",
      "fieldName": "some_metric"
    }
  ],
  "postAggregations": [
    {
      "type": "arithmetic",
      "name": "average",
      "fn": "/",
      "fields": [
        {
          "type": "fieldAccess",
          "name": "some_metric",
          "fieldName": "some_metric"
        },
        {
          "type": "fieldAccess",
          "name": "count",
          "fieldName": "count"
        }
      ]
    }
  ],
  "intervals": [
    "2013-08-31T00:00:00.000/2013-09-03T00:00:00.000"
  ]
}

----------------------------------------

TITLE: Configuring Filtered Aggregator in Druid JSON
DESCRIPTION: The filtered aggregator wraps any given aggregator and only aggregates values that match the specified dimension filter. This allows simultaneous computation of filtered and unfiltered aggregations without multiple queries.

LANGUAGE: json
CODE:
{
  "type" : "filtered",
  "filter" : {
    "type" : "selector",
    "dimension" : <dimension>,
    "value" : <dimension value>
  }
  "aggregator" : <aggregation>
}

----------------------------------------

TITLE: Creating Cassandra Storage Schema
DESCRIPTION: SQL create statements for the required Cassandra tables: index_storage for storing compressed segments and descriptor_storage for segment metadata. The index_storage table uses chunked object storage for large segments.

LANGUAGE: sql
CODE:
CREATE TABLE index_storage(key text,
                           chunk text,
                           value blob,
                           PRIMARY KEY (key, chunk)) WITH COMPACT STORAGE;

CREATE TABLE descriptor_storage(key varchar,
                                lastModified timestamp,
                                descriptor varchar,
                                PRIMARY KEY (key)) WITH COMPACT STORAGE;

----------------------------------------

TITLE: Implementing Arithmetic Post-Aggregation in Druid
DESCRIPTION: Demonstrates the structure for arithmetic post-aggregation that applies mathematical operations to aggregated fields. Supports operations like +, -, *, /, and quotient with optional numeric ordering.

LANGUAGE: json
CODE:
{
  "type"  : "arithmetic",
  "name"  : <output_name>,
  "fn"    : <arithmetic_function>,
  "fields": [<post_aggregator>, <post_aggregator>, ...],
  "ordering" : <null (default), or "numericFirst">
}

----------------------------------------

TITLE: Checking Historical Segment Load Status
DESCRIPTION: Example JSON response from the /druid/historical/v1/loadstatus endpoint to check if all segments are loaded on a Historical node.

LANGUAGE: json
CODE:
{"cacheInitialized":<value>}

----------------------------------------

TITLE: Accessing Druid Console URL
DESCRIPTION: The URL pattern for accessing the main Druid Console, which is hosted by the Router process. This console provides comprehensive management capabilities for datasources, segments, tasks, and cluster configuration.

LANGUAGE: plaintext
CODE:
http://<ROUTER_IP>:<ROUTER_PORT>

----------------------------------------

TITLE: Running the Router Process in Java
DESCRIPTION: Command to start the Router process in Apache Druid using the Main class.

LANGUAGE: java
CODE:
org.apache.druid.cli.Main server router

----------------------------------------

TITLE: Configuring Jetty Server TLS Settings in Apache Druid
DESCRIPTION: These properties configure the TLS/SSL settings for the embedded Jetty web server in Apache Druid. They specify the keystore path, type, certificate alias, and password for HTTPS connections.

LANGUAGE: properties
CODE:
druid.server.https.keyStorePath=
druid.server.https.keyStoreType=
druid.server.https.certAlias=
druid.server.https.keyStorePassword=

----------------------------------------

TITLE: Example Compaction Task Configuration in Apache Druid
DESCRIPTION: This example demonstrates a simple compaction task configuration for Apache Druid. It specifies the task type as 'compact', targets the 'wikipedia' data source, and defines an interval for the segments to be compacted.

LANGUAGE: json
CODE:
{
  "type" : "compact",
  "dataSource" : "wikipedia",
  "interval" : "2017-01-01/2018-01-01"
}

----------------------------------------

TITLE: Executing a groupBy Query in Apache Druid
DESCRIPTION: Example of a groupBy query object in Apache Druid, demonstrating various query components such as dimensions, filters, aggregations, and post-aggregations.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "sample_datasource",
  "granularity": "day",
  "dimensions": ["country", "device"],
  "limitSpec": { "type": "default", "limit": 5000, "columns": ["country", "data_transfer"] },
  "filter": {
    "type": "and",
    "fields": [
      { "type": "selector", "dimension": "carrier", "value": "AT&T" },
      { "type": "or", 
        "fields": [
          { "type": "selector", "dimension": "make", "value": "Apple" },
          { "type": "selector", "dimension": "make", "value": "Samsung" }
        ]
      }
    ]
  },
  "aggregations": [
    { "type": "longSum", "name": "total_usage", "fieldName": "user_count" },
    { "type": "doubleSum", "name": "data_transfer", "fieldName": "data_transfer" }
  ],
  "postAggregations": [
    { "type": "arithmetic",
      "name": "avg_usage",
      "fn": "/",
      "fields": [
        { "type": "fieldAccess", "fieldName": "data_transfer" },
        { "type": "fieldAccess", "fieldName": "total_usage" }
      ]
    }
  ],
  "intervals": [ "2012-01-01T00:00:00.000/2012-01-03T00:00:00.000" ],
  "having": {
    "type": "greaterThan",
    "aggregation": "total_usage",
    "value": 100
  }
}

----------------------------------------

TITLE: Configuring Event Count Metric in Druid Ingestion Spec
DESCRIPTION: Example of how to configure a count metric in the metricsSpec section of a Druid ingestion specification.

LANGUAGE: json
CODE:
"metricsSpec" : [
      {
        "type" : "count",
        "name" : "count"
      }
]

----------------------------------------

TITLE: Configuring Realtime Node Spec File in Druid
DESCRIPTION: Example JSON configuration for a Realtime node spec file in Druid. This defines the data schema, IO config, and tuning parameters for stream pull ingestion.

LANGUAGE: json
CODE:
[
  {
    "dataSchema" : {
      "dataSource" : "wikipedia",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "timestampSpec" : {
            "column" : "timestamp",
            "format" : "auto"
          },
          "dimensionsSpec" : {
            "dimensions": ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"],
            "dimensionExclusions" : [],
            "spatialDimensions" : []
          }
        }
      },
      "metricsSpec" : [{
        "type" : "count",
        "name" : "count"
      }, {
        "type" : "doubleSum",
        "name" : "added",
        "fieldName" : "added"
      }, {
        "type" : "doubleSum",
        "name" : "deleted",
        "fieldName" : "deleted"
      }, {
        "type" : "doubleSum",
        "name" : "delta",
        "fieldName" : "delta"
      }],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "DAY",
        "queryGranularity" : "NONE"
      }
    },
    "ioConfig" : {
      "type" : "realtime",
      "firehose": {
        "type": "kafka-0.8",
        "consumerProps": {
          "zookeeper.connect": "localhost:2181",
          "zookeeper.connection.timeout.ms" : "15000",
          "zookeeper.session.timeout.ms" : "15000",
          "zookeeper.sync.time.ms" : "5000",
          "group.id": "druid-example",
          "fetch.message.max.bytes" : "1048586",
          "auto.offset.reset": "largest",
          "auto.commit.enable": "false"
        },
        "feed": "wikipedia"
      },
      "plumber": {
        "type": "realtime"
      }
    },
    "tuningConfig": {
      "type" : "realtime",
      "maxRowsInMemory": 1000000,
      "intermediatePersistPeriod": "PT10M",
      "windowPeriod": "PT10M",
      "basePersistDirectory": "\/tmp\/realtime\/basePersist",
      "rejectionPolicy": {
        "type": "serverTime"
      }
    }
  }
]

----------------------------------------

TITLE: Segment Identifier Example
DESCRIPTION: Example of how Druid segment identifiers are structured, showing the format for a segment in datasource 'clarity-cloud0' with timestamp and partition information.

LANGUAGE: plaintext
CODE:
clarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z_1

----------------------------------------

TITLE: Theta Sketch Estimate Post-Aggregator
DESCRIPTION: Post-aggregator configuration for estimating cardinality from a Theta sketch field.

LANGUAGE: json
CODE:
{
  "type"  : "thetaSketchEstimate",
  "name": <output name>,
  "field"  : <post aggregator of type fieldAccess that refers to a thetaSketch aggregator or that of type thetaSketchSetOp>
}

----------------------------------------

TITLE: Executing SQL Timeseries Query in Druid
DESCRIPTION: This snippet shows a SQL Timeseries query that calculates the sum of deleted lines per hour. It demonstrates how to use time floor functions and aggregations in Druid SQL.

LANGUAGE: sql
CODE:
SELECT FLOOR(__time to HOUR) AS HourTime, SUM(deleted) AS LinesDeleted FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY FLOOR(__time to HOUR);

----------------------------------------

TITLE: Histogram Post-Aggregator Examples
DESCRIPTION: Various post-aggregator configurations for transforming histogram data, including equal buckets, custom buckets, min/max, and quantile calculations.

LANGUAGE: json
CODE:
{
  "type": "equalBuckets",
  "name": "<output_name>",
  "fieldName": "<aggregator_name>",
  "numBuckets": <count>
}

LANGUAGE: json
CODE:
{
  "type": "buckets",
  "name": "<output_name>",
  "fieldName": "<aggregator_name>",
  "bucketSize": <bucket_size>,
  "offset": <offset>
}

LANGUAGE: json
CODE:
{ "type" : "customBuckets", "name" : <output_name>, "fieldName" : <aggregator_name>,
  "breaks" : [ <value>, <value>, ... ] }

----------------------------------------

TITLE: Executing Segment Metadata Query in Apache Druid
DESCRIPTION: This JSON snippet demonstrates how to structure a basic segment metadata query in Apache Druid. It specifies the query type, data source, and time interval for the query.

LANGUAGE: json
CODE:
{
  "queryType":"segmentMetadata",
  "dataSource":"sample_datasource",
  "intervals":["2013-01-01/2014-01-01"]
}

----------------------------------------

TITLE: Connecting to MySQL Command Line
DESCRIPTION: Command to connect to MySQL as root user for database administration.

LANGUAGE: bash
CODE:
> mysql -u root

----------------------------------------

TITLE: Configuring Ingest Segment Firehose in Druid
DESCRIPTION: Configuration for IngestSegmentFirehose that reads data from existing Druid segments. Allows reingesting data with new schema and segment properties.

LANGUAGE: json
CODE:
{
    "type"    : "ingestSegment",
    "dataSource"   : "wikipedia",
    "interval" : "2013-01-01/2013-01-02"
}

----------------------------------------

TITLE: Connecting to MySQL Command Line
DESCRIPTION: Command to connect to MySQL as root user for database administration.

LANGUAGE: bash
CODE:
> mysql -u root

----------------------------------------

TITLE: Executing Segment Metadata Query in Apache Druid
DESCRIPTION: This JSON snippet demonstrates how to structure a basic segment metadata query in Apache Druid. It specifies the query type, data source, and time interval for the query.

LANGUAGE: json
CODE:
{
  "queryType":"segmentMetadata",
  "dataSource":"sample_datasource",
  "intervals":["2013-01-01/2014-01-01"]
}

----------------------------------------

TITLE: Defining Protobuf Message Structure
DESCRIPTION: Protobuf message definition for the metrics data structure. It defines fields for various metrics attributes such as unit, HTTP method, value, timestamp, and server.

LANGUAGE: protobuf
CODE:
syntax = "proto3";
message Metrics {
  string unit = 1;
  string http_method = 2;
  int32 value = 3;
  string timestamp = 4;
  string http_code = 5;
  string page = 6;
  string metricType = 7;
  string server = 8;
}

----------------------------------------

TITLE: Configuring Cached Namespace Lookup with JDBC Source
DESCRIPTION: Example configuration for a globally cached lookup using a JDBC source connecting to MySQL database.

LANGUAGE: json
CODE:
{
    "type": "cachedNamespace",
    "extractionNamespace": {
       "type": "jdbc",
       "connectorConfig": {
         "createTables": true,
         "connectURI": "jdbc:mysql:\/\/localhost:3306\/druid",
         "user": "druid",
         "password": "diurd"
       },
       "table": "lookupTable",
       "keyColumn": "mykeyColumn",
       "valueColumn": "myValueColumn",
       "filter" : "myFilterSQL (Where clause statement  e.g LOOKUPTYPE=1)",
       "tsColumn": "timeColumn"
    },
    "firstCacheTimeout": 120000,
    "injective":true
}

----------------------------------------

TITLE: Example MomentSketch Aggregator Configuration for Druid Ingestion
DESCRIPTION: Example JSON configuration for setting up a MomentSketch aggregator during data ingestion in Druid.

LANGUAGE: json
CODE:
{
  "type": "momentSketch", 
  "name": "sketch", 
  "fieldName": "value", 
  "k": 10, 
  "compress": true,
}

----------------------------------------

TITLE: Defining Expression Virtual Column in Apache Druid
DESCRIPTION: This snippet shows the syntax for defining an expression virtual column in Apache Druid. It includes properties for specifying the column name, the expression to compute the column value, and the output type.

LANGUAGE: json
CODE:
{
  "type": "expression",
  "name": <name of the virtual column>,
  "expression": <row expression>,
  "outputType": <output value type of expression>
}

----------------------------------------

TITLE: Counting Retained Entries in ArrayOfDoublesSketch
DESCRIPTION: This JSON configuration defines a post-aggregator to count the number of retained entries in an ArrayOfDoublesSketch in Druid.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToNumEntries",
  "name": <output name>,
  "field"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>
}

----------------------------------------

TITLE: Counting Retained Entries in ArrayOfDoublesSketch
DESCRIPTION: This JSON configuration defines a post-aggregator to count the number of retained entries in an ArrayOfDoublesSketch in Druid.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToNumEntries",
  "name": <output name>,
  "field"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>
}

----------------------------------------

TITLE: Druid Ingestion Task Specification with Roll-up Enabled
DESCRIPTION: JSON configuration for a Druid ingestion task that enables roll-up and defines the schema for the network flow event data.

LANGUAGE: json
CODE:
{
  "type" : "index",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "rollup-tutorial",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "dimensionsSpec" : {
            "dimensions" : [
              "srcIP",
              "dstIP"
            ]
          },
          "timestampSpec": {
            "column": "timestamp",
            "format": "iso"
          }
        }
      },
      "metricsSpec" : [
        { "type" : "count", "name" : "count" },
        { "type" : "longSum", "name" : "packets", "fieldName" : "packets" },
        { "type" : "longSum", "name" : "bytes", "fieldName" : "bytes" }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "week",
        "queryGranularity" : "minute",
        "intervals" : ["2018-01-01/2018-01-03"],
        "rollup" : true
      }
    },
    "ioConfig" : {
      "type" : "index",
      "firehose" : {
        "type" : "local",
        "baseDir" : "quickstart/tutorial",
        "filter" : "rollup-data.json"
      },
      "appendToExisting" : false
    },
    "tuningConfig" : {
      "type" : "index",
      "targetPartitionSize" : 5000000,
      "maxRowsInMemory" : 25000,
      "forceExtendableShardSpecs" : true
    }
  }
}

----------------------------------------

TITLE: Retrieving Completion Report JSON in Apache Druid
DESCRIPTION: Example of a completion report JSON structure returned by Druid after a task completes. It includes task ID, ingestion state, row statistics, and error information.

LANGUAGE: json
CODE:
{
  "ingestionStatsAndErrors": {
    "taskId": "compact_twitter_2018-09-24T18:24:23.920Z",
    "payload": {
      "ingestionState": "COMPLETED",
      "unparseableEvents": {},
      "rowStats": {
        "determinePartitions": {
          "processed": 0,
          "processedWithError": 0,
          "thrownAway": 0,
          "unparseable": 0
        },
        "buildSegments": {
          "processed": 5390324,
          "processedWithError": 0,
          "thrownAway": 0,
          "unparseable": 0
        }
      },
      "errorMsg": null
    },
    "type": "ingestionStatsAndErrors"
  }
}

----------------------------------------

TITLE: ArrayOfDoublesSketch Entries Count Post-Aggregator
DESCRIPTION: JSON configuration for post-aggregator that returns number of retained entries from ArrayOfDoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToNumEntries",
  "name": <output name>,
  "field"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>
}

----------------------------------------

TITLE: Calculating Percentage Using Post-Aggregators in Druid JSON Query
DESCRIPTION: Provides an example of using post-aggregators to calculate a percentage in a Druid query. It demonstrates the use of arithmetic post-aggregators with field access and constant post-aggregators.

LANGUAGE: json
CODE:
{
  "aggregations" : [
    { "type" : "doubleSum", "name" : "tot", "fieldName" : "total" },
    { "type" : "doubleSum", "name" : "part", "fieldName" : "part" }
  ],
  "postAggregations" : [{
    "type"   : "arithmetic",
    "name"   : "part_percentage",
    "fn"     : "*",
    "fields" : [
       { "type"   : "arithmetic",
         "name"   : "ratio",
         "fn"     : "/",
         "fields" : [
           { "type" : "fieldAccess", "name" : "part", "fieldName" : "part" },
           { "type" : "fieldAccess", "name" : "tot", "fieldName" : "tot" }
         ]
       },
       { "type" : "constant", "name": "const", "value" : 100 }
    ]
  }]
}

----------------------------------------

TITLE: Setting Task Priority in Druid Context
DESCRIPTION: JSON configuration snippet showing how to override the default task priority in Druid task context. The priority value determines the task's ability to preempt locks from other tasks.

LANGUAGE: json
CODE:
{
  "context" : {
    "priority" : 100
  }
}

----------------------------------------

TITLE: Generating Summary of ArrayOfDoublesSketch
DESCRIPTION: This JSON configuration defines a post-aggregator that returns a human-readable summary of an ArrayOfDoublesSketch, useful for debugging purposes.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToString",
  "name": <output name>,
  "field"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>
}

----------------------------------------

TITLE: Configuring Parquet-Avro Parser with Avro ParseSpec in Druid
DESCRIPTION: This snippet illustrates how to configure the Parquet-Avro parser with an Avro parseSpec in a Druid ingestion spec. It includes settings for input format, flattenSpec, timestampSpec, and dimensionsSpec.

LANGUAGE: json
CODE:
{
  "type": "index_hadoop",
  "spec": {
    "ioConfig": {
      "type": "hadoop",
      "inputSpec": {
        "type": "static",
        "inputFormat": "org.apache.druid.data.input.parquet.DruidParquetAvroInputFormat",
        "paths": "path/to/file.parquet"
      },
      ...
    },
    "dataSchema": {
      "dataSource": "example",
      "parser": {
        "type": "parquet-avro",
        "parseSpec": {
          "format": "avro",
          "flattenSpec": {
            "useFieldDiscovery": true,
            "fields": [
              {
                "type": "path",
                "name": "nestedDim",
                "expr": "$.nestedData.dim1"
              },
              {
                "type": "path",
                "name": "listDimFirstItem",
                "expr": "$.listDim[1]"
              }
            ]
          },
          "timestampSpec": {
            "column": "timestamp",
            "format": "auto"
          },
          "dimensionsSpec": {
            "dimensions": [],
            "dimensionExclusions": [],
            "spatialDimensions": []
          }
        }
      },
      ...
    },
    "tuningConfig": <hadoop-tuning-config>
    }
  }
}

----------------------------------------

TITLE: Configuring Druid for MySQL Metadata Storage
DESCRIPTION: Properties to be added to the Druid configuration file to enable and configure MySQL metadata storage.

LANGUAGE: properties
CODE:
druid.extensions.loadList=["mysql-metadata-storage"]
druid.metadata.storage.type=mysql
druid.metadata.storage.connector.connectURI=jdbc:mysql://<host>/druid
druid.metadata.storage.connector.user=druid
druid.metadata.storage.connector.password=druid

----------------------------------------

TITLE: Running Druid MiddleManager Server - Java
DESCRIPTION: Command to start the Druid MiddleManager server process, which manages task execution and Peon workers. This command initializes the MiddleManager that coordinates task distribution across multiple Peon JVMs.

LANGUAGE: java
CODE:
org.apache.druid.cli.Main server middleManager

----------------------------------------

TITLE: Theta Sketch Set Operations Configuration
DESCRIPTION: Configuration for set operations (UNION, INTERSECT, NOT) between multiple theta sketches.

LANGUAGE: json
CODE:
{
  "type"  : "thetaSketchSetOp",
  "name": <output name>,
  "func": <UNION|INTERSECT|NOT>,
  "fields"  : <array of fieldAccess type post aggregators>,
  "size": <16384 by default>
}

----------------------------------------

TITLE: Executing DumpSegment Tool in Java
DESCRIPTION: Command to run the DumpSegment tool from command line. This command requires the Druid library classpath and accepts parameters for input directory and output file location.

LANGUAGE: java
CODE:
java -classpath "/my/druid/lib/*" org.apache.druid.cli.Main tools dump-segment \
  --directory /home/druid/path/to/segment/ \
  --out /home/druid/output.txt

----------------------------------------

TITLE: Configuring JVM Flags for Apache Druid
DESCRIPTION: A set of recommended JVM flags for optimizing Druid performance, including timezone settings, memory management, garbage collection logging, and error handling.

LANGUAGE: java
CODE:
-Duser.timezone=UTC
-Dfile.encoding=UTF-8
-Djava.io.tmpdir=<something other than /tmp which might be mounted to volatile tmpfs file system>
-Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
-Dorg.jboss.logging.provider=slf4j
-Dnet.spy.log.LoggerImpl=net.spy.memcached.compat.log.SLF4JLogger
-Dlog4j.shutdownCallbackRegistry=org.apache.druid.common.config.Log4jShutdown
-Dlog4j.shutdownHookEnabled=true
-XX:+PrintGCDetails
-XX:+PrintGCDateStamps
-XX:+PrintGCTimeStamps
-XX:+PrintGCApplicationStoppedTime
-XX:+PrintGCApplicationConcurrentTime
-Xloggc:/var/logs/druid/historical.gc.log
-XX:+UseGCLogFileRotation
-XX:NumberOfGCLogFiles=50
-XX:GCLogFileSize=10m
-XX:+ExitOnOutOfMemoryError
-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/var/logs/druid/historical.hprof
-XX:MaxDirectMemorySize=10240g

----------------------------------------

TITLE: Metrics Specification for Flattened Fields
DESCRIPTION: Example of how to configure metrics aggregators using the flattened field names.

LANGUAGE: json
CODE:
"metricsSpec" : [ 
{
  "type" : "longSum",
  "name" : "path-metric-sum",
  "fieldName" : "path-metric"
}, 
{
  "type" : "doubleSum",
  "name" : "hello-0-sum",
  "fieldName" : "hello-0"
},
{
  "type" : "longSum",
  "name" : "metrica-sum",
  "fieldName" : "metrica"
}
]

----------------------------------------

TITLE: Scan Query List Format Results
DESCRIPTION: Example response when resultFormat is set to 'list', showing segmentId, columns, and events array with detailed row data.

LANGUAGE: json
CODE:
[{
    "segmentId" : "wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9",
    "columns" : [
      "timestamp",
      "robot",
      "namespace",
      "anonymous",
      "unpatrolled",
      "page",
      "language",
      "newpage",
      "user",
      "count",
      "added",
      "delta",
      "variation",
      "deleted"
    ],
    "events" : [ {
        "timestamp" : "2013-01-01T00:00:00.000Z",
        "robot" : "1",
        "namespace" : "article",
        "anonymous" : "0",
        "unpatrolled" : "0",
        "page" : "11._korpus_(NOVJ)",
        "language" : "sl",
        "newpage" : "0",
        "user" : "EmausBot",
        "count" : 1.0,
        "added" : 39.0,
        "delta" : 39.0,
        "variation" : 39.0,
        "deleted" : 0.0
    }]}

----------------------------------------

TITLE: Querying with Period Granularity in Apache Druid
DESCRIPTION: Example of a groupBy query in Apache Druid using period granularity of 1 day in Pacific timezone. This query aggregates data by language and count for each day in the specified timezone.

LANGUAGE: json
CODE:
{
   "queryType":"groupBy",
   "dataSource":"my_dataSource",
   "granularity":{"type": "period", "period": "P1D", "timeZone": "America/Los_Angeles"},
   "dimensions":[
      "language"
   ],
   "aggregations":[
      {
         "type":"count",
         "name":"count"
      }
   ],
   "intervals":[
      "1999-12-31T16:00:00.000-08:00/2999-12-31T16:00:00.000-08:00"
   ]
}

----------------------------------------

TITLE: Defining Logical AND Filter in Druid
DESCRIPTION: This example demonstrates how to create an AND logical filter in Druid, which combines multiple filters. All specified conditions must be true for a row to be included.

LANGUAGE: json
CODE:
"filter": { "type": "and", "fields": [<filter>, <filter>, ...] }

----------------------------------------

TITLE: Querying with Period Granularity in Apache Druid
DESCRIPTION: Example of a groupBy query in Apache Druid using period granularity of 1 day in Pacific timezone. This query aggregates data by language and count for each day in the specified timezone.

LANGUAGE: json
CODE:
{
   "queryType":"groupBy",
   "dataSource":"my_dataSource",
   "granularity":{"type": "period", "period": "P1D", "timeZone": "America/Los_Angeles"},
   "dimensions":[
      "language"
   ],
   "aggregations":[
      {
         "type":"count",
         "name":"count"
      }
   ],
   "intervals":[
      "1999-12-31T16:00:00.000-08:00/2999-12-31T16:00:00.000-08:00"
   ]
}

----------------------------------------

TITLE: Querying Data Source Metadata in Druid
DESCRIPTION: JSON query structure for retrieving metadata information from a Druid datasource. The query returns the timestamp of the latest ingested event without considering rollup.

LANGUAGE: json
CODE:
{
    "queryType" : "dataSourceMetadata",
    "dataSource": "sample_datasource"
}

----------------------------------------

TITLE: Complete Tier Configuration Example
DESCRIPTION: Example of a complete lookup configuration for a single tier with a country code lookup using JDBC.

LANGUAGE: json
CODE:
{
  "realtime_customer2": {
    "country_code": {
      "version": "v0",
      "lookupExtractorFactory": {
        "type": "cachedNamespace",
        "extractionNamespace": {
          "type": "jdbc",
          "connectorConfig": {
            "createTables": true,
            "connectURI": "jdbc:mysql:\/\/localhost:3306\/druid",
            "user": "druid",
            "password": "diurd"
          },
          "table": "lookupValues",
          "keyColumn": "value_id",
          "valueColumn": "value_text",
          "filter": "value_type='country'",
          "tsColumn": "timeColumn"
        },
        "firstCacheTimeout": 120000,
        "injective": true
      }
    }
  }
}

----------------------------------------

TITLE: Configuring SqlFirehose in Apache Druid
DESCRIPTION: This snippet demonstrates the configuration of a SqlFirehose in Druid for ingesting events from RDBMS. It includes database connection details and SQL queries for data retrieval.

LANGUAGE: json
CODE:
{
    "type" : "sql",
    "database": {
        "type": "mysql",
        "connectorConfig" : {
            "connectURI" : "jdbc:mysql://host:port/schema",
            "user" : "user",
            "password" : "password"
        }
     },
    "sqls" : ["SELECT * FROM table1", "SELECT * FROM table2"]
}

----------------------------------------

TITLE: Querying for Ingested Event Count in Druid
DESCRIPTION: This JSON snippet demonstrates how to query for the number of ingested events using a longSum aggregator in Druid.

LANGUAGE: json
CODE:
"aggregations": [
    { "type": "longSum", "name": "numIngestedEvents", "fieldName": "count" }
]

----------------------------------------

TITLE: JSON Specification for Bloom Filter in Druid Queries
DESCRIPTION: This JSON snippet shows the structure for specifying a Bloom filter in Druid queries, including required and optional parameters.

LANGUAGE: json
CODE:
{
  "type" : "bloom",
  "dimension" : <dimension_name>,
  "bloomKFilter" : <serialized_bytes_for_BloomKFilter>,
  "extractionFn" : <extraction_fn>
}

----------------------------------------

TITLE: Configuring Druid for MySQL Metadata Storage
DESCRIPTION: Properties to be added to the Druid configuration file to enable and configure MySQL metadata storage.

LANGUAGE: properties
CODE:
druid.extensions.loadList=["mysql-metadata-storage"]
druid.metadata.storage.type=mysql
druid.metadata.storage.connector.connectURI=jdbc:mysql://<host>/druid
druid.metadata.storage.connector.user=druid
druid.metadata.storage.connector.password=druid

----------------------------------------

TITLE: Configuring Approximate Histogram Aggregator in Druid
DESCRIPTION: JSON configuration for setting up an approximate histogram aggregator. Includes parameters for resolution, bucket count, and value limits.

LANGUAGE: json
CODE:
{
  "type" : "approxHistogram or approxHistogramFold (at ingestion time), approxHistogramFold (at query time)",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "resolution" : <integer>,
  "numBuckets" : <integer>,
  "lowerLimit" : <float>,
  "upperLimit" : <float>
}

----------------------------------------

TITLE: Configuring Quantiles Doubles Sketch Aggregator in Druid
DESCRIPTION: JSON configuration for the quantilesDoublesSketch aggregator. This defines how to build or ingest sketches during data ingestion or querying.

LANGUAGE: json
CODE:
{
  "type" : "quantilesDoublesSketch",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "k": <parameter that controls size and accuracy>
 }

----------------------------------------

TITLE: Configuring DBCP Properties for Metadata Storage
DESCRIPTION: Example of setting custom DBCP (Database Connection Pool) properties for Druid's metadata storage connector. Shows how to configure connection lifetime and query timeout settings.

LANGUAGE: properties
CODE:
druid.metadata.storage.connector.dbcp.maxConnLifetimeMillis=1200000
druid.metadata.storage.connector.dbcp.defaultQueryTimeout=30000

----------------------------------------

TITLE: Theta Sketch Set Operations Configuration
DESCRIPTION: Configuration for performing set operations (UNION, INTERSECT, NOT) on Theta sketches.

LANGUAGE: json
CODE:
{
  "type"  : "thetaSketchSetOp",
  "name": <output name>,
  "func": <UNION|INTERSECT|NOT>,
  "fields"  : <array of fieldAccess type post aggregators>,
  "size": <16384 by default>
}

----------------------------------------

TITLE: Router Runtime Properties Configuration
DESCRIPTION: Core runtime properties configuration for the Router process including service settings and HTTP parameters

LANGUAGE: properties
CODE:
druid.host=#{IP_ADDR}:8080
druid.plaintextPort=8080
druid.service=druid/router

druid.router.defaultBrokerServiceName=druid:broker-cold
druid.router.coordinatorServiceName=druid:coordinator
druid.router.tierToBrokerMap={"hot":"druid:broker-hot","_default_tier":"druid:broker-cold"}
druid.router.http.numConnections=50
druid.router.http.readTimeout=PT5M

# Number of threads used by the Router proxy http client
druid.router.http.numMaxThreads=100

druid.server.http.numThreads=100

----------------------------------------

TITLE: Configuring Inline Schema Avro Bytes Decoder in Druid
DESCRIPTION: This JSON snippet shows how to configure an inline schema-based Avro bytes decoder. It includes a sample schema definition for a user record with FullName and Country fields.

LANGUAGE: json
CODE:
{
  "type": "schema_inline",
  "schema": {
    "namespace": "org.apache.druid.data",
    "name": "User",
    "type": "record",
    "fields": [
      { "name": "FullName", "type": "string" },
      { "name": "Country", "type": "string" }
    ]
  }
}

----------------------------------------

TITLE: Quantiles Doubles Sketch to String Post Aggregator
DESCRIPTION: Post aggregator configuration for generating a debug string representation of a DoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "quantilesDoublesSketchToString",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch>
}

----------------------------------------

TITLE: Creating a Bound Filter in Druid
DESCRIPTION: This example demonstrates how to define a bound filter in Druid for range filtering. It can be used for various comparison operations like greater than, less than, or between.

LANGUAGE: json
CODE:
{
    "type": "bound",
    "dimension": "age",
    "lower": "21",
    "upper": "31" ,
    "ordering": "numeric"
}

----------------------------------------

TITLE: Configuring TSV Data Ingestion in Apache Druid
DESCRIPTION: This snippet shows the parseSpec configuration for ingesting TSV (tab-separated values) formatted data into Druid. It specifies the delimiter, column names, and dimensions to be extracted.

LANGUAGE: json
CODE:
{
  "parseSpec": {
    "format" : "tsv",
    "timestampSpec" : {
      "column" : "timestamp"
    },
    "columns" : ["timestamp","page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city","added","deleted","delta"],
    "delimiter":"|",
    "dimensionsSpec" : {
      "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"]
    }
  }
}

----------------------------------------

TITLE: Executing SQL GroupBy Query in Druid
DESCRIPTION: This snippet demonstrates a SQL GroupBy query that calculates the sum of added lines per channel. It shows how to use GROUP BY, ORDER BY, and LIMIT clauses in Druid SQL.

LANGUAGE: sql
CODE:
SELECT channel, SUM(added) FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY channel ORDER BY SUM(added) DESC LIMIT 5;

----------------------------------------

TITLE: Expression Language Function Example
DESCRIPTION: Example of function syntax in Druid's expression language showing pattern for general functions like cast, if, nvl, and case statements.

LANGUAGE: Expression Language
CODE:
cast(expr,'LONG' or 'DOUBLE' or 'STRING')
if(predicate,then,else)
nvl(expr,expr-for-null)
like(expr, pattern[, escape])
case_searched(expr1, result1, [[expr2, result2, ...], else-result])
case_simple(expr, value1, result1, [[value2, result2, ...], else-result])

----------------------------------------

TITLE: Configuring TSV Data Ingestion in Apache Druid
DESCRIPTION: This snippet shows the parseSpec configuration for ingesting TSV (tab-separated values) formatted data into Druid. It specifies the delimiter, column names, and dimensions to be extracted.

LANGUAGE: json
CODE:
{
  "parseSpec": {
    "format" : "tsv",
    "timestampSpec" : {
      "column" : "timestamp"
    },
    "columns" : ["timestamp","page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city","added","deleted","delta"],
    "delimiter":"|",
    "dimensionsSpec" : {
      "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"]
    }
  }
}

----------------------------------------

TITLE: Configuring DBCP Properties for Metadata Storage
DESCRIPTION: Example of setting custom DBCP (Database Connection Pool) properties for Druid's metadata storage connector. Shows how to configure connection lifetime and query timeout settings.

LANGUAGE: properties
CODE:
druid.metadata.storage.connector.dbcp.maxConnLifetimeMillis=1200000
druid.metadata.storage.connector.dbcp.defaultQueryTimeout=30000

----------------------------------------

TITLE: Configuring Quantiles Doubles Sketch to String Post-Aggregator
DESCRIPTION: JSON configuration for the quantilesDoublesSketchToString post-aggregator. This returns a summary of the sketch that can be used for debugging, which is the result of calling the toString() method.

LANGUAGE: json
CODE:
{
  "type"  : "quantilesDoublesSketchToString",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>
}

----------------------------------------

TITLE: Example Compaction Task Configuration in JSON for Apache Druid
DESCRIPTION: This JSON example demonstrates a simple Compaction Task configuration for Apache Druid. It specifies the task type as 'compact', sets the data source to 'wikipedia', and defines the interval for compaction as the entire year of 2017.

LANGUAGE: json
CODE:
{
  "type" : "compact",
  "dataSource" : "wikipedia",
  "interval" : "2017-01-01/2018-01-01"
}

----------------------------------------

TITLE: Query Filter Having Clause in Druid
DESCRIPTION: Demonstrates how to use Druid query filters in the Having clause of a groupBy query. Allows all Druid query filters to be used for filtering results.

LANGUAGE: json
CODE:
{
    "queryType": "groupBy",
    "dataSource": "sample_datasource",
    ...
    "having": 
        {
            "type" : "filter",
            "filter" : <any Druid query filter>
        }
}

----------------------------------------

TITLE: Coordinator Leader Election Path in ZooKeeper
DESCRIPTION: The ZooKeeper path used for Coordinator leader election using Curator LeadershipLatch recipe.

LANGUAGE: plaintext
CODE:
${druid.zk.paths.coordinatorPath}/_COORDINATOR

----------------------------------------

TITLE: Using dsql Client for SQL Queries in Druid
DESCRIPTION: This snippet shows how to use the dsql command-line client to execute SQL queries in Druid. It demonstrates the interactive nature of the client and how to submit queries directly.

LANGUAGE: bash
CODE:
bin/dsql

LANGUAGE: sql
CODE:
SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;

----------------------------------------

TITLE: Configuring HDFS Storage Properties in Druid
DESCRIPTION: Configuration properties for setting up HDFS as deep storage in Druid. Includes settings for basic HDFS connection and Kerberos authentication.

LANGUAGE: yaml
CODE:
druid.storage.type: hdfs
druid.storage.storageDirectory: <directory-path>
druid.hadoop.security.kerberos.principal: druid@EXAMPLE.COM
druid.hadoop.security.kerberos.keytab: /etc/security/keytabs/druid.headlessUser.keytab

----------------------------------------

TITLE: Implementing FloatLast Aggregator in Druid
DESCRIPTION: The floatLast aggregator computes the metric value with the maximum timestamp or 0 if no row exists. It can only be used in queries, not in ingestion specs.

LANGUAGE: json
CODE:
{
  "type" : "floatLast",
  "name" : <output_name>,
  "fieldName" : <metric_name>
}

----------------------------------------

TITLE: Implementing zscore2sample Post Aggregator in Druid
DESCRIPTION: Configuration for calculating z-score using two-sample z-test. Converts binary variables to continuous variables by computing proportions from success counts and sample sizes.

LANGUAGE: json
CODE:
{
  "type": "zscore2sample",
  "name": "<output_name>",
  "successCount1": <post_aggregator> success count of sample 1,
  "sample1Size": <post_aggregaror> sample 1 size,
  "successCount2": <post_aggregator> success count of sample 2,
  "sample2Size" : <post_aggregator> sample 2 size
}

----------------------------------------

TITLE: Configuring HLLSketchMerge Aggregator in Druid
DESCRIPTION: This JSON configuration defines the HLLSketchMerge aggregator for merging HLL sketch objects during querying.

LANGUAGE: json
CODE:
{
  "type" : "HLLSketchMerge",
  "name" : <output name>,
  "fieldName" : <metric name>,
  "lgK" : <size and accuracy parameter>,
  "tgtHllType" : <target HLL type>
 }

----------------------------------------

TITLE: Manual Task Submission Using cURL
DESCRIPTION: Direct HTTP POST request to submit an ingestion task to Druid's indexing service without using the helper script.

LANGUAGE: bash
CODE:
curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-index.json http://localhost:8090/druid/indexer/v1/task

----------------------------------------

TITLE: Sketch Summary Post Aggregator Configuration
DESCRIPTION: JSON configuration for the sketch summary post aggregator that returns debug information about the sketch.

LANGUAGE: json
CODE:
{
  "type"  : "quantilesDoublesSketchToString",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch>
}

----------------------------------------

TITLE: ArrayOfDoublesSketch T-Test Post-Aggregator
DESCRIPTION: JSON configuration for post-aggregator that performs Student's t-test on two ArrayOfDoublesSketch instances.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchTTest",
  "name": <output name>,
  "fields"  : <array with two post aggregators to access sketch aggregators or post aggregators referring to an ArrayOfDoublesSketch>
}

----------------------------------------

TITLE: SQL Query Using Bloom Filter Aggregator in Druid
DESCRIPTION: This SQL snippet demonstrates how to compute a Bloom filter in a Druid SQL query using the BLOOM_FILTER aggregator function.

LANGUAGE: sql
CODE:
SELECT BLOOM_FILTER(<expression>, <max number of entries>) FROM druid.foo WHERE dim2 = 'abc'

----------------------------------------

TITLE: SQL Query Using Bloom Filter in Druid
DESCRIPTION: This SQL snippet demonstrates how to use a Bloom filter in a Druid SQL query's WHERE clause using the bloom_filter_test operator.

LANGUAGE: sql
CODE:
SELECT COUNT(*) FROM druid.foo WHERE bloom_filter_test(<expr>, '<serialized_bytes_for_BloomKFilter>')

----------------------------------------

TITLE: Loading Initial Data in Apache Druid
DESCRIPTION: This command loads the initial dataset into the 'updates-tutorial' datasource using a JSON spec file.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/updates-init-index.json

----------------------------------------

TITLE: Configuring Same Interval Merge Task in Apache Druid (Deprecated)
DESCRIPTION: JSON configuration for a Same Interval Merge task in Druid. This deprecated task is a shortcut of the merge task, merging all segments within a specified interval. It includes options for aggregations, rollup, and task context.

LANGUAGE: json
CODE:
{
    "type": "same_interval_merge",
    "id": <task_id>,
    "dataSource": <task_datasource>,
    "aggregations": <list of aggregators>,
    "rollup": <whether or not to rollup data during a merge>,
    "interval": <DataSegment objects in this interval are going to be merged>,
    "context": <task context>
}

----------------------------------------

TITLE: Running DumpSegment Tool in Java for Apache Druid
DESCRIPTION: This command demonstrates how to run the DumpSegment tool using Java. It specifies the classpath, main class, input directory, and output file.

LANGUAGE: java
CODE:
java -classpath "/my/druid/lib/*" org.apache.druid.cli.Main tools dump-segment \
  --directory /home/druid/path/to/segment/ \
  --out /home/druid/output.txt

----------------------------------------

TITLE: Configuring SQL Firehose in Druid
DESCRIPTION: Configuration for ingesting data from RDBMS sources using SQL queries. Supports multiple queries with configurable fetch and cache settings.

LANGUAGE: json
CODE:
{
    "type" : "sql",
    "database": {
        "type": "mysql",
        "connectorConfig" : {
        "connectURI" : "jdbc:mysql://host:port/schema",
        "user" : "user",
        "password" : "password"
        }
     },
    "sqls" : ["SELECT * FROM table1", "SELECT * FROM table2"]
}

----------------------------------------

TITLE: Configuring Globally Cached Lookup with JDBC Namespace
DESCRIPTION: Example configuration for a globally cached lookup using a JDBC namespace. It specifies database connection details, table and column information, and cache timeout.

LANGUAGE: json
CODE:
{
   "type": "cachedNamespace",
   "extractionNamespace": {
      "type": "jdbc",
      "connectorConfig": {
        "createTables": true,
        "connectURI": "jdbc:mysql:\/\/localhost:3306\/druid",
        "user": "druid",
        "password": "diurd"
      },
      "table": "lookupTable",
      "keyColumn": "mykeyColumn",
      "valueColumn": "myValueColumn",
      "filter" : "myFilterSQL (Where clause statement  e.g LOOKUPTYPE=1)",
      "tsColumn": "timeColumn"
   },
   "firstCacheTimeout": 120000,
   "injective":true
}

----------------------------------------

TITLE: Configuring Loading Off-heap MapDB Cache in Druid
DESCRIPTION: JSON configuration example for a loading lookup using MapDB off-heap cache with size and expiration settings.

LANGUAGE: json
CODE:
{
   "type":"loadingLookup",
   "dataFetcher":{ "type":"jdbcDataFetcher", "connectorConfig":"jdbc://mysql://localhost:3306/my_data_base", "table":"lookup_table_name", "keyColumn":"key_column_name", "valueColumn": "value_column_name"},
   "loadingCacheSpec":{"type":"mapDb", "maxEntriesSize":100000},
   "reverseLoadingCacheSpec":{"type":"mapDb", "maxStoreSize":5, "expireAfterAccess":100000, "expireAfterAccess":10000}
}

----------------------------------------

TITLE: Querying Lookups with Druid SQL
DESCRIPTION: Example of how to use the LOOKUP function in a Druid SQL query to apply a lookup named 'lookup-name' to a column.

LANGUAGE: SQL
CODE:
SELECT LOOKUP(column_name, 'lookup-name'), COUNT(*) FROM datasource GROUP BY 1

----------------------------------------

TITLE: Cloning Druid Source Repository
DESCRIPTION: Commands to clone the Apache Druid source code repository from GitHub and navigate to the project directory.

LANGUAGE: bash
CODE:
git clone git@github.com:apache/incubator-druid.git
cd druid

----------------------------------------

TITLE: Configuring Multi-InputSpec for Delta Ingestion in Druid
DESCRIPTION: This JSON configuration shows how to set up a 'multi' inputSpec for delta ingestion in Druid using Hadoop batch ingestion. It combines data from an existing Druid dataSource and a static file path, allowing for the ingestion of both existing and new data.

LANGUAGE: json
CODE:
"ioConfig" : {
  "type" : "hadoop",
  "inputSpec" : {
    "type" : "multi",
    "children": [
      {
        "type" : "dataSource",
        "ingestionSpec" : {
          "dataSource": "wikipedia",
          "intervals": ["2012-01-01T00:00:00.000/2012-01-03T00:00:00.000", "2012-01-05T00:00:00.000/2012-01-07T00:00:00.000"],
          "segments": [
            {
              "dataSource": "test1",
              "interval": "2012-01-01T00:00:00.000/2012-01-03T00:00:00.000",
              "version": "v2",
              "loadSpec": {
                "type": "local",
                "path": "/tmp/index1.zip"
              },
              "dimensions": "host",
              "metrics": "visited_sum,unique_hosts",
              "shardSpec": {
                "type": "none"
              },
              "binaryVersion": 9,
              "size": 2,
              "identifier": "test1_2000-01-01T00:00:00.000Z_3000-01-01T00:00:00.000Z_v2"
            }
          ]
        }
      },
      {
        "type" : "static",
        "paths": "/path/to/more/wikipedia/data/"
      }
    ]  
  },
  ...
}

----------------------------------------

TITLE: Registering Jackson Subtypes for Firehose Factory
DESCRIPTION: Example of registering a custom FirehoseFactory implementation as a Jackson subtype in a Druid module.

LANGUAGE: Java
CODE:
@Override
public List<? extends Module> getJacksonModules()
{
  return ImmutableList.of(
          new SimpleModule().registerSubtypes(new NamedType(StaticS3FirehoseFactory.class, "static-s3"))
  );
}

----------------------------------------

TITLE: Cloning Druid Source Repository
DESCRIPTION: Commands to clone the Apache Druid source code repository from GitHub and navigate to the project directory.

LANGUAGE: bash
CODE:
git clone git@github.com:apache/incubator-druid.git
cd druid

----------------------------------------

TITLE: JSON Specification for Bloom Filter Aggregator in Druid
DESCRIPTION: This JSON snippet shows the structure for specifying a Bloom filter aggregator in Druid queries, including required and optional parameters.

LANGUAGE: json
CODE:
{
      "type": "bloom",
      "name": <output_field_name>,
      "maxNumEntries": <maximum_number_of_elements_for_BloomKFilter>
      "field": <dimension_spec>
    }

----------------------------------------

TITLE: Including DataSketches Extension in Druid Configuration
DESCRIPTION: This snippet shows how to include the DataSketches extension in the Druid configuration file. This is required to use the HLL sketch aggregator.

LANGUAGE: json
CODE:
"druid.extensions.loadList":["druid-datasketches"]

----------------------------------------

TITLE: JSON Specification for Bloom Filter Aggregator in Druid
DESCRIPTION: This JSON snippet shows the structure for specifying a Bloom filter aggregator in Druid queries, including required and optional properties.

LANGUAGE: json
CODE:
{
      "type": "bloom",
      "name": <output_field_name>,
      "maxNumEntries": <maximum_number_of_elements_for_BloomKFilter>
      "field": <dimension_spec>
    }

----------------------------------------

TITLE: Sketch Summary Post-Aggregator
DESCRIPTION: Post-aggregator configuration for generating a human-readable summary of an ArrayOfDoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToString",
  "name": <output name>,
  "field"  : <post aggregator that refers to an ArrayOfDoublesSketch>
}

----------------------------------------

TITLE: JDBC Connection to Druid SQL
DESCRIPTION: Example Java code for connecting to Druid SQL using JDBC.

LANGUAGE: java
CODE:
String url = "jdbc:avatica:remote:url=http://localhost:8082/druid/v2/sql/avatica/";

Properties connectionProperties = new Properties();

try (Connection connection = DriverManager.getConnection(url, connectionProperties)) {
  try (
      final Statement statement = connection.createStatement();
      final ResultSet resultSet = statement.executeQuery(query)
  ) {
    while (resultSet.next()) {
      // Do something
    }
  }
}

----------------------------------------

TITLE: Implementing TopN Query with DistinctCount in Druid
DESCRIPTION: Example of a TopN query using the DistinctCount aggregator to find the top 5 values of sample_dim based on unique visitor counts.

LANGUAGE: json
CODE:
{
  "queryType": "topN",
  "dataSource": "sample_datasource",
  "dimension": "sample_dim",
  "threshold": 5,
  "metric": "uv",
  "granularity": "all",
  "aggregations": [
    {
      "type": "distinctCount",
      "name": "uv",
      "fieldName": "visitor_id"
    }
  ],
  "intervals": [
    "2016-03-06T00:00:00/2016-03-06T23:59:59"
  ]
}

----------------------------------------

TITLE: Compaction Task Specification with Original Granularity
DESCRIPTION: JSON configuration for compacting segments while maintaining the original hourly granularity. Specifies datasource, time interval, and tuning parameters.

LANGUAGE: json
CODE:
{
  "type": "compact",
  "dataSource": "compaction-tutorial",
  "interval": "2015-09-12/2015-09-13",
  "tuningConfig" : {
    "type" : "index",
    "maxRowsPerSegment" : 5000000,
    "maxRowsInMemory" : 25000,
    "forceExtendableShardSpecs" : true
  }
}

----------------------------------------

TITLE: Configuring Blacklisted Workers in Apache Druid Overlord
DESCRIPTION: These properties configure the threshold and timeouts for blacklisting MiddleManagers in Apache Druid. They control the maximum retries before blacklisting, backoff time, cleanup period, and maximum percentage of blacklisted workers.

LANGUAGE: properties
CODE:
druid.indexer.runner.maxRetriesBeforeBlacklist
druid.indexer.runner.workerBlackListBackoffTime
druid.indexer.runner.workerBlackListCleanupPeriod
druid.indexer.runner.maxPercentageBlacklistWorkers

----------------------------------------

TITLE: Creating Basic Authorizer Configuration
DESCRIPTION: Configuration properties for setting up role-based authorization in Druid.

LANGUAGE: properties
CODE:
druid.auth.authorizers=["MyBasicAuthorizer"]

druid.auth.authorizer.MyBasicAuthorizer.type=basic

----------------------------------------

TITLE: Retrieving Scan Query Results in Compacted List Format
DESCRIPTION: Example of Scan query results when resultFormat is set to 'compactedList'. It shows the segmentId, columns, and events in a more compact format.

LANGUAGE: json
CODE:
[{
    "segmentId" : "wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9",
    "columns" : [
      "timestamp", "robot", "namespace", "anonymous", "unpatrolled", "page", "language", "newpage", "user", "count", "added", "delta", "variation", "deleted"
    ],
    "events" : [
     ["2013-01-01T00:00:00.000Z", "1", "article", "0", "0", "11._korpus_(NOVJ)", "sl", "0", "EmausBot", 1.0, 39.0, 39.0, 39.0, 0.0],
     ["2013-01-01T00:00:00.000Z", "0", "article", "0", "0", "112_U.S._580", "en", "1", "MZMcBride", 1.0, 70.0, 70.0, 70.0, 0.0],
     ["2013-01-01T00:00:00.000Z", "0", "article", "0", "0", "113_U.S._243", "en", "1", "MZMcBride", 1.0, 77.0, 77.0, 77.0, 0.0]
    ]
} ]

----------------------------------------

TITLE: Defining Day Granularity Compaction Task in Druid
DESCRIPTION: JSON configuration for a compaction task that changes the segment granularity to DAY while reducing the number of segments.

LANGUAGE: json
CODE:
{
  "type": "compact",
  "dataSource": "compaction-tutorial",
  "interval": "2015-09-12/2015-09-13",
  "segmentGranularity": "DAY",
  "tuningConfig" : {
    "type" : "index",
    "maxRowsPerSegment" : 5000000,
    "maxRowsInMemory" : 25000,
    "forceExtendableShardSpecs" : true
  }
}

----------------------------------------

TITLE: Implementing Logical NOT Filter in Druid
DESCRIPTION: Example of a NOT filter that negates another filter. Rows are included if the specified filter does not match.

LANGUAGE: JSON
CODE:
"filter": { "type": "not", "field": <filter> }

----------------------------------------

TITLE: Loading Initial Data in Apache Druid
DESCRIPTION: Loads Wikipedia edits data into Druid using a pre-defined indexing spec that creates hourly segments.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/deletion-index.json

----------------------------------------

TITLE: Column Comparison Filter in Druid
DESCRIPTION: Filter that compares two dimensions to each other, similar to comparing columns in SQL WHERE clause.

LANGUAGE: json
CODE:
"filter": { "type": "columnComparison", "dimensions": [<dimension_a>, <dimension_b>] }

----------------------------------------

TITLE: Defining Day Granularity Compaction Task in Apache Druid
DESCRIPTION: This JSON configuration defines a compaction task that will compact segments into day granularity segments.

LANGUAGE: json
CODE:
{
  "type": "compact",
  "dataSource": "compaction-tutorial",
  "interval": "2015-09-12/2015-09-13",
  "segmentGranularity": "DAY",
  "tuningConfig" : {
    "type" : "index",
    "maxRowsPerSegment" : 5000000,
    "maxRowsInMemory" : 25000,
    "forceExtendableShardSpecs" : true
  }
}

----------------------------------------

TITLE: Second Query for Exact Aggregates in Apache Druid
DESCRIPTION: This snippet shows the second query in a two-query approach to get approximate rank TopN results with exact aggregates. It uses the results from the first query to filter and get exact aggregates for the top l_orderkey values.

LANGUAGE: json
CODE:
{
    "aggregations": [
             {
                 "fieldName": "L_TAX_doubleSum",
                 "name": "L_TAX_",
                 "type": "doubleSum"
             },
             {
                 "fieldName": "L_DISCOUNT_doubleSum",
                 "name": "L_DISCOUNT_",
                 "type": "doubleSum"
             },
             {
                 "fieldName": "L_EXTENDEDPRICE_doubleSum",
                 "name": "L_EXTENDEDPRICE_",
                 "type": "doubleSum"
             },
             {
                 "fieldName": "L_QUANTITY_longSum",
                 "name": "L_QUANTITY_",
                 "type": "longSum"
             },
             {
                 "name": "count",
                 "type": "count"
             }
    ],
    "dataSource": "tpch_year",
    "dimension":"l_orderkey",
    "filter": {
        "fields": [
            {
                "dimension": "l_orderkey",
                "type": "selector",
                "value": "103136"
            },
            {
                "dimension": "l_orderkey",
                "type": "selector",
                "value": "1648672"
            }
        ],
        "type": "or"
    },
    "granularity": "all",
    "intervals": [
        "1900-01-09T00:00:00.000Z/2992-01-10T00:00:00.000Z"
    ],
    "metric": "L_QUANTITY_",
    "queryType": "topN",
    "threshold": 2
}

----------------------------------------

TITLE: Configuring Blacklisted Workers in Apache Druid Overlord
DESCRIPTION: These properties configure the threshold and timeouts for blacklisting MiddleManagers in Apache Druid. They control the maximum retries before blacklisting, backoff time, cleanup period, and maximum percentage of blacklisted workers.

LANGUAGE: properties
CODE:
druid.indexer.runner.maxRetriesBeforeBlacklist
druid.indexer.runner.workerBlackListBackoffTime
druid.indexer.runner.workerBlackListCleanupPeriod
druid.indexer.runner.maxPercentageBlacklistWorkers

----------------------------------------

TITLE: Configuring DBCP Properties for Metadata Storage
DESCRIPTION: Example of setting custom DBCP (Database Connection Pool) properties for Druid's metadata storage connector.

LANGUAGE: properties
CODE:
druid.metadata.storage.connector.dbcp.maxConnLifetimeMillis=1200000
druid.metadata.storage.connector.dbcp.defaultQueryTimeout=30000

----------------------------------------

TITLE: Configuring Hadoop DataSource InputSpec
DESCRIPTION: JSON configuration for Hadoop batch ingestion using dataSource inputSpec to read data from existing Druid segments. Used for reindexing existing data.

LANGUAGE: json
CODE:
"ioConfig" : {
  "type" : "hadoop",
  "inputSpec" : {
    "type" : "dataSource",
    "ingestionSpec" : {
      "dataSource": "wikipedia",
      "intervals": ["2014-10-20T00:00:00Z/P2W"]
    }
  }
}

----------------------------------------

TITLE: Configuring Ambari Metrics Send-All Converter in JSON
DESCRIPTION: JSON configuration for the 'all' event converter type, which sends all Druid service metrics events to Ambari Metrics. This sets the namespace prefix and application name.

LANGUAGE: json
CODE:
{
"druid.emitter.ambari-metrics.eventConverter":{"type":"all", "namespacePrefix": "druid.test", "appName":"druid"}
}

----------------------------------------

TITLE: Implementing TopN Query with DistinctCount in Druid
DESCRIPTION: Example of a TopN query using the DistinctCount aggregator to find the top 5 values of sample_dim based on unique visitor counts.

LANGUAGE: json
CODE:
{
  "queryType": "topN",
  "dataSource": "sample_datasource",
  "dimension": "sample_dim",
  "threshold": 5,
  "metric": "uv",
  "granularity": "all",
  "aggregations": [
    {
      "type": "distinctCount",
      "name": "uv",
      "fieldName": "visitor_id"
    }
  ],
  "intervals": [
    "2016-03-06T00:00:00/2016-03-06T23:59:59"
  ]
}

----------------------------------------

TITLE: Setting Up HDFS Directories for Druid
DESCRIPTION: Commands to create and configure HDFS directories for Druid segments and to copy input data to HDFS.

LANGUAGE: bash
CODE:
cd /usr/local/hadoop/bin
./hadoop fs -mkdir /druid
./hadoop fs -mkdir /druid/segments
./hadoop fs -mkdir /quickstart
./hadoop fs -chmod 777 /druid
./hadoop fs -chmod 777 /druid/segments
./hadoop fs -chmod 777 /quickstart
./hadoop fs -chmod -R 777 /tmp
./hadoop fs -chmod -R 777 /user
./hadoop fs -put /shared/wikiticker-2015-09-12-sampled.json.gz /quickstart/wikiticker-2015-09-12-sampled.json.gz

----------------------------------------

TITLE: Quantiles Array Post-Aggregator Configuration
DESCRIPTION: Post-aggregator configuration for retrieving multiple quantile values from a DoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "quantilesDoublesSketchToQuantiles",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch>,
  "fractions" : <array of fractional positions from 0 to 1 inclusive>
}

----------------------------------------

TITLE: Configuring Derby Metadata Storage in Druid
DESCRIPTION: Basic configuration properties for setting up Derby as Druid's metadata storage. Note that Derby is not recommended for production use.

LANGUAGE: properties
CODE:
druid.metadata.storage.type=derby
druid.metadata.storage.connector.connectURI=jdbc:derby://localhost:1527//opt/var/druid_state/derby;create=true

----------------------------------------

TITLE: Configuring Approximate Histogram Aggregator in Druid
DESCRIPTION: JSON configuration for setting up an approximate histogram aggregator at ingestion or query time. Includes parameters for resolution, bucket count, and value limits.

LANGUAGE: json
CODE:
{
  "type" : "approxHistogram or approxHistogramFold (at ingestion time), approxHistogramFold (at query time)",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "resolution" : <integer>,
  "numBuckets" : <integer>,
  "lowerLimit" : <float>,
  "upperLimit" : <float>
}

----------------------------------------

TITLE: MIT License Notice for mark.js
DESCRIPTION: Copyright notice for mark.js library by Julian Khnel, version 8.11.1, licensed under MIT.

LANGUAGE: JavaScript
CODE:
/*!***************************************************
* mark.js v8.11.1
* https://markjs.io/
* Copyright (c) 20142018, Julian Khnel
* Released under the MIT license https://git.io/vwTVl
****************************************************/

----------------------------------------

TITLE: Executing MiddleManager Process in Apache Druid
DESCRIPTION: Command to start the MiddleManager server process in Apache Druid. This initializes the worker process responsible for executing submitted tasks.

LANGUAGE: bash
CODE:
org.apache.druid.cli.Main server middleManager

----------------------------------------

TITLE: Filtered Aggregator Configuration in Druid
DESCRIPTION: A wrapper aggregator that filters input values before applying the wrapped aggregator. Allows computing filtered and unfiltered aggregations simultaneously.

LANGUAGE: json
CODE:
{
  "type" : "filtered",
  "filter" : {
    "type" : "selector",
    "dimension" : <dimension>,
    "value" : <dimension value>
  }
  "aggregator" : <aggregation>
}

----------------------------------------

TITLE: Disabling a MiddleManager
DESCRIPTION: Example JSON response from the /druid/worker/v1/disable endpoint to disable a MiddleManager.

LANGUAGE: json
CODE:
{"localhost:8091":"disabled"}

----------------------------------------

TITLE: Submitting Druid Kafka Supervisor Spec
DESCRIPTION: cURL command to submit a Kafka supervisor specification to Druid's overlord, enabling Kafka ingestion.

LANGUAGE: bash
CODE:
curl -XPOST -H'Content-Type: application/json' -d @quickstart/tutorial/wikipedia-kafka-supervisor.json http://localhost:8090/druid/indexer/v1/supervisor

----------------------------------------

TITLE: Basic Maven Build Command
DESCRIPTION: Basic Maven command to build Druid from source, which runs static analysis, unit tests, compiles classes, and packages projects into JARs.

LANGUAGE: bash
CODE:
mvn clean install

----------------------------------------

TITLE: Defining Compaction Task in Druid
DESCRIPTION: JSON configuration for a compaction task that maintains the original segment granularity while reducing the number of segments.

LANGUAGE: json
CODE:
{
  "type": "compact",
  "dataSource": "compaction-tutorial",
  "interval": "2015-09-12/2015-09-13",
  "tuningConfig" : {
    "type" : "index",
    "maxRowsPerSegment" : 5000000,
    "maxRowsInMemory" : 25000
  }
}

----------------------------------------

TITLE: Configuring Metadata Update Spec in JSON for Druid Hadoop Indexer
DESCRIPTION: JSON configuration for the metadataUpdateSpec and segmentOutputPath fields in the ioConfig section of the Hadoop index task spec. This specifies how to update metadata for the Druid cluster to recognize new segments.

LANGUAGE: json
CODE:
"ioConfig" : {
  ...
  "metadataUpdateSpec" : {
    "type":"mysql",
    "connectURI" : "jdbc:mysql://localhost:3306/druid",
    "password" : "diurd",
    "segmentTable" : "druid_segments",
    "user" : "druid"
  },
  "segmentOutputPath" : "/MyDirectory/data/index/output"
}

----------------------------------------

TITLE: Creating a Column Comparison Filter in Druid
DESCRIPTION: A column comparison filter compares two dimensions to each other, similar to comparing columns in a SQL WHERE clause.

LANGUAGE: JSON
CODE:
"filter": { "type": "columnComparison", "dimensions": [<dimension_a>, <dimension_b>] }

----------------------------------------

TITLE: Required Configuration Properties for Druid Segment Insert Tool
DESCRIPTION: Essential configuration properties needed to connect to metadata storage and specify deep storage type.

LANGUAGE: properties
CODE:
druid.metadata.storage.type
druid.metadata.storage.connector.connectURI
druid.metadata.storage.connector.user
druid.metadata.storage.connector.password
druid.storage.type

----------------------------------------

TITLE: Configuring IngestSegmentFirehose in Apache Druid
DESCRIPTION: This snippet shows the configuration for an IngestSegmentFirehose in Druid, used to read data from existing Druid segments. It specifies the data source and time interval for ingestion.

LANGUAGE: json
CODE:
{
    "type"    : "ingestSegment",
    "dataSource"   : "wikipedia",
    "interval" : "2013-01-01/2013-01-02"
}

----------------------------------------

TITLE: Running Druid Broker Process in Java
DESCRIPTION: Command to start the Druid Broker process using the Main class. This initializes the Broker server, which is responsible for routing queries in a distributed Druid cluster.

LANGUAGE: java
CODE:
org.apache.druid.cli.Main server broker

----------------------------------------

TITLE: Configuring Combining Firehose in Druid
DESCRIPTION: Configuration for CombiningFirehose that merges data from multiple firehoses into a single stream.

LANGUAGE: json
CODE:
{
    "type"  :   "combining",
    "delegates" : [ { firehose1 }, { firehose2 }, ..... ]
}

----------------------------------------

TITLE: Running Druid Broker Process in Java
DESCRIPTION: Command to start the Druid Broker process using the Main class. This initializes the Broker server, which is responsible for routing queries in a distributed Druid cluster.

LANGUAGE: java
CODE:
org.apache.druid.cli.Main server broker

----------------------------------------

TITLE: Including DataSketches Extension in Druid Configuration
DESCRIPTION: This snippet shows how to include the DataSketches extension in the Druid configuration file. It adds the 'druid-datasketches' extension to the loadList.

LANGUAGE: json
CODE:
"druid.extensions.loadList=[\"druid-datasketches\"]"

----------------------------------------

TITLE: Accessing Overlord Console URL
DESCRIPTION: URL pattern for accessing the Overlord Console interface for task and worker management.

LANGUAGE: plaintext
CODE:
http://<OVERLORD_IP>:<OVERLORD_PORT>/console.html

----------------------------------------

TITLE: Configuring LongSum Aggregator in Druid JSON
DESCRIPTION: Defines a longSum aggregator to compute the sum of values as a 64-bit signed integer. Specifies the output name and the metric field to sum over.

LANGUAGE: JSON
CODE:
{ "type" : "longSum", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: Configuring Historical Process Storage Settings in Druid
DESCRIPTION: Configuration settings for adjusting storage capacity limits of Druid Historical processes. These settings control segment cache locations and maximum server size to ensure proper segment downloading.

LANGUAGE: bash
CODE:
-Ddruid.segmentCache.locations=[{"path":"/tmp/druid/storageLocation","maxSize":"500000000000"}]
-Ddruid.server.maxSize=500000000000

----------------------------------------

TITLE: Configuring DefaultLimitSpec in Druid GroupBy Queries
DESCRIPTION: Defines the structure for limiting and ordering groupBy query results. The configuration accepts a limit value and an array of columns for ordering operations.

LANGUAGE: json
CODE:
{
    "type"    : "default",
    "limit"   : <integer_value>,
    "columns" : [list of OrderByColumnSpec],
}

----------------------------------------

TITLE: Configuring Single-dimension Partitioning in Apache Druid
DESCRIPTION: JSON configuration for single-dimension partitioning in Apache Druid's Hadoop-based batch ingestion. Specifies target partition size and optional dimension.

LANGUAGE: json
CODE:
"partitionsSpec": {
   "type": "dimension",
   "targetPartitionSize": 5000000
 }

----------------------------------------

TITLE: Configuring Time Boundary Router Strategy in JSON
DESCRIPTION: JSON configuration for the timeBoundary router strategy, which routes all timeBoundary queries to the highest priority Broker.

LANGUAGE: json
CODE:
{
  "type":"timeBoundary"
}

----------------------------------------

TITLE: Defining a Spatial Filter in Druid Query
DESCRIPTION: This snippet shows how to define a spatial filter in a Druid query. It demonstrates the structure for specifying a rectangular bound for filtering data based on spatial coordinates.

LANGUAGE: json
CODE:
"filter" : {
    "type": "spatial",
    "dimension": "spatialDim",
    "bound": {
        "type": "rectangular",
        "minCoords": [10.0, 20.0],
        "maxCoords": [30.0, 40.0]
    }
}

----------------------------------------

TITLE: Including DataSketches Extension in Druid Configuration
DESCRIPTION: This snippet shows how to include the DataSketches extension in the Druid configuration file. It adds the 'druid-datasketches' extension to the loadList.

LANGUAGE: json
CODE:
"druid.extensions.loadList=[\"druid-datasketches\"]"

----------------------------------------

TITLE: Defining a Spatial Filter in Druid Query
DESCRIPTION: This snippet shows how to define a spatial filter in a Druid query. It demonstrates the structure for specifying a rectangular bound for filtering data based on spatial coordinates.

LANGUAGE: json
CODE:
"filter" : {
    "type": "spatial",
    "dimension": "spatialDim",
    "bound": {
        "type": "rectangular",
        "minCoords": [10.0, 20.0],
        "maxCoords": [30.0, 40.0]
    }
}

----------------------------------------

TITLE: Executing GroupBy Query with Variance Aggregator and Standard Deviation Post-Aggregator in Apache Druid
DESCRIPTION: JSON configuration for a GroupBy query using the variance aggregator and standard deviation post-aggregator. It specifies the data source, dimensions, aggregations, post-aggregations, and time interval for the query.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "testing",
  "dimensions": ["alias"],
  "granularity": "all",
  "aggregations": [
    {
      "type": "variance",
      "name": "index_var",
      "fieldName": "index"
    }
  ],
  "postAggregations": [
    {
      "type": "stddev",
      "name": "index_stddev",
      "fieldName": "index_var"
    }
  ],
  "intervals": [
    "2016-03-06T00:00:00/2016-03-06T23:59:59"
  ]
}

----------------------------------------

TITLE: Configuring EventReceiverFirehose in Apache Druid
DESCRIPTION: Illustrates the configuration for an EventReceiverFirehose to ingest events using an HTTP endpoint. This is used for streaming ingestion.

LANGUAGE: json
CODE:
{
  "type": "receiver",
  "serviceName": "eventReceiverServiceName",
  "bufferSize": 10000
}

----------------------------------------

TITLE: Specifying Numeric TopNMetricSpec in Druid JSON
DESCRIPTION: Shows how to specify a simple numeric metric for sorting topN results in a Druid query. It can be a string value or a JSON object with 'type' and 'metric' properties.

LANGUAGE: json
CODE:
"metric": "<metric_name>"

LANGUAGE: json
CODE:
"metric": {
    "type": "numeric",
    "metric": "<metric_name>"
}

----------------------------------------

TITLE: Configuring Loading On-heap Guava Cache in Druid
DESCRIPTION: JSON configuration for a loading on-heap Guava cache with reverse lookup. It uses a JDBC data fetcher and specifies cache parameters such as maximum size and expiration times.

LANGUAGE: json
CODE:
{
   "type":"loadingLookup",
   "dataFetcher":{ "type":"jdbcDataFetcher", "connectorConfig":"jdbc://mysql://localhost:3306/my_data_base", "table":"lookup_table_name", "keyColumn":"key_column_name", "valueColumn": "value_column_name"},
   "loadingCacheSpec":{"type":"guava"},
   "reverseLoadingCacheSpec":{"type":"guava", "maximumSize":500000, "expireAfterAccess":100000, "expireAfterAccess":10000}
}

----------------------------------------

TITLE: Configuring Send-All Graphite Event Converter
DESCRIPTION: JSON configuration for the 'all' event converter that sends all Druid service metrics to Graphite. This configuration ignores hostname and service name in the metric path.

LANGUAGE: json
CODE:
{"type":"all", "namespacePrefix": "druid.test", "ignoreHostname":true, "ignoreServiceName":true}

----------------------------------------

TITLE: Running Historical Process in Apache Druid
DESCRIPTION: This command starts the Historical server process in Apache Druid. It uses the Main class from the org.apache.druid.cli package to initialize and run the Historical server.

LANGUAGE: java
CODE:
org.apache.druid.cli.Main server historical

----------------------------------------

TITLE: Creating Union Data Source in JSON for Apache Druid
DESCRIPTION: This snippet demonstrates how to create a union data source in Apache Druid, which combines two or more table data sources. The data sources being unioned should have the same schema, and union queries should be sent to a Broker/Router process.

LANGUAGE: json
CODE:
{
       "type": "union",
       "dataSources": ["<string_value1>", "<string_value2>", "<string_value3>", ... ]
}

----------------------------------------

TITLE: Implementing Spatial Filters in Druid Queries
DESCRIPTION: Example of a spatial filter configuration using rectangular bounds to filter geographic data based on coordinate ranges.

LANGUAGE: json
CODE:
"filter" : {
    "type": "spatial",
    "dimension": "spatialDim",
    "bound": {
        "type": "rectangular",
        "minCoords": [10.0, 20.0],
        "maxCoords": [30.0, 40.0]
    }
}

----------------------------------------

TITLE: Kafka Supervisor Configuration for Protobuf Ingestion
DESCRIPTION: Complete Kafka supervisor specification for ingesting Protobuf data into Druid, including parser configuration and Kafka connection settings.

LANGUAGE: json
CODE:
{
  "type": "kafka",
  "dataSchema": {
    "dataSource": "metrics-kafka2",
    "parser": {
      "type": "protobuf",
      "descriptor": "file:///tmp/metrics.desc",
      "protoMessageType": "Metrics",
      "parseSpec": {
        "format": "json",
        "timestampSpec": {
          "column": "timestamp",
          "format": "auto"
        },
        "dimensionsSpec": {
          "dimensions": [
            "unit",
            "http_method",
            "http_code",
            "page",
            "metricType",
            "server"
          ],
          "dimensionExclusions": [
            "timestamp",
            "value"
          ]
        }
      }
    },
    "metricsSpec": [
      {
        "name": "count",
        "type": "count"
      },
      {
        "name": "value_sum",
        "fieldName": "value",
        "type": "doubleSum"
      },
      {
        "name": "value_min",
        "fieldName": "value",
        "type": "doubleMin"
      },
      {
        "name": "value_max",
        "fieldName": "value",
        "type": "doubleMax"
      }
    ],
    "granularitySpec": {
      "type": "uniform",
      "segmentGranularity": "HOUR",
      "queryGranularity": "NONE"
    }
  },
  "tuningConfig": {
    "type": "kafka",
    "maxRowsPerSegment": 5000000
  },
  "ioConfig": {
    "topic": "metrics_pb",
    "consumerProperties": {
      "bootstrap.servers": "localhost:9092"
    },
    "taskCount": 1,
    "replicas": 1,
    "taskDuration": "PT1H"
  }
}

----------------------------------------

TITLE: Individual Segment Path in ZooKeeper
DESCRIPTION: The ephemeral ZooKeeper path for individual segments being served by a process.

LANGUAGE: plaintext
CODE:
${druid.zk.paths.servedSegmentsPath}/${druid.host}/_segment_identifier_

----------------------------------------

TITLE: Setting Task Priority in Druid Context
DESCRIPTION: JSON configuration example showing how to override the default task priority in Druid task context. The priority value determines the task's ability to preempt locks from other tasks.

LANGUAGE: json
CODE:
{
  "context" : {
    "priority" : 100
  }
}

----------------------------------------

TITLE: React Core License
DESCRIPTION: Specifies the MIT license for React core production build, version 17.0.2.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Basic Segment Metadata Query in Druid
DESCRIPTION: Example of a basic segment metadata query that retrieves information about a data source for a specific time interval.

LANGUAGE: json
CODE:
{
  "queryType":"segmentMetadata",
  "dataSource":"sample_datasource",
  "intervals":["2013-01-01/2014-01-01"]
}

----------------------------------------

TITLE: Creating a Column Comparison Filter in Druid JSON
DESCRIPTION: Example of a column comparison filter that compares two dimensions. This is equivalent to comparing two columns in a WHERE clause.

LANGUAGE: json
CODE:
"filter": { "type": "columnComparison", "dimensions": [<dimension_a>, <dimension_b>] }

----------------------------------------

TITLE: Configuring Send-All Graphite Event Converter
DESCRIPTION: JSON configuration for the 'all' event converter that sends all Druid service metrics to Graphite. This configuration ignores hostname and service name in the metric path.

LANGUAGE: json
CODE:
{"type":"all", "namespacePrefix": "druid.test", "ignoreHostname":true, "ignoreServiceName":true}

----------------------------------------

TITLE: First Query for Approximate TopN in Apache Druid
DESCRIPTION: This snippet demonstrates the first query in a two-query approach to get approximate rank TopN results with exact aggregates. It retrieves the top 2 l_orderkey values based on the L_QUANTITY_ metric.

LANGUAGE: json
CODE:
{
    "aggregations": [
             {
                 "fieldName": "L_QUANTITY_longSum",
                 "name": "L_QUANTITY_",
                 "type": "longSum"
             }
    ],
    "dataSource": "tpch_year",
    "dimension":"l_orderkey",
    "granularity": "all",
    "intervals": [
        "1900-01-09T00:00:00.000Z/2992-01-10T00:00:00.000Z"
    ],
    "metric": "L_QUANTITY_",
    "queryType": "topN",
    "threshold": 2
}

----------------------------------------

TITLE: Configuring zscore2sample Post Aggregator in Apache Druid
DESCRIPTION: This snippet demonstrates how to configure the zscore2sample post aggregator in Apache Druid. It calculates the z-score using a two-sample z-test, converting binary variables to continuous variables for two population proportions.

LANGUAGE: json
CODE:
{
  "type": "zscore2sample",
  "name": "<output_name>",
  "successCount1": <post_aggregator> success count of sample 1,
  "sample1Size": <post_aggregaror> sample 1 size,
  "successCount2": <post_aggregator> success count of sample 2,
  "sample2Size" : <post_aggregator> sample 2 size
}

----------------------------------------

TITLE: Configuring LongSum Aggregator in Druid JSON
DESCRIPTION: The longSum aggregator computes the sum of values as a 64-bit signed integer. It requires specifying an output name and the name of the metric column to sum over.

LANGUAGE: json
CODE:
{ "type" : "longSum", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: Querying with Hour Granularity in Apache Druid
DESCRIPTION: This example demonstrates a groupBy query with 'hour' granularity. It aggregates data by language and counts occurrences.

LANGUAGE: json
CODE:
{
   "queryType":"groupBy",
   "dataSource":"my_dataSource",
   "granularity":"hour",
   "dimensions":[
      "language"
   ],
   "aggregations":[
      {
         "type":"count",
         "name":"count"
      }
   ],
   "intervals":[
      "2000-01-01T00:00Z/3000-01-01T00:00Z"
   ]
}

----------------------------------------

TITLE: Configuring Globally Cached Lookup with URI Namespace
DESCRIPTION: Example configuration for a globally cached lookup using a URI namespace. This setup specifies a CSV file in S3 as the data source, with a 5-minute polling period for updates.

LANGUAGE: json
CODE:
{
   "type": "cachedNamespace",
   "extractionNamespace": {
      "type": "uri",
      "uri": "s3://bucket/some/key/prefix/renames-0003.gz",
      "namespaceParseSpec": {
        "format": "csv",
        "columns": [
          "key",
          "value"
        ]
      },
      "pollPeriod": "PT5M"
    },
    "firstCacheTimeout": 0
}

----------------------------------------

TITLE: Cloning Apache Druid Repository in Bash
DESCRIPTION: Commands to clone the Apache Druid repository from GitHub and navigate to the project directory.

LANGUAGE: bash
CODE:
git clone git@github.com:apache/incubator-druid.git
cd druid

----------------------------------------

TITLE: Implementing Insensitive Contains Search Query in Druid
DESCRIPTION: Defines a case-insensitive search query that matches if any part of a dimension value contains the specified search value. This is useful for simple substring matching without case sensitivity.

LANGUAGE: json
CODE:
{
  "type"  : "insensitive_contains",
  "value" : "some_value"
}

----------------------------------------

TITLE: Joining Servers and Segments Tables
DESCRIPTION: SQL query example demonstrating a join between servers and segments tables to count segments per server for a specific datasource.

LANGUAGE: sql
CODE:
SELECT count(segments.segment_id) as num_segments from sys.segments as segments 
INNER JOIN sys.server_segments as server_segments 
ON segments.segment_id  = server_segments.segment_id 
INNER JOIN sys.servers as servers 
ON servers.server = server_segments.server
WHERE segments.datasource = 'wikipedia' 
GROUP BY servers.server;

----------------------------------------

TITLE: Configuring Globally Cached Lookup with URI Namespace
DESCRIPTION: Example configuration for a globally cached lookup using a URI namespace. This setup specifies a CSV file in S3 as the data source, with a 5-minute polling period for updates.

LANGUAGE: json
CODE:
{
   "type": "cachedNamespace",
   "extractionNamespace": {
      "type": "uri",
      "uri": "s3://bucket/some/key/prefix/renames-0003.gz",
      "namespaceParseSpec": {
        "format": "csv",
        "columns": [
          "key",
          "value"
        ]
      },
      "pollPeriod": "PT5M"
    },
    "firstCacheTimeout": 0
}

----------------------------------------

TITLE: SQL Query for Retrieving Segment Information in Druid
DESCRIPTION: Example SQL query to retrieve information about all segments for a specific datasource using the sys.segments table.

LANGUAGE: SQL
CODE:
SELECT * FROM sys.segments WHERE datasource = 'wikipedia'

----------------------------------------

TITLE: Submitting Kinesis Supervisor API Call
DESCRIPTION: Example cURL command for submitting a Kinesis supervisor specification to Druid's Overlord.

LANGUAGE: bash
CODE:
curl -X POST -H 'Content-Type: application/json' -d @supervisor-spec.json http://localhost:8090/druid/indexer/v1/supervisor

----------------------------------------

TITLE: Executing Time Boundary Query in Apache Druid
DESCRIPTION: This JSON structure defines a time boundary query in Apache Druid. It specifies the query type, data source, optional time bound, and filter. The query returns the earliest and/or latest timestamps in the dataset.

LANGUAGE: json
CODE:
{
    "queryType" : "timeBoundary",
    "dataSource": "sample_datasource",
    "bound"     : < "maxTime" | "minTime" > # optional, defaults to returning both timestamps if not set 
    "filter"    : { "type": "and", "fields": [<filter>, <filter>, ...] } # optional
}

----------------------------------------

TITLE: Configuring Substring Extraction Function in Druid Query
DESCRIPTION: The Substring Extraction Function returns a substring of the dimension value based on specified index and length.

LANGUAGE: JSON
CODE:
{ "type" : "substring", "index" : 1, "length" : 4 }

----------------------------------------

TITLE: Configuring Environment Variable Password Provider in Apache Druid
DESCRIPTION: This JSON snippet demonstrates how to configure a password provider that retrieves the password from an environment variable. It specifies the type as 'environment' and the name of the environment variable to use.

LANGUAGE: json
CODE:
{ "type": "environment", "variable": "METADATA_STORAGE_PASSWORD" }

----------------------------------------

TITLE: Sum Aggregators in Druid
DESCRIPTION: Configurations for longSum, doubleSum, and floatSum aggregators that compute sums of metric values in different numeric formats.

LANGUAGE: json
CODE:
{ "type" : "longSum", "name" : <output_name>, "fieldName" : <metric_name> }

LANGUAGE: json
CODE:
{ "type" : "doubleSum", "name" : <output_name>, "fieldName" : <metric_name> }

LANGUAGE: json
CODE:
{ "type" : "floatSum", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: Setting Custom Task Priority in Apache Druid
DESCRIPTION: This JSON snippet demonstrates how to override the default task priority by setting a custom priority value in the task context. The priority is set to 100, which is higher than the default priorities for most task types.

LANGUAGE: json
CODE:
{
  "context" : {
    "priority" : 100
  }
}

----------------------------------------

TITLE: Configuring LongMax Aggregator in Druid JSON
DESCRIPTION: Defines a longMax aggregator to compute the maximum of all metric values and Long.MIN_VALUE.

LANGUAGE: JSON
CODE:
{ "type" : "longMax", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: Sample JSON Output for DumpSegment Bitmap Data in Apache Druid
DESCRIPTION: This JSON snippet shows the format of bitmap index output when using the DumpSegment tool with the 'bitmaps' dump option. It includes the bitmap serialization factory type and base64 encoded bitmap data for column values.

LANGUAGE: json
CODE:
{
  "bitmapSerdeFactory": {
    "type": "concise"
  },
  "bitmaps": {
    "isRobot": {
      "false": "//aExfu+Nv3X...",
      "true": "gAl7OoRByQ..."
    }
  }
}

----------------------------------------

TITLE: SQL Query for Retrieving Segment Information in Druid
DESCRIPTION: Example SQL query to retrieve information about all segments for a specific datasource using the sys.segments table.

LANGUAGE: SQL
CODE:
SELECT * FROM sys.segments WHERE datasource = 'wikipedia'

----------------------------------------

TITLE: Prefix Filtered DimensionSpec in Druid
DESCRIPTION: Filters multi-value dimensions by matching values with a specific prefix.

LANGUAGE: json
CODE:
{ "type" : "prefixFiltered", "delegate" : <dimensionSpec>, "prefix": <prefix string> }

----------------------------------------

TITLE: Configuring HttpFirehose in Apache Druid
DESCRIPTION: Shows the configuration for an HttpFirehose to read data from remote sites via HTTP. This firehose is splittable and can be used by native parallel index tasks.

LANGUAGE: json
CODE:
{
    "type"    : "http",
    "uris"  : ["http://example.com/uri1", "http://example2.com/uri2"]
}

----------------------------------------

TITLE: Executing Kill Task for Permanent Deletion
DESCRIPTION: CURL command to submit a Kill Task that permanently removes disabled segments from both metadata and deep storage.

LANGUAGE: bash
CODE:
curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/deletion-kill.json http://localhost:8090/druid/indexer/v1/task

----------------------------------------

TITLE: Configuring RegexFiltered DimensionSpec in Apache Druid
DESCRIPTION: JSON configuration for a RegexFiltered DimensionSpec, which filters multi-value dimensions based on a regular expression pattern.

LANGUAGE: JSON
CODE:
{ "type" : "regexFiltered", "delegate" : <dimensionSpec>, "pattern": <java regex pattern> }

----------------------------------------

TITLE: Configuring Standard Deviation Post-Aggregator in Apache Druid
DESCRIPTION: JSON configuration for the standard deviation post-aggregator. It calculates the standard deviation from the variance and specifies the output name, aggregator name, and estimator.

LANGUAGE: json
CODE:
{
  "type": "stddev",
  "name": "<output_name>",
  "fieldName": "<aggregator_name>",
  "estimator": <string>
}

----------------------------------------

TITLE: Configuring JVM Settings for Druid Router
DESCRIPTION: Example JVM settings for running the Druid Router process on a c3.2xlarge EC2 instance.

LANGUAGE: java
CODE:
-server
-Xmx13g
-Xms13g
-XX:NewSize=256m
-XX:MaxNewSize=256m
-XX:+UseConcMarkSweepGC
-XX:+PrintGCDetails
-XX:+PrintGCTimeStamps
-XX:+UseLargePages
-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/mnt/galaxy/deploy/current/
-Duser.timezone=UTC
-Dfile.encoding=UTF-8
-Djava.io.tmpdir=/mnt/tmp

-Dcom.sun.management.jmxremote.port=17071
-Dcom.sun.management.jmxremote.authenticate=false
-Dcom.sun.management.jmxremote.ssl=false

----------------------------------------

TITLE: Configuring JVM Settings for Druid Router
DESCRIPTION: Example JVM settings for running the Druid Router process on a c3.2xlarge EC2 instance.

LANGUAGE: java
CODE:
-server
-Xmx13g
-Xms13g
-XX:NewSize=256m
-XX:MaxNewSize=256m
-XX:+UseConcMarkSweepGC
-XX:+PrintGCDetails
-XX:+PrintGCTimeStamps
-XX:+UseLargePages
-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/mnt/galaxy/deploy/current/
-Duser.timezone=UTC
-Dfile.encoding=UTF-8
-Djava.io.tmpdir=/mnt/tmp

-Dcom.sun.management.jmxremote.port=17071
-Dcom.sun.management.jmxremote.authenticate=false
-Dcom.sun.management.jmxremote.ssl=false

----------------------------------------

TITLE: Using pull-deps to Download Community Extensions for Druid
DESCRIPTION: This snippet shows how to use the pull-deps tool to download community or third-party extensions for Druid. It demonstrates downloading a hypothetical extension 'com.example:druid-example-extension:1.0.0' and specifies the necessary Java command-line arguments and options.

LANGUAGE: bash
CODE:
java \
  -cp "lib/*" \
  -Ddruid.extensions.directory="extensions" \
  -Ddruid.extensions.hadoopDependenciesDir="hadoop-dependencies" \
  org.apache.druid.cli.Main tools pull-deps \
  --no-default-hadoop \
  -c "com.example:druid-example-extension:1.0.0"

----------------------------------------

TITLE: Configuring StaticGoogleBlobStoreFirehose in Druid
DESCRIPTION: JSON configuration for setting up a static Google Cloud Storage firehose in Druid. This configuration enables ingesting data from multiple GCS blobs with support for caching and prefetching features.

LANGUAGE: json
CODE:
{
    "type" : "static-google-blobstore",
    "blobs": [
        {
          "bucket": "foo",
          "path": "/path/to/your/file.json"
        },
        {
          "bucket": "bar",
          "path": "/another/path.json"
        }
    ]
}

----------------------------------------

TITLE: Configuring Druid PostgreSQL Connection
DESCRIPTION: Properties configuration for connecting Druid to PostgreSQL metadata storage, including extension loading and database connection parameters

LANGUAGE: properties
CODE:
druid.extensions.loadList=["postgresql-metadata-storage"]
druid.metadata.storage.type=postgresql
druid.metadata.storage.connector.connectURI=jdbc:postgresql://<host>/druid
druid.metadata.storage.connector.user=druid
druid.metadata.storage.connector.password=diurd

----------------------------------------

TITLE: Configuring Druid PostgreSQL Connection
DESCRIPTION: Properties configuration for connecting Druid to PostgreSQL metadata storage, including extension loading and database connection parameters

LANGUAGE: properties
CODE:
druid.extensions.loadList=["postgresql-metadata-storage"]
druid.metadata.storage.type=postgresql
druid.metadata.storage.connector.connectURI=jdbc:postgresql://<host>/druid
druid.metadata.storage.connector.user=druid
druid.metadata.storage.connector.password=diurd

----------------------------------------

TITLE: Configuring JVM Settings for Druid Router
DESCRIPTION: Example JVM settings for running the Druid Router process on a c3.2xlarge EC2 instance.

LANGUAGE: java
CODE:
-server
-Xmx13g
-Xms13g
-XX:NewSize=256m
-XX:MaxNewSize=256m
-XX:+UseConcMarkSweepGC
-XX:+PrintGCDetails
-XX:+PrintGCTimeStamps
-XX:+UseLargePages
-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/mnt/galaxy/deploy/current/
-Duser.timezone=UTC
-Dfile.encoding=UTF-8
-Djava.io.tmpdir=/mnt/tmp

-Dcom.sun.management.jmxremote.port=17071
-Dcom.sun.management.jmxremote.authenticate=false
-Dcom.sun.management.jmxremote.ssl=false

----------------------------------------

TITLE: Configuring Druid PostgreSQL Connection
DESCRIPTION: Properties configuration for connecting Druid to PostgreSQL metadata storage, including extension loading and database connection parameters

LANGUAGE: properties
CODE:
druid.extensions.loadList=["postgresql-metadata-storage"]
druid.metadata.storage.type=postgresql
druid.metadata.storage.connector.connectURI=jdbc:postgresql://<host>/druid
druid.metadata.storage.connector.user=druid
druid.metadata.storage.connector.password=diurd

----------------------------------------

TITLE: Configuring Derivative DataSource Supervisor in Druid
DESCRIPTION: JSON configuration for creating and maintaining a derived dataSource from a base dataSource. Specifies dimensions, metrics and Hadoop tuning configurations.

LANGUAGE: json
CODE:
{
    "type": "derivativeDataSource",
    "baseDataSource": "wikiticker",
    "dimensionsSpec": {
        "dimensions": [
            "isUnpatrolled",
            "metroCode",
            "namespace",
            "page",
            "regionIsoCode",
            "regionName",
            "user"
        ]
    },
    "metricsSpec": [
        {
            "name": "count",
            "type": "count"
        },
        {
            "name": "added",
            "type": "longSum",
            "fieldName": "added"
        }
    ],
    "tuningConfig": {
        "type": "hadoop"
    }
}

----------------------------------------

TITLE: Configuring FloatLast Aggregator in Druid JSON
DESCRIPTION: Defines a floatLast aggregator to compute the metric value with the maximum timestamp or 0 if no row exists. Used only in queries, not ingestion.

LANGUAGE: JSON
CODE:
{
  "type" : "floatLast",
  "name" : <output_name>,
  "fieldName" : <metric_name>
}

----------------------------------------

TITLE: Example Druid Query Using Bloom Filter Aggregator
DESCRIPTION: This JSON snippet provides a complete example of a Druid query using the bloom filter aggregator, including the query type, data source, intervals, and aggregations.

LANGUAGE: json
CODE:
{
  "queryType": "timeseries",
  "dataSource": "wikiticker",
  "intervals": [ "2015-09-12T00:00:00.000/2015-09-13T00:00:00.000" ],
  "granularity": "day",
  "aggregations": [
    {
      "type": "bloom",
      "name": "userBloom",
      "maxNumEntries": 100000,
      "field": {
        "type":"default",
        "dimension":"user",
        "outputType": "STRING"
      }
    }
  ]
}

----------------------------------------

TITLE: Starting Druid Data Server Components
DESCRIPTION: Commands to start Historical and MiddleManager processes on data servers

LANGUAGE: bash
CODE:
java `cat conf/druid/historical/jvm.config | xargs` -cp conf/druid/_common:conf/druid/historical:lib/* org.apache.druid.cli.Main server historical
java `cat conf/druid/middleManager/jvm.config | xargs` -cp conf/druid/_common:conf/druid/middleManager:lib/* org.apache.druid.cli.Main server middleManager

----------------------------------------

TITLE: Starting Druid Data Server Components
DESCRIPTION: Commands to start Historical and MiddleManager processes on data servers

LANGUAGE: bash
CODE:
java `cat conf/druid/historical/jvm.config | xargs` -cp conf/druid/_common:conf/druid/historical:lib/* org.apache.druid.cli.Main server historical
java `cat conf/druid/middleManager/jvm.config | xargs` -cp conf/druid/_common:conf/druid/middleManager:lib/* org.apache.druid.cli.Main server middleManager

----------------------------------------

TITLE: Creating Cassandra Tables for Druid Deep Storage
DESCRIPTION: These SQL statements create the required tables 'index_storage' and 'descriptor_storage' in Cassandra for storing Druid segments and their metadata. The index_storage table uses a chunked object structure for efficient storage of large segments.

LANGUAGE: sql
CODE:
CREATE TABLE index_storage(key text,
                           chunk text,
                           value blob,
                           PRIMARY KEY (key, chunk)) WITH COMPACT STORAGE;

CREATE TABLE descriptor_storage(key varchar,
                                lastModified timestamp,
                                descriptor varchar,
                                PRIMARY KEY (key)) WITH COMPACT STORAGE;

----------------------------------------

TITLE: HyperUnique Cardinality Post-Aggregator in Druid
DESCRIPTION: Wraps a hyperUnique object for use in post-aggregations, inheriting rounding behavior from referenced aggregator.

LANGUAGE: json
CODE:
{
  "type"  : "hyperUniqueCardinality",
  "name": <output name>,
  "fieldName"  : <the name field value of the hyperUnique aggregator>
}

----------------------------------------

TITLE: Configuring StringLast Aggregator in Druid JSON
DESCRIPTION: The stringLast aggregator computes the metric value with the maximum timestamp or null if no row exists. It includes optional parameters for maximum string bytes and filtering null values.

LANGUAGE: json
CODE:
{
  "type" : "stringLast",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "maxStringBytes" : <integer> # (optional, defaults to 1024),
  "filterNullValues" : <boolean> # (optional, defaults to false)
}

----------------------------------------

TITLE: Python Kafka Producer for Protobuf Messages
DESCRIPTION: Python script that reads JSON data from stdin and publishes Protobuf-encoded messages to Kafka topic.

LANGUAGE: python
CODE:
#!/usr/bin/env python

import sys
import json

from kafka import KafkaProducer
from metrics_pb2 import Metrics

producer = KafkaProducer(bootstrap_servers='localhost:9092')
topic = 'metrics_pb'
metrics = Metrics()

for row in iter(sys.stdin):
    d = json.loads(row)
    for k, v in d.items():
        setattr(metrics, k, v)
    pb = metrics.SerializeToString()
    producer.send(topic, pb)

----------------------------------------

TITLE: Configuring LongMax Aggregator in Druid
DESCRIPTION: JSON configuration for the longMax aggregator in Druid, which computes the maximum of all metric values and Long.MIN_VALUE.

LANGUAGE: json
CODE:
{ "type" : "longMax", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: HDFS Segment Insert Command Example
DESCRIPTION: Command to insert segments from HDFS storage into MySQL metadata storage using the insert-segment-to-db tool.

LANGUAGE: java
CODE:
java 
-Ddruid.metadata.storage.type=mysql 
-Ddruid.metadata.storage.connector.connectURI=jdbc\:mysql\://localhost\:3306/druid 
-Ddruid.metadata.storage.connector.user=druid 
-Ddruid.metadata.storage.connector.password=diurd 
-Ddruid.extensions.loadList=[\"mysql-metadata-storage\",\"druid-hdfs-storage\"] 
-Ddruid.storage.type=hdfs
-cp $DRUID_CLASSPATH 
org.apache.druid.cli.Main tools insert-segment-to-db --workingDir hdfs://host:port//druid/storage/wikipedia --updateDescriptor true

----------------------------------------

TITLE: Configuring Batch Thrift Ingestion for Apache Druid using HadoopDruidIndexer
DESCRIPTION: JSON configuration for batch ingestion of Thrift data using HadoopDruidIndexer in Apache Druid. Specifies the data schema, input format, and necessary job properties.

LANGUAGE: json
CODE:
{
  "type": "index_hadoop",
  "spec": {
    "dataSchema": {
      "dataSource": "book",
      "parser": {
        "type": "thrift",
        "jarPath": "book.jar",
        "thriftClass": "org.apache.druid.data.input.thrift.Book",
        "protocol": "compact",
        "parseSpec": {
          "format": "json",
          ...
        }
      },
      "metricsSpec": [],
      "granularitySpec": {}
    },
    "ioConfig": {
      "type": "hadoop",
      "inputSpec": {
        "type": "static",
        "inputFormat": "org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat",
        // "inputFormat": "com.twitter.elephantbird.mapreduce.input.LzoThriftBlockInputFormat",
        "paths": "/user/to/some/book.seq"
      }
    },
    "tuningConfig": {
      "type": "hadoop",
      "jobProperties": {
        "tmpjars":"/user/h_user_profile/du00/druid/test/book.jar",
        // "elephantbird.class.for.MultiInputFormat" : "${YOUR_THRIFT_CLASS_NAME}"
      }
    }
  }
}

----------------------------------------

TITLE: Implementing a Bound Filter in Druid
DESCRIPTION: A bound filter is used for range queries on dimension values, supporting various comparison operations.

LANGUAGE: JSON
CODE:
{
    "type": "bound",
    "dimension": "age",
    "lower": "21",
    "upper": "31" ,
    "ordering": "numeric"
}

----------------------------------------

TITLE: Creating PostgreSQL Database for Druid
DESCRIPTION: Command to create a new PostgreSQL database named 'druid' owned by the druid user

LANGUAGE: bash
CODE:
createdb druid -O druid

----------------------------------------

TITLE: Querying Initial Data in Druid SQL
DESCRIPTION: This SQL query retrieves all rows from the 'updates-tutorial' datasource to verify the initial data load.

LANGUAGE: sql
CODE:
select * from "updates-tutorial";

----------------------------------------

TITLE: Implementing FloatMax Aggregator in Druid
DESCRIPTION: The floatMax aggregator computes the maximum of all metric values and Float.NEGATIVE_INFINITY. It requires specifying an output name and the metric field to analyze.

LANGUAGE: json
CODE:
{ "type" : "floatMax", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: Compaction Task Configuration with Daily Granularity
DESCRIPTION: JSON configuration for compacting segments into daily granularity. Modifies the segment granularity while maintaining other compaction parameters.

LANGUAGE: json
CODE:
{
  "type": "compact",
  "dataSource": "compaction-tutorial",
  "interval": "2015-09-12/2015-09-13",
  "segmentGranularity": "DAY",
  "tuningConfig" : {
    "type" : "index",
    "maxRowsPerSegment" : 5000000,
    "maxRowsInMemory" : 25000,
    "forceExtendableShardSpecs" : true
  }
}

----------------------------------------

TITLE: ORC Parser with TimeAndDims ParseSpec
DESCRIPTION: Configuration example showing how to use the ORC parser with timeAndDims parseSpec format. Demonstrates explicit dimension specification without field discovery.

LANGUAGE: json
CODE:
{
  "type": "index_hadoop",
  "spec": {
    "ioConfig": {
      "type": "hadoop",
      "inputSpec": {
        "type": "static",
        "inputFormat": "org.apache.orc.mapreduce.OrcInputFormat",
        "paths": "path/to/file.orc"
      }
    },
    "dataSchema": {
      "dataSource": "example",
      "parser": {
        "type": "orc",
        "parseSpec": {
          "format": "timeAndDims",
          "timestampSpec": {
            "column": "timestamp",
            "format": "auto"
          },
          "dimensionsSpec": {
            "dimensions": [
              "dim1",
              "dim2",
              "dim3",
              "listDim"
            ],
            "dimensionExclusions": [],
            "spatialDimensions": []
          }
        }
      }
    }
  }
}

----------------------------------------

TITLE: Configuring White-List Graphite Event Converter
DESCRIPTION: JSON configuration for the 'whiteList' event converter that selectively sends metrics based on a whitelist. This configuration includes a custom map path for the whitelist definition.

LANGUAGE: json
CODE:
{"type":"whiteList", "namespacePrefix": "druid.test", "ignoreHostname":true, "ignoreServiceName":true, "mapPath":"/pathPrefix/fileName.json"}

----------------------------------------

TITLE: Configuring FloatLast Aggregator in Druid JSON
DESCRIPTION: The floatLast aggregator computes the metric value with the maximum timestamp or 0 if no row exists. It can only be used in queries, not in ingestion specs.

LANGUAGE: json
CODE:
{
  "type" : "floatLast",
  "name" : <output_name>,
  "fieldName" : <metric_name>
}

----------------------------------------

TITLE: Configuring Google Cloud Storage with HDFS Extension in YAML
DESCRIPTION: This YAML configuration snippet demonstrates how to set up Google Cloud Storage as deep storage using the HDFS extension in Apache Druid. It specifies the storage type as HDFS and provides a GCS bucket path for the storage directory.

LANGUAGE: yaml
CODE:
druid.storage.type: hdfs
druid.storage.storageDirectory: gs://bucket/example/directory

----------------------------------------

TITLE: Configuring OpenTSDB Emitter JSON Mapping in Druid
DESCRIPTION: This JSON snippet demonstrates how to map Druid metrics to OpenTSDB dimensions. It shows an example configuration for the 'query/time' metric, specifying 'dataSource' and 'type' as the desired dimensions to be included when emitting this metric to OpenTSDB.

LANGUAGE: json
CODE:
"query/time": [
    "dataSource",
    "type"
]

----------------------------------------

TITLE: Estimating Distinct Keys with ArrayOfDoublesSketch Post-Aggregator
DESCRIPTION: This JSON configuration defines a post-aggregator to estimate the number of distinct keys from an ArrayOfDoublesSketch in Druid.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToEstimate",
  "name": <output name>,
  "field"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>
}

----------------------------------------

TITLE: Query Processing Configuration for Druid Realtime
DESCRIPTION: Settings controlling query processing behavior including buffer sizes, thread counts, and caching parameters.

LANGUAGE: properties
CODE:
druid.processing.buffer.sizeBytes=auto
druid.processing.formatString=processing-%s
druid.processing.numMergeBuffers=max(2, druid.processing.numThreads / 4)
druid.processing.numThreads=Number of cores - 1
druid.processing.columnCache.sizeBytes=0
druid.processing.tmpDir=java.io.tmpdir

----------------------------------------

TITLE: Configuring Druid PostgreSQL Connection
DESCRIPTION: Properties configuration for connecting Druid to PostgreSQL metadata storage, including extension loading and database connection parameters

LANGUAGE: properties
CODE:
druid.extensions.loadList=["postgresql-metadata-storage"]
druid.metadata.storage.type=postgresql
druid.metadata.storage.connector.connectURI=jdbc:postgresql://<host>/druid
druid.metadata.storage.connector.user=druid
druid.metadata.storage.connector.password=diurd

----------------------------------------

TITLE: Configuring Router Strategies with JSON
DESCRIPTION: Example JSON configurations for Router strategies, including timeBoundary, priority, and JavaScript-based routing rules.

LANGUAGE: json
CODE:
{
  "type":"timeBoundary"
}

LANGUAGE: json
CODE:
{
  "type":"priority",
  "minPriority":0,
  "maxPriority":1
}

LANGUAGE: json
CODE:
{
  "type" : "javascript",
  "function" : "function (config, query) { if (query.getAggregatorSpecs && query.getAggregatorSpecs().size() >= 3) { var size = config.getTierToBrokerMap().values().size(); if (size > 0) { return config.getTierToBrokerMap().values().toArray()[size-1] } else { return config.getDefaultBrokerServiceName() } } else { return null } }"
}

----------------------------------------

TITLE: Configuring StringLast Aggregator in Druid JSON
DESCRIPTION: Defines a stringLast aggregator to compute the metric value with the maximum timestamp or null if no row exists. Includes optional maxStringBytes and filterNullValues parameters.

LANGUAGE: JSON
CODE:
{
  "type" : "stringLast",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "maxStringBytes" : <integer> # (optional, defaults to 1024),
  "filterNullValues" : <boolean> # (optional, defaults to false)
}

----------------------------------------

TITLE: Constructing Logical AND Filter in Druid
DESCRIPTION: An AND filter combines multiple filters using logical AND operation.

LANGUAGE: JSON
CODE:
"filter": { "type": "and", "fields": [<filter>, <filter>, ...] }

----------------------------------------

TITLE: Configuring Spatial Dimensions in Druid JSON Data Spec
DESCRIPTION: Example configuration for specifying spatial dimensions in a Druid data ingestion spec. Shows how to define spatial dimensions using latitude and longitude coordinates.

LANGUAGE: json
CODE:
{
	"type": "hadoop",
	"dataSchema": {
		"dataSource": "DatasourceName",
		"parser": {
			"type": "string",
			"parseSpec": {
				"format": "json",
				"timestampSpec": {
					"column": "timestamp",
					"format": "auto"
				},
				"dimensionsSpec": {
					"dimensions": [],
					"spatialDimensions": [{
						"dimName": "coordinates",
						"dims": ["lat", "long"]
					}]
				}
			}
		}
	}
}

----------------------------------------

TITLE: Configuring Redis Cache Properties in Druid
DESCRIPTION: Configuration properties for Redis cache implementation in Druid common.runtime.properties file. These settings control Redis server connection parameters, cache entry expiration, timeout values, and connection pool settings.

LANGUAGE: properties
CODE:
druid.cache.host=<redis-host>
druid.cache.port=<redis-port>
druid.cache.expiration=86400000
druid.cache.timeout=2000
druid.cache.maxTotalConnections=8
druid.cache.maxIdleConnections=8
druid.cache.minIdleConnections=0

----------------------------------------

TITLE: Calculating Percentages with Post-Aggregators
DESCRIPTION: Example showing how to calculate percentages using arithmetic post-aggregators with multiple fields and operations.

LANGUAGE: json
CODE:
{
  "aggregations" : [
    { "type" : "doubleSum", "name" : "tot", "fieldName" : "total" },
    { "type" : "doubleSum", "name" : "part", "fieldName" : "part" }
  ],
  "postAggregations" : [{
    "type"   : "arithmetic",
    "name"   : "part_percentage",
    "fn"     : "*",
    "fields" : [
       { "type"   : "arithmetic",
         "name"   : "ratio",
         "fn"     : "/",
         "fields" : [
           { "type" : "fieldAccess", "name" : "part", "fieldName" : "part" },
           { "type" : "fieldAccess", "name" : "tot", "fieldName" : "tot" }
         ]
       },
       { "type" : "constant", "name": "const", "value" : 100 }
    ]
  }]
}

----------------------------------------

TITLE: Downloading Multiple Druid Extensions Using pull-deps
DESCRIPTION: Example command showing how to download multiple Druid extensions and Hadoop dependencies with specific versions. Uses the --clean flag to remove existing dependencies first.

LANGUAGE: bash
CODE:
java -classpath "/my/druid/lib/*" org.apache.druid.cli.Main tools pull-deps --clean -c org.apache.druid.extensions:mysql-metadata-storage:0.14.0-incubating -c org.apache.druid.extensions.contrib:druid-rabbitmq:0.14.0-incubating -h org.apache.hadoop:hadoop-client:2.3.0 -h org.apache.hadoop:hadoop-client:2.4.0

----------------------------------------

TITLE: Configuring LongMin Aggregator in Druid JSON
DESCRIPTION: The longMin aggregator computes the minimum of all metric values and Long.MAX_VALUE. It requires specifying an output name and the name of the metric column.

LANGUAGE: json
CODE:
{ "type" : "longMin", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: Configuring Library Dependencies and Assembly Merge Strategy in SBT for Apache Druid
DESCRIPTION: This snippet defines the library dependencies for an Apache Druid project, including core Druid modules, extensions, and related libraries. It also specifies custom assembly merge strategies to resolve conflicts during the build process. The configuration excludes certain dependencies and sets specific versions for Jackson libraries.

LANGUAGE: scala
CODE:
libraryDependencies ++= Seq(
  "com.amazonaws" % "aws-java-sdk" % "1.9.23" exclude("common-logging", "common-logging"),
  "org.joda" % "joda-convert" % "1.7",
  "joda-time" % "joda-time" % "2.7",
  "org.apache.druid" % "druid" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid" % "druid-services" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid" % "druid-indexing-service" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid" % "druid-indexing-hadoop" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid.extensions" % "mysql-metadata-storage" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid.extensions" % "druid-s3-extensions" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid.extensions" % "druid-histogram" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid.extensions" % "druid-hdfs-storage" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "com.fasterxml.jackson.core" % "jackson-annotations" % "2.3.0",
  "com.fasterxml.jackson.core" % "jackson-core" % "2.3.0",
  "com.fasterxml.jackson.core" % "jackson-databind" % "2.3.0",
  "com.fasterxml.jackson.datatype" % "jackson-datatype-guava" % "2.3.0",
  "com.fasterxml.jackson.datatype" % "jackson-datatype-joda" % "2.3.0",
  "com.fasterxml.jackson.jaxrs" % "jackson-jaxrs-base" % "2.3.0",
  "com.fasterxml.jackson.jaxrs" % "jackson-jaxrs-json-provider" % "2.3.0",
  "com.fasterxml.jackson.jaxrs" % "jackson-jaxrs-smile-provider" % "2.3.0",
  "com.fasterxml.jackson.module" % "jackson-module-jaxb-annotations" % "2.3.0",
  "com.sun.jersey" % "jersey-servlet" % "1.17.1",
  "mysql" % "mysql-connector-java" % "5.1.34",
  "org.scalatest" %% "scalatest" % "2.2.3" % "test",
  "org.mockito" % "mockito-core" % "1.10.19" % "test"
)

assemblyMergeStrategy in assembly := {
  case path if path contains "pom." => MergeStrategy.first
  case path if path contains "javax.inject.Named" => MergeStrategy.first
  case path if path contains "mime.types" => MergeStrategy.first
  case path if path contains "org/apache/commons/logging/impl/SimpleLog.class" => MergeStrategy.first
  case path if path contains "org/apache/commons/logging/impl/SimpleLog$1.class" => MergeStrategy.first
  case path if path contains "org/apache/commons/logging/impl/NoOpLog.class" => MergeStrategy.first
  case path if path contains "org/apache/commons/logging/LogFactory.class" => MergeStrategy.first
  case path if path contains "org/apache/commons/logging/LogConfigurationException.class" => MergeStrategy.first
  case path if path contains "org/apache/commons/logging/Log.class" => MergeStrategy.first
  case path if path contains "META-INF/jersey-module-version" => MergeStrategy.first
  case path if path contains ".properties" => MergeStrategy.first
  case path if path contains ".class" => MergeStrategy.first
  case x =>
    val oldStrategy = (assemblyMergeStrategy in assembly).value
    oldStrategy(x)
}

----------------------------------------

TITLE: Executing insert-segment-to-db Tool with MySQL and HDFS
DESCRIPTION: Example command to run the insert-segment-to-db tool using MySQL as metadata storage and HDFS as deep storage. It demonstrates how to specify necessary JVM arguments and command-line options.

LANGUAGE: java
CODE:
java 
-Ddruid.metadata.storage.type=mysql 
-Ddruid.metadata.storage.connector.connectURI=jdbc\:mysql\://localhost\:3306/druid 
-Ddruid.metadata.storage.connector.user=druid 
-Ddruid.metadata.storage.connector.password=diurd 
-Ddruid.extensions.loadList=[\"mysql-metadata-storage\",\"druid-hdfs-storage\"] 
-Ddruid.storage.type=hdfs
-cp $DRUID_CLASSPATH 
org.apache.druid.cli.Main tools insert-segment-to-db --workingDir hdfs://host:port//druid/storage/wikipedia --updateDescriptor true

----------------------------------------

TITLE: Markdown Navigation Structure
DESCRIPTION: A hierarchical markdown document defining the navigation structure and organization of Apache Druid's documentation, including links to all major sections and subsections.

LANGUAGE: markdown
CODE:
## Getting Started
  * [Design](/docs/VERSION/design/index.html)
  * [Getting Started](/docs/VERSION/operations/getting-started.html)
    * [Single-server Quickstart](/docs/VERSION/tutorials/index.html)
...

----------------------------------------

TITLE: Configuring DoubleMax Aggregator in Druid JSON
DESCRIPTION: The doubleMax aggregator computes the maximum of all metric values and Double.NEGATIVE_INFINITY. It requires specifying an output name and the name of the metric column.

LANGUAGE: json
CODE:
{ "type" : "doubleMax", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: Configuring Forever Broadcast Rule in Apache Druid
DESCRIPTION: JSON configuration for a forever broadcast rule that specifies how segments of different data sources should be co-located in Historical processes.

LANGUAGE: json
CODE:
{
  "type" : "broadcastForever",
  "colocatedDataSources" : [ "target_source1", "target_source2" ]
}

----------------------------------------

TITLE: Querying Druid INFORMATION_SCHEMA
DESCRIPTION: SQL query example to retrieve metadata for a specific Druid datasource from the INFORMATION_SCHEMA.

LANGUAGE: sql
CODE:
SELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = 'druid' AND TABLE_NAME = 'foo'

----------------------------------------

TITLE: Configuring Parquet Parser with TimeAndDims ParseSpec
DESCRIPTION: Example configuration for ingesting Parquet files using the direct Parquet parser with a timeAndDims parseSpec for simpler dimension specifications.

LANGUAGE: json
CODE:
{
  "type": "index_hadoop",
  "spec": {
    "ioConfig": {
      "type": "hadoop",
      "inputSpec": {
        "type": "static",
        "inputFormat": "org.apache.druid.data.input.parquet.DruidParquetInputFormat",
        "paths": "path/to/file.parquet"
      }
    },
    "dataSchema": {
      "dataSource": "example",
      "parser": {
        "type": "parquet",
        "parseSpec": {
          "format": "timeAndDims",
          "timestampSpec": {
            "column": "timestamp",
            "format": "auto"
          },
          "dimensionsSpec": {
            "dimensions": [
              "dim1",
              "dim2",
              "dim3",
              "listDim"
            ],
            "dimensionExclusions": [],
            "spatialDimensions": []
          }
        }
      }
    }
  }
}

----------------------------------------

TITLE: Configuring Lookup DimensionSpec with External Lookup in Druid Query
DESCRIPTION: The Lookup DimensionSpec can also use an external lookup table or resource that is already registered via configuration or Coordinator.

LANGUAGE: JSON
CODE:
{
  "type":"lookup",
  "dimension":"dimensionName",
  "outputName":"dimensionOutputName",
  "name":"lookupName"
}

----------------------------------------

TITLE: Constructing an AND Logical Filter in Druid JSON
DESCRIPTION: Example of an AND logical filter that combines multiple filters. All specified filters must match for a row to be included.

LANGUAGE: json
CODE:
"filter": { "type": "and", "fields": [<filter>, <filter>, ...] }

----------------------------------------

TITLE: Querying Materialized Views in Druid
DESCRIPTION: Example of a view query that wraps a groupBy query to leverage materialized views for optimized performance. Demonstrates the structure for view-based querying.

LANGUAGE: json
CODE:
{
    "queryType": "view",
    "query": {
        "queryType": "groupBy",
        "dataSource": "wikiticker",
        "granularity": "all",
        "dimensions": [
            "user"
        ],
        "limitSpec": {
            "type": "default",
            "limit": 1,
            "columns": [
                {
                    "dimension": "added",
                    "direction": "descending",
                    "dimensionOrder": "numeric"
                }
            ]
        },
        "aggregations": [
            {
                "type": "longSum",
                "name": "added",
                "fieldName": "added"
            }
        ],
        "intervals": [
            "2015-09-12/2015-09-13"
        ]
    }
}

----------------------------------------

TITLE: Configuring Forever Broadcast Rule in Apache Druid
DESCRIPTION: JSON configuration for a forever broadcast rule that specifies how segments of different data sources should be co-located in Historical processes.

LANGUAGE: json
CODE:
{
  "type" : "broadcastForever",
  "colocatedDataSources" : [ "target_source1", "target_source2" ]
}

----------------------------------------

TITLE: Configuring Local Mount Deep Storage in Apache Druid
DESCRIPTION: YAML configuration for setting up local mount deep storage in Apache Druid. Specifies the storage type and directory for storing segments.

LANGUAGE: yaml
CODE:
druid.storage.type: local
druid.storage.storageDirectory: /path/to/storage

----------------------------------------

TITLE: Configuring TwitterSpritzerFirehose in Druid
DESCRIPTION: This JSON snippet demonstrates how to configure the TwitterSpritzerFirehose in Druid. It specifies the firehose type as 'twitzer' and sets parameters for maximum event count and run duration.

LANGUAGE: json
CODE:
"firehose" : {
    "type" : "twitzer",
    "maxEventCount": -1,
    "maxRunMinutes": 0
}

----------------------------------------

TITLE: Executing Timeseries Query with DistinctCount Aggregator in Apache Druid
DESCRIPTION: This JSON query demonstrates how to use the DistinctCount aggregator in a Timeseries query. It calculates the unique visitor count (uv) based on the visitor_id field over a specified time interval.

LANGUAGE: json
CODE:
{
  "queryType": "timeseries",
  "dataSource": "sample_datasource",
  "granularity": "day",
  "aggregations": [
    {
      "type": "distinctCount",
      "name": "uv",
      "fieldName": "visitor_id"
    }
  ],
  "intervals": [
    "2016-03-01T00:00:00.000/2013-03-20T00:00:00.000"
  ]
}

----------------------------------------

TITLE: Retrieving Active MiddleManager Tasks
DESCRIPTION: Example JSON response from the /druid/worker/v1/tasks endpoint listing active task IDs on a MiddleManager.

LANGUAGE: json
CODE:
["index_wikiticker_2019-02-11T02:20:15.316Z"]

----------------------------------------

TITLE: Configuring Processing Buffer Size in YAML
DESCRIPTION: Sets the size of processing buffers to 500MB. This configuration affects the amount of direct memory allocated for query processing on Historical nodes.

LANGUAGE: yaml
CODE:
druid.processing.buffer.sizeBytes: 524288000

----------------------------------------

TITLE: Filter Example in Druid Transform Spec
DESCRIPTION: Example of a selector filter that only ingests rows where the country column equals 'United States'.

LANGUAGE: json
CODE:
"filter": {
  "type": "selector",
  "dimension": "country",
  "value": "United States"
}

----------------------------------------

TITLE: Configuring Variance Aggregator for Ingestion
DESCRIPTION: Configuration for pre-aggregating variance at ingestion time. Supports input types of float, long, or variance with optional population estimator setting.

LANGUAGE: json
CODE:
{
  "type" : "variance",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "inputType" : <input_type>,
  "estimator" : <string>
}

----------------------------------------

TITLE: Configuring HLLSketchEstimateWithBounds Post-Aggregator in Druid
DESCRIPTION: This JSON configuration defines an HLLSketchEstimateWithBounds post-aggregator for estimating distinct count with error bounds.

LANGUAGE: json
CODE:
{
  "type"  : "HLLSketchEstimateWithBounds",
  "name": <output name>,
  "field"  : <post aggregator that returns an HLL Sketch>,
  "numStdDev" : <number of standard deviations: 1 (default), 2 or 3>
}

----------------------------------------

TITLE: JavaScript Worker Selection Strategy Example
DESCRIPTION: Example configuration for defining custom worker selection logic using JavaScript

LANGUAGE: json
CODE:
{
"type":"javascript",
"function":"function (config, zkWorkers, task) {\nvar batch_workers = new java.util.ArrayList();\nbatch_workers.add(\"10.0.0.1\");\nbatch_workers.add(\"10.0.0.2\");\nworkers = zkWorkers.keySet().toArray();\nvar sortedWorkers = new Array()\n;for(var i = 0; i < workers.length; i++){\n sortedWorkers[i] = workers[i];\n}\nArray.prototype.sort.call(sortedWorkers,function(a, b){return zkWorkers.get(b).getCurrCapacityUsed() - zkWorkers.get(a).getCurrCapacityUsed();});\nvar minWorkerVer = config.getMinWorkerVersion();\nfor (var i = 0; i < sortedWorkers.length; i++) {\n var worker = sortedWorkers[i];\n  var zkWorker = zkWorkers.get(worker);\n  if(zkWorker.canRunTask(task) && zkWorker.isValidVersion(minWorkerVer)){\n    if(task.getType() == 'index_hadoop' && batch_workers.contains(worker)){\n      return worker;\n    } else {\n      if(task.getType() != 'index_hadoop' && !batch_workers.contains(worker)){\n        return worker;\n      }\n    }\n  }\n}\nreturn null;\n}"
}

----------------------------------------

TITLE: Configuring Send-All Converter for Ambari Metrics Emitter in Druid
DESCRIPTION: JSON configuration for the 'all' event converter, which sends all Druid service metrics events to Ambari Metrics. This converter allows control over the namespace prefix, application name, and includes the Druid hostname in the metric path.

LANGUAGE: json
CODE:
"druid.emitter.ambari-metrics.eventConverter={\"type\":\"all\", \"namespacePrefix\": \"druid.test\", \"appName\":\"druid\"}"

----------------------------------------

TITLE: Implementing LongSum Aggregator in Druid
DESCRIPTION: The longSum aggregator computes the sum of values as a 64-bit signed integer. It requires specifying an output name and the metric field to sum over.

LANGUAGE: json
CODE:
{ "type" : "longSum", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: Implementing RegexSearchQuerySpec in Druid
DESCRIPTION: Pattern-based search query specification that matches if any part of a dimension value matches the specified regex pattern.

LANGUAGE: json
CODE:
{
  "type"  : "regex",
  "pattern" : "some_pattern"
}

----------------------------------------

TITLE: Executing SQL Timeseries Query in Druid
DESCRIPTION: This snippet shows a SQL Timeseries query that calculates the sum of deleted lines per hour. It demonstrates how to use time floor functions and aggregations in Druid SQL.

LANGUAGE: sql
CODE:
SELECT FLOOR(__time to HOUR) AS HourTime, SUM(deleted) AS LinesDeleted FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY FLOOR(__time to HOUR);

----------------------------------------

TITLE: Estimating Distinct Keys with ArrayOfDoublesSketch Post-Aggregator
DESCRIPTION: Post-aggregator configuration to estimate the number of distinct keys from an ArrayOfDoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToEstimate",
  "name": <output name>,
  "field"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>
}

----------------------------------------

TITLE: Registering Password Provider in Jackson Module
DESCRIPTION: Example of registering a custom PasswordProvider implementation as a Jackson subtype in a Druid module.

LANGUAGE: Java
CODE:
return ImmutableList.of(
    new SimpleModule("SomePasswordProviderModule")
        .registerSubtypes(
            new NamedType(SomePasswordProvider.class, "some")
        )
);

----------------------------------------

TITLE: Constructing BloomKFilter in Java for Druid Queries
DESCRIPTION: This Java code snippet demonstrates how to create a BloomKFilter, add values to it, and serialize it for use in Druid queries.

LANGUAGE: java
CODE:
BloomKFilter bloomFilter = new BloomKFilter(1500);
bloomFilter.addString("value 1");
bloomFilter.addString("value 2");
bloomFilter.addString("value 3");
ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();
BloomKFilter.serialize(byteArrayOutputStream, bloomFilter);
String base64Serialized = Base64.encodeBase64String(byteArrayOutputStream.toByteArray());

----------------------------------------

TITLE: React Scheduler License
DESCRIPTION: Declares the MIT license for React's scheduler production build, version 0.20.2.

LANGUAGE: JavaScript
CODE:
/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Configuring OpenTSDB Emitter Metrics in JSON
DESCRIPTION: Example JSON configuration for specifying desired metrics and dimensions to be sent to OpenTSDB from Druid. This snippet shows how to configure the 'query/time' metric to include 'dataSource' and 'type' dimensions.

LANGUAGE: json
CODE:
{
"query/time": [
    "dataSource",
    "type"
]
}

----------------------------------------

TITLE: Submitting a Kinesis Supervisor Spec
DESCRIPTION: Example of submitting a supervisor specification for Kinesis ingestion via HTTP POST request.

LANGUAGE: bash
CODE:
curl -X POST -H 'Content-Type: application/json' -d @supervisor-spec.json http://localhost:8090/druid/indexer/v1/supervisor

----------------------------------------

TITLE: Field Accessor Post-Aggregators in Druid
DESCRIPTION: Returns values from specified aggregators. Supports both raw aggregation objects and finalized values for complex aggregators.

LANGUAGE: json
CODE:
{ "type" : "fieldAccess", "name": <output_name>, "fieldName" : <aggregator_name> }

LANGUAGE: json
CODE:
{ "type" : "finalizingFieldAccess", "name": <output_name>, "fieldName" : <aggregator_name> }

----------------------------------------

TITLE: Generating Protobuf Descriptor File
DESCRIPTION: This command uses the protoc compiler to generate a descriptor file from the Protobuf schema. The descriptor file is used by Druid for parsing Protobuf messages.

LANGUAGE: bash
CODE:
protoc -o /tmp/metrics.desc metrics.proto

----------------------------------------

TITLE: Querying for Ingested Event Count in Druid
DESCRIPTION: Example of how to query for the number of ingested events using a longSum aggregator in Druid.

LANGUAGE: json
CODE:
"aggregations": [
    { "type": "longSum", "name": "numIngestedEvents", "fieldName": "count" }
]

----------------------------------------

TITLE: Field Accessor Post-Aggregators in Druid
DESCRIPTION: Returns values from specified aggregators. Supports both raw aggregation objects and finalized values for complex aggregators.

LANGUAGE: json
CODE:
{ "type" : "fieldAccess", "name": <output_name>, "fieldName" : <aggregator_name> }

LANGUAGE: json
CODE:
{ "type" : "finalizingFieldAccess", "name": <output_name>, "fieldName" : <aggregator_name> }

----------------------------------------

TITLE: Defining Greatest Post-Aggregator in Druid Query
DESCRIPTION: Shows the structure of a 'doubleGreatest' post-aggregator that computes the maximum of all specified fields and Double.NEGATIVE_INFINITY.

LANGUAGE: JSON
CODE:
{
  "type"  : "doubleGreatest",
  "name"  : <output_name>,
  "fields": [<post_aggregator>, <post_aggregator>, ...]
}

----------------------------------------

TITLE: Querying Druid SYS Schema Tasks
DESCRIPTION: SQL query example to retrieve information about failed tasks from the SYS schema.

LANGUAGE: sql
CODE:
SELECT * FROM sys.tasks WHERE status='FAILED';

----------------------------------------

TITLE: Configuring TuningConfig in JSON for Hadoop Indexer
DESCRIPTION: JSON configuration for the tuningConfig section of the Hadoop indexer spec file. It includes the workingPath field for specifying the directory for intermediate results.

LANGUAGE: json
CODE:
"tuningConfig" : {
 ...
  "workingPath": "/tmp",
  ...
}

----------------------------------------

TITLE: Cloning Druid Source Repository
DESCRIPTION: Commands to clone the Apache Druid repository from GitHub and navigate to the project directory.

LANGUAGE: bash
CODE:
git clone git@github.com:apache/incubator-druid.git
cd druid

----------------------------------------

TITLE: Configuring Core Extensions in Druid's common.runtime.properties
DESCRIPTION: This snippet demonstrates how to load bundled core extensions in Druid by adding their names to the druid.extensions.loadList property in the common.runtime.properties file. It shows an example of loading the postgresql-metadata-storage and druid-hdfs-storage extensions.

LANGUAGE: properties
CODE:
druid.extensions.loadList=["postgresql-metadata-storage", "druid-hdfs-storage"]

----------------------------------------

TITLE: Configuring Kafka Simple Consumer Firehose in Druid
DESCRIPTION: JSON configuration for setting up a Kafka Simple Consumer firehose in Druid. Specifies broker connections, queue settings, partition handling, and topic details. Used for ingesting data from Kafka topics into Druid's realtime processes.

LANGUAGE: json
CODE:
{
  "firehoseV2": {
    "type" : "kafka-0.8-v2",
    "brokerList" :  ["localhost:4443"],
    "queueBufferLength":10001,
    "resetOffsetToEarliest":"true",
    "partitionIdList" : ["0"],
    "clientId" : "localclient",
    "feed": "wikipedia"
  }
}

----------------------------------------

TITLE: Configuring Lookup DimensionSpec with External Lookup in Apache Druid
DESCRIPTION: JSON configuration for a Lookup DimensionSpec using an external lookup table or resource.

LANGUAGE: JSON
CODE:
{
  "type":"lookup",
  "dimension":"dimensionName",
  "outputName":"dimensionOutputName",
  "name":"lookupName"
}

----------------------------------------

TITLE: Configuring Core Extensions in Druid's common.runtime.properties
DESCRIPTION: This snippet demonstrates how to load bundled core extensions in Druid by adding their names to the druid.extensions.loadList property in the common.runtime.properties file. It shows an example of loading the postgresql-metadata-storage and druid-hdfs-storage extensions.

LANGUAGE: properties
CODE:
druid.extensions.loadList=["postgresql-metadata-storage", "druid-hdfs-storage"]

----------------------------------------

TITLE: Sample Coordinator Dynamic Configuration
DESCRIPTION: Example JSON configuration for dynamically configuring the Coordinator process

LANGUAGE: json
CODE:
{
  "millisToWaitBeforeDeleting": 900000,
  "mergeBytesLimit": 100000000,
  "mergeSegmentsLimit" : 1000,
  "maxSegmentsToMove": 5,
  "replicantLifetime": 15,
  "replicationThrottleLimit": 10,
  "emitBalancingStats": false,
  "killDataSourceWhitelist": ["wikipedia", "testDatasource"],
  "decommissioningNodes": ["localhost:8182", "localhost:8282"],
  "decommissioningMaxPercentOfMaxSegmentsToMove": 70
}

----------------------------------------

TITLE: Cardinality Aggregator Example: Distinct People
DESCRIPTION: An example of using the cardinality aggregator to determine the number of distinct people based on combinations of first and last names. It uses the byRow option set to true for row-based computation.

LANGUAGE: json
CODE:
{
  "type": "cardinality",
  "name": "distinct_people",
  "fields": [ "first_name", "last_name" ],
  "byRow" : true
}

----------------------------------------

TITLE: Configuring Kafka Emitter in Apache Druid
DESCRIPTION: Example configuration for setting up the Kafka Emitter in Apache Druid. This snippet demonstrates how to specify Kafka bootstrap servers, topic names for metrics and alerts, and additional producer configuration.

LANGUAGE: yaml
CODE:
druid.emitter.kafka.bootstrap.servers=hostname1:9092,hostname2:9092
druid.emitter.kafka.metric.topic=druid-metric
druid.emitter.kafka.alert.topic=druid-alert
druid.emitter.kafka.producer.config={"max.block.ms":10000}

----------------------------------------

TITLE: Implementing Field Accessor Post-Aggregators
DESCRIPTION: Shows configuration for field accessor post-aggregators that return values from specified aggregators, with options for raw or finalized access.

LANGUAGE: json
CODE:
{ "type" : "fieldAccess", "name": <output_name>, "fieldName" : <aggregator_name> }

LANGUAGE: json
CODE:
{ "type" : "finalizingFieldAccess", "name": <output_name>, "fieldName" : <aggregator_name> }

----------------------------------------

TITLE: Illustrating Updated Segment Versions in Druid
DESCRIPTION: This snippet shows how segment names change when data is reindexed with a new schema, resulting in a higher version number.

LANGUAGE: plaintext
CODE:
foo_2015-01-01/2015-01-02_v2_0
foo_2015-01-01/2015-01-02_v2_1
foo_2015-01-01/2015-01-02_v2_2

----------------------------------------

TITLE: Implementing Case-Sensitive Contains Search Query in Druid
DESCRIPTION: Defines a case-sensitive search query that matches if any part of a dimension value contains the specified search value. Provides explicit case sensitivity control.

LANGUAGE: json
CODE:
{
  "type"  : "contains",
  "case_sensitive" : true,
  "value" : "some_value"
}

----------------------------------------

TITLE: Configuring Druid Extension for DataSketches
DESCRIPTION: Configuration snippet to include the DataSketches extension in Druid's config file.

LANGUAGE: json
CODE:
"druid.extensions.loadList=[\"druid-datasketches\"]"

----------------------------------------

TITLE: Quantiles Doubles Sketch to Histogram Post Aggregator
DESCRIPTION: Post aggregator configuration for generating a histogram from a DoublesSketch using specified split points.

LANGUAGE: json
CODE:
{
  "type"  : "quantilesDoublesSketchToHistogram",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch>,
  "splitPoints" : <array of split points>
}

----------------------------------------

TITLE: SQL Query Using Bloom Filter Aggregator in Druid
DESCRIPTION: This SQL snippet demonstrates how to compute a bloom filter in a Druid SQL query using the BLOOM_FILTER aggregator function.

LANGUAGE: sql
CODE:
SELECT BLOOM_FILTER(<expression>, <max number of entries>) FROM druid.foo WHERE dim2 = 'abc'

----------------------------------------

TITLE: Compaction Task Specification - Day Granularity
DESCRIPTION: JSON configuration for compacting segments into day-level granularity, combining multiple hour-level segments into single day segments.

LANGUAGE: json
CODE:
{
  "type": "compact",
  "dataSource": "compaction-tutorial",
  "interval": "2015-09-12/2015-09-13",
  "segmentGranularity": "DAY",
  "tuningConfig" : {
    "type" : "index",
    "maxRowsPerSegment" : 5000000,
    "maxRowsInMemory" : 25000,
    "forceExtendableShardSpecs" : true
  }
}

----------------------------------------

TITLE: Querying with Timestamp Min/Max Aggregators in Apache Druid
DESCRIPTION: Example of a groupBy query using timeMin and timeMax aggregators. This query groups data by day and product, calculating the count, minimum timestamp, and maximum timestamp for each group.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "timeMinMax",
  "granularity": "DAY",
  "dimensions": ["product"],
  "aggregations": [
    {
      "type": "count",
      "name": "count"
    },
    {
      "type": "timeMin",
      "name": "<output_name of timeMin>",
      "fieldName": "tmin"
    },
    {
      "type": "timeMax",
      "name": "<output_name of timeMax>",
      "fieldName": "tmax"
    }
  ],
  "intervals": [
    "2010-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z"
  ]
}

----------------------------------------

TITLE: Executing Basic Druid Query via HTTP POST
DESCRIPTION: Example of sending a Druid query using curl with JSON content type. This demonstrates the basic HTTP POST request format for querying Druid endpoints.

LANGUAGE: bash
CODE:
curl -X POST '<queryable_host>:<port>/druid/v2/?pretty' -H 'Content-Type:application/json' -H 'Accept:application/json' -d @<query_json_file>

----------------------------------------

TITLE: Implementing FragmentSearchQuerySpec in Druid
DESCRIPTION: Search query specification that matches if a dimension value contains all specified fragments, with configurable case sensitivity.

LANGUAGE: json
CODE:
{ 
  "type" : "fragment",
  "case_sensitive" : false,
  "values" : ["fragment1", "fragment2"]
}

----------------------------------------

TITLE: Sample StatsD Metric Mapping Configuration
DESCRIPTION: JSON configuration examples showing how to map Druid metrics to StatsD format. Demonstrates metric mapping with dimensions, types, and range conversion settings.

LANGUAGE: json
CODE:
{
  "query/time": { 
    "dimensions": ["dataSource", "type"], 
    "type": "timer"
  },
  "coordinator-segment/count": { 
    "dimensions": ["dataSource"], 
    "type": "gauge" 
  },
  "historical-segment/count": { 
    "dimensions": ["dataSource", "tier", "priority"], 
    "type": "gauge" 
  }
}

----------------------------------------

TITLE: Sample Multi-value Data Structure in Druid
DESCRIPTION: Example data structure showing how multi-value dimensions are stored in Druid, with a 'tags' dimension containing arrays of values.

LANGUAGE: json
CODE:
{"timestamp": "2011-01-12T00:00:00.000Z", "tags": ["t1","t2","t3"]}
{"timestamp": "2011-01-13T00:00:00.000Z", "tags": ["t3","t4","t5"]}
{"timestamp": "2011-01-14T00:00:00.000Z", "tags": ["t5","t6","t7"]}
{"timestamp": "2011-01-14T00:00:00.000Z", "tags": []}

----------------------------------------

TITLE: Defining Arithmetic Post-Aggregator in Druid JSON Query
DESCRIPTION: Demonstrates how to specify an arithmetic post-aggregator in a Druid query. This post-aggregator applies a specified function to given fields, which can be aggregators or other post-aggregators.

LANGUAGE: json
CODE:
{
  "type"  : "arithmetic",
  "name"  : <output_name>,
  "fn"    : <arithmetic_function>,
  "fields": [<post_aggregator>, <post_aggregator>, ...],
  "ordering" : <null (default), or "numericFirst">
}

----------------------------------------

TITLE: Basic Moving Average Query in Druid
DESCRIPTION: Example query demonstrating how to calculate a 7-bucket moving average for Wikipedia edit deltas using 30-minute intervals.

LANGUAGE: json
CODE:
{
  "queryType": "movingAverage",
  "dataSource": "wikipedia",
  "granularity": {
    "type": "period",
    "period": "PT30M"
  },
  "intervals": [
    "2015-09-12T00:00:00Z/2015-09-13T00:00:00Z"
  ],
  "aggregations": [
    {
      "name": "delta30Min",
      "fieldName": "delta",
      "type": "longSum"
    }
  ],
  "averagers": [
    {
      "name": "trailing30MinChanges",
      "fieldName": "delta30Min",
      "type": "longMean",
      "buckets": 7
    }
  ]
}

----------------------------------------

TITLE: Configuring Send-All Converter for Ambari Metrics Emitter in Druid
DESCRIPTION: JSON configuration for the 'all' event converter, which sends all Druid service metrics events to Ambari Metrics. This converter allows control over the namespace prefix, app name, and includes the Druid hostname in the metric path.

LANGUAGE: json
CODE:
druid.emitter.ambari-metrics.eventConverter={"type":"all", "namespacePrefix": "druid.test", "appName":"druid"}

----------------------------------------

TITLE: Quantiles Doubles Sketch to Quantiles Post Aggregator
DESCRIPTION: Post aggregator configuration for extracting multiple quantile values from a DoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "quantilesDoublesSketchToQuantiles",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch>,
  "fractions" : <array of fractional positions>
}

----------------------------------------

TITLE: MySQL Connection in Druid Configuration
DESCRIPTION: Properties configuration for connecting Druid to MySQL metadata storage. Specifies the extension loading, storage type, connection URI, and credentials.

LANGUAGE: properties
CODE:
druid.extensions.loadList=["mysql-metadata-storage"]
druid.metadata.storage.type=mysql
druid.metadata.storage.connector.connectURI=jdbc:mysql://<host>/druid
druid.metadata.storage.connector.user=druid
druid.metadata.storage.connector.password=druid

----------------------------------------

TITLE: Configuring Broker Direct Memory in Java
DESCRIPTION: Calculates and sets the direct memory size for a Broker node based on the number of merge buffers and buffer size. This configuration is typically set as a JVM argument.

LANGUAGE: java
CODE:
-XX:MaxDirectMemorySize=$(( (numMergeBuffers + 2) * bufferSizeBytes ))

----------------------------------------

TITLE: Executing GroupBy Query with DistinctCount Aggregator in Druid
DESCRIPTION: This JSON snippet illustrates the use of the DistinctCount aggregator in a GroupBy query. It groups the results by 'sample_dim' and calculates the distinct count of 'visitor_id' for each group.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "sample_datasource",
  "dimensions": "[sample_dim]",
  "granularity": "all",
  "aggregations": [
    {
      "type": "distinctCount",
      "name": "uv",
      "fieldName": "visitor_id"
    }
  ],
  "intervals": [
    "2016-03-06T00:00:00/2016-03-06T23:59:59"
  ]
}

----------------------------------------

TITLE: Running Command Line Hadoop Indexer in Java
DESCRIPTION: Command to execute the Hadoop Indexer from the command line. It specifies memory allocation, timezone, file encoding, classpath, and the main class to run.

LANGUAGE: java
CODE:
java -Xmx256m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -classpath lib/*:<hadoop_config_dir> org.apache.druid.cli.Main index hadoop <spec_file>

----------------------------------------

TITLE: Submitting Kafka Supervisor Spec to Druid
DESCRIPTION: cURL command to submit a Kafka supervisor specification to Druid's overlord, enabling Kafka ingestion.

LANGUAGE: bash
CODE:
curl -XPOST -H'Content-Type: application/json' -d @quickstart/tutorial/wikipedia-kafka-supervisor.json http://localhost:8090/druid/indexer/v1/supervisor

----------------------------------------

TITLE: Calculating Average Users per Row in Druid Query
DESCRIPTION: Demonstrates a complex post-aggregation example that calculates the average number of unique users per row using hyperUniqueCardinality and arithmetic post-aggregators.

LANGUAGE: JSON
CODE:
"aggregations" : [{
    {"type" : "count", "name" : "rows"},
    {"type" : "hyperUnique", "name" : "unique_users", "fieldName" : "uniques"}
  }],
  "postAggregations" : [{
    "type"   : "arithmetic",
    "name"   : "average_users_per_row",
    "fn"     : "/",
    "fields" : [
      { "type" : "hyperUniqueCardinality", "fieldName" : "unique_users" },
      { "type" : "fieldAccess", "name" : "rows", "fieldName" : "rows" }
    ]
  }]

----------------------------------------

TITLE: Implementing StringFirst Aggregator in Druid
DESCRIPTION: The stringFirst aggregator computes the metric value with the minimum timestamp or null if no row exists. It allows setting a maximum string length and filtering null values.

LANGUAGE: json
CODE:
{
  "type" : "stringFirst",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "maxStringBytes" : <integer> # (optional, defaults to 1024),
  "filterNullValues" : <boolean> # (optional, defaults to false)
}

----------------------------------------

TITLE: Configuring JSON parseSpec for Druid Ingestion
DESCRIPTION: JSON configuration for the parseSpec section of a Druid ingestion task. Specifies the format as JSON and defines timestamp and dimension specifications.

LANGUAGE: json
CODE:
{
  "parseSpec":{
    "format" : "json",
    "timestampSpec" : {
      "column" : "timestamp"
    },
    "dimensionSpec" : {
      "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"]
    }
  }
}

----------------------------------------

TITLE: Configuring TimedShutoffFirehose in Apache Druid
DESCRIPTION: Demonstrates how to configure a TimedShutoffFirehose that will shut down at a specified time. It wraps another firehose as a delegate.

LANGUAGE: json
CODE:
{
    "type"  :   "timed",
    "shutoffTime": "2015-08-25T01:26:05.119Z",
    "delegate": {
          "type": "receiver",
          "serviceName": "eventReceiverServiceName",
          "bufferSize": 100000
     }
}

----------------------------------------

TITLE: Sample Raw Data for Druid Ingestion
DESCRIPTION: Illustrates a sample of raw data representing packet/byte counts between source and destination IPs, used to demonstrate Druid's rollup functionality.

LANGUAGE: plaintext
CODE:
timestamp                 srcIP         dstIP          packets     bytes
2018-01-01T01:01:35Z      1.1.1.1       2.2.2.2            100      1000
2018-01-01T01:01:51Z      1.1.1.1       2.2.2.2            200      2000
2018-01-01T01:01:59Z      1.1.1.1       2.2.2.2            300      3000
2018-01-01T01:02:14Z      1.1.1.1       2.2.2.2            400      4000
2018-01-01T01:02:29Z      1.1.1.1       2.2.2.2            500      5000
2018-01-01T01:03:29Z      1.1.1.1       2.2.2.2            600      6000
2018-01-02T21:33:14Z      7.7.7.7       8.8.8.8            100      1000
2018-01-02T21:33:45Z      7.7.7.7       8.8.8.8            200      2000
2018-01-02T21:35:45Z      7.7.7.7       8.8.8.8            300      3000

----------------------------------------

TITLE: Starting Druid Real-time Server
DESCRIPTION: Command to start a Druid real-time processing server instance. This invokes the main server class with the realtime parameter.

LANGUAGE: java
CODE:
org.apache.druid.cli.Main server realtime

----------------------------------------

TITLE: Configuring Derby as Metadata Storage in Druid
DESCRIPTION: This snippet shows how to configure Derby as the metadata storage in Druid using properties. It sets the storage type and connection URI.

LANGUAGE: properties
CODE:
druid.metadata.storage.type=derby
druid.metadata.storage.connector.connectURI=jdbc:derby://localhost:1527//opt/var/druid_state/derby;create=true

----------------------------------------

TITLE: SQL Query for Retrieving Failed Tasks in Druid
DESCRIPTION: Example SQL query to retrieve information about failed tasks from the sys.tasks table.

LANGUAGE: SQL
CODE:
SELECT * FROM sys.tasks WHERE status='FAILED';

----------------------------------------

TITLE: Distinct Countries Cardinality Example
DESCRIPTION: Example showing how to calculate distinct countries across two dimensions: country of origin and residence.

LANGUAGE: json
CODE:
{
  "type": "cardinality",
  "name": "distinct_countries",
  "fields": [ "country_of_origin", "country_of_residence" ]
}

----------------------------------------

TITLE: Druid Time Function Examples
DESCRIPTION: Example of time-related function syntax showing timestamp manipulation and formatting operations.

LANGUAGE: SQL
CODE:
timestamp(expr[,format-string])
timestamp_ceil(expr, period, [origin, [timezone]])
timestamp_floor(expr, period, [origin, [timezone]])
timestamp_shift(expr, period, step, [timezone])
timestamp_extract(expr, unit, [timezone])
timestamp_parse(string expr, [pattern, [timezone]])
timestamp_format(expr, [pattern, [timezone]])

----------------------------------------

TITLE: Configuring Asynchronous Logging in Druid with Log4j2
DESCRIPTION: XML configuration for Log4j2 that implements asynchronous logging for chatty Druid classes to improve logging performance. Sets up console appender with ISO8601 timestamp pattern and configures specific loggers for inventory management and server view components.

LANGUAGE: xml
CODE:
<?xml version="1.0" encoding="UTF-8" ?>
<Configuration status="WARN">
  <Appenders>
    <Console name="Console" target="SYSTEM_OUT">
      <PatternLayout pattern="%d{ISO8601} %p [%t] %c - %m%n"/>
    </Console>
  </Appenders>
  <Loggers>
    <AsyncLogger name="org.apache.druid.curator.inventory.CuratorInventoryManager" level="debug" additivity="false">
      <AppenderRef ref="Console"/>
    </AsyncLogger>
    <AsyncLogger name="org.apache.druid.client.BatchServerInventoryView" level="debug" additivity="false">
      <AppenderRef ref="Console"/>
    </AsyncLogger>
    <!-- Make extra sure nobody adds logs in a bad way that can hurt performance -->
    <AsyncLogger name="org.apache.druid.client.ServerInventoryView" level="debug" additivity="false">
      <AppenderRef ref="Console"/>
    </AsyncLogger>
    <AsyncLogger name ="org.apache.druid.java.util.http.client.pool.ChannelResourceFactory" level="info" additivity="false">
      <AppenderRef ref="Console"/>
    </AsyncLogger>
    <Root level="info">
      <AppenderRef ref="Console"/>
    </Root>
  </Loggers>
</Configuration>

----------------------------------------

TITLE: Example Results of Scan Query in Compacted List Format
DESCRIPTION: This JSON snippet illustrates the structure of results returned by a Scan query when the resultFormat is set to 'compactedList'. It includes segment information, column names, and event data in a more compact format.

LANGUAGE: json
CODE:
[{
    "segmentId" : "wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9",
    "columns" : [
      "timestamp", "robot", "namespace", "anonymous", "unpatrolled", "page", "language", "newpage", "user", "count", "added", "delta", "variation", "deleted"
    ],
    "events" : [
     ["2013-01-01T00:00:00.000Z", "1", "article", "0", "0", "11._korpus_(NOVJ)", "sl", "0", "EmausBot", 1.0, 39.0, 39.0, 39.0, 0.0],
     ["2013-01-01T00:00:00.000Z", "0", "article", "0", "0", "112_U.S._580", "en", "1", "MZMcBride", 1.0, 70.0, 70.0, 70.0, 0.0],
     ["2013-01-01T00:00:00.000Z", "0", "article", "0", "0", "113_U.S._243", "en", "1", "MZMcBride", 1.0, 77.0, 77.0, 77.0, 0.0],
     ["2013-01-01T00:00:00.000Z", "0", "article", "0", "0", "113_U.S._73", "en", "1", "MZMcBride", 1.0, 70.0, 70.0, 70.0, 0.0],
     ["2013-01-01T00:00:00.000Z", "0", "article", "0", "0", "113_U.S._756", "en", "1", "MZMcBride", 1.0, 68.0, 68.0, 68.0, 0.0]
    ]
} ]

----------------------------------------

TITLE: Setting User Password via API
DESCRIPTION: Example JSON request body for setting a user's password through the authentication API.

LANGUAGE: json
CODE:
{
  "password": "helloworld"
}

----------------------------------------

TITLE: Configuring StaticCloudFilesFirehose in Apache Druid
DESCRIPTION: JSON configuration for setting up a static Cloud Files firehose in Druid. This configuration enables ingesting data from multiple Cloud Files blobs with support for caching and prefetching features.

LANGUAGE: json
CODE:
{
    "type" : "static-cloudfiles",
    "blobs": [
        {
          "region": "DFW"
          "container": "container",
          "path": "/path/to/your/file.json"
        },
        {
          "region": "ORD"
          "container": "anothercontainer",
          "path": "/another/path.json"
        }
    ]
}

----------------------------------------

TITLE: Enabling Grand Totals in Druid Timeseries Query
DESCRIPTION: Example showing how to enable grand totals in a timeseries query by adding the grandTotal context parameter. This adds an additional summary row to the query results.

LANGUAGE: json
CODE:
{
  "queryType": "timeseries",
  "dataSource": "sample_datasource",
  "intervals": [ "2012-01-01T00:00:00.000/2012-01-03T00:00:00.000" ],
  "granularity": "day",
  "aggregations": [
    { "type": "longSum", "name": "sample_name1", "fieldName": "sample_fieldName1" },
    { "type": "doubleSum", "name": "sample_name2", "fieldName": "sample_fieldName2" }
  ],
  "context": {
    "grandTotal": true
  }
}

----------------------------------------

TITLE: Configuring TimeMin Aggregator for Ingestion in Druid
DESCRIPTION: JSON configuration for including a timeMin aggregator during data ingestion in Druid. This aggregator calculates the minimum timestamp for a specified field.

LANGUAGE: json
CODE:
{
    "type": "timeMin",
    "name": "tmin",
    "fieldName": "<field_name, typically column specified in timestamp spec>"
}

----------------------------------------

TITLE: Configuring DoubleFirst Aggregator in Druid JSON
DESCRIPTION: Defines a doubleFirst aggregator to compute the metric value with the minimum timestamp or 0 if no row exists. Used only in queries, not ingestion.

LANGUAGE: JSON
CODE:
{
  "type" : "doubleFirst",
  "name" : <output_name>,
  "fieldName" : <metric_name>
}

----------------------------------------

TITLE: Configuring TimeMin Aggregator for Ingestion in Druid
DESCRIPTION: JSON configuration for including a timeMin aggregator during data ingestion in Druid. This aggregator calculates the minimum timestamp for a specified field.

LANGUAGE: json
CODE:
{
    "type": "timeMin",
    "name": "tmin",
    "fieldName": "<field_name, typically column specified in timestamp spec>"
}

----------------------------------------

TITLE: Submitting Hadoop Batch Ingestion Task to Druid
DESCRIPTION: Command to submit the Hadoop batch ingestion task for loading Wikipedia data into Druid.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/wikipedia-index-hadoop.json

----------------------------------------

TITLE: Local Deep Storage Configuration Example
DESCRIPTION: Sample configuration for local deep storage setup in Druid

LANGUAGE: properties
CODE:
druid.storage.type=local\ndruid.storage.storageDirectory=/druid/segments

----------------------------------------

TITLE: Configuring FloatFirst Aggregator in Druid JSON
DESCRIPTION: Defines a floatFirst aggregator to compute the metric value with the minimum timestamp or 0 if no row exists. Used only in queries, not ingestion.

LANGUAGE: JSON
CODE:
{
  "type" : "floatFirst",
  "name" : <output_name>,
  "fieldName" : <metric_name>
}

----------------------------------------

TITLE: Configuring FloatFirst Aggregator in Druid JSON
DESCRIPTION: Defines a floatFirst aggregator to compute the metric value with the minimum timestamp or 0 if no row exists. Used only in queries, not ingestion.

LANGUAGE: JSON
CODE:
{
  "type" : "floatFirst",
  "name" : <output_name>,
  "fieldName" : <metric_name>
}

----------------------------------------

TITLE: Configuring Extraction DimensionSpec in Apache Druid JSON
DESCRIPTION: Demonstrates how to set up an Extraction DimensionSpec, which transforms dimension values using a specified extraction function. It includes options for dimension, output name, output type, and the extraction function to be applied.

LANGUAGE: JSON
CODE:
{
  "type" : "extraction",
  "dimension" : <dimension>,
  "outputName" :  <output_name>,
  "outputType": <"STRING"|"LONG"|"FLOAT">,
  "extractionFn" : <extraction_function>
}

----------------------------------------

TITLE: Worker Enable Status JSON Response
DESCRIPTION: Example JSON response when enabling a MiddleManager worker

LANGUAGE: json
CODE:
{"localhost:8091":"enabled"}

----------------------------------------

TITLE: Configuring Query Context for Scan Query in Apache Druid
DESCRIPTION: Example of a query context JSON object for Scan queries. It specifies the maximum number of rows queued for ordering and the maximum number of segment partitions ordered in memory.

LANGUAGE: json
CODE:
{
  "maxRowsQueuedForOrdering": 100001,
  "maxSegmentPartitionsOrderedInMemory": 100	
}

----------------------------------------

TITLE: Enabling Grand Totals in Apache Druid Timeseries Query
DESCRIPTION: This snippet shows how to enable the 'grand totals' feature in a timeseries query by adding the 'grandTotal' flag to the query context. This will include an extra row with totals as the last row of the result set.

LANGUAGE: json
CODE:
{
  "queryType": "timeseries",
  "dataSource": "sample_datasource",
  "intervals": [ "2012-01-01T00:00:00.000/2012-01-03T00:00:00.000" ],
  "granularity": "day",
  "aggregations": [
    { "type": "longSum", "name": "sample_name1", "fieldName": "sample_fieldName1" },
    { "type": "doubleSum", "name": "sample_name2", "fieldName": "sample_fieldName2" }
  ],
  "context": {
    "grandTotal": true
  }
}

----------------------------------------

TITLE: Configuring Forever Drop Rule in Druid
DESCRIPTION: JSON configuration for a Forever Drop Rule that permanently removes matching segments from the cluster.

LANGUAGE: json
CODE:
{
  "type" : "dropForever"  
}

----------------------------------------

TITLE: Configuring Local Firehose in Druid
DESCRIPTION: Configuration for reading data from local disk files. Used for POCs and supports parallel index tasks where each worker reads a file.

LANGUAGE: json
CODE:
{
    "type"    : "local",
    "filter"   : "*.csv",
    "baseDir"  : "/data/directory"
}

----------------------------------------

TITLE: Configuring Interval Drop Rule in Apache Druid
DESCRIPTION: JSON configuration for an interval drop rule that indicates segments within a specific time interval should be dropped from the cluster.

LANGUAGE: json
CODE:
{
  "type" : "dropByInterval",
  "interval" : "2012-01-01/2013-01-01"
}

----------------------------------------

TITLE: Submitting Druid Kafka Supervisor Spec
DESCRIPTION: cURL command to submit a Kafka supervisor specification to Druid's overlord, enabling Kafka ingestion.

LANGUAGE: bash
CODE:
curl -XPOST -H'Content-Type: application/json' -d @quickstart/tutorial/wikipedia-kafka-supervisor.json http://localhost:8090/druid/indexer/v1/supervisor

----------------------------------------

TITLE: Configuring Cardinality Aggregator in Druid
DESCRIPTION: JSON configuration for the cardinality aggregator that computes the cardinality of Druid dimensions using HyperLogLog. Supports single or multiple dimensions and can compute cardinality by value or by row.

LANGUAGE: json
CODE:
{
  "type": "cardinality",
  "name": "<output_name>",
  "fields": [ <dimension1>, <dimension2>, ... ],
  "byRow": <false | true> # (optional, defaults to false),
  "round": <false | true> # (optional, defaults to false)
}

----------------------------------------

TITLE: Basic Select Query Structure in Druid
DESCRIPTION: Example of a basic select query to retrieve raw rows from Druid with pagination. The query includes essential parameters like dataSource, time intervals, and paging specification.

LANGUAGE: json
CODE:
{
   "queryType": "select",
   "dataSource": "wikipedia",
   "descending": "false",
   "dimensions":[],
   "metrics":[],
   "granularity": "all",
   "intervals": [
     "2013-01-01/2013-01-02"
   ],
   "pagingSpec":{"pagingIdentifiers": {}, "threshold":5}
 }

----------------------------------------

TITLE: Configuring Cardinality Aggregator in Druid
DESCRIPTION: JSON configuration for the cardinality aggregator that computes the cardinality of Druid dimensions using HyperLogLog. Supports single or multiple dimensions and can compute cardinality by value or by row.

LANGUAGE: json
CODE:
{
  "type": "cardinality",
  "name": "<output_name>",
  "fields": [ <dimension1>, <dimension2>, ... ],
  "byRow": <false | true> # (optional, defaults to false),
  "round": <false | true> # (optional, defaults to false)
}

----------------------------------------

TITLE: Configuring Asynchronous Logging in Druid using Log4j2
DESCRIPTION: XML configuration for Log4j2 that implements asynchronous logging for chatty Druid classes to improve logging performance. Sets up console appender with ISO8601 timestamp pattern and configures specific loggers for various Druid components.

LANGUAGE: xml
CODE:
<?xml version="1.0" encoding="UTF-8" ?>
<Configuration status="WARN">
  <Appenders>
    <Console name="Console" target="SYSTEM_OUT">
      <PatternLayout pattern="%d{ISO8601} %p [%t] %c - %m%n"/>
    </Console>
  </Appenders>
  <Loggers>
    <AsyncLogger name="org.apache.druid.curator.inventory.CuratorInventoryManager" level="debug" additivity="false">
      <AppenderRef ref="Console"/>
    </AsyncLogger>
    <AsyncLogger name="org.apache.druid.client.BatchServerInventoryView" level="debug" additivity="false">
      <AppenderRef ref="Console"/>
    </AsyncLogger>
    <!-- Make extra sure nobody adds logs in a bad way that can hurt performance -->
    <AsyncLogger name="org.apache.druid.client.ServerInventoryView" level="debug" additivity="false">
      <AppenderRef ref="Console"/>
    </AsyncLogger>
    <AsyncLogger name ="org.apache.druid.java.util.http.client.pool.ChannelResourceFactory" level="info" additivity="false">
      <AppenderRef ref="Console"/>
    </AsyncLogger>
    <Root level="info">
      <AppenderRef ref="Console"/>
    </Root>
  </Loggers>
</Configuration>

----------------------------------------

TITLE: Configuring JavaScript-based Data Ingestion in Apache Druid
DESCRIPTION: This snippet shows the parseSpec configuration for ingesting data using a custom JavaScript function in Druid. It allows for complex parsing logic to be applied to the input data.

LANGUAGE: json
CODE:
{
  "parseSpec":{
    "format" : "javascript",
    "timestampSpec" : {
      "column" : "timestamp"
    },        
    "dimensionsSpec" : {
      "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"]
    },
    "function" : "function(str) { var parts = str.split(\"-\"); return { one: parts[0], two: parts[1] } }"
  }
}

----------------------------------------

TITLE: Configuring RegexFiltered DimensionSpec in Druid
DESCRIPTION: Retains only the values matching the specified regex pattern in multi-value dimensions.

LANGUAGE: json
CODE:
{ "type" : "regexFiltered", "delegate" : <dimensionSpec>, "pattern": <java regex pattern> }

----------------------------------------

TITLE: Sample JSON Output for Bitmap Dump
DESCRIPTION: This snippet illustrates the JSON output format when dumping bitmap indexes from a segment. It includes the bitmap serialization factory type and base64-encoded bitmap data.

LANGUAGE: json
CODE:
{
  "bitmapSerdeFactory": {
    "type": "concise"
  },
  "bitmaps": {
    "isRobot": {
      "false": "//aExfu+Nv3X...",
      "true": "gAl7OoRByQ..."
    }
  }
}

----------------------------------------

TITLE: Curl Command for Querying Druid Broker with Kerberos
DESCRIPTION: Specific example of sending a query to Druid Broker using SPNEGO authentication

LANGUAGE: bash
CODE:
curl --negotiate -u:anyUser -b ~/cookies.txt -c ~/cookies.txt -X POST -H'Content-Type: application/json'  http://broker-host:port/druid/v2/?pretty -d @query.json

----------------------------------------

TITLE: Configuring Real-time Thrift Ingestion in Druid
DESCRIPTION: JSON configuration for real-time ingestion of Thrift data using Tranquility. Specifies the data schema, parser type, and Thrift class.

LANGUAGE: json
CODE:
{
  "dataSources": [{
    "spec": {
      "dataSchema": {
        "dataSource": "book",
        "granularitySpec": {          },
        "parser": {
          "type": "thrift",
          "thriftClass": "org.apache.druid.data.input.thrift.Book",
          "protocol": "compact",
          "parseSpec": {
            "format": "json",
            ...
          }
        },
        "metricsSpec": [...]
      },
      "tuningConfig": {...}
    },
    "properties": {...}
  }],
  "properties": {...}
}

----------------------------------------

TITLE: Defining Query Datasource in Druid
DESCRIPTION: Illustrates the structure of a query datasource used for nested groupBy queries. This type is specifically designed for use with groupBy queries only.

LANGUAGE: json
CODE:
{
	"type": "query",
	"query": {
		"type": "groupBy",
		...
	}
}

----------------------------------------

TITLE: Set Operations Post-Aggregator
DESCRIPTION: Post-aggregator configuration for performing set operations (union, intersection, difference) on ArrayOfDoublesSketch instances.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchSetOp",
  "name": <output name>,
  "operation": <"UNION"|"INTERSECT"|"NOT">,
  "fields"  : <array of post aggregators>,
  "nominalEntries" : <accuracy parameter>,
  "numberOfValues" : <values per key>
}

----------------------------------------

TITLE: Derby Import SQL Commands
DESCRIPTION: SQL commands for importing exported metadata into Derby database

LANGUAGE: sql
CODE:
CALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_SEGMENTS','/tmp/csv/druid_segments.csv',',','"',null,0);\n\nCALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_RULES','/tmp/csv/druid_rules.csv',',','"',null,0);\n\nCALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_CONFIG','/tmp/csv/druid_config.csv',',','"',null,0);\n\nCALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_DATASOURCE','/tmp/csv/druid_dataSource.csv',',','"',null,0);\n\nCALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_SUPERVISORS','/tmp/csv/druid_supervisors.csv',',','"',null,0);

----------------------------------------

TITLE: Configuring InfluxDB Parser in Druid
DESCRIPTION: JSON configuration for setting up the InfluxDB Line Protocol parser in Druid, including timestamp specification, dimension exclusions, and measurement whitelist.

LANGUAGE: json
CODE:
{
      "type": "string",
      "parseSpec": {
        "format": "influx",
        "timestampSpec": {
          "column": "__ts",
          "format": "millis"
        },
        "dimensionsSpec": {
          "dimensionExclusions": [
            "__ts"
          ]
        },
        "whitelistMeasurements": [
          "cpu"
        ]
      }
}

----------------------------------------

TITLE: Loading DataSketches Extension Configuration in Druid
DESCRIPTION: Configuration snippet showing how to include the DataSketches extension in Druid's config file.

LANGUAGE: json
CODE:
druid.extensions.loadList=["druid-datasketches"]

----------------------------------------

TITLE: Configuring Kafka Simple Consumer Firehose in Apache Druid
DESCRIPTION: This JSON configuration snippet demonstrates how to set up a Kafka Simple Consumer firehose in Apache Druid. It includes essential properties such as broker list, queue buffer length, offset reset behavior, partition IDs, client ID, and Kafka topic.

LANGUAGE: json
CODE:
{
  "firehoseV2": {
    "type" : "kafka-0.8-v2",
    "brokerList" :  ["localhost:4443"],
    "queueBufferLength":10001,
    "resetOffsetToEarliest":"true",
    "partitionIdList" : ["0"],
    "clientId" : "localclient",
    "feed": "wikipedia"
  }
}

----------------------------------------

TITLE: Configuring PrefixFiltered DimensionSpec in Apache Druid JSON
DESCRIPTION: Demonstrates how to set up a PrefixFiltered DimensionSpec, which retains only the values starting with a specified prefix in multi-value dimensions.

LANGUAGE: JSON
CODE:
{ "type" : "prefixFiltered", "delegate" : <dimensionSpec>, "prefix": <prefix string> }

----------------------------------------

TITLE: Loading Data into Apache Druid using Post-Index-Task
DESCRIPTION: This command uses the post-index-task script to load data into Druid based on the specified ingestion task JSON file.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/rollup-index.json

----------------------------------------

TITLE: Default DimensionSpec Configuration in Druid
DESCRIPTION: Configuration for default DimensionSpec that returns dimension values as-is with optional renaming and type conversion.

LANGUAGE: json
CODE:
{
  "type" : "default",
  "dimension" : <dimension>,
  "outputName": <output_name>,
  "outputType": <"STRING"|"LONG"|"FLOAT">
}

----------------------------------------

TITLE: Configuring Druid SQLServer Connection Properties
DESCRIPTION: Essential configuration parameters for connecting Apache Druid to Microsoft SQLServer metadata storage. Includes storage type specification, connection URI, and authentication credentials.

LANGUAGE: properties
CODE:
druid.metadata.storage.type=sqlserver
druid.metadata.storage.connector.connectURI=jdbc:sqlserver://<host>;databaseName=druid
druid.metadata.storage.connector.user=druid
druid.metadata.storage.connector.password=diurd

----------------------------------------

TITLE: Segment Metadata JSON Structure
DESCRIPTION: Example JSON structure showing the format of segment metadata stored in the segments table payload column. Contains information about data source, interval, version and load specifications.

LANGUAGE: json
CODE:
{
 "dataSource":"wikipedia",
 "interval":"2012-05-23T00:00:00.000Z/2012-05-24T00:00:00.000Z",
 "version":"2012-05-24T00:10:00.046Z",
 "loadSpec":{
    "type":"s3_zip",
    "bucket":"bucket_for_segment",
    "key":"path/to/segment/on/s3"
 },
 "dimensions":"comma-delimited-list-of-dimension-names",
 "metrics":"comma-delimited-list-of-metric-names",
 "shardSpec":{"type":"none"},
 "binaryVersion":9,
 "size":size_of_segment,
 "identifier":"wikipedia_2012-05-23T00:00:00.000Z_2012-05-24T00:00:00.000Z_2012-05-23T00:10:00.046Z"
}

----------------------------------------

TITLE: Configuring Regex ParseSpec in Druid
DESCRIPTION: Example configuration for ingesting data using a regular expression parser in Druid. Specifies the timestamp column, dimensions, input columns, and regex pattern for parsing.

LANGUAGE: json
CODE:
{
  "parseSpec":{
    "format" : "regex",
    "timestampSpec" : {
      "column" : "timestamp"
    },        
    "dimensionsSpec" : {
      "dimensions" : [<your_list_of_dimensions>]
    },
    "columns" : [<your_columns_here>],
    "pattern" : <regex pattern for partitioning data>
  }
}

----------------------------------------

TITLE: Creating Authorizer Configuration
DESCRIPTION: Configuration properties for setting up a Basic authorizer in Druid.

LANGUAGE: properties
CODE:
druid.auth.authorizers=["MyBasicAuthorizer"]

druid.auth.authorizer.MyBasicAuthorizer.type=basic

----------------------------------------

TITLE: Configuring Append Task in Druid
DESCRIPTION: Deprecated task that combines multiple segments sequentially into a single segment. Includes parameters for task identification, data source specification, segment list, and optional aggregations.

LANGUAGE: json
CODE:
{
    "type": "append",
    "id": <task_id>,
    "dataSource": <task_datasource>,
    "segments": <JSON list of DataSegment objects to append>,
    "aggregations": <optional list of aggregators>,
    "context": <task context>
}

----------------------------------------

TITLE: Querying for Ingested Event Count in Druid
DESCRIPTION: Example of how to query for the number of ingested events using a longSum aggregator in Druid.

LANGUAGE: json
CODE:
"aggregations": [
    { "type": "longSum", "name": "numIngestedEvents", "fieldName": "count" }
]

----------------------------------------

TITLE: Configuring Client Certificate Authentication in Apache Druid
DESCRIPTION: These properties control client certificate authentication settings in Apache Druid. They include options for requiring client certificates, configuring the trust store, and validating hostnames.

LANGUAGE: properties
CODE:
druid.server.https.requireClientCertificate=false
druid.server.https.trustStoreType=
druid.server.https.trustStorePath=
druid.server.https.trustStoreAlgorithm=
druid.server.https.trustStorePassword=
druid.server.https.validateHostnames=true
druid.server.https.crlPath=

----------------------------------------

TITLE: Implementing a Regular Expression Filter in Druid
DESCRIPTION: This snippet illustrates how to create a regular expression filter in Druid. It matches a specified dimension with a given pattern using Java regular expressions.

LANGUAGE: json
CODE:
"filter": { "type": "regex", "dimension": <dimension_string>, "pattern": <pattern_string> }

----------------------------------------

TITLE: Implementing a Regular Expression Filter in Druid
DESCRIPTION: This snippet illustrates how to create a regular expression filter in Druid. It matches a specified dimension with a given pattern using Java regular expressions.

LANGUAGE: json
CODE:
"filter": { "type": "regex", "dimension": <dimension_string>, "pattern": <pattern_string> }

----------------------------------------

TITLE: Configuring HLLSketchToString Post-Aggregator in Druid
DESCRIPTION: This JSON configuration defines an HLLSketchToString post-aggregator for generating a human-readable sketch summary for debugging purposes.

LANGUAGE: json
CODE:
{
  "type"  : "HLLSketchToString",
  "name": <output name>,
  "field"  : <post aggregator that returns an HLL Sketch>
}

----------------------------------------

TITLE: Configuring Interval Broadcast Rule in Druid
DESCRIPTION: JSON configuration for an Interval Broadcast Rule that co-locates segments for a specific time interval.

LANGUAGE: json
CODE:
{
  "type" : "broadcastByInterval",
  "colocatedDataSources" : [ "target_source1", "target_source2" ],
  "interval" : "2012-01-01/2013-01-01"
}

----------------------------------------

TITLE: Timeseries Query with Skip Empty Buckets
DESCRIPTION: Example demonstrating how to skip empty time buckets in the results by setting the skipEmptyBuckets context parameter. This eliminates zero-filled results for time periods with no data.

LANGUAGE: json
CODE:
{
  "queryType": "timeseries",
  "dataSource": "sample_datasource",
  "granularity": "day",
  "aggregations": [
    { "type": "longSum", "name": "sample_name1", "fieldName": "sample_fieldName1" }
  ],
  "intervals": [ "2012-01-01T00:00:00.000/2012-01-04T00:00:00.000" ],
  "context" : {
    "skipEmptyBuckets": "true"
  }
}

----------------------------------------

TITLE: Streaming Data to Kafka Topic
DESCRIPTION: Commands to set up and run a Kafka console producer, streaming sample data to the 'wikipedia' topic.

LANGUAGE: bash
CODE:
export KAFKA_OPTS="-Dfile.encoding=UTF-8"
./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic wikipedia < {PATH_TO_DRUID}/quickstart/tutorial/wikiticker-2015-09-12-sampled.json

----------------------------------------

TITLE: Configuring Same Interval Merge Task in Apache Druid (Deprecated)
DESCRIPTION: Defines the JSON structure for a Same Interval Merge task in Druid, which merges all segments within a specified interval. It requires task ID, data source, aggregations, rollup option, interval, and task context.

LANGUAGE: json
CODE:
{
    "type": "same_interval_merge",
    "id": <task_id>,
    "dataSource": <task_datasource>,
    "aggregations": <list of aggregators>,
    "rollup": <whether or not to rollup data during a merge>,
    "interval": <DataSegment objects in this interval are going to be merged>,
    "context": <task context>
}

----------------------------------------

TITLE: Configuring Forever Broadcast Rule in Apache Druid
DESCRIPTION: This JSON snippet defines a forever broadcast rule in Apache Druid. It specifies how segments of different data sources should be co-located in Historical processes.

LANGUAGE: json
CODE:
{
  "type" : "broadcastForever",
  "colocatedDataSources" : [ "target_source1", "target_source2" ]
}

----------------------------------------

TITLE: Accessing Coordinator Console URL Format
DESCRIPTION: URL format for accessing the version 2 Coordinator Console interface.

LANGUAGE: plaintext
CODE:
http://<COORDINATOR_IP>:<COORDINATOR_PORT>

----------------------------------------

TITLE: Theta Sketch Aggregator Configuration
DESCRIPTION: JSON configuration for the thetaSketch aggregator defining output name, field name, input type and size parameters.

LANGUAGE: json
CODE:
{
  "type" : "thetaSketch",
  "name" : <output_name>,
  "fieldName" : <metric_name>,  
  "isInputThetaSketch": false,
  "size": 16384
 }

----------------------------------------

TITLE: SQL Query for Aggregating Segment Statistics in Druid
DESCRIPTION: Example SQL query to retrieve aggregated statistics about segments grouped by datasource.

LANGUAGE: SQL
CODE:
SELECT
    datasource,
    SUM("size") AS total_size,
    CASE WHEN SUM("size") = 0 THEN 0 ELSE SUM("size") / (COUNT(*) FILTER(WHERE "size" > 0)) END AS avg_size,
    CASE WHEN SUM(num_rows) = 0 THEN 0 ELSE SUM("num_rows") / (COUNT(*) FILTER(WHERE num_rows > 0)) END AS avg_num_rows,
    COUNT(*) AS num_segments
FROM sys.segments
GROUP BY 1
ORDER BY 2 DESC

----------------------------------------

TITLE: Applying Logical AND Filter in groupBy Query (JSON)
DESCRIPTION: Shows how to use a logical AND filter in the having clause of a groupBy query, combining multiple conditions.

LANGUAGE: json
CODE:
{
    "queryType": "groupBy",
    "dataSource": "sample_datasource",
    ...
    "having": 
        {
            "type": "and",
            "havingSpecs": [        
                {
                    "type": "greaterThan",
                    "aggregation": "<aggregate_metric>",
                    "value": <numeric_value>
                },
                {
                    "type": "lessThan",
                    "aggregation": "<aggregate_metric>",
                    "value": <numeric_value>
                }
            ]
        }
}

----------------------------------------

TITLE: Configuring Druid Basic Authenticator
DESCRIPTION: Configuration properties for setting up basic authentication in Druid with an initial admin user and internal client.

LANGUAGE: properties
CODE:
druid.auth.authenticatorChain=["MyBasicAuthenticator"]

druid.auth.authenticator.MyBasicAuthenticator.type=basic
druid.auth.authenticator.MyBasicAuthenticator.initialAdminPassword=password1
druid.auth.authenticator.MyBasicAuthenticator.initialInternalClientPassword=password2
druid.auth.authenticator.MyBasicAuthenticator.authorizerName=MyBasicAuthorizer

----------------------------------------

TITLE: Disabling Zero-filling in Apache Druid Timeseries Query
DESCRIPTION: This snippet demonstrates how to disable zero-filling for empty time buckets in a timeseries query by setting the 'skipEmptyBuckets' flag in the query context. This will omit data points for time periods with no data.

LANGUAGE: json
CODE:
{
  "queryType": "timeseries",
  "dataSource": "sample_datasource",
  "granularity": "day",
  "aggregations": [
    { "type": "longSum", "name": "sample_name1", "fieldName": "sample_fieldName1" }
  ],
  "intervals": [ "2012-01-01T00:00:00.000/2012-01-04T00:00:00.000" ],
  "context" : {
    "skipEmptyBuckets": "true"
  }
}

----------------------------------------

TITLE: SBT Assembly Plugin Configuration
DESCRIPTION: Configuration for SBT assembly plugin used to build fat jars for resolving dependency conflicts.

LANGUAGE: scala
CODE:
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.13.0")

----------------------------------------

TITLE: Starting Druid Coordinator and Overlord
DESCRIPTION: Commands to start the Druid Coordinator and Overlord processes on the Master server.

LANGUAGE: bash
CODE:
java `cat conf/druid/coordinator/jvm.config | xargs` -cp conf/druid/_common:conf/druid/coordinator:lib/* org.apache.druid.cli.Main server coordinator
java `cat conf/druid/overlord/jvm.config | xargs` -cp conf/druid/_common:conf/druid/overlord:lib/* org.apache.druid.cli.Main server overlord

----------------------------------------

TITLE: Starting Druid Coordinator and Overlord
DESCRIPTION: Commands to start the Druid Coordinator and Overlord processes on the Master server.

LANGUAGE: bash
CODE:
java `cat conf/druid/coordinator/jvm.config | xargs` -cp conf/druid/_common:conf/druid/coordinator:lib/* org.apache.druid.cli.Main server coordinator
java `cat conf/druid/overlord/jvm.config | xargs` -cp conf/druid/_common:conf/druid/overlord:lib/* org.apache.druid.cli.Main server overlord

----------------------------------------

TITLE: Starting Druid Master Server Components
DESCRIPTION: Commands to start the Coordinator and Overlord services on the master server

LANGUAGE: bash
CODE:
java `cat conf/druid/coordinator/jvm.config | xargs` -cp conf/druid/_common:conf/druid/coordinator:lib/* org.apache.druid.cli.Main server coordinator
java `cat conf/druid/overlord/jvm.config | xargs` -cp conf/druid/_common:conf/druid/overlord:lib/* org.apache.druid.cli.Main server overlord

----------------------------------------

TITLE: Displaying Segment Naming Convention in Druid
DESCRIPTION: This snippet demonstrates the naming convention for Druid segments, showing how datasource, interval, version, and partition number are represented in segment names.

LANGUAGE: plaintext
CODE:
foo_2015-01-01/2015-01-02_v1_0
foo_2015-01-01/2015-01-02_v1_1
foo_2015-01-01/2015-01-02_v1_2

----------------------------------------

TITLE: Running the Druid Coordinator Process
DESCRIPTION: Command to start the Druid Coordinator process using the Main class.

LANGUAGE: java
CODE:
org.apache.druid.cli.Main server coordinator

----------------------------------------

TITLE: Configuring JavaScript Router Strategy in JSON
DESCRIPTION: JSON configuration for a custom JavaScript router strategy that routes queries with 3 or more aggregators to the lowest priority Broker.

LANGUAGE: json
CODE:
{
  "type" : "javascript",
  "function" : "function (config, query) { if (query.getAggregatorSpecs && query.getAggregatorSpecs().size() >= 3) { var size = config.getTierToBrokerMap().values().size(); if (size > 0) { return config.getTierToBrokerMap().values().toArray()[size-1] } else { return config.getDefaultBrokerServiceName() } } else { return null } }"
}

----------------------------------------

TITLE: MIT License Notice for object-assign
DESCRIPTION: Copyright notice for the object-assign library by Sindre Sorhus, licensed under MIT.

LANGUAGE: JavaScript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

----------------------------------------

TITLE: Logical AND Having Specification in Druid
DESCRIPTION: Shows how to combine multiple having conditions using the AND logical operator.

LANGUAGE: json
CODE:
{
    "queryType": "groupBy",
    "dataSource": "sample_datasource",
    ...
    "having": 
        {
            "type": "and",
            "havingSpecs": [        
                {
                    "type": "greaterThan",
                    "aggregation": "<aggregate_metric>",
                    "value": <numeric_value>
                },
                {
                    "type": "lessThan",
                    "aggregation": "<aggregate_metric>",
                    "value": <numeric_value>
                }
            ]
        }
}

----------------------------------------

TITLE: Role Permissions JSON Example
DESCRIPTION: Example JSON structure for defining role permissions with resource access patterns and actions.

LANGUAGE: json
CODE:
[
{
  "resource": {
    "name": "wiki.*",
    "type": "DATASOURCE"
  },
  "action": "READ"
},
{
  "resource": {
    "name": "wikiticker",
    "type": "DATASOURCE"
  },
  "action": "WRITE"
}
]

----------------------------------------

TITLE: Configuring Graphite Emitter with Send-All Converter in JSON
DESCRIPTION: JSON configuration for the Graphite Emitter using the 'all' event converter. This setup sends all Druid service metrics events to Graphite, with options to ignore hostname and service name in the metric path.

LANGUAGE: json
CODE:
{
"druid.emitter.graphite.eventConverter": {
  "type": "all",
  "namespacePrefix": "druid.test",
  "ignoreHostname": true,
  "ignoreServiceName": true
}
}

----------------------------------------

TITLE: Hour Granularity GroupBy Query in Druid
DESCRIPTION: Example of a groupBy query using hourly granularity to aggregate data.

LANGUAGE: json
CODE:
{
   "queryType":"groupBy",
   "dataSource":"my_dataSource",
   "granularity":"hour",
   "dimensions":[
      "language"
   ],
   "aggregations":[
      {
         "type":"count",
         "name":"count"
      }
   ],
   "intervals":[
      "2000-01-01T00:00Z/3000-01-01T00:00Z"
   ]
}

----------------------------------------

TITLE: Configuring Field Access Post-Aggregators in Druid
DESCRIPTION: Demonstrates field accessor post-aggregators that return values from specified aggregators, with options for raw or finalized access.

LANGUAGE: json
CODE:
{ "type" : "fieldAccess", "name": <output_name>, "fieldName" : <aggregator_name> }

LANGUAGE: json
CODE:
{ "type" : "finalizingFieldAccess", "name": <output_name>, "fieldName" : <aggregator_name> }

----------------------------------------

TITLE: Configuring Graphite Emitter with Send-All Converter in JSON
DESCRIPTION: JSON configuration for the Graphite Emitter using the 'all' event converter. This setup sends all Druid service metrics events to Graphite, with options to ignore hostname and service name in the metric path.

LANGUAGE: json
CODE:
{
"druid.emitter.graphite.eventConverter": {
  "type": "all",
  "namespacePrefix": "druid.test",
  "ignoreHostname": true,
  "ignoreServiceName": true
}
}

----------------------------------------

TITLE: Configuring HLLSketchToString Post-Aggregator in Druid
DESCRIPTION: This JSON configuration defines an HLLSketchToString post-aggregator for generating a human-readable summary of an HLL sketch for debugging purposes in Druid queries.

LANGUAGE: json
CODE:
{
  "type"  : "HLLSketchToString",
  "name": <output name>,
  "field"  : <post aggregator that returns an HLL Sketch>
}

----------------------------------------

TITLE: List Filtered DimensionSpec Configuration in Druid
DESCRIPTION: Configuration for filtering multi-value dimensions using whitelist/blacklist functionality.

LANGUAGE: json
CODE:
{ "type" : "listFiltered", "delegate" : <dimensionSpec>, "values": <array of strings>, "isWhitelist": <optional attribute for true/false, default is true> }

----------------------------------------

TITLE: Quantiles Post Aggregator Configuration
DESCRIPTION: JSON configuration for the quantiles post aggregator that returns an array of quantiles for given fractions.

LANGUAGE: json
CODE:
{
  "type"  : "quantilesDoublesSketchToQuantiles",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch>,
  "fractions" : <array of fractional positions>
}

----------------------------------------

TITLE: Segment Load Queue Path in ZooKeeper
DESCRIPTION: Path where Coordinator writes instructions for Historical processes to load or drop segments.

LANGUAGE: plaintext
CODE:
${druid.zk.paths.loadQueuePath}/_host_of_historical_process/_segment_identifier

----------------------------------------

TITLE: Defining ZooKeeper Path for Process Announcements in Druid
DESCRIPTION: Specifies the ZooKeeper path where Historical and Realtime processes in Druid create ephemeral znodes to announce their existence.

LANGUAGE: plaintext
CODE:
${druid.zk.paths.announcementsPath}/${druid.host}

----------------------------------------

TITLE: Implementing Post-Aggregation Operations in Druid
DESCRIPTION: Collection of JSON configurations for various histogram post-aggregation operations including equal buckets, custom buckets, min/max, and quantile calculations.

LANGUAGE: json
CODE:
{
  "type": "equalBuckets",
  "name": "<output_name>",
  "fieldName": "<aggregator_name>",
  "numBuckets": <count>
}

LANGUAGE: json
CODE:
{
  "type": "buckets",
  "name": "<output_name>",
  "fieldName": "<aggregator_name>",
  "bucketSize": <bucket_size>,
  "offset": <offset>
}

LANGUAGE: json
CODE:
{
  "type": "customBuckets",
  "name": <output_name>,
  "fieldName": <aggregator_name>,
  "breaks": [<value>, <value>, ...]
}

----------------------------------------

TITLE: Querying Segment Statistics in Apache Druid using SQL
DESCRIPTION: This SQL query retrieves statistics about published segments in a Druid datasource, including average number of rows, average size, and total size. It helps identify if segment optimization is needed.

LANGUAGE: sql
CODE:
SELECT
  "start",
  "end",
  version,
  COUNT(*) AS num_segments,
  AVG("num_rows") AS avg_num_rows,
  SUM("num_rows") AS total_num_rows,
  AVG("size") AS avg_size,
  SUM("size") AS total_size
FROM
  sys.segments A
WHERE
  datasource = 'your_dataSource' AND
  is_published = 1
GROUP BY 1, 2, 3
ORDER BY 1, 2, 3 DESC;

----------------------------------------

TITLE: Defining Theta Sketch Aggregator in Druid
DESCRIPTION: JSON configuration for the Theta Sketch aggregator, including properties like type, name, fieldName, isInputThetaSketch, and size.

LANGUAGE: json
CODE:
{
  "type" : "thetaSketch",
  "name" : <output_name>,
  "fieldName" : <metric_name>,  
  "isInputThetaSketch": false,
  "size": 16384
 }

----------------------------------------

TITLE: Configuring String Format Extraction Function in Apache Druid
DESCRIPTION: JSON configuration for a String Format Extraction Function, which formats dimension values according to a given format string.

LANGUAGE: JSON
CODE:
{ "type" : "stringFormat", "format" : <sprintf_expression>, "nullHandling" : <optional attribute for handling null value> }

----------------------------------------

TITLE: Configuring Anonymous Authenticator in Apache Druid
DESCRIPTION: This JSON snippet demonstrates how to configure the Anonymous Authenticator with the druid-basic-security extension in Apache Druid. It includes settings for the authenticator chain, anonymous authenticator type, identity, and authorizer name.

LANGUAGE: json
CODE:
"druid.auth.authenticatorChain":["basic", "anonymous"]

"druid.auth.authenticator.anonymous.type":"anonymous"
"druid.auth.authenticator.anonymous.identity":"defaultUser"
"druid.auth.authenticator.anonymous.authorizerName":"myBasicAuthorizer"

----------------------------------------

TITLE: Example Query Using zscore2sample and pvalue2tailedZtest in Druid
DESCRIPTION: This example demonstrates a complete JSON query using both zscore2sample and pvalue2tailedZtest post aggregators. It calculates the z-score using sample data and then uses that z-score to compute the p-value.

LANGUAGE: json
CODE:
{
  ...
    "postAggregations" : {
    "type"   : "pvalue2tailedZtest",
    "name"   : "pvalue",
    "zScore" : 
    {
     "type"   : "zscore2sample",
     "name"   : "zscore",
     "successCount1" :
       { "type"   : "constant",
         "name"   : "successCountFromPopulation1Sample",
         "value"  : 300
       },
     "sample1Size" :
       { "type"   : "constant",
         "name"   : "sampleSizeOfPopulation1",
         "value"  : 500
       },
     "successCount2":
       { "type"   : "constant",
         "name"   : "successCountFromPopulation2Sample",
         "value"  : 450
       },
     "sample2Size" :
       { "type"   : "constant",
         "name"   : "sampleSizeOfPopulation2",
         "value"  : 600
       }
     }
    }
}

----------------------------------------

TITLE: Implementing Kafka Producer for Protobuf Messages in Python
DESCRIPTION: Python script for a Kafka producer that reads JSON input, converts it to Protobuf format, and sends it to a Kafka topic.

LANGUAGE: python
CODE:
#!/usr/bin/env python

import sys
import json

from kafka import KafkaProducer
from metrics_pb2 import Metrics

producer = KafkaProducer(bootstrap_servers='localhost:9092')
topic = 'metrics_pb'
metrics = Metrics()

for row in iter(sys.stdin):
    d = json.loads(row)
    for k, v in d.items():
        setattr(metrics, k, v)
    pb = metrics.SerializeToString()
    producer.send(topic, pb)

----------------------------------------

TITLE: Example Lookup Tier Configuration
DESCRIPTION: Example of a complete lookup tier configuration for a single cached namespace lookup. This configuration sets up a JDBC-based lookup for country codes.

LANGUAGE: json
CODE:
{
  "realtime_customer2": {
    "country_code": {
      "version": "v0",
      "lookupExtractorFactory": {
        "type": "cachedNamespace",
        "extractionNamespace": {
          "type": "jdbc",
          "connectorConfig": {
            "createTables": true,
            "connectURI": "jdbc:mysql:\/\/localhost:3306\/druid",
            "user": "druid",
            "password": "diurd"
          },
          "table": "lookupValues",
          "keyColumn": "value_id",
          "valueColumn": "value_text",
          "filter": "value_type='country'",
          "tsColumn": "timeColumn"
        },
        "firstCacheTimeout": 120000,
        "injective": true
      }
    }
  }
}

----------------------------------------

TITLE: Response Example - Worker Tasks List API
DESCRIPTION: Example JSON response from the MiddleManager worker tasks list endpoint, showing active task IDs.

LANGUAGE: json
CODE:
["index_wikiticker_2019-02-11T02:20:15.316Z"]

----------------------------------------

TITLE: HLL Sketch Estimate Post-Aggregator Configuration
DESCRIPTION: JSON configuration for HLLSketchEstimateWithBounds post-aggregator that provides cardinality estimates with error bounds.

LANGUAGE: json
CODE:
{
  "type"  : "HLLSketchEstimateWithBounds",
  "name": <output name>,
  "field"  : <post aggregator that returns an HLL Sketch>,
  "numStdDev" : <number of standard deviations: 1 (default), 2 or 3>
}

----------------------------------------

TITLE: Configuring Local Index Task in Druid
DESCRIPTION: Example configuration for a local index task that processes Wikipedia data on a single worker node. Shows dataSchema, ioConfig, and tuningConfig settings for local processing.

LANGUAGE: json
CODE:
{
  "type" : "index",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "wikipedia",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "timestampSpec" : {
            "column" : "timestamp",
            "format" : "auto"
          },
          "dimensionsSpec" : {
            "dimensions": ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"],
            "dimensionExclusions" : [],
            "spatialDimensions" : []
          }
        }
      },
      "metricsSpec" : [
        {
          "type" : "count",
          "name" : "count"
        },
        {
          "type" : "doubleSum",
          "name" : "added",
          "fieldName" : "added"
        },
        {
          "type" : "doubleSum",
          "name" : "deleted",
          "fieldName" : "deleted"
        },
        {
          "type" : "doubleSum",
          "name" : "delta",
          "fieldName" : "delta"
        }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "DAY",
        "queryGranularity" : "NONE",
        "intervals" : [ "2013-08-31/2013-09-01" ]
      }
    },
    "ioConfig" : {
      "type" : "index",
      "firehose" : {
        "type" : "local",
        "baseDir" : "examples/indexing/",
        "filter" : "wikipedia_data.json"
       }
    },
    "tuningConfig" : {
      "type" : "index",
      "maxRowsPerSegment" : 5000000,
      "maxRowsInMemory" : 1000000
    }
  }
}

----------------------------------------

TITLE: Querying Segment Statistics in Apache Druid using SQL
DESCRIPTION: This SQL query retrieves statistics about published segments in a Druid datasource, including average number of rows, average size, and total size. It helps identify if segment optimization is needed.

LANGUAGE: sql
CODE:
SELECT
  "start",
  "end",
  version,
  COUNT(*) AS num_segments,
  AVG("num_rows") AS avg_num_rows,
  SUM("num_rows") AS total_num_rows,
  AVG("size") AS avg_size,
  SUM("size") AS total_size
FROM
  sys.segments A
WHERE
  datasource = 'your_dataSource' AND
  is_published = 1
GROUP BY 1, 2, 3
ORDER BY 1, 2, 3 DESC;

----------------------------------------

TITLE: Installing and Starting Tranquility
DESCRIPTION: Commands to download, extract and start Tranquility for stream ingestion into Druid.

LANGUAGE: bash
CODE:
curl -O http://static.druid.io/tranquility/releases/tranquility-distribution-0.8.0.tgz
tar -xzf tranquility-distribution-0.8.0.tgz
cd tranquility-distribution-0.8.0
bin/tranquility <server or kafka> -configFile <path_to_druid_distro>/conf/tranquility/<server or kafka>.json

----------------------------------------

TITLE: Running MiddleManager Process in Apache Druid
DESCRIPTION: Command to start the MiddleManager server process in Apache Druid. This command initializes the MiddleManager, which is responsible for executing submitted tasks and managing Peons.

LANGUAGE: java
CODE:
org.apache.druid.cli.Main server middleManager

----------------------------------------

TITLE: Starting Druid Router Process in Java
DESCRIPTION: Command to start the Druid Router server process.

LANGUAGE: java
CODE:
org.apache.druid.cli.Main server router

----------------------------------------

TITLE: Defining ZooKeeper Path for Coordinator Leader Election in Druid
DESCRIPTION: Specifies the ZooKeeper path used for Coordinator leader election in Druid using the Curator LeadershipLatch recipe.

LANGUAGE: plaintext
CODE:
${druid.zk.paths.coordinatorPath}/_COORDINATOR

----------------------------------------

TITLE: Configuring RegexSearchQuerySpec in Druid JSON
DESCRIPTION: Specifies a search query that uses a regular expression pattern to match dimension values. This provides the most flexible searching capabilities, allowing complex pattern matching.

LANGUAGE: json
CODE:
{
  "type"  : "regex",
  "pattern" : "some_pattern"
}

----------------------------------------

TITLE: Defining Constant Post-Aggregator in Druid JSON Query
DESCRIPTION: Illustrates the syntax for a constant post-aggregator in a Druid query. This post-aggregator always returns the specified numerical value.

LANGUAGE: json
CODE:
{ "type"  : "constant", "name"  : <output_name>, "value" : <numerical_value> }

----------------------------------------

TITLE: Implementing Math Function Examples
DESCRIPTION: Examples of mathematical function usage in Druid expressions for numerical calculations

LANGUAGE: SQL
CODE:
abs(x)
cos(x)
log(x)
pow(x, y)
sqrt(x)

----------------------------------------

TITLE: Running Druid Broker Server
DESCRIPTION: Command to start the Druid Broker server process. This is the main entry point for launching a Broker node in a Druid cluster.

LANGUAGE: java
CODE:
org.apache.druid.cli.Main server broker

----------------------------------------

TITLE: Required Druid Configuration Properties
DESCRIPTION: Essential configuration properties needed for the insert-segment-to-db tool to connect to metadata storage and deep storage.

LANGUAGE: properties
CODE:
druid.metadata.storage.type
druid.metadata.storage.connector.connectURI
druid.metadata.storage.connector.user
druid.metadata.storage.connector.password
druid.storage.type

----------------------------------------

TITLE: Defining Constant Post-Aggregator in Druid JSON Query
DESCRIPTION: Illustrates the syntax for a constant post-aggregator in a Druid query. This post-aggregator always returns the specified numerical value.

LANGUAGE: json
CODE:
{ "type"  : "constant", "name"  : <output_name>, "value" : <numerical_value> }

----------------------------------------

TITLE: Configuring Regex Search Query in Druid
DESCRIPTION: Defines a regex-based search query that matches if any part of a dimension value matches the specified pattern.

LANGUAGE: json
CODE:
{
  "type"  : "regex",
  "pattern" : "some_pattern"
}

----------------------------------------

TITLE: Configuring HLLSketchUnion Post-Aggregator in Druid
DESCRIPTION: This JSON configuration defines the HLLSketchUnion post-aggregator for combining multiple HLL sketches.

LANGUAGE: json
CODE:
{
  "type"  : "HLLSketchUnion",
  "name": <output name>,
  "fields"  : <array of post aggregators that return HLL sketches>,
  "lgK": <log2 of K for the target sketch>,
  "tgtHllType" : <target HLL type>
}

----------------------------------------

TITLE: Extraction DimensionSpec Configuration in Druid
DESCRIPTION: Configuration for Extraction DimensionSpec that transforms dimension values using an extraction function.

LANGUAGE: json
CODE:
{
  "type" : "extraction",
  "dimension" : <dimension>,
  "outputName" :  <output_name>,
  "outputType": <"STRING"|"LONG"|"FLOAT">,
  "extractionFn" : <extraction_function>
}

----------------------------------------

TITLE: Example Druid Query Using Bloom Filter Aggregator
DESCRIPTION: This example demonstrates a complete Druid query using the Bloom Filter aggregator to create a filter based on the 'user' dimension, with a maximum of 100,000 entries.

LANGUAGE: json
CODE:
{
  "queryType": "timeseries",
  "dataSource": "wikiticker",
  "intervals": [ "2015-09-12T00:00:00.000/2015-09-13T00:00:00.000" ],
  "granularity": "day",
  "aggregations": [
    {
      "type": "bloom",
      "name": "userBloom",
      "maxNumEntries": 100000,
      "field": {
        "type":"default",
        "dimension":"user",
        "outputType": "STRING"
      }
    }
  ]
}

----------------------------------------

TITLE: Installing Zookeeper Dependency
DESCRIPTION: Commands to download and setup Apache Zookeeper, which is required for Druid's distributed coordination.

LANGUAGE: bash
CODE:
curl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz
tar -xzf zookeeper-3.4.11.tar.gz
mv zookeeper-3.4.11 zk

----------------------------------------

TITLE: Coordinator Leader Election Path in ZooKeeper
DESCRIPTION: ZooKeeper path used for Coordinator leader election using Curator LeadershipLatch recipe.

LANGUAGE: plaintext
CODE:
${druid.zk.paths.coordinatorPath}/_COORDINATOR

----------------------------------------

TITLE: Setting up FloatFirst Aggregator in Druid
DESCRIPTION: The floatFirst aggregator computes the metric value with the minimum timestamp or 0 if no row exists. It can only be used in queries, not in ingestion specs.

LANGUAGE: json
CODE:
{
  "type" : "floatFirst",
  "name" : <output_name>,
  "fieldName" : <metric_name>
}

----------------------------------------

TITLE: Configuring ContainsSearchQuerySpec in Druid JSON
DESCRIPTION: Defines a case-sensitive search query spec that matches if any part of a dimension value contains the specified value. This spec is useful when case sensitivity is important in the search.

LANGUAGE: json
CODE:
{
  "type"  : "contains",
  "case_sensitive" : true,
  "value" : "some_value"
}

----------------------------------------

TITLE: Running Druid Peon Process Command
DESCRIPTION: Shell command for running a Druid Peon process independently. Takes a task file containing the task JSON and a status file path for output. This is primarily used for development purposes as Peons are typically managed by MiddleManager.

LANGUAGE: shell
CODE:
org.apache.druid.cli.Main internal peon <task_file> <status_file>

----------------------------------------

TITLE: Lunr.js Component Copyright Headers
DESCRIPTION: Collection of copyright notices for the various components of Lunr.js, a lightweight search library inspired by Solr. Includes core functionality like indexing, tokenization, stemming, and text processing utilities.

LANGUAGE: javascript
CODE:
/*!
 * lunr - http://lunrjs.com - A bit like Solr, but much smaller and not as bright - 2.3.9
 * Copyright (C) 2020 Oliver Nightingale
 * @license MIT
 */

----------------------------------------

TITLE: Initializing PostgreSQL Metadata Tables for Druid
DESCRIPTION: This command initializes the metadata tables for Druid in a PostgreSQL database. It uses the metadata-init tool, specifying the PostgreSQL extension and connection details.

LANGUAGE: bash
CODE:
cd ${DRUID_ROOT}
java -classpath "lib/*" -Dlog4j.configurationFile=conf/druid/cluster/_common/log4j2.xml -Ddruid.extensions.directory="extensions" -Ddruid.extensions.loadList=[\"postgresql-metadata-storage\"] -Ddruid.metadata.storage.type=postgresql org.apache.druid.cli.Main tools metadata-init --connectURI="<postgresql-uri>" --user <user> --password <pass> --base druid

----------------------------------------

TITLE: Initiating a Paginated Select Query in Druid
DESCRIPTION: This snippet shows how to initiate a paginated Select query in Druid. It uses a PagingSpec with a threshold of 5 and an empty pagingIdentifiers object. This configuration tells Druid to return results in pages of 5 rows each.

LANGUAGE: json
CODE:
"pagingSpec":{"pagingIdentifiers": {}, "threshold":5}

----------------------------------------

TITLE: Configuring Default DimensionSpec in Apache Druid
DESCRIPTION: JSON configuration for a Default DimensionSpec, which returns dimension values as-is and optionally renames the dimension.

LANGUAGE: JSON
CODE:
{
  "type" : "default",
  "dimension" : <dimension>,
  "outputName": <output_name>,
  "outputType": <"STRING"|"LONG"|"FLOAT">
}

----------------------------------------

TITLE: Registering Jersey Resources in Java
DESCRIPTION: Example of how to register a new Jersey resource in a Druid module.

LANGUAGE: Java
CODE:
Jerseys.addResource(binder, NewResource.class);

----------------------------------------

TITLE: Avro Hadoop Parser Configuration
DESCRIPTION: Example configuration for Hadoop-based batch ingestion using Avro format, including custom schema file specification.

LANGUAGE: json
CODE:
{
  "type" : "index_hadoop",  
  "spec" : {
    "dataSchema" : {
      "dataSource" : "",
      "parser" : {
        "type" : "avro_hadoop",
        "parseSpec" : {
          "format": "avro",
          "timestampSpec": <standard timestampSpec>,
          "dimensionsSpec": <standard dimensionsSpec>,
          "flattenSpec": <optional>
        }
      }
    },
    "ioConfig" : {
      "type" : "hadoop",
      "inputSpec" : {
        "type" : "static",
        "inputFormat": "org.apache.druid.data.input.avro.AvroValueInputFormat",
        "paths" : ""
      }
    },
    "tuningConfig" : {
       "jobProperties" : {
          "avro.schema.input.value.path" : "/path/to/my/schema.avsc"
      }
    }
  }
}

----------------------------------------

TITLE: Complete Druid Ingestion Specification
DESCRIPTION: Full ingestion specification for batch indexing task including dataSchema configuration, input source definition, and tuning parameters.

LANGUAGE: json
CODE:
{
  "type" : "index",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "ingestion-tutorial",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "timestampSpec" : {
            "format" : "iso",
            "column" : "ts"
          },
          "dimensionsSpec" : {
            "dimensions": [
              "srcIP",
              { "name" : "srcPort", "type" : "long" },
              { "name" : "dstIP", "type" : "string" },
              { "name" : "dstPort", "type" : "long" },
              { "name" : "protocol", "type" : "string" }
            ]
          }      
        }
      },
      "metricsSpec" : [
        { "type" : "count", "name" : "count" },
        { "type" : "longSum", "name" : "packets", "fieldName" : "packets" },
        { "type" : "longSum", "name" : "bytes", "fieldName" : "bytes" },
        { "type" : "doubleSum", "name" : "cost", "fieldName" : "cost" }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "HOUR",
        "queryGranularity" : "MINUTE",
        "intervals" : ["2018-01-01/2018-01-02"],
        "rollup" : true
      }
    },
    "ioConfig" : {
      "type" : "index",
      "firehose" : {
        "type" : "local",
        "baseDir" : "quickstart/",
        "filter" : "ingestion-tutorial-data.json"
      }
    },
    "tuningConfig" : {
      "type" : "index",
      "targetPartitionSize" : 5000000
    }
  }
}

----------------------------------------

TITLE: Configuring Local Mount Deep Storage in Apache Druid
DESCRIPTION: Configuration settings for using a local mount as deep storage in Apache Druid. This includes setting the storage type and specifying the storage directory.

LANGUAGE: markdown
CODE:
|Property|Possible Values|Description|Default|
|--------|---------------|-----------|-------|
|`druid.storage.type`|local||Must be set.|
|`druid.storage.storageDirectory`||Directory for storing segments.|Must be set.|

----------------------------------------

TITLE: Querying Grouped Data in Druid SQL
DESCRIPTION: This SQL query demonstrates how to group and aggregate data across multiple segments in Druid.

LANGUAGE: sql
CODE:
select __time, animal, SUM("count"), SUM("number") from "updates-tutorial" group by __time, animal;

----------------------------------------

TITLE: Configuring Local Mount Deep Storage in Apache Druid
DESCRIPTION: Configuration settings for using a local mount as deep storage in Apache Druid. This includes setting the storage type and specifying the storage directory.

LANGUAGE: markdown
CODE:
|Property|Possible Values|Description|Default|
|--------|---------------|-----------|-------|
|`druid.storage.type`|local||Must be set.|
|`druid.storage.storageDirectory`||Directory for storing segments.|Must be set.|

----------------------------------------

TITLE: Starting Tranquility Server in Bash
DESCRIPTION: Command to start the Tranquility server with a configuration file. The server allows sending data to Druid without developing a JVM application.

LANGUAGE: bash
CODE:
bin/tranquility server -configFile <path_to_config_file>/server.json

----------------------------------------

TITLE: Configuring Tuning Config in JSON for Hadoop Indexer
DESCRIPTION: JSON configuration for the tuningConfig section of the Hadoop Indexer spec file. It includes the working path setting for intermediate results.

LANGUAGE: json
CODE:
"tuningConfig" : {
 ...
  "workingPath": "/tmp",
  ...
}

----------------------------------------

TITLE: Configuring Local Mount Deep Storage in Apache Druid
DESCRIPTION: Configuration settings for using a local mount as deep storage in Apache Druid. This includes setting the storage type and specifying the storage directory.

LANGUAGE: markdown
CODE:
|Property|Possible Values|Description|Default|
|--------|---------------|-----------|-------|
|`druid.storage.type`|local||Must be set.|
|`druid.storage.storageDirectory`||Directory for storing segments.|Must be set.|

----------------------------------------

TITLE: Basic Select Query Structure in Druid
DESCRIPTION: Example of a basic Select query to retrieve raw data from the Wikipedia dataset with pagination. Demonstrates core query parameters including dataSource, intervals, and pagingSpec.

LANGUAGE: json
CODE:
{
   "queryType": "select",
   "dataSource": "wikipedia",
   "descending": "false",
   "dimensions":[],
   "metrics":[],
   "granularity": "all",
   "intervals": [
     "2013-01-01/2013-01-02"
   ],
   "pagingSpec":{"pagingIdentifiers": {}, "threshold":5}
 }

----------------------------------------

TITLE: Implementing FloatMax Aggregator in Druid
DESCRIPTION: JSON configuration for the floatMax aggregator in Druid, which computes the maximum of all metric values and Float.NEGATIVE_INFINITY.

LANGUAGE: json
CODE:
{ "type" : "floatMax", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: Sample Bitmap Output Format in JSON
DESCRIPTION: Example of bitmap index output from the DumpSegment tool when using the bitmap dump format. Shows the bitmap serde factory type and encoded bitmap data for column values.

LANGUAGE: json
CODE:
{
  "bitmapSerdeFactory": {
    "type": "concise"
  },
  "bitmaps": {
    "isRobot": {
      "false": "//aExfu+Nv3X...",
      "true": "gAl7OoRByQ..."
    }
  }
}

----------------------------------------

TITLE: Configuring Loading Off-heap MapDB Cache in Druid
DESCRIPTION: Example configuration for a loading lookup using off-heap MapDB cache with custom settings for both forward and reverse lookup caches.

LANGUAGE: json
CODE:
{
   "type":"loadingLookup",
   "dataFetcher":{ "type":"jdbcDataFetcher", "connectorConfig":"jdbc://mysql://localhost:3306/my_data_base", "table":"lookup_table_name", "keyColumn":"key_column_name", "valueColumn": "value_column_name"},
   "loadingCacheSpec":{"type":"mapDb", "maxEntriesSize":100000},
   "reverseLoadingCacheSpec":{"type":"mapDb", "maxStoreSize":5, "expireAfterAccess":100000, "expireAfterAccess":10000}
}

----------------------------------------

TITLE: Configuring Druid Compaction Task Specification
DESCRIPTION: JSON specification for a Druid compaction task that defines parameters for merging segments. Includes all available configuration options like dataSource, interval, dimensions, granularity settings, and tuning configs.

LANGUAGE: json
CODE:
{
    "type": "compact",
    "id": <task_id>,
    "dataSource": <task_datasource>,
    "interval": <interval to specify segments to be merged>,
    "dimensions" <custom dimensionsSpec>,
    "keepSegmentGranularity": <true or false>,
    "segmentGranularity": <segment granularity after compaction>,
    "targetCompactionSizeBytes": <target size of compacted segments>
    "tuningConfig" <index task tuningConfig>,
    "context": <task context>
}

----------------------------------------

TITLE: Configuring Druid Compaction Task Specification
DESCRIPTION: JSON specification for a Druid compaction task that defines parameters for merging segments. Includes all available configuration options like dataSource, interval, dimensions, granularity settings, and tuning configs.

LANGUAGE: json
CODE:
{
    "type": "compact",
    "id": <task_id>,
    "dataSource": <task_datasource>,
    "interval": <interval to specify segments to be merged>,
    "dimensions" <custom dimensionsSpec>,
    "keepSegmentGranularity": <true or false>,
    "segmentGranularity": <segment granularity after compaction>,
    "targetCompactionSizeBytes": <target size of compacted segments>
    "tuningConfig" <index task tuningConfig>,
    "context": <task context>
}

----------------------------------------

TITLE: Creating Druid Database and User in MySQL
DESCRIPTION: SQL commands to create a Druid database with UTF-8 encoding, create a Druid user, and grant necessary permissions.

LANGUAGE: sql
CODE:
CREATE DATABASE druid DEFAULT CHARACTER SET utf8mb4;

CREATE USER 'druid'@'localhost' IDENTIFIED BY 'diurd';

GRANT ALL PRIVILEGES ON druid.* TO 'druid'@'localhost';

----------------------------------------

TITLE: Configuring Lookup DimensionSpec with External Lookup in Druid
DESCRIPTION: Defines a lookup implementation as a dimension spec using an external lookup table or resource.

LANGUAGE: json
CODE:
{
  "type":"lookup",
  "dimension":"dimensionName",
  "outputName":"dimensionOutputName",
  "name":"lookupName"
}

----------------------------------------

TITLE: GroupBy Query with Selector Filter on Multi-value Dimensions in Apache Druid
DESCRIPTION: Example of a groupBy query that filters for rows where 'tags' contains 't3', then groups by the 'tags' dimension. This demonstrates how filtering is applied before dimension explosion.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "test",
  "intervals": [
    "1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"
  ],
  "filter": {
    "type": "selector",
    "dimension": "tags",
    "value": "t3"
  },
  "granularity": {
    "type": "all"
  },
  "dimensions": [
    {
      "type": "default",
      "dimension": "tags",
      "outputName": "tags"
    }
  ],
  "aggregations": [
    {
      "type": "count",
      "name": "count"
    }
  ]
}

----------------------------------------

TITLE: Running Druid Peon Process Command
DESCRIPTION: Shell command for running a Druid Peon process independently. Takes a task file containing the task JSON and a status file path for output. This is primarily used for development purposes as Peons are typically managed by MiddleManager.

LANGUAGE: shell
CODE:
org.apache.druid.cli.Main internal peon <task_file> <status_file>

----------------------------------------

TITLE: Configuring Append Task in Apache Druid (Deprecated)
DESCRIPTION: Defines the JSON structure for an Append task in Druid, which appends a list of segments into a single segment. It requires specifying the task ID, data source, segments to append, and optionally includes aggregations and task context.

LANGUAGE: json
CODE:
{
    "type": "append",
    "id": <task_id>,
    "dataSource": <task_datasource>,
    "segments": <JSON list of DataSegment objects to append>,
    "aggregations": <optional list of aggregators>,
    "context": <task context>
}

----------------------------------------

TITLE: Coordinator Dynamic Configuration Example
DESCRIPTION: Example JSON configuration for dynamically configuring the Coordinator

LANGUAGE: json
CODE:
{
  "millisToWaitBeforeDeleting": 900000,
  "mergeBytesLimit": 100000000,
  "mergeSegmentsLimit" : 1000,
  "maxSegmentsToMove": 5,
  "replicantLifetime": 15,
  "replicationThrottleLimit": 10,
  "emitBalancingStats": false,
  "killDataSourceWhitelist": ["wikipedia", "testDatasource"],
  "decommissioningNodes": ["localhost:8182", "localhost:8282"],
  "decommissioningMaxPercentOfMaxSegmentsToMove": 70
}

----------------------------------------

TITLE: Router Runtime Configuration - Properties
DESCRIPTION: Runtime properties configuration for the Router including service definitions, connection settings and thread configurations

LANGUAGE: properties
CODE:
druid.host=#{IP_ADDR}:8080
druid.plaintextPort=8080
druid.service=druid/router

druid.router.defaultBrokerServiceName=druid:broker-cold
druid.router.coordinatorServiceName=druid:coordinator
druid.router.tierToBrokerMap={"hot":"druid:broker-hot","_default_tier":"druid:broker-cold"}
druid.router.http.numConnections=50
druid.router.http.readTimeout=PT5M

# Number of threads used by the Router proxy http client
druid.router.http.numMaxThreads=100

druid.server.http.numThreads=100

----------------------------------------

TITLE: Sample Result of Scan Query with 'compactedList' Format
DESCRIPTION: This JSON snippet illustrates the structure of the result when the resultFormat is set to 'compactedList'. It includes the segmentId, columns, and an array of arrays containing the queried data in a more compact format.

LANGUAGE: json
CODE:
[{
    "segmentId" : "wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9",
    "columns" : [
      "timestamp", "robot", "namespace", "anonymous", "unpatrolled", "page", "language", "newpage", "user", "count", "added", "delta", "variation", "deleted"
    ],
    "events" : [
     ["2013-01-01T00:00:00.000Z", "1", "article", "0", "0", "11._korpus_(NOVJ)", "sl", "0", "EmausBot", 1.0, 39.0, 39.0, 39.0, 0.0],
     ["2013-01-01T00:00:00.000Z", "0", "article", "0", "0", "112_U.S._580", "en", "1", "MZMcBride", 1.0, 70.0, 70.0, 70.0, 0.0],
     ["2013-01-01T00:00:00.000Z", "0", "article", "0", "0", "113_U.S._243", "en", "1", "MZMcBride", 1.0, 77.0, 77.0, 77.0, 0.0],
     ["2013-01-01T00:00:00.000Z", "0", "article", "0", "0", "113_U.S._73", "en", "1", "MZMcBride", 1.0, 70.0, 70.0, 70.0, 0.0],
     ["2013-01-01T00:00:00.000Z", "0", "article", "0", "0", "113_U.S._756", "en", "1", "MZMcBride", 1.0, 68.0, 68.0, 68.0, 0.0]
    ]
} ]

----------------------------------------

TITLE: Configuring Interval Broadcast Rule in Apache Druid
DESCRIPTION: This JSON snippet defines an interval broadcast rule, which specifies how segments of different data sources should be co-located in Historical processes for a specific time interval.

LANGUAGE: json
CODE:
{
  "type" : "broadcastByInterval",
  "colocatedDataSources" : [ "target_source1", "target_source2" ],
  "interval" : "2012-01-01/2013-01-01"
}

----------------------------------------

TITLE: Combining and Overwriting Data in Druid
DESCRIPTION: This command submits a task that combines existing data with new data and overwrites the original datasource.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/updates-append-index.json

----------------------------------------

TITLE: Submitting Index Task
DESCRIPTION: Command to submit the Wikipedia indexing task to Druid.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/wikipedia-index-hadoop.json --url http://localhost:8081

----------------------------------------

TITLE: Configuring Period Drop Before Rule in Apache Druid
DESCRIPTION: This JSON snippet defines a period drop before rule, which specifies that segments before a certain rolling time period should be dropped from the cluster.

LANGUAGE: json
CODE:
{
  "type" : "dropBeforeByPeriod",
  "period" : "P1M"
}

----------------------------------------

TITLE: Configuring Log4j2 XML for Druid
DESCRIPTION: Example log4j2.xml configuration file for Apache Druid that sets up console logging with timestamp patterns and configurable log levels. Includes an optional section for HTTP request logging.

LANGUAGE: xml
CODE:
<?xml version="1.0" encoding="UTF-8" ?>
<Configuration status="WARN">
  <Appenders>
    <Console name="Console" target="SYSTEM_OUT">
      <PatternLayout pattern="%d{ISO8601} %p [%t] %c - %m%n"/>
    </Console>
  </Appenders>
  <Loggers>
    <Root level="info">
      <AppenderRef ref="Console"/>
    </Root>

    <!-- Uncomment to enable logging of all HTTP requests
    <Logger name="org.apache.druid.jetty.RequestLog" additivity="false" level="DEBUG">
        <AppenderRef ref="Console"/>
    </Logger>
    -->
  </Loggers>
</Configuration>

----------------------------------------

TITLE: Configuring Guava-based Loading Lookup Cache in Druid
DESCRIPTION: Configuration for an on-heap loading cache implementation using Guava cache with customized eviction policies and size limits.

LANGUAGE: json
CODE:
{
   "type":"loadingLookup",
   "dataFetcher":{ "type":"jdbcDataFetcher", "connectorConfig":"jdbc://mysql://localhost:3306/my_data_base", "table":"lookup_table_name", "keyColumn":"key_column_name", "valueColumn": "value_column_name"},
   "loadingCacheSpec":{"type":"guava"},
   "reverseLoadingCacheSpec":{"type":"guava", "maximumSize":500000, "expireAfterAccess":100000, "expireAfterAccess":10000}
}

----------------------------------------

TITLE: Inline Schema Avro Decoder Configuration
DESCRIPTION: Configuration for inline schema-based Avro bytes decoder that uses a fixed schema defined directly in the task configuration.

LANGUAGE: json
CODE:
"avroBytesDecoder": {
  "type": "schema_inline",
  "schema": {
    "namespace": "org.apache.druid.data",
    "name": "User",
    "type": "record",
    "fields": [
      { "name": "FullName", "type": "string" },
      { "name": "Country", "type": "string" }
    ]
  }
}

----------------------------------------

TITLE: Segment Naming Convention Example - Version 2
DESCRIPTION: Shows the naming pattern for reindexed segments with a new schema version (v2) covering the same time interval.

LANGUAGE: plaintext
CODE:
foo_2015-01-01/2015-01-02_v2_0
foo_2015-01-01/2015-01-02_v2_1
foo_2015-01-01/2015-01-02_v2_2

----------------------------------------

TITLE: Configuring StringFirst Aggregator in Druid JSON
DESCRIPTION: Defines a stringFirst aggregator to compute the metric value with the minimum timestamp or null if no row exists. Includes optional maxStringBytes and filterNullValues parameters.

LANGUAGE: JSON
CODE:
{
  "type" : "stringFirst",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "maxStringBytes" : <integer> # (optional, defaults to 1024),
  "filterNullValues" : <boolean> # (optional, defaults to false)
}

----------------------------------------

TITLE: ResetCluster Tool Help Output
DESCRIPTION: The help output for the ResetCluster tool, showing available options and their descriptions.

LANGUAGE: text
CODE:
NAME
        druid tools reset-cluster - Cleanup all persisted state from metadata
        and deep storage.

SYNOPSIS
        druid tools reset-cluster [--all] [--hadoopWorkingPath]
                [--metadataStore] [--segmentFiles] [--taskLogs]

OPTIONS
        --all
            delete all state stored in metadata and deep storage

        --hadoopWorkingPath
            delete hadoopWorkingPath

        --metadataStore
            delete all records in metadata storage

        --segmentFiles
            delete all segment files from deep storage

        --taskLogs
            delete all tasklogs

----------------------------------------

TITLE: Configuring Druid Worker Blacklist Properties
DESCRIPTION: Configuration properties for managing worker blacklisting behavior in Druid. These settings control the retry threshold before blacklisting, blacklist duration, cleanup intervals, and maximum percentage of workers that can be blacklisted.

LANGUAGE: properties
CODE:
druid.indexer.runner.maxRetriesBeforeBlacklist
druid.indexer.runner.workerBlackListBackoffTime
druid.indexer.runner.workerBlackListCleanupPeriod
druid.indexer.runner.maxPercentageBlacklistWorkers

----------------------------------------

TITLE: Configuring Period Drop Before Rule in Apache Druid
DESCRIPTION: This JSON snippet defines a period drop before rule in Apache Druid. It specifies that segments before the given period should be dropped from the cluster.

LANGUAGE: json
CODE:
{
  "type" : "dropBeforeByPeriod",
  "period" : "P1M"
}

----------------------------------------

TITLE: Configuring Avro Stream Parser with Schema Repo
DESCRIPTION: Example configuration for using Avro stream parser with schema repo bytes decoder. Includes settings for subject/id converter and schema repository endpoint.

LANGUAGE: json
CODE:
{
  "parser" : {
    "type" : "avro_stream",
    "avroBytesDecoder" : {
      "type" : "schema_repo",
      "subjectAndIdConverter" : {
        "type" : "avro_1124",
        "topic" : "${YOUR_TOPIC}"
      },
      "schemaRepository" : {
        "type" : "avro_1124_rest_client",
        "url" : "${YOUR_SCHEMA_REPO_END_POINT}"
      }
    },
    "parseSpec" : {
      "format": "avro",
      "timestampSpec": "<standard timestampSpec>",
      "dimensionsSpec": "<standard dimensionsSpec>",
      "flattenSpec": "<optional>"
    }
  }
}

----------------------------------------

TITLE: Running DumpSegment Tool in Java for Apache Druid
DESCRIPTION: This snippet demonstrates how to run the DumpSegment tool using Java. It specifies the classpath, main class, and required arguments for dumping a segment's contents to a file.

LANGUAGE: java
CODE:
java -classpath "/my/druid/lib/*" org.apache.druid.cli.Main tools dump-segment \
  --directory /home/druid/path/to/segment/ \
  --out /home/druid/output.txt

----------------------------------------

TITLE: Installing and Extracting Kafka
DESCRIPTION: Commands to download Kafka 0.10.2.0 and extract it to the local filesystem

LANGUAGE: bash
CODE:
curl -O https://archive.apache.org/dist/kafka/0.10.2.0/kafka_2.11-0.10.2.0.tgz
tar -xzf kafka_2.11-0.10.2.0.tgz
cd kafka_2.11-0.10.2.0

----------------------------------------

TITLE: Configuring Time Format Extraction Function in Apache Druid
DESCRIPTION: JSON configuration for a Time Format Extraction Function, which formats dimension values according to the given format string, time zone, and locale.

LANGUAGE: JSON
CODE:
{ "type" : "timeFormat",
  "format" : <output_format> (optional),
  "timeZone" : <time_zone> (optional, default UTC),
  "locale" : <locale> (optional, default current locale),
  "granularity" : <granularity> (optional, default none) },
  "asMillis" : <true or false> (optional) }

----------------------------------------

TITLE: Example JSON Input Structure
DESCRIPTION: Sample JSON document showing nested fields, arrays, and mixed data types that can be flattened during ingestion.

LANGUAGE: json
CODE:
{
 "timestamp": "2015-09-12T12:10:53.155Z",
 "dim1": "qwerty",
 "dim2": "asdf",
 "dim3": "zxcv",
 "ignore_me": "ignore this",
 "metrica": 9999,
 "foo": {"bar": "abc"},
 "foo.bar": "def",
 "nestmet": {"val": 42},
 "hello": [1.0, 2.0, 3.0, 4.0, 5.0],
 "mixarray": [1.0, 2.0, 3.0, 4.0, {"last": 5}],
 "world": [{"hey": "there"}, {"tree": "apple"}],
 "thing": {"food": ["sandwich", "pizza"]}
}

----------------------------------------

TITLE: ArrayOfDoublesSketch Aggregator Configuration
DESCRIPTION: JSON configuration for the ArrayOfDoublesSketch aggregator including all available parameters.

LANGUAGE: json
CODE:
{
  "type" : "arrayOfDoublesSketch",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "nominalEntries": <number>,
  "numberOfValues" : <number>,
  "metricColumns" : <array of strings>
 }

----------------------------------------

TITLE: Configuring Variance Aggregator for Ingestion in Druid
DESCRIPTION: Configuration for setting up variance aggregation during data ingestion. Supports input types of float, long, or variance with optional population estimator setting.

LANGUAGE: json
CODE:
{
  "type" : "variance",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "inputType" : <input_type>,
  "estimator" : <string>
}

----------------------------------------

TITLE: Configuring Avro Stream Parser with Schema Repo
DESCRIPTION: Example configuration for using Avro stream parser with schema repo bytes decoder. Includes settings for subject/id converter and schema repository endpoint.

LANGUAGE: json
CODE:
{
  "parser" : {
    "type" : "avro_stream",
    "avroBytesDecoder" : {
      "type" : "schema_repo",
      "subjectAndIdConverter" : {
        "type" : "avro_1124",
        "topic" : "${YOUR_TOPIC}"
      },
      "schemaRepository" : {
        "type" : "avro_1124_rest_client",
        "url" : "${YOUR_SCHEMA_REPO_END_POINT}"
      }
    },
    "parseSpec" : {
      "format": "avro",
      "timestampSpec": "<standard timestampSpec>",
      "dimensionsSpec": "<standard dimensionsSpec>",
      "flattenSpec": "<optional>"
    }
  }
}

----------------------------------------

TITLE: Configuring log4j2 for Apache Druid Logging
DESCRIPTION: This XML snippet demonstrates a sample log4j2 configuration for Apache Druid. It sets up console logging with a specific pattern layout and configures the root logger level. It also includes a commented section for enabling logging of all HTTP requests.

LANGUAGE: XML
CODE:
<?xml version="1.0" encoding="UTF-8" ?>
<Configuration status="WARN">
  <Appenders>
    <Console name="Console" target="SYSTEM_OUT">
      <PatternLayout pattern="%d{ISO8601} %p [%t] %c - %m%n"/>
    </Console>
  </Appenders>
  <Loggers>
    <Root level="info">
      <AppenderRef ref="Console"/>
    </Root>

    <!-- Uncomment to enable logging of all HTTP requests
    <Logger name="org.apache.druid.jetty.RequestLog" additivity="false" level="DEBUG">
        <AppenderRef ref="Console"/>
    </Logger>
    -->
  </Loggers>
</Configuration>

----------------------------------------

TITLE: Configuring log4j2 for Apache Druid Logging
DESCRIPTION: This XML snippet demonstrates a sample log4j2 configuration for Apache Druid. It sets up console logging with a specific pattern layout and configures the root logger level. It also includes a commented section for enabling logging of all HTTP requests.

LANGUAGE: XML
CODE:
<?xml version="1.0" encoding="UTF-8" ?>
<Configuration status="WARN">
  <Appenders>
    <Console name="Console" target="SYSTEM_OUT">
      <PatternLayout pattern="%d{ISO8601} %p [%t] %c - %m%n"/>
    </Console>
  </Appenders>
  <Loggers>
    <Root level="info">
      <AppenderRef ref="Console"/>
    </Root>

    <!-- Uncomment to enable logging of all HTTP requests
    <Logger name="org.apache.druid.jetty.RequestLog" additivity="false" level="DEBUG">
        <AppenderRef ref="Console"/>
    </Logger>
    -->
  </Loggers>
</Configuration>

----------------------------------------

TITLE: Building Apache Druid Distribution with Advanced Options
DESCRIPTION: Comprehensive Maven command to build Druid source and binary distributions with signatures, checksums, license auditing, and skipping unit tests.

LANGUAGE: bash
CODE:
mvn clean install -Papache-release,dist,rat -DskipTests

----------------------------------------

TITLE: Accessing Druid Console URL Format
DESCRIPTION: Example URL format for accessing the Druid Console through the Router process, where ROUTER_IP and ROUTER_PORT should be replaced with actual values.

LANGUAGE: plaintext
CODE:
http://<ROUTER_IP>:<ROUTER_PORT>

----------------------------------------

TITLE: ArrayOfDoublesSketch Aggregator Configuration
DESCRIPTION: JSON configuration for the ArrayOfDoublesSketch aggregator including all available parameters.

LANGUAGE: json
CODE:
{
  "type" : "arrayOfDoublesSketch",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "nominalEntries": <number>,
  "numberOfValues" : <number>,
  "metricColumns" : <array of strings>
 }

----------------------------------------

TITLE: Sample JSON Input Structure
DESCRIPTION: Example of a nested JSON document structure that can be flattened during ingestion, showing various data types and nested structures.

LANGUAGE: json
CODE:
{
 "timestamp": "2015-09-12T12:10:53.155Z",
 "dim1": "qwerty",
 "dim2": "asdf",
 "dim3": "zxcv",
 "ignore_me": "ignore this",
 "metrica": 9999,
 "foo": {"bar": "abc"},
 "foo.bar": "def",
 "nestmet": {"val": 42},
 "hello": [1.0, 2.0, 3.0, 4.0, 5.0],
 "mixarray": [1.0, 2.0, 3.0, 4.0, {"last": 5}],
 "world": [{"hey": "there"}, {"tree": "apple"}],
 "thing": {"food": ["sandwich", "pizza"]}
}

----------------------------------------

TITLE: Configuring Period Load Rule in Apache Druid
DESCRIPTION: This JSON snippet defines a period load rule in Apache Druid. It specifies how many replicas of a segment should exist in different server tiers for a specific time period, with an option to include future data.

LANGUAGE: json
CODE:
{
  "type" : "loadByPeriod",
  "period" : "P1M",
  "includeFuture" : true,
  "tieredReplicants": {
      "hot": 1,
      "_default_tier" : 1
  }
}

----------------------------------------

TITLE: GroupBy Query with Selector Filter
DESCRIPTION: Example of a GroupBy query using a selector filter to filter multi-value dimensions based on specific tag values.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "test",
  "intervals": [
    "1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"
  ],
  "filter": {
    "type": "selector",
    "dimension": "tags",
    "value": "t3"
  },
  "granularity": {
    "type": "all"
  },
  "dimensions": [
    {
      "type": "default",
      "dimension": "tags",
      "outputName": "tags"
    }
  ],
  "aggregations": [
    {
      "type": "count",
      "name": "count"
    }
  ]
}

----------------------------------------

TITLE: Starting Tranquility Kafka for Druid Stream Ingestion
DESCRIPTION: Command to start Tranquility Kafka using a configuration file. This allows loading data from Kafka into Druid without writing code. Note that this method is deprecated.

LANGUAGE: bash
CODE:
bin/tranquility kafka -configFile <path_to_config_file>/kafka.json

----------------------------------------

TITLE: Configuring Period Load Rule in Apache Druid
DESCRIPTION: This JSON snippet defines a period load rule, which specifies how many replicas of a segment should exist in different server tiers for a rolling time period.

LANGUAGE: json
CODE:
{
  "type" : "loadByPeriod",
  "period" : "P1M",
  "includeFuture" : true,
  "tieredReplicants": {
      "hot": 1,
      "_default_tier" : 1
  }
}

----------------------------------------

TITLE: MySQL HDFS Integration Command
DESCRIPTION: Example command for inserting segments from HDFS into MySQL metadata storage using the insert-segment-to-db tool.

LANGUAGE: bash
CODE:
java 
-Ddruid.metadata.storage.type=mysql 
-Ddruid.metadata.storage.connector.connectURI=jdbc\:mysql\://localhost\:3306/druid 
-Ddruid.metadata.storage.connector.user=druid 
-Ddruid.metadata.storage.connector.password=diurd 
-Ddruid.extensions.loadList=[\"mysql-metadata-storage\",\"druid-hdfs-storage\"] 
-Ddruid.storage.type=hdfs
-cp $DRUID_CLASSPATH 
org.apache.druid.cli.Main tools insert-segment-to-db --workingDir hdfs://host:port//druid/storage/wikipedia --updateDescriptor true

----------------------------------------

TITLE: Configuring On-heap Polling Lookup in Druid
DESCRIPTION: JSON configuration for a polling cache that updates its on-heap cache every 10 minutes using JDBC data fetcher.

LANGUAGE: json
CODE:
{
    "type":"pollingLookup",
   "pollPeriod":"PT10M",
   "dataFetcher":{ "type":"jdbcDataFetcher", "connectorConfig":"jdbc://mysql://localhost:3306/my_data_base", "table":"lookup_table_name", "keyColumn":"key_column_name", "valueColumn": "value_column_name"},
   "cacheFactory":{"type":"onHeapPolling"}
}

----------------------------------------

TITLE: Registering Jackson Subtypes for Firehose Factory
DESCRIPTION: Example of registering a FirehoseFactory with Jackson's polymorphic serialization/deserialization layer in a Druid module. This allows the system to load the custom FirehoseFactory when specified in the realtime config.

LANGUAGE: Java
CODE:
@Override
public List<? extends Module> getJacksonModules()
{
  return ImmutableList.of(
          new SimpleModule().registerSubtypes(new NamedType(StaticS3FirehoseFactory.class, "static-s3"))
  );
}

----------------------------------------

TITLE: Configuring Graphite Emitter with White-List Converter in JSON
DESCRIPTION: JSON configuration for the Graphite Emitter using the 'whiteList' event converter. This setup allows selective sending of metrics and dimensions to Graphite based on a whitelist, with an option to specify a custom whitelist map file.

LANGUAGE: json
CODE:
{
"druid.emitter.graphite.eventConverter": {
  "type": "whiteList",
  "namespacePrefix": "druid.test",
  "ignoreHostname": true,
  "ignoreServiceName": true,
  "mapPath": "/pathPrefix/fileName.json"
}
}

----------------------------------------

TITLE: Example JSON Response for Overlord Leader Check
DESCRIPTION: Example JSON response from the /druid/indexer/v1/isLeader endpoint to check if an Overlord is the current leader.

LANGUAGE: json
CODE:
{
  "leader": true
}

----------------------------------------

TITLE: Specifying Hadoop Dependencies in Druid Tasks
DESCRIPTION: Example of using hadoopDependencyCoordinates in a Hadoop Index Task to specify the version of Hadoop client libraries to load.

LANGUAGE: json
CODE:
"hadoopDependencyCoordinates": ["org.apache.hadoop:hadoop-client:2.4.0"]

----------------------------------------

TITLE: Listing Druid Segments in Deep Storage
DESCRIPTION: Command to list all segments in the deletion-tutorial datasource directory to verify segment presence in deep storage.

LANGUAGE: bash
CODE:
ls -l1 var/druid/segments/deletion-tutorial/

----------------------------------------

TITLE: Configuring Timed Shutoff Firehose in Druid
DESCRIPTION: Configuration for TimedShutoffFirehose that automatically shuts down at a specified time. Wraps another firehose configuration.

LANGUAGE: json
CODE:
{
    "type"  :   "timed",
    "shutoffTime": "2015-08-25T01:26:05.119Z",
    "delegate": {
          "type": "receiver",
          "serviceName": "eventReceiverServiceName",
          "bufferSize": 100000
     }
}

----------------------------------------

TITLE: Specifying Hadoop Dependencies in Hadoop Index Task
DESCRIPTION: This snippet shows how to specify the Hadoop dependencies to be loaded by Druid in the Hadoop Index Task configuration.

LANGUAGE: json
CODE:
"hadoopDependencyCoordinates": ["org.apache.hadoop:hadoop-client:2.4.0"]

----------------------------------------

TITLE: Starting Druid Router Process - Java
DESCRIPTION: Command to start the Druid Router server process

LANGUAGE: java
CODE:
org.apache.druid.cli.Main server router

----------------------------------------

TITLE: Extract Druid Distribution Archive
DESCRIPTION: Commands to download and extract the Apache Druid distribution package

LANGUAGE: bash
CODE:
tar -xzf apache-druid-0.15.0-incubating-bin.tar.gz
cd apache-druid-0.15.0-incubating

----------------------------------------

TITLE: Configuring Forever Load Rule in Apache Druid
DESCRIPTION: This JSON snippet defines a forever load rule in Druid. It specifies how many replicas of a segment should exist in different server tiers indefinitely.

LANGUAGE: json
CODE:
{
  "type" : "loadForever",  
  "tieredReplicants": {
    "hot": 1,
    "_default_tier" : 1
  }
}

----------------------------------------

TITLE: Configuring Search Query Extraction Function in Apache Druid
DESCRIPTION: JSON configuration for a Search Query Extraction Function, which returns the dimension value unchanged if the given SearchQuerySpec matches, otherwise returns null.

LANGUAGE: JSON
CODE:
{ "type" : "searchQuery", "query" : <search_query_spec> }

----------------------------------------

TITLE: ResetCluster Tool Usage Documentation
DESCRIPTION: Detailed usage documentation for the ResetCluster tool, including available options and their descriptions.

LANGUAGE: text
CODE:
NAME
        druid tools reset-cluster - Cleanup all persisted state from metadata
        and deep storage.

SYNOPSIS
        druid tools reset-cluster [--all] [--hadoopWorkingPath]
                [--metadataStore] [--segmentFiles] [--taskLogs]

OPTIONS
        --all
            delete all state stored in metadata and deep storage

        --hadoopWorkingPath
            delete hadoopWorkingPath

        --metadataStore
            delete all records in metadata storage

        --segmentFiles
            delete all segment files from deep storage

        --taskLogs
            delete all tasklogs

----------------------------------------

TITLE: Registering DataSegmentPuller and DataSegmentPusher in Java
DESCRIPTION: This code snippet demonstrates how to register custom DataSegmentPuller and DataSegmentPusher implementations in a Druid module using Guice bindings.

LANGUAGE: Java
CODE:
Binders.dataSegmentPullerBinder(binder)
       .addBinding("hdfs")
       .to(HdfsDataSegmentPuller.class).in(LazySingleton.class);

Binders.dataSegmentPusherBinder(binder)
       .addBinding("hdfs")
       .to(HdfsDataSegmentPusher.class).in(LazySingleton.class);

----------------------------------------

TITLE: Querying Segment Statistics with SQL
DESCRIPTION: SQL query to analyze segment statistics from Druid's system schema. This query returns metrics including average number of rows, segment counts, and total sizes grouped by time intervals and versions.

LANGUAGE: sql
CODE:
SELECT
  "start",
  "end",
  version,
  COUNT(*) AS num_segments,
  AVG("num_rows") AS avg_num_rows,
  SUM("num_rows") AS total_num_rows,
  AVG("size") AS avg_size,
  SUM("size") AS total_size
FROM
  sys.segments A
WHERE
  datasource = 'your_dataSource' AND
  is_published = 1
GROUP BY 1, 2, 3
ORDER BY 1, 2, 3 DESC;

----------------------------------------

TITLE: Filtering Multi-value Dimensions with OR in Apache Druid
DESCRIPTION: Example of an 'or' filter that matches rows where the 'tags' dimension contains either 't1' or 't3'. This filter would match row1 and row2 in the sample dataset.

LANGUAGE: json
CODE:
{
  "type": "or",
  "fields": [
    {
      "type": "selector",
      "dimension": "tags",
      "value": "t1"
    },
    {
      "type": "selector",
      "dimension": "tags",
      "value": "t3"
    }
  ]
}

----------------------------------------

TITLE: Creating an Interval Filter in Druid JSON
DESCRIPTION: Example of an interval filter for range filtering on time-based columns. This filter selects data from two specific date ranges.

LANGUAGE: json
CODE:
{
    "type" : "interval",
    "dimension" : "__time",
    "intervals" : [
      "2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z",
      "2014-11-15T00:00:00.000Z/2014-11-16T00:00:00.000Z"
    ]
}

----------------------------------------

TITLE: Configuring Period Load Rule in Apache Druid
DESCRIPTION: JSON configuration for a period load rule that specifies how many replicas of a segment should exist in different server tiers for a rolling time period.

LANGUAGE: json
CODE:
{
  "type" : "loadByPeriod",
  "period" : "P1M",
  "includeFuture" : true,
  "tieredReplicants": {
      "hot": 1,
      "_default_tier" : 1
  }
}

----------------------------------------

TITLE: Configuring Inline Lookup Extraction Function in Druid
DESCRIPTION: Specifies an inline lookup map for dimension value replacement without registering one in the cluster-wide configuration.

LANGUAGE: json
CODE:
{
  "type":"lookup",
  "lookup":{
    "type":"map",
    "map":{"foo":"bar", "baz":"bat"}
  },
  "retainMissingValue":true,
  "injective":true
}

----------------------------------------

TITLE: Retrieving Completion Report JSON in Apache Druid
DESCRIPTION: Example of a completion report JSON output from Druid's ingestion task. It includes information about the ingestion state, row statistics, and any error messages.

LANGUAGE: json
CODE:
{
  "ingestionStatsAndErrors": {
    "taskId": "compact_twitter_2018-09-24T18:24:23.920Z",
    "payload": {
      "ingestionState": "COMPLETED",
      "unparseableEvents": {},
      "rowStats": {
        "determinePartitions": {
          "processed": 0,
          "processedWithError": 0,
          "thrownAway": 0,
          "unparseable": 0
        },
        "buildSegments": {
          "processed": 5390324,
          "processedWithError": 0,
          "thrownAway": 0,
          "unparseable": 0
        }
      },
      "errorMsg": null
    },
    "type": "ingestionStatsAndErrors"
  }
}

----------------------------------------

TITLE: Timeseries Query Context Properties
DESCRIPTION: Configuration parameters specific to Timeseries queries in Druid.

LANGUAGE: json
CODE:
{
  "skipEmptyBuckets": false
}

----------------------------------------

TITLE: Configuring Expression Transform in JSON for Apache Druid
DESCRIPTION: This example shows how to define an expression transform in Apache Druid. It creates a new field 'fooPage' by prepending 'foo' to the values of the 'page' column.

LANGUAGE: json
CODE:
{
  "type": "expression",
  "name": "fooPage",
  "expression": "concat('foo' + page)"
}

----------------------------------------

TITLE: Explaining SQL Query Plan in Druid
DESCRIPTION: This snippet demonstrates how to use the EXPLAIN PLAN FOR clause to see the native Druid query that a SQL query will be translated into. It helps in understanding query optimization in Druid.

LANGUAGE: sql
CODE:
EXPLAIN PLAN FOR SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;

----------------------------------------

TITLE: Extraction DimensionSpec Configuration in Druid
DESCRIPTION: Dimension specification that transforms values using an extraction function with support for type conversion.

LANGUAGE: json
CODE:
{
  "type" : "extraction",
  "dimension" : <dimension>,
  "outputName" :  <output_name>,
  "outputType": <"STRING"|"LONG"|"FLOAT">,
  "extractionFn" : <extraction_function>
}

----------------------------------------

TITLE: Single Lookup Update Configuration
DESCRIPTION: JSON payload for updating a single lookup configuration for a specific tier and ID.

LANGUAGE: json
CODE:
{\n  "version": "v1",\n  "lookupExtractorFactory": {\n    "type": "map",\n    "map": {\n      "847632": "Internal Use Only"\n    }\n  }\n}

----------------------------------------

TITLE: Setting Up StringLast Aggregator in Druid
DESCRIPTION: JSON configuration for the stringLast aggregator in Druid, which computes the metric value with the maximum timestamp or null if no row exists.

LANGUAGE: json
CODE:
{
  "type" : "stringLast",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "maxStringBytes" : <integer> # (optional, defaults to 1024),
  "filterNullValues" : <boolean> # (optional, defaults to false)
}

----------------------------------------

TITLE: Setting Up StringLast Aggregator in Druid
DESCRIPTION: JSON configuration for the stringLast aggregator in Druid, which computes the metric value with the maximum timestamp or null if no row exists.

LANGUAGE: json
CODE:
{
  "type" : "stringLast",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "maxStringBytes" : <integer> # (optional, defaults to 1024),
  "filterNullValues" : <boolean> # (optional, defaults to false)
}

----------------------------------------

TITLE: Configuring Experimental Extensions in Druid Properties
DESCRIPTION: Configuration example showing how to load experimental features in Druid by modifying the runtime.properties file. This setting needs to be applied to all indexer and query nodes.

LANGUAGE: properties
CODE:
druid.extensions.loadList=["druid-histogram"]

----------------------------------------

TITLE: Sample Input Data JSON Format
DESCRIPTION: Example JSON data containing timestamp, animal, location, and number fields that will be transformed during ingestion.

LANGUAGE: json
CODE:
{
"timestamp":"2018-01-01T07:01:35Z","animal":"octopus",  "location":1, "number":100}
{"timestamp":"2018-01-01T05:01:35Z","animal":"mongoose", "location":2,"number":200}
{"timestamp":"2018-01-01T06:01:35Z","animal":"snake", "location":3, "number":300}
{"timestamp":"2018-01-01T01:01:35Z","animal":"lion", "location":4, "number":300}

----------------------------------------

TITLE: Starting Druid Router Process - Java
DESCRIPTION: Command to start the Druid Router server process

LANGUAGE: java
CODE:
org.apache.druid.cli.Main server router

----------------------------------------

TITLE: Configuring InsensitiveContainsSearchQuerySpec in Druid JSON
DESCRIPTION: Defines a case-insensitive search query spec that matches if any part of a dimension value contains the specified value. This spec is useful for flexible, case-insensitive searching.

LANGUAGE: json
CODE:
{
  "type"  : "insensitive_contains",
  "value" : "some_value"
}

----------------------------------------

TITLE: Running pull-deps Tool with Default Version
DESCRIPTION: Example command to run the pull-deps tool using a default version for extensions. This simplifies the command by allowing omission of version information in the coordinates for extensions.

LANGUAGE: bash
CODE:
java -classpath "/my/druid/lib/*" org.apache.druid.cli.Main tools pull-deps --defaultVersion 0.14.1-incubating --clean -c org.apache.druid.extensions:mysql-metadata-storage -c org.apache.druid.extensions.contrib:druid-rabbitmq -h org.apache.hadoop:hadoop-client:2.3.0 -h org.apache.hadoop:hadoop-client:2.4.0

----------------------------------------

TITLE: Setting up LongMax Aggregator in Druid
DESCRIPTION: The longMax aggregator computes the maximum of all metric values and Long.MIN_VALUE. It requires specifying an output name and the metric field to analyze.

LANGUAGE: json
CODE:
{ "type" : "longMax", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: Submitting Supervisor via HTTP
DESCRIPTION: Command to submit the Kafka supervisor configuration to Druid's overlord service using curl.

LANGUAGE: bash
CODE:
curl -XPOST -H'Content-Type: application/json' -d @quickstart/tutorial/wikipedia-kafka-supervisor.json http://localhost:8081/druid/indexer/v1/supervisor

----------------------------------------

TITLE: Importing Metadata into PostgreSQL Database
DESCRIPTION: SQL commands to import the exported CSV files into PostgreSQL database tables. This includes importing data for segments, rules, config, datasource, and supervisors tables.

LANGUAGE: sql
CODE:
COPY druid_segments(id,dataSource,created_date,start,"end",partitioned,version,used,payload) FROM '/tmp/csv/druid_segments.csv' DELIMITER ',' CSV;

COPY druid_rules(id,dataSource,version,payload) FROM '/tmp/csv/druid_rules.csv' DELIMITER ',' CSV;

COPY druid_config(name,payload) FROM '/tmp/csv/druid_config.csv' DELIMITER ',' CSV;

COPY druid_dataSource(dataSource,created_date,commit_metadata_payload,commit_metadata_sha1) FROM '/tmp/csv/druid_dataSource.csv' DELIMITER ',' CSV;

COPY druid_supervisors(id,spec_id,created_date,payload) FROM '/tmp/csv/druid_supervisors.csv' DELIMITER ',' CSV;

----------------------------------------

TITLE: Configuring Local File Firehose in Druid
DESCRIPTION: Configuration for LocalFirehose that reads data from files on local disk. Supports wildcards for file filtering and can be used with native parallel index tasks.

LANGUAGE: json
CODE:
{
    "type"    : "local",
    "filter"   : "*.csv",
    "baseDir"  : "/data/directory"
}

----------------------------------------

TITLE: Using Dimension Selector Filter in groupBy Query (JSON)
DESCRIPTION: Demonstrates how to use a dimension selector filter in the having clause of a groupBy query to match rows with specific dimension values.

LANGUAGE: json
CODE:
{
    "queryType": "groupBy",
    "dataSource": "sample_datasource",
    ...
    "having": 
       {
            "type": "dimSelector",
            "dimension": "<dimension>",
            "value": <dimension_value>
        }
}

----------------------------------------

TITLE: Configuring Forever Drop Rule in Apache Druid
DESCRIPTION: This JSON snippet defines a Forever Drop Rule, which indicates that all matching segments should be dropped from the cluster permanently.

LANGUAGE: json
CODE:
{
  "type" : "dropForever"  
}

----------------------------------------

TITLE: Using Dimension Selector Filter in groupBy Query (JSON)
DESCRIPTION: Demonstrates how to use a dimension selector filter in the having clause of a groupBy query to match rows with specific dimension values.

LANGUAGE: json
CODE:
{
    "queryType": "groupBy",
    "dataSource": "sample_datasource",
    ...
    "having": 
       {
            "type": "dimSelector",
            "dimension": "<dimension>",
            "value": <dimension_value>
        }
}

----------------------------------------

TITLE: Configuring PrefixFiltered DimensionSpec in Druid
DESCRIPTION: Retains only the values starting with the specified prefix in multi-value dimensions.

LANGUAGE: json
CODE:
{ "type" : "prefixFiltered", "delegate" : <dimensionSpec>, "prefix": <prefix string> }

----------------------------------------

TITLE: Configuring Period Load Rule in Apache Druid
DESCRIPTION: This JSON snippet defines a period load rule in Druid. It specifies how many replicas of a segment should exist in different server tiers for a specific time period.

LANGUAGE: json
CODE:
{
  "type" : "loadByPeriod",
  "period" : "P1M",
  "tieredReplicants": {
      "hot": 1,
      "_default_tier" : 1
  }
}

----------------------------------------

TITLE: Accessing Coordinator Console v2 URL
DESCRIPTION: The URL pattern for accessing the version 2 of the Coordinator Console, which provides cluster information and rule configuration capabilities. This is part of the legacy console system.

LANGUAGE: plaintext
CODE:
http://<COORDINATOR_IP>:<COORDINATOR_PORT>

----------------------------------------

TITLE: Numeric Having Filter in Druid
DESCRIPTION: Example of a numeric having filter using greaterThan comparison on aggregate metrics.

LANGUAGE: json
CODE:
{
    "queryType": "groupBy",
    "dataSource": "sample_datasource",
    ...,
    "having": 
        {
            "type": "greaterThan",
            "aggregation": "<aggregate_metric>",
            "value": <numeric_value>
        }
}

----------------------------------------

TITLE: Lookup Status Response Example
DESCRIPTION: Example response showing the lookup configuration status for a process.

LANGUAGE: json
CODE:
{\n  "site_id_customer2": {\n    "version": "v1",\n    "lookupExtractorFactory": {\n      "type": "map",\n      "map": {\n        "AHF77": "Home"\n      }\n    }\n  }\n}

----------------------------------------

TITLE: Basic Injective Lookup Example
DESCRIPTION: Example of an injective lookup mapping where each key maps to a unique value.

LANGUAGE: text
CODE:
1 -> Foo\n2 -> Bar\n3 -> Billy

----------------------------------------

TITLE: Configuring Period Load Rule in Apache Druid
DESCRIPTION: This JSON snippet defines a period load rule in Druid. It specifies how many replicas of a segment should exist in different server tiers for a specific time period.

LANGUAGE: json
CODE:
{
  "type" : "loadByPeriod",
  "period" : "P1M",
  "tieredReplicants": {
      "hot": 1,
      "_default_tier" : 1
  }
}

----------------------------------------

TITLE: Configuring Cardinality Aggregator in Druid
DESCRIPTION: Basic configuration for cardinality aggregator that computes the cardinality of Druid dimensions using HyperLogLog estimation. Supports multiple dimensions and optional row-based calculation.

LANGUAGE: json
CODE:
{
  "type": "cardinality",
  "name": "<output_name>",
  "fields": [ <dimension1>, <dimension2>, ... ],
  "byRow": <false | true> # (optional, defaults to false),
  "round": <false | true> # (optional, defaults to false)
}

----------------------------------------

TITLE: Configuring Jetty Server TLS Settings in Apache Druid
DESCRIPTION: Configuration properties for setting up TLS/SSL in the embedded Jetty web server used by Apache Druid.

LANGUAGE: markdown
CODE:
|Property|Description|Default|Required|
|--------|-----------|-------|--------|
|`druid.server.https.keyStorePath`|The file path or URL of the TLS/SSL Key store.|none|yes|
|`druid.server.https.keyStoreType`|The type of the key store.|none|yes|
|`druid.server.https.certAlias`|Alias of TLS/SSL certificate for the connector.|none|yes|
|`druid.server.https.keyStorePassword`|The [Password Provider](../operations/password-provider.html) or String password for the Key Store.|none|yes|

----------------------------------------

TITLE: JavaScript Router Strategy Configuration
DESCRIPTION: Example configuration for JavaScript-based router strategy that routes queries based on aggregator count

LANGUAGE: json
CODE:
{
  "type" : "javascript",
  "function" : "function (config, query) { if (query.getAggregatorSpecs && query.getAggregatorSpecs().size() >= 3) { var size = config.getTierToBrokerMap().values().size(); if (size > 0) { return config.getTierToBrokerMap().values().toArray()[size-1] } else { return config.getDefaultBrokerServiceName() } } else { return null } }"
}

----------------------------------------

TITLE: Advanced TLS Configuration Properties
DESCRIPTION: Advanced TLS configuration options including cipher suites, protocols, and key manager settings

LANGUAGE: properties
CODE:
druid.server.https.keyManagerFactoryAlgorithm=javax.net.ssl.KeyManagerFactory.getDefaultAlgorithm()
druid.server.https.includeCipherSuites=Jetty\'s default include cipher list
druid.server.https.excludeCipherSuites=Jetty\'s default exclude cipher list
druid.server.https.includeProtocols=Jetty\'s default include protocol list
druid.server.https.excludeProtocols=Jetty\'s default exclude protocol list

----------------------------------------

TITLE: Cardinality Aggregator Example: Distinct Last Name First Characters
DESCRIPTION: An example of using the cardinality aggregator with a dimension spec to determine the number of distinct starting characters of last names. It uses an extraction function to get the first character of the last name.

LANGUAGE: json
CODE:
{
  "type": "cardinality",
  "name": "distinct_last_name_first_char",
  "fields": [
    {
     "type" : "extraction",
     "dimension" : "last_name",
     "outputName" :  "last_name_first_char",
     "extractionFn" : { "type" : "substring", "index" : 0, "length" : 1 }
    }
  ],
  "byRow" : true
}

----------------------------------------

TITLE: Linear Sharding Configuration in Druid
DESCRIPTION: Example of configuring linear sharding in a Druid Realtime process, which allows non-sequential partition numbering and doesn't require updating fileSpec configurations when adding new processes.

LANGUAGE: json
CODE:
"shardSpec": {
    "type": "linear",
    "partitionNum": 0
}

----------------------------------------

TITLE: Configuring Polling On-heap Lookup in Druid
DESCRIPTION: Example configuration for a polling cache that updates its on-heap cache every 10 minutes using JDBC data fetcher.

LANGUAGE: json
CODE:
{
    "type":"pollingLookup",
   "pollPeriod":"PT10M",
   "dataFetcher":{ "type":"jdbcDataFetcher", "connectorConfig":"jdbc://mysql://localhost:3306/my_data_base", "table":"lookup_table_name", "keyColumn":"key_column_name", "valueColumn": "value_column_name"},
   "cacheFactory":{"type":"onHeapPolling"}
}

----------------------------------------

TITLE: Executing Search Query in Druid JSON
DESCRIPTION: Example of a search query to find dimension values matching a search specification. The query includes datasource, granularity, search dimensions, and time intervals.

LANGUAGE: json
CODE:
{
  "queryType": "search",
  "dataSource": "sample_datasource",
  "granularity": "day",
  "searchDimensions": [
    "dim1",
    "dim2"
  ],
  "query": {
    "type": "insensitive_contains",
    "value": "Ke"
  },
  "sort" : {
    "type": "lexicographic"
  },
  "intervals": [
    "2013-01-01T00:00:00.000/2013-01-03T00:00:00.000"
  ]
}

----------------------------------------

TITLE: Configuring Variance Fold Aggregator for Querying in Druid
DESCRIPTION: Configuration for querying pre-aggregated variance data using the varianceFold aggregator type.

LANGUAGE: json
CODE:
{
  "type" : "varianceFold",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "estimator" : <string>
}

----------------------------------------

TITLE: Defining a Selector Filter in Druid
DESCRIPTION: Example of a selector filter that matches a specific dimension with a specific value. This is equivalent to a WHERE clause in SQL.

LANGUAGE: JSON
CODE:
"filter": { "type": "selector", "dimension": <dimension_string>, "value": <dimension_value_string> }

----------------------------------------

TITLE: Defining OrderByColumnSpec for Druid Query Result Sorting
DESCRIPTION: This JSON structure defines an OrderByColumnSpec, which specifies how to order the results. It includes the dimension or metric name, sort direction, and the type of ordering to apply.

LANGUAGE: json
CODE:
{
    "dimension" : "<Any dimension or metric name>",
    "direction" : <"ascending"|"descending">,
    "dimensionOrder" : <"lexicographic"(default)|"alphanumeric"|"strlen"|"numeric">
}

----------------------------------------

TITLE: Querying Segment Statistics in Apache Druid SQL
DESCRIPTION: This SQL query retrieves statistics about published segments in a Druid datasource, including average number of rows, average size, and total counts. It helps identify if compaction is necessary.

LANGUAGE: sql
CODE:
SELECT
  "start",
  "end",
  version,
  COUNT(*) AS num_segments,
  AVG("num_rows") AS avg_num_rows,
  SUM("num_rows") AS total_num_rows,
  AVG("size") AS avg_size,
  SUM("size") AS total_size
FROM
  sys.segments A
WHERE
  datasource = 'your_dataSource' AND
  is_published = 1
GROUP BY 1, 2, 3
ORDER BY 1, 2, 3 DESC;

----------------------------------------

TITLE: Setting Task Priority in JSON Context
DESCRIPTION: This JSON snippet demonstrates how to override the default task priority by setting a custom priority in the task context. The priority is set to 100, which is higher than the default priorities for most task types.

LANGUAGE: json
CODE:
{
  "context" : {
    "priority" : 100
  }
}

----------------------------------------

TITLE: Implementing Multi-value Column Storage in Druid
DESCRIPTION: Demonstrates how Druid handles multi-value columns by modifying the basic column data structures to support multiple values per row, including array representation in column data and multiple non-zero entries in bitmaps.

LANGUAGE: plaintext
CODE:
1: Dictionary that encodes column values
  {
    "Justin Bieber": 0,
    "Ke$ha":         1
  }

2: Column data
  [0,
   [0,1],  <--Row value of multi-value column can have array of values
   1,
   1]

3: Bitmaps - one for each unique value
  value="Justin Bieber": [1,1,0,0]
  value="Ke$ha":         [0,1,1,1]

----------------------------------------

TITLE: Configuring TimeMin Aggregator
DESCRIPTION: JSON configuration for the timeMin aggregator at ingestion time. Specifies the output name and field to analyze.

LANGUAGE: json
CODE:
{
    "type": "timeMin",
    "name": "tmin",
    "fieldName": "<field_name, typically column specified in timestamp spec>"
}

----------------------------------------

TITLE: Loading Initial Data in Druid
DESCRIPTION: This command loads the initial dataset into Druid using a pre-defined index task specification.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/updates-init-index.json

----------------------------------------

TITLE: Example First Query for Exact Aggregates in Apache Druid
DESCRIPTION: This snippet demonstrates the first query in a two-query approach to get exact aggregates for high-cardinality dimensions. It retrieves the approximate topN dimension values.

LANGUAGE: json
CODE:
{
    "aggregations": [
             {
                 "fieldName": "L_QUANTITY_longSum",
                 "name": "L_QUANTITY_",
                 "type": "longSum"
             }
    ],
    "dataSource": "tpch_year",
    "dimension":"l_orderkey",
    "granularity": "all",
    "intervals": [
        "1900-01-09T00:00:00.000Z/2992-01-10T00:00:00.000Z"
    ],
    "metric": "L_QUANTITY_",
    "queryType": "topN",
    "threshold": 2
}

----------------------------------------

TITLE: Configuring Druid Metadata Storage for Microsoft SQLServer
DESCRIPTION: These properties configure Druid to use Microsoft SQLServer as its metadata storage backend. The connectURI specifies the database location, while the user and password properties provide authentication credentials.

LANGUAGE: properties
CODE:
druid.metadata.storage.type=sqlserver
druid.metadata.storage.connector.connectURI=jdbc:sqlserver://<host>;databaseName=druid
druid.metadata.storage.connector.user=druid
druid.metadata.storage.connector.password=diurd

----------------------------------------

TITLE: Multiple Quantiles Post-Aggregator Configuration
DESCRIPTION: JSON configuration for retrieving multiple quantile values from a DoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "quantilesDoublesSketchToQuantiles",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch>,
  "fractions" : <array of fractional positions>
}

----------------------------------------

TITLE: Configuring HTTP Compression Properties in Apache Druid
DESCRIPTION: This table defines the configuration properties for HTTP compression in Apache Druid. It includes settings for compression level and inflate buffer size for request decompression.

LANGUAGE: markdown
CODE:
|Property|Description|Default|
|--------|-----------|-------|
|`druid.server.http.compressionLevel`|The compression level. Value should be between [-1,9], -1 for default level, 0 for no compression.|-1 (default compression level)|
|`druid.server.http.inflateBufferSize`|The buffer size used by gzip decoder. Set to 0 to disable request decompression.|4096|

----------------------------------------

TITLE: Configuring HTTP Compression Properties in Apache Druid
DESCRIPTION: This table defines the configuration properties for HTTP compression in Apache Druid. It includes settings for compression level and inflate buffer size for request decompression.

LANGUAGE: markdown
CODE:
|Property|Description|Default|
|--------|-----------|-------|
|`druid.server.http.compressionLevel`|The compression level. Value should be between [-1,9], -1 for default level, 0 for no compression.|-1 (default compression level)|
|`druid.server.http.inflateBufferSize`|The buffer size used by gzip decoder. Set to 0 to disable request decompression.|4096|

----------------------------------------

TITLE: JSON Flatten Specification Configuration
DESCRIPTION: Complete parseSpec configuration showing how to flatten nested JSON fields using both field discovery and explicit field definitions.

LANGUAGE: json
CODE:
"parseSpec": {
  "format": "json",
  "flattenSpec": {
    "useFieldDiscovery": true,
    "fields": [
      {
        "type": "root",
        "name": "dim1"
      },
      "dim2",
      {
        "type": "path",
        "name": "foo.bar",
        "expr": "$.foo.bar"
      },
      {
        "type": "root",
        "name": "foo.bar"
      },
      {
        "type": "path",
        "name": "path-metric",
        "expr": "$.nestmet.val"
      },
      {
        "type": "path",
        "name": "hello-0",
        "expr": "$.hello[0]"
      },
      {
        "type": "path",
        "name": "hello-4",
        "expr": "$.hello[4]"
      },
      {
        "type": "path",
        "name": "world-hey",
        "expr": "$.world[0].hey"
      },
      {
        "type": "path",
        "name": "worldtree",
        "expr": "$.world[1].tree"
      },
      {
        "type": "path",
        "name": "first-food",
        "expr": "$.thing.food[0]"
      },
      {
        "type": "path",
        "name": "second-food",
        "expr": "$.thing.food[1]"
      },
      {
        "type": "jq",
        "name": "first-food-by-jq",
        "expr": ".thing.food[1]"
      },
      {
        "type": "jq",
        "name": "hello-total",
        "expr": ".hello | sum"
      }
    ]
  },
  "dimensionsSpec" : {
   "dimensions" : [],
   "dimensionsExclusions": ["ignore_me"]
  },
  "timestampSpec" : {
   "format" : "auto",
   "column" : "timestamp"
  }
}

----------------------------------------

TITLE: JSON Flatten Specification Configuration
DESCRIPTION: Complete parseSpec configuration showing how to flatten nested JSON fields using both field discovery and explicit field definitions.

LANGUAGE: json
CODE:
"parseSpec": {
  "format": "json",
  "flattenSpec": {
    "useFieldDiscovery": true,
    "fields": [
      {
        "type": "root",
        "name": "dim1"
      },
      "dim2",
      {
        "type": "path",
        "name": "foo.bar",
        "expr": "$.foo.bar"
      },
      {
        "type": "root",
        "name": "foo.bar"
      },
      {
        "type": "path",
        "name": "path-metric",
        "expr": "$.nestmet.val"
      },
      {
        "type": "path",
        "name": "hello-0",
        "expr": "$.hello[0]"
      },
      {
        "type": "path",
        "name": "hello-4",
        "expr": "$.hello[4]"
      },
      {
        "type": "path",
        "name": "world-hey",
        "expr": "$.world[0].hey"
      },
      {
        "type": "path",
        "name": "worldtree",
        "expr": "$.world[1].tree"
      },
      {
        "type": "path",
        "name": "first-food",
        "expr": "$.thing.food[0]"
      },
      {
        "type": "path",
        "name": "second-food",
        "expr": "$.thing.food[1]"
      },
      {
        "type": "jq",
        "name": "first-food-by-jq",
        "expr": ".thing.food[1]"
      },
      {
        "type": "jq",
        "name": "hello-total",
        "expr": ".hello | sum"
      }
    ]
  },
  "dimensionsSpec" : {
   "dimensions" : [],
   "dimensionsExclusions": ["ignore_me"]
  },
  "timestampSpec" : {
   "format" : "auto",
   "column" : "timestamp"
  }
}

----------------------------------------

TITLE: Configuring Interval Broadcast Rule in Apache Druid
DESCRIPTION: This JSON snippet defines an interval broadcast rule in Druid. It specifies how segments of different data sources should be co-located in historical nodes for a specific time interval.

LANGUAGE: json
CODE:
{
  "type" : "broadcastByInterval",
  "colocatedDataSources" : [ "target_source1", "target_source2" ],
  "interval" : "2012-01-01/2013-01-01"
}

----------------------------------------

TITLE: Compaction Task Configuration with Original Granularity
DESCRIPTION: JSON configuration for compacting segments while maintaining the original time granularity. Sets maxRowsPerSegment to 5000000 and targets the specific time interval.

LANGUAGE: json
CODE:
{
  "type": "compact",
  "dataSource": "compaction-tutorial",
  "interval": "2015-09-12/2015-09-13",
  "tuningConfig" : {
    "type" : "index",
    "maxRowsPerSegment" : 5000000,
    "maxRowsInMemory" : 25000,
    "forceExtendableShardSpecs" : true
  }
}

----------------------------------------

TITLE: Extracting Druid Distribution Files
DESCRIPTION: Commands to download and extract the Druid distribution package to begin cluster setup

LANGUAGE: bash
CODE:
tar -xzf apache-druid-0.14.1-incubating-bin.tar.gz
cd apache-druid-0.14.1-incubating

----------------------------------------

TITLE: Demonstrating Multi-Interval Segment Updates in Apache Druid
DESCRIPTION: Illustrates how updates spanning multiple segment intervals are handled, showing a mixture of v1 and v2 segments during the update process.

LANGUAGE: plaintext
CODE:
foo_2015-01-01/2015-01-02_v1_0
foo_2015-01-02/2015-01-03_v2_1
foo_2015-01-03/2015-01-04_v1_2

----------------------------------------

TITLE: Accessing Overlord Console URL
DESCRIPTION: The URL pattern for accessing the Overlord Console. Replace OVERLORD_IP and OVERLORD_PORT with actual values.

LANGUAGE: plaintext
CODE:
http://<OVERLORD_IP>:<OVERLORD_PORT>/console.html

----------------------------------------

TITLE: Configuring Druid Metadata Storage Extension for MySQL
DESCRIPTION: Properties to add to the Druid configuration file for using MySQL as the metadata store. Replace <host> with the actual database location.

LANGUAGE: properties
CODE:
druid.extensions.loadList=["mysql-metadata-storage"]
druid.metadata.storage.type=mysql
druid.metadata.storage.connector.connectURI=jdbc:mysql://<host>/druid
druid.metadata.storage.connector.user=druid
druid.metadata.storage.connector.password=druid

----------------------------------------

TITLE: Example JSON Response for MiddleManager Enabled State
DESCRIPTION: Example JSON response from the /druid/worker/v1/enabled endpoint to check if a MiddleManager is enabled.

LANGUAGE: json
CODE:
{
  "localhost:8091": true
}

----------------------------------------

TITLE: Response Example - Worker Disable API
DESCRIPTION: Example JSON response when disabling a MiddleManager worker node.

LANGUAGE: json
CODE:
{"localhost:8091":"disabled"}

----------------------------------------

TITLE: Configuring HDFS Storage Properties in Druid
DESCRIPTION: Basic configuration properties for setting up HDFS as deep storage in Druid. Includes storage type, directory location, and Kerberos authentication settings.

LANGUAGE: properties
CODE:
druid.storage.type=hdfs
druid.storage.storageDirectory=/path/to/storage
druid.hadoop.security.kerberos.principal=druid@EXAMPLE.COM
druid.hadoop.security.kerberos.keytab=/etc/security/keytabs/druid.headlessUser.keytab

----------------------------------------

TITLE: Configuring Dependencies and Assembly Strategy for Apache Druid in SBT
DESCRIPTION: Defines library dependencies for Apache Druid and its extensions, including AWS SDK, Joda Time, Jackson libraries, and testing frameworks. Also configures assembly merge strategies for handling duplicate files during the build process. Excludes specific Jackson and ASM dependencies to avoid conflicts.

LANGUAGE: scala
CODE:
libraryDependencies ++= Seq(
  "com.amazonaws" % "aws-java-sdk" % "1.9.23" exclude("common-logging", "common-logging"),
  "org.joda" % "joda-convert" % "1.7",
  "joda-time" % "joda-time" % "2.7",
  "org.apache.druid" % "druid" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid" % "druid-services" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid" % "druid-indexing-service" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid" % "druid-indexing-hadoop" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid.extensions" % "mysql-metadata-storage" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid.extensions" % "druid-s3-extensions" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid.extensions" % "druid-histogram" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid.extensions" % "druid-hdfs-storage" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "com.fasterxml.jackson.core" % "jackson-annotations" % "2.3.0",
  "com.fasterxml.jackson.core" % "jackson-core" % "2.3.0",
  "com.fasterxml.jackson.core" % "jackson-databind" % "2.3.0",
  "com.fasterxml.jackson.datatype" % "jackson-datatype-guava" % "2.3.0",
  "com.fasterxml.jackson.datatype" % "jackson-datatype-joda" % "2.3.0",
  "com.fasterxml.jackson.jaxrs" % "jackson-jaxrs-base" % "2.3.0",
  "com.fasterxml.jackson.jaxrs" % "jackson-jaxrs-json-provider" % "2.3.0",
  "com.fasterxml.jackson.jaxrs" % "jackson-jaxrs-smile-provider" % "2.3.0",
  "com.fasterxml.jackson.module" % "jackson-module-jaxb-annotations" % "2.3.0",
  "com.sun.jersey" % "jersey-servlet" % "1.17.1",
  "mysql" % "mysql-connector-java" % "5.1.34",
  "org.scalatest" %% "scalatest" % "2.2.3" % "test",
  "org.mockito" % "mockito-core" % "1.10.19" % "test"
)

assemblyMergeStrategy in assembly := {
  case path if path contains "pom." => MergeStrategy.first
  case path if path contains "javax.inject.Named" => MergeStrategy.first
  case path if path contains "mime.types" => MergeStrategy.first
  case path if path contains "org/apache/commons/logging/impl/SimpleLog.class" => MergeStrategy.first
  case path if path contains "org/apache/commons/logging/impl/SimpleLog$1.class" => MergeStrategy.first
  case path if path contains "org/apache/commons/logging/impl/NoOpLog.class" => MergeStrategy.first
  case path if path contains "org/apache/commons/logging/LogFactory.class" => MergeStrategy.first
  case path if path contains "org/apache/commons/logging/LogConfigurationException.class" => MergeStrategy.first
  case path if path contains "org/apache/commons/logging/Log.class" => MergeStrategy.first
  case path if path contains "META-INF/jersey-module-version" => MergeStrategy.first
  case path if path contains ".properties" => MergeStrategy.first
  case path if path contains ".class" => MergeStrategy.first
  case x =>
    val oldStrategy = (assemblyMergeStrategy in assembly).value
    oldStrategy(x)
}

----------------------------------------

TITLE: Displaying Redis Cache Metrics in Markdown
DESCRIPTION: Markdown table showing the additional metrics reported by the Redis cache implementation in Druid, including the metric name, description, and expected values.

LANGUAGE: markdown
CODE:
|Metric|Description|Normal value|
|------|-----------|------------|
|`query/cache/redis/*/requests`|Count of requests to redis cache|whatever request to redis will increase request count by 1|

----------------------------------------

TITLE: GCS Storage Configuration Parameters
DESCRIPTION: Configuration properties for setting up Google Cloud Storage as deep storage using the HDFS extension in Druid.

LANGUAGE: yaml
CODE:
druid.storage.type: hdfs
druid.storage.storageDirectory: gs://bucket/example/directory

----------------------------------------

TITLE: Configuring HLLSketchUnion Post-Aggregator in Druid
DESCRIPTION: This JSON configuration shows how to set up the HLLSketchUnion post-aggregator. It performs a union operation on multiple HLL sketches.

LANGUAGE: json
CODE:
{
  "type"  : "HLLSketchUnion",
  "name": <output name>,
  "fields"  : <array of post aggregators that return HLL sketches>,
  "lgK": <log2 of K for the target sketch>,
  "tgtHllType" : <target HLL type>
}

----------------------------------------

TITLE: Documenting MIT License for React scheduler
DESCRIPTION: This code snippet provides license information for the React scheduler component, which is under the MIT license and created by Facebook, Inc. and its affiliates.

LANGUAGE: JavaScript
CODE:
/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Querying Added Lines by Channel and Page with Druid SQL
DESCRIPTION: This SQL query groups the sum of added lines by channel and page for a specific date range. It demonstrates multiple grouping and ordering.

LANGUAGE: sql
CODE:
SELECT channel, page, SUM(added)
FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00'
GROUP BY channel, page
ORDER BY SUM(added) DESC

----------------------------------------

TITLE: Declaring MIT License for NProgress
DESCRIPTION: Copyright notice and MIT license declaration for the NProgress library by Rico Sta. Cruz.

LANGUAGE: JavaScript
CODE:
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

----------------------------------------

TITLE: Creating Kafka Topic
DESCRIPTION: Command to create a Kafka topic named 'wikipedia' with specified partitions and replication factor.

LANGUAGE: bash
CODE:
./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic wikipedia

----------------------------------------

TITLE: Syncing Druid Configuration to Master Server
DESCRIPTION: Using rsync to copy Druid distribution and configurations to the coordination server

LANGUAGE: bash
CODE:
rsync -az apache-druid-0.14.1-incubating/ COORDINATION_SERVER:apache-druid-0.14.1-incubating/

----------------------------------------

TITLE: Configuring Client Certificate Authentication in Druid
DESCRIPTION: Configuration table showing optional parameters for client certificate authentication including keystore settings and certificate validation options.

LANGUAGE: markdown
CODE:
|Property|Description|Default|Required|
|--------|-----------|-------|--------|
|`druid.client.https.keyStorePath`|The file path or URL of the TLS/SSL Key store containing the client certificate that Druid will use when communicating with other Druid services. If this is null, the other properties in this table are ignored.|none|yes|
|`druid.client.https.keyStoreType`|The type of the key store.|none|yes|
|`druid.client.https.certAlias`|Alias of TLS client certificate in the keystore.|none|yes|
|`druid.client.https.keyStorePassword`|The [Password Provider](../../operations/password-provider.html) or String password for the Key Store.|none|no|
|`druid.client.https.keyManagerFactoryAlgorithm`|Algorithm to use for creating KeyManager, more details [here](https://docs.oracle.com/javase/7/docs/technotes/guides/security/jsse/JSSERefGuide.html#KeyManager).|`javax.net.ssl.KeyManagerFactory.getDefaultAlgorithm()`|no|
|`druid.client.https.keyManagerPassword`|The [Password Provider](../../operations/password-provider.html) or String password for the Key Manager.|none|no|
|`druid.client.https.validateHostnames`|Validate the hostname of the server. This should not be disabled unless you are using [custom TLS certificate checks](../../operations/tls-support.html#custom-tls-certificate-checks) and know that standard hostname validation is not needed.|true|no|

----------------------------------------

TITLE: Submitting Hadoop Batch Ingestion Task to Druid
DESCRIPTION: Command to submit a Hadoop-based batch ingestion task to Druid for loading Wikipedia edit data.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/wikipedia-index-hadoop.json

----------------------------------------

TITLE: Configuring General TLS Settings in Apache Druid
DESCRIPTION: These properties control the enabling/disabling of HTTP and HTTPS connectors in Apache Druid. They allow for configuring both plaintext and TLS ports simultaneously, though this is not recommended.

LANGUAGE: properties
CODE:
druid.enablePlaintextPort=true
druid.enableTlsPort=false

----------------------------------------

TITLE: Example Druid Query for Unique Users of Product A
DESCRIPTION: Complete Druid query example using Theta Sketch to count unique users who visited product A. This demonstrates how to use the Theta Sketch aggregator in a real query scenario.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "test_datasource",
  "granularity": "ALL",
  "dimensions": [],
  "aggregations": [
    { "type": "thetaSketch", "name": "unique_users", "fieldName": "user_id_sketch" }
  ],
  "filter": { "type": "selector", "dimension": "product", "value": "A" },
  "intervals": [ "2014-10-19T00:00:00.000Z/2014-10-22T00:00:00.000Z" ]
}

----------------------------------------

TITLE: Creating a JavaScript Filter in Druid
DESCRIPTION: Demonstrates how to define a JavaScript filter that matches a dimension against a specified JavaScript function predicate. The function takes the dimension value as an argument and returns true or false.

LANGUAGE: JSON
CODE:
{
  "type" : "javascript",
  "dimension" : <dimension_string>,
  "function" : "function(value) { <...> }"
}

----------------------------------------

TITLE: Moving Average Query with Post-Averager in Druid
DESCRIPTION: Illustrates a moving average query that calculates a 7-bucket moving average and a ratio between the current period and the moving average for Wikipedia edit deltas.

LANGUAGE: json
CODE:
{
  "queryType": "movingAverage",
  "dataSource": "wikipedia",
  "granularity": {
    "type": "period",
    "period": "PT30M"
  },
  "intervals": [
    "2015-09-12T22:00:00Z/2015-09-13T00:00:00Z"
  ],
  "aggregations": [
    {
      "name": "delta30Min",
      "fieldName": "delta",
      "type": "longSum"
    }
  ],
  "averagers": [
    {
      "name": "trailing30MinChanges",
      "fieldName": "delta30Min",
      "type": "longMean",
      "buckets": 7
    }
  ],
  "postAveragers" : [
    {
      "name": "ratioTrailing30MinChanges",
      "type": "arithmetic",
      "fn": "/",
      "fields": [
        {
          "type": "fieldAccess",
          "fieldName": "delta30Min"
        },
        {
          "type": "fieldAccess",
          "fieldName": "trailing30MinChanges"
        }
      ]
    }
  ]
}

----------------------------------------

TITLE: Segment Metadata JSON Structure in Apache Druid
DESCRIPTION: This JSON structure represents the payload column in the segments table of Druid's metadata storage. It contains detailed information about a segment, including its data source, interval, version, and load specification.

LANGUAGE: json
CODE:
{
 "dataSource":"wikipedia",
 "interval":"2012-05-23T00:00:00.000Z/2012-05-24T00:00:00.000Z",
 "version":"2012-05-24T00:10:00.046Z",
 "loadSpec":{
    "type":"s3_zip",
    "bucket":"bucket_for_segment",
    "key":"path/to/segment/on/s3"
 },
 "dimensions":"comma-delimited-list-of-dimension-names",
 "metrics":"comma-delimited-list-of-metric-names",
 "shardSpec":{"type":"none"},
 "binaryVersion":9,
 "size":size_of_segment,
 "identifier":"wikipedia_2012-05-23T00:00:00.000Z_2012-05-24T00:00:00.000Z_2012-05-23T00:10:00.046Z"
}

----------------------------------------

TITLE: Loading Initial Data Using Post-Index Task
DESCRIPTION: Command to load the initial Wikipedia edits sample data using a pre-defined ingestion spec.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/compaction-init-index.json

----------------------------------------

TITLE: Sending Wikipedia Sample Data to Tranquility
DESCRIPTION: Commands to decompress the sample Wikipedia data file and POST it to the Tranquility server endpoint using curl.

LANGUAGE: bash
CODE:
gunzip -k quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz 
curl -XPOST -H'Content-Type: application/json' --data-binary @quickstart/tutorial/wikiticker-2015-09-12-sampled.json http://localhost:8200/v1/post/wikipedia

----------------------------------------

TITLE: Accessing Legacy Coordinator Console URL
DESCRIPTION: The URL pattern for accessing the older version of the Coordinator Console. Replace COORDINATOR_IP and COORDINATOR_PORT with actual values.

LANGUAGE: plaintext
CODE:
http://<COORDINATOR_IP>:<COORDINATOR_PORT>/old-console

----------------------------------------

TITLE: Configuring Loading Off-heap MapDB Cache in Druid
DESCRIPTION: Example configuration for a loading-based off-heap cache using MapDB implementation with size and time-based eviction policies.

LANGUAGE: json
CODE:
{
   "type":"loadingLookup",
   "dataFetcher":{ "type":"jdbcDataFetcher", "connectorConfig":"jdbc://mysql://localhost:3306/my_data_base", "table":"lookup_table_name", "keyColumn":"key_column_name", "valueColumn": "value_column_name"},
   "loadingCacheSpec":{"type":"mapDb", "maxEntriesSize":100000},
   "reverseLoadingCacheSpec":{"type":"mapDb", "maxStoreSize":5, "expireAfterAccess":100000, "expireAfterAccess":10000}
}

----------------------------------------

TITLE: Declaring MIT License for React Is
DESCRIPTION: Copyright notice and MIT license declaration for the React Is production build.

LANGUAGE: JavaScript
CODE:
/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Configuring SSL Properties in Druid
DESCRIPTION: Configuration table showing basic SSL properties including protocol settings, trust store configuration, and certificate management options.

LANGUAGE: markdown
CODE:
|Property|Description|Default|Required|
|--------|-----------|-------|--------|
|`druid.client.https.protocol`|SSL protocol to use.|`TLSv1.2`|no|
|`druid.client.https.trustStoreType`|The type of the key store where trusted root certificates are stored.|`java.security.KeyStore.getDefaultType()`|no|
|`druid.client.https.trustStorePath`|The file path or URL of the TLS/SSL Key store where trusted root certificates are stored.|none|yes|
|`druid.client.https.trustStoreAlgorithm`|Algorithm to be used by TrustManager to validate certificate chains|`javax.net.ssl.TrustManagerFactory.getDefaultAlgorithm()`|no|
|`druid.client.https.trustStorePassword`|The [Password Provider](../../operations/password-provider.html) or String password for the Trust Store.|none|yes|

----------------------------------------

TITLE: Including DataSketches Extension in Druid Configuration
DESCRIPTION: This snippet shows how to include the DataSketches extension in the Druid configuration file. This is required to use the HLL sketch aggregator.

LANGUAGE: json
CODE:
"druid.extensions.loadList":["druid-datasketches"]

----------------------------------------

TITLE: Using Default Version with pull-deps
DESCRIPTION: Example command demonstrating how to use the --defaultVersion flag to specify a common version for multiple extensions, simplifying the coordinate specification.

LANGUAGE: bash
CODE:
java -classpath "/my/druid/lib/*" org.apache.druid.cli.Main tools pull-deps --defaultVersion 0.14.0-incubating --clean -c org.apache.druid.extensions:mysql-metadata-storage -c org.apache.druid.extensions.contrib:druid-rabbitmq -h org.apache.hadoop:hadoop-client:2.3.0 -h org.apache.hadoop:hadoop-client:2.4.0

----------------------------------------

TITLE: Hadoop Indexer Command
DESCRIPTION: Bash command for running the Hadoop indexer with a self-contained jar file and proper classpath configuration.

LANGUAGE: bash
CODE:
java -Xmx32m \
  -Dfile.encoding=UTF-8 -Duser.timezone=UTC \
  -classpath config/hadoop:config/overlord:config/_common:$SELF_CONTAINED_JAR:$HADOOP_DISTRIBUTION/etc/hadoop \
  -Djava.security.krb5.conf=$KRB5 \
  org.apache.druid.cli.Main index hadoop \
  $config_path

----------------------------------------

TITLE: NProgress License
DESCRIPTION: MIT license notice for NProgress library by Rico Sta. Cruz

LANGUAGE: javascript
CODE:
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

----------------------------------------

TITLE: GroupBy Query with Hour Granularity
DESCRIPTION: Example of a Druid GroupBy query using hourly time bucketing with count aggregation.

LANGUAGE: json
CODE:
{
   "queryType":"groupBy",
   "dataSource":"my_dataSource",
   "granularity":"hour",
   "dimensions":[
      "language"
   ],
   "aggregations":[
      {
         "type":"count",
         "name":"count"
      }
   ],
   "intervals":[
      "2000-01-01T00:00Z/3000-01-01T00:00Z"
   ]
}

----------------------------------------

TITLE: Configuring Noop Task in Druid
DESCRIPTION: A testing task configuration that sleeps for a specified duration. Used primarily for testing purposes with optional parameters for task ID, segment interval, runtime duration, and firehose testing.

LANGUAGE: json
CODE:
{
    "type": "noop",
    "id": <optional_task_id>,
    "interval" : <optional_segment_interval>,
    "runTime" : <optional_millis_to_sleep>,
    "firehose": <optional_firehose_to_test_connect>
}

----------------------------------------

TITLE: ArrayOfDoublesSketch Variances Post-Aggregator
DESCRIPTION: JSON configuration for post-aggregator that returns list of variance values from ArrayOfDoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToVariances",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>
}

----------------------------------------

TITLE: Configuring Merge Task in Apache Druid (Deprecated)
DESCRIPTION: Defines the JSON structure for a Merge task in Druid, which merges a list of segments. It allows specifying whether to rollup data during the merge and requires task ID, data source, aggregations, segments to merge, and task context.

LANGUAGE: json
CODE:
{
    "type": "merge",
    "id": <task_id>,
    "dataSource": <task_datasource>,
    "aggregations": <list of aggregators>,
    "rollup": <whether or not to rollup data during a merge>,
    "segments": <JSON list of DataSegment objects to merge>,
    "context": <task context>
}

----------------------------------------

TITLE: ResetCluster Tool Help Output
DESCRIPTION: Complete help documentation showing all available options and their descriptions for the ResetCluster tool.

LANGUAGE: text
CODE:
NAME
        druid tools reset-cluster - Cleanup all persisted state from metadata
        and deep storage.

SYNOPSIS
        druid tools reset-cluster [--all] [--hadoopWorkingPath]
                [--metadataStore] [--segmentFiles] [--taskLogs]

OPTIONS
        --all
            delete all state stored in metadata and deep storage

        --hadoopWorkingPath
            delete hadoopWorkingPath

        --metadataStore
            delete all records in metadata storage

        --segmentFiles
            delete all segment files from deep storage

        --taskLogs
            delete all tasklogs

----------------------------------------

TITLE: Configuring Merge Task in Apache Druid (Deprecated)
DESCRIPTION: Defines the JSON structure for a Merge task in Druid, which merges a list of segments. It allows specifying whether to rollup data during the merge and requires task ID, data source, aggregations, segments to merge, and task context.

LANGUAGE: json
CODE:
{
    "type": "merge",
    "id": <task_id>,
    "dataSource": <task_datasource>,
    "aggregations": <list of aggregators>,
    "rollup": <whether or not to rollup data during a merge>,
    "segments": <JSON list of DataSegment objects to merge>,
    "context": <task context>
}

----------------------------------------

TITLE: Configuring Kill Task in Apache Druid
DESCRIPTION: JSON configuration for a Kill Task that permanently deletes segment information from Druid's metadata store and deep storage. The task targets segments within a specified interval that have been marked as unused.

LANGUAGE: json
CODE:
{
    "type": "kill",
    "id": <task_id>,
    "dataSource": <task_datasource>,
    "interval" : <all_segments_in_this_interval_will_die!>,
    "context": <task context>
}

----------------------------------------

TITLE: Configuring Blacklisting Thresholds in Druid Overlord Properties
DESCRIPTION: These properties control the thresholds and timeouts for blacklisting MiddleManagers in Druid's Overlord process. They define the maximum retries before blacklisting, blacklist backoff time, cleanup period, and maximum percentage of blacklisted workers.

LANGUAGE: properties
CODE:
druid.indexer.runner.maxRetriesBeforeBlacklist
druid.indexer.runner.workerBlackListBackoffTime
druid.indexer.runner.workerBlackListCleanupPeriod
druid.indexer.runner.maxPercentageBlacklistWorkers

----------------------------------------

TITLE: Configuring White-List Converter for Ambari Metrics Emitter in Druid
DESCRIPTION: JSON configuration for the 'whiteList' event converter, which sends only white-listed metrics and dimensions to Ambari Metrics. This example demonstrates how to specify a custom map file path, set the namespace prefix, and ignore the hostname in the metric path.

LANGUAGE: json
CODE:
"druid.emitter.ambari-metrics.eventConverter={\"type\":\"whiteList\", \"namespacePrefix\": \"druid.test\", \"ignoreHostname\":true, \"appName\":\"druid\", \"mapPath\":\"/pathPrefix/fileName.json\"}"

----------------------------------------

TITLE: Histogram Post-Aggregator Configurations
DESCRIPTION: Various post-aggregator configurations for transforming histogram data including equal buckets, custom buckets, min/max, and quantile calculations.

LANGUAGE: json
CODE:
{
  "type": "equalBuckets",
  "name": "<output_name>",
  "fieldName": "<aggregator_name>",
  "numBuckets": <count>
}

LANGUAGE: json
CODE:
{
  "type": "buckets",
  "name": "<output_name>",
  "fieldName": "<aggregator_name>",
  "bucketSize": <bucket_size>,
  "offset": <offset>
}

LANGUAGE: json
CODE:
{
  "type": "customBuckets",
  "name": "<output_name>",
  "fieldName": "<aggregator_name>",
  "breaks": [<value>, <value>, ...]
}

----------------------------------------

TITLE: Enabling a MiddleManager
DESCRIPTION: Example JSON response from the /druid/worker/v1/enable endpoint to enable a MiddleManager.

LANGUAGE: json
CODE:
{"localhost:8091":"enabled"}

----------------------------------------

TITLE: Student's t-test Post-Aggregator
DESCRIPTION: Post-aggregator configuration for performing Student's t-test between two ArrayOfDoublesSketch instances.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchTTest",
  "name": <output name>,
  "fields"  : <array with two post aggregators>
}

----------------------------------------

TITLE: DefaultLimitSpec Configuration in Druid
DESCRIPTION: Defines the structure for limiting and ordering groupBy query results. Requires a limit value and an array of OrderByColumnSpec definitions to specify sorting behavior.

LANGUAGE: json
CODE:
{
    "type"    : "default",
    "limit"   : <integer_value>,
    "columns" : [list of OrderByColumnSpec],
}

----------------------------------------

TITLE: Specifying Duration Granularity in Apache Druid
DESCRIPTION: This example shows how to specify a duration granularity of 2 hours (7,200,000 milliseconds) for aggregation.

LANGUAGE: json
CODE:
{"type": "duration", "duration": 7200000}

----------------------------------------

TITLE: DefaultLimitSpec Configuration in Druid
DESCRIPTION: Defines the structure for limiting and ordering groupBy query results. Requires a limit value and an array of OrderByColumnSpec definitions to specify sorting behavior.

LANGUAGE: json
CODE:
{
    "type"    : "default",
    "limit"   : <integer_value>,
    "columns" : [list of OrderByColumnSpec],
}

----------------------------------------

TITLE: Preparing Sample Data for Ingestion
DESCRIPTION: Commands to extract the sample Wikipedia data file for ingestion into Kafka.

LANGUAGE: bash
CODE:
cd quickstart/tutorial
gunzip -k wikiticker-2015-09-12-sampled.json.gz

----------------------------------------

TITLE: Configuring OpenTSDB Metric Mapping in JSON
DESCRIPTION: JSON configuration example showing how to map Druid metrics to OpenTSDB dimensions. This defines which metrics and their associated dimensions should be sent to OpenTSDB.

LANGUAGE: json
CODE:
{
    "query/time": [
        "dataSource",
        "type"
    ]
}

----------------------------------------

TITLE: GroupBy Query with Selector Filter
DESCRIPTION: Example of a GroupBy query using a selector filter on a multi-value dimension, showing how filtering affects the result set.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "test",
  "intervals": [
    "1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"
  ],
  "filter": {
    "type": "selector",
    "dimension": "tags",
    "value": "t3"
  },
  "granularity": {
    "type": "all"
  },
  "dimensions": [
    {
      "type": "default",
      "dimension": "tags",
      "outputName": "tags"
    }
  ],
  "aggregations": [
    {
      "type": "count",
      "name": "count"
    }
  ]
}

----------------------------------------

TITLE: Retrieving Live Row Statistics JSON in Apache Druid
DESCRIPTION: Example of a live row statistics report JSON output from Druid's ingestion task. It includes moving averages and total counts for processed, unparseable, thrown away, and processed with error rows.

LANGUAGE: json
CODE:
{
  "movingAverages": {
    "buildSegments": {
      "5m": {
        "processed": 3.392158326408501,
        "unparseable": 0,
        "thrownAway": 0,
        "processedWithError": 0
      },
      "15m": {
        "processed": 1.736165476881023,
        "unparseable": 0,
        "thrownAway": 0,
        "processedWithError": 0
      },
      "1m": {
        "processed": 4.206417693750045,
        "unparseable": 0,
        "thrownAway": 0,
        "processedWithError": 0
      }
    }
  },
  "totals": {
    "buildSegments": {
      "processed": 1994,
      "processedWithError": 0,
      "thrownAway": 0,
      "unparseable": 0
    }
  }
}

----------------------------------------

TITLE: Sample JSON Output for DumpSegment Row Data in Apache Druid
DESCRIPTION: This JSON snippet illustrates the format of a single row output when using the DumpSegment tool with the default 'rows' dump option. It includes various fields such as timestamp, user actions, and geographic information.

LANGUAGE: json
CODE:
{
  "__time": 1442018818771,
  "added": 36,
  "channel": "#en.wikipedia",
  "cityName": null,
  "comment": "added project",
  "count": 1,
  "countryIsoCode": null,
  "countryName": null,
  "deleted": 0,
  "delta": 36,
  "isAnonymous": "false",
  "isMinor": "false",
  "isNew": "false",
  "isRobot": "false",
  "isUnpatrolled": "false",
  "iuser": "00001553",
  "metroCode": null,
  "namespace": "Talk",
  "page": "Talk:Oswald Tilghman",
  "regionIsoCode": null,
  "regionName": null,
  "user": "GELongstreet"
}

----------------------------------------

TITLE: Configuring Polling On-heap Lookup Cache in Druid
DESCRIPTION: Example configuration for a polling-based on-heap cache that updates every 10 minutes. Uses JDBC data fetcher to connect to MySQL database.

LANGUAGE: json
CODE:
{
    "type":"pollingLookup",
   "pollPeriod":"PT10M",
   "dataFetcher":{ "type":"jdbcDataFetcher", "connectorConfig":"jdbc://mysql://localhost:3306/my_data_base", "table":"lookup_table_name", "keyColumn":"key_column_name", "valueColumn": "value_column_name"},
   "cacheFactory":{"type":"onHeapPolling"}
}

----------------------------------------

TITLE: MIT License Notice for React DOM
DESCRIPTION: Copyright notice for React DOM production build, version 17.0.2, licensed under MIT.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: License Comments Collection
DESCRIPTION: Collection of copyright notices and MIT license declarations for various project dependencies. Includes notices for React core, React DOM, React-is, scheduler, Prism, object-assign, NProgress, and classnames packages.

LANGUAGE: javascript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

/*!
	Copyright (c) 2018 Jed Watson.
	Licensed under the MIT License (MIT), see
	http://jedwatson.github.io/classnames
*/

/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Configuring Core Extensions in Druid Properties File
DESCRIPTION: This snippet shows how to configure the common.runtime.properties file to load core extensions in Druid. It demonstrates loading the postgresql-metadata-storage and druid-hdfs-storage extensions.

LANGUAGE: properties
CODE:
druid.extensions.loadList=["postgresql-metadata-storage", "druid-hdfs-storage"]

----------------------------------------

TITLE: Executing Batch Ingestion Task in Apache Druid
DESCRIPTION: This bash command uses the post-index-task script to submit the ingestion task to Druid Overlord and monitor the ingestion process.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/wikipedia-index.json

----------------------------------------

TITLE: Executing SQL GroupBy Query in Druid
DESCRIPTION: This snippet demonstrates a SQL GroupBy query that calculates the sum of added lines per channel. It shows how to use GROUP BY and ORDER BY clauses in Druid SQL.

LANGUAGE: sql
CODE:
SELECT channel, SUM(added) FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY channel ORDER BY SUM(added) DESC LIMIT 5;

----------------------------------------

TITLE: Folder Structure Configuration Example
DESCRIPTION: Example showing recommended folder structure for organizing Druid configuration files

LANGUAGE: plaintext
CODE:
$ ls -R conf
druid       tranquility

conf/druid:
_common       broker        coordinator   historical    middleManager overlord

conf/druid/_common:
common.runtime.properties log4j2.xml

conf/druid/broker:
jvm.config         runtime.properties

conf/druid/coordinator:
jvm.config         runtime.properties

conf/druid/historical:
jvm.config         runtime.properties

conf/druid/middleManager:
jvm.config         runtime.properties

conf/druid/overlord:
jvm.config         runtime.properties

conf/tranquility:
kafka.json  server.json

----------------------------------------

TITLE: JavaScript Aggregator Configuration in Druid
DESCRIPTION: Custom JavaScript aggregator that allows defining arbitrary aggregation functions over multiple columns with combine and reset capabilities.

LANGUAGE: json
CODE:
{
  "type": "javascript",
  "name": "sum(log(x)*y) + 10",
  "fieldNames": ["x", "y"],
  "fnAggregate" : "function(current, a, b)      { return current + (Math.log(a) * b); }",
  "fnCombine"   : "function(partialA, partialB) { return partialA + partialB; }",
  "fnReset"     : "function()                   { return 10; }"
}

----------------------------------------

TITLE: Example Druid Query Using Bloom Filter Aggregator
DESCRIPTION: This JSON snippet provides a complete example of a Druid timeseries query using a Bloom filter aggregator, including query parameters and expected response format.

LANGUAGE: json
CODE:
{
  "queryType": "timeseries",
  "dataSource": "wikiticker",
  "intervals": [ "2015-09-12T00:00:00.000/2015-09-13T00:00:00.000" ],
  "granularity": "day",
  "aggregations": [
    {
      "type": "bloom",
      "name": "userBloom",
      "maxNumEntries": 100000,
      "field": {
        "type":"default",
        "dimension":"user",
        "outputType": "STRING"
      }
    }
  ]
}

----------------------------------------

TITLE: Demonstrating Multi-value Column Storage
DESCRIPTION: Illustrates how Druid stores multi-value columns using the same three data structures but with modified column data and bitmap entries.

LANGUAGE: plaintext
CODE:
1: Dictionary that encodes column values
  {
    "Justin Bieber": 0,
    "Ke$ha":         1
  }

2: Column data
  [0,
   [0,1],  <--Row value of multi-value column can have array of values
   1,
   1]

3: Bitmaps - one for each unique value
  value="Justin Bieber": [1,1,0,0]
  value="Ke$ha":         [0,1,1,1]
                            ^
                            |
                            |
    Multi-value column has multiple non-zero entries

----------------------------------------

TITLE: Configuring Kafka Eight Firehose in Apache Druid
DESCRIPTION: This JSON snippet demonstrates how to configure the Kafka Eight Firehose in Apache Druid. It specifies the firehose type, consumer properties for Zookeeper connection and Kafka consumer settings, and the Kafka topic to consume from.

LANGUAGE: json
CODE:
{
  "firehose": {
    "type": "kafka-0.8",
    "consumerProps": {
      "zookeeper.connect": "localhost:2181",
      "zookeeper.connection.timeout.ms" : "15000",
      "zookeeper.session.timeout.ms" : "15000",
      "zookeeper.sync.time.ms" : "5000",
      "group.id": "druid-example",
      "fetch.message.max.bytes" : "1048586",
      "auto.offset.reset": "largest",
      "auto.commit.enable": "false"
    },
    "feed": "wikipedia"
  }
}

----------------------------------------

TITLE: Implementing SQL-like CASE Expression in Druid
DESCRIPTION: Examples of CASE expression syntax in Druid showing both searched and simple case formats. These expressions allow conditional logic similar to SQL CASE statements.

LANGUAGE: expression
CODE:
case_searched(expr1, result1, [[expr2, result2, ...], else-result])
case_simple(expr, value1, result1, [[value2, result2, ...], else-result])

----------------------------------------

TITLE: JVM Configuration Settings
DESCRIPTION: JVM configuration parameters for running the Router process in production

LANGUAGE: shell
CODE:
-server
-Xmx13g
-Xms13g
-XX:NewSize=256m
-XX:MaxNewSize=256m
-XX:+UseConcMarkSweepGC
-XX:+PrintGCDetails
-XX:+PrintGCTimeStamps
-XX:+UseLargePages
-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/mnt/galaxy/deploy/current/
-Duser.timezone=UTC
-Dfile.encoding=UTF-8
-Djava.io.tmpdir=/mnt/tmp

-Dcom.sun.management.jmxremote.port=17071
-Dcom.sun.management.jmxremote.authenticate=false
-Dcom.sun.management.jmxremote.ssl=false

----------------------------------------

TITLE: Sending Wikipedia Data to Tranquility Server
DESCRIPTION: Commands to decompress and send sample Wikipedia edit data to Tranquility Server via HTTP POST request.

LANGUAGE: bash
CODE:
gunzip -k quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz 
curl -XPOST -H'Content-Type: application/json' --data-binary @quickstart/tutorial/wikiticker-2015-09-12-sampled.json http://localhost:8200/v1/post/wikipedia

----------------------------------------

TITLE: Sample JSON Output for Data Dump
DESCRIPTION: This snippet shows an example of the JSON output format when dumping rows from a segment. It includes various fields such as timestamp, metrics, and dimensions.

LANGUAGE: json
CODE:
{
  "__time": 1442018818771,
  "added": 36,
  "channel": "#en.wikipedia",
  "cityName": null,
  "comment": "added project",
  "count": 1,
  "countryIsoCode": null,
  "countryName": null,
  "deleted": 0,
  "delta": 36,
  "isAnonymous": "false",
  "isMinor": "false",
  "isNew": "false",
  "isRobot": "false",
  "isUnpatrolled": "false",
  "iuser": "00001553",
  "metroCode": null,
  "namespace": "Talk",
  "page": "Talk:Oswald Tilghman",
  "regionIsoCode": null,
  "regionName": null,
  "user": "GELongstreet"
}

----------------------------------------

TITLE: Executing Scan SQL Query in Druid
DESCRIPTION: This snippet demonstrates a Scan SQL query in Druid, which retrieves raw data for a specific time range without any aggregation.

LANGUAGE: sql
CODE:
SELECT user, page FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 02:00:00' AND TIMESTAMP '2015-09-12 03:00:00' LIMIT 5;

----------------------------------------

TITLE: Configuring Timed Shutoff Firehose in Druid
DESCRIPTION: Configuration for a firehose that automatically shuts down at a specified time. Wraps another firehose delegate.

LANGUAGE: json
CODE:
{
    "type"  :   "timed",
    "shutoffTime": "2015-08-25T01:26:05.119Z",
    "delegate": {
          "type": "receiver",
          "serviceName": "eventReceiverServiceName",
          "bufferSize": 100000
     }
}

----------------------------------------

TITLE: Registering Query-related Classes in Druid Module
DESCRIPTION: Example of registering custom QueryToolChest and QueryRunnerFactory implementations for a new query type in a Druid module.

LANGUAGE: Java
CODE:
DruidBinders.queryToolChestBinder(binder)
            .addBinding(SegmentMetadataQuery.class)
            .to(SegmentMetadataQueryQueryToolChest.class);
    
DruidBinders.queryRunnerFactoryBinder(binder)
            .addBinding(SegmentMetadataQuery.class)
            .to(SegmentMetadataQueryRunnerFactory.class);

----------------------------------------

TITLE: Configuring Globally Cached Lookup with URI Namespace
DESCRIPTION: Example configuration for a globally cached lookup using a URI namespace. It specifies a file source, CSV format, and polling period for updates.

LANGUAGE: json
CODE:
{
   "type": "cachedNamespace",
   "extractionNamespace": {
      "type": "uri",
      "uri": "file:/tmp/prefix/",
      "namespaceParseSpec": {
        "format": "csv",
        "columns": [
          "key",
          "value"
        ]
      },
      "pollPeriod": "PT5M"
    },
    "firstCacheTimeout": 0
}

----------------------------------------

TITLE: First/Last String Aggregators in Druid
DESCRIPTION: String aggregators that compute first/last values based on timestamp ordering with optional maxStringBytes and null filtering configurations.

LANGUAGE: json
CODE:
{
  "type" : "stringFirst",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "maxStringBytes" : <integer>,
  "filterNullValues" : <boolean>
}

LANGUAGE: json
CODE:
{
  "type" : "stringLast",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "maxStringBytes" : <integer>,
  "filterNullValues" : <boolean>
}

----------------------------------------

TITLE: Object Assign License Header
DESCRIPTION: MIT license header for object-assign library by Sindre Sorhus

LANGUAGE: javascript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

----------------------------------------

TITLE: Loading Batch Data
DESCRIPTION: Command to submit the ingestion task for loading Wikipedia sample data.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/wikipedia-index-hadoop.json

----------------------------------------

TITLE: Example Druid Query Using Bloom Filter Aggregator
DESCRIPTION: This JSON snippet provides a complete example of a Druid timeseries query using a Bloom filter aggregator, including query parameters and expected response format.

LANGUAGE: json
CODE:
{
  "queryType": "timeseries",
  "dataSource": "wikiticker",
  "intervals": [ "2015-09-12T00:00:00.000/2015-09-13T00:00:00.000" ],
  "granularity": "day",
  "aggregations": [
    {
      "type": "bloom",
      "name": "userBloom",
      "maxNumEntries": 100000,
      "field": {
        "type":"default",
        "dimension":"user",
        "outputType": "STRING"
      }
    }
  ]
}

----------------------------------------

TITLE: Configuring Kafka Emitter Properties in Apache Druid
DESCRIPTION: This snippet demonstrates how to configure the Kafka Emitter extension in Apache Druid. It specifies the Kafka bootstrap servers, topic names for metrics and alerts, additional producer configurations, and an optional cluster name.

LANGUAGE: properties
CODE:
druid.emitter.kafka.bootstrap.servers=hostname1:9092,hostname2:9092
druid.emitter.kafka.metric.topic=druid-metric
druid.emitter.kafka.alert.topic=druid-alert
druid.emitter.kafka.producer.config={"max.block.ms":10000}

----------------------------------------

TITLE: Accessing Druid Console URL
DESCRIPTION: The URL pattern for accessing the Druid Console hosted by the Router process. Replace ROUTER_IP and ROUTER_PORT with actual values.

LANGUAGE: plaintext
CODE:
http://<ROUTER_IP>:<ROUTER_PORT>

----------------------------------------

TITLE: Implementing SQL-like CASE Expression in Druid
DESCRIPTION: Examples of CASE expression syntax in Druid showing both searched and simple case formats. These expressions allow conditional logic similar to SQL CASE statements.

LANGUAGE: expression
CODE:
case_searched(expr1, result1, [[expr2, result2, ...], else-result])
case_simple(expr, value1, result1, [[value2, result2, ...], else-result])

----------------------------------------

TITLE: Configuring Kafka Eight Firehose in Apache Druid
DESCRIPTION: This JSON snippet demonstrates how to configure the Kafka Eight Firehose in Apache Druid. It specifies the firehose type, consumer properties for Zookeeper connection and Kafka consumer settings, and the Kafka topic to consume from.

LANGUAGE: json
CODE:
{
  "firehose": {
    "type": "kafka-0.8",
    "consumerProps": {
      "zookeeper.connect": "localhost:2181",
      "zookeeper.connection.timeout.ms" : "15000",
      "zookeeper.session.timeout.ms" : "15000",
      "zookeeper.sync.time.ms" : "5000",
      "group.id": "druid-example",
      "fetch.message.max.bytes" : "1048586",
      "auto.offset.reset": "largest",
      "auto.commit.enable": "false"
    },
    "feed": "wikipedia"
  }
}

----------------------------------------

TITLE: Implementing SQL-like CASE Expression in Druid
DESCRIPTION: Examples of CASE expression syntax in Druid showing both searched and simple case formats. These expressions allow conditional logic similar to SQL CASE statements.

LANGUAGE: expression
CODE:
case_searched(expr1, result1, [[expr2, result2, ...], else-result])
case_simple(expr, value1, result1, [[value2, result2, ...], else-result])

----------------------------------------

TITLE: Configuring FloatMax Aggregator in Druid JSON
DESCRIPTION: Defines a floatMax aggregator to compute the maximum of all metric values and Float.NEGATIVE_INFINITY.

LANGUAGE: JSON
CODE:
{ "type" : "floatMax", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: Copyright Notice for React Is
DESCRIPTION: Copyright notice for the React Is production build (v16.13.1), released under the MIT license by Facebook, Inc. and its affiliates.

LANGUAGE: JavaScript
CODE:
/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Enabling Experimental Features in Druid Configuration
DESCRIPTION: This snippet demonstrates how to enable experimental features in Apache Druid by adding them to the loadList in the runtime.properties configuration file. In this example, the druid-histogram feature is enabled.

LANGUAGE: properties
CODE:
druid.extensions.loadList=["druid-histogram"]

----------------------------------------

TITLE: Registering Jersey Resource in Druid Module
DESCRIPTION: Example of how to bind a new Jersey resource in a Druid module. This is used to add new REST endpoints to Druid.

LANGUAGE: Java
CODE:
Jerseys.addResource(binder, NewResource.class);

----------------------------------------

TITLE: Executing Time Boundary Query in Apache Druid
DESCRIPTION: Query structure for retrieving temporal boundaries of a dataset in Druid. Supports optional filtering and the ability to request either minimum time, maximum time, or both timestamps. The query returns timestamps in ISO-8601 format.

LANGUAGE: json
CODE:
{
    "queryType" : "timeBoundary",
    "dataSource": "sample_datasource",
    "bound"     : < "maxTime" | "minTime" > # optional, defaults to returning both timestamps if not set 
    "filter"    : { "type": "and", "fields": [<filter>, <filter>, ...] } # optional
}

----------------------------------------

TITLE: Submitting Ingestion Task in Druid
DESCRIPTION: Command to submit an ingestion task specification that creates hourly segments from Wikipedia edits sample data. The spec file creates a datasource named 'retention-tutorial'.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/retention-index.json

----------------------------------------

TITLE: Enabling Experimental Features in Druid Configuration
DESCRIPTION: This snippet demonstrates how to enable experimental features in Apache Druid by adding them to the loadList in the runtime.properties configuration file. In this example, the druid-histogram feature is enabled.

LANGUAGE: properties
CODE:
druid.extensions.loadList=["druid-histogram"]

----------------------------------------

TITLE: Defining Query Data Source in Apache Druid JSON
DESCRIPTION: Illustrates how to define a query data source in Apache Druid using JSON. Query data sources are used for nested groupBys and are currently only supported for groupBy queries. They allow for subquery-like functionality.

LANGUAGE: json
CODE:
{
	"type": "query",
	"query": {
		"type": "groupBy",
		...
	}
}

----------------------------------------

TITLE: Quantile Post-Aggregator Configurations
DESCRIPTION: Configurations for single quantile and multiple quantiles post-aggregators to compute percentile values from histograms.

LANGUAGE: json
CODE:
{ "type" : "quantile", "name" : <output_name>, "fieldName" : <aggregator_name>,
  "probability" : <quantile> }
{ "type" : "quantiles", "name" : <output_name>, "fieldName" : <aggregator_name>,
  "probabilities" : [ <quantile>, <quantile>, ... ] }

----------------------------------------

TITLE: Configuring Interval Load Rule in Apache Druid
DESCRIPTION: JSON configuration for an interval load rule that specifies how many replicas of a segment should exist in different server tiers for a specific time interval.

LANGUAGE: json
CODE:
{
  "type" : "loadByInterval",
  "interval": "2012-01-01/2013-01-01",
  "tieredReplicants": {
    "hot": 1,
    "_default_tier" : 1
  }
}

----------------------------------------

TITLE: Pseudocode for Druid Rollup Operation
DESCRIPTION: Represents the rollup operation in Druid as SQL-like pseudocode, grouping by truncated timestamp and dimensions, and summing metrics.

LANGUAGE: sql
CODE:
GROUP BY TRUNCATE(timestamp, MINUTE), srcIP, dstIP :: SUM(packets), SUM(bytes)

----------------------------------------

TITLE: Configuring Approximate Histogram Aggregator in Druid
DESCRIPTION: Configuration schema for setting up approximate histogram aggregators during ingestion and query time. Includes parameters for controlling resolution, bucket count, and value limits.

LANGUAGE: json
CODE:
{
  "type" : "approxHistogram or approxHistogramFold (at ingestion time), approxHistogramFold (at query time)",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "resolution" : <integer>,
  "numBuckets" : <integer>,
  "lowerLimit" : <float>,
  "upperLimit" : <float>
}

----------------------------------------

TITLE: Configuring Parquet-Avro Parser with Avro ParseSpec in Druid
DESCRIPTION: This JSON configuration illustrates how to set up the Parquet-Avro parser with an Avro parseSpec in Druid. It includes settings for the Avro input format, flattenSpec for nested data handling, and timestamp and dimension specifications.

LANGUAGE: json
CODE:
{
  "type": "index_hadoop",
  "spec": {
    "ioConfig": {
      "type": "hadoop",
      "inputSpec": {
        "type": "static",
        "inputFormat": "org.apache.druid.data.input.parquet.DruidParquetAvroInputFormat",
        "paths": "path/to/file.parquet"
      },
      ...
    },
    "dataSchema": {
      "dataSource": "example",
      "parser": {
        "type": "parquet-avro",
        "parseSpec": {
          "format": "avro",
          "flattenSpec": {
            "useFieldDiscovery": true,
            "fields": [
              {
                "type": "path",
                "name": "nestedDim",
                "expr": "$.nestedData.dim1"
              },
              {
                "type": "path",
                "name": "listDimFirstItem",
                "expr": "$.listDim[1]"
              }
            ]
          },
          "timestampSpec": {
            "column": "timestamp",
            "format": "auto"
          },
          "dimensionsSpec": {
            "dimensions": [],
            "dimensionExclusions": [],
            "spatialDimensions": []
          }
        }
      },
      ...
    },
    "tuningConfig": <hadoop-tuning-config>
    }
  }
}

----------------------------------------

TITLE: Building Apache Druid with Maven in Bash
DESCRIPTION: Basic Maven command to build Druid from source, running static analysis, unit tests, compiling classes, and packaging projects into JARs.

LANGUAGE: bash
CODE:
mvn clean install

----------------------------------------

TITLE: Declaring MIT License for Prism
DESCRIPTION: Copyright notice and MIT license declaration for the Prism syntax highlighting library by Lea Verou.

LANGUAGE: JavaScript
CODE:
/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

----------------------------------------

TITLE: Segment Announcement Path in ZooKeeper
DESCRIPTION: The ZooKeeper path where Historical and Realtime processes announce their presence using ephemeral znodes.

LANGUAGE: plaintext
CODE:
${druid.zk.paths.announcementsPath}/${druid.host}

----------------------------------------

TITLE: Starting Druid Broker Server
DESCRIPTION: Command to start the Druid Broker server process. The Broker is responsible for query routing and result merging in a distributed Druid cluster.

LANGUAGE: java
CODE:
org.apache.druid.cli.Main server broker

----------------------------------------

TITLE: Copyright and License Notice for React Scheduler
DESCRIPTION: This snippet contains the copyright and license information for the React Scheduler package, which is released under the MIT license.

LANGUAGE: JavaScript
CODE:
/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Starting Druid Historical and MiddleManager
DESCRIPTION: Commands to start the Druid Historical and MiddleManager processes on Data servers.

LANGUAGE: bash
CODE:
java `cat conf/druid/historical/jvm.config | xargs` -cp conf/druid/_common:conf/druid/historical:lib/* org.apache.druid.cli.Main server historical
java `cat conf/druid/middleManager/jvm.config | xargs` -cp conf/druid/_common:conf/druid/middleManager:lib/* org.apache.druid.cli.Main server middleManager

----------------------------------------

TITLE: Equal Buckets Post-Aggregator Configuration
DESCRIPTION: Configuration for the equal buckets post-aggregator that creates histogram representation with equal-sized bins.

LANGUAGE: json
CODE:
{
  "type": "equalBuckets",
  "name": "<output_name>",
  "fieldName": "<aggregator_name>",
  "numBuckets": <count>
}

----------------------------------------

TITLE: Executing SQL Timeseries Query in Apache Druid
DESCRIPTION: This snippet shows a SQL Timeseries query that calculates the sum of deleted lines per hour. It demonstrates how to use time floor functions and aggregations in Druid SQL.

LANGUAGE: sql
CODE:
SELECT FLOOR(__time to HOUR) AS HourTime, SUM(deleted) AS LinesDeleted FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY FLOOR(__time to HOUR);

----------------------------------------

TITLE: Configuring Detailed Numeric TopNMetricSpec in Druid JSON
DESCRIPTION: Shows the JSON object representation of a numeric TopNMetricSpec, specifying the type and metric name for sorting topN results.

LANGUAGE: json
CODE:
"metric": {
    "type": "numeric",
    "metric": "<metric_name>"
}

----------------------------------------

TITLE: Configuring Period Drop Rule in Druid
DESCRIPTION: JSON configuration for a Period Drop Rule that removes segments based on a time period including future data handling.

LANGUAGE: json
CODE:
{
  "type" : "dropByPeriod",
  "period" : "P1M",
  "includeFuture" : true
}

----------------------------------------

TITLE: Configuring Hadoop InputSpec for Druid DataSource Reindexing
DESCRIPTION: This JSON snippet demonstrates how to configure the 'inputSpec' in the 'ioConfig' for reindexing data from an existing Druid dataSource using Hadoop batch ingestion. It specifies the dataSource name and time intervals to be processed.

LANGUAGE: json
CODE:
"ioConfig" : {
  "type" : "hadoop",
  "inputSpec" : {
    "type" : "dataSource",
    "ingestionSpec" : {
      "dataSource": "wikipedia",
      "intervals": ["2014-10-20T00:00:00Z/P2W"]
    }
  },
  ...
}

----------------------------------------

TITLE: Submitting Initial Ingestion Task in Apache Druid
DESCRIPTION: This command submits an ingestion task to create the initial datasource with multiple small segments per hour.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/compaction-init-index.json --url http://localhost:8081

----------------------------------------

TITLE: Configuring RabbitMQ Firehose in Druid
DESCRIPTION: JSON configuration specification for setting up a RabbitMQ firehose in Druid. Includes connection parameters, queue settings, and retry configurations for reliable message ingestion.

LANGUAGE: json
CODE:
{
   "type" : "rabbitmq",
   "connection" : {
     "host": "localhost",
     "port": "5672",
     "username": "test-dude",
     "password": "test-word",
     "virtualHost": "test-vhost",
     "uri": "amqp://mqserver:1234/vhost"
   },
   "config" : {
     "exchange": "test-exchange",
     "queue" : "druidtest",
     "routingKey": "#",
     "durable": "true",
     "exclusive": "false",
     "autoDelete": "false",
     "maxRetries": "10",
     "retryIntervalSeconds": "1",
     "maxDurationSeconds": "300"
   }
}

----------------------------------------

TITLE: Configuring Segment Ingestion Firehose in Druid
DESCRIPTION: Configuration for reading data from existing Druid segments. Allows reingesting data with new schema modifications.

LANGUAGE: json
CODE:
{
    "type"    : "ingestSegment",
    "dataSource"   : "wikipedia",
    "interval" : "2013-01-01/2013-01-02"
}

----------------------------------------

TITLE: Implementing Regex Search Query in Druid
DESCRIPTION: Specifies a regex-based search query that matches if any part of a dimension value matches the given regex pattern. Allows for complex pattern matching using regular expressions.

LANGUAGE: json
CODE:
{
  "type"  : "regex",
  "pattern" : "some_pattern"
}

----------------------------------------

TITLE: Configuring P-Value Calculation in Druid
DESCRIPTION: Configuration for calculating two-tailed p-value from a z-score. Takes a z-score input to compute the statistical significance.

LANGUAGE: json
CODE:
{
  "type": "pvalue2tailedZtest",
  "name": "<output_name>",
  "zScore": <zscore post_aggregator>
}

----------------------------------------

TITLE: Setting Task Priority in Apache Druid JSON Configuration
DESCRIPTION: This JSON snippet demonstrates how to override the default task priority by setting a custom priority value in the task context. The priority is set to 100, which is higher than the default priorities for most task types.

LANGUAGE: json
CODE:
{
  "context" : {
    "priority" : 100
  }
}

----------------------------------------

TITLE: Configuring Period Broadcast Rule in Apache Druid
DESCRIPTION: This JSON snippet defines a Period Broadcast Rule, which specifies how segments of different data sources should be co-located in Historical processes for a specific time period.

LANGUAGE: json
CODE:
{
  "type" : "broadcastByPeriod",
  "colocatedDataSources" : [ "target_source1", "target_source2" ],
  "period" : "P1M",
  "includeFuture" : true
}

----------------------------------------

TITLE: Configuring Kerberos Authenticator in Apache Druid YAML
DESCRIPTION: YAML configuration for adding a Kerberos authenticator to the authenticator chain in Apache Druid. This snippet shows how to define a Kerberos authenticator named 'MyKerberosAuthenticator'.

LANGUAGE: yaml
CODE:
druid.auth.authenticatorChain=["MyKerberosAuthenticator"]

druid.auth.authenticator.MyKerberosAuthenticator.type=kerberos

----------------------------------------

TITLE: Appending Data in Apache Druid
DESCRIPTION: This command appends new data to the existing 'updates-tutorial' datasource without overwriting.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/updates-append-index2.json --url http://localhost:8081

----------------------------------------

TITLE: Configuring Period Drop Rule in Druid
DESCRIPTION: JSON configuration for a Period Drop Rule that removes segments based on a time period including future data handling.

LANGUAGE: json
CODE:
{
  "type" : "dropByPeriod",
  "period" : "P1M",
  "includeFuture" : true
}

----------------------------------------

TITLE: Installing ZooKeeper Dependency
DESCRIPTION: Commands to download and set up Apache ZooKeeper, which is required for Druid's distributed coordination.

LANGUAGE: bash
CODE:
curl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz\ntar -xzf zookeeper-3.4.11.tar.gz\nmv zookeeper-3.4.11 zk

----------------------------------------

TITLE: Period Granularity Configuration
DESCRIPTION: Examples of period-based granularity configuration in Druid with timezone and origin specifications.

LANGUAGE: javascript
CODE:
{"type": "period", "period": "P2D", "timeZone": "America/Los_Angeles"}

{"type": "period", "period": "P3M", "timeZone": "America/Los_Angeles", "origin": "2012-02-01T00:00:00-08:00"}

----------------------------------------

TITLE: Running Druid Coordinator Server
DESCRIPTION: Command to start the Druid Coordinator server process using the Main class.

LANGUAGE: java
CODE:
org.apache.druid.cli.Main server coordinator

----------------------------------------

TITLE: Expression Transform Example in Druid
DESCRIPTION: Example of an expression transform that prepends 'foo' to values in a page column and creates a new fooPage column.

LANGUAGE: json
CODE:
{
      "type": "expression",
      "name": "fooPage",
      "expression": "concat('foo' + page)"
    }

----------------------------------------

TITLE: Loading DataSketches Extension Configuration in Druid
DESCRIPTION: Configuration snippet showing how to include the DataSketches extension in Druid's config file.

LANGUAGE: json
CODE:
"druid.extensions.loadList=[\"druid-datasketches\"]"

----------------------------------------

TITLE: Declaring MIT License for React DOM
DESCRIPTION: This comment block declares the MIT license for React DOM's production build.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Segment Serving Path in ZooKeeper
DESCRIPTION: Permanent znode path where processes register their served segments.

LANGUAGE: plaintext
CODE:
${druid.zk.paths.servedSegmentsPath}/${druid.host}

----------------------------------------

TITLE: Registering Password Provider in Java
DESCRIPTION: Example of how to register a custom PasswordProvider implementation as a Jackson subtype in a Druid module.

LANGUAGE: Java
CODE:
return ImmutableList.of(
    new SimpleModule("SomePasswordProviderModule")
        .registerSubtypes(
            new NamedType(SomePasswordProvider.class, "some")
        )
);

----------------------------------------

TITLE: License Information for React
DESCRIPTION: Copyright notice and MIT license information for React production build.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Custom Password Provider Configuration Template in Druid
DESCRIPTION: Template for configuring a custom password provider implementation in Druid. Shows the basic structure for defining a custom password provider with type and additional properties.

LANGUAGE: json
CODE:
{ "type": "<registered_password_provider_name>", "<jackson_property>": "<value>", ... }

----------------------------------------

TITLE: Extracting Druid Installation Files
DESCRIPTION: Commands to extract the downloaded Druid binary package and navigate to its directory.

LANGUAGE: bash
CODE:
tar -xzf apache-druid-0.14.2-incubating-bin.tar.gz
cd apache-druid-0.14.2-incubating

----------------------------------------

TITLE: Executing Original Granularity Compaction
DESCRIPTION: Command to execute the compaction task that maintains original granularity.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/compaction-keep-granularity.json

----------------------------------------

TITLE: Executing Original Granularity Compaction
DESCRIPTION: Command to execute the compaction task that maintains original granularity.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/compaction-keep-granularity.json

----------------------------------------

TITLE: First Query for Approximate TopN in Apache Druid
DESCRIPTION: This snippet demonstrates the first query in a two-query approach for getting exact aggregates with approximate rank in high-cardinality dimensions. It retrieves the approximate top N dimension values.

LANGUAGE: json
CODE:
{
    "aggregations": [
             {
                 "fieldName": "L_QUANTITY_longSum",
                 "name": "L_QUANTITY_",
                 "type": "longSum"
             }
    ],
    "dataSource": "tpch_year",
    "dimension":"l_orderkey",
    "granularity": "all",
    "intervals": [
        "1900-01-09T00:00:00.000Z/2992-01-10T00:00:00.000Z"
    ],
    "metric": "L_QUANTITY_",
    "queryType": "topN",
    "threshold": 2
}

----------------------------------------

TITLE: Configuring HLLSketchToString Post-Aggregator in Druid
DESCRIPTION: This JSON configuration defines the HLLSketchToString post-aggregator for generating a human-readable sketch summary for debugging.

LANGUAGE: json
CODE:
{
  "type"  : "HLLSketchToString",
  "name": <output name>,
  "field"  : <post aggregator that returns an HLL Sketch>
}

----------------------------------------

TITLE: Configuring JavaScript parseSpec for Druid Ingestion
DESCRIPTION: JSON configuration for the parseSpec section of a Druid ingestion task using JavaScript parsing. Specifies the format, timestamp, dimensions, and JavaScript function for parsing.

LANGUAGE: json
CODE:
{
  "parseSpec":{
    "format" : "javascript",
    "timestampSpec" : {
      "column" : "timestamp"
    },        
    "dimensionsSpec" : {
      "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"]
    },
    "function" : "function(str) { var parts = str.split(\"-\"); return { one: parts[0], two: parts[1] } }"
  }
}

----------------------------------------

TITLE: TopN Query Result Format
DESCRIPTION: Example of the JSON response format for a TopN query, showing timestamp and results array containing dimension values and their associated metrics.

LANGUAGE: json
CODE:
[
  {
    "timestamp": "2013-08-31T00:00:00.000Z",
    "result": [
      {
        "dim1": "dim1_val",
        "count": 111,
        "some_metrics": 10669,
        "average": 96.11711711711712
      },
      {
        "dim1": "another_dim1_val",
        "count": 88,
        "some_metrics": 28344,
        "average": 322.09090909090907
      },
      {
        "dim1": "dim1_val3",
        "count": 70,
        "some_metrics": 871,
        "average": 12.442857142857143
      },
      {
        "dim1": "dim1_val4",
        "count": 62,
        "some_metrics": 815,
        "average": 13.14516129032258
      },
      {
        "dim1": "dim1_val5",
        "count": 60,
        "some_metrics": 2787,
        "average": 46.45
      }
    ]
  }
]

----------------------------------------

TITLE: Aggregating Segment Information by Datasource
DESCRIPTION: SQL query example to retrieve aggregated segment information (total size, average size, average number of rows, and number of segments) per datasource.

LANGUAGE: sql
CODE:
SELECT
    datasource,
    SUM("size") AS total_size,
    CASE WHEN SUM("size") = 0 THEN 0 ELSE SUM("size") / (COUNT(*) FILTER(WHERE "size" > 0)) END AS avg_size,
    CASE WHEN SUM(num_rows) = 0 THEN 0 ELSE SUM("num_rows") / (COUNT(*) FILTER(WHERE num_rows > 0)) END AS avg_num_rows,
    COUNT(*) AS num_segments
FROM sys.segments
GROUP BY 1
ORDER BY 2 DESC

----------------------------------------

TITLE: Configuring Kafka Supervisor for Protobuf Ingestion
DESCRIPTION: This JSON configuration sets up a Kafka supervisor for ingesting Protobuf data. It specifies the data schema, parser configuration, and Kafka connection details.

LANGUAGE: json
CODE:
{
  "type": "kafka",
  "dataSchema": {
    "dataSource": "metrics-kafka2",
    "parser": {
      "type": "protobuf",
      "descriptor": "file:///tmp/metrics.desc",
      "protoMessageType": "Metrics",
      "parseSpec": {
        "format": "json",
        "timestampSpec": {
          "column": "timestamp",
          "format": "auto"
        },
        "dimensionsSpec": {
          "dimensions": [
            "unit",
            "http_method",
            "http_code",
            "page",
            "metricType",
            "server"
          ],
          "dimensionExclusions": [
            "timestamp",
            "value"
          ]
        }
      }
    },
    "metricsSpec": [
      {
        "name": "count",
        "type": "count"
      },
      {
        "name": "value_sum",
        "fieldName": "value",
        "type": "doubleSum"
      },
      {
        "name": "value_min",
        "fieldName": "value",
        "type": "doubleMin"
      },
      {
        "name": "value_max",
        "fieldName": "value",
        "type": "doubleMax"
      }
    ],
    "granularitySpec": {
      "type": "uniform",
      "segmentGranularity": "HOUR",
      "queryGranularity": "NONE"
    }
  },
  "tuningConfig": {
    "type": "kafka",
    "maxRowsPerSegment": 5000000
  },
  "ioConfig": {
    "topic": "metrics_pb",
    "consumerProperties": {
      "bootstrap.servers": "localhost:9092"
    },
    "taskCount": 1,
    "replicas": 1,
    "taskDuration": "PT1H"
  }
}

----------------------------------------

TITLE: Basic Transform Spec Structure in Druid
DESCRIPTION: Basic structure of a transform specification showing the main components: transforms list and filter.

LANGUAGE: json
CODE:
"transformSpec": {
  "transforms: <List of transforms>,
  "filter": <filter>
}

----------------------------------------

TITLE: Configuring Druid Extension for DataSketches
DESCRIPTION: JSON configuration to include the DataSketches extension in Druid. This needs to be added to the Druid config file to enable the module.

LANGUAGE: json
CODE:
"druid.extensions.loadList":["druid-datasketches"]

----------------------------------------

TITLE: Configuring Served Segments Path in ZooKeeper
DESCRIPTION: Permanent ZooKeeper path where nodes create znodes to indicate which segments they are serving.

LANGUAGE: plaintext
CODE:
${druid.zk.paths.servedSegmentsPath}/${druid.host}

----------------------------------------

TITLE: js-yaml License Declaration
DESCRIPTION: License declaration for js-yaml library version 4.1.0, released under MIT license.

LANGUAGE: javascript
CODE:
/*! js-yaml 4.1.0 https://github.com/nodeca/js-yaml @license MIT */

----------------------------------------

TITLE: Multi-Interval Segment Example - Version 1
DESCRIPTION: Illustrates segment naming across multiple time intervals with version 1.

LANGUAGE: plaintext
CODE:
foo_2015-01-01/2015-01-02_v1_0
foo_2015-01-02/2015-01-03_v1_1
foo_2015-01-03/2015-01-04_v1_2

----------------------------------------

TITLE: Multi-Interval Segment Example - Version 1
DESCRIPTION: Illustrates segment naming across multiple time intervals with version 1.

LANGUAGE: plaintext
CODE:
foo_2015-01-01/2015-01-02_v1_0
foo_2015-01-02/2015-01-03_v1_1
foo_2015-01-03/2015-01-04_v1_2

----------------------------------------

TITLE: Query Filter Having Specification in Druid
DESCRIPTION: Demonstrates how to use Druid query filters in the Having clause of a groupBy query. Allows any Druid query filter to be used for filtering results.

LANGUAGE: json
CODE:
{
    "queryType": "groupBy",
    "dataSource": "sample_datasource",
    ...
    "having": 
        {
            "type" : "filter",
            "filter" : <any Druid query filter>
        }
}

----------------------------------------

TITLE: Configuring TwitterSpritzerFirehose in Druid
DESCRIPTION: JSON configuration specification for connecting to Twitter's spritzer data stream. Includes parameters for controlling event count limits and runtime duration to manage data consumption and throttling.

LANGUAGE: json
CODE:
"firehose" : {
    "type" : "twitzer",
    "maxEventCount": -1,
    "maxRunMinutes": 0
}

----------------------------------------

TITLE: Cloning Apache Druid Repository in Bash
DESCRIPTION: Commands to clone the Apache Druid repository from GitHub and navigate to the project directory.

LANGUAGE: bash
CODE:
git clone git@github.com:apache/incubator-druid.git
cd druid

----------------------------------------

TITLE: Filtered GroupBy Query for Multi-value Dimension
DESCRIPTION: GroupBy query using a selector filter to filter rows based on the multi-value 'tags' dimension.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "test",
  "intervals": [
    "1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"
  ],
  "filter": {
    "type": "selector",
    "dimension": "tags",
    "value": "t3"
  },
  "granularity": {
    "type": "all"
  },
  "dimensions": [
    {
      "type": "default",
      "dimension": "tags",
      "outputName": "tags"
    }
  ],
  "aggregations": [
    {
      "type": "count",
      "name": "count"
    }
  ]
}

----------------------------------------

TITLE: Including DataSketches Extension in Druid Configuration
DESCRIPTION: This snippet shows how to include the DataSketches extension in the Druid configuration file.

LANGUAGE: json
CODE:
"druid.extensions.loadList":["druid-datasketches"]

----------------------------------------

TITLE: Including js-yaml License in JavaScript
DESCRIPTION: This comment block contains the license information for js-yaml version 4.1.0. It provides a link to the GitHub repository and specifies that it's released under the MIT license.

LANGUAGE: JavaScript
CODE:
/*! js-yaml 4.1.0 https://github.com/nodeca/js-yaml @license MIT */

----------------------------------------

TITLE: Configuring Task Priority in Druid Context Object
DESCRIPTION: JSON configuration snippet showing how to override the default task priority by setting a custom priority value in the task context. The priority value determines lock acquisition precedence.

LANGUAGE: json
CODE:
"context" : {
  "priority" : 100
}

----------------------------------------

TITLE: Creating Escalator Configuration
DESCRIPTION: Configuration properties for setting up the Basic escalator with internal client credentials and authorizer name.

LANGUAGE: properties
CODE:
druid.escalator.type=basic
druid.escalator.internalClientUsername=druid_system
druid.escalator.internalClientPassword=password2
druid.escalator.authorizerName=MyBasicAuthorizer

----------------------------------------

TITLE: Aggregating Segment Information by Datasource
DESCRIPTION: SQL query example to retrieve aggregated segment information (total size, average size, average number of rows, and number of segments) per datasource.

LANGUAGE: sql
CODE:
SELECT
    datasource,
    SUM("size") AS total_size,
    CASE WHEN SUM("size") = 0 THEN 0 ELSE SUM("size") / (COUNT(*) FILTER(WHERE "size" > 0)) END AS avg_size,
    CASE WHEN SUM(num_rows) = 0 THEN 0 ELSE SUM("num_rows") / (COUNT(*) FILTER(WHERE num_rows > 0)) END AS avg_num_rows,
    COUNT(*) AS num_segments
FROM sys.segments
GROUP BY 1
ORDER BY 2 DESC

----------------------------------------

TITLE: Configuring Quantiles Doubles Sketch to Histogram Post-Aggregator
DESCRIPTION: JSON configuration for the quantilesDoublesSketchToHistogram post-aggregator. This returns an approximation of the histogram given an array of split points that define the histogram bins.

LANGUAGE: json
CODE:
{
  "type"  : "quantilesDoublesSketchToHistogram",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,
  "splitPoints" : <array of split points>
}

----------------------------------------

TITLE: Custom Password Provider Configuration Template in Druid
DESCRIPTION: Template for configuring a custom password provider implementation in Druid. Shows the basic structure needed to reference a registered custom password provider with additional configuration properties.

LANGUAGE: json
CODE:
{ "type": "<registered_password_provider_name>", "<jackson_property>": "<value>", ... }

----------------------------------------

TITLE: Compaction Task Specification with Daily Granularity
DESCRIPTION: JSON configuration for compacting segments into day-level granularity. Includes segment granularity setting and tuning parameters.

LANGUAGE: json
CODE:
{
  "type": "compact",
  "dataSource": "compaction-tutorial",
  "interval": "2015-09-12/2015-09-13",
  "segmentGranularity": "DAY",
  "tuningConfig" : {
    "type" : "index",
    "maxRowsPerSegment" : 5000000,
    "maxRowsInMemory" : 25000,
    "forceExtendableShardSpecs" : true
  }
}

----------------------------------------

TITLE: Configuring Authorizers in Apache Druid
DESCRIPTION: This JSON snippet shows how to enable the 'basic' authorizer implementation from the druid-basic-security extension.

LANGUAGE: json
CODE:
"druid.auth.authorizers":["basic"]

----------------------------------------

TITLE: Configuring Regex parseSpec for Druid Ingestion
DESCRIPTION: JSON configuration for the parseSpec section of a Druid ingestion task using regex parsing. Specifies the format, timestamp, dimensions, columns, and regex pattern.

LANGUAGE: json
CODE:
{
  "parseSpec":{
    "format" : "regex",
    "timestampSpec" : {
      "column" : "timestamp"
    },        
    "dimensionsSpec" : {
      "dimensions" : [<your_list_of_dimensions>]
    },
    "columns" : [<your_columns_here>],
    "pattern" : <regex pattern for partitioning data>
  }
}

----------------------------------------

TITLE: Configuring StaticAzureBlobStoreFirehose in JSON for Apache Druid
DESCRIPTION: JSON configuration for setting up the StaticAzureBlobStoreFirehose in Apache Druid. This firehose allows ingestion of data from Azure Blob Storage, supporting multiple blobs across different containers.

LANGUAGE: json
CODE:
"firehose" : {
    "type" : "static-azure-blobstore",
    "blobs": [
        {
          "container": "container",
          "path": "/path/to/your/file.json"
        },
        {
          "container": "anothercontainer",
          "path": "/another/path.json"
        }
    ]
}

----------------------------------------

TITLE: Performing Student's t-test on ArrayOfDoublesSketch
DESCRIPTION: Post-aggregator configuration to perform Student's t-test on two ArrayOfDoublesSketch instances and return a list of p-values.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchTTest",
  "name": <output name>,
  "fields"  : <array with two post aggregators to access sketch aggregators or post aggregators referring to an ArrayOfDoublesSketch>,
}

----------------------------------------

TITLE: Sending Wikipedia Edit Data to Tranquility Server in Apache Druid
DESCRIPTION: This command sends sample Wikipedia edit data to the Tranquility Server using a POST request. It unzips the sample data file and sends it as JSON to the server's endpoint.

LANGUAGE: bash
CODE:
gunzip -k quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz 
curl -XPOST -H'Content-Type: application/json' --data-binary @quickstart/tutorial/wikiticker-2015-09-12-sampled.json http://localhost:8200/v1/post/wikipedia

----------------------------------------

TITLE: Waiting for Document Load in JavaScript
DESCRIPTION: This comment indicates that the code waits for the document to be fully loaded before starting execution. It's a common practice in JavaScript to ensure all DOM elements are available before manipulating them.

LANGUAGE: JavaScript
CODE:
/*!\n   * Wait for document loaded before starting the execution\n   */

----------------------------------------

TITLE: Numeric Filter Implementation in Druid
DESCRIPTION: Demonstrates implementing a numeric filter in the Having clause for filtering based on aggregate metric values.

LANGUAGE: json
CODE:
{
    "queryType": "groupBy",
    "dataSource": "sample_datasource",
    ...
    "having": 
        {
            "type": "greaterThan",
            "aggregation": "<aggregate_metric>",
            "value": <numeric_value>
        }
}

----------------------------------------

TITLE: Numeric Filter Implementation in Druid
DESCRIPTION: Demonstrates implementing a numeric filter in the Having clause for filtering based on aggregate metric values.

LANGUAGE: json
CODE:
{
    "queryType": "groupBy",
    "dataSource": "sample_datasource",
    ...
    "having": 
        {
            "type": "greaterThan",
            "aggregation": "<aggregate_metric>",
            "value": <numeric_value>
        }
}

----------------------------------------

TITLE: Documenting MIT License for classnames
DESCRIPTION: This code snippet provides license information for the classnames library, which is under the MIT license and created by Jed Watson.

LANGUAGE: JavaScript
CODE:
/*!
	Copyright (c) 2018 Jed Watson.
	Licensed under the MIT License (MIT), see
	http://jedwatson.github.io/classnames
*/

----------------------------------------

TITLE: DOMPurify License Declaration
DESCRIPTION: License comment block for DOMPurify library version 2.4.3, released under Apache 2.0 and Mozilla Public License 2.0.

LANGUAGE: javascript
CODE:
/*! @license DOMPurify 2.4.3 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/2.4.3/LICENSE */

----------------------------------------

TITLE: Cancelling Druid Query via HTTP DELETE
DESCRIPTION: Example of cancelling a running Druid query using its query ID via an HTTP DELETE request.

LANGUAGE: bash
CODE:
curl -X DELETE "http://host:port/druid/v2/abc123"

----------------------------------------

TITLE: MIT License Notice for NProgress
DESCRIPTION: Copyright notice for the NProgress library by Rico Sta. Cruz, licensed under MIT.

LANGUAGE: JavaScript
CODE:
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

----------------------------------------

TITLE: Multiple Inline Schemas Avro Decoder Configuration
DESCRIPTION: Configuration for multiple schemas-based Avro bytes decoder that supports different schemas for different input events using schema IDs.

LANGUAGE: json
CODE:
{
  "avroBytesDecoder": {
    "type": "multiple_schemas_inline",
    "schemas": {
      "1": {
        "namespace": "org.apache.druid.data",
        "name": "User",
        "type": "record",
        "fields": [
          { "name": "FullName", "type": "string" },
          { "name": "Country", "type": "string" }
        ]
      },
      "2": {
        "namespace": "org.apache.druid.otherdata",
        "name": "UserIdentity",
        "type": "record",
        "fields": [
          { "name": "Name", "type": "string" },
          { "name": "Location", "type": "string" }
        ]
      }
    }
  }
}

----------------------------------------

TITLE: Configuring CSV ParseSpec in Druid
DESCRIPTION: Example configuration for ingesting CSV formatted data into Druid. Specifies the timestamp column, input columns, and dimensions to be extracted.

LANGUAGE: json
CODE:
{
  "parseSpec": {
    "format" : "csv",
    "timestampSpec" : {
      "column" : "timestamp"
    },
    "columns" : ["timestamp","page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city","added","deleted","delta"],
    "dimensionsSpec" : {
      "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"]
    }
  }
}

----------------------------------------

TITLE: Configuring RegexFiltered DimensionSpec in Druid Query
DESCRIPTION: The RegexFiltered DimensionSpec retains only the values matching a specified Java regex pattern in multi-value dimensions.

LANGUAGE: JSON
CODE:
{ "type" : "regexFiltered", "delegate" : <dimensionSpec>, "pattern": <java regex pattern> }

----------------------------------------

TITLE: Configuring S3 Deep Storage in Druid
DESCRIPTION: Example configuration for using S3 as deep storage in Druid's common.runtime.properties file

LANGUAGE: properties
CODE:
druid.extensions.loadList=["druid-s3-extensions"]

#druid.storage.type=local
#druid.storage.storageDirectory=var/druid/segments

druid.storage.type=s3
druid.storage.bucket=your-bucket
druid.storage.baseKey=druid/segments
druid.s3.accessKey=...
druid.s3.secretKey=...

#druid.indexer.logs.type=file
#druid.indexer.logs.directory=var/druid/indexing-logs

druid.indexer.logs.type=s3
druid.indexer.logs.s3Bucket=your-bucket
druid.indexer.logs.s3Prefix=druid/indexing-logs

----------------------------------------

TITLE: Multiple Inline Schemas Avro Decoder Configuration
DESCRIPTION: Configuration for multiple schemas-based Avro bytes decoder that supports different schemas for different input events using schema IDs.

LANGUAGE: json
CODE:
{
  "avroBytesDecoder": {
    "type": "multiple_schemas_inline",
    "schemas": {
      "1": {
        "namespace": "org.apache.druid.data",
        "name": "User",
        "type": "record",
        "fields": [
          { "name": "FullName", "type": "string" },
          { "name": "Country", "type": "string" }
        ]
      },
      "2": {
        "namespace": "org.apache.druid.otherdata",
        "name": "UserIdentity",
        "type": "record",
        "fields": [
          { "name": "Name", "type": "string" },
          { "name": "Location", "type": "string" }
        ]
      }
    }
  }
}

----------------------------------------

TITLE: Illustrating Version Update in Apache Druid Segments
DESCRIPTION: Shows how segment names change when data is reindexed with a new schema, resulting in a higher version id.

LANGUAGE: plaintext
CODE:
foo_2015-01-01/2015-01-02_v2_0
foo_2015-01-01/2015-01-02_v2_1
foo_2015-01-01/2015-01-02_v2_2

----------------------------------------

TITLE: Configuring SSL/TLS Settings in Apache Druid
DESCRIPTION: This configuration table outlines the properties for setting up SSL/TLS in Apache Druid. It includes options for specifying the SSL protocol, trust store settings, and other related parameters.

LANGUAGE: markdown
CODE:
|Property|Description|Default|Required|
|--------|-----------|-------|--------|
|`druid.client.https.protocol`|SSL protocol to use.|`TLSv1.2`|no|
|`druid.client.https.trustStoreType`|The type of the key store where trusted root certificates are stored.|`java.security.KeyStore.getDefaultType()`|no|
|`druid.client.https.trustStorePath`|The file path or URL of the TLS/SSL Key store where trusted root certificates are stored.|none|yes|
|`druid.client.https.trustStoreAlgorithm`|Algorithm to be used by TrustManager to validate certificate chains|`javax.net.ssl.TrustManagerFactory.getDefaultAlgorithm()`|no|
|`druid.client.https.trustStorePassword`|The [Password Provider](../../operations/password-provider.html) or String password for the Trust Store.|none|yes|

----------------------------------------

TITLE: Configuring SSL/TLS Settings in Apache Druid
DESCRIPTION: This configuration table outlines the properties for setting up SSL/TLS in Apache Druid. It includes options for specifying the SSL protocol, trust store settings, and other related parameters.

LANGUAGE: markdown
CODE:
|Property|Description|Default|Required|
|--------|-----------|-------|--------|
|`druid.client.https.protocol`|SSL protocol to use.|`TLSv1.2`|no|
|`druid.client.https.trustStoreType`|The type of the key store where trusted root certificates are stored.|`java.security.KeyStore.getDefaultType()`|no|
|`druid.client.https.trustStorePath`|The file path or URL of the TLS/SSL Key store where trusted root certificates are stored.|none|yes|
|`druid.client.https.trustStoreAlgorithm`|Algorithm to be used by TrustManager to validate certificate chains|`javax.net.ssl.TrustManagerFactory.getDefaultAlgorithm()`|no|
|`druid.client.https.trustStorePassword`|The [Password Provider](../../operations/password-provider.html) or String password for the Trust Store.|none|yes|

----------------------------------------

TITLE: Illustrating Version Update in Apache Druid Segments
DESCRIPTION: Shows how segment names change when data is reindexed with a new schema, resulting in a higher version id.

LANGUAGE: plaintext
CODE:
foo_2015-01-01/2015-01-02_v2_0
foo_2015-01-01/2015-01-02_v2_1
foo_2015-01-01/2015-01-02_v2_2

----------------------------------------

TITLE: Configuring Inverted TopNMetricSpec in Druid
DESCRIPTION: Configuration for inverting the sort order of another metric specification, typically used for ascending order sorting.

LANGUAGE: json
CODE:
"metric": {
    "type": "inverted",
    "metric": <delegate_top_n_metric_spec>
}

----------------------------------------

TITLE: Configuring Upper and Lower Extraction Functions in Druid
DESCRIPTION: Converts dimension values to all upper case or lower case, with optional locale specification.

LANGUAGE: json
CODE:
{
  "type" : "upper",
  "locale":"fr"
}

----------------------------------------

TITLE: Installing Zookeeper
DESCRIPTION: Commands to download and install Zookeeper on master servers

LANGUAGE: bash
CODE:
curl http://www.gtlib.gatech.edu/pub/apache/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz
tar -xzf zookeeper-3.4.11.tar.gz
mv zookeeper-3.4.11 zk

----------------------------------------

TITLE: Loading Initial Data via Index Task
DESCRIPTION: Command to submit an initial index task that creates a datasource called 'updates-tutorial' from input file.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/updates-init-index.json --url http://localhost:8081

----------------------------------------

TITLE: Advanced Maven Build with Profiles
DESCRIPTION: Complete Maven build command that generates source and binary distributions with signatures and checksums, audits licenses, and skips unit tests.

LANGUAGE: bash
CODE:
mvn clean install -Papache-release,dist,rat -DskipTests

----------------------------------------

TITLE: js-yaml License Declaration
DESCRIPTION: License comment block for js-yaml library version 4.1.0, released under MIT license.

LANGUAGE: javascript
CODE:
/*! js-yaml 4.1.0 https://github.com/nodeca/js-yaml @license MIT */

----------------------------------------

TITLE: Individual Segment Path in ZooKeeper
DESCRIPTION: Ephemeral znode path for individual segments being served by a process.

LANGUAGE: plaintext
CODE:
${druid.zk.paths.servedSegmentsPath}/${druid.host}/_segment_identifier_

----------------------------------------

TITLE: JSON Data Format Example
DESCRIPTION: Example of JSON-formatted data records showing event logs with timestamp and various dimensions.

LANGUAGE: json
CODE:
{"timestamp": "2013-08-31T01:02:33Z", "page": "Gypsy Danger", "language" : "en", "user" : "nuclear", "unpatrolled" : "true", "newPage" : "true", "robot": "false", "anonymous": "false", "namespace":"article", "continent":"North America", "country":"United States", "region":"Bay Area", "city":"San Francisco", "added": 57, "deleted": 200, "delta": -143}

----------------------------------------

TITLE: Accessing Overlord Console URL
DESCRIPTION: HTTP endpoint pattern for accessing the Overlord console interface where users can view pending tasks, running tasks, and worker status.

LANGUAGE: plaintext
CODE:
http://<OVERLORD_IP>:<port>/console.html

----------------------------------------

TITLE: Specifying Hadoop Dependencies in Druid Tasks
DESCRIPTION: This code shows how to specify the Hadoop client version to be used by Druid when processing a Hadoop Index Task.

LANGUAGE: json
CODE:
"hadoopDependencyCoordinates": ["org.apache.hadoop:hadoop-client:2.4.0"]

----------------------------------------

TITLE: Configuring Kerberos Authentication for Druid
DESCRIPTION: This snippet demonstrates how to set up Kerberos authentication in Druid's common.runtime.properties file. It includes properties for specifying the Kerberos principal and keytab file location.

LANGUAGE: properties
CODE:
druid.hadoop.security.kerberos.principal=hdfs-test@EXAMPLE.IO
druid.hadoop.security.kerberos.keytab=/etc/security/keytabs/hdfs.headless.keytab

----------------------------------------

TITLE: Declaring Lunr.js Library Version and License
DESCRIPTION: This code snippet declares the version, description, copyright, and license information for the Lunr.js library.

LANGUAGE: JavaScript
CODE:
/**
 * lunr - http://lunrjs.com - A bit like Solr, but much smaller and not as bright - 2.3.9
 * Copyright (C) 2020 Oliver Nightingale
 * @license MIT
 */

----------------------------------------

TITLE: Configuring Lookup DimensionSpec with Map in Druid
DESCRIPTION: Defines a lookup implementation as a dimension spec using a map passed at query time.

LANGUAGE: json
CODE:
{
  "type":"lookup",
  "dimension":"dimensionName",
  "outputName":"dimensionOutputName",
  "replaceMissingValueWith":"missing_value",
  "retainMissingValue":false,
  "lookup":{"type": "map", "map":{"key":"value"}, "isOneToOne":false}
}

----------------------------------------

TITLE: Configuring Linear Sharding in Druid Realtime Spec
DESCRIPTION: This JSON snippet shows how to configure linear sharding in the Druid Realtime spec. Linear sharding allows for easy addition of new processes without updating existing configurations and supports querying of all unique shards regardless of sequential numbering.

LANGUAGE: json
CODE:
"shardSpec": {
    "type": "linear",
    "partitionNum": 0
}

----------------------------------------

TITLE: Configuring LongFirst Aggregator in Druid
DESCRIPTION: The longFirst aggregator computes the metric value with the minimum timestamp or 0 if no row exists. It can only be used in queries, not in ingestion specs.

LANGUAGE: json
CODE:
{
  "type" : "longFirst",
  "name" : <output_name>,
  "fieldName" : <metric_name>
}

----------------------------------------

TITLE: Configuring Spatial Dimensions in Druid JSON Spec
DESCRIPTION: Example configuration for defining spatial dimensions in a Druid data ingestion spec. Shows how to specify spatial dimensions using latitude and longitude coordinates within the dimensionsSpec.

LANGUAGE: json
CODE:
{
	"type": "hadoop",
	"dataSchema": {
		"dataSource": "DatasourceName",
		"parser": {
			"type": "string",
			"parseSpec": {
				"format": "json",
				"timestampSpec": {
					"column": "timestamp",
					"format": "auto"
				},
				"dimensionsSpec": {
					"dimensions": [],
					"spatialDimensions": [{
						"dimName": "coordinates",
						"dims": ["lat", "long"]
					}]
				}
			}
		}
	}
}

----------------------------------------

TITLE: Document Load Handler Comment
DESCRIPTION: Documentation comment indicating code execution should wait for document to be fully loaded.

LANGUAGE: javascript
CODE:
/*!\n   * Wait for document loaded before starting the execution\n   */

----------------------------------------

TITLE: Binding Query Toolchest and Runner Factory
DESCRIPTION: Example code showing how to register custom query types by binding implementations of QueryToolChest and QueryRunnerFactory.

LANGUAGE: java
CODE:
DruidBinders.queryToolChestBinder(binder)
            .addBinding(SegmentMetadataQuery.class)
            .to(SegmentMetadataQueryQueryToolChest.class);
    
DruidBinders.queryRunnerFactoryBinder(binder)
            .addBinding(SegmentMetadataQuery.class)
            .to(SegmentMetadataQueryRunnerFactory.class);

----------------------------------------

TITLE: ArrayOfDoublesSketch to Quantiles Post-Aggregator
DESCRIPTION: JSON configuration for post-aggregator that converts ArrayOfDoublesSketch column to quantiles sketch.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToQuantilesSketch",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,
  "column" : <number>,
  "k" : <parameter that determines the accuracy and size of the quantiles sketch>
}

----------------------------------------

TITLE: Binding Query Toolchest and Runner Factory
DESCRIPTION: Example code showing how to register custom query types by binding implementations of QueryToolChest and QueryRunnerFactory.

LANGUAGE: java
CODE:
DruidBinders.queryToolChestBinder(binder)
            .addBinding(SegmentMetadataQuery.class)
            .to(SegmentMetadataQueryQueryToolChest.class);
    
DruidBinders.queryRunnerFactoryBinder(binder)
            .addBinding(SegmentMetadataQuery.class)
            .to(SegmentMetadataQueryRunnerFactory.class);

----------------------------------------

TITLE: Configuring CombiningFirehose in Apache Druid
DESCRIPTION: Shows how to configure a CombiningFirehose to combine and merge data from multiple firehoses.

LANGUAGE: json
CODE:
{
    "type"  :   "combining",
    "delegates" : [ { firehose1 }, { firehose2 }, ..... ]
}

----------------------------------------

TITLE: Query Error Response Structure in Apache Druid
DESCRIPTION: This JSON structure represents the error response returned by Apache Druid when a query fails. It includes fields for error code, error message, error class, and host information.

LANGUAGE: json
CODE:
{
  "error" : "Query timeout",
  "errorMessage" : "Timeout waiting for task.",
  "errorClass" : "java.util.concurrent.TimeoutException",
  "host" : "druid1.example.com:8083"
}

----------------------------------------

TITLE: Multi-value Dimension Sample Data in JSON
DESCRIPTION: Example dataset showing rows with a multi-value 'tags' dimension containing arrays of string values.

LANGUAGE: json
CODE:
{
"timestamp": "2011-01-12T00:00:00.000Z", "tags": ["t1","t2","t3"]}
{"timestamp": "2011-01-13T00:00:00.000Z", "tags": ["t3","t4","t5"]}
{"timestamp": "2011-01-14T00:00:00.000Z", "tags": ["t5","t6","t7"]}
{"timestamp": "2011-01-14T00:00:00.000Z", "tags": []}

----------------------------------------

TITLE: Configuring Druid Historical Process Storage and Size Limits
DESCRIPTION: This snippet shows how to configure the storage location and maximum size for Druid Historical processes. It sets the segment cache location and maximum size, as well as the overall server maximum size.

LANGUAGE: properties
CODE:
-Ddruid.segmentCache.locations=[{"path":"/tmp/druid/storageLocation","maxSize":"500000000000"}]
-Ddruid.server.maxSize=500000000000

----------------------------------------

TITLE: Submitting Druid Ingestion Task via Helper Script
DESCRIPTION: Bash command to use the Druid helper script for submitting the ingestion task. This script simplifies the process of task submission and monitoring.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/wikipedia-index.json

----------------------------------------

TITLE: Indexing with Timestamp Max Aggregator in Apache Druid
DESCRIPTION: JSON configuration for including a timeMax aggregator during data ingestion. This aggregator calculates the maximum timestamp for the specified field.

LANGUAGE: json
CODE:
{
    "type": "timeMax",
    "name": "tmax",
    "fieldName": "<field_name, typically column specified in timestamp spec>"
}

----------------------------------------

TITLE: Dimension Selector Having Filter in Druid
DESCRIPTION: Implementation of a dimension selector filter to match rows with specific dimension values.

LANGUAGE: json
CODE:
{
    "queryType": "groupBy",
    "dataSource": "sample_datasource",
    ...,
    "having": 
       {
            "type": "dimSelector",
            "dimension": "<dimension>",
            "value": <dimension_value>
        }
}

----------------------------------------

TITLE: Installing Apache ZooKeeper
DESCRIPTION: Commands to download, extract, and set up ZooKeeper, which is required for Druid's distributed coordination.

LANGUAGE: bash
CODE:
curl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz
tar -xzf zookeeper-3.4.11.tar.gz
mv zookeeper-3.4.11 zk

----------------------------------------

TITLE: Configuring DoubleMax Aggregator in Druid JSON
DESCRIPTION: Defines a doubleMax aggregator to compute the maximum of all metric values and Double.NEGATIVE_INFINITY.

LANGUAGE: JSON
CODE:
{ "type" : "doubleMax", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: Google Cloud Storage Configuration
DESCRIPTION: Configuration for using Google Cloud Storage through the HDFS extension. Requires GCS connector jar in the classpath.

LANGUAGE: yaml
CODE:
druid.storage.type: hdfs
druid.storage.storageDirectory: gs://bucket/example/directory

----------------------------------------

TITLE: Defining ContainsSearchQuerySpec in Druid JSON
DESCRIPTION: Specifies a case-sensitive contains search query. A match occurs if any part of a dimension value contains the specified value.

LANGUAGE: json
CODE:
{
  "type"  : "contains",
  "case_sensitive" : true,
  "value" : "some_value"
}

----------------------------------------

TITLE: License Comments Collection - JavaScript Dependencies
DESCRIPTION: Collection of license and copyright comments from various JavaScript libraries and dependencies used in the project. All components are licensed under MIT.

LANGUAGE: javascript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

/*!
	Copyright (c) 2018 Jed Watson.
	Licensed under the MIT License (MIT), see
	http://jedwatson.github.io/classnames
*/

/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Configuring Partial Extraction Function in Druid
DESCRIPTION: Returns the dimension value unchanged if the regular expression matches, otherwise returns null.

LANGUAGE: json
CODE:
{ "type" : "partial", "expr" : <regular_expression> }

----------------------------------------

TITLE: Initializing MySQL Metadata Tables for Druid
DESCRIPTION: This command initializes the metadata tables for Druid in a MySQL database. It uses the metadata-init tool, specifying the MySQL extension and connection details.

LANGUAGE: bash
CODE:
cd ${DRUID_ROOT}
java -classpath "lib/*" -Dlog4j.configurationFile=conf/druid/cluster/_common/log4j2.xml -Ddruid.extensions.directory="extensions" -Ddruid.extensions.loadList=[\"mysql-metadata-storage\"] -Ddruid.metadata.storage.type=mysql org.apache.druid.cli.Main tools metadata-init --connectURI="<mysql-uri>" --user <user> --password <pass> --base druid

----------------------------------------

TITLE: Druid Extension Configuration for Cassandra
DESCRIPTION: Markdown text specifying the required extension for Cassandra integration with Druid. The druid-cassandra-storage extension must be included to enable Cassandra as a deep storage option.

LANGUAGE: markdown
CODE:
To use this Apache Druid (incubating) extension, make sure to [include](../../operations/including-extensions.html) `druid-cassandra-storage` extension.

----------------------------------------

TITLE: Sample Wikipedia Page Edit Event
DESCRIPTION: Example JSON structure showing the format of Wikipedia page edit events used in the tutorial dataset.

LANGUAGE: json
CODE:
{
  "timestamp":"2015-09-12T20:03:45.018Z",
  "channel":"#en.wikipedia",
  "namespace":"Main",
  "page":"Spider-Man's powers and equipment",
  "user":"foobar",
  "comment":"/* Artificial web-shooters */",
  "cityName":"New York",
  "regionName":"New York",
  "regionIsoCode":"NY",
  "countryName":"United States",
  "countryIsoCode":"US",
  "isAnonymous":false,
  "isNew":false,
  "isMinor":false,
  "isRobot":false,
  "isUnpatrolled":false,
  "added":99,
  "delta":99,
  "deleted":0
}

----------------------------------------

TITLE: Running pull-deps Tool in Java
DESCRIPTION: Example command to run the pull-deps tool for downloading Druid extensions and Hadoop dependencies. This command cleans existing directories, downloads MySQL metadata storage and RabbitMQ extensions, and specifies two Hadoop client versions.

LANGUAGE: bash
CODE:
java -classpath "/my/druid/lib/*" org.apache.druid.cli.Main tools pull-deps --clean -c org.apache.druid.extensions:mysql-metadata-storage:0.14.1-incubating -c org.apache.druid.extensions.contrib:druid-rabbitmq:0.14.1-incubating -h org.apache.hadoop:hadoop-client:2.3.0 -h org.apache.hadoop:hadoop-client:2.4.0

----------------------------------------

TITLE: Histogram Post-Aggregator Examples in Druid
DESCRIPTION: Collection of post-aggregator configurations for transforming histogram data, including equal buckets, custom buckets, min/max values, and quantile calculations.

LANGUAGE: json
CODE:
{
  "type": "equalBuckets",
  "name": "<output_name>",
  "fieldName": "<aggregator_name>",
  "numBuckets": <count>
}

LANGUAGE: json
CODE:
{
  "type": "buckets",
  "name": "<output_name>",
  "fieldName": "<aggregator_name>",
  "bucketSize": <bucket_size>,
  "offset": <offset>
}

LANGUAGE: json
CODE:
{
  "type": "quantiles",
  "name": "<output_name>",
  "fieldName": "<aggregator_name>",
  "probabilities": [ <quantile>, <quantile>, ... ]
}

----------------------------------------

TITLE: Implementing GroupBy Query with DistinctCount in Druid
DESCRIPTION: Example of a GroupBy query using the DistinctCount aggregator to count unique visitors grouped by sample_dim dimension.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "sample_datasource",
  "dimensions": "[sample_dim]",
  "granularity": "all",
  "aggregations": [
    {
      "type": "distinctCount",
      "name": "uv",
      "fieldName": "visitor_id"
    }
  ],
  "intervals": [
    "2016-03-06T00:00:00/2016-03-06T23:59:59"
  ]
}

----------------------------------------

TITLE: Configuring Basic Authorizer in Druid
DESCRIPTION: Configuration properties for setting up the basic authorizer.

LANGUAGE: properties
CODE:
druid.auth.authorizers=["MyBasicAuthorizer"]

druid.auth.authorizer.MyBasicAuthorizer.type=basic

----------------------------------------

TITLE: HLL Sketch ToString Post-Aggregator Configuration
DESCRIPTION: JSON configuration for HLLSketchToString post-aggregator that provides human-readable sketch summaries for debugging purposes.

LANGUAGE: json
CODE:
{
  "type"  : "HLLSketchToString",
  "name": <output name>,
  "field"  : <post aggregator that returns an HLL Sketch>
}

----------------------------------------

TITLE: Configuring Combining Firehose in Druid
DESCRIPTION: Configuration for combining and merging data from multiple firehoses into a single stream.

LANGUAGE: json
CODE:
{
    "type"  :   "combining",
    "delegates" : [ { firehose1 }, { firehose2 }, ..... ]
}

----------------------------------------

TITLE: Configuring Same Interval Merge Task in Druid
DESCRIPTION: Deprecated task that serves as a shortcut to merge all segments within a specified interval. Simplifies the merge task by using an interval instead of explicitly listing segments.

LANGUAGE: json
CODE:
{
    "type": "same_interval_merge",
    "id": <task_id>,
    "dataSource": <task_datasource>,
    "aggregations": <list of aggregators>,
    "rollup": <whether or not to rollup data during a merge>,
    "interval": <DataSegment objects in this interval are going to be merged>,
    "context": <task context>
}

----------------------------------------

TITLE: ArrayOfDoublesSketch Estimate Post-Aggregator
DESCRIPTION: Post-aggregator configuration for estimating distinct key count from an ArrayOfDoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToEstimate",
  "name": <output name>,
  "field"  : <post aggregator that refers to an ArrayOfDoublesSketch>
}

----------------------------------------

TITLE: Configuring OrderByColumnSpec in Druid GroupBy Queries
DESCRIPTION: Specifies how to perform ordering operations on individual columns. Supports dimension name, sort direction (ascending/descending), and dimensionOrder (lexicographic, alphanumeric, strlen, or numeric).

LANGUAGE: json
CODE:
{
    "dimension" : "<Any dimension or metric name>",
    "direction" : <"ascending"|"descending">,
    "dimensionOrder" : <"lexicographic"(default)|"alphanumeric"|"strlen"|"numeric">
}

----------------------------------------

TITLE: Creating Druid Database and User in MySQL
DESCRIPTION: SQL commands to create a Druid database with UTF-8 encoding, create a Druid user, and grant necessary permissions.

LANGUAGE: sql
CODE:
CREATE DATABASE druid DEFAULT CHARACTER SET utf8mb4;

CREATE USER 'druid'@'localhost' IDENTIFIED BY 'diurd';

GRANT ALL PRIVILEGES ON druid.* TO 'druid'@'localhost';

----------------------------------------

TITLE: Calculating Variance Values from ArrayOfDoublesSketch
DESCRIPTION: Post-aggregator configuration to return a list of variance values from an ArrayOfDoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToVariances",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>
}

----------------------------------------

TITLE: Backwards Compatible Paginated Select Query in Apache Druid
DESCRIPTION: This JSON snippet demonstrates a Select query with the 'fromNext' option set to false for backwards compatibility, requiring manual offset incrementation for pagination.

LANGUAGE: json
CODE:
{
   "queryType": "select",
   "dataSource": "wikipedia",
   "descending": "false",
   "dimensions":[],
   "metrics":[],
   "granularity": "all",
   "intervals": [
     "2013-01-01/2013-01-02"
   ],
   "pagingSpec":{"fromNext": "false", "pagingIdentifiers": {}, "threshold":5}
 }

----------------------------------------

TITLE: Configuring RabbitMQFirehose in Apache Druid
DESCRIPTION: This JSON snippet demonstrates how to configure a RabbitMQFirehose in Apache Druid. It specifies connection details and queue configuration for ingesting events from a RabbitMQ queue.

LANGUAGE: json
CODE:
"firehose" : {
   "type" : "rabbitmq",
   "connection" : {
     "host": "localhost",
     "port": "5672",
     "username": "test-dude",
     "password": "test-word",
     "virtualHost": "test-vhost",
     "uri": "amqp://mqserver:1234/vhost"
   },
   "config" : {
     "exchange": "test-exchange",
     "queue" : "druidtest",
     "routingKey": "#",
     "durable": "true",
     "exclusive": "false",
     "autoDelete": "false",
     "maxRetries": "10",
     "retryIntervalSeconds": "1",
     "maxDurationSeconds": "300"
   }
}

----------------------------------------

TITLE: Configuring Hadoop-based Parquet Ingestion in Druid
DESCRIPTION: JSON configuration for ingesting Parquet data using Hadoop in Druid. Specifies the input format, data schema, dimensions, and tuning parameters for the ingestion process.

LANGUAGE: json
CODE:
{
  "type": "index_hadoop",
  "spec": {
    "ioConfig": {
      "type": "hadoop",
      "inputSpec": {
        "type": "static",
        "inputFormat": "org.apache.druid.data.input.parquet.DruidParquetInputFormat",
        "paths": "no_metrics"
      }
    },
    "dataSchema": {
      "dataSource": "no_metrics",
      "parser": {
        "type": "parquet",
        "parseSpec": {
          "format": "timeAndDims",
          "timestampSpec": {
            "column": "time",
            "format": "auto"
          },
          "dimensionsSpec": {
            "dimensions": [
              "name"
            ],
            "dimensionExclusions": [],
            "spatialDimensions": []
          }
        }
      },
      "metricsSpec": [{
        "type": "count",
        "name": "count"
      }],
      "granularitySpec": {
        "type": "uniform",
        "segmentGranularity": "DAY",
        "queryGranularity": "ALL",
        "intervals": ["2015-12-31/2016-01-02"]
      }
    },
    "tuningConfig": {
      "type": "hadoop",
      "partitionsSpec": {
        "targetPartitionSize": 5000000
      },
      "jobProperties" : {},
      "leaveIntermediate": true
    }
  }
}

----------------------------------------

TITLE: Starting ZooKeeper on Master Server
DESCRIPTION: Commands to download, install, and start ZooKeeper on the Master server

LANGUAGE: bash
CODE:
curl http://www.gtlib.gatech.edu/pub/apache/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz
tar -xzf zookeeper-3.4.11.tar.gz
cd zookeeper-3.4.11
cp conf/zoo_sample.cfg conf/zoo.cfg
./bin/zkServer.sh start

----------------------------------------

TITLE: Configuring Druid SQLServer Metadata Storage Connection
DESCRIPTION: Configuration properties needed to connect Druid to Microsoft SQLServer as metadata storage. Includes database connection URI, user credentials, and storage type specification.

LANGUAGE: properties
CODE:
druid.metadata.storage.type=sqlserver
druid.metadata.storage.connector.connectURI=jdbc:sqlserver://<host>;databaseName=druid
druid.metadata.storage.connector.user=druid
druid.metadata.storage.connector.password=diurd

----------------------------------------

TITLE: Multi-value Column Data Structures
DESCRIPTION: Example demonstrating how Druid represents multi-value columns using modified dictionary encoding, array-based column data, and bitmap indexes that support multiple values per row.

LANGUAGE: text
CODE:
1: Dictionary that encodes column values
  {
    "Justin Bieber": 0,
    "Ke$ha":         1
  }

2: Column data
  [0,
   [0,1],  <--Row value of multi-value column can have array of values
   1,
   1]

3: Bitmaps - one for each unique value
  value="Justin Bieber": [1,1,0,0]
  value="Ke$ha":         [0,1,1,1]

----------------------------------------

TITLE: Creating PostgreSQL Database for Druid
DESCRIPTION: Command to create a PostgreSQL database named 'druid' owned by the druid user

LANGUAGE: bash
CODE:
createdb druid -O druid

----------------------------------------

TITLE: Configuring Client Certificate Authentication in Druid
DESCRIPTION: Optional configuration properties for enabling client certificate authentication between Druid services. Includes settings for keystore configuration and certificate management.

LANGUAGE: properties
CODE:
druid.client.https.keyStorePath=<path_to_keystore>
druid.client.https.keyStoreType=<keystore_type>
druid.client.https.certAlias=<certificate_alias>
druid.client.https.keyStorePassword=<keystore_password>
druid.client.https.keyManagerFactoryAlgorithm=javax.net.ssl.KeyManagerFactory.getDefaultAlgorithm()
druid.client.https.keyManagerPassword=<key_manager_password>

----------------------------------------

TITLE: Registering Data Segment Puller and Pusher in Druid Module
DESCRIPTION: Example of how to add bindings for DataSegmentPusher and DataSegmentPuller objects in a custom Druid module. This code is taken from the HdfsStorageDruidModule.

LANGUAGE: Java
CODE:
Binders.dataSegmentPullerBinder(binder)
       .addBinding("hdfs")
       .to(HdfsDataSegmentPuller.class).in(LazySingleton.class);

Binders.dataSegmentPusherBinder(binder)
       .addBinding("hdfs")
       .to(HdfsDataSegmentPusher.class).in(LazySingleton.class);

----------------------------------------

TITLE: Running ResetCluster Tool in Java for Apache Druid
DESCRIPTION: This command demonstrates how to run the ResetCluster tool using Java. It allows for selective reset of different components of the Druid cluster.

LANGUAGE: bash
CODE:
java org.apache.druid.cli.Main tools reset-cluster [--metadataStore] [--segmentFiles] [--taskLogs] [--hadoopWorkingPath]

----------------------------------------

TITLE: Configuring HyperUnique Metric for Unique Counts
DESCRIPTION: Example of how to configure a hyperUnique metric in the metricsSpec section of a Druid ingestion specification for fast unique counts.

LANGUAGE: json
CODE:
{ "type" : "hyperUnique", "name" : "devices", "fieldName" : "device_id_met" }

----------------------------------------

TITLE: HLL Sketch Union Post-Aggregator Configuration
DESCRIPTION: JSON configuration for HLLSketchUnion post-aggregator that combines multiple HLL sketches.

LANGUAGE: json
CODE:
{
  "type"  : "HLLSketchUnion",
  "name": <output name>,
  "fields"  : <array of post aggregators that return HLL sketches>,
  "lgK": <log2 of K for the target sketch>,
  "tgtHllType" : <target HLL type>
}

----------------------------------------

TITLE: Starting Hadoop Docker Container for Druid
DESCRIPTION: Command to start the Hadoop Docker container with necessary port mappings and volume mounts for use with Druid.

LANGUAGE: bash
CODE:
docker run -it  -h druid-hadoop-demo --name druid-hadoop-demo -p 50010:50010 -p 50020:50020 -p 50075:50075 -p 50090:50090 -p 8020:8020 -p 10020:10020 -p 19888:19888 -p 8030:8030 -p 8031:8031 -p 8032:8032 -p 8033:8033 -p 8040:8040 -p 8042:8042 -p 8088:8088 -p 8443:8443 -p 2049:2049 -p 9000:9000 -p 49707:49707 -p 2122:2122  -p 34455:34455 -v /tmp/shared:/shared druid-hadoop-demo:2.8.3 /etc/bootstrap.sh -bash

----------------------------------------

TITLE: Creating Cassandra Storage Schema for Druid
DESCRIPTION: SQL create statements for the required Cassandra tables: index_storage for storing compressed segments and descriptor_storage for segment metadata.

LANGUAGE: sql
CODE:
CREATE TABLE index_storage(key text,
                           chunk text,
                           value blob,
                           PRIMARY KEY (key, chunk)) WITH COMPACT STORAGE;

CREATE TABLE descriptor_storage(key varchar,
                                lastModified timestamp,
                                descriptor varchar,
                                PRIMARY KEY (key)) WITH COMPACT STORAGE;

----------------------------------------

TITLE: Example InfluxDB Line Protocol Format
DESCRIPTION: Shows a sample line of InfluxDB Line Protocol data containing a measurement, tags, measurements, and timestamp.

LANGUAGE: text
CODE:
cpu,application=dbhost=prdb123,region=us-east-1 usage_idle=99.24,usage_user=0.55 1520722030000000000

----------------------------------------

TITLE: Running Router Node Command in Java
DESCRIPTION: Command to start a Druid Router node server

LANGUAGE: java
CODE:
org.apache.druid.cli.Main server router

----------------------------------------

TITLE: Configuring InfluxDB Parser in Druid
DESCRIPTION: JSON configuration for setting up the InfluxDB Line Protocol parser in Druid ingestion spec. Includes timestamp specification, dimension exclusions, and optional measurement whitelist.

LANGUAGE: json
CODE:
{
      "type": "string",
      "parseSpec": {
        "format": "influx",
        "timestampSpec": {
          "column": "__ts",
          "format": "millis"
        },
        "dimensionsSpec": {
          "dimensionExclusions": [
            "__ts"
          ]
        },
        "whitelistMeasurements": [
          "cpu"
        ]
      }
}

----------------------------------------

TITLE: Configuring Numbered ShardSpec in JSON for Apache Druid
DESCRIPTION: Example JSON configuration for a numbered shardSpec in Apache Druid. This configuration sets up a numbered sharding strategy with partition number 0 and a total of 2 partitions.

LANGUAGE: json
CODE:
"shardSpec": {
    "type": "numbered",
    "partitionNum": 0,
    "partitions": 2
}

----------------------------------------

TITLE: Configuring Druid Metadata Storage Extension for MySQL
DESCRIPTION: Properties to be added to the Druid configuration file to enable and configure the MySQL metadata storage extension.

LANGUAGE: properties
CODE:
druid.extensions.loadList=["mysql-metadata-storage"]
druid.metadata.storage.type=mysql
druid.metadata.storage.connector.connectURI=jdbc:mysql://<host>/druid
druid.metadata.storage.connector.user=druid
druid.metadata.storage.connector.password=diurd

----------------------------------------

TITLE: Default DimensionSpec Configuration in Druid
DESCRIPTION: Basic dimension specification that returns dimension values as-is with optional renaming and type conversion.

LANGUAGE: json
CODE:
{
  "type" : "default",
  "dimension" : <dimension>,
  "outputName": <output_name>,
  "outputType": <"STRING"|"LONG"|"FLOAT">
}

----------------------------------------

TITLE: Sample JSON Output for DumpSegment Bitmap Data in Apache Druid
DESCRIPTION: This snippet illustrates the JSON output format when dumping bitmap data from a Druid segment. It includes the bitmap serialization factory type and base64-encoded bitmap data for column values.

LANGUAGE: json
CODE:
{
  "bitmapSerdeFactory": {
    "type": "concise"
  },
  "bitmaps": {
    "isRobot": {
      "false": "//aExfu+Nv3X...",
      "true": "gAl7OoRByQ..."
    }
  }
}

----------------------------------------

TITLE: Executing Data Source Metadata Query in Druid
DESCRIPTION: This JSON snippet demonstrates the structure of a Data Source Metadata query in Druid. It includes the required 'queryType' and 'dataSource' fields. The query returns metadata information, specifically the timestamp of the latest ingested event for the specified dataSource.

LANGUAGE: json
CODE:
{
    "queryType" : "dataSourceMetadata",
    "dataSource": "sample_datasource"
}

----------------------------------------

TITLE: Executing Compaction Task with Original Granularity
DESCRIPTION: Command to submit the compaction task that maintains original segment granularity.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/compaction-keep-granularity.json

----------------------------------------

TITLE: Configuring DoubleSum Aggregator in Druid JSON
DESCRIPTION: The doubleSum aggregator computes and stores the sum of values as a 64-bit floating point value. It is similar to longSum but uses double precision.

LANGUAGE: json
CODE:
{ "type" : "doubleSum", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: Configuring JavaScript Extraction Function in Apache Druid
DESCRIPTION: JSON configuration for a JavaScript Extraction Function, which transforms dimension values using a specified JavaScript function.

LANGUAGE: JSON
CODE:
{
  "type" : "javascript",
  "function" : "function(str) { return str.substr(0, 3); }"
}

----------------------------------------

TITLE: Implementing Basic Numeric TopNMetricSpec in Druid
DESCRIPTION: Simple string-based metric specification for sorting topN results by a metric name.

LANGUAGE: json
CODE:
"metric": "<metric_name>"

----------------------------------------

TITLE: Configuring Extraction DimensionSpec in Druid
DESCRIPTION: Returns dimension values transformed using the given extraction function. Allows specifying output type for type conversion before merging.

LANGUAGE: json
CODE:
{
  "type" : "extraction",
  "dimension" : <dimension>,
  "outputName" :  <output_name>,
  "outputType": <"STRING"|"LONG"|"FLOAT">,
  "extractionFn" : <extraction_function>
}

----------------------------------------

TITLE: Configuring Derby as Metadata Storage in Apache Druid
DESCRIPTION: This snippet shows how to configure Derby as the metadata storage for Druid. It sets the storage type and connection URI.

LANGUAGE: properties
CODE:
druid.metadata.storage.type=derby
druid.metadata.storage.connector.connectURI=jdbc:derby://localhost:1527//opt/var/druid_state/derby;create=true

----------------------------------------

TITLE: Configuring JavaScript Extraction Function in Druid
DESCRIPTION: Transforms dimension values using a specified JavaScript function. Supports both regular dimensions and __time dimension.

LANGUAGE: json
CODE:
{
  "type" : "javascript",
  "function" : "function(str) { return str.substr(0, 3); }"
}

----------------------------------------

TITLE: Help Command Usage
DESCRIPTION: Command to display the complete help documentation for the ResetCluster tool.

LANGUAGE: bash
CODE:
java org.apache.druid.cli.Main help tools reset-cluster

----------------------------------------

TITLE: ArrayOfDoublesSketch Aggregator Configuration
DESCRIPTION: JSON configuration for the ArrayOfDoublesSketch aggregator showing all available parameters including nominalEntries, numberOfValues, and metricColumns.

LANGUAGE: json
CODE:
{
  "type" : "arrayOfDoublesSketch",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "nominalEntries": <number>,
  "numberOfValues" : <number>,
  "metricColumns" : <array of strings>
 }

----------------------------------------

TITLE: Configuring Polling Off-heap Lookup Cache in Druid
DESCRIPTION: Example configuration for a polling-based off-heap cache that loads once without periodic updates. Uses JDBC data fetcher to connect to MySQL database.

LANGUAGE: json
CODE:
{
    "type":"pollingLookup",
   "dataFetcher":{ "type":"jdbcDataFetcher", "connectorConfig":"jdbc://mysql://localhost:3306/my_data_base", "table":"lookup_table_name", "keyColumn":"key_column_name", "valueColumn": "value_column_name"},
   "cacheFactory":{"type":"offHeapPolling"}
}

----------------------------------------

TITLE: Configuring Cached Namespace Lookup with URI Source
DESCRIPTION: Example configuration for a globally cached lookup using a URI source with CSV format parsing specification.

LANGUAGE: json
CODE:
 {
    "type": "cachedNamespace",
    "extractionNamespace": {
       "type": "uri",
       "uri": "file:/tmp/prefix/",
       "namespaceParseSpec": {
         "format": "csv",
         "columns": [
           "key",
           "value"
         ]
       },
       "pollPeriod": "PT5M"
     },
     "firstCacheTimeout": 0
 }

----------------------------------------

TITLE: Example TopN Query with Variance and Standard Deviation
DESCRIPTION: Complete example of a TopN query using both variance aggregation and standard deviation post-aggregation.

LANGUAGE: json
CODE:
{
  "queryType": "topN",
  "dataSource": "testing",
  "dimensions": ["alias"],
  "threshold": 5,
  "granularity": "all",
  "aggregations": [
    {
      "type": "variance",
      "name": "index_var",
      "fieldName": "index"
    }
  ],
  "postAggregations": [
    {
      "type": "stddev",
      "name": "index_stddev",
      "fieldName": "index_var"
    }
  ],
  "intervals": [
    "2016-03-06T00:00:00/2016-03-06T23:59:59"
  ]
}

----------------------------------------

TITLE: Example Lookup Configuration for Coordinator Endpoint
DESCRIPTION: Complete example of a lookup configuration that would be returned by the coordinator endpoint. It includes version information and a JDBC-based lookup extractor factory.

LANGUAGE: json
CODE:
{
  "version": "v0",
  "lookupExtractorFactory": {
    "type": "cachedNamespace",
    "extractionNamespace": {
      "type": "jdbc",
      "connectorConfig": {
        "createTables": true,
        "connectURI": "jdbc:mysql://localhost:3306/druid",
        "user": "druid",
        "password": "diurd"
      },
      "table": "lookupValues",
      "keyColumn": "value_id",
      "valueColumn": "value_text",
      "filter": "value_type='country'",
      "tsColumn": "timeColumn"
    },
    "firstCacheTimeout": 120000,
    "injective": true
  }
}

----------------------------------------

TITLE: Example of Expression Transform in JSON for Apache Druid
DESCRIPTION: This example shows a specific implementation of an expression transform in Apache Druid. It prepends 'foo' to the values of a 'page' column in the input data, creating a new 'fooPage' column.

LANGUAGE: json
CODE:
{
  "type": "expression",
  "name": "fooPage",
  "expression": "concat('foo' + page)"
}

----------------------------------------

TITLE: Configuring Arithmetic Post-Aggregator in Druid
DESCRIPTION: Demonstrates how to set up arithmetic post-aggregations using supported functions (+, -, *, /, quotient) with optional numeric ordering.

LANGUAGE: json
CODE:
{
  "type"  : "arithmetic",
  "name"  : <output_name>,
  "fn"    : <arithmetic_function>,
  "fields": [<post_aggregator>, <post_aggregator>, ...],
  "ordering" : <null (default), or "numericFirst">
}

----------------------------------------

TITLE: Quantiles Doubles Sketch to Quantile Post Aggregator
DESCRIPTION: Post aggregator configuration for extracting a single quantile value from a DoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "quantilesDoublesSketchToQuantile",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch>,
  "fraction" : <fractional position in the hypothetical sorted stream>
}

----------------------------------------

TITLE: Implementing Detailed Numeric TopNMetricSpec in Druid
DESCRIPTION: JSON object-based metric specification for sorting topN results by numeric values, requiring type and metric name.

LANGUAGE: json
CODE:
"metric": {
    "type": "numeric",
    "metric": "<metric_name>"
}

----------------------------------------

TITLE: Building Hadoop Docker Image
DESCRIPTION: Commands to build a Docker image for Hadoop 2.8.3 cluster from the provided Dockerfile.

LANGUAGE: bash
CODE:
cd quickstart/tutorial/hadoop/docker
docker build -t druid-hadoop-demo:2.8.3 .

----------------------------------------

TITLE: Configuring Cached Namespace Lookup with JDBC
DESCRIPTION: Example configuration for a globally cached lookup using a JDBC connection to retrieve data from a MySQL database.

LANGUAGE: json
CODE:
{
    "type": "cachedNamespace",
    "extractionNamespace": {
       "type": "jdbc",
       "connectorConfig": {
         "createTables": true,
         "connectURI": "jdbc:mysql:\/\/localhost:3306\/druid",
         "user": "druid",
         "password": "diurd"
       },
       "table": "lookupTable",
       "keyColumn": "mykeyColumn",
       "valueColumn": "myValueColumn",
       "filter" : "myFilterSQL",
       "tsColumn": "timeColumn"
    },
    "firstCacheTimeout": 120000,
    "injective":true
}

----------------------------------------

TITLE: Querying Grouped Data in Apache Druid
DESCRIPTION: This SQL query performs a GroupBy operation on the 'updates-tutorial' datasource, demonstrating how rows are grouped at query time.

LANGUAGE: sql
CODE:
select __time, animal, SUM("count"), SUM("number") from "updates-tutorial" group by __time, animal;

----------------------------------------

TITLE: ArrayOfDoublesSketch ToString Post-Aggregator
DESCRIPTION: JSON configuration for post-aggregator that returns human-readable summary of ArrayOfDoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToString",
  "name": <output name>,
  "field"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>
}

----------------------------------------

TITLE: Submit Compaction Task - Day Granularity
DESCRIPTION: Command to submit the compaction task that converts segments to day-level granularity.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/compaction-day-granularity.json

----------------------------------------

TITLE: Defining RegexSearchQuerySpec in Druid JSON
DESCRIPTION: Specifies a regex search query. A match occurs if any part of a dimension value matches the specified regex pattern.

LANGUAGE: json
CODE:
{
  "type"  : "regex",
  "pattern" : "some_pattern"
}

----------------------------------------

TITLE: Example Druid Live Row Stats Report
DESCRIPTION: Sample JSON response showing live statistics including moving averages and totals for row processing metrics.

LANGUAGE: json
CODE:
{
  "movingAverages": {
    "buildSegments": {
      "5m": {
        "processed": 3.392158326408501,
        "unparseable": 0,
        "thrownAway": 0,
        "processedWithError": 0
      },
      "15m": {
        "processed": 1.736165476881023,
        "unparseable": 0,
        "thrownAway": 0,
        "processedWithError": 0
      },
      "1m": {
        "processed": 4.206417693750045,
        "unparseable": 0,
        "thrownAway": 0,
        "processedWithError": 0
      }
    }
  },
  "totals": {
    "buildSegments": {
      "processed": 1994,
      "processedWithError": 0,
      "thrownAway": 0,
      "unparseable": 0
    }
  }
}

----------------------------------------

TITLE: Configuring Variance Aggregator for Ingestion in Apache Druid
DESCRIPTION: JSON configuration for the variance aggregator used during data ingestion. It specifies the output name, metric name, input type, and estimator for variance calculation.

LANGUAGE: json
CODE:
{
  "type" : "variance",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "inputType" : <input_type>,
  "estimator" : <string>
}

----------------------------------------

TITLE: Configuring Core Extensions in Druid Properties File
DESCRIPTION: This snippet shows how to load core extensions in Druid by adding their names to the druid.extensions.loadList property in the common.runtime.properties file. It demonstrates loading the postgresql-metadata-storage and druid-hdfs-storage extensions.

LANGUAGE: properties
CODE:
druid.extensions.loadList=["postgresql-metadata-storage", "druid-hdfs-storage"]

----------------------------------------

TITLE: ArrayOfDoublesSketch ToString Post-Aggregator
DESCRIPTION: JSON configuration for post-aggregator that returns human-readable summary of ArrayOfDoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToString",
  "name": <output name>,
  "field"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>
}

----------------------------------------

TITLE: Sample JSON Output for DumpSegment Row Data in Apache Druid
DESCRIPTION: This snippet shows an example of the JSON output format when dumping row data from a Druid segment. It includes various fields such as timestamp, metrics, and dimensions.

LANGUAGE: json
CODE:
{
  "__time": 1442018818771,
  "added": 36,
  "channel": "#en.wikipedia",
  "cityName": null,
  "comment": "added project",
  "count": 1,
  "countryIsoCode": null,
  "countryName": null,
  "deleted": 0,
  "delta": 36,
  "isAnonymous": "false",
  "isMinor": "false",
  "isNew": "false",
  "isRobot": "false",
  "isUnpatrolled": "false",
  "iuser": "00001553",
  "metroCode": null,
  "namespace": "Talk",
  "page": "Talk:Oswald Tilghman",
  "regionIsoCode": null,
  "regionName": null,
  "user": "GELongstreet"
}

----------------------------------------

TITLE: Configuring Numbered Sharding in Druid
DESCRIPTION: JSON configuration for numbered sharding in Druid's Realtime node spec. This provides explicit control over partition numbering and total partition count.

LANGUAGE: json
CODE:
"shardSpec": {
    "type": "numbered",
    "partitionNum": 0,
    "partitions": 2
}

----------------------------------------

TITLE: Post Index Task Command - Bash
DESCRIPTION: Command to submit an indexing task using the post-index-task script

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/updates-init-index.json

----------------------------------------

TITLE: Configuring JavaScript Aggregator in Druid JSON
DESCRIPTION: The JavaScript aggregator computes an arbitrary JavaScript function over a set of columns. It requires specifying the output name, field names, and JavaScript functions for aggregation, combination, and reset operations.

LANGUAGE: json
CODE:
{ "type": "javascript",
  "name": "<output_name>",
  "fieldNames"  : [ <column1>, <column2>, ... ],
  "fnAggregate" : "function(current, column1, column2, ...) {
                     <updates partial aggregate (current) based on the current row values>
                     return <updated partial aggregate>
                   }",
  "fnCombine"   : "function(partialA, partialB) { return <combined partial results>; }",
  "fnReset"     : "function()                   { return <initial value>; }"
}

----------------------------------------

TITLE: Configuring Regular Expression Extraction Function in Apache Druid
DESCRIPTION: JSON configuration for a Regular Expression Extraction Function, which returns the first matching group for a given regular expression.

LANGUAGE: JSON
CODE:
{
  "type" : "regex",
  "expr" : <regular_expression>,
  "index" : <group to extract, default 1>
  "replaceMissingValue" : true,
  "replaceMissingValueWith" : "foobar"
}

----------------------------------------

TITLE: Configuring Extraction DimensionSpec in Druid Query
DESCRIPTION: The Extraction DimensionSpec transforms dimension values using a specified extraction function. It can also specify the output type for type conversion before merging.

LANGUAGE: JSON
CODE:
{
  "type" : "extraction",
  "dimension" : <dimension>,
  "outputName" :  <output_name>,
  "outputType": <"STRING"|"LONG"|"FLOAT">,
  "extractionFn" : <extraction_function>
}

----------------------------------------

TITLE: Configuring Union Datasource in Druid
DESCRIPTION: Shows how to create a union datasource that combines multiple table datasources. All datasources being unioned must share the same schema and these queries must be sent to broker/router nodes.

LANGUAGE: json
CODE:
{
       "type": "union",
       "dataSources": ["<string_value1>", "<string_value2>", "<string_value3>", ... ]
}

----------------------------------------

TITLE: Setting Hadoop Job Properties for Classloader Isolation
DESCRIPTION: This snippet shows how to set Hadoop job properties in the task definition to enable classloader isolation, which can prevent library conflicts between Druid and Hadoop.

LANGUAGE: json
CODE:
"jobProperties": {
  "mapreduce.job.classloader": "true",
  "mapreduce.job.classloader.system.classes": "-javax.validation.,java.,javax.,org.apache.commons.logging.,org.apache.log4j.,org.apache.hadoop."
}

----------------------------------------

TITLE: Configuring Regular Expression Extraction Function in Druid
DESCRIPTION: Returns the first matching group for the given regular expression. Allows specifying group index and handling of missing values.

LANGUAGE: json
CODE:
{
  "type" : "regex",
  "expr" : <regular_expression>,
  "index" : <group to extract, default 1>
  "replaceMissingValue" : true,
  "replaceMissingValueWith" : "foobar"
}

----------------------------------------

TITLE: Configuring Batch Thrift Ingestion with Hadoop
DESCRIPTION: JSON configuration for batch ingestion of Thrift data using HadoopDruidIndexer. Includes settings for input format, data schema, and job properties for handling Thrift files.

LANGUAGE: json
CODE:
{
  "type": "index_hadoop",
  "spec": {
    "dataSchema": {
      "dataSource": "book",
      "parser": {
        "type": "thrift",
        "jarPath": "book.jar",
        "thriftClass": "org.apache.druid.data.input.thrift.Book",
        "protocol": "compact",
        "parseSpec": {
          "format": "json",
          ...
        }
      },
      "metricsSpec": [],
      "granularitySpec": {}
    },
    "ioConfig": {
      "type": "hadoop",
      "inputSpec": {
        "type": "static",
        "inputFormat": "org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat",
        "paths": "/user/to/some/book.seq"
      }
    },
    "tuningConfig": {
      "type": "hadoop",
      "jobProperties": {
        "tmpjars":"/user/h_user_profile/du00/druid/test/book.jar"
      }
    }
  }
}

----------------------------------------

TITLE: Quantiles Doubles Sketch Aggregator Configuration
DESCRIPTION: JSON configuration for the quantilesDoublesSketch aggregator that builds or merges sketches from input data.

LANGUAGE: json
CODE:
{
  "type" : "quantilesDoublesSketch",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "k": <parameter that controls size and accuracy>
 }

----------------------------------------

TITLE: Loading DataSketches Extension in Druid Configuration
DESCRIPTION: Configuration snippet showing how to include the DataSketches extension in Druid's config file by adding it to the loadList property.

LANGUAGE: yaml
CODE:
druid.extensions.loadList=["druid-datasketches"]

----------------------------------------

TITLE: Tranquility Server Response JSON
DESCRIPTION: Example JSON response from Tranquility Server after successfully receiving and sending events to Druid.

LANGUAGE: json
CODE:
{"result":{"received":39244,"sent":39244}}

----------------------------------------

TITLE: Defining Table Datasource in Apache Druid (JSON)
DESCRIPTION: This snippet shows how to define a table datasource in Apache Druid using JSON. Table datasources are the most common type and can be represented by a string or a full structure with 'type' and 'name' fields.

LANGUAGE: json
CODE:
{
	"type": "table",
	"name": "<string_value>"
}

----------------------------------------

TITLE: Overwriting Existing Data in Apache Druid
DESCRIPTION: This command overwrites the existing data in the 'updates-tutorial' datasource using a new JSON spec file.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/updates-overwrite-index.json

----------------------------------------

TITLE: Starting Druid Query Server
DESCRIPTION: Command to start the Broker process on query servers

LANGUAGE: bash
CODE:
java `cat conf/druid/broker/jvm.config | xargs` -cp conf/druid/_common:conf/druid/broker:lib/* org.apache.druid.cli.Main server broker

----------------------------------------

TITLE: Configuring Globally Cached Lookup with URI Namespace
DESCRIPTION: Example configuration for a globally cached lookup using a URI namespace. This setup specifies a CSV file on S3 as the lookup source, with polling for updates every 5 minutes.

LANGUAGE: json
CODE:
{
   "type": "cachedNamespace",
   "extractionNamespace": {
      "type": "uri",
      "uri": "s3://bucket/some/key/prefix/renames-0003.gz",
      "namespaceParseSpec": {
        "format": "csv",
        "columns": [
          "key",
          "value"
        ]
      },
      "pollPeriod": "PT5M"
    },
    "firstCacheTimeout": 0
}

----------------------------------------

TITLE: Demonstrating Druid Column Data Structures
DESCRIPTION: Shows how dictionary encoding, column data, and bitmaps are structured for a simple dimension column containing artist names.

LANGUAGE: plaintext
CODE:
1: Dictionary that encodes column values
  {
    "Justin Bieber": 0,
    "Ke$ha":         1
  }

2: Column data
  [0,
   0,
   1,
   1]

3: Bitmaps - one for each unique value of the column
  value="Justin Bieber": [1,1,0,0]
  value="Ke$ha":         [0,0,1,1]

----------------------------------------

TITLE: Demonstrating Druid Column Data Structures
DESCRIPTION: Shows how dictionary encoding, column data, and bitmaps are structured for a simple dimension column containing artist names.

LANGUAGE: plaintext
CODE:
1: Dictionary that encodes column values
  {
    "Justin Bieber": 0,
    "Ke$ha":         1
  }

2: Column data
  [0,
   0,
   1,
   1]

3: Bitmaps - one for each unique value of the column
  value="Justin Bieber": [1,1,0,0]
  value="Ke$ha":         [0,0,1,1]

----------------------------------------

TITLE: Rolled Up Data Example
DESCRIPTION: Example showing network traffic data after rollup aggregation at minute granularity.

LANGUAGE: text
CODE:
timestamp                 srcIP         dstIP          packets     bytes
2018-01-01T01:01:00Z      1.1.1.1       2.2.2.2            600      6000
2018-01-01T01:02:00Z      1.1.1.1       2.2.2.2            900      9000
2018-01-01T01:03:00Z      1.1.1.1       2.2.2.2            600      6000
2018-01-02T21:33:00Z      7.7.7.7       8.8.8.8            300      3000
2018-01-02T21:35:00Z      7.7.7.7       8.8.8.8            300      3000

----------------------------------------

TITLE: Example Timeseries Query with Variance
DESCRIPTION: Complete example of a timeseries query using the variance aggregator.

LANGUAGE: json
CODE:
{
  "queryType": "timeseries",
  "dataSource": "testing",
  "granularity": "day",
  "aggregations": [
    {
      "type": "variance",
      "name": "index_var",
      "fieldName": "index_var"
    }
  ],
  "intervals": [
    "2016-03-01T00:00:00.000/2013-03-20T00:00:00.000"
  ]
}

----------------------------------------

TITLE: Complete Example of Statistical Testing in Druid
DESCRIPTION: Full example demonstrating the combined usage of zscore2sample and pvalue2tailedZtest post aggregators with constant values for sample sizes and success counts.

LANGUAGE: json
CODE:
{
  "postAggregations" : {
    "type"   : "pvalue2tailedZtest",
    "name"   : "pvalue",
    "zScore" : 
    {
     "type"   : "zscore2sample",
     "name"   : "zscore",
     "successCount1" :
       { "type"   : "constant",
         "name"   : "successCountFromPopulation1Sample",
         "value"  : 300
       },
     "sample1Size" :
       { "type"   : "constant",
         "name"   : "sampleSizeOfPopulation1",
         "value"  : 500
       },
     "successCount2":
       { "type"   : "constant",
         "name"   : "successCountFromPopulation2Sample",
         "value"  : 450
       },
     "sample2Size" :
       { "type"   : "constant",
         "name"   : "sampleSizeOfPopulation2",
         "value"  : 600
       }
     }
    }
}

----------------------------------------

TITLE: Overwriting Existing Data
DESCRIPTION: Command to submit an overwrite task that replaces existing data in the datasource.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/updates-overwrite-index.json --url http://localhost:8081

----------------------------------------

TITLE: Handling Unique IDs with Dimensions and Metrics
DESCRIPTION: Example showing how to configure the same column as both dimension and metric for unique ID handling.

LANGUAGE: json
CODE:
{"device_id_dim":123, "device_id_met":123}

LANGUAGE: json
CODE:
{ "type" : "hyperUnique", "name" : "devices", "fieldName" : "device_id_met" }

----------------------------------------

TITLE: Querying Initial Data in Apache Druid
DESCRIPTION: This SQL query retrieves all rows from the 'updates-tutorial' datasource, showing the initial data loaded.

LANGUAGE: sql
CODE:
select * from "updates-tutorial";

----------------------------------------

TITLE: Configuring Avro Hadoop Parser for Batch Ingestion in Druid
DESCRIPTION: This JSON snippet shows how to configure the Avro Hadoop parser for batch ingestion using HadoopDruidIndexer. It includes settings for the parser type, parseSpec, and input format, along with a custom schema file path in the tuning config.

LANGUAGE: json
CODE:
{
  "type" : "index_hadoop",  
  "spec" : {
    "dataSchema" : {
      "dataSource" : "",
      "parser" : {
        "type" : "avro_hadoop",
        "parseSpec" : {
          "format": "avro",
          "timestampSpec": <standard timestampSpec>,
          "dimensionsSpec": <standard dimensionsSpec>,
          "flattenSpec": <optional>
        }
      }
    },
    "ioConfig" : {
      "type" : "hadoop",
      "inputSpec" : {
        "type" : "static",
        "inputFormat": "org.apache.druid.data.input.avro.AvroValueInputFormat",
        "paths" : ""
      }
    },
    "tuningConfig" : {
       "jobProperties" : {
          "avro.schema.input.value.path" : "/path/to/my/schema.avsc"
      }
    }
  }
}

----------------------------------------

TITLE: Displaying Rollup Pseudocode
DESCRIPTION: Pseudocode representing the rollup operation in Druid with a queryGranularity of minute.

LANGUAGE: sql
CODE:
GROUP BY TRUNCATE(timestamp, MINUTE), srcIP, dstIP :: SUM(packets), SUM(bytes)

----------------------------------------

TITLE: Querying Rolled-up Data in Druid SQL
DESCRIPTION: SQL query to retrieve all data from the 'rollup-tutorial' datasource, showing the effects of roll-up.

LANGUAGE: sql
CODE:
select * from "rollup-tutorial";

----------------------------------------

TITLE: Importing Metadata into MySQL Database
DESCRIPTION: SQL commands to import the exported CSV files into MySQL database tables. This includes importing data for segments, rules, config, datasource, and supervisors tables.

LANGUAGE: sql
CODE:
LOAD DATA INFILE '/tmp/csv/druid_segments.csv' INTO TABLE druid_segments FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"' (id,dataSource,created_date,start,end,partitioned,version,used,payload); SHOW WARNINGS;

LOAD DATA INFILE '/tmp/csv/druid_rules.csv' INTO TABLE druid_rules FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"' (id,dataSource,version,payload); SHOW WARNINGS;

LOAD DATA INFILE '/tmp/csv/druid_config.csv' INTO TABLE druid_config FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"' (name,payload); SHOW WARNINGS;

LOAD DATA INFILE '/tmp/csv/druid_dataSource.csv' INTO TABLE druid_dataSource FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"' (dataSource,created_date,commit_metadata_payload,commit_metadata_sha1); SHOW WARNINGS;

LOAD DATA INFILE '/tmp/csv/druid_supervisors.csv' INTO TABLE druid_supervisors FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"' (id,spec_id,created_date,payload); SHOW WARNINGS;

----------------------------------------

TITLE: Configuring HLLSketchUnion Post-Aggregator in Druid
DESCRIPTION: This JSON configuration defines an HLLSketchUnion post-aggregator for merging multiple HLL sketches.

LANGUAGE: json
CODE:
{
  "type"  : "HLLSketchUnion",
  "name": <output name>,
  "fields"  : <array of post aggregators that return HLL sketches>,
  "lgK": <log2 of K for the target sketch>,
  "tgtHllType" : <target HLL type>
}

----------------------------------------

TITLE: Creating Union Datasource in Apache Druid JSON
DESCRIPTION: This snippet demonstrates how to create a union datasource in Apache Druid, which combines two or more table datasources. The datasources being unioned should have the same schema, and union queries should be sent to a Broker/Router process.

LANGUAGE: json
CODE:
{
       "type": "union",
       "dataSources": ["<string_value1>", "<string_value2>", "<string_value3>", ... ]
}

----------------------------------------

TITLE: Cardinality Aggregator Example: Distinct Countries
DESCRIPTION: An example of using the cardinality aggregator to determine the number of distinct countries people are living in or have come from. It uses two fields: country_of_origin and country_of_residence.

LANGUAGE: json
CODE:
{
  "type": "cardinality",
  "name": "distinct_countries",
  "fields": [ "country_of_origin", "country_of_residence" ]
}

----------------------------------------

TITLE: Copyright and License Notice for Prism
DESCRIPTION: This snippet provides the copyright and license information for Prism, a syntax highlighting library, which is licensed under the MIT license.

LANGUAGE: JavaScript
CODE:
/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

----------------------------------------

TITLE: Performing TopN Query with DistinctCount Aggregator in Druid
DESCRIPTION: This JSON snippet shows how to use the DistinctCount aggregator in a TopN query. It retrieves the top 5 values of 'sample_dim' based on the distinct count of 'visitor_id'.

LANGUAGE: json
CODE:
{
  "queryType": "topN",
  "dataSource": "sample_datasource",
  "dimension": "sample_dim",
  "threshold": 5,
  "metric": "uv",
  "granularity": "all",
  "aggregations": [
    {
      "type": "distinctCount",
      "name": "uv",
      "fieldName": "visitor_id"
    }
  ],
  "intervals": [
    "2016-03-06T00:00:00/2016-03-06T23:59:59"
  ]
}

----------------------------------------

TITLE: ArrayOfDoublesSketch Estimate with Bounds Post-Aggregator
DESCRIPTION: Post-aggregator configuration for estimating distinct key count with error bounds from an ArrayOfDoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToEstimateAndBounds",
  "name": <output name>,
  "field"  : <post aggregator that refers to an ArrayOfDoublesSketch>,
  "numStdDevs": <number from 1 to 3>
}

----------------------------------------

TITLE: Including DOMPurify License in JavaScript
DESCRIPTION: This comment block contains the license information for DOMPurify version 2.4.3. It specifies the Apache license 2.0 and Mozilla Public License 2.0, and provides a link to the full license text on GitHub.

LANGUAGE: JavaScript
CODE:
/*! @license DOMPurify 2.4.3 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/2.4.3/LICENSE */

----------------------------------------

TITLE: Sample JSON Data Format
DESCRIPTION: Example JSON structure showing the metrics data format that will be converted to Protobuf

LANGUAGE: json
CODE:
{
  "unit": "milliseconds",
  "http_method": "GET",
  "value": 44,
  "timestamp": "2017-04-06T02:36:22Z",
  "http_code": "200",
  "page": "/",
  "metricType": "request/latency",
  "server": "www1.example.com"
}

----------------------------------------

TITLE: Example GroupBy Query with Variance and StdDev
DESCRIPTION: Complete example of a GroupBy query using both variance aggregator and standard deviation post-aggregator.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "testing",
  "dimensions": ["alias"],
  "granularity": "all",
  "aggregations": [
    {
      "type": "variance",
      "name": "index_var",
      "fieldName": "index"
    }
  ],
  "postAggregations": [
    {
      "type": "stddev",
      "name": "index_stddev",
      "fieldName": "index_var"
    }
  ],
  "intervals": [
    "2016-03-06T00:00:00/2016-03-06T23:59:59"
  ]
}

----------------------------------------

TITLE: React-Is License
DESCRIPTION: Provides license information for the React-Is production module, which is released under the MIT license.

LANGUAGE: JavaScript
CODE:
/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: ArrayOfDoublesSketch Estimate with Bounds Post-Aggregator
DESCRIPTION: Post-aggregator configuration for estimating distinct key count with error bounds from an ArrayOfDoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToEstimateAndBounds",
  "name": <output name>,
  "field"  : <post aggregator that refers to an ArrayOfDoublesSketch>,
  "numStdDevs": <number from 1 to 3>
}

----------------------------------------

TITLE: Configuring Quantiles Doubles Sketch to Quantile Post-Aggregator
DESCRIPTION: JSON configuration for the quantilesDoublesSketchToQuantile post-aggregator. This returns an approximation of the value at a given fraction in a hypothetical sorted version of the input stream.

LANGUAGE: json
CODE:
{
  "type"  : "quantilesDoublesSketchToQuantile",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,
  "fraction" : <fractional position in the hypothetical sorted stream, number from 0 to 1 inclusive>
}

----------------------------------------

TITLE: Querying Task Completion Report Endpoint
DESCRIPTION: HTTP endpoint pattern for retrieving task completion reports from the Druid Overlord.

LANGUAGE: plaintext
CODE:
http://<OVERLORD-HOST>:<OVERLORD-PORT>/druid/indexer/v1/task/<task-id>/reports

----------------------------------------

TITLE: Starting Tranquility Server
DESCRIPTION: Command to start a Tranquility server instance with a configuration file that manages streaming data ingestion into Druid.

LANGUAGE: bash
CODE:
bin/tranquility server -configFile <path_to_config_file>/server.json

----------------------------------------

TITLE: Shutting Down a Task
DESCRIPTION: Example JSON response when shutting down a task via the /druid/worker/v1/task/{taskid}/shutdown endpoint.

LANGUAGE: json
CODE:
{"task":"index_kafka_wikiticker_f7011f8ffba384b_fpeclode"}

----------------------------------------

TITLE: Time Boundary Query Response Format in Apache Druid
DESCRIPTION: This snippet shows the expected response format for a time boundary query in Apache Druid. It includes the timestamp and result object containing minTime and maxTime.

LANGUAGE: json
CODE:
[ {
  "timestamp" : "2013-05-09T18:24:00.000Z",
  "result" : {
    "minTime" : "2013-05-09T18:24:00.000Z",
    "maxTime" : "2013-05-09T18:37:00.000Z"
  }
} ]

----------------------------------------

TITLE: Configuring Variance Fold Aggregator for Querying in Apache Druid
DESCRIPTION: JSON configuration for the variance fold aggregator used during querying. It specifies the output name, metric name, and estimator for variance calculation.

LANGUAGE: json
CODE:
{
  "type" : "varianceFold",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "estimator" : <string>
}

----------------------------------------

TITLE: Documenting MIT License for React
DESCRIPTION: This code snippet provides license information for the React library, which is under the MIT license and created by Facebook, Inc. and its affiliates.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Displaying Segment Identifier Without Partition Number
DESCRIPTION: Example of a Druid segment identifier for partition number 0, which omits the partition number.

LANGUAGE: plaintext
CODE:
clarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z

----------------------------------------

TITLE: Downloading and Extracting Kafka
DESCRIPTION: Commands to download Kafka 2.1.0 and extract it to the local filesystem.

LANGUAGE: bash
CODE:
curl -O https://archive.apache.org/dist/kafka/2.1.0/kafka_2.12-2.1.0.tgz
tar -xzf kafka_2.12-2.1.0.tgz
cd kafka_2.12-2.1.0

----------------------------------------

TITLE: Documenting MIT License for React use-sync-external-store-shim
DESCRIPTION: This code snippet provides license information for the React use-sync-external-store-shim component, which is under the MIT license and created by Facebook, Inc. and its affiliates.

LANGUAGE: JavaScript
CODE:
/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Configuring Multi InputSpec for Delta Ingestion in Druid
DESCRIPTION: JSON configuration showing how to set up a multi inputSpec combining multiple data sources for delta ingestion, including both existing Druid segments and static file paths.

LANGUAGE: json
CODE:
"ioConfig" : {
  "type" : "hadoop",
  "inputSpec" : {
    "type" : "multi",
    "children": [
      {
        "type" : "dataSource",
        "ingestionSpec" : {
          "dataSource": "wikipedia",
          "intervals": ["2012-01-01T00:00:00.000/2012-01-03T00:00:00.000", "2012-01-05T00:00:00.000/2012-01-07T00:00:00.000"],
          "segments": [
            {
              "dataSource": "test1",
              "interval": "2012-01-01T00:00:00.000/2012-01-03T00:00:00.000",
              "version": "v2",
              "loadSpec": {
                "type": "local",
                "path": "/tmp/index1.zip"
              },
              "dimensions": "host",
              "metrics": "visited_sum,unique_hosts",
              "shardSpec": {
                "type": "none"
              },
              "binaryVersion": 9,
              "size": 2,
              "identifier": "test1_2000-01-01T00:00:00.000Z_3000-01-01T00:00:00.000Z_v2"
            }
          ]
        }
      },
      {
        "type" : "static",
        "paths": "/path/to/more/wikipedia/data/"
      }
    ]  
  }
}

----------------------------------------

TITLE: Executing HTTP POST Query in Apache Druid with Jackson Smile
DESCRIPTION: This snippet shows how to execute a query in Apache Druid using curl, specifying the Accept header as 'application/x-jackson-smile' for Jackson Smile format response.

LANGUAGE: bash
CODE:
curl -X POST '<queryable_host>:<port>/druid/v2/?pretty' -H 'Content-Type:application/json' -H 'Accept:application/x-jackson-smile' -d @<query_json_file>

----------------------------------------

TITLE: Basic Pull-deps Usage with Multiple Extensions
DESCRIPTION: Example command showing how to download multiple Druid extensions (mysql-metadata-storage and druid-rabbitmq) and Hadoop client dependencies with specific versions. Uses the --clean flag to remove existing dependencies first.

LANGUAGE: bash
CODE:
java -classpath "/my/druid/lib/*" org.apache.druid.cli.Main tools pull-deps --clean -c org.apache.druid.extensions:mysql-metadata-storage:0.15.1-incubating -c org.apache.druid.extensions.contrib:druid-rabbitmq:0.15.1-incubating -h org.apache.hadoop:hadoop-client:2.3.0 -h org.apache.hadoop:hadoop-client:2.4.0

----------------------------------------

TITLE: TopN Query Example with Variance and Standard Deviation in Apache Druid
DESCRIPTION: JSON example of a TopN query using variance aggregator and standard deviation post-aggregator. It calculates variance and standard deviation of the 'index' field for top 5 results grouped by 'alias'.

LANGUAGE: json
CODE:
{
  "queryType": "topN",
  "dataSource": "testing",
  "dimensions": ["alias"],
  "threshold": 5,
  "granularity": "all",
  "aggregations": [
    {
      "type": "variance",
      "name": "index_var",
      "fieldName": "index"
    }
  ],
  "postAggregations": [
    {
      "type": "stddev",
      "name": "index_stddev",
      "fieldName": "index_var"
    }
  ],
  "intervals": [
    "2016-03-06T00:00:00/2016-03-06T23:59:59"
  ]
}

----------------------------------------

TITLE: Implementing DoubleMin Aggregator in Druid
DESCRIPTION: The doubleMin aggregator computes the minimum of all metric values and Double.POSITIVE_INFINITY. It requires specifying an output name and the metric field to analyze.

LANGUAGE: json
CODE:
{ "type" : "doubleMin", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: Sample Role Permissions JSON
DESCRIPTION: Example JSON payload for defining role permissions with resource and action definitions.

LANGUAGE: json
CODE:
[
{
  "resource": {
    "name": "wiki.*",
    "type": "DATASOURCE"
  },
  "action": "READ"
},
{
  "resource": {
    "name": "wikiticker",
    "type": "DATASOURCE"
  },
  "action": "WRITE"
}
]

----------------------------------------

TITLE: Example Timeseries Query with Variance
DESCRIPTION: Example of a timeseries query implementing the variance aggregator.

LANGUAGE: json
CODE:
{
  "queryType": "timeseries",
  "dataSource": "testing",
  "granularity": "day",
  "aggregations": [
    {
      "type": "variance",
      "name": "index_var",
      "fieldName": "index_var"
    }
  ],
  "intervals": [
    "2016-03-01T00:00:00.000/2013-03-20T00:00:00.000"
  ]
}

----------------------------------------

TITLE: Configuring StaticS3Firehose for S3 Object Ingestion in Apache Druid
DESCRIPTION: This JSON snippet demonstrates how to configure a StaticS3Firehose in Apache Druid. It specifies the firehose type as 'static-s3' and provides a list of S3 object URIs to ingest. This firehose is splittable and can be used with native parallel index tasks.

LANGUAGE: json
CODE:
"firehose" : {
    "type" : "static-s3",
    "uris": ["s3://foo/bar/file.gz", "s3://bar/foo/file2.gz"]
}

----------------------------------------

TITLE: React Is Production License
DESCRIPTION: Copyright notice and MIT license information for the react-is.production.min.js file from React.

LANGUAGE: JavaScript
CODE:
/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Copyright Notice for mark.js
DESCRIPTION: Copyright notice for the mark.js library by Julian Khnel, version 8.11.1, released under the MIT license.

LANGUAGE: JavaScript
CODE:
/*!***************************************************
* mark.js v8.11.1
* https://markjs.io/
* Copyright (c) 20142018, Julian Khnel
* Released under the MIT license https://git.io/vwTVl
****************************************************/

----------------------------------------

TITLE: Duration Granularity Configuration
DESCRIPTION: Examples of configuring duration-based granularity in Druid, showing both basic duration and origin-based duration settings.

LANGUAGE: javascript
CODE:
{"type": "duration", "duration": 7200000}

{"type": "duration", "duration": 3600000, "origin": "2012-01-01T00:30:00Z"}

----------------------------------------

TITLE: Implementing JavaScript Post-Aggregator in Druid
DESCRIPTION: Shows how to create a JavaScript-based post-aggregator that applies custom JavaScript functions to aggregated fields.

LANGUAGE: json
CODE:
{
  "type": "javascript",
  "name": "absPercent",
  "fieldNames": ["delta", "total"],
  "function": "function(delta, total) { return 100 * Math.abs(delta) / total; }"
}

----------------------------------------

TITLE: Post Averager Moving Average Query in Druid
DESCRIPTION: Advanced query showing how to calculate both a 7-bucket moving average and a ratio between current period and moving average.

LANGUAGE: json
CODE:
{
  "queryType": "movingAverage",
  "dataSource": "wikipedia",
  "granularity": {
    "type": "period",
    "period": "PT30M"
  },
  "intervals": [
    "2015-09-12T22:00:00Z/2015-09-13T00:00:00Z"
  ],
  "aggregations": [
    {
      "name": "delta30Min",
      "fieldName": "delta",
      "type": "longSum"
    }
  ],
  "averagers": [
    {
      "name": "trailing30MinChanges",
      "fieldName": "delta30Min",
      "type": "longMean",
      "buckets": 7
    }
  ],
  "postAveragers" : [
    {
      "name": "ratioTrailing30MinChanges",
      "type": "arithmetic",
      "fn": "/",
      "fields": [
        {
          "type": "fieldAccess",
          "fieldName": "delta30Min"
        },
        {
          "type": "fieldAccess",
          "fieldName": "trailing30MinChanges"
        }
      ]
    }
  ]
}

----------------------------------------

TITLE: Executing Select Query in Apache Druid
DESCRIPTION: This JSON snippet demonstrates a basic Select query in Apache Druid. It specifies the query type, data source, time interval, and paging configuration to retrieve raw data rows.

LANGUAGE: json
CODE:
{
   "queryType": "select",
   "dataSource": "wikipedia",
   "descending": "false",
   "dimensions":[],
   "metrics":[],
   "granularity": "all",
   "intervals": [
     "2013-01-01/2013-01-02"
   ],
   "pagingSpec":{"pagingIdentifiers": {}, "threshold":5}
 }

----------------------------------------

TITLE: Task Submission and Query Commands
DESCRIPTION: Commands to submit the ingestion task and query the ingested data using Druid SQL.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/ingestion-tutorial-index.json
bin/dsql

----------------------------------------

TITLE: Downloading and Setting up Tranquility Server
DESCRIPTION: Commands to download, extract and set up Tranquility distribution package for use with Apache Druid.

LANGUAGE: bash
CODE:
curl http://static.druid.io/tranquility/releases/tranquility-distribution-0.8.3.tgz -o tranquility-distribution-0.8.3.tgz
tar -xzf tranquility-distribution-0.8.3.tgz
mv tranquility-distribution-0.8.3 tranquility

----------------------------------------

TITLE: Mapping Druid Metrics to StatsD Events in JSON
DESCRIPTION: Example JSON configuration for mapping Druid metrics to StatsD events. Demonstrates how to specify dimensions, event type, and range conversion for different metrics.

LANGUAGE: json
CODE:
{
  "query/time": { 
    "dimensions": ["dataSource", "type"], 
    "type": "timer"
  },
  "coordinator-segment/count": { 
    "dimensions": ["dataSource"], 
    "type": "gauge" 
  },
  "historical-segment/count": { 
    "dimensions": ["dataSource", "tier", "priority"], 
    "type": "gauge" 
  }
}

----------------------------------------

TITLE: ResetCluster Help Output
DESCRIPTION: Displays the complete help documentation showing all available options and their descriptions for the ResetCluster tool.

LANGUAGE: text
CODE:
NAME
        druid tools reset-cluster - Cleanup all persisted state from metadata
        and deep storage.

SYNOPSIS
        druid tools reset-cluster [--all] [--hadoopWorkingPath]
                [--metadataStore] [--segmentFiles] [--taskLogs]

OPTIONS
        --all
            delete all state stored in metadata and deep storage

        --hadoopWorkingPath
            delete hadoopWorkingPath

        --metadataStore
            delete all records in metadata storage

        --segmentFiles
            delete all segment files from deep storage

        --taskLogs
            delete all tasklogs

----------------------------------------

TITLE: Displaying Segment Identifier Example
DESCRIPTION: Example of a Druid segment identifier showing datasource name, time interval, version number, and partition number.

LANGUAGE: plaintext
CODE:
clarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z_1

----------------------------------------

TITLE: Starting Kafka Server
DESCRIPTION: Command to start the Kafka broker using default configuration.

LANGUAGE: bash
CODE:
./bin/kafka-server-start.sh config/server.properties

----------------------------------------

TITLE: Running Hadoop Indexer Command
DESCRIPTION: Command to execute the Druid Command Line Hadoop Indexer. Requires Java with specific JVM settings for timezone and encoding.

LANGUAGE: bash
CODE:
java -Xmx256m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -classpath lib/*:<hadoop_config_dir> org.apache.druid.cli.Main index hadoop <spec_file>

----------------------------------------

TITLE: Sample Result of Data Source Metadata Query in JSON for Apache Druid
DESCRIPTION: This snippet demonstrates the format of the result returned by a Data Source Metadata query in Apache Druid. It includes the timestamp and the maxIngestedEventTime.

LANGUAGE: json
CODE:
[ {
  "timestamp" : "2013-05-09T18:24:00.000Z",
  "result" : {
    "maxIngestedEventTime" : "2013-05-09T18:24:09.007Z"
  }
} ]

----------------------------------------

TITLE: Sample JSON Output Format
DESCRIPTION: Example of a single row output from the DumpSegment tool showing various fields and their values. The output is formatted as newline-separated JSON objects.

LANGUAGE: json
CODE:
{
  "__time": 1442018818771,
  "added": 36,
  "channel": "#en.wikipedia",
  "cityName": null,
  "comment": "added project",
  "count": 1,
  "countryIsoCode": null,
  "countryName": null,
  "deleted": 0,
  "delta": 36,
  "isAnonymous": "false",
  "isMinor": "false",
  "isNew": "false",
  "isRobot": "false",
  "isUnpatrolled": "false",
  "iuser": "00001553",
  "metroCode": null,
  "namespace": "Talk",
  "page": "Talk:Oswald Tilghman",
  "regionIsoCode": null,
  "regionName": null,
  "user": "GELongstreet"
}

----------------------------------------

TITLE: Sample JSON Data for Druid Ingestion
DESCRIPTION: This snippet contains sample JSON data representing animal information with timestamp, animal type, location, and number fields.

LANGUAGE: json
CODE:
[
  {"timestamp":"2018-01-01T07:01:35Z","animal":"octopus",  "location":1, "number":100},
  {"timestamp":"2018-01-01T05:01:35Z","animal":"mongoose", "location":2,"number":200},
  {"timestamp":"2018-01-01T06:01:35Z","animal":"snake", "location":3, "number":300},
  {"timestamp":"2018-01-01T01:01:35Z","animal":"lion", "location":4, "number":300}
]

----------------------------------------

TITLE: Sum Aggregators in Druid
DESCRIPTION: Three types of sum aggregators - longSum (64-bit integer), doubleSum (64-bit float), and floatSum (32-bit float). Each computes the sum of values from a specified metric column.

LANGUAGE: json
CODE:
{ "type" : "longSum", "name" : <output_name>, "fieldName" : <metric_name> }

LANGUAGE: json
CODE:
{ "type" : "doubleSum", "name" : <output_name>, "fieldName" : <metric_name> }

LANGUAGE: json
CODE:
{ "type" : "floatSum", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: Implementing an IN Filter in Druid JSON
DESCRIPTION: Example of an IN filter that matches a dimension against a list of values. This is equivalent to the SQL IN operator.

LANGUAGE: json
CODE:
{
    "type": "in",
    "dimension": "outlaw",
    "values": ["Good", "Bad", "Ugly"]
}

----------------------------------------

TITLE: Configuring ListFiltered DimensionSpec in Apache Druid JSON
DESCRIPTION: Shows how to configure a ListFiltered DimensionSpec, which acts as a whitelist or blacklist for values in multi-value dimensions. It includes options for delegate DimensionSpec, values to filter, and whether it's a whitelist or blacklist.

LANGUAGE: JSON
CODE:
{ "type" : "listFiltered", "delegate" : <dimensionSpec>, "values": <array of strings>, "isWhitelist": <optional attribute for true/false, default is true> }

----------------------------------------

TITLE: Example HDFS Directory Structure
DESCRIPTION: Sample directory structure showing how segments are organized in HDFS storage.

LANGUAGE: plaintext
CODE:
Directory path: /druid/storage/wikipedia

 2013-08-31T000000.000Z_2013-09-01T000000.000Z
    2015-10-21T22_07_57.074Z
            0_descriptor.json
            0_index.zip
 2013-09-01T000000.000Z_2013-09-02T000000.000Z
    2015-10-21T22_07_57.074Z
            0_descriptor.json
            0_index.zip
 2013-09-02T000000.000Z_2013-09-03T000000.000Z
    2015-10-21T22_07_57.074Z
            0_descriptor.json
            0_index.zip
 2013-09-03T000000.000Z_2013-09-04T000000.000Z
     2015-10-21T22_07_57.074Z
             0_descriptor.json
             0_index.zip

----------------------------------------

TITLE: Configuring Loading On-heap Guava Cache in Druid
DESCRIPTION: Example configuration for a loading lookup using on-heap Guava cache with custom settings for the reverse lookup cache.

LANGUAGE: json
CODE:
{
   "type":"loadingLookup",
   "dataFetcher":{ "type":"jdbcDataFetcher", "connectorConfig":"jdbc://mysql://localhost:3306/my_data_base", "table":"lookup_table_name", "keyColumn":"key_column_name", "valueColumn": "value_column_name"},
   "loadingCacheSpec":{"type":"guava"},
   "reverseLoadingCacheSpec":{"type":"guava", "maximumSize":500000, "expireAfterAccess":100000, "expireAfterAccess":10000}
}

----------------------------------------

TITLE: Quantile Post Aggregator Configuration
DESCRIPTION: JSON configuration for the quantile post aggregator that returns an approximation of a value at a given fraction in the sorted stream.

LANGUAGE: json
CODE:
{
  "type"  : "quantilesDoublesSketchToQuantile",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch>,
  "fraction" : <fractional position in the hypothetical sorted stream>
}

----------------------------------------

TITLE: Example Multi-Interval Segment Naming
DESCRIPTION: Demonstrates segment naming across multiple time intervals, showing how segments are organized for different time periods.

LANGUAGE: plaintext
CODE:
foo_2015-01-01/2015-01-02_v1_0
foo_2015-01-02/2015-01-03_v1_1
foo_2015-01-03/2015-01-04_v1_2

----------------------------------------

TITLE: Configuring Environment Variable Password Provider in Druid
DESCRIPTION: JSON configuration for setting up an environment variable-based password provider. This provider reads passwords from specified environment variables instead of storing them in configuration files.

LANGUAGE: json
CODE:
{ "type": "environment", "variable": "METADATA_STORAGE_PASSWORD" }

----------------------------------------

TITLE: ResetCluster All-Components Command
DESCRIPTION: Shows how to reset all cluster components at once using the --all flag.

LANGUAGE: bash
CODE:
java org.apache.druid.cli.Main tools reset-cluster --all

----------------------------------------

TITLE: Loading DataSketches Extension in Druid Configuration
DESCRIPTION: Configuration snippet showing how to include the DataSketches extension in Druid's configuration file.

LANGUAGE: text
CODE:
druid.extensions.loadList=["druid-datasketches"]

----------------------------------------

TITLE: Configuring Graphite Emitter with White-List Converter in JSON
DESCRIPTION: JSON configuration for the Graphite Emitter using the 'whiteList' event converter. This setup sends only white-listed metrics and dimensions to Graphite, with a custom map file path.

LANGUAGE: json
CODE:
{
  "druid.emitter.graphite.eventConverter": {
    "type": "whiteList",
    "namespacePrefix": "druid.test",
    "ignoreHostname": true,
    "ignoreServiceName": true,
    "mapPath": "/pathPrefix/fileName.json"
  }
}

----------------------------------------

TITLE: React-DOM License
DESCRIPTION: Declares the license for the React-DOM production module, which is released under the MIT license.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Configuring ListFiltered DimensionSpec in Apache Druid JSON
DESCRIPTION: Shows how to configure a ListFiltered DimensionSpec, which acts as a whitelist or blacklist for values in multi-value dimensions. It includes options for delegate DimensionSpec, values to filter, and whether it's a whitelist or blacklist.

LANGUAGE: JSON
CODE:
{ "type" : "listFiltered", "delegate" : <dimensionSpec>, "values": <array of strings>, "isWhitelist": <optional attribute for true/false, default is true> }

----------------------------------------

TITLE: Declaring MIT License for React DOM
DESCRIPTION: Copyright notice and MIT license declaration for the React DOM production build.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Creating Supervisor HTTP Request
DESCRIPTION: Example cURL command for submitting a Kinesis supervisor specification.

LANGUAGE: bash
CODE:
curl -X POST -H 'Content-Type: application/json' -d @supervisor-spec.json http://localhost:8090/druid/indexer/v1/supervisor

----------------------------------------

TITLE: Implementing Arithmetic Post-Aggregator in Druid
DESCRIPTION: Defines an arithmetic post-aggregator that applies mathematical operations to aggregated fields. Supports operations like +, -, *, /, and quotient with optional numeric ordering.

LANGUAGE: json
CODE:
{
  "type"  : "arithmetic",
  "name"  : <output_name>,
  "fn"    : <arithmetic_function>,
  "fields": [<post_aggregator>, <post_aggregator>, ...],
  "ordering" : <null (default), or "numericFirst">
}

----------------------------------------

TITLE: Creating Kafka Topic
DESCRIPTION: Command to create a Kafka topic named 'wikipedia' with single partition and replication factor.

LANGUAGE: bash
CODE:
./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic wikipedia

----------------------------------------

TITLE: Distinct Count Estimate Post-Aggregator
DESCRIPTION: Post-aggregator configuration for estimating distinct key count from an ArrayOfDoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToEstimate",
  "name": <output name>,
  "field"  : <post aggregator that refers to an ArrayOfDoublesSketch>
}

----------------------------------------

TITLE: Configuring TimeMax Aggregator in Apache Druid
DESCRIPTION: JSON configuration for adding a timeMax aggregator at ingestion time. This aggregator calculates the maximum timestamp for a specified field.

LANGUAGE: json
CODE:
{
    "type": "timeMax",
    "name": "tmax",
    "fieldName": "<field_name, typically column specified in timestamp spec>"
}

----------------------------------------

TITLE: Response Example - Worker Enabled Status API
DESCRIPTION: Example JSON response from the MiddleManager worker enabled status endpoint, showing enabled state for a specific host:port.

LANGUAGE: json
CODE:
{"localhost:8091":true}

----------------------------------------

TITLE: Configuring Registered Lookup Extraction Function in Druid
DESCRIPTION: Refers to a lookup that has been registered in the cluster-wide configuration for dimension value replacement.

LANGUAGE: json
CODE:
{
  "type":"registeredLookup",
  "lookup":"some_lookup_name",
  "retainMissingValue":true
}

----------------------------------------

TITLE: Example HDFS Directory Structure
DESCRIPTION: Sample directory structure showing how segments are organized in HDFS storage.

LANGUAGE: plaintext
CODE:
Directory path: /druid/storage/wikipedia

 2013-08-31T000000.000Z_2013-09-01T000000.000Z
    2015-10-21T22_07_57.074Z
            0_descriptor.json
            0_index.zip
 2013-09-01T000000.000Z_2013-09-02T000000.000Z
    2015-10-21T22_07_57.074Z
            0_descriptor.json
            0_index.zip
 2013-09-02T000000.000Z_2013-09-03T000000.000Z
    2015-10-21T22_07_57.074Z
            0_descriptor.json
            0_index.zip
 2013-09-03T000000.000Z_2013-09-04T000000.000Z
     2015-10-21T22_07_57.074Z
             0_descriptor.json
             0_index.zip

----------------------------------------

TITLE: Documenting MIT License for object-assign
DESCRIPTION: This code snippet provides license information for the object-assign library, which is under the MIT license and created by Sindre Sorhus.

LANGUAGE: JavaScript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

----------------------------------------

TITLE: Starting Tranquility Kafka
DESCRIPTION: Command to start Tranquility Kafka integration for loading data from Kafka into Druid. Note that this method is deprecated in favor of the Kafka Indexing Service.

LANGUAGE: bash
CODE:
bin/tranquility kafka -configFile <path_to_config_file>/kafka.json

----------------------------------------

TITLE: Querying Rolled-up Data in Apache Druid using DSQL
DESCRIPTION: This SQL query retrieves all data from the 'rollup-tutorial' datasource, demonstrating the effects of roll-up on the ingested data.

LANGUAGE: sql
CODE:
select * from "rollup-tutorial";

----------------------------------------

TITLE: PostgreSQL Import SQL Commands
DESCRIPTION: SQL commands for importing exported metadata into PostgreSQL database

LANGUAGE: sql
CODE:
COPY druid_segments(id,dataSource,created_date,start,\"end\",partitioned,version,used,payload) FROM '/tmp/csv/druid_segments.csv' DELIMITER ',' CSV;\n\nCOPY druid_rules(id,dataSource,version,payload) FROM '/tmp/csv/druid_rules.csv' DELIMITER ',' CSV;\n\nCOPY druid_config(name,payload) FROM '/tmp/csv/druid_config.csv' DELIMITER ',' CSV;\n\nCOPY druid_dataSource(dataSource,created_date,commit_metadata_payload,commit_metadata_sha1) FROM '/tmp/csv/druid_dataSource.csv' DELIMITER ',' CSV;\n\nCOPY druid_supervisors(id,spec_id,created_date,payload) FROM '/tmp/csv/druid_supervisors.csv' DELIMITER ',' CSV;

----------------------------------------

TITLE: Copyright and License Notice for mark.js
DESCRIPTION: This snippet provides the copyright and license information for mark.js, a text highlighting library, which is licensed under the MIT license.

LANGUAGE: JavaScript
CODE:
/*!***************************************************
* mark.js v8.11.1
* https://markjs.io/
* Copyright (c) 20142018, Julian Khnel
* Released under the MIT license https://git.io/vwTVl
****************************************************/

----------------------------------------

TITLE: Preparing and Loading Sample Data
DESCRIPTION: Commands to prepare sample data and send it to Kafka using the console producer.

LANGUAGE: bash
CODE:
cd quickstart/tutorial
gunzip -k wikiticker-2015-09-12-sampled.json.gz
export KAFKA_OPTS="-Dfile.encoding=UTF-8"
./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic wikipedia < {PATH_TO_DRUID}/quickstart/tutorial/wikiticker-2015-09-12-sampled.json

----------------------------------------

TITLE: Expression Virtual Column Syntax
DESCRIPTION: Defines the basic structure for creating an expression-based virtual column, including required name and expression fields, and optional outputType that defaults to FLOAT.

LANGUAGE: json
CODE:
{
  "type": "expression",
  "name": <name of the virtual column>,
  "expression": <row expression>,
  "outputType": <output value type of expression>
}

----------------------------------------

TITLE: Executing Kill Task in Druid
DESCRIPTION: Command to submit a Kill Task that permanently removes disabled segments from both metadata and deep storage.

LANGUAGE: bash
CODE:
curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/deletion-kill.json http://localhost:8090/druid/indexer/v1/task

----------------------------------------

TITLE: Displaying Segment Naming Convention in Apache Druid
DESCRIPTION: Demonstrates the naming convention for Druid segments, showing how datasource, interval, version, and partition number are represented in segment names.

LANGUAGE: plaintext
CODE:
foo_2015-01-01/2015-01-02_v1_0
foo_2015-01-01/2015-01-02_v1_1
foo_2015-01-01/2015-01-02_v1_2

----------------------------------------

TITLE: Displaying Segment Naming Convention in Apache Druid
DESCRIPTION: Demonstrates the naming convention for Druid segments, showing how datasource, interval, version, and partition number are represented in segment names.

LANGUAGE: plaintext
CODE:
foo_2015-01-01/2015-01-02_v1_0
foo_2015-01-01/2015-01-02_v1_1
foo_2015-01-01/2015-01-02_v1_2

----------------------------------------

TITLE: Druid Search Query Response Format
DESCRIPTION: Example response format showing search results grouped by timestamp. Each result includes matching dimension values with their counts.

LANGUAGE: json
CODE:
[
  {
    "timestamp": "2012-01-01T00:00:00.000Z",
    "result": [
      {
        "dimension": "dim1",
        "value": "Ke$ha",
        "count": 3
      },
      {
        "dimension": "dim2",
        "value": "Ke$haForPresident",
        "count": 1
      }
    ]
  },
  {
    "timestamp": "2012-01-02T00:00:00.000Z",
    "result": [
      {
        "dimension": "dim1",
        "value": "SomethingThatContainsKe",
        "count": 1
      },
      {
        "dimension": "dim2",
        "value": "SomethingElseThatContainsKe",
        "count": 2
      }
    ]
  }
]

----------------------------------------

TITLE: Testing Kafka Rename Functionality
DESCRIPTION: Bash command for testing Kafka rename functionality using the console producer. Allows sending key-value pairs to rename dimension values.

LANGUAGE: bash
CODE:
./bin/kafka-console-producer.sh --property parse.key=true --property key.separator="->" --broker-list localhost:9092 --topic testTopic

----------------------------------------

TITLE: Configuring ArrayOfDoublesSketch Aggregator in Druid
DESCRIPTION: This JSON configuration defines an ArrayOfDoublesSketch aggregator in Druid, specifying parameters like output name, input field, nominal entries, number of values, and metric columns.

LANGUAGE: json
CODE:
{
  "type" : "arrayOfDoublesSketch",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "nominalEntries": <number>,
  "numberOfValues" : <number>,
  "metricColumns" : <array of strings>
 }

----------------------------------------

TITLE: Sample JSON Data for Druid Ingestion
DESCRIPTION: Example JSON data containing timestamp, animal, location, and number fields to be ingested into Druid.

LANGUAGE: json
CODE:
[
  {"timestamp":"2018-01-01T07:01:35Z","animal":"octopus",  "location":1, "number":100},
  {"timestamp":"2018-01-01T05:01:35Z","animal":"mongoose", "location":2,"number":200},
  {"timestamp":"2018-01-01T06:01:35Z","animal":"snake", "location":3, "number":300},
  {"timestamp":"2018-01-01T01:01:35Z","animal":"lion", "location":4, "number":300}
]

----------------------------------------

TITLE: Configuring Aggregators for Flattened JSON Fields in Druid
DESCRIPTION: This snippet shows how to configure aggregators in Druid to use the metric column names defined in the flattenSpec. It includes examples of longSum and doubleSum aggregators for flattened fields.

LANGUAGE: json
CODE:
"metricsSpec" : [ 
{
  "type" : "longSum",
  "name" : "path-metric-sum",
  "fieldName" : "path-metric"
}, 
{
  "type" : "doubleSum",
  "name" : "hello-0-sum",
  "fieldName" : "hello-0"
},
{
  "type" : "longSum",
  "name" : "metrica-sum",
  "fieldName" : "metrica"
}
]

----------------------------------------

TITLE: Querying sys.servers Table
DESCRIPTION: SQL query to retrieve information about all servers from sys.servers

LANGUAGE: sql
CODE:
SELECT * FROM sys.servers;

----------------------------------------

TITLE: Simple Raw Data Example in Druid
DESCRIPTION: Sample raw data entries showing timestamp-based records in Druid's ingestion format.

LANGUAGE: json
CODE:
{"timestamp": "2013-08-31T01:02:33Z", "page": "AAA", "language" : "en"}
{"timestamp": "2013-09-01T01:02:33Z", "page": "BBB", "language" : "en"}
{"timestamp": "2013-09-02T23:32:45Z", "page": "CCC", "language" : "en"}
{"timestamp": "2013-09-03T03:32:45Z", "page": "DDD", "language" : "en"}

----------------------------------------

TITLE: Dependency License Declarations
DESCRIPTION: Collection of license and copyright notices for third-party dependencies used in the project. All components are licensed under MIT.

LANGUAGE: javascript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

/*!***************************************************
* mark.js v8.11.1
* https://markjs.io/
* Copyright (c) 20142018, Julian Khnel
* Released under the MIT license https://git.io/vwTVl
*****************************************************/

/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Example Druid Query Using Bloom Filter Aggregator
DESCRIPTION: This JSON snippet provides a complete example of a Druid timeseries query using a Bloom filter aggregator, including query parameters and expected response format.

LANGUAGE: json
CODE:
{
  "queryType": "timeseries",
  "dataSource": "wikiticker",
  "intervals": [ "2015-09-12T00:00:00.000/2015-09-13T00:00:00.000" ],
  "granularity": "day",
  "aggregations": [
    {
      "type": "bloom",
      "name": "userBloom",
      "maxNumEntries": 100000,
      "field": {
        "type":"default",
        "dimension":"user",
        "outputType": "STRING"
      }
    }
  ]
}

----------------------------------------

TITLE: Min/Max Aggregators in Druid
DESCRIPTION: Configurations for minimum and maximum value aggregators across double, float and long data types.

LANGUAGE: json
CODE:
{ "type" : "doubleMin", "name" : <output_name>, "fieldName" : <metric_name> }

LANGUAGE: json
CODE:
{ "type" : "doubleMax", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: Configuring OpenTSDB Metrics Mapping in JSON
DESCRIPTION: JSON configuration example showing how to map Druid metrics to OpenTSDB dimensions. This configuration defines which metrics and their associated dimensions should be sent to OpenTSDB.

LANGUAGE: json
CODE:
"query/time": [
    "dataSource",
    "type"
]

----------------------------------------

TITLE: Sample Network Flow Data in JSON
DESCRIPTION: Example network flow data containing IP addresses, ports, protocol numbers and traffic metrics

LANGUAGE: json
CODE:
{"ts":"2018-01-01T01:01:35Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2", "srcPort":2000, "dstPort":3000, "protocol": 6, "packets":10, "bytes":1000, "cost": 1.4}

----------------------------------------

TITLE: Adding Jackson Relocation to Maven Shade Plugin
DESCRIPTION: This XML snippet demonstrates how to configure the Maven Shade Plugin to relocate Jackson packages, which can help resolve conflicts between Druid and Hadoop Jackson versions.

LANGUAGE: xml
CODE:
<plugin>
     <groupId>org.apache.maven.plugins</groupId>
     <artifactId>maven-shade-plugin</artifactId>
     <executions>
         <execution>
             <phase>package</phase>
             <goals>
                 <goal>shade</goal>
             </goals>
             <configuration>
                 <outputFile>
                     ${project.build.directory}/${project.artifactId}-${project.version}-selfcontained.jar
                 </outputFile>
                 <relocations>
                     <relocation>
                         <pattern>com.fasterxml.jackson</pattern>
                         <shadedPattern>shade.com.fasterxml.jackson</shadedPattern>
                     </relocation>
                 </relocations>
                 <artifactSet>
                     <includes>
                         <include>*:*</include>
                     </includes>
                 </artifactSet>
                 <filters>
                     <filter>
                         <artifact>*:*</artifact>
                         <excludes>
                             <exclude>META-INF/*.SF</exclude>
                             <exclude>META-INF/*.DSA</exclude>
                             <exclude>META-INF/*.RSA</exclude>
                         </excludes>
                     </filter>
                 </filters>
                 <transformers>
                     <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                 </transformers>
             </configuration>
         </execution>
     </executions>
 </plugin>

----------------------------------------

TITLE: Configuring Hadoop InputSpec for Druid DataSource Reindexing
DESCRIPTION: This JSON snippet demonstrates how to configure the ioConfig section of a Hadoop batch ingestion task to reindex data from an existing Druid dataSource. It specifies the dataSource name and time intervals to process.

LANGUAGE: json
CODE:
"ioConfig" : {
  "type" : "hadoop",
  "inputSpec" : {
    "type" : "dataSource",
    "ingestionSpec" : {
      "dataSource": "wikipedia",
      "intervals": ["2014-10-20T00:00:00Z/P2W"]
    }
  },
  ...
}

----------------------------------------

TITLE: Resetting Kafka State
DESCRIPTION: Command to remove Kafka logs after completing the Kafka streaming tutorial, allowing for a clean state reset.

LANGUAGE: bash
CODE:
rm -rf /tmp/kafka-logs

----------------------------------------

TITLE: Building Hadoop Docker Image
DESCRIPTION: Commands to build a Docker image for Hadoop 2.8.3 cluster from the provided Dockerfile.

LANGUAGE: bash
CODE:
cd quickstart/tutorial/hadoop/docker
docker build -t druid-hadoop-demo:2.8.3 .

----------------------------------------

TITLE: Buckets Post-Aggregator Configuration
DESCRIPTION: Configuration for the buckets post-aggregator that allows custom bucket size and offset definition.

LANGUAGE: json
CODE:
{
  "type": "buckets",
  "name": "<output_name>",
  "fieldName": "<aggregator_name>",
  "bucketSize": <bucket_size>,
  "offset": <offset>
}

----------------------------------------

TITLE: Druid Ingestion Spec - Final Configuration
DESCRIPTION: Complete ingestion specification for batch loading network flow data, including data schema, input configuration, and tuning parameters.

LANGUAGE: json
CODE:
{
  "type" : "index",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "ingestion-tutorial",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "timestampSpec" : {
            "format" : "iso",
            "column" : "ts"
          },
          "dimensionsSpec" : {
            "dimensions": [
              "srcIP",
              { "name" : "srcPort", "type" : "long" },
              { "name" : "dstIP", "type" : "string" },
              { "name" : "dstPort", "type" : "long" },
              { "name" : "protocol", "type" : "string" }
            ]
          }      
        }
      },
      "metricsSpec" : [
        { "type" : "count", "name" : "count" },
        { "type" : "longSum", "name" : "packets", "fieldName" : "packets" },
        { "type" : "longSum", "name" : "bytes", "fieldName" : "bytes" },
        { "type" : "doubleSum", "name" : "cost", "fieldName" : "cost" }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "HOUR",
        "queryGranularity" : "MINUTE",
        "intervals" : ["2018-01-01/2018-01-02"],
        "rollup" : true
      }
    },
    "ioConfig" : {
      "type" : "index",
      "firehose" : {
        "type" : "local",
        "baseDir" : "quickstart/",
        "filter" : "ingestion-tutorial-data.json"
      }
    },
    "tuningConfig" : {
      "type" : "index",
      "maxRowsPerSegment" : 5000000
    }
  }
}

----------------------------------------

TITLE: Combining and Overwriting Data in Druid
DESCRIPTION: This command submits a task that combines existing data with new data and overwrites the original data in the 'updates-tutorial' datasource.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/updates-append-index.json

----------------------------------------

TITLE: Downloading and Extracting Kafka
DESCRIPTION: Commands to download and extract Apache Kafka 0.10.2.2 for use with Druid.

LANGUAGE: bash
CODE:
curl -O https://archive.apache.org/dist/kafka/0.10.2.2/kafka_2.12-0.10.2.2.tgz
tar -xzf kafka_2.12-0.10.2.2.tgz
cd kafka_2.12-0.10.2.2

----------------------------------------

TITLE: Configuring Event Receiver Firehose in Druid
DESCRIPTION: Configuration for EventReceiverFirehose that ingests events via HTTP endpoint. Used with stream-pull ingestion and Tranquility stream push.

LANGUAGE: json
CODE:
{
  "type": "receiver",
  "serviceName": "eventReceiverServiceName",
  "bufferSize": 10000
}

----------------------------------------

TITLE: Displaying Segment Identifier Example in Druid
DESCRIPTION: Shows the format of a segment identifier in Druid, including datasource name, time interval, version number, and partition number.

LANGUAGE: plaintext
CODE:
clarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z_1

----------------------------------------

TITLE: Configuring Dependencies and Assembly for Druid SBT Build
DESCRIPTION: Defines library dependencies for Druid projects including core modules, extensions, and supporting libraries. Includes specific version requirements and exclusion rules. Also configures assembly merge strategies for handling duplicate files during JAR assembly.

LANGUAGE: scala
CODE:
libraryDependencies ++= Seq(
  "com.amazonaws" % "aws-java-sdk" % "1.9.23" exclude("common-logging", "common-logging"),
  "org.joda" % "joda-convert" % "1.7",
  "joda-time" % "joda-time" % "2.7",
  "org.apache.druid" % "druid" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid" % "druid-services" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid" % "druid-indexing-service" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid" % "druid-indexing-hadoop" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid.extensions" % "mysql-metadata-storage" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid.extensions" % "druid-s3-extensions" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid.extensions" % "druid-histogram" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "org.apache.druid.extensions" % "druid-hdfs-storage" % "0.8.1" excludeAll (
    ExclusionRule("org.ow2.asm"),
    ExclusionRule("com.fasterxml.jackson.core"),
    ExclusionRule("com.fasterxml.jackson.datatype"),
    ExclusionRule("com.fasterxml.jackson.dataformat"),
    ExclusionRule("com.fasterxml.jackson.jaxrs"),
    ExclusionRule("com.fasterxml.jackson.module")
  ),
  "com.fasterxml.jackson.core" % "jackson-annotations" % "2.3.0",
  "com.fasterxml.jackson.core" % "jackson-core" % "2.3.0",
  "com.fasterxml.jackson.core" % "jackson-databind" % "2.3.0",
  "com.fasterxml.jackson.datatype" % "jackson-datatype-guava" % "2.3.0",
  "com.fasterxml.jackson.datatype" % "jackson-datatype-joda" % "2.3.0",
  "com.fasterxml.jackson.jaxrs" % "jackson-jaxrs-base" % "2.3.0",
  "com.fasterxml.jackson.jaxrs" % "jackson-jaxrs-json-provider" % "2.3.0",
  "com.fasterxml.jackson.jaxrs" % "jackson-jaxrs-smile-provider" % "2.3.0",
  "com.fasterxml.jackson.module" % "jackson-module-jaxb-annotations" % "2.3.0",
  "com.sun.jersey" % "jersey-servlet" % "1.17.1",
  "mysql" % "mysql-connector-java" % "5.1.34",
  "org.scalatest" %% "scalatest" % "2.2.3" % "test",
  "org.mockito" % "mockito-core" % "1.10.19" % "test"
)

assemblyMergeStrategy in assembly := {
  case path if path contains "pom." => MergeStrategy.first
  case path if path contains "javax.inject.Named" => MergeStrategy.first
  case path if path contains "mime.types" => MergeStrategy.first
  case path if path contains "org/apache/commons/logging/impl/SimpleLog.class" => MergeStrategy.first
  case path if path contains "org/apache/commons/logging/impl/SimpleLog$1.class" => MergeStrategy.first
  case path if path contains "org/apache/commons/logging/impl/NoOpLog.class" => MergeStrategy.first
  case path if path contains "org/apache/commons/logging/LogFactory.class" => MergeStrategy.first
  case path if path contains "org/apache/commons/logging/LogConfigurationException.class" => MergeStrategy.first
  case path if path contains "org/apache/commons/logging/Log.class" => MergeStrategy.first
  case path if path contains "META-INF/jersey-module-version" => MergeStrategy.first
  case path if path contains ".properties" => MergeStrategy.first
  case path if path contains ".class" => MergeStrategy.first
  case x =>
    val oldStrategy = (assemblyMergeStrategy in assembly).value
    oldStrategy(x)
}

----------------------------------------

TITLE: Creating Quantiles Sketch from ArrayOfDoublesSketch Column
DESCRIPTION: This JSON configuration defines a post-aggregator to create a quantiles sketch from a specified column of an ArrayOfDoublesSketch in Druid.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToQuantilesSketch",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,
  "column" : <number>,
  "k" : <parameter that determines the accuracy and size of the quantiles sketch>
}

----------------------------------------

TITLE: Task Shutdown JSON Response
DESCRIPTION: Example JSON response when shutting down a specific task

LANGUAGE: json
CODE:
{"task":"index_kafka_wikiticker_f7011f8ffba384b_fpeclode"}

----------------------------------------

TITLE: Configuring Parquet-Avro Parser with Avro ParseSpec
DESCRIPTION: Example configuration for ingesting Parquet files using the parquet-avro parser with field flattening and discovery. Demonstrates setup for Avro-based conversion of Parquet data.

LANGUAGE: JSON
CODE:
{
  "type": "index_hadoop",
  "spec": {
    "ioConfig": {
      "type": "hadoop",
      "inputSpec": {
        "type": "static",
        "inputFormat": "org.apache.druid.data.input.parquet.DruidParquetAvroInputFormat",
        "paths": "path/to/file.parquet"
      }
    },
    "dataSchema": {
      "dataSource": "example",
      "parser": {
        "type": "parquet-avro",
        "parseSpec": {
          "format": "avro",
          "flattenSpec": {
            "useFieldDiscovery": true,
            "fields": [
              {
                "type": "path",
                "name": "nestedDim",
                "expr": "$.nestedData.dim1"
              },
              {
                "type": "path",
                "name": "listDimFirstItem",
                "expr": "$.listDim[1]"
              }
            ]
          },
          "timestampSpec": {
            "column": "timestamp",
            "format": "auto"
          },
          "dimensionsSpec": {
            "dimensions": [],
            "dimensionExclusions": [],
            "spatialDimensions": []
          }
        }
      }
    }
  }
}

----------------------------------------

TITLE: Configuring Period Broadcast Rule in Apache Druid
DESCRIPTION: JSON configuration for a period broadcast rule that specifies how segments of different data sources should be co-located for a rolling time period.

LANGUAGE: json
CODE:
{
  "type" : "broadcastByPeriod",
  "colocatedDataSources" : [ "target_source1", "target_source2" ],
  "period" : "P1M",
  "includeFuture" : true
}

----------------------------------------

TITLE: Applying Logical AND Filter in Apache Druid groupBy Query (JSON)
DESCRIPTION: This snippet demonstrates how to use a logical AND filter in the having clause of a groupBy query. It combines multiple conditions using the 'and' type.

LANGUAGE: JSON
CODE:
{
    "queryType": "groupBy",
    "dataSource": "sample_datasource",
    ...
    "having": 
        {
            "type": "and",
            "havingSpecs": [        
                {
                    "type": "greaterThan",
                    "aggregation": "<aggregate_metric>",
                    "value": <numeric_value>
                },
                {
                    "type": "lessThan",
                    "aggregation": "<aggregate_metric>",
                    "value": <numeric_value>
                }
            ]
        }
}

----------------------------------------

TITLE: Configuring TimeMin Aggregator
DESCRIPTION: JSON configuration for timeMin aggregator at ingestion time, specifying output name and field to analyze.

LANGUAGE: json
CODE:
{
    "type": "timeMin",
    "name": "tmin",
    "fieldName": "<field_name, typically column specified in timestamp spec>"
}

----------------------------------------

TITLE: Copyright and License Notice for Prism
DESCRIPTION: This snippet provides the copyright and license information for Prism, a lightweight syntax highlighting library, released under the MIT license.

LANGUAGE: JavaScript
CODE:
/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

----------------------------------------

TITLE: Defining JSON Ingestion Task for Roll-up in Apache Druid
DESCRIPTION: This JSON configuration defines an indexing task for Druid, specifying data schema, input source, and enabling roll-up. It sets dimensions, metrics, and granularity for the roll-up process.

LANGUAGE: json
CODE:
{
  "type" : "index",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "rollup-tutorial",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "dimensionsSpec" : {
            "dimensions" : [
              "srcIP",
              "dstIP"
            ]
          },
          "timestampSpec": {
            "column": "timestamp",
            "format": "iso"
          }
        }
      },
      "metricsSpec" : [
        { "type" : "count", "name" : "count" },
        { "type" : "longSum", "name" : "packets", "fieldName" : "packets" },
        { "type" : "longSum", "name" : "bytes", "fieldName" : "bytes" }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "week",
        "queryGranularity" : "minute",
        "intervals" : ["2018-01-01/2018-01-03"],
        "rollup" : true
      }
    },
    "ioConfig" : {
      "type" : "index",
      "firehose" : {
        "type" : "local",
        "baseDir" : "quickstart/tutorial",
        "filter" : "rollup-data.json"
      },
      "appendToExisting" : false
    },
    "tuningConfig" : {
      "type" : "index",
      "maxRowsPerSegment" : 5000000,
      "maxRowsInMemory" : 25000,
      "forceExtendableShardSpecs" : true
    }
  }
}

----------------------------------------

TITLE: Copyright and License Notice for React
DESCRIPTION: This snippet provides the copyright and license information for the core React library, which is released under the MIT license.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Configuring HyperUnique Aggregator in Druid
DESCRIPTION: JSON configuration for the hyperUnique aggregator that computes estimated cardinality using pre-aggregated HyperLogLog values.

LANGUAGE: json
CODE:
{ 
  "type" : "hyperUnique",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "isInputHyperUnique" : false,
  "round" : false
}

----------------------------------------

TITLE: Configuring Event Count Metrics in Druid
DESCRIPTION: Configuration example for counting ingested events using count aggregator in metricsSpec.

LANGUAGE: json
CODE:
"metricsSpec" : [
      {
        "type" : "count",
        "name" : "count"
      }
]

LANGUAGE: json
CODE:
"aggregations": [
    { "type": "longSum", "name": "numIngestedEvents", "fieldName": "count" }
]

----------------------------------------

TITLE: JSON payload for disabling segments by segment IDs in Apache Druid
DESCRIPTION: This JSON payload specifies the segment IDs to be marked as 'unused' when sent to the Druid Coordinator API. It targets segments for hours 13 and 14.

LANGUAGE: json
CODE:
{
  "segmentIds":
  [
    "deletion-tutorial_2015-09-12T13:00:00.000Z_2015-09-12T14:00:00.000Z_2019-05-01T17:38:46.961Z",
    "deletion-tutorial_2015-09-12T14:00:00.000Z_2015-09-12T15:00:00.000Z_2019-05-01T17:38:46.961Z"
  ]
}

----------------------------------------

TITLE: Starting Kafka Broker
DESCRIPTION: Command to start a Kafka broker using the default server properties.

LANGUAGE: bash
CODE:
./bin/kafka-server-start.sh config/server.properties

----------------------------------------

TITLE: Configuring Send-All Ambari Metrics Converter in JSON
DESCRIPTION: JSON configuration for the 'all' event converter type, which sends all Druid service metrics events to Ambari Metrics. It specifies the namespace prefix and application name.

LANGUAGE: json
CODE:
{
"druid.emitter.ambari-metrics.eventConverter":{"type":"all", "namespacePrefix": "druid.test", "appName":"druid"}
}

----------------------------------------

TITLE: Defining Compaction Task in Apache Druid
DESCRIPTION: This JSON configuration defines a compaction task that will compact segments while maintaining the original granularity.

LANGUAGE: json
CODE:
{
  "type": "compact",
  "dataSource": "compaction-tutorial",
  "interval": "2015-09-12/2015-09-13",
  "tuningConfig" : {
    "type" : "index",
    "maxRowsPerSegment" : 5000000,
    "maxRowsInMemory" : 25000
  }
}

----------------------------------------

TITLE: Example InfluxDB Line Protocol Data
DESCRIPTION: A sample line of data in the InfluxDB Line Protocol format, demonstrating the structure including measurement, tags, fields, and timestamp.

LANGUAGE: plaintext
CODE:
cpu,application=dbhost=prdb123,region=us-east-1 usage_idle=99.24,usage_user=0.55 1520722030000000000

----------------------------------------

TITLE: Querying Grouped Data in Druid
DESCRIPTION: This SQL query performs a GroupBy operation on the 'updates-tutorial' datasource to show how rows are grouped at query time.

LANGUAGE: sql
CODE:
select __time, animal, SUM("count"), SUM("number") from "updates-tutorial" group by __time, animal;

----------------------------------------

TITLE: Configuring ORC Parser without Auto Discovery
DESCRIPTION: This example illustrates how to configure the ORC parser without auto discovery. It includes a flatten specification with field discovery disabled and a dimensions specification.

LANGUAGE: JSON
CODE:
{
  "type": "index_hadoop",
  "spec": {
    "ioConfig": {
      "type": "hadoop",
      "inputSpec": {
        "type": "static",
        "inputFormat": "org.apache.orc.mapreduce.OrcInputFormat",
        "paths": "path/to/file.orc"
      },
      ...
    },
    "dataSchema": {
      "dataSource": "example",
      "parser": {
        "type": "orc",
        "parseSpec": {
          "format": "orc",
          "flattenSpec": {
            "useFieldDiscovery": false,
            "fields": [
              {
                "type": "path",
                "name": "nestedDim",
                "expr": "$.nestedData.dim1"
              },
              {
                "type": "path",
                "name": "listDimFirstItem",
                "expr": "$.listDim[1]"
              }
            ]
          },
          "timestampSpec": {
            "column": "timestamp",
            "format": "millis"
          },
          "dimensionsSpec": {
            "dimensions": [
              "dim1",
              "dim3",
              "nestedDim",
              "listDimFirstItem"
            ],
            "dimensionExclusions": [],
            "spatialDimensions": []
          }
        }
      },
      ...
    },
    "tuningConfig": <hadoop-tuning-config>
    }
  }
}

----------------------------------------

TITLE: Middle Manager API Endpoints for Rolling Updates
DESCRIPTION: HTTP endpoints used to manage Middle Manager state during rolling updates. Includes endpoints for enabling/disabling workers and checking task status.

LANGUAGE: http
CODE:
POST <MiddleManager_IP:PORT>/druid/worker/v1/disable\nGET <MiddleManager_IP:PORT>/druid/worker/v1/enabled\nGET <MiddleManager_IP:PORT>/druid/worker/v1/tasks\nPOST <MiddleManager_IP:PORT>/druid/worker/v1/enable

----------------------------------------

TITLE: React Use-Sync-External-Store Shim License
DESCRIPTION: Declares the MIT license for React's use-sync-external-store-shim production build.

LANGUAGE: JavaScript
CODE:
/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Listing Segments in Druid Deep Storage
DESCRIPTION: Shows how to list the contents of the deep storage directory for the 'deletion-tutorial' datasource.

LANGUAGE: bash
CODE:
ls -l1 var/druid/segments/deletion-tutorial/

----------------------------------------

TITLE: Setting Merge Buffer Ratio in YAML
DESCRIPTION: Configures the number of merge buffers to be 1/4 of the number of processing threads. This setting affects the cluster's ability to handle concurrent GroupBy V2 queries.

LANGUAGE: yaml
CODE:
druid.processing.numMergeBuffers: ${druid.processing.numThreads/4}

----------------------------------------

TITLE: Configuring Druid Extension for DataSketches Tuple Sketch
DESCRIPTION: This snippet shows how to include the DataSketches extension in the Druid config file.

LANGUAGE: json
CODE:
"druid.extensions.loadList=[\"druid-datasketches\"]"

----------------------------------------

TITLE: Tranquility Server Response Format
DESCRIPTION: Example JSON response from Tranquility Server showing the number of events received and sent to Druid.

LANGUAGE: json
CODE:
{"result":{"received":39244,"sent":39244}}

LANGUAGE: json
CODE:
{"result":{"received":39244,"sent":0}}

----------------------------------------

TITLE: GroupBy Query with Filtered DimensionSpec
DESCRIPTION: Advanced GroupBy query example using a filtered dimensionSpec to filter both the rows and the exploded values, providing more precise control over the results.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "test",
  "intervals": [
    "1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"
  ],
  "filter": {
    "type": "selector",
    "dimension": "tags",
    "value": "t3"
  },
  "granularity": {
    "type": "all"
  },
  "dimensions": [
    {
      "type": "listFiltered",
      "delegate": {
        "type": "default",
        "dimension": "tags",
        "outputName": "tags"
      },
      "values": ["t3"]
    }
  ],
  "aggregations": [
    {
      "type": "count",
      "name": "count"
    }
  ]
}

----------------------------------------

TITLE: JSON Specification for Bloom Filter in Druid Queries
DESCRIPTION: This JSON snippet shows the structure for specifying a bloom filter in Druid queries, including the required fields and their descriptions.

LANGUAGE: json
CODE:
{
  "type" : "bloom",
  "dimension" : <dimension_name>,
  "bloomKFilter" : <serialized_bytes_for_BloomKFilter>,
  "extractionFn" : <extraction_fn>
}

----------------------------------------

TITLE: Compacted List Format Result Example in Druid
DESCRIPTION: Example of scan query results in compactedList format, demonstrating a more space-efficient representation of the same data.

LANGUAGE: json
CODE:
[{
    "segmentId" : "wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9",
    "columns" : [
      "timestamp", "robot", "namespace", "anonymous", "unpatrolled", "page", "language", "newpage", "user", "count", "added", "delta", "variation", "deleted"
    ],
    "events" : [
     ["2013-01-01T00:00:00.000Z", "1", "article", "0", "0", "11._korpus_(NOVJ)", "sl", "0", "EmausBot", 1.0, 39.0, 39.0, 39.0, 0.0]
    ]
} ]

----------------------------------------

TITLE: Quantile Post-Aggregator Configuration
DESCRIPTION: Post-aggregator configuration for retrieving specific quantile values from a DoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "quantilesDoublesSketchToQuantile",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch>,
  "fraction" : <fractional position from 0 to 1 inclusive>
}

----------------------------------------

TITLE: Column Mean Values Post-Aggregator
DESCRIPTION: Post-aggregator configuration for calculating mean values for each column in an ArrayOfDoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToMeans",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch>
}

----------------------------------------

TITLE: Implementing Interval Filter in Druid
DESCRIPTION: Example of an interval filter for filtering on time ranges. This is particularly useful for the __time column and uses ISO 8601 interval strings.

LANGUAGE: JSON
CODE:
{
    "type" : "interval",
    "dimension" : "__time",
    "intervals" : [
      "2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z",
      "2014-11-15T00:00:00.000Z/2014-11-16T00:00:00.000Z"
    ]
}

----------------------------------------

TITLE: Configuring Period Load Rule in Apache Druid
DESCRIPTION: This JSON snippet defines a Period Load Rule, which specifies how many replicas of a segment should exist in different server tiers for a specific time period.

LANGUAGE: json
CODE:
{
  "type" : "loadByPeriod",
  "period" : "P1M",
  "includeFuture" : true,
  "tieredReplicants": {
      "hot": 1,
      "_default_tier" : 1
  }
}

----------------------------------------

TITLE: Defining an Expression Virtual Column in Druid
DESCRIPTION: This snippet shows the syntax for defining an expression virtual column in Druid. It includes the required properties such as type, name, expression, and optional outputType.

LANGUAGE: json
CODE:
{
  "type": "expression",
  "name": <name of the virtual column>,
  "expression": <row expression>,
  "outputType": <output value type of expression>
}

----------------------------------------

TITLE: DOMPurify License Declaration
DESCRIPTION: License declaration for DOMPurify library version 2.4.3, released under Apache 2.0 and Mozilla Public License 2.0.

LANGUAGE: javascript
CODE:
/*! @license DOMPurify 2.4.3 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/2.4.3/LICENSE */

----------------------------------------

TITLE: Tranquility Server Response Format
DESCRIPTION: Example JSON response from Tranquility Server showing the number of events received and sent to Druid.

LANGUAGE: json
CODE:
{"result":{"received":39244,"sent":39244}}

LANGUAGE: json
CODE:
{"result":{"received":39244,"sent":0}}

----------------------------------------

TITLE: GET /druid/v2/datasources Example
DESCRIPTION: HTTP GET endpoint for retrieving queryable datasources from Druid Broker node.

LANGUAGE: HTTP
CODE:
GET /druid/v2/datasources

----------------------------------------

TITLE: Configuring Anonymous Authenticator with Basic Security
DESCRIPTION: Configuration example demonstrating how to set up Anonymous Authenticator alongside basic authentication, including identity and authorizer settings.

LANGUAGE: json
CODE:
druid.auth.authenticatorChain=["basic", "anonymous"]

druid.auth.authenticator.anonymous.type=anonymous
druid.auth.authenticator.anonymous.identity=defaultUser
druid.auth.authenticator.anonymous.authorizerName=myBasicAuthorizer

----------------------------------------

TITLE: Configuring Linear ShardSpec in JSON for Apache Druid
DESCRIPTION: Example JSON configuration for a linear shardSpec in Apache Druid. This configuration sets up a linear sharding strategy with partition number 0.

LANGUAGE: json
CODE:
"shardSpec": {
    "type": "linear",
    "partitionNum": 0
}

----------------------------------------

TITLE: Basic Druid Build Command
DESCRIPTION: Basic Maven command to build Druid from source, which runs static analysis, unit tests, compiles classes, and packages projects into JARs.

LANGUAGE: bash
CODE:
mvn clean install

----------------------------------------

TITLE: JSON Specification for Bloom Filter in Druid Queries
DESCRIPTION: This JSON structure defines how to use a Bloom Filter in Druid queries, specifying the dimension to filter, the serialized Bloom Filter, and an optional extraction function.

LANGUAGE: json
CODE:
{
  "type" : "bloom",
  "dimension" : <dimension_name>,
  "bloomKFilter" : <serialized_bytes_for_BloomKFilter>,
  "extractionFn" : <extraction_fn>
}

----------------------------------------

TITLE: JSON Specification for Bloom Filter in Druid Queries
DESCRIPTION: This JSON structure defines how to use a Bloom Filter in Druid queries, specifying the dimension to filter, the serialized Bloom Filter, and an optional extraction function.

LANGUAGE: json
CODE:
{
  "type" : "bloom",
  "dimension" : <dimension_name>,
  "bloomKFilter" : <serialized_bytes_for_BloomKFilter>,
  "extractionFn" : <extraction_fn>
}

----------------------------------------

TITLE: Retrieving Worker Status JSON Response
DESCRIPTION: Example JSON response showing the enabled status of a MiddleManager worker

LANGUAGE: json
CODE:
{"localhost:8091":true}

----------------------------------------

TITLE: Copyright Notice for React DOM
DESCRIPTION: Copyright notice for the React DOM production build (v17.0.2), released under the MIT license by Facebook, Inc. and its affiliates.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Combining and Overwriting Data
DESCRIPTION: Command to submit a task that combines existing data with new data and overwrites the original dataset.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/updates-append-index.json --url http://localhost:8081

----------------------------------------

TITLE: Estimating Distinct Keys with Error Bounds using ArrayOfDoublesSketch
DESCRIPTION: This JSON configuration defines a post-aggregator that estimates the number of distinct keys with error bounds from an ArrayOfDoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToEstimateAndBounds",
  "name": <output name>,
  "field"  : <post aggregator that refers to an  ArrayOfDoublesSketch (fieldAccess or another post aggregator)>,
  "numStdDevs", <number from 1 to 3>
}

----------------------------------------

TITLE: Querying with Duration Granularity in Apache Druid
DESCRIPTION: Example of a groupBy query in Apache Druid using duration granularity of 24 hours. This query aggregates data by language and count for each 24-hour period.

LANGUAGE: json
CODE:
{
   "queryType":"groupBy",
   "dataSource":"my_dataSource",
   "granularity":{"type": "duration", "duration": "86400000"},
   "dimensions":[
      "language"
   ],
   "aggregations":[
      {
         "type":"count",
         "name":"count"
      }
   ],
   "intervals":[
      "2000-01-01T00:00Z/3000-01-01T00:00Z"
   ]
}

----------------------------------------

TITLE: Configuring Interval Drop Rule in Apache Druid
DESCRIPTION: This JSON snippet defines an Interval Drop Rule, which specifies that segments within a specific time interval should be dropped from the cluster.

LANGUAGE: json
CODE:
{
  "type" : "dropByInterval",
  "interval" : "2012-01-01/2013-01-01"
}

----------------------------------------

TITLE: Retrieving Active Lookups JSON
DESCRIPTION: Example JSON response when querying the internal API of a Druid node for its active lookups.

LANGUAGE: json
CODE:
{
  "site_id_customer2": {
    "version": "v1",
    "lookupExtractorFactory": {
      "type": "map",
      "map": {
        "AHF77": "Home"
      }
    }
  }
}

----------------------------------------

TITLE: React-DOM License
DESCRIPTION: Declares the MIT license for React-DOM production build, version 17.0.2.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Loading DataSketches Extension Configuration
DESCRIPTION: Configuration snippet showing how to include the DataSketches extension in Druid's config file.

LANGUAGE: json
CODE:
"druid.extensions.loadList=[\"druid-datasketches\"]"

----------------------------------------

TITLE: Middle Manager API Endpoints for Rolling Updates
DESCRIPTION: HTTP API endpoints used for gracefully managing Middle Manager nodes during updates. Includes endpoints for enabling/disabling workers and checking task status.

LANGUAGE: http
CODE:
POST <MiddleManager_IP:PORT>/druid/worker/v1/disable\nGET <MiddleManager_IP:PORT>/druid/worker/v1/enabled\nGET <MiddleManager_IP:PORT>/druid/worker/v1/tasks\nPOST <MiddleManager_IP:PORT>/druid/worker/v1/enable

----------------------------------------

TITLE: Filtering for Null Values in Multi-value Dimensions in Apache Druid
DESCRIPTION: Example of a 'selector' filter that matches rows where the 'tags' dimension is null or empty. This filter would match row4 in the sample dataset.

LANGUAGE: json
CODE:
{
  "type": "selector",
  "dimension": "tags",
  "value": null
}

----------------------------------------

TITLE: Running the Druid Coordinator Process
DESCRIPTION: Command to start the Druid Coordinator process using the Main class.

LANGUAGE: java
CODE:
org.apache.druid.cli.Main server coordinator

----------------------------------------

TITLE: Configuring HyperUnique Aggregator in Druid
DESCRIPTION: JSON configuration for the hyperUnique aggregator that computes estimated cardinality using HyperLogLog algorithm on pre-aggregated metrics.

LANGUAGE: json
CODE:
{ 
  "type" : "hyperUnique",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "isInputHyperUnique" : false,
  "round" : false
}

----------------------------------------

TITLE: Configuring Kafka Lookup Extractor Factory in Apache Druid
DESCRIPTION: This JSON configuration sets up a Kafka lookup extractor factory. It specifies the Kafka topic to read from and the Zookeeper connection properties. This setup allows Druid to use Kafka as a source for dimension value renaming.

LANGUAGE: json
CODE:
{
  "type":"kafka",
  "kafkaTopic":"testTopic",
  "kafkaProperties":{"zookeeper.connect":"somehost:2181/kafka"}
}

----------------------------------------

TITLE: Displaying ResetCluster Tool Usage Documentation
DESCRIPTION: Command to display the usage documentation for the ResetCluster tool.

LANGUAGE: bash
CODE:
java org.apache.druid.cli.Main help tools reset-cluster

----------------------------------------

TITLE: GroupBy Query on Multi-value Dimensions in Apache Druid
DESCRIPTION: Example of a groupBy query that groups results by the 'tags' dimension without any filtering. This query demonstrates how multi-value dimensions are exploded into multiple rows.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "test",
  "intervals": [
    "1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"
  ],
  "granularity": {
    "type": "all"
  },
  "dimensions": [
    {
      "type": "default",
      "dimension": "tags",
      "outputName": "tags"
    }
  ],
  "aggregations": [
    {
      "type": "count",
      "name": "count"
    }
  ]
}

----------------------------------------

TITLE: React Core License
DESCRIPTION: Provides license information for the core React production module, which is released under the MIT license.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Applying Numeric Filter in groupBy Query (JSON)
DESCRIPTION: Illustrates how to use a numeric filter (greaterThan) in the having clause of a groupBy query in Druid.

LANGUAGE: json
CODE:
{
    "queryType": "groupBy",
    "dataSource": "sample_datasource",
    ...
    "having": 
        {
            "type": "greaterThan",
            "aggregation": "<aggregate_metric>",
            "value": <numeric_value>
        }
}

----------------------------------------

TITLE: JavaScript Router Strategy Configuration
DESCRIPTION: Example of a JavaScript-based routing strategy that routes queries with 3+ aggregators to the lowest priority broker.

LANGUAGE: json
CODE:
{
  "type" : "javascript",
  "function" : "function (config, query) { if (query.getAggregatorSpecs && query.getAggregatorSpecs().size() >= 3) { var size = config.getTierToBrokerMap().values().size(); if (size > 0) { return config.getTierToBrokerMap().values().toArray()[size-1] } else { return config.getDefaultBrokerServiceName() } } else { return null } }"
}

----------------------------------------

TITLE: Copyright Notice for NProgress
DESCRIPTION: Copyright notice for the NProgress library by Rico Sta. Cruz, released under the MIT license.

LANGUAGE: JavaScript
CODE:
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

----------------------------------------

TITLE: Appending Data in Druid
DESCRIPTION: This command submits a task to append new data to the existing data in the 'updates-tutorial' datasource.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/updates-append-index2.json

----------------------------------------

TITLE: JavaScript Router Strategy Configuration
DESCRIPTION: Example of a JavaScript-based routing strategy that routes queries with 3+ aggregators to the lowest priority broker.

LANGUAGE: json
CODE:
{
  "type" : "javascript",
  "function" : "function (config, query) { if (query.getAggregatorSpecs && query.getAggregatorSpecs().size() >= 3) { var size = config.getTierToBrokerMap().values().size(); if (size > 0) { return config.getTierToBrokerMap().values().toArray()[size-1] } else { return config.getDefaultBrokerServiceName() } } else { return null } }"
}

----------------------------------------

TITLE: Importing Metadata into Derby Database
DESCRIPTION: SQL commands to import the exported CSV files into Derby database tables. This includes importing data for segments, rules, config, datasource, and supervisors tables.

LANGUAGE: sql
CODE:
CALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_SEGMENTS','/tmp/csv/druid_segments.csv',',','"',null,0);

CALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_RULES','/tmp/csv/druid_rules.csv',',','"',null,0);

CALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_CONFIG','/tmp/csv/druid_config.csv',',','"',null,0);

CALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_DATASOURCE','/tmp/csv/druid_dataSource.csv',',','"',null,0);

CALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_SUPERVISORS','/tmp/csv/druid_supervisors.csv',',','"',null,0);

----------------------------------------

TITLE: Segment Metadata Query Response Format
DESCRIPTION: Example response format showing various metadata properties including segment ID, intervals, column information, aggregators, query granularity, and size statistics.

LANGUAGE: json
CODE:
[ {
  "id" : "some_id",
  "intervals" : [ "2013-05-13T00:00:00.000Z/2013-05-14T00:00:00.000Z" ],
  "columns" : {
    "__time" : { "type" : "LONG", "hasMultipleValues" : false, "size" : 407240380, "cardinality" : null, "errorMessage" : null },
    "dim1" : { "type" : "STRING", "hasMultipleValues" : false, "size" : 100000, "cardinality" : 1944, "errorMessage" : null },
    "dim2" : { "type" : "STRING", "hasMultipleValues" : true, "size" : 100000, "cardinality" : 1504, "errorMessage" : null },
    "metric1" : { "type" : "FLOAT", "hasMultipleValues" : false, "size" : 100000, "cardinality" : null, "errorMessage" : null }
  },
  "aggregators" : {
    "metric1" : { "type" : "longSum", "name" : "metric1", "fieldName" : "metric1" }
  },
  "queryGranularity" : {
    "type": "none"
  },
  "size" : 300000,
  "numRows" : 5000000
} ]

----------------------------------------

TITLE: Defining Expression Virtual Column
DESCRIPTION: JSON schema for defining an expression-based virtual column in Druid. Specifies required and optional properties including name, expression, and output type.

LANGUAGE: json
CODE:
{
  "type": "expression",
  "name": <name of the virtual column>,
  "expression": <row expression>,
  "outputType": <output value type of expression>
}

----------------------------------------

TITLE: Configuring Interval Drop Rule in Apache Druid
DESCRIPTION: This JSON snippet defines an Interval Drop Rule, which specifies that segments within a specific time interval should be dropped from the cluster.

LANGUAGE: json
CODE:
{
  "type" : "dropByInterval",
  "interval" : "2012-01-01/2013-01-01"
}

----------------------------------------

TITLE: Copyright and License Notice for mark.js
DESCRIPTION: This snippet provides the copyright and license information for mark.js, a JavaScript keyword highlighting library, released under the MIT license.

LANGUAGE: JavaScript
CODE:
/*!***************************************************
* mark.js v8.11.1
* https://markjs.io/
* Copyright (c) 20142018, Julian Khnel
* Released under the MIT license https://git.io/vwTVl
****************************************************/

----------------------------------------

TITLE: Defining ZooKeeper Path for Segment Load/Drop Instructions in Druid
DESCRIPTION: Specifies the ZooKeeper path where the Coordinator writes instructions for Historical processes to load or drop segments.

LANGUAGE: plaintext
CODE:
${druid.zk.paths.loadQueuePath}/_host_of_historical_process/_segment_identifier

----------------------------------------

TITLE: Constructing a groupBy Query in Apache Druid
DESCRIPTION: Example of a groupBy query structure in Apache Druid, including dimensions, filters, aggregations, and post-aggregations. This query groups data by country and device, applies filters, and calculates total usage and data transfer metrics.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "sample_datasource",
  "granularity": "day",
  "dimensions": ["country", "device"],
  "limitSpec": { "type": "default", "limit": 5000, "columns": ["country", "data_transfer"] },
  "filter": {
    "type": "and",
    "fields": [
      { "type": "selector", "dimension": "carrier", "value": "AT&T" },
      { "type": "or", 
        "fields": [
          { "type": "selector", "dimension": "make", "value": "Apple" },
          { "type": "selector", "dimension": "make", "value": "Samsung" }
        ]
      }
    ]
  },
  "aggregations": [
    { "type": "longSum", "name": "total_usage", "fieldName": "user_count" },
    { "type": "doubleSum", "name": "data_transfer", "fieldName": "data_transfer" }
  ],
  "postAggregations": [
    { "type": "arithmetic",
      "name": "avg_usage",
      "fn": "/",
      "fields": [
        { "type": "fieldAccess", "fieldName": "data_transfer" },
        { "type": "fieldAccess", "fieldName": "total_usage" }
      ]
    }
  ],
  "intervals": [ "2012-01-01T00:00:00.000/2012-01-03T00:00:00.000" ],
  "having": {
    "type": "greaterThan",
    "aggregation": "total_usage",
    "value": 100
  }
}

----------------------------------------

TITLE: Constructing a groupBy Query in Apache Druid
DESCRIPTION: Example of a groupBy query structure in Apache Druid, including dimensions, filters, aggregations, and post-aggregations. This query groups data by country and device, applies filters, and calculates total usage and data transfer metrics.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "sample_datasource",
  "granularity": "day",
  "dimensions": ["country", "device"],
  "limitSpec": { "type": "default", "limit": 5000, "columns": ["country", "data_transfer"] },
  "filter": {
    "type": "and",
    "fields": [
      { "type": "selector", "dimension": "carrier", "value": "AT&T" },
      { "type": "or", 
        "fields": [
          { "type": "selector", "dimension": "make", "value": "Apple" },
          { "type": "selector", "dimension": "make", "value": "Samsung" }
        ]
      }
    ]
  },
  "aggregations": [
    { "type": "longSum", "name": "total_usage", "fieldName": "user_count" },
    { "type": "doubleSum", "name": "data_transfer", "fieldName": "data_transfer" }
  ],
  "postAggregations": [
    { "type": "arithmetic",
      "name": "avg_usage",
      "fn": "/",
      "fields": [
        { "type": "fieldAccess", "fieldName": "data_transfer" },
        { "type": "fieldAccess", "fieldName": "total_usage" }
      ]
    }
  ],
  "intervals": [ "2012-01-01T00:00:00.000/2012-01-03T00:00:00.000" ],
  "having": {
    "type": "greaterThan",
    "aggregation": "total_usage",
    "value": 100
  }
}

----------------------------------------

TITLE: Prism Syntax Highlighter License
DESCRIPTION: MIT license header for Prism syntax highlighting library by Lea Verou

LANGUAGE: javascript
CODE:
/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

----------------------------------------

TITLE: Configuring Standard Deviation Post-Aggregator in Apache Druid
DESCRIPTION: JSON configuration for the standard deviation post-aggregator. It specifies the output name, aggregator name to use as input, and estimator type.

LANGUAGE: json
CODE:
{
  "type": "stddev",
  "name": "<output_name>",
  "fieldName": "<aggregator_name>",
  "estimator": <string>
}

----------------------------------------

TITLE: Updating a Single Lookup in Druid JSON
DESCRIPTION: Example JSON payload for updating a specific lookup via POST request to the Druid coordinator API.

LANGUAGE: json
CODE:
{
  "version": "v1",
  "lookupExtractorFactory": {
    "type": "map",
    "map": {
      "847632": "Internal Use Only"
    }
  }
}

----------------------------------------

TITLE: Configuring DataSketches HLL Sketch Extension in Druid
DESCRIPTION: This snippet shows how to include the DataSketches HLL Sketch extension in the Druid configuration file.

LANGUAGE: json
CODE:
"druid.extensions.loadList":["druid-datasketches"]

----------------------------------------

TITLE: Declaring MIT License for object-assign
DESCRIPTION: This comment block declares the MIT license for the object-assign library by Sindre Sorhus.

LANGUAGE: JavaScript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

----------------------------------------

TITLE: Example Druid Query for Unique Users of Both Product A and B
DESCRIPTION: Complex Druid query example using Theta Sketch to count unique users who visited both product A and B. This showcases the use of filtered aggregators and set operations on Theta Sketches.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "test_datasource",
  "granularity": "ALL",
  "dimensions": [],
  "filter": {
    "type": "or",
    "fields": [
      {"type": "selector", "dimension": "product", "value": "A"},
      {"type": "selector", "dimension": "product", "value": "B"}
    ]
  },
  "aggregations": [
    {
      "type" : "filtered",
      "filter" : {
        "type" : "selector",
        "dimension" : "product",
        "value" : "A"
      },
      "aggregator" :     {
        "type": "thetaSketch", "name": "A_unique_users", "fieldName": "user_id_sketch"
      }
    },
    {
      "type" : "filtered",
      "filter" : {
        "type" : "selector",
        "dimension" : "product",
        "value" : "B"
      },
      "aggregator" :     {
        "type": "thetaSketch", "name": "B_unique_users", "fieldName": "user_id_sketch"
      }
    }
  ],
  "postAggregations": [
    {
      "type": "thetaSketchEstimate",
      "name": "final_unique_users",
      "field":
      {
        "type": "thetaSketchSetOp",
        "name": "final_unique_users_sketch",
        "func": "INTERSECT",
        "fields": [
          {
            "type": "fieldAccess",
            "fieldName": "A_unique_users"
          },
          {
            "type": "fieldAccess",
            "fieldName": "B_unique_users"
          }
        ]
      }
    }
  ],
  "intervals": [
    "2014-10-19T00:00:00.000Z/2014-10-22T00:00:00.000Z"
  ]
}

----------------------------------------

TITLE: Configuring TuningConfig in JSON for Hadoop Indexer
DESCRIPTION: JSON configuration for the tuningConfig section of the Hadoop Indexer spec file. It includes the workingPath field for specifying the directory for intermediate results.

LANGUAGE: json
CODE:
"tuningConfig" : {
 ...
  "workingPath": "/tmp",
  ...
}

----------------------------------------

TITLE: Installing and Starting Tranquility
DESCRIPTION: Commands to set up Tranquility for stream ingestion

LANGUAGE: bash
CODE:
curl -O http://static.druid.io/tranquility/releases/tranquility-distribution-0.8.0.tgz
tar -xzf tranquility-distribution-0.8.0.tgz
cd tranquility-distribution-0.8.0
bin/tranquility <server or kafka> -configFile <path_to_druid_distro>/conf/tranquility/<server or kafka>.json

----------------------------------------

TITLE: Configuring JavaScript Aggregator in Druid
DESCRIPTION: JSON configuration for the JavaScript aggregator in Druid, which computes an arbitrary JavaScript function over a set of columns.

LANGUAGE: json
CODE:
{ "type": "javascript",
  "name": "<output_name>",
  "fieldNames"  : [ <column1>, <column2>, ... ],
  "fnAggregate" : "function(current, column1, column2, ...) {
                     <updates partial aggregate (current) based on the current row values>
                     return <updated partial aggregate>
                   }",
  "fnCombine"   : "function(partialA, partialB) { return <combined partial results>; }",
  "fnReset"     : "function()                   { return <initial value>; }"
}

----------------------------------------

TITLE: JVM Configuration Settings for Router Node
DESCRIPTION: JVM settings for optimal Router node performance in a production environment, including memory allocation and GC configuration

LANGUAGE: properties
CODE:
-server
-Xmx13g
-Xms13g
-XX:NewSize=256m
-XX:MaxNewSize=256m
-XX:+UseConcMarkSweepGC
-XX:+PrintGCDetails
-XX:+PrintGCTimeStamps
-XX:+UseLargePages
-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/mnt/galaxy/deploy/current/
-Duser.timezone=UTC
-Dfile.encoding=UTF-8
-Djava.io.tmpdir=/mnt/tmp

-Dcom.sun.management.jmxremote.port=17071
-Dcom.sun.management.jmxremote.authenticate=false
-Dcom.sun.management.jmxremote.ssl=false

----------------------------------------

TITLE: Declaring MIT License for React
DESCRIPTION: This comment block declares the MIT license for React's production build.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Configuring White-List Based Graphite Event Converter
DESCRIPTION: JSON configuration for the 'whiteList' event converter that selectively sends metrics based on a whitelist map. Includes options for namespace prefix, hostname handling, and custom whitelist map file path.

LANGUAGE: json
CODE:
{"type":"whiteList", "namespacePrefix": "druid.test", "ignoreHostname":true, "ignoreServiceName":true, "mapPath":"/pathPrefix/fileName.json"}

----------------------------------------

TITLE: Declaring MIT License for mark.js
DESCRIPTION: This comment block declares the MIT license for the mark.js library by Julian Khnel.

LANGUAGE: JavaScript
CODE:
/*!***************************************************
* mark.js v8.11.1
* https://markjs.io/
* Copyright (c) 20142018, Julian Khnel
* Released under the MIT license https://git.io/vwTVl
****************************************************/

----------------------------------------

TITLE: Accessing Druid Console URL Format
DESCRIPTION: URL pattern for accessing the main Druid Console through the Router process.

LANGUAGE: plain
CODE:
http://<ROUTER_IP>:<ROUTER_PORT>

----------------------------------------

TITLE: License Information for object-assign
DESCRIPTION: Copyright notice and MIT license information for the object-assign library by Sindre Sorhus.

LANGUAGE: JavaScript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

----------------------------------------

TITLE: Implementing Runge-Kutta Spring Physics in JavaScript
DESCRIPTION: A Runge-Kutta spring physics function generator adapted from Framer.js. This utility can be used for creating realistic physics-based animations or simulations in the Druid project.

LANGUAGE: JavaScript
CODE:
/*! Runge-Kutta spring physics function generator. Adapted from Framer.js, copyright Koen Bok. MIT License: http://en.wikipedia.org/wiki/MIT_License */

----------------------------------------

TITLE: Constructing ZooKeeper Path for Druid Process Announcements
DESCRIPTION: Defines the ZooKeeper path where Historical and Realtime processes in Druid announce their existence by creating an ephemeral znode.

LANGUAGE: plaintext
CODE:
${druid.zk.paths.announcementsPath}/${druid.host}

----------------------------------------

TITLE: Querying with Hour Granularity in Apache Druid
DESCRIPTION: Example of a groupBy query in Apache Druid using 'hour' granularity. This query aggregates data by language and count for each hour.

LANGUAGE: json
CODE:
{
   "queryType":"groupBy",
   "dataSource":"my_dataSource",
   "granularity":"hour",
   "dimensions":[
      "language"
   ],
   "aggregations":[
      {
         "type":"count",
         "name":"count"
      }
   ],
   "intervals":[
      "2000-01-01T00:00Z/3000-01-01T00:00Z"
   ]
}

----------------------------------------

TITLE: Querying with Hour Granularity in Apache Druid
DESCRIPTION: Example of a groupBy query in Apache Druid using 'hour' granularity. This query aggregates data by language and count for each hour.

LANGUAGE: json
CODE:
{
   "queryType":"groupBy",
   "dataSource":"my_dataSource",
   "granularity":"hour",
   "dimensions":[
      "language"
   ],
   "aggregations":[
      {
         "type":"count",
         "name":"count"
      }
   ],
   "intervals":[
      "2000-01-01T00:00Z/3000-01-01T00:00Z"
   ]
}

----------------------------------------

TITLE: Configuring Period Broadcast Rule in Apache Druid
DESCRIPTION: This JSON snippet defines a period broadcast rule in Druid. It specifies how segments of different data sources should be co-located in historical nodes for a specific time period.

LANGUAGE: json
CODE:
{
  "type" : "broadcastByPeriod",
  "colocatedDataSources" : [ "target_source1", "target_source2" ],
  "period" : "P1M"
}

----------------------------------------

TITLE: Configuring Asynchronous Logging for Chatty Classes in Apache Druid
DESCRIPTION: This XML configuration sets up asynchronous logging for specific chatty classes in Apache Druid. It defines AsyncLoggers for various Druid components and sets their log levels individually, while maintaining a default info level for the root logger.

LANGUAGE: XML
CODE:
<?xml version="1.0" encoding="UTF-8" ?>
<Configuration status="WARN">
  <Appenders>
    <Console name="Console" target="SYSTEM_OUT">
      <PatternLayout pattern="%d{ISO8601} %p [%t] %c - %m%n"/>
    </Console>
  </Appenders>
  <Loggers>
    <AsyncLogger name="org.apache.druid.curator.inventory.CuratorInventoryManager" level="debug" additivity="false">
      <AppenderRef ref="Console"/>
    </AsyncLogger>
    <AsyncLogger name="org.apache.druid.client.BatchServerInventoryView" level="debug" additivity="false">
      <AppenderRef ref="Console"/>
    </AsyncLogger>
    <!-- Make extra sure nobody adds logs in a bad way that can hurt performance -->
    <AsyncLogger name="org.apache.druid.client.ServerInventoryView" level="debug" additivity="false">
      <AppenderRef ref="Console"/>
    </AsyncLogger>
    <AsyncLogger name ="org.apache.druid.java.util.http.client.pool.ChannelResourceFactory" level="info" additivity="false">
      <AppenderRef ref="Console"/>
    </AsyncLogger>
    <Root level="info">
      <AppenderRef ref="Console"/>
    </Root>
  </Loggers>
</Configuration>

----------------------------------------

TITLE: Declaring MIT License for classnames
DESCRIPTION: Copyright notice and MIT license declaration for the classnames library by Jed Watson.

LANGUAGE: JavaScript
CODE:
/*!
	Copyright (c) 2018 Jed Watson.
	Licensed under the MIT License (MIT), see
	http://jedwatson.github.io/classnames
*/

----------------------------------------

TITLE: Configuring Parquet Parser with Parquet ParseSpec
DESCRIPTION: Example configuration for ingesting Parquet files using the direct Parquet parser with a Parquet parseSpec. Includes flattening specification for nested data structures.

LANGUAGE: json
CODE:
{
  "type": "index_hadoop",
  "spec": {
    "ioConfig": {
      "type": "hadoop",
      "inputSpec": {
        "type": "static",
        "inputFormat": "org.apache.druid.data.input.parquet.DruidParquetInputFormat",
        "paths": "path/to/file.parquet"
      }
    },
    "dataSchema": {
      "dataSource": "example",
      "parser": {
        "type": "parquet",
        "parseSpec": {
          "format": "parquet",
          "flattenSpec": {
            "useFieldDiscovery": true,
            "fields": [
              {
                "type": "path",
                "name": "nestedDim",
                "expr": "$.nestedData.dim1"
              },
              {
                "type": "path",
                "name": "listDimFirstItem",
                "expr": "$.listDim[1]"
              }
            ]
          },
          "timestampSpec": {
            "column": "timestamp",
            "format": "auto"
          },
          "dimensionsSpec": {
            "dimensions": [],
            "dimensionExclusions": [],
            "spatialDimensions": []
          }
        }
      }
    }
  }
}

----------------------------------------

TITLE: Defining a Selector Filter in Druid
DESCRIPTION: This snippet demonstrates how to create a selector filter in Druid, which matches a specific dimension with a specific value. It's equivalent to a WHERE clause in SQL.

LANGUAGE: json
CODE:
"filter": { "type": "selector", "dimension": <dimension_string>, "value": <dimension_value_string> }

----------------------------------------

TITLE: Network Flow Data Sample - JSON
DESCRIPTION: Example network flow data containing IP traffic information like source/destination IPs, ports, protocols and metrics.

LANGUAGE: json
CODE:
{"ts":"2018-01-01T01:01:35Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2", "srcPort":2000, "dstPort":3000, "protocol": 6, "packets":10, "bytes":1000, "cost": 1.4}

----------------------------------------

TITLE: Implementing Minimal Promises/A+ in JavaScript
DESCRIPTION: An embeddable, strictly-compliant implementation of Promises/A+ 1.1.1 Thenable. This utility provides a minimal promises functionality adhering to the Promises/A+ specification.

LANGUAGE: JavaScript
CODE:
/*!\n  Embeddable Minimum Strictly-Compliant Promises/A+ 1.1.1 Thenable\n  Copyright (c) 2013-2014 Ralf S. Engelschall (http://engelschall.com)\n  Licensed under The MIT License (http://opensource.org/licenses/MIT)\n  */

----------------------------------------

TITLE: Configuring JVM Settings for Router Process
DESCRIPTION: Example JVM settings for running the Router process on a c3.2xlarge EC2 instance.

LANGUAGE: java
CODE:
-server
-Xmx13g
-Xms13g
-XX:NewSize=256m
-XX:MaxNewSize=256m
-XX:+UseConcMarkSweepGC
-XX:+PrintGCDetails
-XX:+PrintGCTimeStamps
-XX:+UseLargePages
-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/mnt/galaxy/deploy/current/
-Duser.timezone=UTC
-Dfile.encoding=UTF-8
-Djava.io.tmpdir=/mnt/tmp

-Dcom.sun.management.jmxremote.port=17071
-Dcom.sun.management.jmxremote.authenticate=false
-Dcom.sun.management.jmxremote.ssl=false

----------------------------------------

TITLE: Loading Data Using Druid Helper Script
DESCRIPTION: Command to load batch data using Druid's post-index-task helper script, which submits the ingestion task and monitors its progress.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/wikipedia-index.json

----------------------------------------

TITLE: Basic Timeseries Query Structure in Druid
DESCRIPTION: Example of a complete timeseries query showcasing filters, aggregations, and post-aggregations. This query filters data from a sample datasource, performs sum aggregations, and includes a division post-aggregation.

LANGUAGE: json
CODE:
{
  "queryType": "timeseries",
  "dataSource": "sample_datasource",
  "granularity": "day",
  "descending": "true",
  "filter": {
    "type": "and",
    "fields": [
      { "type": "selector", "dimension": "sample_dimension1", "value": "sample_value1" },
      { "type": "or",
        "fields": [
          { "type": "selector", "dimension": "sample_dimension2", "value": "sample_value2" },
          { "type": "selector", "dimension": "sample_dimension3", "value": "sample_value3" }
        ]
      }
    ]
  },
  "aggregations": [
    { "type": "longSum", "name": "sample_name1", "fieldName": "sample_fieldName1" },
    { "type": "doubleSum", "name": "sample_name2", "fieldName": "sample_fieldName2" }
  ],
  "postAggregations": [
    { "type": "arithmetic",
      "name": "sample_divide",
      "fn": "/",
      "fields": [
        { "type": "fieldAccess", "name": "postAgg__sample_name1", "fieldName": "sample_name1" },
        { "type": "fieldAccess", "name": "postAgg__sample_name2", "fieldName": "sample_name2" }
      ]
    }
  ],
  "intervals": [ "2012-01-01T00:00:00.000/2012-01-03T00:00:00.000" ]
}

----------------------------------------

TITLE: Building Hadoop Docker Image
DESCRIPTION: Commands to build a Docker image for Hadoop 2.8.3 cluster that will be used for batch indexing.

LANGUAGE: bash
CODE:
cd quickstart/tutorial/hadoop/docker
docker build -t druid-hadoop-demo:2.8.3 .

----------------------------------------

TITLE: Configuring Advanced TLS Settings in Apache Druid
DESCRIPTION: These properties provide advanced TLS configuration options for Apache Druid. They include settings for key manager algorithm, cipher suites, and protocol selection.

LANGUAGE: properties
CODE:
druid.server.https.keyManagerFactoryAlgorithm=javax.net.ssl.KeyManagerFactory.getDefaultAlgorithm()
druid.server.https.keyManagerPassword=none
druid.server.https.includeCipherSuites=Jetty's default include cipher list
druid.server.https.excludeCipherSuites=Jetty's default exclude cipher list
druid.server.https.includeProtocols=Jetty's default include protocol list
druid.server.https.excludeProtocols=Jetty's default exclude protocol list

----------------------------------------

TITLE: Configuring OrderByColumnSpec in Druid GroupBy Queries
DESCRIPTION: Specifies the ordering configuration for individual columns in groupBy query results. Supports different ordering types including lexicographic, alphanumeric, strlen, and numeric ordering.

LANGUAGE: json
CODE:
{
    "dimension" : "<Any dimension or metric name>",
    "direction" : <"ascending"|"descending">,
    "dimensionOrder" : <"lexicographic"(default)|"alphanumeric"|"strlen"|"numeric">
}

----------------------------------------

TITLE: Configuring Count Aggregator in Druid JSON
DESCRIPTION: The count aggregator computes the count of Druid rows that match the specified filters. It's important to note that this count may not reflect the number of raw events ingested due to possible data roll-up at ingestion time.

LANGUAGE: json
CODE:
{ "type" : "count", "name" : <output_name> }

----------------------------------------

TITLE: Starting Hadoop Container
DESCRIPTION: Docker command to start the Hadoop container with necessary port mappings and volume mounts.

LANGUAGE: bash
CODE:
docker run -it  -h druid-hadoop-demo --name druid-hadoop-demo -p 50010:50010 -p 50020:50020 -p 50075:50075 -p 50090:50090 -p 8020:8020 -p 10020:10020 -p 19888:19888 -p 8030:8030 -p 8031:8031 -p 8032:8032 -p 8033:8033 -p 8040:8040 -p 8042:8042 -p 8088:8088 -p 8443:8443 -p 2049:2049 -p 9000:9000 -p 49707:49707 -p 2122:2122  -p 34455:34455 -v /tmp/shared:/shared druid-hadoop-demo:2.8.3 /etc/bootstrap.sh -bash

----------------------------------------

TITLE: Declaring MIT License for React's use-sync-external-store-shim
DESCRIPTION: This comment block declares the MIT license for React's use-sync-external-store-shim production build.

LANGUAGE: JavaScript
CODE:
/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Configuring Anonymous Authenticator in Apache Druid
DESCRIPTION: This JSON snippet demonstrates how to enable the Anonymous Authenticator with the druid-basic-security extension, including setting the identity and authorizer name.

LANGUAGE: json
CODE:
"druid.auth.authenticatorChain":["basic", "anonymous"],

"druid.auth.authenticator.anonymous.type":"anonymous",
"druid.auth.authenticator.anonymous.identity":"defaultUser",
"druid.auth.authenticator.anonymous.authorizerName":"myBasicAuthorizer"

----------------------------------------

TITLE: Example of InfluxDB Line Protocol Data
DESCRIPTION: This snippet shows an example of a typical InfluxDB Line Protocol data point. It includes a measurement name, tags, field values, and a timestamp in nanoseconds since the Unix epoch.

LANGUAGE: text
CODE:
cpu,application=dbhost=prdb123,region=us-east-1 usage_idle=99.24,usage_user=0.55 1520722030000000000

----------------------------------------

TITLE: Basic GroupBy Query for Multi-value Dimensions
DESCRIPTION: Example of a GroupBy query that aggregates data across multi-value dimensions without filtering.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "test",
  "intervals": [
    "1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"
  ],
  "granularity": {
    "type": "all"
  },
  "dimensions": [
    {
      "type": "default",
      "dimension": "tags",
      "outputName": "tags"
    }
  ],
  "aggregations": [
    {
      "type": "count",
      "name": "count"
    }
  ]
}

----------------------------------------

TITLE: Declaring MIT License for NProgress
DESCRIPTION: This comment block declares the MIT license for the NProgress library by Rico Sta. Cruz.

LANGUAGE: JavaScript
CODE:
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

----------------------------------------

TITLE: React Scheduler License
DESCRIPTION: Declares the license for the React Scheduler production module, which is released under the MIT license.

LANGUAGE: JavaScript
CODE:
/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Implementing Promises/A+ 1.1.1 Thenable in JavaScript
DESCRIPTION: An embeddable, minimum strictly-compliant implementation of Promises/A+ 1.1.1 Thenable. This implementation provides a foundation for asynchronous programming in the Druid project.

LANGUAGE: JavaScript
CODE:
/*!\n  Embeddable Minimum Strictly-Compliant Promises/A+ 1.1.1 Thenable\n  Copyright (c) 2013-2014 Ralf S. Engelschall (http://engelschall.com)\n  Licensed under The MIT License (http://opensource.org/licenses/MIT)\n  */

----------------------------------------

TITLE: Network Flow Data Example
DESCRIPTION: Sample JSON network flow data containing IP traffic information including source/destination addresses, ports, protocol, and metrics.

LANGUAGE: json
CODE:
{"ts":"2018-01-01T01:01:35Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2", "srcPort":2000, "dstPort":3000, "protocol": 6, "packets":10, "bytes":1000, "cost": 1.4}

----------------------------------------

TITLE: Configuring Forever Drop Rule in Apache Druid
DESCRIPTION: This JSON snippet defines a forever drop rule in Druid. It specifies that all segments matching this rule should be dropped from the cluster.

LANGUAGE: json
CODE:
{
  "type" : "dropForever"  
}

----------------------------------------

TITLE: Configuring Jetty Server TLS Settings in Apache Druid
DESCRIPTION: These properties configure the TLS/SSL settings for the embedded Jetty web server in Apache Druid. They include settings for the key store, certificate alias, and key store password.

LANGUAGE: properties
CODE:
druid.server.https.keyStorePath=none
druid.server.https.keyStoreType=none
druid.server.https.certAlias=none
druid.server.https.keyStorePassword=none

----------------------------------------

TITLE: Constructing a Timeseries Query in Apache Druid
DESCRIPTION: This snippet demonstrates how to construct a timeseries query object in Druid. It includes various components such as queryType, dataSource, granularity, filter, aggregations, postAggregations, and intervals.

LANGUAGE: json
CODE:
{
  "queryType": "timeseries",
  "dataSource": "sample_datasource",
  "granularity": "day",
  "descending": "true",
  "filter": {
    "type": "and",
    "fields": [
      { "type": "selector", "dimension": "sample_dimension1", "value": "sample_value1" },
      { "type": "or",
        "fields": [
          { "type": "selector", "dimension": "sample_dimension2", "value": "sample_value2" },
          { "type": "selector", "dimension": "sample_dimension3", "value": "sample_value3" }
        ]
      }
    ]
  },
  "aggregations": [
    { "type": "longSum", "name": "sample_name1", "fieldName": "sample_fieldName1" },
    { "type": "doubleSum", "name": "sample_name2", "fieldName": "sample_fieldName2" }
  ],
  "postAggregations": [
    { "type": "arithmetic",
      "name": "sample_divide",
      "fn": "/",
      "fields": [
        { "type": "fieldAccess", "name": "postAgg__sample_name1", "fieldName": "sample_name1" },
        { "type": "fieldAccess", "name": "postAgg__sample_name2", "fieldName": "sample_name2" }
      ]
    }
  ],
  "intervals": [ "2012-01-01T00:00:00.000/2012-01-03T00:00:00.000" ]
}

----------------------------------------

TITLE: Logical NOT Having Specification in Druid
DESCRIPTION: Shows how to negate a having condition using the NOT logical operator.

LANGUAGE: json
CODE:
{
    "queryType": "groupBy",
    "dataSource": "sample_datasource",
    ...
    "having": 
        {
        "type": "not",
        "havingSpec":         
            {
                "type": "equalTo",
                "aggregation": "<aggregate_metric>",
                "value": <numeric_value>
            }
        }
}

----------------------------------------

TITLE: Configuring HTTP Firehose in Druid
DESCRIPTION: Configuration for reading data from remote HTTP sources. Supports parallel index tasks and includes caching and prefetch capabilities.

LANGUAGE: json
CODE:
{
    "type"    : "http",
    "uris"  : ["http://example.com/uri1", "http://example2.com/uri2"]
}

----------------------------------------

TITLE: React DOM Production License
DESCRIPTION: Copyright notice and MIT license information for the react-dom.production.min.js file from React.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Router Strategy Configuration - JSON
DESCRIPTION: Example JSON configurations for Router strategies including timeBoundary and JavaScript-based routing rules

LANGUAGE: json
CODE:
{
  "type":"timeBoundary"
}

LANGUAGE: json
CODE:
{
  "type":"priority",
  "minPriority":0,
  "maxPriority":1
}

LANGUAGE: json
CODE:
{
  "type" : "javascript",
  "function" : "function (config, query) { if (query.getAggregatorSpecs && query.getAggregatorSpecs().size() >= 3) { var size = config.getTierToBrokerMap().values().size(); if (size > 0) { return config.getTierToBrokerMap().values().toArray()[size-1] } else { return config.getDefaultBrokerServiceName() } } else { return null } }"
}

----------------------------------------

TITLE: Configuring Standard Deviation Post-Aggregator in Apache Druid
DESCRIPTION: JSON configuration for the standard deviation post-aggregator. It calculates the standard deviation from a variance aggregator's results.

LANGUAGE: json
CODE:
{
  "type": "stddev",
  "name": "<output_name>",
  "fieldName": "<aggregator_name>",
  "estimator": <string>
}

----------------------------------------

TITLE: Querying Materialized Views in Apache Druid
DESCRIPTION: This JSON snippet shows how to construct a view query in Apache Druid. It wraps a groupBy query within a view query type, allowing Druid to optimize the query execution using available materialized views.

LANGUAGE: json
CODE:
{
    "queryType": "view",
    "query": {
        "queryType": "groupBy",
        "dataSource": "wikiticker",
        "granularity": "all",
        "dimensions": [
            "user"
        ],
        "limitSpec": {
            "type": "default",
            "limit": 1,
            "columns": [
                {
                    "dimension": "added",
                    "direction": "descending",
                    "dimensionOrder": "numeric"
                }
            ]
        },
        "aggregations": [
            {
                "type": "longSum",
                "name": "added",
                "fieldName": "added"
            }
        ],
        "intervals": [
            "2015-09-12/2015-09-13"
        ]
    }
}

----------------------------------------

TITLE: Configuring Quantiles Doubles Sketch to Histogram Post-Aggregator in Druid
DESCRIPTION: JSON configuration for the quantilesDoublesSketchToHistogram post-aggregator. This returns an approximation of the histogram given split points.

LANGUAGE: json
CODE:
{
  "type"  : "quantilesDoublesSketchToHistogram",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,
  "splitPoints" : <array of split points>
}

----------------------------------------

TITLE: Configuring Quantiles Doubles Sketch to Histogram Post-Aggregator in Druid
DESCRIPTION: JSON configuration for the quantilesDoublesSketchToHistogram post-aggregator. This returns an approximation of the histogram given split points.

LANGUAGE: json
CODE:
{
  "type"  : "quantilesDoublesSketchToHistogram",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,
  "splitPoints" : <array of split points>
}

----------------------------------------

TITLE: Configuring DataSketches Extension in Druid
DESCRIPTION: This snippet shows how to include the DataSketches extension in the Druid config file.

LANGUAGE: json
CODE:
"druid.extensions.loadList=[\"druid-datasketches\"]"

----------------------------------------

TITLE: Configuring DataSketches Extension in Druid
DESCRIPTION: This snippet shows how to include the DataSketches extension in the Druid config file.

LANGUAGE: json
CODE:
"druid.extensions.loadList=[\"druid-datasketches\"]"

----------------------------------------

TITLE: Configuring Loading Off-heap MapDB Cache in Druid
DESCRIPTION: JSON configuration for a loading off-heap MapDB cache with reverse lookup. It uses a JDBC data fetcher and specifies cache parameters such as maximum entries, store size, and expiration times.

LANGUAGE: json
CODE:
{
   "type":"loadingLookup",
   "dataFetcher":{ "type":"jdbcDataFetcher", "connectorConfig":"jdbc://mysql://localhost:3306/my_data_base", "table":"lookup_table_name", "keyColumn":"key_column_name", "valueColumn": "value_column_name"},
   "loadingCacheSpec":{"type":"mapDb", "maxEntriesSize":100000},
   "reverseLoadingCacheSpec":{"type":"mapDb", "maxStoreSize":5, "expireAfterAccess":100000, "expireAfterAccess":10000}
}

----------------------------------------

TITLE: License Comments Collection for JavaScript Libraries
DESCRIPTION: A compilation of license and copyright declarations for various JavaScript libraries and dependencies used in the project. All components are licensed under MIT.

LANGUAGE: JavaScript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

/*!***************************************************
* mark.js v8.11.1
* https://markjs.io/
* Copyright (c) 20142018, Julian Khnel
* Released under the MIT license https://git.io/vwTVl
*****************************************************/

/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Defining Virtual Columns in a Druid Scan Query
DESCRIPTION: This snippet demonstrates how to include virtual columns in a Druid scan query. It defines two virtual columns: 'fooPage' concatenating 'foo' with the 'page' column, and 'tripleWordCount' multiplying the 'wordCount' column by 3.

LANGUAGE: json
CODE:
{
 "queryType": "scan",
 "dataSource": "page_data",
 "columns":[],
 "virtualColumns": [
    {
      "type": "expression",
      "name": "fooPage",
      "expression": "concat('foo' + page)",
      "outputType": "STRING"
    },
    {
      "type": "expression",
      "name": "tripleWordCount",
      "expression": "wordCount * 3",
      "outputType": "LONG"
    }
  ],
 "intervals": [
   "2013-01-01/2019-01-02"
 ] 
}

----------------------------------------

TITLE: Defining Virtual Columns in a Druid Scan Query
DESCRIPTION: This snippet demonstrates how to include virtual columns in a Druid scan query. It defines two virtual columns: 'fooPage' concatenating 'foo' with the 'page' column, and 'tripleWordCount' multiplying the 'wordCount' column by 3.

LANGUAGE: json
CODE:
{
 "queryType": "scan",
 "dataSource": "page_data",
 "columns":[],
 "virtualColumns": [
    {
      "type": "expression",
      "name": "fooPage",
      "expression": "concat('foo' + page)",
      "outputType": "STRING"
    },
    {
      "type": "expression",
      "name": "tripleWordCount",
      "expression": "wordCount * 3",
      "outputType": "LONG"
    }
  ],
 "intervals": [
   "2013-01-01/2019-01-02"
 ] 
}

----------------------------------------

TITLE: Prism Syntax Highlighter License
DESCRIPTION: Provides license and author information for Prism, a lightweight syntax highlighting library, which is released under the MIT license.

LANGUAGE: JavaScript
CODE:
/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

----------------------------------------

TITLE: Configuring Druid Runtime Properties for Cassandra
DESCRIPTION: Configuration properties to enable Cassandra backend in Druid Historical and realtime runtime properties files.

LANGUAGE: properties
CODE:
druid.extensions.loadList=["druid-cassandra-storage"]
druid.storage.type=c*
druid.storage.host=localhost:9160
druid.storage.keyspace=druid

----------------------------------------

TITLE: License Information for React DOM
DESCRIPTION: Copyright notice and MIT license information for React DOM production build.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Creating Temporary Shared Directories
DESCRIPTION: Commands to create temporary directories for sharing files between host and Hadoop container.

LANGUAGE: bash
CODE:
mkdir -p /tmp/shared
mkdir -p /tmp/shared/hadoop_xml

----------------------------------------

TITLE: Permission Configuration JSON Example
DESCRIPTION: Example JSON structure for defining access permissions on Druid resources.

LANGUAGE: json
CODE:
[
{
  "resource": {
    "name": "wiki.*",
    "type": "DATASOURCE"
  },
  "action": "READ"
},
{
  "resource": {
    "name": "wikiticker",
    "type": "DATASOURCE"
  },
  "action": "WRITE"
}
]

----------------------------------------

TITLE: Configuring Druid Runtime Properties for Cassandra
DESCRIPTION: Configuration properties to enable Cassandra backend in Druid Historical and realtime runtime properties files.

LANGUAGE: properties
CODE:
druid.extensions.loadList=["druid-cassandra-storage"]
druid.storage.type=c*
druid.storage.host=localhost:9160
druid.storage.keyspace=druid

----------------------------------------

TITLE: Loading Data into Kafka
DESCRIPTION: Commands to set UTF-8 encoding and publish sample data to Kafka topic using console producer

LANGUAGE: bash
CODE:
export KAFKA_OPTS="-Dfile.encoding=UTF-8"
./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic wikipedia < {PATH_TO_DRUID}/quickstart/tutorial/wikiticker-2015-09-12-sampled.json

----------------------------------------

TITLE: Copyright Notice for React Scheduler
DESCRIPTION: License information for the React Scheduler production minified version, which is under the MIT license.

LANGUAGE: JavaScript
CODE:
/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Sample Result Format for Druid Select Query
DESCRIPTION: This snippet shows the format of results returned by a Select query in Druid. It includes pagingIdentifiers for pagination and an array of events with their corresponding segmentId, offset, and event data.

LANGUAGE: json
CODE:
 [{
  "timestamp" : "2013-01-01T00:00:00.000Z",
  "result" : {
    "pagingIdentifiers" : {
      "wikipedia_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9" : 4
    },
    "events" : [ {
      "segmentId" : "wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9",
      "offset" : 0,
      "event" : {
        "timestamp" : "2013-01-01T00:00:00.000Z",
        "robot" : "1",
        "namespace" : "article",
        "anonymous" : "0",
        "unpatrolled" : "0",
        "page" : "11._korpus_(NOVJ)",
        "language" : "sl",
        "newpage" : "0",
        "user" : "EmausBot",
        "count" : 1.0,
        "added" : 39.0,
        "delta" : 39.0,
        "variation" : 39.0,
        "deleted" : 0.0
      }
    }, {
      "segmentId" : "wikipedia_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9",
      "offset" : 1,
      "event" : {
        "timestamp" : "2013-01-01T00:00:00.000Z",
        "robot" : "0",
        "namespace" : "article",
        "anonymous" : "0",
        "unpatrolled" : "0",
        "page" : "112_U.S._580",
        "language" : "en",
        "newpage" : "1",
        "user" : "MZMcBride",
        "count" : 1.0,
        "added" : 70.0,
        "delta" : 70.0,
        "variation" : 70.0,
        "deleted" : 0.0
      }
    }, {
      "segmentId" : "wikipedia_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9",
      "offset" : 2,
      "event" : {
        "timestamp" : "2013-01-01T00:00:00.000Z",
        "robot" : "0",
        "namespace" : "article",
        "anonymous" : "0",
        "unpatrolled" : "0",
        "page" : "113_U.S._243",
        "language" : "en",
        "newpage" : "1",
        "user" : "MZMcBride",
        "count" : 1.0,
        "added" : 77.0,
        "delta" : 77.0,
        "variation" : 77.0,
        "deleted" : 0.0
      }
    }, {
      "segmentId" : "wikipedia_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9",
      "offset" : 3,
      "event" : {
        "timestamp" : "2013-01-01T00:00:00.000Z",
        "robot" : "0",
        "namespace" : "article",
        "anonymous" : "0",
        "unpatrolled" : "0",
        "page" : "113_U.S._73",
        "language" : "en",
        "newpage" : "1",
        "user" : "MZMcBride",
        "count" : 1.0,
        "added" : 70.0,
        "delta" : 70.0,
        "variation" : 70.0,
        "deleted" : 0.0
      }
    }, {
      "segmentId" : "wikipedia_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9",
      "offset" : 4,
      "event" : {
        "timestamp" : "2013-01-01T00:00:00.000Z",
        "robot" : "0",
        "namespace" : "article",
        "anonymous" : "0",
        "unpatrolled" : "0",
        "page" : "113_U.S._756",
        "language" : "en",
        "newpage" : "1",
        "user" : "MZMcBride",
        "count" : 1.0,
        "added" : 68.0,
        "delta" : 68.0,
        "variation" : 68.0,
        "deleted" : 0.0
      }
    } ]
  }
} ]

----------------------------------------

TITLE: Configuring Polling On-heap Lookup Cache in Druid
DESCRIPTION: JSON configuration for a polling on-heap lookup cache that updates every 10 minutes. It uses a JDBC data fetcher to retrieve data from a MySQL database.

LANGUAGE: json
CODE:
{
    "type":"pollingLookup",
   "pollPeriod":"PT10M",
   "dataFetcher":{ "type":"jdbcDataFetcher", "connectorConfig":"jdbc://mysql://localhost:3306/my_data_base", "table":"lookup_table_name", "keyColumn":"key_column_name", "valueColumn": "value_column_name"},
   "cacheFactory":{"type":"onHeapPolling"}
}

----------------------------------------

TITLE: Configuring Druid Extension Loading
DESCRIPTION: Configuration snippet showing how to include the DataSketches extension in Druid's config file.

LANGUAGE: json
CODE:
"druid.extensions.loadList=[\"druid-datasketches\"]"

----------------------------------------

TITLE: Using an IN Filter in Druid
DESCRIPTION: An IN filter checks if a dimension's value is in a specified set of values, similar to SQL's IN clause.

LANGUAGE: JSON
CODE:
{
    "type": "in",
    "dimension": "outlaw",
    "values": ["Good", "Bad", "Ugly"]
}

----------------------------------------

TITLE: Configuring Realtime Thrift Ingestion in Druid
DESCRIPTION: JSON configuration for realtime ingestion of Thrift data using Tranquility. Specifies the parser type, Thrift class, and other required parameters.

LANGUAGE: json
CODE:
{
  "dataSources": [{
    "spec": {
      "dataSchema": {
        "dataSource": "book",
        "granularitySpec": {          },
        "parser": {
          "type": "thrift",
          "thriftClass": "org.apache.druid.data.input.thrift.Book",
          "protocol": "compact",
          "parseSpec": {
            "format": "json",
            ...
          }
        },
        "metricsSpec": [...]
      },
      "tuningConfig": {...}
    },
    "properties": {...}
  }],
  "properties": {...}
}

----------------------------------------

TITLE: Initializing Document Load Event Handler in JavaScript
DESCRIPTION: This snippet indicates the intention to wait for the document to be fully loaded before starting execution. It's a common practice in JavaScript to ensure all DOM elements are available before manipulating them.

LANGUAGE: JavaScript
CODE:
/*!\n   * Wait for document loaded before starting the execution\n   */

----------------------------------------

TITLE: Configuring Realtime Thrift Ingestion in Druid
DESCRIPTION: JSON configuration for realtime ingestion of Thrift data using Tranquility. Specifies the parser type, Thrift class, and other required parameters.

LANGUAGE: json
CODE:
{
  "dataSources": [{
    "spec": {
      "dataSchema": {
        "dataSource": "book",
        "granularitySpec": {          },
        "parser": {
          "type": "thrift",
          "thriftClass": "org.apache.druid.data.input.thrift.Book",
          "protocol": "compact",
          "parseSpec": {
            "format": "json",
            ...
          }
        },
        "metricsSpec": [...]
      },
      "tuningConfig": {...}
    },
    "properties": {...}
  }],
  "properties": {...}
}

----------------------------------------

TITLE: Adding HDFS Deep Storage Bindings in Druid Module
DESCRIPTION: Example of registering data segment puller and pusher implementations for HDFS storage in a DruidModule using Guice bindings.

LANGUAGE: java
CODE:
Binders.dataSegmentPullerBinder(binder)
       .addBinding("hdfs")
       .to(HdfsDataSegmentPuller.class).in(LazySingleton.class);

Binders.dataSegmentPusherBinder(binder)
       .addBinding("hdfs")
       .to(HdfsDataSegmentPusher.class).in(LazySingleton.class);

----------------------------------------

TITLE: Example JSON Metrics Data Structure
DESCRIPTION: Sample JSON structure showing the metrics data format that will be converted to Protobuf.

LANGUAGE: json
CODE:
{
  "unit": "milliseconds",
  "http_method": "GET",
  "value": 44,
  "timestamp": "2017-04-06T02:36:22Z",
  "http_code": "200",
  "page": "/",
  "metricType": "request/latency",
  "server": "www1.example.com"
}

----------------------------------------

TITLE: Creating an Interval Filter in Druid
DESCRIPTION: An interval filter is used for filtering time ranges, particularly useful for the __time column.

LANGUAGE: JSON
CODE:
{
    "type" : "interval",
    "dimension" : "__time",
    "intervals" : [
      "2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z",
      "2014-11-15T00:00:00.000Z/2014-11-16T00:00:00.000Z"
    ]
}

----------------------------------------

TITLE: Processing Check Comment
DESCRIPTION: Comment indicating a check for previous processing state.

LANGUAGE: javascript
CODE:
/*! Check if previously processed */

----------------------------------------

TITLE: Defining Field Accessor Post-Aggregators in Druid Query
DESCRIPTION: Shows two types of field accessor post-aggregators: 'fieldAccess' for raw aggregation objects and 'finalizingFieldAccess' for finalized values like estimated cardinality.

LANGUAGE: JSON
CODE:
{ "type" : "fieldAccess", "name": <output_name>, "fieldName" : <aggregator_name> }

LANGUAGE: JSON
CODE:
{ "type" : "finalizingFieldAccess", "name": <output_name>, "fieldName" : <aggregator_name> }

----------------------------------------

TITLE: Executing Command Line Hadoop Indexer in Java
DESCRIPTION: Command to run the Hadoop Indexer from the command line. It sets the maximum heap size, timezone, file encoding, and specifies the classpath and main class to execute.

LANGUAGE: java
CODE:
java -Xmx256m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -classpath lib/*:<hadoop_config_dir> org.apache.druid.cli.Main index hadoop <spec_file>

----------------------------------------

TITLE: Adding HDFS Deep Storage Bindings in Druid Module
DESCRIPTION: Example of registering data segment puller and pusher implementations for HDFS storage in a DruidModule using Guice bindings.

LANGUAGE: java
CODE:
Binders.dataSegmentPullerBinder(binder)
       .addBinding("hdfs")
       .to(HdfsDataSegmentPuller.class).in(LazySingleton.class);

Binders.dataSegmentPusherBinder(binder)
       .addBinding("hdfs")
       .to(HdfsDataSegmentPusher.class).in(LazySingleton.class);

----------------------------------------

TITLE: Using Bound Filter in Druid
DESCRIPTION: Example of a bound filter for range filtering on dimension values. This can be used for greater than, less than, and between comparisons.

LANGUAGE: JSON
CODE:
{
    "type": "bound",
    "dimension": "age",
    "lower": "21",
    "upper": "31" ,
    "ordering": "numeric"
}

----------------------------------------

TITLE: Copyright and License Notice for NProgress
DESCRIPTION: This snippet contains the copyright and license information for NProgress, a progress bar library, which is licensed under the MIT license.

LANGUAGE: JavaScript
CODE:
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

----------------------------------------

TITLE: Running MySQL Metadata Storage Insert Command
DESCRIPTION: Example command for inserting segments into MySQL metadata storage using HDFS as deep storage.

LANGUAGE: bash
CODE:
java 
-Ddruid.metadata.storage.type=mysql 
-Ddruid.metadata.storage.connector.connectURI=jdbc\:mysql\://localhost\:3306/druid 
-Ddruid.metadata.storage.connector.user=druid 
-Ddruid.metadata.storage.connector.password=diurd 
-Ddruid.extensions.loadList=[\"mysql-metadata-storage\",\"druid-hdfs-storage\"] 
-Ddruid.storage.type=hdfs
-cp $DRUID_CLASSPATH 
org.apache.druid.cli.Main tools insert-segment-to-db --workingDir hdfs://host:port//druid/storage/wikipedia --updateDescriptor true

----------------------------------------

TITLE: Defining Selector Filter in JSON for Apache Druid
DESCRIPTION: This snippet illustrates how to set up a selector filter in Apache Druid's transformSpec. It filters input rows to include only those where the 'country' column has the value 'United States'.

LANGUAGE: json
CODE:
"filter": {
  "type": "selector",
  "dimension": "country",
  "value": "United States"
}

----------------------------------------

TITLE: GET Response for Individual Lookup Configuration
DESCRIPTION: Example JSON response when retrieving the configuration for a specific lookup 'site_id_customer2' in the 'realtime_customer2' tier using a GET request to the Coordinator API.

LANGUAGE: JSON
CODE:
{
  "version": "v1",
  "lookupExtractorFactory": {
    "type": "map",
    "map": {
      "AHF77": "Home"
    }
  }
}

----------------------------------------

TITLE: Adding HDFS Deep Storage Bindings in Druid Module
DESCRIPTION: Example of registering data segment puller and pusher implementations for HDFS storage in a DruidModule using Guice bindings.

LANGUAGE: java
CODE:
Binders.dataSegmentPullerBinder(binder)
       .addBinding("hdfs")
       .to(HdfsDataSegmentPuller.class).in(LazySingleton.class);

Binders.dataSegmentPusherBinder(binder)
       .addBinding("hdfs")
       .to(HdfsDataSegmentPusher.class).in(LazySingleton.class);

----------------------------------------

TITLE: Copyright Notice for Prism
DESCRIPTION: License information for Prism, a syntax highlighting library, which is under the MIT license.

LANGUAGE: JavaScript
CODE:
/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

----------------------------------------

TITLE: Copyright Notice for Prism
DESCRIPTION: License information for Prism, a syntax highlighting library, which is under the MIT license.

LANGUAGE: JavaScript
CODE:
/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

----------------------------------------

TITLE: Executing a Paginated Select Query with fromNext in Druid
DESCRIPTION: This snippet demonstrates a paginated Select query with the 'fromNext' option set to false. This setting is used for backwards compatibility, requiring manual offset incrementation. The query retrieves data from the 'wikipedia' datasource for a specific date range.

LANGUAGE: json
CODE:
 {
   "queryType": "select",
   "dataSource": "wikipedia",
   "descending": "false",
   "dimensions":[],
   "metrics":[],
   "granularity": "all",
   "intervals": [
     "2013-01-01/2013-01-02"
   ],
   "pagingSpec":{"fromNext": "false", "pagingIdentifiers": {}, "threshold":5}
 }

----------------------------------------

TITLE: Copyright Notice for React use-sync-external-store-shim
DESCRIPTION: License information for the React use-sync-external-store-shim production minified version, which is under the MIT license.

LANGUAGE: JavaScript
CODE:
/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Defining String Functions in Markdown Table
DESCRIPTION: A markdown table listing string manipulation functions available in Apache Druid's expression language. It includes functions for concatenation, pattern matching, substring extraction, and case conversion.

LANGUAGE: markdown
CODE:
|name|description|
|----|-----------|
|concat|concat(expr, expr...) concatenate a list of strings|
|like|like(expr, pattern[, escape]) is equivalent to SQL `expr LIKE pattern`|
|lookup|lookup(expr, lookup-name) looks up expr in a registered [query-time lookup](../querying/lookups.html)|
|regexp_extract|regexp_extract(expr, pattern[, index]) applies a regular expression pattern and extracts a capture group index, or null if there is no match. If index is unspecified or zero, returns the substring that matched the pattern.|
|replace|replace(expr, pattern, replacement) replaces pattern with replacement|
|substring|substring(expr, index, length) behaves like java.lang.String's substring|
|strlen|strlen(expr) returns length of a string in UTF-16 code units|
|strpos|strpos(haystack, needle[, fromIndex]) returns the position of the needle within the haystack, with indexes starting from 0. The search will begin at fromIndex, or 0 if fromIndex is not specified. If the needle is not found then the function returns -1.|
|trim|trim(expr[, chars]) remove leading and trailing characters from `expr` if they are present in `chars`. `chars` defaults to ' ' (space) if not provided.|
|ltrim|ltrim(expr[, chars]) remove leading characters from `expr` if they are present in `chars`. `chars` defaults to ' ' (space) if not provided.|
|rtrim|rtrim(expr[, chars]) remove trailing characters from `expr` if they are present in `chars`. `chars` defaults to ' ' (space) if not provided.|
|lower|lower(expr) converts a string to lowercase|
|upper|upper(expr) converts a string to uppercase|

----------------------------------------

TITLE: Configuring Druid PostgreSQL Connection
DESCRIPTION: Configuration properties for connecting Druid to PostgreSQL metadata storage, including extension loading and database credentials

LANGUAGE: properties
CODE:
druid.extensions.loadList=["postgresql-metadata-storage"]
druid.metadata.storage.type=postgresql
druid.metadata.storage.connector.connectURI=jdbc:postgresql://<host>/druid
druid.metadata.storage.connector.user=druid
druid.metadata.storage.connector.password=diurd

----------------------------------------

TITLE: Configuring MomentSketch Post-Aggregator for Quantiles in Druid
DESCRIPTION: JSON configuration for the momentSketchSolveQuantiles post-aggregator, used to query a set of quantiles from MomentSketch aggregations.

LANGUAGE: json
CODE:
{
  "type"  : "momentSketchSolveQuantiles",
  "name" : <output_name>,
  "field" : <reference to moment sketch>,
  "fractions" : <array of doubles in [0,1]>
}

----------------------------------------

TITLE: Configuring Forever Load Rule in Apache Druid
DESCRIPTION: JSON configuration for a forever load rule that specifies how many replicas of a segment should exist in different server tiers indefinitely.

LANGUAGE: json
CODE:
{
  "type" : "loadForever",  
  "tieredReplicants": {
    "hot": 1,
    "_default_tier" : 1
  }
}

----------------------------------------

TITLE: Joining sys.servers and sys.segments Tables
DESCRIPTION: SQL query joining sys.servers and sys.segments to count segments per server for a specific datasource

LANGUAGE: sql
CODE:
SELECT count(segments.segment_id) as num_segments from sys.segments as segments
INNER JOIN sys.server_segments as server_segments
ON segments.segment_id  = server_segments.segment_id
INNER JOIN sys.servers as servers
ON servers.server = server_segments.server
WHERE segments.datasource = 'wikipedia'
GROUP BY servers.server;

----------------------------------------

TITLE: Configuring Interval Broadcast Rule in Apache Druid
DESCRIPTION: This JSON snippet defines an interval broadcast rule in Apache Druid. It specifies how segments of different data sources should be co-located for a specific time interval.

LANGUAGE: json
CODE:
{
  "type" : "broadcastByInterval",
  "colocatedDataSources" : [ "target_source1", "target_source2" ],
  "interval" : "2012-01-01/2013-01-01"
}

----------------------------------------

TITLE: Implementing a Regular Expression Filter in Druid
DESCRIPTION: A regular expression filter matches a dimension against a Java regular expression pattern.

LANGUAGE: JSON
CODE:
"filter": { "type": "regex", "dimension": <dimension_string>, "pattern": <pattern_string> }

----------------------------------------

TITLE: Configuring StaticS3Firehose in Apache Druid
DESCRIPTION: This JSON snippet demonstrates how to configure a StaticS3Firehose in Apache Druid. It specifies the firehose type as 'static-s3' and provides a list of S3 URIs for the objects to be ingested.

LANGUAGE: json
CODE:
"firehose" : {
    "type" : "static-s3",
    "uris": ["s3://foo/bar/file.gz", "s3://bar/foo/file2.gz"]
}

----------------------------------------

TITLE: Implementing a Regular Expression Filter in Druid
DESCRIPTION: A regular expression filter matches a dimension against a Java regular expression pattern.

LANGUAGE: JSON
CODE:
"filter": { "type": "regex", "dimension": <dimension_string>, "pattern": <pattern_string> }

----------------------------------------

TITLE: HLL Sketch Build Aggregator Configuration
DESCRIPTION: JSON configuration for HLLSketchBuild aggregator that creates HLL sketch objects during data ingestion. Includes parameters for output name, input field, size accuracy, and HLL type.

LANGUAGE: json
CODE:
{
  "type" : "HLLSketchBuild",
  "name" : <output name>,
  "fieldName" : <metric name>,
  "lgK" : <size and accuracy parameter>,
  "tgtHllType" : <target HLL type>
 }

----------------------------------------

TITLE: Enabling a MiddleManager
DESCRIPTION: Example JSON response when enabling a MiddleManager via the /druid/worker/v1/enable endpoint.

LANGUAGE: json
CODE:
{"localhost:8091":"enabled"}

----------------------------------------

TITLE: Configuring CombiningFirehose in Apache Druid
DESCRIPTION: This snippet shows how to configure a CombiningFirehose in Druid to combine and merge data from multiple firehoses. It requires a list of delegate firehoses.

LANGUAGE: json
CODE:
{
    "type"  :   "combining",
    "delegates" : [ { "firehose1" }, { "firehose2" }, ..... ]
}

----------------------------------------

TITLE: Defining Union Data Source in Apache Druid JSON
DESCRIPTION: Shows how to define a union data source in Apache Druid using JSON. Union data sources combine two or more table data sources. The data sources being unioned should have the same schema, and union queries should be sent to a Broker/Router process.

LANGUAGE: json
CODE:
{
       "type": "union",
       "dataSources": ["<string_value1>", "<string_value2>", "<string_value3>", ... ]
}

----------------------------------------

TITLE: Configuring All Metrics Converter for Graphite Emitter in JSON
DESCRIPTION: JSON configuration for the 'all' event converter that sends all Druid service metrics to Graphite. This example ignores hostname and service name in the metric path.

LANGUAGE: json
CODE:
{
  "druid.emitter.graphite.eventConverter": {
    "type": "all",
    "namespacePrefix": "druid.test",
    "ignoreHostname": true,
    "ignoreServiceName": true
  }
}

----------------------------------------

TITLE: React Use-Sync-External-Store License
DESCRIPTION: Declares the license for the use-sync-external-store-shim production module of React, which is under the MIT license.

LANGUAGE: JavaScript
CODE:
/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Implementing a Regular Expression Filter in Druid
DESCRIPTION: A regular expression filter matches a dimension against a Java regular expression pattern.

LANGUAGE: JSON
CODE:
"filter": { "type": "regex", "dimension": <dimension_string>, "pattern": <pattern_string> }

----------------------------------------

TITLE: Prism License
DESCRIPTION: MIT license notice for Prism syntax highlighting library by Lea Verou

LANGUAGE: javascript
CODE:
/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

----------------------------------------

TITLE: Resetting Kafka State in Bash
DESCRIPTION: Command to remove Kafka logs for resetting the cluster state after completing the Kafka streaming tutorial.

LANGUAGE: bash
CODE:
rm -rf /tmp/kafka-logs

----------------------------------------

TITLE: Querying with Duration Granularity in Apache Druid
DESCRIPTION: This example demonstrates a groupBy query with a duration granularity of 24 hours (86,400,000 milliseconds).

LANGUAGE: json
CODE:
{
   "queryType":"groupBy",
   "dataSource":"my_dataSource",
   "granularity":{"type": "duration", "duration": "86400000"},
   "dimensions":[
      "language"
   ],
   "aggregations":[
      {
         "type":"count",
         "name":"count"
      }
   ],
   "intervals":[
      "2000-01-01T00:00Z/3000-01-01T00:00Z"
   ]
}

----------------------------------------

TITLE: Running Hadoop Indexer with Self-Contained Jar
DESCRIPTION: This bash command demonstrates how to run the Hadoop indexer using a self-contained jar file, which includes all necessary dependencies to avoid conflicts.

LANGUAGE: bash
CODE:
java -Xmx32m \
  -Dfile.encoding=UTF-8 -Duser.timezone=UTC \
  -classpath config/hadoop:config/overlord:config/_common:$SELF_CONTAINED_JAR:$HADOOP_DISTRIBUTION/etc/hadoop \
  -Djava.security.krb5.conf=$KRB5 \
  org.apache.druid.cli.Main index hadoop \
  $config_path

----------------------------------------

TITLE: Sketch Summary Post-Aggregator Configuration
DESCRIPTION: Post-aggregator configuration for generating a debug-friendly string representation of a DoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "quantilesDoublesSketchToString",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch>
}

----------------------------------------

TITLE: Sample Bitmap Index Output
DESCRIPTION: Example of bitmap index output showing the bitmap serialization format and encoded bitmap data for column values.

LANGUAGE: json
CODE:
{
  "bitmapSerdeFactory": {
    "type": "concise"
  },
  "bitmaps": {
    "isRobot": {
      "false": "//aExfu+Nv3X...",
      "true": "gAl7OoRByQ..."
    }
  }
}

----------------------------------------

TITLE: License Information for NProgress
DESCRIPTION: Copyright notice and MIT license information for the NProgress library by Rico Sta. Cruz.

LANGUAGE: JavaScript
CODE:
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

----------------------------------------

TITLE: Data Ingestion Command
DESCRIPTION: Command to execute the ingestion task using Druid's post-index-task script.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/rollup-index.json --url http://localhost:8081

----------------------------------------

TITLE: Estimating Distinct Keys with Error Bounds using ArrayOfDoublesSketch
DESCRIPTION: Post-aggregator configuration to estimate the number of distinct keys with error bounds from an ArrayOfDoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToEstimateAndBounds",
  "name": <output name>,
  "field"  : <post aggregator that refers to an  ArrayOfDoublesSketch (fieldAccess or another post aggregator)>,
  "numStdDevs", <number from 1 to 3>
}

----------------------------------------

TITLE: Configuring Forever Drop Rule in Apache Druid
DESCRIPTION: This JSON snippet defines a forever drop rule in Apache Druid. It indicates that all segments matching this rule should be dropped from the cluster.

LANGUAGE: json
CODE:
{
  "type" : "dropForever"  
}

----------------------------------------

TITLE: Configuring Kerberos Authenticator in Druid
DESCRIPTION: Basic configuration to enable Kerberos authenticator in Druid's authentication chain. Sets up a named authenticator of type 'kerberos'.

LANGUAGE: properties
CODE:
druid.auth.authenticatorChain=["MyKerberosAuthenticator"]

druid.auth.authenticator.MyKerberosAuthenticator.type=kerberos

----------------------------------------

TITLE: Performing Set Operations on ArrayOfDoublesSketch
DESCRIPTION: This JSON configuration defines a post-aggregator to perform set operations (union, intersection, or set difference) on multiple ArrayOfDoublesSketch instances in Druid.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchSetOp",
  "name": <output name>,
  "operation": <"UNION"|"INTERSECT"|"NOT">,
  "fields"  : <array of post aggregators to access sketch aggregators or post aggregators to allow arbitrary combination of set operations>,
  "nominalEntries" : <parameter that determines the accuracy and size of the sketch>,
  "numberOfValues" : <number of values associated with each distinct key>
}

----------------------------------------

TITLE: Retrieving Loadstatus JSON Response
DESCRIPTION: Example JSON response from the /druid/historical/v1/loadstatus endpoint indicating if all segments in the local cache have been loaded.

LANGUAGE: json
CODE:
{"cacheInitialized":<value>}

----------------------------------------

TITLE: Constant Post-Aggregator in Druid
DESCRIPTION: Returns a specified constant numerical value.

LANGUAGE: json
CODE:
{ "type"  : "constant", "name"  : <output_name>, "value" : <numerical_value> }

----------------------------------------

TITLE: Configuring Detailed Numeric TopNMetricSpec in Druid
DESCRIPTION: JSON object-based metric specification for sorting by numeric values, requiring type and metric name parameters.

LANGUAGE: json
CODE:
"metric": {
    "type": "numeric",
    "metric": "<metric_name>"
}

----------------------------------------

TITLE: Configuring Variance Fold Aggregator for Querying in Apache Druid
DESCRIPTION: JSON configuration for the varianceFold aggregator used during querying. It specifies the output name, metric name, and estimator type.

LANGUAGE: json
CODE:
{
  "type" : "varianceFold",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "estimator" : <string>
}

----------------------------------------

TITLE: Configuring LongLast Aggregator in Druid
DESCRIPTION: JSON configuration for the longLast aggregator in Druid, which computes the metric value with the maximum timestamp or 0 if no row exists.

LANGUAGE: json
CODE:
{ 
  "type" : "longLast",
  "name" : <output_name>, 
  "fieldName" : <metric_name>,
}

----------------------------------------

TITLE: Starting Tranquility Server
DESCRIPTION: Commands to download, extract, and start Tranquility Server for stream ingestion.

LANGUAGE: bash
CODE:
curl -O http://static.druid.io/tranquility/releases/tranquility-distribution-0.8.0.tgz
tar -xzf tranquility-distribution-0.8.0.tgz
cd tranquility-distribution-0.8.0
bin/tranquility <server or kafka> -configFile <path_to_druid_distro>/conf/tranquility/<server or kafka>.json

----------------------------------------

TITLE: Configuring JavaScript ParseSpec in Druid
DESCRIPTION: Example configuration for ingesting data using a JavaScript parser in Druid. Specifies the timestamp column, dimensions, and a JavaScript function for parsing the input data.

LANGUAGE: json
CODE:
{
  "parseSpec":{
    "format" : "javascript",
    "timestampSpec" : {
      "column" : "timestamp"
    },        
    "dimensionsSpec" : {
      "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"]
    },
    "function" : "function(str) { var parts = str.split(\"-\"); return { one: parts[0], two: parts[1] } }"
  }
}

----------------------------------------

TITLE: Copyright Notice for NProgress
DESCRIPTION: License information for the NProgress library, which is under the MIT license.

LANGUAGE: JavaScript
CODE:
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

----------------------------------------

TITLE: Curl Command for Accessing Druid Endpoints with Kerberos
DESCRIPTION: Example curl commands to access Druid HTTP endpoints using SPNEGO authentication with cookie storage

LANGUAGE: bash
CODE:
curl --negotiate -u:anyUser -b ~/cookies.txt -c ~/cookies.txt -X POST -H'Content-Type: application/json' <HTTP_END_POINT>

----------------------------------------

TITLE: Configuring SQL Firehose in Druid
DESCRIPTION: Configuration for SqlFirehose that ingests data from RDBMS sources. Supports multiple SQL queries and connection configuration options.

LANGUAGE: json
CODE:
{
    "type" : "sql",
    "database": {
        "type": "mysql",
        "connectorConfig" : {
        "connectURI" : "jdbc:mysql://host:port/schema",
        "user" : "user",
        "password" : "password"
        }
     },
    "sqls" : ["SELECT * FROM table1", "SELECT * FROM table2"]
}

----------------------------------------

TITLE: License Comments Collection
DESCRIPTION: A collection of license and copyright declaration comments from various JavaScript libraries. All components use the MIT license and include attribution to their respective authors and organizations.

LANGUAGE: javascript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

/*!***************************************************
* mark.js v8.11.1
* https://markjs.io/
* Copyright (c) 20142018, Julian Khnel
* Released under the MIT license https://git.io/vwTVl
*****************************************************/

/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Loading Initial Data in Apache Druid
DESCRIPTION: This command loads the initial dataset into a datasource called 'updates-tutorial' using a JSON specification file.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/updates-init-index.json --url http://localhost:8081

----------------------------------------

TITLE: Copyright and License Notice for React DOM
DESCRIPTION: This snippet contains the copyright and license information for React DOM, which is released under the MIT license.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Complete Cluster Reset Command
DESCRIPTION: Command for running ResetCluster tool to perform a complete cleanup of all cluster state. Uses the --all flag to reset everything at once.

LANGUAGE: bash
CODE:
java org.apache.druid.cli.Main tools reset-cluster --all

----------------------------------------

TITLE: Configuring Period Drop Before Rule in Apache Druid
DESCRIPTION: This JSON snippet defines a Period Drop Before Rule, which specifies that segments before a certain time period should be dropped from the cluster.

LANGUAGE: json
CODE:
{
  "type" : "dropBeforeByPeriod",
  "period" : "P1M"
}

----------------------------------------

TITLE: Configuring Realtime Thrift Ingestion with Tranquility
DESCRIPTION: JSON configuration for setting up realtime Thrift data ingestion using Tranquility. Specifies the data schema, parser configuration, and Thrift class details for streaming ingestion.

LANGUAGE: json
CODE:
{
  "dataSources": [{
    "spec": {
      "dataSchema": {
        "dataSource": "book",
        "granularitySpec": {          },
        "parser": {
          "type": "thrift",
          "thriftClass": "org.apache.druid.data.input.thrift.Book",
          "protocol": "compact",
          "parseSpec": {
            "format": "json",
            ...
          }
        },
        "metricsSpec": [...]
      },
      "tuningConfig": {...}
    },
    "properties": {...}
  }],
  "properties": {...}
}

----------------------------------------

TITLE: Defining ZooKeeper Path for Served Segments in Druid
DESCRIPTION: Specifies the ZooKeeper path where Druid processes create a permanent znode to indicate the segments they are serving.

LANGUAGE: plaintext
CODE:
${druid.zk.paths.servedSegmentsPath}/${druid.host}

----------------------------------------

TITLE: Combining and Overwriting Data in Apache Druid
DESCRIPTION: This command combines existing data with new data and overwrites the original data in the 'updates-tutorial' datasource.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/updates-append-index.json --url http://localhost:8081

----------------------------------------

TITLE: MIT License Notice for Prism
DESCRIPTION: Copyright notice for Prism, a syntax highlighting library by Lea Verou, licensed under MIT.

LANGUAGE: JavaScript
CODE:
/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

----------------------------------------

TITLE: JSON Specification for Bloom Filter Aggregator
DESCRIPTION: This JSON structure defines how to use the Bloom Filter aggregator in Druid queries, specifying the output field name, maximum number of entries, and the dimension to aggregate.

LANGUAGE: json
CODE:
{
      "type": "bloom",
      "name": <output_field_name>,
      "maxNumEntries": <maximum_number_of_elements_for_BloomKFilter>
      "field": <dimension_spec>
    }

----------------------------------------

TITLE: Last Name Character Cardinality Example
DESCRIPTION: Advanced example demonstrating cardinality calculation with dimension extraction function.

LANGUAGE: json
CODE:
{
  "type": "cardinality",
  "name": "distinct_last_name_first_char",
  "fields": [
    {
     "type" : "extraction",
     "dimension" : "last_name",
     "outputName" :  "last_name_first_char",
     "extractionFn" : { "type" : "substring", "index" : 0, "length" : 1 }
    }
  ],
  "byRow" : true
}

----------------------------------------

TITLE: Cardinality Aggregator Example - Distinct People
DESCRIPTION: Example showing how to count distinct combinations of first and last names using row-based cardinality calculation.

LANGUAGE: json
CODE:
{
  "type": "cardinality",
  "name": "distinct_people",
  "fields": [ "first_name", "last_name" ],
  "byRow" : true
}

----------------------------------------

TITLE: Combining and Overwriting Data in Apache Druid
DESCRIPTION: This command combines existing data with new data and overwrites the original data in the 'updates-tutorial' datasource.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/updates-append-index.json

----------------------------------------

TITLE: Loading initial data in Apache Druid
DESCRIPTION: This command loads the Wikipedia edits data into Druid using a pre-defined indexing spec that creates hourly segments for a datasource called 'deletion-tutorial'.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/deletion-index.json --url http://localhost:8081

----------------------------------------

TITLE: Creating Druid User in PostgreSQL
DESCRIPTION: Command to create a Druid user in PostgreSQL with a password prompt.

LANGUAGE: bash
CODE:
createuser druid -P

----------------------------------------

TITLE: License Comments for JavaScript Dependencies
DESCRIPTION: A collection of MIT license declarations and copyright notices for various JavaScript libraries used in the project. Each comment block specifies the package name, version, copyright holder, and license terms.

LANGUAGE: javascript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

/*!***************************************************
* mark.js v8.11.1
* https://markjs.io/
* Copyright (c) 20142018, Julian Khnel
* Released under the MIT license https://git.io/vwTVl
*****************************************************/

/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Sample Kinesis Supervisor Specification
DESCRIPTION: Example JSON configuration for a Kinesis supervisor, including dataSchema, tuningConfig, and ioConfig.

LANGUAGE: json
CODE:
{
  "type": "kinesis",
  "dataSchema": {
    "dataSource": "metrics-kinesis",
    "parser": {
      "type": "string",
      "parseSpec": {
        "format": "json",
        "timestampSpec": {
          "column": "timestamp",
          "format": "auto"
        },
        "dimensionsSpec": {
          "dimensions": [],
          "dimensionExclusions": [
            "timestamp",
            "value"
          ]
        }
      }
    },
    "metricsSpec": [
      {
        "name": "count",
        "type": "count"
      },
      {
        "name": "value_sum",
        "fieldName": "value",
        "type": "doubleSum"
      },
      {
        "name": "value_min",
        "fieldName": "value",
        "type": "doubleMin"
      },
      {
        "name": "value_max",
        "fieldName": "value",
        "type": "doubleMax"
      }
    ],
    "granularitySpec": {
      "type": "uniform",
      "segmentGranularity": "HOUR",
      "queryGranularity": "NONE"
    }
  },
  "tuningConfig": {
    "type": "kinesis",
    "maxRowsPerSegment": 5000000
  },
  "ioConfig": {
    "stream": "metrics",
    "endpoint": "kinesis.us-east-1.amazonaws.com",
    "taskCount": 1,
    "replicas": 1,
    "taskDuration": "PT1H",
    "recordsPerFetch": 2000,
    "fetchDelayMillis": 1000
  }
}

----------------------------------------

TITLE: Standard Deviation Post-Aggregator Configuration
DESCRIPTION: JSON configuration for calculating standard deviation from variance results using a post-aggregator.

LANGUAGE: json
CODE:
{
  "type": "stddev",
  "name": "<output_name>",
  "fieldName": "<aggregator_name>",
  "estimator": <string>
}

----------------------------------------

TITLE: Configuring RabbitMQFirehose in Apache Druid
DESCRIPTION: This JSON snippet demonstrates how to configure the RabbitMQFirehose in Apache Druid. It includes connection details for the RabbitMQ broker and configuration settings for the exchange, queue, and other parameters.

LANGUAGE: json
CODE:
"firehose" : {
   "type" : "rabbitmq",
   "connection" : {
     "host": "localhost",
     "port": "5672",
     "username": "test-dude",
     "password": "test-word",
     "virtualHost": "test-vhost",
     "uri": "amqp://mqserver:1234/vhost"
   },
   "config" : {
     "exchange": "test-exchange",
     "queue" : "druidtest",
     "routingKey": "#",
     "durable": "true",
     "exclusive": "false",
     "autoDelete": "false",
     "maxRetries": "10",
     "retryIntervalSeconds": "1",
     "maxDurationSeconds": "300"
   }
}

----------------------------------------

TITLE: Configuring Inline Lookup Extraction Function in Apache Druid
DESCRIPTION: JSON configuration for an Inline Lookup Extraction Function, which specifies an inline lookup map without registering one in the cluster-wide configuration.

LANGUAGE: JSON
CODE:
{
  "type":"lookup",
  "lookup":{
    "type":"map",
    "map":{"foo":"bar", "baz":"bat"}
  },
  "retainMissingValue":true,
  "injective":true
}

----------------------------------------

TITLE: Including js-yaml Library License in JavaScript
DESCRIPTION: This snippet includes the license information for js-yaml, a JavaScript library for YAML parsing and dumping. It specifies the version, GitHub repository, and MIT license.

LANGUAGE: JavaScript
CODE:
/*! js-yaml 4.1.0 https://github.com/nodeca/js-yaml @license MIT */

----------------------------------------

TITLE: NProgress License Declaration
DESCRIPTION: Specifies the MIT license for NProgress library by Rico Sta. Cruz.

LANGUAGE: JavaScript
CODE:
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

----------------------------------------

TITLE: Sending Wikipedia Data to Tranquility Server
DESCRIPTION: Commands to decompress and POST sample Wikipedia edit data to the Tranquility Server endpoint

LANGUAGE: bash
CODE:
gunzip -k quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz 
curl -XPOST -H'Content-Type: application/json' --data-binary @quickstart/tutorial/wikiticker-2015-09-12-sampled.json http://localhost:8200/v1/post/wikipedia

----------------------------------------

TITLE: Creating Basic Authenticator Configuration
DESCRIPTION: Configuration properties for setting up basic authentication in Druid with an authenticator chain and required parameters.

LANGUAGE: properties
CODE:
druid.auth.authenticatorChain=["MyBasicAuthenticator"]

druid.auth.authenticator.MyBasicAuthenticator.type=basic
druid.auth.authenticator.MyBasicAuthenticator.initialAdminPassword=password1
druid.auth.authenticator.MyBasicAuthenticator.initialInternalClientPassword=password2
druid.auth.authenticator.MyBasicAuthenticator.authorizerName=MyBasicAuthorizer

----------------------------------------

TITLE: Accessing Druid HTTP Endpoints with Kerberos Using curl in Bash
DESCRIPTION: Bash command using curl to access Druid HTTP endpoints with Kerberos authentication. This example shows how to send a POST request to a Druid Broker with a JSON query file.

LANGUAGE: bash
CODE:
curl --negotiate -u:anyUser -b ~/cookies.txt -c ~/cookies.txt -X POST -H'Content-Type: application/json'  http://broker-host:port/druid/v2/?pretty -d @query.json

----------------------------------------

TITLE: Configuring Forever Load Rule in Druid
DESCRIPTION: JSON configuration for a Forever Load Rule that specifies how many replicas of a segment should exist permanently in different server tiers.

LANGUAGE: json
CODE:
{
  "type" : "loadForever",  
  "tieredReplicants": {
    "hot": 1,
    "_default_tier" : 1
  }
}

----------------------------------------

TITLE: Example Druid Query for Unique Users of Product A
DESCRIPTION: Sample Druid query using thetaSketch aggregator to count unique users who visited product A within a specified time range.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "test_datasource",
  "granularity": "ALL",
  "dimensions": [],
  "aggregations": [
    { "type": "thetaSketch", "name": "unique_users", "fieldName": "user_id_sketch" }
  ],
  "filter": { "type": "selector", "dimension": "product", "value": "A" },
  "intervals": [ "2014-10-19T00:00:00.000Z/2014-10-22T00:00:00.000Z" ]
}

----------------------------------------

TITLE: Running Command Line Hadoop Indexer in Java
DESCRIPTION: Command to execute the Hadoop Indexer from the command line. It sets memory, timezone, and encoding parameters, and specifies the classpath and main class to run.

LANGUAGE: java
CODE:
java -Xmx256m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -classpath lib/*:<hadoop_config_dir> org.apache.druid.cli.Main index hadoop <spec_file>

----------------------------------------

TITLE: Configuring TwitterSpritzerFirehose in Apache Druid
DESCRIPTION: This JSON snippet demonstrates how to configure a TwitterSpritzerFirehose in Apache Druid. It specifies the firehose type as 'twitzer' and sets parameters for maximum event count and run duration.

LANGUAGE: json
CODE:
"firehose" : {
    "type" : "twitzer",
    "maxEventCount": -1,
    "maxRunMinutes": 0
}

----------------------------------------

TITLE: Filtering Multi-value Dimensions with OR Logic in Druid
DESCRIPTION: This JSON snippet demonstrates an 'or' filter that matches rows where the 'tags' dimension contains either 't1' or 't3'. It would match rows 1 and 2 in the example dataset.

LANGUAGE: json
CODE:
{
  "type": "or",
  "fields": [
    {
      "type": "selector",
      "dimension": "tags",
      "value": "t1"
    },
    {
      "type": "selector",
      "dimension": "tags",
      "value": "t3"
    }
  ]
}

----------------------------------------

TITLE: Sample Wikipedia Edit Event JSON
DESCRIPTION: Example of a Wikipedia page edit event structure used in the tutorial dataset.

LANGUAGE: json
CODE:
{
  "timestamp":"2015-09-12T20:03:45.018Z",
  "channel":"#en.wikipedia",
  "namespace":"Main",
  "page":"Spider-Man's powers and equipment",
  "user":"foobar",
  "comment":"/* Artificial web-shooters */",
  "cityName":"New York",
  "regionName":"New York",
  "regionIsoCode":"NY",
  "countryName":"United States",
  "countryIsoCode":"US",
  "isAnonymous":false,
  "isNew":false,
  "isMinor":false,
  "isRobot":false,
  "isUnpatrolled":false,
  "added":99,
  "delta":99,
  "deleted":0
}

----------------------------------------

TITLE: Configuring Kafka 0.8.x Firehose in Druid
DESCRIPTION: JSON configuration for setting up a Kafka 0.8.x firehose in Druid. Specifies consumer properties including Zookeeper connection details, group ID, message size limits, and offset behavior. The configuration connects to a Wikipedia feed topic.

LANGUAGE: json
CODE:
"firehose": {
  "type": "kafka-0.8",
  "consumerProps": {
    "zookeeper.connect": "localhost:2181",
    "zookeeper.connection.timeout.ms" : "15000",
    "zookeeper.session.timeout.ms" : "15000",
    "zookeeper.sync.time.ms" : "5000",
    "group.id": "druid-example",
    "fetch.message.max.bytes" : "1048586",
    "auto.offset.reset": "largest",
    "auto.commit.enable": "false"
  },
  "feed": "wikipedia"
}

----------------------------------------

TITLE: Defining a Bound Filter in Druid JSON
DESCRIPTION: Example of a bound filter used for range filtering on dimension values. This specific example filters for ages between 21 and 31 inclusive.

LANGUAGE: json
CODE:
{
    "type": "bound",
    "dimension": "age",
    "lower": "21",
    "upper": "31" ,
    "ordering": "numeric"
}

----------------------------------------

TITLE: ArrayOfDoublesSketch Means Post-Aggregator
DESCRIPTION: JSON configuration for post-aggregator that returns list of mean values from ArrayOfDoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToMeans",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>
}

----------------------------------------

TITLE: Query Count - Initial Data Verification
DESCRIPTION: SQL query to check the total row count in the compaction-tutorial datasource.

LANGUAGE: sql
CODE:
select count(*) from "compaction-tutorial";

----------------------------------------

TITLE: Configuring Cascade Extraction Function in Apache Druid
DESCRIPTION: JSON configuration for a Cascade Extraction Function, which provides chained execution of multiple extraction functions.

LANGUAGE: JSON
CODE:
{
  "type" : "cascade", 
  "extractionFns": [
    { 
      "type" : "regex", 
      "expr" : "/([^/]+)/", 
      "replaceMissingValue": false,
      "replaceMissingValueWith": null
    },
    { 
      "type" : "javascript", 
      "function" : "function(str) { return \"the \".concat(str) }" 
    },
    { 
      "type" : "substring", 
      "index" : 0, "length" : 7 
    }
  ]
}

----------------------------------------

TITLE: Sample Coordinator Dynamic Configuration
DESCRIPTION: Example JSON configuration for Druid Coordinator dynamic settings

LANGUAGE: json
CODE:
{
  "millisToWaitBeforeDeleting": 900000,
  "mergeBytesLimit": 100000000,
  "mergeSegmentsLimit" : 1000,
  "maxSegmentsToMove": 5,
  "replicantLifetime": 15,
  "replicationThrottleLimit": 10,
  "emitBalancingStats": false,
  "killDataSourceWhitelist": ["wikipedia", "testDatasource"],
  "decommissioningNodes": ["localhost:8182", "localhost:8282"],
  "decommissioningMaxPercentOfMaxSegmentsToMove": 70
}

----------------------------------------

TITLE: GroupBy Query Example with Variance Aggregator and Standard Deviation Post-Aggregator in Apache Druid
DESCRIPTION: Example of a groupBy query using the variance aggregator and standard deviation post-aggregator. It specifies dimensions, aggregations, and post-aggregations.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "testing",
  "dimensions": ["alias"],
  "granularity": "all",
  "aggregations": [
    {
      "type": "variance",
      "name": "index_var",
      "fieldName": "index"
    }
  ],
  "postAggregations": [
    {
      "type": "stddev",
      "name": "index_stddev",
      "fieldName": "index_var"
    }
  ],
  "intervals": [
    "2016-03-06T00:00:00/2016-03-06T23:59:59"
  ]
}

----------------------------------------

TITLE: Copyright Notice for mark.js
DESCRIPTION: License information for the mark.js library, version 8.11.1, which is under the MIT license.

LANGUAGE: JavaScript
CODE:
/*!***************************************************
* mark.js v8.11.1
* https://markjs.io/
* Copyright (c) 20142018, Julian Khnel
* Released under the MIT license https://git.io/vwTVl
****************************************************/

----------------------------------------

TITLE: GroupBy Query Example with Variance Aggregator and Standard Deviation Post-Aggregator in Apache Druid
DESCRIPTION: Example of a groupBy query using the variance aggregator and standard deviation post-aggregator. It specifies dimensions, aggregations, and post-aggregations.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "testing",
  "dimensions": ["alias"],
  "granularity": "all",
  "aggregations": [
    {
      "type": "variance",
      "name": "index_var",
      "fieldName": "index"
    }
  ],
  "postAggregations": [
    {
      "type": "stddev",
      "name": "index_stddev",
      "fieldName": "index_var"
    }
  ],
  "intervals": [
    "2016-03-06T00:00:00/2016-03-06T23:59:59"
  ]
}

----------------------------------------

TITLE: Configuring Kafka 0.8.x Firehose in JSON
DESCRIPTION: Example configuration for setting up a Kafka 0.8.x firehose in Druid. Specifies connection parameters for Zookeeper, consumer group settings, and message handling configurations. The firehose connects to a topic named 'wikipedia' and uses specified consumer properties for connectivity and behavior control.

LANGUAGE: json
CODE:
{
  "firehose": {
    "type": "kafka-0.8",
    "consumerProps": {
      "zookeeper.connect": "localhost:2181",
      "zookeeper.connection.timeout.ms" : "15000",
      "zookeeper.session.timeout.ms" : "15000",
      "zookeeper.sync.time.ms" : "5000",
      "group.id": "druid-example",
      "fetch.message.max.bytes" : "1048586",
      "auto.offset.reset": "largest",
      "auto.commit.enable": "false"
    },
    "feed": "wikipedia"
  }
}

----------------------------------------

TITLE: Representing Multi-value Columns in Druid Segments
DESCRIPTION: This example illustrates how Druid's segment data structures are modified to handle multi-value columns. It shows changes in the column data array and bitmaps to accommodate multiple values for a single row.

LANGUAGE: json
CODE:
1: Dictionary that encodes column values
  {
    "Justin Bieber": 0,
    "Ke$ha":         1
  }

2: Column data
  [0,
   [0,1],  <--Row value of multi-value column can have array of values
   1,
   1]

3: Bitmaps - one for each unique value
  value="Justin Bieber": [1,1,0,0]
  value="Ke$ha":         [0,1,1,1]
                            ^
                            |
                            |
    Multi-value column has multiple non-zero entries

----------------------------------------

TITLE: Configuring Lower Case Extraction Function in Apache Druid
DESCRIPTION: JSON configuration for a Lower Case Extraction Function, which converts dimension values to lower case.

LANGUAGE: JSON
CODE:
{
  "type" : "lower"
}

----------------------------------------

TITLE: Copyright Notice for React Is
DESCRIPTION: License information for the React Is production minified version, which is under the MIT license.

LANGUAGE: JavaScript
CODE:
/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Example HDFS Directory Structure for Druid Segments
DESCRIPTION: Shows the expected directory structure for segments stored in HDFS that can be processed by the tool.

LANGUAGE: bash
CODE:
Directory path: /druid/storage/wikipedia

 2013-08-31T000000.000Z_2013-09-01T000000.000Z
    2015-10-21T22_07_57.074Z
            0_descriptor.json
            0_index.zip
 2013-09-01T000000.000Z_2013-09-02T000000.000Z
    2015-10-21T22_07_57.074Z
            0_descriptor.json
            0_index.zip
 2013-09-02T000000.000Z_2013-09-03T000000.000Z
    2015-10-21T22_07_57.074Z
            0_descriptor.json
            0_index.zip
 2013-09-03T000000.000Z_2013-09-04T000000.000Z
     2015-10-21T22_07_57.074Z
             0_descriptor.json
             0_index.zip

----------------------------------------

TITLE: Min/Max Post-Aggregator Configurations
DESCRIPTION: Configurations for min and max post-aggregators to extract minimum and maximum values from histograms.

LANGUAGE: json
CODE:
{ "type" : "min", "name" : <output_name>, "fieldName" : <aggregator_name> }
{ "type" : "max", "name" : <output_name>, "fieldName" : <aggregator_name> }

----------------------------------------

TITLE: Creating Kafka Topic
DESCRIPTION: Command to create a Kafka topic named 'wikipedia' with one partition and replication factor of 1.

LANGUAGE: bash
CODE:
./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic wikipedia

----------------------------------------

TITLE: Configuring log4j2 for Apache Druid in XML
DESCRIPTION: This XML configuration sets up logging for Apache Druid using log4j2. It defines a console appender with a specific pattern layout and sets the root logger level to info. It also includes a commented-out section for enabling HTTP request logging.

LANGUAGE: XML
CODE:
<?xml version="1.0" encoding="UTF-8" ?>
<Configuration status="WARN">
  <Appenders>
    <Console name="Console" target="SYSTEM_OUT">
      <PatternLayout pattern="%d{ISO8601} %p [%t] %c - %m%n"/>
    </Console>
  </Appenders>
  <Loggers>
    <Root level="info">
      <AppenderRef ref="Console"/>
    </Root>

    <!-- Uncomment to enable logging of all HTTP requests
    <Logger name="org.apache.druid.jetty.RequestLog" additivity="false" level="DEBUG">
        <AppenderRef ref="Console"/>
    </Logger>
    -->
  </Loggers>
</Configuration>

----------------------------------------

TITLE: Initial Data Load Query - Druid SQL
DESCRIPTION: SQL query to verify the initial data load containing animal dimension and number metric

LANGUAGE: sql
CODE:
select * from "updates-tutorial";

----------------------------------------

TITLE: Declaring MIT License for Prism
DESCRIPTION: This comment block declares the MIT license for the Prism syntax highlighting library by Lea Verou.

LANGUAGE: JavaScript
CODE:
/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

----------------------------------------

TITLE: Including MomentSketch Extension in Druid Configuration
DESCRIPTION: Configuration snippet to include the druid-momentsketch extension in the Druid loadList.

LANGUAGE: json
CODE:
"druid.extensions.loadList=[\"druid-momentsketch\"]"

----------------------------------------

TITLE: Copyright and License Notice for React Is
DESCRIPTION: This snippet provides the copyright and license information for the React Is package, which is released under the MIT license.

LANGUAGE: JavaScript
CODE:
/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Illustrating Druid Multi-Value Column Data Structures
DESCRIPTION: This code snippet shows how the three data structures for a dimension column are modified to support multi-value columns in Druid, allowing a single row to have multiple values for a column.

LANGUAGE: json
CODE:
1: Dictionary that encodes column values
  {
    "Justin Bieber": 0,
    "Ke$ha":         1
  }

2: Column data
  [0,
   [0,1],  <--Row value of multi-value column can have array of values
   1,
   1]

3: Bitmaps - one for each unique value
  value="Justin Bieber": [1,1,0,0]
  value="Ke$ha":         [0,1,1,1]
                            ^
                            |
                            |
    Multi-value column has multiple non-zero entries

----------------------------------------

TITLE: Druid Query Error Response Structure
DESCRIPTION: Example JSON structure returned when a query fails, showing error details including error code, message, class and host information.

LANGUAGE: json
CODE:
{
  "error" : "Query timeout",
  "errorMessage" : "Timeout waiting for task.",
  "errorClass" : "java.util.concurrent.TimeoutException",
  "host" : "druid1.example.com:8083"
}

----------------------------------------

TITLE: Bound Filter Implementation
DESCRIPTION: Filter for numeric range comparisons with support for strict/non-strict bounds and different ordering types.

LANGUAGE: json
CODE:
{
    "type": "bound",
    "dimension": "age",
    "lower": "21",
    "upper": "31",
    "ordering": "numeric"
}

----------------------------------------

TITLE: Copyright Notice for object-assign
DESCRIPTION: Copyright notice for the object-assign library by Sindre Sorhus, released under the MIT license.

LANGUAGE: JavaScript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

----------------------------------------

TITLE: Querying Materialized Views in Druid
DESCRIPTION: Example of a view query that wraps a groupBy query to leverage materialized view optimization. Demonstrates how to structure queries to take advantage of view selection.

LANGUAGE: json
CODE:
{
    "queryType": "view",
    "query": {
        "queryType": "groupBy",
        "dataSource": "wikiticker",
        "granularity": "all",
        "dimensions": [
            "user"
        ],
        "limitSpec": {
            "type": "default",
            "limit": 1,
            "columns": [
                {
                    "dimension": "added",
                    "direction": "descending",
                    "dimensionOrder": "numeric"
                }
            ]
        },
        "aggregations": [
            {
                "type": "longSum",
                "name": "added",
                "fieldName": "added"
            }
        ],
        "intervals": [
            "2015-09-12/2015-09-13"
        ]
    }
}

----------------------------------------

TITLE: Loading DataSketches Extension in Druid Config
DESCRIPTION: Configuration snippet showing how to include the DataSketches extension in Druid's configuration file.

LANGUAGE: json
CODE:
"druid.extensions.loadList=[\"druid-datasketches\"]"

----------------------------------------

TITLE: Configuring Fixed Buckets Histogram in Druid
DESCRIPTION: JSON configuration for fixed buckets histogram aggregator with specified value range and bucket count. Includes support for different outlier handling modes.

LANGUAGE: json
CODE:
{
  "type" : "fixedBucketsHistogram",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "numBuckets" : <integer>,
  "lowerLimit" : <double>,
  "upperLimit" : <double>,
  "outlierHandlingMode": <mode>
}

----------------------------------------

TITLE: Defining Field Accessor Post-Aggregator in Druid JSON Query
DESCRIPTION: Shows how to define a field accessor post-aggregator in a Druid query. This post-aggregator returns the value produced by a specified aggregator.

LANGUAGE: json
CODE:
{ "type" : "fieldAccess", "name": <output_name>, "fieldName" : <aggregator_name> }

----------------------------------------

TITLE: Running ResetCluster Tool in Java
DESCRIPTION: Command to run the ResetCluster tool using Java, with options to specify which components to reset.

LANGUAGE: bash
CODE:
java org.apache.druid.cli.Main tools reset-cluster [--metadataStore] [--segmentFiles] [--taskLogs] [--hadoopWorkingPath]

----------------------------------------

TITLE: Configuring Filtered Aggregator in Druid JSON
DESCRIPTION: Defines a filtered aggregator that wraps another aggregator and only processes values matching a specified dimension filter. Useful for simultaneous filtered and unfiltered aggregations.

LANGUAGE: JSON
CODE:
{
  "type" : "filtered",
  "filter" : {
    "type" : "selector",
    "dimension" : <dimension>,
    "value" : <dimension value>
  }
  "aggregator" : <aggregation>
}

----------------------------------------

TITLE: Configuring Fragment Search Query in Druid
DESCRIPTION: Defines a search query that matches if a dimension value contains all specified fragments. Includes optional case sensitivity control.

LANGUAGE: json
CODE:
{ 
  "type" : "fragment",
  "case_sensitive" : false,
  "values" : ["fragment1", "fragment2"]
}

----------------------------------------

TITLE: Example JSON Event Structure in Druid
DESCRIPTION: This snippet shows an example of a complex JSON event structure that can be flattened using the JSON Flatten Spec in Druid. It includes nested objects, arrays, and various data types.

LANGUAGE: json
CODE:
{
 "timestamp": "2015-09-12T12:10:53.155Z",
 "dim1": "qwerty",
 "dim2": "asdf",
 "dim3": "zxcv",
 "ignore_me": "ignore this",
 "metrica": 9999,
 "foo": {"bar": "abc"},
 "foo.bar": "def",
 "nestmet": {"val": 42},
 "hello": [1.0, 2.0, 3.0, 4.0, 5.0],
 "mixarray": [1.0, 2.0, 3.0, 4.0, {"last": 5}],
 "world": [{"hey": "there"}, {"tree": "apple"}],
 "thing": {"food": ["sandwich", "pizza"]}
}

----------------------------------------

TITLE: Running S3 Metadata Storage Insert Command
DESCRIPTION: Example command for inserting segments into MySQL metadata storage using S3 as deep storage.

LANGUAGE: bash
CODE:
java
-Ddruid.metadata.storage.type=mysql 
-Ddruid.metadata.storage.connector.connectURI=jdbc\:mysql\://localhost\:3306/druid 
-Ddruid.metadata.storage.connector.user=druid 
-Ddruid.metadata.storage.connector.password=diurd
-Ddruid.extensions.loadList=[\"mysql-metadata-storage\",\"druid-s3-extensions\"]
-Ddruid.storage.type=s3
-Ddruid.s3.accessKey=... 
-Ddruid.s3.secretKey=...
-Ddruid.storage.bucket=your-bucket
-Ddruid.storage.baseKey=druid/storage/wikipedia
-Ddruid.storage.maxListingLength=1000
-cp $DRUID_CLASSPATH
org.apache.druid.cli.Main tools insert-segment-to-db --workingDir "druid/storage/wikipedia" --updateDescriptor true

----------------------------------------

TITLE: Basic ResetCluster Command Usage
DESCRIPTION: Command for running ResetCluster tool with selective cleanup options. Allows specifying which components to reset including metadata store, segment files, task logs, and Hadoop working path.

LANGUAGE: bash
CODE:
java org.apache.druid.cli.Main tools reset-cluster [--metadataStore] [--segmentFiles] [--taskLogs] [--hadoopWorkingPath]

----------------------------------------

TITLE: Declaring MIT License for React's scheduler
DESCRIPTION: This comment block declares the MIT license for React's scheduler production build.

LANGUAGE: JavaScript
CODE:
/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Example groupBy Query Response Format
DESCRIPTION: Shows the format of the response returned by a groupBy query including version, timestamp and event data.

LANGUAGE: json
CODE:
[
  {
    "version" : "v1",
    "timestamp" : "2012-01-01T00:00:00.000Z",
    "event" : {
      "country" : "<some_dim_value_one>",
      "device" : "<some_dim_value_two>",
      "total_usage" : "<some_value_one>",
      "data_transfer" :"<some_value_two>",
      "avg_usage" : "<some_avg_usage_value>"
    }
  }
]

----------------------------------------

TITLE: Executing Native JSON TopN Query in Druid
DESCRIPTION: This snippet shows a native JSON TopN query to retrieve the 10 Wikipedia pages with the most edits on 2015-09-12. It demonstrates how to structure the query and submit it to the Druid broker using curl.

LANGUAGE: json
CODE:
{
  "queryType" : "topN",
  "dataSource" : "wikipedia",
  "intervals" : ["2015-09-12/2015-09-13"],
  "granularity" : "all",
  "dimension" : "page",
  "metric" : "count",
  "threshold" : 10,
  "aggregations" : [
    {
      "type" : "count",
      "name" : "count"
    }
  ]
}

LANGUAGE: bash
CODE:
curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-top-pages.json http://localhost:8082/druid/v2?pretty

----------------------------------------

TITLE: Implementing ContainsSearchQuerySpec in Druid
DESCRIPTION: Case-sensitive search query specification that matches if any part of a dimension value contains the specified search value.

LANGUAGE: json
CODE:
{
  "type"  : "contains",
  "case_sensitive" : true,
  "value" : "some_value"
}

----------------------------------------

TITLE: Promises/A+ Copyright Header
DESCRIPTION: MIT License copyright header for an embeddable minimum strictly-compliant Promises/A+ 1.1.1 Thenable implementation by Ralf S. Engelschall.

LANGUAGE: text
CODE:
/*!
  Embeddable Minimum Strictly-Compliant Promises/A+ 1.1.1 Thenable
  Copyright (c) 2013-2014 Ralf S. Engelschall (http://engelschall.com)
  Licensed under The MIT License (http://opensource.org/licenses/MIT)
  */

----------------------------------------

TITLE: Defining Druid to StatsD Event Conversion in JSON
DESCRIPTION: JSON structure for mapping Druid metrics to StatsD events. Specifies dimensions to include, StatsD type, and optional range conversion for each metric.

LANGUAGE: json
CODE:
{
  "query/time": { "dimensions": ["dataSource", "type"], "type": "timer" },
  "coordinator-segment/count": { "dimensions": ["dataSource"], "type": "gauge" },
  "historical-segment/count": { "dimensions": ["dataSource", "tier", "priority"], "type": "gauge" }
}

----------------------------------------

TITLE: Configuring Cached Namespace Lookup with JDBC Source
DESCRIPTION: Example configuration for a globally cached lookup using a JDBC database source. Includes database connection details, table mapping configuration, and cache timeout settings.

LANGUAGE: json
CODE:
{
    "type": "cachedNamespace",
    "extractionNamespace": {
       "type": "jdbc",
       "connectorConfig": {
         "createTables": true,
         "connectURI": "jdbc:mysql:\/\/localhost:3306\/druid",
         "user": "druid",
         "password": "diurd"
       },
       "table": "lookupTable",
       "keyColumn": "mykeyColumn",
       "valueColumn": "myValueColumn",
       "filter" : "myFilterSQL",
       "tsColumn": "timeColumn"
    },
    "firstCacheTimeout": 120000,
    "injective":true
}

----------------------------------------

TITLE: Defining InsensitiveContainsSearchQuerySpec in Druid JSON
DESCRIPTION: Specifies a case-insensitive contains search query. A match occurs if any part of a dimension value contains the specified value, regardless of case.

LANGUAGE: json
CODE:
{
  "type"  : "insensitive_contains",
  "value" : "some_value"
}

----------------------------------------

TITLE: Configuring Forever Broadcast Rule in Apache Druid
DESCRIPTION: This JSON snippet defines a forever broadcast rule in Druid. It specifies how segments of different data sources should be co-located in historical nodes indefinitely.

LANGUAGE: json
CODE:
{
  "type" : "broadcastForever",
  "colocatedDataSources" : [ "target_source1", "target_source2" ]
}

----------------------------------------

TITLE: Calculating Rollup Ratio with Druid SQL
DESCRIPTION: SQL query to measure the rollup ratio of a datasource by comparing the number of rows in Druid with the number of ingested events.

LANGUAGE: sql
CODE:
SELECT SUM("event_count") / COUNT(*) * 1.0 FROM datasource

----------------------------------------

TITLE: Configuring TimeMax Aggregator for Druid Ingestion
DESCRIPTION: JSON configuration for including a timeMax aggregator during data ingestion in Druid. This aggregator calculates the maximum timestamp for a specified field.

LANGUAGE: json
CODE:
{
    "type": "timeMax",
    "name": "tmax",
    "fieldName": "<field_name, typically column specified in timestamp spec>"
}

----------------------------------------

TITLE: Query Filter HavingSpec Implementation in Druid
DESCRIPTION: Shows how to implement a basic query filter HavingSpec in a Druid groupBy query. This allows using any Druid query filter in the Having clause.

LANGUAGE: json
CODE:
{
    "queryType": "groupBy",
    "dataSource": "sample_datasource",
    ...
    "having": 
        {
            "type" : "filter",
            "filter" : <any Druid query filter>
        }
}

----------------------------------------

TITLE: Object-Assign License Declaration
DESCRIPTION: Declares the MIT license for the object-assign library by Sindre Sorhus.

LANGUAGE: JavaScript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

----------------------------------------

TITLE: Implementing FloatLast Aggregator in Druid
DESCRIPTION: JSON configuration for the floatLast aggregator in Druid, which computes the metric value with the maximum timestamp or 0 if no row exists.

LANGUAGE: json
CODE:
{
  "type" : "floatLast",
  "name" : <output_name>,
  "fieldName" : <metric_name>
}

----------------------------------------

TITLE: Implementing FloatLast Aggregator in Druid
DESCRIPTION: JSON configuration for the floatLast aggregator in Druid, which computes the metric value with the maximum timestamp or 0 if no row exists.

LANGUAGE: json
CODE:
{
  "type" : "floatLast",
  "name" : <output_name>,
  "fieldName" : <metric_name>
}

----------------------------------------

TITLE: Configuring HttpFirehose in Apache Druid
DESCRIPTION: Shows how to set up an HttpFirehose to read data from remote sites via HTTP. This firehose is splittable and can be used by native parallel index tasks. Includes optional authentication configuration.

LANGUAGE: json
CODE:
{
    "type"    : "http",
    "uris"  : ["http://example.com/uri1", "http://example2.com/uri2"]
}

LANGUAGE: json
CODE:
{
    "type": "http",
    "uris": ["http://example.com/uri1", "http://example2.com/uri2"],
    "httpAuthenticationUsername": "username",
    "httpAuthenticationPassword": "password123"
}

----------------------------------------

TITLE: Query Results Format
DESCRIPTION: Example output showing the format of results returned by a query using timeMin and timeMax aggregators

LANGUAGE: text
CODE:
2015-07-28T00:00:00.000Z A 4 2015-07-28T01:00:00.000Z 2015-07-28T05:00:00.000Z
2015-07-28T00:00:00.000Z B 2 2015-07-28T04:00:00.000Z 2015-07-28T06:00:00.000Z
2015-07-29T00:00:00.000Z A 2 2015-07-29T03:00:00.000Z 2015-07-29T04:00:00.000Z
2015-07-29T00:00:00.000Z C 2 2015-07-29T01:00:00.000Z 2015-07-29T02:00:00.000Z

----------------------------------------

TITLE: Configuring Kafka Lookup Extractor in Druid
DESCRIPTION: JSON configuration for setting up a Kafka-based lookup extractor. Requires specification of kafka topic and zookeeper connection properties. The lookup enables reading name/key pairs from a Kafka feed for dimension value renaming.

LANGUAGE: json
CODE:
{
  "type":"kafka",
  "kafkaTopic":"testTopic",
  "kafkaProperties":{"zookeeper.connect":"somehost:2181/kafka"}
}

----------------------------------------

TITLE: Executing Standalone JVM Parquet Ingestion
DESCRIPTION: Bash command for running Parquet ingestion in a standalone JVM environment with Hadoop classpath configuration.

LANGUAGE: bash
CODE:
HADOOP_CLASS_PATH=`hadoop classpath | sed s/*.jar/*/g`

java -Xmx32m -Duser.timezone=UTC -Dfile.encoding=UTF-8 \
  -classpath config/overlord:config/_common:lib/*:$HADOOP_CLASS_PATH:extensions/druid-avro-extensions/*  \
  org.apache.druid.cli.Main index hadoop \
  wikipedia_hadoop_parquet_job.json

----------------------------------------

TITLE: Implementing an Interval Filter in Druid
DESCRIPTION: This snippet illustrates how to create an interval filter in Druid for filtering on time ranges. It uses ISO 8601 time intervals and is suitable for timestamp columns.

LANGUAGE: json
CODE:
{
    "type" : "interval",
    "dimension" : "__time",
    "intervals" : [
      "2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z",
      "2014-11-15T00:00:00.000Z/2014-11-16T00:00:00.000Z"
    ]
}

----------------------------------------

TITLE: Configuring pvalue2tailedZtest Post Aggregator in Druid
DESCRIPTION: This snippet shows how to set up the pvalue2tailedZtest post aggregator in Druid queries. It calculates the p-value of a two-sided z-test from a given z-score, which can be obtained using the zscore2sample post aggregator.

LANGUAGE: json
CODE:
{
  "type": "pvalue2tailedZtest",
  "name": "<output_name>",
  "zScore": <zscore post_aggregator>
}

----------------------------------------

TITLE: React-Is License
DESCRIPTION: Specifies the MIT license for React-Is production build, version 16.13.1.

LANGUAGE: JavaScript
CODE:
/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Manually disabling a segment in Apache Druid
DESCRIPTION: This cURL command sends a DELETE request to the Coordinator to disable a specific segment, where {SEGMENT-ID} is the full segment ID.

LANGUAGE: bash
CODE:
curl -XDELETE http://localhost:8081/druid/coordinator/v1/datasources/deletion-tutorial/segments/{SEGMENT-ID}

----------------------------------------

TITLE: Loading MomentSketch Extension in Druid Config
DESCRIPTION: Configuration snippet showing how to include the druid-momentsketch extension in Druid's config file.

LANGUAGE: json
CODE:
"druid.extensions.loadList=[\"druid-momentsketch\"]"

----------------------------------------

TITLE: Configuring Quantiles Doubles Sketch Aggregator
DESCRIPTION: JSON configuration for the quantilesDoublesSketch aggregator that builds or processes numeric quantiles sketches.

LANGUAGE: json
CODE:
{
  "type" : "quantilesDoublesSketch",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "k": <parameter that controls size and accuracy>
 }

----------------------------------------

TITLE: Executing COUNT Query on Apache Druid Datasource
DESCRIPTION: This SQL query counts the total number of rows in the 'compaction-tutorial' datasource.

LANGUAGE: sql
CODE:
select count(*) from "compaction-tutorial";

----------------------------------------

TITLE: TopN Query Result Format in Apache Druid
DESCRIPTION: This snippet shows the expected format of results returned by a TopN query in Apache Druid. It includes a timestamp and an array of result objects, each containing dimension values and aggregated metrics.

LANGUAGE: json
CODE:
[
  {
    "timestamp": "2013-08-31T00:00:00.000Z",
    "result": [
      {
        "dim1": "dim1_val",
        "count": 111,
        "some_metrics": 10669,
        "average": 96.11711711711712
      },
      {
        "dim1": "another_dim1_val",
        "count": 88,
        "some_metrics": 28344,
        "average": 322.09090909090907
      },
      {
        "dim1": "dim1_val3",
        "count": 70,
        "some_metrics": 871,
        "average": 12.442857142857143
      },
      {
        "dim1": "dim1_val4",
        "count": 62,
        "some_metrics": 815,
        "average": 13.14516129032258
      },
      {
        "dim1": "dim1_val5",
        "count": 60,
        "some_metrics": 2787,
        "average": 46.45
      }
    ]
  }
]

----------------------------------------

TITLE: Defining Logical AND Filter in Druid
DESCRIPTION: Shows how to create an AND filter that combines multiple filters. The filters in the fields can be any other filter type defined in the document.

LANGUAGE: JSON
CODE:
"filter": { "type": "and", "fields": [<filter>, <filter>, ...] }

----------------------------------------

TITLE: License Headers Collection
DESCRIPTION: A collection of MIT license headers and copyright notices for various JavaScript libraries and dependencies used in the Druid project. Each header specifies the copyright holder and references to the MIT license.

LANGUAGE: javascript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

/*!
	Copyright (c) 2018 Jed Watson.
	Licensed under the MIT License (MIT), see
	http://jedwatson.github.io/classnames
*/

/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Configuring Merge Task in Apache Druid (Deprecated)
DESCRIPTION: JSON configuration for a Merge task in Druid. This deprecated task merges a list of segments, with options to specify aggregations, enable/disable rollup, and define the segments to merge along with task context.

LANGUAGE: json
CODE:
{
    "type": "merge",
    "id": <task_id>,
    "dataSource": <task_datasource>,
    "aggregations": <list of aggregators>,
    "rollup": <whether or not to rollup data during a merge>,
    "segments": <JSON list of DataSegment objects to merge>,
    "context": <task context>
}

----------------------------------------

TITLE: Accessing Legacy Coordinator Console URL Format
DESCRIPTION: URL format for accessing the version 1 (legacy) Coordinator Console interface.

LANGUAGE: plaintext
CODE:
http://<COORDINATOR_IP>:<COORDINATOR_PORT>/old-console

----------------------------------------

TITLE: JavaScript Post-Aggregator in Druid
DESCRIPTION: Applies a custom JavaScript function to specified fields. Requires JavaScript functionality to be enabled in Druid.

LANGUAGE: json
CODE:
{
  "type": "javascript",
  "name": "absPercent",
  "fieldNames": ["delta", "total"],
  "function": "function(delta, total) { return 100 * Math.abs(delta) / total; }"
}

----------------------------------------

TITLE: Creating a JavaScript Filter in Druid
DESCRIPTION: A JavaScript filter uses a custom JavaScript function to filter dimension values. Note that JavaScript functionality may be disabled by default.

LANGUAGE: JSON
CODE:
{
  "type" : "javascript",
  "dimension" : <dimension_string>,
  "function" : "function(value) { <...> }"
}

----------------------------------------

TITLE: Creating a JavaScript Filter in Druid
DESCRIPTION: A JavaScript filter uses a custom JavaScript function to filter dimension values. Note that JavaScript functionality may be disabled by default.

LANGUAGE: JSON
CODE:
{
  "type" : "javascript",
  "dimension" : <dimension_string>,
  "function" : "function(value) { <...> }"
}

----------------------------------------

TITLE: Configuring DoubleLast Aggregator in Druid JSON
DESCRIPTION: The doubleLast aggregator computes the metric value with the maximum timestamp or 0 if no row exists. It can only be used in queries, not in ingestion specs.

LANGUAGE: json
CODE:
{
  "type" : "doubleLast",
  "name" : <output_name>,
  "fieldName" : <metric_name>
}

----------------------------------------

TITLE: General TLS Port Configuration Properties
DESCRIPTION: Basic configuration properties for enabling/disabling HTTP and HTTPS connectors in Druid

LANGUAGE: properties
CODE:
druid.enablePlaintextPort=true
druid.enableTlsPort=false

----------------------------------------

TITLE: Configuring Substring Extraction Function in Druid
DESCRIPTION: Returns a substring of the dimension value starting from the supplied index and of the desired length.

LANGUAGE: json
CODE:
{ "type" : "substring", "index" : 1, "length" : 4 }

----------------------------------------

TITLE: Configuring zscore2sample Post Aggregator in Druid
DESCRIPTION: This snippet demonstrates how to configure the zscore2sample post aggregator in Druid queries. It calculates the z-score using a two-sample z-test, converting binary variables to continuous variables for two population proportions.

LANGUAGE: json
CODE:
{
  "type": "zscore2sample",
  "name": "<output_name>",
  "successCount1": <post_aggregator> success count of sample 1,
  "sample1Size": <post_aggregaror> sample 1 size,
  "successCount2": <post_aggregator> success count of sample 2,
  "sample2Size" : <post_aggregator> sample 2 size
}

----------------------------------------

TITLE: Configuring Event Receiver Firehose in Druid
DESCRIPTION: Configuration for ingesting events through an HTTP endpoint. Used with stream-pull ingestion and Tranquility stream push.

LANGUAGE: json
CODE:
{
  "type": "receiver",
  "serviceName": "eventReceiverServiceName",
  "bufferSize": 10000
}

----------------------------------------

TITLE: Downloading and Extracting Tranquility Server for Apache Druid
DESCRIPTION: These commands download the Tranquility distribution tarball, extract it, and move it to the correct location in the Druid package root.

LANGUAGE: bash
CODE:
curl http://static.druid.io/tranquility/releases/tranquility-distribution-0.8.2.tgz -o tranquility-distribution-0.8.2.tgz
tar -xzf tranquility-distribution-0.8.2.tgz
mv tranquility-distribution-0.8.2 tranquility

----------------------------------------

TITLE: Configuring DoubleLast Aggregator in Druid JSON
DESCRIPTION: The doubleLast aggregator computes the metric value with the maximum timestamp or 0 if no row exists. It can only be used in queries, not in ingestion specs.

LANGUAGE: json
CODE:
{
  "type" : "doubleLast",
  "name" : <output_name>,
  "fieldName" : <metric_name>
}

----------------------------------------

TITLE: Simple Data Input Example - JSON
DESCRIPTION: Sample input data showing timestamp-based records with page and language fields.

LANGUAGE: json
CODE:
{"timestamp": "2013-08-31T01:02:33Z", "page": "AAA", "language" : "en"}
{"timestamp": "2013-09-01T01:02:33Z", "page": "BBB", "language" : "en"}
{"timestamp": "2013-09-02T23:32:45Z", "page": "CCC", "language" : "en"}
{"timestamp": "2013-09-03T03:32:45Z", "page": "DDD", "language" : "en"}

----------------------------------------

TITLE: Object-Assign License Declaration
DESCRIPTION: Declares the license information for the object-assign library, which is under the MIT license.

LANGUAGE: JavaScript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

----------------------------------------

TITLE: Pull-deps with Default Version
DESCRIPTION: Simplified command using --defaultVersion parameter to specify a common version for multiple extensions, reducing the need to specify versions individually in coordinates.

LANGUAGE: bash
CODE:
java -classpath "/my/druid/lib/*" org.apache.druid.cli.Main tools pull-deps --defaultVersion 0.15.1-incubating --clean -c org.apache.druid.extensions:mysql-metadata-storage -c org.apache.druid.extensions.contrib:druid-rabbitmq -h org.apache.hadoop:hadoop-client:2.3.0 -h org.apache.hadoop:hadoop-client:2.4.0

----------------------------------------

TITLE: Setting up HDFS Directories
DESCRIPTION: Commands to create and configure HDFS directories required for Druid data ingestion.

LANGUAGE: bash
CODE:
cd /usr/local/hadoop/bin
./hadoop fs -mkdir /druid
./hadoop fs -mkdir /druid/segments
./hadoop fs -mkdir /quickstart
./hadoop fs -chmod 777 /druid
./hadoop fs -chmod 777 /druid/segments
./hadoop fs -chmod 777 /quickstart
./hadoop fs -chmod -R 777 /tmp
./hadoop fs -chmod -R 777 /user
./hadoop fs -put /shared/wikiticker-2015-09-12-sampled.json.gz /quickstart/wikiticker-2015-09-12-sampled.json.gz

----------------------------------------

TITLE: Arithmetic Post-Aggregator in Druid
DESCRIPTION: Defines an arithmetic post-aggregator that applies mathematical functions (+, -, *, /, quotient) to aggregated fields. Includes optional ordering parameter for sorting results.

LANGUAGE: json
CODE:
{
  "type"  : "arithmetic",
  "name"  : <output_name>,
  "fn"    : <arithmetic_function>,
  "fields": [<post_aggregator>, <post_aggregator>, ...],
  "ordering" : <null (default), or "numericFirst">
}

----------------------------------------

TITLE: Setting up HDFS Directories
DESCRIPTION: Commands to create and configure HDFS directories required for Druid data ingestion.

LANGUAGE: bash
CODE:
cd /usr/local/hadoop/bin
./hadoop fs -mkdir /druid
./hadoop fs -mkdir /druid/segments
./hadoop fs -mkdir /quickstart
./hadoop fs -chmod 777 /druid
./hadoop fs -chmod 777 /druid/segments
./hadoop fs -chmod 777 /quickstart
./hadoop fs -chmod -R 777 /tmp
./hadoop fs -chmod -R 777 /user
./hadoop fs -put /shared/wikiticker-2015-09-12-sampled.json.gz /quickstart/wikiticker-2015-09-12-sampled.json.gz

----------------------------------------

TITLE: Router JVM Configuration - Properties
DESCRIPTION: Example JVM settings for running the Router process on a c3.2xlarge EC2 instance with memory and GC configurations

LANGUAGE: properties
CODE:
-server
-Xmx13g
-Xms13g
-XX:NewSize=256m
-XX:MaxNewSize=256m
-XX:+UseConcMarkSweepGC
-XX:+PrintGCDetails
-XX:+PrintGCTimeStamps
-XX:+UseLargePages
-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/mnt/galaxy/deploy/current/
-Duser.timezone=UTC
-Dfile.encoding=UTF-8
-Djava.io.tmpdir=/mnt/tmp

-Dcom.sun.management.jmxremote.port=17071
-Dcom.sun.management.jmxremote.authenticate=false
-Dcom.sun.management.jmxremote.ssl=false

----------------------------------------

TITLE: Jetty Server TLS Configuration Properties
DESCRIPTION: Essential TLS/SSL configuration properties for the Jetty server including keystore settings and client certificate authentication options

LANGUAGE: properties
CODE:
druid.server.https.keyStorePath=none
druid.server.https.keyStoreType=none
druid.server.https.certAlias=none
druid.server.https.keyStorePassword=none
druid.server.https.requireClientCertificate=false
druid.server.https.trustStoreType=java.security.KeyStore.getDefaultType()
druid.server.https.validateHostnames=true

----------------------------------------

TITLE: Configuring EMR Job Properties
DESCRIPTION: Configuration settings for running Druid ingestion on Amazon EMR, including S3 access credentials and codec settings.

LANGUAGE: json
CODE:
"jobProperties" : {
   "fs.s3.awsAccessKeyId" : "YOUR_ACCESS_KEY",
   "fs.s3.awsSecretAccessKey" : "YOUR_SECRET_KEY",
   "fs.s3.impl" : "org.apache.hadoop.fs.s3native.NativeS3FileSystem",
   "fs.s3n.awsAccessKeyId" : "YOUR_ACCESS_KEY",
   "fs.s3n.awsSecretAccessKey" : "YOUR_SECRET_KEY",
   "fs.s3n.impl" : "org.apache.hadoop.fs.s3native.NativeS3FileSystem",
   "io.compression.codecs" : "org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec"
}

----------------------------------------

TITLE: Defining Druid Compaction Task Schema
DESCRIPTION: JSON schema for configuring a Druid compaction task. Specifies required and optional parameters including task type, dataSource, interval, and various configuration options for controlling the compaction process.

LANGUAGE: json
CODE:
{
    "type": "compact",
    "id": <task_id>,
    "dataSource": <task_datasource>,
    "interval": <interval to specify segments to be merged>,
    "dimensions" <custom dimensionsSpec>,
    "keepSegmentGranularity": <true or false>,
    "targetCompactionSizeBytes": <target size of compacted segments>
    "tuningConfig" <index task tuningConfig>,
    "context": <task context>
}

----------------------------------------

TITLE: Querying Materialized Views in Druid
DESCRIPTION: Example of a view query that wraps a groupBy query to leverage materialized views for optimized performance when querying the wikiticker dataset.

LANGUAGE: json
CODE:
{
    "queryType": "view",
    "query": {
        "queryType": "groupBy",
        "dataSource": "wikiticker",
        "granularity": "all",
        "dimensions": [
            "user"
        ],
        "limitSpec": {
            "type": "default",
            "limit": 1,
            "columns": [
                {
                    "dimension": "added",
                    "direction": "descending",
                    "dimensionOrder": "numeric"
                }
            ]
        },
        "aggregations": [
            {
                "type": "longSum",
                "name": "added",
                "fieldName": "added"
            }
        ],
        "intervals": [
            "2015-09-12/2015-09-13"
        ]
    }
}

----------------------------------------

TITLE: License Comments Block for Multiple Dependencies
DESCRIPTION: A collection of license declarations and copyright notices for various JavaScript libraries and dependencies. Each block specifies the MIT license terms and copyright holders.

LANGUAGE: javascript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

/*!
	Copyright (c) 2018 Jed Watson.
	Licensed under the MIT License (MIT), see
	http://jedwatson.github.io/classnames
*/

/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Theta Sketch Set Operation Post-Aggregator in Druid
DESCRIPTION: JSON configuration for a Theta Sketch Set Operation post-aggregator in Druid. This allows for set operations (union, intersect, not) on Theta Sketches.

LANGUAGE: json
CODE:
{
  "type"  : "thetaSketchSetOp",
  "name": <output name>,
  "func": <UNION|INTERSECT|NOT>,
  "fields"  : <array of fieldAccess type post aggregators to access the thetaSketch aggregators or thetaSketchSetOp type post aggregators to allow arbitrary combination of set operations>,
  "size": <16384 by default, must be max of size from sketches in fields input>
}

----------------------------------------

TITLE: Configuring Priority Router Strategy in JSON
DESCRIPTION: JSON configuration for the priority router strategy, which routes queries based on their priority level.

LANGUAGE: json
CODE:
{
  "type":"priority",
  "minPriority":0,
  "maxPriority":1
}

----------------------------------------

TITLE: License Information for React Is
DESCRIPTION: Copyright notice and MIT license information for React Is production build.

LANGUAGE: JavaScript
CODE:
/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Mixed Version Segments Example
DESCRIPTION: Shows a mixed state during updates where some intervals have been updated to v2 while others remain on v1.

LANGUAGE: plaintext
CODE:
foo_2015-01-01/2015-01-02_v1_0
foo_2015-01-02/2015-01-03_v2_1
foo_2015-01-03/2015-01-04_v1_2

----------------------------------------

TITLE: Configuring RegexFiltered DimensionSpec in Apache Druid JSON
DESCRIPTION: Illustrates the configuration of a RegexFiltered DimensionSpec, which retains only the values matching a specified regex pattern in multi-value dimensions.

LANGUAGE: JSON
CODE:
{ "type" : "regexFiltered", "delegate" : <dimensionSpec>, "pattern": <java regex pattern> }

----------------------------------------

TITLE: MIT License Declarations for JavaScript Dependencies
DESCRIPTION: Collection of license headers and copyright notices for various JavaScript libraries and dependencies used in the project. Each header specifies the component name, version (where applicable), copyright holders, and MIT license reference.

LANGUAGE: javascript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

/*!***************************************************
* mark.js v8.11.1
* https://markjs.io/
* Copyright (c) 20142018, Julian Khnel
* Released under the MIT license https://git.io/vwTVl
*****************************************************/

/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Preparing Sample Data for Kafka
DESCRIPTION: Commands to extract sample data for streaming into Kafka.

LANGUAGE: bash
CODE:
cd quickstart/tutorial
gunzip -k wikiticker-2015-09-12-sampled.json.gz

----------------------------------------

TITLE: Preparing Sample Data for Kafka
DESCRIPTION: Commands to extract sample data for streaming into Kafka.

LANGUAGE: bash
CODE:
cd quickstart/tutorial
gunzip -k wikiticker-2015-09-12-sampled.json.gz

----------------------------------------

TITLE: Configuring White-list Converter for Ambari Metrics Emitter in Druid
DESCRIPTION: JSON configuration for the 'whiteList' event converter, which sends only white-listed metrics and dimensions to Ambari Metrics. This example shows how to specify a custom file path for the white list map JSON object.

LANGUAGE: json
CODE:
druid.emitter.ambari-metrics.eventConverter={"type":"whiteList", "namespacePrefix": "druid.test", "ignoreHostname":true, "appName":"druid", "mapPath":"/pathPrefix/fileName.json"}

----------------------------------------

TITLE: Copyright Notice for React DOM
DESCRIPTION: License information for the React DOM production minified version, which is under the MIT license.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Configuring Polling On-heap Lookup in Druid
DESCRIPTION: JSON configuration example for a polling cache that updates its on-heap cache every 10 minutes using JDBC data fetcher.

LANGUAGE: json
CODE:
{
    "type":"pollingLookup",
   "pollPeriod":"PT10M",
   "dataFetcher":{ "type":"jdbcDataFetcher", "connectorConfig":"jdbc://mysql://localhost:3306/my_data_base", "table":"lookup_table_name", "keyColumn":"key_column_name", "valueColumn": "value_column_name"},
   "cacheFactory":{"type":"onHeapPolling"}
}

----------------------------------------

TITLE: License Documentation Comments
DESCRIPTION: Collection of license headers and copyright notices for various JavaScript libraries and dependencies, all using MIT license.

LANGUAGE: javascript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

/*!
	Copyright (c) 2018 Jed Watson.
	Licensed under the MIT License (MIT), see
	http://jedwatson.github.io/classnames
*/

/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Configuring Polling On-heap Lookup in Druid
DESCRIPTION: JSON configuration example for a polling cache that updates its on-heap cache every 10 minutes using JDBC data fetcher.

LANGUAGE: json
CODE:
{
    "type":"pollingLookup",
   "pollPeriod":"PT10M",
   "dataFetcher":{ "type":"jdbcDataFetcher", "connectorConfig":"jdbc://mysql://localhost:3306/my_data_base", "table":"lookup_table_name", "keyColumn":"key_column_name", "valueColumn": "value_column_name"},
   "cacheFactory":{"type":"onHeapPolling"}
}

----------------------------------------

TITLE: Configuring HTTP Compression Properties in YAML
DESCRIPTION: YAML configuration for setting HTTP compression properties in Apache Druid. It includes the compression level and inflate buffer size for request decompression.

LANGUAGE: yaml
CODE:
druid.server.http.compressionLevel: -1
druid.server.http.inflateBufferSize: 4096

----------------------------------------

TITLE: Example Authorization Permission JSON
DESCRIPTION: JSON format for defining resource access permissions in the authorization system.

LANGUAGE: json
CODE:
[
{
  "resource": {
    "name": "wiki.*",
    "type": "DATASOURCE"
  },
  "action": "READ"
},
{
  "resource": {
    "name": "wikiticker",
    "type": "DATASOURCE"
  },
  "action": "WRITE"
}
]

----------------------------------------

TITLE: Time Boundary Query Result Format in Apache Druid
DESCRIPTION: This JSON structure shows the format of the result returned by a time boundary query in Druid. It includes the timestamp of the query execution and the minimum and maximum timestamps found in the dataset.

LANGUAGE: json
CODE:
[ {
  "timestamp" : "2013-05-09T18:24:00.000Z",
  "result" : {
    "minTime" : "2013-05-09T18:24:00.000Z",
    "maxTime" : "2013-05-09T18:37:00.000Z"
  }
} ]

----------------------------------------

TITLE: License Notice for React
DESCRIPTION: This snippet provides the license information for the React production build, which is licensed under the MIT license.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Checking Previous Processing in JavaScript
DESCRIPTION: This comment suggests that there's a check to determine if something has been previously processed. It's likely used to prevent duplicate operations or to optimize performance by skipping unnecessary steps.

LANGUAGE: JavaScript
CODE:
/*! Check if previously processed */

----------------------------------------

TITLE: Configuring Core Extensions in Druid Properties
DESCRIPTION: Example configuration for loading core extensions like postgresql-metadata-storage and hdfs-storage in Druid's common.runtime.properties file.

LANGUAGE: properties
CODE:
druid.extensions.loadList=["postgresql-metadata-storage", "druid-hdfs-storage"]

----------------------------------------

TITLE: Accessing Druid Task Completion Report Endpoint
DESCRIPTION: API endpoint format for retrieving task completion reports from the Druid Overlord.

LANGUAGE: plaintext
CODE:
http://<OVERLORD-HOST>:<OVERLORD-PORT>/druid/indexer/v1/task/<task-id>/reports

----------------------------------------

TITLE: Configuring Broker Connection Pool in YAML
DESCRIPTION: Sets the number of connections each Broker can open to a Historical or Task node. This configuration affects the number of concurrent queries a Broker can process.

LANGUAGE: yaml
CODE:
druid.broker.http.numConnections: 50

----------------------------------------

TITLE: Mark.js License Declaration
DESCRIPTION: Includes license and version information for mark.js, a JavaScript library released under the MIT license.

LANGUAGE: JavaScript
CODE:
/*!***************************************************
* mark.js v8.11.1
* https://markjs.io/
* Copyright (c) 20142018, Julian Khnel
* Released under the MIT license https://git.io/vwTVl
****************************************************/

----------------------------------------

TITLE: Example Druid Task Completion Report JSON
DESCRIPTION: Sample JSON response showing ingestion statistics including row counts and error information for different ingestion phases.

LANGUAGE: json
CODE:
{
  "ingestionStatsAndErrors": {
    "taskId": "compact_twitter_2018-09-24T18:24:23.920Z",
    "payload": {
      "ingestionState": "COMPLETED",
      "unparseableEvents": {},
      "rowStats": {
        "determinePartitions": {
          "processed": 0,
          "processedWithError": 0,
          "thrownAway": 0,
          "unparseable": 0
        },
        "buildSegments": {
          "processed": 5390324,
          "processedWithError": 0,
          "thrownAway": 0,
          "unparseable": 0
        }
      },
      "errorMsg": null
    },
    "type": "ingestionStatsAndErrors"
  }
}

----------------------------------------

TITLE: Configuring Default DimensionSpec in Apache Druid JSON
DESCRIPTION: Defines how to configure a Default DimensionSpec, which returns dimension values as-is and optionally renames the dimension. It includes options for specifying the dimension, output name, and output type.

LANGUAGE: JSON
CODE:
{
  "type" : "default",
  "dimension" : <dimension>,
  "outputName": <output_name>,
  "outputType": <"STRING"|"LONG"|"FLOAT">
}

----------------------------------------

TITLE: Accessing Coordinator Console URL Format
DESCRIPTION: URL pattern for accessing the version 2 Coordinator Console.

LANGUAGE: plain
CODE:
http://<COORDINATOR_IP>:<COORDINATOR_PORT>

----------------------------------------

TITLE: Configuring StatsD Emitter Parameters in JSON
DESCRIPTION: JSON configuration for StatsD Emitter in Druid. Includes parameters for hostname, port, prefix, separator, and other options to customize the emitter behavior.

LANGUAGE: json
CODE:
{
  "druid.emitter.statsd.hostname": "statsD_server_hostname",
  "druid.emitter.statsd.port": 8125,
  "druid.emitter.statsd.prefix": "druid",
  "druid.emitter.statsd.separator": ".",
  "druid.emitter.statsd.includeHost": false,
  "druid.emitter.statsd.dimensionMapPath": "/path/to/dimension/map.json",
  "druid.emitter.statsd.blankHolder": "-"
}

----------------------------------------

TITLE: Configuring Custom Password Provider in Druid
DESCRIPTION: This snippet shows the general structure for configuring a custom password provider in Druid. It includes the type of the registered password provider and additional Jackson properties as needed.

LANGUAGE: json
CODE:
{ "type": "<registered_password_provider_name>", "<jackson_property>": "<value>", ... }

----------------------------------------

TITLE: Maven Dependencies for Druid Extensions
DESCRIPTION: XML configuration showing how to add Druid extensions dependencies in Maven pom.xml

LANGUAGE: xml
CODE:
<dependency>
      <groupId>org.apache.druid.extensions</groupId>
      <artifactId>druid-avro-extensions</artifactId>
      <version>${project.parent.version}</version>
  </dependency>

  <dependency>
      <groupId>org.apache.druid.extensions</groupId>
      <artifactId>druid-parquet-extensions</artifactId>
      <version>${project.parent.version}</version>
  </dependency>

  <dependency>
      <groupId>org.apache.druid.extensions</groupId>
      <artifactId>druid-hdfs-storage</artifactId>
      <version>${project.parent.version}</version>
  </dependency>

  <dependency>
      <groupId>org.apache.druid.extensions</groupId>
      <artifactId>mysql-metadata-storage</artifactId>
      <version>${project.parent.version}</version>
  </dependency>

----------------------------------------

TITLE: Creating Temporary Shared Directories for Hadoop
DESCRIPTION: Commands to create temporary shared directories for file transfer between host and Hadoop container.

LANGUAGE: bash
CODE:
mkdir -p /tmp/shared
mkdir -p /tmp/shared/hadoop_xml

----------------------------------------

TITLE: License Notice for React Is
DESCRIPTION: This snippet provides the license information for the react-is production build, which is licensed under the MIT license.

LANGUAGE: JavaScript
CODE:
/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Configuring subtotals in a Druid groupBy Query
DESCRIPTION: Example of using the subtotalsSpec parameter in a Druid groupBy query to compute multiple sub-groupings in a single query. This snippet shows how to specify dimension output names and create subgroups.

LANGUAGE: json
CODE:
{
"type": "groupBy",
 ...
 ...
"dimensions": [
  {
  "type" : "default",
  "dimension" : "d1col",
  "outputName": "D1"
  },
  {
  "type" : "extraction",
  "dimension" : "d2col",
  "outputName" :  "D2",
  "extractionFn" : extraction_func
  },
  {
  "type":"lookup",
  "dimension":"d3col",
  "outputName":"D3",
  "name":"my_lookup"
  }
],
...
...
"subtotalsSpec":[ ["D1", "D2", "D3"], ["D1", "D3"], ["D3"]],
..

}

----------------------------------------

TITLE: Configuring subtotals in a Druid groupBy Query
DESCRIPTION: Example of using the subtotalsSpec parameter in a Druid groupBy query to compute multiple sub-groupings in a single query. This snippet shows how to specify dimension output names and create subgroups.

LANGUAGE: json
CODE:
{
"type": "groupBy",
 ...
 ...
"dimensions": [
  {
  "type" : "default",
  "dimension" : "d1col",
  "outputName": "D1"
  },
  {
  "type" : "extraction",
  "dimension" : "d2col",
  "outputName" :  "D2",
  "extractionFn" : extraction_func
  },
  {
  "type":"lookup",
  "dimension":"d3col",
  "outputName":"D3",
  "name":"my_lookup"
  }
],
...
...
"subtotalsSpec":[ ["D1", "D2", "D3"], ["D1", "D3"], ["D3"]],
..

}

----------------------------------------

TITLE: Configuring HyperUnique Aggregator in Apache Druid
DESCRIPTION: JSON configuration for the hyperUnique aggregator in Apache Druid. It uses HyperLogLog to compute the estimated cardinality of a pre-aggregated dimension. Parameters include name, fieldName, isInputHyperUnique, and round options.

LANGUAGE: json
CODE:
{ 
  "type" : "hyperUnique",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "isInputHyperUnique" : false,
  "round" : false
}

----------------------------------------

TITLE: Mark.js License Declaration
DESCRIPTION: Provides license information for mark.js library, version 8.11.1, under MIT license.

LANGUAGE: JavaScript
CODE:
/*!***************************************************
* mark.js v8.11.1
* https://markjs.io/
* Copyright (c) 20142018, Julian Khnel
* Released under the MIT license https://git.io/vwTVl
****************************************************/

----------------------------------------

TITLE: CSV ParseSpec Configuration
DESCRIPTION: Configuration specification for parsing CSV data in Druid, including column definitions and dimension specifications.

LANGUAGE: json
CODE:
"parseSpec": {
  "format" : "csv",
  "timestampSpec" : {
    "column" : "timestamp"
  },
  "columns" : ["timestamp","page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city","added","deleted","delta"],
  "dimensionsSpec" : {
    "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"]
  }
}

----------------------------------------

TITLE: Running Hadoop Indexer Command
DESCRIPTION: Bash command to run the Hadoop indexer with custom classpath and configuration settings

LANGUAGE: bash
CODE:
java -Xmx32m \
  -Dfile.encoding=UTF-8 -Duser.timezone=UTC \
  -classpath config/hadoop:config/overlord:config/_common:$SELF_CONTAINED_JAR:$HADOOP_DISTRIBUTION/etc/hadoop \
  -Djava.security.krb5.conf=$KRB5 \
  org.apache.druid.cli.Main index hadoop \
  $config_path

----------------------------------------

TITLE: Druid String Function Examples
DESCRIPTION: Examples of string manipulation functions including concatenation, pattern matching, and case conversion.

LANGUAGE: SQL
CODE:
concat(expr, expr...)
like(expr, pattern[, escape])
lookup(expr, lookup-name)
regexp_extract(expr, pattern[, index])
replace(expr, pattern, replacement)
substring(expr, index, length)
strlen(expr)
strpos(haystack, needle[, fromIndex])
trim(expr[, chars])
lower(expr)
upper(expr)

----------------------------------------

TITLE: Extracting Druid Installation Files
DESCRIPTION: Commands to extract the downloaded Druid package and navigate to its directory.

LANGUAGE: bash
CODE:
tar -xzf apache-druid-0.14.1-incubating-bin.tar.gz\ncd apache-druid-0.14.1-incubating

----------------------------------------

TITLE: MIT License Declarations for Dependencies
DESCRIPTION: A compilation of license headers and copyright notices from various JavaScript libraries and dependencies used in the Druid project. Each comment block contains license information, copyright holders, and links to original sources.

LANGUAGE: javascript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

/*!
	Copyright (c) 2018 Jed Watson.
	Licensed under the MIT License (MIT), see
	http://jedwatson.github.io/classnames
*/

/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Bezier Curve Generator Copyright Header
DESCRIPTION: MIT License copyright header for a Bezier curve function generator by Gaetan Renaudeau.

LANGUAGE: text
CODE:
/*! Bezier curve function generator. Copyright Gaetan Renaudeau. MIT License: http://en.wikipedia.org/wiki/MIT_License */

----------------------------------------

TITLE: Configuring FragmentSearchQuerySpec in Druid JSON
DESCRIPTION: Specifies a search query that matches if a dimension value contains all specified fragments. It allows case-sensitive or case-insensitive matching. This is useful for multi-term searches.

LANGUAGE: json
CODE:
{ 
  "type" : "fragment",
  "case_sensitive" : false,
  "values" : ["fragment1", "fragment2"]
}

----------------------------------------

TITLE: Example Druid Live Row Stats Report JSON
DESCRIPTION: Sample JSON response showing live statistics including moving averages and totals for row processing metrics during task execution.

LANGUAGE: json
CODE:
{
  "movingAverages": {
    "buildSegments": {
      "5m": {
        "processed": 3.392158326408501,
        "unparseable": 0,
        "thrownAway": 0,
        "processedWithError": 0
      },
      "15m": {
        "processed": 1.736165476881023,
        "unparseable": 0,
        "thrownAway": 0,
        "processedWithError": 0
      },
      "1m": {
        "processed": 4.206417693750045,
        "unparseable": 0,
        "thrownAway": 0,
        "processedWithError": 0
      }
    }
  },
  "totals": {
    "buildSegments": {
      "processed": 1994,
      "processedWithError": 0,
      "thrownAway": 0,
      "unparseable": 0
    }
  }
}

----------------------------------------

TITLE: Configuring ListFiltered DimensionSpec in Druid
DESCRIPTION: Acts as a whitelist or blacklist for values in multi-value dimensions. It takes a delegate DimensionSpec and filtering criteria.

LANGUAGE: json
CODE:
{ "type" : "listFiltered", "delegate" : <dimensionSpec>, "values": <array of strings>, "isWhitelist": <optional attribute for true/false, default is true> }

----------------------------------------

TITLE: Timeseries Query with Grand Totals
DESCRIPTION: Example showing how to enable grand totals in a timeseries query by adding the grandTotal context parameter. This returns an additional row with totals across all time buckets.

LANGUAGE: json
CODE:
{
  "queryType": "timeseries",
  "dataSource": "sample_datasource",
  "intervals": [ "2012-01-01T00:00:00.000/2012-01-03T00:00:00.000" ],
  "granularity": "day",
  "aggregations": [
    { "type": "longSum", "name": "sample_name1", "fieldName": "sample_fieldName1" },
    { "type": "doubleSum", "name": "sample_name2", "fieldName": "sample_fieldName2" }
  ],
  "context": {
    "grandTotal": true
  }
}

----------------------------------------

TITLE: Submitting Day Granularity Compaction Task in Druid
DESCRIPTION: Command to submit the compaction task that changes the segment granularity to DAY.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/compaction-day-granularity.json --url http://localhost:8081

----------------------------------------

TITLE: Submitting Day Granularity Compaction Task in Druid
DESCRIPTION: Command to submit the compaction task that changes the segment granularity to DAY.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/compaction-day-granularity.json --url http://localhost:8081

----------------------------------------

TITLE: Submitting Day Granularity Compaction Task in Apache Druid
DESCRIPTION: This command submits the compaction task to compact segments into day granularity segments.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/compaction-day-granularity.json --url http://localhost:8081

----------------------------------------

TITLE: Submitting Day Granularity Compaction Task in Apache Druid
DESCRIPTION: This command submits the compaction task to compact segments into day granularity segments.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/compaction-day-granularity.json --url http://localhost:8081

----------------------------------------

TITLE: Configuring Protobuf Parser in Apache Druid
DESCRIPTION: JSON configuration for the Protobuf parser in Apache Druid. It specifies the parser type, descriptor file, message type, and parse specification for ingesting Protobuf data.

LANGUAGE: json
CODE:
{
  "type": "protobuf",
  "descriptor": "file:///tmp/metrics.desc",
  "protoMessageType": "Metrics",
  "parseSpec": {
    "format": "json",
    "timestampSpec": {
      "column": "timestamp",
      "format": "auto"
    },
    "dimensionsSpec": {
      "dimensions": [
        "unit",
        "http_method",
        "http_code",
        "page",
        "metricType",
        "server"
      ],
      "dimensionExclusions": [
        "timestamp",
        "value"
      ]
    }
  }
}

----------------------------------------

TITLE: Configuring Contains Search Query in Druid
DESCRIPTION: Defines a case-sensitive search query that matches if any part of a dimension value contains the specified search value.

LANGUAGE: json
CODE:
{
  "type"  : "contains",
  "case_sensitive" : true,
  "value" : "some_value"
}

----------------------------------------

TITLE: Generating Runge-Kutta Spring Physics Functions in JavaScript
DESCRIPTION: A function generator for Runge-Kutta spring physics calculations. Adapted from Framer.js, this utility can be used for creating realistic spring animations or simulations in JavaScript applications.

LANGUAGE: JavaScript
CODE:
/*! Runge-Kutta spring physics function generator. Adapted from Framer.js, copyright Koen Bok. MIT License: http://en.wikipedia.org/wiki/MIT_License */

----------------------------------------

TITLE: Configuring Local Index Task in Druid
DESCRIPTION: Example configuration for a local index task that processes Wikipedia data on a single node. Includes data schema, metrics specifications, and tuning parameters.

LANGUAGE: json
CODE:
{
  "type" : "index",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "wikipedia",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "timestampSpec" : {
            "column" : "timestamp",
            "format" : "auto"
          },
          "dimensionsSpec" : {
            "dimensions": ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"],
            "dimensionExclusions" : [],
            "spatialDimensions" : []
          }
        }
      },
      "metricsSpec" : [
        {
          "type" : "count",
          "name" : "count"
        },
        {
          "type" : "doubleSum",
          "name" : "added",
          "fieldName" : "added"
        },
        {
          "type" : "doubleSum",
          "name" : "deleted",
          "fieldName" : "deleted"
        },
        {
          "type" : "doubleSum",
          "name" : "delta",
          "fieldName" : "delta"
        }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "DAY",
        "queryGranularity" : "NONE",
        "intervals" : [ "2013-08-31/2013-09-01" ]
      }
    },
    "ioConfig" : {
      "type" : "index",
      "firehose" : {
        "type" : "local",
        "baseDir" : "examples/indexing/",
        "filter" : "wikipedia_data.json"
       }
    },
    "tuningConfig" : {
      "type" : "index",
      "targetPartitionSize" : 5000000,
      "maxRowsInMemory" : 1000000
    }
  }
}

----------------------------------------

TITLE: Starting Druid Services in Bash
DESCRIPTION: Command to start all Druid services using the supervise script with a specified configuration file.

LANGUAGE: bash
CODE:
bin/supervise -c quickstart/tutorial/conf/tutorial-cluster.conf

----------------------------------------

TITLE: Package License Comments
DESCRIPTION: Collection of license and copyright notice comments for various JavaScript packages and dependencies. All packages are licensed under MIT with their respective copyright holders including Facebook, Sindre Sorhus, Rico Sta. Cruz, Jed Watson, and Lea Verou.

LANGUAGE: javascript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

/*!
	Copyright (c) 2018 Jed Watson.
	Licensed under the MIT License (MIT), see
	http://jedwatson.github.io/classnames
*/

/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Defining FragmentSearchQuerySpec in Druid JSON
DESCRIPTION: Specifies a fragment search query. A match occurs if a dimension value contains all specified fragments, with optional case sensitivity.

LANGUAGE: json
CODE:
{ 
  "type" : "fragment",
  "case_sensitive" : false,
  "values" : ["fragment1", "fragment2"]
}

----------------------------------------

TITLE: Running Command Line Hadoop Indexer in Bash
DESCRIPTION: Command to execute the Hadoop Indexer from the command line. It sets Java options for memory, timezone, and encoding, then specifies the classpath and main class to run.

LANGUAGE: bash
CODE:
java -Xmx256m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -classpath lib/*:<hadoop_config_dir> org.apache.druid.cli.Main index hadoop <spec_file>

----------------------------------------

TITLE: Configuring StatsD Mapping in JSON
DESCRIPTION: JSON configuration example showing how to map Druid metrics to StatsD format, including dimension specifications and metric type definitions.

LANGUAGE: json
CODE:
{
  "query/time": { 
    "dimensions": ["dataSource", "type"], 
    "type": "timer"
  },
  "coordinator-segment/count": { 
    "dimensions": ["dataSource"], 
    "type": "gauge" 
  },
  "historical-segment/count": { 
    "dimensions": ["dataSource", "tier", "priority"], 
    "type": "gauge" 
  }
}

----------------------------------------

TITLE: Configuring FloatMax Aggregator in Druid JSON
DESCRIPTION: The floatMax aggregator computes the maximum of all metric values and Float.NEGATIVE_INFINITY. It requires specifying an output name and the name of the metric column.

LANGUAGE: json
CODE:
{ "type" : "floatMax", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: License Declarations for JavaScript Dependencies
DESCRIPTION: Collection of MIT license declarations and copyright notices for various JavaScript libraries used in the project. Includes React core, React DOM, React-is, Scheduler, Prism syntax highlighter, mark.js, NProgress, and object-assign.

LANGUAGE: JavaScript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

/*!***************************************************
* mark.js v8.11.1
* https://markjs.io/
* Copyright (c) 20142018, Julian Khnel
* Released under the MIT license https://git.io/vwTVl
*****************************************************/

/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Loading DataSketches Extension in Druid Configuration
DESCRIPTION: Configuration required to load the DataSketches extension in Druid config file.

LANGUAGE: plaintext
CODE:
druid.extensions.loadList=["druid-datasketches"]

----------------------------------------

TITLE: Configuring StaticS3Firehose in Apache Druid
DESCRIPTION: JSON configuration for the StaticS3Firehose, which ingests events from a predefined list of S3 objects. This firehose is splittable and can be used by native parallel index tasks.

LANGUAGE: json
CODE:
"firehose" : {
    "type" : "static-s3",
    "uris": ["s3://foo/bar/file.gz", "s3://bar/foo/file2.gz"]
}

----------------------------------------

TITLE: GET /druid/coordinator/v1/metadata/datasources Example
DESCRIPTION: HTTP GET endpoint to retrieve a list of enabled datasource names from the cluster metadata store.

LANGUAGE: HTTP
CODE:
GET /druid/coordinator/v1/metadata/datasources

----------------------------------------

TITLE: POST /druid/coordinator/v1/rules Example
DESCRIPTION: HTTP POST endpoint to update rules for a datasource, accepting a JSON body containing the rules and optional audit headers.

LANGUAGE: HTTP
CODE:
POST /druid/coordinator/v1/rules/{dataSourceName}
X-Druid-Author: author
X-Druid-Comment: comment

----------------------------------------

TITLE: Running ResetCluster Tool with All Options in Java for Apache Druid
DESCRIPTION: This command shows how to run the ResetCluster tool using Java with the --all option, which resets all components of the Druid cluster.

LANGUAGE: bash
CODE:
java org.apache.druid.cli.Main tools reset-cluster --all

----------------------------------------

TITLE: Running ResetCluster Tool with All Options
DESCRIPTION: Command to run the ResetCluster tool with the --all flag to clear all cluster state at once.

LANGUAGE: bash
CODE:
java org.apache.druid.cli.Main tools reset-cluster --all

----------------------------------------

TITLE: Declaring MIT License for React use-sync-external-store-shim
DESCRIPTION: Copyright notice and MIT license declaration for the React use-sync-external-store-shim production build.

LANGUAGE: JavaScript
CODE:
/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Druid Ingestion Spec with Transforms
DESCRIPTION: Ingestion specification that defines data transformation rules including expression transforms for animal names and number multiplication, plus filtering conditions.

LANGUAGE: json
CODE:
{
  "type" : "index",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "transform-tutorial",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "dimensionsSpec" : {
            "dimensions" : [
              "animal",
              { "name": "location", "type": "long" }
            ]
          },
          "timestampSpec": {
            "column": "timestamp",
            "format": "iso"
          }
        }
      },
      "metricsSpec" : [
        { "type" : "count", "name" : "count" },
        { "type" : "longSum", "name" : "number", "fieldName" : "number" },
        { "type" : "longSum", "name" : "triple-number", "fieldName" : "triple-number" }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "week",
        "queryGranularity" : "minute",
        "intervals" : ["2018-01-01/2018-01-03"],
        "rollup" : true
      },
      "transformSpec": {
        "transforms": [
          {
            "type": "expression",
            "name": "animal",
            "expression": "concat('super-', animal)"
          },
          {
            "type": "expression",
            "name": "triple-number",
            "expression": "number * 3"
          }
        ],
        "filter": {
          "type":"or",
          "fields": [
            { "type": "selector", "dimension": "animal", "value": "super-mongoose" },
            { "type": "selector", "dimension": "triple-number", "value": "300" },
            { "type": "selector", "dimension": "location", "value": "3" }
          ]
        }
      }
    },
    "ioConfig" : {
      "type" : "index",
      "firehose" : {
        "type" : "local",
        "baseDir" : "quickstart/tutorial",
        "filter" : "transform-data.json"
      },
      "appendToExisting" : false
    },
    "tuningConfig" : {
      "type" : "index",
      "targetPartitionSize" : 5000000,
      "maxRowsInMemory" : 25000,
      "forceExtendableShardSpecs" : true
    }
  }
}

----------------------------------------

TITLE: Histogram Post Aggregator Configuration
DESCRIPTION: JSON configuration for the histogram post aggregator that returns an approximation of the histogram based on split points.

LANGUAGE: json
CODE:
{
  "type"  : "quantilesDoublesSketchToHistogram",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch>,
  "splitPoints" : <array of split points>
}

----------------------------------------

TITLE: Launching Druid Single Server Deployments
DESCRIPTION: Command line instructions for starting Druid in different single-server configurations. Each configuration is sized for specific hardware requirements, from micro-quickstart (4 CPU, 16GB RAM) to xlarge (64 CPU, 512GB RAM).

LANGUAGE: bash
CODE:
bin/start-micro-quickstart

LANGUAGE: bash
CODE:
bin/start-small

LANGUAGE: bash
CODE:
bin/start-medium

LANGUAGE: bash
CODE:
bin/start-large

LANGUAGE: bash
CODE:
bin/start-xlarge

----------------------------------------

TITLE: GroupBy Query with Filtered DimensionSpec on Multi-value Dimensions in Druid
DESCRIPTION: This JSON snippet shows a groupBy query using a filtered dimensionSpec to precisely control which values are included in the result after dimension explosion. This approach is more efficient than using a having spec.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "test",
  "intervals": [
    "1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"
  ],
  "filter": {
    "type": "selector",
    "dimension": "tags",
    "value": "t3"
  },
  "granularity": {
    "type": "all"
  },
  "dimensions": [
    {
      "type": "listFiltered",
      "delegate": {
        "type": "default",
        "dimension": "tags",
        "outputName": "tags"
      },
      "values": ["t3"]
    }
  ],
  "aggregations": [
    {
      "type": "count",
      "name": "count"
    }
  ]
}

----------------------------------------

TITLE: Configuring HTTP Compression Properties in Apache Druid
DESCRIPTION: This configuration table specifies the properties for controlling HTTP compression in Apache Druid. It includes settings for compression level and inflate buffer size.

LANGUAGE: markdown
CODE:
|Property|Description|Default|
|--------|-----------|-------|
|`druid.server.http.compressionLevel`|The compression level. Value should be between [-1,9], -1 for default level, 0 for no compression.|-1 (default compression level)|
|`druid.server.http.inflateBufferSize`|The buffer size used by gzip decoder. Set to 0 to disable request decompression.|4096|

----------------------------------------

TITLE: License Information for mark.js
DESCRIPTION: Copyright notice and MIT license information for mark.js library by Julian Khnel.

LANGUAGE: JavaScript
CODE:
/*!***************************************************
* mark.js v8.11.1
* https://markjs.io/
* Copyright (c) 20142018, Julian Khnel
* Released under the MIT license https://git.io/vwTVl
****************************************************/

----------------------------------------

TITLE: Protobuf Message Definition
DESCRIPTION: Protocol Buffer message definition for the metrics data structure using proto3 syntax.

LANGUAGE: protobuf
CODE:
syntax = "proto3";
message Metrics {
  string unit = 1;
  string http_method = 2;
  int32 value = 3;
  string timestamp = 4;
  string http_code = 5;
  string page = 6;
  string metricType = 7;
  string server = 8;
}

----------------------------------------

TITLE: Configuring Period Drop Rule in Apache Druid
DESCRIPTION: This JSON snippet defines a period drop rule, which specifies that segments within a rolling time period should be dropped from the cluster.

LANGUAGE: json
CODE:
{
  "type" : "dropByPeriod",
  "period" : "P1M",
  "includeFuture" : true
}

----------------------------------------

TITLE: Prism Syntax Highlighter License
DESCRIPTION: Specifies the MIT license for Prism, a lightweight syntax highlighting library by Lea Verou.

LANGUAGE: JavaScript
CODE:
/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

----------------------------------------

TITLE: Declaring MIT License for React Scheduler
DESCRIPTION: Copyright notice and MIT license declaration for the React Scheduler production build.

LANGUAGE: JavaScript
CODE:
/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Protobuf Message Definition
DESCRIPTION: Protocol Buffer message definition for the metrics data structure using proto3 syntax.

LANGUAGE: protobuf
CODE:
syntax = "proto3";
message Metrics {
  string unit = 1;
  string http_method = 2;
  int32 value = 3;
  string timestamp = 4;
  string http_code = 5;
  string page = 6;
  string metricType = 7;
  string server = 8;
}

----------------------------------------

TITLE: Configuring Kerberos Authenticator in Druid
DESCRIPTION: Basic configuration to set up a Kerberos authenticator in the authentication chain. Specifies the authenticator type as 'kerberos'.

LANGUAGE: properties
CODE:
druid.auth.authenticatorChain=["MyKerberosAuthenticator"]

druid.auth.authenticator.MyKerberosAuthenticator.type=kerberos

----------------------------------------

TITLE: Creating Cassandra Storage Schema for Druid
DESCRIPTION: SQL statements to create the required Cassandra tables for Druid deep storage. Creates index_storage table for segment data and descriptor_storage table for segment metadata.

LANGUAGE: sql
CODE:
CREATE TABLE index_storage(key text,
                           chunk text,
                           value blob,
                           PRIMARY KEY (key, chunk)) WITH COMPACT STORAGE;

CREATE TABLE descriptor_storage(key varchar,
                                lastModified timestamp,
                                descriptor varchar,
                                PRIMARY KEY (key)) WITH COMPACT STORAGE;

----------------------------------------

TITLE: Configuring Inverted TopNMetricSpec in Druid JSON
DESCRIPTION: Demonstrates the configuration of an Inverted TopNMetricSpec, which inverts the order of a delegate metric spec for ascending order sorting.

LANGUAGE: json
CODE:
"metric": {
    "type": "inverted",
    "metric": <delegate_top_n_metric_spec>
}

----------------------------------------

TITLE: Configuring Kerberos Authenticator Properties in Apache Druid
DESCRIPTION: Example of setting Kerberos authenticator properties in Druid. This includes specifying the server principal, keytab location, and other optional settings like excluded paths and cookie signature secret.

LANGUAGE: properties
CODE:
druid.auth.authenticator.kerberos.serverPrincipal=HTTP/_HOST@EXAMPLE.COM
druid.auth.authenticator.kerberos.serverKeytab=/etc/security/keytabs/spnego.service.keytab
druid.auth.authenticator.kerberos.authToLocal=RULE:[1:$1@$0](druid@EXAMPLE.COM)s/.*/druid DEFAULT
druid.auth.authenticator.kerberos.excludedPaths=['/status','/health']
druid.auth.authenticator.kerberos.cookieSignatureSecret=secretString
druid.auth.authenticator.kerberos.authorizerName=MyAuthorizer

----------------------------------------

TITLE: Configuring Client Certificate Authentication in Apache Druid
DESCRIPTION: These properties control client certificate authentication settings in Apache Druid. They include options for requiring client certificates, configuring the trust store, and validating hostnames.

LANGUAGE: properties
CODE:
druid.server.https.requireClientCertificate=false
druid.server.https.trustStoreType=java.security.KeyStore.getDefaultType()
druid.server.https.trustStorePath=none
druid.server.https.trustStoreAlgorithm=javax.net.ssl.TrustManagerFactory.getDefaultAlgorithm()
druid.server.https.trustStorePassword=none
druid.server.https.validateHostnames=true
druid.server.https.crlPath=null

----------------------------------------

TITLE: Configuring Interval Drop Rule in Apache Druid
DESCRIPTION: This JSON snippet defines an interval drop rule in Druid. It specifies that segments within the given interval should be dropped from the cluster.

LANGUAGE: json
CODE:
{
  "type" : "dropByInterval",
  "interval" : "2012-01-01/2013-01-01"
}

----------------------------------------

TITLE: License Headers Collection - JavaScript Dependencies
DESCRIPTION: A compilation of MIT license headers from various JavaScript libraries used in the Druid project. Each header specifies the component name, copyright holder, and license information.

LANGUAGE: javascript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

/*!***************************************************
* mark.js v8.11.1
* https://markjs.io/
* Copyright (c) 20142018, Julian Khnel
* Released under the MIT license https://git.io/vwTVl
*****************************************************/

/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Documenting MIT License for React-is
DESCRIPTION: This code snippet provides license information for the React-is library, which is under the MIT license and created by Facebook, Inc. and its affiliates.

LANGUAGE: JavaScript
CODE:
/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Generating Explain Plan for SQL Query in Druid
DESCRIPTION: This snippet demonstrates how to generate an explain plan for a SQL query in Druid. It shows how to use the EXPLAIN PLAN FOR clause to see the native Druid query that a SQL query will be translated into.

LANGUAGE: sql
CODE:
EXPLAIN PLAN FOR SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;

----------------------------------------

TITLE: Starting Druid Real-time Server
DESCRIPTION: Command to start the Apache Druid real-time processing server using the Main class.

LANGUAGE: java
CODE:
org.apache.druid.cli.Main server realtime

----------------------------------------

TITLE: Configuring Fragment Search Query in Druid
DESCRIPTION: Defines a search query that matches if a dimension value contains all specified fragments. Case sensitivity can be configured.

LANGUAGE: json
CODE:
{ 
  "type" : "fragment",
  "case_sensitive" : false,
  "values" : ["fragment1", "fragment2"]
}

----------------------------------------

TITLE: Example Results of Scan Query in List Format
DESCRIPTION: This JSON snippet shows the structure of results returned by a Scan query when the resultFormat is set to 'list'. It includes segment information, column names, and event data.

LANGUAGE: json
CODE:
[{
    "segmentId" : "wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9",
    "columns" : [
      "timestamp",
      "robot",
      "namespace",
      "anonymous",
      "unpatrolled",
      "page",
      "language",
      "newpage",
      "user",
      "count",
      "added",
      "delta",
      "variation",
      "deleted"
    ],
    "events" : [ {
        "timestamp" : "2013-01-01T00:00:00.000Z",
        "robot" : "1",
        "namespace" : "article",
        "anonymous" : "0",
        "unpatrolled" : "0",
        "page" : "11._korpus_(NOVJ)",
        "language" : "sl",
        "newpage" : "0",
        "user" : "EmausBot",
        "count" : 1.0,
        "added" : 39.0,
        "delta" : 39.0,
        "variation" : 39.0,
        "deleted" : 0.0
    }, {
        "timestamp" : "2013-01-01T00:00:00.000Z",
        "robot" : "0",
        "namespace" : "article",
        "anonymous" : "0",
        "unpatrolled" : "0",
        "page" : "112_U.S._580",
        "language" : "en",
        "newpage" : "1",
        "user" : "MZMcBride",
        "count" : 1.0,
        "added" : 70.0,
        "delta" : 70.0,
        "variation" : 70.0,
        "deleted" : 0.0
    }, {
        "timestamp" : "2013-01-01T00:00:00.000Z",
        "robot" : "0",
        "namespace" : "article",
        "anonymous" : "0",
        "unpatrolled" : "0",
        "page" : "113_U.S._243",
        "language" : "en",
        "newpage" : "1",
        "user" : "MZMcBride",
        "count" : 1.0,
        "added" : 77.0,
        "delta" : 77.0,
        "variation" : 77.0,
        "deleted" : 0.0
    }, {
        "timestamp" : "2013-01-01T00:00:00.000Z",
        "robot" : "0",
        "namespace" : "article",
        "anonymous" : "0",
        "unpatrolled" : "0",
        "page" : "113_U.S._73",
        "language" : "en",
        "newpage" : "1",
        "user" : "MZMcBride",
        "count" : 1.0,
        "added" : 70.0,
        "delta" : 70.0,
        "variation" : 70.0,
        "deleted" : 0.0
    }, {
        "timestamp" : "2013-01-01T00:00:00.000Z",
        "robot" : "0",
        "namespace" : "article",
        "anonymous" : "0",
        "unpatrolled" : "0",
        "page" : "113_U.S._756",
        "language" : "en",
        "newpage" : "1",
        "user" : "MZMcBride",
        "count" : 1.0,
        "added" : 68.0,
        "delta" : 68.0,
        "variation" : 68.0,
        "deleted" : 0.0
    } ]
} ]

----------------------------------------

TITLE: Configuring Interval Load Rule in Apache Druid
DESCRIPTION: This JSON snippet defines an interval load rule in Druid. It specifies how many replicas of a segment should exist in different server tiers for a specific time interval.

LANGUAGE: json
CODE:
{
  "type" : "loadByInterval",
  "interval": "2012-01-01/2013-01-01",
  "tieredReplicants": {
    "hot": 1,
    "_default_tier" : 1
  }
}

----------------------------------------

TITLE: License Notice for React Scheduler
DESCRIPTION: This snippet contains the license information for the scheduler production build of React, which is licensed under the MIT license.

LANGUAGE: JavaScript
CODE:
/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Documenting MIT License for NProgress
DESCRIPTION: This code snippet provides license information for the NProgress library, which is under the MIT license and created by Rico Sta. Cruz.

LANGUAGE: JavaScript
CODE:
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

----------------------------------------

TITLE: Building Hadoop Docker Image
DESCRIPTION: Commands to build a Docker image for Hadoop 2.8.3 cluster from the provided Dockerfile.

LANGUAGE: bash
CODE:
cd quickstart/tutorial/hadoop/docker
docker build -t druid-hadoop-demo:2.8.3 .

----------------------------------------

TITLE: Histogram Post-Aggregator Configuration
DESCRIPTION: Post-aggregator configuration for generating histogram data from a DoublesSketch using specified split points.

LANGUAGE: json
CODE:
{
  "type"  : "quantilesDoublesSketchToHistogram",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch>,
  "splitPoints" : <array of split points>
}

----------------------------------------

TITLE: Applying Query Filter in Apache Druid groupBy Query (JSON)
DESCRIPTION: This snippet demonstrates how to use a query filter HavingSpec in a groupBy query. It allows all Druid query filters to be used in the Having part of the query.

LANGUAGE: JSON
CODE:
{
    "queryType": "groupBy",
    "dataSource": "sample_datasource",
    ...
    "having": 
        {
            "type" : "filter",
            "filter" : <any Druid query filter>
        }
}

----------------------------------------

TITLE: Configuring Lookups in Druid JSON
DESCRIPTION: Example JSON configuration for defining multiple lookups across different tiers in Druid. Demonstrates map-based and JDBC-based lookup types.

LANGUAGE: json
CODE:
{
  "__default": {
    "country_code": {
      "version": "v0",
      "lookupExtractorFactory": {
        "type": "map",
        "map": {
          "77483": "United States"
        }
      }
    },
    "site_id": {
      "version": "v0",
      "lookupExtractorFactory": {
        "type": "cachedNamespace",
        "extractionNamespace": {
          "type": "jdbc",
          "connectorConfig": {
            "createTables": true,
            "connectURI": "jdbc:mysql:\/\/localhost:3306\/druid",
            "user": "druid",
            "password": "diurd"
          },
          "table": "lookupTable",
          "keyColumn": "country_id",
          "valueColumn": "country_name",
          "tsColumn": "timeColumn"
        },
        "firstCacheTimeout": 120000,
        "injective": true
      }
    },
    "site_id_customer1": {
      "version": "v0",
      "lookupExtractorFactory": {
        "type": "map",
        "map": {
          "847632": "Internal Use Only"
        }
      }
    },
    "site_id_customer2": {
      "version": "v0",
      "lookupExtractorFactory": {
        "type": "map",
        "map": {
          "AHF77": "Home"
        }
      }
    }
  },
  "realtime_customer1": {
    "country_code": {
      "version": "v0",
      "lookupExtractorFactory": {
        "type": "map",
        "map": {
          "77483": "United States"
        }
      }
    },
    "site_id_customer1": {
      "version": "v0",
      "lookupExtractorFactory": {
        "type": "map",
        "map": {
          "847632": "Internal Use Only"
        }
      }
    }
  },
  "realtime_customer2": {
    "country_code": {
      "version": "v0",
      "lookupExtractorFactory": {
        "type": "map",
        "map": {
          "77483": "United States"
        }
      }
    },
    "site_id_customer2": {
      "version": "v0",
      "lookupExtractorFactory": {
        "type": "map",
        "map": {
          "AHF77": "Home"
        }
      }
    }
  }
}

----------------------------------------

TITLE: Configuring Druid for SQLServer Metadata Storage
DESCRIPTION: This snippet shows the necessary properties to configure Druid to use Microsoft SQLServer as its metadata storage. It includes settings for the storage type, connection URI, username, and password.

LANGUAGE: properties
CODE:
druid.metadata.storage.type=sqlserver
druid.metadata.storage.connector.connectURI=jdbc:sqlserver://<host>;databaseName=druid
druid.metadata.storage.connector.user=druid
druid.metadata.storage.connector.password=diurd

----------------------------------------

TITLE: Configuring Interval Load Rule in Apache Druid
DESCRIPTION: This JSON snippet defines an interval load rule in Apache Druid. It specifies how many replicas of a segment should exist in different server tiers for a specific time interval.

LANGUAGE: json
CODE:
{
  "type" : "loadByInterval",
  "interval": "2012-01-01/2013-01-01",
  "tieredReplicants": {
    "hot": 1,
    "_default_tier" : 1
  }
}

----------------------------------------

TITLE: React Scheduler License
DESCRIPTION: Copyright notice and MIT license information for the scheduler.production.min.js file from React.

LANGUAGE: JavaScript
CODE:
/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Generating Bezier Curve Functions in JavaScript
DESCRIPTION: A utility for generating Bezier curve functions. This generator can be used to create custom easing functions for animations or other mathematical applications requiring Bezier curves.

LANGUAGE: JavaScript
CODE:
/*! Bezier curve function generator. Copyright Gaetan Renaudeau. MIT License: http://en.wikipedia.org/wiki/MIT_License */

----------------------------------------

TITLE: Executing SQL Scan Query in Druid
DESCRIPTION: This snippet shows a SQL Scan query that retrieves raw data rows. It demonstrates how to select specific columns and apply time filters in Druid SQL.

LANGUAGE: sql
CODE:
SELECT user, page FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 02:00:00' AND TIMESTAMP '2015-09-12 03:00:00' LIMIT 5;

----------------------------------------

TITLE: Configuring Log4j2 XML for Druid
DESCRIPTION: Example log4j2.xml configuration file that defines console logging with timestamp pattern and configurable log levels. Includes commented section for enabling HTTP request logging.

LANGUAGE: xml
CODE:
<?xml version="1.0" encoding="UTF-8" ?>
<Configuration status="WARN">
  <Appenders>
    <Console name="Console" target="SYSTEM_OUT">
      <PatternLayout pattern="%d{ISO8601} %p [%t] %c - %m%n"/>
    </Console>
  </Appenders>
  <Loggers>
    <Root level="info">
      <AppenderRef ref="Console"/>
    </Root>

    <!-- Uncomment to enable logging of all HTTP requests
    <Logger name="org.apache.druid.jetty.RequestLog" additivity="false" level="DEBUG">
        <AppenderRef ref="Console"/>
    </Logger>
    -->
  </Loggers>
</Configuration>

----------------------------------------

TITLE: Querying for Ingested Event Count in Druid
DESCRIPTION: Example of how to query for the number of ingested events using a longSum aggregator in Druid.

LANGUAGE: json
CODE:
"aggregations": [
    { "type": "longSum", "name": "numIngestedEvents", "fieldName": "count" }
]

----------------------------------------

TITLE: Setting ZooKeeper Path for Served Segments in Apache Druid
DESCRIPTION: Specifies the ZooKeeper path where Historical and Realtime processes create permanent znodes to indicate the segments they are serving.

LANGUAGE: plaintext
CODE:
${druid.zk.paths.servedSegmentsPath}/${druid.host}

----------------------------------------

TITLE: React Production License
DESCRIPTION: Copyright notice and MIT license information for the react.production.min.js file from React.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Example Second Query for Exact Aggregates in Apache Druid
DESCRIPTION: This snippet shows the second query in a two-query approach to get exact aggregates for high-cardinality dimensions. It uses the results from the first query to filter and retrieve exact aggregates.

LANGUAGE: json
CODE:
{
    "aggregations": [
             {
                 "fieldName": "L_TAX_doubleSum",
                 "name": "L_TAX_",
                 "type": "doubleSum"
             },
             {
                 "fieldName": "L_DISCOUNT_doubleSum",
                 "name": "L_DISCOUNT_",
                 "type": "doubleSum"
             },
             {
                 "fieldName": "L_EXTENDEDPRICE_doubleSum",
                 "name": "L_EXTENDEDPRICE_",
                 "type": "doubleSum"
             },
             {
                 "fieldName": "L_QUANTITY_longSum",
                 "name": "L_QUANTITY_",
                 "type": "longSum"
             },
             {
                 "name": "count",
                 "type": "count"
             }
    ],
    "dataSource": "tpch_year",
    "dimension":"l_orderkey",
    "filter": {
        "fields": [
            {
                "dimension": "l_orderkey",
                "type": "selector",
                "value": "103136"
            },
            {
                "dimension": "l_orderkey",
                "type": "selector",
                "value": "1648672"
            }
        ],
        "type": "or"
    },
    "granularity": "all",
    "intervals": [
        "1900-01-09T00:00:00.000Z/2992-01-10T00:00:00.000Z"
    ],
    "metric": "L_QUANTITY_",
    "queryType": "topN",
    "threshold": 2
}

----------------------------------------

TITLE: js-yaml Library Reference in JavaScript
DESCRIPTION: This comment indicates the use of js-yaml library version 4.1.0. It includes a link to the GitHub repository and mentions that it's released under the MIT license. This library is commonly used for parsing and stringifying YAML data in JavaScript applications.

LANGUAGE: JavaScript
CODE:
/*! js-yaml 4.1.0 https://github.com/nodeca/js-yaml @license MIT */

----------------------------------------

TITLE: Downloading and Extracting Zookeeper in Bash
DESCRIPTION: Commands to download Apache Zookeeper, extract it, and rename the directory for use with Druid.

LANGUAGE: bash
CODE:
curl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz
tar -xzf zookeeper-3.4.11.tar.gz
mv zookeeper-3.4.11 zk

----------------------------------------

TITLE: ArrayOfDoublesSketch Estimate with Bounds Post-Aggregator
DESCRIPTION: JSON configuration for post-aggregator that returns distinct count estimate with error bounds from ArrayOfDoublesSketch.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchToEstimateAndBounds",
  "name": <output name>,
  "field"  : <post aggregator that refers to an  ArrayOfDoublesSketch (fieldAccess or another post aggregator)>,
  "numStdDevs": <number from 1 to 3>
}

----------------------------------------

TITLE: Indexing with Timestamp Min Aggregator in Apache Druid
DESCRIPTION: JSON configuration for including a timeMin aggregator during data ingestion. This aggregator calculates the minimum timestamp for the specified field.

LANGUAGE: json
CODE:
{
    "type": "timeMin",
    "name": "tmin",
    "fieldName": "<field_name, typically column specified in timestamp spec>"
}

----------------------------------------

TITLE: Configuring Google Cloud Storage as Deep Storage in Apache Druid
DESCRIPTION: This YAML configuration snippet demonstrates how to set up Google Cloud Storage as deep storage in Apache Druid using the HDFS extension. It specifies the storage type as HDFS and provides a GCS bucket path for the storage directory.

LANGUAGE: yaml
CODE:
druid.storage.type: hdfs
druid.storage.storageDirectory: gs://bucket/example/directory

----------------------------------------

TITLE: Configuring Default DimensionSpec in Druid
DESCRIPTION: Defines how dimension values are transformed prior to aggregation. This spec returns dimension values as-is and optionally renames the dimension.

LANGUAGE: json
CODE:
{
  "type" : "default",
  "dimension" : <dimension>,
  "outputName": <output_name>,
  "outputType": <"STRING"|"LONG"|"FLOAT">
}

----------------------------------------

TITLE: Mark.js License Notice
DESCRIPTION: Copyright notice and MIT license information for the mark.js library by Julian Khnel.

LANGUAGE: JavaScript
CODE:
/*!***************************************************
* mark.js v8.11.1
* https://markjs.io/
* Copyright (c) 20142018, Julian Khnel
* Released under the MIT license https://git.io/vwTVl
****************************************************/

----------------------------------------

TITLE: Executing Druid Query via HTTP POST
DESCRIPTION: Example of sending a native query to Druid using curl. The query is sent as a JSON file to the queryable host with appropriate content type headers.

LANGUAGE: bash
CODE:
curl -X POST '<queryable_host>:<port>/druid/v2/?pretty' -H 'Content-Type:application/json' -H 'Accept:application/json' -d @<query_json_file>

----------------------------------------

TITLE: Initializing Document Load Check in JavaScript
DESCRIPTION: This comment indicates that the script should wait for the document to be fully loaded before starting execution. It's a common practice to ensure all DOM elements are available before manipulating them.

LANGUAGE: JavaScript
CODE:
/*!\n   * Wait for document loaded before starting the execution\n   */

----------------------------------------

TITLE: Executing SQL Scan Query in Druid
DESCRIPTION: This snippet shows a SQL Scan query that retrieves raw data rows. It demonstrates how to select specific columns and use time filters in Druid SQL.

LANGUAGE: sql
CODE:
SELECT user, page FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 02:00:00' AND TIMESTAMP '2015-09-12 03:00:00' LIMIT 5;

----------------------------------------

TITLE: Executing pull-deps Tool with Java in Apache Druid
DESCRIPTION: This command demonstrates how to run the pull-deps tool to download specific Druid extensions and Hadoop dependencies. It uses the --clean option to remove existing dependencies before downloading new ones.

LANGUAGE: bash
CODE:
java -classpath "/my/druid/lib/*" org.apache.druid.cli.Main tools pull-deps --clean -c org.apache.druid.extensions:mysql-metadata-storage:0.13.0-incubating -c org.apache.druid.extensions.contrib:druid-rabbitmq:0.13.0-incubating -h org.apache.hadoop:hadoop-client:2.3.0 -h org.apache.hadoop:hadoop-client:2.4.0

----------------------------------------

TITLE: Configuring Standalone JVM Parquet Ingestion in Druid
DESCRIPTION: Complete JSON configuration for Parquet ingestion in standalone JVM mode, including metadata update specifications and additional required fields.

LANGUAGE: json
CODE:
{
  "type": "index_hadoop",
  "spec": {
    "ioConfig": {
      "type": "hadoop",
      "inputSpec": {
        "type": "static",
        "inputFormat": "org.apache.druid.data.input.parquet.DruidParquetInputFormat",
        "paths": "no_metrics"
      },
      "metadataUpdateSpec": {
        "type": "postgresql",
        "connectURI": "jdbc:postgresql://localhost/druid",
        "user" : "druid",
        "password" : "asdf",
        "segmentTable": "druid_segments"
      },
      "segmentOutputPath": "tmp/segments"
    },
    "dataSchema": {
      "dataSource": "no_metrics",
      "parser": {
        "type": "parquet",
        "parseSpec": {
          "format": "timeAndDims",
          "timestampSpec": {
            "column": "time",
            "format": "auto"
          },
          "dimensionsSpec": {
            "dimensions": [
              "name"
            ],
            "dimensionExclusions": [],
            "spatialDimensions": []
          }
        }
      },
      "metricsSpec": [{
        "type": "count",
        "name": "count"
      }],
      "granularitySpec": {
        "type": "uniform",
        "segmentGranularity": "DAY",
        "queryGranularity": "ALL",
        "intervals": ["2015-12-31/2016-01-02"]
      }
    },
    "tuningConfig": {
      "type": "hadoop",
      "workingPath": "tmp/working_path",
      "partitionsSpec": {
        "targetPartitionSize": 5000000
      },
      "jobProperties" : {},
      "leaveIntermediate": true
    }
  }
}

----------------------------------------

TITLE: Configuring Strlen Extraction Function in Apache Druid
DESCRIPTION: JSON configuration for a Strlen Extraction Function, which returns the length of dimension values in Unicode code units.

LANGUAGE: JSON
CODE:
{ "type" : "strlen" }

----------------------------------------

TITLE: Executing SQL Scan Query in Druid
DESCRIPTION: This snippet shows a SQL Scan query that retrieves raw data rows. It demonstrates how to select specific columns and use time filters in Druid SQL.

LANGUAGE: sql
CODE:
SELECT user, page FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 02:00:00' AND TIMESTAMP '2015-09-12 03:00:00' LIMIT 5;

----------------------------------------

TITLE: License Information for React Scheduler
DESCRIPTION: Copyright notice and MIT license information for React's scheduler production build.

LANGUAGE: JavaScript
CODE:
/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Making Smile-encoded Query Request in Druid
DESCRIPTION: Demonstrates how to make a query request using Jackson Smile binary JSON format for potentially better performance.

LANGUAGE: bash
CODE:
curl -X POST '<queryable_host>:<port>/druid/v2/?pretty' -H 'Content-Type:application/json' -H 'Accept:x-jackson-smile' -d @<query_json_file>

----------------------------------------

TITLE: Configuring Kafka Lookup in Druid
DESCRIPTION: JSON configuration for setting up a Kafka lookup extractor factory. Specifies the kafka topic and required connection properties for the lookup system.

LANGUAGE: json
CODE:
{
  "type":"kafka",
  "kafkaTopic":"testTopic",
  "kafkaProperties":{"zookeeper.connect":"somehost:2181/kafka"}
}

----------------------------------------

TITLE: Configuring Parquet Parser with Parquet ParseSpec in Druid
DESCRIPTION: This snippet demonstrates how to configure the Parquet parser with a Parquet parseSpec in a Druid ingestion spec. It includes settings for input format, flattenSpec, timestampSpec, and dimensionsSpec.

LANGUAGE: json
CODE:
{
  "type": "index_hadoop",
  "spec": {
    "ioConfig": {
      "type": "hadoop",
      "inputSpec": {
        "type": "static",
        "inputFormat": "org.apache.druid.data.input.parquet.DruidParquetInputFormat",
        "paths": "path/to/file.parquet"
      },
      ...
    },
    "dataSchema": {
      "dataSource": "example",
      "parser": {
        "type": "parquet",
        "parseSpec": {
          "format": "parquet",
          "flattenSpec": {
            "useFieldDiscovery": true,
            "fields": [
              {
                "type": "path",
                "name": "nestedDim",
                "expr": "$.nestedData.dim1"
              },
              {
                "type": "path",
                "name": "listDimFirstItem",
                "expr": "$.listDim[1]"
              }
            ]
          },
          "timestampSpec": {
            "column": "timestamp",
            "format": "auto"
          },
          "dimensionsSpec": {
            "dimensions": [],
            "dimensionExclusions": [],
            "spatialDimensions": []
          }
        }
      },
      ...
    },
    "tuningConfig": <hadoop-tuning-config>
    }
  }
}

----------------------------------------

TITLE: Processing Check Comment
DESCRIPTION: Comment indicating a check for previous processing status.

LANGUAGE: javascript
CODE:
/*! Check if previously processed */

----------------------------------------

TITLE: Configuring Batch Thrift Ingestion with Hadoop
DESCRIPTION: JSON configuration for batch ingestion of Thrift data using HadoopDruidIndexer. Includes settings for input formats, Thrift class specifications, and job properties for processing Sequence files or LzoThriftBlock files.

LANGUAGE: json
CODE:
{
  "type": "index_hadoop",
  "spec": {
    "dataSchema": {
      "dataSource": "book",
      "parser": {
        "type": "thrift",
        "jarPath": "book.jar",
        "thriftClass": "org.apache.druid.data.input.thrift.Book",
        "protocol": "compact",
        "parseSpec": {
          "format": "json",
          ...
        }
      },
      "metricsSpec": [],
      "granularitySpec": {}
    },
    "ioConfig": {
      "type": "hadoop",
      "inputSpec": {
        "type": "static",
        "inputFormat": "org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat",
        "paths": "/user/to/some/book.seq"
      }
    },
    "tuningConfig": {
      "type": "hadoop",
      "jobProperties": {
        "tmpjars":"/user/h_user_profile/du00/druid/test/book.jar"
      }
    }
  }
}

----------------------------------------

TITLE: Configuring Time Parsing Extraction Function in Druid
DESCRIPTION: Parses dimension values as timestamps using the given input format and returns them formatted using the given output format.

LANGUAGE: json
CODE:
{ "type" : "time",
  "timeFormat" : <input_format>,
  "resultFormat" : <output_format>,
  "joda" : <true, false> }

----------------------------------------

TITLE: MIT License Notice for React Scheduler
DESCRIPTION: Copyright notice for React's scheduler production build, version 0.20.2, licensed under MIT.

LANGUAGE: JavaScript
CODE:
/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: GroupBy Query for Aggregated Results
DESCRIPTION: SQL query demonstrating how multiple rows for the same dimension value are grouped together at query time.

LANGUAGE: sql
CODE:
select __time, animal, SUM("count"), SUM("number") from "updates-tutorial" group by __time, animal;

----------------------------------------

TITLE: Performing Set Operations on ArrayOfDoublesSketch
DESCRIPTION: Post-aggregator configuration to perform set operations (union, intersection, difference) on multiple ArrayOfDoublesSketch instances.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchSetOp",
  "name": <output name>,
  "operation": <"UNION"|"INTERSECT"|"NOT">,
  "fields"  : <array of post aggregators to access sketch aggregators or post aggregators to allow arbitrary combination of set operations>,
  "nominalEntries" : <parameter that determines the accuracy and size of the sketch>,
  "numberOfValues" : <number of values associated with each distinct key>
}

----------------------------------------

TITLE: Registering Jackson Subtypes for Firehose Factory in Java
DESCRIPTION: This code snippet shows how to register a custom FirehoseFactory implementation as a Jackson subtype in a Druid module.

LANGUAGE: Java
CODE:
@Override
public List<? extends Module> getJacksonModules()
{
  return ImmutableList.of(
          new SimpleModule().registerSubtypes(new NamedType(StaticS3FirehoseFactory.class, "static-s3"))
  );
}

----------------------------------------

TITLE: License Declarations - JavaScript Libraries
DESCRIPTION: Collection of MIT license declarations for various JavaScript libraries including React core, React DOM, React-is, Scheduler, mark.js, Prism, NProgress, and object-assign. Each declaration includes copyright information and license terms.

LANGUAGE: JavaScript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

/*!***************************************************
* mark.js v8.11.1
* https://markjs.io/
* Copyright (c) 20142018, Julian Khnel
* Released under the MIT license https://git.io/vwTVl
*****************************************************/

/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Setting up Inverted TopNMetricSpec in Druid JSON
DESCRIPTION: Shows how to configure an Inverted TopNMetricSpec to sort dimension values in inverted order. It requires specifying the type and a delegate metric spec.

LANGUAGE: json
CODE:
"metric": {
    "type": "inverted",
    "metric": <delegate_top_n_metric_spec>
}

----------------------------------------

TITLE: Collection of MIT License Headers
DESCRIPTION: A compilation of license headers from various JavaScript libraries and dependencies used in the Druid project. Each header includes copyright information and MIT license references.

LANGUAGE: javascript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

/*!***************************************************
* mark.js v8.11.1
* https://markjs.io/
* Copyright (c) 20142018, Julian Khnel
* Released under the MIT license https://git.io/vwTVl
*****************************************************/

/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Loading initial data with indexing spec in Apache Druid
DESCRIPTION: This command loads the Wikipedia edits data using an indexing spec that creates hourly segments for a datasource called 'deletion-tutorial'.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/deletion-index.json

----------------------------------------

TITLE: Histogram Post-Aggregator Configuration
DESCRIPTION: JSON configuration for generating histogram data from a DoublesSketch using specified split points.

LANGUAGE: json
CODE:
{
  "type"  : "quantilesDoublesSketchToHistogram",
  "name": <output name>,
  "field"  : <post aggregator that refers to a DoublesSketch>,
  "splitPoints" : <array of split points>
}

----------------------------------------

TITLE: Configuring Period Drop Before Rule in Apache Druid
DESCRIPTION: JSON configuration for a period drop before rule that indicates segments before a specified period should be dropped from the cluster.

LANGUAGE: json
CODE:
{
  "type" : "dropBeforeByPeriod",
  "period" : "P1M"
}

----------------------------------------

TITLE: Creating Temporary Directories
DESCRIPTION: Creates shared directories for file transfer between host and Hadoop container.

LANGUAGE: bash
CODE:
mkdir -p /tmp/shared
mkdir -p /tmp/shared/hadoop_xml

----------------------------------------

TITLE: Configuring Bucket Extraction Function in Druid
DESCRIPTION: Buckets numerical values into ranges of the given size by converting them to the same base value. Non-numeric values are converted to null.

LANGUAGE: json
CODE:
{
  "type" : "bucket",
  "size" : 5,
  "offset" : 2
}

----------------------------------------

TITLE: License Notice for React DOM
DESCRIPTION: This snippet contains the license information for the React DOM production build, which is licensed under the MIT license.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Loading Initial Index Data in Druid
DESCRIPTION: Command to load Wikipedia edits data into Druid using an indexing specification that creates hourly segments.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/deletion-index.json

----------------------------------------

TITLE: License Notice for React use-sync-external-store-shim
DESCRIPTION: This snippet contains the license information for the use-sync-external-store-shim production build of React, which is licensed under the MIT license.

LANGUAGE: JavaScript
CODE:
/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Setting Up DoubleMax Aggregator in Druid
DESCRIPTION: JSON configuration for the doubleMax aggregator in Druid, which computes the maximum of all metric values and Double.NEGATIVE_INFINITY.

LANGUAGE: json
CODE:
{ "type" : "doubleMax", "name" : <output_name>, "fieldName" : <metric_name> }

----------------------------------------

TITLE: Configuring Period Broadcast Rule in Apache Druid
DESCRIPTION: This JSON snippet defines a period broadcast rule in Apache Druid. It specifies how segments of different data sources should be co-located for a specific time period, with an option to include future data.

LANGUAGE: json
CODE:
{
  "type" : "broadcastByPeriod",
  "colocatedDataSources" : [ "target_source1", "target_source2" ],
  "period" : "P1M",
  "includeFuture" : true
}

----------------------------------------

TITLE: Copyright and License Notice for NProgress
DESCRIPTION: This snippet contains the copyright and license information for NProgress, a progress bar library for Ajax'y applications, released under the MIT license.

LANGUAGE: JavaScript
CODE:
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

----------------------------------------

TITLE: Setting Segment Announcement Path in ZooKeeper
DESCRIPTION: ZooKeeper path where Historical and Realtime nodes announce their presence using ephemeral znodes.

LANGUAGE: plaintext
CODE:
${druid.zk.paths.announcementsPath}/${druid.host}

----------------------------------------

TITLE: Classnames License
DESCRIPTION: MIT license notice for classnames library by Jed Watson

LANGUAGE: javascript
CODE:
/*!
	Copyright (c) 2018 Jed Watson.
	Licensed under the MIT License (MIT), see
	http://jedwatson.github.io/classnames
*/

----------------------------------------

TITLE: Extracting Druid Distribution Package
DESCRIPTION: Commands to download and extract the Apache Druid distribution package into a local directory.

LANGUAGE: bash
CODE:
tar -xzf apache-druid-0.13.0-incubating-bin.tar.gz
cd apache-druid-0.13.0-incubating

----------------------------------------

TITLE: Copyright Notice for React
DESCRIPTION: Copyright notice for the React production build (v17.0.2), released under the MIT license by Facebook, Inc. and its affiliates.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Disabling segments by interval in Apache Druid
DESCRIPTION: This curl command sends a POST request to the Druid Coordinator API to mark segments within a specified time interval as 'unused'. It disables segments between 18:00 and 20:00 on 2015-09-12.

LANGUAGE: bash
CODE:
curl -X 'POST' -H 'Content-Type:application/json' -d '{ "interval" : "2015-09-12T18:00:00.000Z/2015-09-12T20:00:00.000Z" }' http://localhost:8081/druid/coordinator/v1/datasources/deletion-tutorial/markUnused

----------------------------------------

TITLE: Copyright and License Notice for React use-sync-external-store-shim
DESCRIPTION: This snippet contains the copyright and license information for the use-sync-external-store-shim React package, which is released under the MIT license.

LANGUAGE: JavaScript
CODE:
/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Setting Segment Announcement Path in ZooKeeper
DESCRIPTION: ZooKeeper path where Historical and Realtime nodes announce their presence using ephemeral znodes.

LANGUAGE: plaintext
CODE:
${druid.zk.paths.announcementsPath}/${druid.host}

----------------------------------------

TITLE: Object Assign License
DESCRIPTION: MIT license notice for object-assign library by Sindre Sorhus

LANGUAGE: javascript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

----------------------------------------

TITLE: Downloading and Extracting Apache ZooKeeper
DESCRIPTION: Commands to download Apache ZooKeeper, extract it, and rename the directory for use with Druid.

LANGUAGE: bash
CODE:
curl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz
tar -xzf zookeeper-3.4.11.tar.gz
mv zookeeper-3.4.11 zk

----------------------------------------

TITLE: Declaring MIT License for React's react-is
DESCRIPTION: This comment block declares the MIT license for React's react-is production build.

LANGUAGE: JavaScript
CODE:
/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Sample Network Flow Event Data in JSON
DESCRIPTION: Example JSON data representing network flow events with timestamp, source IP, destination IP, packet count, and byte count.

LANGUAGE: json
CODE:
[
{"timestamp":"2018-01-01T01:01:35Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":20,"bytes":9024},
{"timestamp":"2018-01-01T01:01:51Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":255,"bytes":21133},
{"timestamp":"2018-01-01T01:01:59Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":11,"bytes":5780},
{"timestamp":"2018-01-01T01:02:14Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":38,"bytes":6289},
{"timestamp":"2018-01-01T01:02:29Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":377,"bytes":359971},
{"timestamp":"2018-01-01T01:03:29Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":49,"bytes":10204},
{"timestamp":"2018-01-02T21:33:14Z","srcIP":"7.7.7.7", "dstIP":"8.8.8.8","packets":38,"bytes":6289},
{"timestamp":"2018-01-02T21:33:45Z","srcIP":"7.7.7.7", "dstIP":"8.8.8.8","packets":123,"bytes":93999},
{"timestamp":"2018-01-02T21:35:45Z","srcIP":"7.7.7.7", "dstIP":"8.8.8.8","packets":12,"bytes":2818}
]

----------------------------------------

TITLE: Declaring MIT License for React
DESCRIPTION: Copyright notice and MIT license declaration for the React production build.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Creating Cassandra Tables for Druid Deep Storage
DESCRIPTION: SQL commands to create the required tables 'index_storage' and 'descriptor_storage' in Cassandra for storing Druid segments and their metadata.

LANGUAGE: sql
CODE:
CREATE TABLE index_storage(key text,
                           chunk text,
                           value blob,
                           PRIMARY KEY (key, chunk)) WITH COMPACT STORAGE;

CREATE TABLE descriptor_storage(key varchar,
                                lastModified timestamp,
                                descriptor varchar,
                                PRIMARY KEY (key)) WITH COMPACT STORAGE;

----------------------------------------

TITLE: Including DOMPurify Library License in JavaScript
DESCRIPTION: This snippet includes the license information for DOMPurify, a library for HTML sanitization. It specifies the version, copyright, and license types (Apache 2.0 and Mozilla Public License 2.0).

LANGUAGE: JavaScript
CODE:
/*! @license DOMPurify 2.4.3 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/2.4.3/LICENSE */

----------------------------------------

TITLE: Defining Math Functions in Markdown Table
DESCRIPTION: A markdown table listing mathematical functions available in Apache Druid's expression language. It includes a wide range of trigonometric, logarithmic, and general mathematical operations.

LANGUAGE: markdown
CODE:
|name|description|
|----|-----------|
|abs|abs(x) would return the absolute value of x|
|acos|acos(x) would return the arc cosine of x|
|asin|asin(x) would return the arc sine of x|
|atan|atan(x) would return the arc tangent of x|
|atan2|atan2(y, x) would return the angle theta from the conversion of rectangular coordinates (x, y) to polar * coordinates (r, theta)|
|cbrt|cbrt(x) would return the cube root of x|
|ceil|ceil(x) would return the smallest (closest to negative infinity) double value that is greater than or equal to x and is equal to a mathematical integer|
|copysign|copysign(x) would return the first floating-point argument with the sign of the second floating-point argument|
|cos|cos(x) would return the trigonometric cosine of x|
|cosh|cosh(x) would return the hyperbolic cosine of x|
|div|div(x,y) is integer division of x by y|
|exp|exp(x) would return Euler's number raised to the power of x|
|expm1|expm1(x) would return e^x-1|
|floor|floor(x) would return the largest (closest to positive infinity) double value that is less than or equal to x and is equal to a mathematical integer|
|getExponent|getExponent(x) would return the unbiased exponent used in the representation of x|
|hypot|hypot(x, y) would return sqrt(x^2+y^2) without intermediate overflow or underflow|
|log|log(x) would return the natural logarithm of x|
|log10|log10(x) would return the base 10 logarithm of x|
|log1p|log1p(x) would the natural logarithm of x + 1|
|max|max(x, y) would return the greater of two values|
|min|min(x, y) would return the smaller of two values|
|nextafter|nextafter(x, y) would return the floating-point number adjacent to the x in the direction of the y|
|nextUp|nextUp(x) would return the floating-point value adjacent to x in the direction of positive infinity|
|pow|pow(x, y) would return the value of the x raised to the power of y|
|remainder|remainder(x, y) would return the remainder operation on two arguments as prescribed by the IEEE 754 standard|
|rint|rint(x) would return value that is closest in value to x and is equal to a mathematical integer|
|round|round(x) would return the closest long value to x, with ties rounding up|
|scalb|scalb(d, sf) would return d * 2^sf rounded as if performed by a single correctly rounded floating-point multiply to a member of the double value set|
|signum|signum(x) would return the signum function of the argument x|
|sin|sin(x) would return the trigonometric sine of an angle x|
|sinh|sinh(x) would return the hyperbolic sine of x|
|sqrt|sqrt(x) would return the correctly rounded positive square root of x|
|tan|tan(x) would return the trigonometric tangent of an angle x|
|tanh|tanh(x) would return the hyperbolic tangent of x|
|todegrees|todegrees(x) converts an angle measured in radians to an approximately equivalent angle measured in degrees|
|toradians|toradians(x) converts an angle measured in degrees to an approximately equivalent angle measured in radians|
|ulp|ulp(x) would return the size of an ulp of the argument x|

----------------------------------------

TITLE: Declaring MIT License for object-assign
DESCRIPTION: Copyright notice and MIT license declaration for the object-assign library by Sindre Sorhus.

LANGUAGE: JavaScript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

----------------------------------------

TITLE: Creating Cassandra Tables for Druid Deep Storage
DESCRIPTION: SQL commands to create the required tables 'index_storage' and 'descriptor_storage' in Cassandra for storing Druid segments and their metadata.

LANGUAGE: sql
CODE:
CREATE TABLE index_storage(key text,
                           chunk text,
                           value blob,
                           PRIMARY KEY (key, chunk)) WITH COMPACT STORAGE;

CREATE TABLE descriptor_storage(key varchar,
                                lastModified timestamp,
                                descriptor varchar,
                                PRIMARY KEY (key)) WITH COMPACT STORAGE;

----------------------------------------

TITLE: Configuring Azure Blob Store Firehose in Druid
DESCRIPTION: JSON configuration for setting up a static Azure Blob Store firehose in Druid. This config enables ingesting data from Azure Blob Storage with support for caching and prefetching features.

LANGUAGE: json
CODE:
{
    "type" : "static-azure-blobstore",
    "blobs": [
        {
          "container": "container",
          "path": "/path/to/your/file.json"
        },
        {
          "container": "anothercontainer",
          "path": "/another/path.json"
        }
    ]
}

----------------------------------------

TITLE: Configuring SBT Assembly Plugin
DESCRIPTION: SBT configuration to add the assembly plugin for building fat jars

LANGUAGE: scala
CODE:
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.13.0")

----------------------------------------

TITLE: License Declarations for Multiple Dependencies
DESCRIPTION: A compilation of MIT license declarations and copyright notices for various third-party libraries used in the Druid project. Each comment block represents the license information for a specific dependency.

LANGUAGE: text
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

/*!
	Copyright (c) 2018 Jed Watson.
	Licensed under the MIT License (MIT), see
	http://jedwatson.github.io/classnames
*/

/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Configuring MomentSketch Post-Aggregator for Min/Max in Druid
DESCRIPTION: JSON configuration for the momentSketchMin and momentSketchMax post-aggregators, used to query the min or max of a distribution from MomentSketch aggregations.

LANGUAGE: json
CODE:
{
  "type" : "momentSketchMin" | "momentSketchMax",
  "name" : <output_name>,
  "field" : <reference to moment sketch>,
}

----------------------------------------

TITLE: TuningConfig JSON Configuration
DESCRIPTION: JSON configuration for the tuningConfig section of the Hadoop indexer spec, specifying the working path for intermediate results.

LANGUAGE: json
CODE:
  "tuningConfig" : {
   ...
    "workingPath": "/tmp",
    ...
  }

----------------------------------------

TITLE: Avro Hadoop Parser Configuration
DESCRIPTION: Configuration for batch ingestion using Avro Hadoop parser with custom schema file specification in the tuning config.

LANGUAGE: json
CODE:
{
  "type" : "index_hadoop",  
  "spec" : {
    "dataSchema" : {
      "dataSource" : "",
      "parser" : {
        "type" : "avro_hadoop",
        "parseSpec" : {
          "format": "avro",
          "timestampSpec": <standard timestampSpec>,
          "dimensionsSpec": <standard dimensionsSpec>,
          "flattenSpec": <optional>
        }
      }
    },
    "ioConfig" : {
      "type" : "hadoop",
      "inputSpec" : {
        "type" : "static",
        "inputFormat": "org.apache.druid.data.input.avro.AvroValueInputFormat",
        "paths" : ""
      }
    },
    "tuningConfig" : {
       "jobProperties" : {
          "avro.schema.input.value.path" : "/path/to/my/schema.avsc"
      }
    }
  }
}

----------------------------------------

TITLE: Copyright Notice for React use-sync-external-store-shim
DESCRIPTION: Copyright notice for the React use-sync-external-store-shim production build, released under the MIT license by Facebook, Inc. and its affiliates.

LANGUAGE: JavaScript
CODE:
/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Running a Peon in Apache Druid
DESCRIPTION: Command to run a Peon independently for development purposes. It requires a task file containing the task JSON object and a status file to output the task status.

LANGUAGE: bash
CODE:
org.apache.druid.cli.Main internal peon <task_file> <status_file>

----------------------------------------

TITLE: Starting Kafka Broker
DESCRIPTION: Command to start a Kafka broker using the default configuration.

LANGUAGE: bash
CODE:
./bin/kafka-server-start.sh config/server.properties

----------------------------------------

TITLE: Displaying Raw Data for Rollup Example
DESCRIPTION: Example of raw data representing packet/byte counts between source and destination IPs, used to demonstrate Druid's rollup functionality.

LANGUAGE: plaintext
CODE:
timestamp                 srcIP         dstIP          packets     bytes
2018-01-01T01:01:35Z      1.1.1.1       2.2.2.2            100      1000
2018-01-01T01:01:51Z      1.1.1.1       2.2.2.2            200      2000
2018-01-01T01:01:59Z      1.1.1.1       2.2.2.2            300      3000
2018-01-01T01:02:14Z      1.1.1.1       2.2.2.2            400      4000
2018-01-01T01:02:29Z      1.1.1.1       2.2.2.2            500      5000
2018-01-01T01:03:29Z      1.1.1.1       2.2.2.2            600      6000
2018-01-02T21:33:14Z      7.7.7.7       8.8.8.8            100      1000
2018-01-02T21:33:45Z      7.7.7.7       8.8.8.8            200      2000
2018-01-02T21:35:45Z      7.7.7.7       8.8.8.8            300      3000

----------------------------------------

TITLE: Performing Student's t-test on ArrayOfDoublesSketch
DESCRIPTION: This JSON configuration defines a post-aggregator to perform Student's t-test on two ArrayOfDoublesSketch instances in Druid, returning a list of p-values.

LANGUAGE: json
CODE:
{
  "type"  : "arrayOfDoublesSketchTTest",
  "name": <output name>,
  "fields"  : <array with two post aggregators to access sketch aggregators or post aggregators referring to an ArrayOfDoublesSketch>,
}

----------------------------------------

TITLE: React Use Sync External Store Shim License
DESCRIPTION: Copyright notice and MIT license information for the use-sync-external-store-shim.production.min.js file from React.

LANGUAGE: JavaScript
CODE:
/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Configuring StringFirst Aggregator in Druid JSON
DESCRIPTION: The stringFirst aggregator computes the metric value with the minimum timestamp or null if no row exists. It includes optional parameters for maximum string bytes and filtering null values.

LANGUAGE: json
CODE:
{
  "type" : "stringFirst",
  "name" : <output_name>,
  "fieldName" : <metric_name>,
  "maxStringBytes" : <integer> # (optional, defaults to 1024),
  "filterNullValues" : <boolean> # (optional, defaults to false)
}

----------------------------------------

TITLE: Sample JSON Data Format for Druid Ingestion
DESCRIPTION: Example showing JSON-formatted data with timestamp, page information, and various metrics that can be ingested into Druid.

LANGUAGE: json
CODE:
{"timestamp": "2013-08-31T01:02:33Z", "page": "Gypsy Danger", "language" : "en", "user" : "nuclear", "unpatrolled" : "true", "newPage" : "true", "robot": "false", "anonymous": "false", "namespace":"article", "continent":"North America", "country":"United States", "region":"Bay Area", "city":"San Francisco", "added": 57, "deleted": 200, "delta": -143}

----------------------------------------

TITLE: Document Load Handler Comment
DESCRIPTION: Comment block indicating code should wait for document to be fully loaded before execution.

LANGUAGE: javascript
CODE:
/*!\n   * Wait for document loaded before starting the execution\n   */

----------------------------------------

TITLE: Configuring Period Drop Rule in Apache Druid
DESCRIPTION: This JSON snippet defines a Period Drop Rule, which specifies that segments within a specific time period should be dropped from the cluster.

LANGUAGE: json
CODE:
{
  "type" : "dropByPeriod",
  "period" : "P1M",
  "includeFuture" : true
}

----------------------------------------

TITLE: Checking Previous Processing in JavaScript
DESCRIPTION: This comment suggests that there's a check in place to determine if some processing has already occurred. This is often used to prevent duplicate initialization or processing in JavaScript applications.

LANGUAGE: JavaScript
CODE:
/*! Check if previously processed */

----------------------------------------

TITLE: Configuring GCS Storage Properties in Druid
DESCRIPTION: Configuration properties for using Google Cloud Storage through the HDFS extension in Druid. Specifies storage type and GCS bucket location.

LANGUAGE: properties
CODE:
druid.storage.type=hdfs
druid.storage.storageDirectory=gs://bucket/example/directory

----------------------------------------

TITLE: Configuring White-list Converter for Ambari Metrics Emitter in Druid
DESCRIPTION: JSON configuration for the 'whiteList' event converter, which sends only white-listed metrics and dimensions to Ambari Metrics. This example shows how to specify a custom file path for the white list map, set the namespace prefix, and ignore the hostname in the metric path.

LANGUAGE: json
CODE:
druid.emitter.ambari-metrics.eventConverter={"type":"whiteList", "namespacePrefix": "druid.test", "ignoreHostname":true, "appName":"druid", "mapPath":"/pathPrefix/fileName.json"}

----------------------------------------

TITLE: Classnames License Header
DESCRIPTION: MIT license header for classnames library by Jed Watson

LANGUAGE: javascript
CODE:
/*!
	Copyright (c) 2018 Jed Watson.
	Licensed under the MIT License (MIT), see
	http://jedwatson.github.io/classnames
*/

----------------------------------------

TITLE: Inline Schema Based Avro Bytes Decoder Configuration in Druid
DESCRIPTION: This snippet shows how to configure an inline schema-based Avro bytes decoder. It includes the schema definition within the task JSON, suitable for cases where all input events use the same schema.

LANGUAGE: json
CODE:
"avroBytesDecoder": {
  "type": "schema_inline",
  "schema": {
    "namespace": "org.apache.druid.data",
    "name": "User",
    "type": "record",
    "fields": [
      { "name": "FullName", "type": "string" },
      { "name": "Country", "type": "string" }
    ]
  }
}

----------------------------------------

TITLE: Processing Check Comment in JavaScript
DESCRIPTION: This comment suggests that there's a check in place to determine if some content has been previously processed. It's likely used to prevent redundant operations or to manage the state of processed items.

LANGUAGE: JavaScript
CODE:
/*! Check if previously processed */

----------------------------------------

TITLE: Including DataSketches Extension in Druid Configuration
DESCRIPTION: This snippet shows how to include the DataSketches extension in the Druid configuration file. It specifies the extension to be loaded as part of the Druid setup.

LANGUAGE: yaml
CODE:
druid.extensions.loadList=["druid-datasketches"]

----------------------------------------

TITLE: Configuring MomentSketch Aggregator in Apache Druid
DESCRIPTION: JSON configuration for the momentSketch and momentSketchMerge aggregators, specifying parameters like type, name, fieldName, k, and compress.

LANGUAGE: json
CODE:
{
  "type" : <aggregator_type>,
  "name" : <output_name>,
  "fieldName" : <input_name>,
  "k" : <int>,
  "compress" : <boolean>
 }

----------------------------------------

TITLE: Creating PostgreSQL Druid Database
DESCRIPTION: Command to create a PostgreSQL database named 'druid' owned by the druid user

LANGUAGE: bash
CODE:
createdb druid -O druid

----------------------------------------

TITLE: Prism Syntax Highlighter License
DESCRIPTION: Copyright notice and MIT license information for the Prism syntax highlighting library by Lea Verou.

LANGUAGE: JavaScript
CODE:
/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

----------------------------------------

TITLE: Submitting Kafka Supervisor via REST API
DESCRIPTION: cURL command to submit a Kafka supervisor specification to the Druid overlord.

LANGUAGE: bash
CODE:
curl -X POST -H 'Content-Type: application/json' -d @supervisor-spec.json http://localhost:8090/druid/indexer/v1/supervisor

----------------------------------------

TITLE: Creating PostgreSQL Druid Database
DESCRIPTION: Command to create a PostgreSQL database named 'druid' owned by the druid user

LANGUAGE: bash
CODE:
createdb druid -O druid

----------------------------------------

TITLE: Configuring pvalue2tailedZtest Post Aggregator in Apache Druid
DESCRIPTION: This snippet shows the configuration for the pvalue2tailedZtest post aggregator in Apache Druid. It calculates the p-value of a two-sided z-test from a given z-score, which can be obtained using the zscore2sample post aggregator.

LANGUAGE: json
CODE:
{
  "type": "pvalue2tailedZtest",
  "name": "<output_name>",
  "zScore": <zscore post_aggregator>
}

----------------------------------------

TITLE: MIT License Notice for React Is
DESCRIPTION: Copyright notice for React Is production build, version 16.13.1, licensed under MIT.

LANGUAGE: JavaScript
CODE:
/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: NProgress License Notice
DESCRIPTION: Copyright notice and MIT license information for the NProgress library by Rico Sta. Cruz.

LANGUAGE: JavaScript
CODE:
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

----------------------------------------

TITLE: Configuring Registered Lookup Extraction Function in Apache Druid
DESCRIPTION: JSON configuration for a Registered Lookup Extraction Function, which uses a lookup registered in the cluster-wide configuration to replace dimension values.

LANGUAGE: JSON
CODE:
{
  "type":"registeredLookup",
  "lookup":"some_lookup_name",
  "retainMissingValue":true
}

----------------------------------------

TITLE: Last Name Character Cardinality Example
DESCRIPTION: Advanced example showing cardinality calculation with dimension extraction for first characters of last names.

LANGUAGE: json
CODE:
{
  "type": "cardinality",
  "name": "distinct_last_name_first_char",
  "fields": [
    {
     "type" : "extraction",
     "dimension" : "last_name",
     "outputName" :  "last_name_first_char",
     "extractionFn" : { "type" : "substring", "index" : 0, "length" : 1 }
    }
  ],
  "byRow" : true
}

----------------------------------------

TITLE: License Information for React use-sync-external-store-shim
DESCRIPTION: Copyright notice and MIT license information for React's use-sync-external-store-shim production build.

LANGUAGE: JavaScript
CODE:
/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: License Comments Collection - JavaScript
DESCRIPTION: A collection of license and copyright notices for various JavaScript libraries and tools used in the project. All components are licensed under MIT.

LANGUAGE: javascript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

/*!***************************************************
* mark.js v8.11.1
* https://markjs.io/
* Copyright (c) 20142018, Julian Khnel
* Released under the MIT license https://git.io/vwTVl
*****************************************************/

/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Druid PostgreSQL Configuration Properties
DESCRIPTION: Essential configuration properties for connecting Druid to PostgreSQL metadata storage, including extension loading, connection URI, and credentials

LANGUAGE: properties
CODE:
druid.extensions.loadList=["postgresql-metadata-storage"]
druid.metadata.storage.type=postgresql
druid.metadata.storage.connector.connectURI=jdbc:postgresql://<host>/druid
druid.metadata.storage.connector.user=druid
druid.metadata.storage.connector.password=diurd

----------------------------------------

TITLE: Running Hadoop Indexer with Custom Druid Fat Jar
DESCRIPTION: Bash command for running the Hadoop indexer using a custom self-contained Druid jar to avoid dependency conflicts.

LANGUAGE: bash
CODE:
java -Xmx32m \
  -Dfile.encoding=UTF-8 -Duser.timezone=UTC \
  -classpath config/hadoop:config/overlord:config/_common:$SELF_CONTAINED_JAR:$HADOOP_DISTRIBUTION/etc/hadoop \
  -Djava.security.krb5.conf=$KRB5 \
  org.apache.druid.cli.Main index hadoop \
  $config_path

----------------------------------------

TITLE: Running Hadoop Indexer with Custom Druid Fat Jar
DESCRIPTION: Bash command for running the Hadoop indexer using a custom self-contained Druid jar to avoid dependency conflicts.

LANGUAGE: bash
CODE:
java -Xmx32m \
  -Dfile.encoding=UTF-8 -Duser.timezone=UTC \
  -classpath config/hadoop:config/overlord:config/_common:$SELF_CONTAINED_JAR:$HADOOP_DISTRIBUTION/etc/hadoop \
  -Djava.security.krb5.conf=$KRB5 \
  org.apache.druid.cli.Main index hadoop \
  $config_path

----------------------------------------

TITLE: License Information for Prism
DESCRIPTION: Copyright notice and MIT license information for Prism, a syntax highlighting library by Lea Verou.

LANGUAGE: JavaScript
CODE:
/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

----------------------------------------

TITLE: Copyright Notice for React Scheduler
DESCRIPTION: Copyright notice for the React Scheduler production build (v0.20.2), released under the MIT license by Facebook, Inc. and its affiliates.

LANGUAGE: JavaScript
CODE:
/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Druid SQL Query Results
DESCRIPTION: Example output from querying the transformed data showing the modified animal names and calculated triple-number values.

LANGUAGE: bash
CODE:
dsql> select * from "transform-tutorial";

 __time                    animal          count  location  number  triple-number 

 2018-01-01T05:01:00.000Z  super-mongoose      1         2     200            600 
 2018-01-01T06:01:00.000Z  super-snake         1         3     300            900 
 2018-01-01T07:01:00.000Z  super-octopus       1         1     100            300 


----------------------------------------

TITLE: Maven Shade Plugin Configuration
DESCRIPTION: XML configuration for Maven Shade plugin to create a fat jar with relocated Jackson packages

LANGUAGE: xml
CODE:
<plugin>
     <groupId>org.apache.maven.plugins</groupId>
     <artifactId>maven-shade-plugin</artifactId>
     <executions>
         <execution>
             <phase>package</phase>
             <goals>
                 <goal>shade</goal>
             </goals>
             <configuration>
                 <outputFile>
                     ${project.build.directory}/${project.artifactId}-${project.version}-selfcontained.jar
                 </outputFile>
                 <relocations>
                     <relocation>
                         <pattern>com.fasterxml.jackson</pattern>
                         <shadedPattern>shade.com.fasterxml.jackson</shadedPattern>
                     </relocation>
                 </relocations>
                 <artifactSet>
                     <includes>
                         <include>*:*</include>
                     </includes>
                 </artifactSet>
                 <filters>
                     <filter>
                         <artifact>*:*</artifact>
                         <excludes>
                             <exclude>META-INF/*.SF</exclude>
                             <exclude>META-INF/*.DSA</exclude>
                             <exclude>META-INF/*.RSA</exclude>
                         </excludes>
                     </filter>
                 </filters>
                 <transformers>
                     <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                 </transformers>
             </configuration>
         </execution>
     </executions>
 </plugin>

----------------------------------------

TITLE: License Header Documentation
DESCRIPTION: Collection of MIT license declarations and copyright notices for various JavaScript libraries and dependencies used in the project

LANGUAGE: javascript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

/*!
	Copyright (c) 2018 Jed Watson.
	Licensed under the MIT License (MIT), see
	http://jedwatson.github.io/classnames
*/

/**
 * @license React
 * use-sync-external-store-shim.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/**
 * Prism: Lightweight, robust, elegant syntax highlighting
 *
 * @license MIT <https://opensource.org/licenses/MIT>
 * @author Lea Verou <https://lea.verou.me>
 * @namespace
 * @public
 */

/** @license React v0.20.2
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Accessing Legacy Coordinator Console URL
DESCRIPTION: URL pattern for accessing the version 1 (legacy) Coordinator Console interface.

LANGUAGE: plaintext
CODE:
http://<COORDINATOR_IP>:<COORDINATOR_PORT>/old-console

----------------------------------------

TITLE: Object-Assign License Notice
DESCRIPTION: Copyright notice and MIT license information for the object-assign library by Sindre Sorhus.

LANGUAGE: JavaScript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

----------------------------------------

TITLE: Configuring pvalue2tailedZtest Post Aggregator in Druid
DESCRIPTION: JSON configuration for calculating p-value from a z-score using two-tailed z-test. Takes a z-score as input to compute the corresponding p-value.

LANGUAGE: json
CODE:
{
  "type": "pvalue2tailedZtest",
  "name": "<output_name>",
  "zScore": <zscore post_aggregator>
}

----------------------------------------

TITLE: JavaScript Parser Configuration for Druid
DESCRIPTION: Configuration for using custom JavaScript functions to parse data in Druid, with timestamp and dimension specifications.

LANGUAGE: json
CODE:
"parseSpec":{ "format" : "javascript", "timestampSpec" : { "column" : "timestamp" }, "dimensionsSpec" : { "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"] }, "function" : "function(str) { var parts = str.split(\"-\"); return { one: parts[0], two: parts[1] } }" }

----------------------------------------

TITLE: JavaScript Parser Configuration for Druid
DESCRIPTION: Configuration for using custom JavaScript functions to parse data in Druid, with timestamp and dimension specifications.

LANGUAGE: json
CODE:
"parseSpec":{ "format" : "javascript", "timestampSpec" : { "column" : "timestamp" }, "dimensionsSpec" : { "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"] }, "function" : "function(str) { var parts = str.split(\"-\"); return { one: parts[0], two: parts[1] } }" }

----------------------------------------

TITLE: JavaScript Parser Configuration for Druid
DESCRIPTION: Configuration for using custom JavaScript functions to parse data in Druid, with timestamp and dimension specifications.

LANGUAGE: json
CODE:
"parseSpec":{ "format" : "javascript", "timestampSpec" : { "column" : "timestamp" }, "dimensionsSpec" : { "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"] }, "function" : "function(str) { var parts = str.split(\"-\"); return { one: parts[0], two: parts[1] } }" }

----------------------------------------

TITLE: Maven Dependencies for Druid Extensions
DESCRIPTION: Maven XML configuration showing how to include various Druid extensions in the project.

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.druid.extensions</groupId>
    <artifactId>druid-avro-extensions</artifactId>
    <version>${project.parent.version}</version>
</dependency>

<dependency>
    <groupId>org.apache.druid.extensions.contrib</groupId>
    <artifactId>druid-parquet-extensions</artifactId>
    <version>${project.parent.version}</version>
</dependency>

<dependency>
    <groupId>org.apache.druid.extensions</groupId>
    <artifactId>druid-hdfs-storage</artifactId>
    <version>${project.parent.version}</version>
</dependency>

<dependency>
    <groupId>org.apache.druid.extensions</groupId>
    <artifactId>mysql-metadata-storage</artifactId>
    <version>${project.parent.version}</version>
</dependency>

----------------------------------------

TITLE: Maven Dependencies for Druid Extensions
DESCRIPTION: Maven XML configuration showing how to include various Druid extensions in the project.

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.druid.extensions</groupId>
    <artifactId>druid-avro-extensions</artifactId>
    <version>${project.parent.version}</version>
</dependency>

<dependency>
    <groupId>org.apache.druid.extensions.contrib</groupId>
    <artifactId>druid-parquet-extensions</artifactId>
    <version>${project.parent.version}</version>
</dependency>

<dependency>
    <groupId>org.apache.druid.extensions</groupId>
    <artifactId>druid-hdfs-storage</artifactId>
    <version>${project.parent.version}</version>
</dependency>

<dependency>
    <groupId>org.apache.druid.extensions</groupId>
    <artifactId>mysql-metadata-storage</artifactId>
    <version>${project.parent.version}</version>
</dependency>

----------------------------------------

TITLE: Implementing JavaScript Aggregator Example in Druid
DESCRIPTION: Example JSON configuration for the JavaScript aggregator in Druid, demonstrating a custom calculation.

LANGUAGE: json
CODE:
{
  "type": "javascript",
  "name": "sum(log(x)*y) + 10",
  "fieldNames": ["x", "y"],
  "fnAggregate" : "function(current, a, b)      { return current + (Math.log(a) * b); }",
  "fnCombine"   : "function(partialA, partialB) { return partialA + partialB; }",
  "fnReset"     : "function()                   { return 10; }"
}

----------------------------------------

TITLE: Executing Kill Task in Apache Druid
DESCRIPTION: This JSON structure defines a Kill Task in Druid, which is used to permanently delete segments marked as unused. The task specifies the data source, interval for deletion, and optional context information.

LANGUAGE: json
CODE:
{
    "type": "kill",
    "id": <task_id>,
    "dataSource": <task_datasource>,
    "interval" : <all_segments_in_this_interval_will_die!>,
    "context": <task context>
}

----------------------------------------

TITLE: Executing Kill Task in Apache Druid
DESCRIPTION: This JSON structure defines a Kill Task in Druid, which is used to permanently delete segments marked as unused. The task specifies the data source, interval for deletion, and optional context information.

LANGUAGE: json
CODE:
{
    "type": "kill",
    "id": <task_id>,
    "dataSource": <task_datasource>,
    "interval" : <all_segments_in_this_interval_will_die!>,
    "context": <task context>
}

----------------------------------------

TITLE: Configuring Count Metric in Druid Ingestion Spec
DESCRIPTION: This JSON snippet shows how to configure a count metric in the metricsSpec of a Druid ingestion specification.

LANGUAGE: json
CODE:
"metricsSpec" : [
      {
        "type" : "count",
        "name" : "count"
      }
]

----------------------------------------

TITLE: Runge-Kutta Physics Generator Copyright Header
DESCRIPTION: MIT License copyright header for a Runge-Kutta spring physics function generator adapted from Framer.js by Koen Bok.

LANGUAGE: text
CODE:
/*! Runge-Kutta spring physics function generator. Adapted from Framer.js, copyright Koen Bok. MIT License: http://en.wikipedia.org/wiki/MIT_License */

----------------------------------------

TITLE: Configuring Filter in JSON for Apache Druid TransformSpec
DESCRIPTION: This example demonstrates how to set up a filter within a transformSpec to ingest only rows where a 'country' column has the value 'United States'.

LANGUAGE: json
CODE:
"filter": {
  "type": "selector",
  "dimension": "country",
  "value": "United States"
}

----------------------------------------

TITLE: Example Unique Users Query
DESCRIPTION: Complete query example demonstrating how to count unique users who visited a specific product.

LANGUAGE: json
CODE:
{
  "queryType": "groupBy",
  "dataSource": "test_datasource",
  "granularity": "ALL",
  "dimensions": [],
  "aggregations": [
    { "type": "thetaSketch", "name": "unique_users", "fieldName": "user_id_sketch" }
  ],
  "filter": { "type": "selector", "dimension": "product", "value": "A" },
  "intervals": [ "2014-10-19T00:00:00.000Z/2014-10-22T00:00:00.000Z" ]
}

----------------------------------------

TITLE: Defining General Functions in Markdown Table
DESCRIPTION: A markdown table listing general-purpose functions available in Apache Druid's expression language. It includes functions for type casting, conditional logic, and null handling.

LANGUAGE: markdown
CODE:
|name|description|
|----|-----------|
|cast|cast(expr,'LONG' or 'DOUBLE' or 'STRING') returns expr with specified type. exception can be thrown |
|if|if(predicate,then,else) returns 'then' if 'predicate' evaluates to a positive number, otherwise it returns 'else' |
|nvl|nvl(expr,expr-for-null) returns 'expr-for-null' if 'expr' is null (or empty string for string type) |
|like|like(expr, pattern[, escape]) is equivalent to SQL `expr LIKE pattern`|
|case_searched|case_searched(expr1, result1, \[\[expr2, result2, ...\], else-result\])|
|case_simple|case_simple(expr, value1, result1, \[\[value2, result2, ...\], else-result\])|
|bloom_filter_test|bloom_filter_test(expr, filter) tests the value of 'expr' against 'filter', a bloom filter serialized as a base64 string. See [bloom filter extension](../development/extensions-core/bloom-filter.html) documentation for additional details.|

----------------------------------------

TITLE: Sample Wikipedia Page Edit Event JSON
DESCRIPTION: An example of a JSON object representing a Wikipedia page edit event from the sample dataset used in the tutorials.

LANGUAGE: json
CODE:
{
  "timestamp":"2015-09-12T20:03:45.018Z",
  "channel":"#en.wikipedia",
  "namespace":"Main",
  "page":"Spider-Man's powers and equipment",
  "user":"foobar",
  "comment":"/* Artificial web-shooters */",
  "cityName":"New York",
  "regionName":"New York",
  "regionIsoCode":"NY",
  "countryName":"United States",
  "countryIsoCode":"US",
  "isAnonymous":false,
  "isNew":false,
  "isMinor":false,
  "isRobot":false,
  "isUnpatrolled":false,
  "added":99,
  "delta":99,
  "deleted":0
}

----------------------------------------

TITLE: Configuring Upper Case Extraction Function in Apache Druid
DESCRIPTION: JSON configuration for an Upper Case Extraction Function, which converts dimension values to upper case.

LANGUAGE: JSON
CODE:
{
  "type" : "upper",
  "locale":"fr"
}

----------------------------------------

TITLE: Configuring TimedShutoffFirehose in Apache Druid
DESCRIPTION: This snippet shows the configuration for a TimedShutoffFirehose in Druid, which shuts down at a specified time. It includes the shutoff time and a delegate firehose configuration.

LANGUAGE: json
CODE:
{
    "type"  :   "timed",
    "shutoffTime": "2015-08-25T01:26:05.119Z",
    "delegate": {
          "type": "receiver",
          "serviceName": "eventReceiverServiceName",
          "bufferSize": 100000
     }
}

----------------------------------------

TITLE: Internal System User Interface Implementation
DESCRIPTION: Java interface showing required methods for implementing internal system user authentication handling in custom Escalators.

LANGUAGE: java
CODE:
public HttpClient createEscalatedClient(HttpClient baseClient);

public org.eclipse.jetty.client.HttpClient createEscalatedJettyClient(org.eclipse.jetty.client.HttpClient baseClient);

public AuthenticationResult createEscalatedAuthenticationResult();

----------------------------------------

TITLE: Creating Kafka Topic
DESCRIPTION: Command to create a Kafka topic named 'wikipedia' for ingesting data into Druid.

LANGUAGE: bash
CODE:
./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic wikipedia

----------------------------------------

TITLE: Expression Virtual Column Configuration Schema
DESCRIPTION: Schema definition for configuring an expression-type virtual column, showing required and optional parameters including name, expression, and output type.

LANGUAGE: json
CODE:
{
  "type": "expression",
  "name": <name of the virtual column>,
  "expression": <row expression>,
  "outputType": <output value type of expression>
}

----------------------------------------

TITLE: Filtering Multi-value Dimensions with AND in Apache Druid
DESCRIPTION: Example of an 'and' filter that matches rows where the 'tags' dimension contains both 't1' and 't3'. This filter would only match row1 in the sample dataset.

LANGUAGE: json
CODE:
{
  "type": "and",
  "fields": [
    {
      "type": "selector",
      "dimension": "tags",
      "value": "t1"
    },
    {
      "type": "selector",
      "dimension": "tags",
      "value": "t3"
    }
  ]
}

----------------------------------------

TITLE: Defining OrderByColumnSpec for Sorting Criteria in Druid Queries
DESCRIPTION: This JSON structure defines an OrderByColumnSpec, which specifies how to perform order-by operations on individual columns. It includes the dimension name, sort direction, and optional dimensionOrder for custom sorting.

LANGUAGE: json
CODE:
{
    "dimension" : "<Any dimension or metric name>",
    "direction" : <"ascending"|"descending">,
    "dimensionOrder" : <"lexicographic"(default)|"alphanumeric"|"strlen"|"numeric">
}

----------------------------------------

TITLE: Sample groupBy Query Response in Apache Druid
DESCRIPTION: Example of the JSON response format for a groupBy query in Apache Druid, showing the structure of returned data points.

LANGUAGE: json
CODE:
[ 
  {
    "version" : "v1",
    "timestamp" : "2012-01-01T00:00:00.000Z",
    "event" : {
      "country" : <some_dim_value_one>,
      "device" : <some_dim_value_two>,
      "total_usage" : <some_value_one>,
      "data_transfer" :<some_value_two>,
      "avg_usage" : <some_avg_usage_value>
    }
  }, 
  {
    "version" : "v1",
    "timestamp" : "2012-01-01T00:00:12.000Z",
    "event" : {
      "dim1" : <some_other_dim_value_one>,
      "dim2" : <some_other_dim_value_two>,
      "sample_name1" : <some_other_value_one>,
      "sample_name2" :<some_other_value_two>,
      "avg_usage" : <some_other_avg_usage_value>
    }
  },
...
]

----------------------------------------

TITLE: Sample groupBy Query Response in Apache Druid
DESCRIPTION: Example of the JSON response format for a groupBy query in Apache Druid, showing the structure of returned data points.

LANGUAGE: json
CODE:
[ 
  {
    "version" : "v1",
    "timestamp" : "2012-01-01T00:00:00.000Z",
    "event" : {
      "country" : <some_dim_value_one>,
      "device" : <some_dim_value_two>,
      "total_usage" : <some_value_one>,
      "data_transfer" :<some_value_two>,
      "avg_usage" : <some_avg_usage_value>
    }
  }, 
  {
    "version" : "v1",
    "timestamp" : "2012-01-01T00:00:12.000Z",
    "event" : {
      "dim1" : <some_other_dim_value_one>,
      "dim2" : <some_other_dim_value_two>,
      "sample_name1" : <some_other_value_one>,
      "sample_name2" :<some_other_value_two>,
      "avg_usage" : <some_other_avg_usage_value>
    }
  },
...
]

----------------------------------------

TITLE: Data Source Metadata Query Response Format
DESCRIPTION: Example response format from a Druid Data Source Metadata query showing the maximum ingested event timestamp.

LANGUAGE: json
CODE:
[ {
  "timestamp" : "2013-05-09T18:24:00.000Z",
  "result" : {
    "maxIngestedEventTime" : "2013-05-09T18:24:09.007Z"
  }
} ]

----------------------------------------

TITLE: Configuring Client Certificate Authentication in Apache Druid
DESCRIPTION: This configuration table provides optional parameters for setting up client certificate authentication in Apache Druid. It includes properties for specifying the key store, certificate alias, and related settings.

LANGUAGE: markdown
CODE:
|Property|Description|Default|Required|
|--------|-----------|-------|--------|
|`druid.client.https.keyStorePath`|The file path or URL of the TLS/SSL Key store containing the client certificate that Druid will use when communicating with other Druid services. If this is null, the other properties in this table are ignored.|none|yes|
|`druid.client.https.keyStoreType`|The type of the key store.|none|yes|
|`druid.client.https.certAlias`|Alias of TLS client certificate in the keystore.|none|yes|
|`druid.client.https.keyStorePassword`|The [Password Provider](../../operations/password-provider.html) or String password for the Key Store.|none|no|
|`druid.client.https.keyManagerFactoryAlgorithm`|Algorithm to use for creating KeyManager, more details [here](https://docs.oracle.com/javase/7/docs/technotes/guides/security/jsse/JSSERefGuide.html#KeyManager).|`javax.net.ssl.KeyManagerFactory.getDefaultAlgorithm()`|no|
|`druid.client.https.keyManagerPassword`|The [Password Provider](../../operations/password-provider.html) or String password for the Key Manager.|none|no|
|`druid.client.https.validateHostnames`|Validate the hostname of the server. This should not be disabled unless you are using [custom TLS certificate checks](../../operations/tls-support.html#custom-tls-certificate-checks) and know that standard hostname validation is not needed.|true|no|

----------------------------------------

TITLE: Starting Hadoop Docker Container
DESCRIPTION: Command to start the Hadoop container with necessary port mappings and volume mounts.

LANGUAGE: bash
CODE:
docker run -it  -h druid-hadoop-demo --name druid-hadoop-demo -p 50010:50010 -p 50020:50020 -p 50075:50075 -p 50090:50090 -p 8020:8020 -p 10020:10020 -p 19888:19888 -p 8030:8030 -p 8031:8031 -p 8032:8032 -p 8033:8033 -p 8040:8040 -p 8042:8042 -p 8088:8088 -p 8443:8443 -p 2049:2049 -p 9000:9000 -p 49707:49707 -p 2122:2122  -p 34455:34455 -v /tmp/shared:/shared druid-hadoop-demo:2.8.3 /etc/bootstrap.sh -bash

----------------------------------------

TITLE: Creating Shared Directories for Hadoop Container
DESCRIPTION: Commands to create temporary shared directories for file transfer between the host and Hadoop container.

LANGUAGE: bash
CODE:
mkdir -p /tmp/shared
mkdir -p /tmp/shared/hadoop_xml

----------------------------------------

TITLE: Installing and Starting ZooKeeper
DESCRIPTION: Commands to download, extract, configure, and start ZooKeeper on the Master server.

LANGUAGE: bash
CODE:
curl http://www.gtlib.gatech.edu/pub/apache/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz
tar -xzf zookeeper-3.4.11.tar.gz
cd zookeeper-3.4.11
cp conf/zoo_sample.cfg conf/zoo.cfg
./bin/zkServer.sh start

----------------------------------------

TITLE: Accessing Coordinator Console URL
DESCRIPTION: URL pattern for accessing the version 2 Coordinator Console interface.

LANGUAGE: plaintext
CODE:
http://<COORDINATOR_IP>:<COORDINATOR_PORT>

----------------------------------------

TITLE: Manually Submitting Druid Ingestion Task via cURL
DESCRIPTION: cURL command to manually submit the Druid ingestion task by POSTing the JSON configuration to the Druid overlord. This method provides more control over the submission process.

LANGUAGE: bash
CODE:
curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-index.json http://localhost:8090/druid/indexer/v1/task

----------------------------------------

TITLE: Demonstrating Multi-Interval Segment Example
DESCRIPTION: Shows segment naming across multiple time intervals with version v1. Used to explain non-atomic updates across intervals.

LANGUAGE: plaintext
CODE:
foo_2015-01-01/2015-01-02_v1_0
foo_2015-01-02/2015-01-03_v1_1
foo_2015-01-03/2015-01-04_v1_2

----------------------------------------

TITLE: Configuring Segment Load Queue Path in ZooKeeper
DESCRIPTION: ZooKeeper path used by Coordinator to signal Historical nodes to load or drop segments.

LANGUAGE: plaintext
CODE:
${druid.zk.paths.loadQueuePath}/_host_of_historical_node/_segment_identifier

----------------------------------------

TITLE: Configuring Coordinator Leader Election Path in ZooKeeper
DESCRIPTION: ZooKeeper path configuration for Druid Coordinator leader election using Curator LeadershipLatch recipe.

LANGUAGE: plaintext
CODE:
${druid.zk.paths.coordinatorPath}/_COORDINATOR

----------------------------------------

TITLE: Querying Row Count in Druid SQL
DESCRIPTION: SQL query to count total rows in the compaction-tutorial datasource before compaction.

LANGUAGE: bash
CODE:
dsql> select count(*) from "compaction-tutorial";

----------------------------------------

TITLE: Loading Initial Data with Druid CLI
DESCRIPTION: Command to submit the initial indexing task that creates 24 hourly segments from Wikipedia edit data.

LANGUAGE: bash
CODE:
bin/post-index-task --file quickstart/tutorial/compaction-init-index.json

----------------------------------------

TITLE: Paginated Select Query with fromNext Parameter
DESCRIPTION: Example of a paginated select query with backwards compatibility mode enabled through the fromNext parameter set to false.

LANGUAGE: json
CODE:
{
   "queryType": "select",
   "dataSource": "wikipedia",
   "descending": "false",
   "dimensions":[],
   "metrics":[],
   "granularity": "all",
   "intervals": [
     "2013-01-01/2013-01-02"
   ],
   "pagingSpec":{"fromNext": "false", "pagingIdentifiers": {}, "threshold":5}
 }

----------------------------------------

TITLE: Configuring JDBC Lookup
DESCRIPTION: Example configuration for a JDBC-based lookup. It specifies the database connection details, table and column information, and polling period for updates.

LANGUAGE: json
CODE:
{
  "type":"jdbc",
  "namespace":"some_lookup",
  "connectorConfig":{
    "createTables":true,
    "connectURI":"jdbc:mysql://localhost:3306/druid",
    "user":"druid",
    "password":"diurd"
  },
  "table":"some_lookup_table",
  "keyColumn":"the_old_dim_value",
  "valueColumn":"the_new_dim_value",
  "tsColumn":"timestamp_column",
  "pollPeriod":600000
}

----------------------------------------

TITLE: NProgress License Header
DESCRIPTION: MIT license header for NProgress library by Rico Sta. Cruz

LANGUAGE: javascript
CODE:
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */

----------------------------------------

TITLE: Copyright Notice for React
DESCRIPTION: License information for the React production minified version, which is under the MIT license.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Copyright Notice for object-assign
DESCRIPTION: License information for the object-assign library, which is under the MIT license.

LANGUAGE: JavaScript
CODE:
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/

----------------------------------------

TITLE: Documenting MIT License for React-dom
DESCRIPTION: This code snippet provides license information for the React-dom library, which is under the MIT license and created by Facebook, Inc. and its affiliates.

LANGUAGE: JavaScript
CODE:
/** @license React v17.0.2
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

----------------------------------------

TITLE: Configuring Robots.txt for Apache Druid Website
DESCRIPTION: This robots.txt configuration prevents web crawlers from accessing older documentation versions and an unused blog endpoint on the Apache Druid website. It applies to all user agents and uses wildcard patterns to block access to documentation versions from 0.x to 9.x, as well as the blog path.

LANGUAGE: robots.txt
CODE:
User-agent: *
Disallow: /docs/0*/
Disallow: /docs/1*/
Disallow: /docs/2*/
Disallow: /docs/3*/
Disallow: /docs/4*/
Disallow: /docs/5*/
Disallow: /docs/6*/
Disallow: /docs/7*/
Disallow: /docs/8*/
Disallow: /docs/9*/
Disallow: /blog*