TITLE: Defining Time Windows in Apache Flink Scala API
DESCRIPTION: This snippet demonstrates how to create tumbling and sliding time windows on a DataStream of vehicle counts. It shows keying the stream by sensor ID and applying time-based windows with sum aggregation.

LANGUAGE: scala
CODE:
// Stream of (sensorId, carCnt)
val vehicleCnts: DataStream[(Int, Int)] = ...

val tumblingCnts: DataStream[(Int, Int)] = vehicleCnts
  // key stream by sensorId
  .keyBy(0) 
  // tumbling time window of 1 minute length
  .timeWindow(Time.minutes(1))
  // compute sum over carCnt
  .sum(1) 

val slidingCnts: DataStream[(Int, Int)] = vehicleCnts
  .keyBy(0) 
  // sliding time window of 1 minute length and 30 secs trigger interval
  .timeWindow(Time.minutes(1), Time.seconds(30))
  .sum(1)

----------------------------------------

TITLE: Creating Kafka Source Table in Flink SQL
DESCRIPTION: DDL statement to create a table connecting to Kafka topic with JSON format data, including processing time and watermark definitions

LANGUAGE: sql
CODE:
CREATE TABLE user_behavior (
    user_id BIGINT,
    item_id BIGINT,
    category_id BIGINT,
    behavior STRING,
    ts TIMESTAMP(3),
    proctime AS PROCTIME(),
    WATERMARK FOR ts AS ts - INTERVAL '5' SECOND
) WITH (
    'connector' = 'kafka',
    'topic' = 'user_behavior',
    'scan.startup.mode' = 'earliest-offset',
    'properties.bootstrap.servers' = 'kafka:9094',
    'format' = 'json'
);

----------------------------------------

TITLE: Sessionizing Clickstream with DataStream API in Java
DESCRIPTION: Example showing how to process a clickstream to count clicks per session using Flink's DataStream API. Demonstrates mapping, keying, and windowing operations.

LANGUAGE: java
CODE:
DataStream<Tuple2<String, Long>> result = clicks
  // project clicks to userId and add a 1 for counting
  .map(
    // define function by implementing the MapFunction interface.
    new MapFunction<Click, Tuple2<String, Long>>() {
      @Override
      public Tuple2<String, Long> map(Click click) {
        return Tuple2.of(click.userId, 1L);
      }
    })
  // key by userId (field 0)
  .keyBy(0)
  // define session window with 30 minute gap
  .window(EventTimeSessionWindows.withGap(Time.minutes(30L)))
  // count clicks per session. Define function as lambda function.
  .reduce((a, b) -> Tuple2.of(a.f0, a.f1 + b.f1));

----------------------------------------

TITLE: Configuring Execution Mode in Flink Java API
DESCRIPTION: Reference to the ExecutionMode enum in Flink's Java API that allows switching between BATCH and PIPELINED execution modes for fault tolerance behavior configuration.

LANGUAGE: java
CODE:
org.apache.flink.api.common.ExecutionMode.BATCH
org.apache.flink.api.common.ExecutionMode.PIPELINED

----------------------------------------

TITLE: TPCDS Query3 SQL Implementation
DESCRIPTION: SQL query performing analytics on store sales data with joins, filters, aggregations and ordering

LANGUAGE: sql
CODE:
SELECT dt.d_year, item.i_brand_id brand_id, item.i_brand brand,SUM(ss_ext_sales_price) sum_agg
FROM  date_dim dt, store_sales, item
WHERE dt.d_date_sk = store_sales.ss_sold_date_sk
AND store_sales.ss_item_sk = item.i_item_sk
AND item.i_manufact_id = 128
AND dt.d_moy=11
GROUP BY dt.d_year, item.i_brand, item.i_brand_id
ORDER BY dt.d_year, sum_agg desc, brand_id
LIMIT 100

----------------------------------------

TITLE: Implementing Word Count with Batch Execution in Flink's DataStream API
DESCRIPTION: A complete example of a word count program using Flink's DataStream API, which can be run in both streaming and batch modes. It demonstrates file source configuration, windowing, and late data handling.

LANGUAGE: java
CODE:
public class WindowWordCount {
	private static final OutputTag<String[]> LATE_DATA = new OutputTag<>(
		"late-data",
		BasicArrayTypeInfo.STRING_ARRAY_TYPE_INFO);

	public static void main(String[] args) throws Exception {

		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
		ParameterTool config = ParameterTool.fromArgs(args);

		Path path = new Path(config.get("path"));
		SingleOutputStreamOperator<Tuple4<String, Integer, String, String>> dataStream = env
			.fromSource(
				FileSource.forRecordStreamFormat(new TsvFormat(), path).build(),
				WatermarkStrategy.<String[]>forBoundedOutOfOrderness(Duration.ofDays(1))
					.withTimestampAssigner(new OrderTimestampAssigner()),
				"Text file"
			)
			.keyBy(value -> value[4]) // group by currency
			.window(TumblingEventTimeWindows.of(Time.days(1)))
			.sideOutputLateData(LATE_DATA)
			.aggregate(
				new CountFunction(), // count number of orders in a given currency
				new CombineWindow());

		int i = 0;
		DataStream<String[]> lateData = dataStream.getSideOutput(LATE_DATA);
		try (CloseableIterator<String[]> results = lateData.executeAndCollect()) {
			while (results.hasNext()) {
				String[] late = results.next();
				if (i < 100) {
					System.out.println(Arrays.toString(late));
				}
				i++;
			}
		}
		System.out.println("Number of late records: " + i);

		try (CloseableIterator<Tuple4<String, Integer, String, String>> results 
				= dataStream.executeAndCollect()) {
			while (results.hasNext()) {
				System.out.println(results.next());
			}
		}
	}
}

----------------------------------------

TITLE: Creating a Materialized Table in Flink SQL
DESCRIPTION: Example of creating a materialized table in Flink SQL with a data freshness specification and a query. It also shows how to pause, resume, and manually refresh partitions of the materialized table.

LANGUAGE: SQL
CODE:
CREATE MATERIALIZED TABLE dwd_orders
(
 PRIMARY KEY(ds, id) NOT ENFORCED
)
PARTITIONED BY (ds)
FRESHNESS = INTERVAL '3' MINUTE
AS SELECT 
 o.ds
 o.id,
 o.order_number,
 o.user_id,
...
FROM 
 orders as o
 LEFT JOIN products FOR SYSTEM_TIME AS OF proctime() AS prod
 ON o.product_id = prod.id
 LEFT JOIN order_pay AS pay
 ON o.id = pay.order_id and o.ds = pay.ds;

ALTER MATERIALIZED TABLE dwd_orders SUSPEND;

ALTER MATERIALIZED TABLE dwd_orders RESUME
WITH(
 'sink.parallesim' = '10'
);

ALTER MATERIALIZED TABLE dwd_orders REFRESH PARTITION(ds='20231023');

----------------------------------------

TITLE: MNIST Classifier Implementation with GPU Acceleration
DESCRIPTION: Java implementation of MNIST classifier using JCuda for GPU-accelerated matrix multiplication. The code demonstrates how to access GPU resources and perform inference using Flink's External Resource Framework.

LANGUAGE: java
CODE:
class MNISTClassifier extends RichMapFunction<List<Float>, Integer> {
    @Override
    public void open(Configuration parameters) {
        // Get the GPU information and select the first GPU.
        final Set<ExternalResourceInfo> externalResourceInfos = getRuntimeContext().getExternalResourceInfos(resourceName);
        final Optional<String> firstIndexOptional = externalResourceInfos.iterator().next().getProperty("index");

        // Initialize JCublas with the selected GPU
        JCuda.cudaSetDevice(Integer.parseInt(firstIndexOptional.get()));
        JCublas.cublasInit();
    }

   @Override
   public Integer map(List<Float> value) {
       // Performs multiplication using JCublas. The matrixPointer points to our pre-trained model.
       JCublas.cublasSgemv('n', DIMENSIONS.f1, DIMENSIONS.f0, 1.0f,
               matrixPointer, DIMENSIONS.f1, inputPointer, 1, 0.0f, outputPointer, 1);

       // Read the result back from GPU.
       JCublas.cublasGetVector(DIMENSIONS.f1, Sizeof.FLOAT, outputPointer, 1, Pointer.to(output), 1);
       int result = 0;
       for (int i = 0; i < DIMENSIONS.f1; ++i) {
           result = output[i] > output[result] ? i : result;
       }
       return result;
   }
}

----------------------------------------

TITLE: Creating Kafka Table and ElasticSearch Table with SQL DDL in Flink
DESCRIPTION: This snippet demonstrates how to create tables in Flink SQL, one backed by a Kafka topic and another by ElasticSearch. It also includes a query that analyzes order data from Kafka and writes results into ElasticSearch.

LANGUAGE: SQL
CODE:
-- Define a table called orders that is backed by a Kafka topic
-- The definition includes all relevant Kafka properties,
-- the underlying format (JSON) and even defines a
-- watermarking algorithm based on one of the fields
-- so that this table can be used with event time.
CREATE TABLE orders (
	user_id    BIGINT,
	product    STRING,
	order_time TIMESTAMP(3),
	WATERMARK FOR order_time AS order_time - '5' SECONDS
) WITH (
	'connector.type'    	 = 'kafka',
	'connector.version' 	 = 'universal',
	'connector.topic'   	 = 'orders',
	'connector.startup-mode' = 'earliest-offset',
	'connector.properties.bootstrap.servers' = 'localhost:9092',
	'format.type' = 'json' 
);

-- Define a table called product_analysis
-- on top of ElasticSearch 7 where we 
-- can write the results of our query. 
CREATE TABLE product_analysis (
	product 	STRING,
	tracking_time 	TIMESTAMP(3),
	units_sold 	BIGINT
) WITH (
	'connector.type'    = 'elasticsearch',
	'connector.version' = '7',
	'connector.hosts'   = 'localhost:9200',
	'connector.index'   = 'ProductAnalysis',
	'connector.document.type' = 'analysis' 
);

-- A simple query that analyzes order data
-- from Kafka and writes results into 
-- ElasticSearch. 
INSERT INTO product_analysis
SELECT
	product_id,
	TUMBLE_START(order_time, INTERVAL '1' DAY) as tracking_time,
	COUNT(*) as units_sold
FROM orders
GROUP BY
	product_id,
	TUMBLE(order_time, INTERVAL '1' DAY);

----------------------------------------

TITLE: Using Catalog-Related Syntax in Flink SQL
DESCRIPTION: Examples of using new catalog-related syntax in Flink SQL to create, describe, and modify catalogs.

LANGUAGE: SQL
CODE:
CREATE CATALOG `cat` WITH ('type'='generic_in_memory', 'default-database'='db');

SHOW CREATE CATALOG `cat`;

DESCRIBE CATALOG `cat`;

ALTER CATALOG `cat` SET ('default-database'='new-db');

SHOW CREATE CATALOG `cat`;

----------------------------------------

TITLE: Migrating from FlinkKinesisConsumer to KinesisStreamsSource in Java
DESCRIPTION: Code example showing migration from legacy FlinkKinesisConsumer to new KinesisStreamsSource. Demonstrates configuration of stream position, watermark strategy, and source builder pattern.

LANGUAGE: java
CODE:
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

// Old FlinkKinesisConsumer to read from stream test-stream from TRIM_HORIZON
Properties consumerConfig = new Properties();
consumerConfig.put(AWSConfigConstants.AWS_REGION, "us-east-1");
consumerConfig.put(ConsumerConfigConstants.STREAM_INITIAL_POSITION, "TRIM_HORIZON");
FlinkKinesisConsumer<String> oldKinesisConsumer = 
    new FlinkKinesisConsumer<>("test-stream", new SimpleStringSchema(), consumerConfig);
DataStream<String> kinesisRecordsFromOldKinesisConsumer = env.addSource(oldKinesisConsumer)
    .uid("custom-uid")
    .assignTimestampsAndWatermarks(
        WatermarkStrategy.<String>forMonotonousTimestamps().withIdleness(Duration.ofSeconds(1)))

// New KinesisStreamsSource to read from stream test-stream from TRIM_HORIZON
Configuration sourceConfig = new Configuration();
sourceConfig.set(KinesisSourceConfigOptions.STREAM_INITIAL_POSITION, KinesisSourceConfigOptions.InitialPosition.TRIM_HORIZON); 
KinesisStreamsSource<String> newKdsSource =
    KinesisStreamsSource.<String>builder()
        .setStreamArn("arn:aws:kinesis:us-east-1:123456789012:stream/test-stream")
        .setSourceConfig(sourceConfig)
        .setDeserializationSchema(new SimpleStringSchema())
        .build();
DataStream<String> kinesisRecordsWithEventTimeWatermarks = env.fromSource(
    kdsSource, 
    WatermarkStrategy.<String>forMonotonousTimestamps().withIdleness(Duration.ofSeconds(1)), 
    "Kinesis source")
        .returns(TypeInformation.of(String.class))
        .uid("custom-uid");

----------------------------------------

TITLE: Setting State TTL for Join and Aggregation in Flink SQL
DESCRIPTION: Shows how to use the STATE_TTL hint to specify custom time-to-live (TTL) values for state in regular joins and group aggregations directly within SQL queries.

LANGUAGE: sql
CODE:
SELECT /*+ STATE_TTL('Orders'= '1d', 'Customers' = '20d') */ *
FROM Orders LEFT OUTER JOIN Customers
    ON Orders.o_custkey = Customers.c_custkey;

SELECT /*+ STATE_TTL('o' = '1d') */ o_orderkey, SUM(o_totalprice) AS revenue
FROM Orders AS o
GROUP BY o_orderkey;

----------------------------------------

TITLE: Calculating Hourly Trading Volume
DESCRIPTION: SQL query using tumbling window to calculate hourly purchase counts and insert results into Elasticsearch

LANGUAGE: sql
CODE:
INSERT INTO buy_cnt_per_hour
SELECT HOUR(TUMBLE_START(ts, INTERVAL '1' HOUR)), COUNT(*)
FROM user_behavior
WHERE behavior = 'buy'
GROUP BY TUMBLE(ts, INTERVAL '1' HOUR);

----------------------------------------

TITLE: IO Scheduling Algorithm for Flink Sort-Based Shuffle
DESCRIPTION: Pseudocode implementation of the IO scheduling algorithm used in Flink's sort-based blocking shuffle. The algorithm implements an elevator-like approach to optimize sequential disk reads across multiple data regions and readers.

LANGUAGE: Java
CODE:
// let data_regions as the data region list indexed from 0 to n - 1
// let data_readers as the concurrent downstream data readers queue indexed from 0 to m - 1
for (data_region in data_regions) {
    data_reader = poll_reader_of_the_smallest_file_offset(data_readers);
    if (data_reader == null)
        break;
    reading_buffers = request_reading_buffers();
    if (reading_buffers.isEmpty())
        break;
    read_data(data_region, data_reader, reading_buffers);
}

----------------------------------------

TITLE: Implementing Windowed Aggregates with Table API and SQL in Flink
DESCRIPTION: Example showing equivalent Table API and SQL queries to compute windowed aggregates on a stream of temperature sensor measurements. The code demonstrates how to set up the execution environment, register a table source, and define a 1-hour tumbling window aggregate.

LANGUAGE: scala
CODE:
val env = StreamExecutionEnvironment.getExecutionEnvironment
env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)

val tEnv = TableEnvironment.getTableEnvironment(env)

// define a table source to read sensor data (sensorId, time, room, temp)
val sensorTable = ??? // can be a CSV file, Kafka topic, database, or ...
// register the table source
tEnv.registerTableSource("sensors", sensorTable)

// Table API
val tapiResult: Table = tEnv.scan("sensors")   // scan sensors table
 .window(Tumble over 1.hour on 'rowtime as 'w) // define 1-hour window
 .groupBy('w, 'room)                           // group by window and room
 .select('room, 'w.end, 'temp.avg as 'avgTemp) // compute average temperature

// SQL
val sqlResult: Table = tEnv.sql("""
 |SELECT room, TUMBLE_END(rowtime, INTERVAL '1' HOUR), AVG(temp) AS avgTemp
 |FROM sensors
 |GROUP BY TUMBLE(rowtime, INTERVAL '1' HOUR), room
 |""".stripMargin)

----------------------------------------

TITLE: Implementing DynamicKeyFunction with Broadcast State in Java
DESCRIPTION: Shows the implementation of DynamicKeyFunction that uses broadcast state to dynamically update and apply rules for data partitioning. It handles both transaction processing and rule updates.

LANGUAGE: Java
CODE:
public class DynamicKeyFunction
    extends BroadcastProcessFunction<Transaction, Rule, Keyed<Transaction, String, Integer>> {


  @Override
  public void processBroadcastElement(Rule rule,
                                     Context ctx,
                                     Collector<Keyed<Transaction, String, Integer>> out) {
    BroadcastState<Integer, Rule> broadcastState = ctx.getBroadcastState(RULES_STATE_DESCRIPTOR);
    broadcastState.put(rule.getRuleId(), rule);
  }

  @Override
  public void processElement(Transaction event,
                           ReadOnlyContext ctx,
                           Collector<Keyed<Transaction, String, Integer>> out){
    ReadOnlyBroadcastState<Integer, Rule> rulesState =
                                  ctx.getBroadcastState(RULES_STATE_DESCRIPTOR);
    for (Map.Entry<Integer, Rule> entry : rulesState.immutableEntries()) {
        final Rule rule = entry.getValue();
        out.collect(
          new Keyed<>(
            event, KeysExtractor.getKey(rule.getGroupingKeyNames(), event), rule.getRuleId()));
    }
  }
}

----------------------------------------

TITLE: Implementing RackFun in Flink Stateful Functions
DESCRIPTION: This snippet demonstrates the RackFun class, which correlates incidents from multiple servers within a rack. It maintains a mapping of server IDs to open incidents and notifies the DataCenterFun about correlated rack alerts.

LANGUAGE: Java
CODE:
@FunctionType(type = "datacenter/rack")
public final class RackFun implements StatefulFunction {
    private final ValueSpec<Map<ServerId, Set<Incident>>> serverIncidents =
        ValueSpec.named("incidents").withMapType(ServerId.class, new TypeName<Set<Incident>>() {});

    @Override
    public void invoke(Context context, Object input) {
        if (input instanceof RackIncidentReport) {
            onRackIncidentReport(context, (RackIncidentReport) input);
        }
    }

    private void onRackIncidentReport(Context context, RackIncidentReport report) {
        Map<ServerId, Set<Incident>> incidents = context.storage().get(serverIncidents).orElse(new HashMap<>());
        incidents.put(report.serverId(), report.incidents());
        
        Set<Incident> correlatedIncidents = correlateIncidents(incidents);
        if (!correlatedIncidents.isEmpty()) {
            context.send(dataCenterAddress(), new DataCenterIncidentReport(context.self(), correlatedIncidents));
        }
        
        context.storage().set(serverIncidents, incidents);
    }
}

----------------------------------------

TITLE: Creating Temperature Warning Pattern with Flink CEP API
DESCRIPTION: Defines a pattern to detect two consecutive high-temperature events within 10 seconds using Flink's Pattern API.

LANGUAGE: java
CODE:
Pattern<MonitoringEvent, ?> warningPattern = Pattern.<MonitoringEvent>begin("First Event")
    .subtype(TemperatureEvent.class)
    .where(evt -> evt.getTemperature() >= TEMPERATURE_THRESHOLD)
    .next("Second Event")
    .subtype(TemperatureEvent.class)
    .where(evt -> evt.getTemperature() >= TEMPERATURE_THRESHOLD)
    .within(Time.seconds(10));

----------------------------------------

TITLE: Implementing Stateful KeyedProcessFunction in Java
DESCRIPTION: Example implementation of a KeyedProcessFunction that matches START and END events, computes time duration between them, and handles state cleanup via timers. Demonstrates state handling and timer usage in Flink.

LANGUAGE: java
CODE:
public static class StartEndDuration
  extends KeyedProcessFunction<String, Tuple2<String, String>, Tuple2<String, Long>> {

  private ValueState<Long> startTime;
  
  @Override
  public void open(Configuration conf) {
    // obtain state handle
    startTime = getRuntimeContext()
      .getState(new ValueStateDescriptor<Long>("startTime", Long.class));
  }

  /** Called for each processed event. */
  @Override
  public void processElement(
      Tuple2<String, String> in,
      Context ctx,
      Collector<Tuple2<String, Long>> out) throws Exception {
  
      switch (in.f1) {
        case "START":
          // set the start time if we receive a start event.
          startTime.update(ctx.timestamp());
          // register a timer in four hours from the start event.
          ctx.timerService()
            .registerEventTimeTimer(ctx.timestamp() + 4 * 60 * 60 * 1000);
          break;
        case "END":
          // emit the duration between start and end event
          Long sTime = startTime.value();
          if (sTime != null) {
            out.collect(Tuple2.of(in.f0, ctx.timestamp() - sTime));
            // clear the state
            startTime.clear();
          }
        default:
          // do nothing
      }
  }

  /** Called when a timer fires. */
  @Override
  public void onTimer(
    long timestamp,
    OnTimerContext ctx,
    Collector<Tuple2<String, Long>> out) {

    // Timeout interval exceeded. Cleaning up the state.
    startTime.clear();
  }
}

----------------------------------------

TITLE: Defining Various Window Types in Flink's Table API using Scala
DESCRIPTION: This snippet demonstrates how to define different types of windows (tumbling, sliding, and session) using Flink's Table API. It shows both processing-time and event-time windows, as well as count-based windows.

LANGUAGE: scala
CODE:
// using processing-time
table.window(Tumble over 100.rows as 'manyRowWindow)
// using event-time
table.window(Session withGap 15.minutes on 'rowtime as 'sessionWindow)
table.window(Slide over 1.day every 1.hour on 'rowtime as 'dailyWindow)

----------------------------------------

TITLE: Cross-Catalog Query for Customer Orders Analysis
DESCRIPTION: Complex SQL query joining tables from multiple catalogs to analyze customer orders by region and priority.

LANGUAGE: sql
CODE:
USE CATALOG postgres;
SELECT
  r_name AS `region`,
  o_orderpriority AS `priority`,
  COUNT(DISTINCT c_custkey) AS `number_of_customers`,
  COUNT(o_orderkey) AS `number_of_orders`
FROM `hive`.`default`.dev_orders
JOIN prod_customer ON o_custkey = c_custkey
JOIN prod_nation ON c_nationkey = n_nationkey
JOIN prod_region ON n_regionkey = r_regionkey
WHERE
  FLOOR(o_ordertime TO DAY) = TIMESTAMP '2020-04-01 0:00:00.000'
  AND NOT o_orderpriority = '4-NOT SPECIFIED'
GROUP BY r_name, o_orderpriority
ORDER BY r_name, o_orderpriority;

----------------------------------------

TITLE: Breaking API Changes Example
DESCRIPTION: Example of removed DataSet API usage that needs migration to DataStream API

LANGUAGE: java
CODE:
// Old DataSet API - removed in 2.0
DataSet<String> data = env.fromElements("a", "b", "c");

// New DataStream API
DataStream<String> data = env.fromElements("a", "b", "c");

----------------------------------------

TITLE: Initializing Table Environment and Configuring CSV Table Source in Scala
DESCRIPTION: This snippet demonstrates how to set up a StreamExecutionEnvironment, create a TableEnvironment, and configure a CsvTableSource for reading customer data from a CSV file. It also shows how to register the table source and define a simple table program with filtering and selection operations.

LANGUAGE: scala
CODE:
// set up execution environment
val env = StreamExecutionEnvironment.getExecutionEnvironment
val tEnv = TableEnvironment.getTableEnvironment(env)

// configure table source
val customerSource = CsvTableSource.builder()
  .path("/path/to/customer_data.csv")
  .ignoreFirstLine()
  .fieldDelimiter("|")
  .field("id", Types.LONG)
  .field("name", Types.STRING)
  .field("last_update", Types.TIMESTAMP)
  .field("prefs", Types.STRING)
  .build()

// name your table source
tEnv.registerTableSource("customers", customerSource)

// define your table program
val table = tEnv
  .scan("customers")
  .filter('name.isNotNull && 'last_update > "2016-01-01 00:00:00".toTimestamp)
  .select('id, 'name.lowerCase(), 'prefs)

// convert it to a data stream
val ds = table.toDataStream[Row]

ds.print()
env.execute()

----------------------------------------

TITLE: Dynamic Alert Function Implementation
DESCRIPTION: Main processing logic for evaluating transactions against rules and generating alerts in a custom window implementation.

LANGUAGE: java
CODE:
public class DynamicAlertFunction
    extends KeyedBroadcastProcessFunction<
        String, Keyed<Transaction, String, Integer>, Rule, Alert> {

  private transient MapState<Long, Set<Transaction>> windowState;

  @Override
  public void processElement(
      Keyed<Transaction, String, Integer> value, ReadOnlyContext ctx, Collector<Alert> out){

    long currentEventTime = value.getWrapped().getEventTime();                           
    addToStateValuesSet(windowState, currentEventTime, value.getWrapped());

    Rule rule = ctx.getBroadcastState(Descriptors.rulesDescriptor).get(value.getId());    
    Long windowStartTimestampForEvent = rule.getWindowStartTimestampFor(currentEventTime);

    SimpleAccumulator<BigDecimal> aggregator = RuleHelper.getAggregator(rule);            
    for (Long stateEventTime : windowState.keys()) {
      if (isStateValueInWindow(stateEventTime, windowStartForEvent, currentEventTime)) {
        aggregateValuesInState(stateEventTime, aggregator, rule);
      }
    }

    BigDecimal aggregateResult = aggregator.getLocalValue();                              
    boolean isRuleViolated = rule.apply(aggregateResult);
    if (isRuleViolated) {
      long decisionTime = System.currentTimeMillis();
      out.collect(new Alert<>(rule.getRuleId(),
                              rule,
                              value.getKey(),
                              decisionTime,
                              value.getWrapped(),
                              aggregateResult));
    }

    long cleanupTime = (currentEventTime / 1000) * 1000;                                  
    ctx.timerService().registerEventTimeTimer(cleanupTime);
  }

----------------------------------------

TITLE: Implementing State Registration in Apache Flink
DESCRIPTION: Shows how to register state using ValueStateDescriptor with TypeInformation in a KeyedProcessFunction.

LANGUAGE: java
CODE:
public class MyFunction extends KeyedProcessFunction<Key, Input, Output> {

  private transient ValueState<MyState> valueState;

  public void open(Configuration parameters) {
    ValueStateDescriptor<MyState> descriptor =
      new ValueStateDescriptor<>("my-state", TypeInformation.of(MyState.class));

    valueState = getRuntimeContext().getState(descriptor);
  }
}

----------------------------------------

TITLE: Initializing Temporal Table Function in Java
DESCRIPTION: Example of creating and registering a temporal table function in Flink using Java. The code demonstrates setting up stream and table environments, creating a sample rates history dataset, and defining a temporal table function with versioning field and primary key.

LANGUAGE: java
CODE:
import org.apache.flink.table.functions.TemporalTableFunction;

(...)

// Get the stream and table environments.
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
StreamTableEnvironment tEnv = StreamTableEnvironment.getTableEnvironment(env);

// Provide a sample static data set of the rates history table.
List <Tuple2<String, Long>>ratesHistoryData =new ArrayList<>();

ratesHistoryData.add(Tuple2.of("USD", 102L)); 
ratesHistoryData.add(Tuple2.of("EUR", 114L)); 
ratesHistoryData.add(Tuple2.of("YEN", 1L)); 
ratesHistoryData.add(Tuple2.of("EUR", 116L)); 
ratesHistoryData.add(Tuple2.of("USD", 105L));

// Create and register an example table using the sample data set.
DataStream<Tuple2<String, Long>> ratesHistoryStream = env.fromCollection(ratesHistoryData);

Table ratesHistory = tEnv.fromDataStream(ratesHistoryStream, "r_currency, r_rate, r_proctime.proctime");

tEnv.registerTable("RatesHistory", ratesHistory);

// Create and register the temporal table function "rates".
// Define "r_proctime" as the versioning field and "r_currency" as the primary key.
TemporalTableFunction rates = ratesHistory.createTemporalTableFunction("r_proctime", "r_currency");

tEnv.registerFunction("Rates", rates);

(...)

----------------------------------------

TITLE: Initializing Data Streams and KeyedStream
DESCRIPTION: Shows the basic setup of Action and Pattern data streams along with keying the action stream by userId.

LANGUAGE: java
CODE:
DataStream<Action> actions = ???
DataStream<Pattern> patterns = ???

KeyedStream<Action, Long> actionsByUser = actions
  .keyBy((KeySelector<Action, Long>) action -> action.userId);

----------------------------------------

TITLE: Implementing a Join Operation with Batch Execution in Flink's DataStream API
DESCRIPTION: Demonstrates how to implement a join operation between two data streams using Flink's DataStream API. This example shows joining orders with customer information and can be run in both streaming and batch modes.

LANGUAGE: java
CODE:
DataStreamSource<String[]> orders = env
    .fromSource(
        FileSource.forRecordStreamFormat(new TsvFormat(), ordersPath).build(),
        WatermarkStrategy.<String[]>noWatermarks()
            .withTimestampAssigner((record, previous) -> -1),
        "Text file"
    );

Path customersPath = new Path(config.get("customers"));
DataStreamSource<String[]> customers = env
    .fromSource(
        FileSource.forRecordStreamFormat(new TsvFormat(), customersPath).build(),
        WatermarkStrategy.<String[]>noWatermarks()
            .withTimestampAssigner((record, previous) -> -1),
        "Text file"
    );

DataStream<Tuple2<String, String>> dataStream = orders.join(customers)
    .where(order -> order[1]).equalTo(customer -> customer[0]) // join on customer id
    .window(GlobalWindows.create())
    .trigger(ContinuousProcessingTimeTrigger.of(Time.seconds(5)))
    .apply(new ProjectFunction());

----------------------------------------

TITLE: Implementing AsyncSinkWriter Abstract Class in Java
DESCRIPTION: This code snippet shows the key methods of the AsyncSinkWriter abstract class that need to be implemented when creating a custom sink.

LANGUAGE: Java
CODE:
public abstract class AsyncSinkWriter<InputT, RequestEntryT extends Serializable>
        implements StatefulSink.StatefulSinkWriter<InputT, BufferedRequestState<RequestEntryT>> {
    // ...
    protected abstract void submitRequestEntries(
            List<RequestEntryT> requestEntries, Consumer<List<RequestEntryT>> requestResult);
    protected abstract long getSizeInBytes(RequestEntryT requestEntry);
    // ...
}

----------------------------------------

TITLE: Neighborhood Aggregation in Java
DESCRIPTION: Code demonstrating how to compute the sum of values for incoming neighbors of each vertex using the reduceOnNeighbors() method.

LANGUAGE: java
CODE:
graph.reduceOnNeighbors(new SumValues(), EdgeDirection.IN);

----------------------------------------

TITLE: Connecting Broadcast Rules Stream to Main Transaction Stream in Java
DESCRIPTION: Demonstrates how to create a broadcast stream from a rules update stream and connect it to the main transaction stream. This enables dynamic rule updates in the fraud detection pipeline.

LANGUAGE: Java
CODE:
// Streams setup
DataStream<Transaction> transactions = [...]
DataStream<Rule> rulesUpdateStream = [...]

BroadcastStream<Rule> rulesStream = rulesUpdateStream.broadcast(RULES_STATE_DESCRIPTOR);

// Processing pipeline setup
 DataStream<Alert> alerts =
     transactions
         .connect(rulesStream)
         .process(new DynamicKeyFunction())
         .keyBy((keyed) -> keyed.getKey())
         .connect(rulesStream)
         .process(new DynamicAlertFunction())

----------------------------------------

TITLE: Catalog and Table Navigation Commands
DESCRIPTION: SQL commands demonstrating how to list and navigate between catalogs, databases, and tables in Flink SQL.

LANGUAGE: sql
CODE:
show catalogs;
use catalog hive;
show databases;
show tables;
use catalog postgres;
show tables;
describe prod_customer

----------------------------------------

TITLE: Creating Elasticsearch Sink Table for Hourly Metrics
DESCRIPTION: DDL statement to create an Elasticsearch table for storing hourly trading volume statistics

LANGUAGE: sql
CODE:
CREATE TABLE buy_cnt_per_hour (
    hour_of_day BIGINT,
    buy_cnt BIGINT
) WITH (
    'connector' = 'elasticsearch-7',
    'hosts' = 'http://elasticsearch:9200',
    'index' = 'buy_cnt_per_hour'
);

----------------------------------------

TITLE: Python UDF with Third-party Dependencies
DESCRIPTION: Example showing how to use and manage third-party dependencies (mpmath library) in Python UDFs.

LANGUAGE: python
CODE:
@udf(input_types=[DataTypes.BIGINT(), DataTypes.BIGINT()], result_type=DataTypes.BIGINT())
def add(i, j):
    from mpmath import fadd # add third-party dependency
    return int(fadd(i, j))

LANGUAGE: bash
CODE:
$ cd /tmp
$ echo mpmath==1.1.0 > requirements.txt
$ pip download -d cached_dir -r requirements.txt --no-binary :all:

LANGUAGE: python
CODE:
t_env.set_python_requirements("/tmp/requirements.txt", "/tmp/cached_dir")

----------------------------------------

TITLE: Complete PyFlink Pandas UDF Implementation Example
DESCRIPTION: Full example showing how to set up a PyFlink environment, register a Pandas UDF, and process data using file system connectors.

LANGUAGE: python
CODE:
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment, DataTypes
from pyflink.table.udf import udf
import pandas as pd

env = StreamExecutionEnvironment.get_execution_environment()
env.set_parallelism(1)
t_env = StreamTableEnvironment.create(env)
t_env.get_config().get_configuration().set_boolean("python.fn-execution.memory.managed", True)

@udf(input_types=[DataTypes.STRING(), DataTypes.FLOAT()],
     result_type=DataTypes.FLOAT(), udf_type='pandas')
def interpolate(id, temperature):
    # takes id: pandas.Series and temperature: pandas.Series as input
    df = pd.DataFrame({'id': id, 'temperature': temperature})

    # use interpolate() to interpolate the missing temperature
    interpolated_df = df.groupby('id').apply(
        lambda group: group.interpolate(limit_direction='both'))

    # output temperature: pandas.Series
    return interpolated_df['temperature']

t_env.register_function("interpolate", interpolate)

my_source_ddl = """
    create table mySource (
        id INT,
        temperature FLOAT 
    ) with (
        'connector.type' = 'filesystem',
        'format.type' = 'csv',
        'connector.path' = '/tmp/input'
    )
"""

my_sink_ddl = """
    create table mySink (
        id INT,
        temperature FLOAT 
    ) with (
        'connector.type' = 'filesystem',
        'format.type' = 'csv',
        'connector.path' = '/tmp/output'
    )
"""

t_env.execute_sql(my_source_ddl)
t_env.execute_sql(my_sink_ddl)

t_env.from_path('mySource')\
    .select("id, interpolate(id, temperature) as temperature") \
    .insert_into('mySink')

t_env.execute("pandas_udf_demo")

----------------------------------------

TITLE: SQL Windowed Outer Equi-Join Query
DESCRIPTION: Example of a windowed outer equi-join SQL query in Flink that joins Departures and Arrivals tables based on rideId within a 2-hour time window.

LANGUAGE: sql
CODE:
SELECT d.rideId, d.departureTime, a.arrivalTime
FROM Departures d LEFT OUTER JOIN Arrivals a
  ON d.rideId = a.rideId
  AND a.arrivalTime BETWEEN 
      d.deptureTime AND d.departureTime + '2' HOURS

----------------------------------------

TITLE: Tumbling Window Aggregation in SQL
DESCRIPTION: SQL query example showing how to perform tumbling window aggregation on streaming data using Calcite's proposed streaming SQL syntax.

LANGUAGE: sql
CODE:
SELECT STREAM 
  TUMBLE_END(time, INTERVAL '1' DAY) AS day, 
  location AS room, 
  AVG((tempF - 32) * 0.556) AS avgTempC
FROM sensorData
WHERE location LIKE 'room%'
GROUP BY TUMBLE(time, INTERVAL '1' DAY), location

----------------------------------------

TITLE: Creating Postgres and Hive Catalogs in Flink SQL
DESCRIPTION: SQL commands to create catalog connections to Postgres and Hive metastores with their respective configuration parameters.

LANGUAGE: sql
CODE:
CREATE CATALOG postgres WITH (
    'type'='jdbc',
    'property-version'='1',
    'base-url'='jdbc:postgresql://postgres:5432/',
    'default-database'='postgres',
    'username'='postgres',
    'password'='example'
);

CREATE CATALOG hive WITH (
    'type'='hive',
    'property-version'='1',
    'hive-version'='2.3.6',
    'hive-conf-dir'='/opt/hive-conf'
);

----------------------------------------

TITLE: Implementing Pattern Evaluator
DESCRIPTION: Complete implementation of PatternEvaluator class that extends KeyedBroadcastProcessFunction to process actions and patterns.

LANGUAGE: java
CODE:
public static class PatternEvaluator
    extends KeyedBroadcastProcessFunction<Long, Action, Pattern, Tuple2<Long, Pattern>> {
 
  // handle for keyed state (per user)
  ValueState<String> prevActionState;
  // broadcast state descriptor
  MapStateDescriptor<Void, Pattern> patternDesc;
 
  @Override
  public void open(Configuration conf) {
    // initialize keyed state
    prevActionState = getRuntimeContext().getState(
      new ValueStateDescriptor<>("lastAction", Types.STRING));
    patternDesc = 
      new MapStateDescriptor<>("patterns", Types.VOID, Types.POJO(Pattern.class));
  }

  @Override
  public void processElement(
     Action action, 
     ReadOnlyContext ctx, 
     Collector<Tuple2<Long, Pattern>> out) throws Exception {
   Pattern pattern = ctx
     .getBroadcastState(this.patternDesc)
     .get(null);
   String prevAction = prevActionState.value();
   if (pattern != null && prevAction != null) {
     if (pattern.firstAction.equals(prevAction) && 
         pattern.secondAction.equals(action.action)) {
       out.collect(new Tuple2<>(ctx.getCurrentKey(), pattern));
     }
   }
   prevActionState.update(action.action);
 }

 @Override
 public void processBroadcastElement(
     Pattern pattern, 
     Context ctx, 
     Collector<Tuple2<Long, Pattern>> out) throws Exception {
   BroadcastState<Void, Pattern> bcState = ctx.getBroadcastState(patternDesc);
   bcState.put(null, pattern);
 }
}

----------------------------------------

TITLE: Implementing Sort Buffer Interface in Java for Flink Shuffle
DESCRIPTION: Interface definition for the sort buffer component in Flink's sort-based blocking shuffle. This interface handles data appending, copying, and management of records in memory with methods for tracking buffer state and resource management.

LANGUAGE: Java
CODE:
public interface SortBuffer {
    /** Appends data of the specified channel to this SortBuffer. */
    boolean append(ByteBuffer source, int targetChannel, Buffer.DataType dataType) throws IOException;

    /** Copies data in this SortBuffer to the target MemorySegment. */
    BufferWithChannel copyIntoSegment(MemorySegment target);

    long numRecords();

    long numBytes();

    boolean hasRemaining();

    void finish();

    boolean isFinished();

    void release();

    boolean isReleased();
}

----------------------------------------

TITLE: Testing Stateful FlatMap Operator in Flink
DESCRIPTION: Implementation and testing of a stateful FlatMapFunction using KeyedOperatorHarness to test state management.

LANGUAGE: java
CODE:
public class StatefulFlatMap extends RichFlatMapFunction<String, String> {
  ValueState<String> previousInput;

  @Override
  public void open(Configuration parameters) throws Exception {
    previousInput = getRuntimeContext().getState(
      new ValueStateDescriptor<String>("previousInput", Types.STRING));
  }

  @Override
  public void flatMap(String in, Collector<String> collector) throws Exception {
    String out = "hello " + in;
    if(previousInput.value() != null){
      out = out + " " + previousInput.value();
    }
    previousInput.update(in);
    collector.collect(out);
  }
}

LANGUAGE: java
CODE:
@Test
public void testFlatMap() throws Exception{
  StatefulFlatMap statefulFlatMap = new StatefulFlatMap();

  OneInputStreamOperatorTestHarness<String, String> testHarness = 
    new KeyedOneInputStreamOperatorTestHarness<>(
      new StreamFlatMap<>(statefulFlatMap), x -> "1", Types.STRING);
  testHarness.open();

  testHarness.processElement("world", 10);
  ValueState<String> previousInput = 
    statefulFlatMap.getRuntimeContext().getState(
      new ValueStateDescriptor<>("previousInput", Types.STRING));
  String stateValue = previousInput.value();
  Assert.assertEquals(
    Lists.newArrayList(new StreamRecord<>("hello world", 10)), 
    testHarness.extractOutputStreamRecords());
  Assert.assertEquals("world", stateValue);

  testHarness.processElement("parallel", 20);
  Assert.assertEquals(
    Lists.newArrayList(
      new StreamRecord<>("hello world", 10), 
      new StreamRecord<>("hello parallel world", 20)), 
    testHarness.extractOutputStreamRecords());
  Assert.assertEquals("parallel", previousInput.value());
}

----------------------------------------

TITLE: Implementing Checkpointed Kafka Consumer in Java
DESCRIPTION: Example implementation of a Flink Kafka consumer that demonstrates operator state management using the CheckpointedFunction interface. Shows how to initialize, restore, and snapshot partition offset state.

LANGUAGE: java
CODE:
public class FlinkKafkaConsumer<T> extends RichParallelSourceFunction<T> implements CheckpointedFunction {
     // ...

   private transient ListState<Tuple2<KafkaTopicPartition, Long>> offsetsOperatorState;

   @Override
   public void initializeState(FunctionInitializationContext context) throws Exception {

      OperatorStateStore stateStore = context.getOperatorStateStore();
      // register the state with the backend
      this.offsetsOperatorState = stateStore.getSerializableListState("kafka-offsets");

      // if the job was restarted, we set the restored offsets
      if (context.isRestored()) {
         for (Tuple2<KafkaTopicPartition, Long> kafkaOffset : offsetsOperatorState.get()) {
            // ... restore logic
         }
      }
   }

   @Override
   public void snapshotState(FunctionSnapshotContext context) throws Exception {

      this.offsetsOperatorState.clear();

      // write the partition offsets to the list of operator states
      for (Map.Entry<KafkaTopicPartition, Long> partition : this.subscribedPartitionOffsets.entrySet()) {
         this.offsetsOperatorState.add(Tuple2.of(partition.getKey(), partition.getValue()));
      }
   }

   // ...

}

----------------------------------------

TITLE: Calculating Shortest Paths in Gelly (Flink's Graph API) - Java
DESCRIPTION: Demonstrates how to use Flink's Gelly API to calculate single source shortest paths in a graph. The example shows creating a graph from datasets and applying the SingleSourceShortestPaths algorithm.

LANGUAGE: java
CODE:
Graph<Long, Double, Double> graph = Graph.fromDataSet(vertices, edges, env);

DataSet<Vertex<Long, Double>> singleSourceShortestPaths = graph
     .run(new SingleSourceShortestPaths<Long>(srcVertexId,
           maxIterations)).getVertices();

----------------------------------------

TITLE: Stream Join Between Stock Warnings and Twitter Mentions
DESCRIPTION: Implements a streaming join between stock price warnings and Twitter mentions to compute rolling correlation within 30-second windows.

LANGUAGE: scala
CODE:
//Join warnings and parsed tweets
val tweetsAndWarning = warningsPerStock.join(tweetsPerStock)
  .onWindow(30, SECONDS)
  .where("symbol")
  .equalTo("symbol") { (c1, c2) => (c1.count, c2.count) }

val rollingCorrelation = tweetsAndWarning.window(Time.of(30, SECONDS))
  .mapWindow(computeCorrelation _)

----------------------------------------

TITLE: Configuring RocksDBStateBackend at Job Level in Flink
DESCRIPTION: This Java code snippet sets RocksDB as the state backend for a specific Flink job, enabling incremental checkpointing and specifying the checkpoint storage location.

LANGUAGE: java
CODE:
env.setStateBackend(new RocksDBStateBackend("hdfs:///fink-checkpoints", true));

----------------------------------------

TITLE: Testing Stateless Map Operator in Flink
DESCRIPTION: Implementation and testing of a simple stateless MapFunction that prepends 'hello' to input strings.

LANGUAGE: java
CODE:
public class MyStatelessMap implements MapFunction<String, String> {
  @Override
  public String map(String in) throws Exception {
    String out = "hello " + in;
    return out;
  }
}

LANGUAGE: java
CODE:
@Test
public void testMap() throws Exception {
  MyStatelessMap statelessMap = new MyStatelessMap();
  String out = statelessMap.map("world");
  Assert.assertEquals("hello world", out);
}

----------------------------------------

TITLE: Keyed Wrapper Class Definition
DESCRIPTION: Generic wrapper class for carrying original events along with their extracted keys and rule IDs.

LANGUAGE: java
CODE:
public class Keyed<IN, KEY, ID> {
  private IN wrapped;
  private KEY key;
  private ID id;

  ...
  public KEY getKey(){
      return key;
  }
}

----------------------------------------

TITLE: Implementing Async Functions with Python SDK
DESCRIPTION: Demonstrates how to register asynchronous Python functions as stateful functions using AsyncRequestReplyHandler. This allows integration with async IO web frameworks like aiohttp.

LANGUAGE: python
CODE:
from statefun import StatefulFunctions
from statefun import AsyncRequestReplyHandler

functions = StatefulFunctions()

@functions.bind("example/greeter")
async def greeter(context, message):
  html = await fetch(session, 'http://....')
  context.pack_and_reply(SomeProtobufMessage(html))

# expose this handler via an async web framework
handler = AsyncRequestReplyHandler(functions)

----------------------------------------

TITLE: Complete PyFlink UDF Example
DESCRIPTION: Full example demonstrating UDF registration, table creation, and data processing using PyFlink's Table API.

LANGUAGE: python
CODE:
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment, DataTypes
from pyflink.table.descriptors import Schema, OldCsv, FileSystem
from pyflink.table.udf import udf

env = StreamExecutionEnvironment.get_execution_environment()
env.set_parallelism(1)
t_env = StreamTableEnvironment.create(env)

add = udf(lambda i, j: i + j, [DataTypes.BIGINT(), DataTypes.BIGINT()], DataTypes.BIGINT())

t_env.register_function("add", add)

t_env.connect(FileSystem().path('/tmp/input')) \
    .with_format(OldCsv()
                 .field('a', DataTypes.BIGINT())
                 .field('b', DataTypes.BIGINT())) \
    .with_schema(Schema()
                 .field('a', DataTypes.BIGINT())
                 .field('b', DataTypes.BIGINT())) \
    .create_temporary_table('mySource')

t_env.connect(FileSystem().path('/tmp/output')) \
    .with_format(OldCsv()
                 .field('sum', DataTypes.BIGINT())) \
    .with_schema(Schema()
                 .field('sum', DataTypes.BIGINT())) \
    .create_temporary_table('mySink')

t_env.from_path('mySource')\
    .select("add(a, b)") \
    .insert_into('mySink')

t_env.execute("tutorial_job")

----------------------------------------

TITLE: Implementing HybridMemorySegment for Heap and Off-heap Memory in Java
DESCRIPTION: Demonstrates how to implement a HybridMemorySegment class that can handle both heap and off-heap memory using sun.misc.Unsafe. The getLong method seamlessly works with both memory types by exploiting how Unsafe interprets object references.

LANGUAGE: Java
CODE:
public class HybridMemorySegment {

  private final byte[] heapMemory;  // non-null in heap case, null in off-heap case
  private final long address;       // may be absolute, or relative to byte[]


  // method of interest
  public long getLong(int pos) {
    return UNSAFE.getLong(heapMemory, address + pos);
  }


  // initialize for heap memory
  public HybridMemorySegment(byte[] heapMemory) {
    this.heapMemory = heapMemory;
    this.address = UNSAFE.arrayBaseOffset(byte[].class)
  }
  
  // initialize for off-heap memory
  public HybridMemorySegment(long offheapPointer) {
    this.heapMemory = null;
    this.address = offheapPointer
  }
}

----------------------------------------

TITLE: Windowed Aggregations on Stock Prices
DESCRIPTION: Implements time-based window aggregations to compute statistics like minimum price, maximum price per stock, and rolling mean prices using 10-second windows with 5-second slides.

LANGUAGE: scala
CODE:
//Define the desired time window
val windowedStream = stockStream
  .window(Time.of(10, SECONDS)).every(Time.of(5, SECONDS))

//Compute some simple statistics on a rolling window
val lowest = windowedStream.minBy("price")
val maxByStock = windowedStream.groupBy("symbol").maxBy("price")
val rollingMean = windowedStream.groupBy("symbol").mapWindow(mean _)

----------------------------------------

TITLE: Testing Timed Process Operator in Flink
DESCRIPTION: Implementation and testing of a ProcessFunction with timer functionality using test harness for time control.

LANGUAGE: java
CODE:
public class MyProcessFunction extends KeyedProcessFunction<String, String, String> {
  @Override
  public void processElement(String in, Context context, Collector<String> collector) throws Exception {
    context.timerService().registerProcessingTimeTimer(50);
    String out = "hello " + in;
    collector.collect(out);
  }

  @Override
  public void onTimer(long timestamp, OnTimerContext ctx, Collector<String> out) throws Exception {
    out.collect(String.format("Timer triggered at timestamp %d", timestamp));
  }
}

LANGUAGE: java
CODE:
@Test
public void testProcessElement() throws Exception{
  MyProcessFunction myProcessFunction = new MyProcessFunction();
  OneInputStreamOperatorTestHarness<String, String> testHarness = 
    new KeyedOneInputStreamOperatorTestHarness<>(
      new KeyedProcessOperator<>(myProcessFunction), x -> "1", Types.STRING);

  testHarness.open();
  testHarness.processElement("world", 10);

  Assert.assertEquals(
    Lists.newArrayList(new StreamRecord<>("hello world", 10)), 
    testHarness.extractOutputStreamRecords());
}

@Test
public void testOnTimer() throws Exception {
  MyProcessFunction myProcessFunction = new MyProcessFunction();
  OneInputStreamOperatorTestHarness<String, String> testHarness = 
    new KeyedOneInputStreamOperatorTestHarness<>(
      new KeyedProcessOperator<>(myProcessFunction), x -> "1", Types.STRING);

  testHarness.open();
  testHarness.processElement("world", 10);
  Assert.assertEquals(1, testHarness.numProcessingTimeTimers());
      
  testHarness.setProcessingTime(50);
  Assert.assertEquals(
    Lists.newArrayList(
      new StreamRecord<>("hello world", 10), 
      new StreamRecord<>("Timer triggered at timestamp 50")), 
    testHarness.extractOutputStreamRecords());
}

----------------------------------------

TITLE: Temporal Join Query with Currency Conversion
DESCRIPTION: SQL query demonstrating a temporal join between streaming data from Kafka and static reference data for currency conversion.

LANGUAGE: sql
CODE:
USE CATALOG postgres;
SELECT
  l_proctime AS `querytime`,
  l_orderkey AS `order`,
  l_linenumber AS `linenumber`,
  l_currency AS `currency`,
  rs_rate AS `cur_rate`,
  (l_extendedprice * (1 - l_discount) * (1 + l_tax)) / rs_rate AS `open_in_euro`
FROM hive.`default`.prod_lineitem
JOIN prod_rates FOR SYSTEM_TIME AS OF l_proctime ON rs_symbol = l_currency
WHERE
  l_linestatus = 'O';

----------------------------------------

TITLE: Creating Python UDF for String Uppercase Conversion in Flink
DESCRIPTION: Shows how to implement a Python scalar function for uppercase string conversion and register it as a UDF in Flink's Python table environment.

LANGUAGE: python
CODE:
class PythonUpper(ScalarFunction):
def eval(self, s):
 return s.upper()

bt_env.register_function("python_upper", udf(PythonUpper(), DataTypes.STRING(), DataTypes.STRING()))


----------------------------------------

TITLE: Implementing ImapTableSource as ScanTableSource
DESCRIPTION: Implementation of the dynamic table source that handles scanning capabilities and runtime provider creation.

LANGUAGE: java
CODE:
import org.apache.flink.table.connector.ChangelogMode;
import org.apache.flink.table.connector.source.DynamicTableSource;
import org.apache.flink.table.connector.source.ScanTableSource;
import org.apache.flink.table.connector.source.SourceFunctionProvider;

public class ImapTableSource implements ScanTableSource {
  @Override
  public ChangelogMode getChangelogMode() {
    return ChangelogMode.insertOnly();
  }

  @Override
  public ScanRuntimeProvider getScanRuntimeProvider(ScanContext ctx) {
    boolean bounded = true;
    final ImapSource source = new ImapSource();
    return SourceFunctionProvider.of(source, bounded);
  }

  @Override
  public DynamicTableSource copy() {
    return new ImapTableSource();
  }

  @Override
  public String asSummaryString() {
    return "IMAP Table Source";
  }
}

----------------------------------------

TITLE: Creating Kafka Table in Hive Catalog
DESCRIPTION: DDL statement for creating a Kafka-sourced table with watermark specification in Hive catalog.

LANGUAGE: sql
CODE:
USE CATALOG hive;
CREATE TABLE prod_lineitem (
  l_orderkey INTEGER,
  l_partkey INTEGER,
  l_suppkey INTEGER,
  l_linenumber INTEGER,
  l_quantity DOUBLE,
  l_extendedprice DOUBLE,
  l_discount DOUBLE,
  l_tax DOUBLE,
  l_currency STRING,
  l_returnflag STRING,
  l_linestatus STRING,
  l_ordertime TIMESTAMP(3),
  l_shipinstruct STRING,
  l_shipmode STRING,
  l_comment STRING,
  l_proctime AS PROCTIME(),
  WATERMARK FOR l_ordertime AS l_ordertime - INTERVAL '5' SECONDS
) WITH (
  'connector'='kafka',
  'topic'='lineitem',
  'scan.startup.mode'='earliest-offset',
  'properties.bootstrap.servers'='kafka:9092',
  'properties.group.id'='testGroup',
  'format'='csv',
  'csv.field-delimiter'='|'
);

----------------------------------------

TITLE: Generating Temperature Alerts from Warning Patterns
DESCRIPTION: Uses flatSelect to generate TemperatureAlert events when consecutive warnings show increasing temperatures.

LANGUAGE: java
CODE:
DataStream<TemperatureAlert> alerts = alertPatternStream.flatSelect(
    (Map<String, TemperatureWarning> pattern, Collector<TemperatureAlert> out) -> {
        TemperatureWarning first = pattern.get("First Event");
        TemperatureWarning second = pattern.get("Second Event");

        if (first.getAverageTemperature() < second.getAverageTemperature()) {
            out.collect(new TemperatureAlert(first.getRackID()));
        }
    });

----------------------------------------

TITLE: Converting Between PyFlink Table and Pandas DataFrame
DESCRIPTION: Example demonstrating how to convert between PyFlink Tables and Pandas DataFrames using from_pandas() and to_pandas() methods.

LANGUAGE: python
CODE:
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment
import pandas as pd
import numpy as np

env = StreamExecutionEnvironment.get_execution_environment()
t_env = StreamTableEnvironment.create(env)

# Create a PyFlink Table
pdf = pd.DataFrame(np.random.rand(1000, 2))
table = t_env.from_pandas(pdf, ["a", "b"]).filter("a > 0.5")

# Convert the PyFlink Table to a Pandas DataFrame
pdf = table.to_pandas()
print(pdf)

----------------------------------------

TITLE: Configuring State TTL in Apache Flink
DESCRIPTION: This snippet demonstrates how to create a StateTtlConfig object and apply it to a ValueStateDescriptor in Apache Flink. It sets a 7-day TTL, configures update type and state visibility options.

LANGUAGE: java
CODE:
import org.apache.flink.api.common.state.StateTtlConfig;
import org.apache.flink.api.common.time.Time;
import org.apache.flink.api.common.state.ValueStateDescriptor;

StateTtlConfig ttlConfig = StateTtlConfig
    .newBuilder(Time.days(7))
    .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)
    .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)
    .build();
    
ValueStateDescriptor<Long> lastUserLogin = 
    new ValueStateDescriptor<>("lastUserLogin", Long.class);

lastUserLogin.enableTimeToLive(ttlConfig);

----------------------------------------

TITLE: Configuring Changelog State Backend in Flink
DESCRIPTION: YAML configuration snippet to enable the changelog state backend feature in Flink. It sets the backend to enabled, specifies the storage type as filesystem, and sets the base path for the durable short-term log (DSTL).

LANGUAGE: yaml
CODE:
state.backend.changelog.enabled: true
state.backend.changelog.storage: filesystem 
dstl.dfs.base-path: <location similar to state.checkpoints.dir>

----------------------------------------

TITLE: Implementing Pulsar Streaming Source and Sink in Flink DataStream
DESCRIPTION: Demonstrates how to use Pulsar as both a streaming source and sink in a Flink DataStream application, including a word count example with window operations.

LANGUAGE: java
CODE:
// create and configure Pulsar consumer
PulsarSourceBuilder<String>builder = PulsarSourceBuilder
   .builder(new SimpleStringSchema())
   .serviceUrl(serviceUrl)
   .topic(inputTopic)
   .subscriptionName(subscription);
SourceFunction<String> src = builder.build();
// ingest DataStream with Pulsar consumer
DataStream<String> words = env.addSource(src);

// perform computation on DataStream (here a simple WordCount)
DataStream<WordWithCount> wc = words
   .flatMap((FlatMapFunction<String, WordWithCount>) (word, collector) -> {
       collector.collect(new WordWithCount(word, 1));
   })
   .returns(WordWithCount.class)
   .keyBy("word")
   .timeWindow(Time.seconds(5))
   .reduce((ReduceFunction<WordWithCount>) (c1, c2) ->
       new WordWithCount(c1.word, c1.count + c2.count));

// emit result via Pulsar producer
wc.addSink(new FlinkPulsarProducer<>(
   serviceUrl,
   outputTopic,
   new AuthenticationDisabled(),
   wordWithCount -> wordWithCount.toString().getBytes(UTF_8),
   wordWithCount -> wordWithCount.word)
);

----------------------------------------

TITLE: Using Pulsar with Flink SQL and Table API
DESCRIPTION: Shows how to integrate Pulsar as a streaming source and table sink for Flink SQL queries, including time-windowed aggregations.

LANGUAGE: java
CODE:
// obtain a DataStream with words
DataStream<String> words = ...

// register DataStream as Table "words" with two attributes ("word", "ts"). 
//   "ts" is an event-time timestamp.
tableEnvironment.registerDataStream("words", words, "word, ts.rowtime");

// create a TableSink that produces to Pulsar
TableSink sink = new PulsarJsonTableSink(
   serviceUrl,
   outputTopic,
   new AuthenticationDisabled(),
   ROUTING_KEY);

// register Pulsar TableSink as table "wc"
tableEnvironment.registerTableSink(
   "wc",
   sink.configure(
      new String[]{"word", "cnt"},
      new TypeInformation[]{Types.STRING, Types.LONG}));

// count words per 5 seconds and write result to table "wc"
tableEnvironment.sqlUpdate(
   "INSERT INTO wc " +
   "SELECT word, COUNT(*) AS cnt " +
   "FROM words " +
   "GROUP BY word, TUMBLE(ts, INTERVAL '5' SECOND)");

----------------------------------------

TITLE: Configuring Broadcast State
DESCRIPTION: Demonstrates how to set up broadcast state using MapStateDescriptor and create a broadcast stream for patterns.

LANGUAGE: java
CODE:
MapStateDescriptor<Void, Pattern> bcStateDescriptor = 
  new MapStateDescriptor<>("patterns", Types.VOID, Types.POJO(Pattern.class));

BroadcastStream<Pattern> bcedPatterns = patterns.broadcast(bcStateDescriptor);

----------------------------------------

TITLE: Demonstrating Table API Queries in Scala
DESCRIPTION: Example showing how to use Flink's Table API to perform grouping and joining operations on tables using SQL-like syntax. The code demonstrates selecting and counting from a clicks table and joining with a users table.

LANGUAGE: scala
CODE:
val clickCounts = clicks
  .groupBy('user).select('userId, 'url.count as 'count)

val activeUsers = users.join(clickCounts)
  .where('id === 'userId && 'count > 10).select('username, 'count, ...)

----------------------------------------

TITLE: Implementing Stateful Function in Java
DESCRIPTION: Example of implementing a greeter function using the new Java SDK in StateFun 3.0.0. Demonstrates state handling, message processing, and response generation.

LANGUAGE: java
CODE:
static final class Greeter implements StatefulFunction {
    static final ValueSpec<Integer> VISITS = ValueSpec.named("visits").withIntType();

    @Override
    public CompletableFuture<Void> apply(Context context, Message message){
        // update the visits count
        int visits = context.storage().get(VISITS).orElse(0);
        visits++;
        context.storage().set(VISITS, visits);

        // compute a greeting
        var name = message.asUtf8String();
        var greeting = String.format("Hello there %s at the %d-th time!\n", name, visits);

        // reply to the caller with a greeting
        var caller = context.caller().get();
        context.send(
            MessageBuilder.forAddress(caller)
                .withValue(greeting)
                .build()
        );

        return context.done();
    }
}

----------------------------------------

TITLE: Configuring Session Windows in Java
DESCRIPTION: This code shows how to set up session windows in Flink 1.1.0's DataStream API, which are useful for cases where window boundaries need to adjust to incoming data.

LANGUAGE: Java
CODE:
input.keyBy(<key selector>)
    .window(EventTimeSessionWindows.withGap(Time.minutes(10)))
    .<windowed transformation>(<window function>);

----------------------------------------

TITLE: Creating tables and executing a join query in SQL
DESCRIPTION: This SQL snippet creates customers and orders tables, then performs a join to select customer IDs and names where the customer ID matches the order customer ID.

LANGUAGE: SQL
CODE:
CREATE TABLE customers (
    customerId int,
    name varchar(255)
);

CREATE TABLE orders (
    orderId int,
    orderCustomerId int
);

--fill tables with data

SELECT customerId, name
FROM customers, orders
WHERE customerId = orderCustomerId

----------------------------------------

TITLE: Complete Fraud Detection Pipeline
DESCRIPTION: Main processing pipeline showing dynamic key function integration with keying and alert processing.

LANGUAGE: java
CODE:
DataStream<Alert> alerts =
    transactions
        .process(new DynamicKeyFunction())
        .keyBy((keyed) -> keyed.getKey());
        .process(new DynamicAlertFunction())

----------------------------------------

TITLE: Python Shopping Cart StateFun Application
DESCRIPTION: A shopping cart application demonstrating stateful functions for cart and inventory management using StateFun's Python SDK. The code shows function implementations for maintaining cart items and inventory levels.

LANGUAGE: python
CODE:
# Note: Full code available at https://github.com/tzulitai/statefun-aws-demo/blob/master/app/shopping_cart.py

----------------------------------------

TITLE: Creating IMAP Source Class Implementation
DESCRIPTION: Implements the core IMAP source functionality including server connection, message listening and processing logic.

LANGUAGE: Java
CODE:
import jakarta.mail.*;
import org.apache.flink.streaming.api.functions.source.RichSourceFunction;
import org.apache.flink.streaming.api.functions.source.SourceFunction;
import org.apache.flink.table.data.RowData;

public class ImapSource extends RichSourceFunction<RowData> {
    private transient volatile boolean running = false;

    @Override
    public void run(SourceFunction.SourceContext<RowData> ctx) throws Exception {
        connect();
        running = true;

        while (running) {
            folder.getMessageCount();
            Thread.sleep(250);
        }
    }

    @Override
    public void cancel() {
        running = false;
    }

    @Override
    public void close() throws Exception {
        if (folder != null) {
            folder.close();
        }

        if (store != null) {
            store.close();
        }
    }
}

----------------------------------------

TITLE: Configuring Log4j Security Settings in Flink
DESCRIPTION: Configuration snippet for flink-conf.yaml to mitigate the Log4j Zero Day vulnerability (CVE-2021-44228). This setting disables JNDI lookups in message formatting to prevent potential remote code execution attacks.

LANGUAGE: yaml
CODE:
env.java.opts: -Dlog4j2.formatMsgNoLookups=true

----------------------------------------

TITLE: Defining Python UDFs - Multiple Approaches
DESCRIPTION: Different methods to define Python scalar functions that take two BIGINT columns as input and return their sum. Shows class-based, function-based, lambda, callable, and partial function approaches.

LANGUAGE: python
CODE:
# option 1: extending the base class `ScalarFunction`
class Add(ScalarFunction):
  def eval(self, i, j):
    return i + j

add = udf(Add(), [DataTypes.BIGINT(), DataTypes.BIGINT()], DataTypes.BIGINT())

# option 2: Python function
@udf(input_types=[DataTypes.BIGINT(), DataTypes.BIGINT()], result_type=DataTypes.BIGINT())
def add(i, j):
  return i + j

# option 3: lambda function
add = udf(lambda i, j: i + j, [DataTypes.BIGINT(), DataTypes.BIGINT()], DataTypes.BIGINT())

# option 4: callable function
class CallableAdd(object):
  def __call__(self, i, j):
    return i + j

add = udf(CallableAdd(), [DataTypes.BIGINT(), DataTypes.BIGINT()], DataTypes.BIGINT())

# option 5: partial function
def partial_add(i, j, k):
  return i + j + k

add = udf(functools.partial(partial_add, k=1), [DataTypes.BIGINT(), DataTypes.BIGINT()],
          DataTypes.BIGINT())

----------------------------------------

TITLE: Defining Count Windows in Apache Flink Scala API
DESCRIPTION: This snippet shows how to create tumbling and sliding count windows on a DataStream of vehicle counts. It demonstrates keying the stream by sensor ID and applying count-based windows with sum aggregation.

LANGUAGE: scala
CODE:
// Stream of (sensorId, carCnt)
val vehicleCnts: DataStream[(Int, Int)] = ...

val tumblingCnts: DataStream[(Int, Int)] = vehicleCnts
  // key stream by sensorId
  .keyBy(0)
  // tumbling count window of 100 elements size
  .countWindow(100)
  // compute the carCnt sum 
  .sum(1)

val slidingCnts: DataStream[(Int, Int)] = vehicleCnts
  .keyBy(0)
  // sliding count window of 100 elements size and 10 elements trigger interval
  .countWindow(100, 10)
  .sum(1)

----------------------------------------

TITLE: Initializing Fraud Detection Pipeline with Dynamic Key and Alert Functions in Java
DESCRIPTION: Sets up the main data processing pipeline for fraud detection using DynamicKeyFunction and DynamicAlertFunction. This snippet demonstrates the basic structure without dynamic rule updates.

LANGUAGE: Java
CODE:
DataStream<Alert> alerts =
    transactions
        .process(new DynamicKeyFunction())
        .keyBy((keyed) -> keyed.getKey());
        .process(new DynamicAlertFunction())

----------------------------------------

TITLE: Implementing FlinkPravegaWriter
DESCRIPTION: Example of setting up a Flink Pravega writer sink with exactly-once processing guarantees and parallel processing configuration.

LANGUAGE: java
CODE:
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

// Define the Pravega configuration
PravegaConfig config = PravegaConfig.fromParams(params);

// Define the event serializer
SerializationSchema<MyClass> serializer = ...

// Define the event router for selecting the Routing Key
PravegaEventRouter<MyClass> router = ...

// Define the sink function
FlinkPravegaWriter<MyClass> pravegaSink = FlinkPravegaWriter.<MyClass>builder()
   .forStream(...)
   .withPravegaConfig(config)
   .withSerializationSchema(serializer)
   .withEventRouter(router)
   .withWriterMode(EXACTLY_ONCE)
   .build();

DataStream<MyClass> stream = ...
stream.addSink(pravegaSink)
    .setParallelism(4)
    .uid("pravega-sink");

----------------------------------------

TITLE: Enabling Fully Async Mode for RocksDB State Backend
DESCRIPTION: Java configuration code to enable fully async mode for RocksDB state backend, which is recommended for compatibility with Flink 1.2 upgrade path via savepoints.

LANGUAGE: java
CODE:
RocksDBStateBackend backend = new RocksDBStateBackend("...");
backend.enableFullyAsyncSnapshots();

----------------------------------------

TITLE: Configuring Watermark Interval in Flink
DESCRIPTION: Sets the auto watermark interval to 100ms for faster watermark generation in Flink applications aiming for sub-second latencies.

LANGUAGE: yaml
CODE:
pipeline.auto-watermark-interval: 100ms

----------------------------------------

TITLE: Python Stateful Window Implementation
DESCRIPTION: Example demonstrating custom counting window implementation using state in PyFlink DataStream API.

LANGUAGE: python
CODE:
class CountWindowAverage(FlatMapFunction):
    def __init__(self, window_size):
        self.window_size = window_size

    def open(self, runtime_context: RuntimeContext):
        descriptor = ValueStateDescriptor("average", Types.TUPLE([Types.LONG(), Types.LONG()]))
        self.sum = runtime_context.get_state(descriptor)

    def flat_map(self, value):
        current_sum = self.sum.value()
        if current_sum is None:
            current_sum = (0, 0)
        # update the count
        current_sum = (current_sum[0] + 1, current_sum[1] + value[1])
        # if the count reaches window_size, emit the average and clear the state
        if current_sum[0] >= self.window_size:
            self.sum.clear()
            yield value[0], current_sum[1] // current_sum[0]
        else:
            self.sum.update(current_sum)

ds = ...  # type: DataStream
ds.key_by(lambda row: row[0]) \
  .flat_map(CountWindowAverage(5))

----------------------------------------

TITLE: Defining Broadcast State Descriptor for Rules in Java
DESCRIPTION: Creates a MapStateDescriptor for storing rules in the broadcast state. This descriptor is used when creating the broadcast stream and accessing the state in processing functions.

LANGUAGE: Java
CODE:
public static final MapStateDescriptor<Integer, Rule> RULES_STATE_DESCRIPTOR =
        new MapStateDescriptor<>("rules", Integer.class, Rule.class);

----------------------------------------

TITLE: Executing SQL Query on Streaming Data
DESCRIPTION: This snippet demonstrates how to execute a SQL query on streaming data using Flink's SQL support in version 1.1.0.

LANGUAGE: Java
CODE:
Table result = tableEnv.sql(
  "SELECT STREAM product, amount FROM Orders WHERE product LIKE '%Rubber%'");

----------------------------------------

TITLE: Implementing FlinkPravegaReader
DESCRIPTION: Example of setting up a Flink Pravega reader source with checkpointing enabled and parallel processing configuration.

LANGUAGE: java
CODE:
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

// Enable Flink checkpoint to make state fault tolerant
env.enableCheckpointing(60000);

// Define the Pravega configuration
ParameterTool params = ParameterTool.fromArgs(args);
PravegaConfig config = PravegaConfig.fromParams(params);

// Define the event deserializer
DeserializationSchema<MyClass> deserializer = ...

// Define the data stream
FlinkPravegaReader<MyClass> pravegaSource = FlinkPravegaReader.<MyClass>builder()
    .forStream(...)
    .withPravegaConfig(config)
    .withDeserializationSchema(deserializer)
    .build();
DataStream<MyClass> stream = env.addSource(pravegaSource)
    .setParallelism(4)
    .uid("pravega-source");

----------------------------------------

TITLE: Demonstrating PLAN_ADVICE for Non-Deterministic Updates in Flink SQL
DESCRIPTION: Example of PLAN_ADVICE output warning about non-deterministic columns that may affect correct processing of update messages in Flink SQL queries.

LANGUAGE: sql
CODE:
== Optimized Physical Plan With Advice ==
...

advice[1]: [WARNING] The column(s): day(generated by non-deterministic function: CURRENT_TIMESTAMP ) can not satisfy the determinism requirement for correctly processing update message('UB'/'UA'/'D' in changelogMode, not 'I' only), this usually happens when input node has no upsertKey(upsertKeys=[{}]) or current node outputs non-deterministic update messages. Please consider removing these non-deterministic columns or making them deterministic by using deterministic functions.

----------------------------------------

TITLE: Benchmark SQL Query Performance Comparison
DESCRIPTION: SQL query performance comparison between sort-based and hash-based shuffle implementations showing execution times and speedup factors for various TPC-DS queries.

LANGUAGE: sql
CODE:
q4.sql, q11.sql, q14b.sql, q17.sql, q23a.sql, q23b.sql, q25.sql, q29.sql, q31.sql, q50.sql, q64.sql, q74.sql, q75.sql, q93.sql

----------------------------------------

TITLE: Using Pulsar Schema with Producer/Consumer
DESCRIPTION: Shows how to use Pulsar's built-in schema system with both producers and consumers, demonstrating AVRO schema implementation.

LANGUAGE: java
CODE:
Producer<User> producer = client.newProducer(Schema.AVRO(User.class)).create();
producer.newMessage()
  .value(User.builder()
    .userName("pulsar-user")
    .userId(1L)
    .build())
  .send();

Consumer<User> consumer = client.newCOnsumer(Schema.AVRO(User.class)).create();
consumer.receive();

----------------------------------------

TITLE: Configuring Append-only Tables in Flink SQL
DESCRIPTION: SQL configuration for creating append-only tables that only accept INSERT_ONLY data, optimized for use cases without updates like log data synchronization. Includes asynchronous compaction for streaming writes.

LANGUAGE: sql
CODE:
CREATE TABLE my_table (
    ...
) WITH (
    'write-mode' = 'append-only',
    ...
)

----------------------------------------

TITLE: Using DISTRIBUTED BY Clause in Flink SQL
DESCRIPTION: Examples of using the new DISTRIBUTED BY clause in Flink SQL to specify bucketing behavior for tables.

LANGUAGE: SQL
CODE:
CREATE TABLE MyTable (uid BIGINT, name STRING) DISTRIBUTED BY HASH(uid) INTO 4 BUCKETS;

CREATE TABLE MyTable (uid BIGINT, name STRING) DISTRIBUTED BY (uid) INTO 4 BUCKETS;

CREATE TABLE MyTable (uid BIGINT, name STRING) DISTRIBUTED BY (uid);

CREATE TABLE MyTable (uid BIGINT, name STRING) DISTRIBUTED INTO 4 BUCKETS;

----------------------------------------

TITLE: Kafka Stream Processing with SQL in Scala
DESCRIPTION: Demonstrates how to process streaming data from Kafka using SQL queries in Flink, including source configuration and query execution.

LANGUAGE: scala
CODE:
// get environments
val execEnv = StreamExecutionEnvironment.getExecutionEnvironment
val tableEnv = TableEnvironment.getTableEnvironment(execEnv)

// configure Kafka connection
val kafkaProps = ...
// define a JSON encoded Kafka topic as external table
val sensorSource = new KafkaJsonSource[(String, Long, Double)](
    "sensorTopic",
    kafkaProps,
    ("location", "time", "tempF"))

// register external table
tableEnv.registerTableSource("sensorData", sensorSource)

// define query in external table
val roomSensors: Table = tableEnv.sql(
    "SELECT STREAM time, location AS room, (tempF - 32) * 0.556 AS tempC " +
    "FROM sensorData " +
    "WHERE location LIKE 'room%'"
  )

// define a JSON encoded Kafka topic as external sink
val roomSensorSink = new KafkaJsonSink(...)

// define sink for room sensor data and execute query
roomSensors.toSink(roomSensorSink)
execEnv.execute()

----------------------------------------

TITLE: Revised Task Finish Protocol Implementation
DESCRIPTION: Shows the revised protocol steps for task finishing after FLIP-147 implementation. Includes new EndOfData event handling and modified checkpoint coordination.

LANGUAGE: text
CODE:
1. Source tasks finished due to no more records or stop-with-savepoint.
    a. if no more records or stop-with-savepoint –-drain
        i. source operators emit MAX_WATERMARK
        ii. endInput(inputId) for all the operators
        iii. finish() for all the operators
        iv. emit EndOfData[isDrain = true] event
    b. else if stop-with-savepoint
        i. emit EndOfData[isDrain = false] event
    c. Wait for the next checkpoint / the savepoint after operator finished complete
    d. close() for all the operators
    e. Emit EndOfPartitionEvent
    f. Task cleanup
2. On received MAX_WATERMARK for non-source operators
    a. Trigger all the event times
    b. Emit MAX_WATERMARK
3. On received EndOfData for non-source tasks
    a. If isDrain
        i. endInput(int inputId) for all the operators
        ii. finish() for all the operators
    b. Emit EndOfData[isDrain = the flag value of the received event]
4. On received EndOfPartitionEvent for non-source tasks
    a. Wait for the next checkpoint / the savepoint after operator finished complete
    b. close() for all the operators
    c. Emit EndOfPartitionEvent
    d. Task cleanup

----------------------------------------

TITLE: Configuring Pulsar Producer as Flink Sink
DESCRIPTION: Demonstrates how to set up a Pulsar producer as a sink for Flink output, including service URL configuration and data serialization.

LANGUAGE: java
CODE:
wc.addSink(new FlinkPulsarProducer<>(
  serviceUrl,
  outputTopic,
  new AuthentificationDisabled(),
  wordWithCount -> wordWithCount.toString().getBytes(UTF_8),
  wordWithCount -> wordWithCount.word)
);

----------------------------------------

TITLE: Structuring Configuration Keys in YAML for Apache Flink
DESCRIPTION: Demonstrates the recommended hierarchical structure for configuration keys in Flink's YAML files. This approach organizes settings logically, making them easier to manage and understand.

LANGUAGE: yaml
CODE:
taskmanager: {
  jvm-exit-on-oom: true,
  network: {
    detailed-metrics: false,
    request-backoff: {
      initial: 100,
      max: 10000
    },
    memory: {
      fraction: 0.1,
      min: 64MB,
      max: 1GB,
      buffers-per-channel: 2,
      floating-buffers-per-gate: 16
    }
  }
}

----------------------------------------

TITLE: Stop-with-savepoint Protocol Implementation
DESCRIPTION: Shows the protocol steps for handling stop-with-savepoint commands in Flink before FLIP-147. Details the savepoint trigger process and task cleanup procedures.

LANGUAGE: text
CODE:
1. Trigger a savepoint
2. Sources received savepoint trigger RPC
    a. If with –-drain
        i. source operators emit MAX_WATERMARK
    b. Source emits savepoint barrier
3. On received MAX_WATERMARK for non-source operators
    a. Trigger all the event times
    b. Emit MAX_WATERMARK
4. On received savepoint barrier for non-source operators
    a. The task blocks till the savepoint succeed
5. Finish the source tasks actively
    a. If with –-drain
        ii. endInput(inputId) for all the operators
    b. close() for all the operators
    c. dispose() for all the operators
    d. Emit EndOfPartitionEvent
    e. Task cleanup
6. On received EndOfPartitionEvent for non-source tasks
    a. If with –-drain
        i. endInput(int inputId) for all the operators
    b. close() for all the operators
    c. dispose() for all the operators
    d. Emit EndOfPartitionEvent
    e. Task cleanup

----------------------------------------

TITLE: Configuring Flink Autotuning in YAML
DESCRIPTION: YAML configuration to enable Flink Autotuning feature alongside Autoscaling. Requires autoscaling to be enabled as a prerequisite.

LANGUAGE: yaml
CODE:
# Autoscaling needs to be enabled
job.autoscaler.enabled: true
# Turn on Autotuning
job.autoscaler.memory.tuning.enabled: true

----------------------------------------

TITLE: Full Partition Processing on DataStream API
DESCRIPTION: Example of using the new FullPartitionWindow API to perform aggregations on non-keyed streams in the DataStream API.

LANGUAGE: Java
CODE:
inputStream.fullWindowPartition()
                .mapPartition(
         new MapPartitionFunction<Record, Long>() {
                    @Override
                    public void mapPartition(
                            Iterable<Record> values, Collector<Long> out)
                            throws Exception {
                        long counter = 0;
                        for (Record value : values) {
                            counter++;
                        }
                        out.collect(counter));
                    }
          })

----------------------------------------

TITLE: Creating FileSystem Table with Partitioning in SQL
DESCRIPTION: Demonstrates creating a partitioned table using the new FileSystem connector with format support

LANGUAGE: sql
CODE:
CREATE TABLE my_table (
  column_name1 INT,
  column_name2 STRING,
  ...
  part_name1 INT,
  part_name2 STRING
) PARTITIONED BY (part_name1, part_name2) WITH (
  'connector' = 'filesystem',         
  'path' = 'file:///path/to/file,
  'format' = '...',  -- supported formats: Avro, ORC, Parquet, CSV, JSON         
  ...
);

----------------------------------------

TITLE: Configuring GPU External Resource in Flink
DESCRIPTION: Configuration settings required to enable GPU support in Flink's External Resource Framework, including driver settings, resource allocation, and platform-specific configurations for Yarn and Kubernetes.

LANGUAGE: bash
CODE:
external-resources: gpu
# Define the driver factory class of gpu resource.
external-resource.gpu.driver-factory.class: org.apache.flink.externalresource.gpu.GPUDriverFactory
# Define the amount of gpu resource per TaskManager.
external-resource.gpu.amount: 1
# Enable the coordination mode if you run it in standalone mode
external-resource.gpu.param.discovery-script.args: --enable-coordination


# If you run it on Yarn
external-resource.gpu.yarn.config-key: yarn.io/gpu
# If you run it on Kubernetes
external-resource.gpu.kubernetes.config-key: nvidia.com/gpu

----------------------------------------

TITLE: Configuring Restart Time Tracking in YAML
DESCRIPTION: YAML configuration to enable automatic tracking of restart times for autoscaling operations.

LANGUAGE: yaml
CODE:
job.autoscaler.restart.time-tracking.enabled: true

----------------------------------------

TITLE: Executing Streaming ETL in Flink SQL
DESCRIPTION: SQL insert statement that triggers the stream processing job by moving and transforming data from source to sink table.

LANGUAGE: sql
CODE:
INSERT INTO sink_table
SELECT id, name, processed_data
FROM source_table

----------------------------------------

TITLE: Implementing IMAP Source Options Configuration
DESCRIPTION: Defines configuration options class for IMAP connection parameters including host, port, username and password using Lombok annotations.

LANGUAGE: Java
CODE:
import lombok.Data;
import lombok.experimental.SuperBuilder;
import javax.annotation.Nullable;
import java.io.Serializable;

@Data
@SuperBuilder(toBuilder = true)
public class ImapSourceOptions implements Serializable {
    private static final long serialVersionUID = 1L;

    private final String host;
    private final @Nullable Integer port;
    private final @Nullable String user;
    private final @Nullable String password;
}

----------------------------------------

TITLE: SQL DDL for Static Partition Writing
DESCRIPTION: SQL syntax example for writing to static partitions in Hive tables using INSERT INTO/OVERWRITE commands.

LANGUAGE: sql
CODE:
INSERT { INTO | OVERWRITE } TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement;

----------------------------------------

TITLE: Creating Source Table in Flink SQL
DESCRIPTION: SQL statement to create a source table that represents the input data stream for ETL processing in Flink.

LANGUAGE: sql
CODE:
CREATE TABLE source_table (
  id int,
  name string,
  data string
) WITH (
  'connector' = 'datagen'
)

----------------------------------------

TITLE: Canary FlinkDeployment Configuration
DESCRIPTION: YAML configuration for creating a canary FlinkDeployment resource used to monitor operator health and detect reconciliation issues.

LANGUAGE: yaml
CODE:
apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  name: canary
  labels:
    "flink.apache.org/canary": "true"

----------------------------------------

TITLE: Setting Custom Parallelism for DataGen Source in Flink SQL
DESCRIPTION: Demonstrates how to set a custom parallelism for the DataGen source connector using the 'scan.parallelism' option in a CREATE TABLE statement or via dynamic table options.

LANGUAGE: sql
CODE:
CREATE TABLE Orders (
    order_number BIGINT,
    price        DECIMAL(32,2),
    buyer        ROW<first_name STRING, last_name STRING>,
    order_time   TIMESTAMP(3)
) WITH (
    'connector' = 'datagen',
    'scan.parallelism' = '4'
);

SELECT * FROM Orders /*+ OPTIONS('scan.parallelism'='4') */;

----------------------------------------

TITLE: Joining Datasets in Apache Flink using Scala
DESCRIPTION: Example demonstrating how to join two datasets (PageVisit and User) using Flink's Scala API. Shows case class definitions, dataset filtering, and equi-join operations with key selectors.

LANGUAGE: scala
CODE:
// define your data types
case class PageVisit(url: String, ip: String, userId: Long)
case class User(id: Long, name: String, email: String, country: String)

// get your data from somewhere
val visits: DataSet[PageVisit] = ...
val users: DataSet[User] = ...

// filter the users data set
val germanUsers = users.filter((u) => u.country.equals("de"))
// join data sets
val germanVisits: DataSet[(PageVisit, User)] =
      // equi-join condition (PageVisit.userId = User.id)
     visits.join(germanUsers).where("userId").equalTo("id")


----------------------------------------

TITLE: SQL Window Definition with CUMULATE Function
DESCRIPTION: Example showing how to define a CUMULATE window function that assigns windows with expanding step size until maximum window size is reached.

LANGUAGE: sql
CODE:
SELECT window_time, window_start, window_end, SUM(price) AS total_price 
  FROM TABLE(CUMULATE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '2' MINUTES, INTERVAL '10' MINUTES))
GROUP BY window_start, window_end, window_time;

----------------------------------------

TITLE: Configuring Ephemeral Storage in Task Manager Specification
DESCRIPTION: Example of configuring ephemeral storage size along with other resources in the task manager specification using the new CRD field.

LANGUAGE: yaml
CODE:
spec:
  ...
  taskManager:
    resource:
      memory: "2048m"
      cpu: 8
      ephemeralStorage: "10G"

----------------------------------------

TITLE: Starting Flink Application Cluster
DESCRIPTION: Command to start a Flink application cluster on Kubernetes with HA configuration, including settings for parallelism, resource allocation, and high availability storage.

LANGUAGE: bash
CODE:
$ ./bin/flink run-application \
    --detached \
    --parallelism 4 \
    --target kubernetes-application \
    -Dkubernetes.cluster-id=k8s-ha-app-1 \
    -Dkubernetes.container.image=<user-image> \
    -Dkubernetes.jobmanager.cpu=0.5 \
    -Dkubernetes.taskmanager.cpu=0.5 \
    -Dtaskmanager.numberOfTaskSlots=4 \
    -Dkubernetes.rest-service.exposed.type=NodePort \
    -Dhigh-availability=org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory \
    -Dhigh-availability.storageDir=s3://flink-bucket/flink-ha \
    -Drestart-strategy=fixed-delay \
    -Drestart-strategy.fixed-delay.attempts=10 \
    -Dcontainerized.master.env.ENABLE_BUILT_IN_PLUGINS=flink-s3-fs-hadoop-1.12.1.jar \
    -Dcontainerized.taskmanager.env.ENABLE_BUILT_IN_PLUGINS=flink-s3-fs-hadoop-1.12.1.jar \
    local:///opt/flink/usrlib/my-flink-job.jar

----------------------------------------

TITLE: Using Remote Application Jar in YARN
DESCRIPTION: Complete example showing how to use both remote Flink distribution and application jar from HDFS for minimal bandwidth usage.

LANGUAGE: bash
CODE:
./bin/flink run-application -t yarn-application \
    -Djobmanager.memory.process.size=2048m \
    -Dtaskmanager.memory.process.size=4096m \
    -Dyarn.provided.lib.dirs="hdfs://myhdfs/remote-flink-dist-dir" \
    hdfs://myhdfs/jars/MyApplication.jar

----------------------------------------

TITLE: Scaling Flink Job in Reactive Mode
DESCRIPTION: These bash commands show how to scale up and down a Flink job running in Reactive Mode by adding or removing TaskManager instances.

LANGUAGE: bash
CODE:
# Start additional TaskManager
./bin/taskmanager.sh start

# Remove a TaskManager
./bin/taskmanager.sh stop

----------------------------------------

TITLE: Installing Flink Kubernetes Operator via Helm
DESCRIPTION: Bash commands to add the Helm repository for Flink Kubernetes Operator 1.7.0 and install it using Helm.

LANGUAGE: bash
CODE:
$ helm repo add flink-kubernetes-operator-1.7.0 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.7.0/
$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.7.0/flink-kubernetes-operator --set webhook.create=false

----------------------------------------

TITLE: Applying CEP Pattern to Event Stream in Flink
DESCRIPTION: Applies the defined temperature warning pattern to the input event stream, keyed by rack ID.

LANGUAGE: java
CODE:
PatternStream<MonitoringEvent> tempPatternStream = CEP.pattern(
    inputEventStream.keyBy("rackID"),
    warningPattern);

----------------------------------------

TITLE: Implementing Pulsar as Batch Sink in Flink
DESCRIPTION: Demonstrates how to use Pulsar as a batch sink where results are pushed after Flink completes computation on a static dataset.

LANGUAGE: java
CODE:
// obtain DataSet from arbitrary computation
DataSet<WordWithCount> wc = ...

// create PulsarOutputFormat instance
OutputFormat pulsarOutputFormat = new PulsarOutputFormat(
   serviceUrl, 
   topic, 
   new AuthenticationDisabled(), 
   wordWithCount -> wordWithCount.toString().getBytes());
// write DataSet to Pulsar
wc.output(pulsarOutputFormat);

----------------------------------------

TITLE: Configuring Operator Rate Limiter in YAML
DESCRIPTION: Default configuration for the operator rate limiter, which helps prevent overloading the API server.

LANGUAGE: yaml
CODE:
kubernetes.operator.rate-limiter.limit: 5
kubernetes.operator.rate-limiter.refresh-period: 15 s

----------------------------------------

TITLE: Implementing a User-Defined Table Function in Scala
DESCRIPTION: This code defines a user-defined table function called 'PropertiesExtractor' that extracts color and size properties from a string of key-value pairs. It demonstrates how to implement complex data extraction logic and emit multiple output rows from a single input value.

LANGUAGE: scala
CODE:
class PropertiesExtractor extends TableFunction[Row] {
  def eval(prefs: String): Unit = {
    // split string into (key, value) pairs
    val pairs = prefs
      .split(",")
      .map { kv =>
        val split = kv.split("=")
        (split(0), split(1))
      }

    val color = pairs.find(_._1 == "color").map(_._2)
    val size = pairs.find(_._1 == "size").map(_._2)

    // emit a row if color and size are specified
    (color, size) match {
      case (Some(c), Some(s)) => collect(Row.of(c, s))
      case _ => // skip
    }
  }

  override def getResultType = new RowTypeInfo(Types.STRING, Types.STRING)
}

----------------------------------------

TITLE: SQL DDL for Dynamic Partition Writing
DESCRIPTION: SQL syntax example for writing to dynamic partitions in Hive tables using INSERT INTO/OVERWRITE commands.

LANGUAGE: sql
CODE:
INSERT { INTO | OVERWRITE } TABLE tablename1 select_statement1 FROM from_statement;

----------------------------------------

TITLE: Implementing ThreadLocal DataOutputSerializer for Split Serialization in Java
DESCRIPTION: Creates a ThreadLocal cache for DataOutputSerializer to optimize split serialization performance. This approach avoids creating new serializer instances for each split, improving efficiency when dealing with many splits.

LANGUAGE: Java
CODE:
ThreadLocal<DataOutputSerializer> SERIALIZER_CACHE =
ThreadLocal.withInitial(() -> new DataOutputSerializer(64));

----------------------------------------

TITLE: Defining Pandas UDF in PyFlink
DESCRIPTION: Shows how to define a vectorized Python UDF using pandas for improved performance

LANGUAGE: python
CODE:
@udf(input_types=[DataTypes.BIGINT(), DataTypes.BIGINT()], result_type=DataTypes.BIGINT(), udf_type="pandas")
def add(i, j):
  return i + j

----------------------------------------

TITLE: Configuring Autoscaler Settings in YAML
DESCRIPTION: Example of configuring autoscaler settings using the new key format without the 'kubernetes.operator.' prefix.

LANGUAGE: yaml
CODE:
job.autoscaler.enabled
job.autoscaler.metrics.window

----------------------------------------

TITLE: Implementing ElementConverter Interface in Java
DESCRIPTION: This code snippet shows the ElementConverter interface, which is used to convert input elements to the payload type required by the sink.

LANGUAGE: Java
CODE:
public interface ElementConverter<InputT, RequestEntryT> extends Serializable {
    RequestEntryT apply(InputT element, SinkWriter.Context context);
}

----------------------------------------

TITLE: Launching Flink TaskManager Container
DESCRIPTION: Starts a Docker container for a Flink TaskManager, connecting it to the same network as the JobManager.

LANGUAGE: bash
CODE:
docker run \
      --rm \
      --name=taskmanager \
      --network flink-network \
      --env FLINK_PROPERTIES="${FLINK_PROPERTIES}" \
      flink:1.11.1 taskmanager

----------------------------------------

TITLE: Implementing Unordered Result Checking in Java
DESCRIPTION: Overrides the default ordered result checking to support unordered data checks in integration tests.

LANGUAGE: java
CODE:
@Override
protected void checkResultWithSemantic(
  CloseableIterator<Pojo> resultIterator,
  List<List<Pojo>> testData,
  CheckpointingMode semantic,
  Integer limit) {
    if (limit != null) {
      Runnable runnable =
      () -> CollectIteratorAssertions.assertUnordered(resultIterator)
        .withNumRecordsLimit(limit)
        .matchesRecordsFromSource(testData, semantic);
      assertThat(runAsync(runnable)).succeedsWithin(DEFAULT_COLLECT_DATA_TIMEOUT);
    } else {
        CollectIteratorAssertions.assertUnordered(resultIterator)
                .matchesRecordsFromSource(testData, semantic);
    }
}

----------------------------------------

TITLE: Querying Temporal Table Function in SQL
DESCRIPTION: Example of querying a temporal table function in Flink SQL to get conversion rates at a specific point in time.

LANGUAGE: sql
CODE:
SELECT * FROM Rates('11:00');

----------------------------------------

TITLE: Schema Evolution Examples in SQL
DESCRIPTION: SQL statements demonstrating schema evolution capabilities including adding columns, renaming columns, and inserting data with evolved schema.

LANGUAGE: sql
CODE:
CREATE TABLE T (i INT, j INT);

INSERT INTO T (1, 1);

ALTER TABLE T ADD COLUMN k INT;

ALTER TABLE T RENAME COLUMN i to a;

INSERT INTO T (2, 2, 2);

SELECT * FROM T;

----------------------------------------

TITLE: Implementing StateFun Server with JavaScript SDK
DESCRIPTION: Example of creating a stateful function server using the new JavaScript SDK for NodeJS. The code demonstrates binding a greeter function that maintains state and sends messages using the StateFun framework.

LANGUAGE: javascript
CODE:
const http = require("http");
const {messageBuilder, StateFun, Context} = require("apache-flink-statefun");

let statefun = new StateFun();
statefun.bind({
    typename: "com.example.fns/greeter",
    fn(context, message) {
        const name = message.asString();
        let seen = context.storage.seen || 0;
        seen = seen + 1;
        context.storage.seen = seen;

        context.send(
            messageBuilder({typename: 'com.example.fns/inbox',
                            id: name,
                            value: `"Hello ${name} for the ${seen}th time!"`})
        );
    },
    specs: [{
        name: "seen",
        type: StateFun.intType(),
    }]
});

http.createServer(statefun.handler()).listen(8000);

----------------------------------------

TITLE: Testing Stateless FlatMap Operator in Flink
DESCRIPTION: Implementation and testing of a stateless FlatMapFunction using Flink's ListCollector for output collection.

LANGUAGE: java
CODE:
public class MyStatelessFlatMap implements FlatMapFunction<String, String> {
  @Override
  public void flatMap(String in, Collector<String> collector) throws Exception {
    String out = "hello " + in;
    collector.collect(out);
  }
}

LANGUAGE: java
CODE:
@Test
public void testFlatMap() throws Exception {
  MyStatelessFlatMap statelessFlatMap = new MyStatelessFlatMap();
  List<String> out = new ArrayList<>();
  ListCollector<String> listCollector = new ListCollector<>(out);
  statelessFlatMap.flatMap("world", listCollector);
  Assert.assertEquals(Lists.newArrayList("hello world"), out);
}

----------------------------------------

TITLE: Table API Data Processing in Scala
DESCRIPTION: Example showing how to convert a DataSet to a Table, perform transformations, and convert back to a DataSet in Apache Flink's Table API.

LANGUAGE: scala
CODE:
val execEnv = ExecutionEnvironment.getExecutionEnvironment
val tableEnv = TableEnvironment.getTableEnvironment(execEnv)

// obtain a DataSet from somewhere
val tempData: DataSet[(String, Long, Double)] =

// convert the DataSet to a Table
val tempTable: Table = tempData.toTable(tableEnv, 'location, 'time, 'tempF)
// compute your result
val avgTempCTable: Table = tempTable
 .where('location.like("room%"))
 .select(
   ('time / (3600 * 24)) as 'day, 
   'Location as 'room, 
   (('tempF - 32) * 0.556) as 'tempC
  )
 .groupBy('day, 'room)
 .select('day, 'room, 'tempC.avg as 'avgTempC)
// convert result Table back into a DataSet and print it
avgTempCTable.toDataSet[Row].print()

----------------------------------------

TITLE: Configuring Flink Job Health Check and Restart in YAML
DESCRIPTION: YAML configuration for enabling the operator to monitor Flink job health and trigger restarts based on retry count thresholds within a specified time window.

LANGUAGE: yaml
CODE:
kubernetes.operator.job.health-check.enabled: false
kubernetes.operator.job.restart-check.duration-window: 2m
kubernetes.operator.job.restart-check.threshold: 64

----------------------------------------

TITLE: Configuring PyFlink Thread Mode in Python Table API
DESCRIPTION: Example showing how to configure PyFlink execution mode between process and thread modes using table environment configuration.

LANGUAGE: python
CODE:
# Specify `process` mode
table_env.get_config().set("python.execution-mode", "process")

# Specify `thread` mode
table_env.get_config().set("python.execution-mode", "thread")

----------------------------------------

TITLE: Configuring Namespace and Flink Version Specific Defaults in YAML
DESCRIPTION: Example YAML configuration for setting default configurations specific to Flink versions and namespaces in the Kubernetes Operator. This allows for version-specific and namespace-specific settings.

LANGUAGE: yaml
CODE:
# Version Specific Defaults
kubernetes.operator.default-configuration.flink-version.v1_17.key: value

# Namespace Specific Defaults
kubernetes.operator.default-configuration.namespace.ns1.key: value

----------------------------------------

TITLE: Configuring Autoscaler Utilization Parameters
DESCRIPTION: This properties file snippet demonstrates how to configure the improved target utilization and utilization boundaries for the Flink autoscaler. It sets the target, minimum, and maximum utilization values.

LANGUAGE: properties
CODE:
job.autoscaler.utilization.target: 0.7
job.autoscaler.utilization.min: 0.5
job.autoscaler.utilization.max: 1.0

----------------------------------------

TITLE: Adding Cassandra Sink in Java
DESCRIPTION: This snippet demonstrates how to add a Cassandra sink to a Flink job, allowing writing data to Apache Cassandra with exactly-once guarantees for idempotent queries.

LANGUAGE: Java
CODE:
CassandraSink.addSink(input)

----------------------------------------

TITLE: Defining POJO Class Example in Java
DESCRIPTION: Example showing how to define a simple POJO (Plain Old Java Object) class for Person with id and name fields that can be used with Flink's serialization system.

LANGUAGE: java
CODE:
public class Person {
    public int id;
    public String name;
}

----------------------------------------

TITLE: Defining FlinkDeployment Custom Resource in YAML
DESCRIPTION: This YAML snippet defines a minimal FlinkDeployment custom resource for deploying a Flink application cluster. It specifies the Flink version, configuration, resource requirements, and job details.

LANGUAGE: yaml
CODE:
apiVersion: flink.apache.org/v1alpha1
kind: FlinkDeployment
metadata:
  namespace: default
  name: basic-example
spec:
  image: flink:1.14
  flinkVersion: v1_14
  flinkConfiguration:
    taskmanager.numberOfTaskSlots: "2"
  serviceAccount: flink
  jobManager:
    replicas: 1
    resource:
      memory: "2048m"
      cpu: 1
  taskManager:
    resource:
      memory: "2048m"
      cpu: 1
  job:
    jarURI: local:///opt/flink/examples/streaming/StateMachineExample.jar
    parallelism: 2
    upgradeMode: stateless

----------------------------------------

TITLE: Implementing Static Data Output in ImapSource
DESCRIPTION: Enhanced implementation of ImapSource that outputs static test data through the source context.

LANGUAGE: java
CODE:
import org.apache.flink.streaming.api.functions.source.RichSourceFunction;
import org.apache.flink.table.data.GenericRowData;
import org.apache.flink.table.data.RowData;
import org.apache.flink.table.data.StringData;

public class ImapSource extends RichSourceFunction<RowData> {
  @Override
  public void run(SourceContext<RowData> ctx) throws Exception {
      ctx.collect(GenericRowData.of(
          StringData.fromString("Subject 1"),
          StringData.fromString("Hello, World!")
      ));
  }

  @Override
  public void cancel(){}
}

----------------------------------------

TITLE: Enabling Full Snapshot Cleanup for State TTL in Apache Flink
DESCRIPTION: This code snippet shows how to enable automatic eviction of expired state when a full snapshot for a checkpoint or savepoint is taken in Apache Flink. It configures a 7-day TTL with full snapshot cleanup.

LANGUAGE: java
CODE:
StateTtlConfig ttlConfig = StateTtlConfig
    .newBuilder(Time.days(7))
    .cleanupFullSnapshot()
    .build();

----------------------------------------

TITLE: Configuring Unix Domain Sockets for Remote Modules
DESCRIPTION: YAML configuration showing how to enable Unix Domain Sockets (UDS) transport in remote modules for improved performance in co-located deployments.

LANGUAGE: yaml
CODE:
functions:
  - function:
     spec:
       - endpoint: http(s)+unix://<socket-file-path>/<serve-url-path>

----------------------------------------

TITLE: Configuring Custom RateLimitingStrategy in AsyncSinkWriter
DESCRIPTION: Example showing how to specify a custom RateLimitingStrategy in the AsyncSinkWriter configuration

LANGUAGE: java
CODE:
class MyCustomSinkWriter<InputT> extends AsyncSinkWriter<InputT, MyCustomRequestEntry> {

    MyCustomSinkWriter(
        ElementConverter<InputT, MyCustomRequestEntry> elementConverter,
        Sink.InitContext context,
        Collection<BufferedRequestState<MyCustomRequestEntry>> states) {
            super(
                elementConverter,
                context,
                AsyncSinkWriterConfiguration.builder()
                    // ...
                    .setRateLimitingStrategy(new TokenBucketRateLimitingStrategy())
                    .build(),
                states);
    }
    
}

----------------------------------------

TITLE: Configuring Batch Execution Mode via Command Line in Flink
DESCRIPTION: Shows how to set the execution runtime mode to BATCH using command line parameters when submitting a Flink job.

LANGUAGE: bash
CODE:
$ bin/flink run -Dexecution.runtime-mode=BATCH examples/streaming/WordCount.jar

----------------------------------------

TITLE: Implementing a Stateful Function in GoLang
DESCRIPTION: Demonstrates how to implement a stateful function using the new GoLang SDK in Stateful Functions 3.1.0, including state management and message handling.

LANGUAGE: go
CODE:
import (
  "fmt"
  "github.com/apache/flink-statefun/statefun-sdk-go/v3/pkg/statefun"
  "net/http"
)
 
type Greeter struct {
  SeenCount statefun.ValueSpec
}
 
func (g *Greeter) Invoke(ctx statefun.Context, message statefun.Message) error {
  storage := ctx.Storage()
 
  // Read the current value of the state
  // or zero value if no value is set
  var count int32
  storage.Get(g.SeenCount, &count)
 
  count += 1
 
  // Update the state which will
  // be made persistent by the runtime
  storage.Set(g.SeenCount, count)
 
  name := message.AsString()
  greeting := fmt.Sprintf("Hello there %s at the %d-th time!\n", name, count)
 
  ctx.Send(statefun.MessageBuilder{
     Target:    *ctx.Caller(),
     Value:     greeting,
  })
 
  return nil
}
 
 
func main() {
  greeter := &Greeter{
     SeenCount: statefun.ValueSpec{
        Name:      "seen_count",
        ValueType: statefun.Int32Type,
     },
  }
 
  builder := statefun.StatefulFunctionsBuilder()
  _ = builder.WithSpec(statefun.StatefulFunctionSpec{
     FunctionType: statefun.TypeNameFrom("com.example.fns/greeter"),
     States:       []statefun.ValueSpec{greeter.SeenCount},
     Function:     greeter,
  })
 
  http.Handle("/statefun", builder.AsHandler())
  _ = http.ListenAndServe(":8000", nil)
}

----------------------------------------

TITLE: Enabling RocksDB Compaction Filter for State TTL in Apache Flink
DESCRIPTION: This snippet shows how to enable the RocksDB compaction filter cleanup strategy for State TTL in Apache Flink. It configures a 7-day TTL with RocksDB compaction filter cleanup.

LANGUAGE: java
CODE:
StateTtlConfig ttlConfig = StateTtlConfig
    .newBuilder(Time.days(7))
    .cleanupInRocksdbCompactFilter()
    .build();

----------------------------------------

TITLE: Launching Flink JobManager Container
DESCRIPTION: Starts a Docker container for the Flink JobManager, configuring network settings and exposing the web interface port.

LANGUAGE: bash
CODE:
docker run \
       --rm \
       --name=jobmanager \
       --network flink-network \
       -p 8081:8081 \
       --env FLINK_PROPERTIES="${FLINK_PROPERTIES}" \
       flink:1.11.1 jobmanager

----------------------------------------

TITLE: Dynamic State Registration in Java SDK
DESCRIPTION: Demonstrates the new PersistedStateRegistry construct that enables dynamic registration of persisted state at runtime, rather than requiring eager definition in the function class.

LANGUAGE: java
CODE:
public class MyFunction implements StatefulFunction {
    @Persisted
    private final PersistedStateRegistry registry = new PersistedStateRegistry();
    private final PersistedValue<String> myValue;

    public void invoke(Context context, Object input) {
        if (myValue == null) {
            myValue = registry.registerValue(PersistedValue.of("my-value", String.class));
        }
        ...
    }
}

----------------------------------------

TITLE: Configuring Batch Execution Mode in Flink
DESCRIPTION: Examples showing how to configure batch execution mode through command line and programmatically.

LANGUAGE: bash
CODE:
bin/flink run -Dexecution.runtime-mode=BATCH examples/streaming/WordCount.jar

LANGUAGE: java
CODE:
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

env.setRuntimeMode(RuntimeMode.BATCH);

----------------------------------------

TITLE: Configuring DynamoDB Connector Dependencies in Maven
DESCRIPTION: Maven dependency configuration example showing how to include the DynamoDB connector for Flink 1.17. Demonstrates the new versioning format which follows <major>.<minor>.<patch>-<flink-major>.<flink-minor>.

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-dynamodb</artifactId>
    <version>4.1.0-1.17</version>
</dependency>

----------------------------------------

TITLE: State Cleanup Timer Implementation
DESCRIPTION: Timer-based cleanup logic for removing outdated events from state based on the widest window rule.

LANGUAGE: java
CODE:
@Override
public void onTimer(final long timestamp, final OnTimerContext ctx, final Collector<Alert> out)
    throws Exception {

  Rule widestWindowRule = ctx.getBroadcastState(Descriptors.rulesDescriptor).get(WIDEST_RULE_KEY);

  Optional<Long> cleanupEventTimeWindow =
      Optional.ofNullable(widestWindowRule).map(Rule::getWindowMillis);
  Optional<Long> cleanupEventTimeThreshold =
      cleanupEventTimeWindow.map(window -> timestamp - window);
  cleanupEventTimeThreshold.ifPresent(this::evictOutOfScopeElementsFromWindow);
}

private void evictOutOfScopeElementsFromWindow(Long threshold) {
  try {
    Iterator<Long> keys = windowState.keys().iterator();
    while (keys.hasNext()) {
      Long stateEventTime = keys.next();
      if (stateEventTime < threshold) {
        keys.remove();
      }
    }
  } catch (Exception ex) {
    throw new RuntimeException(ex);
  }
}

----------------------------------------

TITLE: Stop Flink Job with Savepoint Command
DESCRIPTION: CLI command to stop a Flink job with a savepoint, ensuring output persistence for exactly-once sinks.

LANGUAGE: bash
CODE:
bin/flink stop -p [:targetDirectory] :jobId

----------------------------------------

TITLE: DataStream Integration with Java SDK
DESCRIPTION: Shows how to combine Flink DataStream API pipelines with Stateful Functions programming constructs. Enables building complex streaming applications with both APIs.

LANGUAGE: java
CODE:
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

DataStream<RoutableMessage> namesIngress = ...

StatefulFunctionEgressStreams egresses =
    StatefulFunctionDataStreamBuilder.builder("example")
        .withDataStreamAsIngress(namesIngress)
        .withRequestReplyRemoteFunction(
            RequestReplyFunctionBuilder.requestReplyFunctionBuilder(
                    REMOTE_GREET, URI.create("http://..."))
                .withPersistedState("seen_count")
        .withFunctionProvider(GREET, unused -> new MyFunction())
        .withEgressId(GREETINGS)
        .build(env);

DataStream<String> responsesEgress = getDataStreamForEgressId(GREETINGS);

----------------------------------------

TITLE: Demonstrating PLAN_ADVICE for Optimization Suggestions in Flink SQL
DESCRIPTION: Example of PLAN_ADVICE output suggesting optimization by enabling local-global two-phase aggregation in Flink SQL queries.

LANGUAGE: sql
CODE:
== Optimized Physical Plan With Advice ==
...
advice[1]: [ADVICE] You might want to enable local-global two-phase optimization by configuring ('table.optimizer.agg-phase-strategy' to 'AUTO').

----------------------------------------

TITLE: Fraud Detection Rule JSON Schema
DESCRIPTION: JSON structure defining parameters for fraud detection rules including grouping fields, aggregation functions, limits and window duration.

LANGUAGE: json
CODE:
{
  "ruleId": 1,
  "ruleState": "ACTIVE",
  "groupingKeyNames": ["payerId", "beneficiaryId"],
  "aggregateFieldName": "paymentAmount",
  "aggregatorFunctionType": "SUM",
  "limitOperatorType": "GREATER",
  "limit": 1000000,
  "windowMinutes": 10080
}

----------------------------------------

TITLE: Sessionizing Clickstream with SQL
DESCRIPTION: SQL query example showing how to count clicks per session using Flink SQL. Demonstrates session window aggregation.

LANGUAGE: sql
CODE:
SELECT userId, COUNT(*)
FROM clicks
GROUP BY SESSION(clicktime, INTERVAL '30' MINUTE), userId

----------------------------------------

TITLE: Configuration Migration Example
DESCRIPTION: Example of migrating from flink-conf.yaml to config.yaml format

LANGUAGE: yaml
CODE:
# Old flink-conf.yaml
rest.port: 8081

# New config.yaml
rest:
  port: 8081

----------------------------------------

TITLE: Installing Flink Kubernetes Operator 1.3.0 using Helm
DESCRIPTION: Bash commands to add the Flink Kubernetes Operator 1.3.0 Helm chart repository and install the operator with webhooks disabled.

LANGUAGE: bash
CODE:
$ helm repo add flink-kubernetes-operator-1.3.0 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.3.0/
$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.3.0/flink-kubernetes-operator --set webhook.create=false

----------------------------------------

TITLE: Basic Flink Transaction Stream Keying
DESCRIPTION: Example showing basic stream keying by account ID using Flink's DataStream API.

LANGUAGE: java
CODE:
DataStream<Transaction> input = // [...]
DataStream<...> windowed = input
  .keyBy(Transaction::getAccountId)
  .window(/*window specification*/);

----------------------------------------

TITLE: Sending Delayed Messages with Cancellation in Python
DESCRIPTION: Demonstrates how to send a delayed message with a cancellation token in Stateful Functions 3.1.0 using Python. This allows setting time limits on operations with the ability to cancel if no longer needed.

LANGUAGE: python
CODE:
context.send_after(timedelta(days=3),
                  message_builder(target_typename="fns/onboarding",
                                  target_id="user-1234",
                                  str_value="send a reminder email"),
                  cancellation_token="flow-1234")

----------------------------------------

TITLE: Configuring Test Context Factory in Java
DESCRIPTION: Sets up the test context factory to provide Flink with means to interact with the backend during integration tests.

LANGUAGE: java
CODE:
@TestContext
TestContextFactory contextFactory = new TestContextFactory(testEnvironment);

----------------------------------------

TITLE: Using SESSION Window TVF in Streaming Mode
DESCRIPTION: Demonstrates how to use SESSION Window Table-Valued Function (TVF) in streaming mode, including partitioning and aggregation.

LANGUAGE: sql
CODE:
SELECT * FROM TABLE(
   SESSION(TABLE Bid PARTITION BY item, DESCRIPTOR(bidtime), INTERVAL '5' MINUTES));

SELECT window_start, window_end, item, SUM(price) AS total_price
FROM TABLE(
    SESSION(TABLE Bid PARTITION BY item, DESCRIPTOR(bidtime), INTERVAL '5' MINUTES))
GROUP BY item, window_start, window_end;

----------------------------------------

TITLE: Implementing Custom Flink Metrics in Java
DESCRIPTION: Java code snippet demonstrating how to add custom metrics to a Flink job using a RichMapFunction. It shows the implementation of an event counter metric.

LANGUAGE: java
CODE:
class FlinkMetricsExposingMapFunction extends RichMapFunction<Integer, Integer> {
  private transient Counter eventCounter;

  @Override
  public void open(Configuration parameters) {
    eventCounter = getRuntimeContext().getMetricGroup().counter("events");
  }

  @Override
  public Integer map(Integer value) {
    eventCounter.inc();
    return value;
  }
}

----------------------------------------

TITLE: Configuring Remote Module Name in Flink
DESCRIPTION: Configuration example showing how to override the default remote module file name in Flink configuration.

LANGUAGE: yaml
CODE:
statefun.remote.module-name: /flink/usrlib/prod.yaml

----------------------------------------

TITLE: Configuring Email Message Collection Logic
DESCRIPTION: Implements message collection and processing logic to handle incoming emails and convert them to Flink row data format.

LANGUAGE: Java
CODE:
private void collectMessage(SourceFunction.SourceContext<RowData> ctx, Message message)
    throws MessagingException {
    final GenericRowData row = new GenericRowData(columnNames.size());

    for (int i = 0; i < columnNames.size(); i++) {
        switch (columnNames.get(i)) {
            case "SUBJECT":
                row.setField(i, StringData.fromString(message.getSubject()));
                break;
            case "SENT":
                row.setField(i, TimestampData.fromInstant(message.getSentDate().toInstant()));
                break;
            case "RECEIVED":
                row.setField(i, TimestampData.fromInstant(message.getReceivedDate().toInstant()));
                break;
        }
    }

    ctx.collect(row);
}

----------------------------------------

TITLE: Installing Flink Kubernetes Operator via Helm
DESCRIPTION: Helm commands for adding the Flink Kubernetes Operator repository and installing the operator with webhook creation disabled.

LANGUAGE: bash
CODE:
$ helm repo add flink-kubernetes-operator-1.2.0 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.2.0/
$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.2.0/flink-kubernetes-operator --set webhook.create=false

----------------------------------------

TITLE: Comparing Java Windowing Implementation Classes in Flink
DESCRIPTION: Reference to two Java class implementations demonstrating windowing with and without incremental aggregation: WindowingJob (with AggregateFunction) and WindowingJobNoAggregation (without incremental aggregation).

LANGUAGE: java
CODE:
// Referenced classes:
// WindowingJob.java
// WindowingJobNoAggregation.java

----------------------------------------

TITLE: Implementing Scala UDF for String Uppercase Conversion in Flink
DESCRIPTION: Demonstrates how to create a Scala scalar function that converts strings to uppercase and register it as a UDF in Flink's table environment.

LANGUAGE: scala
CODE:
class ScalaUpper extends ScalarFunction {
def eval(str: String) = str.toUpperCase
}
btenv.registerFunction("scala_upper", new ScalaUpper())


----------------------------------------

TITLE: Starting Kubernetes Proxy for Flink Dashboard Access
DESCRIPTION: Command to start the Kubernetes proxy for accessing the Flink dashboard from outside the cluster.

LANGUAGE: bash
CODE:
kubectl proxy

----------------------------------------

TITLE: Installing Kafka and Flink Operators with KUDO
DESCRIPTION: Commands to install Kafka and Flink Operators using KUDO, without creating instances.

LANGUAGE: bash
CODE:
kubectl kudo install kafka --version=1.2.0 --skip-instance
kubectl kudo install flink --version=0.2.1 --skip-instance

----------------------------------------

TITLE: Configuring Incremental Cleanup for Heap State Backends in Apache Flink
DESCRIPTION: This example demonstrates how to enable incremental cleanup for Heap state backends in Apache Flink. It sets a 7-day TTL and configures the cleanup to check 10 keys for every state access.

LANGUAGE: java
CODE:
StateTtlConfig ttlConfig = StateTtlConfig
    .newBuilder(Time.days(7))
    // check 10 keys for every state access
    .cleanupIncrementally(10, false)
    .build();

----------------------------------------

TITLE: Comparing Java Enrichment Implementation Classes in Flink
DESCRIPTION: Reference to two Java class implementations showing synchronous vs asynchronous I/O approaches: EnrichingJobSync (synchronous implementation) and EnrichingJobAsync (using Flink's Async I/O API).

LANGUAGE: java
CODE:
// Referenced classes:
// EnrichingJobSync.java
// EnrichingJobAsync.java

----------------------------------------

TITLE: Configuring Checkpointing Semantics in Java
DESCRIPTION: Specifies the checkpointing semantics for the source, verifying support for exactly-once processing.

LANGUAGE: java
CODE:
@TestSemantics
CheckpointingMode[] semantics = new CheckpointingMode[] {CheckpointingMode.EXACTLY_ONCE};

----------------------------------------

TITLE: Initializing Kinesis Consumer in Java
DESCRIPTION: This code shows how to create a Kinesis consumer in Flink 1.1.0, which allows reading from Amazon Kinesis Streams.

LANGUAGE: Java
CODE:
DataStream<String> kinesis = env.addSource(
  new FlinkKinesisConsumer<>("stream-name", schema, config));

----------------------------------------

TITLE: Checking KUDO Plan Status
DESCRIPTION: Command to check the status of the KUDO deployment plan for the Flink demo.

LANGUAGE: bash
CODE:
kubectl kudo plan status --instance flink-demo

----------------------------------------

TITLE: Implementing Basic ImapSource with RichSourceFunction
DESCRIPTION: Initial implementation of the ImapSource class extending RichSourceFunction to define the runtime behavior of the connector.

LANGUAGE: java
CODE:
import org.apache.flink.streaming.api.functions.source.RichSourceFunction;
import org.apache.flink.table.data.RowData;

public class ImapSource extends RichSourceFunction<RowData> {
  @Override
  public void run(SourceContext<RowData> ctx) throws Exception {}

  @Override
  public void cancel() {}
}

----------------------------------------

TITLE: Explicit Type Declaration in Scala
DESCRIPTION: Examples demonstrating the preferred way to declare types explicitly for class members in Scala, rather than relying on type inference. Shows both incorrect and correct approaches.

LANGUAGE: scala
CODE:
var expressions = new java.util.ArrayList[String]()

LANGUAGE: scala
CODE:
var expressions: java.util.List[String] = new java.util.ArrayList[]()

----------------------------------------

TITLE: Installing KUDO CLI with Homebrew
DESCRIPTION: Commands to install the KUDO CLI using Homebrew package manager on macOS.

LANGUAGE: bash
CODE:
brew tap kudobuilder/tap
brew install kudo-cli

----------------------------------------

TITLE: Enabling Incremental Checkpointing in Scala
DESCRIPTION: Code example showing how to enable incremental checkpointing with RocksDB state backend in a Scala Flink application. The boolean parameter 'true' enables incremental checkpointing.

LANGUAGE: scala
CODE:
val env = StreamExecutionEnvironment.getExecutionEnvironment()
env.setStateBackend(new RocksDBStateBackend(filebackend, true))

----------------------------------------

TITLE: Configuring RocksDBStateBackend at Cluster Level in Flink
DESCRIPTION: This YAML configuration sets RocksDB as the default state backend for the entire Flink cluster, enables incremental checkpointing, and specifies the location to store checkpoints.

LANGUAGE: yaml
CODE:
state.backend: rocksdb
state.backend.incremental: true
state.checkpoints.dir: hdfs:///flink-checkpoints

----------------------------------------

TITLE: SQL Table Creation and Query
DESCRIPTION: SQL commands to create and query a table using the custom IMAP connector.

LANGUAGE: sql
CODE:
CREATE TABLE T (subject STRING, content STRING) WITH ('connector' = 'imap');

SELECT * FROM T;

----------------------------------------

TITLE: Using Flink Expressions for Logical Queries - Scala
DESCRIPTION: Illustrates the use of Flink's new table module for logical queries. The example shows reading CSV files, assigning logical schemas, and applying transformations like filters and joins using logical attributes.

LANGUAGE: scala
CODE:
val customers = getCustomerDataSet(env)
 .as('id, 'mktSegment)
 .filter( 'mktSegment === "AUTOMOBILE" )

val orders = getOrdersDataSet(env)
 .filter( o => dateFormat.parse(o.orderDate).before(date) )
 .as('orderId, 'custId, 'orderDate, 'shipPrio)

val items =
 orders.join(customers)
   .where('custId === 'id)
   .select('orderId, 'orderDate, 'shipPrio)

----------------------------------------

TITLE: Installing ZooKeeper Operator with KUDO
DESCRIPTION: Command to install the ZooKeeper Operator using KUDO, without creating an instance.

LANGUAGE: bash
CODE:
kubectl kudo install zookeeper --version=0.3.0 --skip-instance

----------------------------------------

TITLE: Graph Transformation Example in Java
DESCRIPTION: Example showing how to transform a graph by chaining multiple operations like getting an undirected graph and mapping edges.

LANGUAGE: java
CODE:
inputGraph.getUndirected().mapEdges(new CustomEdgeMapper());

----------------------------------------

TITLE: Cloning KUDO Operators Repository
DESCRIPTION: Command to clone the KUDO operators repository from GitHub.

LANGUAGE: bash
CODE:
git clone https://github.com/kudobuilder/operators.git

----------------------------------------

TITLE: SQL DDL for Function Creation
DESCRIPTION: SQL syntax for creating different types of functions including temporary and system functions.

LANGUAGE: sql
CODE:
CREATE [TEMPORARY|TEMPORARY SYSTEM] FUNCTION 

  [IF NOT EXISTS] [catalog_name.][db_name.]function_name 

AS identifier [LANGUAGE JAVA|SCALA]

----------------------------------------

TITLE: Configuring Memory for Flink Application Mode
DESCRIPTION: Example showing how to specify JobManager and TaskManager memory sizes when launching a Flink application.

LANGUAGE: bash
CODE:
./bin/flink run-application -t yarn-application \
    -Djobmanager.memory.process.size=2048m \
    -Dtaskmanager.memory.process.size=4096m \
    ./MyApplication.jar

----------------------------------------

TITLE: Code Nesting Example - Do
DESCRIPTION: Example of improved code structure using early returns to reduce nesting.

LANGUAGE: java
CODE:
if (!a) {
	return ..
}

if (!b) {
	return ...
}

if (!c) {
	return ...
}

the main path

----------------------------------------

TITLE: Configuring Prometheus Reporter in Flink
DESCRIPTION: YAML configuration for enabling the Prometheus reporter in Flink's flink-conf.yaml file. It specifies the reporter class and the port to expose metrics.

LANGUAGE: yaml
CODE:
metrics.reporters: prom
metrics.reporter.prom.class: org.apache.flink.metrics.prometheus.PrometheusReporter
metrics.reporter.prom.port: 9999

----------------------------------------

TITLE: Network Stack Configuration Example
DESCRIPTION: Example configuration to disable credit-based flow control in Flink

LANGUAGE: yaml
CODE:
taskmanager.network.credit-model: false

----------------------------------------

TITLE: Code Nesting Example - Don't
DESCRIPTION: Example of poor code structure with deep nesting that reduces readability.

LANGUAGE: java
CODE:
if (a) {
    if (b) {
        if (c) {
            the main path
        }
    }
}

----------------------------------------

TITLE: Running a Local Flink Cluster with Docker
DESCRIPTION: Command to start a local Flink cluster using Docker, with one TaskManager and the Web UI exposed on port 8081.

LANGUAGE: bash
CODE:
docker run -t -p 8081:8081 flink local

----------------------------------------

TITLE: Configuring Pulsar Source in Flink 1.6+
DESCRIPTION: Demonstrates how to create and configure a Pulsar consumer as a source for Flink DataStream, including service URL, topic, and subscription settings.

LANGUAGE: java
CODE:
PulsarSourceBuilder<String>builder = PulsarSourceBuilder  
  .builder(new SimpleStringSchema()) 
  .serviceUrl(serviceUrl)
  .topic(inputTopic)
  .subsciptionName(subscription);
SourceFunction<String> src = builder.build();
DataStream<String> words = env.addSource(src);

----------------------------------------

TITLE: Configuring Prometheus Scrape Targets
DESCRIPTION: YAML configuration for Prometheus to scrape metrics from Flink job managers and task managers. It defines static targets for a basic setup.

LANGUAGE: yaml
CODE:
scrape_configs:
- job_name: 'flink'
  static_configs:
  - targets: ['job-cluster:9999', 'taskmanager1:9999', 'taskmanager2:9999']

----------------------------------------

TITLE: Simulating JobManager Failure
DESCRIPTION: Command to kill the JobManager pod to test the HA functionality.

LANGUAGE: bash
CODE:
$ kubectl exec {jobmanager_pod_name} -- /bin/sh -c "kill 1"

----------------------------------------

TITLE: Java Class Comment Example - Do
DESCRIPTION: Example of a good class-level JavaDoc comment that clearly explains the purpose and role of the class.

LANGUAGE: java
CODE:
/**
 * An expression that wraps a single specific symbol.
 * A symbol could be a unit, an alias, a variable, etc.
 */
public class CommonSymbolExpression {}

----------------------------------------

TITLE: Pulling the Latest Flink Docker Image
DESCRIPTION: Command to download the latest version of the Apache Flink Docker image from Docker Hub.

LANGUAGE: bash
CODE:
docker pull flink

----------------------------------------

TITLE: Python JSON Processing UDF Implementation
DESCRIPTION: Example Python UDF that processes JSON data by converting a field to lowercase, demonstrating typical PyFlink UDF usage.

LANGUAGE: python
CODE:
@udf(result_type=DataTypes.STRING(), func_type="general")
def json_value_lower(s: str):
    import json
    a = json.loads(s)
    a['a'] = a['a'].lower()
    return json.dumps(a)

----------------------------------------

TITLE: Adding AsyncBaseSink Dependency in Maven
DESCRIPTION: This XML snippet shows how to add the flink-connector-base dependency to a Maven project, which is required to use the AsyncBaseSink.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-connector-base</artifactId>
  <version>${flink.version}</version>
</dependency>

----------------------------------------

TITLE: Implementing Avro Serialization Snapshot in Flink
DESCRIPTION: Implementation of TypeSerializerSnapshot interface for Apache Avro serialization support with schema evolution capabilities.

LANGUAGE: java
CODE:
public class AvroSerializerSnapshot<T> implements TypeSerializerSnapshot<T> {
  private Schema runtimeSchema;
  private Schema previousSchema;

  @SuppressWarnings("WeakerAccess")
  public AvroSerializerSnapshot() { }

  AvroSerializerSnapshot(Schema schema) {
    this.runtimeSchema = schema;
  }

----------------------------------------

TITLE: Optimizing Map Operations in Java Collections
DESCRIPTION: Examples of efficient map operations in Java, avoiding multiple lookups and using more performant methods.

LANGUAGE: Java
CODE:
// Instead of:
if (map.contains(key)) {
    value = map.get(key);
}

// Do:
value = map.get(key);
if (value != null) {
    // use value
}

// Instead of:
if (!map.contains(key)) {
    map.put(key, value);
}

// Do:
map.putIfAbsent(key, value);
// or
map.computeIfAbsent(key, k -> computeValue(k));

----------------------------------------

TITLE: Defining Monitoring Event Classes in Java
DESCRIPTION: Defines abstract MonitoringEvent class and its subclasses TemperatureEvent and PowerEvent to represent data center monitoring events.

LANGUAGE: java
CODE:
public abstract class MonitoringEvent {
    private int rackID;
    ...
}

public class TemperatureEvent extends MonitoringEvent {
    private double temperature;
    ...
}

public class PowerEvent extends MonitoringEvent {
    private double voltage;
    ...
}

----------------------------------------

TITLE: Apache License Header Template
DESCRIPTION: Standard Apache License header that must be included at the top of every source file in the Flink project.

LANGUAGE: java
CODE:
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

----------------------------------------

TITLE: Alternative Flink-Style Topology Execution
DESCRIPTION: Simplified approach for executing Storm topologies using Flink's context-based execution model.

LANGUAGE: java
CODE:
FlinkTopology topology = FlinkTopology.createTopology(builder);

topology.execute()

----------------------------------------

TITLE: Generating Temperature Warnings from Matched Patterns
DESCRIPTION: Uses a PatternSelectFunction to generate TemperatureWarning events from matched patterns.

LANGUAGE: java
CODE:
DataStream<TemperatureWarning> warnings = tempPatternStream.select(
    (Map<String, MonitoringEvent> pattern) -> {
        TemperatureEvent first = (TemperatureEvent) pattern.get("First Event");
        TemperatureEvent second = (TemperatureEvent) pattern.get("Second Event");

        return new TemperatureWarning(
            first.getRackID(), 
            (first.getTemperature() + second.getTemperature()) / 2);
    }
);

----------------------------------------

TITLE: Triggering Savepoint with Specified Format
DESCRIPTION: Commands to trigger a savepoint or stop a job with a savepoint, specifying the desired format (native or canonical). This allows creating savepoints in the native format of state backends for improved performance.

LANGUAGE: bash
CODE:
# take an intermediate savepoint
$ bin/flink savepoint --type [native/canonical] :jobId [:targetDirectory]

# stop the job with a savepoint
$ bin/flink stop --type [native/canonical] --savepointPath [:targetDirectory] :jobId

----------------------------------------

TITLE: Pushing Changes to GitHub
DESCRIPTION: Git command to push local changes to a dedicated branch on the forked repository before creating a pull request.

LANGUAGE: bash
CODE:
git push origin myBranch

----------------------------------------

TITLE: Assembling Storm Topology in Flink
DESCRIPTION: Example showing how to assemble a Storm Word Count topology using standard Storm syntax that can be executed in Flink. Demonstrates setting up Spouts and Bolts with appropriate groupings.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("source", new StormFileSpout(inputFilePath));
builder.setBolt("tokenizer", new StormBoltTokenizer())
       .shuffleGrouping("source");
builder.setBolt("counter", new StormBoltCounter())
       .fieldsGrouping("tokenizer", new Fields("word"));
builder.setBolt("sink", new StormBoltFileSink(outputFilePath))
       .shuffleGrouping("counter");

----------------------------------------

TITLE: Executing Flink Job with Restore Mode
DESCRIPTION: Command to run a Flink job, specifying a savepoint path and restore mode. This allows control over how Flink handles ownership of snapshot files after restoration.

LANGUAGE: bash
CODE:
$ bin/flink run -s :savepointPath -restoreMode :mode -n [:runArgs]

----------------------------------------

TITLE: Embedding Storm Components in Flink DataStream API
DESCRIPTION: Example showing how to integrate Storm Spouts and Bolts within a Flink DataStream program using wrapper classes and type specification.

LANGUAGE: java
CODE:
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

DataStream<Tuple1<String>> source = 
  env.addSource(new SpoutWrapper<String>(new FileSpout(localFilePath)), 
                TypeExtractor.getForObject(new Tuple1<String>(""))); 
DataStream<Tuple1<String>> text = source.setParallelism(1);

DataStream<Tuple2<String,Integer> tokens = text.flatMap(new Tokenizer()).keyBy(0);

DataStream<Tuple2<String,Integer> counts =
  tokens.transform("Counter",
                   TypeExtractor.getForObject(new Tuple2<String,Integer>("",0))
                   new BoltWrapper<String,Tuple2<String,Integer>>(new BoltCounter()));

counts.writeAsText(outputPath);

env.execute("WordCount with Spout source and Bolt counter");

----------------------------------------

TITLE: Enabling Incremental Checkpointing in Java
DESCRIPTION: Code example showing how to enable incremental checkpointing with RocksDB state backend in a Java Flink application. The boolean parameter 'true' enables incremental checkpointing.

LANGUAGE: java
CODE:
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.setStateBackend(new RocksDBStateBackend(filebackend, true));

----------------------------------------

TITLE: Switching to Website Branch
DESCRIPTION: Commands to navigate to the website directory and checkout the asf-site branch containing the website content.

LANGUAGE: bash
CODE:
cd flink-web
git checkout asf-site

----------------------------------------

TITLE: Adding External Documentation Links
DESCRIPTION: Markdown syntax for adding external links to official Flink documentation within website content.

LANGUAGE: markdown
CODE:
{{</* docs_link file="relative_path/" name="Title"*/>}}

LANGUAGE: markdown
CODE:
{{</* docs_link file="flink-docs-stable/docs/dev/datastream/side_output/" name="Side Output"*/>}}

----------------------------------------

TITLE: Implementing ServerFun in Flink Stateful Functions
DESCRIPTION: This snippet shows the structure of the ServerFun class, which represents a digital twin for each physical server. It handles metric reports, maintains a sliding window of metrics, and evaluates server health.

LANGUAGE: Java
CODE:
@FunctionType(type = "datacenter/server")
public final class ServerFun implements StatefulFunction {
    private final ValueSpec<ServerHealthState> serverHealthState =
        ValueSpec.named("health").withJsonType(ServerHealthState.class);
    private final ValueSpec<CircularBuffer> metricsHistory =
        ValueSpec.named("metrics").withCustomType(CircularBuffer.class);

    @Override
    public void invoke(Context context, Object input) {
        if (input instanceof ServerMetricReport) {
            onServerMetricReport(context, (ServerMetricReport) input);
        } else if (input instanceof CommissionServer) {
            onCommissionServer(context, (CommissionServer) input);
        } else if (input instanceof DecommissionServer) {
            onDecommissionServer(context, (DecommissionServer) input);
        }
    }

    private void onServerMetricReport(Context context, ServerMetricReport report) {
        ServerHealthState health = context.storage().get(serverHealthState).orElse(ServerHealthState.empty());
        CircularBuffer history = context.storage().get(metricsHistory).orElse(CircularBuffer.empty());
        
        history.add(report);
        context.storage().set(metricsHistory, history);
        
        ServerHealthState newHealth = evaluateServerHealth(report, history, health);
        context.storage().set(serverHealthState, newHealth);
        
        if (!newHealth.isHealthy()) {
            context.send(KafkaEgress.of(ALERTS_TOPIC), new Alert(context.self(), newHealth));
            context.send(rackAddress(context.self()), new RackIncidentReport(context.self(), newHealth.incidents()));
        }
    }
}

----------------------------------------

TITLE: Executing Storm Topology in Flink
DESCRIPTION: Code showing how to transform and submit a Storm topology to a Flink cluster. Demonstrates both local and remote execution options using FlinkLocalCluster and FlinkSubmitter.

LANGUAGE: java
CODE:
FlinkTopology topology = FlinkTopology.createTopology(builder);

Config conf = new Config();
if(runLocal) {
	FlinkLocalCluster cluster = FlinkLocalCluster.getLocalCluster();
	cluster.submitTopology("WordCount", conf, topology);
} else {
	FlinkSubmitter.submitTopology("WordCount", conf, topology);
}

----------------------------------------

TITLE: Defining and Using Named Parameters for Table Functions in Flink
DESCRIPTION: Illustrates how to define a table function with named parameters using Java annotations and how to call it in SQL using named arguments.

LANGUAGE: java
CODE:
public static class NamedArgumentsTableFunction extends TableFunction<Object> {

	@FunctionHint(
			output = @DataTypeHint("STRING"),
			arguments = {
					@ArgumentHint(name = "in1", isOptional = false, type = @DataTypeHint("STRING")),
					@ArgumentHint(name = "in2", isOptional = true, type = @DataTypeHint("STRING")),
					@ArgumentHint(name = "in3", isOptional = true, type = @DataTypeHint("STRING"))})
	public void eval(String arg1, String arg2, String arg3) {
		collect(arg1 + ", " + arg2 + "," + arg3);
	}

}

LANGUAGE: sql
CODE:
SELECT * FROM TABLE(myFunction(in1 => 'v1', in3 => 'v3', in2 => 'v2'));

SELECT * FROM TABLE(myFunction(in1 => 'v1'));

----------------------------------------

TITLE: Writing Snapshot Configuration in Flink
DESCRIPTION: Shows how to implement snapshot version control and configuration writing in TypeSerializerSnapshot.

LANGUAGE: java
CODE:
  @Override
  public int getCurrentVersion() {
    return 1;
  }

  @Override
  public void writeSnapshot(DataOutputView out) throws IOException {
    out.writeUTF(runtimeSchema.toString(false));
  }

----------------------------------------

TITLE: Pushing Documentation Changes
DESCRIPTION: Git command to push local documentation changes to a remote branch on the forked repository.

LANGUAGE: bash
CODE:
git push origin myBranch

----------------------------------------

TITLE: Adding Custom Metrics in Java
DESCRIPTION: This code example shows how to add custom metrics (in this case, a counter) to a Flink application using the new metrics system in version 1.1.0.

LANGUAGE: Java
CODE:
Counter counter = getRuntimeContext()
  .getMetricGroup()
  .counter("my-counter");

----------------------------------------

TITLE: Viewing RocksDB Threads in Flink TaskManager Process
DESCRIPTION: This bash command shows RocksDB threads running within a Flink TaskManager process on Linux.

LANGUAGE: bash
CODE:
$ ps -T -p 32513 | grep -i rocksdb

----------------------------------------

TITLE: Defining Serial Version UID in Java
DESCRIPTION: Shows how to properly define a Serial Version UID for serializable classes in Java.

LANGUAGE: Java
CODE:
private static final long serialVersionUID = 1L;

----------------------------------------

TITLE: Calculating Subpartition Range for Consumer Execution Vertices
DESCRIPTION: Formula to determine the range of subpartitions consumed by each execution vertex in the flexible subpartition mapping approach.

LANGUAGE: text
CODE:
[k * P / N, (k + 1) * P / N)

----------------------------------------

TITLE: Original Task Finish Protocol Implementation
DESCRIPTION: Shows the protocol steps for how tasks finish when sources complete in Flink before FLIP-147. This includes emitting MAX_WATERMARK, handling end of input, and cleanup procedures.

LANGUAGE: text
CODE:
1. Source operators emit MAX_WATERMARK
2. On received MAX_WATERMARK for non-source operators
    a. Trigger all the event-time timers
    b. Emit MAX_WATERMARK
3. Source tasks finished
    a. endInput(inputId) for all the operators
    b. close() for all the operators
    c. dispose() for all the operators
    d. Emit EndOfPartitionEvent
    e. Task cleanup
4. On received EndOfPartitionEvent for non-source tasks
    a. endInput(int inputId) for all the operators
    b. close() for all the operators
    c. dispose() for all the operators
    d. Emit EndOfPartitionEvent
    e. Task cleanup

----------------------------------------

TITLE: Using Table API for Data Transformation in Java
DESCRIPTION: This code example shows how to use the Table API in Flink 1.1.0 to perform SQL-like operations on DataSet or DataStream.

LANGUAGE: Java
CODE:
Table custT = tableEnv
  .toTable(custDs, "name, zipcode")
  .where("zipcode = '12345'")
  .select("name")

----------------------------------------

TITLE: POJO Data Type Definition
DESCRIPTION: Example POJO class definition showing the structure used in serialization benchmarks

LANGUAGE: Java
CODE:
public class MyPojo {
  public int id;
  private String name;
  private String[] operationNames;
  private MyOperation[] operations;
  private int otherId1;
  private int otherId2;
  private int otherId3;
  private Object someObject;
}
public class MyOperation {
  int id;
  protected String name;
}

----------------------------------------

TITLE: Using Method References in Java Lambdas
DESCRIPTION: Demonstrates how to use method references instead of inline lambdas for cleaner code in Java.

LANGUAGE: Java
CODE:
map.computeIfAbsent(key, Loader::load);

----------------------------------------

TITLE: Configuring Continuous File System Source in Java
DESCRIPTION: This snippet demonstrates how to set up a continuous file system source in Flink 1.1.0, which monitors a directory and processes files continuously.

LANGUAGE: Java
CODE:
DataStream<String> stream = env.readFile(
  textInputFormat,
  "hdfs:///file-path",
  FileProcessingMode.PROCESS_CONTINUOUSLY,
  5000, // monitoring interval (millis)
  FilePathFilter.createDefaultFilter()); // file path filter

----------------------------------------

TITLE: Using AssertJ for Readable Test Assertions in Java
DESCRIPTION: An example of how to use AssertJ to write more readable and expressive test assertions in Java unit tests.

LANGUAGE: Java
CODE:
assertThat(list)
    .hasSize(10)
    .allMatch(item -> item.length() < 10);

----------------------------------------

TITLE: Setting up Docker Environment for Flink SQL Demo
DESCRIPTION: Commands to create directory, download docker-compose configuration and start the demo environment containers

LANGUAGE: bash
CODE:
mkdir flink-sql-demo; cd flink-sql-demo;
wget https://raw.githubusercontent.com/wuchong/flink-sql-demo/v1.11-EN/docker-compose.yml
docker-compose up -d

----------------------------------------

TITLE: Demonstrating Proper Precondition and Logging Usage in Java
DESCRIPTION: Examples of correct and incorrect ways to use Preconditions and logging statements in Java, emphasizing the use of parameterized messages instead of string concatenation.

LANGUAGE: Java
CODE:
// Don't:
Preconditions.checkState(value <= threshold, "value must be below " + threshold);
LOG.debug("value is " + value);

// Do:
Preconditions.checkState(value <= threshold, "value must be below %s", threshold);
LOG.debug("value is {}", value);

----------------------------------------

TITLE: Demonstrating Proper Precondition and Logging Usage in Java
DESCRIPTION: Examples of correct and incorrect ways to use Preconditions and logging statements in Java, emphasizing the use of parameterized messages instead of string concatenation.

LANGUAGE: Java
CODE:
// Don't:
Preconditions.checkState(value <= threshold, "value must be below " + threshold);
LOG.debug("value is " + value);

// Do:
Preconditions.checkState(value <= threshold, "value must be below %s", threshold);
LOG.debug("value is {}", value);

----------------------------------------

TITLE: Initializing KUDO on Kubernetes Cluster
DESCRIPTION: Command to initialize KUDO on the Kubernetes cluster, setting up necessary resources and the KUDO controller.

LANGUAGE: bash
CODE:
kubectl kudo init

----------------------------------------

TITLE: Configuring State TTL in Remote Functions
DESCRIPTION: YAML configuration example for setting up state time-to-live in remote functions using module.yaml configuration file.

LANGUAGE: yaml
CODE:
functions:
  - function:
     states:
       - name: xxxx
         expireAfter: 5min # optional key

----------------------------------------

TITLE: Processing DataStream with WordCount Example
DESCRIPTION: Shows how to process data from Pulsar using Flink's DataStream API, implementing a simple word count operation with time windows.

LANGUAGE: java
CODE:
DataStream<WordWithCount> wc = words
  .flatmap((FlatMapFunction<String, WordWithCount>) (word, collector) -> {
    collector.collect(new WordWithCount(word, 1));
  })
 
  .returns(WordWithCount.class)
  .keyBy("word")
  .timeWindow(Time.seconds(5))
  .reduce((ReduceFunction<WordWithCount>) (c1, c2) ->
    new WordWithCount(c1.word, c1.count + c2.count));

----------------------------------------

TITLE: Configuring Asynchronous Non-blocking Remote Function Invocation in YAML
DESCRIPTION: Shows how to enable the new asynchronous, non-blocking transport for remote function invocations using YAML configuration in Stateful Functions 3.1.0.

LANGUAGE: yaml
CODE:
kind: io.statefun.endpoints.v2/http
spec:
 functions: fns/*
 urlPathTemplate: https://api-gateway.foo.bar/{function.name}
 maxNumBatchRequests: 10000
 transport:
   type: io.statefun.transports.v1/async
   call: 2m
   connect: 20s

----------------------------------------

TITLE: Defining Transform UDFs in YAML
DESCRIPTION: Example demonstrating how to declare and use custom User Defined Functions (UDFs) in transform operations within a CDC pipeline.

LANGUAGE: yaml
CODE:
transform:
  - source-table: db.tbl
    projection: "fmt('id -> %d', id) as fmt_id"
    filter: "inc(id) < 100"

pipeline:
  user-defined-function:
    - name: inc
      classpath: org.apache.flink.cdc.udf.examples.java.AddOneFunctionClass
    - name: fmt
      classpath: org.apache.flink.cdc.udf.examples.java.FormatFunctionClass

----------------------------------------

TITLE: Running Dedicated Compaction Job in Flink
DESCRIPTION: Command to submit a dedicated compaction job for table maintenance using Flink's command-line interface. Requires specifying warehouse path, database name and table name.

LANGUAGE: bash
CODE:
<FLINK_HOME>/bin/flink run \
    -c org.apache.flink.table.store.connector.action.FlinkActions \
    /path/to/flink-table-store-dist-<version>.jar \
    compact \
    --warehouse <warehouse-path> \
    --database <database-name> \
    --table <table-name>

----------------------------------------

TITLE: Configuring State TTL in Java SDK
DESCRIPTION: Example showing how to configure state time-to-live (TTL) for persisted values in the Java SDK. This allows state to expire after a specified duration following write operations.

LANGUAGE: java
CODE:
@Persisted
PersistedValue<Integer> table = PersistedValue.of(
    "my-value",
    Integer.class,
    Expiration.expireAfterWriting(Duration.ofHours(1)));

----------------------------------------

TITLE: Widest Rule Window Update Logic
DESCRIPTION: Logic for tracking and updating the widest time window rule in broadcast state for optimizing state cleanup.

LANGUAGE: java
CODE:
@Override
public void processBroadcastElement(Rule rule, Context ctx, Collector<Alert> out){
  ...
  updateWidestWindowRule(rule, broadcastState);
}

private void updateWidestWindowRule(Rule rule, BroadcastState<Integer, Rule> broadcastState){
  Rule widestWindowRule = broadcastState.get(WIDEST_RULE_KEY);

  if (widestWindowRule == null) {
    broadcastState.put(WIDEST_RULE_KEY, rule);
    return;
  }

  if (widestWindowRule.getWindowMillis() < rule.getWindowMillis()) {
    broadcastState.put(WIDEST_RULE_KEY, rule);
  }
}

----------------------------------------

TITLE: Setting Flink Properties and Creating Docker Network
DESCRIPTION: Sets up environment variables for Flink properties and creates a Docker network for container communication.

LANGUAGE: bash
CODE:
FLINK_PROPERTIES="jobmanager.rpc.address: jobmanager"
docker network create flink-network

----------------------------------------

TITLE: HTML Image Elements for Logo Display
DESCRIPTION: HTML markup for displaying different variants of the Flink logo with proper alt text, titles, and styling.

LANGUAGE: html
CODE:
<img src='/img/logo/png/200/flink_squirrel_200_color.png' alt='Apache Flink Logo' title='Apache Flink Logo' width='200px' />

----------------------------------------

TITLE: Cancelling Delayed Messages in Python
DESCRIPTION: Shows how to cancel a previously sent delayed message using the cancellation token in Stateful Functions 3.1.0 with Python.

LANGUAGE: python
CODE:
context.cancel_delayed_message("flow-1234")

----------------------------------------

TITLE: Command Line Example in Bash
DESCRIPTION: Example of properly formatted command line instructions using long parameter names and proper indentation.

LANGUAGE: bash
CODE:
$ ./bin/flink run-application \
--target kubernetes-application \
-Dkubernetes.cluster-id=my-first-application-cluster \
-Dkubernetes.container.image=custom-image-name \
local:///opt/flink/usrlib/my-flink-job.jar

----------------------------------------

TITLE: Migrating from FlinkDynamoDBStreamsConsumer to DynamoDbStreamsSource in Java
DESCRIPTION: Code example showing migration from legacy FlinkDynamoDBStreamsConsumer to new DynamoDbStreamsSource. Shows configuration of stream ARN, watermark strategy, and custom deserialization.

LANGUAGE: java
CODE:
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

// Old FlinkDynamoDBStreamsConsumer to read from stream test stream from TRIM_HORIZON
Properties consumerConfig = new Properties();
consumerConfig.put(AWSConfigConstants.AWS_REGION, "us-east-1");
consumerConfig.put(ConsumerConfigConstants.STREAM_INITIAL_POSITION, "TRIM_HORIZON");
FlinkDynamoDBStreamsConsumer<String> oldDynamodbStreamsConsumer = 
    new FlinkDynamoDBStreamsConsumer<>("arn:aws:dynamodb:us-east-1:1231231230:table/test/stream/2024-04-11T07:14:19.380", new SimpleStringSchema(), consumerConfig);
DataStream<String> dynamodbRecordsFromOldDynamodbStreamsConsumer = env.addSource(oldDynamodbStreamsConsumer)
    .uid("custom-uid")
    .assignTimestampsAndWatermarks(
        WatermarkStrategy.<String>forMonotonousTimestamps().withIdleness(Duration.ofSeconds(1)))

// New DynamoDbStreamsSource to read from stream test stream from TRIM_HORIZON
Configuration sourceConfig = new Configuration();
sourceConfig.set(DynamodbStreamsSourceConfigConstants.STREAM_INITIAL_POSITION, DynamodbStreamsSourceConfigConstants.InitialPosition.TRIM_HORIZON); 
KinesisStreamsSource<String> newDynamoDbStreamsSource =
    DynamoDbStreamsSource.<String>builder()
        .setStreamArn("arn:aws:dynamodb:us-east-1:1231231230:table/test/stream/2024-04-11T07:14:19.380")
        .setSourceConfig(sourceConfig)
        // User must implement their own deserialization schema to translate change data capture events into custom data types    
        .setDeserializationSchema(dynamodbDeserializationSchema) 
        .build();
DataStream<String> dynamodbRecordsWithEventTimeWatermarks = env.fromSource(
    newDynamoDbStreamsSource, 
    WatermarkStrategy.<String>forMonotonousTimestamps().withIdleness(Duration.ofSeconds(1)), 
    "DynamoDB Streams source")
        .returns(TypeInformation.of(String.class))
        .uid("custom-uid");

----------------------------------------

TITLE: Dynamic Key Function Implementation
DESCRIPTION: Process function that prepares transaction events for dynamic partitioning by extracting grouping keys based on rule definitions.

LANGUAGE: java
CODE:
public class DynamicKeyFunction
    extends ProcessFunction<Transaction, Keyed<Transaction, String, Integer>> {
   ...
  List<Rule> rules = /* Rules that are initialized somehow.
                        Details will be discussed in a future blog post. */;

  @Override
  public void processElement(
      Transaction event,
      Context ctx,
      Collector<Keyed<Transaction, String, Integer>> out) {

      for (Rule rule :rules) {
       out.collect(
           new Keyed<>(
               event,
               KeysExtractor.getKey(rule.getGroupingKeyNames(), event),
               rule.getRuleId()));
      }
  }
  ...
}

----------------------------------------

TITLE: Implementing Classifier Function in Python
DESCRIPTION: Python implementation example of a simple classifier function using the StateFun SDK, showing function decoration and message handling.

LANGUAGE: python
CODE:
@functions.bind("com.example/classify")
def classify(context, message):
    if message.is_type(Message):
        result = model.predict(message.data)
        context.send(result_topic, result)

----------------------------------------

TITLE: Stopping Flink and Prometheus Docker Environment
DESCRIPTION: Gradle command to tear down the Docker Compose setup, stopping the Flink job cluster and Prometheus instance.

LANGUAGE: bash
CODE:
./gradlew composeDown

----------------------------------------

TITLE: Front Matter License Template in Markdown
DESCRIPTION: Template showing the required Apache License block that must follow the front matter in documentation files.

LANGUAGE: markdown
CODE:
---
title: Concepts
layout: redirect
---
<!--
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at
  http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->

----------------------------------------

TITLE: Implementing a User-Defined Scalar Function in Scala
DESCRIPTION: This code snippet shows how to implement a user-defined scalar function called 'parseProperties' that converts a string of key-value pairs into a Properties object. This function can be used within Table API queries to parse and transform data.

LANGUAGE: scala
CODE:
object parseProperties extends ScalarFunction {
  def eval(str: String): Properties = {
    val props = new Properties()
    str
      .split(",")
      .map(_.split("="))
      .foreach(split => props.setProperty(split(0), split(1)))
    props
  }
}

----------------------------------------

TITLE: Starting Flink and Prometheus Docker Environment
DESCRIPTION: Command to build a Flink job using Gradle and start a local environment with Docker Compose, running a Flink job cluster and Prometheus instance.

LANGUAGE: bash
CODE:
./gradlew composeUp

----------------------------------------

TITLE: Flink Documentation Heading Structure
DESCRIPTION: Shows the recommended heading structure for Flink documentation pages, using markdown syntax. Demonstrates levels 1 through 5.

LANGUAGE: markdown
CODE:
# Level-1 Heading  <- Used for the title of the page 
## Level-2 Heading <- Start with this one for content
### Level-3 heading
#### Level-4 heading
##### Level-5 heading

----------------------------------------

TITLE: SQL DDL Configuration - Version 2.7.0
DESCRIPTION: Example of SQL DDL configuration for creating a Pulsar table in version 2.7.0, demonstrating the new simplified parameter naming convention without the 'connector.' prefix.

LANGUAGE: SQL
CODE:
create table topic1(
    `rip` VARCHAR,
    `rtime` VARCHAR,
    `uid` bigint,
    `client_ip` VARCHAR,
    `day` as TO_DATE(rtime),
    `hour` as date_format(rtime,'HH')
) with (
    'connector' ='pulsar',
    'topic' ='persistent://public/default/test_flink_sql',
    'service-url' ='pulsar://xxx',
    'admin-url' ='http://xxx',
    'scan.startup.mode' ='earliest',
    'properties.pulsar.reader.readername' = 'testReaderName',
    'format' ='json'
);

----------------------------------------

TITLE: Defining a Scala Function with TypeInformation
DESCRIPTION: This snippet demonstrates how to define a Scala function that takes a generic parameter with a TypeInformation context bound. This is useful when working with Flink operations that require type information for generic parameters.

LANGUAGE: scala
CODE:
def myFunction[T: TypeInformation](input: DataSet[T]): DataSet[Seq[T]] = {
  input.reduceGroup( i => i.toSeq )
}

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.1.4
DESCRIPTION: XML snippet showing how to update Maven dependencies to Apache Flink version 1.1.4 for Java, streaming, and client modules.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.1.4</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.10</artifactId>
  <version>1.1.4</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.10</artifactId>
  <version>1.1.4</version>
</dependency>

----------------------------------------

TITLE: Using Remote Flink Distribution in YARN
DESCRIPTION: Command demonstrating how to use a pre-uploaded Flink distribution from HDFS to save bandwidth during deployment.

LANGUAGE: bash
CODE:
./bin/flink run-application -t yarn-application \
    -Djobmanager.memory.process.size=2048m \
    -Dtaskmanager.memory.process.size=4096m \
    -Dyarn.provided.lib.dirs="hdfs://myhdfs/remote-flink-dist-dir" \
    ./MyApplication.jar

----------------------------------------

TITLE: Rebuilding Flink Website Non-Incrementally with Docker
DESCRIPTION: Rebuilds the Flink website documentation non-incrementally using a Docker image containing Hugo. This generates static HTML files in the 'content' folder for serving the project website.

LANGUAGE: bash
CODE:
./docker-build.sh build

----------------------------------------

TITLE: Java JSON Processing UDF Implementation
DESCRIPTION: Equivalent Java UDF implementation for JSON processing, used for performance comparison with Python implementation.

LANGUAGE: java
CODE:
public class JsonValueLower extends ScalarFunction {
    private transient ObjectMapper mapper;
    private transient ObjectWriter writer;

    @Override
    public void open(FunctionContext context) throws Exception {
        this.mapper = new ObjectMapper();
        this.writer = mapper.writerWithDefaultPrettyPrinter();
    }

    public String eval(String s) {
        try {
            StringObject object = mapper.readValue(s, StringObject.class);
            object.setA(object.a.toLowerCase());
            return writer.writeValueAsString(object);
        } catch (JsonProcessingException e) {
            throw new RuntimeException("Failed to read json value", e);
        }
    }

    private static class StringObject {
        private String a;

        public String getA() {
            return a;
        }

        public void setA(String a) {
            this.a = a;
        }

        @Override
        public String toString() {
            return "StringObject{" + "a='" + a + '\'' + "}";
        }
    }
}

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.9.2
DESCRIPTION: XML snippet showing how to update Maven dependencies to use Apache Flink version 1.9.2. Includes dependencies for flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.9.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.9.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.9.2</version>
</dependency>

----------------------------------------

TITLE: Filtering Bad Records with CoGroup in Java
DESCRIPTION: Example showing how to filter out invalid records from a dataset of user-song-play triplets using a coGroup operation.

LANGUAGE: java
CODE:
DataSet<Tuple3<String, String, Integer>> validTriplets = triplets
        .coGroup(mismatches).where(1).equalTo(0)
        .with(new CoGroupFunction() {
                void coGroup(Iterable triplets, Iterable invalidSongs, Collector out) {
                        if (!invalidSongs.iterator().hasNext()) {
                            for (Tuple3 triplet : triplets) { // valid triplet
                                out.collect(triplet);
                            }
                        }
                    }
                }

----------------------------------------

TITLE: Starting Flink Job in Reactive Mode
DESCRIPTION: This bash script demonstrates how to start a Flink job in Reactive Mode locally. It creates a directory for the job JAR, copies the example JAR, starts the job with specific parameters, and launches the first TaskManager.

LANGUAGE: bash
CODE:
# These instructions assume you are in the root directory of a Flink distribution.
# Put Job into usrlib/ directory
mkdir usrlib
cp ./examples/streaming/TopSpeedWindowing.jar usrlib/
# Submit Job in Reactive Mode
./bin/standalone-job.sh start -Dscheduler-mode=reactive -Dexecution.checkpointing.interval="10s" -j org.apache.flink.streaming.examples.windowing.TopSpeedWindowing
# Start first TaskManager
./bin/taskmanager.sh start

----------------------------------------

TITLE: Configuring Markdown Frontmatter for Flink Documentation
DESCRIPTION: Sets up the YAML frontmatter for the documentation page, specifying title, section collapsing, weight, and menu weight.

LANGUAGE: yaml
CODE:
---
title: Documentation
bookCollapseSection: true
weight: 15
menu_weight: 1
---

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.6.3
DESCRIPTION: XML snippet showing how to update Maven dependencies for Apache Flink 1.6.3. It includes dependencies for flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.6.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.6.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.6.3</version>
</dependency>

----------------------------------------

TITLE: Removing Scala Dependencies from Flink Distribution
DESCRIPTION: Command to remove Scala dependencies from Flink's lib directory, enabling a Scala-free classpath for user code.

LANGUAGE: bash
CODE:
$ rm flink-dist/lib/flink-scala*

----------------------------------------

TITLE: Configuring About Page YAML Frontmatter
DESCRIPTION: YAML configuration block that defines the properties of the About page section, including title, collapsible section behavior, and menu weights for navigation ordering.

LANGUAGE: yaml
CODE:
---
title: About
bookCollapseSection: true
weight: 5
menu_weight: 1
---

----------------------------------------

TITLE: Creating Flink SQL Table Definition
DESCRIPTION: SQL statement to create a table using the custom IMAP connector with configuration parameters.

LANGUAGE: SQL
CODE:
CREATE TABLE T (
    subject STRING,
    sent TIMESTAMP(3),
    received TIMESTAMP(3)
) WITH (
    'connector' = 'imap',
    'host' = 'greenmail',
    'port' = '3143',
    'user' = 'alice',
    'password' = 'alice'
);

----------------------------------------

TITLE: Calculating Parallelism in Adaptive Batch Scheduler
DESCRIPTION: Formula used by the VertexParallelismDecider to compute proper parallelisms of job vertices based on consumed dataset sizes.

LANGUAGE: text
CODE:
P = normalize(min(max(totalBytes_non-broadcast + min(totalBytes_broadcast, maxBroadcastRatio * V) / V, 1), maxParallelism))

----------------------------------------

TITLE: Submitting Flink Job to Kubernetes Cluster
DESCRIPTION: Command line example showing how to submit a Flink job to an existing Kubernetes session using the unified CLI configuration.

LANGUAGE: bash
CODE:
./bin/flink run -d -e kubernetes-session -Dkubernetes.cluster-id=<ClusterId> examples/streaming/WindowJoin.jar

----------------------------------------

TITLE: SQL DDL Configuration - Previous Version
DESCRIPTION: Example of SQL DDL configuration for creating a Pulsar table in versions prior to 2.7.0, showing the older parameter naming convention with 'connector.' prefix.

LANGUAGE: SQL
CODE:
create table topic1(
    `rip` VARCHAR,
    `rtime` VARCHAR,
    `uid` bigint,
    `client_ip` VARCHAR,
    `day` as TO_DATE(rtime),
    `hour` as date_format(rtime,'HH')
) with (
    'connector.type' ='pulsar',
    'connector.version' = '1',
    'connector.topic' ='persistent://public/default/test_flink_sql',
    'connector.service-url' ='pulsar://xxx',
    'connector.admin-url' ='http://xxx',
    'connector.startup-mode' ='earliest',
    'connector.properties.0.key' ='pulsar.reader.readerName',
    'connector.properties.0.value' ='testReaderName',
    'format.type' ='json',
    'update-mode' ='append'
);

----------------------------------------

TITLE: Configuring YAML Frontmatter for Flink ML Documentation
DESCRIPTION: YAML frontmatter configuration for Flink ML documentation page, specifying weight, title, and bookHref properties. The title includes a variable for the Flink ML stable short version.

LANGUAGE: yaml
CODE:
---
weight: 8
title: ML $FlinkMLStableShortVersion (stable)
bookHref: "https://nightlies.apache.org/flink/flink-ml-docs-stable/"
---

----------------------------------------

TITLE: Configuring Maven Dependencies for Apache Flink 1.7.2
DESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java, and flink-clients. These dependencies are required for building Flink applications with version 1.7.2.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.7.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.7.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.7.2</version>
</dependency>

----------------------------------------

TITLE: Configuring Markdown Frontmatter
DESCRIPTION: YAML frontmatter configuration for documentation page weight and external book reference URL.

LANGUAGE: yaml
CODE:
---
weight: 7
title: CDC Master (snapshot)
bookHref: "https://nightlies.apache.org/flink/flink-cdc-docs-master"
---

----------------------------------------

TITLE: Creating Table Store Catalog in Flink SQL
DESCRIPTION: SQL commands for creating and using a Table Store catalog with optional Hive metastore integration. The catalog can be configured with a warehouse location and metastore connection details.

LANGUAGE: sql
CODE:
CREATE CATALOG tablestore WITH (
  'type'='table-store',
  'warehouse'='hdfs://nn:8020/warehouse/path',
  -- optional hive metastore
  'metastore'='hive',
  'uri'='thrift://<hive-metastore-host-name>:<port>'
);

USE CATALOG tablestore;

CREATE TABLE my_table ...

----------------------------------------

TITLE: Specifying StateFun Components in YAML
DESCRIPTION: Illustrates the new way to specify StateFun components as standalone YAML documents in a module.yaml file, including HTTP endpoints, Kafka ingress, and Kafka egress configurations.

LANGUAGE: yaml
CODE:
kind: io.statefun.endpoints.v2/http
spec:
  functions: com.example/*
  urlPathTemplate: https://bar.foo.com/{function.name}
---
kind: io.statefun.kafka.v1/ingress
spec:
  id: com.example/my-ingress
  address: kafka-broker:9092
  consumerGroupId: my-consumer-group
  topics:
    - topic: message-topic
      valueType: io.statefun.types/string
      targets:
        - com.example/greeter
---
kind: io.statefun.kafka.v1/egress
spec:
  id: com.example/my-egress
  address: kafka-broker:9092
  deliverySemantic:
    type: exactly-once
    transactionTimeout: 15min

----------------------------------------

TITLE: Defining Remote Function Module in YAML
DESCRIPTION: YAML configuration example showing how to declare a remote endpoint and function type in Stateful Functions 2.0.

LANGUAGE: yaml
CODE:
kind: io.statefun.endpoints.v2/http
spec:
  functions: com.example/*
  urlPathTemplate: http://host:port/statefun
---
kind: io.statefun.functions.v2/function
spec:
  function: com.example/classify
  type: http

----------------------------------------

TITLE: Configuring Maven Dependencies for Apache Flink 1.16.1
DESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java, and flink-clients, all at version 1.16.1.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.16.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java</artifactId>
  <version>1.16.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients</artifactId>
  <version>1.16.1</version>
</dependency>

----------------------------------------

TITLE: Installing Flink Kubernetes Operator 1.9.0 using Helm
DESCRIPTION: Commands to add the Helm chart repository for Flink Kubernetes Operator 1.9.0 and install it with webhook creation disabled.

LANGUAGE: bash
CODE:
$ helm repo add flink-kubernetes-operator-1.9.0 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.9.0/
$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.9.0/flink-kubernetes-operator --set webhook.create=false

----------------------------------------

TITLE: Triggering Checkpoints via Command Line in Flink
DESCRIPTION: Shows how to manually trigger a checkpoint for a running Flink job using the command line interface, with an option for full checkpoints.

LANGUAGE: shell
CODE:
./bin/flink checkpoint $JOB_ID [-full]

----------------------------------------

TITLE: Installing Flink Kubernetes Operator via Helm
DESCRIPTION: Shell commands to add and install the Flink Kubernetes Operator 1.8.0 Helm chart to local registry.

LANGUAGE: shell
CODE:
$ helm repo add flink-kubernetes-operator-1.8.0 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.8.0/
$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.8.0/flink-kubernetes-operator --set webhook.create=false

----------------------------------------

TITLE: Implementing Stateful Function in Python
DESCRIPTION: Example of implementing a greeter function using the updated Python SDK in StateFun 3.0.0. Shows state management, message handling, and response sending.

LANGUAGE: python
CODE:
@functions.bind(typename="example/greeter", specs=[ValueSpec(name="visits", type=IntType)])
async def greeter(context: Context, message: Message):
    # update the visit count.
    visits = context.storage.visits or 0
    visits += 1
    context.storage.visits = visits

    # compute a greeting
    name = message.as_string()
    greeting = f"Hello there {name} at the {visits}th time!"

    caller = context.caller

    context.send(message_builder(target_typename=caller.typename,
                                 target_id=caller.id,
                                 str_value=greeting))

----------------------------------------

TITLE: Configuring Maven Dependencies for Apache Flink 1.5.4
DESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java, and flink-clients. These dependencies should be added to the project's pom.xml file to use Flink 1.5.4.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.5.4</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.5.4</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.5.4</version>
</dependency>

----------------------------------------

TITLE: Configuring Flink Test Environment in Java
DESCRIPTION: Sets up the Flink test environment using MiniClusterTestEnvironment for integration testing.

LANGUAGE: java
CODE:
@TestEnv
MiniClusterTestEnvironment flinkTestEnvironment = new MiniClusterTestEnvironment();

----------------------------------------

TITLE: Configuring Max Parallelism Default Values in StreamExecutionEnvironment
DESCRIPTION: Default configuration values for the max parallelism parameter in StreamExecutionEnvironment. For parallelism <= 128, the value is set to 128. For parallelism > 128, it's set to the minimum of the next power of two of (parallelism + parallelism/2) and 2^15.

LANGUAGE: text
CODE:
128 : for all parallelism <= 128
MIN(nextPowerOfTwo(parallelism + (parallelism / 2)), 2^15): for all parallelism > 128

----------------------------------------

TITLE: Installing Flink Kubernetes Operator via Helm
DESCRIPTION: Commands to add the Helm repository for Flink Kubernetes Operator 1.4.0 and install it with webhook creation disabled.

LANGUAGE: bash
CODE:
$ helm repo add flink-kubernetes-operator-1.4.0 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.4.0/
$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.4.0/flink-kubernetes-operator --set webhook.create=false

----------------------------------------

TITLE: Defining Maven Dependencies for Apache Flink 1.16.3
DESCRIPTION: XML snippet showing how to include Apache Flink 1.16.3 dependencies in a Maven project. It includes core Flink modules such as flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.16.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java</artifactId>
  <version>1.16.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients</artifactId>
  <version>1.16.3</version>
</dependency>

----------------------------------------

TITLE: Defining FlinkSessionJob in YAML for Kubernetes
DESCRIPTION: This YAML snippet demonstrates how to define a FlinkSessionJob resource for managing Flink jobs on a running Session deployment. It specifies the API version, kind, metadata, and job details including the JAR URI and parallelism.

LANGUAGE: yaml
CODE:
apiVersion: flink.apache.org/v1beta1
kind: FlinkSessionJob
metadata:
  name: basic-session-job-example
spec:
  deploymentName: basic-session-cluster
  job:
    jarURI: https://repo1.maven.org/maven2/org/apache/flink/flink-examples-streaming_2.12/1.15.0/flink-examples-streaming_2.12-1.15.0-TopSpeedWindowing.jar
    parallelism: 4
    upgradeMode: stateless

----------------------------------------

TITLE: Building and Pushing Docker Image
DESCRIPTION: Commands to build the custom Flink application Docker image and push it to a remote repository.

LANGUAGE: bash
CODE:
$ docker build -t <user-image> .

$ docker push <user-image>

----------------------------------------

TITLE: Defining Prometheus Remote-Write Protocol in Protobuf
DESCRIPTION: Protobuf definition of the Remote-Write interface for Prometheus, showing the structure of WriteRequest, TimeSeries, Label, and Sample messages.

LANGUAGE: protobuf
CODE:
func Send(WriteRequest)

message WriteRequest {
  repeated TimeSeries timeseries = 1;
  // [...]
}

message TimeSeries {
  repeated Label labels   = 1;
  repeated Sample samples = 2;
}

message Label {
  string name  = 1;
  string value = 2;
}

message Sample {
  double value    = 1;
  int64 timestamp = 2;
}

----------------------------------------

TITLE: Reading Multiple Stock Price Streams in Flink
DESCRIPTION: Demonstrates reading stock price data from multiple sources including a socket stream and generated data streams. Creates a unified stream by merging individual stock price streams.

LANGUAGE: scala
CODE:
def main(args: Array[String]) {
  val env = StreamExecutionEnvironment.getExecutionEnvironment

  //Read from a socket stream at map it to StockPrice objects
  val socketStockStream = env.socketTextStream("localhost", 9999).map(x => {
    val split = x.split(",")
    StockPrice(split(0), split(1).toDouble)
  })

  //Generate other stock streams
  val SPX_Stream = env.addSource(generateStock("SPX")(10) _)
  val FTSE_Stream = env.addSource(generateStock("FTSE")(20) _)
  val DJI_Stream = env.addSource(generateStock("DJI")(30) _)
  val BUX_Stream = env.addSource(generateStock("BUX")(40) _)

  //Merge all stock streams together
  val stockStream = socketStockStream.merge(SPX_Stream, FTSE_Stream, 
    DJI_Stream, BUX_Stream)

  stockStream.print()

  env.execute("Stock stream")
}

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.11.2
DESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java and flink-clients, updated to version 1.11.2.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.11.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.11.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.11.2</version>
</dependency>

----------------------------------------

TITLE: Installing Flink Kubernetes Operator using Helm
DESCRIPTION: This Bash snippet demonstrates how to add the Flink Kubernetes Operator Helm repository and install the operator using Helm. It uses the official release archive as the Helm repository.

LANGUAGE: bash
CODE:
$ helm repo add flink-kubernetes-operator-0.1.0 https://archive.apache.org/dist/flink/flink-kubernetes-operator-0.1.0/
$ helm install flink-kubernetes-operator flink-kubernetes-operator-0.1.0/flink-kubernetes-operator --set webhook.create=false

----------------------------------------

TITLE: Enabling Checkpoints After Tasks Finish in Flink Configuration
DESCRIPTION: Configuration setting to enable checkpoints after tasks finish, allowing mixing of bounded and unbounded streams.

LANGUAGE: properties
CODE:
execution.checkpointing.checkpoints-after-tasks-finish.enabled: true

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.12.4
DESCRIPTION: XML snippet showing how to update Maven dependencies to use Apache Flink version 1.12.4. It includes dependencies for flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.12.4</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.12.4</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.12.4</version>
</dependency>

----------------------------------------

TITLE: Declaring Maven Dependencies for Apache Flink 1.18.1
DESCRIPTION: XML snippet showing how to declare Maven dependencies for Apache Flink 1.18.1. It includes dependencies for flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.18.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java</artifactId>
  <version>1.18.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients</artifactId>
  <version>1.18.1</version>
</dependency>

----------------------------------------

TITLE: SQL Metadata Column Declaration
DESCRIPTION: Example showing how to declare metadata columns in SQL CREATE TABLE statement for Kafka.

LANGUAGE: sql
CODE:
CREATE TABLE kafka_table (
  id BIGINT,
  name STRING,
  event_time TIMESTAMP(3) METADATA FROM 'timestamp', -- access Kafka 'timestamp' metadata
  headers MAP<STRING, BYTES> METADATA  -- access Kafka 'headers' metadata
) WITH (
  'connector' = 'kafka',
  'topic' = 'test-topic', 
  'format' = 'avro'
);

----------------------------------------

TITLE: Configuring Flink Deployment Mode in Kubernetes
DESCRIPTION: YAML configuration snippet showing how to set the deployment mode (native/standalone) for a FlinkDeployment resource in Kubernetes.

LANGUAGE: yaml
CODE:
apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
...
spec:
  ...
  mode: native/standalone

----------------------------------------

TITLE: Extending AsyncSinkBase for Custom Sink in Java
DESCRIPTION: This code snippet demonstrates how to extend the AsyncSinkBase class to create a custom sink implementation.

LANGUAGE: Java
CODE:
class MySink<InputT> extends AsyncSinkBase<InputT, RequestEntryT> {
    // ...
    @Override
    public StatefulSinkWriter<InputT, BufferedRequestState<RequestEntryT>> createWriter(InitContext context) {
        return new MySinkWriter(context);
    }
    // ...
}

----------------------------------------

TITLE: Extending AsyncSinkBase for Custom Sink in Java
DESCRIPTION: This code snippet demonstrates how to extend the AsyncSinkBase class to create a custom sink implementation.

LANGUAGE: Java
CODE:
class MySink<InputT> extends AsyncSinkBase<InputT, RequestEntryT> {
    // ...
    @Override
    public StatefulSinkWriter<InputT, BufferedRequestState<RequestEntryT>> createWriter(InitContext context) {
        return new MySinkWriter(context);
    }
    // ...
}

----------------------------------------

TITLE: Declaring Maven Dependencies for Apache Flink 1.14.4
DESCRIPTION: XML snippet showing how to declare Maven dependencies for Apache Flink 1.14.4 modules including flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.14.4</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.14.4</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.14.4</version>
</dependency>

----------------------------------------

TITLE: Configuring Maven Dependencies for Apache Flink 1.20.1
DESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java, and flink-clients, all using version 1.20.1.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.20.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java</artifactId>
  <version>1.20.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients</artifactId>
  <version>1.20.1</version>
</dependency>

----------------------------------------

TITLE: Declaring Maven Dependencies for Apache Flink 1.15.3
DESCRIPTION: XML snippet showing how to declare Maven dependencies for Apache Flink 1.15.3 core components including flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.15.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java</artifactId>
  <version>1.15.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients</artifactId>
  <version>1.15.3</version>
</dependency>

----------------------------------------

TITLE: Configuring Flink Version-Specific Properties
DESCRIPTION: This properties file snippet shows how to set default configurations for specific Flink versions using the new flexible version-based config syntax. It sets a configuration key for Flink 1.18 and later versions.

LANGUAGE: properties
CODE:
# Set config for Flink 1.18 and larger
kubernetes.operator.default-configuration.flink-version.v1_18+.key: value

----------------------------------------

TITLE: Configuring Savepoint History Limits
DESCRIPTION: YAML configuration for setting savepoint history retention policies including maximum count and age limits.

LANGUAGE: yaml
CODE:
kubernetes.operator.savepoint.history.max.count: 5
kubernetes.operator.savepoint.history.max.age: 48h

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.12.3
DESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java, and flink-clients. These dependencies should be added to the project's pom.xml file to use Flink 1.12.3.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.12.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.12.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.12.3</version>
</dependency>

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.13.3
DESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java and flink-clients. These dependencies should be updated to version 1.13.3.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.13.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.13.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.13.3</version>
</dependency>

----------------------------------------

TITLE: Using Non-Capturing Lambdas in Java
DESCRIPTION: Demonstrates the preferred use of non-capturing lambdas in Java for better performance, as they can reuse the same instance for each invocation.

LANGUAGE: Java
CODE:
// Don't:
map.computeIfAbsent(key, x -> key.toLowerCase());

// Do:
map.computeIfAbsent(key, k -> k.toLowerCase());

// Even better (using method reference):
map.computeIfAbsent(key, Loader::load);

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.14.3
DESCRIPTION: XML snippet showing how to update Maven dependencies to use Apache Flink version 1.14.3.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.14.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.14.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.14.3</version>
</dependency>

----------------------------------------

TITLE: Configuring Maven Dependencies for Apache Flink 1.13.1
DESCRIPTION: Maven dependency configurations for core Flink modules including flink-java, flink-streaming-java, and flink-clients. These dependencies should be added to the project's pom.xml file to use Flink 1.13.1.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.13.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.13.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.13.1</version>
</dependency>

----------------------------------------

TITLE: Updated Autoscaler Configuration Settings in Kubernetes
DESCRIPTION: Default configuration changes for the autoscaler component showing updated timing and threshold values for better out-of-box performance.

LANGUAGE: yaml
CODE:
kubernetes.operator.job.autoscaler.metrics.window: 5m -> 10m
kubernetes.operator.job.autoscaler.target.utilization.boundary: 0.1 -> 0.4
kubernetes.operator.job.autoscaler.scale-up.grace-period: 10m -> 1h

kubernetes.operator.job.autoscaler.history.max.count: 1 -> 3
kubernetes.operator.job.autoscaler.scaling.effectiveness.detection.enabled: true -> false

kubernetes.operator.job.autoscaler.catch-up.duration: 10m -> 5m
kubernetes.operator.job.autoscaler.restart.time: 5m -> 3m

----------------------------------------

TITLE: Configuring Periodic Savepoints in Flink
DESCRIPTION: YAML configuration for enabling periodic savepoints with a 6-hour interval in Flink applications.

LANGUAGE: yaml
CODE:
flinkConfiguration:
  ...
  kubernetes.operator.periodic.savepoint.interval: 6h

----------------------------------------

TITLE: Using Non-Capturing Lambdas in Java
DESCRIPTION: Illustrates the preferred way to use non-capturing lambdas for better performance in Java.

LANGUAGE: Java
CODE:
map.computeIfAbsent(key, k -> k.toLowerCase());

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.11.1
DESCRIPTION: XML snippet showing how to update Maven dependencies to Apache Flink version 1.11.1 for flink-java, flink-streaming-java, and flink-clients artifacts.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.11.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.11.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.11.1</version>
</dependency>

----------------------------------------

TITLE: Updating Maven Dependencies for Flink 1.12.1
DESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java, and flink-clients with version 1.12.1.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.12.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.12.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.12.1</version>
</dependency>

----------------------------------------

TITLE: Improving Code Readability with Early Returns in Java
DESCRIPTION: An example of how to improve code readability by using early returns instead of deep nesting in conditional statements.

LANGUAGE: Java
CODE:
if (!a) {
	return ..
}

if (!b) {
	return ...
}

if (!c) {
	return ...
}

the main path

----------------------------------------

TITLE: Updating Maven Dependencies for Flink 1.10.1
DESCRIPTION: XML snippet showing how to update Maven dependencies to Flink 1.10.1 version for core Flink modules.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.10.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.10.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.10.1</version>
</dependency>

----------------------------------------

TITLE: Deploying Flink 2.0 Preview with Kubernetes Operator
DESCRIPTION: This YAML snippet demonstrates how to deploy a Flink 2.0 preview version using the Flink Kubernetes Operator. It specifies the FlinkDeployment custom resource with the appropriate image and Flink version.

LANGUAGE: yaml
CODE:
apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  name: basic-example
spec:
  image: flink:2.0
  flinkVersion: v2_0
...

----------------------------------------

TITLE: Enabling Cluster Resource Capacity Check in YAML
DESCRIPTION: YAML configuration to enable automatic resource capacity checking for Kubernetes cluster.

LANGUAGE: yaml
CODE:
kubernetes.operator.cluster.resource-view.refresh-interval: 5 min

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.9.1
DESCRIPTION: XML snippet showing how to update Maven dependencies to use Apache Flink version 1.9.1. It includes dependencies for flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.9.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.9.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.9.1</version>
</dependency>

----------------------------------------

TITLE: Java Function Arguments Line Breaking Example
DESCRIPTION: Demonstrates the proper way to break long function declarations across multiple lines while maintaining proper indentation and comma placement.

LANGUAGE: java
CODE:
public void func(
    int arg1,
    int arg2,
    ...) throws E1, E2, E3 {

}

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.8.2
DESCRIPTION: XML snippet showing how to update Maven dependencies to use Apache Flink version 1.8.2. It includes dependencies for flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.8.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.8.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.8.2</version>
</dependency>

----------------------------------------

TITLE: Defining Maven Dependencies for Apache Flink 1.1.2
DESCRIPTION: XML snippet showing how to include Apache Flink 1.1.2 dependencies in a Maven project. It includes core Flink modules: flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.1.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.10</artifactId>
  <version>1.1.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.10</artifactId>
  <version>1.1.2</version>
</dependency>

----------------------------------------

TITLE: Configuring Maven Dependencies for Apache Flink 1.7.1
DESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java, and flink-clients. These dependencies are required for building Flink applications with version 1.7.1.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.7.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.7.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.7.1</version>
</dependency>

----------------------------------------

TITLE: PyFlink DataStream API Example
DESCRIPTION: Simple example showing usage of the new Python DataStream API with a map function.

LANGUAGE: python
CODE:
from pyflink.common.typeinfo import Types
from pyflink.datastream import MapFunction, StreamExecutionEnvironment

class MyMapFunction(MapFunction):

    def map(self, value):
        return value + 1


env = StreamExecutionEnvironment.get_execution_environment()
data_stream = env.from_collection([1, 2, 3, 4, 5], type_info=Types.INT())
mapped_stream = data_stream.map(MyMapFunction(), output_type=Types.INT())
mapped_stream.print()
env.execute("datastream job")

----------------------------------------

TITLE: Java Method Chaining Line Breaking Example
DESCRIPTION: Shows the correct way to break long chains of method calls across multiple lines, with proper dot placement and indentation.

LANGUAGE: java
CODE:
values
    .stream()
    .map(...)
    .collect(...);

----------------------------------------

TITLE: Breaking Long Function Arguments in Java
DESCRIPTION: Demonstrates how to properly format long lists of function arguments in Java when they exceed the line length limit. Each argument is placed on a separate line with additional indentation.

LANGUAGE: Java
CODE:
public void func(
    int arg1,
    int arg2,
    ...) throws E1, E2, E3 {

}

----------------------------------------

TITLE: Java Method Chaining Line Breaking Example
DESCRIPTION: Shows the correct way to break long chains of method calls across multiple lines, with proper dot placement and indentation.

LANGUAGE: java
CODE:
values
    .stream()
    .map(...)
    .collect(...);

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.8.1
DESCRIPTION: XML snippet showing how to update Maven dependencies to Apache Flink version 1.8.1 for flink-java, flink-streaming-java, and flink-clients modules.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.8.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.8.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.8.1</version>
</dependency>

----------------------------------------

TITLE: Configuring Maven Dependencies for Apache Flink 1.6.2
DESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java, and flink-clients. These dependencies are required for building Flink applications using version 1.6.2.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.6.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.6.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.6.2</version>
</dependency>

----------------------------------------

TITLE: Installing Flink Kubernetes Operator via Helm
DESCRIPTION: Helm commands for adding the Flink Kubernetes Operator repository and installing the operator with webhook creation disabled.

LANGUAGE: bash
CODE:
$ helm repo add flink-kubernetes-operator-1.5.0 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.5.0/
$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.5.0/flink-kubernetes-operator --set webhook.create=false

----------------------------------------

TITLE: Configuring Maven Dependencies for Apache Flink 1.6.1
DESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java and flink-clients. These dependencies should be added to the project's pom.xml file.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.6.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.6.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.6.1</version>
</dependency>

----------------------------------------

TITLE: Simulating Flink Task Manager Failure
DESCRIPTION: Docker command to kill a Flink task manager container, simulating a partial failure scenario for testing Prometheus alerts.

LANGUAGE: bash
CODE:
docker kill taskmanager1

----------------------------------------

TITLE: Installing Flink Kubernetes Operator 1.6.1 via Helm
DESCRIPTION: Commands to add and install the Flink Kubernetes Operator 1.6.1 Helm chart to local registry. The installation is configured with webhook creation disabled.

LANGUAGE: bash
CODE:
$ helm repo add flink-kubernetes-operator-1.6.1 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.6.1/
$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.6.1/flink-kubernetes-operator --set webhook.create=false

----------------------------------------

TITLE: Apache License Notice in HTML Comments
DESCRIPTION: Standard Apache License 2.0 notice included as an HTML comment block, specifying terms of use and distribution for the documentation file.

LANGUAGE: html
CODE:
<!--
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->

----------------------------------------

TITLE: Configuring Maven Dependencies for Apache Flink 1.3.1
DESCRIPTION: XML snippet showing how to add Apache Flink 1.3.1 dependencies to a Maven project. It includes core Flink modules such as flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.3.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.10</artifactId>
  <version>1.3.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.10</artifactId>
  <version>1.3.1</version>
</dependency>

----------------------------------------

TITLE: Configuring Maven Dependencies for Apache Flink 1.3.2
DESCRIPTION: Maven dependency configuration snippets for core Flink modules including flink-java, flink-streaming-java and flink-clients, all updated to version 1.3.2.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.3.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.10</artifactId>
  <version>1.3.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.10</artifactId>
  <version>1.3.2</version>
</dependency>

----------------------------------------

TITLE: Configuring Retained Checkpoints
DESCRIPTION: Configuration parameter to specify the number of completed checkpoints to retain. By default, Flink keeps 1 checkpoint.

LANGUAGE: java
CODE:
state.checkpoints.num-retained

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.5.3
DESCRIPTION: This XML snippet shows how to update Maven dependencies for Apache Flink 1.5.3. It includes dependencies for flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.5.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.5.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.5.3</version>
</dependency>

----------------------------------------

TITLE: Adding Apache Flink 1.1.5 Maven Dependencies
DESCRIPTION: XML snippet showing how to add Apache Flink 1.1.5 dependencies to a Maven project. It includes core Flink modules: flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.1.5</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.10</artifactId>
  <version>1.1.5</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.10</artifactId>
  <version>1.1.5</version>
</dependency>

----------------------------------------

TITLE: Demonstrating Table API Operations in Scala
DESCRIPTION: Example showing how to perform grouping and joining operations using Flink's new Table API. The code demonstrates counting clicks per user and joining with a users table to find active users with more than 10 clicks.

LANGUAGE: scala
CODE:
val clickCounts = clicks
  .groupBy('user).select('userId, 'url.count as 'count)

val activeUsers = users.join(clickCounts)
  .where('id === 'userId && 'count > 10).select('username, 'count, ...)

----------------------------------------

TITLE: Implementing CongestionControlRateLimitingStrategy in Java
DESCRIPTION: TCP congestion control-inspired implementation that dynamically adjusts request rates based on destination response

LANGUAGE: java
CODE:
public class CongestionControlRateLimitingStrategy implements RateLimitingStrategy {
    // ...
    @Override
    public void registerInFlightRequest(RequestInfo requestInfo) {
        currentInFlightRequests++;
        currentInFlightMessages += requestInfo.getBatchSize();
    }
    
    @Override
    public void registerCompletedRequest(ResultInfo resultInfo) {
        currentInFlightRequests = Math.max(0, currentInFlightRequests - 1);
        currentInFlightMessages = Math.max(0, currentInFlightMessages - resultInfo.getBatchSize());
        
        if (resultInfo.getFailedMessages() > 0) {
            maxInFlightMessages = scalingStrategy.scaleDown(maxInFlightMessages);
        } else {
            maxInFlightMessages = scalingStrategy.scaleUp(maxInFlightMessages);
        }
    }

    public boolean shouldBlock(RequestInfo requestInfo) {
        return currentInFlightRequests >= maxInFlightRequests
                || (currentInFlightMessages + requestInfo.getBatchSize() > maxInFlightMessages);
    }
    // ...
}

----------------------------------------

TITLE: Counting Git Commits with Bash
DESCRIPTION: Command to count the total number of git commits in the Flink repository for the year 2015.

LANGUAGE: bash
CODE:
git log --pretty=oneline --after=1/1/2015  | wc -l

----------------------------------------

TITLE: Configuring Maven Dependencies for Apache Flink 1.9.3
DESCRIPTION: Maven dependency configurations for core Flink modules including flink-java, flink-streaming-java, and flink-clients. These dependencies are required for building Flink applications with version 1.9.3.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.9.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.9.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.9.3</version>
</dependency>

----------------------------------------

TITLE: Configuring Maven Dependencies for Apache Flink 1.5.5
DESCRIPTION: Maven dependency configuration for core Flink components including flink-java, flink-streaming-java, and flink-clients modules. These dependencies should be added to the project's pom.xml file.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.5.5</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.5.5</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.5.5</version>
</dependency>

----------------------------------------

TITLE: Including Apache License 2.0 Notice in Flink Documentation
DESCRIPTION: This snippet contains the Apache License 2.0 notice as a comment in the documentation file, specifying the terms under which the documentation is distributed.

LANGUAGE: html
CODE:
<!--
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->

----------------------------------------

TITLE: Reading Snapshot Configuration in Flink
DESCRIPTION: Implementation of snapshot reading and schema restoration in TypeSerializerSnapshot.

LANGUAGE: java
CODE:
  @Override
  public void readSnapshot(
      int readVersion,
      DataInputView in,
      ClassLoader userCodeClassLoader) throws IOException {

    assert readVersion == 1;
    final String previousSchemaDefinition = in.readUTF();
    this.previousSchema = parseAvroSchema(previousSchemaDefinition);
    this.runtimeType = findClassOrFallbackToGeneric(
      userCodeClassLoader,
      previousSchema.getFullName());

    this.runtimeSchema = tryExtractAvroSchema(userCodeClassLoader, runtimeType);
  }

----------------------------------------

TITLE: Markdown Link to Flink K8s Operator Guide
DESCRIPTION: External link shortcode for Flink Kubernetes Operator getting started documentation.

LANGUAGE: markdown
CODE:
{{< external_link name="Read how you can get started with Flink Kubernetes Operator here.">}}

----------------------------------------

TITLE: Canceling Flink Application
DESCRIPTION: Command to cancel a running Flink application on Kubernetes.

LANGUAGE: bash
CODE:
$ ./bin/flink cancel --target kubernetes-application -Dkubernetes.cluster-id=<ClusterID> <JobID>

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.4.2
DESCRIPTION: This XML snippet shows how to update Maven dependencies to use Apache Flink version 1.4.2. It includes dependencies for flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.4.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.4.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.4.2</version>
</dependency>

----------------------------------------

TITLE: Markdown Content for Documentation Landing Page
DESCRIPTION: Markdown content providing the page heading and external documentation link reference.

LANGUAGE: markdown
CODE:
# Flink CDC documentation (latest stable release)

{{< external_link name="You can find the Flink CDC documentation for the latest stable release here.">}}

----------------------------------------

TITLE: Defining Maven Dependencies for Apache Flink 1.19.1
DESCRIPTION: XML snippet showing how to include Apache Flink 1.19.1 dependencies in a Maven project. It includes core Flink modules such as flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.19.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java</artifactId>
  <version>1.19.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients</artifactId>
  <version>1.19.1</version>
</dependency>

----------------------------------------

TITLE: Creating Temperature Alert Pattern in Flink CEP
DESCRIPTION: Defines a pattern to detect two consecutive temperature warnings within 20 seconds.

LANGUAGE: java
CODE:
Pattern<TemperatureWarning, ?> alertPattern = Pattern.<TemperatureWarning>begin("First Event")
    .next("Second Event")
    .within(Time.seconds(20));

----------------------------------------

TITLE: TPC-H Query 3 Implementation Reference
DESCRIPTION: Reference to the TPC-H Query 3 example implementation used for testing fine-grained recovery behavior, with modifications for instrumentation purposes.

LANGUAGE: java
CODE:
https://github.com/rmetzger/flip1-bench/blob/master/flip1-bench-jobs/src/main/java/com/ververica/TPCHQuery3.java

----------------------------------------

TITLE: Adding Apache Flink Kubernetes Operator Maven Dependency
DESCRIPTION: XML snippet for adding the Apache Flink Kubernetes Operator dependency to a Maven project.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-kubernetes-operator</artifactId>
  <version>{{< param FlinkKubernetesOperatorStableVersion >}}</version>
</dependency>

----------------------------------------

TITLE: Generating Git Statistics
DESCRIPTION: A bash command to generate Git statistics for the Flink repository using Gitstats.

LANGUAGE: bash
CODE:
gitstats flink/ flink-stats/

----------------------------------------

TITLE: HTML External Link Component for Flink Training
DESCRIPTION: HTML template component that creates an external link to Flink's training course documentation using Hugo shortcode syntax.

LANGUAGE: html
CODE:
{{< external_link name="Read all about the Flink Training Course here.">}}

----------------------------------------

TITLE: Configuring Custom Transport for Remote Function Invocations in YAML
DESCRIPTION: Demonstrates how to configure a custom transport mechanism for invoking remote stateful functions using YAML configuration in Stateful Functions 3.1.0.

LANGUAGE: yaml
CODE:
kind: io.statefun.endpoints.v2/http
spec:
 functions: com.foo.bar/*
 urlPathTemplate: https://{function.name}/
 maxNumBatchRequests: 10000
 transport:
   type: com.foo.bar/pubsub
   some_property1: some_value1

----------------------------------------

TITLE: Installing PyFlink Using Pip
DESCRIPTION: Commands to check Python version and install PyFlink package using pip. Requires Python 3.5 or higher.

LANGUAGE: bash
CODE:
$ python --version
Python 3.7.6

$ python -m pip install apache-flink

----------------------------------------

TITLE: Demonstrating Logical Key Expression in Java
DESCRIPTION: Example showing the new logical key expression syntax for joining datasets in Flink 0.7.0

LANGUAGE: java
CODE:
persons.join(cities).where("zip").equalTo("zipcode")

----------------------------------------

TITLE: Viewing Actor Pod Logs for Fraud Detection
DESCRIPTION: Command to view the logs of the actor pod, which displays detected fraudulent transactions.

LANGUAGE: bash
CODE:
kubectl logs $(kubectl get pod -l actor=flink-demo -o jsonpath="{.items[0].metadata.name}")

----------------------------------------

TITLE: Installing Flink Kubernetes Operator 1.11.0 with Helm
DESCRIPTION: This bash script shows how to add the Flink Kubernetes Operator 1.11.0 Helm chart to a local registry and install it using Helm. It disables webhook creation during installation.

LANGUAGE: bash
CODE:
$ helm repo add flink-kubernetes-operator-1.11.0 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.11.0/
$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.11.0/flink-kubernetes-operator --set webhook.create=false

----------------------------------------

TITLE: SQL DDL for Watermark Definition
DESCRIPTION: SQL syntax example showing how to define watermark generation strategies in table creation DDL.

LANGUAGE: sql
CODE:
CREATE TABLE table_name (

  WATERMARK FOR columnName AS <watermark_strategy_expression>

) WITH (
  ...
)

----------------------------------------

TITLE: Configuring Backend Test Environment in Java
DESCRIPTION: Sets up the backend test environment for integration testing, providing access to the backend container and associated resources.

LANGUAGE: java
CODE:
@TestExternalSystem
MyBackendTestEnvironment backendTestEnvironment = new MyBackendTestEnvironment();

----------------------------------------

TITLE: Installing Flink ML Python Package
DESCRIPTION: Command to install the Apache Flink ML Python package using pip package manager.

LANGUAGE: bash
CODE:
pip install apache-flink-ml

----------------------------------------

TITLE: Accessing Table API Examples in Java and Scala
DESCRIPTION: Links to example code for using the new Table API in both Java and Scala. The Table API provides a SQL-like interface for working with data in Flink.

LANGUAGE: markdown
CODE:
[here (Java)](https://github.com/apache/flink/blob/master/flink-libraries/flink-table/src/main/java/org/apache/flink/examples/java/JavaTableExample.java) and [here (Scala)](https://github.com/apache/flink/tree/master/flink-libraries/flink-table/src/main/scala/org/apache/flink/examples/scala)

----------------------------------------

TITLE: Autoscaler Metric Format
DESCRIPTION: Format specification for autoscaler metrics reporting under the Kubernetes Operator Resource metric group.

LANGUAGE: plaintext
CODE:
[resource_prefix].Autoscaler.[jobVertexID].[ScalingMetric].Current/Average

----------------------------------------

TITLE: Implementing Pandas UDF for Temperature Interpolation
DESCRIPTION: Example of a Pandas UDF that interpolates missing temperature data using pandas.Series as input and output. Uses the DataFrame.interpolate() function for data processing.

LANGUAGE: python
CODE:
@udf(input_types=[DataTypes.STRING(), DataTypes.FLOAT()],
     result_type=DataTypes.FLOAT(), udf_type='pandas')
def interpolate(id, temperature):
    # takes id: pandas.Series and temperature: pandas.Series as input
    df = pd.DataFrame({'id': id, 'temperature': temperature})

    # use interpolate() to interpolate the missing temperature
    interpolated_df = df.groupby('id').apply(
        lambda group: group.interpolate(limit_direction='both'))

    # output temperature: pandas.Series
    return interpolated_df['temperature']

----------------------------------------

TITLE: Declaring Maven Dependencies for Apache Flink 1.17.2
DESCRIPTION: XML snippet showing how to declare Maven dependencies for Apache Flink 1.17.2 core components including flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.17.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java</artifactId>
  <version>1.17.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients</artifactId>
  <version>1.17.2</version>
</dependency>

----------------------------------------

TITLE: Running Flink Application in Application Mode on YARN
DESCRIPTION: Basic command to launch a Flink application in Application Mode on YARN cluster.

LANGUAGE: bash
CODE:
./bin/flink run-application -t yarn-application ./MyApplication.jar

----------------------------------------

TITLE: Adding Apache Flink ML Maven Dependencies
DESCRIPTION: XML snippet for adding Apache Flink ML dependencies to a Maven project. Includes flink-ml-core, flink-ml-iteration, and flink-ml-lib artifacts.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-ml-core</artifactId>
  <version>{{< param FlinkMLStableVersion >}}</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-ml-iteration</artifactId>
  <version>{{< param FlinkMLStableVersion >}}</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-ml-lib</artifactId>
  <version>{{< param FlinkMLStableVersion >}}</version>
</dependency>

----------------------------------------

TITLE: Implementing ResultTypeQueryable for Row Source
DESCRIPTION: Example showing how to implement ResultTypeQueryable interface for a Row source to provide type information

LANGUAGE: Java
CODE:
public static class RowSource implements SourceFunction<Row>, ResultTypeQueryable<Row> {
  // ...

  @Override
  public TypeInformation<Row> getProducedType() {
    return Types.ROW(Types.INT, Types.STRING, Types.OBJECT_ARRAY(Types.STRING));
  }
}

----------------------------------------

TITLE: Adding Apache Flink Stateful Functions Maven Dependencies
DESCRIPTION: XML snippet for adding Apache Flink Stateful Functions dependencies to a Maven project. Includes statefun-sdk and statefun-flink-harness artifacts.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>statefun-sdk</artifactId>
  <version>{{< param StateFunStableVersion >}}</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>statefun-flink-harness</artifactId>
  <version>{{< param StateFunStableVersion >}}</version>
</dependency>

----------------------------------------

TITLE: Installing PyFlink via pip
DESCRIPTION: Command to install the PyFlink package using pip package manager.

LANGUAGE: bash
CODE:
pip install apache-flink

----------------------------------------

TITLE: Configuring Maven Dependencies for Apache Flink 1.15.4
DESCRIPTION: Maven dependency configuration for core Flink components including flink-java, flink-streaming-java, and flink-clients modules. These dependencies are required for building Flink applications.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.15.4</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java</artifactId>
  <version>1.15.4</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients</artifactId>
  <version>1.15.4</version>
</dependency>

----------------------------------------

TITLE: Flink Documentation Front Matter Example
DESCRIPTION: Demonstrates the structure of front matter used in Flink documentation markdown files. Includes title, weight for ordering, and aliases for redirects.

LANGUAGE: markdown
CODE:
---
title: "Title of the Page" <-- Title rendered in the side nave
weight: 1 <-- Weight controls the ordering of pages in the side nav.
aliases:  <-- Alias to setup redirect from removed page to this one
  - /alias/to/removed/page.html
---

----------------------------------------

TITLE: Flink Documentation External Link in Markdown
DESCRIPTION: A markdown snippet that creates an external link to Flink's latest documentation snapshot using Hugo shortcode syntax.

LANGUAGE: markdown
CODE:
{{< external_link name="You can find the Flink documentation for the latest snapshot here.">}}

----------------------------------------

TITLE: Installing PyFlink using pip
DESCRIPTION: Command to install PyFlink package using pip package manager. Requires Python 3.5 or higher.

LANGUAGE: bash
CODE:
$ python -m pip install apache-flink

----------------------------------------

TITLE: Building Flink Website with Docker
DESCRIPTION: Builds the Flink website documentation using a Docker image containing Hugo. This command serves the built site locally at http://localhost:1313/.

LANGUAGE: bash
CODE:
$ ./docker-build.sh

----------------------------------------

TITLE: Declaring Maven Dependencies for Apache Flink 1.14.6
DESCRIPTION: XML snippet showing how to declare Maven dependencies for Apache Flink 1.14.6 core components including flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.14.6</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.14.6</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.14.6</version>
</dependency>

----------------------------------------

TITLE: YAML Configuration for Flink ML Documentation
DESCRIPTION: Front matter configuration specifying weight, title and documentation URL for the Flink ML documentation page.

LANGUAGE: yaml
CODE:
weight: 8
title: ML $FlinkMLStableShortVersion (stable)
bookHref: "https://nightlies.apache.org/flink/flink-ml-docs-stable/"

----------------------------------------

TITLE: External Link Reference in Markdown
DESCRIPTION: Markdown shortcode for rendering an external link to the Flink ML documentation.

LANGUAGE: markdown
CODE:
{{< external_link name="You can find the Flink ML documentation for the latest snapshot here.">}}

----------------------------------------

TITLE: State Backend Migration Example
DESCRIPTION: Example of migrating state backend configuration

LANGUAGE: java
CODE:
// Old state backend config
env.setStateBackend(new FsStateBackend("hdfs://namenode:40010/flink/checkpoints"));

// New state backend config
Configuration config = new Configuration();
config.set(StateBackendOptions.STATE_BACKEND_TYPE, "filesystem");
config.set(CheckpointingOptions.CHECKPOINTS_DIRECTORY, "hdfs://namenode:40010/flink/checkpoints");

----------------------------------------

TITLE: Configuring Maven Dependencies for Apache Flink 1.15.2
DESCRIPTION: Maven dependency configurations required to use Apache Flink 1.15.2 core components including flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.15.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java</artifactId>
  <version>1.15.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients</artifactId>
  <version>1.15.2</version>
</dependency>

----------------------------------------

TITLE: Configuring Front Matter for Flink CDC Documentation
DESCRIPTION: YAML front matter configuration defining weight, title and documentation link for the Flink CDC documentation page.

LANGUAGE: yaml
CODE:
weight: 6
title: CDC $FlinkCDCStableShortVersion (stable)
bookHref: "https://nightlies.apache.org/flink/flink-cdc-docs-stable"

----------------------------------------

TITLE: Implementing Custom Windowing Logic in Apache Flink Scala API
DESCRIPTION: This series of snippets demonstrates how to implement custom windowing logic using Flink's internal windowing mechanics. It shows the step-by-step process of creating a keyed stream, applying a window assigner, specifying triggers and evictors, and applying a window function.

LANGUAGE: scala
CODE:
val input: DataStream[IN] = ...

// created a keyed stream using a key selector function
val keyed: KeyedStream[IN, KEY] = input
  .keyBy(myKeySel: (IN) => KEY)

LANGUAGE: scala
CODE:
// create windowed stream using a WindowAssigner
var windowed: WindowedStream[IN, KEY, WINDOW] = keyed
  .window(myAssigner: WindowAssigner[IN, WINDOW])

LANGUAGE: scala
CODE:
// override the default trigger of the WindowAssigner
windowed = windowed
  .trigger(myTrigger: Trigger[IN, WINDOW])

LANGUAGE: scala
CODE:
// specify an optional evictor
windowed = windowed
  .evictor(myEvictor: Evictor[IN, WINDOW])

LANGUAGE: scala
CODE:
// apply window function to windowed stream
val output: DataStream[OUT] = windowed
  .apply(myWinFunc: WindowFunction[IN, OUT, KEY, WINDOW])

----------------------------------------

TITLE: Declaring Maven Dependencies for Apache Flink 1.15.1
DESCRIPTION: XML snippet showing how to declare Maven dependencies for Apache Flink 1.15.1 core components including flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.15.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java</artifactId>
  <version>1.15.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients</artifactId>
  <version>1.15.1</version>
</dependency>

----------------------------------------

TITLE: Configuring Markdown Metadata for Flink Documentation
DESCRIPTION: This snippet defines the metadata for the documentation page, including the title, section collapsing behavior, and menu weights.

LANGUAGE: markdown
CODE:
---
title: Documentation
bookCollapseSection: true
weight: 15
menu_weight: 1
---

----------------------------------------

TITLE: Adding Apache License Header in Java
DESCRIPTION: The standard Apache License header that must be included at the top of each source file in the Apache Flink project.

LANGUAGE: Java
CODE:
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

----------------------------------------

TITLE: Starting Minikube Cluster for Flink and Kafka Demo
DESCRIPTION: Command to start a Minikube cluster with sufficient resources for running the Flink and Kafka demo.

LANGUAGE: bash
CODE:
minikube start --cpus=6 --memory=9216 --disk-size=10g

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.12.2
DESCRIPTION: XML snippet showing how to update Maven dependencies to Apache Flink version 1.12.2. It includes dependencies for flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.12.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.12.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.12.2</version>
</dependency>

----------------------------------------

TITLE: Building Documentation Preview
DESCRIPTION: Command to build and preview the documentation locally. Starts a local web server at http://localhost:1313/ with live reload functionality.

LANGUAGE: shell
CODE:
./build_docs.sh -p

----------------------------------------

TITLE: SQL Non-Windowed Inner Join Query
DESCRIPTION: Example of a non-windowed inner join SQL query in Flink that joins Users and Orders tables based on userId for full-history matching.

LANGUAGE: sql
CODE:
SELECT u.name, u.address, o.productId, o.amount
FROM Users u JOIN Orders o
  ON u.userId = o.userId

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.11.3
DESCRIPTION: XML snippet showing how to update Maven dependencies to Apache Flink version 1.11.3 for Java, Streaming Java, and Clients modules.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.11.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.11.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.11.3</version>
</dependency>

----------------------------------------

TITLE: Cloning Flink Repository
DESCRIPTION: Command to clone a forked Flink repository to the local machine. Requires Git to be installed and a GitHub account with a forked Flink repository.

LANGUAGE: bash
CODE:
git clone https://github.com/<your-user-name>/flink.git

----------------------------------------

TITLE: Multi-Language Code Tabs in HTML/Markdown
DESCRIPTION: Example showing how to create tabbed code blocks for multiple programming languages using HTML and Markdown.

LANGUAGE: html
CODE:
<div class="codetabs" markdown="1">

  <div data-lang="java" markdown="1"> 

  ```java
   // Java Code
  ```

  </div>

  <div data-lang="scala" markdown="1">

  ```scala
   // Scala Code
  ```

  </div> 

</div>

----------------------------------------

TITLE: Installing Financial Fraud Demo Operator
DESCRIPTION: Commands to change directory and install the financial fraud demo operator from the local filesystem.

LANGUAGE: bash
CODE:
cd operators
kubectl kudo install repository/flink/docs/demo/financial-fraud/demo-operator --instance flink-demo

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.10.2
DESCRIPTION: Maven dependency declarations required to upgrade to Flink 1.10.2. Includes core Flink modules: flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.10.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.10.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.10.2</version>
</dependency>

----------------------------------------

TITLE: Rebuilding Flink Website Non-Incrementally with Local Hugo
DESCRIPTION: Rebuilds the Flink website documentation non-incrementally using a local Hugo installation. Generates static HTML files in the 'content' folder for serving the project website.

LANGUAGE: bash
CODE:
./build.sh build

----------------------------------------

TITLE: Counting Git Commits in Bash
DESCRIPTION: Command to count the total number of git commits in the Flink repository after December 31, 2016.

LANGUAGE: bash
CODE:
git log --pretty=oneline --after=12/31/2016 | wc -l

----------------------------------------

TITLE: Configuring Maven Dependencies for Apache Flink 1.1.3
DESCRIPTION: Maven dependency configuration for core Flink components including flink-java, flink-streaming-java, and flink-clients modules at version 1.1.3.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.1.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.10</artifactId>
  <version>1.1.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.10</artifactId>
  <version>1.1.3</version>
</dependency>

----------------------------------------

TITLE: Building Flink Website with Local Hugo Installation
DESCRIPTION: Builds the Flink website documentation using a local Hugo installation. Requires Hugo to be installed on the system. Serves the built site at http://localhost:1313/.

LANGUAGE: bash
CODE:
$ ./build.sh

----------------------------------------

TITLE: Generating Git Statistics with gitstats
DESCRIPTION: Series of commands to clone the Flink repository and generate statistical analysis using gitstats tool.

LANGUAGE: bash
CODE:
git clone git@github.com:apache/flink.git

LANGUAGE: bash
CODE:
gitstats flink/ flink-stats/

LANGUAGE: bash
CODE:
open flink-stats/index.html

----------------------------------------

TITLE: Configuring Maven Dependencies for Apache Flink 1.16.2
DESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java, and flink-clients at version 1.16.2

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.16.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java</artifactId>
  <version>1.16.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients</artifactId>
  <version>1.16.2</version>
</dependency>

----------------------------------------

TITLE: Generating Git Statistics
DESCRIPTION: Commands to generate and view Git statistics using Gitstats tool.

LANGUAGE: bash
CODE:
gitstats flink/ flink-stats/

LANGUAGE: bash
CODE:
chrome flink-stats/index.html

----------------------------------------

TITLE: Java Class Comment Example - Don't
DESCRIPTION: Example of a poor class-level JavaDoc comment that provides no meaningful information.

LANGUAGE: java
CODE:
/**
 * The symbol expression.
 */
public class CommonSymbolExpression {}

----------------------------------------

TITLE: Installing Gitstats on macOS
DESCRIPTION: Command to install the Gitstats tool using Homebrew package manager on macOS.

LANGUAGE: bash
CODE:
brew install --HEAD homebrew/head-only/gitstats

----------------------------------------

TITLE: Hugo Page Configuration in YAML
DESCRIPTION: YAML frontmatter configuration for a Hugo documentation page, setting up the About section properties including collapse behavior and menu weights.

LANGUAGE: yaml
CODE:
---
title: About
bookCollapseSection: true
weight: 5
menu_weight: 1
---

----------------------------------------

TITLE: Cloning Apache Flink Git Repository
DESCRIPTION: A Git command to clone the Apache Flink repository from GitHub.

LANGUAGE: bash
CODE:
git clone git@github.com:apache/flink.git

----------------------------------------

TITLE: Flink Documentation Frontmatter in Markdown
DESCRIPTION: YAML frontmatter configuration for a Flink documentation page, specifying weight, title and external documentation link.

LANGUAGE: markdown
CODE:
---
weight: 1
title: With Flink
bookHref: "https://nightlies.apache.org/flink/flink-docs-stable/docs/try-flink/local_installation/"
---

----------------------------------------

TITLE: Viewing Kafka Deployment Plan Status
DESCRIPTION: Command to view the deployment plan status for the Kafka instance in the Flink demo.

LANGUAGE: bash
CODE:
kubectl kudo plan status --instance flink-demo-kafka

----------------------------------------

TITLE: Configuring Flink Blog Section in YAML
DESCRIPTION: This YAML snippet defines the configuration for the Flink blog section. It sets the title, weight for ordering, menu weight, and enables the section link.

LANGUAGE: yaml
CODE:
---
title: Flink Blog
weight: 1
menu_weight: 2
section_link: true
---

----------------------------------------

TITLE: Setting Network Buffer Timeout in Flink
DESCRIPTION: Configures the network buffer timeout to 10ms for reduced latency in Flink applications, potentially at the cost of some throughput.

LANGUAGE: yaml
CODE:
execution.buffer-timeout: 10 ms

----------------------------------------

TITLE: Apache License Header Comment Block
DESCRIPTION: Standard Apache 2.0 License header included as an HTML comment to specify the licensing terms for this documentation.

LANGUAGE: html
CODE:
<!--
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->

----------------------------------------

TITLE: Apache License Header Comment Block
DESCRIPTION: Standard Apache 2.0 License header included as an HTML comment to specify the licensing terms for this documentation.

LANGUAGE: html
CODE:
<!--
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->

----------------------------------------

TITLE: Installing Flink Kubernetes Operator 1.6.0 using Helm
DESCRIPTION: Bash commands to add the Flink Kubernetes Operator 1.6.0 Helm chart to a local registry and install it. This enables users to quickly try out the new features of the 1.6.0 release.

LANGUAGE: bash
CODE:
$ helm repo add flink-kubernetes-operator-1.6.0 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.6.0/
$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.6.0/flink-kubernetes-operator --set webhook.create=false

----------------------------------------

TITLE: Counting Git Commits in Bash
DESCRIPTION: A bash command to count the number of Git commits in the Flink repository after December 31, 2015.

LANGUAGE: bash
CODE:
git log --pretty=oneline --after=12/31/2015 | wc -l

----------------------------------------

TITLE: Viewing Kubernetes Events Output for Flink Deployments
DESCRIPTION: Example showing the format of Kubernetes events emitted by the operator for Flink deployment and job status changes.

LANGUAGE: text
CODE:
Events:
  Type    Reason         Age   From                  Message
  ----    ------         ----  ----                  -------
  Normal  Submit         53m   JobManagerDeployment  Starting deployment
  Normal  StatusChanged  52m   Job                   Job status changed from RECONCILING to CREATED
  Normal  StatusChanged  52m   Job                   Job status changed from CREATED to RUNNING

----------------------------------------

TITLE: Installing Gitstats with Homebrew
DESCRIPTION: A bash command to install the Gitstats tool using Homebrew package manager on macOS.

LANGUAGE: bash
CODE:
brew install --HEAD homebrew/head-only/gitstats

----------------------------------------

TITLE: Installing Flink Kubernetes Operator 1.3.1 using Helm
DESCRIPTION: This snippet demonstrates how to add the Helm chart repository for Flink Kubernetes Operator 1.3.1 and install it using Helm. It adds the repository and installs the operator with the webhook creation disabled.

LANGUAGE: bash
CODE:
$ helm repo add flink-kubernetes-operator-1.3.1 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.3.1/
$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.3.1/flink-kubernetes-operator --set webhook.create=false

----------------------------------------

TITLE: Cloning Flink Repository
DESCRIPTION: Git command to clone the Apache Flink repository.

LANGUAGE: bash
CODE:
git clone git@github.com:apache/flink.git

----------------------------------------

TITLE: Installing Flink Kubernetes Operator 1.10.0 via Helm
DESCRIPTION: Commands to add the Flink Kubernetes Operator 1.10.0 Helm repository and install the operator with webhook creation disabled. This allows users to quickly deploy the operator in their Kubernetes environment.

LANGUAGE: bash
CODE:
$ helm repo add flink-kubernetes-operator-1.10.0 https://archive.apache.org/dist/flink/flink-kubernetes-operator-1.10.0/
$ helm install flink-kubernetes-operator flink-kubernetes-operator-1.10.0/flink-kubernetes-operator --set webhook.create=false

----------------------------------------

TITLE: Opening Generated Git Statistics
DESCRIPTION: A bash command to open the generated Git statistics HTML page in the default browser.

LANGUAGE: bash
CODE:
open flink-stats/index.html

----------------------------------------

TITLE: Creating CDC Table with Debezium JSON Format in SQL
DESCRIPTION: Shows how to create a table that can consume Change Data Capture events using Debezium JSON format

LANGUAGE: sql
CODE:
CREATE TABLE my_table (
  ...
) WITH (
  'connector'='...', -- e.g. 'kafka'
  'format'='debezium-json',
  'debezium-json.schema-include'='true' -- default: false (Debezium can be configured to include or exclude the message schema)
  'debezium-json.ignore-parse-errors'='true' -- default: false
);

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.5.1
DESCRIPTION: XML snippet showing how to update Maven dependencies to Apache Flink version 1.5.1. It includes dependencies for flink-java, flink-streaming-java, and flink-clients modules.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.5.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.5.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.5.1</version>
</dependency>

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.5.2
DESCRIPTION: XML snippet showing how to update Maven dependencies to Apache Flink version 1.5.2. It includes dependencies for flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.5.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.5.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.5.2</version>
</dependency>

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.5.6
DESCRIPTION: XML snippet showing how to update Maven dependencies to Apache Flink version 1.5.6. Includes dependencies for flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.5.6</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.5.6</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.5.6</version>
</dependency>

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.10.3
DESCRIPTION: XML snippet showing how to update Maven dependencies to Apache Flink 1.10.3 for flink-java, flink-streaming-java, and flink-clients modules.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.10.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.10.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.10.3</version>
</dependency>

----------------------------------------

TITLE: Updating Maven Dependencies for Apache Flink 1.3.3
DESCRIPTION: This XML snippet shows how to update Maven dependencies for Apache Flink 1.3.3. It includes dependencies for flink-java, flink-streaming-java, and flink-clients, all set to version 1.3.3.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.3.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.10</artifactId>
  <version>1.3.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.10</artifactId>
  <version>1.3.3</version>
</dependency>

----------------------------------------

TITLE: Declaring Maven Dependencies for Apache Flink 1.17.1
DESCRIPTION: XML snippet showing how to declare Maven dependencies for Apache Flink 1.17.1 core components including flink-java, flink-streaming-java, and flink-clients.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.17.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java</artifactId>
  <version>1.17.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients</artifactId>
  <version>1.17.1</version>
</dependency>

----------------------------------------

TITLE: Configuring Maven Dependencies for Apache Flink 1.14.5
DESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java and flink-clients for version 1.14.5.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.14.5</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.14.5</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.14.5</version>
</dependency>

----------------------------------------

TITLE: Configuring Maven Dependencies for Flink 1.13.2
DESCRIPTION: XML configuration for core Flink Maven dependencies including flink-java, flink-streaming-java, and flink-clients with version 1.13.2

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.13.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.13.2</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.13.2</version>
</dependency>

----------------------------------------

TITLE: Configuring Maven Dependencies for Apache Flink 1.8.3
DESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java, and flink-clients, all updated to version 1.8.3.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.8.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.8.3</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.8.3</version>
</dependency>

----------------------------------------

TITLE: Configuring Maven Dependencies for Apache Flink 1.2.1
DESCRIPTION: Maven dependency configuration for core Flink modules including flink-java, flink-streaming-java and flink-clients. These dependencies are required to build Flink applications using version 1.2.1.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.2.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.10</artifactId>
  <version>1.2.1</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.10</artifactId>
  <version>1.2.1</version>
</dependency>

----------------------------------------

TITLE: Updating Maven Dependencies for Flink 1.11.4
DESCRIPTION: Maven dependency configuration block showing the core Flink artifacts that should be updated to version 1.11.4. Includes flink-java, flink-streaming-java, and flink-clients dependencies.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>1.11.4</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java_2.11</artifactId>
  <version>1.11.4</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients_2.11</artifactId>
  <version>1.11.4</version>
</dependency>

----------------------------------------

TITLE: Adding Apache Flink Maven Dependencies
DESCRIPTION: XML snippet for adding core Apache Flink dependencies to a Maven project. Includes flink-java, flink-streaming-java, and flink-clients artifacts.

LANGUAGE: XML
CODE:
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-java</artifactId>
  <version>{{< param FlinkStableVersion >}}</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-streaming-java</artifactId>
  <version>{{< param FlinkStableVersion >}}</version>
</dependency>
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-clients</artifactId>
  <version>{{< param FlinkStableVersion >}}</version>
</dependency>

----------------------------------------

TITLE: Implementing serialVersionUID for Serializable Classes in Java
DESCRIPTION: Demonstrates the correct way to define a serialVersionUID for classes that implement Java Serialization, which is used internally in Flink for RPC communication.

LANGUAGE: Java
CODE:
private static final long serialVersionUID = 1L;

----------------------------------------

TITLE: Cloning Flink Website Repository
DESCRIPTION: Git command to clone the forked Flink website repository to local machine. Requires a GitHub account and forked repository.

LANGUAGE: bash
CODE:
git clone https://github.com/<your-user-name>/flink-web.git

----------------------------------------

TITLE: Running Website Build Script
DESCRIPTION: Command to build the website locally and start a development server for preview at http://localhost:1313.

LANGUAGE: bash
CODE:
./build.sh

----------------------------------------

TITLE: Inserting Recent Posts in HTML
DESCRIPTION: This snippet uses a custom Hugo shortcode to insert recent blog posts into the page. It's likely part of a documentation site generator setup.

LANGUAGE: html
CODE:
{{< recent_posts >}}

----------------------------------------

TITLE: Inserting Recent Posts in HTML
DESCRIPTION: This snippet uses a custom Hugo shortcode to insert recent blog posts into the page. It's likely part of a documentation site generator setup.

LANGUAGE: html
CODE:
{{< recent_posts >}}

----------------------------------------

TITLE: Front Matter Configuration in Markdown
DESCRIPTION: YAML front matter configuration for the documentation page, specifying weight, title and book reference URL.

LANGUAGE: yaml
CODE:
---
weight: 5
title: Kubernetes Operator Main (snapshot)
bookHref: "https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main"
---

----------------------------------------

TITLE: YAML Configuration for Logo Assets Table
DESCRIPTION: YAML configuration defining the structure and content of tables displaying Flink logo variants, including metadata for different formats and display options.

LANGUAGE: yaml
CODE:
tables:
    png:
        name: "png"
        cols: 
          - id: "Colored"
            name: "Colored logo"
          - id: "WhiteFilled"
            name: "White filled logo"
          - id: "BlackOutline"
            name: "Black outline logo"

----------------------------------------

TITLE: Configuring Flink ML Documentation Header in YAML
DESCRIPTION: YAML front matter block defining metadata properties for the Flink ML documentation page including weight, title and documentation URL.

LANGUAGE: yaml
CODE:
weight: 9
title: ML Master (snapshot)
bookHref: "https://nightlies.apache.org/flink/flink-ml-docs-master"

----------------------------------------

TITLE: Markdown Documentation Header for Flink CDC
DESCRIPTION: Markdown frontmatter and configuration for a documentation page about Flink CDC, including weight, title, and external book reference.

LANGUAGE: markdown
CODE:
---
weight: 3
title: With Flink CDC
bookHref: "https://nightlies.apache.org/flink/flink-cdc-docs-stable/docs/get-started/introduction/"
---

----------------------------------------

TITLE: Markdown Front Matter Configuration for Flink Documentation
DESCRIPTION: YAML front matter configuration block defining weight, title and documentation URL for Flink 2.0 preview documentation.

LANGUAGE: yaml
CODE:
---
weight: 2
title: Flink 2.0 (preview)
bookHref: "https://nightlies.apache.org/flink/flink-docs-release-2.0-preview1/"
---

----------------------------------------

TITLE: Network Metrics Table Styling
DESCRIPTION: CSS styles for formatting tables displaying network metrics information, including border collapse, padding, and cell alignment.

LANGUAGE: css
CODE:
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{padding:10px 10px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{padding:10px 10px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;background-color:#eff0f1;}
.tg .tg-wide{padding:10px 30px;}
.tg .tg-top{vertical-align:top}
.tg .tg-topcenter{text-align:center;vertical-align:top}
.tg .tg-center{text-align:center;vertical-align:center}