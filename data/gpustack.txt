TITLE: Installing GPUStack on Linux or macOS
DESCRIPTION: This command installs GPUStack as a service on systemd or launchd based systems with default port 80.

LANGUAGE: bash
CODE:
curl -sfL https://get.gpustack.ai | sh -s -

----------------------------------------

TITLE: Running chat with llama3.2 model
DESCRIPTION: Command to run and chat with the llama3.2 model using GPUStack.

LANGUAGE: bash
CODE:
gpustack chat llama3.2 "tell me a joke."

----------------------------------------

TITLE: Installing GPUStack on Linux/macOS
DESCRIPTION: Shell commands for installing and configuring GPUStack service on Linux and macOS systems with various options including custom ports, data directories, TLS, and database configurations.

LANGUAGE: shell
CODE:
# Run server.
curl -sfL https://get.gpustack.ai | sh -s -

# Run server with non-default port.
curl -sfL https://get.gpustack.ai | sh -s - --port 8080

# Run server with a custom data path.
curl -sfL https://get.gpustack.ai | sh -s - --data-dir /data/gpustack-data

# Run server without the embedded worker.
curl -sfL https://get.gpustack.ai | sh -s - --disable-worker

# Run server with TLS.
curl -sfL https://get.gpustack.ai | sh -s - --ssl-keyfile /path/to/keyfile --ssl-certfile /path/to/certfile

# Run server with external postgresql database.
curl -sfL https://get.gpustack.ai | sh -s - --database-url "postgresql://username:password@host:port/database_name"

# Run worker with specified IP.
curl -sfL https://get.gpustack.ai | sh -s - --server-url http://myserver --token mytoken --worker-ip 192.168.1.100

# Install with a custom index URL.
curl -sfL https://get.gpustack.ai | INSTALL_INDEX_URL=https://pypi.tuna.tsinghua.edu.cn/simple sh -s -

# Install a custom wheel package other than releases form pypi.org.
curl -sfL https://get.gpustack.ai | INSTALL_PACKAGE_SPEC=https://repo.mycompany.com/my-gpustack.whl sh -s -

# Install a specific version with extra audio dependencies.
curl -sfL https://get.gpustack.ai | INSTALL_PACKAGE_SPEC=gpustack[audio]==0.4.0 sh -s -

----------------------------------------

TITLE: Installing GPUStack on Windows
DESCRIPTION: PowerShell commands for installing and configuring GPUStack service on Windows systems, including custom settings for ports, data paths, TLS, and worker configurations.

LANGUAGE: powershell
CODE:
# Run server.
Invoke-Expression (Invoke-WebRequest -Uri "https://get.gpustack.ai" -UseBasicParsing).Content

# Run server with non-default port.
Invoke-Expression "& { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --port 8080"

# Run server with a custom data path.
Invoke-Expression "& { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --data-dir 'D:\gpustack-data'"

# Run server without the embedded worker.
Invoke-Expression "& { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --disable-worker"

# Run server with TLS.
Invoke-Expression "& { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --ssl-keyfile 'C:\path\to\keyfile' --ssl-certfile 'C:\path\to\certfile'"

# Run server with external postgresql database.
Invoke-Expression "& { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --database-url 'postgresql://username:password@host:port/database_name'"

# Run worker with specified IP.
Invoke-Expression "& { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --server-url 'http://myserver' --token 'mytoken' --worker-ip '192.168.1.100'"

# Run worker with customize reserved resource.
Invoke-Expression "& { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --server-url 'http://myserver' --token 'mytoken' --system-reserved '{\"ram\":5, \"vram\":5}'"

# Install with a custom index URL.
$env:INSTALL_INDEX_URL = "https://pypi.tuna.tsinghua.edu.cn/simple"
Invoke-Expression (Invoke-WebRequest -Uri "https://get.gpustack.ai" -UseBasicParsing).Content

# Install a custom wheel package other than releases form pypi.org.
$env:INSTALL_PACKAGE_SPEC = "https://repo.mycompany.com/my-gpustack.whl"
Invoke-Expression (Invoke-WebRequest -Uri "https://get.gpustack.ai" -UseBasicParsing).Content

# Install a specific version with extra audio dependencies.
$env:INSTALL_PACKAGE_SPEC = "gpustack[audio]==0.4.0"
Invoke-Expression (Invoke-WebRequest -Uri "https://get.gpustack.ai" -UseBasicParsing).Content

----------------------------------------

TITLE: Running LLM Chat with GPUStack
DESCRIPTION: This command demonstrates how to run and chat with the llama3.2 model using GPUStack.

LANGUAGE: bash
CODE:
gpustack chat llama3.2 "tell me a joke."

----------------------------------------

TITLE: Using GPUStack OpenAI-compatible API with curl
DESCRIPTION: This bash script demonstrates how to use the GPUStack OpenAI-compatible API with curl. It sets the API key and sends a chat completion request to the server.

LANGUAGE: bash
CODE:
export GPUSTACK_API_KEY=myapikey
curl http://myserver/v1-openai/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $GPUSTACK_API_KEY" \
  -d '{
    "model": "llama3.2",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Hello!"
      }
    ],
    "stream": true
  }'

----------------------------------------

TITLE: Sending Chat Completion Request with curl
DESCRIPTION: This snippet demonstrates how to use curl to send a chat completion request to GPUStack's OpenAI-compatible API. It sets the API key, specifies the endpoint, and sends a JSON payload with the model and messages.

LANGUAGE: bash
CODE:
export GPUSTACK_API_KEY=myapikey
curl http://myserver/v1-openai/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $GPUSTACK_API_KEY" \
  -d '{
    "model": "llama3",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Hello!"
      }
    ],
    "stream": true
  }'

----------------------------------------

TITLE: Running GPUStack Server with NVIDIA GPU Support
DESCRIPTION: Docker commands to run GPUStack server with NVIDIA GPU support using either host network or specific port mappings.

LANGUAGE: shell
CODE:
docker run -d --name gpustack \
    --restart=unless-stopped \
    --gpus all \
    --network=host \
    --ipc=host \
    -v gpustack-data:/var/lib/gpustack \
    gpustack/gpustack

----------------------------------------

TITLE: Using API Key with Curl for GPUStack API Request in Bash
DESCRIPTION: This snippet demonstrates how to use a GPUStack API key as a bearer token in a curl request to the chat completions endpoint. It sets the API key as an environment variable and includes it in the Authorization header of the request. The example also shows how to structure the JSON payload for a chat completion request.

LANGUAGE: bash
CODE:
export GPUSTACK_API_KEY=myapikey
curl http://myserver/v1-openai/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $GPUSTACK_API_KEY" \
  -d '{
    "model": "llama3",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Hello!"
      }
    ],
    "stream": true
  }'

----------------------------------------

TITLE: Custom Dockerfile for GPUStack with CUDA Support
DESCRIPTION: Dockerfile to build a custom GPUStack image with configurable CUDA version and necessary dependencies.

LANGUAGE: dockerfile
CODE:
# Example Dockerfile
ARG CUDA_VERSION=12.4.1

FROM nvidia/cuda:$CUDA_VERSION-cudnn-runtime-ubuntu22.04

ARG TARGETPLATFORM
ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y \
    git \
    curl \
    wget \
    tzdata \
    iproute2 \
    python3 \
    python3-pip \
    python3-venv \
    && rm -rf /var/lib/apt/lists/*

COPY . /workspace/gpustack
RUN cd /workspace/gpustack && \
    make build

RUN if [ "$TARGETPLATFORM" = "linux/amd64" ]; then \
    WHEEL_PACKAGE="$(ls /workspace/gpustack/dist/*.whl)[all]"; \
    else  \
    WHEEL_PACKAGE="$(ls /workspace/gpustack/dist/*.whl)[audio]"; \
    fi && \
    pip install pipx && \
    pip install $WHEEL_PACKAGE && \
    pip cache purge && \
    rm -rf /workspace/gpustack

RUN gpustack download-tools

ENTRYPOINT [ "gpustack", "start" ]

----------------------------------------

TITLE: Generating Text Embeddings with GPUStack API using curl
DESCRIPTION: This bash script demonstrates how to use curl to make an API call to GPUStack for generating text embeddings. It sets environment variables for the server URL and API key, then sends a POST request with JSON payload to the embeddings endpoint.

LANGUAGE: bash
CODE:
export SERVER_URL=<your-server-url>
export GPUSTACK_API_KEY=<your-api-key>
curl $SERVER_URL/v1-openai/embeddings \
  -H "Authorization: Bearer $GPUSTACK_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "input": "The food was delicious and the waiter...",
    "model": "bge-small-en-v1.5",
    "encoding_format": "float"
  }'

----------------------------------------

TITLE: Generating image with stable-diffusion model
DESCRIPTION: Command to run and generate an image using the stable-diffusion-v3-5-large-turbo model with GPUStack. This command downloads the model from Hugging Face and requires sufficient disk space and VRAM.

LANGUAGE: bash
CODE:
gpustack draw hf.co/gpustack/stable-diffusion-v3-5-large-turbo-GGUF:stable-diffusion-v3-5-large-turbo-Q4_0.gguf \
"A minion holding a sign that says 'GPUStack'. The background is filled with futuristic elements like neon lights, circuit boards, and holographic displays. The minion is wearing a tech-themed outfit, possibly with LED lights or digital patterns. The sign itself has a sleek, modern design with glowing edges. The overall atmosphere is high-tech and vibrant, with a mix of dark and neon colors." \
--sample-steps 5 --show

----------------------------------------

TITLE: Configuring Llama3.2 Model Set in YAML for GPUStack Model Catalog
DESCRIPTION: This YAML snippet defines a model set for Llama3.2 in the GPUStack Model Catalog. It includes metadata about the model, such as description, capabilities, and sizes, as well as deployment templates for different quantizations and backends.

LANGUAGE: yaml
CODE:
- name: Llama3.2
  description: The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.
  home: https://www.llama.com/
  icon: /static/catalog_icons/meta.png
  categories:
    - llm
  capabilities:
    - context/128k
    - tools
  sizes:
    - 1
    - 3
  licenses:
    - llama3.2
  release_date: "2024-09-25"
  order: 2
  templates:
    - quantizations:
        - Q3_K_L
        - Q4_K_M
        - Q5_K_M
        - Q6_K_L
        - Q8_0
        - f16
      source: huggingface
      huggingface_repo_id: bartowski/Llama-3.2-{size}B-Instruct-GGUF
      huggingface_filename: "*-{quantization}*.gguf"
      replicas: 1
      backend: llama-box
      cpu_offloading: true
      distributed_inference_across_workers: true
    - quantizations: ["BF16"]
      source: huggingface
      huggingface_repo_id: unsloth/Llama-3.2-{size}B-Instruct
      replicas: 1
      backend: vllm
      backend_parameters:
        - --enable-auto-tool-choice
        - --tool-call-parser=llama3_json
        - --chat-template={data_dir}/chat_templates/tool_chat_template_llama3.2_json.jinja

----------------------------------------

TITLE: Available Interactive Commands
DESCRIPTION: List of commands available during interactive chat sessions for control and help.

LANGUAGE: text
CODE:
Commands:
  \q or \quit - Quit the chat
  \c or \clear - Clear chat context in prompt
  \? or \h or \help - Print this help message

----------------------------------------

TITLE: Generating Image with GPUStack Stable Diffusion Model
DESCRIPTION: This command generates an image using the stable-diffusion-v3-5-large-turbo model. It includes a detailed prompt and CLI options for customization.

LANGUAGE: bash
CODE:
gpustack draw hf.co/gpustack/stable-diffusion-v3-5-large-turbo-GGUF:stable-diffusion-v3-5-large-turbo-Q4_0.gguf \
"A minion holding a sign that says 'GPUStack'. The background is filled with futuristic elements like neon lights, circuit boards, and holographic displays. The minion is wearing a tech-themed outfit, possibly with LED lights or digital patterns. The sign itself has a sleek, modern design with glowing edges. The overall atmosphere is high-tech and vibrant, with a mix of dark and neon colors." \
--sample-steps 5 --show

----------------------------------------

TITLE: Default Password Location
DESCRIPTION: Default system path for admin password file

LANGUAGE: markdown
CODE:
/var/lib/gpustack/initial_admin_password

----------------------------------------

TITLE: Installing GPUStack in Air-Gapped Environment (Bash)
DESCRIPTION: Commands for installing GPUStack in an air-gapped environment using pre-downloaded packages and tools. This includes installing from local packages and loading pre-downloaded tools.

LANGUAGE: bash
CODE:
pip install --no-index --find-links=gpustack_offline_packages gpustack

gpustack download-tools --load-archive gpustack_offline_tools.tar.gz

----------------------------------------

TITLE: Starting GPUStack Worker
DESCRIPTION: Command to start a GPUStack worker and register it with the GPUStack server, requiring server URL, token, and worker IP address.

LANGUAGE: shell
CODE:
gpustack start --server-url http://your_gpustack_url --token your_gpustack_token --worker-ip your_worker_host_ip

----------------------------------------

TITLE: Installing GPUStack CLI
DESCRIPTION: Commands to install GPUStack CLI using pip, with options for additional dependencies.

LANGUAGE: shell
CODE:
pip install gpustack

LANGUAGE: shell
CODE:
# vllm is currently only available for Linux on AMD64
pip install gpustack[all]

----------------------------------------

TITLE: Running GPUStack Docker Container
DESCRIPTION: Docker command to start GPUStack container with necessary configurations for Hygon DCU support, including device mappings and resource allocations.

LANGUAGE: bash
CODE:
docker run -itd --shm-size 500g \
   --network=host --privileged \
   --group-add video \
   --cap-add=SYS_PTRACE \
   --security-opt seccomp=unconfined \
   --device=/dev/kfd --device=/dev/dri \
   -v /opt/hyhal:/opt/hyhal:ro \
   gpustack/gpustack:v0.5.1-dcu

----------------------------------------

TITLE: Querying GPUStack Rerank API using cURL
DESCRIPTION: This snippet demonstrates how to make a request to the GPUStack Rerank API using cURL. It includes setting the API key, specifying the model, query, and documents to rerank.

LANGUAGE: bash
CODE:
export GPUSTACK_API_KEY=myapikey
curl http://myserver/v1/rerank \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $GPUSTACK_API_KEY" \
    -d '{
        "model": "bge-reranker-v2-m3",
        "query": "What is a panda?",
        "top_n": 3,
        "documents": [
            "hi",
            "it is a bear",
            "The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China."
        ]
    }' | jq

----------------------------------------

TITLE: Using gpustack download-tools Command
DESCRIPTION: Command to download dependency tools including llama-box, gguf-parser, and fastfetch. Supports various configuration options for specifying download URLs, archive handling, and target platform specifications.

LANGUAGE: bash
CODE:
gpustack download-tools [OPTIONS]

----------------------------------------

TITLE: Using GPUStack API with curl
DESCRIPTION: Example of using curl to access the OpenAI-compatible API provided by GPUStack. This command sets the API key and sends a chat completion request.

LANGUAGE: bash
CODE:
export GPUSTACK_API_KEY=myapikey
curl http://myserver/v1-openai/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $GPUSTACK_API_KEY" \
  -d '{
    "model": "llama3.2",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Hello!"
      }
    ],
    "stream": true
  }'

----------------------------------------

TITLE: Making Tool Calling API Request with Curl
DESCRIPTION: Example of making a weather query API request to GPUStack using curl. Demonstrates how to structure the request with tool definitions and handle authentication.

LANGUAGE: bash
CODE:
export GPUSTACK_SERVER_URL=<your-server-url>
export GPUSTACK_API_KEY=<your-api-key>
curl $GPUSTACK_SERVER_URL/v1-openai/chat/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $GPUSTACK_API_KEY" \
-d '{
  "model": "qwen2.5-7b-instruct",
  "messages": [
    {
      "role": "user",
      "content": "What\'s the weather like in Boston today?"
    }
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_current_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "The city and state, e.g. San Francisco, CA"
            },
            "unit": {
              "type": "string",
              "enum": ["celsius", "fahrenheit"]
            }
          },
          "required": ["location"]
        }
      }
    }
  ],
  "tool_choice": "auto"}'

----------------------------------------

TITLE: Configuring Image Editing Parameters in GPUStack
DESCRIPTION: This code block specifies the parameters used for image editing in GPUStack. It includes settings for image size, sampling method, steps, guidance, scale, strength, and seed for reproducibility.

LANGUAGE: plaintext
CODE:
Size: 768x1024(3:4)
Sample Method: euler
Schedule Method: discrete
Sampling Steps: 50
Guidance: 30.0
CFG Scale: 1.0
Strength: 1.0
Seed: 656821733471329
Text Prompt: Pink short hair bang, natural

----------------------------------------

TITLE: Streaming Image Generation Request in JSON
DESCRIPTION: Example JSON request for streaming image generation API with basic options.

LANGUAGE: json
CODE:
{
  "n": 1,
  "response_format": "b64_json",
  "size": "512x512",
  "prompt": "A lovely cat",
  "quality": "standard",
  "stream": true,
  "stream_options": {
    "include_usage": true
  }
}

----------------------------------------

TITLE: Adding Dependencies to gpustack Project
DESCRIPTION: These commands demonstrate how to add new dependencies to the gpustack project using Poetry. The first command adds a general dependency, while the second adds a development or testing dependency.

LANGUAGE: bash
CODE:
poetry add <something>

LANGUAGE: bash
CODE:
poetry add --group dev <something>

----------------------------------------

TITLE: GPUStack Environment Variables Configuration
DESCRIPTION: Example of environment variables configuration file for GPUStack service, showing how to set Hugging Face token and endpoint.

LANGUAGE: shell
CODE:
HF_TOKEN="mytoken"
HF_ENDPOINT="https://my-hf-endpoint"

----------------------------------------

TITLE: Monitoring NPU Device Status and Process Information
DESCRIPTION: Console output from npu-smi tool showing comprehensive system monitoring data for 8 NPU devices. Information includes device health, power consumption, temperature, memory utilization, and details of running processes. Each NPU runs multiple instances of hlt_host_devmm_ process.

LANGUAGE: console
CODE:
+-------------------------------------------------------------------------------------------+
| npu-smi 24.1.RC2                          Version: 24.1.RC2                               |
+----------------------+---------------+----------------------------------------------------+
| NPU   Name           | Health        | Power(W)    Temp(C)           Hugepages-Usage(page)|
| Chip                 | Bus-Id        | AICore(%)   Memory-Usage(MB)  HBM-Usage(MB)        |
+======================+===============+====================================================+
| 0     xxx            | OK            | 70.5        42                1232 / 1232          |
| 0                    | 0000:C1:00.0  | 0           4252 / 15137      0    / 32768         |
+======================+===============+====================================================+
| 1     xxx            | OK            | 71.2        36                1232 / 1232          |
| 0                    | 0000:81:00.0  | 0           4666 / 15137      0    / 32768         |
+======================+===============+====================================================+
| 2     xxx            | OK            | 69.0        35                1232 / 1232          |
| 0                    | 0000:41:00.0  | 0           4317 / 15137      0    / 32768         |
+======================+===============+====================================================+
| 3     xxx            | Warning       | 206.9       39                1232 / 1232          |
| 0                    | 0000:01:00.0  | 0           3616 / 15039      0    / 32768         |
+======================+===============+====================================================+
| 4     xxx            | OK            | 70.2        39                1232 / 1232          |
| 0                    | 0000:C2:00.0  | 0           3708 / 15137      0    / 32768         |
+======================+===============+====================================================+
| 5     xxx            | OK            | 64.9        37                1232 / 1232          |
| 0                    | 0000:82:00.0  | 0           4346 / 15137      0    / 32768         |
+======================+===============+====================================================+
| 6     xxx            | OK            | 69.4        40                1232 / 1232          |
| 0                    | 0000:42:00.0  | 0           4261 / 15137      0    / 32768         |
+======================+===============+====================================================+
| 7     xxx            | Warning       | 203.2       42                1232 / 1232          |
| 0                    | 0000:02:00.0  | 0           4521 / 15039      0    / 32768         |
+======================+===============+====================================================+

----------------------------------------

TITLE: Tool Calling API Response Example
DESCRIPTION: Example JSON response from the GPUStack API showing tool calling results, including the function call details and token usage statistics.

LANGUAGE: json
CODE:
{
  "model": "qwen2.5-7b-instruct",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": null,
        "tool_calls": [
          {
            "id": "chatcmpl-tool-b99d32848b324eaea4bac5a5830d00b8",
            "type": "function",
            "function": {
              "name": "get_current_weather",
              "arguments": "{\"location\": \"Boston, MA\", \"unit\": \"fahrenheit\"}"
            }
          }
        ]
      },
      "finish_reason": "tool_calls"
    }
  ],
  "usage": {
    "prompt_tokens": 212,
    "total_tokens": 242,
    "completion_tokens": 30
  }
}

----------------------------------------

TITLE: Generating Image with FLUX.1-dev Model
DESCRIPTION: Example prompt and parameter configuration for generating an image of a singing kangaroo using the FLUX.1-dev model with recommended settings.

LANGUAGE: text
CODE:
Prompt: A kangaroo holding a beer,wearing ski goggles and passionately singing silly songs.
Size: 1024x1024
Sampler: euler
Scheduler: discrete
Steps: 20
CFG: 1.0
Seed: 838887451

----------------------------------------

TITLE: One-time Chat Example
DESCRIPTION: Example of using gpustack chat for a single interaction with a language model using llama3.

LANGUAGE: bash
CODE:
gpustack chat llama3 "tell me a joke."

----------------------------------------

TITLE: GPUStack Rerank API JSON Response
DESCRIPTION: This snippet shows the JSON response structure from the GPUStack Rerank API. It includes the model used, reranked results with relevance scores, and token usage information.

LANGUAGE: json
CODE:
{
  "model": "bge-reranker-v2-m3",
  "object": "list",
  "results": [
    {
      "document": {
        "text": "The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China."
      },
      "index": 2,
      "relevance_score": 1.951932668685913
    },
    {
      "document": {
        "text": "it is a bear"
      },
      "index": 1,
      "relevance_score": -3.7347371578216553
    },
    {
      "document": {
        "text": "hi"
      },
      "index": 0,
      "relevance_score": -6.157620906829834
    }
  ],
  "usage": {
    "prompt_tokens": 69,
    "total_tokens": 69
  }
}

----------------------------------------

TITLE: Displaying NPU Status Information using npu-smi
DESCRIPTION: This snippet shows the output of the npu-smi command, version 20.2.0. It displays detailed status information for four NPU devices, including their health status, power consumption, temperature, AICore usage percentage, and memory usage.

LANGUAGE: plaintext
CODE:
+------------------------------------------------------------------------------+
| npu-smi 20.2.0                       Version: 20.2.0                         |
+-------------------+-----------------+----------------------------------------+
| NPU     Name      | Health          | Power(W)          Temp(C)              |
| Chip    Device    | Bus-Id          | AICore(%)         Memory-Usage(MB)     |
+===================+=================+========================================+
| 215     310       | OK              | 12.8              56                   |
| 0       0         | 0000:DA:00.0    | 0                 2703 / 8192          |
+-------------------+-----------------+----------------------------------------+
| 215     310       | OK              | 12.8              57                   |
| 1       1         | 0000:DC:00.0    | 0                 2703 / 8192          |
+-------------------+-----------------+----------------------------------------+
| 215     310       | OK              | 12.8              57                   |
| 2       2         | 0000:DD:00.0    | 0                 2703 / 8192          |
+-------------------+-----------------+----------------------------------------+
| 215     310       | OK              | 12.8              55                   |
| 3       3         | 0000:DB:00.0    | 0                 2703 / 8192          |
+===================+=================+========================================+

----------------------------------------

TITLE: HTML Meta Redirect to Overview Directory
DESCRIPTION: HTML meta refresh tag that automatically redirects the browser to the overview/ subdirectory after 0 seconds. This is a common technique for URL forwarding.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=overview/" />

----------------------------------------

TITLE: GPUStack Reranker API Response Format
DESCRIPTION: Example JSON response from the GPUStack reranker API showing the ranked results with relevance scores and token usage statistics.

LANGUAGE: json
CODE:
{
  "model": "bge-reranker-v2-m3",
  "object": "list",
  "results": [
    {
      "document": {
        "text": "The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China."
      },
      "index": 2,
      "relevance_score": 1.951932668685913
    },
    {
      "document": {
        "text": "it is a bear"
      },
      "index": 1,
      "relevance_score": -3.7347371578216553
    },
    {
      "document": {
        "text": "hi"
      },
      "index": 0,
      "relevance_score": -6.157620906829834
    }
  ],
  "usage": {
    "prompt_tokens": 69,
    "total_tokens": 69
  }
}

----------------------------------------

TITLE: Installing GPUStack Docker Container for Moore Threads GPUs
DESCRIPTION: Docker command to set up an isolated environment for GPUStack, exposing the web interface on port 9009 and mounting a volume for data storage.

LANGUAGE: bash
CODE:
docker run -d --name gpustack-musa -p 9009:80 --ipc=host -v gpustack-data:/var/lib/gpustack \
    gpustack/gpustack:main-musa

----------------------------------------

TITLE: Retrieving GPUStack Token on Windows
DESCRIPTION: PowerShell command to retrieve the authentication token from the server node on Windows systems.

LANGUAGE: bash
CODE:
Get-Content -Path "$env:APPDATA\gpustack\token" -Raw

----------------------------------------

TITLE: Installing Ascend Toolkit and Kernels
DESCRIPTION: Commands for installing the Ascend CANN toolkit and kernel packages.

LANGUAGE: bash
CODE:
chmod +x Ascend-cann-toolkit_{vesion}_linux-{arch}.run
chmod +x Ascend-cann-kernels-{chip_type}_{version}_linux-{arch}.run

sh Ascend-cann-toolkit_{vesion}_linux-{arch}.run --install
sh Ascend-cann-kernels-{chip_type}_{version}_linux-{arch}.run --install

----------------------------------------

TITLE: Verifying GPUStack Installation
DESCRIPTION: Command to verify the GPUStack installation by checking its version.

LANGUAGE: shell
CODE:
gpustack version

----------------------------------------

TITLE: Configuring Ascend Environment Variables
DESCRIPTION: Commands to set up environment variables for Ascend toolkit operation.

LANGUAGE: bash
CODE:
echo "source ~/Ascend/ascend-toolkit/set_env.sh" >> ~/.bashrc
source ~/.bashrc

----------------------------------------

TITLE: Creating Chat Completion with OpenAI Python API
DESCRIPTION: This snippet shows how to use the OpenAI Python API library to create a chat completion request to GPUStack's API. It initializes the client with the custom base URL and API key, then creates a completion with the specified model and messages.

LANGUAGE: python
CODE:
from openai import OpenAI

client = OpenAI(base_url="http://myserver/v1-openai", api_key="myapikey")

completion = client.chat.completions.create(
  model="llama3",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
  ]
)

print(completion.choices[0].message)

----------------------------------------

TITLE: Creating Chat Completion with OpenAI Node.js API
DESCRIPTION: This snippet demonstrates how to use the OpenAI Node.js API library to create a chat completion request to GPUStack's API. It initializes the OpenAI client with the custom base URL and API key, then creates an async function to send the completion request with the specified model and messages.

LANGUAGE: javascript
CODE:
const OpenAI = require("openai");

const openai = new OpenAI({
  apiKey: "myapikey",
  baseURL: "http://myserver/v1-openai",
});

async function main() {
  const params = {
    model: "llama3",
    messages: [
      {
        role: "system",
        content: "You are a helpful assistant.",
      },
      {
        role: "user",
        content: "Hello!",
      },
    ],
  };
  const chatCompletion = await openai.chat.completions.create(params);
  console.log(chatCompletion.choices[0].message);
}
main();

----------------------------------------

TITLE: Installing Python Dependencies for Ascend
DESCRIPTION: Commands to install required Python packages for Ascend toolkit operation.

LANGUAGE: bash
CODE:
pip3 install --upgrade pip
pip3 install attrs numpy decorator sympy cffi pyyaml pathlib2 psutil protobuf scipy requests absl-py wheel typing_extensions

----------------------------------------

TITLE: Checking GPUStack Container Logs
DESCRIPTION: Command to view the logs of the running GPUStack container, which helps verify successful startup.

LANGUAGE: bash
CODE:
docker logs -f gpustack-musa

----------------------------------------

TITLE: Downloading GPUStack Packages and Tools (Bash)
DESCRIPTION: Commands to download GPUStack packages and tools in an online environment for later use in an air-gapped setup. This includes downloading wheel packages, installing GPUStack CLI, and saving dependency tools as an archive.

LANGUAGE: bash
CODE:
PACKAGE_SPEC="gpustack"

pip wheel $PACKAGE_SPEC -w gpustack_offline_packages

pip install gpustack

gpustack download-tools --save-archive gpustack_offline_tools.tar.gz

----------------------------------------

TITLE: Upgrading GPUStack Docker Installation
DESCRIPTION: Command to upgrade GPUStack by pulling a new version of the Docker image. Requires specifying the desired version tag.

LANGUAGE: bash
CODE:
docker pull gpustack/gpustack:vX.Y.Z

----------------------------------------

TITLE: Starting GPUStack Server
DESCRIPTION: Commands to start the GPUStack server, including an option to specify a custom data directory.

LANGUAGE: shell
CODE:
gpustack start

LANGUAGE: shell
CODE:
gpustack start --data-dir mypath

----------------------------------------

TITLE: Installing GPUStack Server on Linux/macOS
DESCRIPTION: Command to install GPUStack server component on Linux or macOS systems using curl.

LANGUAGE: bash
CODE:
curl -sfL https://get.gpustack.ai | sh -s -

----------------------------------------

TITLE: Installing GPUStack Server on Linux/macOS
DESCRIPTION: Command to install GPUStack server component on Linux or macOS systems using curl.

LANGUAGE: bash
CODE:
curl -sfL https://get.gpustack.ai | sh -s -

----------------------------------------

TITLE: Configuring Server Log Level
DESCRIPTION: Command to change the GPUStack server's log level at runtime using a HTTP PUT request. Supports multiple log levels including trace, debug, info, warning, error, and critical.

LANGUAGE: bash
CODE:
curl -X PUT http://localhost/debug/log_level -d "debug"

----------------------------------------

TITLE: Configuring Worker Log Level
DESCRIPTION: Command to change the GPUStack worker's log level at runtime using a HTTP PUT request. Uses port 10150 and supports the same log levels as the server.

LANGUAGE: bash
CODE:
curl -X PUT http://localhost:10150/debug/log_level -d "debug"

----------------------------------------

TITLE: Bootstrap Password Parameter
DESCRIPTION: Command line parameter for setting custom admin password

LANGUAGE: markdown
CODE:
--bootstrap-password

----------------------------------------

TITLE: Bootstrap Password Parameter
DESCRIPTION: Command line parameter for setting custom admin password

LANGUAGE: markdown
CODE:
--bootstrap-password

----------------------------------------

TITLE: Checking GLIBC Version on Linux for GPUStack Installation
DESCRIPTION: Command to check the GLIBC version on a Linux system. GPUStack worker installation requires GLIBC version 2.29 or higher. If the version is lower, Docker installation is recommended as an alternative.

LANGUAGE: bash
CODE:
ldd --version

----------------------------------------

TITLE: Manual Uninstallation Steps for GPUStack
DESCRIPTION: Series of commands for manually uninstalling GPUStack by stopping the service, removing system files, uninstalling the CLI, and cleaning up data directory. These commands are for Linux systems and may need modification based on specific setup.

LANGUAGE: bash
CODE:
# Stop and remove the service.
systemctl stop gpustack.service
rm /etc/systemd/system/gpustack.service
systemctl daemon-reload
# Uninstall the CLI.
pip uninstall gpustack
# Remove the data directory.
rm -rf /var/lib/gpustack

----------------------------------------

TITLE: Manual Uninstallation Steps for GPUStack
DESCRIPTION: Series of commands for manually uninstalling GPUStack by stopping the service, removing system files, uninstalling the CLI, and cleaning up data directory. These commands are for Linux systems and may need modification based on specific setup.

LANGUAGE: bash
CODE:
# Stop and remove the service.
systemctl stop gpustack.service
rm /etc/systemd/system/gpustack.service
systemctl daemon-reload
# Uninstall the CLI.
pip uninstall gpustack
# Remove the data directory.
rm -rf /var/lib/gpustack

----------------------------------------

TITLE: Installing ROCm on Ubuntu for AMD GPU Support
DESCRIPTION: This snippet shows the steps to install ROCm on Ubuntu, which is necessary for AMD GPU support in GPUStack. It includes updating packages, downloading and installing AMD GPU drivers, and setting up user permissions.

LANGUAGE: bash
CODE:
sudo apt update
wget https://repo.radeon.com/amdgpu-install/6.2.4/ubuntu/jammy/amdgpu-install_6.2.60204-1_all.deb
sudo apt install ./amdgpu-install_6.2.60204-1_all.deb

amdgpu-install -y --usecase=graphics,rocm
sudo reboot

LANGUAGE: bash
CODE:
sudo usermod -a -G render,video $LOGNAME
sudo reboot

LANGUAGE: bash
CODE:
groups

dkms status

rocminfo

rocm-smi -i --showmeminfo vram --showpower --showserial --showuse --showtemp --showproductname

----------------------------------------

TITLE: Installing ROCm on Ubuntu for AMD GPU Support
DESCRIPTION: This snippet shows the steps to install ROCm on Ubuntu, which is necessary for AMD GPU support in GPUStack. It includes updating packages, downloading and installing AMD GPU drivers, and setting up user permissions.

LANGUAGE: bash
CODE:
sudo apt update
wget https://repo.radeon.com/amdgpu-install/6.2.4/ubuntu/jammy/amdgpu-install_6.2.60204-1_all.deb
sudo apt install ./amdgpu-install_6.2.60204-1_all.deb

amdgpu-install -y --usecase=graphics,rocm
sudo reboot

LANGUAGE: bash
CODE:
sudo usermod -a -G render,video $LOGNAME
sudo reboot

LANGUAGE: bash
CODE:
groups

dkms status

rocminfo

rocm-smi -i --showmeminfo vram --showpower --showserial --showuse --showtemp --showproductname

----------------------------------------

TITLE: Viewing GPUStack Logs - Linux/macOS
DESCRIPTION: Displays the path to access GPUStack log files on Linux or macOS systems. Logs are stored in the system's /var/log directory.

LANGUAGE: bash
CODE:
/var/log/gpustack.log

----------------------------------------

TITLE: Adding Worker Nodes on Windows
DESCRIPTION: PowerShell command to install GPUStack on worker nodes and connect them to the server using authentication token on Windows systems.

LANGUAGE: bash
CODE:
Invoke-Expression "& { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } --server-url http://myserver --token mytoken"

----------------------------------------

TITLE: Adding Worker Nodes on Windows
DESCRIPTION: PowerShell command to install GPUStack on worker nodes and connect them to the server using authentication token on Windows systems.

LANGUAGE: bash
CODE:
Invoke-Expression "& { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } --server-url http://myserver --token mytoken"

----------------------------------------

TITLE: Advanced Image Edit Request in multipart/form-data
DESCRIPTION: Example multipart/form-data request for image edit API with advanced options like sampler, schedule, and cfg_scale.

LANGUAGE: plaintext
CODE:
n=1
response_format=b64_json
size=512x512
prompt="A lovely cat"
image=...
mask=...
sampler=euler
schedule=default
seed=null
cfg_scale=4.5
sample_steps=20
negative_prompt=""
stream=true
stream_options_include_usage=true

----------------------------------------

TITLE: Advanced Image Edit Request in multipart/form-data
DESCRIPTION: Example multipart/form-data request for image edit API with advanced options like sampler, schedule, and cfg_scale.

LANGUAGE: plaintext
CODE:
n=1
response_format=b64_json
size=512x512
prompt="A lovely cat"
image=...
mask=...
sampler=euler
schedule=default
seed=null
cfg_scale=4.5
sample_steps=20
negative_prompt=""
stream=true
stream_options_include_usage=true

----------------------------------------

TITLE: Viewing GPUStack Logs - Windows
DESCRIPTION: Displays the path to access GPUStack log files on Windows systems. Logs are stored in the user's AppData directory.

LANGUAGE: powershell
CODE:
"$env:APPDATA\gpustack\log\gpustack.log"

----------------------------------------

TITLE: Setting up Self-hosted Ollama Model Registry with Docker
DESCRIPTION: Commands to run a self-hosted OCI registry, push a model using Ollama, and start GPUStack server with a custom Ollama library URL. This enables deploying models from a private registry.

LANGUAGE: bash
CODE:
# Run a self-hosted OCI registry
docker run -d -p 5001:5000 --name registry registry:2

# Push a model to the registry using Ollama
ollama pull llama3
ollama cp llama3 localhost:5001/library/llama3
ollama push localhost:5001/library/llama3 --insecure

# Start GPUStack server with the custom Ollama library URL
curl -sfL https://get.gpustack.ai | sh -s - --ollama-library-base-url http://localhost:5001

----------------------------------------

TITLE: Warning About vLLM Distributed Inference Limitations
DESCRIPTION: Warning block describing known limitations when using vLLM for distributed inference across multiple workers

LANGUAGE: markdown
CODE:
!!! warning "Known Limitations"

    1. Both the GPUStack server and all participating workers must run on Linux.
    2. Model files must be accessible at the same path on all participating workers. Currently, GPUStack downloads model files only to the main worker. You must either use a shared file system or manually copy the model files to the same path on all participating workers.
    3. Each worker can only be assigned to one distributed vLLM model instance at a time.

----------------------------------------

TITLE: Retrieving GPUStack Admin Password
DESCRIPTION: Command to get the default admin password from the GPUStack installation

LANGUAGE: bash
CODE:
cat /var/lib/gpustack/initial_admin_password

----------------------------------------

TITLE: Retrieving admin password on Linux or macOS
DESCRIPTION: Command to retrieve the default admin password for GPUStack on Linux or macOS systems.

LANGUAGE: bash
CODE:
cat /var/lib/gpustack/initial_admin_password

----------------------------------------

TITLE: Running GPUStack Start Command in Bash
DESCRIPTION: Shows the basic syntax for running the 'gpustack start' command with options. This command is used to start the GPUStack server or worker with various configuration options.

LANGUAGE: bash
CODE:
gpustack start [OPTIONS]

----------------------------------------

TITLE: Running Tests for gpustack Project
DESCRIPTION: This command runs the test suite for the gpustack project using the make utility.

LANGUAGE: bash
CODE:
make test

----------------------------------------

TITLE: Using GPUStack API with Python
DESCRIPTION: Example of using the official OpenAI Python API library to consume GPUStack's OpenAI-compatible API. This code creates a chat completion using the llama3.2 model.

LANGUAGE: python
CODE:
from openai import OpenAI
client = OpenAI(base_url="http://myserver/v1-openai", api_key="myapikey")

completion = client.chat.completions.create(
  model="llama3.2",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
  ]
)

print(completion.choices[0].message)

----------------------------------------

TITLE: Configuring Llama3.2 Model Set for Air-Gapped Environments in YAML
DESCRIPTION: This YAML snippet demonstrates how to customize the Model Catalog for Llama3.2 in air-gapped environments. It uses local paths instead of online sources for model files, supporting both GGUF and directory-based models.

LANGUAGE: yaml
CODE:
- name: Llama3.2
  description: The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.
  home: https://www.llama.com/
  icon: /static/catalog_icons/meta.png
  categories:
    - llm
  capabilities:
    - context/128k
    - tools
  sizes:
    - 1
    - 3
  licenses:
    - llama3.2
  release_date: "2024-09-25"
  order: 2
  templates:
    - quantizations:
        - Q3_K_L
        - Q4_K_M
        - Q5_K_M
        - Q6_K_L
        - Q8_0
        - f16
      source: local_path
      # assuming you have all the GGUF model files in /path/to/the/model/directory
      local_path: /path/to/the/model/directory/Llama-3.2-{size}B-Instruct-{quantization}.gguf
      replicas: 1
      backend: llama-box
      cpu_offloading: true
      distributed_inference_across_workers: true
    - quantizations: ["BF16"]
      source: local_path
      # assuming you have both /path/to/Llama-3.2-1B-Instruct and /path/to/Llama-3.2-3B-Instruct directories
      local_path: /path/to/Llama-3.2-{size}B-Instruct
      replicas: 1
      backend: vllm
      backend_parameters:
        - --enable-auto-tool-choice
        - --tool-call-parser=llama3_json
        - --chat-template={data_dir}/chat_templates/tool_chat_template_llama3.2_json.jinja

----------------------------------------

TITLE: Downloading Additional macOS Dependencies (Bash)
DESCRIPTION: Commands for downloading additional dependencies required for deploying the speech-to-text CosyVoice model on macOS. This includes running a custom script and downloading specific packages.

LANGUAGE: bash
CODE:
./save_macos_dependencies.sh

export AUDIO_DEPENDENCY_PACKAGE_SPEC="wetextprocessing==1.0.4.1"
export CPLUS_INCLUDE_PATH=$(brew --prefix openfst@1.8.3)/include
export LIBRARY_PATH=$(brew --prefix openfst@1.8.3)/lib

pip wheel $AUDIO_DEPENDENCY_PACKAGE_SPEC -w gpustack_audio_dependency_offline_packages
mv gpustack_audio_dependency_offline_packages/* gpustack_offline_packages/ && rm -rf gpustack_audio_dependency_offline_packages

----------------------------------------

TITLE: Uninstalling GPUStack on Windows using PowerShell
DESCRIPTION: PowerShell command to bypass execution policy and run the uninstallation script on Windows. Assumes default installation path in AppData directory.

LANGUAGE: powershell
CODE:
Set-ExecutionPolicy Bypass -Scope Process -Force; & "$env:APPDATA\gpustack\uninstall.ps1"

----------------------------------------

TITLE: Advanced Image Generation Request in JSON
DESCRIPTION: Example JSON request for image generation API with advanced options like sampler, schedule, and cfg_scale.

LANGUAGE: json
CODE:
{
  "n": 1,
  "response_format": "b64_json",
  "size": "512x512",
  "prompt": "A lovely cat",
  "sampler": "euler",
  "schedule": "default",
  "seed": null,
  "cfg_scale": 4.5,
  "sample_steps": 20,
  "negative_prompt": "",
  "stream": true,
  "stream_options": {
    "include_usage": true
  }
}

----------------------------------------

TITLE: Installing Additional macOS Dependencies in Air-Gapped Environment (Bash)
DESCRIPTION: Commands for installing additional dependencies required for the speech-to-text CosyVoice model on macOS in an air-gapped environment. This includes running a custom script and installing specific packages from local sources.

LANGUAGE: bash
CODE:
./load_macos_dependencies.sh --load-dir ./

pip install --no-index --find-links=gpustack_offline_packages wetextprocessing

----------------------------------------

TITLE: Specifying Text Prompt for Image Editing in GPUStack
DESCRIPTION: This snippet shows the text prompt used to instruct the image model for editing. It describes the desired changes to be made to the selected area of the image.

LANGUAGE: plaintext
CODE:
Pink short hair bang, natural

----------------------------------------

TITLE: Configuring GPUStack as a Systemd Service
DESCRIPTION: Systemd service configuration for running GPUStack as a startup service, including service file creation and commands to enable the service.

LANGUAGE: ini
CODE:
[Unit]
Description=GPUStack Service
Wants=network-online.target
After=network-online.target

[Service]
EnvironmentFile=-/etc/default/%N
ExecStart=gpustack start
Restart=always
RestartSec=3
StandardOutput=append:/var/log/gpustack.log
StandardError=append:/var/log/gpustack.log

[Install]
WantedBy=multi-user.target

LANGUAGE: shell
CODE:
systemctl daemon-reload
systemctl enable gpustack

----------------------------------------

TITLE: Upgrading GPUStack on Windows using PowerShell
DESCRIPTION: PowerShell commands to upgrade GPUStack to the latest or specific version on Windows systems. Requires setting environment variables and existing installation arguments.

LANGUAGE: powershell
CODE:
$env:<EXISTING_INSTALL_ENV> = <EXISTING_INSTALL_ENV_VALUE>
Invoke-Expression (Invoke-WebRequest -Uri "https://get.gpustack.ai" -UseBasicParsing).Content

LANGUAGE: powershell
CODE:
$env:INSTALL_PACKAGE_SPEC = gpustack==x.y.z
$env:<EXISTING_INSTALL_ENV> = <EXISTING_INSTALL_ENV_VALUE>
Invoke-Expression "& { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } <EXISTING_GPUSTACK_ARGS>"

----------------------------------------

TITLE: Upgrading GPUStack on Linux/macOS using Installation Script
DESCRIPTION: Commands to upgrade GPUStack to the latest or specific version on Linux and macOS systems using the installation script. Requires existing installation environment variables and arguments.

LANGUAGE: bash
CODE:
curl -sfL https://get.gpustack.ai | <EXISTING_INSTALL_ENV> sh -s - <EXISTING_GPUSTACK_ARGS>

LANGUAGE: bash
CODE:
curl -sfL https://get.gpustack.ai | INSTALL_PACKAGE_SPEC=gpustack==x.y.z <EXISTING_INSTALL_ENV> sh -s - <EXISTING_GPUSTACK_ARGS>

----------------------------------------

TITLE: Verifying GPU Utilization with mthreads-gmi
DESCRIPTION: Command to check if the model is offloaded to the Moore Threads GPU and view GPU utilization details using the mthreads-gmi tool.

LANGUAGE: bash
CODE:
root@a414c45864ee:/# mthreads-gmi

----------------------------------------

TITLE: Displaying User Role Type
DESCRIPTION: Inline code showing the User role type designation

LANGUAGE: markdown
CODE:
User

----------------------------------------

TITLE: Sample Text Generation Prompt for DeepSeek R1
DESCRIPTION: An example prompt demonstrating number sequence completion using DeepSeek R1 model in GPUStack's playground.

LANGUAGE: plaintext
CODE:
2, 4, 6, 8, > What is the next number?

----------------------------------------

TITLE: Displaying Admin Role Type
DESCRIPTION: Inline code showing the Admin role type designation

LANGUAGE: markdown
CODE:
Admin

----------------------------------------

TITLE: GPUStack YAML Configuration File Example
DESCRIPTION: Provides a complete example of a YAML configuration file for starting GPUStack server or worker. This file includes all possible configuration options for common, server, and worker settings.

LANGUAGE: yaml
CODE:
# Common Options
debug: false
data_dir: /path/to/data_dir
cache_dir: /path/to/cache_dir
token: mytoken
ollama_library_base_url: https://registry.mycompany.com
enable_ray: false
ray_args: ["--port=6379", "--verbose"]

# Server Options
host: 0.0.0.0
port: 80
disable_worker: false
database_url: postgresql://user:password@hostname:port/db_name
ssl_keyfile: /path/to/keyfile
ssl_certfile: /path/to/certfile
force_auth_localhost: false
bootstrap_password: myadminpassword
disable_update_check: false
model_catalog_file: /path_or_url/to/model_catalog_file
ray_port: 40096
ray_client_server_port: 40097

# Worker Options
server_url: http://myserver
worker_name: myworker
worker_ip: 192.168.1.101
disable_metrics: false
disable_rpc_servers: false
metrics_port: 10151
worker_port: 10150
service_port_range: 40000-40063
rpc_server_port_range: 40064-40095
ray_node_manager_port: 40098
ray_object_manager_port: 40099
ray_worker_port_range: 40100-40131
log_dir: /path/to/log_dir
rpc_server_args: ["--verbose"]
system_reserved:
  ram: 2
  vram: 1
tools_download_base_url: https://mirror.mycompany.com

----------------------------------------

TITLE: Installing GPUStack on Windows using PowerShell
DESCRIPTION: This PowerShell command installs GPUStack on Windows. It should be run in an administrator PowerShell, avoiding PowerShell ISE.

LANGUAGE: powershell
CODE:
Invoke-Expression (Invoke-WebRequest -Uri "https://get.gpustack.ai" -UseBasicParsing).Content

----------------------------------------

TITLE: Verifying Hygon DCU Installation
DESCRIPTION: Commands to verify the successful installation of Hygon DCU drivers and tools by checking device information, memory, power, temperature and other metrics.

LANGUAGE: bash
CODE:
# Verify the rocminfo.
# Expected result: Device Type: DCU
rocminfo | grep DCU

# Check if the GPU is listed as an agent.
rocminfo

# Check rocm-smi.
rocm-smi -i --showmeminfo vram --showpower --showserial --showuse --showtemp --showproductname

# Check hy-smi
hy-smi

----------------------------------------

TITLE: Retrieving admin password on Windows
DESCRIPTION: PowerShell command to retrieve the default admin password for GPUStack on Windows systems.

LANGUAGE: powershell
CODE:
Get-Content -Path "$env:APPDATA\gpustack\initial_admin_password" -Raw

----------------------------------------

TITLE: Basic GPUStack Chat Command
DESCRIPTION: The basic command structure for using gpustack chat with a specified model and optional prompt.

LANGUAGE: bash
CODE:
gpustack chat model [prompt]

----------------------------------------

TITLE: Interactive Chat Example
DESCRIPTION: Example of starting an interactive chat session with the llama3 model.

LANGUAGE: bash
CODE:
gpustack chat llama3

----------------------------------------

TITLE: Verifying Container Runtime Configuration for Moore Threads GPUs
DESCRIPTION: Commands to set up the Docker runtime for Moore Threads GPUs and verify the configuration. This ensures that the default runtime is set to 'mthreads'.

LANGUAGE: bash
CODE:
$ (cd /usr/bin/musa && sudo ./docker setup $PWD)
$ docker info | grep mthreads

----------------------------------------

TITLE: Installing GPUStack with Hugging Face Integration
DESCRIPTION: Command to install GPUStack using curl with a Hugging Face API token for model access

LANGUAGE: bash
CODE:
curl -sfL https://get.gpustack.ai | sh -s - --huggingface-token <Hugging Face API Key>

----------------------------------------

TITLE: Running GPUStack Docker Container for AMD GPUs
DESCRIPTION: This snippet demonstrates how to run the GPUStack Docker container with the necessary configurations for AMD GPU support. It includes setting up network, IPC, device access, and other required parameters.

LANGUAGE: bash
CODE:
docker run -itd \
   --network=host \
   --ipc=host \
   --group-add=video \
   --cap-add=SYS_PTRACE \
   --security-opt seccomp=unconfined \
   --device /dev/kfd \
   --device /dev/dri \
   gpustack/gpustack:v0.5.0-rocm

----------------------------------------

TITLE: Installing Ascend NPU Driver
DESCRIPTION: Commands for installing the Ascend NPU driver package with full installation for all users.

LANGUAGE: bash
CODE:
sudo chmod +x Ascend-hdk-xxx-npu-driver_x.x.x_linux-{arch}.run
sudo sh Ascend-hdk-xxx-npu-driver_x.x.x_linux-{arch}.run --full --install-for-all

----------------------------------------

TITLE: Building gpustack Project
DESCRIPTION: This command builds the gpustack project using the make utility. The resulting artifacts can be found in the 'dist' directory.

LANGUAGE: bash
CODE:
make build

----------------------------------------

TITLE: Code Block Note About Multimodal Projector Files
DESCRIPTION: Note block explaining how GPUStack handles multimodal projector files when deploying vision language models

LANGUAGE: markdown
CODE:
!!! Note

    When deploying a vision language model, GPUStack downloads and uses the multimodal projector file with the pattern `*mmproj*.gguf` by default. If multiple files match the pattern, GPUStack selects the file with higher precision (e.g., `f32` over `f16`). If the default pattern does not match the projector file or you want to use a specific one, you can customize the multimodal projector file by setting the `--mmproj` parameter in the model configuration. You can specify the relative path to the projector file in the model source. This syntax acts as shorthand, and GPUStack will download the file from the source and normalize the path when using it.

----------------------------------------

TITLE: Editing Image with curl
DESCRIPTION: Bash command using curl to edit an image with GPUStack's image edit API.

LANGUAGE: bash
CODE:
export GPUSTACK_API_KEY=myapikey
curl http://myserver/v1-openai/image/edit \
    -H "Authorization: Bearer $GPUSTACK_API_KEY" \
    -F image="@otter.png" \
    -F mask="@mask.png" \
    -F prompt="A lovely cat" \
    -F n=1 \
    -F size="512x512"

----------------------------------------

TITLE: Configuring Docker Daemon for NVIDIA GPUs
DESCRIPTION: JSON configuration for Docker daemon to handle NVIDIA GPU workloads properly by setting the cgroup driver to cgroupfs.

LANGUAGE: json
CODE:
{
  "runtimes": {
    "nvidia": {
      "args": [],
      "path": "nvidia-container-runtime"
    }
  },
  "exec-opts": ["native.cgroupdriver=cgroupfs"]
}

----------------------------------------

TITLE: Resetting Admin Password
DESCRIPTION: Command to reset the GPUStack admin password when executed on the server node. Used when the current admin password has been forgotten.

LANGUAGE: bash
CODE:
gpustack reset-admin-password

----------------------------------------

TITLE: HSA System Configuration Output
DESCRIPTION: Formatted system output showing detailed hardware specifications including runtime versions, system attributes, and agent-specific information for both CPU and GPU components

LANGUAGE: text
CODE:
ROCk module version 6.2.4 is loaded
=====================
HSA System Attributes
=====================
Runtime Version:         1.13
Runtime Ext Version:     1.4
System Timestamp Freq.:  1000.000000MHz
Sig. Max Wait Duration:  18446744073709551615 (0xFFFFFFFFFFFFFFFF) (timestamp count)
Machine Model:           LARGE
System Endianness:       LITTLE
Mwaitx:                  DISABLED
DMAbuf Support:          YES

----------------------------------------

TITLE: Generating Image with Stable-Diffusion-v3-5-Large
DESCRIPTION: Parameters for creating pop art style image using Stable Diffusion v3.5 Large with specific attention to artistic details and text integration.

LANGUAGE: text
CODE:
Prompt: Lucky flower pop art style with pink color scheme,happy cute girl character wearing oversized headphones and smiling while listening to music in the air with her eyes closed,vibrant colorful Japanese anime cartoon illustration with bold outlines and bright colors,colorful text "GPUStack" on top of background,high resolution,detailed,
Size: 1024x1024
Sampler: dpm++2m
Scheduler: discrete
Steps: 25
CFG: 5
Seed: 3520225659

----------------------------------------

TITLE: Reranking API Call with cURL in Bash
DESCRIPTION: Example of how to make a reranking API request to GPUStack using cURL. The script demonstrates sending a query and list of documents for reranking, with environment variables for server URL and API key authentication.

LANGUAGE: bash
CODE:
export SERVER_URL=<your-server-url>
export GPUSTACK_API_KEY=<your-api-key>
curl $SERVER_URL/v1/rerank \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $GPUSTACK_API_KEY" \
    -d '{
        "model": "bge-reranker-v2-m3",
        "query": "What is a panda?",
        "top_n": 3,
        "documents": [
            "hi",
            "it is a bear",
            "The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China."
        ]
    }' | jq

----------------------------------------

TITLE: Installing Dependencies for Ascend Toolkit
DESCRIPTION: Command to install required system packages for the Ascend toolkit on Ubuntu.

LANGUAGE: bash
CODE:
sudo apt-get install -y gcc g++ make cmake zlib1g zlib1g-dev openssl libsqlite3-dev libssl-dev libffi-dev libbz2-dev libxslt1-dev unzip pciutils net-tools libblas-dev gfortran libblas3

----------------------------------------

TITLE: Example JSON Response from GPUStack Embeddings API
DESCRIPTION: This JSON snippet shows an example response from the GPUStack embeddings API. It includes the generated embedding vector, model information, and token usage statistics.

LANGUAGE: json
CODE:
{
  "data": [
    {
      "embedding": [
        -0.012189436703920364, 0.016934078186750412, 0.003965042531490326,
        -0.03453584015369415, -0.07623119652271271, -0.007116147316992283,
        0.11278388649225235, 0.019714849069714546, 0.010370955802500248,
        -0.04219457507133484, -0.029902394860982895, 0.01122555136680603,
        0.022912170737981796, 0.031186765059828758, 0.006303929258137941,
        # ... additional values
      ],
      "index": 0,
      "object": "embedding"
    }
  ],
  "model": "bge-small-en-v1.5",
  "object": "list",
  "usage": { "prompt_tokens": 12, "total_tokens": 12 }
}