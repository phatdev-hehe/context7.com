TITLE: Implementing Core Evaluator-Optimizer Workflow Functions in Python
DESCRIPTION: Defines core functions for generating, evaluating and looping through LLM responses with feedback. Includes generate() for creating solutions, evaluate() for assessing them, and loop() for managing the iteration process until requirements are met.

LANGUAGE: python
CODE:
from util import llm_call, extract_xml

def generate(prompt: str, task: str, context: str = "") -> tuple[str, str]:
    """Generate and improve a solution based on feedback."""
    full_prompt = f"{prompt}\n{context}\nTask: {task}" if context else f"{prompt}\nTask: {task}"
    response = llm_call(full_prompt)
    thoughts = extract_xml(response, "thoughts")
    result = extract_xml(response, "response")
    
    print("\n=== GENERATION START ===")
    print(f"Thoughts:\n{thoughts}\n")
    print(f"Generated:\n{result}")
    print("=== GENERATION END ===\n")
    
    return thoughts, result

def evaluate(prompt: str, content: str, task: str) -> tuple[str, str]:
    """Evaluate if a solution meets requirements."""
    full_prompt = f"{prompt}\nOriginal task: {task}\nContent to evaluate: {content}"
    response = llm_call(full_prompt)
    evaluation = extract_xml(response, "evaluation")
    feedback = extract_xml(response, "feedback")
    
    print("=== EVALUATION START ===")
    print(f"Status: {evaluation}")
    print(f"Feedback: {feedback}")
    print("=== EVALUATION END ===\n")
    
    return evaluation, feedback

def loop(task: str, evaluator_prompt: str, generator_prompt: str) -> tuple[str, list[dict]]:
    """Keep generating and evaluating until requirements are met."""
    memory = []
    chain_of_thought = []
    
    thoughts, result = generate(generator_prompt, task)
    memory.append(result)
    chain_of_thought.append({"thoughts": thoughts, "result": result})
    
    while True:
        evaluation, feedback = evaluate(evaluator_prompt, result, task)
        if evaluation == "PASS":
            return result, chain_of_thought
            
        context = "\n".join([
            "Previous attempts:",
            *[f"- {m}" for m in memory],
            f"\nFeedback: {feedback}"
        ])
        
        thoughts, result = generate(generator_prompt, task, context)
        memory.append(result)
        chain_of_thought.append({"thoughts": thoughts, "result": result})

----------------------------------------

TITLE: Implementing Multi-LLM Workflow Functions in Python
DESCRIPTION: Defines three functions for multi-LLM workflows: chain for sequential processing, parallel for concurrent processing, and route for dynamic path selection based on input classification.

LANGUAGE: python
CODE:
def chain(input: str, prompts: List[str]) -> str:
    """Chain multiple LLM calls sequentially, passing results between steps."""
    result = input
    for i, prompt in enumerate(prompts, 1):
        print(f"\nStep {i}:")
        result = llm_call(f"{prompt}\nInput: {result}")
        print(result)
    return result

def parallel(prompt: str, inputs: List[str], n_workers: int = 3) -> List[str]:
    """Process multiple inputs concurrently with the same prompt."""
    with ThreadPoolExecutor(max_workers=n_workers) as executor:
        futures = [executor.submit(llm_call, f"{prompt}\nInput: {x}") for x in inputs]
        return [f.result() for f in futures]

def route(input: str, routes: Dict[str, str]) -> str:
    """Route input to specialized prompt using content classification."""
    # First determine appropriate route using LLM with chain-of-thought
    print(f"\nAvailable routes: {list(routes.keys())}")
    selector_prompt = f"""
    Analyze the input and select the most appropriate support team from these options: {list(routes.keys())}
    First explain your reasoning, then provide your selection in this XML format:

    <reasoning>
    Brief explanation of why this ticket should be routed to a specific team.
    Consider key terms, user intent, and urgency level.
    </reasoning>

    <selection>
    The chosen team name
    </selection>

    Input: {input}""".strip()
    
    route_response = llm_call(selector_prompt)
    reasoning = extract_xml(route_response, 'reasoning')
    route_key = extract_xml(route_response, 'selection').strip().lower()
    
    print("Routing Analysis:")
    print(reasoning)
    print(f"\nSelected route: {route_key}")
    
    # Process input with selected specialized prompt
    selected_prompt = routes[route_key]
    return llm_call(f"{selected_prompt}\nInput: {input}")

----------------------------------------

TITLE: Handling User Queries with Claude 3
DESCRIPTION: Implements a function to process user queries using vector search and Claude 3 model, combining retrieved context with the query to generate responses.

LANGUAGE: python
CODE:
import anthropic
client = anthropic.Client(api_key=userdata.get("ANTHROPIC_API_KEY"))

def handle_user_query(query, collection):

  get_knowledge = vector_search(query, collection)

  search_result = ''
  for result in get_knowledge:
    search_result += (
        f"Title: {result.get('title', 'N/A')}, "
        f"Company Name: {result.get('companyName', 'N/A')}, "
        f"Company URL: {result.get('companyUrl', 'N/A')}, "
        f"Date Published: {result.get('published_at', 'N/A')}, "
        f"Article URL: {result.get('url', 'N/A')}, "
        f"Description: {result.get('description', 'N/A')}, \n"
    )

  response = client.messages.create(
    model="claude-3-opus-20240229",
    max_tokens=1024,
    system="You are Venture Captital Tech Analyst with access to some tech company articles and information. You use the information you are given to provide advice.",
    messages=[
        {"role": "user", "content": "Answer this user query: " + query + " with the following context: " + search_result}
    ]
  )

  return (response.content[0].text), search_result

----------------------------------------

TITLE: Implementing FlexibleOrchestrator Class in Python
DESCRIPTION: Defines a FlexibleOrchestrator class that manages task breakdown and execution using worker LLMs. It includes methods for parsing XML tasks, formatting prompts, and processing complex tasks by delegating subtasks to workers.

LANGUAGE: python
CODE:
from typing import Dict, List, Optional
from util import llm_call, extract_xml

def parse_tasks(tasks_xml: str) -> List[Dict]:
    """Parse XML tasks into a list of task dictionaries."""
    tasks = []
    current_task = {}
    
    for line in tasks_xml.split('\n'):
        line = line.strip()
        if not line:
            continue
            
        if line.startswith("<task>"):
            current_task = {}
        elif line.startswith("<type>"):
            current_task["type"] = line[6:-7].strip()
        elif line.startswith("<description>"):
            current_task["description"] = line[12:-13].strip()
        elif line.startswith("</task>"):
            if "description" in current_task:
                if "type" not in current_task:
                    current_task["type"] = "default"
                tasks.append(current_task)
    
    return tasks

class FlexibleOrchestrator:
    """Break down tasks and run them in parallel using worker LLMs."""
    
    def __init__(
        self,
        orchestrator_prompt: str,
        worker_prompt: str,
    ):
        """Initialize with prompt templates."""
        self.orchestrator_prompt = orchestrator_prompt
        self.worker_prompt = worker_prompt

    def _format_prompt(self, template: str, **kwargs) -> str:
        """Format a prompt template with variables."""
        try:
            return template.format(**kwargs)
        except KeyError as e:
            raise ValueError(f"Missing required prompt variable: {e}")

    def process(self, task: str, context: Optional[Dict] = None) -> Dict:
        """Process task by breaking it down and running subtasks in parallel."""
        context = context or {}
        
        # Step 1: Get orchestrator response
        orchestrator_input = self._format_prompt(
            self.orchestrator_prompt,
            task=task,
            **context
        )
        orchestrator_response = llm_call(orchestrator_input)
        
        # Parse orchestrator response
        analysis = extract_xml(orchestrator_response, "analysis")
        tasks_xml = extract_xml(orchestrator_response, "tasks")
        tasks = parse_tasks(tasks_xml)
        
        print("\n=== ORCHESTRATOR OUTPUT ===")
        print(f"\nANALYSIS:\n{analysis}")
        print(f"\nTASKS:\n{tasks}")
        
        # Step 2: Process each task
        worker_results = []
        for task_info in tasks:
            worker_input = self._format_prompt(
                self.worker_prompt,
                original_task=task,
                task_type=task_info['type'],
                task_description=task_info['description'],
                **context
            )
            
            worker_response = llm_call(worker_input)
            result = extract_xml(worker_response, "response")
            
            worker_results.append({
                "type": task_info["type"],
                "description": task_info["description"],
                "result": result
            })
            
            print(f"\n=== WORKER RESULT ({task_info['type']}) ===\n{result}\n")
        
        return {
            "analysis": analysis,
            "worker_results": worker_results,
        }

----------------------------------------

TITLE: Legal Summary Indexed Documents Class
DESCRIPTION: Defines a class for implementing Summary Indexed Documents, an advanced RAG approach. It includes methods for adding documents, generating summaries, ranking documents, and extracting relevant clauses.

LANGUAGE: Python
CODE:
class LegalSummaryIndexedDocuments:

    def __init__(self, client):
        self.client = client # Claude client
        self.documents: List[Dict[str, str]] = [] # List of docs to store
        self.summaries: List[str] = []

    def add_document(self, doc_id: str, content: str):
        # Adds a document to the index
        self.documents.append({"id": doc_id, "content": content})

    def generate_summaries(self):
        # Generates summaries for all documents in the index
        for doc in self.documents:
            summary = self._generate_legal_summary(doc["content"])
            self.summaries.append(summary)

    def _generate_legal_summary(self, content: str) -> str:

        # Note how we constrain the content to a maximum of 2000 words. We do this because we don't need that much information for the intial ranking.
        prompt = f"""
        Summarize the following sublease agreement. Focus on these key aspects:

        1. Parties involved (sublessor, sublessee, original lessor)
        2. Property details (address, description, permitted use)
        3. Term and rent (start date, end date, monthly rent, security deposit)
        4. Responsibilities (utilities, maintenance, repairs)
        5. Consent and notices (landlord's consent, notice requirements)
        6. Special provisions (furniture, parking, subletting restrictions)

        Provide the summary in bullet points nested within the XML header for each section. For example:

        <parties involved>
        - Sublessor: [Name]
        // Add more details as needed
        </parties involved>
        
        If any information is not explicitly stated in the document, note it as "Not specified".

        Sublease agreement text:
        {content[:2000]}...

        Summary:
        """

        response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=500,
            temperature=0.2,
            messages=[
                {"role": "user", "content": prompt},
                {"role": "assistant", "content": "Here is the summary of the legal document: <summary>"}
            ],
            stop_sequences=["</summary>"]        
        )
        return response.content[0].text

    def rank_documents(self, query: str, top_k: int = 3) -> List[Tuple[str, float]]:
        """
        Rank documents based on their relevance to the given query.
        We use Haiku here as a cheaper, faster model for ranking. 
        """
        ranked_scores = []
        for summary in self.summaries:

            prompt=f"Legal document summary: {summary}\n\nLegal query: {query}\n\nRate the relevance of this legal document to the query on a scale of 0 to 10. Only output the numeric value:"

            response = client.messages.create(
                model="claude-3-haiku-20240307",
                max_tokens=2,
                temperature=0,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            ranked_score = float(response.content[0].text)
            ranked_scores.append(ranked_score)

        ranked_indices = np.argsort(ranked_scores)[::-1][:top_k]
        return [(self.documents[i]["id"], ranked_scores[i]) for i in ranked_indices]

    def extract_relevant_clauses(self, doc_id: str, query: str) -> List[str]:
        """
        Extracts relevant clauses from a document based on a query.
        """
        doc_content = next(doc["content"] for doc in self.documents if doc["id"] == doc_id)
        
        prompt = f"""
        Given the following legal query and document content, extract the most relevant clauses or sections and write the answer to the query. 
        Provide each relevant clause or section separately, preserving the original legal language:

        Legal query: {query}

        Document content: {doc_content}...

        Relevant clauses or sections (separated by '---'):"""

        response = client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1000,
                temperature=0,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
        
        clauses = re.split(r'\n\s*---\s*\n', response.content[0].text.strip())
        return [clause.strip() for clause in clauses if clause.strip()]

----------------------------------------

TITLE: Article Summarization with Claude Tool Use
DESCRIPTION: This code demonstrates how to use Claude with a custom tool to generate a JSON summary of an article, including fields for author, topics, summary, coherence score, and persuasion score.

LANGUAGE: python
CODE:
tools = [
    {
        "name": "print_summary",
        "description": "Prints a summary of the article.",
        "input_schema": {
            "type": "object",
            "properties": {
                "author": {"type": "string", "description": "Name of the article author"},
                "topics": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": 'Array of topics, e.g. ["tech", "politics"]. Should be as specific as possible, and can overlap.'
                },
                "summary": {"type": "string", "description": "Summary of the article. One or two paragraphs max."},
                "coherence": {"type": "integer", "description": "Coherence of the article's key points, 0-100 (inclusive)"},
                "persuasion": {"type": "number", "description": "Article's persuasion score, 0.0-1.0 (inclusive)"}
            },
            "required": ['author', 'topics', 'summary', 'coherence', 'persuasion', 'counterpoint']
        }
    }
]

url = "https://www.anthropic.com/news/third-party-testing"
response = requests.get(url)
soup = BeautifulSoup(response.text, "html.parser")
article = " ".join([p.text for p in soup.find_all("p")])

query = f"""
<article>
{article}
</article>

Use the `print_summary` tool.
"""

response = client.messages.create(
    model=MODEL_NAME,
    max_tokens=4096,
    tools=tools,
    messages=[{"role": "user", "content": query}]
)
json_summary = None
for content in response.content:
    if content.type == "tool_use" and content.name == "print_summary":
        json_summary = content.input
        break

if json_summary:
    print("JSON Summary:")
    print(json.dumps(json_summary, indent=2))
else:
    print("No JSON summary found in the response.")

----------------------------------------

TITLE: Implementing Chain of Thought Prompting for Content Moderation
DESCRIPTION: This code demonstrates how to use chain-of-thought prompting to improve Claude's content moderation capabilities by encouraging step-by-step reasoning.

LANGUAGE: python
CODE:
cot_prompt = '''You are a content moderation expert tasked with categorizing user-generated text based on the following guidelines:

BLOCK CATEGORY:
- Content that is not related to rollercoasters, theme parks, or the amusement industry
- Explicit violence, hate speech, or illegal activities
- Spam, advertisements, or self-promotion

ALLOW CATEGORY:
- Discussions about rollercoaster designs, ride experiences, and park reviews
- Sharing news, rumors, or updates about new rollercoaster projects
- Respectful debates about the best rollercoasters, parks, or ride manufacturers
- Some mild profanity or crude language, as long as it is not directed at individuals

First, inside of <thinking> tags, identify any potentially concerning aspects of the post based on the guidelines below and consider whether those aspects are serious enough to block the post or not. Finally, classify this text as either ALLOW or BLOCK inside <output> tags. Return nothing else.

Given those instructions, here is the post to categorize:

<user_post>{user_post}</user_post>'''

user_post = "Introducing my new band - Coaster Shredders. Check us out on YouTube!!"

response = client.messages.create(
        model=MODEL_NAME,
        max_tokens=1000,
        messages=[{"role": "user", "content": cot_prompt.format(user_post=user_post)}]
    ).content[0].text

print(response)

----------------------------------------

TITLE: Implementing Chatbot Interaction with Claude 3
DESCRIPTION: Defines a function to handle user messages, process tool calls, and return final responses from the Claude 3 model.

LANGUAGE: python
CODE:
import json

def chatbot_interaction(user_message):
    print(f"\n{'='*50}\nUser Message: {user_message}\n{'='*50}")

    messages = [
        {"role": "user", "content": user_message}
    ]

    response = client.messages.create(
        model=MODEL_NAME,
        max_tokens=4096,
        tools=tools,
        messages=messages
    )

    print(f"\nInitial Response:")
    print(f"Stop Reason: {response.stop_reason}")
    print(f"Content: {response.content}")

    while response.stop_reason == "tool_use":
        tool_use = next(block for block in response.content if block.type == "tool_use")
        tool_name = tool_use.name
        tool_input = tool_use.input

        print(f"\nTool Used: {tool_name}")
        print(f"Tool Input:")
        print(json.dumps(tool_input, indent=2))

        tool_result = process_tool_call(tool_name, tool_input)

        print(f"\nTool Result:")
        print(json.dumps(tool_result, indent=2))

        messages = [
            {"role": "user", "content": user_message},
            {"role": "assistant", "content": response.content},
            {
                "role": "user",
                "content": [
                    {
                        "type": "tool_result",
                        "tool_use_id": tool_use.id,
                        "content": str(tool_result),
                    }
                ],
            },
        ]

        response = client.messages.create(
            model=MODEL_NAME,
            max_tokens=4096,
            tools=tools,
            messages=messages
        )

        print(f"\nResponse:")
        print(f"Stop Reason: {response.stop_reason}")
        print(f"Content: {response.content}")

    final_response = next(
        (block.text for block in response.content if hasattr(block, "text")),
        None,
    )

    print(f"\nFinal Response: {final_response}")

    return final_response

----------------------------------------

TITLE: Implementing Multiple Tool Calls with Extended Thinking
DESCRIPTION: Shows how to handle multiple tool calls using Claude's extended thinking feature. It sets up weather and news tools, processes the initial request, and handles multiple iterations of tool use and response generation.

LANGUAGE: python
CODE:
def multiple_tool_calls_with_thinking():
    # Define tools
    tools = [
        {
            "name": "weather",
            "description": "Get current weather information for a location.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The location to get weather for."
                    }
                },
                "required": ["location"]
            }
        },
        {
            "name": "news",
            "description": "Get latest news headlines for a topic.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "topic": {
                        "type": "string",
                        "description": "The topic to get news about."
                    }
                },
                "required": ["topic"]
            }
        }
    ]
    
    def weather(location):
        # Mock weather data
        weather_data = {
            "New York": {"temperature": 72, "condition": "Sunny"},
            "London": {"temperature": 62, "condition": "Cloudy"},
            "Tokyo": {"temperature": 80, "condition": "Partly cloudy"},
            "Paris": {"temperature": 65, "condition": "Rainy"},
            "Sydney": {"temperature": 85, "condition": "Clear"},
            "Berlin": {"temperature": 60, "condition": "Foggy"},
        }
        
        return weather_data.get(location, {"error": f"No weather data available for {location}"})
    
    def news(topic):
        # Mock news data
        news_data = {
            "technology": [
                "New AI breakthrough announced by research lab",
                "Tech company releases latest smartphone model",
                "Quantum computing reaches milestone achievement"
            ],
            "sports": [
                "Local team wins championship game",
                "Star player signs record-breaking contract",
                "Olympic committee announces host city for 2036"
            ],
            "weather": [
                "Storm system developing in the Atlantic",
                "Record temperatures recorded across Europe",
                "Climate scientists release new research findings"
            ]
        }
        
        return {"headlines": news_data.get(topic.lower(), ["No news available for this topic"])}
    
    # Initial request
    response = client.messages.create(
        model=MODEL_NAME,
        max_tokens=MAX_TOKENS,
        thinking={
                "type": "enabled",
                "budget_tokens": THINKING_BUDGET_TOKENS
        },
        tools=tools,
        messages=[{
            "role": "user",
            "content": "What's the weather in London, and can you also tell me the latest news about technology?"
        }]
    )
    
    # Print detailed information about initial response
    print("\n=== INITIAL RESPONSE ===")
    print(f"Response ID: {response.id}")
    print(f"Stop reason: {response.stop_reason}")
    print(f"Model: {response.model}")
    print(f"Content blocks: {len(response.content)} blocks")
    
    # Print each content block
    for i, block in enumerate(response.content):
        print(f"\nBlock {i+1}: Type = {block.type}")
        if block.type == "thinking":
            print(f"Thinking content: {block.thinking[:150]}...")
            print(f"Signature available: {bool(getattr(block, 'signature', None))}")
        elif block.type == "text":
            print(f"Text content: {block.text}")
        elif block.type == "tool_use":
            print(f"Tool: {block.name}")
            print(f"Tool input: {block.input}")
            print(f"Tool ID: {block.id}")
    print("=== END INITIAL RESPONSE ===\n")
    
    # Handle potentially multiple tool calls
    full_conversation = [{
        "role": "user",
        "content": "What's the weather in London, and can you also tell me the latest news about technology?"
    }]
    
    # Track iteration count for multi-turn tool use
    iteration = 0
    
    while response.stop_reason == "tool_use":
        iteration += 1
        print(f"\n=== TOOL USE ITERATION {iteration} ===")
        
        # Extract thinking blocks and tool use to include in conversation history
        assistant_blocks = []
        for block in response.content:
            if block.type in ["thinking", "redacted_thinking", "tool_use"]:
                assistant_blocks.append(block)
        
        # Add assistant response with thinking blocks and tool use
        full_conversation.append({
            "role": "assistant",
            "content": assistant_blocks
        })
        
        # Find the tool_use block
        tool_use_block = next((block for block in response.content if block.type == "tool_use"), None)
        if tool_use_block:
            print(f"\n=== EXECUTING TOOL ===")
            print(f"Tool name: {tool_use_block.name}")
            
            # Execute the appropriate tool
            if tool_use_block.name == "weather":
                print(f"Location to check: {tool_use_block.input['location']}")
                tool_result = weather(tool_use_block.input["location"])
            elif tool_use_block.name == "news":
                print(f"Topic to check: {tool_use_block.input['topic']}")
                tool_result = news(tool_use_block.input["topic"])
            else:
                tool_result = {"error": "Unknown tool"}
                
            print(f"Result: {tool_result}")
            print("=== TOOL EXECUTION COMPLETE ===\n")
            
            # Add tool result to conversation
            full_conversation.append({
                "role": "user",
                "content": [{
                    "type": "tool_result",
                    "tool_use_id": tool_use_block.id,
                    "content": json.dumps(tool_result)
                }]
            })
            
            # Continue the conversation
            print("\n=== SENDING FOLLOW-UP REQUEST WITH TOOL RESULT ===")
            response = client.messages.create(
                model=MODEL_NAME,
                max_tokens=MAX_TOKENS,
                thinking={
                        "type": "enabled",
                        "budget_tokens": THINKING_BUDGET_TOKENS
                },
                tools=tools,
                messages=full_conversation
            )
            
            # Print follow-up response details
            print(f"\n=== FOLLOW-UP RESPONSE (ITERATION {iteration}) ===")
            print(f"Response ID: {response.id}")
            print(f"Stop reason: {response.stop_reason}")
            print(f"Content blocks: {len(response.content)} blocks")
            
            for i, block in enumerate(response.content):
                print(f"\nBlock {i+1}: Type = {block.type}")
                if block.type == "thinking":
                    print(f"Thinking content preview: {block.thinking[:100]}...")
                elif block.type == "text":
                    print(f"Text content preview: {block.text[:100]}...")
                elif block.type == "tool_use":
                    print(f"Tool: {block.name}")
                    print(f"Tool input preview: {str(block.input)[:100]}")
            print(f"=== END FOLLOW-UP RESPONSE (ITERATION {iteration}) ===\n")
            
            if response.stop_reason != "tool_use":
                print("\n=== FINAL RESPONSE ===")
                print_thinking_response(response)
                print("=== END FINAL RESPONSE ===")
        else:
            print("No tool_use block found in response.")
            break

# Run the example
multiple_tool_calls_with_thinking()

----------------------------------------

TITLE: Text Classification with Claude Tool Use
DESCRIPTION: This code shows how to use Claude with a custom tool to classify a given text into predefined categories and return the classification results in a structured JSON format.

LANGUAGE: python
CODE:
tools = [
    {
        "name": "print_classification",
        "description": "Prints the classification results.",
        "input_schema": {
            "type": "object",
            "properties": {
                "categories": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "name": {"type": "string", "description": "The category name."},
                            "score": {"type": "number", "description": "The classification score for the category, ranging from 0.0 to 1.0."}
                        },
                        "required": ["name", "score"]
                    }
                }
            },
            "required": ["categories"]
        }
    }
]

text = "The new quantum computing breakthrough could revolutionize the tech industry."

query = f"""
<document>
{text}
</document>

Use the print_classification tool. The categories can be Politics, Sports, Technology, Entertainment, Business.
"""

response = client.messages.create(
    model=MODEL_NAME,
    max_tokens=4096,
    tools=tools,
    messages=[{"role": "user", "content": query}]
)

json_classification = None
for content in response.content:
    if content.type == "tool_use" and content.name == "print_classification":
        json_classification = content.input
        break

if json_classification:
    print("Text Classification (JSON):")
    print(json.dumps(json_classification, indent=2))
else:
    print("No text classification found in the response.")

----------------------------------------

TITLE: Implementing Conversational Memory for Agent
DESCRIPTION: Adds conversational memory to the agent using ConversationBufferWindowMemory and implements helper functions for managing chat history.

LANGUAGE: python
CODE:
from langchain.chains.conversation.memory import ConversationBufferWindowMemory

# conversational memory
conversational_memory = ConversationBufferWindowMemory(
    memory_key='chat_history',
    k=5,
    return_messages=True
)

LANGUAGE: python
CODE:
from langchain_core.messages.human import HumanMessage

def memory2str(memory: ConversationBufferWindowMemory):
    messages = memory.chat_memory.messages
    memory_list = [
        f"Human: {mem.content}" if isinstance(mem, HumanMessage) \
        else f"AI: {mem.content}" for mem in messages
    ]
    memory_str = "\n".join(memory_list)
    return memory_str

LANGUAGE: python
CODE:
def chat(text: str):
    out = agent_executor.invoke({
        "input": text,
        "chat_history": memory2str(conversational_memory)
    })
    conversational_memory.chat_memory.add_user_message(text)
    conversational_memory.chat_memory.add_ai_message(out["output"])
    return out["output"]

----------------------------------------

TITLE: Implementing Single Tool Call with Extended Thinking
DESCRIPTION: Demonstrates how to use Claude's extended thinking feature with a single tool call, using a mock weather tool. The function sets up the tool, makes an initial request, handles the tool execution, and processes the follow-up response.

LANGUAGE: python
CODE:
def tool_use_with_thinking():
    # Define a weather tool
    tools = [
        {
            "name": "weather",
            "description": "Get current weather information for a location.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The location to get weather for."
                    }
                },
                "required": ["location"]
            }
        }
    ]
    
    def weather(location):
        # Mock weather data
        weather_data = {
            "New York": {"temperature": 72, "condition": "Sunny"},
            "London": {"temperature": 62, "condition": "Cloudy"},
            "Tokyo": {"temperature": 80, "condition": "Partly cloudy"},
            "Paris": {"temperature": 65, "condition": "Rainy"},
            "Sydney": {"temperature": 85, "condition": "Clear"},
            "Berlin": {"temperature": 60, "condition": "Foggy"},
        }
        
        return weather_data.get(location, {"error": f"No weather data available for {location}"})
    
    # Initial request with tool use and thinking
    response = client.messages.create(
        model=MODEL_NAME,
        max_tokens=MAX_TOKENS,
        thinking={
            "type": "enabled",
            "budget_tokens": THINKING_BUDGET_TOKENS
        },
        tools=tools,
        messages=[{
            "role": "user",
            "content": "What's the weather like in Paris today?"
        }]
    )
    
    # Detailed diagnostic output of initial response
    print("\n=== INITIAL RESPONSE ===")
    print(f"Response ID: {response.id}")
    print(f"Stop reason: {response.stop_reason}")
    print(f"Model: {response.model}")
    print(f"Content blocks: {len(response.content)} blocks")
    
    for i, block in enumerate(response.content):
        print(f"\nBlock {i+1}: Type = {block.type}")
        if block.type == "thinking":
            print(f"Thinking content: {block.thinking[:150]}...")
            print(f"Signature available: {bool(getattr(block, 'signature', None))}")
        elif block.type == "text":
            print(f"Text content: {block.text}")
        elif block.type == "tool_use":
            print(f"Tool: {block.name}")
            print(f"Tool input: {block.input}")
            print(f"Tool ID: {block.id}")
    print("=== END INITIAL RESPONSE ===\n")
    
    # Extract thinking blocks to include in the conversation history
    assistant_blocks = []
    for block in response.content:
        if block.type in ["thinking", "redacted_thinking", "tool_use"]:
            assistant_blocks.append(block)
            
    # Handle tool use if required
    full_conversation = [{
        "role": "user",
        "content": "What's the weather like in Paris today?"
    }]
    
    if response.stop_reason == "tool_use":
        # Add entire assistant response with thinking blocks and tool use
        full_conversation.append({
            "role": "assistant",
            "content": assistant_blocks
        })
        
        # Find the tool_use block
        tool_use_block = next((block for block in response.content if block.type == "tool_use"), None)
        if tool_use_block:
            # Execute the tool
            print(f"\n=== EXECUTING TOOL ===")
            print(f"Tool name: {tool_use_block.name}")
            print(f"Location to check: {tool_use_block.input['location']}")
            tool_result = weather(tool_use_block.input["location"])
            print(f"Result: {tool_result}")
            print("=== TOOL EXECUTION COMPLETE ===\n")
            
            # Add tool result to conversation
            full_conversation.append({
                "role": "user",
                "content": [{
                    "type": "tool_result",
                    "tool_use_id": tool_use_block.id,
                    "content": json.dumps(tool_result)
                }]
            })
            
            # Continue the conversation with the same thinking configuration
            print("\n=== SENDING FOLLOW-UP REQUEST WITH TOOL RESULT ===")
            response = client.messages.create(
                model=MODEL_NAME,
                max_tokens=MAX_TOKENS,
                thinking={
                    "type": "enabled",
                    "budget_tokens": THINKING_BUDGET_TOKENS
                },
                tools=tools,
                messages=full_conversation
            )
            print(f"Follow-up response received. Stop reason: {response.stop_reason}")
    
    print_thinking_response(response)

# Run the example
tool_use_with_thinking()

----------------------------------------

TITLE: Implementing Claude Interaction Handler
DESCRIPTION: Defines functions to process tool calls and handle conversation flow with Claude, including handling tool results and responses

LANGUAGE: python
CODE:
def process_tool_call(tool_name, tool_input):
    if tool_name == "calculator":
        return calculate(tool_input["expression"])

def chat_with_claude(user_message):
    print(f"\n{'='*50}\nUser Message: {user_message}\n{'='*50}")

    message = client.messages.create(
        model=MODEL_NAME,
        max_tokens=4096,
        messages=[{"role": "user", "content": user_message}],
        tools=tools,
    )

    print(f"\nInitial Response:")
    print(f"Stop Reason: {message.stop_reason}")
    print(f"Content: {message.content}")

    if message.stop_reason == "tool_use":
        tool_use = next(block for block in message.content if block.type == "tool_use")
        tool_name = tool_use.name
        tool_input = tool_use.input

        print(f"\nTool Used: {tool_name}")
        print(f"Tool Input: {tool_input}")

        tool_result = process_tool_call(tool_name, tool_input)

        print(f"Tool Result: {tool_result}")

        response = client.messages.create(
            model=MODEL_NAME,
            max_tokens=4096,
            messages=[
                {"role": "user", "content": user_message},
                {"role": "assistant", "content": message.content},
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "tool_result",
                            "tool_use_id": tool_use.id,
                            "content": tool_result,
                        }
                    ],
                },
            ],
            tools=tools,
        )
    else:
        response = message

    final_response = next(
        (block.text for block in response.content if hasattr(block, "text")),
        None,
    )
    print(response.content)
    print(f"\nFinal Response: {final_response}")

    return final_response

----------------------------------------

TITLE: Building ReAct Agents for Document Processing
DESCRIPTION: Creates ReAct agents with vector and summary tools for each city document, enabling both specific retrievals and summarization capabilities.

LANGUAGE: python
CODE:
from llama_index.core.agent import ReActAgent
from llama_index.core import VectorStoreIndex, SummaryIndex
from llama_index.core.tools import QueryEngineTool, ToolMetadata

agents = {}

for wiki_title in wiki_titles:
    vector_index = VectorStoreIndex.from_documents(city_docs[wiki_title])
    summary_index = SummaryIndex.from_documents(city_docs[wiki_title])
    
    vector_query_engine = vector_index.as_query_engine()
    summary_query_engine = summary_index.as_query_engine()

    query_engine_tools = [
        QueryEngineTool(
            query_engine=vector_query_engine,
            metadata=ToolMetadata(
                name="vector_tool",
                description=f"Useful for retrieving specific context from {wiki_title}"
            ),
        ),
        QueryEngineTool(
            query_engine=summary_query_engine,
            metadata=ToolMetadata(
                name="summary_tool",
                description=f"Useful for summarization questions related to {wiki_title}"
            ),
        ),
    ]

    agent = ReActAgent.from_tools(
        query_engine_tools,
        llm=llm,
        verbose=True,
    )

    agents[wiki_title] = agent

----------------------------------------

TITLE: Parallelizing LLM Calls for Stakeholder Analysis in Python
DESCRIPTION: Demonstrates the parallel workflow by concurrently processing impact analysis for multiple stakeholder groups using ThreadPoolExecutor.

LANGUAGE: python
CODE:
stakeholders = [
    """Customers:
    - Price sensitive
    - Want better tech
    - Environmental concerns""",
    
    """Employees:
    - Job security worries
    - Need new skills
    - Want clear direction""",
    
    """Investors:
    - Expect growth
    - Want cost control
    - Risk concerns""",
    
    """Suppliers:
    - Capacity constraints
    - Price pressures
    - Tech transitions"""
]

impact_results = parallel(
    """Analyze how market changes will impact this stakeholder group.
    Provide specific impacts and recommended actions.
    Format with clear sections and priorities.""",
    stakeholders
)

for result in impact_results:
    print(result)

----------------------------------------

TITLE: Guided Sublease Summary Function
DESCRIPTION: A specialized guided summarization function for sublease agreements. It prompts the model to focus on specific aspects relevant to subleases, such as property details and responsibilities.

LANGUAGE: Python
CODE:
def guided_sublease_summary(text, model="claude-3-5-sonnet-20241022", max_tokens=1000):

    # Prompt the model to summarize the sublease agreement
    prompt = f"""Summarize the following sublease agreement. Focus on these key aspects:

    1. Parties involved (sublessor, sublessee, original lessor)
    2. Property details (address, description, permitted use)
    3. Term and rent (start date, end date, monthly rent, security deposit)
    4. Responsibilities (utilities, maintenance, repairs)
    5. Consent and notices (landlord's consent, notice requirements)
    6. Special provisions (furniture, parking, subletting restrictions)

    Provide the summary in bullet points nested within the XML header for each section. For example:

    <parties involved>
    - Sublessor: [Name]
    // Add more details as needed
    </parties involved>
    
    If any information is not explicitly stated in the document, note it as "Not specified". Do not preamble.

    Sublease agreement text:
    {text}
    
    """

    response = client.messages.create(
        model=model,
        max_tokens=max_tokens,
        system="You are a legal analyst specializing in real estate law, known for highly accurate and detailed summaries of sublease agreements.",
        messages=[
            {
                "role": "user", 
                "content": prompt
            },
            {
                "role": "assistant",
                "content": "Here is the summary of the sublease agreement: <summary>" 
            }
        ],
        stop_sequences=["</summary>"]
    )

    return response.content[0].text

----------------------------------------

TITLE: Defining Client-Side Tools for Customer Service Chatbot
DESCRIPTION: Defines three tools: get_customer_info, get_order_details, and cancel_order. Each tool has a name, description, and input schema.

LANGUAGE: python
CODE:
tools = [
    {
        "name": "get_customer_info",
        "description": "Retrieves customer information based on their customer ID. Returns the customer's name, email, and phone number.",
        "input_schema": {
            "type": "object",
            "properties": {
                "customer_id": {
                    "type": "string",
                    "description": "The unique identifier for the customer."
                }
            },
            "required": ["customer_id"]
        }
    },
    {
        "name": "get_order_details",
        "description": "Retrieves the details of a specific order based on the order ID. Returns the order ID, product name, quantity, price, and order status.",
        "input_schema": {
            "type": "object",
            "properties": {
                "order_id": {
                    "type": "string",
                    "description": "The unique identifier for the order."
                }
            },
            "required": ["order_id"]
        }
    },
    {
        "name": "cancel_order",
        "description": "Cancels an order based on the provided order ID. Returns a confirmation message if the cancellation is successful.",
        "input_schema": {
            "type": "object",
            "properties": {
                "order_id": {
                    "type": "string",
                    "description": "The unique identifier for the order to be cancelled."
                }
            },
            "required": ["order_id"]
        }
    }
]

----------------------------------------

TITLE: Implementing Model-based Grading for Open-ended Questions
DESCRIPTION: This snippet defines functions for model-based grading of open-ended questions, including a grader prompt template and a grading function that uses Claude to evaluate responses.

LANGUAGE: python
CODE:
import re

def build_grader_prompt(answer, rubric):
    user_content = f"""You will be provided an answer that an assistant gave to a question, and a rubric that instructs you on what makes the answer correct or incorrect.
    
    Here is the answer that the assistant gave to the question.
    <answer>{answer}</answer>
    
    Here is the rubric on what makes the answer correct or incorrect.
    <rubric>{rubric}</rubric>
    
    An answer is correct if it entirely meets the rubric criteria, and is otherwise incorrect. =
    First, think through whether the answer is correct or incorrect based on the rubric inside <thinking></thinking> tags. Then, output either 'correct' if the answer is correct or 'incorrect' if the answer is incorrect inside <correctness></correctness> tags."""

    messages = [{'role': 'user', 'content': user_content}]
    return messages

def grade_completion(output, golden_answer):
    messages = build_grader_prompt(output, golden_answer)
    completion = get_completion(messages)
    pattern = r'<correctness>(.*?)</correctness>'
    match = re.search(pattern, completion, re.DOTALL)
    if match:
        return match.group(1).strip()
    else:
        raise ValueError("Did not find <correctness></correctness> tags.")

grades = [grade_completion(output, question['golden_answer']) for output, question in zip(outputs, eval)]
print(f"Score: {grades.count('correct')/len(grades)*100}%")

----------------------------------------

TITLE: Structuring Building Effective Agents Cookbook in Markdown
DESCRIPTION: This Markdown snippet outlines the structure of the Building Effective Agents Cookbook repository. It includes sections for basic building blocks and advanced workflows, as well as links to Jupyter notebooks with detailed examples.

LANGUAGE: markdown
CODE:
# Building Effective Agents Cookbook

Reference implementation for [Building Effective Agents](https://anthropic.com/research/building-effective-agents) by Erik Schluntz and Barry Zhang.

This repository contains example minimal implementations of common agent workflows discussed in the blog:

- Basic Building Blocks
  - Prompt Chaining
  - Routing
  - Multi-LLM Parallelization
- Advanced Workflows
  - Orchestrator-Subagents
  - Evaluator-Optimizer

## Getting Started
See the Jupyter notebooks for detailed examples:

- [Basic Workflows](basic_workflows.ipynb)
- [Evaluator-Optimizer Workflow](evaluator_optimizer.ipynb) 
- [Orchestrator-Workers Workflow](orchestrator_workers.ipynb)

----------------------------------------

TITLE: Creating SubQuestionQueryEngine
DESCRIPTION: Sets up query engine tools and initializes the SubQuestionQueryEngine for complex queries

LANGUAGE: python
CODE:
from llama_index.core.tools import QueryEngineTool, ToolMetadata
from llama_index.core.query_engine import SubQuestionQueryEngine

query_engine_tools = [
    QueryEngineTool(
        query_engine=lyft_engine,
        metadata=ToolMetadata(name='lyft_10k', description='Provides information about Lyft financials for year 2021')
    ),
    QueryEngineTool(
        query_engine=uber_engine,
        metadata=ToolMetadata(name='uber_10k', description='Provides information about Uber financials for year 2021')
    ),
]

sub_question_query_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools)

----------------------------------------

TITLE: Enhancing Moderation with Few-Shot Learning Examples
DESCRIPTION: This code snippet demonstrates how to improve moderation performance by including example classifications in the prompt, providing Claude with initial training data for better understanding of category boundaries.

LANGUAGE: python
CODE:
examples_prompt = '''You are a content moderation expert tasked with categorizing user-generated text based on the following guidelines:

BLOCK CATEGORY:
- Content that is not related to rollercoasters, theme parks, or the amusement industry
- Explicit violence, hate speech, or illegal activities
- Spam, advertisements, or self-promotion

ALLOW CATEGORY:
- Discussions about rollercoaster designs, ride experiences, and park reviews
- Sharing news, rumors, or updates about new rollercoaster projects
- Respectful debates about the best rollercoasters, parks, or ride manufacturers
- Some mild profanity or crude language, as long as it is not directed at individuals

Here are some examples:
<examples>
Text: I'm selling weight loss products, check my link to buy!
Category: BLOCK

Text: I hate my local park, the operations and customer service are terrible. I wish that place would just burn down.
Category: BLOCK

Text: Did anyone ride the new RMC raptor Trek Plummet 2 yet? I've heard it's insane!
Category: ALLOW

Text: Hercs > B&Ms. That's just facts, no cap! Arrow > Intamin for classic woodies too.
Category: ALLOW
</examples>

Given those examples, here is the user-generated text to categorize:
<user_text>{user_text}</user_text>

Based on the guidelines above, classify this text as either ALLOW or BLOCK. Return nothing else.'''

user_post = "Why Boomerang Coasters Ain't It (Don't @ Me)"

response = client.messages.create(
        model=MODEL_NAME,
        max_tokens=1000,
        messages=[{"role": "user", "content": examples_prompt.format(user_text=user_post)}]
    ).content[0].text

print(response)

----------------------------------------

TITLE: Using FlexibleOrchestrator for Marketing Variation Generation in Python
DESCRIPTION: Demonstrates how to use the FlexibleOrchestrator class to generate marketing variations for an eco-friendly water bottle. It defines prompts for the orchestrator and workers, then processes a task to create product descriptions.

LANGUAGE: python
CODE:
ORCHESTRATOR_PROMPT = """
Analyze this task and break it down into 2-3 distinct approaches:

Task: {task}

Return your response in this format:

<analysis>
Explain your understanding of the task and which variations would be valuable.
Focus on how each approach serves different aspects of the task.
</analysis>

<tasks>
    <task>
    <type>formal</type>
    <description>Write a precise, technical version that emphasizes specifications</description>
    </task>
    <task>
    <type>conversational</type>
    <description>Write an engaging, friendly version that connects with readers</description>
    </task>
</tasks>
"""

WORKER_PROMPT = """
Generate content based on:
Task: {original_task}
Style: {task_type}
Guidelines: {task_description}

Return your response in this format:

<response>
Your content here, maintaining the specified style and fully addressing requirements.
</response>
"""


orchestrator = FlexibleOrchestrator(
    orchestrator_prompt=ORCHESTRATOR_PROMPT,
    worker_prompt=WORKER_PROMPT,
)

results = orchestrator.process(
    task="Write a product description for a new eco-friendly water bottle",
    context={
        "target_audience": "environmentally conscious millennials",
        "key_features": ["plastic-free", "insulated", "lifetime warranty"]
    }
)

----------------------------------------

TITLE: Guided Legal Summary Function
DESCRIPTION: Implements a guided summarization function specifically for legal documents. It prompts the model to focus on key aspects like parties involved, main subject matter, and important dates.

LANGUAGE: Python
CODE:
def guided_legal_summary(text, max_tokens=1000):

    # Prompt the model to summarize the text
    prompt = f"""Summarize the following legal document. Focus on these key aspects:

    1. Parties involved
    2. Main subject matter
    3. Key terms and conditions
    4. Important dates or deadlines
    5. Any unusual or notable clauses

    Provide the summary in bullet points under each category.

    Document text:
    {text}
    
    """

    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=max_tokens,
        system="You are a legal analyst known for highly accurate and detailed summaries of legal documents.",
        messages=[
            {
                "role": "user", 
                "content": prompt
            },
            {
                "role": "assistant",
                "content": "Here is the summary of the legal document: <summary>" 
            }
        ],
        stop_sequences=["</summary>"]
    )

    return response.content[0].text

----------------------------------------

TITLE: Routing Support Tickets using LLM Classification in Python
DESCRIPTION: Demonstrates the route workflow by dynamically classifying and routing customer support tickets to appropriate specialized LLM handlers based on content analysis.

LANGUAGE: python
CODE:
support_routes = {
    "billing": """You are a billing support specialist. Follow these guidelines:
    1. Always start with "Billing Support Response:"
    2. First acknowledge the specific billing issue
    3. Explain any charges or discrepancies clearly
    4. List concrete next steps with timeline
    5. End with payment options if relevant
    
    Keep responses professional but friendly.
    
    Input: """,
    
    "technical": """You are a technical support engineer. Follow these guidelines:
    1. Always start with "Technical Support Response:"
    2. List exact steps to resolve the issue
    3. Include system requirements if relevant
    4. Provide workarounds for common problems
    5. End with escalation path if needed
    
    Use clear, numbered steps and technical details.
    
    Input: """,
    
    "account": """You are an account security specialist. Follow these guidelines:
    1. Always start with "Account Support Response:"
    2. Prioritize account security and verification
    3. Provide clear steps for account recovery/changes
    4. Include security tips and warnings
    5. Set clear expectations for resolution time
    
    Maintain a serious, security-focused tone.
    
    Input: """,
    
    "product": """You are a product specialist. Follow these guidelines:
    1. Always start with "Product Support Response:"
    2. Focus on feature education and best practices
    3. Include specific examples of usage
    4. Link to relevant documentation sections
    5. Suggest related features that might help
    
    Be educational and encouraging in tone.
    
    Input: """
}

# Test with different support tickets
tickets = [
    """Subject: Can't access my account
    Message: Hi, I've been trying to log in for the past hour but keep getting an 'invalid password' error. 
    I'm sure I'm using the right password. Can you help me regain access? This is urgent as I need to 
    submit a report by end of day.
    - John""",
    
    """Subject: Unexpected charge on my card
    Message: Hello, I just noticed a charge of $49.99 on my credit card from your company, but I thought
    I was on the $29.99 plan. Can you explain this charge and adjust it if it's a mistake?
    Thanks,
    Sarah""",
    
    """Subject: How to export data?
    Message: I need to export all my project data to Excel. I've looked through the docs but can't
    figure out how to do a bulk export. Is this possible? If so, could you walk me through the steps?
    Best regards,
    Mike"""
]

print("Processing support tickets...\n")
for i, ticket in enumerate(tickets, 1):
    print(f"\nTicket {i}:")
    print("-" * 40)
    print(ticket)
    print("\nResponse:")
    print("-" * 40)
    response = route(ticket, support_routes)
    print(response)

----------------------------------------

TITLE: Initializing Pinecone Vector Database
DESCRIPTION: Set up and initialize Pinecone vector database with serverless specification and index configuration

LANGUAGE: python
CODE:
from pinecone import Pinecone, ServerlessSpec

pc = Pinecone(api_key=PINECONE_API_KEY)

spec = ServerlessSpec(
    cloud="aws", region="us-west-2"
)

index_name = 'amazon-products'
if index_name not in [index_info["name"] for index_info in pc.list_indexes()]:
    pc.create_index(
        index_name,
        dimension=1024,
        metric='dotproduct',
        spec=spec
    )
    while not pc.describe_index(index_name).status['ready']:
        time.sleep(1)

index = pc.Index(index_name)

----------------------------------------

TITLE: Defining Wolfram Alpha LLM API Tool for Claude
DESCRIPTION: Creates a function to query the Wolfram Alpha LLM API and defines a tool description for Claude to use.

LANGUAGE: python
CODE:
def wolfram_alpha_query(query):    
    # URL-encode the query
    encoded_query = urllib.parse.quote(query)
    
    # Make a request to the Wolfram Alpha LLM API
    url = f'https://www.wolframalpha.com/api/v1/llm-api?input={encoded_query}&appid={WOLFRAM_APP_ID}'
    response = requests.get(url)
    
    if response.status_code == 200:
        return response.text
    else:
        return f"Error: {response.status_code} - {response.text}"

tools = [
    {
        "name": "wolfram_alpha",
        "description": "A tool that allows querying the Wolfram Alpha knowledge base. Useful for mathematical calculations, scientific data, and general knowledge questions.",
        "input_schema": {
            "type": "object",
            "properties": {
                "search_query": {
                    "type": "string",
                    "description": "The query to send to the Wolfram Alpha API."
                }
            },
            "required": ["query"]
        }
    }
]

----------------------------------------

TITLE: Basic Example of Extended Thinking
DESCRIPTION: Demonstrates a basic example of using extended thinking to solve a puzzle.

LANGUAGE: python
CODE:
def basic_thinking_example():
    response = client.messages.create(
        model="claude-3-7-sonnet-20250219",
        max_tokens=4000,
        thinking= {
            "type": "enabled",
            "budget_tokens": 2000
        },
        messages=[{
            "role": "user",
            "content": "Solve this puzzle: Three people check into a hotel. They pay $30 to the manager. The manager finds out that the room only costs $25 so he gives $5 to the bellboy to return to the three people. The bellboy, however, decides to keep $2 and gives $1 back to each person. Now, each person paid $10 and got back $1, so they paid $9 each, totaling $27. The bellboy kept $2, which makes $29. Where is the missing $1?"
        }]
    )
    
    print_thinking_response(response)

basic_thinking_example()

----------------------------------------

TITLE: Indexing Data with VectorStoreIndex in LlamaIndex
DESCRIPTION: Creates a VectorStoreIndex from the loaded documents for efficient retrieval in the RAG pipeline.

LANGUAGE: python
CODE:
index = VectorStoreIndex.from_documents(
    documents,
)

----------------------------------------

TITLE: Implementing Chatbot Interaction
DESCRIPTION: Main function for handling user interactions with the chatbot and tool usage

LANGUAGE: python
CODE:
def chatbot_interaction(user_message):
    print(f"\n{'='*50}\nUser Message: {user_message}\n{'='*50}")

    messages = [
        {"role": "user", "content": user_message}
    ]

    message = client.messages.create(
        model=MODEL_NAME,
        max_tokens=4096,
        tools=tools,
        messages=messages
    )

    print(f"\nInitial Response:")
    print(f"Stop Reason: {message.stop_reason}")
    print(f"Content: {message.content}")

    if message.stop_reason == "tool_use":
        tool_use = next(block for block in message.content if block.type == "tool_use")
        tool_name = tool_use.name
        tool_input = tool_use.input

        print(f"\nTool Used: {tool_name}")
        print(f"Tool Input: {tool_input}")

        save_note_response = process_tool_call(tool_name, tool_input)

        print(f"Tool Result: {save_note_response}")

        response = client.messages.create(
            model=MODEL_NAME,
            max_tokens=4096,
            messages=[
                {"role": "user", "content": user_message},
                {"role": "assistant", "content": message.content},
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "tool_result",
                            "tool_use_id": tool_use.id,
                            "content": str(save_note_response),
                        }
                    ],
                },
            ],
            tools=tools,
        )
    else:
        response = message

    final_response = next(
        (block.text for block in response.content if hasattr(block, "text")),
        None,
    )
    print(response.content)
    print(f"\nFinal Response: {final_response}")

    return final_response

----------------------------------------

TITLE: Demonstrating Thinking Block Preservation in Tool Use
DESCRIPTION: Illustrates the importance of preserving thinking blocks when using tools with Claude's extended thinking feature. It shows the difference between including and excluding thinking blocks in follow-up requests after tool use.

LANGUAGE: python
CODE:
def thinking_block_preservation_example():
    # Define a simple weather tool
    tools = [
        {
            "name": "weather",
            "description": "Get current weather information for a location.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The location to get weather for."
                    }
                },
                "required": ["location"]
            }
        }
    ]
    
    def weather(location):
        # Mock weather data
        weather_data = {
            "New York": {"temperature": 72, "condition": "Sunny"},
            "London": {"temperature": 62, "condition": "Cloudy"},
            "Tokyo": {"temperature": 80, "condition": "Partly cloudy"},
            "Paris": {"temperature": 65, "condition": "Rainy"},
            "Sydney": {"temperature": 85, "condition": "Clear"},
            "Berlin": {"temperature": 60, "condition": "Foggy"},
        }
        
        return weather_data.get(location, {"error": f"No weather data available for {location}"})
    
    # Initial request with tool use and thinking
    response = client.messages.create(
        model=MODEL_NAME,
        max_tokens=MAX_TOKENS,
        thinking={
            "type": "enabled",
            "budget_tokens": THINKING_BUDGET_TOKENS
        },
        tools=tools,
        messages=[{
            "role": "user",
            "content": "What's the weather like in Berlin right now?"
        }]
    )
    
    # Extract blocks from response
    thinking_blocks = [b for b in response.content if b.type == "thinking"]
    tool_use_blocks = [b for b in response.content if b.type == "tool_use"]
    
    print("\n=== INITIAL RESPONSE ===")
    print(f"Response contains:")
    print(f"- {len(thinking_blocks)} thinking blocks")
    print(f"- {len(tool_use_blocks)} tool use blocks")
    
    # Check if tool use was triggered
    if tool_use_blocks:
        tool_block = tool_use_blocks[0]
        print(f"\nTool called: {tool_block.name}")
        print(f"Location to check: {tool_block.input['location']}")
        
        # Execute the tool
        tool_result = weather(tool_block.input["location"])
        print(f"Tool result: {tool_result}")
        
        # First, let's try WITHOUT including the thinking block
        print("\n=== TEST 1: WITHOUT thinking block ===")
        try:
            # Notice we're only including the tool_use block, not the thinking block
            partial_blocks = tool_use_blocks
            
            incomplete_response = client.messages.create(
                model=MODEL_NAME,
                max_tokens=MAX_TOKENS,
                thinking={
                        "type": "enabled",
                        "budget_tokens": THINKING_BUDGET_TOKENS
                },
                tools=tools,
                messages=[
                    {"role": "user", "content": "What's the weather like in Berlin right now?"},
                    {"role": "assistant", "content": partial_blocks},
                    {"role": "user", "content": [{
                        "type": "tool_result",
                        "tool_use_id": tool_block.id,
                        "content": json.dumps(tool_result)
                    }]}
                ]
            )
            print("SUCCESS: Response received without thinking block (not expected)")
        except Exception as e:
            print(f"ERROR: {e}")
            print("This demonstrates that thinking blocks must be preserved")
        
        # Now try WITH the thinking block included (correct approach)
        print("\n=== TEST 2: WITH thinking block (correct approach) ===")
        try:
            # Include all blocks from the response
            complete_blocks = thinking_blocks + tool_use_blocks
            
            complete_response = client.messages.create(
                model=MODEL_NAME,
                max_tokens=MAX_TOKENS,
                thinking={
                    "type": "enabled",
                    "budget_tokens": THINKING_BUDGET_TOKENS
                },
                tools=tools,
                messages=[
                    {"role": "user", "content": "What's the weather like in Berlin right now?"},
                    {"role": "assistant", "content": complete_blocks},
                    {"role": "user", "content": [{
                        "type": "tool_result",
                        "tool_use_id": tool_block.id,
                        "content": json.dumps(tool_result)
                    }]}
                ]
            )
            print("SUCCESS: Response received with thinking blocks included")
            
            # Check if second response has thinking blocks
            second_thinking = [b for b in complete_response.content if b.type == "thinking"]
            second_text = [b for b in complete_response.content if b.type == "text"]
            
            print(f"\nSecond response contains:")
            print(f"- {len(second_thinking)} thinking blocks")
            print(f"- {len(second_text)} text blocks")
            
            if second_text:
                print(f"\nFinal answer: {second_text[0].text}")
            
            print("\nNote: The second response after tool use doesn't contain thinking blocks.")
            print("This is expected behavior - thinking is shown before tool use but not after receiving tool results.")
            
        except Exception as e:
            print(f"ERROR: {e}")
    
# Uncomment to run the example
thinking_block_preservation_example()

----------------------------------------

TITLE: Extracting Information from PDFs Using Haiku Sub-Agents
DESCRIPTION: Processes PDFs concurrently using Haiku sub-agent models to extract relevant financial information.

LANGUAGE: python
CODE:
def extract_info(pdf_path, haiku_prompt):
    base64_encoded_pngs = pdf_to_base64_pngs(pdf_path)
    
    messages = [
        {
            "role": "user",
            "content": [
                *[{"type": "image", "source": {"type": "base64", "media_type": "image/png", "data": base64_encoded_png}} for base64_encoded_png in base64_encoded_pngs],
                {"type": "text", "text": haiku_prompt}
            ]
        }
    ]
    
    response = client.messages.create(
        model="claude-3-haiku-20240307",
        max_tokens=2048,
        messages=messages
    )
    
    return response.content[0].text, pdf_path

def process_pdf(pdf_path):
    return extract_info(pdf_path, haiku_prompt)

# Process the PDFs concurrently with Haiku sub-agent models
with ThreadPoolExecutor() as executor:
    extracted_info_list = list(executor.map(process_pdf, pdf_paths))

extracted_info = ""
# Display the extracted information from each model call
for info in extracted_info_list:
    extracted_info += "<info quarter=\"" + info[1].split("/")[-1].split("_")[1] + "\">" + info[0] + "</info>\n"
print(extracted_info)

----------------------------------------

TITLE: Generating Search Queries with Claude
DESCRIPTION: Uses Claude to generate relevant search queries based on the user's question.

LANGUAGE: Python
CODE:
import anthropic

client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)

def get_completion(prompt: str):
    message = client.completions.create(
        model='claude-2.1',
        max_tokens_to_sample=1024,
        temperature=.5,
        prompt=prompt
    )
    return message.completion
queries_json = "{" + get_completion(GENERATE_QUERIES)
print(queries_json)

----------------------------------------

TITLE: Moderating User Comments with Claude
DESCRIPTION: This code demonstrates how to use the 'moderate_text' function to classify an array of user comments based on example moderation guidelines.

LANGUAGE: python
CODE:
example_guidelines = '''BLOCK CATEGORY:
    - Promoting violence, illegal activities, or hate speech
    - Explicit sexual content
    - Harmful misinformation or conspiracy theories

    ALLOW CATEGORY:
    - Most other content is allowed, as long as it is not explicitly disallowed
'''

user_comments = [
    "This movie was great, I really enjoyed it. The main actor really killed it!",
    "Delete this post now or you better hide. I am coming after you and your family.",
    "Stay away from the 5G cellphones!! They are using 5G to control you.",
    "Thanks for the helpful information!",
]

for comment in user_comments:
    classification = moderate_text(comment, example_guidelines)
    print(f"Comment: {comment}\nClassification: {classification}\n")

----------------------------------------

TITLE: Executing a Sample Query with the RAG System
DESCRIPTION: Demonstrates how to use the implemented RAG system by processing a sample query and displaying the response along with source information.

LANGUAGE: python
CODE:
# Conduct query with retrieval of sources
query = "Give me the best tech stock to invest in and tell me why"
response, source_information = handle_user_query(query, collection)

print(f"Response: {response}")
print(f"\nSource Information: \n{source_information}")

----------------------------------------

TITLE: Testing Calculator Tool with Claude
DESCRIPTION: Example usage of the calculator tool with Claude, demonstrating various arithmetic operations

LANGUAGE: python
CODE:
chat_with_claude("What is the result of 1,984,135 * 9,343,116?")
chat_with_claude("Calculate (12851 - 593) * 301 + 76")
chat_with_claude("What is 15910385 divided by 193053?")

----------------------------------------

TITLE: Managing Multi-turn Conversation History
DESCRIPTION: Implements a ConversationHistory class to manage cached conversation turns and system messages

LANGUAGE: python
CODE:
class ConversationHistory:
    def __init__(self):
        self.turns = []

    def add_turn_assistant(self, content):
        self.turns.append({
            "role": "assistant",
            "content": [
                {
                    "type": "text",
                    "text": content
                }
            ]
        })

    def add_turn_user(self, content):
        self.turns.append({
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": content
                }
            ]
        })

    def get_turns(self):
        result = []
        user_turns_processed = 0
        for turn in reversed(self.turns):
            if turn["role"] == "user" and user_turns_processed < 1:
                result.append({
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": turn["content"][0]["text"],
                            "cache_control": {"type": "ephemeral"}
                        }
                    ]
                })
                user_turns_processed += 1
            else:
                result.append(turn)
        return list(reversed(result))

----------------------------------------

TITLE: Downloading and Preprocessing Government Documents
DESCRIPTION: Fetches XML documents from govinfo.gov, splits them into chunks, and filters based on token length using Claude's tokenizer

LANGUAGE: python
CODE:
url = 'https://www.govinfo.gov/content/pkg/FR-2023-07-13/xml/FR-2023-07-13.xml'

response = requests.get(url)
soup = BeautifulSoup(response.text, 'xml')

text = soup.get_text()
chunks = text.split('BILLING CODE')
chunks[0] = chunks[0][chunks[0].index('DEPARTMENT OF TRANSPORTATION'):] 

tokenizer = CLIENT.get_tokenizer()
chunks = [c for c in chunks if len(tokenizer.encode(c)) <= 5000 and len(tokenizer.encode(c)) > 200]
print(len(chunks))
print(chunks[2])

----------------------------------------

TITLE: Connecting to MongoDB and Ingesting Data
DESCRIPTION: Establishes a connection to MongoDB, clears existing records, and ingests the preprocessed dataset into the specified collection.

LANGUAGE: python
CODE:
import pymongo
from google.colab import userdata

def get_mongo_client(mongo_uri):
  """Establish connection to the MongoDB."""
  try:
    client = pymongo.MongoClient(mongo_uri)
    print("Connection to MongoDB successful")
    return client
  except pymongo.errors.ConnectionFailure as e:
    print(f"Connection failed: {e}")
    return None

mongo_uri = userdata.get('MONGO_URI')
if not mongo_uri:
  print("MONGO_URI not set in environment variables")

mongo_client = get_mongo_client(mongo_uri)

DB_NAME="tech_news"
COLLECTION_NAME="hacker_noon_tech_news"

db = mongo_client[DB_NAME]
collection = db[COLLECTION_NAME]

# To ensure we are working with a fresh collection
# delete any existing records in the collection
collection.delete_many({})

# Data Ingestion
combined_df_json = combined_df.to_dict(orient='records')
collection.insert_many(combined_df_json)

----------------------------------------

TITLE: Creating and Submitting a Batch of Message Requests
DESCRIPTION: Prepares a list of questions, creates batch requests, and submits them to the Anthropic API.

LANGUAGE: python
CODE:
# Prepare a list of questions for batch processing
questions = [
    "How do solar panels convert sunlight into electricity?",
    "What's the difference between mutual funds and ETFs?",
    "What is a pick and roll in basketball?",
    "Why do leaves change color in autumn?"
]

# Create batch requests
batch_requests = [
    {
        "custom_id": f"question-{i}",
        "params": {
            "model": MODEL_NAME,
            "max_tokens": 1024,
            "messages": [
                {"role": "user", "content": question}
            ]
        }
    }
    for i, question in enumerate(questions)
]

# Submit the batch
response = client.beta.messages.batches.create(
    requests=batch_requests
)

print(f"Batch ID: {response.id}")
print(f"Status: {response.processing_status}")
print(f"Created at: {response.created_at}")

----------------------------------------

TITLE: Implementing Claude Interaction with Wolfram Alpha Tool
DESCRIPTION: Defines functions to process tool calls and chat with Claude using the Wolfram Alpha tool.

LANGUAGE: python
CODE:
def process_tool_call(tool_name, tool_input):
    if tool_name == "wolfram_alpha":
        return wolfram_alpha_query(tool_input["search_query"])

def chat_with_claude(user_message):
    print(f"\n{'='*50}\nUser Message: {user_message}\n{'='*50}")
    prompt = f"""Here is a question: {user_message}. Please use the Wolfram Alpha tool to answer it. Do not reflect on the quality of the returned search results in your response."""

    message = client.beta.tools.messages.create(
        model=MODEL_NAME,
        max_tokens=4096,
        tools=tools,
        messages=[{"role": "user", "content": prompt}]
    )
    
    print(f"\nInitial Response:")
    print(f"Stop Reason: {message.stop_reason}")
    print(f"Content: {message.content}")
    
    if message.stop_reason == "tool_use":
        tool_use = next(block for block in message.content if block.type == "tool_use")
        tool_name = tool_use.name
        tool_input = tool_use.input
        
        print(f"\nTool Used: {tool_name}")
        print(f"Tool Input:")
        print(json.dumps(tool_input, indent=2))
        
        tool_result = process_tool_call(tool_name, tool_input)
        
        print(f"\nTool Result:")
        print(str(json.dumps(tool_result, indent=2)))
    
        
        response = client.beta.tools.messages.create(
            model=MODEL_NAME,
            max_tokens=2000,
            tools=tools,
            messages=[
                {"role": "user", "content": prompt},
                {"role": "assistant", "content": message.content},
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "tool_result",
                            "tool_use_id": tool_use.id,
                            "content": str(tool_result)
                        }
                    ]
                }
            ]
        )
        
        print(f"\nResponse:")
        print(f"Stop Reason: {response.stop_reason}")
        print(f"Content: {response.content}")
    else:
        response = message
    
    final_response = None
    for block in response.content:
        if hasattr(block, 'text'):
            final_response = block.text
            break
    
    print(f"\nFinal Response: {final_response}")
    
    return final_response

----------------------------------------

TITLE: Deepgram Audio Transcription Implementation
DESCRIPTION: Implements audio file transcription using Deepgram's API. Downloads audio from a URL, configures transcription options, and saves the result to a JSON file.

LANGUAGE: python
CODE:
from deepgram import DeepgramClient, PrerecordedOptions, FileSource
import requests

DG_KEY = "🔑🔑🔑 Your API Key here! 🔑🔑🔑"
AUDIO_FILE_URL = "https://static.deepgram.com/examples/nasa-spacewalk-interview.wav"
TRANSCRIPT_FILE = "transcript.json"

def main():
    try:
        deepgram = DeepgramClient(DG_KEY)
        response = requests.get(AUDIO_FILE_URL)
        if response.status_code == 200:
            buffer_data = response.content
        else:
            print("Failed to download audio file")
            return

        payload: FileSource = {
            "buffer": buffer_data,
        }

        options = PrerecordedOptions(
            model="nova-2",
            smart_format=True,
        )

        response = deepgram.listen.prerecorded.v("1").transcribe_file(payload, options)

        with open(TRANSCRIPT_FILE, "w") as transcript_file:
            transcript_file.write(response.to_json(indent=4))

        print("Transcript JSON file generated successfully.")

    except Exception as e:
        print(f"Exception: {e}")

if __name__ == "__main__":
    main()

----------------------------------------

TITLE: Launching Finetuning Job for Claude 3 Haiku on Amazon Bedrock
DESCRIPTION: This code initiates the finetuning job on Amazon Bedrock using the boto3 client. It sets up the job with the configured parameters, including the base model, hyperparameters, training data, and output location.

LANGUAGE: python
CODE:
bedrock = boto3.client(service_name="bedrock")
bedrock_runtime = boto3.client(service_name="bedrock-runtime")

bedrock.create_model_customization_job(
    customizationType="FINE_TUNING",
    jobName=job_name,
    customModelName=custom_model_name,
    roleArn=role,
    baseModelIdentifier=base_model_id,
    hyperParameters = {
        "epochCount": f"{epoch_count}",
        "batchSize": f"{batch_size}",
        "learningRateMultiplier": f"{learning_rate_multiplier}",
    },
    trainingDataConfig={"s3Uri": f"s3://{bucket_name}/{s3_path}"},
    outputDataConfig={"s3Uri": output_path},
)

----------------------------------------

TITLE: Importing Dependencies for Multi-LLM Workflows in Python
DESCRIPTION: Imports necessary modules for implementing multi-LLM workflows, including concurrent futures for parallelization, typing for type hints, and custom utility functions for LLM calls and XML extraction.

LANGUAGE: python
CODE:
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Callable
from util import llm_call, extract_xml

----------------------------------------

TITLE: Multi-Shot Basic Summarization Function
DESCRIPTION: Enhances the basic summarization function by adding multiple examples to guide the model's output format.

LANGUAGE: Python
CODE:
from data.multiple_subleases import document1, document2, document3, sample1, sample2, sample3

def basic_summarize_multishot(text, max_tokens=1000):

    # Prompt the model to summarize the text
    prompt = f"""Summarize the following text in bullet points. Focus on the main ideas and key details:
        {text}

    Do not preamble.

    Use these examples for guidance in summarizing:

    <example1>
        <original1>
            {document1}
        </original1>

        <summary1>
            {sample1}
        </summary1>
    </example1>

    <example2>
        <original2>
            {document2}
        </original2>

        <summary2>
            {sample2}
        </summary2>
    </example2>

    <example3>
        <original3>
            {document3}
        </original3>

        <summary3>
            {sample3}
        </summary3>
    </example3>
    """

    response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=max_tokens,
            system="You are a legal analyst known for highly accurate and detailed summaries of legal documents.",
            messages=[
                {
                    "role": "user", 
                    "content": prompt
                },
                {
                    "role": "assistant",
                    "content": "Here is the summary of the legal document: <summary>" 
                }
            ],
            stop_sequences=["</summary>"]
        )

    return response.content[0].text

----------------------------------------

TITLE: Highlighting Cited Text in PDFs
DESCRIPTION: Processes PDF citations from the API response and creates a new PDF with highlighted cited text using PyMuPDF.

LANGUAGE: python
CODE:
import fitz  # PyMuPDF

# Setup paths and read PDF
pdf_path = 'data/Amazon-com-Inc-2023-Shareholder-Letter.pdf'
output_pdf_path = 'data/Amazon-com-Inc-2023-Shareholder-Letter-highlighted.pdf'

# Read and encode the PDF
with open(pdf_path, "rb") as f:
    pdf_data = base64.b64encode(f.read()).decode()

response = client.messages.create(
    model="claude-3-5-sonnet-latest",
    max_tokens=1024,
    temperature=0,
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "document",
                    "source": {
                        "type": "base64",
                        "media_type": "application/pdf",
                        "data": pdf_data
                    },
                    "title": "Amazon 2023 Shareholder Letter",
                    "citations": {"enabled": True}
                },
                {
                    "type": "text",
                    "text": "What was Amazon's total revenue in 2023 and how much did it grow year-over-year?"
                }
            ]
        }
    ]
)

print(visualize_raw_response(response))

# Collect PDF citations
pdf_citations = []
for content in response.content:
    if hasattr(content, 'citations') and content.citations:
        for citation in content.citations:
            if citation.type == "page_location":
                pdf_citations.append(citation)

doc = fitz.open(pdf_path)

# Process each citation
for citation in pdf_citations:
    if citation.type == "page_location":
        text_to_find = citation.cited_text.replace('\u0002', '')
        start_page = citation.start_page_number - 1  # Convert to 0-based index
        end_page = citation.end_page_number - 2
        
        # Process each page in the citation range
        for page_num in range(start_page, end_page + 1):
            page = doc[page_num]
            
            text_instances = page.search_for(text_to_find.strip())
            
            if text_instances:
                print(f"Found cited text on page {page_num + 1}")
                for inst in text_instances:
                    highlight = page.add_highlight_annot(inst)
                    highlight.set_colors({"stroke":(1, 1, 0)})  # Yellow highlight
                    highlight.update()
            else:
                print(f"{text_to_find} not found on page {page_num + 1}")

# Save the new PDF
doc.save(output_pdf_path)
doc.close()

print(f"\nCreated highlighted PDF at: {output_pdf_path}")

----------------------------------------

TITLE: Summarizing Long Documents Function
DESCRIPTION: Implements a function to summarize long documents by breaking them into chunks, summarizing each chunk, and then combining the summaries.

LANGUAGE: Python
CODE:
from data.multiple_subleases import document1, document2, document3

def chunk_text(text, chunk_size=2000):
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

def summarize_long_document(text, max_tokens=2000):

    chunks = chunk_text(text)

    # Iterate over chunks and summarize each one
    # We use guided_legal_summary here, but you can use basic_summarize or any other summarization function
    # Note that we'll also use haiku for the interim summaries, and the 3.5 sonnet for the final summary
    chunk_summaries = [guided_sublease_summary(chunk, model="claude-3-haiku-20240307", max_tokens=max_tokens) for chunk in chunks]
    
    final_summary_prompt = f"""
    
    You are looking at the chunked summaries of multiple documents that are all related. Combine the following summaries of the document from different truthful sources into a coherent overall summary:

    {"".join(chunk_summaries)}

    1. Parties involved (sublessor, sublessee, original lessor)
    2. Property details (address, description, permitted use)
    3. Term and rent (start date, end date, monthly rent, security deposit)
    4. Responsibilities (utilities, maintenance, repairs)
    5. Consent and notices (landlord's consent, notice requirements)
    6. Special provisions (furniture, parking, subletting restrictions)

    Provide the summary in bullet points nested within the XML header for each section. For example:

    <parties involved>
    - Sublessor: [Name]
    // Add more details as needed
    </parties involved>
    
    If any information is not explicitly stated in the document, note it as "Not specified".

    Summary:
    """

    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=max_tokens,
        system="You are a legal expert that summarizes notes on one document.",
        messages=[
            {
                "role": "user", 
                "content": final_summary_prompt
            },
            {
                "role": "assistant",
                "content": "Here is the summary of the legal document: <summary>" 
            }
        ],
        stop_sequences=["</summary>"]
    )
    
    return response.content[0].text

----------------------------------------

TITLE: Implementing Weather and Time Tools for Claude in Python
DESCRIPTION: Defines functions and tool specifications for getting weather and time information, along with a function to process tool calls.

LANGUAGE: python
CODE:
def get_weather(location):
    # Pretend to get the weather, and just return a fixed value.
    return f"The weather in {location} is 72 degrees and sunny."

def get_time(location):
    # Pretend to get the time, and just return a fixed value.
    return f"The time in {location} is 12:32 PM."

weather_tool = {
    "name": "get_weather",
    "description": "Gets the weather for in a given location",
    "input_schema": {
        "type": "object",
        "properties": {
            "location": {
                "type": "string",
                "description": "The city and state, e.g. San Francisco, CA",
            },
        },
        "required": ["location"]
    }
}

time_tool = {
    "name": "get_time",
    "description": "Gets the time in a given location",
    "input_schema": {
        "type": "object",
        "properties": {
            "location": {
                "type": "string",
                "description": "The city and state, e.g. San Francisco, CA",
            },
        },
        "required": ["location"]
    }
}

def process_tool_call(tool_name, tool_input):
    if tool_name == "get_weather":
        return get_weather(tool_input["location"])
    elif tool_name == "get_time":
        return get_time(tool_input["location"])
    else:
        raise ValueError(f"Unexpected tool name: {tool_name}")

----------------------------------------

TITLE: Executing Generated SQL Query
DESCRIPTION: Demonstrates sending a natural language question to Claude and executing the returned SQL query

LANGUAGE: python
CODE:
# Example natural language question
question = "What are the names and salaries of employees in the Engineering department?"
# Send the question to Claude and get the SQL query
sql_query = ask_claude(question, schema_str)
print(sql_query)

----------------------------------------

TITLE: Implementing SMS Chatbot with 'Any' Tool Choice
DESCRIPTION: Creates an SMS chatbot that must use one of two provided tools (send text or get customer info) for every response.

LANGUAGE: python
CODE:
def send_text_to_user(text):
    print(f"TEXT MESSAGE SENT: {text}")

def get_customer_info(username):
    return {
        "username": username,
        "email": f"{username}@email.com",
        "purchases": [
            {"id": 1, "product": "computer mouse"},
            {"id": 2, "product": "screen protector"},
            {"id": 3, "product": "usb charging cable"},
        ]
    }

tools = [
    {
        "name": "send_text_to_user",
        "description": "Sends a text message to a user",
        "input_schema": {
            "type": "object",
            "properties": {
                "text": {"type": "string", "description": "The piece of text to be sent to the user via text message"},
            },
            "required": ["text"]
        }
    },
    {
        "name": "get_customer_info",
        "description": "gets information on a customer based on the customer's username.  Response includes email, username, and previous purchases. Only call this tool once a user has provided you with their username",
        "input_schema": {
            "type": "object",
            "properties": {
                "username": {"type": "string", "description": "The username of the user in question. "},
            },
            "required": ["username"]
        }
    },
]

system_prompt = """
All your communication with a user is done via text message.
Only call tools when you have enough information to accurately call them.  
Do not call the get_customer_info tool until a user has provided you with their username. This is important.
If you do not know a user's username, simply ask a user for their username.
"""

def sms_chatbot(user_message):
    messages = [{"role": "user", "content":user_message}]

    response = client.messages.create(
        system=system_prompt,
        model=MODEL_NAME,
        max_tokens=4096,
        tools=tools,
        tool_choice={"type": "any"},
        messages=messages
    )
    if response.stop_reason == "tool_use":
        last_content_block = response.content[-1]
        if last_content_block.type == 'tool_use':
            tool_name = last_content_block.name
            tool_inputs = last_content_block.input
            print(f"=======Claude Wants To Call The {tool_name} Tool=======")
            if tool_name == "send_text_to_user":
                send_text_to_user(tool_inputs["text"])
            elif tool_name == "get_customer_info":
                print(get_customer_info(tool_inputs["username"]))
            else:
                print("Oh dear, that tool doesn't exist!")
            
    else:
        print("No tool was called. This shouldn't happen!")

----------------------------------------

TITLE: Configuring LlamaIndex Settings for RAG Pipeline
DESCRIPTION: Sets up global LlamaIndex settings with the initialized LLM, embedding model, and chunk size.

LANGUAGE: python
CODE:
from llama_index.core import Settings
Settings.llm = llm
Settings.embed_model = embed_model
Settings.chunk_size = 512

----------------------------------------

TITLE: PDF to Base64 Conversion
DESCRIPTION: Reads a PDF file and converts it to base64 encoded string for API submission

LANGUAGE: python
CODE:
import base64

# Start by reading in the PDF and encoding it as base64
file_name = "../multimodal/documents/constitutional-ai-paper.pdf"
with open(file_name, "rb") as pdf_file:
  binary_data = pdf_file.read()
  base64_encoded_data = base64.standard_b64encode(binary_data)
  base64_string = base64_encoded_data.decode("utf-8")


----------------------------------------

TITLE: Claude Client with Retrieval Functionality
DESCRIPTION: Implements a Claude client class that handles the retrieval workflow including extracting search queries, running searches, and managing the conversation flow with Claude

LANGUAGE: python
CODE:
def extract_between_tags(tag: str, string: str, strip: bool = True) -> list[str]:
    ext_list = re.findall(f"<{tag}\s?>(.+?)</{tag}\s?>", string, re.DOTALL)
    if strip:
        ext_list = [e.strip() for e in ext_list]
    return ext_list

class ClientWithRetrieval(Anthropic):

    def __init__(self, search_tool: SearchTool, verbose: bool = True, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.search_tool = search_tool
        self.verbose = verbose

    def _search_query_stop(self, partial_completion: str, n_search_results_to_use: int) -> Tuple[list[SearchResult], str]:
        search_query = extract_between_tags('search_query', partial_completion + '</search_query>') 
        if search_query is None:
            raise Exception(f'Completion with retrieval failed as partial completion returned mismatched <search_query> tags.')
        print(f'Running search query against SearchTool: {search_query}')
        search_results = self.search_tool.raw_search(search_query, n_search_results_to_use)
        extracted_search_results = self.search_tool.process_raw_search_results(search_results)
        formatted_search_results = self.search_tool.wrap_search_results(extracted_search_results)
        return search_results, formatted_search_results
    
    def retrieve(self, query: str, model: str, n_search_results_to_use: int = 3,
                       stop_sequences: list[str] = [HUMAN_PROMPT], max_tokens_to_sample: int = 1000,
                       max_searches_to_try: int = 5, temperature: float = 1.0) -> tuple[list[SearchResult], str]:
        
        prompt = f"{HUMAN_PROMPT} {wikipedia_prompt} {retrieval_prompt.format(query=query)}{AI_PROMPT}"
        starting_prompt = prompt
        print("Starting prompt:", starting_prompt)
        token_budget = max_tokens_to_sample
        all_raw_search_results: list[SearchResult] = []
        for tries in range(max_searches_to_try):
            partial_completion = self.completions.create(prompt = prompt,
                                                     stop_sequences=stop_sequences + ['</search_query>'],
                                                     model=model,
                                                     max_tokens_to_sample = token_budget,
                                                     temperature = temperature)
            partial_completion, stop_reason, stop_seq = partial_completion.completion, partial_completion.stop_reason, partial_completion.stop
            print(partial_completion)
            token_budget -= self.count_tokens(partial_completion)
            prompt += partial_completion
            if stop_reason == 'stop_sequence' and stop_seq == '</search_query>':
                print(f'Attempting search number {tries}.')
                raw_search_results, formatted_search_results = self._search_query_stop(partial_completion, n_search_results_to_use)
                prompt += '</search_query>' + formatted_search_results
                all_raw_search_results += raw_search_results
            else:
                break
        final_model_response = prompt[len(starting_prompt):]
        return all_raw_search_results, final_model_response
    
    def completion_with_retrieval(self, query: str, model: str, n_search_results_to_use: int = 3,
                                        stop_sequences: list[str] = [HUMAN_PROMPT],
                                        max_tokens_to_sample: int = 1000,
                                        max_searches_to_try: int = 5,
                                        temperature: float = 1.0) -> str:
        
        _, retrieval_response = self.retrieve(query, model=model,
                                                 n_search_results_to_use=n_search_results_to_use, stop_sequences=stop_sequences,
                                                 max_tokens_to_sample=max_tokens_to_sample,
                                                 max_searches_to_try=max_searches_to_try,
                                                 temperature=temperature)
        information = extract_between_tags('information', retrieval_response)[-1]
        prompt = f"{HUMAN_PROMPT} {answer_prompt.format(query=query, information=information)}{AI_PROMPT}"
        print("Summarizing:\n", prompt)
        answer = self.completions.create(
            prompt = prompt, model=model, temperature=temperature, max_tokens_to_sample=1000
        ).completion
        return answer

----------------------------------------

TITLE: Implementing Note Saving Function
DESCRIPTION: Simple implementation of the note saving functionality (mock implementation)

LANGUAGE: python
CODE:
def save_note(note: str, author: dict, priority: int = 3, is_public: bool = False) -> None:
    print("Note saved successfully!")

----------------------------------------

TITLE: Chaining LLM Calls for Data Processing in Python
DESCRIPTION: Demonstrates the chain workflow by processing a performance report through multiple LLM calls, progressively transforming raw text into a formatted table.

LANGUAGE: python
CODE:
data_processing_steps = [
    """Extract only the numerical values and their associated metrics from the text.
    Format each as 'value: metric' on a new line.
    Example format:
    92: customer satisfaction
    45%: revenue growth""",
    
    """Convert all numerical values to percentages where possible.
    If not a percentage or points, convert to decimal (e.g., 92 points -> 92%).
    Keep one number per line.
    Example format:
    92%: customer satisfaction
    45%: revenue growth""",
    
    """Sort all lines in descending order by numerical value.
    Keep the format 'value: metric' on each line.
    Example:
    92%: customer satisfaction
    87%: employee satisfaction""",
    
    """Format the sorted data as a markdown table with columns:
    | Metric | Value |
    |:--|--:|
    | Customer Satisfaction | 92% |"""
]

report = """
Q3 Performance Summary:
Our customer satisfaction score rose to 92 points this quarter.
Revenue grew by 45% compared to last year.
Market share is now at 23% in our primary market.
Customer churn decreased to 5% from 8%.
New user acquisition cost is $43 per user.
Product adoption rate increased to 78%.
Employee satisfaction is at 87 points.
Operating margin improved to 34%.
"""

print("\nInput text:")
print(report)
formatted_result = chain(report, data_processing_steps)
print(formatted_result)

----------------------------------------

TITLE: Monitoring Batch Processing Status
DESCRIPTION: Defines a function to monitor the batch processing status and continuously poll for updates until completion.

LANGUAGE: python
CODE:
def monitor_batch(batch_id, polling_interval=5):
    while True:
        batch_update = client.beta.messages.batches.retrieve(batch_id)
        batch_update_status = batch_update.processing_status
        print(batch_update)
        print(f"Status: {batch_update_status}")
        if batch_update_status == "ended":  
            return batch_update
        
        time.sleep(polling_interval)

# Monitor our batch
batch_result = monitor_batch(response.id) 
print("\nBatch processing complete!")
print("\nRequest counts:")
print(f"  Succeeded: {batch_result.request_counts.succeeded}")
print(f"  Errored: {batch_result.request_counts.errored}")
print(f"  Processing: {batch_result.request_counts.processing}")
print(f"  Canceled: {batch_result.request_counts.canceled}")
print(f"  Expired: {batch_result.request_counts.expired}")

----------------------------------------

TITLE: Configuring Tool Schema
DESCRIPTION: Definition of the save_note tool schema with input validation requirements

LANGUAGE: python
CODE:
tools = [
    {
        "name": "save_note",
        "description": "A tool that saves a note with the author and metadata.",
        "input_schema": {
            "type": "object",
            "properties": {
                "note": {
                    "type": "string",
                    "description": "The content of the note to be saved."
                },
                "author": {
                    "type": "object",
                    "properties": {
                        "name": {
                            "type": "string",
                            "description": "The name of the author."
                        },
                        "email": {
                            "type": "string",
                            "format": "email",
                            "description": "The email address of the author."
                        }
                    },
                    "required": ["name", "email"]
                },
                "priority": {
                    "type": "integer",
                    "minimum": 1,
                    "maximum": 5,
                    "default": 3,
                    "description": "The priority level of the note (1-5)."
                },
                "is_public": {
                    "type": "boolean",
                    "default": false,
                    "description": "Indicates whether the note is publicly accessible."
                }
            },
            "required": ["note", "author"]
        }
    }
]

----------------------------------------

TITLE: XML Tag Based JSON Extraction
DESCRIPTION: Function to extract multiple JSON objects wrapped in XML-style tags from Claude's response

LANGUAGE: python
CODE:
def extract_between_tags(tag: str, string: str, strip: bool = False) -> list[str]:
    ext_list = re.findall(f"<{tag}>(.+?)</{tag}>", string, re.DOTALL)
    if strip:
        ext_list = [e.strip() for e in ext_list]
    return ext_list

----------------------------------------

TITLE: API Completion Helper Function
DESCRIPTION: Defines a helper function to send requests to the Claude API and retrieve responses

LANGUAGE: python
CODE:
def get_completion(client, messages):
    return client.messages.create(
        model=MODEL_NAME,
        max_tokens=2048,
        messages=messages
    ).content[0].text

----------------------------------------

TITLE: Implementing Calculator Tool
DESCRIPTION: Defines a calculator function that can evaluate mathematical expressions and sets up the tool configuration for Claude

LANGUAGE: python
CODE:
import re

def calculate(expression):
    # Remove any non-digit or non-operator characters from the expression
    expression = re.sub(r'[^0-9+\-*/().]', '', expression)
    
    try:
        # Evaluate the expression using the built-in eval() function
        result = eval(expression)
        return str(result)
    except (SyntaxError, ZeroDivisionError, NameError, TypeError, OverflowError):
        return "Error: Invalid expression"

tools = [
    {
        "name": "calculator",
        "description": "A simple calculator that performs basic arithmetic operations.",
        "input_schema": {
            "type": "object",
            "properties": {
                "expression": {
                    "type": "string",
                    "description": "The mathematical expression to evaluate (e.g., '2 + 3 * 4')."
                }
            },
            "required": ["expression"]
        }
    }
]

----------------------------------------

TITLE: Example Usage: MinStack Implementation with Evaluator-Optimizer Loop
DESCRIPTION: Demonstrates the workflow in action by implementing a MinStack class with O(1) operations. Shows how the system generates code, receives feedback, and iteratively improves the implementation based on evaluation criteria.

LANGUAGE: python
CODE:
evaluator_prompt = """
Evaluate this following code implementation for:
1. code correctness
2. time complexity
3. style and best practices

You should be evaluating only and not attemping to solve the task.
Only output "PASS" if all criteria are met and you have no further suggestions for improvements.
Output your evaluation concisely in the following format.

<evaluation>PASS, NEEDS_IMPROVEMENT, or FAIL</evaluation>
<feedback>
What needs improvement and why.
</feedback>
"""

generator_prompt = """
Your goal is to complete the task based on <user input>. If there are feedback 
from your previous generations, you should reflect on them to improve your solution

Output your answer concisely in the following format: 

<thoughts>
[Your understanding of the task and feedback and how you plan to improve]
</thoughts>

<response>
[Your code implementation here]
</response>
"""

task = """
<user input>
Implement a Stack with:
1. push(x)
2. pop()
3. getMin()
All operations should be O(1).
</user input>
"""

loop(task, evaluator_prompt, generator_prompt)

----------------------------------------

TITLE: Processing Plain Text Documents for Citations
DESCRIPTION: Reads help center articles from a directory, creates a list of documents, and sends a request to the Anthropic API for citation-enabled responses.

LANGUAGE: python
CODE:
# Read all help center articles and create a list of documents
articles_dir = './data/help_center_articles'
documents = []

for filename in sorted(os.listdir(articles_dir)):
    if filename.endswith('.txt'):
        with open(os.path.join(articles_dir, filename), 'r') as f:
            content = f.read()
            # Split into title and body
            title_line, body = content.split('\n', 1)
            title = title_line.replace('title: ', '')
            documents.append({
                "type": "document",
                "source": {
                    "type": "text",
                    "media_type": "text/plain",
                    "data": body
                },
                "title": title,
                "citations": {"enabled": True}
            })

QUESTION = "I just checked out, where is my order tracking number? Track package is not available on the website yet for my order."

# Add the question to the content
content = documents 

response = client.messages.create(
    model="claude-3-5-sonnet-latest",
    temperature=0.0,
    max_tokens=1024,
    system='You are a customer support bot working for PetWorld. Your task is to provide short, helpful answers to user questions. Since you are in a chat interface avoid providing extra details. You will be given access to PetWorld\'s help center articles to help you answer questions.',
    messages=[
        {
            "role": "user",
            "content": documents
        },
        {
            "role": "user",
            "content": [{"type": "text", "text": f'Here is the user\'s question: {QUESTION}'}]
        },

    ]
)

----------------------------------------

TITLE: Defining Calculator Tools for ReAct Agent
DESCRIPTION: Create simple calculator tools (multiply and add) to be used by the ReAct Agent

LANGUAGE: python
CODE:
def multiply(a: int, b: int) -> int:
    """Multiply two integers and returns the result integer"""
    return a * b

def add(a: int, b: int) -> int:
    """Add two integers and returns the result integer"""
    return a + b

add_tool = FunctionTool.from_defaults(fn=add)
multiply_tool = FunctionTool.from_defaults(fn=multiply)

----------------------------------------

TITLE: Handling Common Error Cases with Extended Thinking
DESCRIPTION: Demonstrates common error cases when using extended thinking, including setting a too small thinking budget, using incompatible features, and exceeding the context window.

LANGUAGE: python
CODE:
def demonstrate_common_errors():
    # 1. Error from setting thinking budget too small
    try:
        response = client.messages.create(
            model="claude-3-7-sonnet-20250219",
            max_tokens=4000,
            thinking={
                "type": "enabled",
                "budget_tokens": 500  # Too small, minimum is 1024
            },
            messages=[{
                "role": "user",
                "content": "Explain quantum computing."
            }]
        )
    except Exception as e:
        print(f"\nError with too small thinking budget: {e}")
    
    # 2. Error from using temperature with thinking
    try:
        response = client.messages.create(
            model="claude-3-7-sonnet-20250219",
            max_tokens=4000,
            temperature=0.7,  # Not compatible with thinking
            thinking={
                "type": "enabled",
                "budget_tokens": 2000
            },
            messages=[{
                "role": "user",
                "content": "Write a creative story."
            }]
        )
    except Exception as e:
        print(f"\nError with temperature and thinking: {e}")
    
    # 3. Error from exceeding context window
    try:
        # Create a very large prompt
        long_content = "Please analyze this text. " + "This is sample text. " * 150000
        
        response = client.messages.create(
            model="claude-3-7-sonnet-20250219",
            max_tokens=20000,  # This plus the long prompt will exceed context window
            thinking={
                "type": "enabled",
                "budget_tokens": 10000
            },
            messages=[{
                "role": "user",
                "content": long_content
            }]
        )
    except Exception as e:
        print(f"\nError from exceeding context window: {e}")

# Run the common error examples
demonstrate_common_errors()

----------------------------------------

TITLE: Anthropic Interview Question Generation
DESCRIPTION: Uses Anthropic's Claude model to generate interview questions based on the transcribed text. Includes transcript processing and API integration.

LANGUAGE: python
CODE:
import anthropic
import json

transcription_file = "transcript.json"

def get_transcript(transcription_file):
    with open(transcription_file, "r") as file:
        data = json.load(file)
        result = data['results']['channels'][0]['alternatives'][0]['transcript']
        return result

message_text = get_transcript(transcription_file)

client = anthropic.Anthropic(api_key="🔑🔑🔑 Your API Key here! 🔑🔑🔑")

formatted_messages = [
    {
        "role": "user",
        "content": message_text
    }
]

response = client.messages.create(
    model="claude-3-opus-20240229",
    max_tokens=1000,
    temperature=0.5,
    system="Your task is to generate a series of thoughtful, open-ended questions for an interview based on the given context. The questions should be designed to elicit insightful and detailed responses from the interviewee, allowing them to showcase their knowledge, experience, and critical thinking skills. Avoid yes/no questions or those with obvious answers. Instead, focus on questions that encourage reflection, self-assessment, and the sharing of specific examples or anecdotes.",
    messages=formatted_messages
)

content = ''.join(block.text for block in response.content)
parts = content.split('\n\n')

for part in parts:
    print(part)
    print('\n')

----------------------------------------

TITLE: Extracting Text from PDF
DESCRIPTION: Defines functions to extract text from a PDF file, clean the text, and prepare it for input to the language model.

LANGUAGE: Python
CODE:
import pypdf
import re

pdf_path = "data/Sample Sublease Agreement.pdf"

def extract_text_from_pdf(pdf_path):
    with open(pdf_path, 'rb') as file:
        reader = pypdf.PdfReader(file)
        text = ""
        for page in reader.pages:
            text += page.extract_text() + "\n"
    return text

def clean_text(text):
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text)
    # Remove page numbers
    text = re.sub(r'\n\s*\d+\s*\n', '\n', text)
    return text.strip()

def prepare_for_llm(text, max_tokens=180000):
    # Truncate text to fit within token limit (approximate)
    return text[:max_tokens * 4]  # Assuming average of 4 characters per token

def get_llm_text(path):
    extracted_text = extract_text_from_pdf(path)
    cleaned_text = clean_text(extracted_text)
    llm_ready_text = prepare_for_llm(cleaned_text)
    return llm_ready_text

# You can now use get_llm_text in your LLM prompt
text = get_llm_text(pdf_path)
print(text[:500])

----------------------------------------

TITLE: Streaming with Extended Thinking
DESCRIPTION: Shows how to handle streaming responses with extended thinking enabled.

LANGUAGE: python
CODE:
def streaming_with_thinking():
    with client.messages.stream(
        model="claude-3-7-sonnet-20250219",
        max_tokens=4000,
        thinking={
            "type": "enabled",
            "budget_tokens": 2000
        },
        messages=[{
            "role": "user",
            "content": "Solve this puzzle: Three people check into a hotel. They pay $30 to the manager. The manager finds out that the room only costs $25 so he gives $5 to the bellboy to return to the three people. The bellboy, however, decides to keep $2 and gives $1 back to each person. Now, each person paid $10 and got back $1, so they paid $9 each, totaling $27. The bellboy kept $2, which makes $29. Where is the missing $1?"
        }]
    ) as stream:
        # Track what we're currently building
        current_block_type = None
        current_content = ""
        
        for event in stream:
            if event.type == "content_block_start":
                current_block_type = event.content_block.type
                print(f"\n--- Starting {current_block_type} block ---")
                current_content = ""
                
            elif event.type == "content_block_delta":
                if event.delta.type == "thinking_delta":
                    print(event.delta.thinking, end="", flush=True)  # Just print dots for thinking to avoid clutter
                    current_content += event.delta.thinking
                elif event.delta.type == "text_delta":
                    print(event.delta.text, end="", flush=True)
                    current_content += event.delta.text
                    
            elif event.type == "content_block_stop":
                if current_block_type == "thinking":
                    # Just show a summary for thinking
                    print(f"\n[Completed thinking block, {len(current_content)} characters]")
                elif current_block_type == "redacted_thinking":
                    print("\n[Redacted thinking block]")
                print(f"--- Finished {current_block_type} block ---\n")
                current_block_type = None
                
            elif event.type == "message_stop":
                print("\n--- Message complete ---")

streaming_with_thinking()

----------------------------------------

TITLE: Semantic Search Implementation Example
DESCRIPTION: Complete example demonstrating semantic search using document embeddings and query matching

LANGUAGE: python
CODE:
import voyageai
import numpy as np

vo = voyageai.Client()

# Embed the documents
doc_embds = vo.embed(
    documents, model="voyage-2", input_type="document"
).embeddings

query = "When is Apple's conference call scheduled?"

# Embed the query
query_embd = vo.embed(
    [query], model="voyage-2", input_type="query"
).embeddings[0]

# Compute the similarity
similarities = np.dot(doc_embds, query_embd)

retrieved_id = np.argmax(similarities)
print(documents[retrieved_id])

----------------------------------------

TITLE: Loading Parquet Files for Tech News Dataset
DESCRIPTION: Loads a subset of the tech-news-embeddings dataset from Hugging Face into a single DataFrame using the previously defined function.

LANGUAGE: python
CODE:
parquet_files = [
    "https://huggingface.co/api/datasets/AIatMongoDB/tech-news-embeddings/parquet/default/train/0000.parquet",
]

hf_token = userdata.get("HF_TOKEN")
combined_df = download_and_combine_parquet_files(parquet_files, hf_token)

----------------------------------------

TITLE: Token Counting with Extended Thinking
DESCRIPTION: Demonstrates how to track token usage when using extended thinking.

LANGUAGE: python
CODE:
def token_counting_example():
    # Define a function to create a sample prompt
    def create_sample_messages():
        messages = [{
            "role": "user",
            "content": "Solve this puzzle: Three people check into a hotel. They pay $30 to the manager. The manager finds out that the room only costs $25 so he gives $5 to the bellboy to return to the three people. The bellboy, however, decides to keep $2 and gives $1 back to each person. Now, each person paid $10 and got back $1, so they paid $9 each, totaling $27. The bellboy kept $2, which makes $29. Where is the missing $1?"
        }]
        return messages
    
    # Count tokens without thinking
    base_messages = create_sample_messages()
    base_token_count = count_tokens(base_messages)
    print(f"Base token count (input only): {base_token_count}")
    
    # Make a request with thinking and check actual usage
    response = client.messages.create(
        model="claude-3-7-sonnet-20250219",
        max_tokens=8000,
        thinking = {
            "type": "enabled",
            "budget_tokens": 2000
        },
        messages=base_messages
    )
    
    # Calculate and print token usage stats
    thinking_tokens = sum(
        len(block.thinking.split()) * 1.3  # Rough estimate
        for block in response.content 
        if block.type == "thinking"
    )
    
    final_answer_tokens = sum(
        len(block.text.split()) * 1.3  # Rough estimate
        for block in response.content 
        if block.type == "text"
    )
    
    print(f"\nEstimated thinking tokens used: ~{int(thinking_tokens)}")
    print(f"Estimated final answer tokens: ~{int(final_answer_tokens)}")
    print(f"Total estimated output tokens: ~{int(thinking_tokens + final_answer_tokens)}")
    print(f"Input tokens + max_tokens = {base_token_count + 8000}")
    print(f"Available for final answer after thinking: ~{8000 - int(thinking_tokens)}")
    
    # Demo with escalating thinking budgets
    thinking_budgets = [1024, 2000, 4000, 8000, 16000, 32000]
    context_window = 200000
    for budget in thinking_budgets:
        print(f"\nWith thinking budget of {budget} tokens:")
        print(f"Input tokens: {base_token_count}")
        print(f"Max tokens needed: {base_token_count + budget + 1000}")  # Add 1000 for final answer
        print(f"Remaining context window: {context_window - (base_token_count + budget + 1000)}")
        
        if base_token_count + budget + 1000 > context_window:
            print("WARNING: This would exceed the context window of 200k tokens!")

# Uncomment to run the example
token_counting_example()

----------------------------------------

TITLE: Making Queries with Claude 3.7 Sonnet and Processing Results in Python
DESCRIPTION: Defines a function to make queries to Claude 3.7 Sonnet, process the responses, and print the results. It handles both text and tool use blocks in the response.

LANGUAGE: python
CODE:
def make_query_and_print_result(messages, tools=None):
    response = client.messages.create(
        model=MODEL_NAME,
        messages=messages,
        max_tokens=1000,
        tool_choice={"type": "auto"},
        tools=tools or [weather_tool, time_tool],
    )

    for block in response.content:
        match block.type:
            case "text":
                print(block.text)
            case "tool_use":
                print(f"Tool: {block.name}({block.input})")
            case _:
                raise ValueError(f"Unexpected block type: {block.type}")

    return response


MESSAGES = [
    {"role": "user", "content": "What's the weather and time in San Francisco?"}
]

response = make_query_and_print_result(MESSAGES)

----------------------------------------

TITLE: Importing LlamaIndex Core Components for RAG Pipeline
DESCRIPTION: Imports VectorStoreIndex and SimpleDirectoryReader classes from llama_index.core for data loading and indexing.

LANGUAGE: python
CODE:
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
)

----------------------------------------

TITLE: Handling Redacted Thinking Blocks
DESCRIPTION: Demonstrates how to work with redacted thinking blocks using a special test string that triggers them.

LANGUAGE: python
CODE:
def redacted_thinking_example():
    # Using the special test string that triggers redacted thinking
    response = client.messages.create(
        model="claude-3-7-sonnet-20250219",
        max_tokens=4000,
        thinking={
            "type": "enabled",
            "budget_tokens": 2000
        },
        messages=[{
            "role": "user",
            "content": "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
        }]
    )
    
    # Identify redacted thinking blocks
    redacted_blocks = [block for block in response.content if block.type == "redacted_thinking"]
    thinking_blocks = [block for block in response.content if block.type == "thinking"]
    text_blocks = [block for block in response.content if block.type == "text"]
    print(response.content)
    print(f"Response includes {len(response.content)} total blocks:")
    print(f"- {len(redacted_blocks)} redacted thinking blocks")
    print(f"- {len(thinking_blocks)} regular thinking blocks")
    print(f"- {len(text_blocks)} text blocks")
    
    # Show data properties of redacted blocks
    if redacted_blocks:
        print(f"\nRedacted thinking blocks contain encrypted data:")
        for i, block in enumerate(redacted_blocks[:3]):  # Show first 3 at most
            print(f"Block {i+1} data preview: {block.data[:50]}...")
    
    # Print the final text output
    if text_blocks:
        print(f"\nFinal text response:")
        print(text_blocks[0].text)

# Uncomment to run the example
redacted_thinking_example()

----------------------------------------

TITLE: Constructing Claude's Answer Prompt
DESCRIPTION: Constructs the final prompt for Claude to answer the user's question based on the search results.

LANGUAGE: Python
CODE:
ANSWER_QUESTION = f"""\n\nHuman: I have provided you with the following search results:
{formatted_search_results}

Please answer the user's question using only information from the search results. Reference the relevant search result urls within your answer as links. Keep your answer concise.

User's question: {USER_QUESTION} \n\nAssistant:
"""
print(ANSWER_QUESTION)

----------------------------------------

TITLE: Getting Completions from Claude for Open-ended Questions
DESCRIPTION: This code defines a function to get completions from Claude and then uses it to generate outputs for each open-ended question in the evaluation set.

LANGUAGE: python
CODE:
def get_completion(messages):
    response = client.messages.create(
        model=MODEL_NAME,
        max_tokens=2048,
        messages=messages
    )
    return response.content[0].text

outputs = [get_completion(build_input_prompt(question['question'])) for question in eval]

for output, question in zip(outputs, eval):
    print(f"Question: {question['question']}\nGolden Answer: {question['golden_answer']}\nOutput: {output}\n")

----------------------------------------

TITLE: Implementing Vector Search for MongoDB
DESCRIPTION: Defines a function to perform vector search in the MongoDB collection based on user queries, using the VoyageAI embeddings.

LANGUAGE: python
CODE:
def vector_search(user_query, collection):
    """
    Perform a vector search in the MongoDB collection based on the user query.

    Args:
    user_query (str): The user's query string.
    collection (MongoCollection): The MongoDB collection to search.

    Returns:
    list: A list of matching documents.
    """

    # Generate embedding for the user query
    query_embedding = get_embedding(user_query)

    if query_embedding is None:
        return "Invalid query or embedding generation failed."

    # Define the vector search pipeline
    pipeline = [
        {
            "$vectorSearch": {
                "index": "vector_index",
                "queryVector": query_embedding,
                "path": "embedding",
                "numCandidates": 150,  # Number of candidate matches to consider
                "limit": 5  # Return top 5 matches
            }
        },
        {
            "$project": {
                "_id": 0,  # Exclude the _id field
                "embedding": 0,  # Exclude the embedding field
                "score": {
                    "$meta": "vectorSearchScore"  # Include the search score
                }
            }
        }
    ]

    # Execute the search
    results = collection.aggregate(pipeline)
    return list(results)

----------------------------------------

TITLE: Defining Abstract Search Classes
DESCRIPTION: Implements abstract base classes for search functionality including SearchResult and SearchTool with methods for executing and formatting search results

LANGUAGE: python
CODE:
from dataclasses import dataclass
from abc import ABC, abstractmethod
import wikipedia, re
from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT
from typing import Tuple, Optional

@dataclass
class SearchResult:
    """
    A single search result.
    """
    content: str

class SearchTool:
    """ 
    A search tool that can run a query and return a formatted string of search results.
    """

    def __init__():
        pass

    @abstractmethod
    def raw_search(self, query: str, n_search_results_to_use: int) -> list[SearchResult]:
        raise NotImplementedError()
    
    @abstractmethod
    def process_raw_search_results(self, results: list[SearchResult]) -> list[str]:
        raise NotImplementedError()
    
    def search_results_to_string(self, extracted: list[str]) -> str:
        result = "\n".join(
            [
                f'<item index="{i+1}">\n<page_content>\n{r}\n</page_content>\n</item>'
                for i, r in enumerate(extracted)
            ]
        )
        return result

    def wrap_search_results(self, extracted: list[str]) -> str:
        return f"\n<search_results>\n{self.search_results_to_string(extracted)}\n</search_results>"
    
    def search(self, query: str, n_search_results_to_use: int) -> str:
        raw_search_results = self.raw_search(query, n_search_results_to_use)
        processed_search_results = self.process_raw_search_results(raw_search_results)
        displayable_search_results = self.wrap_search_results(processed_search_results)
        return displayable_search_results

----------------------------------------

TITLE: Processing PDF Documents for Citations
DESCRIPTION: Reads a PDF file, encodes it, and sends a request to the Anthropic API for citation-enabled responses.

LANGUAGE: python
CODE:
import base64
import json

# Read and encode the PDF
pdf_path = 'data/Constitutional AI.pdf'
with open(pdf_path, "rb") as f:
    pdf_data = base64.b64encode(f.read()).decode()

pdf_response = client.messages.create(
    model="claude-3-5-sonnet-latest",
    temperature=0.0,
    max_tokens=1024,
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "document",
                    "source": {
                        "type": "base64",
                        "media_type": "application/pdf",
                        "data": pdf_data
                    },
                    "title": "Constitutional AI Paper",
                    "citations": {"enabled": True}
                },
                {
                    "type": "text",
                    "text": "What is the main idea of Constitutional AI?"
                }
            ]
        }
    ]
)

print(visualize_raw_response(pdf_response))
print(visualize_citations(pdf_response))

----------------------------------------

TITLE: Visualizing Evaluation Results
DESCRIPTION: Loads evaluation results from a CSV file, processes the data, and creates visualizations to compare the performance of different summarization methods and models.

LANGUAGE: Python
CODE:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re

%matplotlib inline
plt.style.use('seaborn')

# Load the data
df = pd.read_csv('data/results.csv')

# Function to extract PASS/FAIL and score
def extract_result(text):
    match = re.search(r'\[(PASS|FAIL)\]\s*\((\d+\.\d+)\)', str(text))
    if match:
        return match.group(1), float(match.group(2))
    return 'UNKNOWN', 0.0

# Apply the extraction to relevant columns
for col in df.columns[2:]:
    df[f'{col}_result'], df[f'{col}_score'] = zip(*df[col].apply(extract_result))

# Prepare data for grouped accuracy score
models = ['3.5 Sonnet', '3.0 Haiku']
prompts = ['basic_summarize', 'guided_legal_summary', 'summarize_long_document']

results = []
for model in models:
    for prompt in prompts:
        col = f'[{model}] prompts.py:{prompt}_result'
        if col in df.columns:
            pass_rate = (df[col] == 'PASS').mean()
            results.append({'Model': model, 'Prompt': prompt, 'Pass Rate': pass_rate})

result_df = pd.DataFrame(results)

# 1. Grouped bar chart for accuracy scores
plt.figure(figsize=(12, 6))
result_pivot = result_df.pivot(index='Prompt', columns='Model', values='Pass Rate')
result_pivot.plot(kind='bar')
plt.title('Pass Rate by Model and Prompt')
plt.ylabel('Pass Rate')
plt.legend(title='Model')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# 2. Box plot of scores
plt.figure(figsize=(8, 8))
score_cols = [col for col in df.columns if col.endswith('_score')]
score_data = df[score_cols].melt()
sns.boxplot(x='variable', y='value', data=score_data)
plt.title('Distribution of Scores')
plt.xticks(rotation=90)
plt.xlabel('Model and Prompt')
plt.ylabel('Score')
plt.tight_layout()
plt.show()

# Display summary statistics
summary_stats = df[[col for col in df.columns if col.endswith('_score')]].describe()
display(summary_stats)

----------------------------------------

TITLE: Implementing Chat Function with Web Search
DESCRIPTION: Creates a function that sends a user query to Claude with the web search tool, using the 'auto' tool choice option.

LANGUAGE: python
CODE:
from datetime import date

def chat_with_web_search(user_query):
    messages = [{"role": "user", "content": user_query}]

    system_prompt=f"""
    Answer as many questions as you can using your existing knowledge.  
    Only search the web for queries that you can not confidently answer.
    Today's date is {date.today().strftime("%B %d %Y")}
    If you think a user's question involves something in the future that hasn't happened yet, use the search tool.
    """

    response = client.messages.create(
        system=system_prompt,
        model=MODEL_NAME,
        messages=messages,
        max_tokens=1000,
        tool_choice={"type": "auto"},
        tools=[web_search_tool]
    )
    last_content_block = response.content[-1]
    if last_content_block.type == "text":
        print("Claude did NOT call a tool")
        print(f"Assistant: {last_content_block.text}")
    elif last_content_block.type == "tool_use":
        print("Claude wants to use a tool")
        print(last_content_block)

----------------------------------------

TITLE: Retrieving and Processing Batch Results
DESCRIPTION: Defines a function to retrieve and process the results of a completed batch, including handling different result types.

LANGUAGE: python
CODE:
def process_results(batch_id):
    # First get the batch status
    batch = client.beta.messages.batches.retrieve(batch_id)
    
    print(f"\nBatch {batch.id} Summary:")
    print(f"Status: {batch.processing_status}")
    print(f"Created: {batch.created_at}")
    print(f"Ended: {batch.ended_at}")
    print(f"Expires: {batch.expires_at}")
    
    if batch.processing_status == "ended":
        print("\nIndividual Results:")
        for result in client.beta.messages.batches.results(batch_id):
            print(f"\nResult for {result.custom_id}:")
            print(f"Status: {result.result.type}")
            
            if result.result.type == "succeeded":
                print(f"Content: {result.result.message.content[0].text[:200]}...")
            elif result.result.type == "errored":
                print("Request errored")
            elif result.result.type == "canceled":
                print("Request was canceled")
            elif result.result.type == "expired":
                print("Request expired")

# Example usage:
batch_status = monitor_batch(response.id)
if batch_status.processing_status == "ended":
    process_results(batch_status.id)

----------------------------------------

TITLE: Generating Embeddings with VoyageAI
DESCRIPTION: Uses the VoyageAI API to generate embeddings for the description field of each document in the dataset.

LANGUAGE: python
CODE:
import voyageai
import time

vo = voyageai.Client(api_key=userdata.get("VOYAGE_API_KEY"))

def get_embedding(text: str) -> list[float]:
    if not text.strip():
      print("Attempted to get embedding for empty text.")
      return []

    embedding = vo.embed(text, model="voyage-large-2", input_type="document")

    return embedding.embeddings[0]

combined_df["embedding"] = combined_df["description"].apply(get_embedding)

combined_df.head()

----------------------------------------

TITLE: Executing Test Query on LlamaIndex RAG Pipeline
DESCRIPTION: Runs a sample query through the RAG pipeline to test its functionality.

LANGUAGE: python
CODE:
response = query_engine.query("What did author do growing up?")

----------------------------------------

TITLE: Invoking Finetuned Claude 3 Haiku Model on Amazon Bedrock
DESCRIPTION: This snippet demonstrates how to invoke the finetuned Claude 3 Haiku model using the Bedrock runtime API. It sets up the request body with the appropriate parameters and sends a sample query to the model.

LANGUAGE: python
CODE:
bedrock = boto3.client('bedrock-runtime', region_name = "us-east-1")
body = json.dumps(
    {
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 1000,
        "system": "JSON Mode: Enabled",
        "messages": [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text":"What is a large language model?"
                    }
                ]
            }
        ]
    }
)
response = bedrock_runtime.invoke_model(
	modelId=provisioned_throughput_arn,
    body=body
)
body = json.loads(response['body'].read().decode('utf-8'))

----------------------------------------

TITLE: Downloading Wikipedia Articles
DESCRIPTION: Downloads Wikipedia content for multiple cities using the Wikipedia API and saves them as text files.

LANGUAGE: python
CODE:
wiki_titles = ["Toronto", "Seattle", "Chicago", "Boston", "Houston"]

from pathlib import Path
import requests

for title in wiki_titles:
    response = requests.get(
        "https://en.wikipedia.org/w/api.php",
        params={
            "action": "query",
            "format": "json",
            "titles": title,
            "prop": "extracts",
            "explaintext": True,
        },
    ).json()
    page = next(iter(response["query"]["pages"].values()))
    wiki_text = page["extract"]

    data_path = Path("data")
    if not data_path.exists():
        Path.mkdir(data_path)

    with open(data_path / f"{title}.txt", "w") as fp:
        fp.write(wiki_text)

----------------------------------------

TITLE: Displaying Query Response from LlamaIndex RAG Pipeline
DESCRIPTION: Prints the response generated by the RAG pipeline for the test query.

LANGUAGE: python
CODE:
print(response)

----------------------------------------

TITLE: Generating Response and Matplotlib Code Using Claude 3 Opus
DESCRIPTION: Uses Claude 3 Opus to generate a response to the user's question and create matplotlib code for visualization.

LANGUAGE: python
CODE:
# Prepare the messages for the powerful model
messages = [
    {
        "role": "user",
        "content": [
            {"type": "text", "text": f"Based on the following extracted information from Apple's earnings releases, please provide a response to the question: {QUESTION}\n\nAlso, please generate Python code using the matplotlib library to accompany your response. Enclose the code within <code> tags.\n\nExtracted Information:\n{extracted_info}"}
        ]
    }
]

# Generate the matplotlib code using the powerful model
response = client.messages.create(
    model="claude-3-opus-20240229",
    max_tokens=4096,
    messages=messages
)

generated_response = response.content[0].text
print("Generated Response:")
print(generated_response)

----------------------------------------

TITLE: Creating Query Engine for LlamaIndex RAG Pipeline
DESCRIPTION: Initializes a query engine from the vector store index, setting the top-k similarity parameter to 3.

LANGUAGE: python
CODE:
query_engine = index.as_query_engine(similarity_top_k=3)

----------------------------------------

TITLE: Preprocessing Tech News Dataset
DESCRIPTION: Removes unnecessary columns from the dataset and limits the number of documents used for the demo due to API rate limits.

LANGUAGE: python
CODE:
# Remove the _id coloum from the intital dataset
combined_df = combined_df.drop(columns=['_id'])

# Remove the initial embedding coloumn as we are going to create new embeddings with VoyageAI embedding model
combined_df = combined_df.drop(columns=['embedding'])

# Limiting the amount of document used to 500 for this demo due to the rate limit on VoyageAI API
# Read more on VoyageAI rate limits: https://docs.voyageai.com/docs/rate-limits
max_documents = 500

if len(combined_df) > max_documents:
    combined_df = combined_df[:max_documents]

----------------------------------------

TITLE: Creating Parallel API Request Handler
DESCRIPTION: Implements async functions to make parallel API requests to Claude while respecting rate limits

LANGUAGE: python
CODE:
def get_completion(client, prompt, max_tokens=3000, model='claude-instant-1.2', temperature=0):
    return client.completions.create(
        prompt=prompt, max_tokens_to_sample=max_tokens, model=model, temperature=temperature, stop_sequences=['\n\nHuman:', '\n\nAssistant:']
    ).completion

async def process_case(limiter, client, prompt, results, output_col_name='completion'):
    async with limiter:
        completion = await trio.to_thread.run_sync(get_completion, client, prompt)
    results.append({'prompt': prompt, output_col_name: completion})

    if len(results) % 10 == 0:
        print(f"{len(results)} test cases processed")

async def get_completions_parallel(client, prompts, output_col_name='completion'):
    async with trio.open_nursery() as nursery:
        limiter = trio.CapacityLimiter(10)
        results = []
        for prompt in prompts:
            nursery.start_soon(process_case, limiter, CLIENT, prompt, results, output_col_name)
    return results

----------------------------------------

TITLE: Loading Data for LlamaIndex RAG Pipeline
DESCRIPTION: Loads the downloaded text data using SimpleDirectoryReader from the specified directory.

LANGUAGE: python
CODE:
documents = SimpleDirectoryReader("./data/paul_graham").load_data()

----------------------------------------

TITLE: Performing Web Searches with Brave API
DESCRIPTION: Uses the Brave search API to perform web searches based on the generated queries.

LANGUAGE: Python
CODE:
import requests
from time import sleep
import json

def get_search_results(search_query : str):
    headers = {"Accept": "application/json", "X-Subscription-Token": BRAVE_API_KEY}
    response = requests.get(
        "https://api.search.brave.com/res/v1/web/search",
        params={"q": search_query,
                "count": 3 # Max number of results to return
                },
        headers=headers,
        timeout=60
    )
    if not response.ok:
        raise Exception(f"HTTP error {response.status_code}")
    sleep(1) # avoid Brave rate limit
    return response.json().get("web", {}).get("results")
queries = json.loads(queries_json)["queries"]

urls_seen = set()
web_search_results = []
for query in queries:
    search_results = get_search_results(query)
    for result in search_results:
        url = result.get("url")
        if not url or url in urls_seen:
            continue
        
        urls_seen.add(url)
        web_search_results.append(result)
        
print(len(web_search_results))

----------------------------------------

TITLE: Defining Evaluation Data for Open-ended Questions
DESCRIPTION: This snippet defines a list of evaluation data for open-ended questions, including the questions and corresponding grading instructions.

LANGUAGE: python
CODE:
eval = [
    {
        "question": 'Please design me a workout for today that features at least 50 reps of pulling leg exercises, at least 50 reps of pulling arm exercises, and ten minutes of core.',
        "golden_answer": 'A correct answer should include a workout plan with 50 or more reps of pulling leg exercises (such as deadlifts, but not such as squats which are a pushing exercise), 50 or more reps of pulling arm exercises (such as rows, but not such as presses which are a pushing exercise), and ten minutes of core workouts. It can but does not have to include stretching or a dynamic warmup, but it cannot include any other meaningful exercises.'
    },
    {
        "question": 'Send Jane an email asking her to meet me in front of the office at 9am to leave for the retreat.',
        "golden_answer": 'A correct answer should decline to send the email since the assistant has no capabilities to send emails. It is okay to suggest a draft of the email, but not to attempt to send the email, call a function that sends the email, or ask for clarifying questions related to sending the email (such as which email address to send it to).'
    },
    {
        "question": 'Who won the super bowl in 2024 and who did they beat?',
        "golden_answer": 'A correct answer states that the Kansas City Chiefs defeated the San Francisco 49ers.'
    }
]

----------------------------------------

TITLE: Setting API Keys
DESCRIPTION: Configure API keys for Anthropic Claude, Pinecone, and Voyage AI services

LANGUAGE: python
CODE:
ANTHROPIC_API_KEY="<YOUR_ANTHROPIC_API_KEY>"
PINECONE_API_KEY="<YOUR_PINECONE_API_KEY>"
VOYAGE_API_KEY="<YOUR_VOYAGE_API_KEY>"

----------------------------------------

TITLE: Fetching Web Page Content
DESCRIPTION: Defines a function to fetch and parse the content of web pages from search results.

LANGUAGE: Python
CODE:
from bs4 import BeautifulSoup

def get_page_content(url : str) -> str:
    html = requests.get(url).text
    soup = BeautifulSoup(html, 'html.parser')
    return soup.get_text(strip=True, separator='\n')

----------------------------------------

TITLE: Processing Custom Content Documents for Citations
DESCRIPTION: Creates custom content documents from help center articles and sends a request to the Anthropic API for citation-enabled responses.

LANGUAGE: python
CODE:
# Read all help center articles and create a list of custom content documents
articles_dir = './data/help_center_articles'
documents = []

for filename in sorted(os.listdir(articles_dir)):
    if filename.endswith('.txt'):
        with open(os.path.join(articles_dir, filename), 'r') as f:
            content = f.read()
            # Split into title and body
            title_line, body = content.split('\n', 1)
            title = title_line.replace('title: ', '')
            
            documents.append({
                "type": "document",
                "source": {
                    "type": "content",
                    "content": [
                        {"type": "text", "text": body}
                    ]
                },
                "title": title,
                "citations": {"enabled": True}
            })

QUESTION = "I just checked out, where is my order tracking number? Track package is not available on the website yet for my order."

custom_content_response = client.messages.create(
    model="claude-3-5-sonnet-latest",
    temperature=0.0,
    max_tokens=1024,
    system='You are a customer support bot working for PetWorld. Your task is to provide short, helpful answers to user questions. Since you are in a chat interface avoid providing extra details. You will be given access to PetWorld\'s help center articles to help you answer questions.',
    messages=[
        {
            "role": "user",
            "content": documents
        },
        {
            "role": "user",
            "content": [{"type": "text", "text": f'Here is the user\'s question: {QUESTION}'}]
        }
    ]
)

print(visualize_raw_response(custom_content_response))
print(visualize_citations(custom_content_response))

----------------------------------------

TITLE: Initializing Anthropic LLM and HuggingFace Embedding Model
DESCRIPTION: Creates instances of the Anthropic Claude 3 Opus LLM and a HuggingFace embedding model (BAAI/bge-base-en-v1.5).

LANGUAGE: python
CODE:
llm = Anthropic(temperature=0.0, model='claude-3-opus-20240229')
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-base-en-v1.5")

----------------------------------------

TITLE: Formatting Search Results for Claude
DESCRIPTION: Formats the search results and page content into a structure suitable for Claude's prompt.

LANGUAGE: Python
CODE:
formatted_search_results = "\n".join(
        [
            f'<item index="{i+1}">\n<source>{result.get("url")}</source>\n<page_content>\n{get_page_content(result.get("url"))}\n</page_content>\n</item>'
            for i, result in enumerate(web_search_results)
        ]
    )
print(formatted_search_results)

----------------------------------------

TITLE: Getting Completions from Claude for Leg Count Task
DESCRIPTION: This code defines a function to get completions from Claude and then uses it to generate outputs for each question in the evaluation set.

LANGUAGE: python
CODE:
def get_completion(messages):
    response = client.messages.create(
        model=MODEL_NAME,
        max_tokens=5,
        messages=messages
    )
    return response.content[0].text

outputs = [get_completion(build_input_prompt(question['animal_statement'])) for question in eval]

for output, question in zip(outputs, eval):
    print(f"Animal Statement: {question['animal_statement']}\nGolden Answer: {question['golden_answer']}\nOutput: {output}\n")

----------------------------------------

TITLE: Constructing API Message with PDF Content
DESCRIPTION: Creates the message structure for the API call including the PDF document and prompt text

LANGUAGE: python
CODE:
prompt = """
Please do the following:
1. Summarize the abstract at a kindergarten reading level. (In <kindergarten_abstract> tags.)
2. Write the Methods section as a recipe from the Moosewood Cookbook. (In <moosewood_methods> tags.)
3. Compose a short poem epistolizing the results in the style of Homer. (In <homer_results> tags.)
"""
messages = [
    {
        "role": 'user',
        "content": [
            {"type": "document", "source": {"type": "base64", "media_type": "application/pdf", "data": base64_string}},
            {"type": "text", "text": prompt}
        ]
    }
]

----------------------------------------

TITLE: Initializing Anthropic Client
DESCRIPTION: Sets up the Anthropic API client and specifies the Claude model to use

LANGUAGE: python
CODE:
from anthropic import Anthropic

client = Anthropic()
MODEL_NAME = "claude-3-opus-20240229"

----------------------------------------

TITLE: Grading Completions for Leg Count Task
DESCRIPTION: This snippet defines a grading function for the leg count task and applies it to the outputs, calculating and printing the final score.

LANGUAGE: python
CODE:
def grade_completion(output, golden_answer):
    return output == golden_answer

grades = [grade_completion(output, question['golden_answer']) for output, question in zip(outputs, eval)]
print(f"Score: {sum(grades)/len(grades)*100}%")

----------------------------------------

TITLE: Creating and Configuring Query Engines
DESCRIPTION: Setting up Summary and Vector indices and their corresponding query engines for different types of queries

LANGUAGE: python
CODE:
from llama_index.core import SummaryIndex, VectorStoreIndex
# Summary Index for summarization questions
summary_index = SummaryIndex.from_documents(documents)

# Vector Index for answering specific context questions
vector_index = VectorStoreIndex.from_documents(documents)

# Summary Index Query Engine
summary_query_engine = summary_index.as_query_engine(
    response_mode="tree_summarize",
    use_async=True,
)

# Vector Index Query Engine
vector_query_engine = vector_index.as_query_engine()

----------------------------------------

TITLE: Configuring LLM and Embedding Models
DESCRIPTION: Initialize Anthropic Claude-3 Opus LLM and HuggingFace embedding model

LANGUAGE: python
CODE:
from llama_index.llms.anthropic import Anthropic
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

llm = Anthropic(temperature=0.0, model='claude-3-opus-20240229')
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-base-en-v1.5")

----------------------------------------

TITLE: Using Context Field in Document
DESCRIPTION: Creates a document with a context field and sends a request to the Anthropic API for citation-enabled responses.

LANGUAGE: python
CODE:
import json

# Create a document with context field
document = {
    "type": "document",
    "source": {
        "type": "text",
        "media_type": "text/plain",
        "data": "PetWorld offers a loyalty program where customers earn 1 point for every dollar spent. Once you accumulate 100 points, you'll receive a $5 reward that can be used on your next purchase. Points expire 12 months after they are earned. You can check your point balance in your account dashboard or by asking customer service."
    },
    "title": "Loyalty Program Details",
    "context": "WARNING: This article has not been updated in 12 months. Content may be out of date. Be sure to inform the user this content may be incorrect after providing guidance.",
    "citations": {"enabled": True}
}

QUESTION = "How does PetWorld's loyalty program work? When do points expire?"

context_response = client.messages.create(
    model="claude-3-5-sonnet-latest",
    temperature=0.0,
    max_tokens=1024,
    messages=[
        {
            "role": "user",
            "content": [
                document,
                {
                    "type": "text",
                    "text": QUESTION
                }
            ]
        }
    ]
)

print(visualize_raw_response(context_response))
print(visualize_citations(context_response))

----------------------------------------

TITLE: Implementing Router Query Engine
DESCRIPTION: Creating the RouterQueryEngine with query engine tools and LLM selector for dynamic query routing

LANGUAGE: python
CODE:
from llama_index.core.tools.query_engine import QueryEngineTool

# Summary Index tool
summary_tool = QueryEngineTool.from_defaults(
    query_engine=summary_query_engine,
    description="Useful for summarization questions related to Paul Graham eassy on What I Worked On.",
)

# Vector Index tool
vector_tool = QueryEngineTool.from_defaults(
    query_engine=vector_query_engine,
    description="Useful for retrieving specific context from Paul Graham essay on What I Worked On.",
)

from llama_index.core.query_engine.router_query_engine import RouterQueryEngine
from llama_index.core.selectors.llm_selectors import LLMSingleSelector, LLMMultiSelector

# Create Router Query Engine
query_engine = RouterQueryEngine(
    selector=LLMSingleSelector.from_defaults(),
    query_engine_tools=[
        summary_tool,
        vector_tool,
    ],
)

----------------------------------------

TITLE: Transcript Reading and Formatting
DESCRIPTION: Reads and formats the transcription JSON file, splitting it into sentences for better readability.

LANGUAGE: python
CODE:
import json
OUTPUT = 'transcript.json'

def print_transcript(transcription_file):
  with open(transcription_file, "r") as file:
        data = json.load(file)
        result = data['results']['channels'][0]['alternatives'][0]['transcript']
        result = result.split('.')
        for sentence in result:
          print(sentence + '.')

print_transcript(OUTPUT)

----------------------------------------

TITLE: Visualizing Formatted Citations
DESCRIPTION: Function to format the API response with numbered citations and a reference list.

LANGUAGE: python
CODE:
def visualize_citations(response):
    """
    Takes a response object and returns a string with numbered citations.
    Example output: "here is the plain text answer [1][2] here is some more text [3]"
    with a list of citations below.
    """
    # Dictionary to store unique citations
    citations_dict = {}
    citation_counter = 1
    
    # Final formatted text
    formatted_text = ""
    citations_list = []

    print("\n" + "="*80 + "\nFormatted response:\n" + "="*80)
    
    for content in response.content:
        if content.type == "text":
            text = content.text
            if hasattr(content, 'citations') and content.citations:
                # Sort citations by their appearance in the text
                def get_sort_key(citation):
                    if hasattr(citation, 'start_char_index'):
                        return citation.start_char_index
                    elif hasattr(citation, 'start_page_number'):
                        return citation.start_page_number
                    elif hasattr(citation, 'start_block_index'):
                        return citation.start_block_index
                    return 0  # fallback

                sorted_citations = sorted(content.citations, key=get_sort_key)
                
                # Process each citation
                for citation in sorted_citations:
                    doc_title = citation.document_title
                    cited_text = citation.cited_text.replace('\n', ' ').replace('\r', ' ')
                    # Remove any multiple spaces that might have been created
                    cited_text = ' '.join(cited_text.split())
                    
                    # Create a unique key for this citation
                    citation_key = f"{doc_title}:{cited_text}"
                    
                    # If this is a new citation, add it to our dictionary
                    if citation_key not in citations_dict:
                        citations_dict[citation_key] = citation_counter
                        citations_list.append(f"[{citation_counter}] \"{cited_text}\" found in \"{doc_title}\"")
                        citation_counter += 1
                    
                    # Add the citation number to the text
                    citation_num = citations_dict[citation_key]
                    text += f" [{citation_num}]"
            
            formatted_text += text
    
    # Combine the formatted text with the citations list
    final_output = formatted_text + "\n\n" + "\n".join(citations_list)
    return final_output

formatted_response = visualize_citations(response)
print(formatted_response)

----------------------------------------

TITLE: Initializing Anthropic Client
DESCRIPTION: Setup of Anthropic client and import of required dependencies

LANGUAGE: python
CODE:
from anthropic import Anthropic
from pydantic import BaseModel, EmailStr, Field
from typing import Optional

client = Anthropic()
MODEL_NAME = "claude-3-opus-20240229"

----------------------------------------

TITLE: Defining Input Prompt Template for Open-ended Questions
DESCRIPTION: This function builds an input prompt template for asking Claude open-ended questions in a general-purpose chat assistant scenario.

LANGUAGE: python
CODE:
def build_input_prompt(question):
    user_content = f"""Please answer the following question:
    <question>{question}</question>"""

    messages = [{'role': 'user', 'content': user_content}]
    return messages

----------------------------------------

TITLE: Loading and Indexing Documents
DESCRIPTION: Loads PDF documents and creates vector store indices for Uber and Lyft 10-K filings

LANGUAGE: python
CODE:
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
lyft_docs = SimpleDirectoryReader(input_files=["lyft_2021.pdf"]).load_data()
uber_docs = SimpleDirectoryReader(input_files=["uber_2021.pdf"]).load_data()

lyft_index = VectorStoreIndex.from_documents(lyft_docs[:100])
uber_index = VectorStoreIndex.from_documents(uber_docs[:100])

----------------------------------------

TITLE: Setting up Environment and API Keys
DESCRIPTION: Configure async support and set up Anthropic API key for LLM access

LANGUAGE: python
CODE:
import nest_asyncio
nest_asyncio.apply()

import os

# Using Anthropic LLM API for LLM
os.environ['ANTHROPIC_API_KEY'] = 'YOUR ANTHROPIC API KEY'

from IPython.display import display, HTML

----------------------------------------

TITLE: Implementing Non-cached API Call
DESCRIPTION: Demonstrates how to make a standard API call without caching enabled

LANGUAGE: python
CODE:
def make_non_cached_api_call():
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "<book>" + book_content + "</book>",
                    "cache_control": {"type": "ephemeral"}
                },
                {
                    "type": "text",
                    "text": "What is the title of this book? Only output the title."
                }
            ]
        }
    ]

    start_time = time.time()
    response = client.messages.create(
        model=MODEL_NAME,
        max_tokens=300,
        messages=messages,
        extra_headers={"anthropic-beta": "prompt-caching-2024-07-31"}
    )
    end_time = time.time()

    return response, end_time - start_time

----------------------------------------

TITLE: Setting Provisioned Throughput ARN for Finetuned Claude 3 Haiku Model
DESCRIPTION: This code sets the ARN (Amazon Resource Name) for the provisioned throughput of the finetuned model. This ARN is required to invoke the model via the Bedrock API.

LANGUAGE: python
CODE:
provisioned_throughput_arn = "YOUR_PROVISIONED_THROUGHPUT_ARN"

----------------------------------------

TITLE: Initializing Anthropic Client and Setting Model Name
DESCRIPTION: Imports the Anthropic library, initializes the client, and sets the model name for Claude 3.

LANGUAGE: python
CODE:
import anthropic

client = anthropic.Client()
MODEL_NAME = "claude-3-opus-20240229"

----------------------------------------

TITLE: Implementing Cached API Call
DESCRIPTION: Shows how to implement prompt caching with the Anthropic API using cache control headers

LANGUAGE: python
CODE:
def make_cached_api_call():
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "<book>" + book_content + "</book>",
                    "cache_control": {"type": "ephemeral"}
                },
                {
                    "type": "text",
                    "text": "What is the title of this book? Only output the title."
                }
            ]
        }
    ]

    start_time = time.time()
    response = client.messages.create(
        model=MODEL_NAME,
        max_tokens=300,
        messages=messages,
        extra_headers={"anthropic-beta": "prompt-caching-2024-07-31"}
    )
    end_time = time.time()

    return response, end_time - start_time

----------------------------------------

TITLE: Downloading and Combining Parquet Files
DESCRIPTION: Defines a function to download Parquet files from provided URLs using a Hugging Face token, and combines them into a single pandas DataFrame.

LANGUAGE: python
CODE:
import os
import requests
from io import BytesIO
import pandas as pd
from google.colab import userdata

def download_and_combine_parquet_files(parquet_file_urls, hf_token):
    """
    Downloads Parquet files from the provided URLs using the given Hugging Face token,
    and returns a combined DataFrame.

    Parameters:
    - parquet_file_urls: List of strings, URLs to the Parquet files.
    - hf_token: String, Hugging Face authorization token.

    Returns:
    - combined_df: A pandas DataFrame containing the combined data from all Parquet files.
    """
    headers = {"Authorization": f"Bearer {hf_token}"}
    all_dataframes = []

    for parquet_file_url in parquet_file_urls:
        response = requests.get(parquet_file_url, headers=headers)
        if response.status_code == 200:
            parquet_bytes = BytesIO(response.content)
            df = pd.read_parquet(parquet_bytes)
            all_dataframes.append(df)
        else:
            print(f"Failed to download Parquet file from {parquet_file_url}: {response.status_code}")

    if all_dataframes:
        combined_df = pd.concat(all_dataframes, ignore_index=True)
        return combined_df
    else:
        print("No dataframes to concatenate.")
        return None

----------------------------------------

TITLE: Initializing Anthropic Client for Claude 3.7 Sonnet in Python
DESCRIPTION: Sets up the Anthropic client and specifies the Claude 3.7 Sonnet model for use in subsequent API calls.

LANGUAGE: python
CODE:
from anthropic import Anthropic

client = Anthropic()
MODEL_NAME = "claude-3-7-sonnet-20250219"

----------------------------------------

TITLE: Processing Slide Deck Narration
DESCRIPTION: Creates a systematic narration of slide deck content using regex pattern matching

LANGUAGE: python
CODE:
import re

# Next we'll parse the response from Claude using regex
pattern = r"<narration>(.*?)</narration>"
match = re.search(pattern, completion.strip(), re.DOTALL)
if match:
    narration = match.group(1)
else:
    raise ValueError("No narration available. Likely due to the model response being truncated.")

----------------------------------------

TITLE: Wikipedia-Specific Search Implementation
DESCRIPTION: Implements WikipediaSearchResult and WikipediaSearchTool classes that provide Wikipedia-specific search functionality including result truncation and error handling

LANGUAGE: python
CODE:
@dataclass
class WikipediaSearchResult(SearchResult):
    title: str
    
class WikipediaSearchTool(SearchTool):

    def __init__(self, truncate_to_n_tokens: Optional[int] = 5000):
        self.truncate_to_n_tokens = truncate_to_n_tokens
        if truncate_to_n_tokens is not None:
            self.tokenizer = Anthropic().get_tokenizer()

    def raw_search(self, query: str, n_search_results_to_use: int) -> list[WikipediaSearchResult]:
        search_results = self._search(query, n_search_results_to_use)
        return search_results
    
    def process_raw_search_results(self, results: list[WikipediaSearchResult]) -> list[str]:
        processed_search_results = [f'Page Title: {result.title.strip()}\nPage Content:\n{self.truncate_page_content(result.content)}' for result in results]
        return processed_search_results

    def truncate_page_content(self, page_content: str) -> str:
        if self.truncate_to_n_tokens is None:
            return page_content.strip()
        else:
            return self.tokenizer.decode(self.tokenizer.encode(page_content).ids[:self.truncate_to_n_tokens]).strip()
        
    def _search(self, query: str, n_search_results_to_use: int) -> list[WikipediaSearchResult]:
        results: list[str] = wikipedia.search(query)
        search_results: list[WikipediaSearchResult] = []
        for result in results:
            if len(search_results) >= n_search_results_to_use:
                break
            try:
                page = wikipedia.page(result)
                print(page.url)
            except:
                continue
            content = page.content
            title = page.title
            search_results.append(WikipediaSearchResult(content=content, title=title))
        return search_results

----------------------------------------

TITLE: Simulating Synthetic Tool Responses for Customer Service Chatbot
DESCRIPTION: Defines functions to simulate responses for get_customer_info, get_order_details, and cancel_order tools using hardcoded data.

LANGUAGE: python
CODE:
def get_customer_info(customer_id):
    # Simulated customer data
    customers = {
        "C1": {"name": "John Doe", "email": "john@example.com", "phone": "123-456-7890"},
        "C2": {"name": "Jane Smith", "email": "jane@example.com", "phone": "987-654-3210"}
    }
    return customers.get(customer_id, "Customer not found")

def get_order_details(order_id):
    # Simulated order data
    orders = {
        "O1": {"id": "O1", "product": "Widget A", "quantity": 2, "price": 19.99, "status": "Shipped"},
        "O2": {"id": "O2", "product": "Gadget B", "quantity": 1, "price": 49.99, "status": "Processing"}
    }
    return orders.get(order_id, "Order not found")

def cancel_order(order_id):
    # Simulated order cancellation
    if order_id in ["O1", "O2"]:
        return True
    else:
        return False

----------------------------------------

TITLE: Creating SQLite Database and Sample Data
DESCRIPTION: Creates a test SQLite database with an employees table and populates it with sample employee data including names, departments, and salaries

LANGUAGE: python
CODE:
# Connect to the test database (or create it if it doesn't exist)
conn = sqlite3.connect("test_db.db")
cursor = conn.cursor()

# Create a sample table
cursor.execute("""
    CREATE TABLE IF NOT EXISTS employees (
        id INTEGER PRIMARY KEY,
        name TEXT,
        department TEXT,
        salary INTEGER
    )
""")

# Insert sample data
sample_data = [
    (1, "John Doe", "Sales", 50000),
    (2, "Jane Smith", "Engineering", 75000),
    (3, "Mike Johnson", "Sales", 60000),
    (4, "Emily Brown", "Engineering", 80000),
    (5, "David Lee", "Marketing", 55000)
]
cursor.executemany("INSERT INTO employees VALUES (?, ?, ?, ?)", sample_data)
conn.commit()

----------------------------------------

TITLE: Getting Claude's Answer to User Question
DESCRIPTION: Uses Claude to generate an answer to the user's question based on the search results.

LANGUAGE: Python
CODE:
print(get_completion(ANSWER_QUESTION))

----------------------------------------

TITLE: Token Counting Example
DESCRIPTION: Shows how to count tokens in a text string before embedding

LANGUAGE: python
CODE:
import voyageai

vo = voyageai.Client()
total_tokens = vo.count_tokens(["Sample text"])

----------------------------------------

TITLE: Displaying Query Results
DESCRIPTION: Executes the SQL query on the database and displays the results

LANGUAGE: python
CODE:
# Execute the SQL query and print the results
results = cursor.execute(sql_query).fetchall()

for row in results:
    print(row)

----------------------------------------

TITLE: Defining Pydantic Models
DESCRIPTION: Definition of Pydantic models for Author, Note, and SaveNoteResponse with validation rules

LANGUAGE: python
CODE:
class Author(BaseModel):
    name: str
    email: EmailStr

class Note(BaseModel):
    note: str
    author: Author
    tags: Optional[list[str]] = None
    priority: int = Field(ge=1, le=5, default=3)
    is_public: bool = False

class SaveNoteResponse(BaseModel):
    success: bool
    message: str

----------------------------------------

TITLE: Viewing Promptfoo Evaluation Results
DESCRIPTION: Command to view the results of the Promptfoo evaluation using the built-in viewer.

LANGUAGE: bash
CODE:
npx promptfoo@latest view

----------------------------------------

TITLE: Configuring Finetuning Job Parameters for Claude 3 Haiku on Bedrock
DESCRIPTION: This snippet sets up the configuration parameters for the finetuning job. It includes job name, model name, AWS role ARN, output path, base model ID, and hyperparameters like epoch count, batch size, and learning rate multiplier.

LANGUAGE: python
CODE:
# Configuration
job_name = "anthropic-finetuning-cookbook-training"
custom_model_name = "anthropic_finetuning_cookbook"
role = "YOUR_AWS_SERVICE_ROLE_ARN"
output_path = f"s3://{bucket_name}/finetuning_example_results/"
base_model_id = "arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0:200k"

# Hyperparameters
epoch_count = 5
batch_size = 4
learning_rate_multiplier = 1.0

----------------------------------------

TITLE: Initializing LLM and Embedding Models
DESCRIPTION: Sets up the Claude-3 Opus LLM and HuggingFace embedding model, and configures global settings.

LANGUAGE: python
CODE:
from llama_index.llms.anthropic import Anthropic
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

llm = Anthropic(temperature=0.0, model='claude-3-opus-20240229')
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-base-en-v1.5")

from llama_index.core import Settings
Settings.llm = llm
Settings.embed_model = embed_model
Settings.chunk_size = 512

----------------------------------------

TITLE: Viewing Evaluation Results
DESCRIPTION: Command to view the evaluation results using Promptfoo's built-in viewer.

LANGUAGE: bash
CODE:
npx promptfoo@latest view

----------------------------------------

TITLE: Checking Finetuning Job Status for Claude 3 Haiku on Bedrock
DESCRIPTION: This snippet retrieves the status of the finetuning job using the job identifier. It can be used to monitor the progress of the finetuning process on Amazon Bedrock.

LANGUAGE: python
CODE:
# Check for the job status
status = bedrock.get_model_customization_job(jobIdentifier=job_name)["status"]

----------------------------------------

TITLE: Populating Pinecone Index with Embedded arXiv Data
DESCRIPTION: Embeds the arXiv document chunks using Voyage AI and uploads them to the Pinecone index along with metadata.

LANGUAGE: python
CODE:
from tqdm.auto import tqdm

# easier to work with dataset as pandas dataframe
data = dataset.to_pandas()

batch_size = 100

for i in tqdm(range(0, len(data), batch_size)):
    i_end = min(len(data), i+batch_size)
    # get batch of data
    batch = data.iloc[i:i_end]
    # generate unique ids for each chunk
    ids = [f"{x['doi']}-{x['chunk-id']}" for i, x in batch.iterrows()]
    # get text to embed
    texts = [x['chunk'] for _, x in batch.iterrows()]
    # embed text
    embeds = embed.embed_documents(texts)
    # get metadata to store in Pinecone
    metadata = [
        {'text': x['chunk'],
         'source': x['source'],
         'title': x['title']} for i, x in batch.iterrows()
    ]
    # add to Pinecone
    index.upsert(vectors=zip(ids, embeds, metadata))

----------------------------------------

TITLE: Setting Up Logging Configuration
DESCRIPTION: Configuration of logging for Jupyter notebook environment, including setting up handlers and log levels

LANGUAGE: python
CODE:
import nest_asyncio

nest_asyncio.apply()

import logging
import sys

logger = logging.getLogger()
logger.setLevel(logging.INFO)

logger.handlers = []

handler = logging.StreamHandler(sys.stdout)
handler.setLevel(logging.INFO)

logger.addHandler(handler)

from IPython.display import display, HTML

----------------------------------------

TITLE: Setting Up Anthropic Client and Helper Functions
DESCRIPTION: Initializes the Anthropic client and defines helper functions for printing responses and counting tokens.

LANGUAGE: python
CODE:
import anthropic
import os

# Set your API key as an environment variable or directly
# os.environ["ANTHROPIC_API_KEY"] = "your-api-key-here"

# Initialize the client
client = anthropic.Anthropic()

# Helper functions
def print_thinking_response(response):
    """Pretty print a message response with thinking blocks."""
    print("\n==== FULL RESPONSE ====")
    for block in response.content:
        if block.type == "thinking":
            print("\n🧠 THINKING BLOCK:")
            # Show truncated thinking for readability
            print(block.thinking[:500] + "..." if len(block.thinking) > 500 else block.thinking)
            print(f"\n[Signature available: {bool(getattr(block, 'signature', None))}]")
            if hasattr(block, 'signature') and block.signature:
                print(f"[Signature (first 50 chars): {block.signature[:50]}...]")
        elif block.type == "redacted_thinking":
            print("\n🔒 REDACTED THINKING BLOCK:")
            print(f"[Data length: {len(block.data) if hasattr(block, 'data') else 'N/A'}]")
        elif block.type == "text":
            print("\n✓ FINAL ANSWER:")
            print(block.text)
    
    print("\n==== END RESPONSE ====")

def count_tokens(messages):
    """Count tokens for a given message list."""
    result = client.messages.count_tokens(
        model="claude-3-7-sonnet-20250219",
        messages=messages
    )
    return result.input_tokens

----------------------------------------

TITLE: Creating ArXiv Search Tool for Agent
DESCRIPTION: Defines a search tool that the agent can use to query the Pinecone index for relevant arXiv paper content based on user queries.

LANGUAGE: python
CODE:
from langchain.agents import tool

@tool
def arxiv_search(query: str) -> str:
    """Use this tool when answering questions about AI, machine learning, data
    science, or other technical questions that may be answered using arXiv
    papers.
    """
    # create query vector
    xq = embed.embed_query(query)
    # perform search
    out = index.query(vector=xq, top_k=5, include_metadata=True)
    # reformat results into string
    results_str = "\n\n".join(
        [x["metadata"]["text"] for x in out["matches"]]
    )
    return results_str

tools = [arxiv_search]

----------------------------------------

TITLE: Importing Anthropic SDK and Initializing Client
DESCRIPTION: Imports the Anthropic SDK and initializes a client for making API calls to Claude.

LANGUAGE: python
CODE:
from anthropic import Anthropic
client = Anthropic()
MODEL_NAME = "claude-3-sonnet-20240229"

----------------------------------------

TITLE: Implementing PDF Download and Conversion Functions
DESCRIPTION: Defines functions to download PDFs and convert them to base64-encoded PNG images for processing by Haiku models.

LANGUAGE: python
CODE:
# Function to download a PDF file from a URL and save it to a specified folder
def download_pdf(url, folder):
    response = requests.get(url)
    if response.status_code == 200:
        file_name = os.path.join(folder, url.split("/")[-1])
        with open(file_name, "wb") as file:
            file.write(response.content)
        return file_name
    else:
        print(f"Failed to download PDF from {url}")
        return None
    
# Define the function to convert a PDF to a list of base64-encoded PNG images
def pdf_to_base64_pngs(pdf_path, quality=75, max_size=(1024, 1024)):
    # Open the PDF file
    doc = fitz.open(pdf_path)

    base64_encoded_pngs = []

    # Iterate through each page of the PDF
    for page_num in range(doc.page_count):
        # Load the page
        page = doc.load_page(page_num)

        # Render the page as a PNG image
        pix = page.get_pixmap(matrix=fitz.Matrix(300/72, 300/72))

        # Convert the pixmap to a PIL Image
        image = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

        # Resize the image if it exceeds the maximum size
        if image.size[0] > max_size[0] or image.size[1] > max_size[1]:
            image.thumbnail(max_size, Image.Resampling.LANCZOS)

        # Convert the PIL Image to base64-encoded PNG
        image_data = io.BytesIO()
        image.save(image_data, format='PNG', optimize=True, quality=quality)
        image_data.seek(0)
        base64_encoded = base64.b64encode(image_data.getvalue()).decode('utf-8')
        base64_encoded_pngs.append(base64_encoded)

    # Close the PDF document
    doc.close()

    return base64_encoded_pngs

# Folder to save the downloaded PDFs
folder = "../images/using_sub_agents"


# Create the directory if it doesn't exist
os.makedirs(folder)

# Download the PDFs concurrently
with ThreadPoolExecutor() as executor:
    pdf_paths = list(executor.map(download_pdf, pdf_urls, [folder] * len(pdf_urls)))

# Remove any None values (failed downloads) from pdf_paths
pdf_paths = [path for path in pdf_paths if path is not None]

----------------------------------------

TITLE: Installing Required Libraries for RAG System
DESCRIPTION: Installs necessary Python libraries including pymongo, datasets, pandas, anthropic, and voyageai for building the RAG system.

LANGUAGE: python
CODE:
!pip install pymongo datasets pandas anthropic voyageai

----------------------------------------

TITLE: Downloading Sample Data for RAG Pipeline
DESCRIPTION: Downloads a sample text file (Paul Graham essay) from GitHub to use as input data for the RAG pipeline.

LANGUAGE: bash
CODE:
!mkdir -p 'data/paul_graham/'
!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'

----------------------------------------

TITLE: Helper Function for Model Completion
DESCRIPTION: Creates a utility function to generate completions from the Claude model with specific parameters

LANGUAGE: python
CODE:
def get_completion(messages):
    response = client.messages.create(
        model=MODEL_NAME,
        max_tokens=8192,
        temperature=0,
        messages=messages
    )
    return response.content[0].text

----------------------------------------

TITLE: Printing Finetuned Claude 3 Haiku Model Response
DESCRIPTION: This code prints the response from the finetuned Claude 3 Haiku model. It extracts the text content from the response body and displays it.

LANGUAGE: python
CODE:
print(body['content'][0]['text'])

----------------------------------------

TITLE: Defining PDF URLs and User Question
DESCRIPTION: Sets up a list of Apple's earnings release PDF URLs and defines the user's question about net sales changes.

LANGUAGE: python
CODE:
# List of Apple's earnings release PDF URLs
pdf_urls = [
    "https://www.apple.com/newsroom/pdfs/fy2023-q4/FY23_Q4_Consolidated_Financial_Statements.pdf",
    "https://www.apple.com/newsroom/pdfs/fy2023-q3/FY23_Q3_Consolidated_Financial_Statements.pdf",
    "https://www.apple.com/newsroom/pdfs/FY23_Q2_Consolidated_Financial_Statements.pdf",
    "https://www.apple.com/newsroom/pdfs/FY23_Q1_Consolidated_Financial_Statements.pdf"
]

# User's question
QUESTION = "How did Apple's net sales change quarter to quarter in the 2023 financial year and what were the key contributors to the changes?"

----------------------------------------

TITLE: Extracting Prompt Template and Variables
DESCRIPTION: This code defines functions to extract the generated prompt template and the variables used in it from the API response.

LANGUAGE: python
CODE:
def extract_between_tags(tag: str, string: str, strip: bool = False) -> list[str]:
    ext_list = re.findall(f"<{tag}>(.+?)</{tag}>", string, re.DOTALL)
    if strip:
        ext_list = [e.strip() for e in ext_list]
    return ext_list

def remove_empty_tags(text):
    return re.sub(r'<(\w+)></\1>$', '', text)

def extract_prompt(metaprompt_response):
    between_tags = extract_between_tags("Instructions", metaprompt_response)[0]
    return remove_empty_tags(remove_empty_tags(between_tags).strip()).strip()

def extract_variables(prompt):
    pattern = r'{([^}]+)}'
    variables = re.findall(pattern, prompt)
    return set(variables)

----------------------------------------

TITLE: Initializing Voyage AI Embeddings and Pinecone Vector Database
DESCRIPTION: Sets up the Voyage AI embedding model and initializes the connection to the Pinecone vector database for storing and querying embeddings.

LANGUAGE: python
CODE:
from langchain_community.embeddings import VoyageEmbeddings

embed = VoyageEmbeddings(
    voyage_api_key=VOYAGE_API_KEY, model="voyage-2"
)

LANGUAGE: python
CODE:
from pinecone import Pinecone

# configure client
pc = Pinecone(api_key=PINECONE_API_KEY)

LANGUAGE: python
CODE:
from pinecone import ServerlessSpec

spec = ServerlessSpec(
    cloud="aws", region="us-west-2"
)

LANGUAGE: python
CODE:
vec = embed.embed_documents(["ello"])
len(vec[0])

LANGUAGE: python
CODE:
import time

index_name = "claude-3-rag"

# check if index already exists (it shouldn't if this is first time)
if index_name not in pc.list_indexes().names():
    # if does not exist, create index
    pc.create_index(
        index_name,
        dimension=len(vec[0]),  # dimensionality of voyage model
        metric='dotproduct',
        spec=spec
    )
    # wait for index to be initialized
    while not pc.describe_index(index_name).status['ready']:
        time.sleep(1)

# connect to index
index = pc.Index(index_name)
time.sleep(1)
# view index stats
index.describe_index_stats()

----------------------------------------

TITLE: Installing Required Packages
DESCRIPTION: Installation of necessary Python packages including llama-index and its Anthropic and HuggingFace integrations

LANGUAGE: bash
CODE:
!pip install llama-index
!pip install llama-index-llms-anthropic
!pip install llama-index-embeddings-huggingface

----------------------------------------

TITLE: Testing Generated Prompt Template with User Input
DESCRIPTION: This code allows users to test the generated prompt template by inputting values for the variables and sending the completed prompt to the Anthropic API.

LANGUAGE: python
CODE:
variable_values = {}
for variable in variables:
    print("Enter value for variable:", variable)
    variable_values[variable] = input()

prompt_with_variables = extracted_prompt_template
for variable in variable_values:
    prompt_with_variables = prompt_with_variables.replace("{" + variable + "}", variable_values[variable])

message = CLIENT.messages.create(
    model="claude-3-haiku-20240307",
    max_tokens=4096,
    messages=[
        {
            "role": "user",
            "content":  prompt_with_variables
        },
    ],
).content[0].text

print("Claude's output on your prompt:\n\n")
pretty_print(message)

----------------------------------------

TITLE: Initializing LLM and Embedding Models
DESCRIPTION: Setting up Anthropic Claude-3 LLM and HuggingFace embedding model instances and configuring global settings

LANGUAGE: python
CODE:
from llama_index.llms.anthropic import Anthropic
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

llm = Anthropic(temperature=0.0, model='claude-3-opus-20240229')
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-base-en-v1.5")

from llama_index.core import Settings
Settings.llm = llm
Settings.embed_model = embed_model
Settings.chunk_size = 512

----------------------------------------

TITLE: Installing Required Dependencies
DESCRIPTION: Install necessary Python packages including llama-index core and integrations for Anthropic and HuggingFace

LANGUAGE: shell
CODE:
!pip install llama-index
!pip install llama-index-llms-anthropic
!pip install llama-index-embeddings-huggingface

----------------------------------------

TITLE: Generating Variable String for Prompt
DESCRIPTION: This code generates a formatted string of variables to be used in the prompt generation process.

LANGUAGE: python
CODE:
variable_string = ""
for variable in VARIABLES:
    variable_string += "\n{" + variable.upper() + "}"
print(variable_string)

----------------------------------------

TITLE: Importing LlamaIndex Models for Anthropic and HuggingFace
DESCRIPTION: Imports the Anthropic LLM and HuggingFace embedding model classes from LlamaIndex.

LANGUAGE: python
CODE:
from llama_index.llms.anthropic import Anthropic
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

----------------------------------------

TITLE: Installing Required Python Libraries
DESCRIPTION: Installs the necessary Python libraries anthropic, requests, and beautifulsoup4 using pip.

LANGUAGE: Python
CODE:
%pip install anthropic requests beautifulsoup4

----------------------------------------

TITLE: Customizing Moderation Guidelines for a Rollercoaster Forum
DESCRIPTION: This code snippet shows how to customize the moderation guidelines for a specific use case (a rollercoaster enthusiast forum) and apply them to moderate post titles.

LANGUAGE: python
CODE:
rollercoaster_guidelines = '''BLOCK CATEGORY:
- Content that is not related to rollercoasters, theme parks, or the amusement industry
- Explicit violence, hate speech, or illegal activities
- Spam, advertisements, or self-promotion

ALLOW CATEGORY:
- Discussions about rollercoaster designs, ride experiences, and park reviews
- Sharing news, rumors, or updates about new rollercoaster projects
- Respectful debates about the best rollercoasters, parks, or ride manufacturers
- Some mild profanity or crude language, as long as it is not directed at individuals
'''

post_titles = [
    "Top 10 Wildest Inversions on Steel Coasters",
    "My Review of the New RMC Raptor Coaster at Cedar Point",
    "Best Places to Buy Cheap Hiking Gear",
    "Rumor: Is Six Flags Planning a Giga Coaster for 2025?",
    "My Thoughts on the Latest Marvel Movie",
]

for title in post_titles:
    classification = moderate_text(title, rollercoaster_guidelines)
    print(f"Title: {title}\nClassification: {classification}\n")

----------------------------------------

TITLE: Setting Up Anthropic API Key for Claude 3 Opus Model
DESCRIPTION: Sets the ANTHROPIC_API_KEY environment variable for authentication with the Anthropic API.

LANGUAGE: python
CODE:
import os
os.environ['ANTHROPIC_API_KEY'] = 'YOUR ANTHROPIC API KEY'

----------------------------------------

TITLE: Setting Up Anthropic Client and Importing Libraries
DESCRIPTION: Imports required libraries and sets up the Anthropic client using an API key. Also defines helper functions for text extraction and processing.

LANGUAGE: Python
CODE:
import os
import re
import anthropic
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from typing import List, Dict, Tuple
import json
import seaborn as sns

# Set up Anthropic client
api_key = 'ANTHROPIC_API_KEY' # Replace ANTHROPIC_API_KEY with your actual API key
client = anthropic.Anthropic(api_key=api_key)

print("Setup complete!")

----------------------------------------

TITLE: Uploading Dataset to Amazon S3 for Claude 3 Haiku Finetuning
DESCRIPTION: This code uploads the prepared dataset to an Amazon S3 bucket. The dataset needs to be available on S3 for the finetuning process. It uses boto3 to interact with the S3 service.

LANGUAGE: python
CODE:
bucket_name = "YOUR_BUCKET_NAME"
s3_path = "json_mode_dataset.jsonl"

s3 = boto3.client('s3')
s3.upload_file(dataset_path, bucket_name, s3_path)

----------------------------------------

TITLE: Defining User Question
DESCRIPTION: Sets the user's question as a variable to be used for generating search queries.

LANGUAGE: Python
CODE:
USER_QUESTION="What show won the Outstanding Drama award at the 2024 Emmys?"

----------------------------------------

TITLE: Installing Required Packages for Summarization
DESCRIPTION: Installs the necessary Python packages for the summarization examples, including anthropic, pypdf, pandas, matplotlib, numpy, rouge-score, nltk, and seaborn.

LANGUAGE: Python
CODE:
!pip install anthropic pypdf pandas matplotlib numpy rouge-score nltk seaborn --quiet

----------------------------------------

TITLE: Visualizing Raw API Response
DESCRIPTION: Function to visualize the raw response from the Anthropic API, including citation data.

LANGUAGE: python
CODE:
def visualize_raw_response(response):
    raw_response = {"content": []}

    print("\n" + "="*80 + "\nRaw response:\n" + "="*80)
    
    for content in response.content:
        if content.type == "text":
            block = {
                "type": "text",
                "text": content.text
            }
            if hasattr(content, 'citations') and content.citations:
                block["citations"] = []
                for citation in content.citations:
                    citation_dict = {
                        "type": citation.type,
                        "cited_text": citation.cited_text,
                        "document_title": citation.document_title,
                    }
                    if citation.type == "page_location":
                        citation_dict.update({
                            "start_page_number": citation.start_page_number,
                            "end_page_number": citation.end_page_number
                        })
                    block["citations"].append(citation_dict)
            raw_response["content"].append(block)
    
    return json.dumps(raw_response, indent=2)

print(visualize_raw_response(response))

----------------------------------------

TITLE: Example Usage of Claude with Wolfram Alpha Tool
DESCRIPTION: Demonstrates how to use the chat_with_claude function to ask questions that require Wolfram Alpha data.

LANGUAGE: python
CODE:
# Example usage
print(chat_with_claude("What are the 5 largest countries in the world by population?"))
print(chat_with_claude("Calculate the square root of 1764."))
print(chat_with_claude("What is the distance between Earth and Mars?"))

----------------------------------------

TITLE: Installing Voyage Python Package
DESCRIPTION: Command to install the voyageai Python package using pip

LANGUAGE: bash
CODE:
pip install -U voyageai

----------------------------------------

TITLE: Initial Story Generation with Claude
DESCRIPTION: Making an initial API call to Claude to generate stories up to the max token limit

LANGUAGE: python
CODE:
import anthropic

client = anthropic.Anthropic(
    api_key="YOUR API KEY HERE",
)
message = client.messages.create(
    model="claude-3-sonnet-20240229",
    max_tokens=4096,
    messages=[
        {
            "role": "user",
            "content": """
Please write five stories. Each should be at least 1000 words. Number the words to make sure you don't lose track. Make each story about a different animal.
Put them in <story_1>, <story_2>, ... tags
"""},

    ]
)

----------------------------------------

TITLE: Installing Required Libraries for LlamaIndex RAG Pipeline
DESCRIPTION: Installs the necessary Python libraries: llama-index, llama-index-llms-anthropic, and llama-index-embeddings-huggingface.

LANGUAGE: bash
CODE:
!pip install llama-index
!pip install llama-index-llms-anthropic
!pip install llama-index-embeddings-huggingface

----------------------------------------

TITLE: Fetching Web Page Content in Python
DESCRIPTION: This code uses the requests library to fetch the content of a web page given its URL. It handles the HTTP response and extracts the page content if successful.

LANGUAGE: python
CODE:
import requests

url = "https://en.wikipedia.org/wiki/96th_Academy_Awards"
response = requests.get(url)

if response.status_code == 200:
    page_content = response.text
else:
    print(f"Failed to fetch the web page. Status code: {response.status_code}")
    exit(1)

----------------------------------------

TITLE: Installing Required Libraries
DESCRIPTION: Install necessary Python packages: anthropic, datasets, pinecone-client, and voyageai

LANGUAGE: python
CODE:
%pip install anthropic datasets pinecone-client voyageai

----------------------------------------

TITLE: Installing Anthropic Python Library
DESCRIPTION: Installs the Anthropic Python library using pip.

LANGUAGE: shell
CODE:
%pip install anthropic

----------------------------------------

TITLE: Preparing Input for Claude API in Python
DESCRIPTION: This code prepares the input for the Claude API by creating a prompt that includes the fetched web page content and instructions for summarization.

LANGUAGE: python
CODE:
prompt = f"<content>{page_content}</content>Please produce a concise summary of the web page content."

messages = [
    {"role": "user", "content": prompt}
]

----------------------------------------

TITLE: Basic Text Embedding with Voyage Python Client
DESCRIPTION: Demonstrates how to create embeddings for multiple text samples using the Voyage Python client

LANGUAGE: python
CODE:
import voyageai

vo = voyageai.Client()
# This will automatically use the environment variable VOYAGE_API_KEY.
# Alternatively, you can use vo = voyageai.Client(api_key="<your secret key>")

texts = ["Sample text 1", "Sample text 2"]

result = vo.embed(texts, model="voyage-2", input_type="document")
print(result.embeddings[0])
print(result.embeddings[1])

----------------------------------------

TITLE: Installing Anthropic Package
DESCRIPTION: Installs the required Anthropic Python package using pip

LANGUAGE: python
CODE:
%pip install anthropic

----------------------------------------

TITLE: Generating Summary with Claude 3 Haiku API in Python
DESCRIPTION: This code calls the Claude 3 Haiku API to generate a summary of the web page content. It specifies the model, maximum tokens, and input messages, then prints the generated summary.

LANGUAGE: python
CODE:
response = client.messages.create(
    model="claude-3-haiku-20240307",
    max_tokens=1024,
    messages=messages
)

summary = response.content[0].text
print(summary)

----------------------------------------

TITLE: Generating Haiku Prompt Using Claude 3 Opus
DESCRIPTION: Uses Claude 3 Opus to generate a specific prompt for Haiku sub-agents based on the user's question.

LANGUAGE: python
CODE:
def generate_haiku_prompt(question):
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": f"Based on the following question, please generate a specific prompt for an LLM sub-agent to extract relevant information from an earning's report PDF. Each sub-agent only has access to a single quarter's earnings report. Output only the prompt and nothing else.\n\nQuestion: {question}"}
            ]
        }
    ]

    response = client.messages.create(
        model="claude-3-opus-20240229",
        max_tokens=2048,
        messages=messages
    )

    return response.content[0].text
    
haiku_prompt = generate_haiku_prompt(QUESTION)
print(haiku_prompt)

----------------------------------------

TITLE: Installing Required Dependencies
DESCRIPTION: Pip installation of required packages anthropic and pydantic with email validation support

LANGUAGE: python
CODE:
%pip install anthropic pydantic 'pydantic[email]'

----------------------------------------

TITLE: Extracting and Executing Matplotlib Code
DESCRIPTION: Extracts the matplotlib code from the generated response and executes it to create a visualization.

LANGUAGE: python
CODE:
# Extract the matplotlib code from the response
# Function to extract the code and non-code parts from the response
def extract_code_and_response(response):
    start_tag = "<code>"
    end_tag = "</code>"
    start_index = response.find(start_tag)
    end_index = response.find(end_tag)
    if start_index != -1 and end_index != -1:
        code = response[start_index + len(start_tag):end_index].strip()
        non_code_response = response[:start_index].strip()
        return code, non_code_response
    else:
        return None, response.strip()

matplotlib_code, non_code_response = extract_code_and_response(generated_response)

print(non_code_response)
if matplotlib_code:

    # Execute the extracted matplotlib code
    exec(matplotlib_code)
else:
    print("No matplotlib code found in the response.")

----------------------------------------

TITLE: Importing Libraries and Setting Up Anthropic API Client
DESCRIPTION: Imports required libraries and initializes the Anthropic API client for using Claude models.

LANGUAGE: python
CODE:
# Import the required libraries
import fitz
import base64
from PIL import Image
import io
from concurrent.futures import ThreadPoolExecutor
from anthropic import Anthropic
import requests
import os

# Set up the Anthropic API client
client = Anthropic()
MODEL_NAME = "claude-3-haiku-20240229"

----------------------------------------

TITLE: Setting Up Anthropic API Client and Importing Libraries
DESCRIPTION: This code sets up the Anthropic API client and imports required libraries for making API requests, parsing HTML, and working with JSON data.

LANGUAGE: python
CODE:
from anthropic import Anthropic
import requests
from bs4 import BeautifulSoup
import json

client = Anthropic()
MODEL_NAME = "claude-3-haiku-20240307"

----------------------------------------

TITLE: Implementing Content Moderation Function with Claude
DESCRIPTION: This code defines a function 'moderate_text' that uses the Anthropic API to classify user-generated text based on provided moderation guidelines. It sends a formatted prompt to Claude and returns the moderation decision.

LANGUAGE: python
CODE:
from anthropic import Anthropic
client = Anthropic()
MODEL_NAME = "claude-3-haiku-20240307"

def moderate_text(user_text, guidelines):
    prompt_template = """
    You are a content moderation expert tasked with categorizing user-generated text based on the following guidelines:

    {guidelines}

    Here is the user-generated text to categorize:
    <user_text>{user_text}</user_text>

    Based on the guidelines above, classify this text as either ALLOW or BLOCK. Return nothing else.
    """

    # Format the prompt with the user text
    prompt = prompt_template.format(user_text=user_text, guidelines=guidelines)

    # Send the prompt to Claude and get the response
    response = client.messages.create(
        model=MODEL_NAME,
        max_tokens=10,
        messages=[{"role": "user", "content": prompt}]
    ).content[0].text

    return response

----------------------------------------

TITLE: Setting up Environment for Wolfram Alpha API with Claude
DESCRIPTION: Installs required libraries, sets up the Anthropic API client, and configures the Wolfram Alpha App ID for API access.

LANGUAGE: python
CODE:
from anthropic import Anthropic
import requests
import urllib.parse
import json

client = Anthropic()

# Replace 'YOUR_APP_ID' with your actual Wolfram Alpha AppID
WOLFRAM_APP_ID = 'YOUR_APP_ID'

----------------------------------------

TITLE: Installing Required Libraries for Claude and Web Scraping
DESCRIPTION: This code snippet installs the necessary Python libraries (anthropic, requests, beautifulsoup4) for using the Anthropic API and web scraping.

LANGUAGE: python
CODE:
%pip install anthropic requests beautifulsoup4

----------------------------------------

TITLE: Running Promptfoo Evaluation
DESCRIPTION: Commands to execute Promptfoo evaluation with default and increased concurrency settings

LANGUAGE: bash
CODE:
npx promptfoo@latest eval

LANGUAGE: bash
CODE:
npx promptfoo@latest eval -j 25

----------------------------------------

TITLE: Setting Anthropic API Key
DESCRIPTION: Setting the Anthropic API key as an environment variable for authentication

LANGUAGE: python
CODE:
import os
os.environ['ANTHROPIC_API_KEY'] = 'YOUR ANTHROPIC API KEY'

----------------------------------------

TITLE: Installing Dependencies with pip
DESCRIPTION: Installs required Python packages including requests, ffmpeg-python, deepgram-sdk, and anthropic.

LANGUAGE: bash
CODE:
! pip install requests ffmpeg-python
! pip install deepgram-sdk --upgrade
! pip install requests
! pip install anthropic

----------------------------------------

TITLE: JSON Extraction Function
DESCRIPTION: Function to extract JSON content from Claude's response by finding content between curly braces

LANGUAGE: python
CODE:
def extract_json(response):
    json_start = response.index("{")
    json_end = response.rfind("}")
    return json.loads(response[json_start:json_end + 1])

----------------------------------------

TITLE: Configuring Anthropic API Key
DESCRIPTION: Sets up the Anthropic API key as an environment variable for authentication.

LANGUAGE: python
CODE:
import os
os.environ['ANTHROPIC_API_KEY'] = 'YOUR ANTHROPIC API KEY'

----------------------------------------

TITLE: Defining Evaluation Data for Leg Count Task
DESCRIPTION: This snippet defines a list of evaluation data for the leg count task, including animal statements and their corresponding golden answers.

LANGUAGE: python
CODE:
eval = [
    {
        "animal_statement": 'The animal is a human.',
        "golden_answer": '2'
    },
        {
        "animal_statement": 'The animal is a snake.',
        "golden_answer": '0'
    },
        {
        "animal_statement": 'The fox lost a leg, but then magically grew back the leg he lost and a mysterious extra leg on top of that.',
        "golden_answer": '5'
    }
]

----------------------------------------

TITLE: Setting Up Logging Configuration
DESCRIPTION: Configures logging for Jupyter notebook environment, including setting up handlers and log levels. Also handles nested event loops using nest_asyncio.

LANGUAGE: python
CODE:
import nest_asyncio

nest_asyncio.apply()

import logging
import sys

logger = logging.getLogger()
logger.setLevel(logging.INFO)

logger.handlers = []

handler = logging.StreamHandler(sys.stdout)
handler.setLevel(logging.INFO)

logger.addHandler(handler)

from IPython.display import display, HTML

----------------------------------------

TITLE: Loading Sample Dataset for Claude 3 Haiku Finetuning
DESCRIPTION: This snippet loads a sample dataset from a JSONL file. The dataset is designed to teach the model to respond to all questions with JSON. It demonstrates the required format for finetuning datasets.

LANGUAGE: python
CODE:
import json

sample_dataset = []
dataset_path = 'datasets/json_mode_dataset.jsonl'
with open(dataset_path, 'r') as f:
    for line in f:
        sample_dataset.append(json.loads(line))

print(json.dumps(sample_dataset[0], indent=2))

----------------------------------------

TITLE: Installing Required Packages
DESCRIPTION: Installs the necessary Python packages: llama-index, llama-index-llms-anthropic, and llama-index-embeddings-huggingface.

LANGUAGE: python
CODE:
!pip install llama-index
!pip install llama-index-llms-anthropic
!pip install llama-index-embeddings-huggingface

----------------------------------------

TITLE: Loading and Encoding PDF Document
DESCRIPTION: Reads a PDF file and encodes it to base64 format for API submission

LANGUAGE: python
CODE:
with open("./documents/cvna_2021_annual_report.pdf", "rb") as pdf_file:
    binary_data = pdf_file.read()
    base_64_encoded_data = base64.b64encode(binary_data)
    base64_string = base_64_encoded_data.decode('utf-8')

----------------------------------------

TITLE: Setting up Anthropic API Key
DESCRIPTION: Configures the Anthropic API key as an environment variable for authentication

LANGUAGE: python
CODE:
import os
os.environ['ANTHROPIC_API_KEY'] = 'YOUR ANTHROPIC API KEY'

----------------------------------------

TITLE: Initializing Anthropic Client with PDF Support
DESCRIPTION: Sets up the Anthropic client with beta headers for PDF support and specifies the Claude model version that supports PDF processing

LANGUAGE: python
CODE:
from anthropic import Anthropic
# While PDF support is in beta, you must pass in the correct beta header
client = Anthropic(default_headers={
    "anthropic-beta": "pdfs-2024-09-25"
  }
)
# For now, only claude-3-5-sonnet-20241022 supports PDFs
MODEL_NAME = "claude-3-5-sonnet-20241022"

----------------------------------------

TITLE: HTTP API Request for Embeddings using cURL
DESCRIPTION: Example of requesting embeddings through Voyage's HTTP API using cURL

LANGUAGE: bash
CODE:
curl https://api.voyageai.com/v1/embeddings \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $VOYAGE_API_KEY" \
  -d '{
    "input": ["Sample text 1", "Sample text 2"],
    "model": "voyage-2"
  }'

----------------------------------------

TITLE: Importing Required Libraries and Initializing Anthropic Client
DESCRIPTION: Imports necessary libraries and initializes the Anthropic client with the specified model.

LANGUAGE: python
CODE:
import anthropic
import time

client = anthropic.Anthropic()
MODEL_NAME = "claude-3-5-sonnet-20241022"

----------------------------------------

TITLE: Installing Required Dependencies
DESCRIPTION: Installs the necessary Python packages: llama-index core, Anthropic integration, and HuggingFace embeddings

LANGUAGE: python
CODE:
!pip install llama-index
!pip install llama-index-llms-anthropic
!pip install llama-index-embeddings-huggingface

----------------------------------------

TITLE: Initializing Anthropic Client
DESCRIPTION: Sets up the Anthropic API client and imports required libraries including SQLite

LANGUAGE: python
CODE:
# Import the required libraries
from anthropic import Anthropic
import sqlite3

# Set up the Anthropic API client
client = Anthropic()
MODEL_NAME = "claude-3-opus-20240229"

----------------------------------------

TITLE: Setting API Keys for Anthropic and Brave
DESCRIPTION: Sets the API keys for Anthropic (Claude) and Brave search as environment variables.

LANGUAGE: Python
CODE:
# Insert your API keys here
ANTHROPIC_API_KEY="<your_anthropic_api_key>"
BRAVE_API_KEY="<your_brave_api_key>"

----------------------------------------

TITLE: Initializing Anthropic Client
DESCRIPTION: Imports required libraries and initializes the Anthropic client using an API key.

LANGUAGE: python
CODE:
import anthropic
import os
import json

ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY")
# ANTHROPIC_API_KEY = "" # Put your API key here!

client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)

----------------------------------------

TITLE: Installing Required Dependencies for RAG Agent
DESCRIPTION: Installs the necessary Python packages for building the RAG agent, including LangChain, Anthropic, Voyage AI, Pinecone, and datasets.

LANGUAGE: bash
CODE:
!pip install -qU \
    langchain==0.1.11 \
    langchain-core==0.1.30 \
    langchain-community==0.0.27 \
    langchain-anthropic==0.1.4 \
    langchainhub==0.1.15 \
    anthropic==0.19.1 \
    voyageai==0.2.1 \
    pinecone-client==3.1.0 \
    datasets==2.16.1

----------------------------------------

TITLE: Initializing Anthropic Client
DESCRIPTION: Sets up the Anthropic client and imports required libraries for the implementation

LANGUAGE: python
CODE:
import anthropic
import time
import requests
from bs4 import BeautifulSoup

client = anthropic.Anthropic()
MODEL_NAME = "claude-3-5-sonnet-20241022"

----------------------------------------

TITLE: Running Promptfoo Evaluation with npx
DESCRIPTION: Command to execute the Promptfoo evaluation using npx, specifying the configuration file and output location.

LANGUAGE: bash
CODE:
npx promptfoo@latest eval -c promptfooconfig.yaml --output ../data/results.csv

----------------------------------------

TITLE: Initializing Anthropic API Client in Python
DESCRIPTION: This code sets up the Anthropic API client and specifies the Claude 3 Haiku model to be used for generating summaries.

LANGUAGE: python
CODE:
# Import the required libraries
from anthropic import Anthropic

# Set up the Anthropic API client
client = Anthropic()
MODEL_NAME = "claude-3-haiku-20240229"

----------------------------------------

TITLE: Setting Anthropic API Key in Bash
DESCRIPTION: Command to set the ANTHROPIC_API_KEY environment variable for authentication with Anthropic's API.

LANGUAGE: bash
CODE:
export ANTHROPIC_API_KEY=YOUR_API_KEY

----------------------------------------

TITLE: Initializing Anthropic Client with PDF Support
DESCRIPTION: Creates an Anthropic client instance with PDF beta support headers and sets the model name

LANGUAGE: python
CODE:
import base64
from anthropic import Anthropic
# While PDF support is in beta, you must pass in the correct beta header
client = Anthropic(default_headers={
    "anthropic-beta": "pdfs-2024-09-25"
  }
)
# For now, only claude-3-5-sonnet-20241022 supports PDFs
MODEL_NAME = "claude-3-5-sonnet-20241022"

----------------------------------------

TITLE: Basic Summarization Function
DESCRIPTION: Implements a basic summarization function using Claude. It prompts the model to summarize the input text in bullet points, focusing on main ideas and key details.

LANGUAGE: Python
CODE:
def basic_summarize(text, max_tokens=1000):

    # Prompt the model to summarize the text
    prompt = f"""Summarize the following text in bullet points. Focus on the main ideas and key details:
    {text}
    """

    response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=max_tokens,
            system="You are a legal analyst known for highly accurate and detailed summaries of legal documents.",
            messages=[
                {
                    "role": "user", 
                    "content": prompt
                },
                {
                    "role": "assistant",
                    "content": "Here is the summary of the legal document: <summary>" 
                }
            ],
            stop_sequences=["</summary>"]
        )

    return response.content[0].text

----------------------------------------

TITLE: Setting Up Environment for Claude 3.7 Sonnet API
DESCRIPTION: Installs the Anthropic package and sets up the environment for using the Claude 3.7 Sonnet API. Defines global variables and helper functions for token counting and response printing.

LANGUAGE: python
CODE:
%pip install anthropic

LANGUAGE: python
CODE:
import anthropic
import os
import json

# Global variables for model and token budgets
MODEL_NAME = "claude-3-7-sonnet-20250219"
MAX_TOKENS = 4000
THINKING_BUDGET_TOKENS = 2000

# Set your API key as an environment variable or directly
# os.environ["ANTHROPIC_API_KEY"] = "your_api_key_here"

# Initialize the client
client = anthropic.Anthropic()

# Helper functions
def print_thinking_response(response):
    """Pretty print a message response with thinking blocks."""
    print("\n==== FULL RESPONSE ====")
    for block in response.content:
        if block.type == "thinking":
            print("\n🧠 THINKING BLOCK:")
            # Show truncated thinking for readability 
            print(block.thinking[:500] + "..." if len(block.thinking) > 500 else block.thinking)
            print(f"\n[Signature available: {bool(getattr(block, 'signature', None))}]")
            if hasattr(block, 'signature') and block.signature:
                print(f"[Signature (first 50 chars): {block.signature[:50]}...]")
        elif block.type == "redacted_thinking":
            print("\n🔒 REDACTED THINKING BLOCK:")
            print(f"[Data length: {len(block.data) if hasattr(block, 'data') else 'N/A'}]")
        elif block.type == "text":
            print("\n✓ FINAL ANSWER:")
            print(block.text)
    
    print("\n==== END RESPONSE ====")

def count_tokens(messages, tools=None):
    """Count tokens for a given message list with optional tools."""
    if tools:
        response = client.messages.count_tokens(
            model=MODEL_NAME,
            messages=messages,
            tools=tools
        )
    else:
        response = client.messages.count_tokens(
            model=MODEL_NAME,
            messages=messages
        )
    return response.input_tokens

----------------------------------------

TITLE: XML Structure for Consent and Notices
DESCRIPTION: XML markup detailing consent requirements and notice procedures for the parties involved.

LANGUAGE: xml
CODE:
<consent_and_notices>
- Landlord's consent: Required and obtained from 1144 Eastlake LLC
- Notice requirements: Written notices to be sent to specified addresses for each party
</consent_and_notices>

----------------------------------------

TITLE: Initializing API Client and Dependencies
DESCRIPTION: Sets up the Anthropic API client and imports required libraries for working with the Claude API and data processing

LANGUAGE: python
CODE:
import anthropic, os, re, requests, trio, pandas as pd
import numpy as np
from bs4 import BeautifulSoup
API_KEY = os.environ['ANTHROPIC_API_KEY']
CLIENT = anthropic.Anthropic(api_key=API_KEY)

----------------------------------------

TITLE: XML Structure for Responsibilities
DESCRIPTION: XML markup defining the responsibilities of parties regarding utilities, maintenance, and operating expenses.

LANGUAGE: xml
CODE:
<responsibilities>
- Utilities: Sublessee responsible for utilities
- Maintenance and repairs: Sublessee responsible for interior, non-structural portions of the Premises
- Operating expenses: Sublessee pays pro-rata share of expenses exceeding base year (2005) costs
</responsibilities>

----------------------------------------

TITLE: Fetching Sample Text Content
DESCRIPTION: Implements a function to fetch and clean text content from Project Gutenberg, used for testing prompt caching

LANGUAGE: python
CODE:
def fetch_article_content(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Remove script and style elements
    for script in soup(["script", "style"]):
        script.decompose()
    
    # Get text
    text = soup.get_text()
    
    # Break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # Break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # Drop blank lines
    text = '\n'.join(chunk for chunk in chunks if chunk)
    
    return text

# Fetch the content of the article
book_url = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt"
book_content = fetch_article_content(book_url)

----------------------------------------

TITLE: XML Structure for Property Details
DESCRIPTION: XML markup describing the property specifications including address, space details, and permitted use.

LANGUAGE: xml
CODE:
<property_details>
- Address: 1144 Eastlake Avenue E, Seattle, Washington 98109
- Description: Approximately 7,128 rentable square feet of space on the 2nd floor (Suite 201) of the Building
- Permitted use: General business office use
</property_details>

----------------------------------------

TITLE: Creating a Complex Batch with Different Message Types
DESCRIPTION: Demonstrates creating a batch with various types of requests, including simple messages, image analysis, system prompts, and multi-turn conversations.

LANGUAGE: python
CODE:
import base64
def create_complex_batch():
    # Get base64 encoded image
    def get_base64_encoded_image(image_path):
        with open(image_path, "rb") as image_file:
            binary_data = image_file.read()
            base_64_encoded_data = base64.b64encode(binary_data)
            base64_string = base_64_encoded_data.decode('utf-8')
            return base64_string

    # Mix of different request types
    batch_requests = [
        {
            "custom_id": "simple-question",
            "params": {
                "model": MODEL_NAME,
                "max_tokens": 1024,
                "messages": [
                    {"role": "user", "content": "What is quantum computing?"}
                ]
            }
        },
        {
            "custom_id": "image-analysis",
            "params": {
                "model": MODEL_NAME,
                "max_tokens": 1024,
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": "image/jpeg",
                                    "data": get_base64_encoded_image("../images/sunset-dawn-nature-mountain-preview.jpg")
                                }
                            },
                            {
                                "type": "text",
                                "text": "Describe this mountain landscape. What time of day does it appear to be, and what weather conditions do you observe?"
                            }
                        ]
                    }
                ]
            }
        },
        {
            "custom_id": "system-prompt",
            "params": {
                "model": MODEL_NAME,
                "max_tokens": 1024,
                "system": "You are a helpful science teacher.",
                "messages": [
                    {"role": "user", "content": "Explain gravity to a 5-year-old."}
                ]
            }
        },
        {
            "custom_id": "multi-turn",
            "params": {
                "model": MODEL_NAME,
                "max_tokens": 1024,
                "messages": [
                    {"role": "user", "content": "What is DNA?"},
                    {"role": "assistant", "content": "DNA is like a blueprint for living things..."},
                    {"role": "user", "content": "How is DNA copied?"}
                ]
            }
        }
    ]
    
    try:
        response = client.beta.messages.batches.create(
            requests=batch_requests
        )
        return response.id
    except Exception as e:
        print(f"Error creating batch: {e}")
        return None
complex_batch_id = create_complex_batch()
print(f"Complex batch ID: {complex_batch_id}")

----------------------------------------

TITLE: XML Structure for Parties Involved
DESCRIPTION: XML markup defining the main parties involved in the sublease agreement including sublessor, sublessee, and original lessor.

LANGUAGE: xml
CODE:
<parties_involved>
- Sublessor: ConocoPhillips Company, a Delaware corporation
- Sublessee: ZymoGenetics, Inc., a Washington corporation
- Original Lessor: 1144 Eastlake LLC, a Washington limited liability company
</parties_involved>

----------------------------------------

TITLE: Skills Directory Structure in Markdown
DESCRIPTION: Markdown structure outlining the available skill guides and their descriptions, including classification, RAG, summarization and text-to-SQL capabilities.

LANGUAGE: markdown
CODE:
# Claude Skills

## Guides

- **[Classification with Claude](./classification/guide.ipynb)**
- **[Retrieval Augmented Generation with Claude](./retrieval_augmented_generation/guide.ipynb)**
- **[Retrieval Augmented Generation with Contextual Embeddings](./contextual-embeddings/guide.ipynb)**
- **[Summarization with Claude](./summarization/guide.ipynb)**
- **[Text-to-SQL with Claude](./text_to_sql/guide.ipynb)**

## Getting Started

To get started with the Skills guides, simply navigate to the desired guide's directory and follow the instructions provided in the `guide.ipynb` file.

----------------------------------------

TITLE: Parking Allocation
DESCRIPTION: Details of included parking spaces and associated rates

LANGUAGE: text
CODE:
- 14 parking stalls in Building
- 3 additional stalls in Building or nearby surface lots
- Initial rate: $75/stall/month (years 1-2)
- Rate increases to $95/stall/month (years 3-5)
- Additional stalls at $125/month with 3% annual increase

----------------------------------------

TITLE: Sublease Agreement Basic Terms
DESCRIPTION: Key terms of the sublease agreement including parties, premises, term length, and rent details

LANGUAGE: text
CODE:
SUBLESSOR: Crown Plaza Executive Suites Corporation
SUBLESSEE: Future Canada China Environment Inc., Jessica Chiang
PREMISES: Suite 437, 4th Floor, Crown Plaza Building, 114 W. Magnolia St, Bellingham WA 98225
TERM: 12 months (March 1, 2008 - February 28, 2009)
BASE RENT: $595.00 per month
SECURITY DEPOSIT: $595.00

----------------------------------------

TITLE: Specifying Rent Amount in Commercial Sublease Agreement
DESCRIPTION: Legal language defining the rent amount and payment terms for the sublease.

LANGUAGE: legal
CODE:
Commencing on the Rent Commencement Date, Sublessee shall pay to Sublessor each month during the term of this Sublease, rent in the amount of Twenty Three Thousand Eight Hundred Twenty Seven and 96/100 Dollars ($23,827.96), in advance, on execution hereof for the first month and on or before the first of each month thereafter ("Base Rent").

----------------------------------------

TITLE: Sublease Financial Terms
DESCRIPTION: Monthly rental rate schedule and square footage calculations for the sublease term

LANGUAGE: text
CODE:
Date/Term                  Monthly Rent         Rate/Sq Ft/Year
10-01-05 to 11-30-05    $0.00               $0.00
12-01-05 to 10-31-06    $13,929.30         $23.45
11-01-06 to 10-31-07    $14,523.30         $24.45
11-01-07 to 10-31-08    $15,117.30         $25.45

----------------------------------------

TITLE: Defining Subleased Premises in Commercial Sublease Agreement
DESCRIPTION: Legal language defining the subleased premises, including location and square footage.

LANGUAGE: legal
CODE:
Sublessor agrees to sublease to Sublessee, and Sublessee agrees to sublease from Sublessor, those certain premises situated in the City of Sunnyvale, County of Santa Clara, State of California, consisting of approximately 45,823 square feet of space known as 475-477 Potrero Avenue, more particularly set forth on Exhibit "A" hereto (the "Subleased Premises").

----------------------------------------

TITLE: XML Structure for Term and Rent Details
DESCRIPTION: XML markup outlining the lease term dates, monthly rent structure, and security deposit requirements.

LANGUAGE: xml
CODE:
<term_and_rent>
- Start date: Later of October 1, 2005, or date Sublessor delivers the Sublease Premises
- End date: October 31, 2008
- Monthly rent: Varies by period (e.g., $13,929.30 from 12/1/05 to 10/31/06)
- Security deposit: None initially; $20,000 may be required if Sublessee defaults
</term_and_rent>

----------------------------------------

TITLE: Key Lease Terms
DESCRIPTION: Base rent schedule and payments terms for the subleased premises

LANGUAGE: text
CODE:
Months		Base Rent per month			Total	
Effective date — 6/30/2012
		$	19,845.00			$	79,380.00	
7/1/2012 — 6/30/2013
		$	19,845.00			$	238,140.00	
7/1/2013 — 6/30/2014
		$	19,845.00			$	238,140.00

----------------------------------------

TITLE: XML Structure for Special Provisions
DESCRIPTION: XML markup listing special provisions including furniture, parking, subletting restrictions, signage, and insurance requirements.

LANGUAGE: xml
CODE:
<special_provisions>
- Furniture: Sublessee has use of existing furniture, fixtures, and equipment at no additional cost
- Parking: Sublessee entitled to use 14 parking stalls in the Building and 3 additional stalls either in the Building or nearby surface lots
- Subletting restrictions: Sublessee may not assign or sublet without prior written consent from both Sublessor and Master Lessor
- Signage: Not specified in the sublease agreement
- Insurance: Sublessee required to maintain comprehensive general liability insurance with $10,000,000 per occurrence limit
</special_provisions>

----------------------------------------

TITLE: Premises Description
DESCRIPTION: Legal description of the subleased premises including square footage and included amenities

LANGUAGE: text
CODE:
The Premises is comprised of 2,000 square feet of the Master Premises, including all furniture in such space and access to file space, printers, copiers, kitchen, conference room facilities and receptionist and secretarial services.

----------------------------------------

TITLE: Specifying Sublease Term in Commercial Agreement
DESCRIPTION: Legal language defining the term of the sublease, including commencement and expiration dates.

LANGUAGE: legal
CODE:
The term of this Sublease shall commence on the date Sublessee fully executes and delivers this Sublease to Sublessor accompanied by the Security Deposit and the Letter of Credit (which Letter of Credit may be delivered within ten (10) days after such full execution) required pursuant to Section 4.2 hereof, and provide copies of all licenses and authorizations that may be required for the lawful operation of Sublessee's business upon the Premises, including, without limitation, any business licenses that may be required by the City of Sunnyvale. This shall be referred to as the "Commencement Date." The Rent shall commence on October 15, 2004 (the "Rent Commencement Date"). The term of this Sublease shall end on June 30, 2006, unless sooner terminated pursuant to any provision of the Master Lease applicable to the Subleased Premises (the "Expiration Date").

----------------------------------------

TITLE: Running Promptfoo Evaluation
DESCRIPTION: Command to execute the evaluation using the configuration file and output results to a CSV file.

LANGUAGE: bash
CODE:
npx promptfoo@latest eval -c promptfooconfig.yaml --output ../data/results.csv

----------------------------------------

TITLE: Fixed Rent Payment Schedule
DESCRIPTION: Monthly and annual fixed rent payment schedule over the 5-year lease term, with annual increases

LANGUAGE: text
CODE:
Months 1-12: $34.50/sqft, $5,750.00 monthly, $69,000.00 annually
Months 13-24: $35.19/sqft, $5,865.00 monthly, $70,380.00 annually
Months 25-36: $35.89/sqft, $5,981.67 monthly, $71,780.00 annually
Months 37-48: $36.61/sqft, $6,101.67 monthly, $73,220.00 annually
Months 49-60: $37.34/sqft, $6,223.33 monthly, $74,680.00 annually

----------------------------------------

TITLE: Setting Anthropic API Key
DESCRIPTION: Command to set the environment variable for Anthropic API authentication.

LANGUAGE: bash
CODE:
export ANTHROPIC_API_KEY=YOUR_API_KEY

----------------------------------------

TITLE: Setting Environment Variables for API Access
DESCRIPTION: Commands to set required API keys for Anthropic and Voyage services as environment variables

LANGUAGE: bash
CODE:
export ANTHROPIC_API_KEY=YOUR_API_KEY
export VOYAGE_API_KEY=YOUR_API_KEY

----------------------------------------

TITLE: Installing Required Python Dependencies
DESCRIPTION: Command to install the necessary Python packages for running custom evaluations with NLTK and ROUGE score metrics.

LANGUAGE: bash
CODE:
pip install nltk rouge-score

----------------------------------------

TITLE: Installing Required Libraries for Financial Analysis
DESCRIPTION: Installs the necessary Python libraries for the financial analysis task using pip.

LANGUAGE: python
CODE:
%pip install anthropic IPython PyMuPDF matplotlib

----------------------------------------

TITLE: Running Promptfoo Evaluation Commands
DESCRIPTION: Commands to execute Promptfoo evaluations for both end-to-end system and retrieval system performance, specifying config files and output locations

LANGUAGE: bash
CODE:
npx promptfoo@latest eval -c promptfooconfig_end_to_end.yaml --output ../data/end_to_end_results.json
npx promptfoo@latest eval -c promptfooconfig_retrieval.yaml --output ../data/retrieval_results.json

----------------------------------------

TITLE: Installing Anthropic Python Library
DESCRIPTION: Installs the Anthropic Python library using pip.

LANGUAGE: bash
CODE:
%pip install anthropic

----------------------------------------

TITLE: Setting Environment Variables for API Keys
DESCRIPTION: Commands to set required API keys as environment variables for Anthropic and Voyage services

LANGUAGE: bash
CODE:
export ANTHROPIC_API_KEY=YOUR_API_KEY
export VOYAGE_API_KEY=YOUR_API_KEY

----------------------------------------

TITLE: Installing Anthropic Python Library
DESCRIPTION: Installs the Anthropic Python library using pip.

LANGUAGE: python
CODE:
%pip install anthropic

----------------------------------------

TITLE: Implementing Sentiment Analysis with Forced Tool Use
DESCRIPTION: Creates a function that forces Claude to use the sentiment analysis tool by setting tool_choice to 'tool'.

LANGUAGE: python
CODE:
def analyze_tweet_sentiment(query):
    response = client.messages.create(
        model=MODEL_NAME,
        max_tokens=4096,
        tools=tools,
        tool_choice={"type": "tool", "name": "print_sentiment_scores"},
        messages=[{"role": "user", "content": query}]
    )
    print(response)

----------------------------------------

TITLE: Installing Anthropic Library
DESCRIPTION: Installs the required Anthropic Python library using pip

LANGUAGE: python
CODE:
%pip install anthropic

----------------------------------------

TITLE: Defining Web Search Tool
DESCRIPTION: Defines a fake web search tool for demonstration purposes, including a function and tool schema.

LANGUAGE: python
CODE:
def web_search(topic):
    print(f"pretending to search the web for {topic}")

web_search_tool = {
    "name": "web_search",
    "description": "A tool to retrieve up to date information on a given topic by searching the web",
    "input_schema": {
        "type": "object",
        "properties": {
            "topic": {
                "type": "string",
                "description": "The topic to search the web for"
            },
        },
        "required": ["topic"]
    }
}

----------------------------------------

TITLE: Installing Anthropic Python Library
DESCRIPTION: Installs the Anthropic Python library using pip.

LANGUAGE: shell
CODE:
!pip install anthropic  --quiet

----------------------------------------

TITLE: Implementing a Batch Tool for Parallel Tool Calls with Claude in Python
DESCRIPTION: Defines a 'batch tool' that allows Claude to invoke multiple tool calls simultaneously, potentially improving overall latency. Includes the tool specification and a function to process batch tool calls.

LANGUAGE: python
CODE:
import json

batch_tool = {
    "name": "batch_tool",
    "description": "Invoke multiple other tool calls simultaneously",
    "input_schema": {
        "type": "object",
        "properties": {
            "invocations": {
                "type": "array",
                "description": "The tool calls to invoke",
                "items": {
                    "types": "object",
                    "properties": {
                        "name": {
                            "types": "string",
                            "description": "The name of the tool to invoke"
                        },
                        "arguments": {
                            "types": "string",
                            "description": "The arguments to the tool"
                        }
                    },
                    "required": ["name", "arguments"]
                }
            }
        },
        "required": ["invocations"]
    }
}

def process_tool_with_maybe_batch(tool_name, tool_input):
    if tool_name == "batch_tool":
        results = []
        for invocation in tool_input["invocations"]:
            results.append(process_tool_call(invocation["name"], json.loads(invocation["arguments"])))
        return '\n'.join(results)
    else:
        return process_tool_call(tool_name, tool_input)

----------------------------------------

TITLE: Installing Anthropic Python Library
DESCRIPTION: This code snippet installs the Anthropic Python library using pip.

LANGUAGE: python
CODE:
%pip install anthropic

----------------------------------------

TITLE: Testing Customer Service Chatbot with Sample Queries
DESCRIPTION: Demonstrates the usage of the chatbot_interaction function with three sample customer service queries.

LANGUAGE: python
CODE:
chatbot_interaction("Can you tell me the email address for customer C1?")
chatbot_interaction("What is the status of order O2?")
chatbot_interaction("Please cancel order O1 for me.")

----------------------------------------

TITLE: Installing Anthropic Python Library
DESCRIPTION: This code snippet shows how to install the Anthropic Python library using pip. The installation command is commented out.

LANGUAGE: python
CODE:
# Install anthropic if necessary
# !pip install anthropic

----------------------------------------

TITLE: Processing Tool Calls for Customer Service Chatbot
DESCRIPTION: Defines a function to process tool calls made by Claude and return appropriate results using the simulated tool functions.

LANGUAGE: python
CODE:
def process_tool_call(tool_name, tool_input):
    if tool_name == "get_customer_info":
        return get_customer_info(tool_input["customer_id"])
    elif tool_name == "get_order_details":
        return get_order_details(tool_input["order_id"])
    elif tool_name == "cancel_order":
        return cancel_order(tool_input["order_id"])

----------------------------------------

TITLE: Installing Anthropic Library in Python
DESCRIPTION: This code snippet installs the Anthropic library using pip within a Jupyter notebook environment.

LANGUAGE: python
CODE:
# Install the necessary libraries
%pip install anthropic

----------------------------------------

TITLE: Setting Voyage API Key Environment Variable
DESCRIPTION: Sets up the VOYAGE_API_KEY environment variable for authentication

LANGUAGE: bash
CODE:
export VOYAGE_API_KEY="<your secret key>"

----------------------------------------

TITLE: Installing Anthropic Client
DESCRIPTION: Installs the Anthropic Python client library using pip

LANGUAGE: python
CODE:
%pip install anthropic

----------------------------------------

TITLE: Continuing Story Generation
DESCRIPTION: Making a second API call using the previous partial response as context to continue the story

LANGUAGE: python
CODE:
message2 = client.messages.create(
    model="claude-3-sonnet-20240229",
    max_tokens=4096,
    messages=[
        {
            "role": "user",
            "content": """
Please write five stories. Each should be at least 1000 words. Number the words to make sure you don't lose track. Make each story about a different animal.
Put them in <story_1>, <story_2>, ... tags
"""},
        {
            "role": "assistant",
            "content": message.content[0].text  # The text of Claude's partially completed message.
        }
    ]
)
print(message2.content[0].text)

----------------------------------------

TITLE: Installing Anthropic Python Client
DESCRIPTION: Installs the Anthropic Python client package using pip

LANGUAGE: python
CODE:
%pip install anthropic

----------------------------------------

TITLE: Printing Stop Reason
DESCRIPTION: Checking if generation stopped due to max tokens limit

LANGUAGE: python
CODE:
print(message.stop_reason)

----------------------------------------

TITLE: Installing Anthropic Python Package
DESCRIPTION: Installs the Anthropic Python package using pip

LANGUAGE: python
CODE:
%pip install anthropic

----------------------------------------

TITLE: Installing Anthropic Python Package
DESCRIPTION: Installing the Anthropic Python client library via pip

LANGUAGE: python
CODE:
%%capture
!pip install anthropic

----------------------------------------

TITLE: Initializing Anthropic Client
DESCRIPTION: Sets up the Anthropic client and specifies the Claude model to use

LANGUAGE: python
CODE:
from anthropic import Anthropic
import json
import re
from pprint import pprint

client = Anthropic()
MODEL_NAME = "claude-3-opus-20240229"

----------------------------------------

TITLE: Generating Prompt Template Using Anthropic API
DESCRIPTION: This code sends a request to the Anthropic API to generate a prompt template based on the given task and variables.

LANGUAGE: python
CODE:
prompt = metaprompt.replace("{{TASK}}", TASK)
assistant_partial = "<Inputs>"
if variable_string:
    assistant_partial += variable_string + "\n</Inputs><Instructions Structure>"

message = CLIENT.messages.create(
    model=MODEL_NAME,
    max_tokens=4096,
    messages=[
        {
            "role": "user",
            "content":  prompt
        },
        {
            "role": "assistant",
            "content": assistant_partial
        }
    ],
    temperature=0
).content[0].text

----------------------------------------

TITLE: Installing Anthropic Package and Initializing Client
DESCRIPTION: This snippet installs the Anthropic package and initializes the client for interacting with the Claude model.

LANGUAGE: python
CODE:
%pip install anthropic

LANGUAGE: python
CODE:
from anthropic import Anthropic
client = Anthropic()
MODEL_NAME = "claude-3-opus-20240229"

----------------------------------------

TITLE: Defining Task and Variables for Prompt Generation
DESCRIPTION: This code allows users to define their task and optionally specify input variables for the prompt generation process.

LANGUAGE: python
CODE:
TASK = "Draft an email responding to a customer complaint" # Replace with your task!
# Optional: specify the input variables you want Claude to use. If you want Claude to choose, you can set `variables` to an empty list!
# VARIABLES = []
VARIABLES = ["CUSTOMER_EMAIL", "COMPANY_NAME"]
# If you want Claude to choose the variables, just leave VARIABLES as an empty list.

# TASK = "Choose an item from a menu for me given my preferences"
# VARIABLES = []
# VARIABLES = ["MENU", "PREFERENCES"]

----------------------------------------

TITLE: Installing Boto3 for AWS SDK in Python
DESCRIPTION: This snippet installs the boto3 library, which is the Amazon Web Services (AWS) SDK for Python. It's used to interact with various AWS services programmatically.

LANGUAGE: bash
CODE:
!pip install boto3

----------------------------------------

TITLE: Initializing Anthropic Client and API Key
DESCRIPTION: This code sets up the Anthropic client by importing the necessary module, defining the API key, specifying the model name, and creating the client object.

LANGUAGE: python
CODE:
import anthropic, re
ANTHROPIC_API_KEY = "" # Put your API key here!
MODEL_NAME = "claude-3-5-sonnet-20241022"
CLIENT = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)

----------------------------------------

TITLE: Importing Boto3 for AWS SDK in Python
DESCRIPTION: This code imports the boto3 library, which allows interaction with AWS services using Python. It's essential for accessing services like S3 and Bedrock in the following steps.

LANGUAGE: python
CODE:
import boto3

----------------------------------------

TITLE: Installing Required Dependencies
DESCRIPTION: Installs the necessary Python packages anthropic and bs4 using pip

LANGUAGE: python
CODE:
%pip install anthropic bs4 --quiet