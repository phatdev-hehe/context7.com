TITLE: Configuring Storm Topology Components and Workers (Java)
DESCRIPTION: This code snippet shows how to configure a complete Storm topology with multiple components (spouts and bolts) and set the number of worker processes. It demonstrates setting parallelism hints and number of tasks for different components.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(2); // use two worker processes

topologyBuilder.setSpout("blue-spout", new BlueSpout(), 2); // set parallelism hint to 2

topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

topologyBuilder.setBolt("yellow-bolt", new YellowBolt(), 6)
               .shuffleGrouping("green-bolt");

StormSubmitter.submitTopology(
        "mytopology",
        conf,
        topologyBuilder.createTopology()
    );

----------------------------------------

TITLE: Building Simple Storm Topology
DESCRIPTION: Java code demonstrating how to build a basic Storm topology with spouts and bolts using TopologyBuilder.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();        
builder.setSpout("words", new TestWordSpout(), 10);        
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
        .shuffleGrouping("words");
builder.setBolt("exclaim2", new ExclamationBolt(), 2)
        .shuffleGrouping("exclaim1");

----------------------------------------

TITLE: Building a Word Count Topology in Java
DESCRIPTION: Example of creating a word count topology with multiple bolts and groupings.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
        
builder.setSpout("sentences", new RandomSentenceSpout(), 5);        
builder.setBolt("split", new SplitSentence(), 8)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 12)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Implementing a Bolt in Java
DESCRIPTION: Full implementation of ExclamationBolt that appends '!!!' to input tuples.

LANGUAGE: java
CODE:
public static class ExclamationBolt implements IRichBolt {
    OutputCollector _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple tuple) {
        _collector.emit(tuple, new Values(tuple.getString(0) + "!!!"));
        _collector.ack(tuple);
    }

    @Override
    public void cleanup() {
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word"));
    }
    
    @Override
    public Map<String, Object> getComponentConfiguration() {
        return null;
    }
}

----------------------------------------

TITLE: Submitting Storm Topology to Cluster in Java
DESCRIPTION: This snippet demonstrates how to use StormSubmitter to submit a topology to a Storm cluster. It sets the number of workers and maximum pending spouts for the topology configuration.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(20);
conf.setMaxSpoutPending(5000);
StormSubmitter.submitTopology("mytopology", conf, topology);

----------------------------------------

TITLE: Initializing Storm Topology with Spouts and Bolts in Java
DESCRIPTION: This snippet demonstrates how to set up a basic Storm topology with a spout and two bolts for processing sentences and counting words.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("sentences", new KestrelSpout("kestrel.backtype.com",
                                               22133,
                                               "sentence_queue",
                                               new StringScheme()));
builder.setBolt("split", new SplitSentence(), 10)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 20)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Configuring Complete Storm Topology in Java
DESCRIPTION: Comprehensive example of configuring a Storm topology with multiple components. Sets up worker processes, spouts, and bolts with different parallelism settings and groupings.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(2); // use two worker processes

topologyBuilder.setSpout("blue-spout", new BlueSpout(), 2); // set parallelism hint to 2

topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

topologyBuilder.setBolt("yellow-bolt", new YellowBolt(), 6)
               .shuffleGrouping("green-bolt");

StormSubmitter.submitTopology(
        "mytopology",
        conf,
        topologyBuilder.createTopology()
    );

----------------------------------------

TITLE: Configuring and Submitting a Windowed Topology in Java
DESCRIPTION: Example of configuring a sliding window bolt in a topology and submitting it for execution. Shows how to set window parameters when creating the bolt.

LANGUAGE: java
CODE:
public static void main(String[] args) {
    TopologyBuilder builder = new TopologyBuilder();
     builder.setSpout("spout", new RandomSentenceSpout(), 1);
     builder.setBolt("slidingwindowbolt", 
                     new SlidingWindowBolt().withWindow(new Count(30), new Count(10)),
                     1).shuffleGrouping("spout");
    Config conf = new Config();
    conf.setDebug(true);
    conf.setNumWorkers(1);

    StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());
	
}

----------------------------------------

TITLE: Configuring Bolt Parallelism in Storm Topology (Java)
DESCRIPTION: This snippet demonstrates how to set the number of executors and tasks for a bolt in a Storm topology. It configures the 'green-bolt' with 2 executors and 4 tasks, using shuffle grouping from 'blue-spout'.

LANGUAGE: java
CODE:
topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

----------------------------------------

TITLE: Implementing Skewed Streaming Top N Pattern in Storm
DESCRIPTION: This code shows how to implement a streaming top N pattern for skewed data in Storm. It uses partial key grouping for load distribution, an extra aggregation layer, and then merging for final results.

LANGUAGE: java
CODE:
builder.setBolt("count", new CountObjects(), parallelism)
  .partialKeyGrouping("objects", new Fields("value"));
builder.setBolt("rank" new AggregateCountsAndRank(), parallelism)
  .fieldsGrouping("count", new Fields("key"))
builder.setBolt("merge", new MergeRanksObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Emitting Tuples with Message ID in Storm Spout
DESCRIPTION: This snippet demonstrates how to emit a tuple from a Storm spout with a message ID for tracking and reliability purposes.

LANGUAGE: java
CODE:
_collector.emit(new Values("field1", "field2", 3) , msgId);

----------------------------------------

TITLE: Building Word Count Topology
DESCRIPTION: Example of building a word count topology with multiple bolts and different stream groupings.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
        
builder.setSpout("sentences", new RandomSentenceSpout(), 5);        
builder.setBolt("split", new SplitSentence(), 8)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 12)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Implementing a Spout in Java
DESCRIPTION: Shows the nextTuple() method of a spout that emits random words.

LANGUAGE: java
CODE:
public void nextTuple() {
    Utils.sleep(100);
    final String[] words = new String[] {"nathan", "mike", "jackson", "golda", "bertels"};
    final Random rand = new Random();
    final String word = words[rand.nextInt(words.length)];
    _collector.emit(new Values(word));
}

----------------------------------------

TITLE: Storm RotatingMap Implementation for Tuple Expiration
DESCRIPTION: Implementation of Storm's RotatingMap data structure that efficiently manages time-based expiration of tuples. It maintains multiple buckets of HashMaps for different expiration cohorts and provides O(1) access time. The structure supports tuple relocation on updates and handles expiration through rotation callbacks.



----------------------------------------

TITLE: Implementing a Bolt in Java
DESCRIPTION: This code snippet shows the implementation of a simple bolt called DoubleAndTripleBolt. It demonstrates how to declare output fields, process input tuples, and emit new tuples in a Storm topology.

LANGUAGE: Java
CODE:
public class DoubleAndTripleBolt extends BaseRichBolt {
    private OutputCollectorBase _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollectorBase collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple input) {
        int val = input.getInteger(0);        
        _collector.emit(input, new Values(val*2, val*3));
        _collector.ack(input);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("double", "triple"));
    }    
}

----------------------------------------

TITLE: Declaring Multiple Streams in Storm Spouts
DESCRIPTION: Example of how to declare multiple streams in a Storm spout using the OutputFieldsDeclarer and emit to specific streams using SpoutOutputCollector.

LANGUAGE: java
CODE:
declareStream(OutputFieldsDeclarer declarer) {
    declarer.declareStream("stream1", new Fields("field1", "field2"));
    declarer.declareStream("stream2", new Fields("field3", "field4"));
}

emit(SpoutOutputCollector collector) {
    collector.emit("stream1", new Values(value1, value2));
    collector.emit("stream2", new Values(value3, value4));
}

----------------------------------------

TITLE: Defining a Simple Storm Topology in Java
DESCRIPTION: Illustrates how to define a simple topology with a spout and two bolts using the TopologyBuilder.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();        
builder.setSpout("words", new TestWordSpout(), 10);        
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
        .shuffleGrouping("words");
builder.setBolt("exclaim2", new ExclamationBolt(), 2)
        .shuffleGrouping("exclaim1");

----------------------------------------

TITLE: Implementing a Bolt in Java (Detailed)
DESCRIPTION: This snippet shows a full implementation of the ExclamationBolt class, demonstrating how to implement the IRichBolt interface. It includes methods for preparation, execution, cleanup, and output field declaration.

LANGUAGE: Java
CODE:
public static class ExclamationBolt implements IRichBolt {
    OutputCollector _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple tuple) {
        _collector.emit(tuple, new Values(tuple.getString(0) + "!!!"));
        _collector.ack(tuple);
    }

    @Override
    public void cleanup() {
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word"));
    }
    
    @Override
    public Map<String, Object> getComponentConfiguration() {
        return null;
    }
}

----------------------------------------

TITLE: Implementing Local DRPC in Storm
DESCRIPTION: Demonstrates how to run DRPC (Distributed Remote Procedure Call) in local mode using LocalDRPC and LocalCluster. Shows topology submission and execution of DRPC requests.

LANGUAGE: java
CODE:
try (LocalDRPC drpc = new LocalDRPC();
     LocalCluster cluster = new LocalCluster();
     LocalTopology topo = cluster.submitTopology("drpc-demo", conf, builder.createLocalTopology(drpc))) {

    System.out.println("Results for 'hello':" + drpc.execute("exclamation", "hello"));
}

----------------------------------------

TITLE: Configuring Maven Assembly Plugin for Storm Topology JAR
DESCRIPTION: This XML configuration for the Maven Assembly Plugin creates a JAR file containing all dependencies for a Storm topology. It specifies the main class and uses the jar-with-dependencies descriptor.

LANGUAGE: xml
CODE:
<plugin>
  <artifactId>maven-assembly-plugin</artifactId>
  <configuration>
    <descriptorRefs>  
      <descriptorRef>jar-with-dependencies</descriptorRef>
    </descriptorRefs>
    <archive>
      <manifest>
        <mainClass>com.path.to.main.Class</mainClass>
      </manifest>
    </archive>
  </configuration>
</plugin>

----------------------------------------

TITLE: Setting Batch Sizes for Message Processing in Storm
DESCRIPTION: Configures batch sizes for intra-worker and inter-worker message passing. Affects throughput and latency trade-offs.

LANGUAGE: java
CODE:
topology.producer.batch.size
topology.transfer.batch.size

----------------------------------------

TITLE: Implementing Stateful Word Count Bolt in Java
DESCRIPTION: Example implementation of a stateful word count bolt that extends BaseStatefulBolt and uses KeyValueState to store word counts. The bolt initializes state, processes tuples to count words, and persists state through the framework.

LANGUAGE: java
CODE:
public class WordCountBolt extends BaseStatefulBolt<KeyValueState<String, Long>> {
    private KeyValueState<String, Long> wordCounts;
    private OutputCollector collector;
    ...
        @Override
        public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
          this.collector = collector;
        }
        @Override
        public void initState(KeyValueState<String, Long> state) {
          wordCounts = state;
        }
        @Override
        public void execute(Tuple tuple) {
          String word = tuple.getString(0);
          Integer count = wordCounts.get(word, 0);
          count++;
          wordCounts.put(word, count);
          collector.emit(tuple, new Values(word, count));
          collector.ack(tuple);
        }
    ...
    }

----------------------------------------

TITLE: Implementing Streaming Word Count in Trident
DESCRIPTION: Demonstrates how to create a Trident topology that computes streaming word count from an input stream of sentences and persistently stores the results.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
     topology.newStream("spout1", spout)
       .each(new Fields("sentence"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))                
       .parallelismHint(6);

----------------------------------------

TITLE: Storm Stream Grouping Declaration Example
DESCRIPTION: Demonstrates the syntax for declaring stream groupings in Storm topologies using TopologyBuilder. The example shows how to subscribe to default streams using shuffle grouping.

LANGUAGE: java
CODE:
declarer.shuffleGrouping("1")  // subscribes to default stream
declarer.shuffleGrouping("1", DEFAULT_STREAM_ID)  // equivalent explicit form

----------------------------------------

TITLE: Implementing Word Count Stream Processing in Trident
DESCRIPTION: Demonstrates how to create a streaming word count topology that splits sentences into words, groups by word, and maintains persistent counts in memory.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
     topology.newStream("spout1", spout)
       .each(new Fields("sentence"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))                
       .parallelismHint(6);

----------------------------------------

TITLE: Implementing a SplitSentence Bolt in Storm
DESCRIPTION: This code shows a bolt implementation that splits a sentence into words. It demonstrates how to anchor emitted tuples to the input tuple and how to ack the input tuple after processing.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseRichBolt {
        OutputCollector _collector;
        
        public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
            _collector = collector;
        }

        public void execute(Tuple tuple) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                _collector.emit(tuple, new Values(word));
            }
            _collector.ack(tuple);
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Initializing Storm Topology with Spouts and Bolts
DESCRIPTION: Example showing how to set up a basic Storm topology with a Kestrel spout for reading sentences and bolts for splitting and counting words.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("sentences", new KestrelSpout("kestrel.backtype.com",
                                               22133,
                                               "sentence_queue",
                                               new StringScheme()));
builder.setBolt("split", new SplitSentence(), 10)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 20)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Creating a Simple Storm Topology in Java
DESCRIPTION: Example of creating a simple Storm topology with a spout and two bolts using the TopologyBuilder class. It demonstrates how to set up components and define their connections.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();        
builder.setSpout("words", new TestWordSpout(), 10);        
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
        .shuffleGrouping("words");
builder.setBolt("exclaim2", new ExclamationBolt(), 2)
        .shuffleGrouping("exclaim1");

----------------------------------------

TITLE: Implementing a Multilingual Bolt in Java
DESCRIPTION: Demonstrates how to create a bolt that uses a Python script for processing.

LANGUAGE: java
CODE:
public static class SplitSentence extends ShellBolt implements IRichBolt {
    public SplitSentence() {
        super("python3", "splitsentence.py");
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word"));
    }
}

----------------------------------------

TITLE: Implementing a Basic Storm Bolt
DESCRIPTION: This snippet shows the structure of a basic Storm bolt that processes input tuples and emits output tuples using the OutputCollector.

LANGUAGE: java
CODE:
public class MyBolt extends BaseRichBolt {
    OutputCollector collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        this.collector = collector;
    }

    @Override
    public void execute(Tuple tuple) {
        // Process the input tuple
        String value = tuple.getStringByField("fieldName");
        // Emit a new tuple
        collector.emit(new Values(processedValue));
        // Acknowledge the input tuple
        collector.ack(tuple);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("processedField"));
    }
}

----------------------------------------

TITLE: Creating Trident Topology for Word Count in Java
DESCRIPTION: Builds a Trident topology that computes streaming word count from an input stream of sentences. It splits sentences into words, groups by word, and persistently aggregates the count using MemoryMapState.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
     topology.newStream("spout1", spout)
       .each(new Fields("sentence"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))                
       .parallelismHint(6);

----------------------------------------

TITLE: Creating a Word Count Topology in Java
DESCRIPTION: Java code snippet demonstrating how to create a Word Count topology using different stream groupings.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
        
builder.setSpout("sentences", new RandomSentenceSpout(), 5);        
builder.setBolt("split", new SplitSentence(), 8)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 12)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Configuring Storm Topology Parallelism (Java)
DESCRIPTION: Illustrates how to configure worker processes, spouts, and bolts in a Storm topology. It sets up a topology with 2 worker processes, configures parallelism for spouts and bolts, and submits the topology.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(2); // use two worker processes

topologyBuilder.setSpout("blue-spout", new BlueSpout(), 2); // set parallelism hint to 2

topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

topologyBuilder.setBolt("yellow-bolt", new YellowBolt(), 6)
               .shuffleGrouping("green-bolt");

StormSubmitter.submitTopology(
        "mytopology",
        conf,
        topologyBuilder.createTopology()
    );

----------------------------------------

TITLE: Implementing ISpout Interface in Java for Storm
DESCRIPTION: This code snippet shows the ISpout interface that spouts must implement in Storm, including methods for tuple emission and failure handling.

LANGUAGE: java
CODE:
public interface ISpout extends Serializable {
    void open(Map conf, TopologyContext context, SpoutOutputCollector collector);
    void close();
    void nextTuple();
    void ack(Object msgId);
    void fail(Object msgId);
}

----------------------------------------

TITLE: Creating a Word Count Topology in Java
DESCRIPTION: Java code to create a word count topology with spouts and bolts using different groupings.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
        
builder.setSpout("sentences", new RandomSentenceSpout(), 5);        
builder.setBolt("split", new SplitSentence(), 8)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 12)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Emitting Tuples in Storm Bolts
DESCRIPTION: Demonstrates how to emit tuples from a Storm bolt using OutputCollector, including anchoring and acknowledging tuples for reliability.

LANGUAGE: java
CODE:
public void execute(Tuple input, OutputCollector collector) {
    // Process input tuple
    // ...

    // Emit new tuple, anchored to the input
    collector.emit(input, new Values(result));

    // Acknowledge the input tuple
    collector.ack(input);
}

----------------------------------------

TITLE: Implementing Stateful Word Count Bolt in Java Storm
DESCRIPTION: Example implementation of a stateful word count bolt using Storm's BaseStatefulBolt and KeyValueState. Shows how to initialize state, handle tuple processing, and maintain word counts across restarts.

LANGUAGE: java
CODE:
public class WordCountBolt extends BaseStatefulBolt<KeyValueState<String, Long>> {
    private KeyValueState<String, Long> wordCounts;
    private OutputCollector collector;
    ...
        @Override
        public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
          this.collector = collector;
        }
        @Override
        public void initState(KeyValueState<String, Long> state) {
          wordCounts = state;
        }
        @Override
        public void execute(Tuple tuple) {
          String word = tuple.getString(0);
          Integer count = wordCounts.get(word, 0);
          count++;
          wordCounts.put(word, count);
          collector.emit(tuple, new Values(word, count));
          collector.ack(tuple);
        }
    ...
    }

----------------------------------------

TITLE: Word Count Topology Implementation
DESCRIPTION: Example of a word count topology using shuffle and fields grouping.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
        
builder.setSpout("sentences", new RandomSentenceSpout(), 5);        
builder.setBolt("split", new SplitSentence(), 8)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 12)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Implementing a Basic Storm Bolt
DESCRIPTION: This code snippet shows the structure of a basic Storm bolt, including the execute method for processing tuples and the declareOutputFields method for defining output fields.

LANGUAGE: java
CODE:
public class MyBolt extends BaseBasicBolt {
    @Override
    public void execute(Tuple input, BasicOutputCollector collector) {
        // Process input tuple
        String value = input.getStringByField("field1");
        // Emit new tuple
        collector.emit(new Values(processedValue));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("processed_field"));
    }
}

----------------------------------------

TITLE: Submitting a Storm Topology using StormSubmitter in Java
DESCRIPTION: This snippet demonstrates how to use StormSubmitter to submit a topology to a Storm cluster. It sets the number of workers and maximum pending spouts in the configuration before submitting.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(20);
conf.setMaxSpoutPending(5000);
StormSubmitter.submitTopology("mytopology", conf, topology);

----------------------------------------

TITLE: Building Simple Storm Topology
DESCRIPTION: Example showing how to build a basic Storm topology with spouts and bolts using TopologyBuilder.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();        
builder.setSpout("words", new TestWordSpout(), 10);        
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
        .shuffleGrouping("words");
builder.setBolt("exclaim2", new ExclamationBolt(), 2)
        .shuffleGrouping("exclaim1");

----------------------------------------

TITLE: Declaring Multiple Streams in Storm Spouts
DESCRIPTION: This snippet demonstrates how to declare multiple streams in a Storm spout using the OutputFieldsDeclarer and emit to specific streams using SpoutOutputCollector.

LANGUAGE: java
CODE:
declareStream(OutputFieldsDeclarer declarer) {
    declarer.declareStream("stream1", new Fields("field1", "field2"));
    declarer.declareStream("stream2", new Fields("field3", "field4"));
}

emit(SpoutOutputCollector collector) {
    collector.emit("stream1", new Values(value1, value2));
    collector.emit("stream2", new Values(value3, value4));
}

----------------------------------------

TITLE: Complete Storm Topology Configuration in Java
DESCRIPTION: Complete example of configuring a Storm topology with multiple components. Shows configuration of workers, spouts, and bolts with different parallelism settings.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(2); // use two worker processes

topologyBuilder.setSpout("blue-spout", new BlueSpout(), 2); // set parallelism hint to 2

topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

topologyBuilder.setBolt("yellow-bolt", new YellowBolt(), 6)
               .shuffleGrouping("green-bolt");

StormSubmitter.submitTopology(
        "mytopology",
        conf,
        topologyBuilder.createTopology()
    );

----------------------------------------

TITLE: Implementing IWindowedBolt Interface in Java
DESCRIPTION: Core interface definition for bolts that need windowing support in Storm. Defines methods for preparing the bolt, executing window operations, and cleanup.

LANGUAGE: java
CODE:
public interface IWindowedBolt extends IComponent {
    void prepare(Map stormConf, TopologyContext context, OutputCollector collector);
    /**
     * Process tuples falling within the window and optionally emit 
     * new tuples based on the tuples in the input window.
     */
    void execute(TupleWindow inputWindow);
    void cleanup();
}

----------------------------------------

TITLE: Storm WordCount Topology Implementation
DESCRIPTION: Implementation of a word counting topology using spouts and bolts with different stream groupings

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
        
builder.setSpout("sentences", new RandomSentenceSpout(), 5);        
builder.setBolt("split", new SplitSentence(), 8)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 12)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Defining a Storm Topology using Flux YAML DSL
DESCRIPTION: Example of defining a simple word count topology using the Flux YAML DSL, including spout and bolt definitions, and stream connections.

LANGUAGE: yaml
CODE:
name: "yaml-topology"
config:
  topology.workers: 1

# spout definitions
spouts:
  - id: "spout-1"
    className: "org.apache.storm.testing.TestWordSpout"
    parallelism: 1

# bolt definitions
bolts:
  - id: "bolt-1"
    className: "org.apache.storm.testing.TestWordCounter"
    parallelism: 1
  - id: "bolt-2"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1

#stream definitions
streams:
  - name: "spout-1 --> bolt-1" # name isn't used (placeholder for logging, UI, etc.)
    from: "spout-1"
    to: "bolt-1"
    grouping:
      type: FIELDS
      args: ["word"]

  - name: "bolt-1 --> bolt2"
    from: "bolt-1"
    to: "bolt-2"
    grouping:
      type: SHUFFLE

# worker hook definitions
workerHooks:
  - id: "base-worker-hook"
    className: "org.apache.storm.hooks.BaseWorkerHook"

----------------------------------------

TITLE: Submitting Storm Topology using StormSubmitter in Java
DESCRIPTION: This snippet demonstrates how to use StormSubmitter to submit a Storm topology to a cluster. It sets up configuration parameters such as the number of workers and maximum pending spouts, then submits the topology with a specified name.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(20);
conf.setMaxSpoutPending(5000);
StormSubmitter.submitTopology("mytopology", conf, topology);

----------------------------------------

TITLE: Creating a Simple Storm Topology in Java
DESCRIPTION: Example of creating a simple Storm topology with a spout and two bolts using the TopologyBuilder class. It demonstrates how to set up components and define their connections.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();        
builder.setSpout("words", new TestWordSpout(), 10);        
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
        .shuffleGrouping("words");
builder.setBolt("exclaim2", new ExclamationBolt(), 2)
        .shuffleGrouping("exclaim1");

----------------------------------------

TITLE: Storm Component Configuration Options
DESCRIPTION: List of configuration keys that can be overridden on a per-bolt/per-spout basis in Storm 0.7.0+. These include debug settings, spout pending limits, parallelism settings, and Kryo registration.

LANGUAGE: yaml
CODE:
"topology.debug"
"topology.max.spout.pending"
"topology.max.task.parallelism"
"topology.kryo.register"

----------------------------------------

TITLE: Implementing a Stateful Word Count Bolt in Java
DESCRIPTION: This snippet demonstrates how to implement a stateful word count bolt using the BaseStatefulBolt class and KeyValueState. It shows how to initialize the state, update word counts, and emit results.

LANGUAGE: java
CODE:
public class WordCountBolt extends BaseStatefulBolt<KeyValueState<String, Long>> {
    private KeyValueState<String, Long> wordCounts;
    private OutputCollector collector;
    ...
        @Override
        public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
          this.collector = collector;
        }
        @Override
        public void initState(KeyValueState<String, Long> state) {
          wordCounts = state;
        }
        @Override
        public void execute(Tuple tuple) {
          String word = tuple.getString(0);
          Integer count = wordCounts.get(word, 0);
          count++;
          wordCounts.put(word, count);
          collector.emit(tuple, new Values(word, count));
          collector.ack(tuple);
        }
    ...
    }

----------------------------------------

TITLE: Configuring Node Resources in YAML
DESCRIPTION: YAML configuration to specify available memory and CPU resources on a Storm node.

LANGUAGE: yaml
CODE:
supervisor.memory.capacity.mb: 20480.0
supervisor.cpu.capacity: 100.0

----------------------------------------

TITLE: Word Count Example using Storm Streams
DESCRIPTION: Complete example showing how to build a word count topology using Stream API with windowing and aggregation.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

builder
   // A stream of random sentences with two partitions
   .newStream(new RandomSentenceSpout(), new ValueMapper<String>(0), 2)
   // a two seconds tumbling window
   .window(TumblingWindows.of(Duration.seconds(2)))
   // split the sentences to words
   .flatMap(s -> Arrays.asList(s.split(" ")))
   // create a stream of (word, 1) pairs
   .mapToPair(w -> Pair.of(w, 1))
   // compute the word counts in the last two second window
   .countByKey()
   // print the results to stdout
   .print();

----------------------------------------

TITLE: Defining IWindowedBolt Interface in Java
DESCRIPTION: Interface definition for bolts that require windowing support in Apache Storm. It includes methods for preparation, execution of windowed operations, and cleanup.

LANGUAGE: java
CODE:
public interface IWindowedBolt extends IComponent {
    void prepare(Map stormConf, TopologyContext context, OutputCollector collector);
    /**
     * Process tuples falling within the window and optionally emit 
     * new tuples based on the tuples in the input window.
     */
    void execute(TupleWindow inputWindow);
    void cleanup();
}

----------------------------------------

TITLE: Word Count Example using Storm Streams
DESCRIPTION: Complete example showing how to build a word count topology using Stream API with windowing and aggregation.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

builder
   // A stream of random sentences with two partitions
   .newStream(new RandomSentenceSpout(), new ValueMapper<String>(0), 2)
   // a two seconds tumbling window
   .window(TumblingWindows.of(Duration.seconds(2)))
   // split the sentences to words
   .flatMap(s -> Arrays.asList(s.split(" ")))
   // create a stream of (word, 1) pairs
   .mapToPair(w -> Pair.of(w, 1))
   // compute the word counts in the last two second window
   .countByKey()
   // print the results to stdout
   .print();

----------------------------------------

TITLE: Building Word Count Topology
DESCRIPTION: Example topology that implements word counting using shuffle and fields grouping.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
        
builder.setSpout("sentences", new RandomSentenceSpout(), 5);        
builder.setBolt("split", new SplitSentence(), 8)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 12)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Implementing a Bolt in Java
DESCRIPTION: Shows the implementation of a simple bolt that appends '!!!' to input strings.

LANGUAGE: java
CODE:
public class DoubleAndTripleBolt extends BaseRichBolt {
    private OutputCollectorBase _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollectorBase collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple input) {
        int val = input.getInteger(0);        
        _collector.emit(input, new Values(val*2, val*3));
        _collector.ack(input);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("double", "triple"));
    }    
}

----------------------------------------

TITLE: Implementing a Sliding Window Bolt in Storm
DESCRIPTION: This example shows how to implement a sliding window bolt by extending BaseWindowedBolt. It includes the execute method for processing windowed tuples and a main method demonstrating how to configure and submit the topology with the windowed bolt.

LANGUAGE: java
CODE:
public class SlidingWindowBolt extends BaseWindowedBolt {
	private OutputCollector collector;
	
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
    	this.collector = collector;
    }
	
    @Override
    public void execute(TupleWindow inputWindow) {
	  for(Tuple tuple: inputWindow.get()) {
	    // do the windowing computation
		...
	  }
	  // emit the results
	  collector.emit(new Values(computedValue));
    }
}

public static void main(String[] args) {
    TopologyBuilder builder = new TopologyBuilder();
     builder.setSpout("spout", new RandomSentenceSpout(), 1);
     builder.setBolt("slidingwindowbolt", 
                     new SlidingWindowBolt().withWindow(new Count(30), new Count(10)),
                     1).shuffleGrouping("spout");
    Config conf = new Config();
    conf.setDebug(true);
    conf.setNumWorkers(1);

    StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());
	
}

----------------------------------------

TITLE: Submitting Storm Topology with Configuration
DESCRIPTION: Demonstrates how to configure and submit a Storm topology to a cluster using StormSubmitter. Sets the number of workers and maximum pending spouts for the topology.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(20);
conf.setMaxSpoutPending(5000);
StormSubmitter.submitTopology("mytopology", conf, topology);

----------------------------------------

TITLE: Implementing QueryFunction for Custom State
DESCRIPTION: Shows how to implement a QueryFunction for retrieving data from a custom State object.

LANGUAGE: Java
CODE:
public class QueryLocation extends BaseQueryFunction<LocationDB, String> {
    public List<String> batchRetrieve(LocationDB state, List<TridentTuple> inputs) {
        List<String> ret = new ArrayList();
        for(TridentTuple input: inputs) {
            ret.add(state.getLocation(input.getLong(0)));
        }
        return ret;
    }

    public void execute(TridentTuple tuple, String location, TridentCollector collector) {
        collector.emit(new Values(location));
    }    
}

----------------------------------------

TITLE: Running a Storm Topology in Java
DESCRIPTION: Example command to run a Storm topology jar file with arguments.

LANGUAGE: bash
CODE:
storm jar all-my-code.jar org.apache.storm.MyTopology arg1 arg2

----------------------------------------

TITLE: Emitting and Acking Tuples in Storm Bolts
DESCRIPTION: Shows how to emit new tuples and acknowledge processed tuples in a Storm bolt using OutputCollector.

LANGUAGE: java
CODE:
public void execute(Tuple input) {
    OutputCollector collector = getCollector();
    // Process input tuple
    collector.emit(new Values(processedValue));
    collector.ack(input);
}

----------------------------------------

TITLE: Configuring Complete Storm Topology
DESCRIPTION: Complete topology configuration example showing how to set up multiple bolts and spouts with different parallelism settings, including worker processes, executors, and tasks.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(2); // use two worker processes

topologyBuilder.setSpout("blue-spout", new BlueSpout(), 2); // set parallelism hint to 2

topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

topologyBuilder.setBolt("yellow-bolt", new YellowBolt(), 6)
               .shuffleGrouping("green-bolt");

StormSubmitter.submitTopology(
        "mytopology",
        conf,
        topologyBuilder.createTopology()
    );

----------------------------------------

TITLE: Storm Tuple Processing in Bolts
DESCRIPTION: Example showing how bolts process tuples and emit new ones using OutputCollector. Shows the pattern for acknowledging tuple processing completion.

LANGUAGE: java
CODE:
// In execute method of a bolt\noutputCollector.emit(tuple, new Values(processedValue));\noutputCollector.ack(tuple);

----------------------------------------

TITLE: Overriding Component Configurations in Java for Storm Topologies
DESCRIPTION: Demonstrates two methods for specifying component-specific configurations in Storm topologies using Java: internally by overriding a method, and externally using the TopologyBuilder.

LANGUAGE: java
CODE:
// Method 1: Internal override
@Override
public Map<String, Object> getComponentConfiguration() {
    // Return component-specific configuration map
}

// Method 2: External configuration
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("spout", new MySpout())
       .addConfiguration("topology.debug", true)
       .addConfigurations(someConfigMap);

----------------------------------------

TITLE: Implementing a Sliding Window Bolt in Storm
DESCRIPTION: This example shows how to implement a sliding window bolt by extending BaseWindowedBolt. It includes the execute method for processing windowed tuples and a main method demonstrating how to configure and submit the topology with the windowed bolt.

LANGUAGE: java
CODE:
public class SlidingWindowBolt extends BaseWindowedBolt {
	private OutputCollector collector;
	
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
    	this.collector = collector;
    }
	
    @Override
    public void execute(TupleWindow inputWindow) {
	  for(Tuple tuple: inputWindow.get()) {
	    // do the windowing computation
		...
	  }
	  // emit the results
	  collector.emit(new Values(computedValue));
    }
}

public static void main(String[] args) {
    TopologyBuilder builder = new TopologyBuilder();
     builder.setSpout("spout", new RandomSentenceSpout(), 1);
     builder.setBolt("slidingwindowbolt", 
                     new SlidingWindowBolt().withWindow(new Count(30), new Count(10)),
                     1).shuffleGrouping("spout");
    Config conf = new Config();
    conf.setDebug(true);
    conf.setNumWorkers(1);

    StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());
	
}

----------------------------------------

TITLE: Acker execute() Method Implementation in Java
DESCRIPTION: The execute() method of the Acker bolt processes incoming tuples, updating the checksum for tupletrees and managing their completion status. It handles initialization, acking, and failing of tuples, as well as notifying spouts of tupletree completion.

LANGUAGE: java
CODE:
// Pseudo-code representation of Acker's execute() method
if (tickTuple) {
    advancePendingTupletreesTowardsDeath();
    return;
}

Record record = updateOrCreateRecord(tuple);
rotatingMap.put(record);

if (record.checksum == 0) {
    removePendingTupletree();
    notifySpoutSuccess();
} else if (record.failed) {
    removePendingTupletree();
    notifySpoutFailure();
}

ackOwnTuple();

----------------------------------------

TITLE: Executing Storm Topology JAR
DESCRIPTION: Command to run a Storm topology by packaging code and dependencies into a JAR file.

LANGUAGE: bash
CODE:
storm jar all-my-code.jar org.apache.storm.MyTopology arg1 arg2

----------------------------------------

TITLE: Implementing a Spout in Java
DESCRIPTION: Shows the implementation of a simple spout that emits random words.

LANGUAGE: java
CODE:
public void nextTuple() {
    Utils.sleep(100);
    final String[] words = new String[] {"nathan", "mike", "jackson", "golda", "bertels"};
    final Random rand = new Random();
    final String word = words[rand.nextInt(words.length)];
    _collector.emit(new Values(word));
}

----------------------------------------

TITLE: Implementing Word Count Using Stream API in Java
DESCRIPTION: This example demonstrates a complete word count topology using the Stream API, including windowing, flatMap, and countByKey operations.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

builder
   // A stream of random sentences with two partitions
   .newStream(new RandomSentenceSpout(), new ValueMapper<String>(0), 2)
   // a two seconds tumbling window
   .window(TumblingWindows.of(Duration.seconds(2)))
   // split the sentences to words
   .flatMap(s -> Arrays.asList(s.split(" ")))
   // create a stream of (word, 1) pairs
   .mapToPair(w -> Pair.of(w, 1))
   // compute the word counts in the last two second window
   .countByKey()
   // print the results to stdout
   .print();

Config config = new Config();
config.setNumWorkers(1);
StormSubmitter.submitTopologyWithProgressBar("topology-name", config, builder.build());

----------------------------------------

TITLE: Windowing and Aggregation in Java Stream API
DESCRIPTION: Demonstrates how to apply windowing and aggregation operations on a stream of numbers.

LANGUAGE: java
CODE:
Stream<Long> numbers = â€¦
// aggregate the numbers and produce a stream of last 10 sec sums.
Stream<Long> sums = numbers.window(TumblingWindows.of(Duration.seconds(10)).aggregate(new Sum());

// the last 10 sec sums computed using reduce
Stream<Long> sums = numbers.window(...).reduce((x, y) -> x + y);

----------------------------------------

TITLE: Implementing Stateful Word Count Bolt in Java
DESCRIPTION: Example implementation of a stateful bolt that counts words using KeyValueState. Shows how to extend BaseStatefulBolt and implement state initialization and execution logic.

LANGUAGE: java
CODE:
public class WordCountBolt extends BaseStatefulBolt<KeyValueState<String, Long>> {
    private KeyValueState<String, Long> wordCounts;
    private OutputCollector collector;
    ...
        @Override
        public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
          this.collector = collector;
        }
        @Override
        public void initState(KeyValueState<String, Long> state) {
          wordCounts = state;
        }
        @Override
        public void execute(Tuple tuple) {
          String word = tuple.getString(0);
          Integer count = wordCounts.get(word, 0);
          count++;
          wordCounts.put(word, count);
          collector.emit(tuple, new Values(word, count));
          collector.ack(tuple);
        }
    ...
    }

----------------------------------------

TITLE: Implementing Streaming Word Count with Trident in Java
DESCRIPTION: Demonstrates how to create a Trident topology for streaming word count. It processes the input stream of sentences, splits them into words, groups by word, and persistently aggregates the count for each word.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
     topology.newStream("spout1", spout)
       .each(new Fields("sentence"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))                
       .parallelismHint(6);

----------------------------------------

TITLE: Building Storm Topology
DESCRIPTION: Example showing how to construct a Storm topology by connecting spouts and bolts with stream groupings.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();        
builder.setSpout("words", new TestWordSpout(), 10);        
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
        .shuffleGrouping("words");
builder.setBolt("exclaim2", new ExclamationBolt(), 2)
        .shuffleGrouping("exclaim1");

----------------------------------------

TITLE: Creating and Transforming Streams in Java
DESCRIPTION: Demonstrates how to create a stream from a spout, transform it by splitting sentences into words, and perform an output operation.

LANGUAGE: java
CODE:
import org.apache.storm.streams.Stream;
import org.apache.storm.streams.StreamBuilder;

StreamBuilder builder = new StreamBuilder();

// a stream of sentences obtained from a source spout
Stream<String> sentences = builder.newStream(new RandomSentenceSpout()).map(tuple -> tuple.getString(0));

// a stream of words obtained by transforming (splitting) the stream of sentences
Stream<String> words = sentences.flatMap(s -> Arrays.asList(s.split(" ")));

// output operation that prints the words to console
words.forEach(w -> System.out.println(w));

----------------------------------------

TITLE: Implementing a Custom Function in Trident
DESCRIPTION: Example of creating a custom function that emits multiple tuples based on the input value.

LANGUAGE: java
CODE:
public class MyFunction extends BaseFunction {
    public void execute(TridentTuple tuple, TridentCollector collector) {
        for(int i=0; i < tuple.getInteger(0); i++) {
            collector.emit(new Values(i));
        }
    }
}

----------------------------------------

TITLE: Running DRPC in Local Mode with Java
DESCRIPTION: Shows how to set up and use Distributed RPC (DRPC) in local mode using LocalDRPC and LocalCluster classes.

LANGUAGE: java
CODE:
try (LocalDRPC drpc = new LocalDRPC();
     LocalCluster cluster = new LocalCluster();
     LocalTopology topo = cluster.submitTopology("drpc-demo", conf, builder.createLocalTopology(drpc))) {

    System.out.println("Results for 'hello':" + drpc.execute("exclamation", "hello"));
}

----------------------------------------

TITLE: Configuring Complete Storm Topology in Java
DESCRIPTION: Complete topology configuration example showing how to set up multiple bolts and spouts with different parallelism settings. Demonstrates setting worker processes, executor counts, and task counts across multiple components.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(2); // use two worker processes

topologyBuilder.setSpout("blue-spout", new BlueSpout(), 2); // set parallelism hint to 2

topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

topologyBuilder.setBolt("yellow-bolt", new YellowBolt(), 6)
               .shuffleGrouping("green-bolt");

StormSubmitter.submitTopology(
        "mytopology",
        conf,
        topologyBuilder.createTopology()
    );

----------------------------------------

TITLE: Running a Storm Topology
DESCRIPTION: Command to run a Storm topology by packaging code and dependencies into a jar file and executing it with specified arguments.

LANGUAGE: bash
CODE:
storm jar all-my-code.jar org.apache.storm.MyTopology arg1 arg2

----------------------------------------

TITLE: Subscribing to Streams in Storm Bolts
DESCRIPTION: Shows how to subscribe to specific streams or all streams from another component in a Storm topology using InputDeclarer.

LANGUAGE: java
CODE:
// Subscribe to specific stream
declarer.shuffleGrouping("1", "streamName");

// Subscribe to default stream
declarer.shuffleGrouping("1");

// Equivalent to subscribing to default stream
declarer.shuffleGrouping("1", DEFAULT_STREAM_ID);

----------------------------------------

TITLE: Complete Word Count Example using Storm's Stream API (Java)
DESCRIPTION: A full example of a word count topology using the Stream API, including windowing and aggregation.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

builder
   // A stream of random sentences with two partitions
   .newStream(new RandomSentenceSpout(), new ValueMapper<String>(0), 2)
   // a two seconds tumbling window
   .window(TumblingWindows.of(Duration.seconds(2)))
   // split the sentences to words
   .flatMap(s -> Arrays.asList(s.split(" ")))
   // create a stream of (word, 1) pairs
   .mapToPair(w -> Pair.of(w, 1))
   // compute the word counts in the last two second window
   .countByKey()
   // print the results to stdout
   .print();

Config config = new Config();
config.setNumWorkers(1);
StormSubmitter.submitTopologyWithProgressBar("topology-name", config, builder.build());

----------------------------------------

TITLE: Building a Simple Storm Topology in Java
DESCRIPTION: Example of creating a topology with a spout and two bolts using TopologyBuilder.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();        
builder.setSpout("words", new TestWordSpout(), 10);        
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
        .shuffleGrouping("words");
builder.setBolt("exclaim2", new ExclamationBolt(), 2)
        .shuffleGrouping("exclaim1");

----------------------------------------

TITLE: Implementing Skewed Top-N Pattern in Storm
DESCRIPTION: Advanced implementation of streaming top-N calculation that handles data skew using partial key grouping and an additional aggregation layer.

LANGUAGE: java
CODE:
builder.setBolt("count", new CountObjects(), parallelism)
  .partialKeyGrouping("objects", new Fields("value"));
builder.setBolt("rank" new AggregateCountsAndRank(), parallelism)
  .fieldsGrouping("count", new Fields("key"))
builder.setBolt("merge", new MergeRanksObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Creating a Storm Topology with Word Count Example
DESCRIPTION: This code snippet demonstrates how to create a simple Storm topology for counting words in sentences. It uses a KestrelSpout to read sentences from a queue, a SplitSentence bolt to split sentences into words, and a WordCount bolt to count occurrences of each word.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("sentences", new KestrelSpout("kestrel.backtype.com",
                                               22133,
                                               "sentence_queue",
                                               new StringScheme()));
builder.setBolt("split", new SplitSentence(), 10)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 20)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Implementing an Update Global Count Bolt in Apache Storm
DESCRIPTION: This code defines an UpdateGlobalCount bolt that updates a global count in a database, ensuring exactly-once semantics even with replays.

LANGUAGE: java
CODE:
public static class UpdateGlobalCount extends BaseTransactionalBolt implements ICommitter {
    TransactionAttempt _attempt;
    BatchOutputCollector _collector;

    int _sum = 0;

    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, TransactionAttempt attempt) {
        _collector = collector;
        _attempt = attempt;
    }

    @Override
    public void execute(Tuple tuple) {
        _sum+=tuple.getInteger(1);
    }

    @Override
    public void finishBatch() {
        Value val = DATABASE.get(GLOBAL_COUNT_KEY);
        Value newval;
        if(val == null || !val.txid.equals(_attempt.getTransactionId())) {
            newval = new Value();
            newval.txid = _attempt.getTransactionId();
            if(val==null) {
                newval.count = _sum;
            } else {
                newval.count = _sum + val.count;
            }
            DATABASE.put(GLOBAL_COUNT_KEY, newval);
        } else {
            newval = val;
        }
        _collector.emit(new Values(_attempt, newval.count));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "sum"));
    }
}

----------------------------------------

TITLE: Complete Storm Topology Configuration in Java
DESCRIPTION: Full example of configuring a Storm topology with multiple components. Demonstrates setting up worker processes, spouts, and bolts with different parallelism settings and shuffle grouping.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(2); // use two worker processes

topologyBuilder.setSpout("blue-spout", new BlueSpout(), 2); // set parallelism hint to 2

topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

topologyBuilder.setBolt("yellow-bolt", new YellowBolt(), 6)
               .shuffleGrouping("green-bolt");

StormSubmitter.submitTopology(
        "mytopology",
        conf,
        topologyBuilder.createTopology()
    );

----------------------------------------

TITLE: Storm Component-Specific Configuration Path
DESCRIPTION: Shows the config preference order for Storm from lowest to highest priority - defaults.yaml < storm.yaml < topology configuration < internal component config < external component config.

LANGUAGE: text
CODE:
defaults.yaml < storm.yaml < topology specific configuration < internal component specific configuration < external component specific configuration

----------------------------------------

TITLE: Implementing SplitSentence as BasicBolt
DESCRIPTION: Simplified bolt implementation using BasicBolt interface which handles anchoring and acking automatically.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseBasicBolt {
        public void execute(Tuple tuple, BasicOutputCollector collector) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                collector.emit(new Values(word));
            }
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Creating Transactional Topology in Storm
DESCRIPTION: Example code showing how to build a transactional topology using TransactionalTopologyBuilder for counting global tuples

LANGUAGE: java
CODE:
MemoryTransactionalSpout spout = new MemoryTransactionalSpout(DATA, new Fields("word"), PARTITION_TAKE_PER_BATCH);
TransactionalTopologyBuilder builder = new TransactionalTopologyBuilder("global-count", "spout", spout, 3);
builder.setBolt("partial-count", new BatchCount(), 5)
        .shuffleGrouping("spout");
builder.setBolt("sum", new UpdateGlobalCount())
        .globalGrouping("partial-count");

----------------------------------------

TITLE: Implementing KafkaBolt with Producer Configuration
DESCRIPTION: Example showing how to configure and implement a KafkaBolt to write data to Kafka, including producer properties, topic selection and tuple mapping.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();

Fields fields = new Fields("key", "message");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
            new Values("storm", "1"),
            new Values("trident", "1"),
            new Values("needs", "1"),
            new Values("javadoc", "1")
);
spout.setCycle(true);
builder.setSpout("spout", spout, 5);
//set producer properties.
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

KafkaBolt bolt = new KafkaBolt()
        .withProducerProperties(props)
        .withTopicSelector(new DefaultTopicSelector("test"))
        .withTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper());
builder.setBolt("forwardToKafka", bolt, 8).shuffleGrouping("spout");

Config conf = new Config();

StormSubmitter.submitTopology("kafkaboltTest", conf, builder.createTopology());

----------------------------------------

TITLE: Subscribing to Streams in Storm Bolts
DESCRIPTION: Shows how to subscribe to specific streams or all streams from another component in a Storm topology using InputDeclarer.

LANGUAGE: java
CODE:
// Subscribe to specific stream
declarer.shuffleGrouping("1", "streamName");

// Subscribe to default stream
declarer.shuffleGrouping("1");

// Equivalent to subscribing to default stream
declarer.shuffleGrouping("1", DEFAULT_STREAM_ID);

----------------------------------------

TITLE: Configuring Storm Maven Dependency
DESCRIPTION: Maven dependency configuration to include Storm client library in a project. Uses provided scope since Storm will be provided by the cluster at runtime.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.storm</groupId>
  <artifactId>storm-client</artifactId>
  <version>{{page.version}}</version>
  <scope>provided</scope>
</dependency>

----------------------------------------

TITLE: Implementing Storm Bolt with Output Fields
DESCRIPTION: Example of a Storm bolt implementation that processes input tuples by multiplying values and emitting the results with declared output fields.

LANGUAGE: java
CODE:
public class DoubleAndTripleBolt extends BaseRichBolt {
    private OutputCollectorBase _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollectorBase collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple input) {
        int val = input.getInteger(0);        
        _collector.emit(input, new Values(val*2, val*3));
        _collector.ack(input);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("double", "triple"));
    }    
}

----------------------------------------

TITLE: Implementing SplitSentence as BasicBolt
DESCRIPTION: Simplified bolt implementation using BasicBolt interface which handles anchoring and acking automatically.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseBasicBolt {
        public void execute(Tuple tuple, BasicOutputCollector collector) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                collector.emit(new Values(word));
            }
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Initializing Storm Topology with Spouts and Bolts in Java
DESCRIPTION: This snippet demonstrates how to set up a basic Storm topology with a spout and two bolts for sentence processing and word counting.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("sentences", new KestrelSpout("kestrel.backtype.com",
                                               22133,
                                               "sentence_queue",
                                               new StringScheme()));
builder.setBolt("split", new SplitSentence(), 10)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 20)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Creating StateFactory for Custom State
DESCRIPTION: This snippet shows how to create a StateFactory for the custom LocationDB state, which is used to instantiate the state object within Trident tasks.

LANGUAGE: java
CODE:
public class LocationDBFactory implements StateFactory {
   public State makeState(Map conf, int partitionIndex, int numPartitions) {
      return new LocationDB();
   } 
}

----------------------------------------

TITLE: Overriding Component Configurations in Java for Apache Storm
DESCRIPTION: Demonstrates two methods for specifying component-specific configurations in Apache Storm using Java: internally by overriding getComponentConfiguration method, and externally using TopologyBuilder's setSpout and setBolt methods.

LANGUAGE: java
CODE:
// Internal method
@Override
public Map<String, Object> getComponentConfiguration() {
    // Return component-specific configuration map
}

// External method
topologyBuilder.setSpout("spoutName", spoutInstance)
    .addConfiguration("key", "value")
    .addConfigurations(configMap);

topologyBuilder.setBolt("boltName", boltInstance)
    .addConfiguration("key", "value")
    .addConfigurations(configMap);

----------------------------------------

TITLE: Configuring Storm Topology Components and Workers (Java)
DESCRIPTION: This code snippet shows how to configure a Storm topology with multiple components (spouts and bolts) and set the number of worker processes. It demonstrates setting parallelism hints, number of tasks, and groupings for different components.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(2); // use two worker processes

topologyBuilder.setSpout("blue-spout", new BlueSpout(), 2); // set parallelism hint to 2

topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

topologyBuilder.setBolt("yellow-bolt", new YellowBolt(), 6)
               .shuffleGrouping("green-bolt");

StormSubmitter.submitTopology(
        "mytopology",
        conf,
        topologyBuilder.createTopology()
    );

----------------------------------------

TITLE: Implementing SplitSentence as BasicBolt in Storm
DESCRIPTION: This code shows how to implement the SplitSentence functionality using Storm's BasicBolt interface, which simplifies tuple processing and acknowledgment.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseBasicBolt {
        public void execute(Tuple tuple, BasicOutputCollector collector) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                collector.emit(new Values(word));
            }
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Implementing Streaming Word Count with Trident in Java
DESCRIPTION: Demonstrates how to create a Trident topology for streaming word count. It processes the input stream of sentences, splits them into words, groups by word, and persistently aggregates the count for each word.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
     topology.newStream("spout1", spout)
       .each(new Fields("sentence"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))                
       .parallelismHint(6);

----------------------------------------

TITLE: Implementing a Trident Function
DESCRIPTION: Example of implementing a Trident Function that emits multiple tuples based on an input value.

LANGUAGE: java
CODE:
public class MyFunction extends BaseFunction {
    public void execute(TridentTuple tuple, TridentCollector collector) {
        for(int i=0; i < tuple.getInteger(0); i++) {
            collector.emit(new Values(i));
        }
    }
}

----------------------------------------

TITLE: Declaring Multiple Streams in a Spout using OutputFieldsDeclarer
DESCRIPTION: This snippet demonstrates how to declare multiple streams in a spout using the declareStream method of OutputFieldsDeclarer. It also shows how to specify the stream to emit to when using the emit method on SpoutOutputCollector.

LANGUAGE: java
CODE:
declareStream("stream1", new Fields("field1", "field2"));
declareStream("stream2", new Fields("field3", "field4"));

// Later, when emitting:
collector.emit("stream1", new Values(value1, value2));
collector.emit("stream2", new Values(value3, value4));

----------------------------------------

TITLE: Complete Storm Topology Configuration in Java
DESCRIPTION: Comprehensive example of configuring a Storm topology with multiple components (spout and bolts) including worker processes, executors, and tasks configuration.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(2); // use two worker processes

topologyBuilder.setSpout("blue-spout", new BlueSpout(), 2); // set parallelism hint to 2

topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

topologyBuilder.setBolt("yellow-bolt", new YellowBolt(), 6)
               .shuffleGrouping("green-bolt");

StormSubmitter.submitTopology(
        "mytopology",
        conf,
        topologyBuilder.createTopology()
    );

----------------------------------------

TITLE: Initializing Storm Topology with Word Count Example
DESCRIPTION: Example showing how to set up a basic Storm topology that reads sentences, splits them into words, and counts occurrences using spouts and bolts.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("sentences", new KestrelSpout("kestrel.backtype.com",
                                               22133,
                                               "sentence_queue",
                                               new StringScheme()));
builder.setBolt("split", new SplitSentence(), 10)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 20)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Defining a Storm Topology with Flux YAML
DESCRIPTION: Example YAML configuration defining a simple word count topology using Flux.

LANGUAGE: yaml
CODE:
name: "yaml-topology"
config:
  topology.workers: 1

# spout definitions
spouts:
  - id: "spout-1"
    className: "org.apache.storm.testing.TestWordSpout"
    parallelism: 1

# bolt definitions
bolts:
  - id: "bolt-1"
    className: "org.apache.storm.testing.TestWordCounter"
    parallelism: 1
  - id: "bolt-2"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1

#stream definitions
streams:
  - name: "spout-1 --> bolt-1" # name isn't used (placeholder for logging, UI, etc.)
    from: "spout-1"
    to: "bolt-1"
    grouping:
      type: FIELDS
      args: ["word"]

  - name: "bolt-1 --> bolt2"
    from: "bolt-1"
    to: "bolt-2"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Implementing Stateful Word Count Bolt in Java
DESCRIPTION: Example implementation of a stateful word count bolt that extends BaseStatefulBolt and uses KeyValueState for maintaining word counts. The bolt demonstrates state initialization, tuple processing, and state management.

LANGUAGE: java
CODE:
public class WordCountBolt extends BaseStatefulBolt<KeyValueState<String, Long>> {
    private KeyValueState<String, Long> wordCounts;
    private OutputCollector collector;
    ...
        @Override
        public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
          this.collector = collector;
        }
        @Override
        public void initState(KeyValueState<String, Long> state) {
          wordCounts = state;
        }
        @Override
        public void execute(Tuple tuple) {
          String word = tuple.getString(0);
          Integer count = wordCounts.get(word, 0);
          count++;
          wordCounts.put(word, count);
          collector.emit(tuple, new Values(word, count));
          collector.ack(tuple);
        }
    ...
    }

----------------------------------------

TITLE: Configuring KafkaSpout in Storm Topology
DESCRIPTION: Example of setting up a KafkaSpout to consume from Kafka in a Storm topology. It demonstrates basic configuration and topic subscription.

LANGUAGE: java
CODE:
final TopologyBuilder tp = new TopologyBuilder();
tp.setSpout("kafka_spout", new KafkaSpout<>(KafkaSpoutConfig.builder("127.0.0.1:" + port, "topic").build()), 1);
tp.setBolt("bolt", new myBolt()).shuffleGrouping("kafka_spout");

----------------------------------------

TITLE: Complete Word Count Example in Java
DESCRIPTION: Shows a complete word count topology implementation using Stream API with windowing and aggregation.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

builder
   // A stream of random sentences with two partitions
   .newStream(new RandomSentenceSpout(), new ValueMapper<String>(0), 2)
   // a two seconds tumbling window
   .window(TumblingWindows.of(Duration.seconds(2)))
   // split the sentences to words
   .flatMap(s -> Arrays.asList(s.split(" ")))
   // create a stream of (word, 1) pairs
   .mapToPair(w -> Pair.of(w, 1))
   // compute the word counts in the last two second window
   .countByKey()
   // print the results to stdout
   .print();

----------------------------------------

TITLE: Stream Builder and Value Mapper Usage
DESCRIPTION: Shows how to create streams using StreamBuilder and extract typed values from tuples using value mappers.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();
// extract the first field from the tuple to get a Stream<String> of sentences
Stream<String> sentences = builder.newStream(new TestWordSpout(), new ValueMapper<String>(0));

// extract first three fields of the tuple emitted by the spout to produce a stream of typed tuples.
Stream<Tuple3<String, Integer, Long>> stream = builder.newStream(new TestSpout(), TupleValueMappers.of(0, 1, 2));

----------------------------------------

TITLE: Complete Storm Topology Configuration in Java
DESCRIPTION: Comprehensive example of configuring a Storm topology with multiple components. Demonstrates setting up worker processes, spouts, and bolts with different parallelism settings and stream groupings.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(2); // use two worker processes

topologyBuilder.setSpout("blue-spout", new BlueSpout(), 2); // set parallelism hint to 2

topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

topologyBuilder.setBolt("yellow-bolt", new YellowBolt(), 6)
               .shuffleGrouping("green-bolt");

StormSubmitter.submitTopology(
        "mytopology",
        conf,
        topologyBuilder.createTopology()
    );

----------------------------------------

TITLE: Specifying DRPC Servers in Storm YAML
DESCRIPTION: This snippet shows how to specify the DRPC servers in the Storm configuration file, which is necessary for workers to find them.

LANGUAGE: yaml
CODE:
drpc.servers: ["111.222.333.44"]

----------------------------------------

TITLE: Implementing Parallel Top N Pattern in Storm
DESCRIPTION: Demonstrates a scalable approach to computing top N items by parallel processing across partitions followed by merging results. Uses fields grouping for initial partitioning and global grouping for final aggregation.

LANGUAGE: java
CODE:
builder.setBolt("rank", new RankObjects(), parallelism)
  .fieldsGrouping("objects", new Fields("value"));
builder.setBolt("merge", new MergeObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Initializing Trident Topology with Persistent Aggregation
DESCRIPTION: Example of creating a Trident topology that performs word count aggregation and persists results to Memcached using opaque transactional semantics.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
      topology.newStream("spout1", spout)
        .each(new Fields("sentence"), new Split(), new Fields("word"))
        .groupBy(new Fields("word"))
        .persistentAggregate(MemcachedState.opaque(serverLocations), new Count(), new Fields("count"))                
        .parallelismHint(6);

----------------------------------------

TITLE: Configuring Remote Storm Cluster in YAML
DESCRIPTION: This YAML configuration snippet shows how to specify the Nimbus seed node for a remote Storm cluster in the ~/.storm/storm.yaml file. It sets the IP address of the master node to enable communication between the local machine and the remote cluster.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["123.45.678.890"]

----------------------------------------

TITLE: Complete Word Count Example in Java Stream API
DESCRIPTION: A complete example demonstrating how to build a word count topology using Stream API, including windowing, transformations, and aggregations.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

builder
   // A stream of random sentences with two partitions
   .newStream(new RandomSentenceSpout(), new ValueMapper<String>(0), 2)
   // a two seconds tumbling window
   .window(TumblingWindows.of(Duration.seconds(2)))
   // split the sentences to words
   .flatMap(s -> Arrays.asList(s.split(" ")))
   // create a stream of (word, 1) pairs
   .mapToPair(w -> Pair.of(w, 1))
   // compute the word counts in the last two second window
   .countByKey()
   // print the results to stdout
   .print();

Config config = new Config();
config.setNumWorkers(1);
StormSubmitter.submitTopologyWithProgressBar("topology-name", config, builder.build());

----------------------------------------

TITLE: Implementing Stateful Word Count Bolt in Java
DESCRIPTION: Example of implementing a stateful word count bolt by extending BaseStatefulBolt and using KeyValueState to store word counts. The bolt initializes state, updates counts on tuple execution, and allows the framework to periodically checkpoint state.

LANGUAGE: java
CODE:
public class WordCountBolt extends BaseStatefulBolt<KeyValueState<String, Long>> {
    private KeyValueState<String, Long> wordCounts;
    private OutputCollector collector;
    ...
        @Override
        public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
          this.collector = collector;
        }
        @Override
        public void initState(KeyValueState<String, Long> state) {
          wordCounts = state;
        }
        @Override
        public void execute(Tuple tuple) {
          String word = tuple.getString(0);
          Integer count = wordCounts.get(word, 0);
          count++;
          wordCounts.put(word, count);
          collector.emit(tuple, new Values(word, count));
          collector.ack(tuple);
        }
    ...
    }

----------------------------------------

TITLE: Implementing a Bolt in Java
DESCRIPTION: Example of a bolt implementation in Java that appends '!!!' to input tuples. It demonstrates the basic structure of a bolt, including prepare, execute, and declareOutputFields methods.

LANGUAGE: java
CODE:
public class DoubleAndTripleBolt extends BaseRichBolt {
    private OutputCollectorBase _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollectorBase collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple input) {
        int val = input.getInteger(0);        
        _collector.emit(input, new Values(val*2, val*3));
        _collector.ack(input);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("double", "triple"));
    }    
}

----------------------------------------

TITLE: Implementing DRPC Topology with LinearDRPCTopologyBuilder in Java
DESCRIPTION: This example shows how to create a simple DRPC topology using LinearDRPCTopologyBuilder. It implements a function that appends '!' to the input argument.

LANGUAGE: java
CODE:
public static class ExclaimBolt extends BaseBasicBolt {
    public void execute(Tuple tuple, BasicOutputCollector collector) {
        String input = tuple.getString(1);
        collector.emit(new Values(tuple.getValue(0), input + "!"));
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "result"));
    }
}

public static void main(String[] args) throws Exception {
    LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("exclamation");
    builder.addBolt(new ExclaimBolt(), 3);
    // ...
}

----------------------------------------

TITLE: Creating Word Count Topology in Java
DESCRIPTION: Implements a streaming word count topology using Trident, splitting sentences into words and maintaining persistent counts.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
     topology.newStream("spout1", spout)
       .each(new Fields("sentence"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))                
       .parallelismHint(6);

----------------------------------------

TITLE: Running a Storm Topology
DESCRIPTION: Command to package and run a Storm topology jar file with arguments.

LANGUAGE: bash
CODE:
storm jar all-my-code.jar org.apache.storm.MyTopology arg1 arg2

----------------------------------------

TITLE: Submitting Storm Topology with Configuration in Java
DESCRIPTION: This snippet demonstrates how to use StormSubmitter to submit a Storm topology to a cluster with specific configuration settings for the number of workers and maximum pending spouts.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(20);
conf.setMaxSpoutPending(5000);
StormSubmitter.submitTopology("mytopology", conf, topology);

----------------------------------------

TITLE: Declaring Multiple Streams in a Spout
DESCRIPTION: Example of how to declare multiple streams in a Storm spout using the OutputFieldsDeclarer and emit to specific streams using SpoutOutputCollector.

LANGUAGE: java
CODE:
declareStream(OutputFieldsDeclarer declarer) {
    declarer.declareStream("stream1", new Fields("field1", "field2"));
    declarer.declareStream("stream2", new Fields("field3", "field4"));
}

nextTuple(SpoutOutputCollector collector) {
    collector.emit("stream1", new Values(value1, value2));
    collector.emit("stream2", new Values(value3, value4));
}

----------------------------------------

TITLE: Configuring Isolation Scheduler Machines in YAML
DESCRIPTION: YAML configuration example showing how to allocate dedicated machines to specific topologies using the isolation scheduler. This configuration maps topology names to their allocated machine count for resource isolation.

LANGUAGE: yaml
CODE:
isolation.scheduler.machines: 
    "my-topology": 8
    "tiny-topology": 1
    "some-other-topology": 3

----------------------------------------

TITLE: Creating and Querying Kafka Streams with Storm SQL
DESCRIPTION: Example showing how to create external tables mapping to Kafka streams and perform filtering and aggregation operations. The query filters orders where total amount exceeds 50 and projects the ID and total amount to a new stream.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://...' ...

CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) 'kafka://...' ...

INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Initializing Local Storm Cluster in Java
DESCRIPTION: Creates and manages an in-process Storm cluster using LocalCluster class for testing topologies. Uses try-with-resources to ensure proper cleanup.

LANGUAGE: java
CODE:
import org.apache.storm.LocalCluster;

...

try (LocalCluster cluster = new LocalCluster()) {
    //Interact with the cluster...
}

----------------------------------------

TITLE: Configuring Remote Cluster Access in YAML
DESCRIPTION: This YAML configuration specifies the Nimbus seed node for connecting to a remote Storm cluster. It should be placed in the '~/.storm/storm.yaml' file to allow the 'storm' client to communicate with the cluster.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["123.45.678.890"]

----------------------------------------

TITLE: Creating a Word Count Topology in Java
DESCRIPTION: Shows how to set up a topology for counting word occurrences using different groupings.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
        
builder.setSpout("sentences", new RandomSentenceSpout(), 5);        
builder.setBolt("split", new SplitSentence(), 8)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 12)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Setting up Maven POM for Storm Project in Java
DESCRIPTION: This snippet shows how to configure the pom.xml file for a Storm project using Maven. It includes the necessary dependencies and build settings for Storm development.

LANGUAGE: XML
CODE:
<pom.xml>

----------------------------------------

TITLE: Implementing a Stateful Persistent Windowed Bolt in Storm
DESCRIPTION: This example demonstrates how to implement a stateful persistent windowed bolt that uses window checkpointing to save its state. It includes methods for initializing state and executing windowed operations using an iterator for efficient tuple processing.

LANGUAGE: java
CODE:
public class MyStatefulPersistentWindowedBolt extends BaseStatefulWindowedBolt<K, V> {
  private KeyValueState<K, V> state;
  
  @Override
  public void initState(KeyValueState<K, V> state) {
    this.state = state;
   // ...
   // restore the state from the last saved state.
   // ...
  }
  
  @Override
  public void execute(TupleWindow window) {      
    // iterate over tuples in the current window
    Iterator<Tuple> it = window.getIter();
    while (it.hasNext()) {
        // compute some result based on the tuples in window
    }
    
    // possibly update any state to be maintained across windows
    state.put(STATE_KEY, updatedValue);
    
    // emit the results downstream
    collector.emit(new Values(result));
  }
}

----------------------------------------

TITLE: Defining Storm Topology Components in YAML
DESCRIPTION: Example YAML configuration defining spouts, bolts, and streams for a simple word count topology.

LANGUAGE: yaml
CODE:
name: "yaml-topology"
config:
  topology.workers: 1

# spout definitions
spouts:
  - id: "spout-1"
    className: "org.apache.storm.testing.TestWordSpout"
    parallelism: 1

# bolt definitions
bolts:
  - id: "bolt-1"
    className: "org.apache.storm.testing.TestWordCounter"
    parallelism: 1
  - id: "bolt-2"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1

#stream definitions
streams:
  - name: "spout-1 --> bolt-1" # name isn't used (placeholder for logging, UI, etc.)
    from: "spout-1"
    to: "bolt-1"
    grouping:
      type: FIELDS
      args: ["word"]

  - name: "bolt-1 --> bolt2"
    from: "bolt-1"
    to: "bolt-2"
    grouping:
      type: SHUFFLE

# worker hook definitions
workerHooks:
  - id: "base-worker-hook"
    className: "org.apache.storm.hooks.BaseWorkerHook"

----------------------------------------

TITLE: Implementing Word Count Stream Processing in Trident
DESCRIPTION: Demonstrates how to process a stream of sentences to count words using Trident's topology API with persistent aggregation.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
     topology.newStream("spout1", spout)
       .each(new Fields("sentence"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))                
       .parallelismHint(6);

----------------------------------------

TITLE: Filtering Kafka Stream Example in Storm SQL
DESCRIPTION: Set of SQL statements that define input and output Kafka streams, and a SELECT statement to filter orders where the total price is greater than 50. This example demonstrates how to use Storm SQL for stream processing.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://orders?bootstrap-servers=localhost:9092,localhost:9093'
CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) LOCATION 'kafka://large_orders?bootstrap-servers=localhost:9092,localhost:9093' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Initializing Local Storm Cluster in Java
DESCRIPTION: Creates an in-process Storm cluster using LocalCluster class for testing topologies. The cluster is automatically closed when leaving the try block scope.

LANGUAGE: java
CODE:
import org.apache.storm.LocalCluster;

...

try (LocalCluster cluster = new LocalCluster()) {
    //Interact with the cluster...
}

----------------------------------------

TITLE: Defining a Simple Storm Topology in Java
DESCRIPTION: Illustrates how to create a topology with a spout and two bolts using the TopologyBuilder.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();        
builder.setSpout("words", new TestWordSpout(), 10);        
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
        .shuffleGrouping("words");
builder.setBolt("exclaim2", new ExclamationBolt(), 2)
        .shuffleGrouping("exclaim1");

----------------------------------------

TITLE: Creating a Stream from a Regular Storm Spout in Trident Topology
DESCRIPTION: This snippet demonstrates how to create a stream in a Trident topology using a regular Storm IRichSpout. It shows the initialization of a TridentTopology object and the creation of a new stream with a unique identifier and a custom spout implementation.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
topology.newStream("myspoutid", new MyRichSpout());

----------------------------------------

TITLE: Storm Spout Implementation
DESCRIPTION: Implementation of a test word spout that emits random words every 100ms.

LANGUAGE: java
CODE:
public void nextTuple() {
    Utils.sleep(100);
    final String[] words = new String[] {"nathan", "mike", "jackson", "golda", "bertels"};
    final Random rand = new Random();
    final String word = words[rand.nextInt(words.length)];
    _collector.emit(new Values(word));
}

----------------------------------------

TITLE: Defining Storm Topology Components in YAML
DESCRIPTION: Example YAML configuration defining spouts, bolts, and streams for a simple word count topology.

LANGUAGE: yaml
CODE:
name: "yaml-topology"
config:
  topology.workers: 1

# spout definitions
spouts:
  - id: "spout-1"
    className: "org.apache.storm.testing.TestWordSpout"
    parallelism: 1

# bolt definitions
bolts:
  - id: "bolt-1"
    className: "org.apache.storm.testing.TestWordCounter"
    parallelism: 1
  - id: "bolt-2"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1

#stream definitions
streams:
  - name: "spout-1 --> bolt-1" # name isn't used (placeholder for logging, UI, etc.)
    from: "spout-1"
    to: "bolt-1"
    grouping:
      type: FIELDS
      args: ["word"]

  - name: "bolt-1 --> bolt2"
    from: "bolt-1"
    to: "bolt-2"
    grouping:
      type: SHUFFLE

# worker hook definitions
workerHooks:
  - id: "base-worker-hook"
    className: "org.apache.storm.hooks.BaseWorkerHook"

----------------------------------------

TITLE: Subscribing to Streams in Storm Bolts
DESCRIPTION: This snippet shows how to subscribe to specific streams of another component in a Storm bolt using InputDeclarer. It includes examples of subscribing to the default stream and a named stream.

LANGUAGE: java
CODE:
builder.setBolt("bolt1", new MyBolt())
    .shuffleGrouping("1"); // Subscribe to default stream of component "1"

builder.setBolt("bolt2", new MyBolt())
    .shuffleGrouping("1", "myStream"); // Subscribe to "myStream" of component "1"

----------------------------------------

TITLE: Implementing a Spout in Java
DESCRIPTION: Example of a simple spout implementation in Java that emits random words at regular intervals. It demonstrates the basic structure of a spout's nextTuple method.

LANGUAGE: java
CODE:
public void nextTuple() {
    Utils.sleep(100);
    final String[] words = new String[] {"nathan", "mike", "jackson", "golda", "bertels"};
    final Random rand = new Random();
    final String word = words[rand.nextInt(words.length)];
    _collector.emit(new Values(word));
}

----------------------------------------

TITLE: Implementing DRPC Topology with LinearDRPCTopologyBuilder in Java
DESCRIPTION: This example demonstrates how to create a simple DRPC topology using LinearDRPCTopologyBuilder that appends '!' to the input argument.

LANGUAGE: java
CODE:
public static class ExclaimBolt extends BaseBasicBolt {
    public void execute(Tuple tuple, BasicOutputCollector collector) {
        String input = tuple.getString(1);
        collector.emit(new Values(tuple.getValue(0), input + "!"));
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "result"));
    }
}

public static void main(String[] args) throws Exception {
    LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("exclamation");
    builder.addBolt(new ExclaimBolt(), 3);
    // ...
}

----------------------------------------

TITLE: Defining Storm Topology in Clojure
DESCRIPTION: Example of defining a Storm topology with multiple spouts and bolts using the topology function. Shows how to wire components together and specify parallelism hints.

LANGUAGE: clojure
CODE:
(topology
 {"1" (spout-spec sentence-spout)
  "2" (spout-spec (sentence-spout-parameterized
                   ["the cat jumped over the door"
                    "greetings from a faraway land"])
                   :p 2)}
 {"3" (bolt-spec {"1" :shuffle "2" :shuffle}
                 split-sentence
                 :p 5)
  "4" (bolt-spec {"3" ["word"]}
                 word-count
                 :p 6)})

----------------------------------------

TITLE: Building Simple Storm Topology
DESCRIPTION: Example of building a Storm topology with spouts and bolts using TopologyBuilder

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();        
builder.setSpout("words", new TestWordSpout(), 10);        
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
        .shuffleGrouping("words");
builder.setBolt("exclaim2", new ExclamationBolt(), 2)
        .shuffleGrouping("exclaim1");

----------------------------------------

TITLE: Creating Trident Word Count Topology
DESCRIPTION: Implements streaming word count using Trident topology, splitting sentences into words and maintaining persistent count aggregation.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
     topology.newStream("spout1", spout)
       .each(new Fields("sentence"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))                
       .parallelismHint(6);

----------------------------------------

TITLE: Configuring a Simple Insecure Kafka Spout in Java
DESCRIPTION: Example of setting up a basic Kafka spout to consume events from a topic.

LANGUAGE: java
CODE:
final TopologyBuilder tp = new TopologyBuilder();
tp.setSpout("kafka_spout", new KafkaSpout<>(KafkaSpoutConfig.builder("127.0.0.1:" + port, "topic").build()), 1);
tp.setBolt("bolt", new myBolt()).shuffleGrouping("kafka_spout");

----------------------------------------

TITLE: Configuring Zookeeper Servers in Storm YAML
DESCRIPTION: Configuration snippet for specifying Zookeeper server addresses in storm.yaml

LANGUAGE: yaml
CODE:
storm.zookeeper.servers:
  - "111.222.333.444"
  - "555.666.777.888"

----------------------------------------

TITLE: Emitting a Tuple with Message ID in Storm Spout
DESCRIPTION: This snippet demonstrates how to emit a tuple from a Storm spout with a message ID for tracking and reliability purposes.

LANGUAGE: java
CODE:
_collector.emit(new Values("field1", "field2", 3) , msgId);

----------------------------------------

TITLE: Component Configuration Java Example
DESCRIPTION: Two approaches for specifying component-specific configurations in Storm: internally by overriding getComponentConfiguration method, and externally using TopologyBuilder's setSpout and setBolt methods.

LANGUAGE: java
CODE:
// Internal Configuration
@Override
public Map<String, Object> getComponentConfiguration() {
    // Return component-specific configuration map
}

// External Configuration
topologyBuilder.setSpout("spout-id", spout)
    .addConfiguration("key", "value")
    .addConfigurations(configMap);

----------------------------------------

TITLE: IWindowedBolt Interface Definition
DESCRIPTION: Core interface that must be implemented by bolts requiring windowing support. Defines methods for preparing, executing window operations, and cleanup.

LANGUAGE: java
CODE:
public interface IWindowedBolt extends IComponent {
    void prepare(Map stormConf, TopologyContext context, OutputCollector collector);
    /**
     * Process tuples falling within the window and optionally emit 
     * new tuples based on the tuples in the input window.
     */
    void execute(TupleWindow inputWindow);
    void cleanup();
}

----------------------------------------

TITLE: Implementing a Stateful Windowed Bolt in Java
DESCRIPTION: Example of implementing a stateful windowed bolt that uses checkpointing to save its state. Shows how to initialize state and process window tuples.

LANGUAGE: java
CODE:
public class MyStatefulPersistentWindowedBolt extends BaseStatefulWindowedBolt<K, V> {
  private KeyValueState<K, V> state;
  
  @Override
  public void initState(KeyValueState<K, V> state) {
    this.state = state;
   // ...
   // restore the state from the last saved state.
   // ...
  }
  
  @Override
  public void execute(TupleWindow window) {      
    // iterate over tuples in the current window
    Iterator<Tuple> it = window.getIter();
    while (it.hasNext()) {
        // compute some result based on the tuples in window
    }
    
    // possibly update any state to be maintained across windows
    state.put(STATE_KEY, updatedValue);
    
    // emit the results downstream
    collector.emit(new Values(result));
  }
}

----------------------------------------

TITLE: Configuring Maven Dependencies for Storm Kinesis Spout
DESCRIPTION: This XML snippet shows the required Maven dependencies for using the Storm Kinesis spout. It includes dependencies for AWS SDK, Storm client, Apache Curator, and JSON Simple library. The AWS SDK version used is 1.10.77.

LANGUAGE: xml
CODE:
 <dependencies>
        <dependency>
            <groupId>com.amazonaws</groupId>
            <artifactId>aws-java-sdk</artifactId>
            <version>${aws-java-sdk.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-client</artifactId>
            <version>${project.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-framework</artifactId>
            <version>${curator.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>com.googlecode.json-simple</groupId>
            <artifactId>json-simple</artifactId>
        </dependency>
 </dependencies>

----------------------------------------

TITLE: Basic Stream Creation and Operations in Java
DESCRIPTION: Demonstrates creating a stream from a spout and performing basic transformations like mapping and filtering. Shows how to create a StreamBuilder and process sentences into words.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

// a stream of sentences obtained from a source spout
Stream<String> sentences = builder.newStream(new RandomSentenceSpout()).map(tuple -> tuple.getString(0));

// a stream of words obtained by transforming (splitting) the stream of sentences
Stream<String> words = sentences.flatMap(s -> Arrays.asList(s.split(" ")));

// output operation that prints the words to console
words.forEach(w -> System.out.println(w));

----------------------------------------

TITLE: Configuring Maven POM for Storm Project
DESCRIPTION: Example shows how to reference Storm dependencies in Maven pom.xml configuration. Used to set up project dependencies and build settings.

LANGUAGE: xml
CODE:
{{page.git-blob-base}}/examples/storm-starter/pom.xml

----------------------------------------

TITLE: Running Storm Local Mode via Command Line
DESCRIPTION: Example of launching a topology in local mode with debug configuration enabled using the storm command line interface.

LANGUAGE: bash
CODE:
storm local topology.jar <MY_MAIN_CLASS> -c topology.debug=true

----------------------------------------

TITLE: Initializing Local Storm Cluster in Java
DESCRIPTION: Creates and manages an in-process Storm cluster using LocalCluster class for testing purposes. The cluster automatically shuts down when the try block ends due to AutoCloseable implementation.

LANGUAGE: java
CODE:
import org.apache.storm.LocalCluster;

...

try (LocalCluster cluster = new LocalCluster()) {
    //Interact with the cluster...
}

----------------------------------------

TITLE: Filtering and Mapping Operations in Storm's Stream API (Java)
DESCRIPTION: Demonstrates basic stream transformations like filtering and mapping.

LANGUAGE: java
CODE:
Stream<String> logs = ...
Stream<String> errors = logs.filter(line -> line.contains("ERROR"));

Stream<String> words = ...
Stream<Integer> wordLengths = words.map(String::length);

----------------------------------------

TITLE: Running a Storm Topology in Java
DESCRIPTION: Example command to run a Storm topology jar file with arguments.

LANGUAGE: bash
CODE:
storm jar all-my-code.jar org.apache.storm.MyTopology arg1 arg2

----------------------------------------

TITLE: Configuring URL Expansion with Fields Grouping in Storm
DESCRIPTION: Example showing URL expansion bolt configuration using fields grouping to ensure consistent URL routing for efficient caching.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .fieldsGrouping("urls", new Fields("url"));

----------------------------------------

TITLE: Overriding Component Configuration in Java for Storm Topologies
DESCRIPTION: Demonstrates two methods for specifying component-specific configurations in Storm topologies using Java. The first method overrides the getComponentConfiguration method, while the second uses the TopologyBuilder's setSpout and setBolt methods.

LANGUAGE: java
CODE:
// Method 1: Internally override getComponentConfiguration
@Override
public Map<String, Object> getComponentConfiguration() {
    return componentSpecificConfig;
}

// Method 2: Externally use TopologyBuilder
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("spout", new MySpout())
       .addConfiguration("topology.debug", true);
builder.setBolt("bolt", new MyBolt())
       .addConfigurations(boltConfig);

----------------------------------------

TITLE: Setting Sampling Rate for Storm Metrics
DESCRIPTION: Controls the frequency of metric computation on Spout and Bolt executors, affecting performance.

LANGUAGE: java
CODE:
topology.stats.sample.rate

----------------------------------------

TITLE: Submitting Storm Topology Using StormSubmitter in Java
DESCRIPTION: Example showing how to submit a Storm topology to a cluster using StormSubmitter with basic configuration settings for workers and spout pending messages.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(20);
conf.setMaxSpoutPending(5000);
StormSubmitter.submitTopology("mytopology", conf, topology);

----------------------------------------

TITLE: Monitoring Storm Topology Throughput
DESCRIPTION: Interactively monitors a topology's throughput with customizable intervals and components.

LANGUAGE: bash
CODE:
storm monitor topology-name [-i interval-secs] [-m component-id] [-s stream-id] [-w [emitted | transferred]]

----------------------------------------

TITLE: Implementing Streaming Word Count in Trident
DESCRIPTION: Demonstrates how to create a Trident topology for streaming word count, including splitting sentences into words, grouping by word, and persisting aggregated counts.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
     topology.newStream("spout1", spout)
       .each(new Fields("sentence"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))                
       .parallelismHint(6);

----------------------------------------

TITLE: Defining a Transactional Topology in Apache Storm
DESCRIPTION: Example of creating a transactional topology that computes a global count of tuples from an input stream using TransactionalTopologyBuilder.

LANGUAGE: java
CODE:
MemoryTransactionalSpout spout = new MemoryTransactionalSpout(DATA, new Fields("word"), PARTITION_TAKE_PER_BATCH);
TransactionalTopologyBuilder builder = new TransactionalTopologyBuilder("global-count", "spout", spout, 3);
builder.setBolt("partial-count", new BatchCount(), 5)
        .shuffleGrouping("spout");
builder.setBolt("sum", new UpdateGlobalCount())
        .globalGrouping("partial-count");

----------------------------------------

TITLE: Configuring HDFS State for Trident API in Java
DESCRIPTION: Example of configuring HDFS State implementation for Trident API to write data to HDFS with specific formats and policies.

LANGUAGE: java
CODE:
Fields hdfsFields = new Fields("field1", "field2");

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withPath("/trident")
        .withPrefix("trident")
        .withExtension(".txt");

RecordFormat recordFormat = new DelimitedRecordFormat()
        .withFields(hdfsFields);

FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, FileSizeRotationPolicy.Units.MB);

HdfsState.Options options = new HdfsState.HdfsFileOptions()
        .withFileNameFormat(fileNameFormat)
        .withRecordFormat(recordFormat)
        .withRotationPolicy(rotationPolicy)
        .withFsUrl("hdfs://localhost:54310");

StateFactory factory = new HdfsStateFactory().withOptions(options);

TridentState state = stream
        .partitionPersist(factory, hdfsFields, new HdfsUpdater(), new Fields());

----------------------------------------

TITLE: Basic Stream Creation and Operations in Java
DESCRIPTION: Demonstrates creating a stream from a spout and performing basic transformations like mapping and filtering. Shows how to create a StreamBuilder and process sentences into words.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

// a stream of sentences obtained from a source spout
Stream<String> sentences = builder.newStream(new RandomSentenceSpout()).map(tuple -> tuple.getString(0));

// a stream of words obtained by transforming (splitting) the stream of sentences
Stream<String> words = sentences.flatMap(s -> Arrays.asList(s.split(" ")));

// output operation that prints the words to console
words.forEach(w -> System.out.println(w));

----------------------------------------

TITLE: Configuring Storm Bolt Parallelism
DESCRIPTION: Example showing how to configure a Storm bolt with specific executor and task settings. Sets up GreenBolt with 2 executors and 4 tasks using shuffleGrouping.

LANGUAGE: java
CODE:
topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

----------------------------------------

TITLE: Implementing Windowing in Trident
DESCRIPTION: Example of implementing tumbling and sliding windows in Trident streams.

LANGUAGE: java
CODE:
HBaseWindowsStoreFactory windowStoreFactory = new HBaseWindowsStoreFactory(new HashMap<String, Object>(), "window-state", "cf".getBytes("UTF-8"), "tuples".getBytes("UTF-8"));
FixedBatchSpout spout = new FixedBatchSpout(new Fields("sentence"), 3, new Values("the cow jumped over the moon"),
        new Values("the man went to the store and bought some candy"), new Values("four score and seven years ago"),
        new Values("how many apples can you eat"), new Values("to be or not to be the person"));
spout.setCycle(true);

TridentTopology topology = new TridentTopology();

Stream stream = topology.newStream("spout1", spout).parallelismHint(16).each(new Fields("sentence"),
        new Split(), new Fields("word"))
        .window(TumblingCountWindow.of(1000), windowStoreFactory, new Fields("word"), new CountAsAggregator(), new Fields("count"))
        .peek(new Consumer() {
            @Override
            public void accept(TridentTuple input) {
                LOG.info("Received tuple: [{}]", input);
            }
        });

StormTopology stormTopology =  topology.build();

----------------------------------------

TITLE: Implementing Stateful Word Count Bolt in Java
DESCRIPTION: Example of a stateful word count bolt that extends BaseStatefulBolt and uses KeyValueState to store word counts. The bolt initializes state, updates counts on each tuple, and emits updated counts.

LANGUAGE: java
CODE:
public class WordCountBolt extends BaseStatefulBolt<KeyValueState<String, Long>> {
    private KeyValueState<String, Long> wordCounts;
    private OutputCollector collector;
    ...
        @Override
        public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
          this.collector = collector;
        }
        @Override
        public void initState(KeyValueState<String, Long> state) {
          wordCounts = state;
        }
        @Override
        public void execute(Tuple tuple) {
          String word = tuple.getString(0);
          Integer count = wordCounts.get(word, 0);
          count++;
          wordCounts.put(word, count);
          collector.emit(tuple, new Values(word, count));
          collector.ack(tuple);
        }
    ...
    }

----------------------------------------

TITLE: Creating and Using LocalCluster in Java
DESCRIPTION: Demonstrates how to create an in-process Storm cluster using the LocalCluster class for testing and development purposes.

LANGUAGE: java
CODE:
import org.apache.storm.LocalCluster;

...

try (LocalCluster cluster = new LocalCluster()) {
    //Interact with the cluster...
}

----------------------------------------

TITLE: Storm JoinBolt Configuration
DESCRIPTION: Example showing how to configure JoinBolt to join multiple streams with field grouping and window configuration. Demonstrates inner and left joins with field selection and tumbling window setup.

LANGUAGE: java
CODE:
JoinBolt jbolt =  new JoinBolt("spout1", "key1")                   // from        spout1  
                    .join     ("spout2", "userId",  "spout1")      // inner join  spout2  on spout2.userId = spout1.key1
                    .join     ("spout3", "key3",    "spout2")      // inner join  spout3  on spout3.key3   = spout2.userId   
                    .leftJoin ("spout4", "key4",    "spout3")      // left join   spout4  on spout4.key4   = spout3.key3
                    .select  ("userId, key4, key2, spout3:key3")   // chose output fields
                    .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;

topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("spout1", new Fields("key1") )
            .fieldsGrouping("spout2", new Fields("userId") )
            .fieldsGrouping("spout3", new Fields("key3") )
            .fieldsGrouping("spout4", new Fields("key4") );

----------------------------------------

TITLE: Implementing Skewed Streaming Top N Pattern in Storm
DESCRIPTION: Example showing how to implement a streaming top N pattern that handles data skew using partial key grouping.

LANGUAGE: java
CODE:
builder.setBolt("count", new CountObjects(), parallelism)
  .partialKeyGrouping("objects", new Fields("value"));
builder.setBolt("rank" new AggregateCountsAndRank(), parallelism)
  .fieldsGrouping("count", new Fields("key"))
builder.setBolt("merge", new MergeRanksObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Storm Bolt Implementation with Anchoring
DESCRIPTION: Example bolt implementation that demonstrates proper tuple anchoring and acknowledgment for reliability.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseRichBolt {
        OutputCollector _collector;
        
        public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
            _collector = collector;
        }

        public void execute(Tuple tuple) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                _collector.emit(tuple, new Values(word));
            }
            _collector.ack(tuple);
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Submitting Storm Topology using StormSubmitter
DESCRIPTION: Code example showing how to configure and submit a Storm topology to a cluster using StormSubmitter. Sets worker count and maximum pending spout configurations.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(20);
conf.setMaxSpoutPending(5000);
StormSubmitter.submitTopology("mytopology", conf, topology);

----------------------------------------

TITLE: Implementing a ReducerAggregator in Trident
DESCRIPTION: Example of implementing a ReducerAggregator for counting tuples.

LANGUAGE: java
CODE:
public class Count implements ReducerAggregator<Long> {
    public Long init() {
        return 0L;
    }
    
    public Long reduce(Long curr, TridentTuple tuple) {
        return curr + 1;
    }
}

----------------------------------------

TITLE: Defining a Transactional Topology in Apache Storm
DESCRIPTION: Example of creating a transactional topology that computes a global count of tuples from an input stream using TransactionalTopologyBuilder.

LANGUAGE: java
CODE:
MemoryTransactionalSpout spout = new MemoryTransactionalSpout(DATA, new Fields("word"), PARTITION_TAKE_PER_BATCH);
TransactionalTopologyBuilder builder = new TransactionalTopologyBuilder("global-count", "spout", spout, 3);
builder.setBolt("partial-count", new BatchCount(), 5)
        .shuffleGrouping("spout");
builder.setBolt("sum", new UpdateGlobalCount())
        .globalGrouping("partial-count");

----------------------------------------

TITLE: Defining IStateful Bolt Hooks in Java
DESCRIPTION: Interface definition for IStateful bolt hooks, including methods for pre-commit, pre-prepare, and pre-rollback actions.

LANGUAGE: java
CODE:
/**
 * This is a hook for the component to perform some actions just before the
 * framework commits its state.
 */
void preCommit(long txid);

/**
 * This is a hook for the component to perform some actions just before the
 * framework prepares its state.
 */
void prePrepare(long txid);

/**
 * This is a hook for the component to perform some actions just before the
 * framework rolls back the prepared state.
 */
void preRollback();

----------------------------------------

TITLE: Defining a Simple Storm Topology in Java
DESCRIPTION: Illustrates how to create a simple topology with a spout and two bolts using the TopologyBuilder class.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();        
builder.setSpout("words", new TestWordSpout(), 10);        
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
        .shuffleGrouping("words");
builder.setBolt("exclaim2", new ExclamationBolt(), 2)
        .shuffleGrouping("exclaim1");

----------------------------------------

TITLE: Implementing a Spout in Java
DESCRIPTION: Example of a Java spout that emits random words every 100ms.

LANGUAGE: java
CODE:
public void nextTuple() {
    Utils.sleep(100);
    final String[] words = new String[] {"nathan", "mike", "jackson", "golda", "bertels"};
    final Random rand = new Random();
    final String word = words[rand.nextInt(words.length)];
    _collector.emit(new Values(word));
}

----------------------------------------

TITLE: Implementing StateUpdater for Custom State
DESCRIPTION: Demonstrates how to implement a StateUpdater for updating a custom State object in bulk.

LANGUAGE: Java
CODE:
public class LocationUpdater extends BaseStateUpdater<LocationDB> {
    public void updateState(LocationDB state, List<TridentTuple> tuples, TridentCollector collector) {
        List<Long> ids = new ArrayList<Long>();
        List<String> locations = new ArrayList<String>();
        for(TridentTuple t: tuples) {
            ids.add(t.getLong(0));
            locations.add(t.getString(1));
        }
        state.setLocationsBulk(ids, locations);
    }
}

----------------------------------------

TITLE: Implementing Word Count using Stream API in Java
DESCRIPTION: Demonstrates a complete word count implementation using Storm's Stream API, including windowing, flatMap, and countByKey operations.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

builder
   // A stream of random sentences with two partitions
   .newStream(new RandomSentenceSpout(), new ValueMapper<String>(0), 2)
   // a two seconds tumbling window
   .window(TumblingWindows.of(Duration.seconds(2)))
   // split the sentences to words
   .flatMap(s -> Arrays.asList(s.split(" ")))
   // create a stream of (word, 1) pairs
   .mapToPair(w -> Pair.of(w, 1))
   // compute the word counts in the last two second window
   .countByKey()
   // print the results to stdout
   .print();

----------------------------------------

TITLE: Component Configuration Example - Java
DESCRIPTION: Example showing how to override component configurations in Storm using two methods: internally by overriding getComponentConfiguration method, and externally using TopologyBuilder's setSpout and setBolt methods.

LANGUAGE: java
CODE:
// Method 1: Internal Configuration
@Override
public Map<String, Object> getComponentConfiguration() {
    // Return component-specific config map
}

// Method 2: External Configuration
topologyBuilder.setSpout("spout-id", spout)
    .addConfiguration("topology.debug", true);

----------------------------------------

TITLE: Implementing Stateful Word Count Bolt in Java
DESCRIPTION: Example of a stateful word count bolt that extends BaseStatefulBolt and uses KeyValueState to store word counts. It shows how to initialize state, update counts in the execute method, and emit results.

LANGUAGE: java
CODE:
public class WordCountBolt extends BaseStatefulBolt<KeyValueState<String, Long>> {
    private KeyValueState<String, Long> wordCounts;
    private OutputCollector collector;
    ...
        @Override
        public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
          this.collector = collector;
        }
        @Override
        public void initState(KeyValueState<String, Long> state) {
          wordCounts = state;
        }
        @Override
        public void execute(Tuple tuple) {
          String word = tuple.getString(0);
          Integer count = wordCounts.get(word, 0);
          count++;
          wordCounts.put(word, count);
          collector.emit(tuple, new Values(word, count));
          collector.ack(tuple);
        }
    ...
    }

----------------------------------------

TITLE: Defining and Querying Kafka Streams with Storm SQL
DESCRIPTION: This SQL snippet demonstrates how to create external tables representing Kafka streams and perform a SELECT query with filtering and projection. It shows table creation for input and output streams, and an INSERT statement that processes data from the input stream.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://...' ...

CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) 'kafka://...' ...

INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Declaring Multiple Streams in Storm Spouts
DESCRIPTION: This snippet demonstrates how to declare multiple streams in a Storm spout using the OutputFieldsDeclarer and emit to specific streams using SpoutOutputCollector.

LANGUAGE: java
CODE:
declareStream(OutputFieldsDeclarer declarer) {
    declarer.declareStream("stream1", new Fields("field1", "field2"));
    declarer.declareStream("stream2", new Fields("field3", "field4"));
}

nextTuple() {
    // Emit to stream1
    collector.emit("stream1", new Values(value1, value2));
    // Emit to stream2
    collector.emit("stream2", new Values(value3, value4));
}

----------------------------------------

TITLE: Implementing SplitSentence Bolt in Storm using BaseRichBolt
DESCRIPTION: This code shows how to implement a bolt that splits sentences into words, demonstrating anchoring and acking of tuples for reliability.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseRichBolt {
        OutputCollector _collector;
        
        public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
            _collector = collector;
        }

        public void execute(Tuple tuple) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                _collector.emit(tuple, new Values(word));
            }
            _collector.ack(tuple);
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Creating User-Defined Functions in Storm SQL
DESCRIPTION: Example of creating a user-defined function in Storm SQL using the CREATE FUNCTION statement.

LANGUAGE: sql
CODE:
CREATE FUNCTION MYPLUS AS 'org.apache.storm.sql.TestUtils$MyPlus'

----------------------------------------

TITLE: Implementing Stateful Word Count Bolt in Java with Storm
DESCRIPTION: Example of a stateful word count bolt that extends BaseStatefulBolt and uses KeyValueState to store word counts. The bolt initializes its state, updates counts on tuple execution, and relies on the framework for periodic checkpointing.

LANGUAGE: java
CODE:
public class WordCountBolt extends BaseStatefulBolt<KeyValueState<String, Long>> {
    private KeyValueState<String, Long> wordCounts;
    private OutputCollector collector;
    ...
        @Override
        public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
          this.collector = collector;
        }
        @Override
        public void initState(KeyValueState<String, Long> state) {
          wordCounts = state;
        }
        @Override
        public void execute(Tuple tuple) {
          String word = tuple.getString(0);
          Integer count = wordCounts.get(word, 0);
          count++;
          wordCounts.put(word, count);
          collector.emit(tuple, new Values(word, count));
          collector.ack(tuple);
        }
    ...
    }

----------------------------------------

TITLE: Configuring Bolt Parallelism in Storm Topology (Java)
DESCRIPTION: Demonstrates how to set the number of executors and tasks for a bolt in a Storm topology. This example configures the 'green-bolt' with 2 executors and 4 tasks, using shuffle grouping from 'blue-spout'.

LANGUAGE: java
CODE:
topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

----------------------------------------

TITLE: Creating and Transforming Streams in Java
DESCRIPTION: Demonstrates creating a stream from a spout, transforming it by splitting sentences into words, and printing the results.

LANGUAGE: java
CODE:
import org.apache.storm.streams.Stream;
import org.apache.storm.streams.StreamBuilder;
...

StreamBuilder builder = new StreamBuilder();

// a stream of sentences obtained from a source spout
Stream<String> sentences = builder.newStream(new RandomSentenceSpout()).map(tuple -> tuple.getString(0));

// a stream of words obtained by transforming (splitting) the stream of sentences
Stream<String> words = sentences.flatMap(s -> Arrays.asList(s.split(" ")));

// output operation that prints the words to console
words.forEach(w -> System.out.println(w));

----------------------------------------

TITLE: Implementing Tuple Counting Bolt with Metrics
DESCRIPTION: Implementation of a bolt that counts incoming tuples using Storm's metrics system. The bolt registers a counter in prepare() and increments it for each received tuple.

LANGUAGE: java
CODE:
public class TupleCountingBolt extends BaseRichBolt {
    private Counter tupleCounter;
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        this.tupleCounter = context.registerCounter("tupleCount");
    }

    @Override
    public void execute(Tuple input) {
        this.tupleCounter.inc();
    }
}

----------------------------------------

TITLE: Submitting Storm Topology with Configuration
DESCRIPTION: Example showing how to configure and submit a Storm topology using StormSubmitter. Sets worker count and spout pending limits before submitting to cluster.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(20);
conf.setMaxSpoutPending(5000);
StormSubmitter.submitTopology("mytopology", conf, topology);

----------------------------------------

TITLE: Configuring and Creating JdbcInsertBolt for Storm-JDBC Integration
DESCRIPTION: This code snippet demonstrates how to configure and create a JdbcInsertBolt using HikariCP as the connection provider and SimpleJdbcMapper for mapping tuples to database rows. It shows two ways of specifying the insert operation: using a table name or an insert query.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);

String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withTableName("user")
                                    .withQueryTimeoutSecs(30);
                                    Or
JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withInsertQuery("insert into user values (?,?)")
                                    .withQueryTimeoutSecs(30);                                    

----------------------------------------

TITLE: Declaring Multiple Streams in Storm Spouts
DESCRIPTION: This snippet demonstrates how to declare multiple streams in a Storm spout using the OutputFieldsDeclarer and emit to specific streams using SpoutOutputCollector.

LANGUAGE: java
CODE:
declareStream(OutputFieldsDeclarer declarer) {
    declarer.declareStream("stream1", new Fields("field1", "field2"));
    declarer.declareStream("stream2", new Fields("field3", "field4"));
}

nextTuple() {
    // Emit to stream1
    collector.emit("stream1", new Values(value1, value2));
    // Emit to stream2
    collector.emit("stream2", new Values(value3, value4));
}

----------------------------------------

TITLE: Simple DRPC Topology Implementation
DESCRIPTION: Implementation of a basic DRPC topology that appends an exclamation mark to input strings, demonstrating the use of LinearDRPCTopologyBuilder.

LANGUAGE: java
CODE:
public static class ExclaimBolt extends BaseBasicBolt {
    public void execute(Tuple tuple, BasicOutputCollector collector) {
        String input = tuple.getString(1);
        collector.emit(new Values(tuple.getValue(0), input + "!"));
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "result"));
    }
}

public static void main(String[] args) throws Exception {
    LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("exclamation");
    builder.addBolt(new ExclaimBolt(), 3);
    // ...
}

----------------------------------------

TITLE: Python Split Sentence Bolt Implementation
DESCRIPTION: Example of implementing a Storm bolt in Python that splits sentences into words

LANGUAGE: python
CODE:
import storm

class SplitSentenceBolt(storm.BasicBolt):
    def process(self, tup):
        words = tup.values[0].split(" ")
        for word in words:
          storm.emit([word])

SplitSentenceBolt().run()

----------------------------------------

TITLE: Defining a Bolt in Java
DESCRIPTION: Example of a Java bolt that appends '!!!' to input tuples and emits the result.

LANGUAGE: java
CODE:
public class DoubleAndTripleBolt extends BaseRichBolt {
    private OutputCollectorBase _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollectorBase collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple input) {
        int val = input.getInteger(0);        
        _collector.emit(input, new Values(val*2, val*3));
        _collector.ack(input);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("double", "triple"));
    }    
}

----------------------------------------

TITLE: Acker Execute Method Implementation in Java
DESCRIPTION: The execute method of Storm's Acker bolt processes incoming tuples to track tuple tree completion using XOR checksums. It handles initialization, acking, and failing of tuple trees, updating the pending ledger accordingly.

LANGUAGE: java
CODE:
// Pseudocode for Acker's execute method
if (tickTuple) {
    advancePendingTupleTreesTowardsExpiration();
    return;
}

Record record = updateOrCreateRecord(tupletree);
if (isInit) {
    record.initializeWithChecksum(checksum);
    record.setSpoutId(spoutId);
} else if (isAck) {
    record.xorPartialChecksumIntoExisting(partialChecksum);
} else if (isFail) {
    record.markAsFailed();
}

rotatingMap.put(tupletreeId, record);

if (record.getTotalChecksum() == 0) {
    rotatingMap.remove(tupletreeId);
    notifySpoutOfSuccess(record.getSpoutId());
} else if (record.hasFailed()) {
    rotatingMap.remove(tupletreeId);
    notifySpoutOfFailure(record.getSpoutId());
}

ackOwnTuple();

----------------------------------------

TITLE: Implementing a Sliding Window Bolt in Storm
DESCRIPTION: Example of a sliding window bolt implementation extending BaseWindowedBolt. It demonstrates how to process tuples within a window and emit results.

LANGUAGE: java
CODE:
public class SlidingWindowBolt extends BaseWindowedBolt {
	private OutputCollector collector;
	
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
    	this.collector = collector;
    }
	
    @Override
    public void execute(TupleWindow inputWindow) {
	  for(Tuple tuple: inputWindow.get()) {
	    // do the windowing computation
		...
	  }
	  // emit the results
	  collector.emit(new Values(computedValue));
    }
}

----------------------------------------

TITLE: Killing a Storm Topology
DESCRIPTION: Terminates a running topology, with an optional wait time for proper shutdown.

LANGUAGE: shell
CODE:
storm kill topology-name [-w wait-time-secs]

----------------------------------------

TITLE: Implementing a Bolt in Python
DESCRIPTION: Shows a Python implementation of a bolt that splits sentences into words.

LANGUAGE: python
CODE:
import storm

class SplitSentenceBolt(storm.BasicBolt):
    def process(self, tup):
        words = tup.values[0].split(" ")
        for word in words:
          storm.emit([word])

SplitSentenceBolt().run()

----------------------------------------

TITLE: Initializing Local Storm Cluster in Java
DESCRIPTION: Creates an in-process Storm cluster using LocalCluster for testing. The cluster is automatically closed when exiting the try block scope.

LANGUAGE: java
CODE:
import org.apache.storm.LocalCluster;

...

try (LocalCluster cluster = new LocalCluster()) {
    //Interact with the cluster...
}

----------------------------------------

TITLE: Initializing Trident Topology with Persistent Aggregation
DESCRIPTION: Example of creating a Trident topology that performs word count aggregation and persists the results using MemcachedState.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
      topology.newStream("spout1", spout)
        .each(new Fields("sentence"), new Split(), new Fields("word"))
        .groupBy(new Fields("word"))
        .persistentAggregate(MemcachedState.opaque(serverLocations), new Count(), new Fields("count"))                
        .parallelismHint(6);

----------------------------------------

TITLE: Linear DRPC Topology Implementation
DESCRIPTION: Example of implementing a simple DRPC topology that appends an exclamation mark to input strings using LinearDRPCTopologyBuilder.

LANGUAGE: java
CODE:
public static class ExclaimBolt extends BaseBasicBolt {
    public void execute(Tuple tuple, BasicOutputCollector collector) {
        String input = tuple.getString(1);
        collector.emit(new Values(tuple.getValue(0), input + "!"));
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "result"));
    }
}

public static void main(String[] args) throws Exception {
    LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("exclamation");
    builder.addBolt(new ExclaimBolt(), 3);
    // ...
}

----------------------------------------

TITLE: Implementing a Sliding Window Bolt in Java
DESCRIPTION: Example of implementing a sliding window bolt by extending BaseWindowedBolt. The execute method processes tuples in each window activation.

LANGUAGE: java
CODE:
public class SlidingWindowBolt extends BaseWindowedBolt {
	private OutputCollector collector;
	
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
    	this.collector = collector;
    }
	
    @Override
    public void execute(TupleWindow inputWindow) {
	  for(Tuple tuple: inputWindow.get()) {
	    // do the windowing computation
		...
	  }
	  // emit the results
	  collector.emit(new Values(computedValue));
    }
}

----------------------------------------

TITLE: Implementing Storm Kinesis Spout Topology in Java
DESCRIPTION: Main topology class that demonstrates how to create and configure a Storm topology using KinesisSpout. It shows the setup of connection parameters, spout configuration, and topology submission.

LANGUAGE: java
CODE:
public class KinesisSpoutTopology {
    public static void main (String args[]) throws InvalidTopologyException, AuthorizationException, AlreadyAliveException {
        String topologyName = args[0];
        RecordToTupleMapper recordToTupleMapper = new TestRecordToTupleMapper();
        KinesisConnectionInfo kinesisConnectionInfo = new KinesisConnectionInfo(new CredentialsProviderChain(), new ClientConfiguration(), Regions.US_WEST_2,
                1000);
        ZkInfo zkInfo = new ZkInfo("localhost:2181", "/kinesisOffsets", 20000, 15000, 10000L, 3, 2000);
        KinesisConfig kinesisConfig = new KinesisConfig(args[1], ShardIteratorType.TRIM_HORIZON,
                recordToTupleMapper, new Date(), new ExponentialBackoffRetrier(), zkInfo, kinesisConnectionInfo, 10000L);
        KinesisSpout kinesisSpout = new KinesisSpout(kinesisConfig);
        TopologyBuilder topologyBuilder = new TopologyBuilder();
        topologyBuilder.setSpout("spout", kinesisSpout, 3);
        topologyBuilder.setBolt("bolt", new KinesisBoltTest(), 1).shuffleGrouping("spout");
        Config topologyConfig = new Config();
        topologyConfig.setDebug(true);
        topologyConfig.setNumWorkers(3);
        StormSubmitter.submitTopology(topologyName, topologyConfig, topologyBuilder.createTopology());
    }
}

----------------------------------------

TITLE: Implementing a Storm Bolt in Java
DESCRIPTION: Complete Java implementation of an ExclamationBolt that appends '!!!' to input tuples.

LANGUAGE: java
CODE:
public static class ExclamationBolt implements IRichBolt {
    OutputCollector _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple tuple) {
        _collector.emit(tuple, new Values(tuple.getString(0) + "!!!"));
        _collector.ack(tuple);
    }

    @Override
    public void cleanup() {
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word"));
    }
    
    @Override
    public Map<String, Object> getComponentConfiguration() {
        return null;
    }
}

----------------------------------------

TITLE: Defining a Word Count Topology in Java
DESCRIPTION: Shows how to define a more complex topology for word counting using different stream groupings.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
        
builder.setSpout("sentences", new RandomSentenceSpout(), 5);        
builder.setBolt("split", new SplitSentence(), 8)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 12)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Initializing Local Storm Cluster in Java
DESCRIPTION: Creates an in-process Storm cluster using LocalCluster class. This allows for testing topologies without launching a full Storm cluster.

LANGUAGE: java
CODE:
import org.apache.storm.LocalCluster;

...

try (LocalCluster cluster = new LocalCluster()) {
    //Interact with the cluster...
}

----------------------------------------

TITLE: Configuring Storm IsolationScheduler Machine Allocation
DESCRIPTION: YAML configuration example showing how to allocate dedicated machines to specific topologies using the IsolationScheduler. The configuration maps topology names to their allocated number of isolated machines.

LANGUAGE: yaml
CODE:
isolation.scheduler.machines: 
    "my-topology": 8
    "tiny-topology": 1
    "some-other-topology": 3

----------------------------------------

TITLE: Configuring UI and Logviewer Filters in YAML
DESCRIPTION: YAML configuration for setting up authentication filters for Storm UI and logviewer processes using javax.servlet.Filter implementations

LANGUAGE: yaml
CODE:
ui.filter: "filter.class"
ui.filter.params: "param1":"value1"
logviewer.filter: "filter.class"
logviewer.filter.params: "param1":"value1"

----------------------------------------

TITLE: Basic Function Implementation in Trident
DESCRIPTION: Example showing how to implement a basic function that takes an input tuple and emits multiple output tuples based on an integer value.

LANGUAGE: java
CODE:
public class MyFunction extends BaseFunction {
    public void execute(TridentTuple tuple, TridentCollector collector) {
        for(int i=0; i < tuple.getInteger(0); i++) {
            collector.emit(new Values(i));
        }
    }
}

----------------------------------------

TITLE: Implementing Skewed Streaming Top N Pattern in Storm
DESCRIPTION: This snippet shows an advanced Streaming Top N pattern for handling skewed data. It uses partialKeyGrouping for load balancing, an extra aggregation layer, and then merges the results.

LANGUAGE: java
CODE:
builder.setBolt("count", new CountObjects(), parallelism)
  .partialKeyGrouping("objects", new Fields("value"));
builder.setBolt("rank" new AggregateCountsAndRank(), parallelism)
  .fieldsGrouping("count", new Fields("key"))
builder.setBolt("merge", new MergeRanksObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Creating and Submitting a Storm Topology using StreamBuilder in Java
DESCRIPTION: Shows how to create a Storm topology using StreamBuilder and submit it using StormSubmitter. This example demonstrates the final step in creating a stream processing pipeline.

LANGUAGE: java
CODE:
StormSubmitter.submitTopologyWithProgressBar("test", new Config(), streamBuilder.build());

----------------------------------------

TITLE: Word Count Topology Construction
DESCRIPTION: Topology implementation for counting word occurrences using split sentence and word count bolts.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
        
builder.setSpout("sentences", new RandomSentenceSpout(), 5);        
builder.setBolt("split", new SplitSentence(), 8)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 12)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Defining a Transactional Topology in Java
DESCRIPTION: Example of how to define a transactional topology using TransactionalTopologyBuilder. This topology computes the global count of tuples from the input stream.

LANGUAGE: java
CODE:
MemoryTransactionalSpout spout = new MemoryTransactionalSpout(DATA, new Fields("word"), PARTITION_TAKE_PER_BATCH);
TransactionalTopologyBuilder builder = new TransactionalTopologyBuilder("global-count", "spout", spout, 3);
builder.setBolt("partial-count", new BatchCount(), 5)
        .shuffleGrouping("spout");
builder.setBolt("sum", new UpdateGlobalCount())
        .globalGrouping("partial-count");

----------------------------------------

TITLE: Implementing Distributed Query for Word Counts with Trident in Java
DESCRIPTION: Demonstrates how to implement a low latency distributed query on word counts using Trident. It processes a list of words, queries the state for their counts, and returns the sum of the counts.

LANGUAGE: java
CODE:
topology.newDRPCStream("words")
       .each(new Fields("args"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .stateQuery(wordCounts, new Fields("word"), new MapGet(), new Fields("count"))
       .each(new Fields("count"), new FilterNull())
       .aggregate(new Fields("count"), new Sum(), new Fields("sum"));

----------------------------------------

TITLE: Persistent Word Count Topology
DESCRIPTION: Complete example of a Storm topology that counts words and persists results to HBase

LANGUAGE: java
CODE:
public class PersistentWordCount {
    private static final String WORD_SPOUT = "WORD_SPOUT";
    private static final String COUNT_BOLT = "COUNT_BOLT";
    private static final String HBASE_BOLT = "HBASE_BOLT";

    public static void main(String[] args) throws Exception {
        Config config = new Config();

        Map<String, Object> hbConf = new HashMap<String, Object>();
        if(args.length > 0){
            hbConf.put("hbase.rootdir", args[0]);
        }
        config.put("hbase.conf", hbConf);

        WordSpout spout = new WordSpout();
        WordCounter bolt = new WordCounter();

        SimpleHBaseMapper mapper = new SimpleHBaseMapper()
                .withRowKeyField("word")
                .withColumnFields(new Fields("word"))
                .withCounterFields(new Fields("count"))
                .withColumnFamily("cf");

        HBaseBolt hbase = new HBaseBolt("WordCount", mapper)
                .withConfigKey("hbase.conf");

        TopologyBuilder builder = new TopologyBuilder();

        builder.setSpout(WORD_SPOUT, spout, 1);
        builder.setBolt(COUNT_BOLT, bolt, 1).shuffleGrouping(WORD_SPOUT);
        builder.setBolt(HBASE_BOLT, hbase, 1).fieldsGrouping(COUNT_BOLT, new Fields("word"));

        String topoName = "test";
        if (args.length > 1) {
            topoName = args[1];
        }
        config.setNumWorkers(3);
        StormSubmitter.submitTopology(topoName, config, builder.createTopology());
    }

----------------------------------------

TITLE: Emitting and Acking Tuples in Storm Bolts
DESCRIPTION: Shows how to emit new tuples and acknowledge processed tuples in a Storm bolt using OutputCollector.

LANGUAGE: java
CODE:
public void execute(Tuple input) {
    // Process input tuple
    // ...

    // Emit new tuples
    collector.emit(input, new Values(result1, result2));

    // Acknowledge the input tuple
    collector.ack(input);
}

----------------------------------------

TITLE: Node Resource Configuration in YAML
DESCRIPTION: Example of configuring available CPU and memory resources for a Storm node.

LANGUAGE: yaml
CODE:
supervisor.memory.capacity.mb: 20480.0
supervisor.cpu.capacity: 100.0

----------------------------------------

TITLE: Submitting Storm Topology using StormSubmitter
DESCRIPTION: Example showing how to configure and submit a Storm topology to a cluster using StormSubmitter. Sets the number of workers and maximum pending spouts for the topology configuration.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(20);
conf.setMaxSpoutPending(5000);
StormSubmitter.submitTopology("mytopology", conf, topology);

----------------------------------------

TITLE: Implementing DRPC Topology with LinearDRPCTopologyBuilder in Java
DESCRIPTION: This code demonstrates how to implement a simple DRPC topology using LinearDRPCTopologyBuilder. The topology appends an exclamation mark to the input argument.

LANGUAGE: java
CODE:
public static class ExclaimBolt extends BaseBasicBolt {
    public void execute(Tuple tuple, BasicOutputCollector collector) {
        String input = tuple.getString(1);
        collector.emit(new Values(tuple.getValue(0), input + "!"));
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "result"));
    }
}

public static void main(String[] args) throws Exception {
    LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("exclamation");
    builder.addBolt(new ExclaimBolt(), 3);
    // ...
}

----------------------------------------

TITLE: Basic Stream Operations in Java
DESCRIPTION: Example showing basic Stream operations including creating a stream from a spout, transforming sentences to words, and printing output

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

// a stream of sentences obtained from a source spout
Stream<String> sentences = builder.newStream(new RandomSentenceSpout()).map(tuple -> tuple.getString(0));

// a stream of words obtained by transforming (splitting) the stream of sentences
Stream<String> words = sentences.flatMap(s -> Arrays.asList(s.split(" ")));

// output operation that prints the words to console
words.forEach(w -> System.out.println(w));

----------------------------------------

TITLE: Implementing KafkaBolt Configuration
DESCRIPTION: Example showing how to configure and use KafkaBolt to write data from a Storm topology to Kafka topics

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();

Fields fields = new Fields("key", "message");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
            new Values("storm", "1"),
            new Values("trident", "1"),
            new Values("needs", "1"),
            new Values("javadoc", "1")
);
spout.setCycle(true);
builder.setSpout("spout", spout, 5);
//set producer properties.
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

KafkaBolt bolt = new KafkaBolt()
        .withProducerProperties(props)
        .withTopicSelector(new DefaultTopicSelector("test"))
        .withTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper());
builder.setBolt("forwardToKafka", bolt, 8).shuffleGrouping("spout");

Config conf = new Config();

StormSubmitter.submitTopology("kafkaboltTest", conf, builder.createTopology());

----------------------------------------

TITLE: Implementing Transactional State for Word Count in Trident
DESCRIPTION: Example of using MemcachedState to implement transactional state for a word count topology in Trident. This demonstrates how to use persistentAggregate for stateful processing.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
      topology.newStream("spout1", spout)
        .each(new Fields("sentence"), new Split(), new Fields("word"))
        .groupBy(new Fields("word"))
        .persistentAggregate(MemcachedState.opaque(serverLocations), new Count(), new Fields("count"))                
        .parallelismHint(6);

----------------------------------------

TITLE: Implementing a BasicBolt for Simple Tuple Processing in Storm
DESCRIPTION: This code demonstrates how to use the BasicBolt interface for bolts that follow a simple pattern of reading an input tuple, emitting based on it, and then acking. It simplifies the implementation by handling anchoring and acking automatically.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseBasicBolt {
        public void execute(Tuple tuple, BasicOutputCollector collector) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                collector.emit(new Values(word));
            }
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Implementing Stateful Word Count Bolt in Java
DESCRIPTION: Example of a word count bolt that extends BaseStatefulBolt and uses KeyValueState to store word counts. The bolt initializes its state, updates counts on tuple execution, and emits results.

LANGUAGE: java
CODE:
public class WordCountBolt extends BaseStatefulBolt<KeyValueState<String, Long>> {
    private KeyValueState<String, Long> wordCounts;
    private OutputCollector collector;
    ...
        @Override
        public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
          this.collector = collector;
        }
        @Override
        public void initState(KeyValueState<String, Long> state) {
          wordCounts = state;
        }
        @Override
        public void execute(Tuple tuple) {
          String word = tuple.getString(0);
          Integer count = wordCounts.get(word, 0);
          count++;
          wordCounts.put(word, count);
          collector.emit(tuple, new Values(word, count));
          collector.ack(tuple);
        }
    ...
    }

----------------------------------------

TITLE: Registering Task Hook in Storm Spout or Bolt
DESCRIPTION: Demonstrates how to register a task hook in the open method of a spout or prepare method of a bolt using the TopologyContext.

LANGUAGE: java
CODE:
TopologyContext#addTaskHook

----------------------------------------

TITLE: Implementing SplitSentence Bolt with Reliability
DESCRIPTION: Example bolt implementation that demonstrates proper tuple anchoring and acking for reliability.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseRichBolt {
        OutputCollector _collector;
        
        public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
            _collector = collector;
        }

        public void execute(Tuple tuple) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                _collector.emit(tuple, new Values(word));
            }
            _collector.ack(tuple);
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Emitting a Tuple from a Spout in Java
DESCRIPTION: This code snippet demonstrates how to emit a tuple from a spout using the SpoutOutputCollector. It shows the emission of a tuple with multiple fields and a message ID for tracking.

LANGUAGE: java
CODE:
_collector.emit(new Values("field1", "field2", 3) , msgId);

----------------------------------------

TITLE: Defining Storm Topology in Clojure
DESCRIPTION: Example of defining a Storm topology with spouts and bolts using the topology function. Shows how to wire components together and specify parallelism hints.

LANGUAGE: clojure
CODE:
(topology
 {"1" (spout-spec sentence-spout)
  "2" (spout-spec (sentence-spout-parameterized
                   ["the cat jumped over the door"
                    "greetings from a faraway land"])
                   :p 2)}
 {"3" (bolt-spec {"1" :shuffle "2" :shuffle}
                 split-sentence
                 :p 5)
  "4" (bolt-spec {"3" ["word"]}
                 word-count
                 :p 6)})

----------------------------------------

TITLE: Configuring Node Resource Availability
DESCRIPTION: Example of configuring available CPU and memory resources for a Storm supervisor node

LANGUAGE: yaml
CODE:
supervisor.memory.capacity.mb: 20480.0
supervisor.cpu.capacity: 100.0

----------------------------------------

TITLE: Configuring and Using KafkaBolt in a Storm Topology
DESCRIPTION: Demonstrates how to configure and use KafkaBolt in a Storm topology to write data to Kafka. It sets up a spout, configures Kafka producer properties, and creates a KafkaBolt instance with topic selection and tuple mapping.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();

Fields fields = new Fields("key", "message");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
            new Values("storm", "1"),
            new Values("trident", "1"),
            new Values("needs", "1"),
            new Values("javadoc", "1")
);
spout.setCycle(true);
builder.setSpout("spout", spout, 5);
//set producer properties.
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

KafkaBolt bolt = new KafkaBolt()
        .withProducerProperties(props)
        .withTopicSelector(new DefaultTopicSelector("test"))
        .withTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper());
builder.setBolt("forwardToKafka", bolt, 8).shuffleGrouping("spout");

Config conf = new Config();

StormSubmitter.submitTopology("kafkaboltTest", conf, builder.createTopology());

----------------------------------------

TITLE: Implementing Storm Bolt with Output Fields
DESCRIPTION: Example of a Storm bolt that processes input tuples and declares output fields for double and triple values.

LANGUAGE: java
CODE:
public class DoubleAndTripleBolt extends BaseRichBolt {
    private OutputCollectorBase _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollectorBase collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple input) {
        int val = input.getInteger(0);        
        _collector.emit(input, new Values(val*2, val*3));
        _collector.ack(input);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("double", "triple"));
    }    
}

----------------------------------------

TITLE: Initializing Local Storm Cluster in Java
DESCRIPTION: Creates and manages an in-process Storm cluster using LocalCluster class for testing topologies. The cluster is automatically closed when exiting the try block scope.

LANGUAGE: java
CODE:
import org.apache.storm.LocalCluster;

...

try (LocalCluster cluster = new LocalCluster()) {
    //Interact with the cluster...
}

----------------------------------------

TITLE: Implementing SplitSentence Bolt using BaseBasicBolt in Storm
DESCRIPTION: This code demonstrates how to use BaseBasicBolt to simplify the implementation of bolts that follow a common pattern of reading, emitting, and acking tuples.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseBasicBolt {
        public void execute(Tuple tuple, BasicOutputCollector collector) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                collector.emit(new Values(word));
            }
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Basic Storm Topology Configuration in YAML
DESCRIPTION: YAML configuration for defining a basic word count Storm topology with spouts, bolts and streams

LANGUAGE: yaml
CODE:
name: "yaml-topology"
config:
  topology.workers: 1

spouts:
  - id: "spout-1"
    className: "org.apache.storm.testing.TestWordSpout"
    parallelism: 1

bolts:
  - id: "bolt-1"
    className: "org.apache.storm.testing.TestWordCounter"
    parallelism: 1
  - id: "bolt-2"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1

streams:
  - name: "spout-1 --> bolt-1"
    from: "spout-1"
    to: "bolt-1"
    grouping:
      type: FIELDS
      args: ["word"]

  - name: "bolt-1 --> bolt2"
    from: "bolt-1"
    to: "bolt-2"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Submitting Storm Topology using StormSubmitter in Java
DESCRIPTION: This snippet demonstrates how to use StormSubmitter to submit a Storm topology to a cluster. It configures the number of workers and maximum pending spouts before submitting the topology.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(20);
conf.setMaxSpoutPending(5000);
StormSubmitter.submitTopology("mytopology", conf, topology);

----------------------------------------

TITLE: Implementing SplitSentence as BasicBolt
DESCRIPTION: Simplified implementation of sentence splitting using BasicBolt interface which handles anchoring and acking automatically.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseBasicBolt {
        public void execute(Tuple tuple, BasicOutputCollector collector) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                collector.emit(new Values(word));
            }
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Configuring HDFS Bolt with Delimited Record Format in Java
DESCRIPTION: Example showing how to configure an HDFS bolt to write pipe-delimited files with custom sync and rotation policies.

LANGUAGE: java
CODE:
RecordFormat format = new DelimitedRecordFormat()
        .withFieldDelimiter("|");

SyncPolicy syncPolicy = new CountSyncPolicy(1000);

FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withPath("/foo/");

HdfsBolt bolt = new HdfsBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withRecordFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy);

----------------------------------------

TITLE: Implementing Stateful Word Count Bolt in Java
DESCRIPTION: Example of a stateful bolt that extends BaseStatefulBolt to implement word counting functionality using KeyValueState. It demonstrates how to initialize state, update counts, and emit results.

LANGUAGE: java
CODE:
public class WordCountBolt extends BaseStatefulBolt<KeyValueState<String, Long>> {
    private KeyValueState<String, Long> wordCounts;
    private OutputCollector collector;
    ...
        @Override
        public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
          this.collector = collector;
        }
        @Override
        public void initState(KeyValueState<String, Long> state) {
          wordCounts = state;
        }
        @Override
        public void execute(Tuple tuple) {
          String word = tuple.getString(0);
          Integer count = wordCounts.get(word, 0);
          count++;
          wordCounts.put(word, count);
          collector.emit(tuple, new Values(word, count));
          collector.ack(tuple);
        }
    ...
    }

----------------------------------------

TITLE: Implementing a SplitSentence Bolt in Java
DESCRIPTION: This code snippet shows the implementation of a SplitSentence bolt that splits a sentence into words. It demonstrates tuple anchoring and proper acknowledgment of processed tuples.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseRichBolt {
        OutputCollector _collector;
        
        public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
            _collector = collector;
        }

        public void execute(Tuple tuple) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                _collector.emit(tuple, new Values(word));
            }
            _collector.ack(tuple);
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Configuring Resource Allocation in Trident Topology
DESCRIPTION: This code snippet demonstrates how to use the Trident RAS API to set CPU and memory resources for various operations in a Trident topology. It shows setting default resources, and then overriding them for specific operations like stream creation, data processing, and aggregation.

LANGUAGE: java
CODE:
TridentTopology topo = new TridentTopology();
topo.setResourceDefaults(new DefaultResourceDeclarer()
                                                      .setMemoryLoad(128)
                                                      .setCPULoad(20));
TridentState wordCounts =
    topology
        .newStream("words", feeder)
        .parallelismHint(5)
        .setCPULoad(20)
        .setMemoryLoad(512,256)
        .each( new Fields("sentence"),  new Split(), new Fields("word"))
        .setCPULoad(10)
        .setMemoryLoad(512)
        .each(new Fields("word"), new BangAdder(), new Fields("word!"))
        .parallelismHint(10)
        .setCPULoad(50)
        .setMemoryLoad(1024)
        .each(new Fields("word!"), new QMarkAdder(), new Fields("word!?"))
        .groupBy(new Fields("word!"))
        .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))
        .setCPULoad(100)
        .setMemoryLoad(2048);

----------------------------------------

TITLE: Defining KafkaTopicSelector Interface in Java
DESCRIPTION: Defines the KafkaTopicSelector interface with a method for selecting the Kafka topic for a tuple.

LANGUAGE: java
CODE:
public interface KafkaTopicSelector {
    String getTopics(Tuple/TridentTuple tuple);
}

----------------------------------------

TITLE: Configuring Metric Reporters in YAML
DESCRIPTION: Example YAML configuration for setting up metric reporters in Storm. This snippet shows how to configure the storm.daemon.metrics.reporter.plugins setting.

LANGUAGE: yaml
CODE:
storm.daemon.metrics.reporter.plugins:
  - "org.apache.storm.daemon.metrics.reporters.ConsolePreparableReporter"
  - "org.apache.storm.daemon.metrics.reporters.CsvPreparableReporter"
  - "org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter"

----------------------------------------

TITLE: Implementing Redis Filter Mapper
DESCRIPTION: Example implementation of RedisFilterMapper interface for filtering words using Redis set data type.

LANGUAGE: java
CODE:
class BlacklistWordFilterMapper implements RedisFilterMapper {
    private RedisDataTypeDescription description;
    private final String setKey = "blacklist";

    public BlacklistWordFilterMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.SET, setKey);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Implementing Streaming Word Count in Trident
DESCRIPTION: Demonstrates how to create a Trident topology for streaming word count, including splitting sentences, grouping by words, and persistent aggregation.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
     topology.newStream("spout1", spout)
       .each(new Fields("sentence"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))                
       .parallelismHint(6);

----------------------------------------

TITLE: Configuring Metric Reporters in YAML
DESCRIPTION: Example YAML configuration for setting up metric reporters in Storm. This snippet shows how to configure the storm.daemon.metrics.reporter.plugins setting.

LANGUAGE: yaml
CODE:
storm.daemon.metrics.reporter.plugins:
  - "org.apache.storm.daemon.metrics.reporters.ConsolePreparableReporter"
  - "org.apache.storm.daemon.metrics.reporters.CsvPreparableReporter"
  - "org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter"

----------------------------------------

TITLE: Implementing Streaming Word Count in Trident
DESCRIPTION: Demonstrates how to create a Trident topology that computes streaming word count from an input stream of sentences, using persistent aggregation to store results.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
     topology.newStream("spout1", spout)
       .each(new Fields("sentence"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))                
       .parallelismHint(6);

----------------------------------------

TITLE: Implementing IWindowedBolt Interface
DESCRIPTION: Core interface definition for Storm bolts that require windowing support. Includes methods for preparation, tuple window execution, and cleanup.

LANGUAGE: java
CODE:
public interface IWindowedBolt extends IComponent {
    void prepare(Map stormConf, TopologyContext context, OutputCollector collector);
    /**
     * Process tuples falling within the window and optionally emit 
     * new tuples based on the tuples in the input window.
     */
    void execute(TupleWindow inputWindow);
    void cleanup();
}

----------------------------------------

TITLE: Implementing Redis Filter Mapper
DESCRIPTION: Example implementation of RedisFilterMapper interface for filtering words using Redis set data type.

LANGUAGE: java
CODE:
class BlacklistWordFilterMapper implements RedisFilterMapper {
    private RedisDataTypeDescription description;
    private final String setKey = "blacklist";

    public BlacklistWordFilterMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.SET, setKey);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Running DRPC in Local Mode with Storm
DESCRIPTION: Demonstrates how to set up and execute DRPC (Distributed Remote Procedure Call) operations in local mode using LocalDRPC and LocalCluster.

LANGUAGE: java
CODE:
try (LocalDRPC drpc = new LocalDRPC();
     LocalCluster cluster = new LocalCluster();
     LocalTopology topo = cluster.submitTopology("drpc-demo", conf, builder.createLocalTopology(drpc))) {

    System.out.println("Results for 'hello':" + drpc.execute("exclamation", "hello"));
}

----------------------------------------

TITLE: Implementing Basic Trident Function
DESCRIPTION: Example showing how to implement a basic Trident function that takes input tuples and emits multiple output tuples using a BaseFunction class.

LANGUAGE: java
CODE:
public class MyFunction extends BaseFunction {
    public void execute(TridentTuple tuple, TridentCollector collector) {
        for(int i=0; i < tuple.getInteger(0); i++) {
            collector.emit(new Values(i));
        }
    }
}

----------------------------------------

TITLE: Declaring Output Fields for a Storm Bolt in Java
DESCRIPTION: Java code snippet showing how to declare output fields for a Storm bolt using the declareOutputFields method.

LANGUAGE: java
CODE:
public class DoubleAndTripleBolt extends BaseRichBolt {
    private OutputCollectorBase _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollectorBase collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple input) {
        int val = input.getInteger(0);        
        _collector.emit(input, new Values(val*2, val*3));
        _collector.ack(input);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("double", "triple"));
    }    
}

----------------------------------------

TITLE: Configuring KafkaBolt in Storm Topology
DESCRIPTION: Demonstrates how to configure and add a KafkaBolt to a Storm topology to write data to Kafka.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();

Fields fields = new Fields("key", "message");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
            new Values("storm", "1"),
            new Values("trident", "1"),
            new Values("needs", "1"),
            new Values("javadoc", "1")
);
spout.setCycle(true);
builder.setSpout("spout", spout, 5);

Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

KafkaBolt bolt = new KafkaBolt()
        .withProducerProperties(props)
        .withTopicSelector(new DefaultTopicSelector("test"))
        .withTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper());
builder.setBolt("forwardToKafka", bolt, 8).shuffleGrouping("spout");

Config conf = new Config();

StormSubmitter.submitTopology("kafkaboltTest", conf, builder.createTopology());

----------------------------------------

TITLE: Implementing SimpleQueryFilterCreator for Storm-MongoDB Integration
DESCRIPTION: Implementation of QueryFilterCreator interface for creating MongoDB query filters from Storm tuples.

LANGUAGE: java
CODE:
public class SimpleQueryFilterCreator implements QueryFilterCreator {

    private String field;
    
    @Override
    public Bson createFilter(ITuple tuple) {
        return Filters.eq(field, tuple.getValueByField(field));
    }

    @Override
    public Bson createFilterByKeys(List<Object> keys) {
        return Filters.eq("_id", MongoUtils.getID(keys));
    }

    public SimpleQueryFilterCreator withField(String field) {
        this.field = field;
        return this;
    }

}

----------------------------------------

TITLE: Complete Storm Bolt Implementation
DESCRIPTION: Full implementation of an ExclamationBolt showing all required methods and functionality.

LANGUAGE: java
CODE:
public static class ExclamationBolt implements IRichBolt {
    OutputCollector _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple tuple) {
        _collector.emit(tuple, new Values(tuple.getString(0) + "!!!"));
        _collector.ack(tuple);
    }

    @Override
    public void cleanup() {
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word"));
    }
    
    @Override
    public Map<String, Object> getComponentConfiguration() {
        return null;
    }
}

----------------------------------------

TITLE: Initializing Trident Topology with Persistent Aggregation
DESCRIPTION: Example of creating a Trident topology that performs word count aggregation and stores results in Memcached using opaque transactional state.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
      topology.newStream("spout1", spout)
        .each(new Fields("sentence"), new Split(), new Fields("word"))
        .groupBy(new Fields("word"))
        .persistentAggregate(MemcachedState.opaque(serverLocations), new Count(), new Fields("count"))                
        .parallelismHint(6);

----------------------------------------

TITLE: Implementing Skewed Streaming Top N Pattern in Storm
DESCRIPTION: This snippet shows how to implement a streaming top N pattern for skewed data in Storm. It uses partial key grouping for load distribution, fields grouping for aggregation, and global grouping for final merging.

LANGUAGE: java
CODE:
builder.setBolt("count", new CountObjects(), parallelism)
  .partialKeyGrouping("objects", new Fields("value"));
builder.setBolt("rank" new AggregateCountsAndRank(), parallelism)
  .fieldsGrouping("count", new Fields("key"))
builder.setBolt("merge", new MergeRanksObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Configuring and Using KafkaBolt in a Storm Topology
DESCRIPTION: Example of setting up a KafkaBolt to write data to Kafka from a Storm topology.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();

Fields fields = new Fields("key", "message");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
            new Values("storm", "1"),
            new Values("trident", "1"),
            new Values("needs", "1"),
            new Values("javadoc", "1")
);
spout.setCycle(true);
builder.setSpout("spout", spout, 5);
//set producer properties.
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

KafkaBolt bolt = new KafkaBolt()
        .withProducerProperties(props)
        .withTopicSelector(new DefaultTopicSelector("test"))
        .withTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper());
builder.setBolt("forwardToKafka", bolt, 8).shuffleGrouping("spout");

Config conf = new Config();

StormSubmitter.submitTopology("kafkaboltTest", conf, builder.createTopology());

----------------------------------------

TITLE: Configuring ExpandUrl Bolt with Fields Grouping in Storm
DESCRIPTION: This snippet shows how to set up an ExpandUrl bolt with fields grouping in a Storm topology. This approach leads to more effective caching as the same URL always goes to the same task.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .fieldsGrouping("urls", new Fields("url"));

----------------------------------------

TITLE: Configuring Storm Topology with Flux YAML
DESCRIPTION: Example YAML configuration for defining a simple word count topology using Flux.

LANGUAGE: yaml
CODE:
name: "yaml-topology"
config:
  topology.workers: 1

# spout definitions
spouts:
  - id: "spout-1"
    className: "org.apache.storm.testing.TestWordSpout"
    parallelism: 1

# bolt definitions
bolts:
  - id: "bolt-1"
    className: "org.apache.storm.testing.TestWordCounter"
    parallelism: 1
  - id: "bolt-2"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1

#stream definitions
streams:
  - name: "spout-1 --> bolt-1" # name isn't used (placeholder for logging, UI, etc.)
    from: "spout-1"
    to: "bolt-1"
    grouping:
      type: FIELDS
      args: ["word"]

  - name: "bolt-1 --> bolt2"
    from: "bolt-1"
    to: "bolt-2"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Creating a Typed Stream Using ValueMapper in Java
DESCRIPTION: This example shows how to use ValueMapper to extract specific fields from tuples emitted by a spout, creating a typed stream.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

// extract the first field from the tuple to get a Stream<String> of sentences
Stream<String> sentences = builder.newStream(new TestWordSpout(), new ValueMapper<String>(0));

// extract first three fields of the tuple emitted by the spout to produce a stream of typed tuples.
Stream<Tuple3<String, Integer, Long>> stream = builder.newStream(new TestSpout(), TupleValueMappers.of(0, 1, 2));

----------------------------------------

TITLE: Implementing Distributed Query for Word Count in Java with Trident
DESCRIPTION: This code snippet demonstrates how to implement a low-latency distributed query on word counts using Trident. It processes a list of words and returns the sum of their counts.

LANGUAGE: java
CODE:
topology.newDRPCStream("words")
       .each(new Fields("args"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .stateQuery(wordCounts, new Fields("word"), new MapGet(), new Fields("count"))
       .each(new Fields("count"), new FilterNull())
       .aggregate(new Fields("count"), new Sum(), new Fields("sum"));

----------------------------------------

TITLE: Initializing LocalCluster in Java for Storm Topology Testing
DESCRIPTION: Creates an in-process Storm cluster using the LocalCluster class for testing topologies. The cluster is automatically closed when exiting the try-with-resources block.

LANGUAGE: java
CODE:
import org.apache.storm.LocalCluster;

...

try (LocalCluster cluster = new LocalCluster()) {
    //Interact with the cluster...
}

----------------------------------------

TITLE: Storm Java Interface Structure
DESCRIPTION: Core Java interfaces that define Storm's functionality including IRichBolt, IRichSpout and TopologyBuilder. Shows interface strategy with base implementations.

LANGUAGE: markdown
CODE:
Main interfaces:\n1. IRichBolt\n2. IRichSpout\n3. TopologyBuilder\n\nInterface Strategy:\n1. Define Java interface\n2. Provide base class with default implementations

----------------------------------------

TITLE: Implementing a Word Count Topology Using Stream API in Java
DESCRIPTION: Provides a complete example of a word count topology using Stream API, including windowing, transformations, and output operations.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

builder
   // A stream of random sentences with two partitions
   .newStream(new RandomSentenceSpout(), new ValueMapper<String>(0), 2)
   // a two seconds tumbling window
   .window(TumblingWindows.of(Duration.seconds(2)))
   // split the sentences to words
   .flatMap(s -> Arrays.asList(s.split(" ")))
   // create a stream of (word, 1) pairs
   .mapToPair(w -> Pair.of(w, 1))
   // compute the word counts in the last two second window
   .countByKey()
   // print the results to stdout
   .print();

Config config = new Config();
config.setNumWorkers(1);
StormSubmitter.submitTopologyWithProgressBar("topology-name", config, builder.build());

----------------------------------------

TITLE: Limiting Worker Heap Size in Java
DESCRIPTION: Java configuration to limit the maximum heap size for workers in a Storm topology.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setTopologyWorkerMaxHeapSize(512.0);

----------------------------------------

TITLE: Configuring KafkaBolt Example
DESCRIPTION: Example showing how to configure and use KafkaBolt to write data from a Storm topology to Kafka topics.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();

Fields fields = new Fields("key", "message");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
            new Values("storm", "1"),
            new Values("trident", "1"),
            new Values("needs", "1"),
            new Values("javadoc", "1")
);
spout.setCycle(true);
builder.setSpout("spout", spout, 5);
//set producer properties.
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

KafkaBolt bolt = new KafkaBolt()
        .withProducerProperties(props)
        .withTopicSelector(new DefaultTopicSelector("test"))
        .withTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper());
builder.setBolt("forwardToKafka", bolt, 8).shuffleGrouping("spout");

Config conf = new Config();

StormSubmitter.submitTopology("kafkaboltTest", conf, builder.createTopology());

----------------------------------------

TITLE: Registering Custom Serializers in Storm Topology Configuration (YAML)
DESCRIPTION: This YAML snippet demonstrates how to register custom serializers for specific classes in a Storm topology configuration. It shows both default FieldsSerializer usage and custom serializer implementation.

LANGUAGE: yaml
CODE:
topology.kryo.register:
  - com.mycompany.CustomType1
  - com.mycompany.CustomType2: com.mycompany.serializer.CustomType2Serializer
  - com.mycompany.CustomType3

----------------------------------------

TITLE: Sample Flux Topology Definition
DESCRIPTION: Example YAML configuration defining a word count topology with spouts, bolts and streams

LANGUAGE: yaml
CODE:
name: "yaml-topology"
config:
  topology.workers: 1

# spout definitions
spouts:
  - id: "spout-1"
    className: "org.apache.storm.testing.TestWordSpout"
    parallelism: 1

# bolt definitions
bolts:
  - id: "bolt-1"
    className: "org.apache.storm.testing.TestWordCounter"
    parallelism: 1
  - id: "bolt-2"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1

#stream definitions
streams:
  - name: "spout-1 --> bolt-1" # name isn't used (placeholder for logging, UI, etc.)
    from: "spout-1"
    to: "bolt-1"
    grouping:
      type: FIELDS
      args: ["word"]

  - name: "bolt-1 --> bolt2"
    from: "bolt-1"
    to: "bolt-2"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Initializing Local Storm Cluster in Java
DESCRIPTION: Creates an in-process Storm cluster using LocalCluster for testing. Uses try-with-resources to ensure proper cleanup.

LANGUAGE: java
CODE:
import org.apache.storm.LocalCluster;

...

try (LocalCluster cluster = new LocalCluster()) {
    //Interact with the cluster...
}

----------------------------------------

TITLE: Configuring Storm Metricstore with YAML
DESCRIPTION: YAML configuration options for setting up Storm's metric storage system. This includes specifying the MetricStore implementation class, WorkerMetricsProcessor class, RocksDB location, and various other RocksDB-specific settings.

LANGUAGE: yaml
CODE:
storm.metricstore.class: "org.apache.storm.metricstore.rocksdb.RocksDbStore"
storm.metricprocessor.class: "org.apache.storm.metricstore.NimbusMetricProcessor"
storm.metricstore.rocksdb.location: "storm_rocks"
storm.metricstore.rocksdb.create_if_missing: true
storm.metricstore.rocksdb.metadata_string_cache_capacity: 4000
storm.metricstore.rocksdb.retention_hours: 240

----------------------------------------

TITLE: Joining Streams in Trident
DESCRIPTION: Example of joining two streams in Trident based on specific fields.

LANGUAGE: java
CODE:
topology.join(stream1, new Fields("key"), stream2, new Fields("x"), new Fields("key", "a", "b", "c"));

----------------------------------------

TITLE: Creating and Querying Kafka Streams with Storm SQL
DESCRIPTION: Example showing how to create external tables mapped to Kafka streams and perform filtering and projection operations. The query calculates total order values and filters for large orders above 50 units.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://...' ...

CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) 'kafka://...' ...

INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Custom Configuration Service Loading
DESCRIPTION: Example path for the ServiceLoader configuration file needed when implementing custom Storm configurations. This file must be included in the JAR's META-INF/services directory.

LANGUAGE: text
CODE:
META-INF/services/org.apache.storm.validation.Validated

----------------------------------------

TITLE: Defining a Storm Topology in Clojure
DESCRIPTION: Example of defining a Storm topology using the Clojure DSL, including spout and bolt specifications with parallelism hints.

LANGUAGE: clojure
CODE:
(topology
 {"1" (spout-spec sentence-spout)
  "2" (spout-spec (sentence-spout-parameterized
                   ["the cat jumped over the door"
                    "greetings from a faraway land"])
                   :p 2)}
 {"3" (bolt-spec {"1" :shuffle "2" :shuffle}
                 split-sentence
                 :p 5)
  "4" (bolt-spec {"3" ["word"]}
                 word-count
                 :p 6)})

----------------------------------------

TITLE: Implementing Streaming Top N Pattern in Storm
DESCRIPTION: This code demonstrates how to implement a streaming top N pattern in Storm using parallel ranking and merging. It uses fields grouping for partitioning and global grouping for final merging.

LANGUAGE: java
CODE:
builder.setBolt("rank", new RankObjects(), parallelism)
  .fieldsGrouping("objects", new Fields("value"));
builder.setBolt("merge", new MergeObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Trident State API Implementation for Cassandra
DESCRIPTION: Demonstrates the implementation of Trident state API for inserting and querying data in Cassandra, including state factory configuration and stream processing.

LANGUAGE: java
CODE:
        CassandraState.Options options = new CassandraState.Options(new CassandraContext());
        CQLStatementTupleMapper insertTemperatureValues = boundQuery(
                "INSERT INTO weather.temperature(weather_station_id, weather_station_name, event_time, temperature) VALUES(?, ?, ?, ?)")
                .bind(with(field("weather_station_id"), field("name").as("weather_station_name"), field("event_time").now(), field("temperature")));
        options.withCQLStatementTupleMapper(insertTemperatureValues);
        CassandraStateFactory insertValuesStateFactory =  new CassandraStateFactory(options);
        TridentState selectState = topology.newStaticState(selectWeatherStationStateFactory);
        stream = stream.stateQuery(selectState, new Fields("weather_station_id"), new CassandraQuery(), new Fields("name"));
        stream = stream.each(new Fields("name"), new PrintFunction(), new Fields("name_x"));
        stream.partitionPersist(insertValuesStateFactory, new Fields("weather_station_id", "name", "event_time", "temperature"), new CassandraStateUpdater(), new Fields());

----------------------------------------

TITLE: KafkaBolt Integration Example
DESCRIPTION: Complete example of integrating KafkaBolt with a Storm topology for writing to Kafka

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();

Fields fields = new Fields("key", "message");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
            new Values("storm", "1"),
            new Values("trident", "1"),
            new Values("needs", "1"),
            new Values("javadoc", "1")
);
spout.setCycle(true);
builder.setSpout("spout", spout, 5);
//set producer properties.
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

KafkaBolt bolt = new KafkaBolt()
        .withProducerProperties(props)
        .withTopicSelector(new DefaultTopicSelector("test"))
        .withTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper());
builder.setBolt("forwardToKafka", bolt, 8).shuffleGrouping("spout");

Config conf = new Config();

StormSubmitter.submitTopology("kafkaboltTest", conf, builder.createTopology());

----------------------------------------

TITLE: Node Resource Configuration
DESCRIPTION: Example configuration for specifying available CPU and memory resources on a Storm supervisor node

LANGUAGE: yaml
CODE:
supervisor.memory.capacity.mb: 20480.0
supervisor.cpu.capacity: 100.0

----------------------------------------

TITLE: Implementing ISpout Interface for Storm Spouts
DESCRIPTION: This code snippet shows the ISpout interface that Storm spouts must implement. It includes methods for opening and closing the spout, emitting tuples, and handling acknowledgments and failures.

LANGUAGE: java
CODE:
public interface ISpout extends Serializable {
    void open(Map conf, TopologyContext context, SpoutOutputCollector collector);
    void close();
    void nextTuple();
    void ack(Object msgId);
    void fail(Object msgId);
}

----------------------------------------

TITLE: Implementing IWindowedBolt Interface in Java
DESCRIPTION: Core interface for implementing windowed operations in Storm bolts. Defines methods for preparing, executing window operations, and cleanup.

LANGUAGE: java
CODE:
public interface IWindowedBolt extends IComponent {
    void prepare(Map stormConf, TopologyContext context, OutputCollector collector);
    /**
     * Process tuples falling within the window and optionally emit 
     * new tuples based on the tuples in the input window.
     */
    void execute(TupleWindow inputWindow);
    void cleanup();
}

----------------------------------------

TITLE: Applying a Trident Function to a Stream
DESCRIPTION: Example of applying a Trident Function to a stream, showing how input and output fields are handled.

LANGUAGE: java
CODE:
mystream.each(new Fields("b"), new MyFunction(), new Fields("d")))

----------------------------------------

TITLE: Configuring Trident Kafka State Example
DESCRIPTION: Example showing how to configure and use TridentKafkaState to write data from a Trident topology to Kafka.

LANGUAGE: java
CODE:
Fields fields = new Fields("word", "count");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
        new Values("storm", "1"),
        new Values("trident", "1"),
        new Values("needs", "1"),
        new Values("javadoc", "1")
);
spout.setCycle(true);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

//set producer properties.
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

TridentKafkaStateFactory stateFactory = new TridentKafkaStateFactory()
        .withProducerProperties(props)
        .withKafkaTopicSelector(new DefaultTopicSelector("test"))
        .withTridentTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper("word", "count"));
stream.partitionPersist(stateFactory, fields, new TridentKafkaStateUpdater(), new Fields());

Config conf = new Config();
StormSubmitter.submitTopology("kafkaTridentTest", conf, topology.build());

----------------------------------------

TITLE: Implementing RedisStoreMapper in Java
DESCRIPTION: Demonstrates how to create a custom RedisStoreMapper for use with RedisStoreBolt. This example stores word counts in a Redis hash.

LANGUAGE: java
CODE:
class WordCountStoreMapper implements RedisStoreMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountStoreMapper() {
        description = new RedisDataTypeDescription(
            RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return tuple.getStringByField("count");
    }
}

----------------------------------------

TITLE: Implementing a Filter in Trident
DESCRIPTION: Example of a custom Filter implementation that keeps tuples based on specific field values.

LANGUAGE: java
CODE:
public class MyFilter extends BaseFilter {
    public boolean isKeep(TridentTuple tuple) {
        return tuple.getInteger(0) == 1 && tuple.getInteger(1) == 2;
    }
}

----------------------------------------

TITLE: Configuring KafkaBolt in Storm Topology
DESCRIPTION: Example of setting up a KafkaBolt in a Storm topology. It configures Kafka producer properties, topic selection, and tuple-to-Kafka mapping.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();

Fields fields = new Fields("key", "message");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
            new Values("storm", "1"),
            new Values("trident", "1"),
            new Values("needs", "1"),
            new Values("javadoc", "1")
);
spout.setCycle(true);
builder.setSpout("spout", spout, 5);
//set producer properties.
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

KafkaBolt bolt = new KafkaBolt()
        .withProducerProperties(props)
        .withTopicSelector(new DefaultTopicSelector("test"))
        .withTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper());
builder.setBolt("forwardToKafka", bolt, 8).shuffleGrouping("spout");

Config conf = new Config();

StormSubmitter.submitTopology("kafkaboltTest", conf, builder.createTopology());

----------------------------------------

TITLE: Configuring Trident Topology Resources with RAS API
DESCRIPTION: Demonstrates how to set CPU and memory resources for different operations in a Trident topology. Shows resource configuration for streams, operations, and how resources are combined within operation boundaries. Includes memory allocation for both on-heap and off-heap usage.

LANGUAGE: java
CODE:
    TridentTopology topo = new TridentTopology();
    topo.setResourceDefaults(new DefaultResourceDeclarer();
                                                          .setMemoryLoad(128)
                                                          .setCPULoad(20));
    TridentState wordCounts =
        topology
            .newStream("words", feeder)
            .parallelismHint(5)
            .setCPULoad(20)
            .setMemoryLoad(512,256)
            .each( new Fields("sentence"),  new Split(), new Fields("word"))
            .setCPULoad(10)
            .setMemoryLoad(512)
            .each(new Fields("word"), new BangAdder(), new Fields("word!"))
            .parallelismHint(10)
            .setCPULoad(50)
            .setMemoryLoad(1024)
            .each(new Fields("word!"), new QMarkAdder(), new Fields("word!?"))
            .groupBy(new Fields("word!"))
            .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))
            .setCPULoad(100)
            .setMemoryLoad(2048);

----------------------------------------

TITLE: Defining a Simple Wordcount Topology in YAML
DESCRIPTION: YAML configuration for a simple wordcount topology using Flux.

LANGUAGE: yaml
CODE:
name: "yaml-topology"
config:
  topology.workers: 1

# spout definitions
spouts:
  - id: "spout-1"
    className: "org.apache.storm.testing.TestWordSpout"
    parallelism: 1

# bolt definitions
bolts:
  - id: "bolt-1"
    className: "org.apache.storm.testing.TestWordCounter"
    parallelism: 1
  - id: "bolt-2"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1

#stream definitions
streams:
  - name: "spout-1 --> bolt-1" # name isn't used (placeholder for logging, UI, etc.)
    from: "spout-1"
    to: "bolt-1"
    grouping:
      type: FIELDS
      args: ["word"]

  - name: "bolt-1 --> bolt2"
    from: "bolt-1"
    to: "bolt-2"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Configuring Storm Bolt Parallelism in Java
DESCRIPTION: Code example showing how to configure a Storm bolt with specific executor and task settings. Sets up a GreenBolt with 2 executors and 4 tasks using shuffle grouping.

LANGUAGE: java
CODE:
topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

----------------------------------------

TITLE: Stream Grouping Declaration Example - Java
DESCRIPTION: Example showing how to declare stream groupings in Storm using declarer.shuffleGrouping(). Demonstrates both default stream ID subscription and explicit stream ID specification.

LANGUAGE: java
CODE:
declarer.shuffleGrouping("1")  // subscribes to default stream\ndeclarer.shuffleGrouping("1", DEFAULT_STREAM_ID)  // equivalent explicit form

----------------------------------------

TITLE: Implementing Streaming Top N Pattern in Storm
DESCRIPTION: This code demonstrates how to implement a streaming top N pattern in Storm using parallel ranking and merging. It uses fields grouping for partitioning and global grouping for final merging.

LANGUAGE: java
CODE:
builder.setBolt("rank", new RankObjects(), parallelism)
  .fieldsGrouping("objects", new Fields("value"));
builder.setBolt("merge", new MergeObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Storm Component Configuration Options
DESCRIPTION: List of configurable options that can be overridden on a per-bolt/per-spout basis in Storm 0.7.0 and later versions.

LANGUAGE: text
CODE:
1. "topology.debug"
2. "topology.max.spout.pending"
3. "topology.max.task.parallelism"
4. "topology.kryo.register"

----------------------------------------

TITLE: Initializing Fixed Batch Spout in Java
DESCRIPTION: Creates a FixedBatchSpout that cycles through a fixed set of sentences to produce a continuous stream of data.

LANGUAGE: java
CODE:
FixedBatchSpout spout = new FixedBatchSpout(new Fields("sentence"), 3,
               new Values("the cow jumped over the moon"),
               new Values("the man went to the store and bought some candy"),
               new Values("four score and seven years ago"),
               new Values("how many apples can you eat"));
spout.setCycle(true);

----------------------------------------

TITLE: Implementing a Sliding Window Bolt in Java
DESCRIPTION: Example implementation of a sliding window bolt in Apache Storm, extending BaseWindowedBolt. It demonstrates how to process tuples within a window and emit results.

LANGUAGE: java
CODE:
public class SlidingWindowBolt extends BaseWindowedBolt {
	private OutputCollector collector;
	
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
    	this.collector = collector;
    }
	
    @Override
    public void execute(TupleWindow inputWindow) {
	  for(Tuple tuple: inputWindow.get()) {
	    // do the windowing computation
		...
	  }
	  // emit the results
	  collector.emit(new Values(computedValue));
    }
}

public static void main(String[] args) {
    TopologyBuilder builder = new TopologyBuilder();
     builder.setSpout("spout", new RandomSentenceSpout(), 1);
     builder.setBolt("slidingwindowbolt", 
                     new SlidingWindowBolt().withWindow(new Count(30), new Count(10)),
                     1).shuffleGrouping("spout");
    Config conf = new Config();
    conf.setDebug(true);
    conf.setNumWorkers(1);

    StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());
	
}

----------------------------------------

TITLE: Implementing Stateful Word Count Bolt in Java
DESCRIPTION: Example of a stateful word count bolt that extends BaseStatefulBolt and uses KeyValueState to store word counts. The bolt initializes its state, updates counts on each tuple, and emits updated counts.

LANGUAGE: java
CODE:
public class WordCountBolt extends BaseStatefulBolt<KeyValueState<String, Long>> {
    private KeyValueState<String, Long> wordCounts;
    private OutputCollector collector;
    ...
        @Override
        public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
          this.collector = collector;
        }
        @Override
        public void initState(KeyValueState<String, Long> state) {
          wordCounts = state;
        }
        @Override
        public void execute(Tuple tuple) {
          String word = tuple.getString(0);
          Integer count = wordCounts.get(word, 0);
          count++;
          wordCounts.put(word, count);
          collector.emit(tuple, new Values(word, count));
          collector.ack(tuple);
        }
    ...
    }

----------------------------------------

TITLE: Running Storm SQL in Explain Mode
DESCRIPTION: Command to run Storm SQL in explain mode, which shows the query plan instead of submitting the topology. It uses the same syntax as submitting a topology but with '--explain' as the topology name.

LANGUAGE: bash
CODE:
$ bin/storm sql order_filtering.sql --explain --artifacts "org.apache.storm:storm-sql-kafka:2.0.0-SNAPSHOT,org.apache.storm:storm-kafka-client:2.0.0-SNAPSHOT,org.apache.kafka:kafka-clients:1.1.0^org.slf4j:slf4j-log4j12"

----------------------------------------

TITLE: Adding Apache Storm Dependency in Maven POM
DESCRIPTION: This XML snippet shows how to include Apache Storm as a development dependency in a Maven project's pom.xml file. It specifies the groupId, artifactId, version, and scope for the Storm client dependency.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.storm</groupId>
  <artifactId>storm-client</artifactId>
  <version>{{page.version}}</version>
  <scope>provided</scope>
</dependency>

----------------------------------------

TITLE: Acker Execution Flow Documentation
DESCRIPTION: The acker bolt handles three main operations: initialization of new tupletrees, processing of acks, and handling of failures. It maintains a checksum that reaches zero when all tuples in a tree are successfully processed. The acker uses a RotatingMap to track pending tuples and manage their expiration.



----------------------------------------

TITLE: Building Simple Storm Topology
DESCRIPTION: Example showing how to build a simple Storm topology with spouts and bolts using TopologyBuilder.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();        
builder.setSpout("words", new TestWordSpout(), 10);        
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
        .shuffleGrouping("words");
builder.setBolt("exclaim2", new ExclamationBolt(), 2)
        .shuffleGrouping("exclaim1");

----------------------------------------

TITLE: Task Message Routing Implementation
DESCRIPTION: Implementation of message routing logic in tasks, handling both direct streams and regular streams with grouping functions.

LANGUAGE: clojure
CODE:
task.clj

----------------------------------------

TITLE: Submitting Storm Topology using StormSubmitter in Java
DESCRIPTION: This snippet demonstrates how to use StormSubmitter to submit a Storm topology to a cluster. It configures the number of workers and maximum pending spouts before submitting the topology.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(20);
conf.setMaxSpoutPending(5000);
StormSubmitter.submitTopology("mytopology", conf, topology);

----------------------------------------

TITLE: Initializing EsIndexBolt for Storm-Elasticsearch Integration in Java
DESCRIPTION: This snippet demonstrates how to initialize an EsIndexBolt for streaming tuples directly into Elasticsearch. It requires an EsConfig object for cluster configuration and an EsTupleMapper for mapping tuples to Elasticsearch documents.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsIndexBolt indexBolt = new EsIndexBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Configuring Resource Consumption for Trident Topology in Java
DESCRIPTION: This snippet demonstrates how to set resource defaults and specify CPU and memory loads for different operations in a Trident topology. It shows the usage of setResourceDefaults, setCPULoad, setMemoryLoad, and parallelismHint methods on various stream operations.

LANGUAGE: java
CODE:
    TridentTopology topo = new TridentTopology();
    topo.setResourceDefaults(new DefaultResourceDeclarer()
                                                          .setMemoryLoad(128)
                                                          .setCPULoad(20));
    TridentState wordCounts =
        topology
            .newStream("words", feeder)
            .parallelismHint(5)
            .setCPULoad(20)
            .setMemoryLoad(512,256)
            .each( new Fields("sentence"),  new Split(), new Fields("word"))
            .setCPULoad(10)
            .setMemoryLoad(512)
            .each(new Fields("word"), new BangAdder(), new Fields("word!"))
            .parallelismHint(10)
            .setCPULoad(50)
            .setMemoryLoad(1024)
            .each(new Fields("word!"), new QMarkAdder(), new Fields("word!?"))
            .groupBy(new Fields("word!"))
            .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))
            .setCPULoad(100)
            .setMemoryLoad(2048);

----------------------------------------

TITLE: Initializing Trident Topology with Persistent Aggregation
DESCRIPTION: Example of creating a Trident topology that performs word count aggregation and persists results to Memcached using opaque transactional semantics.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
      topology.newStream("spout1", spout)
        .each(new Fields("sentence"), new Split(), new Fields("word"))
        .groupBy(new Fields("word"))
        .persistentAggregate(MemcachedState.opaque(serverLocations), new Count(), new Fields("count"))                
        .parallelismHint(6);

----------------------------------------

TITLE: Custom Configuration Setup - Java Service
DESCRIPTION: Example showing how to create custom configuration classes that implement the Validated interface for Storm plugins or custom schedulers.

LANGUAGE: java
CODE:
public class CustomConfig implements Validated {
    public static final String CUSTOM_CONFIG = "custom.config.value";
}

LANGUAGE: text
CODE:
// META-INF/services/org.apache.storm.validation.Validated
com.example.CustomConfig

----------------------------------------

TITLE: Executing Acker Logic in Apache Storm (Java)
DESCRIPTION: The acker bolt's execute method processes new tuple trees, updates checksums, and manages completion of tuple processing. It handles initialization, acking, failing, and notifying spouts of tuple tree status.

LANGUAGE: java
CODE:
// On a tick tuple, just advance pending tupletree checksums towards death and return.
// Otherwise, update or create the record for this tupletree:

// on init: initialize with the given checksum value, and record the spout's id for later.
// on ack:  xor the partial checksum into the existing checksum value
// on fail: just mark it as failed

// Next, put the record into the RotatingMap (thus resetting is countdown to expiry) and take action:

// if the total checksum is zero, the tupletree is complete: remove it from the pending collection and notify the spout of success
// if the tupletree has failed, it is also complete:   remove it from the pending collection and notify the spout of failure

// Finally, pass on an ack of our own.

----------------------------------------

TITLE: Storm JoinBolt Implementation
DESCRIPTION: Java implementation of JoinBolt that replicates the SQL join functionality with 4 spouts and window configuration.

LANGUAGE: java
CODE:
JoinBolt jbolt =  new JoinBolt("spout1", "key1")                   // from        spout1  
                    .join     ("spout2", "userId",  "spout1")      // inner join  spout2  on spout2.userId = spout1.key1
                    .join     ("spout3", "key3",    "spout2")      // inner join  spout3  on spout3.key3   = spout2.userId   
                    .leftJoin ("spout4", "key4",    "spout3")      // left join   spout4  on spout4.key4   = spout3.key3
                    .select  ("userId, key4, key2, spout3:key3")   // chose output fields
                    .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;

topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("spout1", new Fields("key1") )
            .fieldsGrouping("spout2", new Fields("userId") )
            .fieldsGrouping("spout3", new Fields("key3") )
            .fieldsGrouping("spout4", new Fields("key4") );

----------------------------------------

TITLE: Configuring MongoUpdateBolt for Storm-MongoDB Integration
DESCRIPTION: Example of how to configure and use MongoUpdateBolt for updating data in MongoDB from a Storm topology.

LANGUAGE: java
CODE:
MongoUpdateMapper mapper = new SimpleMongoUpdateMapper()
        .withFields("word", "count");

QueryFilterCreator updateQueryCreator = new SimpleQueryFilterCreator()
        .withField("word");

MongoUpdateBolt updateBolt = new MongoUpdateBolt(url, collectionName, updateQueryCreator, mapper);

//if a new document should be inserted if there are no matches to the query filter
//updateBolt.withUpsert(true);

//whether find all documents according to the query filter
//updateBolt.withMany(true);

----------------------------------------

TITLE: Configuring KafkaSpout in Storm Topology
DESCRIPTION: Shows how to set up a KafkaSpout to consume data from Kafka in a Storm topology.

LANGUAGE: java
CODE:
final TopologyBuilder tp = new TopologyBuilder();
tp.setSpout("kafka_spout", new KafkaSpout<>(KafkaSpoutConfig.builder("127.0.0.1:" + port, "topic").build()), 1);
tp.setBolt("bolt", new myBolt()).shuffleGrouping("kafka_spout");

----------------------------------------

TITLE: Implementing Word Count Stream Processing in Trident
DESCRIPTION: Creates a Trident topology that splits sentences into words and maintains persistent word counts using memory map state.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
     topology.newStream("spout1", spout)
       .each(new Fields("sentence"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))                
       .parallelismHint(6);

----------------------------------------

TITLE: Performing Aggregations on Streams in Java
DESCRIPTION: Demonstrates how to perform global and key-based aggregations on streams using aggregate, reduce, aggregateByKey, and reduceByKey operations.

LANGUAGE: java
CODE:
Stream<Long> numbers = â€¦
// aggregate the numbers and produce a stream of last 10 sec sums.
Stream<Long> sums = numbers.window(TumblingWindows.of(Duration.seconds(10))).aggregate(new Sum());

// the last 10 sec sums computed using reduce
Stream<Long> sums = numbers.window(...).reduce((x, y) -> x + y);

Stream<String> words = ...                                              // a windowed stream of words
Stream<String, Long> wordCounts = words.mapToPair(w -> Pair.of(w,1)     // convert to a stream of (word, 1) pairs
                                       .aggregateByKey(new Count<>());  // compute counts per word

Stream<String> words = ...                                              // a windowed stream of words
Stream<String, Long> wordCounts = words.mapToPair(w -> Pair.of(w,1)     // convert to a stream of (word, 1) pairs
                                       .reduceByKey((x, y) -> x + y);   // compute counts per word

----------------------------------------

TITLE: Configuring Complete Storm Topology in Java
DESCRIPTION: Complete example of configuring a Storm topology with multiple components. Shows configuration of worker processes, spouts, and bolts with different parallelism settings.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(2); // use two worker processes

topologyBuilder.setSpout("blue-spout", new BlueSpout(), 2); // set parallelism hint to 2

topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

topologyBuilder.setBolt("yellow-bolt", new YellowBolt(), 6)
               .shuffleGrouping("green-bolt");

StormSubmitter.submitTopology(
        "mytopology",
        conf,
        topologyBuilder.createTopology()
    );

----------------------------------------

TITLE: Windowing Operations in Storm's Stream API (Java)
DESCRIPTION: Shows how to apply windowing operations to streams, including sliding and tumbling windows.

LANGUAGE: java
CODE:
// time based sliding window
stream.window(SlidingWindows.of(Duration.minutes(10), Duration.minutes(1)));

// count based sliding window
stream.window(SlidingWindows.of(Count.(10), Count.of(2)));

// tumbling window
stream.window(TumblingWindows.of(Duration.seconds(10));

// specifying timestamp field for event time based processing and a late tuple stream.
stream.window(TumblingWindows.of(Duration.seconds(10)
                     .withTimestampField("ts")
                     .withLateTupleStream("late_events"));

----------------------------------------

TITLE: Implementing Persistent Word Count Topology in Java
DESCRIPTION: Complete example of a Storm topology that implements persistent word counting using HBase for storage.

LANGUAGE: java
CODE:
public class PersistentWordCount {
    private static final String WORD_SPOUT = "WORD_SPOUT";
    private static final String COUNT_BOLT = "COUNT_BOLT";
    private static final String HBASE_BOLT = "HBASE_BOLT";


    public static void main(String[] args) throws Exception {
        Config config = new Config();

        WordSpout spout = new WordSpout();
        WordCounter bolt = new WordCounter();

        SimpleHBaseMapper mapper = new SimpleHBaseMapper()
                .withRowKeyField("word")
                .withColumnFields(new Fields("word"))
                .withCounterFields(new Fields("count"))
                .withColumnFamily("cf");

        HBaseBolt hbase = new HBaseBolt("WordCount", mapper);


        // wordSpout ==> countBolt ==> HBaseBolt
        TopologyBuilder builder = new TopologyBuilder();

        builder.setSpout(WORD_SPOUT, spout, 1);
        builder.setBolt(COUNT_BOLT, bolt, 1).shuffleGrouping(WORD_SPOUT);
        builder.setBolt(HBASE_BOLT, hbase, 1).fieldsGrouping(COUNT_BOLT, new Fields("word"));


        if (args.length == 0) {
            LocalCluster cluster = new LocalCluster();
            cluster.submitTopology("test", config, builder.createTopology());
            Thread.sleep(10000);
            cluster.killTopology("test");
            cluster.shutdown();
            System.exit(0);
        } else {
            config.setNumWorkers(3);
            StormSubmitter.submitTopology(args[0], config, builder.createTopology());
        }
    }
}

----------------------------------------

TITLE: Configuring Storm Bolt Parallelism in Java
DESCRIPTION: Example showing how to configure a Storm bolt with specific executor and task counts. Sets up a GreenBolt with 2 executors and 4 tasks, using shuffle grouping from a blue-spout stream.

LANGUAGE: java
CODE:
topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

----------------------------------------

TITLE: Maven POM Configuration for Storm Project
DESCRIPTION: Example pom.xml configuration for setting up Maven dependencies in a Storm project. Reference to storm-starter project structure.

LANGUAGE: xml
CODE:
<source path="src/jvm/"/>

----------------------------------------

TITLE: Registering Task Hooks in Storm
DESCRIPTION: There are two methods to register task hooks in Storm: using TopologyContext in the component's lifecycle methods or through Storm configuration. Task hooks extend BaseTaskHook class and override methods for specific events.

LANGUAGE: java
CODE:
TopologyContext#addTaskHook
topology.auto.task.hooks

----------------------------------------

TITLE: Stateful Persistent Windowed Bolt Example
DESCRIPTION: Implementation example of a stateful windowed bolt with persistence support and state management.

LANGUAGE: java
CODE:
public class MyStatefulPersistentWindowedBolt extends BaseStatefulWindowedBolt<K, V> {
  private KeyValueState<K, V> state;
  
  @Override
  public void initState(KeyValueState<K, V> state) {
    this.state = state;
   // ...
   // restore the state from the last saved state.
   // ...
  }
  
  @Override
  public void execute(TupleWindow window) {      
    // iterate over tuples in the current window
    Iterator<Tuple> it = window.getIter();
    while (it.hasNext()) {
        // compute some result based on the tuples in window
    }
    
    // possibly update any state to be maintained across windows
    state.put(STATE_KEY, updatedValue);
    
    // emit the results downstream
    collector.emit(new Values(result));
  }
}

----------------------------------------

TITLE: Configuring Zookeeper Servers in Storm YAML
DESCRIPTION: This YAML configuration specifies the list of Zookeeper servers for the Storm cluster. It's a mandatory configuration in the storm.yaml file.

LANGUAGE: yaml
CODE:
storm.zookeeper.servers:
  - "111.222.333.444"
  - "555.666.777.888"

----------------------------------------

TITLE: Defining a Storm Topology in Clojure
DESCRIPTION: Example of defining a Storm topology using the Clojure DSL, specifying spouts and bolts with their configurations and connections.

LANGUAGE: clojure
CODE:
(topology
 {"1" (spout-spec sentence-spout)
  "2" (spout-spec (sentence-spout-parameterized
                   ["the cat jumped over the door"
                    "greetings from a faraway land"])
                   :p 2)}
 {"3" (bolt-spec {"1" :shuffle "2" :shuffle}
                 split-sentence
                 :p 5)
  "4" (bolt-spec {"3" ["word"]}
                 word-count
                 :p 6)})

----------------------------------------

TITLE: Initializing JdbcInsertBolt in Java for Storm JDBC
DESCRIPTION: Example of initializing a JdbcInsertBolt with a HikariCP connection provider and a SimpleJdbcMapper. It demonstrates setting up the connection, creating the mapper, and configuring the bolt with table name and query timeout.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);

String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withTableName("user")
                                    .withQueryTimeoutSecs(30);
                                    Or
JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withInsertQuery("insert into user values (?,?)")
                                    .withQueryTimeoutSecs(30);   

----------------------------------------

TITLE: Retrieving Topology Summary in Storm UI API
DESCRIPTION: GET request to retrieve summary information for all topologies in the Storm cluster, including status, uptime, and resource usage.

LANGUAGE: json
CODE:
{
  "topologies": [
    {
      "id": "WordCount3-1-1402960825",
      "name": "WordCount3",
      "status": "ACTIVE",
      "uptime": "6m 5s",
      "uptimeSeconds": 365,
      "tasksTotal": 28,
      "workersTotal": 3,
      "executorsTotal": 28,
      "replicationCount": 1,
      "requestedMemOnHeap": 640,
      "requestedMemOffHeap": 128,
      "requestedTotalMem": 768,
      "requestedCpu": 80,
      "assignedMemOnHeap": 640,
      "assignedMemOffHeap": 128,
      "assignedTotalMem": 768,
      "assignedCpu": 80
    }
  ],
  "schedulerDisplayResource": true
}

----------------------------------------

TITLE: Configuring KafkaSpout with Multiple Output Streams
DESCRIPTION: Example of setting up a KafkaSpout that emits to multiple output streams based on the Kafka topic.

LANGUAGE: java
CODE:
final TopologyBuilder tp = new TopologyBuilder();

//By default all topics not covered by another rule, but consumed by the spout will be emitted to "STREAM_1" as "topic", "key", and "value"
ByTopicRecordTranslator<String, String> byTopic = new ByTopicRecordTranslator<>(
    (r) -> new Values(r.topic(), r.key(), r.value()),
    new Fields("topic", "key", "value"), "STREAM_1");
//For topic_2 all events will be emitted to "STREAM_2" as just "key" and "value"
byTopic.forTopic("topic_2", (r) -> new Values(r.key(), r.value()), new Fields("key", "value"), "STREAM_2");

tp.setSpout("kafka_spout", new KafkaSpout<>(KafkaSpoutConfig.builder("127.0.0.1:" + port, "topic_1", "topic_2", "topic_3").build()), 1);
tp.setBolt("bolt", new myBolt()).shuffleGrouping("kafka_spout", "STREAM_1");
tp.setBolt("another", new myOtherBolt()).shuffleGrouping("kafka_spout", "STREAM_2");

----------------------------------------

TITLE: Defining a Simple Wordcount Topology in YAML
DESCRIPTION: YAML configuration defining a simple wordcount topology using Flux, including spouts, bolts, and stream connections.

LANGUAGE: yaml
CODE:
name: "yaml-topology"
config:
  topology.workers: 1

# spout definitions
spouts:
  - id: "spout-1"
    className: "org.apache.storm.testing.TestWordSpout"
    parallelism: 1

# bolt definitions
bolts:
  - id: "bolt-1"
    className: "org.apache.storm.testing.TestWordCounter"
    parallelism: 1
  - id: "bolt-2"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1

#stream definitions
streams:
  - name: "spout-1 --> bolt-1" # name isn't used (placeholder for logging, UI, etc.)
    from: "spout-1"
    to: "bolt-1"
    grouping:
      type: FIELDS
      args: ["word"]

  - name: "bolt-1 --> bolt2"
    from: "bolt-1"
    to: "bolt-2"
    grouping:
      type: SHUFFLE

# worker hook definitions
workerHooks:
  - id: "base-worker-hook"
    className: "org.apache.storm.hooks.BaseWorkerHook"

----------------------------------------

TITLE: Configuring Node Resources in Storm
DESCRIPTION: YAML configuration to specify available memory and CPU resources on a Storm supervisor node.

LANGUAGE: yaml
CODE:
supervisor.memory.capacity.mb: [amount<Double>]
supervisor.cpu.capacity: [amount<Double>]

----------------------------------------

TITLE: Configuring HDFS Bolt in Java
DESCRIPTION: Example of how to configure and use the HDFS Bolt to write pipe-delimited files to HDFS, with file rotation and sync policies.

LANGUAGE: java
CODE:
// use "|" instead of "," for field delimiter
RecordFormat format = new DelimitedRecordFormat()
        .withFieldDelimiter("|");

// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withPath("/foo/");

HdfsBolt bolt = new HdfsBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withRecordFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy);

----------------------------------------

TITLE: Storm Spout Interface Definition
DESCRIPTION: Interface definition for Storm spouts showing core methods for message processing and reliability.

LANGUAGE: java
CODE:
public interface ISpout extends Serializable {
    void open(Map conf, TopologyContext context, SpoutOutputCollector collector);
    void close();
    void nextTuple();
    void ack(Object msgId);
    void fail(Object msgId);
}

----------------------------------------

TITLE: Persistent Word Count Topology Implementation
DESCRIPTION: Complete example of a Storm topology that implements word counting with HBase persistence.

LANGUAGE: java
CODE:
public class PersistentWordCount {
    private static final String WORD_SPOUT = "WORD_SPOUT";
    private static final String COUNT_BOLT = "COUNT_BOLT";
    private static final String HBASE_BOLT = "HBASE_BOLT";

    public static void main(String[] args) throws Exception {
        Config config = new Config();

        Map<String, Object> hbConf = new HashMap<String, Object>();
        if(args.length > 0){
            hbConf.put("hbase.rootdir", args[0]);
        }
        config.put("hbase.conf", hbConf);

        WordSpout spout = new WordSpout();
        WordCounter bolt = new WordCounter();

        SimpleHBaseMapper mapper = new SimpleHBaseMapper()
                .withRowKeyField("word")
                .withColumnFields(new Fields("word"))
                .withCounterFields(new Fields("count"))
                .withColumnFamily("cf");

        HBaseBolt hbase = new HBaseBolt("WordCount", mapper)
                .withConfigKey("hbase.conf");

        TopologyBuilder builder = new TopologyBuilder();

        builder.setSpout(WORD_SPOUT, spout, 1);
        builder.setBolt(COUNT_BOLT, bolt, 1).shuffleGrouping(WORD_SPOUT);
        builder.setBolt(HBASE_BOLT, hbase, 1).fieldsGrouping(COUNT_BOLT, new Fields("word"));

        String topoName = "test";
        if (args.length > 1) {
            topoName = args[1];
        }
        if (args.length == 4) {
            System.out.println("hdfs url: " + args[0] + ", keytab file: " + args[2] + 
                ", principal name: " + args[3] + ", toplogy name: " + topoName);
            hbConf.put(HBaseSecurityUtil.STORM_KEYTAB_FILE_KEY, args[2]);
            hbConf.put(HBaseSecurityUtil.STORM_USER_NAME_KEY, args[3]);
        } else if (args.length == 3 || args.length > 4) {
            System.out.println("Usage: PersistentWordCount <hbase.rootdir> [topology name] [keytab file] [principal name]");
            return;
        }
        config.setNumWorkers(3);
        StormSubmitter.submitTopology(topoName, config, builder.createTopology());
    }
}

----------------------------------------

TITLE: Configuring Isolation Scheduler in YAML
DESCRIPTION: This YAML configuration snippet demonstrates how to set up the Isolation Scheduler in Storm. It specifies the number of isolated machines allocated to different topologies, allowing for resource isolation and prioritization.

LANGUAGE: yaml
CODE:
isolation.scheduler.machines: 
    "my-topology": 8
    "tiny-topology": 1
    "some-other-topology": 3

----------------------------------------

TITLE: Submitting Storm Topology using StormSubmitter in Java
DESCRIPTION: Demonstrates how to configure and submit a Storm topology to a cluster using StormSubmitter. Sets worker count and max pending spout configurations.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(20);
conf.setMaxSpoutPending(5000);
StormSubmitter.submitTopology("mytopology", conf, topology);

----------------------------------------

TITLE: Configuring Bolt Parallelism in Storm Topology (Java)
DESCRIPTION: This snippet demonstrates how to set the number of executors and tasks for a bolt in a Storm topology. It configures the 'GreenBolt' with 2 executors and 4 tasks, using shuffle grouping from 'blue-spout'.

LANGUAGE: java
CODE:
topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

----------------------------------------

TITLE: Creating Storm Topology with KestrelSpout in Java
DESCRIPTION: This code snippet demonstrates how to create a Storm topology that reads sentences from a Kestrel queue using KestrelSpout, splits them into words, and counts word occurrences. It uses KestrelSpout, SplitSentence bolt, and WordCount bolt.

LANGUAGE: Java
CODE:
    TopologyBuilder builder = new TopologyBuilder();
    builder.setSpout("sentences", new KestrelSpout("localhost",22133,"sentence_queue",new StringScheme()));
    builder.setBolt("split", new SplitSentence(), 10)
    	        .shuffleGrouping("sentences");
    builder.setBolt("count", new WordCount(), 20)
	        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Configuring HDFS Bolt with Delimited Record Format
DESCRIPTION: Example showing how to configure an HDFS bolt to write pipe-delimited files with sync and rotation policies

LANGUAGE: java
CODE:
// use "|" instead of "," for field delimiter
RecordFormat format = new DelimitedRecordFormat()
        .withFieldDelimiter("|");

// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withPath("/foo/");

HdfsBolt bolt = new HdfsBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withRecordFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy);

----------------------------------------

TITLE: Configuring Trident Topology with RedisClusterState in Java
DESCRIPTION: Demonstrates how to set up a Trident topology using RedisClusterState for persistence and querying in a Redis cluster environment. This example uses custom mappers for storing and looking up word counts.

LANGUAGE: java
CODE:
Set<InetSocketAddress> nodes = new HashSet<InetSocketAddress>();
for (String hostPort : redisHostPort.split(",")) {
    String[] host_port = hostPort.split(":");
    nodes.add(new InetSocketAddress(host_port[0], Integer.valueOf(host_port[1])));
}
JedisClusterConfig clusterConfig = new JedisClusterConfig.Builder().setNodes(nodes)
                                .build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisLookupMapper lookupMapper = new WordCountLookupMapper();
RedisClusterState.Factory factory = new RedisClusterState.Factory(clusterConfig);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory,
                        fields,
                        new RedisClusterStateUpdater(storeMapper).withExpire(86400000),
                        new Fields());

TridentState state = topology.newStaticState(factory);
stream = stream.stateQuery(state, new Fields("word"),
                        new RedisClusterStateQuerier(lookupMapper),
                        new Fields("columnName","columnValue"));

----------------------------------------

TITLE: Uploading Topology Jar in StormSubmitter (Java)
DESCRIPTION: StormSubmitter uploads the topology jar to Nimbus using its Thrift interface. It uploads the jar in 15KB chunks and finishes the upload process.

LANGUAGE: java
CODE:
// Jar uploading code
beginFileUpload();
uploadChunk();
finishFileUpload();

----------------------------------------

TITLE: Initializing Storm Spout in Trident Topology
DESCRIPTION: Example showing how to create a new stream using a regular Storm IRichSpout in a Trident topology. The spout requires a unique identifier that will be used for storing metadata in Zookeeper.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
topology.newStream("myspoutid", new MyRichSpout());

----------------------------------------

TITLE: DRPC Server Configuration in YAML
DESCRIPTION: Configuration settings for DRPC servers in Storm's yaml configuration file.

LANGUAGE: yaml
CODE:
drpc.servers:
  - "drpc1.foo.com"
  - "drpc2.foo.com"
drpc.http.port: 8081
storm.thrift.transport: "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin"

----------------------------------------

TITLE: Defining JdbcLookupMapper Interface in Java
DESCRIPTION: This code snippet defines the JdbcLookupMapper interface, which is the main API for executing select queries against a database using JDBC. It includes methods for declaring output fields, getting columns, and mapping query results to tuples.

LANGUAGE: java
CODE:
void declareOutputFields(OutputFieldsDeclarer declarer);
List<Column> getColumns(ITuple tuple);
List<Values> toTuple(ITuple input, List<Column> columns);

----------------------------------------

TITLE: Configuring Storm Bolt Parallelism in Java
DESCRIPTION: Example showing how to configure a Storm bolt with specific executor and task settings. Sets up a GreenBolt with 2 executors and 4 tasks, using shuffle grouping from a blue-spout.

LANGUAGE: java
CODE:
topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

----------------------------------------

TITLE: Configuring Zookeeper Servers in Storm YAML
DESCRIPTION: This snippet shows how to configure the Zookeeper servers for a Storm cluster in the storm.yaml file. It specifies the IP addresses of the Zookeeper hosts.

LANGUAGE: yaml
CODE:
storm.zookeeper.servers:
  - "111.222.333.444"
  - "555.666.777.888"

----------------------------------------

TITLE: Configuring Isolation Scheduler in YAML
DESCRIPTION: This YAML configuration snippet demonstrates how to set up the Isolation Scheduler in Storm. It specifies the number of isolated machines allocated to different topologies, allowing for resource isolation and prioritization.

LANGUAGE: yaml
CODE:
isolation.scheduler.machines: 
    "my-topology": 8
    "tiny-topology": 1
    "some-other-topology": 3

----------------------------------------

TITLE: Using persistentAggregate for Word Count in Trident
DESCRIPTION: Example of using persistentAggregate to implement a streaming word count topology in Trident, demonstrating stateful processing with aggregation.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
      topology.newStream("spout1", spout)
        .each(new Fields("sentence"), new Split(), new Fields("word"))
        .groupBy(new Fields("word"))
        .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))

----------------------------------------

TITLE: Implementing Streaming Word Count in Trident
DESCRIPTION: Demonstrates how to create a Trident topology for computing streaming word count from an input stream of sentences, using persistent aggregation with in-memory state.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
     topology.newStream("spout1", spout)
       .each(new Fields("sentence"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))                
       .parallelismHint(6);

----------------------------------------

TITLE: Implementing Custom Redis Bolt for Word Count Lookup
DESCRIPTION: Java class extending AbstractRedisBolt to implement custom logic for looking up word counts in Redis.

LANGUAGE: java
CODE:
public static class LookupWordTotalCountBolt extends AbstractRedisBolt {
    private static final Logger LOG = LoggerFactory.getLogger(LookupWordTotalCountBolt.class);
    private static final Random RANDOM = new Random();

    public LookupWordTotalCountBolt(JedisPoolConfig config) {
        super(config);
    }

    public LookupWordTotalCountBolt(JedisClusterConfig config) {
        super(config);
    }

    @Override
    public void execute(Tuple input) {
        JedisCommands jedisCommands = null;
        try {
            jedisCommands = getInstance();
            String wordName = input.getStringByField("word");
            String countStr = jedisCommands.get(wordName);
            if (countStr != null) {
                int count = Integer.parseInt(countStr);
                this.collector.emit(new Values(wordName, count));

                // print lookup result with low probability
                if(RANDOM.nextInt(1000) > 995) {
                    LOG.info("Lookup result - word : " + wordName + " / count : " + count);
                }
            } else {
                // skip
                LOG.warn("Word not found in Redis - word : " + wordName);
            }
        } finally {
            if (jedisCommands != null) {
                returnInstance(jedisCommands);
            }
            this.collector.ack(input);
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        // wordName, count
        declarer.declare(new Fields("wordName", "count"));
    }
}

----------------------------------------

TITLE: Basic Trident Word Count with Persistent State
DESCRIPTION: Example showing how to implement a basic word count topology using Trident's persistent state capabilities with Memcached

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
      topology.newStream("spout1", spout)
        .each(new Fields("sentence"), new Split(), new Fields("word"))
        .groupBy(new Fields("word"))
        .persistentAggregate(MemcachedState.opaque(serverLocations), new Count(), new Fields("count"))                
        .parallelismHint(6);

----------------------------------------

TITLE: Implementing Skewed Streaming Top N Pattern in Storm
DESCRIPTION: This snippet shows an improved streaming top N pattern for skewed data. It uses partialKeyGrouping for load distribution, an additional aggregation layer, and then merges the results.

LANGUAGE: java
CODE:
builder.setBolt("count", new CountObjects(), parallelism)
  .partialKeyGrouping("objects", new Fields("value"));
builder.setBolt("rank" new AggregateCountsAndRank(), parallelism)
  .fieldsGrouping("count", new Fields("key"))
builder.setBolt("merge", new MergeRanksObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Running Storm Topology Command
DESCRIPTION: Basic command to run a Storm topology by packaging code and dependencies into a jar file

LANGUAGE: bash
CODE:
storm jar all-my-code.jar org.apache.storm.MyTopology arg1 arg2

----------------------------------------

TITLE: Implementing Scalar Function in Java for Storm SQL
DESCRIPTION: Java class implementing a scalar function for use in Storm SQL. The class defines an 'evaluate' method which Storm SQL uses to determine it's a scalar function.

LANGUAGE: java
CODE:
public class MyPlus {
  public static Integer evaluate(Integer x, Integer y) {
    return x + y;
  }
}

----------------------------------------

TITLE: Initializing Fixed Batch Spout in Trident
DESCRIPTION: Creates a fixed batch spout that cycles through a set of sentences to produce a continuous stream of data for processing.

LANGUAGE: java
CODE:
FixedBatchSpout spout = new FixedBatchSpout(new Fields("sentence"), 3,
               new Values("the cow jumped over the moon"),
               new Values("the man went to the store and bought some candy"),
               new Values("four score and seven years ago"),
               new Values("how many apples can you eat"));
spout.setCycle(true);

----------------------------------------

TITLE: Creating StateFactory for Custom State
DESCRIPTION: Example of implementing a StateFactory to create instances of a custom State object (LocationDB) for use in Trident tasks.

LANGUAGE: java
CODE:
public class LocationDBFactory implements StateFactory {
   public State makeState(Map conf, int partitionIndex, int numPartitions) {
      return new LocationDB();
   } 
}

----------------------------------------

TITLE: Configuring SolrUpdateBolt with JSON Mapper in Java
DESCRIPTION: This snippet demonstrates how to set up a SolrUpdateBolt using a JSON mapper and a count-based commit strategy. It configures the Solr connection, defines a mapper for JSON content, and sets up a commit strategy.

LANGUAGE: java
CODE:
new SolrUpdateBolt(solrConfig, solrMapper, solrCommitStgy)

// zkHostString for Solr 'gettingstarted' example
SolrConfig solrConfig = new SolrConfig("127.0.0.1:9983");

// JSON Mapper used to generate 'SolrRequest' requests to update the "gettingstarted" Solr collection with JSON content declared the tuple field with name "JSON"
SolrMapper solrMapper = new SolrJsonMapper.Builder("gettingstarted", "JSON").build(); 
 
// Acks every other five tuples. Setting to null acks every tuple
SolrCommitStrategy solrCommitStgy = new CountBasedCommit(5);          

----------------------------------------

TITLE: Creating and Querying Kafka Streams with Storm SQL
DESCRIPTION: Example showing how to create external tables representing Kafka streams and perform filtering and aggregation operations. The query creates two tables (ORDERS and LARGE_ORDERS) and inserts filtered results from ORDERS into LARGE_ORDERS based on a calculated total value condition.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://...' ...

CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) 'kafka://...' ...

INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Implementing a Bolt in Java with IRichBolt Interface
DESCRIPTION: Demonstrates a full implementation of a bolt using the IRichBolt interface, including all required methods.

LANGUAGE: java
CODE:
public static class ExclamationBolt implements IRichBolt {
    OutputCollector _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple tuple) {
        _collector.emit(tuple, new Values(tuple.getString(0) + "!!!"));
        _collector.ack(tuple);
    }

    @Override
    public void cleanup() {
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word"));
    }
    
    @Override
    public Map<String, Object> getComponentConfiguration() {
        return null;
    }
}

----------------------------------------

TITLE: Configuring SimpleJdbcMapper for Storm-JDBC Integration
DESCRIPTION: Example of configuring SimpleJdbcMapper for transforming Storm tuples to database rows. Shows how to set up connection provider and specify table name or column schema.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);
String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

// For explicit column schema
List<Column> columnSchema = Lists.newArrayList(
    new Column("user_id", java.sql.Types.INTEGER),
    new Column("user_name", java.sql.Types.VARCHAR));
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(columnSchema);

----------------------------------------

TITLE: Basic Stream API Usage in Java
DESCRIPTION: Shows how to create a stream from a spout, transform it using basic operations like map and flatMap, and perform output operations.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

// a stream of sentences obtained from a source spout
Stream<String> sentences = builder.newStream(new RandomSentenceSpout()).map(tuple -> tuple.getString(0));

// a stream of words obtained by transforming (splitting) the stream of sentences
Stream<String> words = sentences.flatMap(s -> Arrays.asList(s.split(" ")));

// output operation that prints the words to console
words.forEach(w -> System.out.println(w));

----------------------------------------

TITLE: Implementing Tuple Counting Bolt with Metrics
DESCRIPTION: Implementation of a bolt that counts received tuples using Storm's metrics system. The bolt registers a counter in prepare() and increments it for each tuple in execute().

LANGUAGE: java
CODE:
public class TupleCountingBolt extends BaseRichBolt {
    private Counter tupleCounter;
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        this.tupleCounter = context.registerCounter("tupleCount");
    }

    @Override
    public void execute(Tuple input) {
        this.tupleCounter.inc();
    }
}

----------------------------------------

TITLE: Configuring Windowing Operations in Java
DESCRIPTION: Shows how to configure different types of windows (sliding, tumbling) based on time or count, including event time processing.

LANGUAGE: java
CODE:
// time based sliding window
stream.window(SlidingWindows.of(Duration.minutes(10), Duration.minutes(1)));

// count based sliding window
stream.window(SlidingWindows.of(Count.(10), Count.of(2)));

// tumbling window
stream.window(TumblingWindows.of(Duration.seconds(10));

// specifying timestamp field for event time based processing and a late tuple stream.
stream.window(TumblingWindows.of(Duration.seconds(10)
                     .withTimestampField("ts")
                     .withLateTupleStream("late_events"));

----------------------------------------

TITLE: Debugging Storm Topology in IDE
DESCRIPTION: Shows how to enable local mode override for debugging Storm topologies in an IDE environment using LocalCluster.withLocalModeOverride.

LANGUAGE: java
CODE:
public static void main(final String args[]) {
    LocalCluster.withLocalModeOverride(() -> originalMain(args), 10);
}

----------------------------------------

TITLE: Configuring a Kafka Spout with Multiple Output Streams in Java
DESCRIPTION: Example of setting up a Kafka spout that emits to multiple streams based on topics.

LANGUAGE: java
CODE:
final TopologyBuilder tp = new TopologyBuilder();

//By default all topics not covered by another rule, but consumed by the spout will be emitted to "STREAM_1" as "topic", "key", and "value"
ByTopicRecordTranslator<String, String> byTopic = new ByTopicRecordTranslator<>(
    (r) -> new Values(r.topic(), r.key(), r.value()),
    new Fields("topic", "key", "value"), "STREAM_1");
//For topic_2 all events will be emitted to "STREAM_2" as just "key" and "value"
byTopic.forTopic("topic_2", (r) -> new Values(r.key(), r.value()), new Fields("key", "value"), "STREAM_2");

tp.setSpout("kafka_spout", new KafkaSpout<>(KafkaSpoutConfig.builder("127.0.0.1:" + port, "topic_1", "topic_2", "topic_3").build()), 1);
tp.setBolt("bolt", new myBolt()).shuffleGrouping("kafka_spout", "STREAM_1");
tp.setBolt("another", new myOtherBolt()).shuffleGrouping("kafka_spout", "STREAM_2");

----------------------------------------

TITLE: Multi-Anchoring Tuples in Storm Bolts
DESCRIPTION: This code snippet demonstrates how to anchor an output tuple to multiple input tuples in a Storm bolt. This is useful for streaming joins or aggregations where multiple input tuples contribute to a single output tuple.

LANGUAGE: java
CODE:
List<Tuple> anchors = new ArrayList<Tuple>();
anchors.add(tuple1);
anchors.add(tuple2);
_collector.emit(anchors, new Values(1, 2, 3));

----------------------------------------

TITLE: Implementing StateUpdater for Bulk Updates in Trident
DESCRIPTION: Example of implementing a StateUpdater (LocationUpdater) to perform bulk updates on a custom State object (LocationDB) in a Trident topology.

LANGUAGE: java
CODE:
public class LocationUpdater extends BaseStateUpdater<LocationDB> {
    public void updateState(LocationDB state, List<TridentTuple> tuples, TridentCollector collector) {
        List<Long> ids = new ArrayList<Long>();
        List<String> locations = new ArrayList<String>();
        for(TridentTuple t: tuples) {
            ids.add(t.getLong(0));
            locations.add(t.getString(1));
        }
        state.setLocationsBulk(ids, locations);
    }
}

----------------------------------------

TITLE: Implementing a Stateful Persistent Windowed Bolt in Java
DESCRIPTION: Example of a stateful persistent windowed bolt in Apache Storm, demonstrating how to initialize state, process windowed tuples, and maintain state across windows using KeyValueState.

LANGUAGE: java
CODE:
public class MyStatefulPersistentWindowedBolt extends BaseStatefulWindowedBolt<K, V> {
  private KeyValueState<K, V> state;
  
  @Override
  public void initState(KeyValueState<K, V> state) {
    this.state = state;
   // ...
   // restore the state from the last saved state.
   // ...
  }
  
  @Override
  public void execute(TupleWindow window) {      
    // iterate over tuples in the current window
    Iterator<Tuple> it = window.getIter();
    while (it.hasNext()) {
        // compute some result based on the tuples in window
    }
    
    // possibly update any state to be maintained across windows
    state.put(STATE_KEY, updatedValue);
    
    // emit the results downstream
    collector.emit(new Values(result));
  }
}

----------------------------------------

TITLE: Implementing Word Split Bolt in Python
DESCRIPTION: Python implementation of a bolt that splits sentences into individual words.

LANGUAGE: python
CODE:
import storm

class SplitSentenceBolt(storm.BasicBolt):
    def process(self, tup):
        words = tup.values[0].split(" ")
        for word in words:
          storm.emit([word])

SplitSentenceBolt().run()

----------------------------------------

TITLE: Initializing Trident Topology with Persistent Aggregation
DESCRIPTION: This snippet demonstrates how to create a Trident topology that computes word counts and stores them in a Memcached state. It uses a persistent aggregate operation with opaque transactional semantics.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
      topology.newStream("spout1", spout)
        .each(new Fields("sentence"), new Split(), new Fields("word"))
        .groupBy(new Fields("word"))
        .persistentAggregate(MemcachedState.opaque(serverLocations), new Count(), new Fields("count"))                
        .parallelismHint(6);

----------------------------------------

TITLE: Creating External Table for Kafka Source/Sink
DESCRIPTION: SQL statement to create an external table representing a Kafka spout and sink in Storm SQL.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Implementing Tuple Counting Bolt with Metrics
DESCRIPTION: Implementation of a bolt that counts received tuples using Storm's metrics system. The bolt registers a counter in prepare() and increments it for each tuple in execute().

LANGUAGE: java
CODE:
public class TupleCountingBolt extends BaseRichBolt {
    private Counter tupleCounter;
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        this.tupleCounter = context.registerCounter("tupleCount");
    }

    @Override
    public void execute(Tuple input) {
        this.tupleCounter.inc();
    }
}

----------------------------------------

TITLE: Kafka Table Configuration Example
DESCRIPTION: Example of creating an external table linked to Kafka stream

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Configuring Buffer Sizes in Apache Storm
DESCRIPTION: Sets the size of message queues for spout/bolt executors and inter-worker communication. These settings affect throughput and latency.

LANGUAGE: java
CODE:
topology.executor.receive.buffer.size
topology.transfer.buffer.size

----------------------------------------

TITLE: Storm Metrics Filter Interface
DESCRIPTION: Interface definition for creating custom metric filters in Storm. Shows the required methods for implementing a custom filter to control which metrics are reported.

LANGUAGE: java
CODE:
public interface StormMetricsFilter extends MetricFilter {

    /**
     * Called after the filter is instantiated.
     * @param config A map of the properties from the 'filter' section of the reporter configuration.
     */
    void prepare(Map<String, Object> config);
    
   /**
    *  Returns true if the given metric should be reported.
    */
    boolean matches(String name, Metric metric);

}

----------------------------------------

TITLE: Configuring Resource Aware Scheduler in Storm YAML
DESCRIPTION: Basic configuration to enable the Resource Aware Scheduler in Storm's configuration file

LANGUAGE: yaml
CODE:
storm.scheduler: "org.apache.storm.scheduler.resource.ResourceAwareScheduler"

----------------------------------------

TITLE: Using Static Factory Methods in Flux
DESCRIPTION: Example of using static factory methods in Java classes and referencing them in Flux YAML configuration.

LANGUAGE: java
CODE:
public class TestBolt extends BaseBasicBolt {
  public static TestBolt newInstance(Duration triggerTime) {
    return new TestBolt(triggerTime);
  }
}

public class Duration {
  public static Duration ofSeconds(long seconds) {
    return new Duration(seconds);
  }
}

LANGUAGE: yaml
CODE:
components:
  - id: "time"
    className: "java.time.Duration"
    factory: "ofSeconds"

bolts:
  - id: "testBolt"
    className: "org.apache.storm.flux.test.TestBolt"
    factory: "newInstance"
    factoryArgs:
      - ref: "time"

----------------------------------------

TITLE: Configuring HDFS Bolt in Java
DESCRIPTION: Example showing how to configure an HDFS bolt with delimited record format, sync policy, rotation policy and file naming format

LANGUAGE: java
CODE:
// use "|" instead of "," for field delimiter
RecordFormat format = new DelimitedRecordFormat()
        .withFieldDelimiter("|");

// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withPath("/foo/");

HdfsBolt bolt = new HdfsBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withRecordFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy);

----------------------------------------

TITLE: Configuring SimpleJdbcLookupMapper in Java
DESCRIPTION: Example of configuring SimpleJdbcLookupMapper for database lookups. It shows how to initialize the mapper with output fields and query parameter columns.

LANGUAGE: java
CODE:
Fields outputFields = new Fields("user_id", "user_name", "create_date");
List<Column> queryParamColumns = Lists.newArrayList(new Column("user_id", Types.INTEGER));
this.jdbcLookupMapper = new SimpleJdbcLookupMapper(outputFields, queryParamColumns);

----------------------------------------

TITLE: Initializing Stream and Performing Basic Operations in Java
DESCRIPTION: Demonstrates creating a StreamBuilder, initializing a stream from a spout, and performing map and flatMap operations. Also shows how to apply an output operation (forEach) to print results.

LANGUAGE: java
CODE:
// imports
import org.apache.storm.streams.Stream;
import org.apache.storm.streams.StreamBuilder;
...

StreamBuilder builder = new StreamBuilder();

// a stream of sentences obtained from a source spout
Stream<String> sentences = builder.newStream(new RandomSentenceSpout()).map(tuple -> tuple.getString(0));

// a stream of words obtained by transforming (splitting) the stream of sentences
Stream<String> words = sentences.flatMap(s -> Arrays.asList(s.split(" ")));

// output operation that prints the words to console
words.forEach(w -> System.out.println(w));

----------------------------------------

TITLE: Defining ConnectionProvider Interface for JDBC in Java
DESCRIPTION: Interface for implementing database connection pooling mechanisms. Includes methods for preparing, getting connections, and cleanup.

LANGUAGE: java
CODE:
public interface ConnectionProvider extends Serializable {
    /**
     * method must be idempotent.
     */
    void prepare();

    /**
     *
     * @return a DB connection over which the queries can be executed.
     */
    Connection getConnection();

    /**
     * called once when the system is shutting down, should be idempotent.
     */
    void cleanup();
}

----------------------------------------

TITLE: Configuring URL Expansion with Shuffle Grouping in Storm
DESCRIPTION: Example showing URL expansion bolt configuration using shuffle grouping for distributed processing of URLs.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .shuffleGrouping(1);

----------------------------------------

TITLE: Setting Storm Local Directory in YAML
DESCRIPTION: This configuration specifies the local directory for Storm to store small amounts of state. It's required for both Nimbus and Supervisor daemons.

LANGUAGE: yaml
CODE:
storm.local.dir: "/mnt/storm"

----------------------------------------

TITLE: Implementing Streaming Word Count in Trident
DESCRIPTION: Demonstrates how to create a Trident topology that computes streaming word count from an input stream of sentences. It uses a FixedBatchSpout as the input source and persists the word counts in memory.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
     topology.newStream("spout1", spout)
       .each(new Fields("sentence"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))                
       .parallelismHint(6);

----------------------------------------

TITLE: Implementing Kafka Message Mapping Interfaces
DESCRIPTION: Interface definitions for mapping Storm tuples to Kafka key-value pairs and selecting Kafka topics

LANGUAGE: java
CODE:
K getKeyFromTuple(Tuple/TridentTuple tuple);
V getMessageFromTuple(Tuple/TridentTuple tuple);

LANGUAGE: java
CODE:
public interface KafkaTopicSelector {
    String getTopics(Tuple/TridentTuple tuple);
}

----------------------------------------

TITLE: Configuring HDFS Bolt in Java
DESCRIPTION: Example showing how to configure an HDFS bolt with delimited record format, sync policy, rotation policy and file naming format

LANGUAGE: java
CODE:
// use "|" instead of "," for field delimiter
RecordFormat format = new DelimitedRecordFormat()
        .withFieldDelimiter("|");

// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withPath("/foo/");

HdfsBolt bolt = new HdfsBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withRecordFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy);

----------------------------------------

TITLE: Retrieving Supervisor Summary in JSON
DESCRIPTION: Sample response from the /api/v1/supervisor/summary endpoint, showing information about Storm supervisors.

LANGUAGE: json
CODE:
{
  "supervisors": [
    {
      "id": "0b879808-2a26-442b-8f7d-23101e0c3696",
      "host": "10.11.1.7",
      "uptime": "5m 58s",
      "uptimeSeconds": 358,
      "slotsTotal": 4,
      "slotsUsed": 3,
      "totalMem": 3000,
      "totalCpu": 400,
      "usedMem": 1280,
      "usedCPU": 160
    }
  ],
  "schedulerDisplayResource": true
}

----------------------------------------

TITLE: Implementing Aggregators in Trident
DESCRIPTION: Examples of implementing different types of Aggregators in Trident: CombinerAggregator, ReducerAggregator, and Aggregator.

LANGUAGE: java
CODE:
public class Count implements CombinerAggregator<Long> {
    public Long init(TridentTuple tuple) {
        return 1L;
    }

    public Long combine(Long val1, Long val2) {
        return val1 + val2;
    }

    public Long zero() {
        return 0L;
    }
}

LANGUAGE: java
CODE:
public class Count implements ReducerAggregator<Long> {
    public Long init() {
        return 0L;
    }
    
    public Long reduce(Long curr, TridentTuple tuple) {
        return curr + 1;
    }
}

LANGUAGE: java
CODE:
public class CountAgg extends BaseAggregator<CountState> {
    static class CountState {
        long count = 0;
    }

    public CountState init(Object batchId, TridentCollector collector) {
        return new CountState();
    }

    public void aggregate(CountState state, TridentTuple tuple, TridentCollector collector) {
        state.count+=1;
    }

    public void complete(CountState state, TridentCollector collector) {
        collector.emit(new Values(state.count));
    }
}

----------------------------------------

TITLE: Configuring Resource Aware Scheduler in Storm YAML
DESCRIPTION: Basic configuration to enable the Resource Aware Scheduler in Storm's configuration file

LANGUAGE: yaml
CODE:
storm.scheduler: "org.apache.storm.scheduler.resource.ResourceAwareScheduler"

----------------------------------------

TITLE: Setting Up Tasks in Storm Worker (Clojure)
DESCRIPTION: The mk-task function sets up individual tasks within a worker. It establishes routing functions and initializes spout or bolt-specific code.

LANGUAGE: clojure
CODE:
(mk-task)

----------------------------------------

TITLE: Defining Storm Topology in Clojure
DESCRIPTION: Example topology definition showing how to wire spouts and bolts together using the Clojure DSL. Demonstrates spout-spec and bolt-spec usage with different stream groupings.

LANGUAGE: clojure
CODE:
(topology
 {"1" (spout-spec sentence-spout)
  "2" (spout-spec (sentence-spout-parameterized
                   ["the cat jumped over the door"
                    "greetings from a faraway land"])
                   :p 2)}
 {"3" (bolt-spec {"1" :shuffle "2" :shuffle}
                 split-sentence
                 :p 5)
  "4" (bolt-spec {"3" ["word"]}
                 word-count
                 :p 6)})

----------------------------------------

TITLE: Implementing Storm Exclamation Bolt
DESCRIPTION: Complete implementation of an ExclamationBolt that appends exclamation marks to input strings.

LANGUAGE: java
CODE:
public static class ExclamationBolt extends BaseRichBolt {
    OutputCollector _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple tuple) {
        _collector.emit(tuple, new Values(tuple.getString(0) + "!!!"));
        _collector.ack(tuple);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word"));
    }    
}

----------------------------------------

TITLE: Simple DRPC Topology Implementation
DESCRIPTION: Implementation of a basic DRPC topology using LinearDRPCTopologyBuilder that appends an exclamation mark to input strings.

LANGUAGE: java
CODE:
public static class ExclaimBolt extends BaseBasicBolt {
    public void execute(Tuple tuple, BasicOutputCollector collector) {
        String input = tuple.getString(1);
        collector.emit(new Values(tuple.getValue(0), input + "!"));
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "result"));
    }
}

public static void main(String[] args) throws Exception {
    LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("exclamation");
    builder.addBolt(new ExclaimBolt(), 3);
    // ...
}

----------------------------------------

TITLE: Multi-anchoring Tuples in Storm Java API
DESCRIPTION: This code snippet demonstrates how to anchor an output tuple to multiple input tuples in Storm. This is useful for streaming joins or aggregations where a failure in processing should cause multiple tuples to be replayed.

LANGUAGE: java
CODE:
List<Tuple> anchors = new ArrayList<Tuple>();
anchors.add(tuple1);
anchors.add(tuple2);
_collector.emit(anchors, new Values(1, 2, 3));

----------------------------------------

TITLE: Implementing Acker Execute Method in Storm (Java)
DESCRIPTION: The execute method of the Acker bolt processes new tuple trees, updates checksums, and manages tuple completion. It handles initialization, acking, failing, and notifying spouts of tuple tree status.

LANGUAGE: java
CODE:
// On a tick tuple, just advance pending tupletree checksums towards death and return.
// Otherwise, update or create the record for this tupletree:

// on init: initialize with the given checksum value, and record the spout's id for later.
// on ack:  xor the partial checksum into the existing checksum value
// on fail: just mark it as failed

// Next, put the record into the RotatingMap (thus resetting is countdown to expiry) and take action:

// if the total checksum is zero, the tupletree is complete: remove it from the pending collection and notify the spout of success
// if the tupletree has failed, it is also complete:   remove it from the pending collection and notify the spout of failure

// Finally, pass on an ack of our own.

----------------------------------------

TITLE: Task Setup and Routing in Clojure
DESCRIPTION: Configures task routing functions and initializes spout/bolt-specific code for topology execution.

LANGUAGE: clojure
CODE:
mk-task

----------------------------------------

TITLE: Running DRPC in Local Mode with Storm
DESCRIPTION: Demonstrates how to set up and use DRPC (Distributed Remote Procedure Call) in local mode with Storm. It creates a LocalDRPC object, submits a topology to a LocalCluster, and executes a DRPC call.

LANGUAGE: java
CODE:
try (LocalDRPC drpc = new LocalDRPC();
     LocalCluster cluster = new LocalCluster();
     LocalTopology topo = cluster.submitTopology("drpc-demo", conf, builder.createLocalTopology(drpc))) {

    System.out.println("Results for 'hello':" + drpc.execute("exclamation", "hello"));
}

----------------------------------------

TITLE: Local Mode Implementation Using Java Queues in Apache Storm
DESCRIPTION: Implementation of the message sending protocol using in-memory Java queues for local mode in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/local.clj

----------------------------------------

TITLE: Implementing a Sliding Window Bolt in Java
DESCRIPTION: Example implementation of a sliding window bolt that extends BaseWindowedBolt. It demonstrates how to process tuples within a window and emit results.

LANGUAGE: java
CODE:
public class SlidingWindowBolt extends BaseWindowedBolt {
	private OutputCollector collector;
	
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
    	this.collector = collector;
    }
	
    @Override
    public void execute(TupleWindow inputWindow) {
	  for(Tuple tuple: inputWindow.get()) {
	    // do the windowing computation
		...
	  }
	  // emit the results
	  collector.emit(new Values(computedValue));
    }
}

public static void main(String[] args) {
    TopologyBuilder builder = new TopologyBuilder();
     builder.setSpout("spout", new RandomSentenceSpout(), 1);
     builder.setBolt("slidingwindowbolt", 
                     new SlidingWindowBolt().withWindow(new Count(30), new Count(10)),
                     1).shuffleGrouping("spout");
    Config conf = new Config();
    conf.setDebug(true);
    conf.setNumWorkers(1);

    StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());
	
}

----------------------------------------

TITLE: Initializing FixedBatchSpout in Java for Trident
DESCRIPTION: Creates a FixedBatchSpout that cycles through a set of sentences to produce a stream of data for Trident processing.

LANGUAGE: java
CODE:
FixedBatchSpout spout = new FixedBatchSpout(new Fields("sentence"), 3,
               new Values("the cow jumped over the moon"),
               new Values("the man went to the store and bought some candy"),
               new Values("four score and seven years ago"),
               new Values("how many apples can you eat"));
spout.setCycle(true);

----------------------------------------

TITLE: Configuring Kryo Serialization Registration in Storm YAML
DESCRIPTION: Example YAML configuration showing how to register custom serializers in Storm using the topology.kryo.register property. Demonstrates both direct class registration and custom serializer implementation registration.

LANGUAGE: yaml
CODE:
topology.kryo.register:
  - com.mycompany.CustomType1
  - com.mycompany.CustomType2: com.mycompany.serializer.CustomType2Serializer
  - com.mycompany.CustomType3

----------------------------------------

TITLE: Configuring HDFS Bolt in Java
DESCRIPTION: Example of how to configure and use the HdfsBolt to write pipe-delimited files to HDFS with specific sync and rotation policies.

LANGUAGE: java
CODE:
// use "|" instead of "," for field delimiter
RecordFormat format = new DelimitedRecordFormat()
        .withFieldDelimiter("|");

// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withPath("/foo/");

HdfsBolt bolt = new HdfsBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withRecordFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy);

----------------------------------------

TITLE: Creating and Transforming Streams in Java
DESCRIPTION: This snippet demonstrates how to create a stream from a spout, transform it using map and flatMap operations, and perform an output operation.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

// a stream of sentences obtained from a source spout
Stream<String> sentences = builder.newStream(new RandomSentenceSpout()).map(tuple -> tuple.getString(0));

// a stream of words obtained by transforming (splitting) the stream of sentences
Stream<String> words = sentences.flatMap(s -> Arrays.asList(s.split(" ")));

// output operation that prints the words to console
words.forEach(w -> System.out.println(w));

----------------------------------------

TITLE: Configuring JdbcLookupBolt in Java
DESCRIPTION: Demonstrates how to configure a JdbcLookupBolt for executing select queries. It shows how to specify the connection provider, lookup mapper, and query timeout.

LANGUAGE: java
CODE:
String selectSql = "select user_name from user_details where user_id = ?";
SimpleJdbcLookupMapper lookupMapper = new SimpleJdbcLookupMapper(outputFields, queryParamColumns)
JdbcLookupBolt userNameLookupBolt = new JdbcLookupBolt(connectionProvider, selectSql, lookupMapper)
        .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Registering Custom Counter in Storm
DESCRIPTION: Example of registering and using a counter metric in a Storm topology context. Shows how metric names are constructed with additional topology information.

LANGUAGE: java
CODE:
Counter myCounter = topologyContext.registerCounter("myCounter");

----------------------------------------

TITLE: Configuring Zookeeper Servers in Storm YAML
DESCRIPTION: This YAML configuration specifies the list of Zookeeper servers for the Storm cluster. It's a mandatory configuration in the storm.yaml file.

LANGUAGE: yaml
CODE:
storm.zookeeper.servers:
  - "111.222.333.444"
  - "555.666.777.888"

----------------------------------------

TITLE: Basic Storm JAR Command Example
DESCRIPTION: Example showing how to submit a topology jar with external dependencies using artifact repositories.

LANGUAGE: bash
CODE:
./bin/storm jar example/storm-starter/storm-starter-topologies-*.jar org.apache.storm.starter.RollingTopWords blobstore-remote2 remote --jars "./external/storm-redis/storm-redis-1.1.0.jar,./external/storm-kafka-client/storm-kafka-client-1.1.0.jar" --artifacts "redis.clients:jedis:2.9.0,org.apache.kafka:kafka-clients:1.0.0^org.slf4j:slf4j-api" --artifactRepositories "jboss-repository^http://repository.jboss.com/maven2,HDPRepo^http://repo.hortonworks.com/content/groups/public/"

----------------------------------------

TITLE: Implementing Reach Calculation DRPC Topology in Trident
DESCRIPTION: Demonstrates a complex DRPC topology that computes the reach of a URL on Twitter. This topology queries two external databases and performs multiple processing steps to calculate reach.

LANGUAGE: java
CODE:
TridentState urlToTweeters =
       topology.newStaticState(getUrlToTweetersState());
TridentState tweetersToFollowers =
       topology.newStaticState(getTweeterToFollowersState());

topology.newDRPCStream("reach")
       .stateQuery(urlToTweeters, new Fields("args"), new MapGet(), new Fields("tweeters"))
       .each(new Fields("tweeters"), new ExpandList(), new Fields("tweeter"))
       .shuffle()
       .stateQuery(tweetersToFollowers, new Fields("tweeter"), new MapGet(), new Fields("followers"))
       .parallelismHint(200)
       .each(new Fields("followers"), new ExpandList(), new Fields("follower"))
       .groupBy(new Fields("follower"))
       .aggregate(new One(), new Fields("one"))
       .parallelismHint(20)
       .aggregate(new Count(), new Fields("reach"));

----------------------------------------

TITLE: Performing Windowing and Aggregation Operations in Java
DESCRIPTION: This snippet demonstrates how to apply windowing to a stream and perform aggregation operations like sum and count.

LANGUAGE: java
CODE:
// time based sliding window
stream.window(SlidingWindows.of(Duration.minutes(10), Duration.minutes(1)));

// count based sliding window
stream.window(SlidingWindows.of(Count.(10), Count.of(2)));

// tumbling window
stream.window(TumblingWindows.of(Duration.seconds(10));

// specifying timestamp field for event time based processing and a late tuple stream.
stream.window(TumblingWindows.of(Duration.seconds(10)
                     .withTimestampField("ts")
                     .withLateTupleStream("late_events"));

Stream<Long> numbers = ...;
// aggregate the numbers and produce a stream of last 10 sec sums.
Stream<Long> sums = numbers.window(TumblingWindows.of(Duration.seconds(10))).aggregate(new Sum());

// the last 10 sec sums computed using reduce
Stream<Long> sums = numbers.window(...).reduce((x, y) -> x + y);

----------------------------------------

TITLE: Defining HBaseMapper Interface
DESCRIPTION: Core interface for interacting with HBase from Storm, defining methods for row key generation and column mapping

LANGUAGE: java
CODE:
public interface HBaseMapper extends Serializable {
    byte[] rowKey(Tuple tuple);
    ColumnList columns(Tuple tuple);
}

----------------------------------------

TITLE: Implementing Skewed Streaming Top N Pattern in Java Storm Topology
DESCRIPTION: This snippet shows an improved streaming top N pattern for skewed data. It uses partialKeyGrouping for load distribution, an aggregation step, and then merges the results. This approach helps handle data skew more effectively.

LANGUAGE: java
CODE:
builder.setBolt("count", new CountObjects(), parallelism)
  .partialKeyGrouping("objects", new Fields("value"));
builder.setBolt("rank" new AggregateCountsAndRank(), parallelism)
  .fieldsGrouping("count", new Fields("key"))
builder.setBolt("merge", new MergeRanksObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Deploying a Flux Topology Locally
DESCRIPTION: Command to run a Flux topology in local mode using the storm jar command.

LANGUAGE: bash
CODE:
storm jar myTopology-0.1.0-SNAPSHOT.jar org.apache.storm.flux.Flux --local my_config.yaml

----------------------------------------

TITLE: Initializing RocketMqSpout in Java for Storm-RocketMQ Integration
DESCRIPTION: This snippet demonstrates how to initialize a RocketMqSpout for reading data from a RocketMQ topic in a Storm topology. It configures essential properties such as nameserver address, consumer group, and topic.

LANGUAGE: java
CODE:
Properties properties = new Properties();
properties.setProperty(SpoutConfig.NAME_SERVER_ADDR, nameserverAddr);
properties.setProperty(SpoutConfig.CONSUMER_GROUP, group);
properties.setProperty(SpoutConfig.CONSUMER_TOPIC, topic);

RocketMqSpout spout = new RocketMqSpout(properties);

----------------------------------------

TITLE: Complex DRPC Example: Twitter URL Reach Calculator
DESCRIPTION: Implementation of a complex DRPC topology that calculates the reach of URLs on Twitter using multiple processing stages.

LANGUAGE: java
CODE:
public class PartialUniquer extends BaseBatchBolt {
    BatchOutputCollector _collector;
    Object _id;
    Set<String> _followers = new HashSet<String>();
    
    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
        _collector = collector;
        _id = id;
    }

    @Override
    public void execute(Tuple tuple) {
        _followers.add(tuple.getString(1));
    }
    
    @Override
    public void finishBatch() {
        _collector.emit(new Values(_id, _followers.size()));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "partial-count"));
    }
}

----------------------------------------

TITLE: Emitting Tuples in a Bolt (Java)
DESCRIPTION: Shows how to emit tuples from a bolt using the OutputCollector, including proper acknowledgment of processed tuples.

LANGUAGE: java
CODE:
public void execute(Tuple input) {
    // Process the input tuple
    // ...

    // Emit new tuples based on the input
    collector.emit(input, new Values(processedValue1, processedValue2));

    // Acknowledge the input tuple
    collector.ack(input);
}

----------------------------------------

TITLE: Configuring and Submitting a Storm Topology with Windowing
DESCRIPTION: Example of configuring a Storm topology with a sliding window bolt, setting window parameters, and submitting the topology.

LANGUAGE: java
CODE:
public static void main(String[] args) {
    TopologyBuilder builder = new TopologyBuilder();
     builder.setSpout("spout", new RandomSentenceSpout(), 1);
     builder.setBolt("slidingwindowbolt", 
                     new SlidingWindowBolt().withWindow(new Count(30), new Count(10)),
                     1).shuffleGrouping("spout");
    Config conf = new Config();
    conf.setDebug(true);
    conf.setNumWorkers(1);

    StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());
	
}

----------------------------------------

TITLE: Configuring and Using KafkaBolt in a Storm Topology
DESCRIPTION: Example of how to configure and use KafkaBolt to write data from a Storm topology to Kafka. It sets up a spout, configures Kafka producer properties, and creates a KafkaBolt instance to forward data to Kafka.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();

Fields fields = new Fields("key", "message");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
            new Values("storm", "1"),
            new Values("trident", "1"),
            new Values("needs", "1"),
            new Values("javadoc", "1")
);
spout.setCycle(true);
builder.setSpout("spout", spout, 5);
//set producer properties.
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

KafkaBolt bolt = new KafkaBolt()
        .withProducerProperties(props)
        .withTopicSelector(new DefaultTopicSelector("test"))
        .withTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper());
builder.setBolt("forwardToKafka", bolt, 8).shuffleGrouping("spout");

Config conf = new Config();

StormSubmitter.submitTopology("kafkaboltTest", conf, builder.createTopology());

----------------------------------------

TITLE: Overriding Component Configuration in Java Storm API
DESCRIPTION: Demonstrates two methods for specifying component-specific configurations in Storm using Java API. The first method overrides the getComponentConfiguration method, while the second uses the TopologyBuilder's setSpout and setBolt methods.

LANGUAGE: java
CODE:
// Method 1: Internally override getComponentConfiguration
@Override
public Map<String, Object> getComponentConfiguration() {
    return componentSpecificConfig;
}

// Method 2: Externally use TopologyBuilder
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("spout", new MySpout())
       .addConfiguration("topology.debug", true);
builder.setBolt("bolt", new MyBolt())
       .addConfigurations(boltConfig);

----------------------------------------

TITLE: Storm Project Structure Example
DESCRIPTION: Standard directory structure for a Storm project, showing key paths for Java source files and multilang resources. Project should include src/jvm/ for Java sources and multilang/resources/ for non-Java implementations.

LANGUAGE: plaintext
CODE:
project/
  src/jvm/        # Java source files
  lib/           # Storm jar dependencies
  lib/dev/       # Development dependencies
  multilang/     # Non-Java implementations
    resources/

----------------------------------------

TITLE: Implementing Skewed Streaming Top N Pattern in Storm
DESCRIPTION: Configuration for handling skewed data in streaming top N calculations using partial key grouping for better load distribution.

LANGUAGE: java
CODE:
builder.setBolt("count", new CountObjects(), parallelism)
  .partialKeyGrouping("objects", new Fields("value"));
builder.setBolt("rank" new AggregateCountsAndRank(), parallelism)
  .fieldsGrouping("count", new Fields("key"))
builder.setBolt("merge", new MergeRanksObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Implementing Map and FlatMap Functions in Trident
DESCRIPTION: Demonstrates how to use map and flatMap operations for one-to-one and one-to-many transformations on streams.

LANGUAGE: java
CODE:
public class UpperCase extends MapFunction {
 @Override
 public Values execute(TridentTuple input) {
   return new Values(input.getString(0).toUpperCase());
 }
}

public class Split extends FlatMapFunction {
  @Override
  public Iterable<Values> execute(TridentTuple input) {
    List<Values> valuesList = new ArrayList<>();
    for (String word : input.getString(0).split(" ")) {
      valuesList.add(new Values(word));
    }
    return valuesList;
  }
}

----------------------------------------

TITLE: Implementing SlidingWindowBolt in Java
DESCRIPTION: Example implementation of a sliding window bolt that extends BaseWindowedBolt. It demonstrates how to process tuples within a window and emit results.

LANGUAGE: java
CODE:
public class SlidingWindowBolt extends BaseWindowedBolt {
	private OutputCollector collector;
	
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
    	this.collector = collector;
    }
	
    @Override
    public void execute(TupleWindow inputWindow) {
	  for(Tuple tuple: inputWindow.get()) {
	    // do the windowing computation
		...
	  }
	  // emit the results
	  collector.emit(new Values(computedValue));
    }
}

----------------------------------------

TITLE: Initializing Fixed Batch Spout in Trident
DESCRIPTION: Creates a fixed batch spout that cycles through a set of predefined sentences to produce an infinite stream. Uses Fields to define the schema and Values to provide the data.

LANGUAGE: java
CODE:
FixedBatchSpout spout = new FixedBatchSpout(new Fields("sentence"), 3,
               new Values("the cow jumped over the moon"),
               new Values("the man went to the store and bought some candy"),
               new Values("four score and seven years ago"),
               new Values("how many apples can you eat"));
spout.setCycle(true);

----------------------------------------

TITLE: Configuring and Submitting a Storm Topology with Windowing
DESCRIPTION: Example of configuring a Storm topology with a sliding window bolt, setting window parameters, and submitting the topology.

LANGUAGE: java
CODE:
public static void main(String[] args) {
    TopologyBuilder builder = new TopologyBuilder();
     builder.setSpout("spout", new RandomSentenceSpout(), 1);
     builder.setBolt("slidingwindowbolt", 
                     new SlidingWindowBolt().withWindow(new Count(30), new Count(10)),
                     1).shuffleGrouping("spout");
    Config conf = new Config();
    conf.setDebug(true);
    conf.setNumWorkers(1);

    StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());
	
}

----------------------------------------

TITLE: Configuring and Using KafkaBolt in Storm Topology
DESCRIPTION: Example of configuring a KafkaBolt with producer properties and adding it to a Storm topology to write data to Kafka.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();

Fields fields = new Fields("key", "message");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
            new Values("storm", "1"),
            new Values("trident", "1"),
            new Values("needs", "1"),
            new Values("javadoc", "1")
);
spout.setCycle(true);
builder.setSpout("spout", spout, 5);
//set producer properties.
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

KafkaBolt bolt = new KafkaBolt()
        .withProducerProperties(props)
        .withTopicSelector(new DefaultTopicSelector("test"))
        .withTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper());
builder.setBolt("forwardToKafka", bolt, 8).shuffleGrouping("spout");

Config conf = new Config();

StormSubmitter.submitTopology("kafkaboltTest", conf, builder.createTopology());

----------------------------------------

TITLE: Configuring Bolt Parallelism in Storm Topology (Java)
DESCRIPTION: This snippet demonstrates how to set the number of executors and tasks for a bolt in a Storm topology. It configures the 'GreenBolt' with 2 executors and 4 tasks, using shuffle grouping from 'blue-spout'.

LANGUAGE: java
CODE:
topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

----------------------------------------

TITLE: Converting Docker Images to Squashfs Format
DESCRIPTION: Example command for using docker-to-squash.py script to convert Docker images to squashfs format and store them in HDFS

LANGUAGE: bash
CODE:
python3 docker-to-squash.py pull-build-push-update --hdfs-root hdfs://hostname:port/containers \
                      docker.xxx.com:4443/hadoop-user-images/storm/rhel7:20201202-232133,storm/rhel7:dev_current --log DEBUG --bootstrap

----------------------------------------

TITLE: Initializing Transactional Topology in Storm
DESCRIPTION: Example of setting up a transactional topology using TransactionalTopologyBuilder with a memory-based spout and counting bolts.

LANGUAGE: java
CODE:
MemoryTransactionalSpout spout = new MemoryTransactionalSpout(DATA, new Fields("word"), PARTITION_TAKE_PER_BATCH);
TransactionalTopologyBuilder builder = new TransactionalTopologyBuilder("global-count", "spout", spout, 3);
builder.setBolt("partial-count", new BatchCount(), 5)
        .shuffleGrouping("spout");
builder.setBolt("sum", new UpdateGlobalCount())
        .globalGrouping("partial-count");

----------------------------------------

TITLE: Configuring JdbcTridentState for Lookups in Java
DESCRIPTION: Example of configuring JdbcTridentState for lookups in Trident topologies. It shows how to set up options including connection provider, lookup mapper, select query, and query timeout.

LANGUAGE: java
CODE:
JdbcState.Options options = new JdbcState.Options()
        .withConnectionProvider(connectionProvider)
        .withJdbcLookupMapper(new SimpleJdbcLookupMapper(new Fields("user_name"), Lists.newArrayList(new Column("user_id", Types.INTEGER))))
        .withSelectQuery("select user_name from user_details where user_id = ?");
        .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Registering User-Defined Counter in Java
DESCRIPTION: Example of registering a custom counter metric named 'myCounter' using the TopologyContext in a Storm topology.

LANGUAGE: java
CODE:
Counter myCounter = topologyContext.registerCounter("myCounter");

----------------------------------------

TITLE: Configuring Storm MetricStore with YAML
DESCRIPTION: Configuration options for Storm's metric store implementation, including class specifications, RocksDB settings, and retention policies. These settings control the metric storage behavior, database location, and caching parameters.

LANGUAGE: yaml
CODE:
storm.metricstore.class: "org.apache.storm.metricstore.rocksdb.RocksDbStore"
storm.metricprocessor.class: "org.apache.storm.metricstore.NimbusMetricProcessor"
storm.metricstore.rocksdb.location: "storm_rocks"
storm.metricstore.rocksdb.create_if_missing: true
storm.metricstore.rocksdb.metadata_string_cache_capacity: 4000
storm.metricstore.rocksdb.retention_hours: 240

----------------------------------------

TITLE: Configuring HDFS Bolt in Java
DESCRIPTION: Example of configuring an HDFS Bolt with pipe-delimited records, sync policy, rotation policy and file naming format

LANGUAGE: java
CODE:
// use "|" instead of "," for field delimiter
RecordFormat format = new DelimitedRecordFormat()
        .withFieldDelimiter("|");

// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withPath("/foo/");

HdfsBolt bolt = new HdfsBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withRecordFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy);

----------------------------------------

TITLE: Implementing RedisLookupMapper for Word Count
DESCRIPTION: This Java class implements RedisLookupMapper for a word count use case. It defines the Redis data type as HASH, specifies how to extract keys from tuples, and declares output fields.

LANGUAGE: java
CODE:
class WordCountRedisLookupMapper implements RedisLookupMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountRedisLookupMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public List<Values> toTuple(ITuple input, Object value) {
        String member = getKeyFromTuple(input);
        List<Values> values = Lists.newArrayList();
        values.add(new Values(member, value));
        return values;
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("wordName", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Configuring HDFS Spout in Java
DESCRIPTION: Example of how to configure and use the HDFS Spout to read text files from HDFS and feed data into a Storm topology.

LANGUAGE: java
CODE:
// Instantiate spout to read text files
HdfsSpout textReaderSpout = new HdfsSpout().setReaderType("text")
                                           .withOutputFields(TextFileReader.defaultFields)                                      
                                           .setHdfsUri("hdfs://localhost:54310")  // required
                                           .setSourceDir("/data/in")              // required                                      
                                           .setArchiveDir("/data/done")           // required
                                           .setBadFilesDir("/data/badfiles");     // required                                      
// If using Kerberos
HashMap hdfsSettings = new HashMap();
hdfsSettings.put("hdfs.keytab.file", "/path/to/keytab");
hdfsSettings.put("hdfs.kerberos.principal","user@EXAMPLE.com");

textReaderSpout.setHdfsClientSettings(hdfsSettings);

// Create topology
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("hdfsspout", textReaderSpout, SPOUT_NUM);

// Setup bolts and wire up topology
     ..snip..

// Submit topology with config
Config conf = new Config();
StormSubmitter.submitTopologyWithProgressBar("topologyName", conf, builder.createTopology());

----------------------------------------

TITLE: Configuring JDBC Insert Bolt Example
DESCRIPTION: Example showing how to configure and initialize a JDBC Insert Bolt with HikariCP connection provider and SimpleJdbcMapper.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);

String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withTableName("user")
                                    .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Defining JdbcMapper Interface in Java
DESCRIPTION: Main API for inserting data into a table using JDBC. It defines how a storm tuple maps to a list of columns representing a row in a database.

LANGUAGE: java
CODE:
public interface JdbcMapper  extends Serializable {
    List<Column> getColumns(ITuple tuple);
}

----------------------------------------

TITLE: Configuring MongoDB Insert Bolt
DESCRIPTION: Example of configuring a bolt for inserting data into MongoDB

LANGUAGE: java
CODE:
String url = "mongodb://127.0.0.1:27017/test";
String collectionName = "wordcount";

MongoMapper mapper = new SimpleMongoMapper()
        .withFields("word", "count");

MongoInsertBolt insertBolt = new MongoInsertBolt(url, collectionName, mapper);

----------------------------------------

TITLE: Implementing Custom State Interface for User Location Database
DESCRIPTION: Example of implementing a custom State interface for a user location database, including methods for setting and getting locations.

LANGUAGE: java
CODE:
public class LocationDB implements State {
    public void beginCommit(Long txid) {    
    }
    
    public void commit(Long txid) {    
    }
    
    public void setLocation(long userId, String location) {
      // code to access database and set location
    }
    
    public String getLocation(long userId) {
      // code to get location from database
    }
}

----------------------------------------

TITLE: Creating IConfigLoader Factory Method
DESCRIPTION: Factory method for creating IConfigLoader instances based on the scheme of the scheduler config loader URI.

LANGUAGE: java
CODE:
ConfigLoaderFactoryService.createConfigLoader(Map<String, Object> conf)

----------------------------------------

TITLE: Defining JdbcLookupMapper Interface in Java
DESCRIPTION: Interface for executing select queries against a database using JDBC. It includes methods for declaring output fields, getting columns, and converting query results to tuples.

LANGUAGE: java
CODE:
    void declareOutputFields(OutputFieldsDeclarer declarer);
    List<Column> getColumns(ITuple tuple);
    List<Values> toTuple(ITuple input, List<Column> columns);

----------------------------------------

TITLE: Configuring UI/Logviewer Authentication Filter
DESCRIPTION: YAML configuration for setting up authentication filters for Storm UI and Logviewer components

LANGUAGE: yaml
CODE:
ui.filter: "filter.class"
ui.filter.params: "param1":"value1"
logviewer.filter: "filter.class"
logviewer.filter.params: "param1":"value1"

----------------------------------------

TITLE: Configuring Storm Bolt Parallelism in Java
DESCRIPTION: Example showing how to configure a Storm bolt with specific executor and task settings. This code sets up a GreenBolt with 2 executors and 4 tasks, using shuffle grouping from a blue-spout stream.

LANGUAGE: java
CODE:
topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

----------------------------------------

TITLE: Creating a Storm Topology with Kinesis Spout in Java
DESCRIPTION: This code snippet demonstrates how to create a sample Storm topology that uses the Kinesis Spout. It configures the spout with various parameters and sets up the topology structure.

LANGUAGE: java
CODE:
public class KinesisSpoutTopology {
    public static void main (String args[]) throws InvalidTopologyException, AuthorizationException, AlreadyAliveException {
        String topologyName = args[0];
        RecordToTupleMapper recordToTupleMapper = new TestRecordToTupleMapper();
        KinesisConnectionInfo kinesisConnectionInfo = new KinesisConnectionInfo(new CredentialsProviderChain(), new ClientConfiguration(), Regions.US_WEST_2,
                1000);
        ZkInfo zkInfo = new ZkInfo("localhost:2181", "/kinesisOffsets", 20000, 15000, 10000L, 3, 2000);
        KinesisConfig kinesisConfig = new KinesisConfig(args[1], ShardIteratorType.TRIM_HORIZON,
                recordToTupleMapper, new Date(), new ExponentialBackoffRetrier(), zkInfo, kinesisConnectionInfo, 10000L);
        KinesisSpout kinesisSpout = new KinesisSpout(kinesisConfig);
        TopologyBuilder topologyBuilder = new TopologyBuilder();
        topologyBuilder.setSpout("spout", kinesisSpout, 3);
        topologyBuilder.setBolt("bolt", new KinesisBoltTest(), 1).shuffleGrouping("spout");
        Config topologyConfig = new Config();
        topologyConfig.setDebug(true);
        topologyConfig.setNumWorkers(3);
        StormSubmitter.submitTopology(topologyName, topologyConfig, topologyBuilder.createTopology());
    }
}

----------------------------------------

TITLE: Implementing RedisLookupMapper in Java
DESCRIPTION: Demonstrates how to create a custom RedisLookupMapper for use with RedisLookupBolt. This example maps word counts stored in a Redis hash.

LANGUAGE: java
CODE:
class WordCountRedisLookupMapper implements RedisLookupMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountRedisLookupMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public List<Values> toTuple(ITuple input, Object value) {
        String member = getKeyFromTuple(input);
        List<Values> values = Lists.newArrayList();
        values.add(new Values(member, value));
        return values;
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("wordName", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Implementing Stateful Word Count Bolt in Java
DESCRIPTION: This code snippet demonstrates how to implement a stateful word count bolt using Storm's KeyValueState abstraction. The bolt extends BaseStatefulBolt, initializes with previous state, and updates word counts in the execute method.

LANGUAGE: java
CODE:
public class WordCountBolt extends BaseStatefulBolt<KeyValueState<String, Long>> {
    private KeyValueState<String, Long> wordCounts;
    private OutputCollector collector;
    ...
        @Override
        public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
          this.collector = collector;
        }
        @Override
        public void initState(KeyValueState<String, Long> state) {
          wordCounts = state;
        }
        @Override
        public void execute(Tuple tuple) {
          String word = tuple.getString(0);
          Integer count = wordCounts.get(word, 0);
          count++;
          wordCounts.put(word, count);
          collector.emit(tuple, new Values(word, count));
          collector.ack(tuple);
        }
    ...
    }

----------------------------------------

TITLE: Implementing a Persistent Stateful Windowed Bolt in Storm
DESCRIPTION: Example of a persistent stateful windowed bolt that uses window checkpointing to save its state. It demonstrates state initialization, window execution, and state updates.

LANGUAGE: java
CODE:
public class MyStatefulPersistentWindowedBolt extends BaseStatefulWindowedBolt<K, V> {
  private KeyValueState<K, V> state;
  
  @Override
  public void initState(KeyValueState<K, V> state) {
    this.state = state;
   // ...
   // restore the state from the last saved state.
   // ...
  }
  
  @Override
  public void execute(TupleWindow window) {      
    // iterate over tuples in the current window
    Iterator<Tuple> it = window.getIter();
    while (it.hasNext()) {
        // compute some result based on the tuples in window
    }
    
    // possibly update any state to be maintained across windows
    state.put(STATE_KEY, updatedValue);
    
    // emit the results downstream
    collector.emit(new Values(result));
  }
}

----------------------------------------

TITLE: Configuring MongoInsertBolt for Storm-MongoDB Integration
DESCRIPTION: Demonstrates how to configure and use the MongoInsertBolt for inserting data into MongoDB. It requires specifying the MongoDB URL, collection name, and a MongoMapper implementation.

LANGUAGE: java
CODE:
String url = "mongodb://127.0.0.1:27017/test";
String collectionName = "wordcount";

MongoMapper mapper = new SimpleMongoMapper()
        .withFields("word", "count");

MongoInsertBolt insertBolt = new MongoInsertBolt(url, collectionName, mapper);

----------------------------------------

TITLE: Creating JdbcLookupBolt for Database Queries in Java
DESCRIPTION: This code snippet demonstrates how to create a JdbcLookupBolt for executing select queries in a Storm topology. It shows the configuration of the connection provider, select query, lookup mapper, and query timeout.

LANGUAGE: java
CODE:
String selectSql = "select user_name from user_details where user_id = ?";
SimpleJdbcLookupMapper lookupMapper = new SimpleJdbcLookupMapper(outputFields, queryParamColumns)
JdbcLookupBolt userNameLookupBolt = new JdbcLookupBolt(connectionProvider, selectSql, lookupMapper)
        .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Configuring Load-Aware Messaging in Storm
DESCRIPTION: Enables or disables load-aware messaging for shuffle grouping, potentially impacting performance.

LANGUAGE: java
CODE:
topology.disable.loadaware.messaging

----------------------------------------

TITLE: Creating JdbcLookupBolt for Database Queries in Java
DESCRIPTION: This code snippet demonstrates how to create a JdbcLookupBolt for executing select queries in a Storm topology. It shows the configuration of the connection provider, select query, lookup mapper, and query timeout.

LANGUAGE: java
CODE:
String selectSql = "select user_name from user_details where user_id = ?";
SimpleJdbcLookupMapper lookupMapper = new SimpleJdbcLookupMapper(outputFields, queryParamColumns)
JdbcLookupBolt userNameLookupBolt = new JdbcLookupBolt(connectionProvider, selectSql, lookupMapper)
        .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Basic Stream Operations in Java Storm
DESCRIPTION: Example showing how to create and transform streams using StreamBuilder, including mapping sentences to words and printing output.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

// a stream of sentences obtained from a source spout
Stream<String> sentences = builder.newStream(new RandomSentenceSpout()).map(tuple -> tuple.getString(0));

// a stream of words obtained by transforming (splitting) the stream of sentences
Stream<String> words = sentences.flatMap(s -> Arrays.asList(s.split(" ")));

// output operation that prints the words to console
words.forEach(w -> System.out.println(w));

----------------------------------------

TITLE: Uploading Topology Jar in StormSubmitter (Java)
DESCRIPTION: StormSubmitter uploads the topology jar to Nimbus using its Thrift interface. It uses beginFileUpload to get a path, uploads in 15KB chunks with uploadChunk, and calls finishFileUpload when done.

LANGUAGE: java
CODE:
StormSubmitter.submitTopology

----------------------------------------

TITLE: Implementing JDBC Mapper Interface for Database Operations
DESCRIPTION: Interface definition for mapping Storm tuples to database columns, used for insert operations.

LANGUAGE: java
CODE:
public interface JdbcMapper  extends Serializable {
    List<Column> getColumns(ITuple tuple);
}

----------------------------------------

TITLE: Creating JdbcLookupBolt for Database Queries in Java
DESCRIPTION: This code snippet demonstrates how to create a JdbcLookupBolt for executing select queries in a Storm topology. It shows the configuration of the connection provider, select query, lookup mapper, and query timeout.

LANGUAGE: java
CODE:
String selectSql = "select user_name from user_details where user_id = ?";
SimpleJdbcLookupMapper lookupMapper = new SimpleJdbcLookupMapper(outputFields, queryParamColumns)
JdbcLookupBolt userNameLookupBolt = new JdbcLookupBolt(connectionProvider, selectSql, lookupMapper)
        .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Defining IWindowedBolt Interface in Java
DESCRIPTION: Interface definition for bolts that require windowing support in Storm. It includes methods for preparation, execution of windowed tuples, and cleanup.

LANGUAGE: java
CODE:
public interface IWindowedBolt extends IComponent {
    void prepare(Map stormConf, TopologyContext context, OutputCollector collector);
    /**
     * Process tuples falling within the window and optionally emit 
     * new tuples based on the tuples in the input window.
     */
    void execute(TupleWindow inputWindow);
    void cleanup();
}

----------------------------------------

TITLE: Defining JdbcLookupMapper Interface in Java
DESCRIPTION: Interface for executing select queries against a database using JDBC. It includes methods for declaring output fields, getting columns, and converting query results to tuples.

LANGUAGE: java
CODE:
    void declareOutputFields(OutputFieldsDeclarer declarer);
    List<Column> getColumns(ITuple tuple);
    List<Values> toTuple(ITuple input, List<Column> columns);

----------------------------------------

TITLE: Implementing Stateful Persistent Windowed Bolt in Java
DESCRIPTION: Example of a stateful persistent windowed bolt that uses window checkpointing to save its state. It demonstrates state initialization and tuple processing within a window.

LANGUAGE: java
CODE:
public class MyStatefulPersistentWindowedBolt extends BaseStatefulWindowedBolt<K, V> {
  private KeyValueState<K, V> state;
  
  @Override
  public void initState(KeyValueState<K, V> state) {
    this.state = state;
   // ...
   // restore the state from the last saved state.
   // ...
  }
  
  @Override
  public void execute(TupleWindow window) {      
    // iterate over tuples in the current window
    Iterator<Tuple> it = window.getIter();
    while (it.hasNext()) {
        // compute some result based on the tuples in window
    }
    
    // possibly update any state to be maintained across windows
    state.put(STATE_KEY, updatedValue);
    
    // emit the results downstream
    collector.emit(new Values(result));
  }
}

----------------------------------------

TITLE: Implementing SimpleMongoMapper for Storm-MongoDB Integration
DESCRIPTION: Implements the SimpleMongoMapper class, which converts Storm tuples to MongoDB documents. It assumes tuple fields have the same names as document fields in the database collection.

LANGUAGE: java
CODE:
public class SimpleMongoMapper implements MongoMapper {
    private String[] fields;

    @Override
    public Document toDocument(ITuple tuple) {
        Document document = new Document();
        for(String field : fields){
            document.append(field, tuple.getValueByField(field));
        }
        return document;
    }

    @Override
    public Document toDocumentByKeys(List<Object> keys) {
        Document document = new Document();
        document.append("_id", MongoUtils.getID(keys));
        return document;
    }

    public SimpleMongoMapper withFields(String... fields) {
        this.fields = fields;
        return this;
    }
}

----------------------------------------

TITLE: Implementing Tuple Counting Bolt with Metrics
DESCRIPTION: Implementation of a bolt that counts incoming tuples using Storm's metrics system. Shows how to register and increment a counter in a bolt's lifecycle methods.

LANGUAGE: java
CODE:
public class TupleCountingBolt extends BaseRichBolt {
    private Counter tupleCounter;
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        this.tupleCounter = context.registerCounter("tupleCount");
    }

    @Override
    public void execute(Tuple input) {
        this.tupleCounter.inc();
    }
}

----------------------------------------

TITLE: Configuring YAML Front Matter for Storm Documentation
DESCRIPTION: YAML front matter defining the title, layout, and documentation status for the Storm performance tuning page.

LANGUAGE: yaml
CODE:
---
title: Performance Tuning
layout: documentation
documentation: true
---

----------------------------------------

TITLE: Defining and Querying Kafka Streams with Storm SQL
DESCRIPTION: Example showing how to create external tables mapped to Kafka streams and perform filtering and projection operations. The query calculates large orders by multiplying unit price and quantity, filtering for totals over 50.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://...' ...

CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) 'kafka://...' ...

INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Debugging Storm Topology in IDE
DESCRIPTION: Shows how to override local mode execution for IDE debugging using LocalCluster.withLocalModeOverride. Allows running topology with debugging enabled.

LANGUAGE: java
CODE:
public static void main(final String args[]) {
    LocalCluster.withLocalModeOverride(() -> originalMain(args), 10);
}

----------------------------------------

TITLE: Initializing HiveBolt with DelimitedRecordHiveMapper (Java)
DESCRIPTION: Java code snippet demonstrating how to set up a HiveBolt using DelimitedRecordHiveMapper. It maps tuple fields to table columns and configures HiveOptions for the bolt.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames));
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper);
HiveBolt hiveBolt = new HiveBolt(hiveOptions);

----------------------------------------

TITLE: Sliding Window Bolt Implementation
DESCRIPTION: Example implementation of a sliding window bolt showing the basic structure and window configuration. Includes topology builder setup and window definition.

LANGUAGE: java
CODE:
public class SlidingWindowBolt extends BaseWindowedBolt {
	private OutputCollector collector;
	
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
    	this.collector = collector;
    }
	
    @Override
    public void execute(TupleWindow inputWindow) {
	  for(Tuple tuple: inputWindow.get()) {
	    // do the windowing computation
		...
	  }
	  // emit the results
	  collector.emit(new Values(computedValue));
    }
}

public static void main(String[] args) {
    TopologyBuilder builder = new TopologyBuilder();
     builder.setSpout("spout", new RandomSentenceSpout(), 1);
     builder.setBolt("slidingwindowbolt", 
                     new SlidingWindowBolt().withWindow(new Count(30), new Count(10)),
                     1).shuffleGrouping("spout");
    Config conf = new Config();
    conf.setDebug(true);
    conf.setNumWorkers(1);

    StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());
	
}

----------------------------------------

TITLE: Complex DRPC Example - URL Reach Calculator
DESCRIPTION: Implementation of a more complex DRPC topology that calculates the reach of URLs on Twitter using parallel processing.

LANGUAGE: java
CODE:
public class PartialUniquer extends BaseBatchBolt {
    BatchOutputCollector _collector;
    Object _id;
    Set<String> _followers = new HashSet<String>();
    
    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
        _collector = collector;
        _id = id;
    }

    @Override
    public void execute(Tuple tuple) {
        _followers.add(tuple.getString(1));
    }
    
    @Override
    public void finishBatch() {
        _collector.emit(new Values(_id, _followers.size()));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "partial-count"));
    }
}

----------------------------------------

TITLE: Implementing Tuple Counter Bolt in Storm
DESCRIPTION: A complete example of a bolt that counts incoming tuples using the metrics API. The bolt registers a counter in the prepare method and increments it for each received tuple.

LANGUAGE: java
CODE:
public class TupleCountingBolt extends BaseRichBolt {
    private Counter tupleCounter;
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        this.tupleCounter = context.registerCounter("tupleCount");
    }

    @Override
    public void execute(Tuple input) {
        this.tupleCounter.inc();
    }
}

----------------------------------------

TITLE: Window Configuration Methods in Storm
DESCRIPTION: Available window configuration methods supporting various combinations of count and time-based windows with different sliding intervals.

LANGUAGE: java
CODE:
withWindow(Count windowLength, Count slidingInterval)
withWindow(Count windowLength)
withWindow(Count windowLength, Duration slidingInterval)
withWindow(Duration windowLength, Duration slidingInterval)
withWindow(Duration windowLength)
withWindow(Duration windowLength, Count slidingInterval)
withTumblingWindow(BaseWindowedBolt.Count count)
withTumblingWindow(BaseWindowedBolt.Duration duration)

----------------------------------------

TITLE: Implementing Tuple Counting Bolt with Metrics
DESCRIPTION: Implementation of a bolt that counts incoming tuples using Storm's metrics system. Shows how to register and increment a counter in a bolt's lifecycle methods.

LANGUAGE: java
CODE:
public class TupleCountingBolt extends BaseRichBolt {
    private Counter tupleCounter;
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        this.tupleCounter = context.registerCounter("tupleCount");
    }

    @Override
    public void execute(Tuple input) {
        this.tupleCounter.inc();
    }
}

----------------------------------------

TITLE: Configuring HDFS Bolt in Java
DESCRIPTION: Example of how to configure and set up an HDFS Bolt to write pipe-delimited files to HDFS with specific sync and rotation policies.

LANGUAGE: java
CODE:
// use "|" instead of "," for field delimiter
RecordFormat format = new DelimitedRecordFormat()
        .withFieldDelimiter("|");

// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withPath("/foo/");

HdfsBolt bolt = new HdfsBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withRecordFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy);

----------------------------------------

TITLE: IConfigLoader Interface Definition
DESCRIPTION: Core interface definition for config loaders that defines a single load() method which returns a configuration map.

LANGUAGE: java
CODE:
public interface IConfigLoader {
    Map<?,?> load();
};

----------------------------------------

TITLE: Initializing FixedBatchSpout in Java for Trident
DESCRIPTION: Creates a FixedBatchSpout that cycles through a set of predefined sentences to produce an infinite stream of data for testing purposes.

LANGUAGE: java
CODE:
FixedBatchSpout spout = new FixedBatchSpout(new Fields("sentence"), 3,
               new Values("the cow jumped over the moon"),
               new Values("the man went to the store and bought some candy"),
               new Values("four score and seven years ago"),
               new Values("how many apples can you eat"));
spout.setCycle(true);

----------------------------------------

TITLE: Implementing Basic Insert Query with CassandraWriterBolt in Java
DESCRIPTION: Demonstrates how to create a basic insert query using CassandraWriterBolt with specified tuple fields. Uses simpleQuery to construct the CQL statement with field mapping.

LANGUAGE: java
CODE:
    new CassandraWriterBolt(
        async(
            simpleQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);")                .with(
                    fields("title", "year", "performer", "genre", "tracks")
                 )
            )
    );

----------------------------------------

TITLE: SQL Join Example for Storm Streams
DESCRIPTION: Example SQL query showing a join between 4 tables that will be recreated using Storm's JoinBolt.

LANGUAGE: sql
CODE:
select  userId, key4, key2, key3
from        table1
inner join  table2  on table2.userId =  table1.key1
inner join  table3  on table3.key3   =  table2.userId
left join   table4  on table4.key4   =  table3.key3

----------------------------------------

TITLE: Implementing Skewed Streaming Top N Pattern with Partial Key Grouping
DESCRIPTION: Enhanced top N pattern using partial key grouping to handle skewed data distribution across multiple bolts.

LANGUAGE: java
CODE:
builder.setBolt("count", new CountObjects(), parallelism)
  .partialKeyGrouping("objects", new Fields("value"));
builder.setBolt("rank" new AggregateCountsAndRank(), parallelism)
  .fieldsGrouping("count", new Fields("key"))
builder.setBolt("merge", new MergeRanksObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Configuring ExpandUrl Bolt with Fields Grouping in Storm
DESCRIPTION: This snippet shows how to set up an ExpandUrl bolt with fields grouping in a Storm topology. This approach improves caching efficiency by ensuring the same URL always goes to the same task.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .fieldsGrouping("urls", new Fields("url"));

----------------------------------------

TITLE: Registering Custom Serializers in Storm Topology Configuration (YAML)
DESCRIPTION: This YAML snippet demonstrates how to register custom serializers for specific classes in a Storm topology configuration. It shows both default FieldsSerializer usage and custom serializer implementation.

LANGUAGE: yaml
CODE:
topology.kryo.register:
  - com.mycompany.CustomType1
  - com.mycompany.CustomType2: com.mycompany.serializer.CustomType2Serializer
  - com.mycompany.CustomType3

----------------------------------------

TITLE: Configuring Metric Reporters in Apache Storm YAML
DESCRIPTION: Example configuration for setting up two metric reporters (Graphite and Console) in Apache Storm. Demonstrates how to specify reporter class, reporting period, and optional filters.

LANGUAGE: yaml
CODE:
topology.metrics.reporters:
  # Graphite Reporter
  - class: "org.apache.storm.metrics2.reporters.GraphiteStormReporter"
    report.period: 60
    report.period.units: "SECONDS"
    graphite.host: "localhost"
    graphite.port: 2003

  # Console Reporter
  - class: "org.apache.storm.metrics2.reporters.ConsoleStormReporter"
    report.period: 10
    report.period.units: "SECONDS"
    filter:
        class: "org.apache.storm.metrics2.filters.RegexFilter"
        expression: ".*my_component.*emitted.*"

----------------------------------------

TITLE: Word Count Topology Using Stream API
DESCRIPTION: Illustrates a complete word count topology implementation using the Stream API, including windowing, transformations, and aggregations.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

builder
   // A stream of random sentences with two partitions
   .newStream(new RandomSentenceSpout(), new ValueMapper<String>(0), 2)
   // a two seconds tumbling window
   .window(TumblingWindows.of(Duration.seconds(2)))
   // split the sentences to words
   .flatMap(s -> Arrays.asList(s.split(" ")))
   // create a stream of (word, 1) pairs
   .mapToPair(w -> Pair.of(w, 1))
   // compute the word counts in the last two second window
   .countByKey()
   // print the results to stdout
   .print();

----------------------------------------

TITLE: Configuring Storm Topology with Spouts and Bolts
DESCRIPTION: Example showing how to set up a basic Storm topology that reads sentences from Kestrel queue, splits them into words, and counts word occurrences using spouts and bolts.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("sentences", new KestrelSpout("kestrel.backtype.com",
                                               22133,
                                               "sentence_queue",
                                               new StringScheme()));
builder.setBolt("split", new SplitSentence(), 10)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 20)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Defining a Transactional Topology in Apache Storm
DESCRIPTION: Example code showing how to define a transactional topology using TransactionalTopologyBuilder for computing a global count of tuples.

LANGUAGE: java
CODE:
MemoryTransactionalSpout spout = new MemoryTransactionalSpout(DATA, new Fields("word"), PARTITION_TAKE_PER_BATCH);
TransactionalTopologyBuilder builder = new TransactionalTopologyBuilder("global-count", "spout", spout, 3);
builder.setBolt("partial-count", new BatchCount(), 5)
        .shuffleGrouping("spout");
builder.setBolt("sum", new UpdateGlobalCount())
        .globalGrouping("partial-count");

----------------------------------------

TITLE: Implementing Custom Metrics in a Bolt
DESCRIPTION: Example showing how to implement and register a custom CountMetric in a Storm bolt to track execution count. Includes metric declaration, registration and usage.

LANGUAGE: java
CODE:
private transient CountMetric countMetric;

LANGUAGE: java
CODE:
@Override
public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
	// other initialization here.
	countMetric = new CountMetric();
	context.registerMetric("execute_count", countMetric, 60);
}

LANGUAGE: java
CODE:
public void execute(Tuple input) {
	countMetric.incr();
	// handle tuple here.	
}

----------------------------------------

TITLE: Initializing Transactional Topology in Storm
DESCRIPTION: Example showing how to build a transactional topology using TransactionalTopologyBuilder with a memory spout and two bolts for counting.

LANGUAGE: java
CODE:
MemoryTransactionalSpout spout = new MemoryTransactionalSpout(DATA, new Fields("word"), PARTITION_TAKE_PER_BATCH);
TransactionalTopologyBuilder builder = new TransactionalTopologyBuilder("global-count", "spout", spout, 3);
builder.setBolt("partial-count", new BatchCount(), 5)
        .shuffleGrouping("spout");
builder.setBolt("sum", new UpdateGlobalCount())
        .globalGrouping("partial-count");

----------------------------------------

TITLE: Sliding Window Bolt Implementation - Java
DESCRIPTION: Example implementation of a sliding window bolt that processes tuples within defined windows and emits computed results.

LANGUAGE: java
CODE:
public class SlidingWindowBolt extends BaseWindowedBolt {
	private OutputCollector collector;
	
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
    	this.collector = collector;
    }
	
    @Override
    public void execute(TupleWindow inputWindow) {
	  for(Tuple tuple: inputWindow.get()) {
	    // do the windowing computation
		...
	  }
	  // emit the results
	  collector.emit(new Values(computedValue));
    }
}

public static void main(String[] args) {
    TopologyBuilder builder = new TopologyBuilder();
     builder.setSpout("spout", new RandomSentenceSpout(), 1);
     builder.setBolt("slidingwindowbolt", 
                     new SlidingWindowBolt().withWindow(new Count(30), new Count(10)),
                     1).shuffleGrouping("spout");
    Config conf = new Config();
    conf.setDebug(true);
    conf.setNumWorkers(1);

    StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());
	
}

----------------------------------------

TITLE: Redis Cluster State Provider Configuration
DESCRIPTION: JSON configuration for Redis Cluster state management setup with node specifications and connection parameters.

LANGUAGE: json
CODE:
{
   "keyClass": "Optional fully qualified class name of the Key type.",
   "valueClass": "Optional fully qualified class name of the Value type.",
   "keySerializerClass": "Optional Key serializer implementation class.",
   "valueSerializerClass": "Optional Value Serializer implementation class.",
   "jedisClusterConfig": {
     "nodes": ["localhost:7379", "localhost:7380", "localhost:7381"],
     "timeout": 2000,
     "maxRedirections": 5
   }
 }

----------------------------------------

TITLE: Diagnosing Nimbus JVM Shutdown in Java
DESCRIPTION: This log output shows the Nimbus JVM shutting down immediately after startup, likely due to an incompatibility between the RocksDB version and the CPU architecture. The solution involves downgrading RocksDB or recompiling it with specific flags.

LANGUAGE: plaintext
CODE:
2024-01-05 18:54:20.404 [o.a.s.v.ConfigValidation] INFO: Will use [class org.apache.storm.DaemonConfig, class org.apache.storm.Config] for validation
2024-01-05 18:54:20.556 [o.a.s.z.AclEnforcement] INFO: SECURITY IS DISABLED NO FURTHER CHECKS...
2024-01-05 18:54:20.740 [o.a.s.m.r.RocksDbStore] INFO: Opening RocksDB from <your-storm-folder>/storm_rocks, storm.metricstore.rocksdb.create_if_missing=true

LANGUAGE: plaintext
CODE:
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  EXCEPTION_ILLEGAL_INSTRUCTION (0xc000001d) at pc=0x00007ff94dc7a56d, pid=12728, tid=0x0000000000001d94
#
# JRE version: OpenJDK Runtime Environment (8.0_232) (build 1.8.0_232-09)
# Java VM: OpenJDK 64-Bit Server VM (25.232-b09 mixed mode windows-amd64 compressed oops)
# Problematic frame:
# C  [librocksdbjni4887247215762585789.dll+0x53a56d]

----------------------------------------

TITLE: Implementing a Function in Trident
DESCRIPTION: Example of a Trident Function that emits multiple tuples based on the input value.

LANGUAGE: java
CODE:
public class MyFunction extends BaseFunction {
    public void execute(TridentTuple tuple, TridentCollector collector) {
        for(int i=0; i < tuple.getInteger(0); i++) {
            collector.emit(new Values(i));
        }
    }
}

----------------------------------------

TITLE: Creating a Storm Topology with Kinesis Spout in Java
DESCRIPTION: This code sample demonstrates how to create a Storm topology that uses the KinesisSpout to consume data from Amazon Kinesis Streams. It includes configuration of various components such as RecordToTupleMapper, KinesisConnectionInfo, ZkInfo, and KinesisConfig.

LANGUAGE: java
CODE:
public class KinesisSpoutTopology {
    public static void main (String args[]) throws InvalidTopologyException, AuthorizationException, AlreadyAliveException {
        String topologyName = args[0];
        RecordToTupleMapper recordToTupleMapper = new TestRecordToTupleMapper();
        KinesisConnectionInfo kinesisConnectionInfo = new KinesisConnectionInfo(new CredentialsProviderChain(), new ClientConfiguration(), Regions.US_WEST_2,
                1000);
        ZkInfo zkInfo = new ZkInfo("localhost:2181", "/kinesisOffsets", 20000, 15000, 10000L, 3, 2000);
        KinesisConfig kinesisConfig = new KinesisConfig(args[1], ShardIteratorType.TRIM_HORIZON,
                recordToTupleMapper, new Date(), new ExponentialBackoffRetrier(), zkInfo, kinesisConnectionInfo, 10000L);
        KinesisSpout kinesisSpout = new KinesisSpout(kinesisConfig);
        TopologyBuilder topologyBuilder = new TopologyBuilder();
        topologyBuilder.setSpout("spout", kinesisSpout, 3);
        topologyBuilder.setBolt("bolt", new KinesisBoltTest(), 1).shuffleGrouping("spout");
        Config topologyConfig = new Config();
        topologyConfig.setDebug(true);
        topologyConfig.setNumWorkers(3);
        StormSubmitter.submitTopology(topologyName, topologyConfig, topologyBuilder.createTopology());
    }
}

----------------------------------------

TITLE: Initializing DRPC Client in Java
DESCRIPTION: This snippet demonstrates how to configure and create a DRPC client to execute a remote function call in Apache Storm.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.put("storm.thrift.transport", "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin");
conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 3);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL, 10);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING, 20);
DRPCClient client = new DRPCClient(conf, "drpc-host", 3772);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Complex DRPC Reach Calculation Implementation
DESCRIPTION: Implementation of a complex DRPC topology for calculating Twitter URL reach using multiple processing stages.

LANGUAGE: java
CODE:
public class PartialUniquer extends BaseBatchBolt {
    BatchOutputCollector _collector;
    Object _id;
    Set<String> _followers = new HashSet<String>();
    
    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
        _collector = collector;
        _id = id;
    }

    @Override
    public void execute(Tuple tuple) {
        _followers.add(tuple.getString(1));
    }
    
    @Override
    public void finishBatch() {
        _collector.emit(new Values(_id, _followers.size()));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "partial-count"));
    }
}

----------------------------------------

TITLE: SQL Join Example for Storm Streams
DESCRIPTION: Example SQL query showing a join between 4 tables that will be recreated using Storm's JoinBolt.

LANGUAGE: sql
CODE:
select  userId, key4, key2, key3
from        table1
inner join  table2  on table2.userId =  table1.key1
inner join  table3  on table3.key3   =  table2.userId
left join   table4  on table4.key4   =  table3.key3

----------------------------------------

TITLE: Implementing Custom State Interface
DESCRIPTION: Example of implementing a custom State interface for a hypothetical LocationDB to store user locations.

LANGUAGE: java
CODE:
public class LocationDB implements State {
    public void beginCommit(Long txid) {    
    }
    
    public void commit(Long txid) {    
    }
    
    public void setLocation(long userId, String location) {
      // code to access database and set location
    }
    
    public String getLocation(long userId) {
      // code to get location from database
    }
}

----------------------------------------

TITLE: Defining JdbcMapper Interface in Java
DESCRIPTION: Core interface for mapping Storm tuples to database columns. The getColumns method defines how tuple fields map to database columns in insert operations.

LANGUAGE: java
CODE:
public interface JdbcMapper  extends Serializable {
    List<Column> getColumns(ITuple tuple);
}

----------------------------------------

TITLE: Python Storm Bolt Implementation
DESCRIPTION: Example of implementing a Storm bolt in Python that splits sentences into words.

LANGUAGE: python
CODE:
import storm

class SplitSentenceBolt(storm.BasicBolt):
    def process(self, tup):
        words = tup.values[0].split(" ")
        for word in words:
          storm.emit([word])

SplitSentenceBolt().run()

----------------------------------------

TITLE: Initializing EsLookupBolt for Storm-Elasticsearch Integration in Java
DESCRIPTION: This code demonstrates how to create an EsLookupBolt for performing get requests to Elasticsearch from Storm. It requires EsConfig, ElasticsearchGetRequest for request creation, and EsLookupResultOutput for handling responses.

LANGUAGE: java
CODE:
EsConfig esConfig = createEsConfig();
ElasticsearchGetRequest getRequestAdapter = createElasticsearchGetRequest();
EsLookupResultOutput output = createOutput();
EsLookupBolt lookupBolt = new EsLookupBolt(esConfig, getRequestAdapter, output);

----------------------------------------

TITLE: Implementing Simple DRPC Topology with ExclaimBolt
DESCRIPTION: Shows implementation of a basic DRPC topology that appends an exclamation mark to input strings using LinearDRPCTopologyBuilder.

LANGUAGE: java
CODE:
public static class ExclaimBolt extends BaseBasicBolt {
    public void execute(Tuple tuple, BasicOutputCollector collector) {
        String input = tuple.getString(1);
        collector.emit(new Values(tuple.getValue(0), input + "!"));
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "result"));
    }
}

public static void main(String[] args) throws Exception {
    LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("exclamation");
    builder.addBolt(new ExclaimBolt(), 3);
    // ...
}

----------------------------------------

TITLE: Implementing Custom Redis Bolt in Java
DESCRIPTION: Demonstrates how to create a custom Redis bolt by extending AbstractRedisBolt. This example looks up word counts from Redis and emits the results.

LANGUAGE: java
CODE:
public static class LookupWordTotalCountBolt extends AbstractRedisBolt {
    private static final Logger LOG = LoggerFactory.getLogger(LookupWordTotalCountBolt.class);
    private static final Random RANDOM = new Random();

    public LookupWordTotalCountBolt(JedisPoolConfig config) {
        super(config);
    }

    public LookupWordTotalCountBolt(JedisClusterConfig config) {
        super(config);
    }

    @Override
    public void execute(Tuple input) {
        JedisCommands jedisCommands = null;
        try {
            jedisCommands = getInstance();
            String wordName = input.getStringByField("word");
            String countStr = jedisCommands.get(wordName);
            if (countStr != null) {
                int count = Integer.parseInt(countStr);
                this.collector.emit(new Values(wordName, count));

                // print lookup result with low probability
                if(RANDOM.nextInt(1000) > 995) {
                    LOG.info("Lookup result - word : " + wordName + " / count : " + count);
                }
            } else {
                // skip
                LOG.warn("Word not found in Redis - word : " + wordName);
            }
        } finally {
            if (jedisCommands != null) {
                returnInstance(jedisCommands);
            }
            this.collector.ack(input);
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        // wordName, count
        declarer.declare(new Fields("wordName", "count"));
    }
}

----------------------------------------

TITLE: Configuring Remote Storm Cluster Connection in YAML
DESCRIPTION: YAML configuration for connecting to a remote Storm cluster by specifying the Nimbus seed nodes in the storm.yaml file. This configuration should be placed in the ~/.storm/storm.yaml file to enable remote cluster communication.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["123.45.678.890"]

----------------------------------------

TITLE: Stream Builder Creation and Topology Submission
DESCRIPTION: Shows how to create streams using StreamBuilder and submit the topology to Storm.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();
Stream<Tuple> sentences = builder.newStream(new TestSentenceSpout());

StormSubmitter.submitTopologyWithProgressBar("test", new Config(), streamBuilder.build());

----------------------------------------

TITLE: Implementing a Map Function in Trident
DESCRIPTION: Example of a custom MapFunction that converts input strings to uppercase.

LANGUAGE: java
CODE:
public class UpperCase extends MapFunction {
 @Override
 public Values execute(TridentTuple input) {
   return new Values(input.getString(0).toUpperCase());
 }
}

----------------------------------------

TITLE: Registering Event Logger in Java Configuration
DESCRIPTION: Example of registering a FileBasedEventLogger implementation in Storm topology configuration using Java.

LANGUAGE: java
CODE:
conf.registerEventLogger(org.apache.storm.metric.FileBasedEventLogger.class);

----------------------------------------

TITLE: Maven Assembly Plugin Configuration for Storm JAR
DESCRIPTION: Maven configuration to package a Storm topology and its dependencies into a single JAR file. Configures the assembly plugin to create a jar-with-dependencies and specify the main class.

LANGUAGE: xml
CODE:
  <plugin>
    <artifactId>maven-assembly-plugin</artifactId>
    <configuration>
      <descriptorRefs>  
        <descriptorRef>jar-with-dependencies</descriptorRef>
      </descriptorRefs>
      <archive>
        <manifest>
          <mainClass>com.path.to.main.Class</mainClass>
        </manifest>
      </archive>
    </configuration>
  </plugin>

----------------------------------------

TITLE: Sliding Window Bolt Implementation Example
DESCRIPTION: Example implementation of a sliding window bolt with topology setup. Shows how to extend BaseWindowedBolt and configure window parameters.

LANGUAGE: java
CODE:
public class SlidingWindowBolt extends BaseWindowedBolt {
	private OutputCollector collector;
	
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
    	this.collector = collector;
    }
	
    @Override
    public void execute(TupleWindow inputWindow) {
	  for(Tuple tuple: inputWindow.get()) {
	    // do the windowing computation
		...
	  }
	  // emit the results
	  collector.emit(new Values(computedValue));
    }
}

public static void main(String[] args) {
    TopologyBuilder builder = new TopologyBuilder();
     builder.setSpout("spout", new RandomSentenceSpout(), 1);
     builder.setBolt("slidingwindowbolt", 
                     new SlidingWindowBolt().withWindow(new Count(30), new Count(10)),
                     1).shuffleGrouping("spout");
    Config conf = new Config();
    conf.setDebug(true);
    conf.setNumWorkers(1);

    StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());
	
}

----------------------------------------

TITLE: Defining IRichBolt Interface in Java
DESCRIPTION: The Java interface for defining rich bolts in Storm. This interface extends IBolt and adds the declareOutputFields method for specifying output streams.

LANGUAGE: Java
CODE:
public interface IRichBolt extends IBolt {
    void declareOutputFields(OutputFieldsDeclarer declarer);
    Map<String, Object> getComponentConfiguration();
}

----------------------------------------

TITLE: Redis State Provider Configuration
DESCRIPTION: JSON configuration for Redis-based state management, including key/value class specifications and Redis connection settings.

LANGUAGE: json
CODE:
{
  "keyClass": "Optional fully qualified class name of the Key type.",
  "valueClass": "Optional fully qualified class name of the Value type.",
  "keySerializerClass": "Optional Key serializer implementation class.",
  "valueSerializerClass": "Optional Value Serializer implementation class.",
  "jedisPoolConfig": {
    "host": "localhost",
    "port": 6379,
    "timeout": 2000,
    "database": 0,
    "password": "xyz"
    }
}

----------------------------------------

TITLE: Defining a Storm Topology in YAML
DESCRIPTION: Example YAML configuration defining a simple wordcount topology with spouts, bolts, and streams.

LANGUAGE: yaml
CODE:
name: "yaml-topology"
config:
  topology.workers: 1

# spout definitions
spouts:
  - id: "spout-1"
    className: "org.apache.storm.testing.TestWordSpout"
    parallelism: 1

# bolt definitions
bolts:
  - id: "bolt-1"
    className: "org.apache.storm.testing.TestWordCounter"
    parallelism: 1
  - id: "bolt-2"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1

#stream definitions
streams:
  - name: "spout-1 --> bolt-1" # name isn't used (placeholder for logging, UI, etc.)
    from: "spout-1"
    to: "bolt-1"
    grouping:
      type: FIELDS
      args: ["word"]

  - name: "bolt-1 --> bolt2"
    from: "bolt-1"
    to: "bolt-2"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Initializing EsIndexBolt for Storm-Elasticsearch Integration in Java
DESCRIPTION: This snippet demonstrates how to create an EsIndexBolt to stream tuples directly into Elasticsearch. It requires an EsConfig object for cluster configuration and an EsTupleMapper to extract necessary fields from input tuples.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsIndexBolt indexBolt = new EsIndexBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Persistent Word Count Implementation
DESCRIPTION: Complete example topology demonstrating persistent word counting using Storm and HBase.

LANGUAGE: java
CODE:
public class PersistentWordCount {
    private static final String WORD_SPOUT = "WORD_SPOUT";
    private static final String COUNT_BOLT = "COUNT_BOLT";
    private static final String HBASE_BOLT = "HBASE_BOLT";


    public static void main(String[] args) throws Exception {
        Config config = new Config();

        Map<String, Object> hbConf = new HashMap<String, Object>();
        if(args.length > 0){
            hbConf.put("hbase.rootdir", args[0]);
        }
        config.put("hbase.conf", hbConf);

        WordSpout spout = new WordSpout();
        WordCounter bolt = new WordCounter();

        SimpleHBaseMapper mapper = new SimpleHBaseMapper()
                .withRowKeyField("word")
                .withColumnFields(new Fields("word"))
                .withCounterFields(new Fields("count"))
                .withColumnFamily("cf");

        HBaseBolt hbase = new HBaseBolt("WordCount", mapper)
                .withConfigKey("hbase.conf");


        // wordSpout ==> countBolt ==> HBaseBolt
        TopologyBuilder builder = new TopologyBuilder();

        builder.setSpout(WORD_SPOUT, spout, 1);
        builder.setBolt(COUNT_BOLT, bolt, 1).shuffleGrouping(WORD_SPOUT);
        builder.setBolt(HBASE_BOLT, hbase, 1).fieldsGrouping(COUNT_BOLT, new Fields("word"));

        String topoName = "test";
        if (args.length > 1) {
            topoName = args[1];
        }
        if (args.length == 4) {
            System.out.println("hdfs url: " + args[0] + ", keytab file: " + args[2] + 
                ", principal name: " + args[3] + ", toplogy name: " + topoName);
            hbConf.put(HBaseSecurityUtil.STORM_KEYTAB_FILE_KEY, args[2]);
            hbConf.put(HBaseSecurityUtil.STORM_USER_NAME_KEY, args[3]);
        } else if (args.length == 3 || args.length > 4) {
            System.out.println("Usage: PersistentWordCount <hbase.rootdir> [topology name] [keytab file] [principal name]");
            return;
        }
        config.setNumWorkers(3);
        StormSubmitter.submitTopology(topoName, config, builder.createTopology());
    }

----------------------------------------

TITLE: Configuring JdbcInsertBolt in Java
DESCRIPTION: Demonstrates how to configure a JdbcInsertBolt using a ConnectionProvider and JdbcMapper. It shows options for specifying table name or insert query, and setting query timeout.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);

String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withTableName("user")
                                    .withQueryTimeoutSecs(30);
                                    Or
JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withInsertQuery("insert into user values (?,?)")
                                    .withQueryTimeoutSecs(30);                                    

----------------------------------------

TITLE: Configuring Wait Strategies in Storm
DESCRIPTION: Sets wait strategies for spouts, bolts, and backpressure situations to manage CPU usage and latency trade-offs.

LANGUAGE: java
CODE:
topology.spout.wait.strategy
topology.bolt.wait.strategy
topology.backpressure.wait.strategy

----------------------------------------

TITLE: Using partitionPersist in Trident Topology
DESCRIPTION: Example of using partitionPersist operation to update a custom state (LocationDB) in a Trident topology.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
TridentState locations = 
    topology.newStream("locations", locationsSpout)
        .partitionPersist(new LocationDBFactory(), new Fields("userid", "location"), new LocationUpdater())

----------------------------------------

TITLE: Adding GPU Resource to Storm Spout
DESCRIPTION: Example showing how to add a GPU resource requirement to a Storm spout using the addResource API method.

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.addResouce("gpu.count", 1.0);

----------------------------------------

TITLE: Defining ConnectionProvider Interface for JDBC in Java
DESCRIPTION: This code snippet defines the ConnectionProvider interface, which should be implemented by different connection pooling mechanisms. It includes methods for preparing, getting a connection, and cleaning up.

LANGUAGE: java
CODE:
public interface ConnectionProvider extends Serializable {
    /**
     * method must be idempotent.
     */
    void prepare();

    /**
     *
     * @return a DB connection over which the queries can be executed.
     */
    Connection getConnection();

    /**
     * called once when the system is shutting down, should be idempotent.
     */
    void cleanup();
}

----------------------------------------

TITLE: Configuring HDFS Bolt in Java
DESCRIPTION: Example showing how to configure an HDFS bolt with pipe-delimited files, sync policy, rotation policy and file naming format.

LANGUAGE: java
CODE:
// use "|" instead of "," for field delimiter
RecordFormat format = new DelimitedRecordFormat()
        .withFieldDelimiter("|");

// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withPath("/foo/");

HdfsBolt bolt = new HdfsBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withRecordFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy);

----------------------------------------

TITLE: Configuring HDFS Spout in Java
DESCRIPTION: Example of configuring an HDFS Spout to read text files from HDFS with required settings

LANGUAGE: java
CODE:
// Instantiate spout to read text files
HdfsSpout textReaderSpout = new HdfsSpout().setReaderType("text")
                                           .withOutputFields(TextFileReader.defaultFields)                                      
                                           .setHdfsUri("hdfs://localhost:54310")  // required
                                           .setSourceDir("/data/in")              // required                                      
                                           .setArchiveDir("/data/done")           // required
                                           .setBadFilesDir("/data/badfiles");     // required                                      
// If using Kerberos
HashMap hdfsSettings = new HashMap();
hdfsSettings.put("hdfs.keytab.file", "/path/to/keytab");
hdfsSettings.put("hdfs.kerberos.principal","user@EXAMPLE.com");

textReaderSpout.setHdfsClientSettings(hdfsSettings);

----------------------------------------

TITLE: Creating a Storm Topology with Kinesis Spout in Java
DESCRIPTION: This code snippet demonstrates how to create a sample Storm topology that uses the Kinesis Spout. It configures the spout with various parameters and sets up the topology builder.

LANGUAGE: java
CODE:
public class KinesisSpoutTopology {
    public static void main (String args[]) throws InvalidTopologyException, AuthorizationException, AlreadyAliveException {
        String topologyName = args[0];
        RecordToTupleMapper recordToTupleMapper = new TestRecordToTupleMapper();
        KinesisConnectionInfo kinesisConnectionInfo = new KinesisConnectionInfo(new CredentialsProviderChain(), new ClientConfiguration(), Regions.US_WEST_2,
                1000);
        ZkInfo zkInfo = new ZkInfo("localhost:2181", "/kinesisOffsets", 20000, 15000, 10000L, 3, 2000);
        KinesisConfig kinesisConfig = new KinesisConfig(args[1], ShardIteratorType.TRIM_HORIZON,
                recordToTupleMapper, new Date(), new ExponentialBackoffRetrier(), zkInfo, kinesisConnectionInfo, 10000L);
        KinesisSpout kinesisSpout = new KinesisSpout(kinesisConfig);
        TopologyBuilder topologyBuilder = new TopologyBuilder();
        topologyBuilder.setSpout("spout", kinesisSpout, 3);
        topologyBuilder.setBolt("bolt", new KinesisBoltTest(), 1).shuffleGrouping("spout");
        Config topologyConfig = new Config();
        topologyConfig.setDebug(true);
        topologyConfig.setNumWorkers(3);
        StormSubmitter.submitTopology(topologyName, topologyConfig, topologyBuilder.createTopology());
    }
}

----------------------------------------

TITLE: Implementing Trident State API for Cassandra Queries
DESCRIPTION: Demonstrates how to use Trident API to query and insert data into Cassandra using state management.

LANGUAGE: java
CODE:
CassandraState.Options options = new CassandraState.Options(new CassandraContext());
CQLStatementTupleMapper insertTemperatureValues = boundQuery("SELECT name FROM weather.station WHERE id = ?")\n         .bind(with(field("weather_station_id").as("id")));
options.withCQLStatementTupleMapper(insertTemperatureValues);
options.withCQLResultSetValuesMapper(new TridentResultSetValuesMapper(new Fields("name")));
CassandraStateFactory selectWeatherStationStateFactory =  new CassandraStateFactory(options);
CassandraStateFactory selectWeatherStationStateFactory = getSelectWeatherStationStateFactory();
TridentState selectState = topology.newStaticState(selectWeatherStationStateFactory);
stream = stream.stateQuery(selectState, new Fields("weather_station_id"), new CassandraQuery(), new Fields("name"));

----------------------------------------

TITLE: Configuring Zookeeper Servers in Storm YAML
DESCRIPTION: Configuration for specifying Zookeeper server hosts in the storm.yaml configuration file. Lists the IP addresses of Zookeeper cluster nodes.

LANGUAGE: yaml
CODE:
storm.zookeeper.servers:
  - "111.222.333.444"
  - "555.666.777.888"

----------------------------------------

TITLE: Storm Local Mode Command Line Example
DESCRIPTION: Demonstrates how to launch a Storm topology in local mode with debug configuration using command line interface.

LANGUAGE: bash
CODE:
storm local topology.jar <MY_MAIN_CLASS> -c topology.debug=true

----------------------------------------

TITLE: Event Hubs Configuration Properties
DESCRIPTION: Configuration properties required for connecting Storm spout to Azure Event Hubs, including authentication, namespace settings and checkpoint intervals

LANGUAGE: properties
CODE:
eventhubspout.username = [username: policy name in EventHubs Portal]
eventhubspout.password = [password: shared access key in EventHubs Portal]
eventhubspout.namespace = [namespace]
eventhubspout.entitypath = [entitypath]
eventhubspout.partitions.count = [partitioncount]

# if not provided, will use storm's zookeeper settings
# zookeeper.connectionstring=zookeeper0:2181,zookeeper1:2181,zookeeper2:2181

eventhubspout.checkpoint.interval = 10
eventhub.receiver.credits = 1024

----------------------------------------

TITLE: Configuring HDFS Spout in Java
DESCRIPTION: Example of how to configure and use the HDFS Spout to read text files from HDFS, including Kerberos authentication settings.

LANGUAGE: java
CODE:
// Instantiate spout to read text files
HdfsSpout textReaderSpout = new HdfsSpout().setReaderType("text")
                                           .withOutputFields(TextFileReader.defaultFields)                                      
                                           .setHdfsUri("hdfs://localhost:54310")  // required
                                           .setSourceDir("/data/in")              // required                                      
                                           .setArchiveDir("/data/done")           // required
                                           .setBadFilesDir("/data/badfiles");     // required                                      
// If using Kerberos
HashMap hdfsSettings = new HashMap();
hdfsSettings.put("hdfs.keytab.file", "/path/to/keytab");
hdfsSettings.put("hdfs.kerberos.principal","user@EXAMPLE.com");

textReaderSpout.setHdfsClientSettings(hdfsSettings);

// Create topology
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("hdfsspout", textReaderSpout, SPOUT_NUM);

// Setup bolts and wire up topology
     ..snip..

// Submit topology with config
Config conf = new Config();
StormSubmitter.submitTopologyWithProgressBar("topologyName", conf, builder.createTopology());

----------------------------------------

TITLE: Using Preconfigured DRPC Client in Java
DESCRIPTION: This code shows how to use a preconfigured DRPC client, which selects a host randomly from the configured set and attempts to find a working host if the selected one is down.

LANGUAGE: java
CODE:
DRPCClient client = DRPCClient.getConfiguredClient(conf);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Configuring Redis State Provider in Storm YAML
DESCRIPTION: JSON configuration for the Redis state provider, specifying connection details and optional serialization classes. This configuration is set in the storm.yaml file or programmatically.

LANGUAGE: json
CODE:
{
  "keyClass": "Optional fully qualified class name of the Key type.",
  "valueClass": "Optional fully qualified class name of the Value type.",
  "keySerializerClass": "Optional Key serializer implementation class.",
  "valueSerializerClass": "Optional Value Serializer implementation class.",
  "jedisPoolConfig": {
    "host": "localhost",
    "port": 6379,
    "timeout": 2000,
    "database": 0,
    "password": "xyz"
    }
}

----------------------------------------

TITLE: Implementing QueryFunction for State Querying
DESCRIPTION: Example of implementing a QueryFunction (QueryLocation) to query a custom State object (LocationDB) for user locations.

LANGUAGE: Java
CODE:
public class QueryLocation extends BaseQueryFunction<LocationDB, String> {
    public List<String> batchRetrieve(LocationDB state, List<TridentTuple> inputs) {
        List<String> ret = new ArrayList();
        for(TridentTuple input: inputs) {
            ret.add(state.getLocation(input.getLong(0)));
        }
        return ret;
    }

    public void execute(TridentTuple tuple, String location, TridentCollector collector) {
        collector.emit(new Values(location));
    }    
}

----------------------------------------

TITLE: Configuring Redis Cluster State Provider in JSON
DESCRIPTION: This JSON configuration snippet demonstrates how to set up a Redis Cluster state provider for Storm. It includes options for key and value classes, serializers, and Redis Cluster connection details.

LANGUAGE: json
CODE:
{
  "keyClass": "Optional fully qualified class name of the Key type.",
  "valueClass": "Optional fully qualified class name of the Value type.",
  "keySerializerClass": "Optional Key serializer implementation class.",
  "valueSerializerClass": "Optional Value Serializer implementation class.",
  "jedisClusterConfig": {
    "nodes": ["localhost:7379", "localhost:7380", "localhost:7381"],
    "timeout": 2000,
    "maxRedirections": 5
  }
}

----------------------------------------

TITLE: Implementing Split Function for Trident in Java
DESCRIPTION: Defines a Split function that takes a sentence and emits individual words. This function is used in the word count topology to split input sentences into words for processing.

LANGUAGE: java
CODE:
public class Split extends BaseFunction {
   public void execute(TridentTuple tuple, TridentCollector collector) {
       String sentence = tuple.getString(0);
       for(String word: sentence.split(" ")) {
           collector.emit(new Values(word));                
       }
   }
}

----------------------------------------

TITLE: Configuring Artifactory Loader in YAML
DESCRIPTION: Example YAML configuration for using the ArtifactoryConfigLoader, specifying the URI and timeout.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "artifactory+http://artifactory.my.company.com:8000/artifactory/configurations/clusters/my_cluster/ras_pools"
scheduler.config.loader.timeout.sec: 30

----------------------------------------

TITLE: Configuring RedisState for Trident Topology
DESCRIPTION: Java code snippet demonstrating how to configure RedisState for use in a Trident topology, including state updates and queries.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
                                .setHost(redisHost).setPort(redisPort)
                                .build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisLookupMapper lookupMapper = new WordCountLookupMapper();
RedisState.Factory factory = new RedisState.Factory(poolConfig);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory,
                        fields,
                        new RedisStateUpdater(storeMapper).withExpire(86400000),
                        new Fields());

TridentState state = topology.newStaticState(factory);
stream = stream.stateQuery(state, new Fields("word"),
                        new RedisStateQuerier(lookupMapper),
                        new Fields("columnName","columnValue"));

----------------------------------------

TITLE: Configuring RedisClusterState for Trident Topology
DESCRIPTION: Java code snippet demonstrating the configuration of RedisClusterState for use in a Trident topology with Redis Cluster, including state updates and queries.

LANGUAGE: java
CODE:
Set<InetSocketAddress> nodes = new HashSet<InetSocketAddress>();
for (String hostPort : redisHostPort.split(",")) {
    String[] host_port = hostPort.split(":");
    nodes.add(new InetSocketAddress(host_port[0], Integer.valueOf(host_port[1])));
}
JedisClusterConfig clusterConfig = new JedisClusterConfig.Builder().setNodes(nodes)
                                .build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisLookupMapper lookupMapper = new WordCountLookupMapper();
RedisClusterState.Factory factory = new RedisClusterState.Factory(clusterConfig);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory,
                        fields,
                        new RedisClusterStateUpdater(storeMapper).withExpire(86400000),
                        new Fields());

TridentState state = topology.newStaticState(factory);
stream = stream.stateQuery(state, new Fields("word"),
                        new RedisClusterStateQuerier(lookupMapper),
                        new Fields("columnName","columnValue"));

----------------------------------------

TITLE: Setting up Trident Topology with RedisClusterState in Java
DESCRIPTION: Java code showing how to set up a Trident topology using RedisClusterState for Redis cluster integration.

LANGUAGE: java
CODE:
Set<InetSocketAddress> nodes = new HashSet<InetSocketAddress>();
for (String hostPort : redisHostPort.split(",")) {
    String[] host_port = hostPort.split(":");
    nodes.add(new InetSocketAddress(host_port[0], Integer.valueOf(host_port[1])));
}
JedisClusterConfig clusterConfig = new JedisClusterConfig.Builder().setNodes(nodes)
                                .build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisLookupMapper lookupMapper = new WordCountLookupMapper();
RedisClusterState.Factory factory = new RedisClusterState.Factory(clusterConfig);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory,
                        fields,
                        new RedisClusterStateUpdater(storeMapper).withExpire(86400000),
                        new Fields());

TridentState state = topology.newStaticState(factory);
stream = stream.stateQuery(state, new Fields("word"),
                        new RedisClusterStateQuerier(lookupMapper),
                        new Fields("columnName","columnValue"));

----------------------------------------

TITLE: Submitting DRPC Topology to Storm Cluster in Java
DESCRIPTION: This code snippet shows how to submit a DRPC topology to a Storm cluster using StormSubmitter.

LANGUAGE: java
CODE:
StormSubmitter.submitTopology("exclamation-drpc", conf, builder.createRemoteTopology());

----------------------------------------

TITLE: Implementing IWindowedBolt Interface in Java
DESCRIPTION: Core interface that must be implemented by bolts requiring windowing support. Includes methods for preparing the bolt, executing window operations, and cleanup.

LANGUAGE: java
CODE:
public interface IWindowedBolt extends IComponent {
    void prepare(Map stormConf, TopologyContext context, OutputCollector collector);
    /**
     * Process tuples falling within the window and optionally emit 
     * new tuples based on the tuples in the input window.
     */
    void execute(TupleWindow inputWindow);
    void cleanup();
}

----------------------------------------

TITLE: Configuring HDFS Bolt in Java
DESCRIPTION: Example of how to configure an HDFS Bolt to write pipe-delimited files to HDFS, with sync and rotation policies.

LANGUAGE: java
CODE:
// use "|" instead of "," for field delimiter
RecordFormat format = new DelimitedRecordFormat()
        .withFieldDelimiter("|");

// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withPath("/foo/");

HdfsBolt bolt = new HdfsBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withRecordFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy);

----------------------------------------

TITLE: Configuring Event Loggers in YAML for Apache Storm
DESCRIPTION: This YAML configuration snippet shows how to register multiple event loggers for Apache Storm, including a custom event logger with additional arguments.

LANGUAGE: yaml
CODE:
topology.event.logger.register:
  - class: "org.apache.storm.metric.FileBasedEventLogger"
  - class: "org.mycompany.MyEventLogger"
    arguments:
      endpoint: "event-logger.mycompany.org"

----------------------------------------

TITLE: Implementing Custom State Interface in Trident
DESCRIPTION: Example of implementing a custom State interface for a location database, including methods for beginning and committing transactions.

LANGUAGE: java
CODE:
public class LocationDB implements State {
    public void beginCommit(Long txid) {    
    }
    
    public void commit(Long txid) {    
    }
    
    public void setLocation(long userId, String location) {
      // code to access database and set location
    }
    
    public String getLocation(long userId) {
      // code to get location from database
    }
}

----------------------------------------

TITLE: Creating Kerberos Principals and Keytabs
DESCRIPTION: Bash commands to create Kerberos principals and generate keytabs for Zookeeper, Nimbus, DRPC and UI components

LANGUAGE: bash
CODE:
# Zookeeper
sudo kadmin.local -q 'addprinc zookeeper/zk1.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/zk.keytab  zookeeper/zk1.example.com@STORM.EXAMPLE.COM"
# Nimbus and DRPC
sudo kadmin.local -q 'addprinc storm/storm.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm/storm.example.com@STORM.EXAMPLE.COM"
# All UI logviewer and Supervisors 
sudo kadmin.local -q 'addprinc storm@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm@STORM.EXAMPLE.COM"

----------------------------------------

TITLE: Configuring SimpleJdbcMapper for Database Operations in Java
DESCRIPTION: This code snippet shows how to configure a SimpleJdbcMapper for transforming Storm tuples to database rows. It demonstrates setting up the connection provider and specifying the table name or column schema.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);
String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

// Alternatively, with explicit column schema:
List<Column> columnSchema = Lists.newArrayList(
    new Column("user_id", java.sql.Types.INTEGER),
    new Column("user_name", java.sql.Types.VARCHAR));
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(columnSchema);

----------------------------------------

TITLE: Using Preconfigured DRPC Client in Java
DESCRIPTION: This code shows how to use a preconfigured DRPC client to execute a request, where the host is selected randomly from the configured set of hosts.

LANGUAGE: java
CODE:
DRPCClient client = DRPCClient.getConfiguredClient(conf);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Initializing EsIndexBolt for Storm-Elasticsearch Integration
DESCRIPTION: Creates an EsIndexBolt instance to stream tuples into Elasticsearch using specified cluster configuration and tuple mapping.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsIndexBolt indexBolt = new EsIndexBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Creating and Transforming Streams in Java
DESCRIPTION: Demonstrates how to create a StreamBuilder, create streams from spouts, and perform basic transformations like map and flatMap.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

// a stream of sentences obtained from a source spout
Stream<String> sentences = builder.newStream(new RandomSentenceSpout()).map(tuple -> tuple.getString(0));

// a stream of words obtained by transforming (splitting) the stream of sentences
Stream<String> words = sentences.flatMap(s -> Arrays.asList(s.split(" ")));

// output operation that prints the words to console
words.forEach(w -> System.out.println(w));

----------------------------------------

TITLE: Invalid Stream Reference Example
DESCRIPTION: Example showing invalid forward referencing of stream names in JoinBolt configuration.

LANGUAGE: java
CODE:
new JoinBolt( "spout1", "key1")                 
  .join     ( "spout2", "userId",  "spout3") //not allowed. spout3 not yet introduced
  .join     ( "spout3", "key3",    "spout1")

----------------------------------------

TITLE: Configuring JdbcTridentState for Storm-JDBC Integration
DESCRIPTION: Example of configuring JdbcTridentState for use with Trident topologies. Shows how to set up connection provider, mapper, and specify table name or insert query.

LANGUAGE: java
CODE:
JdbcState.Options options = new JdbcState.Options()
        .withConnectionProvider(connectionProvider)
        .withMapper(jdbcMapper)
        .withTableName("user_details")
        .withQueryTimeoutSecs(30);
JdbcStateFactory jdbcStateFactory = new JdbcStateFactory(options);

----------------------------------------

TITLE: Implementing a CombinerAggregator for Sum Operation
DESCRIPTION: Demonstrates how to implement a CombinerAggregator interface for a Sum operation, enabling efficient partial aggregation before network transfer.

LANGUAGE: java
CODE:
public class Sum implements CombinerAggregator<Long, Long, Long> {

    // The initial value of the sum
    @Override
    public Long init() {
        return 0L;
    }

    // Updates the sum by adding the value (this could be a partial sum)
    @Override
    public Long apply(Long aggregate, Long value) {
        return aggregate + value;
    }

    // merges the partial sums
    @Override
    public Long merge(Long accum1, Long accum2) {
        return accum1 + accum2;
    }

    // extract result from the accumulator (here the accumulator and result is the same)
    @Override
    public Long result(Long accum) {
        return accum;
    }
}

----------------------------------------

TITLE: Implementing a Scalar User-Defined Function in Java
DESCRIPTION: Example Java class implementing a scalar user-defined function for Storm SQL.

LANGUAGE: java
CODE:
public class MyPlus {
  public static Integer evaluate(Integer x, Integer y) {
    return x + y;
  }
}

----------------------------------------

TITLE: Complex DRPC Example - Twitter URL Reach Calculator
DESCRIPTION: Implementation of a complex DRPC topology that calculates the reach of URLs on Twitter using parallel processing.

LANGUAGE: java
CODE:
LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("reach");
builder.addBolt(new GetTweeters(), 3);
builder.addBolt(new GetFollowers(), 12)
        .shuffleGrouping();
builder.addBolt(new PartialUniquer(), 6)
        .fieldsGrouping(new Fields("id", "follower"));
builder.addBolt(new CountAggregator(), 2)
        .fieldsGrouping(new Fields("id"));

----------------------------------------

TITLE: Configuring User Resource Pools
DESCRIPTION: Example configuration for specifying resource guarantees for different users in the Resource Aware Scheduler.

LANGUAGE: yaml
CODE:
resource.aware.scheduler.user.pools:
    jerry:
        cpu: 1000
        memory: 8192.0
    derek:
        cpu: 10000.0
        memory: 32768
    bobby:
        cpu: 5000.0
        memory: 16384.0

----------------------------------------

TITLE: Storm Window Configuration Options
DESCRIPTION: Available window configuration methods for both count-based and time-based windows with different sliding intervals.

LANGUAGE: java
CODE:
withWindow(Count windowLength, Count slidingInterval)
Tuple count based sliding window that slides after `slidingInterval` number of tuples.

withWindow(Count windowLength)
Tuple count based window that slides with every incoming tuple.

withWindow(Count windowLength, Duration slidingInterval)
Tuple count based sliding window that slides after `slidingInterval` time duration.

withWindow(Duration windowLength, Duration slidingInterval)
Time duration based sliding window that slides after `slidingInterval` time duration.

withWindow(Duration windowLength)
Time duration based window that slides with every incoming tuple.

withWindow(Duration windowLength, Count slidingInterval)
Time duration based sliding window configuration that slides after `slidingInterval` number of tuples.

withTumblingWindow(BaseWindowedBolt.Count count)
Count based tumbling window that tumbles after the specified count of tuples.

withTumblingWindow(BaseWindowedBolt.Duration duration)
Time duration based tumbling window that tumbles after the specified time duration.

----------------------------------------

TITLE: Initializing CassandraWriterBolt with Insert Query in Java
DESCRIPTION: Demonstrates how to create a CassandraWriterBolt with an insert query that includes only specified tuple fields. It uses the simpleQuery method to construct the CQL statement.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
    async(
        simpleQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);")
            .with(
                fields("title", "year", "performer", "genre", "tracks")
             )
        )
);

----------------------------------------

TITLE: HBase State Provider Configuration Example
DESCRIPTION: Java code example showing how to initialize HBase state provider with configuration parameters for table name and column family.

LANGUAGE: java
CODE:
Config conf = new Config();
    Map<String, Object> hbConf = new HashMap<String, Object>();
    hbConf.put("hbase.rootdir", "file:///tmp/hbase");
    conf.put("hbase.conf", hbConf);
    conf.put("topology.state.provider",  "org.apache.storm.hbase.state.HBaseKeyValueStateProvider");
    conf.put("topology.state.provider.config", "{" +
            "   \"hbaseConfigKey\": \"hbase.conf\"," +
            "   \"tableName\": \"state\"," +
            "   \"columnFamily\": \"cf\"" +
            " }");

----------------------------------------

TITLE: Implementing Custom Redis Bolt for Word Count Lookup
DESCRIPTION: This Java class extends AbstractRedisBolt to implement a custom bolt for looking up word counts in Redis. It demonstrates how to use JedisCommands to interact with Redis and emit results.

LANGUAGE: java
CODE:
public static class LookupWordTotalCountBolt extends AbstractRedisBolt {
    private static final Logger LOG = LoggerFactory.getLogger(LookupWordTotalCountBolt.class);
    private static final Random RANDOM = new Random();

    public LookupWordTotalCountBolt(JedisPoolConfig config) {
        super(config);
    }

    public LookupWordTotalCountBolt(JedisClusterConfig config) {
        super(config);
    }

    @Override
    public void execute(Tuple input) {
        JedisCommands jedisCommands = null;
        try {
            jedisCommands = getInstance();
            String wordName = input.getStringByField("word");
            String countStr = jedisCommands.get(wordName);
            if (countStr != null) {
                int count = Integer.parseInt(countStr);
                this.collector.emit(new Values(wordName, count));

                // print lookup result with low probability
                if(RANDOM.nextInt(1000) > 995) {
                    LOG.info("Lookup result - word : " + wordName + " / count : " + count);
                }
            } else {
                // skip
                LOG.warn("Word not found in Redis - word : " + wordName);
            }
        } finally {
            if (jedisCommands != null) {
                returnInstance(jedisCommands);
            }
            this.collector.ack(input);
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        // wordName, count
        declarer.declare(new Fields("wordName", "count"));
    }
}

----------------------------------------

TITLE: Initializing HiveBolt Configuration
DESCRIPTION: Java code demonstrating basic HiveBolt setup with DelimitedRecordHiveMapper for streaming data to Hive.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames));
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper);
HiveBolt hiveBolt = new HiveBolt(hiveOptions);

----------------------------------------

TITLE: Bolt Message Listening in Apache Storm (Clojure)
DESCRIPTION: Implementation of message listening for bolts in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L489

----------------------------------------

TITLE: Implementing Local DRPC in Storm
DESCRIPTION: Demonstrates how to set up and use Distributed RPC (DRPC) in local mode with Storm. Creates LocalDRPC and LocalCluster instances for testing DRPC functionality.

LANGUAGE: java
CODE:
try (LocalDRPC drpc = new LocalDRPC();
     LocalCluster cluster = new LocalCluster();
     LocalTopology topo = cluster.submitTopology("drpc-demo", conf, builder.createLocalTopology(drpc))) {

    System.out.println("Results for 'hello':" + drpc.execute("exclamation", "hello"));
}

----------------------------------------

TITLE: Implementing Bulk Operations for Efficient State Management in Trident
DESCRIPTION: Example of implementing bulk operations (setLocationsBulk and bulkGetLocations) in a custom State object (LocationDB) for more efficient state management in Trident.

LANGUAGE: java
CODE:
public class LocationDB implements State {
    public void beginCommit(Long txid) {    
    }
    
    public void commit(Long txid) {    
    }
    
    public void setLocationsBulk(List<Long> userIds, List<String> locations) {
      // set locations in bulk
    }
    
    public List<String> bulkGetLocations(List<Long> userIds) {
      // get locations in bulk
    }
}

----------------------------------------

TITLE: Adding Apache Storm Dependency in Maven POM
DESCRIPTION: This XML snippet shows how to add Apache Storm as a development dependency in a Maven project's pom.xml file. It specifies the groupId, artifactId, version, and scope for the storm-client artifact.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.storm</groupId>
  <artifactId>storm-client</artifactId>
  <version>{{page.version}}</version>
  <scope>provided</scope>
</dependency>

----------------------------------------

TITLE: Defining ConnectionProvider Interface in Java
DESCRIPTION: Defines the ConnectionProvider interface for implementing different connection pooling mechanisms. It includes methods for preparing, getting connections, and cleanup.

LANGUAGE: java
CODE:
public interface ConnectionProvider extends Serializable {
    /**
     * method must be idempotent.
     */
    void prepare();

    /**
     *
     * @return a DB connection over which the queries can be executed.
     */
    Connection getConnection();

    /**
     * called once when the system is shutting down, should be idempotent.
     */
    void cleanup();
}

----------------------------------------

TITLE: Configuring KafkaSpout in Storm Topology
DESCRIPTION: This code example shows how to configure and use KafkaSpout to read data from Kafka as part of a Storm topology.

LANGUAGE: java
CODE:
final TopologyBuilder tp = new TopologyBuilder();
tp.setSpout("kafka_spout", new KafkaSpout<>(KafkaSpoutConfig.builder("127.0.0.1:" + port, "topic").build()), 1);
tp.setBolt("bolt", new myBolt()).shuffleGrouping("kafka_spout");

----------------------------------------

TITLE: Setting up EsState for Storm Trident-Elasticsearch Integration in Java
DESCRIPTION: This snippet illustrates how to create an EsState for Trident topologies to persist data in Elasticsearch. It uses EsConfig for cluster configuration, EsTupleMapper for mapping tuples, and sets up a StateFactory and TridentState.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();

StateFactory factory = new EsStateFactory(esConfig, tupleMapper);
TridentState state = stream.partitionPersist(factory, esFields, new EsUpdater(), new Fields());

----------------------------------------

TITLE: Implementing Parallel Top-N Pattern in Storm
DESCRIPTION: Implementation of a scalable streaming top-N calculation using parallel processing with fields grouping and global aggregation.

LANGUAGE: java
CODE:
builder.setBolt("rank", new RankObjects(), parallelism)
  .fieldsGrouping("objects", new Fields("value"));
builder.setBolt("merge", new MergeObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Basic JoinBolt Configuration in Storm
DESCRIPTION: Implementation of a JoinBolt that joins four spouts with inner and left joins, corresponding to the SQL example. Includes window configuration and fields grouping setup.

LANGUAGE: java
CODE:
JoinBolt jbolt =  new JoinBolt("spout1", "key1")                   // from        spout1  
                    .join     ("spout2", "userId",  "spout1")      // inner join  spout2  on spout2.userId = spout1.key1
                    .join     ("spout3", "key3",    "spout2")      // inner join  spout3  on spout3.key3   = spout2.userId   
                    .leftJoin ("spout4", "key4",    "spout3")      // left join   spout4  on spout4.key4   = spout3.key3
                    .select  ("userId, key4, key2, spout3:key3")   // chose output fields
                    .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;

topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("spout1", new Fields("key1") )
            .fieldsGrouping("spout2", new Fields("userId") )
            .fieldsGrouping("spout3", new Fields("key3") )
            .fieldsGrouping("spout4", new Fields("key4") );

----------------------------------------

TITLE: Defining Bolt Component in Thrift
DESCRIPTION: The Thrift definition for bolts in Storm, which includes ComponentObject and ComponentCommon structs. ComponentObject defines the bolt implementation, while ComponentCommon specifies streams, parallelism, and configuration.

LANGUAGE: Thrift
CODE:
struct ComponentObject {
  1: binary serialized_java
  2: ShellComponent shell
  3: JavaObject java_object
}

struct ComponentCommon {
  1: map<string, StreamInfo> streams
  2: map<GlobalStreamId, Grouping> inputs
  3: i32 parallelism_hint
  4: map<string, string> json_conf
}

----------------------------------------

TITLE: Configuring SimpleJdbcMapper for Database Operations in Java
DESCRIPTION: This code snippet shows how to configure a SimpleJdbcMapper for transforming Storm tuples to database rows. It demonstrates setting up the connection provider and specifying the table name or column schema.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);
String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

// Alternatively, with explicit column schema:
List<Column> columnSchema = Lists.newArrayList(
    new Column("user_id", java.sql.Types.INTEGER),
    new Column("user_name", java.sql.Types.VARCHAR));
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(columnSchema);

----------------------------------------

TITLE: Configuring Storm Metricstore with YAML
DESCRIPTION: Configuration options for Storm's metric storage system, including class implementations, RocksDB settings, and retention policies. Defines the metric store class, processor class, database location, creation behavior, cache capacity, and retention period.

LANGUAGE: yaml
CODE:
storm.metricstore.class: "org.apache.storm.metricstore.rocksdb.RocksDbStore"
storm.metricprocessor.class: "org.apache.storm.metricstore.NimbusMetricProcessor"
storm.metricstore.rocksdb.location: "storm_rocks"
storm.metricstore.rocksdb.create_if_missing: true
storm.metricstore.rocksdb.metadata_string_cache_capacity: 4000
storm.metricstore.rocksdb.retention_hours: 240

----------------------------------------

TITLE: Configuring RedisState for Trident Topology
DESCRIPTION: Java code snippet demonstrating the configuration of RedisState for use in a Trident topology, including state updates and queries.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
                                .setHost(redisHost).setPort(redisPort)
                                .build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisLookupMapper lookupMapper = new WordCountLookupMapper();
RedisState.Factory factory = new RedisState.Factory(poolConfig);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory,
                        fields,
                        new RedisStateUpdater(storeMapper).withExpire(86400000),
                        new Fields());

TridentState state = topology.newStaticState(factory);
stream = stream.stateQuery(state, new Fields("word"),
                        new RedisStateQuerier(lookupMapper),
                        new Fields("columnName","columnValue"));

----------------------------------------

TITLE: Running MQTT Broker and Publisher in Java
DESCRIPTION: Command to start an MQTT broker on port 1883 and a publisher that sends random temperature/humidity data to a topic.

LANGUAGE: bash
CODE:
java -cp examples/target/storm-mqtt-examples-*-SNAPSHOT.jar org.apache.storm.mqtt.examples.MqttBrokerPublisher

----------------------------------------

TITLE: Basic Stream Operations in Java
DESCRIPTION: Demonstrates creation of streams from spouts and basic transformations like mapping and flat mapping of data streams.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

// a stream of sentences obtained from a source spout
Stream<String> sentences = builder.newStream(new RandomSentenceSpout()).map(tuple -> tuple.getString(0));

// a stream of words obtained by transforming (splitting) the stream of sentences
Stream<String> words = sentences.flatMap(s -> Arrays.asList(s.split(" ")));

// output operation that prints the words to console
words.forEach(w -> System.out.println(w));

----------------------------------------

TITLE: Configuring Resource Aware Scheduler in Storm YAML
DESCRIPTION: Basic configuration to enable the Resource Aware Scheduler in Storm's configuration file.

LANGUAGE: yaml
CODE:
storm.scheduler: "org.apache.storm.scheduler.resource.ResourceAwareScheduler"

----------------------------------------

TITLE: Named Streams Join Configuration
DESCRIPTION: Examples showing how to configure JoinBolt to work with named streams instead of default streams, including stream selector configuration and multiple stream joining.

LANGUAGE: java
CODE:
new JoinBolt(JoinBolt.Selector.STREAM,  "stream1", "key1")
                                  .join("stream2", "key2")
    ...

LANGUAGE: java
CODE:
new JoinBolt(JoinBolt.Selector.STREAM,  "stream1", "key1") 
                             .join     ("stream2", "userId",  "stream1" )
                             .select ("userId, key1, key2")
                             .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;
                             
topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("bolt1", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt2", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt3", "stream2", new Fields("userId") )
            .fieldsGrouping("bolt4", "stream1", new Fields("key1") );

----------------------------------------

TITLE: Configuring Zookeeper Servers in Storm YAML
DESCRIPTION: This snippet shows how to configure the Zookeeper servers for a Storm cluster in the storm.yaml configuration file. It specifies the IP addresses of the Zookeeper servers.

LANGUAGE: yaml
CODE:
storm.zookeeper.servers:
  - "111.222.333.444"
  - "555.666.777.888"

----------------------------------------

TITLE: Registering Metrics Consumer in YAML
DESCRIPTION: Example of how to register metrics consumers in the storm.yaml configuration file, including both LoggingMetricsConsumer and HttpForwardingMetricsConsumer.

LANGUAGE: yaml
CODE:
topology.metrics.consumer.register:
  - class: "org.apache.storm.metric.LoggingMetricsConsumer"
    parallelism.hint: 1
  - class: "org.apache.storm.metric.HttpForwardingMetricsConsumer"
    parallelism.hint: 1
    argument: "http://example.com:8080/metrics/my-topology/"

----------------------------------------

TITLE: Implementing Joins with Storm Core JoinBolt
DESCRIPTION: Java code snippet demonstrating how to use JoinBolt to perform joins on tuples generated by 4 spouts in Apache Storm, equivalent to the SQL join example.

LANGUAGE: java
CODE:
JoinBolt jbolt =  new JoinBolt("spout1", "key1")                   // from        spout1  
                    .join     ("spout2", "userId",  "spout1")      // inner join  spout2  on spout2.userId = spout1.key1
                    .join     ("spout3", "key3",    "spout2")      // inner join  spout3  on spout3.key3   = spout2.userId   
                    .leftJoin ("spout4", "key4",    "spout3")      // left join   spout4  on spout4.key4   = spout3.key3
                    .select  ("userId, key4, key2, spout3:key3")   // chose output fields
                    .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;

topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("spout1", new Fields("key1") )
            .fieldsGrouping("spout2", new Fields("userId") )
            .fieldsGrouping("spout3", new Fields("key3") )
            .fieldsGrouping("spout4", new Fields("key4") );

----------------------------------------

TITLE: Filtering Kafka Stream Example in Storm SQL
DESCRIPTION: Set of SQL statements to filter a Kafka stream of orders, calculate totals, and insert large orders into another Kafka stream. It defines input and output tables and a SELECT statement to perform the filtering and calculation.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://orders?bootstrap-servers=localhost:9092,localhost:9093'
CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) LOCATION 'kafka://large_orders?bootstrap-servers=localhost:9092,localhost:9093' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Implementing RedisFilterMapper for Blacklist Filtering
DESCRIPTION: This Java class implements RedisFilterMapper for filtering words based on a blacklist stored in Redis. It uses a SET data type and defines how to extract keys from tuples.

LANGUAGE: java
CODE:
class BlacklistWordFilterMapper implements RedisFilterMapper {
    private RedisDataTypeDescription description;
    private final String setKey = "blacklist";

    public BlacklistWordFilterMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.SET, setKey);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Using Storm Shell Command for Topology Submission
DESCRIPTION: This command demonstrates how to use the 'storm shell' command to submit a topology. It packages resources into a jar, uploads it to Nimbus, and executes the specified Python script with additional arguments.

LANGUAGE: shell
CODE:
storm shell resources/ python3 topology.py arg1 arg2

----------------------------------------

TITLE: Storm Topology Submission with Blob Mapping
DESCRIPTION: Example command showing how to submit a Storm topology with blob mapping configuration

LANGUAGE: bash
CODE:
storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar org.apache.storm.starter.clj.word_count test_topo -c topology.blobstore.map='{"key1":{"localname":"blob_file", "uncompress":false, "workerRestart":true},"key2":{}}'

----------------------------------------

TITLE: Configuring MongoDB Insert Bolt - Java
DESCRIPTION: Example configuration for MongoInsertBolt using SimpleMongoMapper to insert data into MongoDB collection.

LANGUAGE: java
CODE:
String url = "mongodb://127.0.0.1:27017/test";
String collectionName = "wordcount";

MongoMapper mapper = new SimpleMongoMapper()
        .withFields("word", "count");

MongoInsertBolt insertBolt = new MongoInsertBolt(url, collectionName, mapper);

----------------------------------------

TITLE: Registering Custom Counter in Storm Topology
DESCRIPTION: Example showing how to register a custom counter metric that expands to include topology, hostname, component, and task information.

LANGUAGE: java
CODE:
Counter myCounter = topologyContext.registerCounter("myCounter");

----------------------------------------

TITLE: Complete Word Count Example in Java
DESCRIPTION: A complete example showing how to build a word count topology using Stream API with windowing and aggregation.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

builder
   // A stream of random sentences with two partitions
   .newStream(new RandomSentenceSpout(), new ValueMapper<String>(0), 2)
   // a two seconds tumbling window
   .window(TumblingWindows.of(Duration.seconds(2)))
   // split the sentences to words
   .flatMap(s -> Arrays.asList(s.split(" ")))
   // create a stream of (word, 1) pairs
   .mapToPair(w -> Pair.of(w, 1))
   // compute the word counts in the last two second window
   .countByKey()
   // print the results to stdout
   .print();

----------------------------------------

TITLE: Using Batch Statements with CassandraWriterBolt in Java
DESCRIPTION: Demonstrates how to use batch statements with CassandraWriterBolt. It shows examples of both logged and unlogged batches, which can be used for performance optimization when inserting multiple records.

LANGUAGE: java
CODE:
// Logged
new CassandraWriterBolt(loggedBatch(
        simpleQuery("INSERT INTO titles_per_album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all())),
        simpleQuery("INSERT INTO titles_per_performer (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all()))
    )
);
// UnLogged
new CassandraWriterBolt(unLoggedBatch(
        simpleQuery("INSERT INTO titles_per_album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all())),
        simpleQuery("INSERT INTO titles_per_performer (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all()))
    )
);

----------------------------------------

TITLE: Custom Redis Bolt Implementation
DESCRIPTION: Example of extending AbstractRedisBolt for custom Redis operations, implementing word count total lookup.

LANGUAGE: java
CODE:
public static class LookupWordTotalCountBolt extends AbstractRedisBolt {
    private static final Logger LOG = LoggerFactory.getLogger(LookupWordTotalCountBolt.class);
    private static final Random RANDOM = new Random();

    public LookupWordTotalCountBolt(JedisPoolConfig config) {
        super(config);
    }

    public LookupWordTotalCountBolt(JedisClusterConfig config) {
        super(config);
    }

    @Override
    public void execute(Tuple input) {
        JedisCommands jedisCommands = null;
        try {
            jedisCommands = getInstance();
            String wordName = input.getStringByField("word");
            String countStr = jedisCommands.get(wordName);
            if (countStr != null) {
                int count = Integer.parseInt(countStr);
                this.collector.emit(new Values(wordName, count));

                if(RANDOM.nextInt(1000) > 995) {
                    LOG.info("Lookup result - word : " + wordName + " / count : " + count);
                }
            } else {
                LOG.warn("Word not found in Redis - word : " + wordName);
            }
        } finally {
            if (jedisCommands != null) {
                returnInstance(jedisCommands);
            }
            this.collector.ack(input);
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("wordName", "count"));
    }
}

----------------------------------------

TITLE: Simple DRPC Topology Implementation in Java
DESCRIPTION: Implements a basic DRPC topology that appends an exclamation mark to input strings.

LANGUAGE: java
CODE:
public static class ExclaimBolt extends BaseBasicBolt {
    public void execute(Tuple tuple, BasicOutputCollector collector) {
        String input = tuple.getString(1);
        collector.emit(new Values(tuple.getValue(0), input + "!"));
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "result"));
    }
}

public static void main(String[] args) throws Exception {
    LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("exclamation");
    builder.addBolt(new ExclaimBolt(), 3);
    // ...
}

----------------------------------------

TITLE: Defining a Storm Topology in Clojure
DESCRIPTION: Example of defining a Storm topology using the Clojure DSL, including spout and bolt specifications with different groupings and parallelism hints.

LANGUAGE: clojure
CODE:
(topology
 {"1" (spout-spec sentence-spout)
  "2" (spout-spec (sentence-spout-parameterized
                   ["the cat jumped over the door"
                    "greetings from a faraway land"])
                   :p 2)}
 {"3" (bolt-spec {"1" :shuffle "2" :shuffle}
                 split-sentence
                 :p 5)
  "4" (bolt-spec {"3" ["word"]}
                 word-count
                 :p 6)})

----------------------------------------

TITLE: Implementing a General Aggregator for Counting
DESCRIPTION: Demonstrates how to create a general Aggregator that counts the number of tuples.

LANGUAGE: java
CODE:
public class CountAgg extends BaseAggregator<CountState> {
    static class CountState {
        long count = 0;
    }

    public CountState init(Object batchId, TridentCollector collector) {
        return new CountState();
    }

    public void aggregate(CountState state, TridentTuple tuple, TridentCollector collector) {
        state.count+=1;
    }

    public void complete(CountState state, TridentCollector collector) {
        collector.emit(new Values(state.count));
    }
}

----------------------------------------

TITLE: Overriding Component Configurations in Java Storm Topology
DESCRIPTION: Demonstrates two methods for specifying component-specific configurations in a Storm topology using Java: internally by overriding getComponentConfiguration method, and externally using TopologyBuilder methods.

LANGUAGE: java
CODE:
// Internal method
@Override
public Map<String, Object> getComponentConfiguration() {
    return componentSpecificConfig;
}

// External method
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("spout", new MySpout())
       .addConfiguration("topology.debug", true);
builder.setBolt("bolt", new MyBolt())
       .addConfigurations(boltConfig);

----------------------------------------

TITLE: Debugging Storm Topology in IDE
DESCRIPTION: Shows how to override local mode for debugging within an IDE using LocalCluster.withLocalModeOverride. This allows for easy debugging and testing of Storm topologies.

LANGUAGE: java
CODE:
public static void main(final String args[]) {
    LocalCluster.withLocalModeOverride(() -> originalMain(args), 10);
}

----------------------------------------

TITLE: Sliding Window Bolt Implementation
DESCRIPTION: Example implementation of a sliding window bolt with topology builder configuration.

LANGUAGE: java
CODE:
public class SlidingWindowBolt extends BaseWindowedBolt {
	private OutputCollector collector;
	
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
    	this.collector = collector;
    }
	
    @Override
    public void execute(TupleWindow inputWindow) {
	  for(Tuple tuple: inputWindow.get()) {
	    // do the windowing computation
		...
	  }
	  // emit the results
	  collector.emit(new Values(computedValue));
    }
}

public static void main(String[] args) {
    TopologyBuilder builder = new TopologyBuilder();
     builder.setSpout("spout", new RandomSentenceSpout(), 1);
     builder.setBolt("slidingwindowbolt", 
                     new SlidingWindowBolt().withWindow(new Count(30), new Count(10)),
                     1).shuffleGrouping("spout");
    Config conf = new Config();
    conf.setDebug(true);
    conf.setNumWorkers(1);

    StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());
	
}

----------------------------------------

TITLE: Configuring Hadoop AuthenticationFilter for Storm UI
DESCRIPTION: YAML configuration for using Hadoop's AuthenticationFilter with Kerberos for Storm UI authentication

LANGUAGE: yaml
CODE:
ui.filter: "org.apache.hadoop.security.authentication.server.AuthenticationFilter"
ui.filter.params:
   "type": "kerberos"
   "kerberos.principal": "HTTP/nimbus.witzend.com"
   "kerberos.keytab": "/vagrant/keytabs/http.keytab"
   "kerberos.name.rules": "RULE:[2:$1@$0]([jt]t@.*EXAMPLE.COM)s/.*/$MAPRED_USER/ RULE:[2:$1@$0]([nd]n@.*EXAMPLE.COM)s/.*/$HDFS_USER/DEFAULT"

----------------------------------------

TITLE: Implementing Split Function for Word Processing
DESCRIPTION: Custom function that splits a sentence into individual words, creating a new tuple for each word found in the input sentence.

LANGUAGE: java
CODE:
public class Split extends BaseFunction {
   public void execute(TridentTuple tuple, TridentCollector collector) {
       String sentence = tuple.getString(0);
       for(String word: sentence.split(" ")) {
           collector.emit(new Values(word));                
       }
   }
}

----------------------------------------

TITLE: Implementing Split Function for Word Processing
DESCRIPTION: Custom function that splits a sentence into individual words, creating a new tuple for each word found in the input sentence.

LANGUAGE: java
CODE:
public class Split extends BaseFunction {
   public void execute(TridentTuple tuple, TridentCollector collector) {
       String sentence = tuple.getString(0);
       for(String word: sentence.split(" ")) {
           collector.emit(new Values(word));                
       }
   }
}

----------------------------------------

TITLE: Implementing Complex DRPC Topology for Twitter Reach in Java
DESCRIPTION: Demonstrates the implementation of a more complex DRPC topology for computing the reach of a URL on Twitter using multiple bolts.

LANGUAGE: java
CODE:
LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("reach");
builder.addBolt(new GetTweeters(), 3);
builder.addBolt(new GetFollowers(), 12)
        .shuffleGrouping();
builder.addBolt(new PartialUniquer(), 6)
        .fieldsGrouping(new Fields("id", "follower"));
builder.addBolt(new CountAggregator(), 2)
        .fieldsGrouping(new Fields("id"));

----------------------------------------

TITLE: Defining ConnectionProvider Interface in Java
DESCRIPTION: Interface definition for database connection pooling that must be implemented to provide database connectivity. Includes methods for preparation, connection retrieval, and cleanup.

LANGUAGE: java
CODE:
public interface ConnectionProvider extends Serializable {
    /**
     * method must be idempotent.
     */
    void prepare();

    /**
     *
     * @return a DB connection over which the queries can be executed.
     */
    Connection getConnection();

    /**
     * called once when the system is shutting down, should be idempotent.
     */
    void cleanup();
}

----------------------------------------

TITLE: Monitoring Topology in Nimbus (Clojure)
DESCRIPTION: Nimbus monitors topologies using a finite state machine. The monitor event is triggered periodically to reassign topology as needed based on heartbeats and other factors.

LANGUAGE: clojure
CODE:
(schedule-recurring timer thread (fn [] (transition! :monitor topologies))

----------------------------------------

TITLE: Retrieving Topology Worker Information in JSON
DESCRIPTION: Sample response from the /api/v1/topology-workers/<id> endpoint, showing worker information for a specific topology.

LANGUAGE: json
CODE:
{
  "hostPortList":[
    {
      "host":"192.168.202.2",
      "port":6701
    },
    {
      "host":"192.168.202.2",
      "port":6702
    },
    {
      "host":"192.168.202.3",
      "port":6700
    }
  ],
  "logviewerPort":8000
}

----------------------------------------

TITLE: Emit Command JSON Structure in Storm Multi-Language Protocol
DESCRIPTION: JSON structure for emitting tuples from a shell component, including optional fields for stream, task, and tuple ID.

LANGUAGE: json
CODE:
{
	"command": "emit",
	// The id for the tuple. Leave this out for an unreliable emit. The id can
    // be a string or a number.
	"id": "1231231",
	// The id of the stream this tuple was emitted to. Leave this empty to emit to default stream.
	"stream": "1",
	// If doing an emit direct, indicate the task to send the tuple to
	"task": 9,
	// All the values in this tuple
	"tuple": ["field1", 2, 3]
}

----------------------------------------

TITLE: Exclamation DRPC Example Implementation
DESCRIPTION: Implementation of a simple DRPC topology that appends an exclamation mark to input strings.

LANGUAGE: java
CODE:
public static class ExclaimBolt extends BaseBasicBolt {
    public void execute(Tuple tuple, BasicOutputCollector collector) {
        String input = tuple.getString(1);
        collector.emit(new Values(tuple.getValue(0), input + "!"));
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "result"));
    }
}

public static void main(String[] args) throws Exception {
    LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("exclamation");
    builder.addBolt(new ExclaimBolt(), 3);
    // ...
}

----------------------------------------

TITLE: Maven Shade Plugin Configuration for HDFS Integration
DESCRIPTION: Maven configuration for properly packaging a Storm topology with HDFS dependencies using the maven-shade-plugin.

LANGUAGE: xml
CODE:
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-shade-plugin</artifactId>
    <version>1.4</version>
    <configuration>
        <createDependencyReducedPom>true</createDependencyReducedPom>
    </configuration>
    <executions>
        <execution>
            <phase>package</phase>
            <goals>
                <goal>shade</goal>
            </goals>
            <configuration>
                <transformers>
                    <transformer
                            implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                    <transformer
                            implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                        <mainClass></mainClass>
                    </transformer>
                </transformers>
            </configuration>
        </execution>
    </executions>
</plugin>

----------------------------------------

TITLE: Implementing Split Function for Word Processing
DESCRIPTION: Custom function that splits a sentence into individual words, creating a new tuple for each word found in the input sentence.

LANGUAGE: java
CODE:
public class Split extends BaseFunction {
   public void execute(TridentTuple tuple, TridentCollector collector) {
       String sentence = tuple.getString(0);
       for(String word: sentence.split(" ")) {
           collector.emit(new Values(word));                
       }
   }
}

----------------------------------------

TITLE: Configuring JdbcTridentState for Inserts in Java
DESCRIPTION: Demonstrates how to configure a JdbcTridentState for use with Trident topologies. It shows options for specifying table name, JdbcMapper, and connection provider.

LANGUAGE: java
CODE:
JdbcState.Options options = new JdbcState.Options()
        .withConnectionProvider(connectionProvider)
        .withMapper(jdbcMapper)
        .withTableName("user_details")
        .withQueryTimeoutSecs(30);
JdbcStateFactory jdbcStateFactory = new JdbcStateFactory(options);

----------------------------------------

TITLE: Implementing Basic Insert Query with CassandraWriterBolt
DESCRIPTION: Creates a CassandraWriterBolt that inserts data into an album table with specified tuple fields.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
    async(
        simpleQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);")\n            .with(
                fields("title", "year", "performer", "genre", "tracks")
             )
        )
);

----------------------------------------

TITLE: Creating External Tables with Data Sources
DESCRIPTION: Example showing how to create external tables in Storm SQL to connect to data sources like Kafka

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-hosts=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Storm Docker Configuration Example
DESCRIPTION: Sample configuration for enabling Docker support in Storm on RHEL7. It includes settings for resource isolation, allowed Docker images, cgroup paths, and worker launcher setup.

LANGUAGE: bash
CODE:
storm.resource.isolation.plugin.enable: true
storm.resource.isolation.plugin: "org.apache.storm.container.docker.DockerManager"
storm.oci.allowed.images: ["xxx.xxx.com:8080/storm/storm/rhel7:latest"]
storm.oci.image: "xxx.xxx.com:8080/storm/storm/rhel7:latest"
storm.oci.cgroup.root: "/storm"
storm.oci.cgroup.parent: "/sys/fs/cgroup"
storm.oci.readonly.bindmounts:
    - "/etc/storm"
storm.oci.nscd.dir: "/var/run/nscd"
supervisor.worker.launcher: "/usr/share/apache-storm-2.3.0/bin/worker-launcher"

----------------------------------------

TITLE: Initializing RocketMQ Spout in Storm
DESCRIPTION: Configuration and setup of RocketMqSpout for consuming messages from RocketMQ topics. Uses MQPushConsumer implementation with configurable retry attempts for failed messages.

LANGUAGE: java
CODE:
        Properties properties = new Properties();
        properties.setProperty(SpoutConfig.NAME_SERVER_ADDR, nameserverAddr);
        properties.setProperty(SpoutConfig.CONSUMER_GROUP, group);
        properties.setProperty(SpoutConfig.CONSUMER_TOPIC, topic);

        RocketMqSpout spout = new RocketMqSpout(properties);

----------------------------------------

TITLE: Adding Flux as a Maven Dependency
DESCRIPTION: Maven XML configuration to add Flux as a dependency to a Storm project.

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>flux-core</artifactId>
    <version>${storm.version}</version>
</dependency>

----------------------------------------

TITLE: Configuring and Using JdbcInsertBolt in Java
DESCRIPTION: Example of configuring and using JdbcInsertBolt with HikariCP connection provider and SimpleJdbcMapper. It demonstrates setting up the connection, creating the mapper, and initializing the bolt with table name or insert query.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);

String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withTableName("user")
                                    .withQueryTimeoutSecs(30);
                                    Or
JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withInsertQuery("insert into user values (?,?)")
                                    .withQueryTimeoutSecs(30);                                    

----------------------------------------

TITLE: Defining HBase Mapper Interface in Java
DESCRIPTION: Core interface definition for mapping Storm tuples to HBase operations. Includes methods for generating row keys and column lists from tuples.

LANGUAGE: java
CODE:
public interface HBaseMapper extends Serializable {
    byte[] rowKey(Tuple tuple);

    ColumnList columns(Tuple tuple);
}

----------------------------------------

TITLE: Creating RedisLookupBolt Instance
DESCRIPTION: This Java code snippet demonstrates how to create a RedisLookupBolt instance using a JedisPoolConfig and a RedisLookupMapper. It configures the Redis connection and sets up the lookup mapper.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisLookupMapper lookupMapper = new WordCountRedisLookupMapper();
RedisLookupBolt lookupBolt = new RedisLookupBolt(poolConfig, lookupMapper);

----------------------------------------

TITLE: Creating RedisLookupBolt in Java
DESCRIPTION: Shows how to create a RedisLookupBolt using a JedisPoolConfig and a custom RedisLookupMapper.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisLookupMapper lookupMapper = new WordCountRedisLookupMapper();
RedisLookupBolt lookupBolt = new RedisLookupBolt(poolConfig, lookupMapper);

----------------------------------------

TITLE: Configuring Trident Kafka State
DESCRIPTION: Example showing how to configure and use TridentKafkaState to write data to Kafka using Trident topology.

LANGUAGE: java
CODE:
Fields fields = new Fields("word", "count");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
        new Values("storm", "1"),
        new Values("trident", "1"),
        new Values("needs", "1"),
        new Values("javadoc", "1")
);
spout.setCycle(true);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

//set producer properties.
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

TridentKafkaStateFactory stateFactory = new TridentKafkaStateFactory()
        .withProducerProperties(props)
        .withKafkaTopicSelector(new DefaultTopicSelector("test"))
        .withTridentTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper("word", "count"));
stream.partitionPersist(stateFactory, fields, new TridentKafkaStateUpdater(), new Fields());

Config conf = new Config();
StormSubmitter.submitTopology("kafkaTridentTest", conf, topology.build());

----------------------------------------

TITLE: Basic Flux YAML Configuration
DESCRIPTION: Example YAML configuration for a simple word count topology using Flux.

LANGUAGE: yaml
CODE:
name: "yaml-topology"
config:
  topology.workers: 1

# spout definitions
spouts:
  - id: "spout-1"
    className: "org.apache.storm.testing.TestWordSpout"
    parallelism: 1

# bolt definitions
bolts:
  - id: "bolt-1"
    className: "org.apache.storm.testing.TestWordCounter"
    parallelism: 1
  - id: "bolt-2"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1

#stream definitions
streams:
  - name: "spout-1 --> bolt-1" # name isn't used (placeholder for logging, UI, etc.)
    from: "spout-1"
    to: "bolt-1"
    grouping:
      type: FIELDS
      args: ["word"]

  - name: "bolt-1 --> bolt2"
    from: "bolt-1"
    to: "bolt-2"
    grouping:
      type: SHUFFLE

# worker hook definitions
workerHooks:
  - id: "base-worker-hook"
    className: "org.apache.storm.hooks.BaseWorkerHook"

----------------------------------------

TITLE: Registering Custom Kryo Serializers in Storm Configuration
DESCRIPTION: YAML configuration example showing how to register custom serializer classes in Storm. Demonstrates both automatic FieldsSerializer registration and custom serializer implementation registration.

LANGUAGE: yaml
CODE:
topology.kryo.register:
  - com.mycompany.CustomType1
  - com.mycompany.CustomType2: com.mycompany.serializer.CustomType2Serializer
  - com.mycompany.CustomType3

----------------------------------------

TITLE: Creating SolrJsonMapper in Java
DESCRIPTION: This snippet shows how to create a SolrJsonMapper object to update a Solr collection with JSON content from a tuple field.

LANGUAGE: java
CODE:
SolrMapper solrMapper = new SolrJsonMapper.Builder("gettingstarted", "JSON").build();

----------------------------------------

TITLE: Implementing Complex DRPC Topology for Twitter Reach in Java
DESCRIPTION: This code defines a more complex DRPC topology for calculating the reach of a URL on Twitter, demonstrating the use of multiple bolts and groupings.

LANGUAGE: java
CODE:
LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("reach");
builder.addBolt(new GetTweeters(), 3);
builder.addBolt(new GetFollowers(), 12)
        .shuffleGrouping();
builder.addBolt(new PartialUniquer(), 6)
        .fieldsGrouping(new Fields("id", "follower"));
builder.addBolt(new CountAggregator(), 2)
        .fieldsGrouping(new Fields("id"));

----------------------------------------

TITLE: DRPC Server Configuration in YAML
DESCRIPTION: YAML configuration for setting up DRPC servers in a Storm cluster, including server locations and transport settings.

LANGUAGE: yaml
CODE:
drpc.servers:
  - "drpc1.foo.com"
  - "drpc2.foo.com"
drpc.http.port: 8081
storm.thrift.transport: "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin"

----------------------------------------

TITLE: JDBC Mapper Interface Definition in Java
DESCRIPTION: Interface for mapping Storm tuples to database columns. Used for defining how tuple data maps to database row columns.

LANGUAGE: java
CODE:
public interface JdbcMapper  extends Serializable {
    List<Column> getColumns(ITuple tuple);
}

----------------------------------------

TITLE: User Resource Pool Configuration in YAML
DESCRIPTION: Example configuration for specifying user resource guarantees and pools

LANGUAGE: yaml
CODE:
resource.aware.scheduler.user.pools:
    jerry:
        cpu: 1000
        memory: 8192.0
    derek:
        cpu: 10000.0
        memory: 32768
    bobby:
        cpu: 5000.0
        memory: 16384.0

----------------------------------------

TITLE: Creating Conditional Streams in Trident Topology
DESCRIPTION: Demonstrates how to create multiple conditional streams in a Trident topology using stream variables and filters. Shows branching logic implementation.

LANGUAGE: java
CODE:
Stream s = topology.each(...).groupBy(...).aggregate(...)
Stream branch1 = s.each(..., FilterA)
Stream branch2 = s.each(..., FilterB)

----------------------------------------

TITLE: Defining HBase Mapper Interface in Java
DESCRIPTION: Core interface definition for mapping Storm tuples to HBase operations. Includes methods for generating row keys and column lists from tuples.

LANGUAGE: java
CODE:
public interface HBaseMapper extends Serializable {
    byte[] rowKey(Tuple tuple);

    ColumnList columns(Tuple tuple);
}

----------------------------------------

TITLE: Performing Basic Stream Transformations in Java
DESCRIPTION: Demonstrates basic stream transformations such as filter, map, and flatMap operations on a stream.

LANGUAGE: java
CODE:
Stream<String> logs = ...
Stream<String> errors = logs.filter(line -> line.contains("ERROR"));

Stream<String> words = ...
Stream<Integer> wordLengths = words.map(String::length);

Stream<String> sentences = ...
Stream<String> words = sentences.flatMap(s -> Arrays.asList(s.split(" ")));

----------------------------------------

TITLE: Defining JdbcMapper Interface for Storm-JDBC Integration
DESCRIPTION: This code snippet defines the JdbcMapper interface, which is the main API for inserting data into a table using JDBC. It includes a method for mapping a storm tuple to a list of columns representing a row in a database.

LANGUAGE: java
CODE:
public interface JdbcMapper  extends Serializable {
    List<Column> getColumns(ITuple tuple);
}

----------------------------------------

TITLE: Storm JoinBolt Configuration
DESCRIPTION: Java code demonstrating how to configure JoinBolt to join multiple streams with field grouping and window specifications.

LANGUAGE: java
CODE:
JoinBolt jbolt =  new JoinBolt("spout1", "key1")                   // from        spout1  
                    .join     ("spout2", "userId",  "spout1")      // inner join  spout2  on spout2.userId = spout1.key1
                    .join     ("spout3", "key3",    "spout2")      // inner join  spout3  on spout3.key3   = spout2.userId   
                    .leftJoin ("spout4", "key4",    "spout3")      // left join   spout4  on spout4.key4   = spout3.key3
                    .select  ("userId, key4, key2, spout3:key3")   // chose output fields
                    .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;

topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("spout1", new Fields("key1") )
            .fieldsGrouping("spout2", new Fields("userId") )
            .fieldsGrouping("spout3", new Fields("key3") )
            .fieldsGrouping("spout4", new Fields("key4") );

----------------------------------------

TITLE: Creating User Defined Function in Storm SQL
DESCRIPTION: SQL statement to create a user defined function (scalar) in Storm SQL, using a Java class implementation.

LANGUAGE: sql
CODE:
CREATE FUNCTION MYPLUS AS 'org.apache.storm.sql.TestUtils$MyPlus'

----------------------------------------

TITLE: Filtering Kafka Stream Example in Storm SQL
DESCRIPTION: SQL statements for filtering a Kafka stream of order transactions, calculating totals, and inserting large orders into another Kafka stream.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://orders?bootstrap-servers=localhost:9092,localhost:9093'
CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) LOCATION 'kafka://large_orders?bootstrap-servers=localhost:9092,localhost:9093' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Configuring EsState for Storm-Elasticsearch Trident Integration in Java
DESCRIPTION: This snippet demonstrates how to set up EsState for Trident integration with Elasticsearch. It requires an EsConfig object, an EsTupleMapper, and uses EsStateFactory to create a persistent state.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();

StateFactory factory = new EsStateFactory(esConfig, tupleMapper);
TridentState state = stream.partitionPersist(factory, esFields, new EsUpdater(), new Fields());

----------------------------------------

TITLE: Defining MongoDB Mapper Interface
DESCRIPTION: Core interface for mapping Storm tuples to MongoDB documents. Defines methods for converting tuples to documents and handling document keys.

LANGUAGE: java
CODE:
public interface MongoMapper extends Serializable {
    Document toDocument(ITuple tuple);
    Document toDocumentByKeys(List<Object> keys);
}

----------------------------------------

TITLE: Configuring Trident Topology with RedisState in Java
DESCRIPTION: Shows how to set up a Trident topology using RedisState for persistence and querying. This example uses custom mappers for storing and looking up word counts.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
                                .setHost(redisHost).setPort(redisPort)
                                .build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisLookupMapper lookupMapper = new WordCountLookupMapper();
RedisState.Factory factory = new RedisState.Factory(poolConfig);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory,
                        fields,
                        new RedisStateUpdater(storeMapper).withExpire(86400000),
                        new Fields());

TridentState state = topology.newStaticState(factory);
stream = stream.stateQuery(state, new Fields("word"),
                        new RedisStateQuerier(lookupMapper),
                        new Fields("columnName","columnValue"));

----------------------------------------

TITLE: Configuring MongoUpdateBolt for Storm-MongoDB Integration
DESCRIPTION: This code demonstrates how to configure and use the MongoUpdateBolt, which is used to update data in a MongoDB collection. It specifies the MongoDB URL, collection name, QueryFilterCreator, and MongoUpdateMapper implementations.

LANGUAGE: java
CODE:
MongoUpdateMapper mapper = new SimpleMongoUpdateMapper()
        .withFields("word", "count");

QueryFilterCreator updateQueryCreator = new SimpleQueryFilterCreator()
        .withField("word");

MongoUpdateBolt updateBolt = new MongoUpdateBolt(url, collectionName, updateQueryCreator, mapper);

//if a new document should be inserted if there are no matches to the query filter
//updateBolt.withUpsert(true);

//whether find all documents according to the query filter
//updateBolt.withMany(true);

----------------------------------------

TITLE: HDFS Spout Configuration Example in Java
DESCRIPTION: Example showing how to configure an HDFS spout to read text files with Kerberos authentication.

LANGUAGE: java
CODE:
HdfsSpout textReaderSpout = new HdfsSpout().setReaderType("text")
                                           .withOutputFields(TextFileReader.defaultFields)                                      
                                           .setHdfsUri("hdfs://localhost:54310")  
                                           .setSourceDir("/data/in")              
                                           .setArchiveDir("/data/done")           
                                           .setBadFilesDir("/data/badfiles");

HashMap hdfsSettings = new HashMap();
hdfsSettings.put("hdfs.keytab.file", "/path/to/keytab");
hdfsSettings.put("hdfs.kerberos.principal","user@EXAMPLE.com");

textReaderSpout.setHdfsClientSettings(hdfsSettings);

----------------------------------------

TITLE: Configuring Metric Reporters in YAML
DESCRIPTION: Example configuration for setting up Graphite and Console metric reporters in Storm's storm.yaml file. It shows how to specify reporter classes, daemons, reporting intervals, and metric filters.

LANGUAGE: yaml
CODE:
storm.metrics.reporters:
  # Graphite Reporter
  - class: "org.apache.storm.metrics2.reporters.GraphiteStormReporter"
    daemons:
        - "supervisor"
        - "nimbus"
        - "worker"
    report.period: 60
    report.period.units: "SECONDS"
    graphite.host: "localhost"
    graphite.port: 2003

  # Console Reporter
  - class: "org.apache.storm.metrics2.reporters.ConsoleStormReporter"
    daemons:
        - "worker"
    report.period: 10
    report.period.units: "SECONDS"
    filter:
        class: "org.apache.storm.metrics2.filters.RegexFilter"
        expression: ".*my_component.*emitted.*"

----------------------------------------

TITLE: Configuring Zookeeper Servers in Storm YAML
DESCRIPTION: This snippet shows how to configure the Zookeeper servers for a Storm cluster in the storm.yaml configuration file. It specifies the IP addresses of the Zookeeper servers.

LANGUAGE: yaml
CODE:
storm.zookeeper.servers:
  - "111.222.333.444"
  - "555.666.777.888"

----------------------------------------

TITLE: Defining Leader Election Interface in Java for Storm Nimbus
DESCRIPTION: Java interface defining methods for leader election among Nimbus instances. Includes functions for joining/leaving the leader queue, checking leadership status, and retrieving leader and participant addresses.

LANGUAGE: java
CODE:
public interface ILeaderElector {
    void addToLeaderLockQueue();
    void removeFromLeaderLockQueue();
    boolean isLeader();
    InetSocketAddress getLeaderAddress();
    List<InetSocketAddress> getAllNimbusAddresses();
}

----------------------------------------

TITLE: Value Mapping in Storm Streams
DESCRIPTION: Demonstrates how to use value mappers to extract fields from tuples and create typed streams.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

// extract the first field from the tuple to get a Stream<String> of sentences
Stream<String> sentences = builder.newStream(new TestWordSpout(), new ValueMapper<String>(0));

// extract first three fields of the tuple emitted by the spout to produce a stream of typed tuples.
Stream<Tuple3<String, Integer, Long>> stream = builder.newStream(new TestSpout(), TupleValueMappers.of(0, 1, 2));

----------------------------------------

TITLE: Setting Supervisor Slot Ports
DESCRIPTION: Configuration for defining available ports for worker processes on supervisor nodes.

LANGUAGE: yaml
CODE:
supervisor.slots.ports:
    - 6700
    - 6701
    - 6702
    - 6703

----------------------------------------

TITLE: Create External Table Example
DESCRIPTION: Example showing how to create an external table connected to Kafka

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-hosts=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Configuring Spring ApplicationContext for JMS Integration
DESCRIPTION: Spring XML configuration that defines JMS queue, topic, and connection factory for ActiveMQ integration. Sets up a notification queue and topic with their physical names, and configures a connection factory pointing to localhost:61616.

LANGUAGE: xml
CODE:
<?xml version="1.0" encoding="UTF-8"?>
<beans 
  xmlns="http://www.springframework.org/schema/beans" 
  xmlns:amq="http://activemq.apache.org/schema/core"
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.0.xsd
  http://activemq.apache.org/schema/core http://activemq.apache.org/schema/core/activemq-core.xsd">

    <amq:queue id="notificationQueue" physicalName="backtype.storm.contrib.example.queue" />
    
    <amq:topic id="notificationTopic" physicalName="backtype.storm.contrib.example.topic" />

    <amq:connectionFactory id="jmsConnectionFactory"
        brokerURL="tcp://localhost:61616" />
    
</beans>

----------------------------------------

TITLE: Configuring OpenTSDB Bolt in Storm Topology
DESCRIPTION: This snippet demonstrates how to set up and configure an OpenTSDB bolt in a Storm topology. It creates an OpenTsdbClient, configures the OpenTsdbBolt with a tuple mapper, and sets batch size and flush interval parameters.

LANGUAGE: java
CODE:
OpenTsdbClient.Builder builder =  OpenTsdbClient.newBuilder(openTsdbUrl).sync(30_000).returnDetails();
final OpenTsdbBolt openTsdbBolt = new OpenTsdbBolt(builder, Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
openTsdbBolt.withBatchSize(10).withFlushInterval(2000);
topologyBuilder.setBolt("opentsdb", openTsdbBolt).shuffleGrouping("metric-gen");

----------------------------------------

TITLE: Setting Batch Sizes for Message Processing in Storm
DESCRIPTION: Configures batch sizes for intra-worker and inter-worker message processing. Larger batch sizes can improve throughput at the cost of increased latency.

LANGUAGE: yaml
CODE:
topology.producer.batch.size: 100
topology.transfer.batch.size: 100

----------------------------------------

TITLE: Implementing Persistent Word Count Topology in Java
DESCRIPTION: A complete example of a Storm topology that performs word counting and persists results to HBase.

LANGUAGE: java
CODE:
public class PersistentWordCount {
    private static final String WORD_SPOUT = "WORD_SPOUT";
    private static final String COUNT_BOLT = "COUNT_BOLT";
    private static final String HBASE_BOLT = "HBASE_BOLT";


    public static void main(String[] args) throws Exception {
        Config config = new Config();

        Map<String, Object> hbConf = new HashMap<String, Object>();
        if(args.length > 0){
            hbConf.put("hbase.rootdir", args[0]);
        }
        config.put("hbase.conf", hbConf);

        WordSpout spout = new WordSpout();
        WordCounter bolt = new WordCounter();

        SimpleHBaseMapper mapper = new SimpleHBaseMapper()
                .withRowKeyField("word")
                .withColumnFields(new Fields("word"))
                .withCounterFields(new Fields("count"))
                .withColumnFamily("cf");

        HBaseBolt hbase = new HBaseBolt("WordCount", mapper)
                .withConfigKey("hbase.conf");


        // wordSpout ==> countBolt ==> HBaseBolt
        TopologyBuilder builder = new TopologyBuilder();

        builder.setSpout(WORD_SPOUT, spout, 1);
        builder.setBolt(COUNT_BOLT, bolt, 1).shuffleGrouping(WORD_SPOUT);
        builder.setBolt(HBASE_BOLT, hbase, 1).fieldsGrouping(COUNT_BOLT, new Fields("word"));

        String topoName = "test";
        if (args.length > 1) {
            topoName = args[1];
        }
        if (args.length == 4) {
            System.out.println("hdfs url: " + args[0] + ", keytab file: " + args[2] + 
                ", principal name: " + args[3] + ", toplogy name: " + topoName);
            hbConf.put(HBaseSecurityUtil.STORM_KEYTAB_FILE_KEY, args[2]);
            hbConf.put(HBaseSecurityUtil.STORM_USER_NAME_KEY, args[3]);
        } else if (args.length == 3 || args.length > 4) {
            System.out.println("Usage: PersistentWordCount <hbase.rootdir> [topology name] [keytab file] [principal name]");
            return;
        }
        config.setNumWorkers(3);
        StormSubmitter.submitTopology(topoName, config, builder.createTopology());
    }
}

----------------------------------------

TITLE: Storm Shell Command Usage Example
DESCRIPTION: Command line syntax for using the Storm shell command to package and submit a topology. Shows how to specify resources directory and Python script with arguments.

LANGUAGE: bash
CODE:
storm shell resources/ python3 topology.py arg1 arg2

----------------------------------------

TITLE: Implementing OpenTSDB Trident State in Storm
DESCRIPTION: Example demonstrating how to set up OpenTSDB integration using Trident state patterns. Shows configuration of state factory, topology creation, and stream processing with OpenTSDB state updater.

LANGUAGE: java
CODE:
final OpenTsdbStateFactory openTsdbStateFactory =
        new OpenTsdbStateFactory(OpenTsdbClient.newBuilder(tsdbUrl),
                Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
TridentTopology tridentTopology = new TridentTopology();

final Stream stream = tridentTopology.newStream("metric-tsdb-stream", new MetricGenSpout());

stream.peek(new Consumer() {
    @Override
    public void accept(TridentTuple input) {
        LOG.info("########### Received tuple: [{}]", input);
    }
}).partitionPersist(openTsdbStateFactory, MetricGenSpout.DEFAULT_METRIC_FIELDS, new OpenTsdbStateUpdater());

----------------------------------------

TITLE: Configuring and Creating JdbcInsertBolt in Java
DESCRIPTION: Example of how to configure and create a JdbcInsertBolt using HikariCP connection provider and SimpleJdbcMapper. It demonstrates setting up the connection, mapper, and bolt options.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);

String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withTableName("user")
                                    .withQueryTimeoutSecs(30);
                                    Or
JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withInsertQuery("insert into user values (?,?)")
                                    .withQueryTimeoutSecs(30);                                    

----------------------------------------

TITLE: Initializing DRPC Client in Java
DESCRIPTION: This code snippet demonstrates how to initialize a DRPC client with specific configuration settings, including retry parameters and transport plugin.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.put("storm.thrift.transport", "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin");
conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 3);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL, 10);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING, 20);
DRPCClient client = new DRPCClient(conf, "drpc-host", 3772);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Configuring Trident Topology with RedisClusterState
DESCRIPTION: This Java code snippet shows how to configure a Trident topology using RedisClusterState for Redis Cluster. It sets up the cluster connection, creates state factories, and configures state updates and queries for the cluster.

LANGUAGE: java
CODE:
Set<InetSocketAddress> nodes = new HashSet<InetSocketAddress>();
for (String hostPort : redisHostPort.split(",")) {
    String[] host_port = hostPort.split(":");
    nodes.add(new InetSocketAddress(host_port[0], Integer.valueOf(host_port[1])));
}
JedisClusterConfig clusterConfig = new JedisClusterConfig.Builder().setNodes(nodes)
                                .build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisLookupMapper lookupMapper = new WordCountLookupMapper();
RedisClusterState.Factory factory = new RedisClusterState.Factory(clusterConfig);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory,
                        fields,
                        new RedisClusterStateUpdater(storeMapper).withExpire(86400000),
                        new Fields());

TridentState state = topology.newStaticState(factory);
stream = stream.stateQuery(state, new Fields("word"),
                        new RedisClusterStateQuerier(lookupMapper),
                        new Fields("columnName","columnValue"));

----------------------------------------

TITLE: Complex Reach Topology Implementation
DESCRIPTION: Implementation of a complex DRPC topology for computing Twitter URL reach.

LANGUAGE: java
CODE:
public class PartialUniquer extends BaseBatchBolt {
    BatchOutputCollector _collector;
    Object _id;
    Set<String> _followers = new HashSet<String>();
    
    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
        _collector = collector;
        _id = id;
    }

    @Override
    public void execute(Tuple tuple) {
        _followers.add(tuple.getString(1));
    }
    
    @Override
    public void finishBatch() {
        _collector.emit(new Values(_id, _followers.size()));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "partial-count"));
    }
}

----------------------------------------

TITLE: Custom Redis Bolt Implementation
DESCRIPTION: Example of extending AbstractRedisBolt for custom Redis operations with lookup functionality.

LANGUAGE: java
CODE:
public static class LookupWordTotalCountBolt extends AbstractRedisBolt {
    private static final Logger LOG = LoggerFactory.getLogger(LookupWordTotalCountBolt.class);
    private static final Random RANDOM = new Random();

    public LookupWordTotalCountBolt(JedisPoolConfig config) {
        super(config);
    }

    public LookupWordTotalCountBolt(JedisClusterConfig config) {
        super(config);
    }

    @Override
    public void execute(Tuple input) {
        JedisCommands jedisCommands = null;
        try {
            jedisCommands = getInstance();
            String wordName = input.getStringByField("word");
            String countStr = jedisCommands.get(wordName);
            if (countStr != null) {
                int count = Integer.parseInt(countStr);
                this.collector.emit(new Values(wordName, count));

                if(RANDOM.nextInt(1000) > 995) {
                    LOG.info("Lookup result - word : " + wordName + " / count : " + count);
                }
            } else {
                LOG.warn("Word not found in Redis - word : " + wordName);
            }
        } finally {
            if (jedisCommands != null) {
                returnInstance(jedisCommands);
            }
            this.collector.ack(input);
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("wordName", "count"));
    }
}

----------------------------------------

TITLE: Retrieving Nimbus Summary in JSON (Storm UI API)
DESCRIPTION: GET request to /api/v1/nimbus/summary endpoint. Returns summary information for all Nimbus hosts in the Storm cluster, including status, uptime, and version.

LANGUAGE: json
CODE:
{
  "nimbuses":[
    {
      "host":"192.168.202.1",
      "port":6627,
      "nimbusLogLink":"http:\/\/192.168.202.1:8000\/log?file=nimbus.log",
      "status":"Leader",
      "version":"0.10.0-SNAPSHOT",
      "nimbusUpTime":"3m 33s",
      "nimbusUpTimeSeconds":"213"
    }
  ]
}

----------------------------------------

TITLE: Creating RedisFilterBolt Instance
DESCRIPTION: Java code snippet showing how to create a RedisFilterBolt instance using JedisPoolConfig and RedisFilterMapper.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisFilterMapper filterMapper = new BlacklistWordFilterMapper();
RedisFilterBolt filterBolt = new RedisFilterBolt(poolConfig, filterMapper);

----------------------------------------

TITLE: Initializing Storm Topology with Spouts and Bolts
DESCRIPTION: Example showing how to set up a basic Storm topology that reads sentences from Kestrel queue, splits them into words, and counts occurrences.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("sentences", new KestrelSpout("kestrel.backtype.com",
                                               22133,
                                               "sentence_queue",
                                               new StringScheme()));
builder.setBolt("split", new SplitSentence(), 10)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 20)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Running Storm JMS Example Topology
DESCRIPTION: Commands for starting ActiveMQ and running the example topology locally using Maven.

LANGUAGE: bash
CODE:
$ [ACTIVEMQ_HOME]/bin/activemq
$ mvn exec:java

----------------------------------------

TITLE: Implementing Custom State Interface
DESCRIPTION: This code shows how to implement a custom State interface for a location database, including methods for beginning and committing transactions, as well as setting and getting location data.

LANGUAGE: java
CODE:
public class LocationDB implements State {
    public void beginCommit(Long txid) {    
    }
    
    public void commit(Long txid) {    
    }
    
    public void setLocation(long userId, String location) {
      // code to access database and set location
    }
    
    public String getLocation(long userId) {
      // code to get location from database
    }
}

----------------------------------------

TITLE: Implementing Custom MQTT Tuple Mapper in Java
DESCRIPTION: Example implementation of MqttTupleMapper that creates an MQTT message from tuple fields userId, device, and message.

LANGUAGE: java
CODE:
public class MyTupleMapper implements MqttTupleMapper {
    public MqttMessage toMessage(ITuple tuple) {
        String topic = "users/" + tuple.getStringByField("userId") + "/" + tuple.getStringByField("device");
        byte[] payload = tuple.getStringByField("message").getBytes();
        return new MqttMessage(topic, payload);
    }
}

----------------------------------------

TITLE: Initializing EsLookupBolt for Storm-Elasticsearch Integration in Java
DESCRIPTION: This code demonstrates the setup of an EsLookupBolt for performing get requests to Elasticsearch. It requires an EsConfig, an ElasticsearchGetRequest adapter, and an EsLookupResultOutput to handle the response and define output fields.

LANGUAGE: java
CODE:
EsConfig esConfig = createEsConfig();
ElasticsearchGetRequest getRequestAdapter = createElasticsearchGetRequest();
EsLookupResultOutput output = createOutput();
EsLookupBolt lookupBolt = new EsLookupBolt(esConfig, getRequestAdapter, output);

----------------------------------------

TITLE: Setting CPU Requirements for Storm Components
DESCRIPTION: Java API examples showing how to set CPU requirements for spouts and bolts

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.setCPULoad(15.0);
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
            .shuffleGrouping("word").setCPULoad(10.0);
builder.setBolt("exclaim2", new HeavyBolt(), 1)
                .shuffleGrouping("exclaim1").setCPULoad(450.0);

----------------------------------------

TITLE: Implementing SplitSentence Bolt in Storm
DESCRIPTION: This code shows a Storm bolt implementation that splits sentences into words, demonstrating tuple anchoring and acknowledgment.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseRichBolt {
        OutputCollector _collector;
        
        public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
            _collector = collector;
        }

        public void execute(Tuple tuple) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                _collector.emit(tuple, new Values(word));
            }
            _collector.ack(tuple);
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Configuring Redis Lookup Bolt
DESCRIPTION: Configuration and instantiation of RedisLookupBolt using JedisPoolConfig and lookup mapper.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisLookupMapper lookupMapper = new WordCountRedisLookupMapper();
RedisLookupBolt lookupBolt = new RedisLookupBolt(poolConfig, lookupMapper);

----------------------------------------

TITLE: Implementing Distributed Query in Trident Topology
DESCRIPTION: Demonstrates how to set up a DRPC stream in a Trident topology to handle distributed queries for word counts, including state querying and aggregation.

LANGUAGE: java
CODE:
topology.newDRPCStream("words")
       .each(new Fields("args"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .stateQuery(wordCounts, new Fields("word"), new MapGet(), new Fields("count"))
       .each(new Fields("count"), new FilterNull())
       .aggregate(new Fields("count"), new Sum(), new Fields("sum"));

----------------------------------------

TITLE: Configuring Event Loggers in YAML for Apache Storm
DESCRIPTION: This YAML configuration snippet demonstrates how to register multiple event loggers in the storm.yaml config file. It includes both the default FileBasedEventLogger and a custom MyEventLogger with additional arguments.

LANGUAGE: yaml
CODE:
topology.event.logger.register:
  - class: "org.apache.storm.metric.FileBasedEventLogger"
  - class: "org.mycompany.MyEventLogger"
    arguments:
      endpoint: "event-logger.mycompany.org"

----------------------------------------

TITLE: Implementing RedisFilterMapper in Java
DESCRIPTION: Demonstrates how to create a custom RedisFilterMapper for use with RedisFilterBolt. This example filters words based on a blacklist stored in a Redis set.

LANGUAGE: java
CODE:
class BlacklistWordFilterMapper implements RedisFilterMapper {
    private RedisDataTypeDescription description;
    private final String setKey = "blacklist";

    public BlacklistWordFilterMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.SET, setKey);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Configuring URL Expansion with Fields Grouping in Storm
DESCRIPTION: Shows how to set up a bolt for URL expansion using fields grouping, ensuring the same URL always goes to the same task. This approach optimizes cache efficiency by preventing cache duplication across tasks.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .fieldsGrouping("urls", new Fields("url"));

----------------------------------------

TITLE: Initializing Storm LocalCluster in Java
DESCRIPTION: Creates an in-process Storm cluster using the LocalCluster class for development and testing purposes.

LANGUAGE: java
CODE:
import org.apache.storm.LocalCluster;

LocalCluster cluster = new LocalCluster();

----------------------------------------

TITLE: Running MQTT Broker and Publisher in Bash
DESCRIPTION: Command to start an MQTT broker on port 1883 and initialize a publisher for temperature/humidity data

LANGUAGE: bash
CODE:
java -cp examples/target/storm-mqtt-examples-*-SNAPSHOT.jar org.apache.storm.mqtt.examples.MqttBrokerPublisher

----------------------------------------

TITLE: Implementing User Defined Functions
DESCRIPTION: Example showing how to implement and register a user defined scalar function in Storm SQL

LANGUAGE: java
CODE:
public class MyPlus {
    public static Integer evaluate(Integer x, Integer y) {
      return x + y;
    }
  }

----------------------------------------

TITLE: Implementing User Defined Functions
DESCRIPTION: Example showing how to implement and register a user defined scalar function in Storm SQL

LANGUAGE: java
CODE:
public class MyPlus {
    public static Integer evaluate(Integer x, Integer y) {
      return x + y;
    }
  }

----------------------------------------

TITLE: Initializing DRPC Client in Java
DESCRIPTION: This snippet shows how to configure and create a DRPC client to execute remote function calls. It includes setting up transport security and retry parameters.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.put("storm.thrift.transport", "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin");
conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 3);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL, 10);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING, 20);
DRPCClient client = new DRPCClient(conf, "drpc-host", 3772);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Configuring Resource Consumption for Trident Topology in Java
DESCRIPTION: This code snippet demonstrates how to use the Trident RAS API to set resource defaults and specify CPU and memory consumption for different operations in a Trident topology. It shows setting parallelism hints, CPU load, and memory load for various stream operations.

LANGUAGE: java
CODE:
    TridentTopology topo = new TridentTopology();
    topo.setResourceDefaults(new DefaultResourceDeclarer();
                                                          .setMemoryLoad(128)
                                                          .setCPULoad(20));
    TridentState wordCounts =
        topology
            .newStream("words", feeder)
            .parallelismHint(5)
            .setCPULoad(20)
            .setMemoryLoad(512,256)
            .each( new Fields("sentence"),  new Split(), new Fields("word"))
            .setCPULoad(10)
            .setMemoryLoad(512)
            .each(new Fields("word"), new BangAdder(), new Fields("word!"))
            .parallelismHint(10)
            .setCPULoad(50)
            .setMemoryLoad(1024)
            .each(new Fields("word!"), new QMarkAdder(), new Fields("word!?"))
            .groupBy(new Fields("word!"))
            .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))
            .setCPULoad(100)
            .setMemoryLoad(2048);

----------------------------------------

TITLE: Kafka Table Configuration Example
DESCRIPTION: Example of creating an external table connected to Kafka stream

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Creating a Stream with Regular Storm Spout in Trident Topology
DESCRIPTION: This snippet demonstrates how to create a stream using a regular Storm IRichSpout in a Trident topology. It shows the initialization of a TridentTopology and the creation of a new stream with a unique identifier and a custom spout.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
topology.newStream("myspoutid", new MyRichSpout());

----------------------------------------

TITLE: Implementing PMML Model Scoring Interface
DESCRIPTION: Core interface method for computing scored tuples from raw input data in Storm PMML integration. Returns a map of stream IDs to lists of scored values.

LANGUAGE: java
CODE:
Map<String, List<Object>> scoredTuplePerStream(Tuple input);

----------------------------------------

TITLE: Configuring and Creating JdbcInsertBolt in Java
DESCRIPTION: This code snippet demonstrates how to configure and create a JdbcInsertBolt using HikariCP as the connection provider and SimpleJdbcMapper for mapping tuples to database rows. It shows two ways of specifying the insert operation: using a table name or an insert query.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);

String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withTableName("user")
                                    .withQueryTimeoutSecs(30);
                                    Or
JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withInsertQuery("insert into user values (?,?)")
                                    .withQueryTimeoutSecs(30);   

----------------------------------------

TITLE: Window Configuration Methods in Java
DESCRIPTION: Available window configuration methods for different types of windows including count-based and time-based sliding and tumbling windows.

LANGUAGE: java
CODE:
withWindow(Count windowLength, Count slidingInterval)
withWindow(Count windowLength)
withWindow(Count windowLength, Duration slidingInterval)
withWindow(Duration windowLength, Duration slidingInterval)
withWindow(Duration windowLength)
withWindow(Duration windowLength, Count slidingInterval)
withTumblingWindow(BaseWindowedBolt.Count count)
withTumblingWindow(BaseWindowedBolt.Duration duration)

----------------------------------------

TITLE: Implementing Word Split Function in Trident
DESCRIPTION: Defines a Split function that takes a sentence and emits individual words as tuples.

LANGUAGE: java
CODE:
public class Split extends BaseFunction {
   public void execute(TridentTuple tuple, TridentCollector collector) {
       String sentence = tuple.getString(0);
       for(String word: sentence.split(" ")) {
           collector.emit(new Values(word));                
       }
   }
}

----------------------------------------

TITLE: Initializing HBase State Provider in Java
DESCRIPTION: This Java code snippet shows how to initialize the HBase state provider programmatically in Storm. It sets up the necessary configuration options and specifies the HBase table and column family to use.

LANGUAGE: java
CODE:
Config conf = new Config();
    Map<String, Object> hbConf = new HashMap<String, Object>();
    hbConf.put("hbase.rootdir", "file:///tmp/hbase");
    conf.put("hbase.conf", hbConf);
    conf.put("topology.state.provider",  "org.apache.storm.hbase.state.HBaseKeyValueStateProvider");
    conf.put("topology.state.provider.config", "{" +
            "   \"hbaseConfigKey\": \"hbase.conf\"," +
            "   \"tableName\": \"state\"," +
            "   \"columnFamily\": \"cf\"" +
            " }");

----------------------------------------

TITLE: Configuring Mongo Trident MapState for Storm-MongoDB Integration
DESCRIPTION: Demonstrates how to configure and use Mongo Trident MapState for key-value storage in Trident topologies. It requires specifying the MongoDB URL, collection name, a MongoMapper, and a QueryFilterCreator implementation.

LANGUAGE: java
CODE:
MongoMapper mapper = new SimpleMongoMapper()
        .withFields("word", "count");

QueryFilterCreator filterCreator = new SimpleQueryFilterCreator()
        .withField("word");

MongoMapState.Options options = new MongoMapState.Options();
options.url = url;
options.collectionName = collectionName;
options.mapper = mapper;
options.queryCreator = filterCreator;

StateFactory factory = MongoMapState.transactional(options);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

TridentState state = stream.groupBy(new Fields("word"))
        .persistentAggregate(factory, new Fields("count"), new Sum(), new Fields("sum"));

stream.stateQuery(state, new Fields("word"), new MapGet(), new Fields("sum"))
        .each(new Fields("word", "sum"), new PrintFunction(), new Fields());

----------------------------------------

TITLE: Maven Assembly Plugin Configuration for Storm Topology
DESCRIPTION: Maven plugin configuration to package Storm topology code and dependencies into a single JAR. Creates an executable JAR with dependencies while excluding Storm core libraries.

LANGUAGE: xml
CODE:
  <plugin>
    <artifactId>maven-assembly-plugin</artifactId>
    <configuration>
      <descriptorRefs>  
        <descriptorRef>jar-with-dependencies</descriptorRef>
      </descriptorRefs>
      <archive>
        <manifest>
          <mainClass>com.path.to.main.Class</mainClass>
        </manifest>
      </archive>
    </configuration>
  </plugin>

----------------------------------------

TITLE: Implementing MQTT Message Mapper in Java
DESCRIPTION: Example implementation of MqttMessageMapper interface to convert MQTT messages to Storm tuples based on topic and payload content.

LANGUAGE: java
CODE:
public class CustomMessageMapper implements MqttMessageMapper {
    private static final Logger LOG = LoggerFactory.getLogger(CustomMessageMapper.class);

    public Values toValues(MqttMessage message) {
        String topic = message.getTopic();
        String[] topicElements = topic.split("/");
        String[] payloadElements = new String(message.getMessage()).split("/");

        return new Values(topicElements[2], topicElements[4], topicElements[3], Float.parseFloat(payloadElements[0]), 
                Float.parseFloat(payloadElements[1]));
    }

    public Fields outputFields() {
        return new Fields("user", "deviceId", "location", "temperature", "humidity");
    }
}

----------------------------------------

TITLE: Initializing EsIndexBolt for Elasticsearch Integration in Java
DESCRIPTION: This snippet demonstrates how to create an EsIndexBolt for streaming tuples directly into Elasticsearch. It requires an EsConfig for cluster configuration and an EsTupleMapper for mapping tuples to Elasticsearch documents.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsIndexBolt indexBolt = new EsIndexBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Setting Component Memory Requirements in Storm Topology
DESCRIPTION: API usage examples showing how to set memory requirements for spouts and bolts in a Storm topology.

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.setMemoryLoad(1024.0, 512.0);
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
            .shuffleGrouping("word").setMemoryLoad(512.0);

----------------------------------------

TITLE: Implementing Local DRPC Testing in Storm
DESCRIPTION: Demonstrates how to set up and test DRPC (Distributed Remote Procedure Call) functionality in local mode using LocalDRPC and LocalCluster.

LANGUAGE: java
CODE:
try (LocalDRPC drpc = new LocalDRPC();
     LocalCluster cluster = new LocalCluster();
     LocalTopology topo = cluster.submitTopology("drpc-demo", conf, builder.createLocalTopology(drpc))) {

    System.out.println("Results for 'hello':" + drpc.execute("exclamation", "hello"));
}

----------------------------------------

TITLE: Implementing Efficient QueryFunction with Bulk Operations in Trident
DESCRIPTION: Example of implementing an efficient QueryFunction (QueryLocation) that uses bulk operations of a custom State object (LocationDB) in a Trident topology.

LANGUAGE: java
CODE:
public class QueryLocation extends BaseQueryFunction<LocationDB, String> {
    public List<String> batchRetrieve(LocationDB state, List<TridentTuple> inputs) {
        List<Long> userIds = new ArrayList<Long>();
        for(TridentTuple input: inputs) {
            userIds.add(input.getLong(0));
        }
        return state.bulkGetLocations(userIds);
    }

    public void execute(TridentTuple tuple, String location, TridentCollector collector) {
        collector.emit(new Values(location));
    }    
}

----------------------------------------

TITLE: Defining Leader Election Interface in Java for Storm Nimbus
DESCRIPTION: Interface defining methods for leader election and cluster state management in Storm's Nimbus process. Includes methods for joining/leaving the leader queue, checking leadership status, and retrieving cluster information.

LANGUAGE: java
CODE:
public interface ILeaderElector {
    /**
     * queue up for leadership lock. The call returns immediately and the caller                     
     * must check isLeader() to perform any leadership action.
     */
    void addToLeaderLockQueue();

    /**
     * Removes the caller from the leader lock queue. If the caller is leader
     * also releases the lock.
     */
    void removeFromLeaderLockQueue();

    /**
     *
     * @return true if the caller currently has the leader lock.
     */
    boolean isLeader();

    /**
     *
     * @return the current leader's address , throws exception if noone has has    lock.
     */
    InetSocketAddress getLeaderAddress();

    /**
     * 
     * @return list of current nimbus addresses, includes leader.
     */
    List<InetSocketAddress> getAllNimbusAddresses();
}

----------------------------------------

TITLE: Implementing a Sliding Window Bolt in Storm
DESCRIPTION: Example implementation of a SlidingWindowBolt that extends BaseWindowedBolt. It demonstrates how to prepare the bolt, execute window operations, and emit results.

LANGUAGE: java
CODE:
public class SlidingWindowBolt extends BaseWindowedBolt {
	private OutputCollector collector;
	
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
    	this.collector = collector;
    }
	
    @Override
    public void execute(TupleWindow inputWindow) {
	  for(Tuple tuple: inputWindow.get()) {
	    // do the windowing computation
		...
	  }
	  // emit the results
	  collector.emit(new Values(computedValue));
    }
}

public static void main(String[] args) {
    TopologyBuilder builder = new TopologyBuilder();
     builder.setSpout("spout", new RandomSentenceSpout(), 1);
     builder.setBolt("slidingwindowbolt", 
                     new SlidingWindowBolt().withWindow(new Count(30), new Count(10)),
                     1).shuffleGrouping("spout");
    Config conf = new Config();
    conf.setDebug(true);
    conf.setNumWorkers(1);

    StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());
	
}

----------------------------------------

TITLE: Storm Spout Implementation
DESCRIPTION: Example of a Storm spout that emits random words periodically.

LANGUAGE: java
CODE:
public void nextTuple() {
    Utils.sleep(100);
    final String[] words = new String[] {"nathan", "mike", "jackson", "golda", "bertels"};
    final Random rand = new Random();
    final String word = words[rand.nextInt(words.length)];
    _collector.emit(new Values(word));
}

----------------------------------------

TITLE: Custom Redis Bolt Implementation
DESCRIPTION: Example of extending AbstractRedisBolt for custom Redis operations, implementing word count total lookup.

LANGUAGE: java
CODE:
public static class LookupWordTotalCountBolt extends AbstractRedisBolt {
    private static final Logger LOG = LoggerFactory.getLogger(LookupWordTotalCountBolt.class);
    private static final Random RANDOM = new Random();

    public LookupWordTotalCountBolt(JedisPoolConfig config) {
        super(config);
    }

    public LookupWordTotalCountBolt(JedisClusterConfig config) {
        super(config);
    }

    @Override
    public void execute(Tuple input) {
        JedisCommands jedisCommands = null;
        try {
            jedisCommands = getInstance();
            String wordName = input.getStringByField("word");
            String countStr = jedisCommands.get(wordName);
            if (countStr != null) {
                int count = Integer.parseInt(countStr);
                this.collector.emit(new Values(wordName, count));

                if(RANDOM.nextInt(1000) > 995) {
                    LOG.info("Lookup result - word : " + wordName + " / count : " + count);
                }
            } else {
                LOG.warn("Word not found in Redis - word : " + wordName);
            }
        } finally {
            if (jedisCommands != null) {
                returnInstance(jedisCommands);
            }
            this.collector.ack(input);
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("wordName", "count"));
    }
}

----------------------------------------

TITLE: Implementing Custom State Interface in Trident
DESCRIPTION: This example shows how to implement a custom State interface for a hypothetical LocationDB. It includes methods for beginning and committing transactions, as well as setting and getting location data.

LANGUAGE: java
CODE:
public class LocationDB implements State {
    public void beginCommit(Long txid) {    
    }
    
    public void commit(Long txid) {    
    }
    
    public void setLocation(long userId, String location) {
      // code to access database and set location
    }
    
    public String getLocation(long userId) {
      // code to get location from database
    }
}

----------------------------------------

TITLE: Deactivating Topology in JSON
DESCRIPTION: Sample response from the /api/v1/topology/<id>/deactivate POST endpoint, showing the result of deactivating a topology.

LANGUAGE: json
CODE:
{
  "topologyOperation":"deactivate",
  "topologyId":"wordcount-1-1420308665",
  "status":"success"
}

----------------------------------------

TITLE: Running a Storm Topology JAR
DESCRIPTION: Runs the main method of a specified class with arguments. Storm jars and configs are added to the classpath, and the topology jar is uploaded when submitted.

LANGUAGE: bash
CODE:
storm jar topology-jar-path class ...

----------------------------------------

TITLE: Insert Query with All Tuple Fields
DESCRIPTION: Demonstrates how to create a CassandraWriterBolt that includes all available tuple fields in the insert operation.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
    async(
        simpleQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);")
            .with( all() )
        )
);

----------------------------------------

TITLE: Executing Storm Jar Command
DESCRIPTION: The 'storm jar' command runs the main method of a specified class with given arguments. It configures the process to upload the jar when submitting the topology.

LANGUAGE: shell
CODE:
storm jar topology-jar-path class ...

----------------------------------------

TITLE: Retrieving Cluster Summary in Storm UI REST API
DESCRIPTION: GET request to retrieve summary information about the cluster, including number of supervisors, topologies, and resource utilization.

LANGUAGE: json
CODE:
{
  "stormVersion": "0.9.2-incubating-SNAPSHOT",
  "supervisors": 1,
  "slotsTotal": 4,
  "slotsUsed": 3,
  "slotsFree": 1,
  "executorsTotal": 28,
  "tasksTotal": 28,
  "schedulerDisplayResource": true,
  "totalMem": 4096.0,
  "totalCpu": 400.0,
  "availMem": 1024.0,
  "availCPU": 250.0,
  "memAssignedPercentUtil": 75.0,
  "cpuAssignedPercentUtil": 37.5
}

----------------------------------------

TITLE: Configuring Security Credentials
DESCRIPTION: Configuration example for setting up Kerberos authentication with HBase

LANGUAGE: java
CODE:
Config config = new Config();
...
config.put("storm.keytab.file", "$keytab");
config.put("storm.kerberos.principal", "$principal");
StormSubmitter.submitTopology("$topologyName", config, builder.createTopology());

----------------------------------------

TITLE: Configuring JdbcLookupBolt for Storm-JDBC Integration
DESCRIPTION: Example of configuring JdbcLookupBolt for executing select queries in a Storm topology. Shows how to set up connection provider, lookup mapper, and specify select query and timeout.

LANGUAGE: java
CODE:
String selectSql = "select user_name from user_details where user_id = ?";
SimpleJdbcLookupMapper lookupMapper = new SimpleJdbcLookupMapper(outputFields, queryParamColumns)
JdbcLookupBolt userNameLookupBolt = new JdbcLookupBolt(connectionProvider, selectSql, lookupMapper)
        .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Registering Metrics Consumer in Java
DESCRIPTION: Example of how to register a metrics consumer to a Storm topology using Java configuration.

LANGUAGE: java
CODE:
conf.registerMetricsConsumer(org.apache.storm.metric.LoggingMetricsConsumer.class, 1);

----------------------------------------

TITLE: Defining MongoDB Mapper Interface
DESCRIPTION: Interface definition for converting Storm tuples to MongoDB documents

LANGUAGE: java
CODE:
public interface MongoMapper extends Serializable {
    Document toDocument(ITuple tuple);
    Document toDocumentByKeys(List<Object> keys);
}

----------------------------------------

TITLE: JAAS Configuration for Storm and Zookeeper
DESCRIPTION: Example JAAS configuration file for Storm components and Zookeeper

LANGUAGE: plaintext
CODE:
StormServer {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/storm.keytab"
   storeKey=true
   useTicketCache=false
   principal="storm/storm.example.com@STORM.EXAMPLE.COM";
};
StormClient {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/storm.keytab"
   storeKey=true
   useTicketCache=false
   serviceName="storm"
   principal="storm@STORM.EXAMPLE.COM";
};
Client {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/storm.keytab"
   storeKey=true
   useTicketCache=false
   serviceName="zookeeper"
   principal="storm@STORM.EXAMPLE.COM";
};
Server {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/zk.keytab"
   storeKey=true
   useTicketCache=false
   serviceName="zookeeper"
   principal="zookeeper/zk1.example.com@STORM.EXAMPLE.COM";
};

----------------------------------------

TITLE: Defining a Prepared Bolt with State in Clojure
DESCRIPTION: Example of a prepared bolt that maintains state for word counting. It demonstrates using atom for mutable state and the bolt macro for implementation.

LANGUAGE: clojure
CODE:
(defbolt word-count ["word" "count"] {:prepare true}
  [conf context collector]
  (let [counts (atom {})]
    (bolt
     (execute [tuple]
       (let [word (.getString tuple 0)]
         (swap! counts (partial merge-with +) {word 1})
         (emit-bolt! collector [word (@counts word)] :anchor tuple)
         (ack! collector tuple)
         )))))

----------------------------------------

TITLE: Configuring UI/Logviewer Filter in YAML
DESCRIPTION: Sets up a servlet filter for authentication on the Storm UI and logviewer

LANGUAGE: yaml
CODE:
ui.filter: "filter.class"
ui.filter.params: "param1":"value1"
logviewer.filter: "filter.class"
logviewer.filter.params: "param1":"value1"

----------------------------------------

TITLE: Implementing SplitSentence as a BasicBolt in Storm
DESCRIPTION: This code snippet shows how to implement the SplitSentence functionality as a BasicBolt in Storm. BasicBolt simplifies the implementation by automatically handling anchoring and acknowledgment of tuples.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseBasicBolt {
        public void execute(Tuple tuple, BasicOutputCollector collector) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                collector.emit(new Values(word));
            }
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Creating External Table for Kafka in Storm SQL
DESCRIPTION: Shows an example of creating an external table that represents a Kafka spout and sink using the CREATE EXTERNAL TABLE statement.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Creating RedisStoreBolt Instance
DESCRIPTION: Java code snippet showing how to create a RedisStoreBolt instance using JedisPoolConfig and RedisStoreMapper.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
                .setHost(host).setPort(port).build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisStoreBolt storeBolt = new RedisStoreBolt(poolConfig, storeMapper);

----------------------------------------

TITLE: Implementing Streaming Top N Pattern in Storm
DESCRIPTION: Example showing how to implement a streaming top N pattern using parallel ranking and merging of results.

LANGUAGE: java
CODE:
builder.setBolt("rank", new RankObjects(), parallelism)
  .fieldsGrouping("objects", new Fields("value"));
builder.setBolt("merge", new MergeObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Redis Filter Bolt Configuration
DESCRIPTION: Configuration and initialization of RedisFilterBolt using JedisPoolConfig.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisFilterMapper filterMapper = new BlacklistWordFilterMapper();
RedisFilterBolt filterBolt = new RedisFilterBolt(poolConfig, filterMapper);

----------------------------------------

TITLE: Storm Transfer Queue Implementation
DESCRIPTION: Shows how Storm implements the transfer function for tasks to send tuples via a shared transfer queue

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L56

----------------------------------------

TITLE: Configuring HDFS Spout in Java
DESCRIPTION: Example of how to configure and set up an HDFS Spout to read text files from HDFS with specific directory and security settings.

LANGUAGE: java
CODE:
// Instantiate spout to read text files
HdfsSpout textReaderSpout = new HdfsSpout().setReaderType("text")
                                           .withOutputFields(TextFileReader.defaultFields)                                      
                                           .setHdfsUri("hdfs://localhost:54310")  // required
                                           .setSourceDir("/data/in")              // required                                      
                                           .setArchiveDir("/data/done")           // required
                                           .setBadFilesDir("/data/badfiles");     // required                                      
// If using Kerberos
HashMap hdfsSettings = new HashMap();
hdfsSettings.put("hdfs.keytab.file", "/path/to/keytab");
hdfsSettings.put("hdfs.kerberos.principal","user@EXAMPLE.com");

textReaderSpout.setHdfsClientSettings(hdfsSettings);

// Create topology
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("hdfsspout", textReaderSpout, SPOUT_NUM);

// Setup bolts and wire up topology
     ..snip..

// Submit topology with config
Config conf = new Config();
StormSubmitter.submitTopologyWithProgressBar("topologyName", conf, builder.createTopology());

----------------------------------------

TITLE: Multi-Anchoring Tuples in Storm
DESCRIPTION: This snippet demonstrates how to anchor an output tuple to multiple input tuples in Storm, useful for streaming joins or aggregations.

LANGUAGE: java
CODE:
List<Tuple> anchors = new ArrayList<Tuple>();
anchors.add(tuple1);
anchors.add(tuple2);
_collector.emit(anchors, new Values(1, 2, 3));

----------------------------------------

TITLE: Storm Jar Command Example
DESCRIPTION: Example of using the Storm jar command to submit a topology with external dependencies using jars and Maven artifacts.

LANGUAGE: bash
CODE:
./bin/storm jar example/storm-starter/storm-starter-topologies-*.jar org.apache.storm.starter.RollingTopWords blobstore-remote2 remote --jars "./external/storm-redis/storm-redis-1.1.0.jar,./external/storm-kafka-client/storm-kafka-client-1.1.0.jar" --artifacts "redis.clients:jedis:2.9.0,org.apache.kafka:kafka-clients:1.0.0^org.slf4j:slf4j-api" --artifactRepositories "jboss-repository^http://repository.jboss.com/maven2,HDPRepo^http://repo.hortonworks.com/content/groups/public/"

----------------------------------------

TITLE: Adding Standard HBase Column Example
DESCRIPTION: Example showing how to add a standard column to a ColumnList instance.

LANGUAGE: java
CODE:
ColumnList cols = new ColumnList();
cols.addColumn(this.columnFamily, field.getBytes(), toBytes(tuple.getValueByField(field)));

----------------------------------------

TITLE: Using StateUpdater in Trident Topology
DESCRIPTION: Example of using a custom StateUpdater (LocationUpdater) with partitionPersist in a Trident topology to update a source of state.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
TridentState locations = 
    topology.newStream("locations", locationsSpout)
        .partitionPersist(new LocationDBFactory(), new Fields("userid", "location"), new LocationUpdater())

----------------------------------------

TITLE: Configuring User Resource Pools
DESCRIPTION: YAML configuration for setting up user resource guarantees

LANGUAGE: yaml
CODE:
resource.aware.scheduler.user.pools:
    jerry:
        cpu: 1000
        memory: 8192.0
    derek:
        cpu: 10000.0
        memory: 32768
    bobby:
        cpu: 5000.0
        memory: 16384.0

----------------------------------------

TITLE: Implementing Storm Bolt with Output Fields
DESCRIPTION: Example of a Storm bolt that processes input tuples and emits output with declared fields for double and triple values.

LANGUAGE: java
CODE:
public class DoubleAndTripleBolt extends BaseRichBolt {
    private OutputCollectorBase _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollectorBase collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple input) {
        int val = input.getInteger(0);        
        _collector.emit(input, new Values(val*2, val*3));
        _collector.ack(input);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("double", "triple"));
    }    
}

----------------------------------------

TITLE: Configuring Storm Health Check Settings
DESCRIPTION: Configuration for health check directory location and timeout settings

LANGUAGE: yaml
CODE:
storm.health.check.dir: "healthchecks"

LANGUAGE: yaml
CODE:
storm.health.check.timeout.ms: 5000

----------------------------------------

TITLE: HDFS Spout Configuration in Java
DESCRIPTION: Example showing how to configure an HDFS spout to read text files from HDFS with Kerberos authentication.

LANGUAGE: java
CODE:
// Instantiate spout to read text files
HdfsSpout textReaderSpout = new HdfsSpout().setReaderType("text")
                                           .withOutputFields(TextFileReader.defaultFields)                                      
                                           .setHdfsUri("hdfs://localhost:54310")  // required
                                           .setSourceDir("/data/in")              // required                                      
                                           .setArchiveDir("/data/done")           // required
                                           .setBadFilesDir("/data/badfiles");     // required                                      
// If using Kerberos
HashMap hdfsSettings = new HashMap();
hdfsSettings.put("hdfs.keytab.file", "/path/to/keytab");
hdfsSettings.put("hdfs.kerberos.principal","user@EXAMPLE.com");

textReaderSpout.setHdfsClientSettings(hdfsSettings);

----------------------------------------

TITLE: Adding Standard HBase Columns
DESCRIPTION: Example of adding standard columns to a ColumnList for HBase operations.

LANGUAGE: java
CODE:
ColumnList cols = new ColumnList();
cols.addColumn(this.columnFamily, field.getBytes(), toBytes(tuple.getValueByField(field)));

----------------------------------------

TITLE: Launching Docker Container for Storm Worker
DESCRIPTION: Example docker run command executed by worker-launcher to start a Storm worker in a container. Shows volume mounts, security options, resource limits and other container configurations.

LANGUAGE: bash
CODE:
run --name=8198e1f0-f323-4b9d-8625-e4fd640cd058 \
--user=<uid>:<gid> \
-d \
--net=host \
--read-only \
-v /sys/fs/cgroup:/sys/fs/cgroup:ro \
-v /usr/share/apache-storm-2.3.0:/usr/share/apache-storm-2.3.0:ro \
-v /<storm-local-dir>/supervisor:/<storm-local-dir>/supervisor:ro \
-v /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058:/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
-v /<workers-artifacts-dir>/workers-artifacts/word-count-1-1591895933/6703:/<workers-artifacts-dir>/workers-artifacts/word-count-1-1591895933/6703 \
-v /<storm-local-dir>/workers-users/8198e1f0-f323-4b9d-8625-e4fd640cd058:/<storm-local-dir>/workers-users/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
-v /var/run/nscd:/var/run/nscd \
-v /<storm-local-dir>/supervisor/stormdist/word-count-1-1591895933/shared_by_topology:/<storm-local-dir>/supervisor/stormdist/word-count-1-1591895933/shared_by_topology \
-v /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/tmp:/tmp \
-v /etc/storm:/etc/storm:ro \
--cgroup-parent=/storm \
--group-add <gid> \
--workdir=/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
--cidfile=/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/container.cid \
--cap-drop=ALL \
--security-opt no-new-privileges \
--security-opt seccomp=/usr/share/apache-storm-2.3.0/conf/seccomp.json \
--cpus=2.6 xxx.xxx.com:8080/storm/storm/rhel7:latest \
bash /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/storm-worker-script.sh

----------------------------------------

TITLE: Using partitionPersist for State Updates
DESCRIPTION: Example of using the partitionPersist operation in a Trident topology to update a custom State (LocationDB) with new location information.

LANGUAGE: Java
CODE:
TridentTopology topology = new TridentTopology();
TridentState locations = 
    topology.newStream("locations", locationsSpout)
        .partitionPersist(new LocationDBFactory(), new Fields("userid", "location"), new LocationUpdater())

----------------------------------------

TITLE: Starting Worker Process in Storm (Clojure)
DESCRIPTION: The mk-worker function starts a worker process. It connects to other workers, monitors topology activity, and launches task threads.

LANGUAGE: clojure
CODE:
(mk-worker)

----------------------------------------

TITLE: Configuring Resource Consumption for Trident Topology in Java
DESCRIPTION: This snippet demonstrates how to use the Trident RAS API to set CPU and memory resources for various operations in a Trident topology. It shows setting default resources, and then overriding them for specific operations. The example includes stream creation, parallelism hints, and resource declarations for different processing steps.

LANGUAGE: java
CODE:
TridentTopology topo = new TridentTopology();
topo.setResourceDefaults(new DefaultResourceDeclarer()
                                                      .setMemoryLoad(128)
                                                      .setCPULoad(20));
TridentState wordCounts =
    topology
        .newStream("words", feeder)
        .parallelismHint(5)
        .setCPULoad(20)
        .setMemoryLoad(512,256)
        .each( new Fields("sentence"),  new Split(), new Fields("word"))
        .setCPULoad(10)
        .setMemoryLoad(512)
        .each(new Fields("word"), new BangAdder(), new Fields("word!"))
        .parallelismHint(10)
        .setCPULoad(50)
        .setMemoryLoad(1024)
        .each(new Fields("word!"), new QMarkAdder(), new Fields("word!?"))
        .groupBy(new Fields("word!"))
        .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))
        .setCPULoad(100)
        .setMemoryLoad(2048);

----------------------------------------

TITLE: Storm Jar Command Example
DESCRIPTION: Example of using the Storm jar command to submit a topology with external dependencies using jars and Maven artifacts.

LANGUAGE: bash
CODE:
./bin/storm jar example/storm-starter/storm-starter-topologies-*.jar org.apache.storm.starter.RollingTopWords blobstore-remote2 remote --jars "./external/storm-redis/storm-redis-1.1.0.jar,./external/storm-kafka-client/storm-kafka-client-1.1.0.jar" --artifacts "redis.clients:jedis:2.9.0,org.apache.kafka:kafka-clients:1.0.0^org.slf4j:slf4j-api" --artifactRepositories "jboss-repository^http://repository.jboss.com/maven2,HDPRepo^http://repo.hortonworks.com/content/groups/public/"

----------------------------------------

TITLE: Defining Bolt Component in Thrift
DESCRIPTION: Thrift definition for a bolt component in Storm, including ComponentObject and ComponentCommon structs. ComponentObject specifies the implementation, while ComponentCommon defines streams, configurations, and other metadata.

LANGUAGE: Thrift
CODE:
struct ComponentObject {
  1: binary serialized_java
  2: ShellComponent shell
  3: JavaObject java_object
}

struct ComponentCommon {
  1: map<string, StreamInfo> streams
  2: map<GlobalStreamId, Grouping> inputs
  3: i32 parallelism_hint
  4: JsonConfigs json_conf
}

----------------------------------------

TITLE: JAAS Configuration for Storm and Zookeeper
DESCRIPTION: Example JAAS configuration file for Storm components and Zookeeper

LANGUAGE: plaintext
CODE:
StormServer {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/storm.keytab"
   storeKey=true
   useTicketCache=false
   principal="storm/storm.example.com@STORM.EXAMPLE.COM";
};
StormClient {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/storm.keytab"
   storeKey=true
   useTicketCache=false
   serviceName="storm"
   principal="storm@STORM.EXAMPLE.COM";
};
Client {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/storm.keytab"
   storeKey=true
   useTicketCache=false
   serviceName="zookeeper"
   principal="storm@STORM.EXAMPLE.COM";
};
Server {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/zk.keytab"
   storeKey=true
   useTicketCache=false
   serviceName="zookeeper"
   principal="zookeeper/zk1.example.com@STORM.EXAMPLE.COM";
};

----------------------------------------

TITLE: Defining ComponentObject structure in Thrift for Storm
DESCRIPTION: This Thrift structure defines the ComponentObject union, which specifies how code for spouts or bolts can be represented in Storm. It includes options for serialized Java, shell components, and Java objects.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Maven Dependency Configuration for Flux
DESCRIPTION: XML configuration for adding Flux as a Maven dependency in a Storm project

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>flux-core</artifactId>
    <version>${storm.version}</version>
</dependency>

----------------------------------------

TITLE: Setting Memory and CPU Requirements for Storm Components
DESCRIPTION: Example showing how to set memory and CPU requirements for spouts and bolts in a Storm topology

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.setMemoryLoad(1024.0, 512.0);
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
            .shuffleGrouping("word").setMemoryLoad(512.0);

----------------------------------------

TITLE: Kafka Stream Filtering Example
DESCRIPTION: Complete example of filtering large orders from a Kafka stream

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://orders?bootstrap-servers=localhost:9092,localhost:9093'
CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) LOCATION 'kafka://large_orders?bootstrap-servers=localhost:9092,localhost:9093' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Creating RedisStoreBolt in Java
DESCRIPTION: Shows how to create a RedisStoreBolt using a JedisPoolConfig and a custom RedisStoreMapper.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
                .setHost(host).setPort(port).build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisStoreBolt storeBolt = new RedisStoreBolt(poolConfig, storeMapper);

----------------------------------------

TITLE: Initializing Local Storm Cluster in Java
DESCRIPTION: Creates and manages an in-process Storm cluster using LocalCluster class for testing topologies. The cluster is automatically closed when the try block ends.

LANGUAGE: java
CODE:
import org.apache.storm.LocalCluster;

...

try (LocalCluster cluster = new LocalCluster()) {
    //Interact with the cluster...
}

----------------------------------------

TITLE: Multi-Anchoring Tuples in Storm Bolt
DESCRIPTION: This snippet illustrates how to anchor an output tuple to multiple input tuples, useful for streaming joins or aggregations.

LANGUAGE: java
CODE:
List<Tuple> anchors = new ArrayList<Tuple>();
anchors.add(tuple1);
anchors.add(tuple2);
_collector.emit(anchors, new Values(1, 2, 3));

----------------------------------------

TITLE: Creating User-Defined Functions in Storm SQL
DESCRIPTION: Demonstrates how to create a user-defined function in Storm SQL using the CREATE FUNCTION statement and a Java class.

LANGUAGE: sql
CODE:
CREATE FUNCTION MYPLUS AS 'org.apache.storm.sql.TestUtils$MyPlus'

LANGUAGE: java
CODE:
public class MyPlus {
  public static Integer evaluate(Integer x, Integer y) {
    return x + y;
  }
}

----------------------------------------

TITLE: Implementing Streaming Top N Pattern in Storm
DESCRIPTION: Configuration for a streaming top N calculation using parallel processing with rank and merge bolts.

LANGUAGE: java
CODE:
builder.setBolt("rank", new RankObjects(), parallelism)
  .fieldsGrouping("objects", new Fields("value"));
builder.setBolt("merge", new MergeObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Implementing MqttTupleMapper Interface in Java
DESCRIPTION: Definition of the MqttTupleMapper interface used to map Storm tuples to MQTT messages for publishing.

LANGUAGE: java
CODE:
public interface MqttTupleMapper extends Serializable{

    MqttMessage toMessage(ITuple tuple);

}

----------------------------------------

TITLE: Configuring Kryo Serialization Registration in Storm YAML
DESCRIPTION: Example YAML configuration showing how to register custom types and serializers with Storm's Kryo serialization system. Demonstrates both default FieldsSerializer usage and custom serializer implementation.

LANGUAGE: yaml
CODE:
topology.kryo.register:
  - com.mycompany.CustomType1
  - com.mycompany.CustomType2: com.mycompany.serializer.CustomType2Serializer
  - com.mycompany.CustomType3

----------------------------------------

TITLE: Basic DRPC Client Configuration in Java
DESCRIPTION: Example of configuring and using a DRPC client to make remote procedure calls in Storm. Shows both direct configuration and using a preconfigured client.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.put("storm.thrift.transport", "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin");
conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 3);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL, 10);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING, 20);
DRPCClient client = new DRPCClient(conf, "drpc-host", 3772);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Executing Storm SQL Command
DESCRIPTION: Run the storm sql command to compile SQL statements into a Storm topology and submit it to the Storm cluster.

LANGUAGE: bash
CODE:
$ bin/storm sql <sql-file> <topo-name>

----------------------------------------

TITLE: Worker Connection Management in Storm
DESCRIPTION: Code responsible for refreshing worker connections and maintaining task-to-worker mappings. Called periodically or when ZooKeeper assignments change.

LANGUAGE: clojure
CODE:
refresh-connections

----------------------------------------

TITLE: Registering a Counter Metric in Java
DESCRIPTION: Example of registering a counter metric named 'myCounter' using the TopologyContext. The resulting metric name will include additional information such as topology ID, hostname, component ID, task ID, and worker port.

LANGUAGE: java
CODE:
Counter myCounter = topologyContext.registerCounter("myCounter");

----------------------------------------

TITLE: Configuring JDBC Insert Bolt Example
DESCRIPTION: Example showing how to configure and instantiate a JdbcInsertBolt using HikariCP connection provider and SimpleJdbcMapper.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);

String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withTableName("user")
                                    .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Configuring Storm Bolt Parallelism in Java
DESCRIPTION: Example showing how to configure a Storm bolt with specific executor and task settings. Sets up a GreenBolt with 2 executors and 4 tasks, using shuffle grouping from a blue-spout stream.

LANGUAGE: java
CODE:
topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

----------------------------------------

TITLE: Retrieving Cluster Summary in Storm UI API
DESCRIPTION: GET request to retrieve summary information about the Storm cluster, including version, number of supervisors, topologies, and resource utilization.

LANGUAGE: json
CODE:
{
  "stormVersion": "0.9.2-incubating-SNAPSHOT",
  "supervisors": 1,
  "slotsTotal": 4,
  "slotsUsed": 3,
  "slotsFree": 1,
  "executorsTotal": 28,
  "tasksTotal": 28,
  "schedulerDisplayResource": true,
  "totalMem": 4096.0,
  "totalCpu": 400.0,
  "availMem": 1024.0,
  "availCPU": 250.0,
  "memAssignedPercentUtil": 75.0,
  "cpuAssignedPercentUtil": 37.5
}

----------------------------------------

TITLE: Distributed Mode Message Implementation
DESCRIPTION: Implementation of distributed message passing using ZeroMQ and virtual ports for routing.

LANGUAGE: clojure
CODE:
org/apache/storm/messaging/zmq.clj

LANGUAGE: clojure
CODE:
zilch/virtual_port.clj

----------------------------------------

TITLE: Python Split Sentence Bolt Implementation
DESCRIPTION: Python implementation of a bolt that splits sentences into individual words.

LANGUAGE: python
CODE:
import storm

class SplitSentenceBolt(storm.BasicBolt):
    def process(self, tup):
        words = tup.values[0].split(" ")
        for word in words:
          storm.emit([word])

SplitSentenceBolt().run()

----------------------------------------

TITLE: Implementing Storm Event Logger Interface in Java
DESCRIPTION: Interface definition for IEventLogger that handles logging of topology events. Contains methods for preparation, logging events, and cleanup.

LANGUAGE: java
CODE:
/**
 * EventLogger interface for logging the event info to a sink like log file or db
 * for inspecting the events via UI for debugging.
 */
public interface IEventLogger {
    /**
    * Invoked during eventlogger bolt prepare.
    */
    void prepare(Map stormConf, Map<String, Object> arguments, TopologyContext context);

    /**
     * Invoked when the {@link EventLoggerBolt} receives a tuple from the spouts or bolts that has event logging enabled.
     *
     * @param e the event
     */
    void log(EventInfo e);

    /**
    * Invoked when the event logger bolt is cleaned up
    */
    void close();
}

----------------------------------------

TITLE: Building Event Hubs Storm Integration
DESCRIPTION: Maven command to build the project package

LANGUAGE: bash
CODE:
mvn clean package

----------------------------------------

TITLE: Initializing EsPercolateBolt for Storm-Elasticsearch Integration in Java
DESCRIPTION: This code shows how to set up an EsPercolateBolt for sending percolate requests to Elasticsearch. It uses an EsConfig for cluster settings and an EsTupleMapper to extract necessary information from input tuples.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsPercolateBolt percolateBolt = new EsPercolateBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Deploying Storm Topology Command
DESCRIPTION: Command line syntax for deploying a Storm topology jar with arguments to a cluster.

LANGUAGE: bash
CODE:
storm jar all-my-code.jar org.apache.storm.MyTopology arg1 arg2

----------------------------------------

TITLE: Configuring KafkaTridentSpoutOpaque in Trident Topology
DESCRIPTION: Example of configuring a KafkaTridentSpoutOpaque to consume data from Kafka topics in a Trident topology.

LANGUAGE: java
CODE:
final TridentTopology tridentTopology = new TridentTopology();
final Stream spoutStream = tridentTopology.newStream("kafkaSpout",
    new KafkaTridentSpoutOpaque<>(KafkaSpoutConfig.builder("127.0.0.1:" + port, Pattern.compile("topic.*")).build()))
      .parallelismHint(1)

----------------------------------------

TITLE: Kryo ConcurrentModificationException Stack Trace in Java
DESCRIPTION: This stack trace demonstrates a ConcurrentModificationException occurring during Kryo serialization in Storm. It's typically caused by emitting mutable objects as output tuples, which are being modified during serialization.

LANGUAGE: java
CODE:
java.lang.RuntimeException: java.util.ConcurrentModificationException
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:84)
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:55)
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:56)
	at org.apache.storm.disruptor$consume_loop_STAR_$fn__1597.invoke(disruptor.clj:67)
	at org.apache.storm.util$async_loop$fn__465.invoke(util.clj:377)
	at clojure.lang.AFn.run(AFn.java:24)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.util.ConcurrentModificationException
	at java.util.LinkedHashMap$LinkedHashIterator.nextEntry(LinkedHashMap.java:390)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:409)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:408)
	at java.util.HashMap.writeObject(HashMap.java:1016)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:959)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1480)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at org.apache.storm.serialization.SerializableSerializer.write(SerializableSerializer.java:21)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:554)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:77)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:472)
	at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:27)

----------------------------------------

TITLE: Implementing MqttMessageMapper Interface in Java
DESCRIPTION: Definition of the MqttMessageMapper interface used to map MQTT messages to Storm tuples.

LANGUAGE: java
CODE:
public interface MqttMessageMapper extends Serializable {

    Values toValues(MqttMessage message);

    Fields outputFields();
}

----------------------------------------

TITLE: Configuring CGroups for Apache Storm in cgconfig.conf
DESCRIPTION: Sample configuration for cgconfig.conf file to set up CGroups for Apache Storm. It defines mount points for various subsystems and creates a 'storm' group with specific permissions.

LANGUAGE: bash
CODE:
mount {
	cpuset	= /cgroup/cpuset;
	cpu	= /cgroup/storm_resources;
	cpuacct	= /cgroup/storm_resources;
	memory	= /cgroup/storm_resources;
	devices	= /cgroup/devices;
	freezer	= /cgroup/freezer;
	net_cls	= /cgroup/net_cls;
	blkio	= /cgroup/blkio;
}

group storm {
       perm {
               task {
                      uid = 500;
                      gid = 500;
               }
               admin {
                      uid = 500;
                      gid = 500;
               }
       }
       cpu {
       }
       memory {
       }
       cpuacct {
       }
}

----------------------------------------

TITLE: Registering Custom Counter in Storm Bolt
DESCRIPTION: Example implementation of a bolt that counts received tuples using Storm's metrics API. The bolt registers a counter metric and increments it for each tuple processed.

LANGUAGE: java
CODE:
public class TupleCountingBolt extends BaseRichBolt {
    private Counter tupleCounter;
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        this.tupleCounter = context.registerCounter("tupleCount");
    }

    @Override
    public void execute(Tuple input) {
        this.tupleCounter.inc();
    }
}

----------------------------------------

TITLE: Configuring and Using TridentKafkaStateFactory in a Trident Topology
DESCRIPTION: Example of setting up a TridentKafkaStateFactory to write data to Kafka from a Trident topology.

LANGUAGE: java
CODE:
Fields fields = new Fields("word", "count");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
        new Values("storm", "1"),
        new Values("trident", "1"),
        new Values("needs", "1"),
        new Values("javadoc", "1")
);
spout.setCycle(true);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

//set producer properties.
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

TridentKafkaStateFactory stateFactory = new TridentKafkaStateFactory()
        .withProducerProperties(props)
        .withKafkaTopicSelector(new DefaultTopicSelector("test"))
        .withTridentTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper("word", "count"));
stream.partitionPersist(stateFactory, fields, new TridentKafkaStateUpdater(), new Fields());

Config conf = new Config();
StormSubmitter.submitTopology("kafkaTridentTest", conf, topology.build());

----------------------------------------

TITLE: Defining Cluster Summary Thrift Structures
DESCRIPTION: Thrift structures for representing cluster summary information, including Nimbus details, to support high availability in the Storm API.

LANGUAGE: thrift
CODE:
struct ClusterSummary {
  1: required list<SupervisorSummary> supervisors;
  3: required list<TopologySummary> topologies;
  4: required list<NimbusSummary> nimbuses;
}

struct NimbusSummary {
  1: required string host;
  2: required i32 port;
  3: required i32 uptime_secs;
  4: required bool isLeader;
  5: required string version;
}

----------------------------------------

TITLE: Configuring HDFS Sequence File Bolt in Java
DESCRIPTION: Example of how to configure and use the SequenceFileBolt to write Storm data to HDFS sequence files with specific formats and policies.

LANGUAGE: java
CODE:
// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withExtension(".seq")
        .withPath("/data/");

// create sequence format instance.
DefaultSequenceFormat format = new DefaultSequenceFormat("timestamp", "sentence");

SequenceFileBolt bolt = new SequenceFileBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withSequenceFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy)
        .withCompressionType(SequenceFile.CompressionType.RECORD)
        .withCompressionCodec("deflate");

----------------------------------------

TITLE: Configuring JdbcInsertBolt with HikariCP
DESCRIPTION: Example showing how to configure and instantiate a JdbcInsertBolt using HikariCP connection pool with MySQL database settings.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);

String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withTableName("user")
                                    .withQueryTimeoutSecs(30);
                                    Or
JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withInsertQuery("insert into user values (?,?)")
                                    .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Configuring Health Check Directory in YAML
DESCRIPTION: This configuration specifies the directory where health check scripts are located for monitoring the health of supervisors.

LANGUAGE: yaml
CODE:
storm.health.check.dir: "healthchecks"

----------------------------------------

TITLE: Creating CassandraWriterBolt with Named Setters and Aliases in Java
DESCRIPTION: Demonstrates how to create a CassandraWriterBolt using named setters and aliases for binding values to a prepared statement.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
     async(
        boundQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (:ti, :ye, :pe, :ge, :tr);")
            .bind(
                field("ti"),as("title"),
                field("ye").as("year")),
                field("pe").as("performer")),
                field("ge").as("genre")),
                field("tr").as("tracks"))
            ).byNamedSetters()
     )
);

----------------------------------------

TITLE: Storm DRPC Client Example in Java
DESCRIPTION: Example code showing how to create and use a DRPC client to execute remote procedure calls in Storm. The code demonstrates configuring and using DRPCClient with proper resource handling.

LANGUAGE: java
CODE:
Config conf = new Config();
try (DRPCClient drpc = DRPCClient.getConfiguredClient(conf)) {
  //User the drpc client
  String result = drpc.execute(function, argument);
}

----------------------------------------

TITLE: Specifying DRPC Servers in Storm YAML
DESCRIPTION: This configuration is used to specify the DRPC servers in a Storm cluster, allowing workers to locate them.

LANGUAGE: yaml
CODE:
drpc.servers: ["111.222.333.44"]

----------------------------------------

TITLE: CGroup CPU Metrics Format
DESCRIPTION: JSON structure showing the format of CPU metrics reported by CGroups in Storm, including user and system CPU usage in milliseconds.

LANGUAGE: json
CODE:
{
   "CGroupCPU.user-ms": "number",
   "CGroupCPU.sys-ms": "number"
}

----------------------------------------

TITLE: Dequeuing and Removing Items from Kestrel Queue in Java
DESCRIPTION: This method dequeues items from a Kestrel queue and removes them. It iterates 12 times, dequeuing an item, processing it, and then acknowledging (removing) it from the queue.

LANGUAGE: java
CODE:
    private static void dequeueAndRemoveItems(KestrelClient kestrelClient, String queueName)
    throws IOException, ParseError
		 {
			for(int i=1; i<=12; i++){

				Item item = kestrelClient.dequeue(queueName);


				if(item==null){
					System.out.println("The queue (" + queueName + ") contains no items.");
				}
				else
				{
					int itemID = item._id;


					byte[] data = item._data;

					String receivedVal = new String(data);

					kestrelClient.ack(queueName, itemID);

					System.out.println("receivedItem=" + receivedVal);
				}
			}
	}

----------------------------------------

TITLE: Storm Topology Rebalance Command
DESCRIPTION: CLI command example for rebalancing a running Storm topology. Shows how to modify the number of worker processes and executors for specific components without restart.

LANGUAGE: bash
CODE:
storm rebalance mytopology -n 5 -e blue-spout=3 -e yellow-bolt=10

----------------------------------------

TITLE: Redis Store Mapper Implementation
DESCRIPTION: Implementation of RedisStoreMapper interface for storing word counts using Redis Hash data type.

LANGUAGE: java
CODE:
class WordCountStoreMapper implements RedisStoreMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountStoreMapper() {
        description = new RedisDataTypeDescription(
            RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return tuple.getStringByField("count");
    }
}

----------------------------------------

TITLE: Creating a Flux-Enabled Topology JAR with Maven
DESCRIPTION: XML snippet demonstrating how to configure the Maven shade plugin to create a fat JAR including Flux and topology dependencies.

LANGUAGE: xml
CODE:
<dependencies>
    <!-- Flux include -->
    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>flux-core</artifactId>
        <version>${storm.version}</version>
    </dependency>
    <!-- Flux Wrappers include -->
    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>flux-wrappers</artifactId>
        <version>${storm.version}</version>
    </dependency>

    <!-- add user dependencies here... -->

</dependencies>
<!-- create a fat jar that includes all dependencies -->
<build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-shade-plugin</artifactId>
            <version>1.4</version>
            <configuration>
                <createDependencyReducedPom>true</createDependencyReducedPom>
            </configuration>
            <executions>
                <execution>
                    <phase>package</phase>
                    <goals>
                        <goal>shade</goal>
                    </goals>
                    <configuration>
                        <transformers>
                            <transformer
                                    implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                            <transformer
                                    implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                <mainClass>org.apache.storm.flux.Flux</mainClass>
                            </transformer>
                        </transformers>
                    </configuration>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>

----------------------------------------

TITLE: DRPC Server Configuration in YAML
DESCRIPTION: YAML configuration for setting up DRPC servers in a Storm cluster.

LANGUAGE: yaml
CODE:
drpc.servers:
  - "drpc1.foo.com"
  - "drpc2.foo.com"
drpc.http.port: 8081
storm.thrift.transport: "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin"

----------------------------------------

TITLE: Initializing Trident Topology with Regular Storm Spout
DESCRIPTION: Example showing how to create a new stream in a Trident topology using a regular Storm IRichSpout. The spout requires a unique identifier that will be used for storing metadata in Zookeeper.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
topology.newStream("myspoutid", new MyRichSpout());

----------------------------------------

TITLE: Configuring FileConfigLoader in YAML
DESCRIPTION: Example YAML configuration for using FileConfigLoader, specifying the file URI.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "file:///path/to/my/config.yaml"

----------------------------------------

TITLE: Kafka Stream Filtering Example
DESCRIPTION: Complete example of filtering large orders from a Kafka stream using Storm SQL

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://orders?bootstrap-servers=localhost:9092,localhost:9093'
CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) LOCATION 'kafka://large_orders?bootstrap-servers=localhost:9092,localhost:9093' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Setting Component Memory and CPU Requirements
DESCRIPTION: Example of setting memory and CPU requirements for Storm spouts and bolts

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.setMemoryLoad(1024.0, 512.0);
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
            .shuffleGrouping("word").setMemoryLoad(512.0);

----------------------------------------

TITLE: Configuring JdbcInsertBolt in Java
DESCRIPTION: Example configuration of JdbcInsertBolt using HikariCP connection provider and SimpleJdbcMapper.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);

String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withTableName("user")
                                    .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Creating CassandraWriterBolt with Static Bound Query
DESCRIPTION: Shows how to create a CassandraWriterBolt using a static bound query to insert data into Cassandra.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
     async(
        boundQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);")
            .bind(all());
     )
);

----------------------------------------

TITLE: Event Hubs Configuration Properties
DESCRIPTION: Configuration properties required for connecting Storm spout to Azure Event Hubs, including authentication, namespace settings, and performance parameters

LANGUAGE: properties
CODE:
eventhubspout.username = [username: policy name in EventHubs Portal]
eventhubspout.password = [password: shared access key in EventHubs Portal]
eventhubspout.namespace = [namespace]
eventhubspout.entitypath = [entitypath]
eventhubspout.partitions.count = [partitioncount]

# if not provided, will use storm's zookeeper settings
# zookeeper.connectionstring=zookeeper0:2181,zookeeper1:2181,zookeeper2:2181

eventhubspout.checkpoint.interval = 10
eventhub.receiver.credits = 1024

----------------------------------------

TITLE: Implementing RedisStoreMapper in Java
DESCRIPTION: Java class implementation of RedisStoreMapper for storing word counts in Redis hash.

LANGUAGE: java
CODE:
class WordCountStoreMapper implements RedisStoreMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountStoreMapper() {
        description = new RedisDataTypeDescription(
            RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return tuple.getStringByField("count");
    }
}

----------------------------------------

TITLE: Initializing FixedBatchSpout in Java for Trident
DESCRIPTION: Creates a FixedBatchSpout that cycles through a set of sentences to produce a stream of data for Trident processing. This spout is used as an input source for the word count example.

LANGUAGE: java
CODE:
FixedBatchSpout spout = new FixedBatchSpout(new Fields("sentence"), 3,
               new Values("the cow jumped over the moon"),
               new Values("the man went to the store and bought some candy"),
               new Values("four score and seven years ago"),
               new Values("how many apples can you eat"));
spout.setCycle(true);

----------------------------------------

TITLE: Creating CassandraWriterBolt with Bound Statement from Configuration
DESCRIPTION: Shows how to create a CassandraWriterBolt using a bound statement loaded from Storm configuration.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
     boundQuery(named("insertIntoAlbum"))
        .bind(all());

----------------------------------------

TITLE: Configuring Storm Bolt Parallelism
DESCRIPTION: Example showing how to configure a Storm bolt with specific executor and task settings. Sets up a GreenBolt with 2 executors and 4 tasks, using shuffle grouping from a blue-spout.

LANGUAGE: java
CODE:
topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

----------------------------------------

TITLE: Configuring Remote Storm Cluster Connection in YAML
DESCRIPTION: YAML configuration file contents for ~/.storm/storm.yaml that specifies the Nimbus seed nodes for connecting to a remote Storm cluster. This configuration is required for remote topology deployment.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["123.45.678.890"]

----------------------------------------

TITLE: Defining IWindowedBolt Interface in Java
DESCRIPTION: Interface definition for bolts that need windowing support. The execute method is called for each window activation to process tuples in the current window.

LANGUAGE: java
CODE:
public interface IWindowedBolt extends IComponent {
    void prepare(Map stormConf, TopologyContext context, OutputCollector collector);
    /**
     * Process tuples falling within the window and optionally emit 
     * new tuples based on the tuples in the input window.
     */
    void execute(TupleWindow inputWindow);
    void cleanup();
}

----------------------------------------

TITLE: Registering Metrics Consumer in Java
DESCRIPTION: Example of registering a metrics consumer programmatically using the configuration object.

LANGUAGE: java
CODE:
conf.registerMetricsConsumer(org.apache.storm.metric.LoggingMetricsConsumer.class, 1);

----------------------------------------

TITLE: Prepared Storm Bolt with State in Clojure
DESCRIPTION: Implementation of a stateful word counting bolt that maintains counts in an atom, demonstrating prepared bolts and state management.

LANGUAGE: clojure
CODE:
(defbolt word-count ["word" "count"] {:prepare true}
  [conf context collector]
  (let [counts (atom {})]
    (bolt
     (execute [tuple]
       (let [word (.getString tuple 0)]
         (swap! counts (partial merge-with +) {word 1})
         (emit-bolt! collector [word (@counts word)] :anchor tuple)
         (ack! collector tuple)
         )))))

----------------------------------------

TITLE: Configuring HDFS SequenceFileBolt in Java
DESCRIPTION: Example of how to configure and use the SequenceFileBolt to write Storm data to HDFS sequence files with specific sync, rotation, and compression settings.

LANGUAGE: java
CODE:
// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withExtension(".seq")
        .withPath("/data/");

// create sequence format instance.
DefaultSequenceFormat format = new DefaultSequenceFormat("timestamp", "sentence");

SequenceFileBolt bolt = new SequenceFileBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withSequenceFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy)
        .withCompressionType(SequenceFile.CompressionType.RECORD)
        .withCompressionCodec("deflate");

----------------------------------------

TITLE: Implementing a Trident FlatMap Function
DESCRIPTION: Example of implementing a Trident FlatMap Function for one-to-many transformations.

LANGUAGE: java
CODE:
public class Split extends FlatMapFunction {
  @Override
  public Iterable<Values> execute(TridentTuple input) {
    List<Values> valuesList = new ArrayList<>();
    for (String word : input.getString(0).split(" ")) {
      valuesList.add(new Values(word));
    }
    return valuesList;
  }
}

----------------------------------------

TITLE: Registering Metrics Consumers in YAML Configuration
DESCRIPTION: Example of registering multiple metrics consumers in the Storm YAML configuration file. This registers both the LoggingMetricsConsumer and HttpForwardingMetricsConsumer with specified parallelism hints and arguments.

LANGUAGE: yaml
CODE:
topology.metrics.consumer.register:
  - class: "org.apache.storm.metric.LoggingMetricsConsumer"
    parallelism.hint: 1
  - class: "org.apache.storm.metric.HttpForwardingMetricsConsumer"
    parallelism.hint: 1
    argument: "http://example.com:8080/metrics/my-topology/"

----------------------------------------

TITLE: Simple Wordcount Topology in YAML
DESCRIPTION: YAML configuration defining a simple wordcount topology using Flux.

LANGUAGE: yaml
CODE:
name: "yaml-topology"
config:
  topology.workers: 1

# spout definitions
spouts:
  - id: "spout-1"
    className: "org.apache.storm.testing.TestWordSpout"
    parallelism: 1

# bolt definitions
bolts:
  - id: "bolt-1"
    className: "org.apache.storm.testing.TestWordCounter"
    parallelism: 1
  - id: "bolt-2"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1

#stream definitions
streams:
  - name: "spout-1 --> bolt-1" # name isn't used (placeholder for logging, UI, etc.)
    from: "spout-1"
    to: "bolt-1"
    grouping:
      type: FIELDS
      args: ["word"]

  - name: "bolt-1 --> bolt2"
    from: "bolt-1"
    to: "bolt-2"
    grouping:
      type: SHUFFLE

# worker hook definitions
workerHooks:
  - id: "base-worker-hook"
    className: "org.apache.storm.hooks.BaseWorkerHook"

----------------------------------------

TITLE: Overriding Local Mode in Java for IDE Debugging
DESCRIPTION: This snippet shows how to modify your main method to use LocalCluster.withLocalModeOverride, allowing for easier debugging within an IDE.

LANGUAGE: java
CODE:
public static void main(final String args[]) {
    LocalCluster.withLocalModeOverride(() -> originalMain(args), 10);
}

----------------------------------------

TITLE: Kafka Stream Filtering Example
DESCRIPTION: Complete example of filtering Kafka streams using Storm SQL

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://orders?bootstrap-servers=localhost:9092,localhost:9093'
CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) LOCATION 'kafka://large_orders?bootstrap-servers=localhost:9092,localhost:9093' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Implementing UpdateGlobalCount Bolt in Storm
DESCRIPTION: Implementation of a committer bolt that updates a global count in the database while maintaining transactional guarantees.

LANGUAGE: java
CODE:
public static class UpdateGlobalCount extends BaseTransactionalBolt implements ICommitter {
    TransactionAttempt _attempt;
    BatchOutputCollector _collector;

    int _sum = 0;

    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, TransactionAttempt attempt) {
        _collector = collector;
        _attempt = attempt;
    }

    @Override
    public void execute(Tuple tuple) {
        _sum+=tuple.getInteger(1);
    }

    @Override
    public void finishBatch() {
        Value val = DATABASE.get(GLOBAL_COUNT_KEY);
        Value newval;
        if(val == null || !val.txid.equals(_attempt.getTransactionId())) {
            newval = new Value();
            newval.txid = _attempt.getTransactionId();
            if(val==null) {
                newval.count = _sum;
            } else {
                newval.count = _sum + val.count;
            }
            DATABASE.put(GLOBAL_COUNT_KEY, newval);
        } else {
            newval = val;
        }
        _collector.emit(new Values(_attempt, newval.count));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "sum"));
    }
}

----------------------------------------

TITLE: Executing Storm SQL Command
DESCRIPTION: Command to compile SQL statements into a Storm topology and submit it to the Storm cluster. The sql-file contains SQL statements to be executed, and topo-name is the name of the topology.

LANGUAGE: bash
CODE:
$ bin/storm sql <sql-file> <topo-name>

----------------------------------------

TITLE: Registering Event Loggers in YAML Configuration
DESCRIPTION: Example of registering multiple event loggers in Storm's YAML configuration file, including a custom event logger with arguments.

LANGUAGE: yaml
CODE:
topology.event.logger.register:
  - class: "org.apache.storm.metric.FileBasedEventLogger"
  - class: "org.mycompany.MyEventLogger"
    arguments:
      endpoint: "event-logger.mycompany.org"

----------------------------------------

TITLE: Configuring Azure Event Hubs for Storm Topology
DESCRIPTION: Sample configuration properties for connecting the Storm spout to Azure Event Hubs. Includes settings for authentication, namespace, entity path, and performance tuning.

LANGUAGE: properties
CODE:
eventhubspout.username = [username: policy name in EventHubs Portal]
eventhubspout.password = [password: shared access key in EventHubs Portal]
eventhubspout.namespace = [namespace]
eventhubspout.entitypath = [entitypath]
eventhubspout.partitions.count = [partitioncount]

# if not provided, will use storm's zookeeper settings
# zookeeper.connectionstring=zookeeper0:2181,zookeeper1:2181,zookeeper2:2181

eventhubspout.checkpoint.interval = 10
eventhub.receiver.credits = 1024

----------------------------------------

TITLE: Initializing EsLookupBolt for Elasticsearch Queries
DESCRIPTION: Creates an EsLookupBolt instance to perform get requests to Elasticsearch with custom request and output handling.

LANGUAGE: java
CODE:
EsConfig esConfig = createEsConfig();
ElasticsearchGetRequest getRequestAdapter = createElasticsearchGetRequest();
EsLookupResultOutput output = createOutput();
EsLookupBolt lookupBolt = new EsLookupBolt(esConfig, getRequestAdapter, output);

----------------------------------------

TITLE: Configuring DRPC Servers in YAML
DESCRIPTION: This YAML configuration specifies the DRPC server locations, HTTP port, and Thrift transport settings for a Storm cluster.

LANGUAGE: yaml
CODE:
drpc.servers:
  - "drpc1.foo.com"
  - "drpc2.foo.com"
drpc.http.port: 8081
storm.thrift.transport: "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin"

----------------------------------------

TITLE: Configuring CassandraWriterBolt with Static Bound Query in Java
DESCRIPTION: Creates a CassandraWriterBolt using a static bound query to insert data into a Cassandra table.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
     async(
        boundQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);")
            .bind(all());
     )
);

----------------------------------------

TITLE: Configuring Isolation Scheduler in YAML
DESCRIPTION: This YAML configuration snippet demonstrates how to set up the Isolation Scheduler in Storm. It specifies the number of isolated machines allocated to different topologies.

LANGUAGE: yaml
CODE:
isolation.scheduler.machines: 
    "my-topology": 8
    "tiny-topology": 1
    "some-other-topology": 3

----------------------------------------

TITLE: Custom Redis Bolt Implementation
DESCRIPTION: Example of extending AbstractRedisBolt for custom Redis operations with word count lookup functionality.

LANGUAGE: java
CODE:
public static class LookupWordTotalCountBolt extends AbstractRedisBolt {
    private static final Logger LOG = LoggerFactory.getLogger(LookupWordTotalCountBolt.class);
    private static final Random RANDOM = new Random();

    public LookupWordTotalCountBolt(JedisPoolConfig config) {
        super(config);
    }

    public LookupWordTotalCountBolt(JedisClusterConfig config) {
        super(config);
    }

    @Override
    public void execute(Tuple input) {
        JedisCommands jedisCommands = null;
        try {
            jedisCommands = getInstance();
            String wordName = input.getStringByField("word");
            String countStr = jedisCommands.get(wordName);
            if (countStr != null) {
                int count = Integer.parseInt(countStr);
                this.collector.emit(new Values(wordName, count));

                // print lookup result with low probability
                if(RANDOM.nextInt(1000) > 995) {
                    LOG.info("Lookup result - word : " + wordName + " / count : " + count);
                }
            } else {
                // skip
                LOG.warn("Word not found in Redis - word : " + wordName);
            }
        } finally {
            if (jedisCommands != null) {
                returnInstance(jedisCommands);
            }
            this.collector.ack(input);
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        // wordName, count
        declarer.declare(new Fields("wordName", "count"));
    }
}

----------------------------------------

TITLE: Using partitionPersist for State Updates
DESCRIPTION: This code demonstrates how to use the partitionPersist operation in a Trident topology to update a custom state (LocationDB) with new location information.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
TridentState locations = 
    topology.newStream("locations", locationsSpout)
        .partitionPersist(new LocationDBFactory(), new Fields("userid", "location"), new LocationUpdater())

----------------------------------------

TITLE: Implementing Persistent Word Count Topology in Java
DESCRIPTION: This class demonstrates a complete Storm topology that performs word counting and persists results to HBase. It includes spout and bolt configurations, as well as HBase connection setup.

LANGUAGE: java
CODE:
public class PersistentWordCount {
    private static final String WORD_SPOUT = "WORD_SPOUT";
    private static final String COUNT_BOLT = "COUNT_BOLT";
    private static final String HBASE_BOLT = "HBASE_BOLT";


    public static void main(String[] args) throws Exception {
        Config config = new Config();

        Map<String, Object> hbConf = new HashMap<String, Object>();
        if(args.length > 0){
            hbConf.put("hbase.rootdir", args[0]);
        }
        config.put("hbase.conf", hbConf);

        WordSpout spout = new WordSpout();
        WordCounter bolt = new WordCounter();

        SimpleHBaseMapper mapper = new SimpleHBaseMapper()
                .withRowKeyField("word")
                .withColumnFields(new Fields("word"))
                .withCounterFields(new Fields("count"))
                .withColumnFamily("cf");

        HBaseBolt hbase = new HBaseBolt("WordCount", mapper)
                .withConfigKey("hbase.conf");


        // wordSpout ==> countBolt ==> HBaseBolt
        TopologyBuilder builder = new TopologyBuilder();

        builder.setSpout(WORD_SPOUT, spout, 1);
        builder.setBolt(COUNT_BOLT, bolt, 1).shuffleGrouping(WORD_SPOUT);
        builder.setBolt(HBASE_BOLT, hbase, 1).fieldsGrouping(COUNT_BOLT, new Fields("word"));

        String topoName = "test";
        if (args.length > 1) {
            topoName = args[1];
        }
        if (args.length == 4) {
            System.out.println("hdfs url: " + args[0] + ", keytab file: " + args[2] + 
                ", principal name: " + args[3] + ", toplogy name: " + topoName);
            hbConf.put(HBaseSecurityUtil.STORM_KEYTAB_FILE_KEY, args[2]);
            hbConf.put(HBaseSecurityUtil.STORM_USER_NAME_KEY, args[3]);
        } else if (args.length == 3 || args.length > 4) {
            System.out.println("Usage: PersistentWordCount <hbase.rootdir> [topology name] [keytab file] [principal name]");
            return;
        }
        config.setNumWorkers(3);
        StormSubmitter.submitTopology(topoName, config, builder.createTopology());
    }
}

----------------------------------------

TITLE: Implementing StateUpdater for Bulk Updates in Trident
DESCRIPTION: This snippet demonstrates how to implement a StateUpdater for bulk updates to a custom State (LocationDB). It processes a batch of tuples and updates the state accordingly.

LANGUAGE: java
CODE:
public class LocationUpdater extends BaseStateUpdater<LocationDB> {
    public void updateState(LocationDB state, List<TridentTuple> tuples, TridentCollector collector) {
        List<Long> ids = new ArrayList<Long>();
        List<String> locations = new ArrayList<String>();
        for(TridentTuple t: tuples) {
            ids.add(t.getLong(0));
            locations.add(t.getString(1));
        }
        state.setLocationsBulk(ids, locations);
    }
}

----------------------------------------

TITLE: Declaring Multiple Streams in Storm Spouts
DESCRIPTION: This snippet demonstrates how to declare and emit multiple streams in a Storm spout using OutputFieldsDeclarer and SpoutOutputCollector.

LANGUAGE: java
CODE:
declareStream(OutputFieldsDeclarer declarer) {
    declarer.declareStream("stream1", new Fields("field1", "field2"));
    declarer.declareStream("stream2", new Fields("field3", "field4"));
}

emit(SpoutOutputCollector collector) {
    collector.emit("stream1", new Values(value1, value2));
    collector.emit("stream2", new Values(value3, value4));
}

----------------------------------------

TITLE: Configuring Metric Reporters in Apache Storm
DESCRIPTION: YAML configuration example for setting up multiple metric reporters in Storm. This snippet demonstrates how to configure a Graphite reporter and a Console reporter with custom reporting intervals and filters.

LANGUAGE: yaml
CODE:
topology.metrics.reporters:
  # Graphite Reporter
  - class: "org.apache.storm.metrics2.reporters.GraphiteStormReporter"
    report.period: 60
    report.period.units: "SECONDS"
    graphite.host: "localhost"
    graphite.port: 2003

  # Console Reporter
  - class: "org.apache.storm.metrics2.reporters.ConsoleStormReporter"
    report.period: 10
    report.period.units: "SECONDS"
    filter:
        class: "org.apache.storm.metrics2.filters.RegexFilter"
        expression: ".*my_component.*emitted.*"

----------------------------------------

TITLE: Setting Supervisor Slot Ports in Storm YAML
DESCRIPTION: This YAML configuration defines the ports available for worker processes on each machine. It determines how many workers can run on a machine.

LANGUAGE: yaml
CODE:
supervisor.slots.ports:
    - 6700
    - 6701
    - 6702
    - 6703

----------------------------------------

TITLE: Defining and Querying Kafka Streams with Storm SQL
DESCRIPTION: Example showing how to create external tables mapped to Kafka streams and perform filtering and aggregation operations. The query filters orders where total value exceeds 50 and projects the results into a new stream.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://...' ...

CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) 'kafka://...' ...

INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Registering Event Logger in Java Configuration
DESCRIPTION: Example of registering a FileBasedEventLogger implementation in Storm topology configuration using Java.

LANGUAGE: java
CODE:
conf.registerEventLogger(org.apache.storm.metric.FileBasedEventLogger.class);

----------------------------------------

TITLE: Implementing Tuple Counting Bolt with Metrics
DESCRIPTION: Complete implementation of a bolt that counts incoming tuples using Storm's metrics system. Shows how to register and increment a counter in a bolt's lifecycle.

LANGUAGE: java
CODE:
public class TupleCountingBolt extends BaseRichBolt {
    private Counter tupleCounter;
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        this.tupleCounter = context.registerCounter("tupleCount");
    }

    @Override
    public void execute(Tuple input) {
        this.tupleCounter.inc();
    }
}

----------------------------------------

TITLE: Implementing Local DRPC with Storm
DESCRIPTION: Demonstrates how to run DRPC (Distributed Remote Procedure Call) in local mode using LocalDRPC and LocalCluster.

LANGUAGE: java
CODE:
try (LocalDRPC drpc = new LocalDRPC();
     LocalCluster cluster = new LocalCluster();
     LocalTopology topo = cluster.submitTopology("drpc-demo", conf, builder.createLocalTopology(drpc))) {

    System.out.println("Results for 'hello':" + drpc.execute("exclamation", "hello"));
}

----------------------------------------

TITLE: Configuring HDFS Bolt in Java
DESCRIPTION: Example of configuring an HDFS bolt to write pipe-delimited files to HDFS, with sync and rotation policies.

LANGUAGE: java
CODE:
// use "|" instead of "," for field delimiter
RecordFormat format = new DelimitedRecordFormat()
        .withFieldDelimiter("|");

// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withPath("/foo/");

HdfsBolt bolt = new HdfsBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withRecordFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy);

----------------------------------------

TITLE: Global Count Update Committer Bolt
DESCRIPTION: Implementation of a committer bolt that updates a global count in the database using transactional semantics.

LANGUAGE: java
CODE:
public static class UpdateGlobalCount extends BaseTransactionalBolt implements ICommitter {
    TransactionAttempt _attempt;
    BatchOutputCollector _collector;

    int _sum = 0;

    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, TransactionAttempt attempt) {
        _collector = collector;
        _attempt = attempt;
    }

    @Override
    public void execute(Tuple tuple) {
        _sum+=tuple.getInteger(1);
    }

    @Override
    public void finishBatch() {
        Value val = DATABASE.get(GLOBAL_COUNT_KEY);
        Value newval;
        if(val == null || !val.txid.equals(_attempt.getTransactionId())) {
            newval = new Value();
            newval.txid = _attempt.getTransactionId();
            if(val==null) {
                newval.count = _sum;
            } else {
                newval.count = _sum + val.count;
            }
            DATABASE.put(GLOBAL_COUNT_KEY, newval);
        } else {
            newval = val;
        }
        _collector.emit(new Values(_attempt, newval.count));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "sum"));
    }
}

----------------------------------------

TITLE: Using Trident API for Inserting Data into Cassandra
DESCRIPTION: Demonstrates how to use the Trident API to insert data into Cassandra using the state API.

LANGUAGE: java
CODE:
CassandraState.Options options = new CassandraState.Options(new CassandraContext());
CQLStatementTupleMapper insertTemperatureValues = boundQuery(
        "INSERT INTO weather.temperature(weather_station_id, weather_station_name, event_time, temperature) VALUES(?, ?, ?, ?)")
        .bind(with(field("weather_station_id"), field("name").as("weather_station_name"), field("event_time").now(), field("temperature")));
options.withCQLStatementTupleMapper(insertTemperatureValues);
CassandraStateFactory insertValuesStateFactory =  new CassandraStateFactory(options);
TridentState selectState = topology.newStaticState(selectWeatherStationStateFactory);
stream = stream.stateQuery(selectState, new Fields("weather_station_id"), new CassandraQuery(), new Fields("name"));
stream = stream.each(new Fields("name"), new PrintFunction(), new Fields("name_x"));
stream.partitionPersist(insertValuesStateFactory, new Fields("weather_station_id", "name", "event_time", "temperature"), new CassandraStateUpdater(), new Fields());

----------------------------------------

TITLE: Initializing DRPC Client in Java
DESCRIPTION: Code showing how to configure and create a DRPC client with retry settings and transport configuration.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.put("storm.thrift.transport", "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin");
conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 3);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL, 10);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING, 20);
DRPCClient client = new DRPCClient(conf, "drpc-host", 3772);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Registering Event Logger in Java for Apache Storm Topology
DESCRIPTION: This code snippet demonstrates how to register an event logger to a Storm topology using Java configuration. It uses the FileBasedEventLogger class as an example.

LANGUAGE: java
CODE:
conf.registerEventLogger(org.apache.storm.metric.FileBasedEventLogger.class);

----------------------------------------

TITLE: Implementing a Batch Bolt for Counting in Storm
DESCRIPTION: Definition of BatchCount bolt that extends BaseBatchBolt. It demonstrates how to implement prepare, execute, and finishBatch methods to process batches of tuples and emit partial counts.

LANGUAGE: java
CODE:
public static class BatchCount extends BaseBatchBolt {
    Object _id;
    BatchOutputCollector _collector;

    int _count = 0;

    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
        _collector = collector;
        _id = id;
    }

    @Override
    public void execute(Tuple tuple) {
        _count++;
    }

    @Override
    public void finishBatch() {
        _collector.emit(new Values(_id, _count));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "count"));
    }
}

----------------------------------------

TITLE: Using StateUpdater in Trident Topology
DESCRIPTION: Example of using a custom StateUpdater in a Trident topology to persist location updates.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
TridentState locations = 
    topology.newStream("locations", locationsSpout)
        .partitionPersist(new LocationDBFactory(), new Fields("userid", "location"), new LocationUpdater())

----------------------------------------

TITLE: Configuring RocketMQ Bolt
DESCRIPTION: Setup of RocketMqBolt for writing Storm tuples to RocketMQ topics, including mapper and selector configuration. Supports both synchronous and asynchronous message sending.

LANGUAGE: java
CODE:
        TupleToMessageMapper mapper = new FieldNameBasedTupleToMessageMapper("word", "count");
        TopicSelector selector = new DefaultTopicSelector(topic);

        properties = new Properties();
        properties.setProperty(RocketMqConfig.NAME_SERVER_ADDR, nameserverAddr);

        RocketMqBolt insertBolt = new RocketMqBolt()
                .withMapper(mapper)
                .withSelector(selector)
                .withProperties(properties);

----------------------------------------

TITLE: Initializing Fixed Batch Spout in Java for Trident
DESCRIPTION: Creates a FixedBatchSpout that cycles through a set of predefined sentences to produce an infinite stream of input data for the Trident topology.

LANGUAGE: java
CODE:
FixedBatchSpout spout = new FixedBatchSpout(new Fields("sentence"), 3,
               new Values("the cow jumped over the moon"),
               new Values("the man went to the store and bought some candy"),
               new Values("four score and seven years ago"),
               new Values("how many apples can you eat"));
spout.setCycle(true);

----------------------------------------

TITLE: Implementing a Tumbling Window in Trident
DESCRIPTION: Shows how to create a tumbling window based on tuple count or duration.

LANGUAGE: java
CODE:
public Stream tumblingWindow(int windowCount, WindowsStoreFactory windowStoreFactory,
                                      Fields inputFields, Aggregator aggregator, Fields functionFields);

public Stream tumblingWindow(BaseWindowedBolt.Duration windowDuration, WindowsStoreFactory windowStoreFactory,
                                     Fields inputFields, Aggregator aggregator, Fields functionFields);

----------------------------------------

TITLE: Initializing EsIndexBolt for Storm-Elasticsearch Integration in Java
DESCRIPTION: Sets up an EsIndexBolt to stream tuples from Storm into Elasticsearch. It requires an EsConfig with cluster details and an EsTupleMapper to extract necessary fields from input tuples.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsIndexBolt indexBolt = new EsIndexBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Defining a Prepared Bolt for Word Counting in Clojure
DESCRIPTION: Example of defining a prepared bolt that maintains state to count words using the defbolt macro with :prepare true.

LANGUAGE: clojure
CODE:
(defbolt word-count ["word" "count"] {:prepare true}
  [conf context collector]
  (let [counts (atom {})]
    (bolt
     (execute [tuple]
       (let [word (.getString tuple 0)]
         (swap! counts (partial merge-with +) {word 1})
         (emit-bolt! collector [word (@counts word)] :anchor tuple)
         (ack! collector tuple)
         )))))

----------------------------------------

TITLE: Implementing Windowed Bolt Interface in Java
DESCRIPTION: Interface definition for Storm bolts that require windowing support. Includes methods for preparation, tuple window execution, and cleanup.

LANGUAGE: java
CODE:
public interface IWindowedBolt extends IComponent {
    void prepare(Map stormConf, TopologyContext context, OutputCollector collector);
    /**
     * Process tuples falling within the window and optionally emit 
     * new tuples based on the tuples in the input window.
     */
    void execute(TupleWindow inputWindow);
    void cleanup();
}

----------------------------------------

TITLE: Configuring MongoDB Insert Bolt
DESCRIPTION: Example of configuring a bolt for inserting data into MongoDB

LANGUAGE: java
CODE:
String url = "mongodb://127.0.0.1:27017/test";
String collectionName = "wordcount";

MongoMapper mapper = new SimpleMongoMapper()
        .withFields("word", "count");

MongoInsertBolt insertBolt = new MongoInsertBolt(url, collectionName, mapper);

----------------------------------------

TITLE: Configuring MongoDB Insert Bolt
DESCRIPTION: Example of configuring MongoInsertBolt with connection URL, collection name, and mapper settings.

LANGUAGE: java
CODE:
String url = "mongodb://127.0.0.1:27017/test";
String collectionName = "wordcount";

MongoMapper mapper = new SimpleMongoMapper()
        .withFields("word", "count");

MongoInsertBolt insertBolt = new MongoInsertBolt(url, collectionName, mapper);

----------------------------------------

TITLE: Querying Word Counts with DRPC in Trident Java
DESCRIPTION: Implements a distributed query using DRPC to get the sum of counts for a list of words. It demonstrates how to use stateQuery to access persistent state and aggregate results.

LANGUAGE: java
CODE:
topology.newDRPCStream("words")
       .each(new Fields("args"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .stateQuery(wordCounts, new Fields("word"), new MapGet(), new Fields("count"))
       .each(new Fields("count"), new FilterNull())
       .aggregate(new Fields("count"), new Sum(), new Fields("sum"));

----------------------------------------

TITLE: Initializing Local Storm Cluster in Java
DESCRIPTION: Creates and manages an in-process Storm cluster using LocalCluster for testing. The cluster is automatically closed when exiting the try block scope.

LANGUAGE: java
CODE:
import org.apache.storm.LocalCluster;

...

try (LocalCluster cluster = new LocalCluster()) {
    //Interact with the cluster...
}

----------------------------------------

TITLE: Setting CPU Requirements for Storm Components
DESCRIPTION: Java API examples showing how to set CPU requirements for spouts and bolts

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.setCPULoad(15.0);
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
            .shuffleGrouping("word").setCPULoad(10.0);
builder.setBolt("exclaim2", new HeavyBolt(), 1)
                .shuffleGrouping("exclaim1").setCPULoad(450.0);

----------------------------------------

TITLE: Word Count Example Using Storm Streams
DESCRIPTION: Complete example showing how to implement a word count topology using Storm's Stream API with windowing and aggregation.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

builder
   // A stream of random sentences with two partitions
   .newStream(new RandomSentenceSpout(), new ValueMapper<String>(0), 2)
   // a two seconds tumbling window
   .window(TumblingWindows.of(Duration.seconds(2)))
   // split the sentences to words
   .flatMap(s -> Arrays.asList(s.split(" ")))
   // create a stream of (word, 1) pairs
   .mapToPair(w -> Pair.of(w, 1))
   // compute the word counts in the last two second window
   .countByKey()
   // print the results to stdout
   .print();

----------------------------------------

TITLE: Creating JdbcTridentState for Storm-JDBC Integration
DESCRIPTION: This code snippet demonstrates how to create a JdbcTridentState, which is a persistent state that can be used with Trident topologies. It shows configuration with a table name, JdbcMapper, and connection provider.

LANGUAGE: java
CODE:
JdbcState.Options options = new JdbcState.Options()
        .withConnectionProvider(connectionProvider)
        .withMapper(jdbcMapper)
        .withTableName("user_details")
        .withQueryTimeoutSecs(30);
JdbcStateFactory jdbcStateFactory = new JdbcStateFactory(options);

----------------------------------------

TITLE: Configuring EsState for Storm-Elasticsearch Trident Integration in Java
DESCRIPTION: This snippet shows how to set up an EsState for Trident topologies in Storm-Elasticsearch integration. It requires an EsConfig, EsTupleMapper, and uses EsStateFactory to create a persistent state.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();

StateFactory factory = new EsStateFactory(esConfig, tupleMapper);
TridentState state = stream.partitionPersist(factory, esFields, new EsUpdater(), new Fields());

----------------------------------------

TITLE: Implementing Streaming Top N Pattern in Storm
DESCRIPTION: This code demonstrates the streaming top N pattern in Storm. It uses a RankObjects bolt with fields grouping for parallel processing, followed by a MergeObjects bolt with global grouping to combine results.

LANGUAGE: java
CODE:
builder.setBolt("rank", new RankObjects(), parallelism)
  .fieldsGrouping("objects", new Fields("value"));
builder.setBolt("merge", new MergeObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Configuring IsolationScheduler in YAML
DESCRIPTION: This YAML configuration snippet demonstrates how to allocate isolated machines to specific topologies using the IsolationScheduler in Storm. It maps topology names to the number of dedicated machines they should receive.

LANGUAGE: yaml
CODE:
isolation.scheduler.machines: 
    "my-topology": 8
    "tiny-topology": 1
    "some-other-topology": 3

----------------------------------------

TITLE: Configuring IsolationScheduler in YAML
DESCRIPTION: This YAML configuration snippet demonstrates how to allocate isolated machines to specific topologies using the IsolationScheduler in Storm. It maps topology names to the number of dedicated machines they should receive.

LANGUAGE: yaml
CODE:
isolation.scheduler.machines: 
    "my-topology": 8
    "tiny-topology": 1
    "some-other-topology": 3

----------------------------------------

TITLE: Configuring NUMA Zones in Storm Supervisor
DESCRIPTION: YAML configuration for setting up NUMA zones in Storm supervisor, including core assignments, memory allocation, port assignments, and generic resource mapping.

LANGUAGE: yaml
CODE:
supervisor.numa.meta:
    "0": # Numa zone id
        numa.cores: # Cores in NUMA zone (can be determined by using the numastat command)
            - 0
            - 1
            - 2
            - 3
            - 4
            - 5
            - 12
            - 13
            - 14
            - 15
            - 16

        numa.generic.resources.map: # Generic Resources in the zone to be used for generic resource scheduling (optional)
            network.resource.units: 50.0

        numa.memory.mb: 42461 # Size of memory zone
        numa.ports: # Ports to be assigned to workers pinned to the NUMA zone (this may include ports not specified in SUPERVISOR_SLOTS_PORTS
            - 6700
            - 6701
            - 6702
            - 6703
            - 6704
            - 6705
            - 6706
            - 6707
            - 6708
            - 6709
            - 6710
            - 6711

----------------------------------------

TITLE: Configuring DelimitedRecordHiveMapper (Java)
DESCRIPTION: Java code examples showing different ways to configure DelimitedRecordHiveMapper, including mapping column fields and partition fields.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
    or
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withTimeAsPartitionField("YYYY/MM/DD");

----------------------------------------

TITLE: Configuring RedisStoreBolt in Java
DESCRIPTION: Java code snippet showing how to configure and create a RedisStoreBolt instance.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
                .setHost(host).setPort(port).build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisStoreBolt storeBolt = new RedisStoreBolt(poolConfig, storeMapper);

----------------------------------------

TITLE: Defining IWindowedBolt Interface for Storm Windowing
DESCRIPTION: The IWindowedBolt interface defines the methods required for implementing windowing support in Storm bolts. It includes prepare, execute, and cleanup methods, with execute being the main method for processing windowed tuples.

LANGUAGE: java
CODE:
public interface IWindowedBolt extends IComponent {
    void prepare(Map stormConf, TopologyContext context, OutputCollector collector);
    /**
     * Process tuples falling within the window and optionally emit 
     * new tuples based on the tuples in the input window.
     */
    void execute(TupleWindow inputWindow);
    void cleanup();
}

----------------------------------------

TITLE: Configuring Storm Metric Reporters
DESCRIPTION: YAML configuration example showing how to set up Graphite and Console metric reporters with custom reporting periods and filters.

LANGUAGE: yaml
CODE:
topology.metrics.reporters:
  # Graphite Reporter
  - class: "org.apache.storm.metrics2.reporters.GraphiteStormReporter"
    report.period: 60
    report.period.units: "SECONDS"
    graphite.host: "localhost"
    graphite.port: 2003

  # Console Reporter
  - class: "org.apache.storm.metrics2.reporters.ConsoleStormReporter"
    report.period: 10
    report.period.units: "SECONDS"
    filter:
        class: "org.apache.storm.metrics2.filters.RegexFilter"
        expression: ".*my_component.*emitted.*"

----------------------------------------

TITLE: Implementing a Map Function in Trident
DESCRIPTION: Example of a Map Function that converts words to uppercase.

LANGUAGE: java
CODE:
public class UpperCase extends MapFunction {
 @Override
 public Values execute(TridentTuple input) {
   return new Values(input.getString(0).toUpperCase());
 }
}

----------------------------------------

TITLE: Implementing Tuple Counting Bolt with Metrics
DESCRIPTION: Complete implementation of a bolt that counts incoming tuples using Storm's metrics system. Shows how to register and increment a counter in a bolt's lifecycle.

LANGUAGE: java
CODE:
public class TupleCountingBolt extends BaseRichBolt {
    private Counter tupleCounter;
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        this.tupleCounter = context.registerCounter("tupleCount");
    }

    @Override
    public void execute(Tuple input) {
        this.tupleCounter.inc();
    }
}

----------------------------------------

TITLE: Leader Election Interface Implementation in Java
DESCRIPTION: Interface definition for Storm's leader election mechanism, providing methods to manage leadership queuing, status checks, and cluster node information.

LANGUAGE: java
CODE:
public interface ILeaderElector {
    void addToLeaderLockQueue();
    void removeFromLeaderLockQueue();
    boolean isLeader();
    InetSocketAddress getLeaderAddress();
    List<InetSocketAddress> getAllNimbusAddresses();
}

----------------------------------------

TITLE: Implementing a Custom HDFS Partitioner in Java
DESCRIPTION: Example of how to implement a custom Partitioner interface to partition data into different HDFS directories based on tuple content.

LANGUAGE: java
CODE:
Partitioner partitoner = new Partitioner() {
            @Override
            public String getPartitionPath(Tuple tuple) {
                return Path.SEPARATOR + tuple.getStringByField("city");
            }
     };

----------------------------------------

TITLE: Configuring HiveOptions for HiveBolt (Java)
DESCRIPTION: Java code snippet demonstrating how to configure HiveOptions for use with HiveBolt, including settings for transactions per batch, batch size, and idle timeout.

LANGUAGE: java
CODE:
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
                                .withTxnsPerBatch(10)
                				.withBatchSize(1000)
                	     		.withIdleTimeout(10)

----------------------------------------

TITLE: Debugging Storm Topology in IDE
DESCRIPTION: Shows how to override local mode execution for debugging purposes within an IDE environment using LocalCluster.withLocalModeOverride method.

LANGUAGE: java
CODE:
public static void main(final String args[]) {
    LocalCluster.withLocalModeOverride(() -> originalMain(args), 10);
}

----------------------------------------

TITLE: Configuring Storm Metric Reporters
DESCRIPTION: YAML configuration example showing how to set up Graphite and Console metric reporters with custom reporting periods and filters.

LANGUAGE: yaml
CODE:
topology.metrics.reporters:
  # Graphite Reporter
  - class: "org.apache.storm.metrics2.reporters.GraphiteStormReporter"
    report.period: 60
    report.period.units: "SECONDS"
    graphite.host: "localhost"
    graphite.port: 2003

  # Console Reporter
  - class: "org.apache.storm.metrics2.reporters.ConsoleStormReporter"
    report.period: 10
    report.period.units: "SECONDS"
    filter:
        class: "org.apache.storm.metrics2.filters.RegexFilter"
        expression: ".*my_component.*emitted.*"

----------------------------------------

TITLE: Viewing OCI Image Manifest JSON
DESCRIPTION: JSON content of an OCI image manifest file stored in HDFS.

LANGUAGE: json
CODE:
{
  "schemaVersion": 2,
  "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
  "config": {
    "mediaType": "application/vnd.docker.container.image.v1+json",
    "size": 7877,
    "digest": "sha256:ef1ff2c7167a1a6cd01e106f51b84a4d400611ba971c53cbc28de7919515ca4e"
  },
  "layers": [
    {
      "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
      "size": 26858854,
      "digest": "sha256:3692c3483ef6516fba685b316448e8aaf0fc10bb66818116edc8e5e6800076c7"
    },
    {
      "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
      "size": 123300113,
      "digest": "sha256:ea067172a7138f035d89a5c378db6d66c1581d98b0497b21f256e04c3d2b5303"
    },
    {
      "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
      "size": 12927624,
      "digest": "sha256:1b73e9433ecca0a6bb152bd7525f2b7c233484d51c24f8a6ba483d5cfd3035dc"
    },
    {
      "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
      "size": 567401434,
      "digest": "sha256:18ee671016a1bf3ecab07395d93c2cbecd352d59c497a1551e2074d64e1098d9"
    },
    {
      "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
      "size": 85748864,
      "digest": "sha256:152ee1d2cccea9dfe6393d2bdf9d077b67616b2b417b25eb74fc5ffaadcb96f5"
    },
    {
      "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
      "size": 186,
      "digest": "sha256:344224962010c03c9ca1f11a9bff0dfcc296ac46d0a55e4ff30a0ad13b9817af"
    },
    {
      "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
      "size": 156,
      "digest": "sha256:8710a3d72f75b45c48ab6b9b67eb6d77caea3dac91a0c30e0831f591cba4887e"
    }
  ]
}

----------------------------------------

TITLE: Registering Metrics Consumer in Java Configuration
DESCRIPTION: Shows how to register a metrics consumer to a topology using Java configuration. This allows listening and handling topology metrics.

LANGUAGE: java
CODE:
conf.registerMetricsConsumer(org.apache.storm.metric.LoggingMetricsConsumer.class, 1);

----------------------------------------

TITLE: Defining and Querying Kafka Streams with Storm SQL
DESCRIPTION: This SQL snippet demonstrates how to create external tables representing Kafka streams and perform a filtering and projection query. It creates two tables, ORDERS and LARGE_ORDERS, and inserts filtered data from ORDERS into LARGE_ORDERS based on a condition.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://...' ...

CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) 'kafka://...' ...

INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Storm Topology with KestrelSpout
DESCRIPTION: Storm topology configuration that reads from a Kestrel queue using KestrelSpout, splits sentences into words, and counts word occurrences.

LANGUAGE: java
CODE:
    TopologyBuilder builder = new TopologyBuilder();
    builder.setSpout("sentences", new KestrelSpout("localhost",22133,"sentence_queue",new StringScheme()));
    builder.setBolt("split", new SplitSentence(), 10)
    	        .shuffleGrouping("sentences");
    builder.setBolt("count", new WordCount(), 20)
	        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Creating and Querying Kafka Tables in Storm SQL
DESCRIPTION: Example showing how to create external tables mapped to Kafka streams and perform filtering and aggregation operations. The query filters orders where total amount exceeds 50 and writes results to another Kafka topic.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://...' ...

CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) 'kafka://...' ...

INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Initializing EsLookupBolt in Java
DESCRIPTION: Creates an EsLookupBolt instance for performing get requests to Elasticsearch. It requires an EsConfig, ElasticsearchGetRequest, and EsLookupResultOutput to handle the request and response processing.

LANGUAGE: java
CODE:
EsConfig esConfig = createEsConfig();
ElasticsearchGetRequest getRequestAdapter = createElasticsearchGetRequest();
EsLookupResultOutput output = createOutput();
EsLookupBolt lookupBolt = new EsLookupBolt(esConfig, getRequestAdapter, output);

----------------------------------------

TITLE: Configuring MongoTridentState for Storm-MongoDB Integration
DESCRIPTION: This snippet shows how to create a MongoDB persistent trident state for use with trident topologies. It initializes the state with a MongoDB URL, collection name, and MongoMapper instance.

LANGUAGE: java
CODE:
MongoMapper mapper = new SimpleMongoMapper()
        .withFields("word", "count");

MongoState.Options options = new MongoState.Options()
        .withUrl(url)
        .withCollectionName(collectionName)
        .withMapper(mapper);

StateFactory factory = new MongoStateFactory(options);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory, fields,  new MongoStateUpdater(), new Fields());

----------------------------------------

TITLE: Storm Supervisor GPU Resource Example
DESCRIPTION: Example YAML configuration showing how to specify GPU resources available on a Storm supervisor node.

LANGUAGE: yaml
CODE:
supervisor.resources.map: {"gpu.count" : 2.0}

----------------------------------------

TITLE: Local Mode Message Routing
DESCRIPTION: Implementation of direct tuple routing to in-memory queues for local mode operation.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/local.clj#L21

----------------------------------------

TITLE: Creating SimpleHBaseMapper in Java
DESCRIPTION: Example of creating a SimpleHBaseMapper instance for mapping Storm tuples to HBase columns.

LANGUAGE: java
CODE:
SimpleHBaseMapper mapper = new SimpleHBaseMapper() 
        .withRowKeyField("word")
        .withColumnFields(new Fields("word"))
        .withCounterFields(new Fields("count"))
        .withColumnFamily("cf");

----------------------------------------

TITLE: Adding Storm-Redis Dependency in Maven
DESCRIPTION: XML snippet showing how to include storm-redis as a Maven dependency in a project.

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>storm-redis</artifactId>
    <version>${storm.version}</version>
    <type>jar</type>
</dependency>

----------------------------------------

TITLE: Setting Up Trident State for Querying Data from Cassandra in Java
DESCRIPTION: Shows how to configure Trident state for querying data from Cassandra. It includes setting up options, creating a CQLStatementTupleMapper for the SELECT query, and configuring the stream for data retrieval.

LANGUAGE: java
CODE:
CassandraState.Options options = new CassandraState.Options(new CassandraContext());
CQLStatementTupleMapper insertTemperatureValues = boundQuery("SELECT name FROM weather.station WHERE id = ?")
         .bind(with(field("weather_station_id").as("id")));
options.withCQLStatementTupleMapper(insertTemperatureValues);
options.withCQLResultSetValuesMapper(new TridentResultSetValuesMapper(new Fields("name")));
CassandraStateFactory selectWeatherStationStateFactory =  new CassandraStateFactory(options);
CassandraStateFactory selectWeatherStationStateFactory = getSelectWeatherStationStateFactory();
TridentState selectState = topology.newStaticState(selectWeatherStationStateFactory);
stream = stream.stateQuery(selectState, new Fields("weather_station_id"), new CassandraQuery(), new Fields("name"));

----------------------------------------

TITLE: Creating Kerberos Principals and Keytabs
DESCRIPTION: Bash commands for creating Kerberos principals and generating keytabs for Zookeeper, Nimbus, DRPC, UI, logviewer, and supervisors.

LANGUAGE: bash
CODE:
# Zookeeper (Will need one of these for each box in the Zk ensemble)
sudo kadmin.local -q 'addprinc zookeeper/zk1.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/zk.keytab  zookeeper/zk1.example.com@STORM.EXAMPLE.COM"
# Nimbus and DRPC
sudo kadmin.local -q 'addprinc storm/storm.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm/storm.example.com@STORM.EXAMPLE.COM"
# All UI logviewer and Supervisors
sudo kadmin.local -q 'addprinc storm@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm@STORM.EXAMPLE.COM"

----------------------------------------

TITLE: Defining TopicSelector Interface in Java for Storm-RocketMQ
DESCRIPTION: This snippet shows the TopicSelector interface definition, used for selecting RocketMQ topics and tags based on Storm tuples. It includes methods for getting topic and tag from a tuple.

LANGUAGE: java
CODE:
public interface TopicSelector extends Serializable {
    String getTopic(ITuple tuple);
    String getTag(ITuple tuple);
}

----------------------------------------

TITLE: Defining HBaseMapper Interface in Java
DESCRIPTION: This snippet defines the HBaseMapper interface, which is the main API for interacting with HBase in Storm. It includes methods for generating row keys and defining columns from Storm tuples.

LANGUAGE: java
CODE:
public interface HBaseMapper extends Serializable {
    byte[] rowKey(Tuple tuple);

    ColumnList columns(Tuple tuple);
}

----------------------------------------

TITLE: Implementing Generic Resources in Storm Spout
DESCRIPTION: Example showing how to add GPU resource requirements to a Storm Spout using the addResource API.

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.addResouce("gpu.count", 1.0);

----------------------------------------

TITLE: Implementing RedisStoreMapper for Word Count Storage
DESCRIPTION: This Java class implements RedisStoreMapper for storing word counts in Redis. It uses a HASH data type and defines how to extract keys and values from tuples.

LANGUAGE: java
CODE:
class WordCountStoreMapper implements RedisStoreMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountStoreMapper() {
        description = new RedisDataTypeDescription(
            RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return tuple.getStringByField("count");
    }
}

----------------------------------------

TITLE: Custom Redis Bolt Implementation
DESCRIPTION: Custom implementation of AbstractRedisBolt for looking up word total counts from Redis.

LANGUAGE: java
CODE:
public static class LookupWordTotalCountBolt extends AbstractRedisBolt {
    private static final Logger LOG = LoggerFactory.getLogger(LookupWordTotalCountBolt.class);
    private static final Random RANDOM = new Random();

    public LookupWordTotalCountBolt(JedisPoolConfig config) {
        super(config);
    }

    public LookupWordTotalCountBolt(JedisClusterConfig config) {
        super(config);
    }

    @Override
    public void execute(Tuple input) {
        JedisCommands jedisCommands = null;
        try {
            jedisCommands = getInstance();
            String wordName = input.getStringByField("word");
            String countStr = jedisCommands.get(wordName);
            if (countStr != null) {
                int count = Integer.parseInt(countStr);
                this.collector.emit(new Values(wordName, count));

                if(RANDOM.nextInt(1000) > 995) {
                    LOG.info("Lookup result - word : " + wordName + " / count : " + count);
                }
            } else {
                LOG.warn("Word not found in Redis - word : " + wordName);
            }
        } finally {
            if (jedisCommands != null) {
                returnInstance(jedisCommands);
            }
            this.collector.ack(input);
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("wordName", "count"));
    }
}

----------------------------------------

TITLE: Configuring NUMA Support in Storm Supervisor
DESCRIPTION: This YAML configuration snippet shows how to set up NUMA support in the Storm Supervisor config. It includes settings for NUMA zone cores, generic resources, memory, and port assignments.

LANGUAGE: yaml
CODE:
supervisor.numa.meta:
    "0": # Numa zone id
        numa.cores: # Cores in NUMA zone (can be determined by using the numastat command)
            - 0
            - 1
            - 2
            - 3
            - 4
            - 5
            - 12
            - 13
            - 14
            - 15
            - 16

        numa.generic.resources.map: # Generic Resources in the zone to be used for generic resource scheduling (optional)
            network.resource.units: 50.0

        numa.memory.mb: 42461 # Size of memory zone
        numa.ports: # Ports to be assigned to workers pinned to the NUMA zone (this may include ports not specified in SUPERVISOR_SLOTS_PORTS
            - 6700
            - 6701
            - 6702
            - 6703
            - 6704
            - 6705
            - 6706
            - 6707
            - 6708
            - 6709
            - 6710
            - 6711

----------------------------------------

TITLE: Configuring Resource Consumption for Trident Topology in Java
DESCRIPTION: This code snippet demonstrates how to set resource defaults and configure CPU and memory loads for different operations in a Trident topology. It shows the usage of methods like setResourceDefaults, setCPULoad, and setMemoryLoad on various stream operations.

LANGUAGE: java
CODE:
TridentTopology topo = new TridentTopology();
topo.setResourceDefaults(new DefaultResourceDeclarer();
                                                          .setMemoryLoad(128)
                                                          .setCPULoad(20));
TridentState wordCounts =
    topology
        .newStream("words", feeder)
        .parallelismHint(5)
        .setCPULoad(20)
        .setMemoryLoad(512,256)
        .each( new Fields("sentence"),  new Split(), new Fields("word"))
        .setCPULoad(10)
        .setMemoryLoad(512)
        .each(new Fields("word"), new BangAdder(), new Fields("word!"))
        .parallelismHint(10)
        .setCPULoad(50)
        .setMemoryLoad(1024)
        .each(new Fields("word!"), new QMarkAdder(), new Fields("word!?"))
        .groupBy(new Fields("word!"))
        .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))
        .setCPULoad(100)
        .setMemoryLoad(2048);

----------------------------------------

TITLE: Configuring JdbcInsertBolt for Storm-JDBC Integration
DESCRIPTION: Example of configuring a JdbcInsertBolt using HikariCP connection provider and SimpleJdbcMapper. Shows how to set up database connection and specify insert query or table name.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);

String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withTableName("user")
                                    .withQueryTimeoutSecs(30);
                                    Or
JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withInsertQuery("insert into user values (?,?)")
                                    .withQueryTimeoutSecs(30);                                    

----------------------------------------

TITLE: Registering Task Hook in Java Storm Component
DESCRIPTION: Demonstrates how to register a custom task hook within the open method of a spout or prepare method of a bolt using the TopologyContext. This allows for custom code execution on specific events within Storm components.

LANGUAGE: java
CODE:
TopologyContext#addTaskHook

----------------------------------------

TITLE: Implementing Custom HDFS Partitioner in Java
DESCRIPTION: Example of implementing a custom Partitioner interface to partition HDFS data based on a specific tuple field.

LANGUAGE: java
CODE:
Partitioner partitoner = new Partitioner() {
            @Override
            public String getPartitionPath(Tuple tuple) {
                return Path.SEPARATOR + tuple.getStringByField("city");
            }
     };

----------------------------------------

TITLE: Initializing EsPercolateBolt for Storm-Elasticsearch Integration in Java
DESCRIPTION: This snippet shows how to create an EsPercolateBolt to stream tuples for percolation requests in Elasticsearch. It requires an EsConfig object and an EsTupleMapper to extract necessary fields from input tuples.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsPercolateBolt percolateBolt = new EsPercolateBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Defining Leader Election Interface in Java for Storm Nimbus
DESCRIPTION: Java interface defining methods for leader election, including joining/leaving the leader queue, checking leadership status, and retrieving leader and cluster information.

LANGUAGE: java
CODE:
public interface ILeaderElector {
    void addToLeaderLockQueue();
    void removeFromLeaderLockQueue();
    boolean isLeader();
    InetSocketAddress getLeaderAddress();
    List<InetSocketAddress> getAllNimbusAddresses();
}

----------------------------------------

TITLE: Configuring Worker-Level Metrics in YAML
DESCRIPTION: Example configuration for registering worker-level metrics in the Storm YAML configuration file.

LANGUAGE: yaml
CODE:
worker.metrics: 
  metricA: "aaa.bbb.ccc.ddd.MetricA"
  metricB: "aaa.bbb.ccc.ddd.MetricB"

----------------------------------------

TITLE: Worker-Launcher Configuration File
DESCRIPTION: This configuration file for worker-launcher specifies the group, minimum user ID, and profiler script path. It is typically located at /etc/storm/worker-launcher.cfg and requires specific ownership and permissions.

LANGUAGE: bash
CODE:
storm.worker-launcher.group=$(worker_launcher_group)
min.user.id=$(min_user_id)
worker.profiler.script.path=$(profiler_script_path)

----------------------------------------

TITLE: Registering Worker Hook in Java Using TopologyBuilder
DESCRIPTION: Illustrates how to register a custom worker-level hook using the TopologyBuilder. These hooks are called during worker startup, before any bolts or spouts are prepared or opened.

LANGUAGE: java
CODE:
TopologyBuilder.addWorkerHook

----------------------------------------

TITLE: Creating JdbcLookupBolt for Storm-JDBC Integration
DESCRIPTION: This code snippet demonstrates how to create a JdbcLookupBolt, which is used for executing select queries and enriching storm tuples. It shows configuration with a ConnectionProvider, JdbcLookupMapper, and select query.

LANGUAGE: java
CODE:
String selectSql = "select user_name from user_details where user_id = ?";
SimpleJdbcLookupMapper lookupMapper = new SimpleJdbcLookupMapper(outputFields, queryParamColumns)
JdbcLookupBolt userNameLookupBolt = new JdbcLookupBolt(connectionProvider, selectSql, lookupMapper)
        .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Submitting Topology to Nimbus (Java)
DESCRIPTION: StormSubmitter calls the submitTopology method on the Nimbus Thrift interface to submit the topology. The topology config is serialized as JSON for language-agnostic DSL support.

LANGUAGE: java
CODE:
StormSubmitter.submitTopology(name, conf, topology);

----------------------------------------

TITLE: Defining Tuple Structure in Storm Multi-Language Protocol (JSON)
DESCRIPTION: This JSON structure represents a tuple in the Storm multi-language protocol. It includes the tuple's id, component id, stream id, task id, and the actual tuple values.

LANGUAGE: json
CODE:
{
    "id": -6955786537413359385,
    "comp": 1,
    "stream": 1,
    "task": 9,
    "tuple": ["snow white and the seven dwarfs", "field2", 3]
}

----------------------------------------

TITLE: Implementing HiveState with Trident
DESCRIPTION: Example of implementing HiveState using Trident API with mapper and options configuration.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withTimeAsPartitionField("YYYY/MM/DD");
            
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
                                .withTxnsPerBatch(10)
                				.withBatchSize(1000)
                	     		.withIdleTimeout(10)
                	     		
StateFactory factory = new HiveStateFactory().withOptions(hiveOptions);
TridentState state = stream.partitionPersist(factory, hiveFields, new HiveUpdater(), new Fields());

----------------------------------------

TITLE: Setting ROOT Logger to DEBUG for 30 Seconds via Storm CLI
DESCRIPTION: Example command that sets the ROOT logger to DEBUG level for a duration of 30 seconds in a Storm topology named 'my_topology'.

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -l ROOT=DEBUG:30

----------------------------------------

TITLE: Worker-Launcher Configuration File
DESCRIPTION: This configuration file for worker-launcher specifies the group, minimum user ID, and profiler script path. It is typically located at /etc/storm/worker-launcher.cfg and requires specific ownership and permissions.

LANGUAGE: bash
CODE:
storm.worker-launcher.group=$(worker_launcher_group)
min.user.id=$(min_user_id)
worker.profiler.script.path=$(profiler_script_path)

----------------------------------------

TITLE: Configuring HDFS Bolt with Delimited Record Format in Java
DESCRIPTION: Example showing how to configure an HDFS bolt to write pipe-delimited files with sync and rotation policies.

LANGUAGE: java
CODE:
// use "|" instead of "," for field delimiter
RecordFormat format = new DelimitedRecordFormat()
        .withFieldDelimiter("|");

// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withPath("/foo/");

HdfsBolt bolt = new HdfsBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withRecordFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy);

----------------------------------------

TITLE: Defining JdbcMapper Interface for JDBC Operations in Java
DESCRIPTION: Interface for mapping Storm tuples to database columns. The getColumns() method defines how a Storm tuple maps to a list of columns representing a row in a database.

LANGUAGE: java
CODE:
public interface JdbcMapper  extends Serializable {
    List<Column> getColumns(ITuple tuple);
}

----------------------------------------

TITLE: Acker Execute Process Implementation
DESCRIPTION: The acker bolt's execute process involves tracking tuple trees through XOR checksums. It handles initialization of new tuple trees, processes acknowledgments, and manages tuple failure states. The checksum system ensures reliable message processing by tracking when all tuples in a tree have been successfully processed.



----------------------------------------

TITLE: Using Storm DRPC Client
DESCRIPTION: Sends DRPC requests to a Storm cluster with function and arguments.

LANGUAGE: bash
CODE:
storm drpc-client [options] ([function argument]*)|(argument*)

----------------------------------------

TITLE: Specifying Classpath Wildcards in Java 6+
DESCRIPTION: Example of using classpath wildcards in Java 6 and later versions to include all JAR files in a directory without explicitly listing them. This approach is used by Storm to shorten classpath specifications.

LANGUAGE: java
CODE:
foo/bar/*

----------------------------------------

TITLE: Creating a LocalCluster for Storm Topology Testing in Java
DESCRIPTION: This snippet demonstrates how to create an in-process Storm cluster using the LocalCluster class. It's useful for automated testing without launching a full Storm cluster.

LANGUAGE: java
CODE:
import org.apache.storm.LocalCluster;

...

try (LocalCluster cluster = new LocalCluster()) {
    //Interact with the cluster...
}

----------------------------------------

TITLE: Basic Storm WordCount Topology YAML Configuration
DESCRIPTION: Example YAML configuration file defining a simple word count topology with spouts and bolts connected via streams

LANGUAGE: yaml
CODE:
name: "yaml-topology"
config:
  topology.workers: 1

# spout definitions
spouts:
  - id: "spout-1"
    className: "org.apache.storm.testing.TestWordSpout"
    parallelism: 1

# bolt definitions
bolts:
  - id: "bolt-1"
    className: "org.apache.storm.testing.TestWordCounter"
    parallelism: 1
  - id: "bolt-2"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1

#stream definitions
streams:
  - name: "spout-1 --> bolt-1" # name isn't used (placeholder for logging, UI, etc.)
    from: "spout-1"
    to: "bolt-1"
    grouping:
      type: FIELDS
      args: ["word"]

  - name: "bolt-1 --> bolt2"
    from: "bolt-1"
    to: "bolt-2"
    grouping:
      type: SHUFFLE

# worker hook definitions
workerHooks:
  - id: "base-worker-hook"
    className: "org.apache.storm.hooks.BaseWorkerHook"

----------------------------------------

TITLE: Configuring KafkaTridentSpoutOpaque in Trident Topology
DESCRIPTION: Demonstrates how to use KafkaTridentSpoutOpaque in a Trident topology to consume from Kafka topics matching a pattern.

LANGUAGE: java
CODE:
final TridentTopology tridentTopology = new TridentTopology();
final Stream spoutStream = tridentTopology.newStream("kafkaSpout",
    new KafkaTridentSpoutOpaque<>(KafkaSpoutConfig.builder("127.0.0.1:" + port, Pattern.compile("topic.*")).build()))
      .parallelismHint(1)

----------------------------------------

TITLE: Initializing HiveState for Trident Topology (Java)
DESCRIPTION: Java code snippet showing how to set up HiveState for use in a Trident topology, including configuration of DelimitedRecordHiveMapper and HiveOptions.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withTimeAsPartitionField("YYYY/MM/DD");
            
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
                                .withTxnsPerBatch(10)
                				.withBatchSize(1000)
                	     		.withIdleTimeout(10)
                	     		
StateFactory factory = new HiveStateFactory().withOptions(hiveOptions);
TridentState state = stream.partitionPersist(factory, hiveFields, new HiveUpdater(), new Fields());

----------------------------------------

TITLE: Creating RedisStoreBolt Instance
DESCRIPTION: This Java code snippet demonstrates how to create a RedisStoreBolt instance using a JedisPoolConfig and a RedisStoreMapper. It configures the Redis connection and sets up the store mapper.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
                .setHost(host).setPort(port).build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisStoreBolt storeBolt = new RedisStoreBolt(poolConfig, storeMapper);

----------------------------------------

TITLE: IConfigLoader Interface Definition in Java
DESCRIPTION: Interface definition for IConfigLoader, including the load method which is called by the scheduler to retrieve the most recent configuration map.

LANGUAGE: java
CODE:
public interface IConfigLoader {
    Map<?,?> load();
};

----------------------------------------

TITLE: Initializing EsLookupBolt for Storm-Elasticsearch Integration in Java
DESCRIPTION: This code demonstrates the initialization of an EsLookupBolt, which performs get requests to Elasticsearch. It requires an EsConfig, ElasticsearchGetRequest for converting incoming tuples, and EsLookupResultOutput for specifying output fields and converting responses.

LANGUAGE: java
CODE:
EsConfig esConfig = createEsConfig();
ElasticsearchGetRequest getRequestAdapter = createElasticsearchGetRequest();
EsLookupResultOutput output = createOutput();
EsLookupBolt lookupBolt = new EsLookupBolt(esConfig, getRequestAdapter, output);

----------------------------------------

TITLE: Storm Metric Reporter Configuration
DESCRIPTION: Configuration options for Storm metric reporters including Console, CSV and JMX reporters. Custom reporters can be created by implementing the PreparableReporter interface.

LANGUAGE: properties
CODE:
storm.daemon.metrics.reporter.plugins
org.apache.storm.daemon.metrics.reporters.ConsolePreparableReporter
org.apache.storm.daemon.metrics.reporters.CsvPreparableReporter
org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter

----------------------------------------

TITLE: Window Configuration Options
DESCRIPTION: Available window configuration methods for BaseWindowedBolt showing different ways to configure sliding and tumbling windows based on count or duration.

LANGUAGE: java
CODE:
withWindow(Count windowLength, Count slidingInterval)
withWindow(Count windowLength)
withWindow(Count windowLength, Duration slidingInterval)
withWindow(Duration windowLength, Duration slidingInterval)
withWindow(Duration windowLength)
withWindow(Duration windowLength, Count slidingInterval)
withTumblingWindow(BaseWindowedBolt.Count count)
withTumblingWindow(BaseWindowedBolt.Duration duration)

----------------------------------------

TITLE: Registering Worker Hook in Java using TopologyBuilder
DESCRIPTION: This snippet illustrates how to register a custom worker-level hook using the TopologyBuilder. The addWorkerHook method is used to add the worker hook to the topology.

LANGUAGE: java
CODE:
TopologyBuilder.addWorkerHook

----------------------------------------

TITLE: Implementing Custom Redis Bolt in Java
DESCRIPTION: Java class extending AbstractRedisBolt for custom Redis operations, demonstrating word count lookup.

LANGUAGE: java
CODE:
public static class LookupWordTotalCountBolt extends AbstractRedisBolt {
    private static final Logger LOG = LoggerFactory.getLogger(LookupWordTotalCountBolt.class);
    private static final Random RANDOM = new Random();

    public LookupWordTotalCountBolt(JedisPoolConfig config) {
        super(config);
    }

    public LookupWordTotalCountBolt(JedisClusterConfig config) {
        super(config);
    }

    @Override
    public void execute(Tuple input) {
        JedisCommands jedisCommands = null;
        try {
            jedisCommands = getInstance();
            String wordName = input.getStringByField("word");
            String countStr = jedisCommands.get(wordName);
            if (countStr != null) {
                int count = Integer.parseInt(countStr);
                this.collector.emit(new Values(wordName, count));

                // print lookup result with low probability
                if(RANDOM.nextInt(1000) > 995) {
                    LOG.info("Lookup result - word : " + wordName + " / count : " + count);
                }
            } else {
                // skip
                LOG.warn("Word not found in Redis - word : " + wordName);
            }
        } finally {
            if (jedisCommands != null) {
                returnInstance(jedisCommands);
            }
            this.collector.ack(input);
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        // wordName, count
        declarer.declare(new Fields("wordName", "count"));
    }
}

----------------------------------------

TITLE: Implementing RedisStoreMapper for WordCount
DESCRIPTION: Java class implementing RedisStoreMapper interface for storing word counts in Redis. This mapper is used with RedisStoreBolt.

LANGUAGE: java
CODE:
class WordCountStoreMapper implements RedisStoreMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountStoreMapper() {
        description = new RedisDataTypeDescription(
            RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return tuple.getStringByField("count");
    }
}

----------------------------------------

TITLE: Configuring Spring applicationContext.xml for JMS Integration with Storm
DESCRIPTION: This XML configuration defines JMS queue and topic beans, as well as a connection factory for ActiveMQ. It sets up the necessary components for Storm to interact with JMS using Spring's support.

LANGUAGE: XML
CODE:
<?xml version="1.0" encoding="UTF-8"?>
<beans 
  xmlns="http://www.springframework.org/schema/beans" 
  xmlns:amq="http://activemq.apache.org/schema/core"
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.0.xsd
  http://activemq.apache.org/schema/core http://activemq.apache.org/schema/core/activemq-core.xsd">

	<amq:queue id="notificationQueue" physicalName="backtype.storm.contrib.example.queue" />
	
	<amq:topic id="notificationTopic" physicalName="backtype.storm.contrib.example.topic" />

	<amq:connectionFactory id="jmsConnectionFactory"
		brokerURL="tcp://localhost:61616" />
	
</beans>

----------------------------------------

TITLE: Registering Worker Hook in Java using TopologyBuilder
DESCRIPTION: This snippet illustrates how to register a custom worker-level hook using the TopologyBuilder. The addWorkerHook method is used to add the worker hook to the topology.

LANGUAGE: java
CODE:
TopologyBuilder.addWorkerHook

----------------------------------------

TITLE: Defining ComponentObject in Storm Thrift
DESCRIPTION: This Thrift union defines the structure for specifying spout or bolt code in Storm topologies. It allows for serialized Java, shell components, or Java objects.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Configuring RedisClusterState for Trident Topology
DESCRIPTION: Java code snippet showing how to configure RedisClusterState for use in a Trident topology with a Redis cluster.

LANGUAGE: java
CODE:
Set<InetSocketAddress> nodes = new HashSet<InetSocketAddress>();
for (String hostPort : redisHostPort.split(",")) {
    String[] host_port = hostPort.split(":");
    nodes.add(new InetSocketAddress(host_port[0], Integer.valueOf(host_port[1])));
}
JedisClusterConfig clusterConfig = new JedisClusterConfig.Builder().setNodes(nodes)
                                .build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisLookupMapper lookupMapper = new WordCountLookupMapper();
RedisClusterState.Factory factory = new RedisClusterState.Factory(clusterConfig);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory,
                        fields,
                        new RedisClusterStateUpdater(storeMapper).withExpire(86400000),
                        new Fields());

TridentState state = topology.newStaticState(factory);
stream = stream.stateQuery(state, new Fields("word"),
                        new RedisClusterStateQuerier(lookupMapper),
                        new Fields("columnName","columnValue"));

----------------------------------------

TITLE: Configuring MongoInsertBolt for Storm-MongoDB Integration
DESCRIPTION: This snippet demonstrates how to configure a MongoInsertBolt, which is used to insert data into a MongoDB collection. It specifies the MongoDB URL, collection name, and a MongoMapper implementation.

LANGUAGE: java
CODE:
String url = "mongodb://127.0.0.1:27017/test";
String collectionName = "wordcount";

MongoMapper mapper = new SimpleMongoMapper()
        .withFields("word", "count");

MongoInsertBolt insertBolt = new MongoInsertBolt(url, collectionName, mapper);

----------------------------------------

TITLE: Initializing EsPercolateBolt for Storm-Elasticsearch Integration in Java
DESCRIPTION: This code shows how to create an EsPercolateBolt for sending percolate requests to Elasticsearch from Storm. It requires an EsConfig for cluster configuration and an EsTupleMapper for mapping tuples to percolate requests.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsPercolateBolt percolateBolt = new EsPercolateBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Storm Fail Command JSON Format
DESCRIPTION: JSON structure for indicating tuple processing failure in Storm. Includes command type and tuple ID that failed.

LANGUAGE: json
CODE:
{
    "command": "fail",
    "id": 123123
}

----------------------------------------

TITLE: Initializing EsIndexBolt in Java
DESCRIPTION: Creates an EsIndexBolt instance for streaming tuples directly into Elasticsearch. It requires an EsConfig and EsTupleMapper to extract necessary fields from input tuples.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsIndexBolt indexBolt = new EsIndexBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Configuring JDBC Insert Bolt with HikariCP in Java
DESCRIPTION: Example of configuring and instantiating a JDBC Insert Bolt using HikariCP connection pool.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);

String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

JdbcInsertBolt userPersistanceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withTableName("user")
                                    .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Configuring Spring JMS Connection with ActiveMQ
DESCRIPTION: Spring XML configuration file that sets up JMS connection factory and defines ActiveMQ queue and topic destinations. The configuration includes connection settings for ActiveMQ broker running on localhost:61616.

LANGUAGE: xml
CODE:
<?xml version="1.0" encoding="UTF-8"?>
<beans 
  xmlns="http://www.springframework.org/schema/beans" 
  xmlns:amq="http://activemq.apache.org/schema/core"
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.0.xsd
  http://activemq.apache.org/schema/core http://activemq.apache.org/schema/core/activemq-core.xsd">

    <amq:queue id="notificationQueue" physicalName="backtype.storm.contrib.example.queue" />
    
    <amq:topic id="notificationTopic" physicalName="backtype.storm.contrib.example.topic" />

    <amq:connectionFactory id="jmsConnectionFactory"
        brokerURL="tcp://localhost:61616" />
    
</beans>

----------------------------------------

TITLE: Implementing Scalar Function in Java for Storm SQL
DESCRIPTION: Java class implementing a scalar function for use in Storm SQL. This example defines a MyPlus class with an evaluate method that adds two integers.

LANGUAGE: java
CODE:
  public class MyPlus {
    public static Integer evaluate(Integer x, Integer y) {
      return x + y;
    }
  }

----------------------------------------

TITLE: Adding Generic Resources to Storm Topology Components in Java
DESCRIPTION: This snippet demonstrates how to use the addResource method to specify generic resource requirements for a Storm topology component, such as a Spout or Bolt. It shows how to add a GPU resource requirement to a Spout.

LANGUAGE: java
CODE:
public T addResource(String resourceName, Number resourceValue)

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.addResouce("gpu.count", 1.0);

----------------------------------------

TITLE: Defining Code Distribution Interface in Java for Storm Nimbus
DESCRIPTION: Java interface for distributing topology code across nimbus hosts, including methods for uploading, downloading, checking replication count, and cleanup.

LANGUAGE: java
CODE:
public interface ICodeDistributor {
    void prepare(Map conf);

    File upload(Path dirPath, String topologyId);

    List<File> download(Path destDirPath, String topologyid, File metafile);

    int getReplicationCount(String topologyId);
    
    void cleanup(String topologyid);

    void close(Map conf);
}

----------------------------------------

TITLE: Configuring Redis State Provider in JSON
DESCRIPTION: JSON configuration for the Redis state provider, specifying key and value classes, serializers, and Redis connection details.

LANGUAGE: json
CODE:
{
  "keyClass": "Optional fully qualified class name of the Key type.",
  "valueClass": "Optional fully qualified class name of the Value type.",
  "keySerializerClass": "Optional Key serializer implementation class.",
  "valueSerializerClass": "Optional Value Serializer implementation class.",
  "jedisPoolConfig": {
    "host": "localhost",
    "port": 6379,
    "timeout": 2000,
    "database": 0,
    "password": "xyz"
    }
}

----------------------------------------

TITLE: Creating EsConfig for Storm-Elasticsearch Connection in Java
DESCRIPTION: Demonstrates two ways to create an EsConfig object for connecting Storm to Elasticsearch. The first method uses basic cluster information, while the second includes additional client parameters.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});

LANGUAGE: java
CODE:
Map<String, String> additionalParameters = new HashMap<>();
additionalParameters.put("client.transport.sniff", "true");
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"}, additionalParameters);

----------------------------------------

TITLE: Digest Authentication JAAS Configuration
DESCRIPTION: JAAS configuration structure for Pacemaker digest authentication with username and password.

LANGUAGE: java
CODE:
PacemakerDigest {\n    username="some username"\n    password="some password";\n};

----------------------------------------

TITLE: Initializing Elasticsearch Lookup Bolt in Storm
DESCRIPTION: Creates an EsLookupBolt instance for performing get requests to Elasticsearch with custom request and output adapters.

LANGUAGE: java
CODE:
EsConfig esConfig = createEsConfig();
ElasticsearchGetRequest getRequestAdapter = createElasticsearchGetRequest();
EsLookupResultOutput output = createOutput();
EsLookupBolt lookupBolt = new EsLookupBolt(esConfig, getRequestAdapter, output);

----------------------------------------

TITLE: Creating RedisLookupBolt in Java
DESCRIPTION: Java code snippet demonstrating how to create a RedisLookupBolt using JedisPoolConfig and RedisLookupMapper.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisLookupMapper lookupMapper = new WordCountRedisLookupMapper();
RedisLookupBolt lookupBolt = new RedisLookupBolt(poolConfig, lookupMapper);

----------------------------------------

TITLE: Configuring Kerberos Authentication for Storm-HBase in Java
DESCRIPTION: Example of setting up Kerberos authentication for secure HBase access in a Storm topology.

LANGUAGE: java
CODE:
Config config = new Config();
...
config.put("storm.keytab.file", "$keytab");
config.put("storm.kerberos.principal", "$principal");
StormSubmitter.submitTopology("$topologyName", config, builder.createTopology());

----------------------------------------

TITLE: Adding Generic Resources to Topology Components in Java
DESCRIPTION: This snippet demonstrates how to use the addResource method to specify generic resource requirements for a topology component, such as a Spout or Bolt. It shows how to set a GPU count requirement for a Spout.

LANGUAGE: java
CODE:
public T addResource(String resourceName, Number resourceValue)

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.addResouce("gpu.count", 1.0);

----------------------------------------

TITLE: Creating RedisFilterBolt Instance
DESCRIPTION: This Java code snippet shows how to create a RedisFilterBolt instance using a JedisPoolConfig and a RedisFilterMapper. It sets up the Redis connection and configures the filter mapper.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisFilterMapper filterMapper = new BlacklistWordFilterMapper();
RedisFilterBolt filterBolt = new RedisFilterBolt(poolConfig, filterMapper);

----------------------------------------

TITLE: Creating RedisLookupBolt in Java
DESCRIPTION: Java code snippet demonstrating how to create a RedisLookupBolt using JedisPoolConfig and RedisLookupMapper.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisLookupMapper lookupMapper = new WordCountRedisLookupMapper();
RedisLookupBolt lookupBolt = new RedisLookupBolt(poolConfig, lookupMapper);

----------------------------------------

TITLE: Named Stream Join Configuration
DESCRIPTION: Examples of configuring JoinBolt to work with named streams instead of default streams, showing both basic configuration and complex topology setup.

LANGUAGE: java
CODE:
new JoinBolt(JoinBolt.Selector.STREAM,  "stream1", "key1")
                                  .join("stream2", "key2")
    ...

LANGUAGE: java
CODE:
new JoinBolt(JoinBolt.Selector.STREAM,  "stream1", "key1") 
                             .join     ("stream2", "userId",  "stream1" )
                             .select ("userId, key1, key2")
                             .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;
                             
topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("bolt1", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt2", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt3", "stream2", new Fields("userId") )
            .fieldsGrouping("bolt4", "stream1", new Fields("key1") );

----------------------------------------

TITLE: SimpleHBaseMapper Configuration Example
DESCRIPTION: Example of configuring SimpleHBaseMapper with row key, column fields, counter fields, and column family.

LANGUAGE: java
CODE:
SimpleHBaseMapper mapper = new SimpleHBaseMapper() 
        .withRowKeyField("word")
        .withColumnFields(new Fields("word"))
        .withCounterFields(new Fields("count"))
        .withColumnFamily("cf");

----------------------------------------

TITLE: Configuring Zookeeper Servers in Storm YAML
DESCRIPTION: This YAML configuration specifies the list of Zookeeper servers for the Storm cluster. It's a mandatory configuration in the storm.yaml file.

LANGUAGE: yaml
CODE:
storm.zookeeper.servers:
  - "111.222.333.444"
  - "555.666.777.888"

----------------------------------------

TITLE: Configuring Resource Consumption in Trident Topology
DESCRIPTION: This snippet demonstrates how to set CPU and memory resources for various operations in a Trident topology using the RAS API. It shows setting default resources, and then overriding them for specific operations like stream creation, data processing, and aggregation.

LANGUAGE: java
CODE:
TridentTopology topo = new TridentTopology();
topo.setResourceDefaults(new DefaultResourceDeclarer()
                                                          .setMemoryLoad(128)
                                                          .setCPULoad(20));
TridentState wordCounts =
    topology
        .newStream("words", feeder)
        .parallelismHint(5)
        .setCPULoad(20)
        .setMemoryLoad(512,256)
        .each( new Fields("sentence"),  new Split(), new Fields("word"))
        .setCPULoad(10)
        .setMemoryLoad(512)
        .each(new Fields("word"), new BangAdder(), new Fields("word!"))
        .parallelismHint(10)
        .setCPULoad(50)
        .setMemoryLoad(1024)
        .each(new Fields("word!"), new QMarkAdder(), new Fields("word!?"))
        .groupBy(new Fields("word!"))
        .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))
        .setCPULoad(100)
        .setMemoryLoad(2048);

----------------------------------------

TITLE: Configuring HDFS Spout in Java
DESCRIPTION: Example of how to configure and use the HDFS Spout for reading data from HDFS into a Storm topology.

LANGUAGE: java
CODE:
// Instantiate spout to read text files
HdfsSpout textReaderSpout = new HdfsSpout().setReaderType("text")
                                           .withOutputFields(TextFileReader.defaultFields)                                      
                                           .setHdfsUri("hdfs://localhost:54310")  // required
                                           .setSourceDir("/data/in")              // required                                      
                                           .setArchiveDir("/data/done")           // required
                                           .setBadFilesDir("/data/badfiles");     // required                                      
// If using Kerberos
HashMap hdfsSettings = new HashMap();
hdfsSettings.put("hdfs.keytab.file", "/path/to/keytab");
hdfsSettings.put("hdfs.kerberos.principal","user@EXAMPLE.com");

textReaderSpout.setHdfsClientSettings(hdfsSettings);

// Create topology
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("hdfsspout", textReaderSpout, SPOUT_NUM);

// Setup bolts and wire up topology
     ..snip..

// Submit topology with config
Config conf = new Config();
StormSubmitter.submitTopologyWithProgressBar("topologyName", conf, builder.createTopology());

----------------------------------------

TITLE: Registering Metrics Consumers in YAML Configuration
DESCRIPTION: Demonstrates how to register multiple metrics consumers in the storm.yaml config file. This example registers both LoggingMetricsConsumer and HttpForwardingMetricsConsumer with parallelism hints and arguments.

LANGUAGE: yaml
CODE:
topology.metrics.consumer.register:
  - class: "org.apache.storm.metric.LoggingMetricsConsumer"
    parallelism.hint: 1
  - class: "org.apache.storm.metric.HttpForwardingMetricsConsumer"
    parallelism.hint: 1
    argument: "http://example.com:8080/metrics/my-topology/"

----------------------------------------

TITLE: Configuring a Kafka Spout with Wildcard Topics in Java
DESCRIPTION: Example of setting up a Kafka spout to consume from multiple topics matching a pattern.

LANGUAGE: java
CODE:
final TopologyBuilder tp = new TopologyBuilder();
tp.setSpout("kafka_spout", new KafkaSpout<>(KafkaSpoutConfig.builder("127.0.0.1:" + port, Pattern.compile("topic.*")).build()), 1);
tp.setBolt("bolt", new myBolt()).shuffleGrouping("kafka_spout");

----------------------------------------

TITLE: Storm Shell Command Usage Example
DESCRIPTION: Example command showing how to use the storm shell utility to package and submit a Python topology.

LANGUAGE: bash
CODE:
storm shell resources/ python3 topology.py arg1 arg2

----------------------------------------

TITLE: Creating DRPC Client in Java
DESCRIPTION: Example code showing how to create and use a DRPC client for production use. The client is configured and used within a try-with-resources block to ensure proper cleanup.

LANGUAGE: java
CODE:
Config conf = new Config();
try (DRPCClient drpc = DRPCClient.getConfiguredClient(conf)) {
  //User the drpc client
  String result = drpc.execute(function, argument);
}

----------------------------------------

TITLE: Creating CassandraWriterBolt with All Fields in Java
DESCRIPTION: Shows how to create a CassandraWriterBolt that inserts all tuple fields into Cassandra.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
    async(
        simpleQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);")
            .with( all() )
        )
);

----------------------------------------

TITLE: Slow Log Filtering SQL
DESCRIPTION: Storm SQL query to filter slow logs (response time >= 100ms) with custom time parsing function

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://apache-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
CREATE EXTERNAL TABLE APACHE_SLOW_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_RECEIVED_TIMESTAMP BIGINT, TIME_ELAPSED_MS INT) LOCATION 'kafka://apache-slow-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
CREATE FUNCTION GET_TIME AS 'org.apache.storm.sql.runtime.functions.scalar.datetime.GetTime2'
INSERT INTO APACHE_SLOW_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, GET_TIME(TIME_RECEIVED_UTC_ISOFORMAT, 'yyyy-MM-dd''T''HH:mm:ssZZ') AS TIME_RECEIVED_TIMESTAMP, TIME_US / 1000 AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (TIME_US / 1000) >= 100

----------------------------------------

TITLE: Initializing EsIndexBolt for Storm-Elasticsearch Integration
DESCRIPTION: Creates an EsIndexBolt instance to stream tuples directly into Elasticsearch. Requires EsConfig for cluster configuration and EsTupleMapper for mapping tuple fields to Elasticsearch documents.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsIndexBolt indexBolt = new EsIndexBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Creating IConfigLoader Instance in Java
DESCRIPTION: Method to create an IConfigLoader instance based on the scheme of the scheduler.config.loader.uri configuration.

LANGUAGE: java
CODE:
ConfigLoaderFactoryService.createConfigLoader(Map<String, Object> conf)

----------------------------------------

TITLE: Implementing Distributed Query in Trident
DESCRIPTION: Demonstrates how to implement a low-latency distributed query on word counts. This query takes a list of words and returns the sum of their counts, executed in parallel across a Storm cluster.

LANGUAGE: java
CODE:
topology.newDRPCStream("words")
       .each(new Fields("args"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .stateQuery(wordCounts, new Fields("word"), new MapGet(), new Fields("count"))
       .each(new Fields("count"), new FilterNull())
       .aggregate(new Fields("count"), new Sum(), new Fields("sum"));

----------------------------------------

TITLE: Initializing RocketMqSpout for Reading from RocketMQ Topic in Java
DESCRIPTION: This snippet demonstrates how to create an instance of RocketMqSpout to read data from a RocketMQ topic. It uses a Properties object to configure the name server address, consumer group, and topic.

LANGUAGE: java
CODE:
Properties properties = new Properties();
properties.setProperty(SpoutConfig.NAME_SERVER_ADDR, nameserverAddr);
properties.setProperty(SpoutConfig.CONSUMER_GROUP, group);
properties.setProperty(SpoutConfig.CONSUMER_TOPIC, topic);

RocketMqSpout spout = new RocketMqSpout(properties);

----------------------------------------

TITLE: Implementing Redis Filter Mapper
DESCRIPTION: Example implementation of RedisFilterMapper interface for filtering words using Redis set data type.

LANGUAGE: java
CODE:
class BlacklistWordFilterMapper implements RedisFilterMapper {
    private RedisDataTypeDescription description;
    private final String setKey = "blacklist";

    public BlacklistWordFilterMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.SET, setKey);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Adding Counter Columns to HBase
DESCRIPTION: Example of adding counter columns to a ColumnList for HBase operations.

LANGUAGE: java
CODE:
ColumnList cols = new ColumnList();
cols.addCounter(this.columnFamily, field.getBytes(), toLong(tuple.getValueByField(field)));

----------------------------------------

TITLE: Initializing RocketMqSpout in Java for Storm-RocketMQ Integration
DESCRIPTION: This snippet demonstrates how to create an instance of RocketMqSpout by specifying RocketMQ configuration properties. It sets the name server address, consumer group, and topic for the spout.

LANGUAGE: java
CODE:
Properties properties = new Properties();
properties.setProperty(SpoutConfig.NAME_SERVER_ADDR, nameserverAddr);
properties.setProperty(SpoutConfig.CONSUMER_GROUP, group);
properties.setProperty(SpoutConfig.CONSUMER_TOPIC, topic);

RocketMqSpout spout = new RocketMqSpout(properties);

----------------------------------------

TITLE: Kerberos Client JAAS Configuration
DESCRIPTION: JAAS configuration for Nimbus as a Kerberos client to authenticate with Pacemaker.

LANGUAGE: java
CODE:
PacemakerClient {\n    com.sun.security.auth.module.Krb5LoginModule required\n    useKeyTab=true\n    keyTab="/etc/keytabs/nimbus.keytab"\n    storeKey=true\n    useTicketCache=false\n    serviceName="pacemaker"\n    principal="nimbus@MY.COMPANY.COM";\n};

----------------------------------------

TITLE: Initializing EsPercolateBolt for Elasticsearch in Java
DESCRIPTION: Creates an EsPercolateBolt to stream tuples for percolation in Elasticsearch. Requires an EsConfig for cluster configuration and an EsTupleMapper to extract necessary fields from input tuples.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsPercolateBolt percolateBolt = new EsPercolateBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Implementing Redis Filter Mapper
DESCRIPTION: Example implementation of RedisFilterMapper interface for filtering words using Redis set data type.

LANGUAGE: java
CODE:
class BlacklistWordFilterMapper implements RedisFilterMapper {
    private RedisDataTypeDescription description;
    private final String setKey = "blacklist";

    public BlacklistWordFilterMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.SET, setKey);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: JDBC Insert Bolt Configuration Example
DESCRIPTION: Example showing how to configure and instantiate a JdbcInsertBolt using HikariCP connection provider and SimpleJdbcMapper.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);

String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withTableName("user")
                                    .withQueryTimeoutSecs(30);
                                    Or
JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withInsertQuery("insert into user values (?,?)")
                                    .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Creating a Flux-Enabled Topology JAR with Maven
DESCRIPTION: Maven configuration for creating a fat JAR that includes Flux and all topology dependencies.

LANGUAGE: xml
CODE:
<dependencies>
    <!-- Flux include -->
    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>flux-core</artifactId>
        <version>${storm.version}</version>
    </dependency>
    <!-- Flux Wrappers include -->
    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>flux-wrappers</artifactId>
        <version>${storm.version}</version>
    </dependency>

    <!-- add user dependencies here... -->

</dependencies>
<!-- create a fat jar that includes all dependencies -->
<build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-shade-plugin</artifactId>
            <version>1.4</version>
            <configuration>
                <createDependencyReducedPom>true</createDependencyReducedPom>
            </configuration>
            <executions>
                <execution>
                    <phase>package</phase>
                    <goals>
                        <goal>shade</goal>
                    </goals>
                    <configuration>
                        <transformers>
                            <transformer
                                    implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                            <transformer
                                    implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                <mainClass>org.apache.storm.flux.Flux</mainClass>
                            </transformer>
                        </transformers>
                    </configuration>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>

----------------------------------------

TITLE: Implementing IWindowedBolt Interface
DESCRIPTION: Core interface for implementing windowed operations in Storm bolts. Defines methods for preparing the bolt, executing window operations, and cleanup.

LANGUAGE: java
CODE:
public interface IWindowedBolt extends IComponent {
    void prepare(Map stormConf, TopologyContext context, OutputCollector collector);
    /**
     * Process tuples falling within the window and optionally emit 
     * new tuples based on the tuples in the input window.
     */
    void execute(TupleWindow inputWindow);
    void cleanup();
}

----------------------------------------

TITLE: Using ClientBlobStore in Java
DESCRIPTION: Example of using the ClientBlobStore API in Java to interact with the distributed cache.

LANGUAGE: java
CODE:
Config theconf = new Config();
theconf.putAll(Utils.readStormConfig());
ClientBlobStore clientBlobStore = Utils.getClientBlobStore(theconf);

----------------------------------------

TITLE: Storm JoinBolt Implementation
DESCRIPTION: Java implementation of JoinBolt that recreates the SQL join example using Storm streams with window configuration and field grouping

LANGUAGE: java
CODE:
JoinBolt jbolt =  new JoinBolt("spout1", "key1")                   // from        spout1  
                    .join     ("spout2", "userId",  "spout1")      // inner join  spout2  on spout2.userId = spout1.key1
                    .join     ("spout3", "key3",    "spout2")      // inner join  spout3  on spout3.key3   = spout2.userId   
                    .leftJoin ("spout4", "key4",    "spout3")      // left join   spout4  on spout4.key4   = spout3.key3
                    .select  ("userId, key4, key2, spout3:key3")   // chose output fields
                    .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;

topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("spout1", new Fields("key1") )
            .fieldsGrouping("spout2", new Fields("userId") )
            .fieldsGrouping("spout3", new Fields("key3") )
            .fieldsGrouping("spout4", new Fields("key4") );

----------------------------------------

TITLE: Debugging Kryo ConcurrentModificationException in Storm
DESCRIPTION: This stack trace indicates a concurrent modification exception occurring during Kryo serialization in Storm. It typically happens when emitting mutable objects as output tuples, which should be avoided.

LANGUAGE: java
CODE:
java.lang.RuntimeException: java.util.ConcurrentModificationException
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:84)
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:55)
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:56)
	at org.apache.storm.disruptor$consume_loop_STAR_$fn__1597.invoke(disruptor.clj:67)
	at org.apache.storm.util$async_loop$fn__465.invoke(util.clj:377)
	at clojure.lang.AFn.run(AFn.java:24)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.util.ConcurrentModificationException
	at java.util.LinkedHashMap$LinkedHashIterator.nextEntry(LinkedHashMap.java:390)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:409)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:408)
	at java.util.HashMap.writeObject(HashMap.java:1016)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:959)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1480)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at org.apache.storm.serialization.SerializableSerializer.write(SerializableSerializer.java:21)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:554)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:77)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:472)
	at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:27)

----------------------------------------

TITLE: Implementing OpenTSDB Core Bolt in Storm
DESCRIPTION: Example showing how to configure and implement OpenTsdbBolt for writing time series data to OpenTSDB. The bolt is configured with a client builder, mapper, batch size, and flush interval for optimized writing.

LANGUAGE: java
CODE:
OpenTsdbClient.Builder builder =  OpenTsdbClient.newBuilder(openTsdbUrl).sync(30_000).returnDetails();
final OpenTsdbBolt openTsdbBolt = new OpenTsdbBolt(builder, Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
openTsdbBolt.withBatchSize(10).withFlushInterval(2000);
topologyBuilder.setBolt("opentsdb", openTsdbBolt).shuffleGrouping("metric-gen");

----------------------------------------

TITLE: Defining MongoDB Mapper Interface
DESCRIPTION: Core interface for mapping Storm tuples to MongoDB documents

LANGUAGE: java
CODE:
public interface MongoMapper extends Serializable {
    Document toDocument(ITuple tuple);
    Document toDocumentByKeys(List<Object> keys);
}

----------------------------------------

TITLE: Implementing IWindowedBolt Interface
DESCRIPTION: Core interface for implementing windowed operations in Storm bolts. Defines methods for preparing the bolt, executing window operations, and cleanup.

LANGUAGE: java
CODE:
public interface IWindowedBolt extends IComponent {
    void prepare(Map stormConf, TopologyContext context, OutputCollector collector);
    /**
     * Process tuples falling within the window and optionally emit 
     * new tuples based on the tuples in the input window.
     */
    void execute(TupleWindow inputWindow);
    void cleanup();
}

----------------------------------------

TITLE: Initializing EsPercolateBolt for Elasticsearch Percolation
DESCRIPTION: Creates an EsPercolateBolt instance to send percolate requests to Elasticsearch. Uses EsConfig for cluster configuration and EsTupleMapper for mapping tuple fields.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsPercolateBolt percolateBolt = new EsPercolateBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Configuring Zookeeper Servers in Storm YAML
DESCRIPTION: Configuration for specifying Zookeeper cluster hosts in storm.yaml file

LANGUAGE: yaml
CODE:
storm.zookeeper.servers:
  - "111.222.333.444"
  - "555.666.777.888"

----------------------------------------

TITLE: Defining Code Distribution Interface in Java
DESCRIPTION: Interface definition for distributing code across the Storm cluster. It includes methods for uploading and downloading code, checking replication status, and performing cleanup operations.

LANGUAGE: java
CODE:
public interface ICodeDistributor {
    /**
     * Prepare this code distributor.
     * @param conf
     */
    void prepare(Map conf);

    /**
     * This API will perform the actual upload of the code to the distributed implementation.
     * The API should return a Meta file which should have enough information for downloader 
     * so it can download the code e.g. for bittorrent it will be a torrent file, in case of something         
     * like HDFS or s3  it might have the actual directory or paths for files to be downloaded.
     * @param dirPath local directory where all the code to be distributed exists.
     * @param topologyId the topologyId for which the meta file needs to be created.
     * @return metaFile
     */
    File upload(Path dirPath, String topologyId);

    /**
     * Given the topologyId and metafile, download the actual code and return the downloaded file's list.
     * @param topologyid
     * @param metafile 
     * @param destDirPath the folder where all the files will be downloaded.
     * @return
     */
    List<File> download(Path destDirPath, String topologyid, File metafile);

    /**
      * Given the topologyId, returns number of hosts where the code has been replicated.
      */
    int getReplicationCount(String topologyId);
    
   /**
     * Performs the cleanup.
     * @param topologyid
     */
    void cleanup(String topologyid);

    /**
     * Close this distributor.
     * @param conf
     */
    void close(Map conf);
}

----------------------------------------

TITLE: Configuring RedisClusterState for Trident Topology in Java
DESCRIPTION: Java code snippet demonstrating how to configure RedisClusterState for use in a Trident topology with Redis cluster.

LANGUAGE: java
CODE:
Set<InetSocketAddress> nodes = new HashSet<InetSocketAddress>();
for (String hostPort : redisHostPort.split(",")) {
    String[] host_port = hostPort.split(":");
    nodes.add(new InetSocketAddress(host_port[0], Integer.valueOf(host_port[1])));
}
JedisClusterConfig clusterConfig = new JedisClusterConfig.Builder().setNodes(nodes)
                                .build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisLookupMapper lookupMapper = new WordCountLookupMapper();
RedisClusterState.Factory factory = new RedisClusterState.Factory(clusterConfig);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory,
                        fields,
                        new RedisClusterStateUpdater(storeMapper).withExpire(86400000),
                        new Fields());

TridentState state = topology.newStaticState(factory);
stream = stream.stateQuery(state, new Fields("word"),
                        new RedisClusterStateQuerier(lookupMapper),
                        new Fields("columnName","columnValue"));

----------------------------------------

TITLE: Using Custom State Query in Trident Topology
DESCRIPTION: Example of using a custom state query operation in a Trident topology to retrieve user locations.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
TridentState locations = topology.newStaticState(new LocationDBFactory());
topology.newStream("myspout", spout)
        .stateQuery(locations, new Fields("userid"), new QueryLocation(), new Fields("location"))

----------------------------------------

TITLE: Redis Store Mapper Implementation
DESCRIPTION: Implementation of RedisStoreMapper interface for storing word counts in Redis hash data type.

LANGUAGE: java
CODE:
class WordCountStoreMapper implements RedisStoreMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountStoreMapper() {
        description = new RedisDataTypeDescription(
            RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return tuple.getStringByField("count");
    }
}

----------------------------------------

TITLE: Defining Storm Event Logger Interface in Java
DESCRIPTION: Interface definition for implementing custom event loggers in Storm. Contains methods for preparation, logging events, and cleanup that must be implemented by custom loggers.

LANGUAGE: java
CODE:
public interface IEventLogger {
    void prepare(Map stormConf, Map<String, Object> arguments, TopologyContext context);
    void log(EventInfo e);
    void close();
}

----------------------------------------

TITLE: Starting Pacemaker Daemon
DESCRIPTION: Command to start the Pacemaker daemon for Apache Storm.

LANGUAGE: bash
CODE:
$ storm pacemaker

----------------------------------------

TITLE: Configuring HiveOptions for HiveBolt (Java)
DESCRIPTION: Java code example demonstrating how to configure HiveOptions for use with HiveBolt, including transaction and batch size settings.

LANGUAGE: java
CODE:
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
                                .withTxnsPerBatch(10)
                				.withBatchSize(1000)
                	     		.withIdleTimeout(10)

----------------------------------------

TITLE: Running Storm PMML Examples from Command Line
DESCRIPTION: This command demonstrates how to run the bundled PMML examples in Storm. It requires the Storm home directory, the example JAR file, and specifies the main class along with arguments for the topology name, PMML model file, and raw input data file.

LANGUAGE: java
CODE:
STORM-HOME/bin/storm jar STORM-HOME/examples/storm-pmml-examples/storm-pmml-examples-2.0.0-SNAPSHOT.jar 
org.apache.storm.pmml.JpmmlRunnerTestTopology jpmmlTopology PMMLModel.xml RawInputData.csv

----------------------------------------

TITLE: Configuring JAAS for Storm and Zookeeper
DESCRIPTION: JAAS configuration file content for Storm and Zookeeper authentication.

LANGUAGE: yaml
CODE:
StormServer {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/storm.keytab"
   storeKey=true
   useTicketCache=false
   principal="storm/storm.example.com@STORM.EXAMPLE.COM";
};
StormClient {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/storm.keytab"
   storeKey=true
   useTicketCache=false
   serviceName="storm"
   principal="storm@STORM.EXAMPLE.COM";
};
Client {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/storm.keytab"
   storeKey=true
   useTicketCache=false
   serviceName="zookeeper"
   principal="storm@STORM.EXAMPLE.COM";
};
Server {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/zk.keytab"
   storeKey=true
   useTicketCache=false
   serviceName="zookeeper"
   principal="zookeeper/zk1.example.com@STORM.EXAMPLE.COM";
};

----------------------------------------

TITLE: Configuring Event Loggers in YAML for Apache Storm
DESCRIPTION: This YAML configuration example shows how to register multiple event loggers in the storm.yaml config file, including a custom event logger with additional arguments.

LANGUAGE: yaml
CODE:
topology.event.logger.register:
  - class: "org.apache.storm.metric.FileBasedEventLogger"
  - class: "org.mycompany.MyEventLogger"
    arguments:
      endpoint: "event-logger.mycompany.org"

----------------------------------------

TITLE: Defining HBaseValueMapper Interface in Java
DESCRIPTION: This interface allows transformation of HBase lookup results into Storm Values for emission by HBaseLookupBolt. It includes methods for tuple conversion and output field declaration.

LANGUAGE: java
CODE:
public interface HBaseValueMapper extends Serializable {
    public List<Values> toTuples(Result result) throws Exception;
    void declareOutputFields(OutputFieldsDeclarer declarer);
}

----------------------------------------

TITLE: Configuring MongoDB Trident State for Storm Integration
DESCRIPTION: This code demonstrates how to configure and use MongoDB Trident State, which provides persistent state for Trident topologies. It includes setting up the MongoMapper, MongoState.Options, and creating a StateFactory.

LANGUAGE: java
CODE:
MongoMapper mapper = new SimpleMongoMapper()
        .withFields("word", "count");

MongoState.Options options = new MongoState.Options()
        .withUrl(url)
        .withCollectionName(collectionName)
        .withMapper(mapper);

StateFactory factory = new MongoStateFactory(options);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory, fields,
        new MongoStateUpdater(), new Fields());

TridentState state = topology.newStaticState(factory);
stream = stream.stateQuery(state, new Fields("word"),
        new MongoStateQuery(), new Fields("columnName", "columnValue"));
stream.each(new Fields("word", "columnValue"), new PrintFunction(), new Fields());

----------------------------------------

TITLE: Defining JDBC Connection Provider Interface
DESCRIPTION: Interface definition for database connection pooling mechanisms that provides methods for preparing, getting connections and cleanup.

LANGUAGE: java
CODE:
public interface ConnectionProvider extends Serializable {
    /**
     * method must be idempotent.
     */
    void prepare();

    /**
     *
     * @return a DB connection over which the queries can be executed.
     */
    Connection getConnection();

    /**
     * called once when the system is shutting down, should be idempotent.
     */
    void cleanup();
}

----------------------------------------

TITLE: Implementing SimpleMongoUpdateMapper for Storm-MongoDB Integration
DESCRIPTION: This snippet shows the implementation of SimpleMongoUpdateMapper, which maps Storm tuple fields to MongoDB document fields for update operations using the $set operator.

LANGUAGE: java
CODE:
public class SimpleMongoUpdateMapper implements MongoMapper {
    private String[] fields;

    @Override
    public Document toDocument(ITuple tuple) {
        Document document = new Document();
        for(String field : fields){
            document.append(field, tuple.getValueByField(field));
        }
        return new Document("$set", document);
    }

    public SimpleMongoUpdateMapper withFields(String... fields) {
        this.fields = fields;
        return this;
    }
}

----------------------------------------

TITLE: Implementing Complex DRPC Topology for URL Reach Calculation in Java
DESCRIPTION: This example demonstrates a more complex DRPC topology that calculates the reach of a URL on Twitter using multiple bolts for different stages of computation.

LANGUAGE: java
CODE:
LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("reach");
builder.addBolt(new GetTweeters(), 3);
builder.addBolt(new GetFollowers(), 12)
        .shuffleGrouping();
builder.addBolt(new PartialUniquer(), 6)
        .fieldsGrouping(new Fields("id", "follower"));
builder.addBolt(new CountAggregator(), 2)
        .fieldsGrouping(new Fields("id"));

----------------------------------------

TITLE: Executing Python Topology Script with Storm Shell
DESCRIPTION: This shows how Storm shell executes the Python topology script after uploading resources. It passes Nimbus host, port, and the uploaded jar location as arguments to the script.

LANGUAGE: bash
CODE:
python3 topology.py arg1 arg2 {nimbus-host} {nimbus-port} {uploaded-jar-location}

----------------------------------------

TITLE: Configuring DelimitedRecordHiveMapper
DESCRIPTION: Examples of configuring DelimitedRecordHiveMapper with different options for mapping tuple fields to table columns and partitions, including time-based partitioning.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
    or
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withTimeAsPartitionField("YYYY/MM/DD");

----------------------------------------

TITLE: Creating ACLs for Blobs in Java
DESCRIPTION: Java code snippet showing how to create Access Control Lists (ACLs) for blobs in Storm's distributed cache. This example sets up ACLs and configures the replication factor for a blob.

LANGUAGE: java
CODE:
String stringBlobACL = "u:username:rwa";
AccessControl blobACL = BlobStoreAclHandler.parseAccessControl(stringBlobACL);
List<AccessControl> acls = new LinkedList<AccessControl>();
acls.add(blobACL); // more ACLs can be added here
SettableBlobMeta settableBlobMeta = new SettableBlobMeta(acls);
settableBlobMeta.set_replication_factor(4); // Here we can set the replication factor

----------------------------------------

TITLE: Defining IStateful Bolt Hook Methods in Java
DESCRIPTION: Interface methods for IStateful bolts to implement custom actions before state commit, prepare, and rollback operations.

LANGUAGE: java
CODE:
/**
 * This is a hook for the component to perform some actions just before the
 * framework commits its state.
 */
void preCommit(long txid);

/**
 * This is a hook for the component to perform some actions just before the
 * framework prepares its state.
 */
void prePrepare(long txid);

/**
 * This is a hook for the component to perform some actions just before the
 * framework rolls back the prepared state.
 */
void preRollback();

----------------------------------------

TITLE: Initializing EsIndexBolt for Storm-Elasticsearch Integration in Java
DESCRIPTION: This snippet demonstrates how to create an EsIndexBolt for indexing data into Elasticsearch from Storm. It requires an EsConfig for cluster configuration and an EsTupleMapper for mapping tuples to Elasticsearch documents.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsIndexBolt indexBolt = new EsIndexBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Defining Storm Component Object Structure in Thrift
DESCRIPTION: Thrift union structure defining how component objects (spouts and bolts) can be specified in Storm, supporting Java serialization, shell components, and Java objects.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Running Storm SQL Command
DESCRIPTION: Demonstrates how to use the 'storm sql' command to compile SQL statements into a Storm topology and submit it to the cluster.

LANGUAGE: bash
CODE:
$ bin/storm sql <sql-file> <topo-name>

----------------------------------------

TITLE: Maven Shade Plugin Configuration for Storm-HDFS
DESCRIPTION: Maven configuration for properly packaging Storm topology JARs with HDFS dependencies using shade plugin.

LANGUAGE: xml
CODE:
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-shade-plugin</artifactId>
    <version>1.4</version>
    <configuration>
        <createDependencyReducedPom>true</createDependencyReducedPom>
    </configuration>
    <executions>
        <execution>
            <phase>package</phase>
            <goals>
                <goal>shade</goal>
            </goals>
            <configuration>
                <transformers>
                    <transformer
                            implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                    <transformer
                            implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                        <mainClass></mainClass>
                    </transformer>
                </transformers>
            </configuration>
        </execution>
    </executions>
</plugin>

----------------------------------------

TITLE: Defining HBaseValueMapper Interface in Java
DESCRIPTION: Specifies the interface for transforming HBase lookup results into Storm Values for emission by HBaseLookupBolt.

LANGUAGE: java
CODE:
public interface HBaseValueMapper extends Serializable {
    public List<Values> toTuples(Result result) throws Exception;
    void declareOutputFields(OutputFieldsDeclarer declarer);
}

----------------------------------------

TITLE: Configuring Generic Cluster Resources in YAML
DESCRIPTION: This snippet shows how to specify node resource availability in the Storm cluster by modifying the conf/storm.yaml file. It demonstrates the syntax for defining a map of resource types and their amounts.

LANGUAGE: yaml
CODE:
supervisor.resources.map: {[type<String>] : [amount<Double>]}

LANGUAGE: yaml
CODE:
supervisor.resources.map: {"gpu.count" : 2.0}

----------------------------------------

TITLE: Redis Lookup Mapper Implementation
DESCRIPTION: Implementation of RedisLookupMapper interface for word count lookup operations using Redis Hash data type.

LANGUAGE: java
CODE:
class WordCountRedisLookupMapper implements RedisLookupMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountRedisLookupMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public List<Values> toTuple(ITuple input, Object value) {
        String member = getKeyFromTuple(input);
        List<Values> values = Lists.newArrayList();
        values.add(new Values(member, value));
        return values;
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("wordName", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Executing DRPC Request with Java Client
DESCRIPTION: This snippet demonstrates how to create a DRPC client and execute a request for the 'reach' function with a given URL argument.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.put("storm.thrift.transport", "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin");
conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 3);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL, 10);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING, 20);
DRPCClient client = new DRPCClient(conf, "drpc-host", 3772);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Window Configuration Methods - Java
DESCRIPTION: Available configuration methods for setting up different types of windows including sliding and tumbling windows with count or duration-based parameters.

LANGUAGE: java
CODE:
withWindow(Count windowLength, Count slidingInterval)
withWindow(Count windowLength)
withWindow(Count windowLength, Duration slidingInterval)
withWindow(Duration windowLength, Duration slidingInterval)
withWindow(Duration windowLength)
withWindow(Duration windowLength, Count slidingInterval)
withTumblingWindow(BaseWindowedBolt.Count count)
withTumblingWindow(BaseWindowedBolt.Duration duration)

----------------------------------------

TITLE: Initializing Trident Topology with Persistent Aggregation
DESCRIPTION: Example of creating a Trident topology that performs word count aggregation and persists the results to a Memcached state. This demonstrates using the persistentAggregate operation with an opaque transactional state.

LANGUAGE: Java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
      topology.newStream("spout1", spout)
        .each(new Fields("sentence"), new Split(), new Fields("word"))
        .groupBy(new Fields("word"))
        .persistentAggregate(MemcachedState.opaque(serverLocations), new Count(), new Fields("count"))                
        .parallelismHint(6);

----------------------------------------

TITLE: Configuring HiveState for Trident Topology (Java)
DESCRIPTION: Java code example showing how to configure HiveState for use in a Trident topology. It includes setting up the mapper, HiveOptions, and creating a StateFactory.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withTimeAsPartitionField("YYYY/MM/DD");
            
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
                                .withTxnsPerBatch(10)
                				.withBatchSize(1000)
                	     		.withIdleTimeout(10)
                	     		
StateFactory factory = new HiveStateFactory().withOptions(hiveOptions);
TridentState state = stream.partitionPersist(factory, hiveFields, new HiveUpdater(), new Fields());

----------------------------------------

TITLE: Configuring CassandraWriterBolt with Multiple Queries in Java
DESCRIPTION: Shows how to set up a CassandraWriterBolt that executes multiple insert queries from a single input tuple. This can be used to insert data into multiple tables simultaneously.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
    async(
        simpleQuery("INSERT INTO titles_per_album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all())),
        simpleQuery("INSERT INTO titles_per_performer (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all()))
    )
);

----------------------------------------

TITLE: Implementing Redis Store Mapper
DESCRIPTION: Example implementation of RedisStoreMapper interface for storing word counts using Redis hash data type.

LANGUAGE: java
CODE:
class WordCountStoreMapper implements RedisStoreMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountStoreMapper() {
        description = new RedisDataTypeDescription(
            RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return tuple.getStringByField("count");
    }
}

----------------------------------------

TITLE: Uploading Topology Jar in StormSubmitter (Java)
DESCRIPTION: StormSubmitter uploads the topology jar to Nimbus using its Thrift interface. It uploads the file in 15KB chunks and finalizes the upload process.

LANGUAGE: java
CODE:
// Jar uploading code in StormSubmitter
beginFileUpload();
uploadChunk();
finishFileUpload();

----------------------------------------

TITLE: Implementing Split Sentence Bolt in Clojure
DESCRIPTION: Simple bolt implementation that splits a sentence into individual words and emits each word as a separate tuple.

LANGUAGE: clojure
CODE:
(defbolt split-sentence ["word"] [tuple collector]
  (let [words (.split (.getString tuple 0) " ")]
    (doseq [w words]
      (emit-bolt! collector [w] :anchor tuple))
    (ack! collector tuple)
    ))

----------------------------------------

TITLE: Configuring SimpleJdbcMapper in Java
DESCRIPTION: Example of configuring SimpleJdbcMapper for transforming storm tuples to database rows. It shows how to set up the mapper with table name and connection provider.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);
String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

----------------------------------------

TITLE: Apache Log Parser to JSON Script
DESCRIPTION: Python script to parse Apache logs into JSON format with auto-incrementing IDs using apache-log-parser library.

LANGUAGE: python
CODE:
import sys
import apache_log_parser
import json

auto_incr_id = 1
parser_format = '%a - - %t %D "%r" %s %b "%{Referer}i" "%{User-Agent}i"'
line_parser = apache_log_parser.make_parser(parser_format)
while True:
  # we'll use pipe
  line = sys.stdin.readline()
  if not line:
    break
  parsed_dict = line_parser(line)
  parsed_dict['id'] = auto_incr_id
  auto_incr_id += 1

  parsed_dict = {k.upper(): v for k, v in parsed_dict.iteritems() if not k.endswith('datetimeobj')}
  print(json.dumps(parsed_dict))

----------------------------------------

TITLE: Configuring Storm Metricstore in YAML
DESCRIPTION: YAML configuration options for setting up the Storm Metricstore, including the class implementations, RocksDB location, and retention settings.

LANGUAGE: yaml
CODE:
storm.metricstore.class: "org.apache.storm.metricstore.rocksdb.RocksDbStore"
storm.metricprocessor.class: "org.apache.storm.metricstore.NimbusMetricProcessor"
storm.metricstore.rocksdb.location: "storm_rocks"
storm.metricstore.rocksdb.create_if_missing: true
storm.metricstore.rocksdb.metadata_string_cache_capacity: 4000
storm.metricstore.rocksdb.retention_hours: 240

----------------------------------------

TITLE: Accessing Task Data in Storm
DESCRIPTION: Methods for reading and writing task-level data in Storm components. Available in spout's open and bolt's prepare lifecycle methods.

LANGUAGE: java
CODE:
TopologyContext#setTaskData(String, Object)  // write access
TopologyContext#getTask(String)           // read access

----------------------------------------

TITLE: CSS Syntax Highlighting Classes
DESCRIPTION: Lists syntax highlighting classes for CSS elements including selectors, rules, properties, values, and special constructs like hex colors and functions.



----------------------------------------

TITLE: Setting Up Task in Worker (Clojure)
DESCRIPTION: The mk-task function sets up individual tasks within a worker, including establishing routing functions and initializing spout or bolt-specific code.

LANGUAGE: clojure
CODE:
(defn mk-task [conf^String storm-id^String storm-name^TaskInfo task-info^Integer task-id^WorkerTopologyContext worker-context])

----------------------------------------

TITLE: Configuring Maven Assembly Plugin for Storm Topology JAR Creation
DESCRIPTION: This XML configuration for the Maven Assembly Plugin creates a JAR file containing all dependencies for a Storm topology. It specifies the jar-with-dependencies descriptor and sets the main class for the topology.

LANGUAGE: xml
CODE:
<plugin>
  <artifactId>maven-assembly-plugin</artifactId>
  <configuration>
    <descriptorRefs>  
      <descriptorRef>jar-with-dependencies</descriptorRef>
    </descriptorRefs>
    <archive>
      <manifest>
        <mainClass>com.path.to.main.Class</mainClass>
      </manifest>
    </archive>
  </configuration>
</plugin>

----------------------------------------

TITLE: Setting Memory Requirements for Storm Components
DESCRIPTION: Java API examples showing how to set memory requirements for spouts and bolts

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.setMemoryLoad(1024.0, 512.0);
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
            .shuffleGrouping("word").setMemoryLoad(512.0);

----------------------------------------

TITLE: Configuring Isolation Scheduler in YAML
DESCRIPTION: YAML configuration example showing how to allocate dedicated machines to specific topologies using the Isolation Scheduler. Maps topology names to their allocated machine count for isolated execution.

LANGUAGE: yaml
CODE:
isolation.scheduler.machines: 
    "my-topology": 8
    "tiny-topology": 1
    "some-other-topology": 3

----------------------------------------

TITLE: Value Mapper Example
DESCRIPTION: Demonstrates using value mappers to extract typed values from tuples in a stream.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

// extract the first field from the tuple to get a Stream<String> of sentences
Stream<String> sentences = builder.newStream(new TestWordSpout(), new ValueMapper<String>(0));

// extract first three fields of the tuple emitted by the spout to produce a stream of typed tuples.
Stream<Tuple3<String, Integer, Long>> stream = builder.newStream(new TestSpout(), TupleValueMappers.of(0, 1, 2));

----------------------------------------

TITLE: Implementing Trident Map Function
DESCRIPTION: Example of implementing a mapping function that converts strings to uppercase in a Trident stream.

LANGUAGE: java
CODE:
public class UpperCase extends MapFunction {
 @Override
 public Values execute(TridentTuple input) {
   return new Values(input.getString(0).toUpperCase());
 }
}

----------------------------------------

TITLE: Initializing SimpleJdbcMapper for Storm-JDBC Integration
DESCRIPTION: This code snippet shows how to initialize a SimpleJdbcMapper instance, which is a general-purpose JdbcMapper implementation. It demonstrates configuration with a table name and connection provider, as well as initialization with an explicit column schema.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);
String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

// Initializing with explicit column schema
List<Column> columnSchema = Lists.newArrayList(
    new Column("user_id", java.sql.Types.INTEGER),
    new Column("user_name", java.sql.Types.VARCHAR));
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(columnSchema);

----------------------------------------

TITLE: Configuring ExpandUrl Bolt with Shuffle Grouping in Java
DESCRIPTION: This snippet demonstrates how to set up an ExpandUrl bolt with shuffle grouping in a Storm topology. Shuffle grouping distributes tuples randomly across the bolt's tasks.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .shuffleGrouping(1);

----------------------------------------

TITLE: Implementing Custom State Interface
DESCRIPTION: Example of a custom State implementation for a location database, showing the basic structure and required methods.

LANGUAGE: java
CODE:
public class LocationDB implements State {
    public void beginCommit(Long txid) {    
    }
    
    public void commit(Long txid) {    
    }
    
    public void setLocation(long userId, String location) {
      // code to access database and set location
    }
    
    public String getLocation(long userId) {
      // code to get location from database
    }
}

----------------------------------------

TITLE: Shell Component Constructor Configuration in Java
DESCRIPTION: Example of configuring a custom ShellSpout constructor to work with executables shipped via blob store by disabling the default working directory change.

LANGUAGE: java
CODE:
public MyShellSpout() {
    super("./newPython/bin/python3", "./shell_spout.py");
    changeChildCWD(false);
}

----------------------------------------

TITLE: Implementing QueryFunction for State Querying
DESCRIPTION: Example of implementing a QueryFunction to query a custom State (LocationDB) for user locations.

LANGUAGE: java
CODE:
public class QueryLocation extends BaseQueryFunction<LocationDB, String> {
    public List<String> batchRetrieve(LocationDB state, List<TridentTuple> inputs) {
        List<String> ret = new ArrayList();
        for(TridentTuple input: inputs) {
            ret.add(state.getLocation(input.getLong(0)));
        }
        return ret;
    }

    public void execute(TridentTuple tuple, String location, TridentCollector collector) {
        collector.emit(new Values(location));
    }    
}

----------------------------------------

TITLE: Configuring Impersonation Authorization in YAML
DESCRIPTION: YAML configuration for setting up impersonation authorization in Storm using the ImpersonationAuthorizer.

LANGUAGE: yaml
CODE:
nimbus.impersonation.authorizer: org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer
nimbus.impersonation.acl:
    impersonating_user1:
        hosts:
            [comma separated list of hosts from which impersonating_user1 is allowed to impersonate other users]
        groups:
            [comma separated list of groups whose users impersonating_user1 is allowed to impersonate]
    impersonating_user2:
        hosts:
            [comma separated list of hosts from which impersonating_user2 is allowed to impersonate other users]
        groups:
            [comma separated list of groups whose users impersonating_user2 is allowed to impersonate]

----------------------------------------

TITLE: Storm JoinBolt Implementation
DESCRIPTION: Java code showing how to implement the equivalent SQL join using Storm's JoinBolt, including window configuration and fields grouping

LANGUAGE: java
CODE:
JoinBolt jbolt =  new JoinBolt("spout1", "key1")                   // from        spout1  
                    .join     ("spout2", "userId",  "spout1")      // inner join  spout2  on spout2.userId = spout1.key1
                    .join     ("spout3", "key3",    "spout2")      // inner join  spout3  on spout3.key3   = spout2.userId   
                    .leftJoin ("spout4", "key4",    "spout3")      // left join   spout4  on spout4.key4   = spout3.key3
                    .select  ("userId, key4, key2, spout3:key3")   // chose output fields
                    .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;

topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("spout1", new Fields("key1") )
            .fieldsGrouping("spout2", new Fields("userId") )
            .fieldsGrouping("spout3", new Fields("key3") )
            .fieldsGrouping("spout4", new Fields("key4") );

----------------------------------------

TITLE: Configuring Authorization in Storm YAML
DESCRIPTION: YAML configuration for setting up authorization in Storm

LANGUAGE: yaml
CODE:
nimbus.authorizer: "org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer"

multitenant.scheduler.user.pools: 
    "evans": 10
    "derek": 10

supervisor.run.worker.as.user: true

nimbus.impersonation.authorizer: org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer
nimbus.impersonation.acl:
    impersonating_user1:
        hosts:
            [comma separated list of hosts from which impersonating_user1 is allowed to impersonate other users]
        groups:
            [comma separated list of groups whose users impersonating_user1 is allowed to impersonate]
    impersonating_user2:
        hosts:
            [comma separated list of hosts from which impersonating_user2 is allowed to impersonate other users]
        groups:
            [comma separated list of groups whose users impersonating_user2 is allowed to impersonate]

----------------------------------------

TITLE: Registering Event Logger in Java for Apache Storm Topology
DESCRIPTION: This code snippet shows how to register an event logger to an Apache Storm topology configuration using Java. It demonstrates adding the FileBasedEventLogger class.

LANGUAGE: java
CODE:
conf.registerEventLogger(org.apache.storm.metric.FileBasedEventLogger.class);

----------------------------------------

TITLE: Running Storm SQL in Explain Mode
DESCRIPTION: Command to run Storm SQL in explain mode to show the query plan instead of submitting the topology.

LANGUAGE: bash
CODE:
$ bin/storm sql order_filtering.sql --explain --artifacts "org.apache.storm:storm-sql-kafka:2.0.0-SNAPSHOT,org.apache.storm:storm-kafka-client:2.0.0-SNAPSHOT,org.apache.kafka:kafka-clients:1.1.0^org.slf4j:slf4j-log4j12"

----------------------------------------

TITLE: Configuring EsConfig for Storm-Elasticsearch Integration in Java
DESCRIPTION: Creates an EsConfig object to configure the Elasticsearch cluster connection. It can be initialized with just the cluster name and node addresses, or with additional parameters for the Elasticsearch Transport Client.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});

LANGUAGE: java
CODE:
Map<String, String> additionalParameters = new HashMap<>();
additionalParameters.put("client.transport.sniff", "true");
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"}, additionalParameters);

----------------------------------------

TITLE: Creating a Flux-Enabled Topology JAR with Maven
DESCRIPTION: Maven configuration to create a fat JAR containing Flux and all dependencies for a Storm topology.

LANGUAGE: xml
CODE:
<!-- include Flux and user dependencies in the shaded jar -->
<dependencies>
    <!-- Flux include -->
    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>flux-core</artifactId>
        <version>${storm.version}</version>
    </dependency>
    <!-- Flux Wrappers include -->
    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>flux-wrappers</artifactId>
        <version>${storm.version}</version>
    </dependency>

    <!-- add user dependencies here... -->

</dependencies>
<!-- create a fat jar that includes all dependencies -->
<build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-shade-plugin</artifactId>
            <version>1.4</version>
            <configuration>
                <createDependencyReducedPom>true</createDependencyReducedPom>
            </configuration>
            <executions>
                <execution>
                    <phase>package</phase>
                    <goals>
                        <goal>shade</goal>
                    </goals>
                    <configuration>
                        <transformers>
                            <transformer
                                    implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                            <transformer
                                    implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                <mainClass>org.apache.storm.flux.Flux</mainClass>
                            </transformer>
                        </transformers>
                    </configuration>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>

----------------------------------------

TITLE: Task ID Determination for Tuple Sending in Apache Storm (Clojure)
DESCRIPTION: The 'tasks-fn' returns the task IDs to send tuples to for either regular stream emit or direct stream emit.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L207

----------------------------------------

TITLE: Configuring UI and Logviewer Filters in YAML
DESCRIPTION: YAML configuration for setting up authentication filters for Storm UI and Logviewer processes.

LANGUAGE: yaml
CODE:
ui.filter: "filter.class"
ui.filter.params: "param1":"value1"
logviewer.filter: "filter.class"
logviewer.filter.params: "param1":"value1"

----------------------------------------

TITLE: Setting Shared Memory Configuration
DESCRIPTION: Shows how to configure shared memory between components in a Storm topology

LANGUAGE: java
CODE:
builder.setBolt("exclaim1", new ExclamationBolt(), 3).shuffleGrouping("word")
      .addSharedMemory(new SharedOnHeap(100, "exclaim-cache"));

----------------------------------------

TITLE: Creating CassandraWriterBolt with Multiple Insert Queries in Java
DESCRIPTION: Demonstrates how to create a CassandraWriterBolt that executes multiple insert queries from a single input tuple.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
    async(
        simpleQuery("INSERT INTO titles_per_album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all())),
        simpleQuery("INSERT INTO titles_per_performer (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all()))
    )
);

----------------------------------------

TITLE: Defining Storm Component Object Structure in Thrift
DESCRIPTION: Thrift union structure that defines how component code (spouts and bolts) can be specified in Storm. Supports serialized Java, shell components, and Java objects.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Spout Tuple Transfer in Apache Storm (Clojure)
DESCRIPTION: This code shows how spouts use the worker-provided transfer function to send tuples in Storm.

LANGUAGE: Clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L329

----------------------------------------

TITLE: Acknowledging Tuple in Storm Multi-Language Protocol
DESCRIPTION: This JSON structure represents an ack command in the Storm multi-language protocol. It includes the command type and the id of the tuple to be acknowledged.

LANGUAGE: json
CODE:
{
    "command": "ack",
    "id": 123123
}

----------------------------------------

TITLE: Starting Profiler in JSON
DESCRIPTION: Sample response from the /api/v1/topology/<id>/profiling/start/<host-port>/<timeout> endpoint, showing the result of starting a profiler on a worker.

LANGUAGE: json
CODE:
{
   "status": "ok",
   "id": "10.11.1.7:6701",
   "timeout": "10",
   "dumplink": "http:\/\/10.11.1.7:8000\/dumps\/wordcount-1-1446614150\/10.11.1.7%3A6701"
}

----------------------------------------

TITLE: Configuring SSL/TLS for MQTT in Java
DESCRIPTION: Examples of configuring SSL/TLS for MQTT connections using DefaultKeyStoreLoader in Java.

LANGUAGE: java
CODE:
DefaultKeyStoreLoader ksl = new DefaultKeyStoreLoader("/path/to/keystore.jks", "/path/to/truststore.jks");
ksl.setKeyStorePassword("password");
ksl.setTrustStorePassword("password");
//...

LANGUAGE: java
CODE:
DefaultKeyStoreLoader ksl = new DefaultKeyStoreLoader("/path/to/keystore.jks");
ksl.setKeyStorePassword("password");
//...

----------------------------------------

TITLE: Creating SolrJsonMapper in Java
DESCRIPTION: This snippet demonstrates how to create a SolrJsonMapper object to update the 'gettingstarted' Solr collection with JSON content from a tuple field named 'JSON'.

LANGUAGE: java
CODE:
SolrMapper solrMapper = new SolrJsonMapper.Builder("gettingstarted", "JSON").build();

----------------------------------------

TITLE: Implementing Custom Task-Level Metrics
DESCRIPTION: Example showing how to implement custom task-level metrics by registering an IMetric instance and incrementing it in a bolt's execute method.

LANGUAGE: java
CODE:
private transient CountMetric countMetric;

@Override
public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
	// other initialization here.
	countMetric = new CountMetric();
	context.registerMetric("execute_count", countMetric, 60);
}

public void execute(Tuple input) {
	countMetric.incr();
	// handle tuple here.	
}

----------------------------------------

TITLE: Defining User Function in Storm SQL
DESCRIPTION: SQL statement to create a user-defined function named MYPLUS using the org.apache.storm.sql.TestUtils$MyPlus class. This function can be used in SQL queries as a scalar function.

LANGUAGE: sql
CODE:
CREATE FUNCTION MYPLUS AS 'org.apache.storm.sql.TestUtils$MyPlus'

----------------------------------------

TITLE: Defining QueryFilterCreator Interface for Storm-MongoDB Integration in Java
DESCRIPTION: Defines the QueryFilterCreator interface used to create MongoDB query filters from Storm tuples. This interface is essential for updating specific documents in MongoDB collections based on Storm tuple data.

LANGUAGE: java
CODE:
public interface QueryFilterCreator extends Serializable {
    Bson createFilter(ITuple tuple);
}

----------------------------------------

TITLE: Creating Hive Table for Streaming
DESCRIPTION: SQL statement to create a partitioned table in Hive with ORC format for streaming data ingestion.

LANGUAGE: sql
CODE:
create table test_table ( id INT, name STRING, phone STRING, street STRING) partitioned by (city STRING, state STRING) stored as orc tblproperties ("orc.compress"="NONE");

----------------------------------------

TITLE: Secure HDFS Configuration
DESCRIPTION: Additional Storm configuration for accessing secure HDFS

LANGUAGE: bash
CODE:
storm.hdfs.login.keytab: /etc/keytab
storm.hdfs.login.principal: primary/instance@REALM

----------------------------------------

TITLE: Defining Storm Topology in Clojure
DESCRIPTION: Example of defining a Storm topology with spouts and bolts using the topology function. Shows how to wire components together with different stream groupings.

LANGUAGE: clojure
CODE:
(topology
 {"1" (spout-spec sentence-spout)
  "2" (spout-spec (sentence-spout-parameterized
                   ["the cat jumped over the door"
                    "greetings from a faraway land"])
                   :p 2)}
 {"3" (bolt-spec {"1" :shuffle "2" :shuffle}
                 split-sentence
                 :p 5)
  "4" (bolt-spec {"3" ["word"]}
                 word-count
                 :p 6)})

----------------------------------------

TITLE: Configuring Storm Cluster State Store
DESCRIPTION: Basic configuration to enable Pacemaker as the cluster state store in Storm.

LANGUAGE: yaml
CODE:
storm.cluster.state.store: "org.apache.storm.cluster.PaceMakerStateStorageFactory"

----------------------------------------

TITLE: Task Routing Map Implementation in Apache Storm (Clojure)
DESCRIPTION: This code implements the routing map used by tasks to determine message routing based on stream and component IDs.

LANGUAGE: Clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L198

----------------------------------------

TITLE: Defining HBaseMapper Interface in Java
DESCRIPTION: The HBaseMapper interface defines methods for converting Storm tuples to HBase row keys and columns.

LANGUAGE: java
CODE:
public interface HBaseMapper extends Serializable {
    byte[] rowKey(Tuple tuple);

    ColumnList columns(Tuple tuple);
}

----------------------------------------

TITLE: Configuring Authorization in Storm YAML
DESCRIPTION: YAML configuration for setting up authorization in Storm using SimpleACLAuthorizer.

LANGUAGE: yaml
CODE:
nimbus.authorizer: "org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer"

multitenant.scheduler.user.pools: 
    "evans": 10
    "derek": 10

supervisor.run.worker.as.user: true

nimbus.impersonation.authorizer: org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer
nimbus.impersonation.acl:
    impersonating_user1:
        hosts:
            [comma separated list of hosts from which impersonating_user1 is allowed to impersonate other users]
        groups:
            [comma separated list of groups whose users impersonating_user1 is allowed to impersonate]
    impersonating_user2:
        hosts:
            [comma separated list of hosts from which impersonating_user2 is allowed to impersonate other users]
        groups:
            [comma separated list of groups whose users impersonating_user2 is allowed to impersonate]

----------------------------------------

TITLE: Installing Storm Redis Maven Dependency
DESCRIPTION: Maven dependency configuration for including storm-redis in a Storm project.

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>storm-redis</artifactId>
    <version>${storm.version}</version>
    <type>jar</type>
</dependency>

----------------------------------------

TITLE: Configuring SolrFieldsMapper with Custom Token
DESCRIPTION: Demonstrates creating a SolrFieldsMapper with custom multivalue field token for the 'gettingstarted' collection.

LANGUAGE: java
CODE:
    new SolrFieldsMapper.Builder(
            new RestJsonSchemaBuilder("localhost", "8983", "gettingstarted"), "gettingstarted")
                .setMultiValueFieldToken("%").build();

----------------------------------------

TITLE: Creating IConfigLoader Factory Method
DESCRIPTION: Factory method for creating an IConfigLoader instance based on the scheduler.config.loader.uri scheme.

LANGUAGE: java
CODE:
ConfigLoaderFactoryService.createConfigLoader(Map<String, Object> conf)

----------------------------------------

TITLE: Implementing Reach Calculation with DRPC
DESCRIPTION: Topology that computes URL reach using DRPC, demonstrating parallel processing of database queries and aggregations.

LANGUAGE: java
CODE:
TridentState urlToTweeters =
       topology.newStaticState(getUrlToTweetersState());
TridentState tweetersToFollowers =
       topology.newStaticState(getTweeterToFollowersState());

topology.newDRPCStream("reach")
       .stateQuery(urlToTweeters, new Fields("args"), new MapGet(), new Fields("tweeters"))
       .each(new Fields("tweeters"), new ExpandList(), new Fields("tweeter"))
       .shuffle()
       .stateQuery(tweetersToFollowers, new Fields("tweeter"), new MapGet(), new Fields("followers"))
       .parallelismHint(200)
       .each(new Fields("followers"), new ExpandList(), new Fields("follower"))
       .groupBy(new Fields("follower"))
       .aggregate(new One(), new Fields("one"))
       .parallelismHint(20)
       .aggregate(new Count(), new Fields("reach"));

----------------------------------------

TITLE: Implementing StateUpdater for State Updates
DESCRIPTION: Example of implementing a StateUpdater (LocationUpdater) to update a custom State object (LocationDB) with new location information.

LANGUAGE: Java
CODE:
public class LocationUpdater extends BaseStateUpdater<LocationDB> {
    public void updateState(LocationDB state, List<TridentTuple> tuples, TridentCollector collector) {
        List<Long> ids = new ArrayList<Long>();
        List<String> locations = new ArrayList<String>();
        for(TridentTuple t: tuples) {
            ids.add(t.getLong(0));
            locations.add(t.getString(1));
        }
        state.setLocationsBulk(ids, locations);
    }
}

----------------------------------------

TITLE: Implementing RedisFilterMapper for Blacklist Filtering
DESCRIPTION: Java class implementing RedisFilterMapper interface for filtering words based on a blacklist stored in Redis. This mapper is used with RedisFilterBolt.

LANGUAGE: java
CODE:
class BlacklistWordFilterMapper implements RedisFilterMapper {
    private RedisDataTypeDescription description;
    private final String setKey = "blacklist";

    public BlacklistWordFilterMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.SET, setKey);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Configuring Metric Reporters in YAML
DESCRIPTION: Example configuration for setting up Graphite and Console reporters with custom reporting periods and filters.

LANGUAGE: yaml
CODE:
topology.metrics.reporters:
  # Graphite Reporter
  - class: "org.apache.storm.metrics2.reporters.GraphiteStormReporter"
    report.period: 60
    report.period.units: "SECONDS"
    graphite.host: "localhost"
    graphite.port: 2003

  # Console Reporter
  - class: "org.apache.storm.metrics2.reporters.ConsoleStormReporter"
    report.period: 10
    report.period.units: "SECONDS"
    filter:
        class: "org.apache.storm.metrics2.filters.RegexFilter"
        expression: ".*my_component.*emitted.*"

----------------------------------------

TITLE: Configuring HBase State Provider in Storm
DESCRIPTION: JSON configuration for using HBase as a state provider in Storm. Specifies HBase configuration key, table name, column family, key/value classes, and serializer implementations.

LANGUAGE: json
CODE:
{
  "keyClass": "Optional fully qualified class name of the Key type.",
  "valueClass": "Optional fully qualified class name of the Value type.",
  "keySerializerClass": "Optional Key serializer implementation class.",
  "valueSerializerClass": "Optional Value Serializer implementation class.",
  "hbaseConfigKey": "config key to load hbase configuration from storm root configuration. (similar to storm-hbase)",
  "tableName": "Pre-created table name for state.",
  "columnFamily": "Pre-created column family for state."
}

----------------------------------------

TITLE: Pinning Worker to NUMA Zone using CgroupManager in Linux
DESCRIPTION: This snippet shows how the CgroupManager prefixes the worker launch command with the numactl command to bind the worker to a specific NUMA zone's CPU cores and memory.

LANGUAGE: bash
CODE:
numactl --cpunodebind=<numaId>> --membind=<numaId> <worker launch command>

----------------------------------------

TITLE: Impersonating User Request Examples
DESCRIPTION: Examples of making authenticated REST API requests with user impersonation using doAsUser parameter

LANGUAGE: no-highlight
CODE:
1. http://ui-daemon-host-name:8080/api/v1/topology/wordcount-1-1425844354\?doAsUser=testUSer1
2. curl 'http://localhost:8080/api/v1/topology/wordcount-1-1425844354/activate' -X POST -H 'doAsUser:testUSer1'

----------------------------------------

TITLE: Storm Buffer Configuration Properties
DESCRIPTION: Configuration properties for controlling message queue buffer sizes in Storm topologies. These settings affect both executor receive buffers and worker transfer queues.

LANGUAGE: properties
CODE:
topology.executor.receive.buffer.size
topology.transfer.buffer.size

----------------------------------------

TITLE: Configuring YAML Front Matter for Storm Documentation
DESCRIPTION: YAML front matter block defining metadata for the Storm documentation page. It specifies the title, layout, and documentation flag for the page.

LANGUAGE: yaml
CODE:
---
title: Dynamic Worker Profiling
layout: documentation
documentation: true
---

----------------------------------------

TITLE: Storm OCI Runtime Configuration
DESCRIPTION: Example Storm configuration for using the OCI/Squashfs runtime

LANGUAGE: bash
CODE:
storm.resource.isolation.plugin: "org.apache.storm.container.oci.RuncLibContainerManager"

storm.oci.allowed.images:
    - "storm/rhel7:dev_current"
    - "storm/rhel7:dev_previous"
    - "storm/rhel7:dev_test"
storm.oci.image: "storm/rhel7:dev_current"

storm.oci.cgroup.parent: "/storm"
storm.oci.cgroup.root: "/sys/fs/cgroup"
storm.oci.image.hdfs.toplevel.dir: "hdfs://host:port/containers/"
storm.oci.image.tag.to.manifest.plugin: "org.apache.storm.container.oci.LocalOrHdfsImageTagToManifestPlugin"
storm.oci.local.or.hdfs.image.tag.to.manifest.plugin.hdfs.hash.file: "hdfs://host:port/containers/image-tag-to-hash"
storm.oci.manifest.to.resources.plugin: "org.apache.storm.container.oci.HdfsManifestToResourcesPlugin"
storm.oci.readonly.bindmounts:
    - "/home/y/lib64/storm"
    - "/etc/krb5.conf"

storm.oci.resources.localizer: "org.apache.storm.container.oci.HdfsOciResourcesLocalizer"
storm.oci.seccomp.profile: "/home/y/conf/storm/seccomp.json"

----------------------------------------

TITLE: Storm DRPC Metrics Table Schema
DESCRIPTION: Table structure defining metrics related to DRPC (Distributed Remote Procedure Call) server operations.

LANGUAGE: markdown
CODE:
| Metric Name | Type | Description |
|-------------|------|-------------|
| drpc:HTTP-request-response-duration | timer | how long it takes to execute... |

----------------------------------------

TITLE: Custom MQTT Tuple Mapper Implementation in Java
DESCRIPTION: Example implementation of MqttTupleMapper that converts Storm tuples into MQTT messages with specific topic and payload formats.

LANGUAGE: java
CODE:
public class MyTupleMapper implements MqttTupleMapper {
    public MqttMessage toMessage(ITuple tuple) {
        String topic = "users/" + tuple.getStringByField("userId") + "/" + tuple.getStringByField("device");
        byte[] payload = tuple.getStringByField("message").getBytes();
        return new MqttMessage(topic, payload);
    }
}

----------------------------------------

TITLE: Adding Counter Columns in Java
DESCRIPTION: Example of adding counter columns to a ColumnList for HBase operations.

LANGUAGE: java
CODE:
ColumnList cols = new ColumnList();
cols.addCounter(this.columnFamily, field.getBytes(), toLong(tuple.getValueByField(field)));

----------------------------------------

TITLE: Configuring UI and Logviewer Authentication Filters
DESCRIPTION: YAML configuration for setting up authentication filters for Storm UI and logviewer components using Java servlet filters

LANGUAGE: yaml
CODE:
ui.filter: "filter.class"
ui.filter.params: "param1":"value1"
logviewer.filter: "filter.class"
logviewer.filter.params: "param1":"value1"

----------------------------------------

TITLE: Executing Storm Shell Command for Topology Submission
DESCRIPTION: This command demonstrates how to use the 'storm shell' command to submit a topology. It packages resources into a jar, uploads it to Nimbus, and executes a Python script with specified arguments.

LANGUAGE: shell
CODE:
storm shell resources/ python topology.py arg1 arg2

----------------------------------------

TITLE: Entering Container for Debugging
DESCRIPTION: Command to enter a running container for debugging purposes

LANGUAGE: bash
CODE:
sudo nsenter --target <container-pid> --pid --mount --setuid <uid> --setgid <gid>

----------------------------------------

TITLE: Initializing SimpleJdbcMapper with Explicit Column Schema in Java for Storm JDBC
DESCRIPTION: Example of initializing SimpleJdbcMapper with an explicit column schema. This is necessary when using a custom insert query or when dealing with a subset of table columns.

LANGUAGE: java
CODE:
List<Column> columnSchema = Lists.newArrayList(
    new Column("user_id", java.sql.Types.INTEGER),
    new Column("user_name", java.sql.Types.VARCHAR));
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(columnSchema);

----------------------------------------

TITLE: Implementing QueryFunction for Custom State
DESCRIPTION: Example of implementing a QueryFunction (QueryLocation) to retrieve location information from a custom State (LocationDB).

LANGUAGE: java
CODE:
public class QueryLocation extends BaseQueryFunction<LocationDB, String> {
    public List<String> batchRetrieve(LocationDB state, List<TridentTuple> inputs) {
        List<String> ret = new ArrayList();
        for(TridentTuple input: inputs) {
            ret.add(state.getLocation(input.getLong(0)));
        }
        return ret;
    }

    public void execute(TridentTuple tuple, String location, TridentCollector collector) {
        collector.emit(new Values(location));
    }    
}

----------------------------------------

TITLE: Setting up Trident State for Storm-RocketMQ Integration in Java
DESCRIPTION: This snippet illustrates how to set up a RocketMQ persistent Trident state for use in Trident topologies. It configures the state with a TupleToMessageMapper, TopicSelector, and RocketMQ properties, and demonstrates how to integrate it into a Trident topology.

LANGUAGE: java
CODE:
TupleToMessageMapper mapper = new FieldNameBasedTupleToMessageMapper("word", "count");
TopicSelector selector = new DefaultTopicSelector(topic);

Properties properties = new Properties();
properties.setProperty(RocketMqConfig.NAME_SERVER_ADDR, nameserverAddr);

RocketMqState.Options options = new RocketMqState.Options()
        .withMapper(mapper)
        .withSelector(selector)
        .withProperties(properties);

StateFactory factory = new RocketMqStateFactory(options);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory, fields,
        new RocketMqStateUpdater(), new Fields());

----------------------------------------

TITLE: Configuring OpenTSDB Bolt in Storm Topology
DESCRIPTION: This snippet demonstrates how to set up and configure an OpenTSDB bolt in a Storm topology. It creates an OpenTsdbClient, configures the OpenTsdbBolt with a tuple mapper, and sets batch size and flush interval parameters.

LANGUAGE: java
CODE:
OpenTsdbClient.Builder builder =  OpenTsdbClient.newBuilder(openTsdbUrl).sync(30_000).returnDetails();
final OpenTsdbBolt openTsdbBolt = new OpenTsdbBolt(builder, Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
openTsdbBolt.withBatchSize(10).withFlushInterval(2000);
topologyBuilder.setBolt("opentsdb", openTsdbBolt).shuffleGrouping("metric-gen");

----------------------------------------

TITLE: RecordToTupleMapper Interface Definition in Java
DESCRIPTION: Interface definition for converting Kinesis records to Storm tuples. Includes methods for getting output fields and converting records to tuples.

LANGUAGE: java
CODE:
    Fields getOutputFields ();
    List<Object> getTuple (Record record);

----------------------------------------

TITLE: Emitting Messages with Spout Collector
DESCRIPTION: Example showing how to emit a message with an ID using the SpoutOutputCollector.

LANGUAGE: java
CODE:
_collector.emit(new Values("field1", "field2", 3) , msgId);

----------------------------------------

TITLE: Initializing CassandraWriterBolt with Insert Query in Java
DESCRIPTION: Demonstrates how to create a CassandraWriterBolt with a simple insert query using specified tuple fields. This snippet shows the basic setup for inserting data into a Cassandra table named 'album'.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
    async(
        simpleQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);")
            .with(
                fields("title", "year", "performer", "genre", "tracks")
             )
        )
);

----------------------------------------

TITLE: Configuring HDFS Sequence File Bolt in Java
DESCRIPTION: Example of how to configure and set up an HDFS Sequence File Bolt to write data to HDFS sequence files with specific compression settings.

LANGUAGE: java
CODE:
// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withExtension(".seq")
        .withPath("/data/");

// create sequence format instance.
DefaultSequenceFormat format = new DefaultSequenceFormat("timestamp", "sentence");

SequenceFileBolt bolt = new SequenceFileBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withSequenceFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy)
        .withCompressionType(SequenceFile.CompressionType.RECORD)
        .withCompressionCodec("deflate");

----------------------------------------

TITLE: Defining JdbcLookupMapper Interface in Java
DESCRIPTION: Defines the JdbcLookupMapper interface for executing select queries and enriching Storm tuples. It includes methods for declaring output fields, getting columns, and mapping results to tuples.

LANGUAGE: java
CODE:
    void declareOutputFields(OutputFieldsDeclarer declarer);
    List<Column> getColumns(ITuple tuple);
    List<Values> toTuple(ITuple input, List<Column> columns);

----------------------------------------

TITLE: Error Log Filtering SQL
DESCRIPTION: Storm SQL query to filter error logs (status >= 400) from Apache logs and write to a new Kafka topic

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://apache-logs?bootstrap-servers=localhost:9092'
CREATE EXTERNAL TABLE APACHE_ERROR_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_ELAPSED_MS INT) LOCATION 'kafka://apache-error-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO APACHE_ERROR_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, (TIME_US / 1000) AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (CAST(STATUS AS INT) / 100) >= 4

----------------------------------------

TITLE: Local Mode Implementation using Java Queues in Apache Storm (Clojure)
DESCRIPTION: Implementation of the message sending protocol using in-memory Java queues for local mode in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/local.clj

----------------------------------------

TITLE: Registering Metrics Consumer in YAML
DESCRIPTION: Example of how to register metrics consumers in the Storm YAML configuration file.

LANGUAGE: yaml
CODE:
topology.metrics.consumer.register:
  - class: "org.apache.storm.metric.LoggingMetricsConsumer"
    parallelism.hint: 1
  - class: "org.apache.storm.metric.HttpForwardingMetricsConsumer"
    parallelism.hint: 1
    argument: "http://example.com:8080/metrics/my-topology/"

----------------------------------------

TITLE: Adding Generic Resources to Storm Components
DESCRIPTION: API method for specifying resource requirements for topology components like Spouts or Bolts. The method takes a resource name and value as parameters.

LANGUAGE: java
CODE:
public T addResource(String resourceName, Number resourceValue)

----------------------------------------

TITLE: Configuring Automatic Task Hooks in Storm Configuration
DESCRIPTION: Shows how to configure automatic task hooks using the Storm configuration property 'topology.auto.task.hooks'. These hooks are automatically registered in every spout or bolt.

LANGUAGE: java
CODE:
Config#TOPOLOGY_AUTO_TASK_HOOKS

----------------------------------------

TITLE: SQL Grammar Definition in BNF Form
DESCRIPTION: Defines the grammar for supported SQL statements in Storm SQL using Backus-Naur Form notation.

LANGUAGE: sql
CODE:
statement:
      setStatement
  |   resetStatement
  |   explain
  |   describe
  |   insert
  |   update
  |   merge
  |   delete
  |   query

setStatement:
      [ ALTER ( SYSTEM | SESSION ) ] SET identifier '=' expression

resetStatement:
      [ ALTER ( SYSTEM | SESSION ) ] RESET identifier
  |   [ ALTER ( SYSTEM | SESSION ) ] RESET ALL

explain:
      EXPLAIN PLAN
      [ WITH TYPE | WITH IMPLEMENTATION | WITHOUT IMPLEMENTATION ]
      [ EXCLUDING ATTRIBUTES | INCLUDING [ ALL ] ATTRIBUTES ]
      FOR ( query | insert | update | merge | delete )

describe:
      DESCRIBE DATABASE databaseName
   |  DESCRIBE CATALOG [ databaseName . ] catalogName
   |  DESCRIBE SCHEMA [ [ databaseName . ] catalogName ] . schemaName
   |  DESCRIBE [ TABLE ] [ [ [ databaseName . ] catalogName . ] schemaName . ] tableName [ columnName ]
   |  DESCRIBE [ STATEMENT ] ( query | insert | update | merge | delete )

insert:
      ( INSERT | UPSERT ) INTO tablePrimary
      [ '(' column [, column ]* ')' ]
      query

update:
      UPDATE tablePrimary
      SET assign [, assign ]*
      [ WHERE booleanExpression ]

assign:
      identifier '=' expression

merge:
      MERGE INTO tablePrimary [ [ AS ] alias ]
      USING tablePrimary
      ON booleanExpression
      [ WHEN MATCHED THEN UPDATE SET assign [, assign ]* ]
      [ WHEN NOT MATCHED THEN INSERT VALUES '(' value [ , value ]* ')' ]

delete:
      DELETE FROM tablePrimary [ [ AS ] alias ]
      [ WHERE booleanExpression ]

query:
      values
  |   WITH withItem [ , withItem ]* query
  |   {
          select
      |   selectWithoutFrom
      |   query UNION [ ALL ] query
      |   query EXCEPT query
      |   query INTERSECT query
      }
      [ ORDER BY orderItem [, orderItem ]* ]
      [ LIMIT { count | ALL } ]
      [ OFFSET start { ROW | ROWS } ]
      [ FETCH { FIRST | NEXT } [ count ] { ROW | ROWS } ]

withItem:
      name
      [ '(' column [, column ]* ')' ]
      AS '(' query ')'

orderItem:
      expression [ ASC | DESC ] [ NULLS FIRST | NULLS LAST ]

select:
      SELECT [ STREAM ] [ ALL | DISTINCT ]
          { * | projectItem [, projectItem ]* }
      FROM tableExpression
      [ WHERE booleanExpression ]
      [ GROUP BY { groupItem [, groupItem ]* } ]
      [ HAVING booleanExpression ]
      [ WINDOW windowName AS windowSpec [, windowName AS windowSpec ]* ]

selectWithoutFrom:
      SELECT [ ALL | DISTINCT ]
          { * | projectItem [, projectItem ]* }

projectItem:
      expression [ [ AS ] columnAlias ]
  |   tableAlias . *

tableExpression:
      tableReference [, tableReference ]*
  |   tableExpression [ NATURAL ] [ LEFT | RIGHT | FULL ] JOIN tableExpression [ joinCondition ]

joinCondition:
      ON booleanExpression
  |   USING '(' column [, column ]* ')'

tableReference:
      tablePrimary
      [ [ AS ] alias [ '(' columnAlias [, columnAlias ]* ')' ] ]

tablePrimary:
      [ [ catalogName . ] schemaName . ] tableName
      '(' TABLE [ [ catalogName . ] schemaName . ] tableName ')'
  |   [ LATERAL ] '(' query ')'
  |   UNNEST '(' expression ')' [ WITH ORDINALITY ]
  |   [ LATERAL ] TABLE '(' [ SPECIFIC ] functionName '(' expression [, expression ]* ')' ')'

values:
      VALUES expression [, expression ]*

groupItem:
      expression
  |   '(' ')'
  |   '(' expression [, expression ]* ')'
  |   CUBE '(' expression [, expression ]* ')'
  |   ROLLUP '(' expression [, expression ]* ')'
  |   GROUPING SETS '(' groupItem [, groupItem ]* ')'

windowRef:
      windowName
  |   windowSpec

windowSpec:
      [ windowName ]
      '('
      [ ORDER BY orderItem [, orderItem ]* ]
      [ PARTITION BY expression [, expression ]* ]
      [
          RANGE numericOrIntervalExpression { PRECEDING | FOLLOWING }
      |   ROWS numericExpression { PRECEDING | FOLLOWING }
      ]
      ')'

----------------------------------------

TITLE: Configuring Scheduling Priority Strategy
DESCRIPTION: Configuration to set the scheduling priority strategy for the Resource Aware Scheduler.

LANGUAGE: yaml
CODE:
resource.aware.scheduler.priority.strategy: "org.apache.storm.scheduler.resource.strategies.priority.DefaultSchedulingPriorityStrategy"

----------------------------------------

TITLE: Implementing MongoDB Update Mapper
DESCRIPTION: Implementation for mapping Storm tuples to MongoDB update operations using the $set operator.

LANGUAGE: java
CODE:
public class SimpleMongoUpdateMapper extends SimpleMongoMapper implements MongoUpdateMapper {

    private String[] fields;

    @Override
    public Document toDocument(ITuple tuple) {
        Document document = new Document();
        for(String field : fields){
            document.append(field, tuple.getValueByField(field));
        }
        return new Document("$set", document);
    }

    public SimpleMongoUpdateMapper withFields(String... fields) {
        this.fields = fields;
        return this;
    }
}

----------------------------------------

TITLE: Configuring Pacemaker Servers
DESCRIPTION: YAML configuration for specifying Pacemaker server hosts.

LANGUAGE: yaml
CODE:
pacemaker.servers:\n    - somehost.mycompany.com\n    - someotherhost.mycompany.com

----------------------------------------

TITLE: Defining Components in Flux YAML
DESCRIPTION: Example of defining reusable components in Flux YAML, including constructor arguments and properties.

LANGUAGE: yaml
CODE:
components:
  - id: "stringScheme"
    className: "org.apache.storm.kafka.StringScheme"

  - id: "stringMultiScheme"
    className: "org.apache.storm.spout.SchemeAsMultiScheme"
    constructorArgs:
      - ref: "stringScheme" # component with id "stringScheme" must be declared above.

  - id: "spoutConfig"
    className: "org.apache.storm.kafka.SpoutConfig"
    constructorArgs:
      # brokerHosts
      - ref: "zkHosts"
      # topic
      - "myKafkaTopic"
      # zkRoot
      - "/kafkaSpout"
      # id
      - "myId"
    properties:
      - name: "ignoreZkOffsets"
        value: true
      - name: "scheme"
        ref: "stringMultiScheme"

----------------------------------------

TITLE: Creating MQTT Topology with Storm Core Java API
DESCRIPTION: Java code to create an MQTT topology using the Storm Core API, equivalent to the Flux YAML configuration.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
MqttOptions options = new MqttOptions();
options.setTopics(Arrays.asList("/users/tgoetz/#"));
options.setCleanConnection(false);
MqttSpout spout = new MqttSpout(new StringMessageMapper(), options);

MqttBolt bolt = new LogInfoBolt();

builder.setSpout("mqtt-spout", spout);
builder.setBolt("log-bolt", bolt).shuffleGrouping("mqtt-spout");

return builder.createTopology();

----------------------------------------

TITLE: Configuring Pacemaker Servers
DESCRIPTION: YAML configuration specifying the hosts running Pacemaker daemons.

LANGUAGE: yaml
CODE:
pacemaker.servers:\n    - somehost.mycompany.com\n    - someotherhost.mycompany.com

----------------------------------------

TITLE: Direct Tuple Sending in Local Mode for Apache Storm (Clojure)
DESCRIPTION: This code shows how tuples are sent directly to an in-memory queue for the receiving task in Storm's local mode.

LANGUAGE: Clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/local.clj#L21

----------------------------------------

TITLE: Setting Memory Requirements for Storm Components
DESCRIPTION: Example of setting memory requirements for spouts and bolts in a Storm topology.

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.setMemoryLoad(1024.0, 512.0);
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
            .shuffleGrouping("word").setMemoryLoad(512.0);

----------------------------------------

TITLE: Configuring JAAS for Storm and Zookeeper
DESCRIPTION: Example JAAS configuration file for Storm nodes, including sections for StormServer, StormClient, Client, and Server.

LANGUAGE: yaml
CODE:
StormServer {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/storm.keytab"
   storeKey=true
   useTicketCache=false
   principal="storm/storm.example.com@STORM.EXAMPLE.COM";
};
StormClient {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/storm.keytab"
   storeKey=true
   useTicketCache=false
   serviceName="storm"
   principal="storm@STORM.EXAMPLE.COM";
};
Client {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/storm.keytab"
   storeKey=true
   useTicketCache=false
   serviceName="zookeeper"
   principal="storm@STORM.EXAMPLE.COM";
};
Server {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/zk.keytab"
   storeKey=true
   useTicketCache=false
   serviceName="zookeeper"
   principal="zookeeper/zk1.example.com@STORM.EXAMPLE.COM";
};

----------------------------------------

TITLE: Simple Bolt Definition in Clojure
DESCRIPTION: Implementation of a simple bolt that splits sentences into words, demonstrating basic bolt functionality

LANGUAGE: clojure
CODE:
(defbolt split-sentence ["word"] [tuple collector]
  (let [words (.split (.getString tuple 0) " ")]
    (doseq [w words]
      (emit-bolt! collector [w] :anchor tuple))
    (ack! collector tuple)
    ))

----------------------------------------

TITLE: Storm Worker Process Initialization
DESCRIPTION: Clojure code showing how worker processes are initialized, including connection to other workers and task management.

LANGUAGE: clojure
CODE:
(mk-worker)
(connect-to-other-workers)
(monitor-topology-active-state)
(launch-worker-tasks)

----------------------------------------

TITLE: Configuring Kerberos Authentication in Storm YAML
DESCRIPTION: Storm YAML configuration for enabling Kerberos authentication

LANGUAGE: yaml
CODE:
storm.thrift.transport: "org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin"
java.security.auth.login.config: "/path/to/jaas.conf"

nimbus.childopts: "-Xmx1024m -Djava.security.auth.login.config=/path/to/jaas.conf"
ui.childopts: "-Xmx768m -Djava.security.auth.login.config=/path/to/jaas.conf"
supervisor.childopts: "-Xmx256m -Djava.security.auth.login.config=/path/to/jaas.conf"

----------------------------------------

TITLE: Storm Topology Configuration in YAML
DESCRIPTION: Flux YAML configuration for creating an MQTT-enabled Storm topology

LANGUAGE: yaml
CODE:
name: "mqtt-topology"

components:
   ########## MQTT Spout Config ############
  - id: "mqtt-type"
    className: "org.apache.storm.mqtt.examples.CustomMessageMapper"

  - id: "mqtt-options"
    className: "org.apache.storm.mqtt.common.MqttOptions"
    properties:
      - name: "url"
        value: "tcp://localhost:1883"
      - name: "topics"
        value:
          - "/users/tgoetz/#"

config:
  topology.workers: 1
  topology.max.spout.pending: 1000

spouts:
  - id: "mqtt-spout"
    className: "org.apache.storm.mqtt.spout.MqttSpout"
    constructorArgs:
      - ref: "mqtt-type"
      - ref: "mqtt-options"
    parallelism: 1

bolts:
  - id: "log"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1

streams:
  - from: "mqtt-spout"
    to: "log"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Implementing Kafka Topic Selector Interface
DESCRIPTION: Interface definition for selecting Kafka topics in Storm, used to determine which topic a tuple should be published to.

LANGUAGE: java
CODE:
public interface KafkaTopicSelector {
    String getTopics(Tuple/TridentTuple tuple);
}

----------------------------------------

TITLE: Implementing a Python Bolt for Storm
DESCRIPTION: This Python code implements a SplitSentenceBolt that splits input sentences into words. It demonstrates how to write a Storm bolt in Python using the storm module.

LANGUAGE: Python
CODE:
import storm

class SplitSentenceBolt(storm.BasicBolt):
    def process(self, tup):
        words = tup.values[0].split(" ")
        for word in words:
          storm.emit([word])

SplitSentenceBolt().run()

----------------------------------------

TITLE: Configuring Pacemaker State Storage in Storm
DESCRIPTION: Sets the cluster state store to use the PaceMakerStateStorageFactory, enabling Pacemaker for worker heartbeats.

LANGUAGE: yaml
CODE:
storm.cluster.state.store: "org.apache.storm.cluster.PaceMakerStateStorageFactory"

----------------------------------------

TITLE: SQL Setup for Storm JDBC Example
DESCRIPTION: SQL queries for setting up tables and initial data for the Storm JDBC example. It includes create table statements, inserts, and a select query.

LANGUAGE: sql
CODE:
create table if not exists user (user_id integer, user_name varchar(100), dept_name varchar(100), create_date date);
create table if not exists department (dept_id integer, dept_name varchar(100));
create table if not exists user_department (user_id integer, dept_id integer);
insert into department values (1, 'R&D');
insert into department values (2, 'Finance');
insert into department values (3, 'HR');
insert into department values (4, 'Sales');
insert into user_department values (1, 1);
insert into user_department values (2, 2);
insert into user_department values (3, 3);
insert into user_department values (4, 4);
select dept_name from department, user_department where department.dept_id = user_department.dept_id and user_department.user_id = ?;

----------------------------------------

TITLE: Implementing SimpleMongoUpdateMapper for Storm-MongoDB Integration
DESCRIPTION: Implementation of MongoUpdateMapper interface for updating MongoDB documents from Storm tuples.

LANGUAGE: java
CODE:
public class SimpleMongoUpdateMapper extends SimpleMongoMapper implements MongoUpdateMapper {

    private String[] fields;

    @Override
    public Document toDocument(ITuple tuple) {
        Document document = new Document();
        for(String field : fields){
            document.append(field, tuple.getValueByField(field));
        }
        //$set operator: Sets the value of a field in a document.
        return new Document("$set", document);
    }

    public SimpleMongoUpdateMapper withFields(String... fields) {
        this.fields = fields;
        return this;
    }
}

----------------------------------------

TITLE: Configuring Authorization in Storm YAML
DESCRIPTION: Storm YAML configuration for setting up authorization

LANGUAGE: yaml
CODE:
nimbus.authorizer: "org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer"

multitenant.scheduler.user.pools: 
    "evans": 10
    "derek": 10

supervisor.run.worker.as.user: true

nimbus.impersonation.authorizer: org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer
nimbus.impersonation.acl:
    impersonating_user1:
        hosts:
            [comma separated list of hosts from which impersonating_user1 is allowed to impersonate other users]
        groups:
            [comma separated list of groups whose users impersonating_user1 is allowed to impersonate]
    impersonating_user2:
        hosts:
            [comma separated list of hosts from which impersonating_user2 is allowed to impersonate other users]
        groups:
            [comma separated list of groups whose users impersonating_user2 is allowed to impersonate]

----------------------------------------

TITLE: Initializing EsLookupBolt for Storm-Elasticsearch Integration in Java
DESCRIPTION: This snippet shows how to create an EsLookupBolt to perform get requests to Elasticsearch. It requires an EsConfig object, an ElasticsearchGetRequest adapter, and an EsLookupResultOutput to specify output fields and convert responses.

LANGUAGE: java
CODE:
EsConfig esConfig = createEsConfig();
ElasticsearchGetRequest getRequestAdapter = createElasticsearchGetRequest();
EsLookupResultOutput output = createOutput();
EsLookupBolt lookupBolt = new EsLookupBolt(esConfig, getRequestAdapter, output);

----------------------------------------

TITLE: Setting Shared Memory in Storm Components
DESCRIPTION: Example of configuring shared memory between components

LANGUAGE: java
CODE:
builder.setBolt("exclaim1", new ExclamationBolt(), 3).shuffleGrouping("word")
      .addSharedMemory(new SharedOnHeap(100, "exclaim-cache"));

----------------------------------------

TITLE: Sample JSONP Storm Configuration Response
DESCRIPTION: Example JSON response showing cluster configuration values returned by the /api/v1/cluster/configuration endpoint

LANGUAGE: json
CODE:
{
    "dev.zookeeper.path": "/tmp/dev-storm-zookeeper",
    "topology.tick.tuple.freq.secs": null,
    "topology.builtin.metrics.bucket.size.secs": 60,
    "topology.fall.back.on.java.serialization": false,
    "topology.max.error.report.per.interval": 5,
    "zmq.linger.millis": 5000
}

----------------------------------------

TITLE: Rebalancing Topology in JSON
DESCRIPTION: Example response from the /api/v1/topology/<id>/rebalance/<wait-time> POST endpoint, which rebalances a topology.

LANGUAGE: json
CODE:
{
  "topologyOperation":"rebalance",
  "topologyId":"wordcount-1-1420308665",
  "status":"success"
}

----------------------------------------

TITLE: Performing Node Health Check
DESCRIPTION: Runs health checks on the local supervisor node.

LANGUAGE: shell
CODE:
storm node-health-check

----------------------------------------

TITLE: Configuring Kafka Bolt in Storm Topology
DESCRIPTION: Example showing how to configure and use KafkaBolt in a Storm topology to write data to Kafka.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();

Fields fields = new Fields("key", "message");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
            new Values("storm", "1"),
            new Values("trident", "1"),
            new Values("needs", "1"),
            new Values("javadoc", "1")
);
spout.setCycle(true);
builder.setSpout("spout", spout, 5);
//set producer properties.
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

KafkaBolt bolt = new KafkaBolt()
        .withProducerProperties(props)
        .withTopicSelector(new DefaultTopicSelector("test"))
        .withTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper());
builder.setBolt("forwardToKafka", bolt, 8).shuffleGrouping("spout");

Config conf = new Config();

StormSubmitter.submitTopology("kafkaboltTest", conf, builder.createTopology());

----------------------------------------

TITLE: Basic Cassandra Insert Query with Specified Fields
DESCRIPTION: Creates a CassandraWriterBolt that inserts data into an album table using specified tuple fields.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
    async(
        simpleQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(
            fields("title", "year", "performer", "genre", "tracks")
        )
    )
);

----------------------------------------

TITLE: Initializing JdbcLookupBolt in Java for Storm JDBC
DESCRIPTION: Example of creating a JdbcLookupBolt for executing select queries in a Storm topology. It demonstrates setting up the bolt with a connection provider, select query, lookup mapper, and query timeout.

LANGUAGE: java
CODE:
String selectSql = "select user_name from user_details where user_id = ?";
SimpleJdbcLookupMapper lookupMapper = new SimpleJdbcLookupMapper(outputFields, queryParamColumns)
JdbcLookupBolt userNameLookupBolt = new JdbcLookupBolt(connectionProvider, selectSql, lookupMapper)
        .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Using Value Mappers in Stream Creation
DESCRIPTION: Shows how to use value mappers to extract specific fields from tuples emitted by a spout, creating typed streams of values or tuples.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

// extract the first field from the tuple to get a Stream<String> of sentences
Stream<String> sentences = builder.newStream(new TestWordSpout(), new ValueMapper<String>(0));

// extract first three fields of the tuple emitted by the spout to produce a stream of typed tuples.
Stream<Tuple3<String, Integer, Long>> stream = builder.newStream(new TestSpout(), TupleValueMappers.of(0, 1, 2));

----------------------------------------

TITLE: Retrieving Topology Summary in JSON
DESCRIPTION: Example response from the /api/v1/topology/summary endpoint, which returns summary information for all topologies.

LANGUAGE: json
CODE:
{
  "topologies": [
    {
      "id": "WordCount3-1-1402960825",
      "name": "WordCount3",
      "status": "ACTIVE",
      "uptime": "6m 5s",
      "uptimeSeconds": 365,
      "tasksTotal": 28,
      "workersTotal": 3,
      "executorsTotal": 28,
      "replicationCount": 1,
      "requestedMemOnHeap": 640,
      "requestedMemOffHeap": 128,
      "requestedTotalMem": 768,
      "requestedCpu": 80,
      "assignedMemOnHeap": 640,
      "assignedMemOffHeap": 128,
      "assignedTotalMem": 768,
      "assignedCpu": 80
    }
  ],
  "schedulerDisplayResource": true
}

----------------------------------------

TITLE: Configuring Pacemaker Servers in Storm
DESCRIPTION: Specifies the hostnames of the Pacemaker servers in the Storm cluster configuration.

LANGUAGE: yaml
CODE:
pacemaker.servers:
    - somehost.mycompany.com
    - someotherhost.mycompany.com

----------------------------------------

TITLE: Storm Cluster Metrics Table Schema
DESCRIPTION: Table structure defining cluster-level metrics reported by the active Nimbus instance to monitor overall cluster state.

LANGUAGE: markdown
CODE:
| Metric Name | Type | Description |
|-------------|------|-------------|
| cluster:num-nimbus-leaders | gauge | Number of nimbuses marked as a leader... |

----------------------------------------

TITLE: Configuring Buffer Sizes in Apache Storm
DESCRIPTION: Sets the size of message queues for executors and inter-worker communication. These settings affect isolation between producers and consumers, potentially impacting throughput and latency.

LANGUAGE: yaml
CODE:
topology.executor.receive.buffer.size: 1024
topology.transfer.buffer.size: 1024

----------------------------------------

TITLE: Launching Docker Container for Storm Worker
DESCRIPTION: Example Docker run command used by Storm's DockerManager to launch a worker container. It demonstrates various options for isolation, resource limits, and volume mounts.

LANGUAGE: bash
CODE:
run --name=8198e1f0-f323-4b9d-8625-e4fd640cd058 \
--user=<uid>:<gid> \
-d \
--net=host \
--read-only \
-v /sys/fs/cgroup:/sys/fs/cgroup:ro \
-v /usr/share/apache-storm-2.3.0:/usr/share/apache-storm-2.3.0:ro \
-v /<storm-local-dir>/supervisor:/<storm-local-dir>/supervisor:ro \
-v /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058:/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
-v /<workers-artifacts-dir>/workers-artifacts/word-count-1-1591895933/6703:/<workers-artifacts-dir>/workers-artifacts/word-count-1-1591895933/6703 \
-v /<storm-local-dir>/workers-users/8198e1f0-f323-4b9d-8625-e4fd640cd058:/<storm-local-dir>/workers-users/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
-v /var/run/nscd:/var/run/nscd \
-v /<storm-local-dir>/supervisor/stormdist/word-count-1-1591895933/shared_by_topology:/<storm-local-dir>/supervisor/stormdist/word-count-1-1591895933/shared_by_topology \
-v /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/tmp:/tmp \
-v /etc/storm:/etc/storm:ro \
--cgroup-parent=/storm \
--group-add <gid> \
--workdir=/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
--cidfile=/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/container.cid \
--cap-drop=ALL \
--security-opt no-new-privileges \
--security-opt seccomp=/usr/share/apache-storm-2.3.0/conf/seccomp.json \
--cpus=2.6 xxx.xxx.com:8080/storm/storm/rhel7:latest \
bash /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/storm-worker-script.sh

----------------------------------------

TITLE: Initializing EsLookupBolt for Elasticsearch Queries in Java
DESCRIPTION: This code demonstrates how to create an EsLookupBolt for performing get requests to Elasticsearch. It requires an EsConfig, an ElasticsearchGetRequest adapter, and an EsLookupResultOutput for handling the response.

LANGUAGE: java
CODE:
EsConfig esConfig = createEsConfig();
ElasticsearchGetRequest getRequestAdapter = createElasticsearchGetRequest();
EsLookupResultOutput output = createOutput();
EsLookupBolt lookupBolt = new EsLookupBolt(esConfig, getRequestAdapter, output);

----------------------------------------

TITLE: Retrieving Topology Summary in JSON
DESCRIPTION: Example response from the /api/v1/topology/summary endpoint showing summary information for all topologies in JSON format.

LANGUAGE: json
CODE:
{
  "topologies": [
    {
      "id": "WordCount3-1-1402960825",
      "name": "WordCount3",
      "status": "ACTIVE",
      "uptime": "6m 5s",
      "uptimeSeconds": 365,
      "tasksTotal": 28,
      "workersTotal": 3,
      "executorsTotal": 28,
      "replicationCount": 1,
      "requestedMemOnHeap": 640,
      "requestedMemOffHeap": 128,
      "requestedTotalMem": 768,
      "requestedCpu": 80,
      "assignedMemOnHeap": 640,
      "assignedMemOffHeap": 128,
      "assignedTotalMem": 768,
      "assignedCpu": 80
    }
  ],
  "schedulerDisplayResource": true
}

----------------------------------------

TITLE: Implementing DRPC Topology with LinearDRPCTopologyBuilder in Java
DESCRIPTION: This example demonstrates how to create a simple DRPC topology using LinearDRPCTopologyBuilder that appends an exclamation mark to the input.

LANGUAGE: java
CODE:
public static class ExclaimBolt extends BaseBasicBolt {
    public void execute(Tuple tuple, BasicOutputCollector collector) {
        String input = tuple.getString(1);
        collector.emit(new Values(tuple.getValue(0), input + "!"));
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "result"));
    }
}

public static void main(String[] args) throws Exception {
    LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("exclamation");
    builder.addBolt(new ExclaimBolt(), 3);
    // ...
}

----------------------------------------

TITLE: Configuring KafkaSpout with Multiple Output Streams
DESCRIPTION: Shows how to set up a KafkaSpout that emits to multiple output streams based on the topic.

LANGUAGE: java
CODE:
final TopologyBuilder tp = new TopologyBuilder();

ByTopicRecordTranslator<String, String> byTopic = new ByTopicRecordTranslator<>(
    (r) -> new Values(r.topic(), r.key(), r.value()),
    new Fields("topic", "key", "value"), "STREAM_1");
byTopic.forTopic("topic_2", (r) -> new Values(r.key(), r.value()), new Fields("key", "value"), "STREAM_2");

tp.setSpout("kafka_spout", new KafkaSpout<>(KafkaSpoutConfig.builder("127.0.0.1:" + port, "topic_1", "topic_2", "topic_3").build()), 1);
tp.setBolt("bolt", new myBolt()).shuffleGrouping("kafka_spout", "STREAM_1");
tp.setBolt("another", new myOtherBolt()).shuffleGrouping("kafka_spout", "STREAM_2");

----------------------------------------

TITLE: Configuring RedisClusterState for Trident Topology
DESCRIPTION: Java code snippet demonstrating how to configure and use RedisClusterState in a Trident topology. This example shows setting up RedisClusterState for both storing and querying data in a Redis Cluster.

LANGUAGE: java
CODE:
Set<InetSocketAddress> nodes = new HashSet<InetSocketAddress>();
for (String hostPort : redisHostPort.split(",")) {
    String[] host_port = hostPort.split(":");
    nodes.add(new InetSocketAddress(host_port[0], Integer.valueOf(host_port[1])));
}
JedisClusterConfig clusterConfig = new JedisClusterConfig.Builder().setNodes(nodes)
                                .build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisLookupMapper lookupMapper = new WordCountLookupMapper();
RedisClusterState.Factory factory = new RedisClusterState.Factory(clusterConfig);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory,
                        fields,
                        new RedisClusterStateUpdater(storeMapper).withExpire(86400000),
                        new Fields());

TridentState state = topology.newStaticState(factory);
stream = stream.stateQuery(state, new Fields("word"),
                        new RedisClusterStateQuerier(lookupMapper),
                        new Fields("columnName","columnValue"));

----------------------------------------

TITLE: Defining IEventLogger Interface in Java for Apache Storm
DESCRIPTION: This code snippet shows the IEventLogger interface used by the event logger bolt to log events. It includes methods for preparation, logging events, and cleanup.

LANGUAGE: java
CODE:
public interface IEventLogger {
    void prepare(Map stormConf, Map<String, Object> arguments, TopologyContext context);
    void log(EventInfo e);
    void close();
}

----------------------------------------

TITLE: Image Tag to Manifest Mapping File
DESCRIPTION: Contents of the image-tag-to-manifest-file showing the mapping between image tags and manifest hashes.

LANGUAGE: bash
CODE:
-bash-4.2$ hdfs dfs -cat /containers/image-tag-to-hash
storm/rhel7:dev_current:26fd443859325d5911f3be5c5e231dddca88ee0d526456c0c92dd794148d8585#docker.xxx.com:4443/hadoop-user-images/storm/rhel7:20201202-232133

----------------------------------------

TITLE: Starting MQTT Broker and Publisher in Java
DESCRIPTION: Command to start an MQTT broker on port 1883 and a publisher that sends random temperature/humidity values to an MQTT topic.

LANGUAGE: bash
CODE:
java -cp examples/target/storm-mqtt-examples-*-SNAPSHOT.jar org.apache.storm.mqtt.examples.MqttBrokerPublisher

----------------------------------------

TITLE: Kafka Table Configuration Example
DESCRIPTION: Example of creating an external table connected to Kafka stream

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Metric Configuration Plugin Example
DESCRIPTION: Example configuration pattern for Storm metrics reporter plugins showing supported reporters. The reporters help expose Storm metrics through different channels like console, CSV files and JMX.

LANGUAGE: markdown
CODE:
* Console Reporter (`org.apache.storm.daemon.metrics.reporters.ConsolePreparableReporter`):
  Reports metrics to `System.out`.
* CSV Reporter (`org.apache.storm.daemon.metrics.reporters.CsvPreparableReporter`):
  Reports metrics to a CSV file.
* JMX Reporter (`org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter`):
  Exposes metrics via JMX.

----------------------------------------

TITLE: Component Configuration Settings in Java
DESCRIPTION: Lists the four main topology configurations that can be overridden per bolt/spout basis in Storm 0.7.0+: topology.debug, topology.max.spout.pending, topology.max.task.parallelism, and topology.kryo.register.

LANGUAGE: java
CODE:
"topology.debug"
"topology.max.spout.pending"
"topology.max.task.parallelism"
"topology.kryo.register"

----------------------------------------

TITLE: IConfigLoader Interface Definition
DESCRIPTION: Core interface definition for config loaders, containing a single load() method that returns the configuration map.

LANGUAGE: java
CODE:
public interface IConfigLoader {
    Map<?,?> load();
};

----------------------------------------

TITLE: Storm Shell Python Execution Format
DESCRIPTION: Format showing how Storm shell executes the Python topology script with Nimbus connection parameters.

LANGUAGE: shell
CODE:
python topology.py arg1 arg2 {nimbus-host} {nimbus-port} {uploaded-jar-location}

----------------------------------------

TITLE: User Defined Function Implementation
DESCRIPTION: Java implementation example of a scalar UDF for Storm SQL.

LANGUAGE: java
CODE:
  public class MyPlus {
    public static Integer evaluate(Integer x, Integer y) {
      return x + y;
    }
  }

----------------------------------------

TITLE: Registering Event Logger in YAML Configuration
DESCRIPTION: Example of registering multiple event logger implementations with custom arguments in Storm's YAML configuration file.

LANGUAGE: yaml
CODE:
topology.event.logger.register:
  - class: "org.apache.storm.metric.FileBasedEventLogger"
  - class: "org.mycompany.MyEventLogger"
    arguments:
      endpoint: "event-logger.mycompany.org"

----------------------------------------

TITLE: Reading Task-Level Data in Storm Components
DESCRIPTION: Shows how to read task-level data that has been shared across components using TopologyContext.

LANGUAGE: java
CODE:
TopologyContext#getTask(String)

----------------------------------------

TITLE: Setting Component Memory Requirements in Storm Topology
DESCRIPTION: API calls to set memory requirements for spouts and bolts in a Storm topology.

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.setMemoryLoad(1024.0, 512.0);
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
            .shuffleGrouping("word").setMemoryLoad(512.0);

----------------------------------------

TITLE: Setting Memory Requirements for Storm Components
DESCRIPTION: Java API calls to set on-heap and off-heap memory requirements for Storm spouts and bolts.

LANGUAGE: java
CODE:
public T setMemoryLoad(Number onHeap, Number offHeap)

public T setMemoryLoad(Number onHeap)

----------------------------------------

TITLE: Storm Emit Command JSON Format
DESCRIPTION: JSON structure for emitting tuples from a shell process to Storm. Specifies anchors, stream ID, target task, and tuple values.

LANGUAGE: json
CODE:
{
    "command": "emit",
    "anchors": [1231231, -234234234],
    "stream": 1,
    "task": 9,
    "tuple": ["field1", 2, 3]
}

----------------------------------------

TITLE: Digest Authentication JAAS Configuration
DESCRIPTION: JAAS configuration structure for Pacemaker digest authentication.

LANGUAGE: java
CODE:
PacemakerDigest {\n    username="some username"\n    password="some password";\n};

----------------------------------------

TITLE: Setting Storm Local Directory Configuration
DESCRIPTION: YAML configuration for specifying Storm's local directory for storing state data, with examples for both Linux and Windows paths

LANGUAGE: yaml
CODE:
storm.local.dir: "/mnt/storm"

LANGUAGE: yaml
CODE:
storm.local.dir: "C:\\storm-local"

----------------------------------------

TITLE: Implementing Storm Core Bolt with OpenTSDB
DESCRIPTION: Example showing how to configure and implement OpenTSDB bolt in Storm topology. The code demonstrates setting up OpenTsdbBolt with client builder, batch size configuration, and topology integration.

LANGUAGE: java
CODE:
OpenTsdbClient.Builder builder =  OpenTsdbClient.newBuilder(openTsdbUrl).sync(30_000).returnDetails();
final OpenTsdbBolt openTsdbBolt = new OpenTsdbBolt(builder, Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
openTsdbBolt.withBatchSize(10).withFlushInterval(2000);
topologyBuilder.setBolt("opentsdb", openTsdbBolt).shuffleGrouping("metric-gen");

----------------------------------------

TITLE: Local Mode Implementation Using Java Queues in Apache Storm (Clojure)
DESCRIPTION: This code implements the local mode message passing using in-memory Java queues in Storm, allowing for easy local usage without ZeroMQ installation.

LANGUAGE: Clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/local.clj

----------------------------------------

TITLE: Implementing Conditional Streams in Trident Topology
DESCRIPTION: Demonstrates how to create multiple conditional streams in a Trident topology using stream variables and filters. Shows branching logic implementation for routing tuples based on conditions.

LANGUAGE: java
CODE:
Stream s = topology.each(...).groupBy(...).aggregate(...)
Stream branch1 = s.each(..., FilterA)
Stream branch2 = s.each(..., FilterB)

----------------------------------------

TITLE: Configuring Kerberos Authentication for Storm UI in YAML
DESCRIPTION: YAML configuration example for setting up Kerberos authentication for the Storm UI using AuthenticationFilter from hadoop-auth.

LANGUAGE: yaml
CODE:
ui.filter: "org.apache.hadoop.security.authentication.server.AuthenticationFilter"
ui.filter.params:
   "type": "kerberos"
   "kerberos.principal": "HTTP/nimbus.witzend.com"
   "kerberos.keytab": "/vagrant/keytabs/http.keytab"
   "kerberos.name.rules": "RULE:[2:$1@$0]([jt]t@.*EXAMPLE.COM)s/.*/$MAPRED_USER/ RULE:[2:$1@$0]([nd]n@.*EXAMPLE.COM)s/.*/$HDFS_USER/DEFAULT"

----------------------------------------

TITLE: Configuring Trident Kafka Spout
DESCRIPTION: Example of setting up a KafkaTridentSpoutOpaque in a Trident topology to consume from Kafka topics matching a pattern.

LANGUAGE: java
CODE:
final TridentTopology tridentTopology = new TridentTopology();
final Stream spoutStream = tridentTopology.newStream("kafkaSpout",
    new KafkaTridentSpoutOpaque<>(KafkaSpoutConfig.builder("127.0.0.1:" + port, Pattern.compile("topic.*")).build()))
      .parallelismHint(1)

----------------------------------------

TITLE: Implementing an Aggregator in Trident
DESCRIPTION: Example of implementing a Count Aggregator.

LANGUAGE: java
CODE:
public class CountAgg extends BaseAggregator<CountState> {
    static class CountState {
        long count = 0;
    }

    public CountState init(Object batchId, TridentCollector collector) {
        return new CountState();
    }

    public void aggregate(CountState state, TridentTuple tuple, TridentCollector collector) {
        state.count+=1;
    }

    public void complete(CountState state, TridentCollector collector) {
        collector.emit(new Values(state.count));
    }
}

----------------------------------------

TITLE: Configuring Supervisor Slots Ports in Storm YAML
DESCRIPTION: This configuration defines the ports available for workers on each machine. It determines how many workers can run on a single machine.

LANGUAGE: yaml
CODE:
supervisor.slots.ports:
    - 6700
    - 6701
    - 6702
    - 6703

----------------------------------------

TITLE: Leader Election Interface Implementation in Java
DESCRIPTION: Interface defining the core functionality for leader election among Nimbus nodes. Includes methods for queue management, leadership status checks, and node address retrieval.

LANGUAGE: java
CODE:
public interface ILeaderElector {
    void addToLeaderLockQueue();
    void removeFromLeaderLockQueue();
    boolean isLeader();
    InetSocketAddress getLeaderAddress();
    List<InetSocketAddress> getAllNimbusAddresses();
}

----------------------------------------

TITLE: Bolt Tuple Transfer in Apache Storm (Clojure)
DESCRIPTION: Implementation of tuple transfer for bolts using the worker-provided transfer function.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L429

----------------------------------------

TITLE: Configuring NUMA Zones in Storm Supervisor
DESCRIPTION: This YAML configuration snippet demonstrates how to set up NUMA zones in the Storm Supervisor config. It includes settings for cores, generic resources, memory, and ports for each NUMA zone.

LANGUAGE: yaml
CODE:
supervisor.numa.meta:
    "0": # Numa zone id
        numa.cores: # Cores in NUMA zone (can be determined by using the numastat command)
            - 0
            - 1
            - 2
            - 3
            - 4
            - 5
            - 12
            - 13
            - 14
            - 15
            - 16

        numa.generic.resources.map: # Generic Resources in the zone to be used for generic resource scheduling (optional)
            network.resource.units: 50.0

        numa.memory.mb: 42461 # Size of memory zone
        numa.ports: # Ports to be assigned to workers pinned to the NUMA zone (this may include ports not specified in SUPERVISOR_SLOTS_PORTS
            - 6700
            - 6701
            - 6702
            - 6703
            - 6704
            - 6705
            - 6706
            - 6707
            - 6708
            - 6709
            - 6710
            - 6711

----------------------------------------

TITLE: Initializing RocketMqBolt in Java for Storm-RocketMQ Integration
DESCRIPTION: This code demonstrates how to initialize a RocketMqBolt for writing data to a RocketMQ topic in a Storm topology. It configures the bolt with a TupleToMessageMapper, TopicSelector, and RocketMQ properties.

LANGUAGE: java
CODE:
TupleToMessageMapper mapper = new FieldNameBasedTupleToMessageMapper("word", "count");
TopicSelector selector = new DefaultTopicSelector(topic);

properties = new Properties();
properties.setProperty(RocketMqConfig.NAME_SERVER_ADDR, nameserverAddr);

RocketMqBolt insertBolt = new RocketMqBolt()
        .withMapper(mapper)
        .withSelector(selector)
        .withProperties(properties);

----------------------------------------

TITLE: Retrieving Owner Resource Usage in JSON
DESCRIPTION: Example response from the /api/v1/owner-resources endpoint, which returns resource usage information aggregated by topology owner.

LANGUAGE: json
CODE:
{
  "owners": [
    {
      "totalReqOnHeapMem": 896,
      "owner": "ownerA",
      "totalExecutors": 7,
      "cpuGuaranteeRemaining": 30,
      "totalReqMem": 896,
      "cpuGuarantee": 100,
      "isolatedNodes": "N/A",
      "memoryGuarantee": 4000,
      "memoryGuaranteeRemaining": 3104,
      "totalTasks": 7,
      "totalMemoryUsage": 896,
      "totalReqOffHeapMem": 0,
      "totalReqCpu": 70,
      "totalWorkers": 2,
      "totalCpuUsage": 70,
      "totalAssignedOffHeapMem": 0,
      "totalAssignedOnHeapMem": 896,
      "totalTopologies": 1
    }
  ],
  "schedulerDisplayResource": true
}

----------------------------------------

TITLE: Defining IEventLogger Interface in Java for Storm Event Logging
DESCRIPTION: This code snippet defines the IEventLogger interface used by the event logger bolt to log events in Storm. It includes methods for preparation, logging events, and cleanup.

LANGUAGE: java
CODE:
public interface IEventLogger {
    void prepare(Map stormConf, Map<String, Object> arguments, TopologyContext context);

    void log(EventInfo e);

    void close();
}

----------------------------------------

TITLE: Configuring Kerberos Authentication for Storm-HBase in Java
DESCRIPTION: Shows how to set up Kerberos authentication for secure HBase connections in a Storm topology configuration.

LANGUAGE: java
CODE:
Config config = new Config();
...
config.put("storm.keytab.file", "$keytab");
config.put("storm.kerberos.principal", "$principle");
StormSubmitter.submitTopology("$topologyName", config, builder.createTopology());

----------------------------------------

TITLE: Defining MongoMapper Interface for Storm-MongoDB Integration
DESCRIPTION: Interface definition for MongoMapper, which is used to convert Storm tuples to MongoDB documents.

LANGUAGE: java
CODE:
public interface MongoMapper extends Serializable {
    Document toDocument(ITuple tuple);
    Document toDocumentByKeys(List<Object> keys);
}

----------------------------------------

TITLE: Configuring UI Filter in Apache Storm YAML
DESCRIPTION: Example YAML configuration for setting up a UI filter in Apache Storm, including filter class and parameters.

LANGUAGE: yaml
CODE:
ui.filter: "filter.class"
ui.filter.params: "param1":"value1"

----------------------------------------

TITLE: Running Storm SQL Command
DESCRIPTION: Basic command to compile SQL statements into a Storm topology and submit to cluster

LANGUAGE: bash
CODE:
$ bin/storm sql <sql-file> <topo-name>

----------------------------------------

TITLE: Registering Task Hooks via Configuration
DESCRIPTION: Configuration example showing how to register task hooks automatically in Storm using the topology.auto.task.hooks config property. These hooks are applied to all spouts and bolts.

LANGUAGE: java
CODE:
"topology.auto.task.hooks"

----------------------------------------

TITLE: Local Mode Message Receiving in Apache Storm (Clojure)
DESCRIPTION: This code shows how tuples are sent directly to an in-memory queue for the receiving task in local mode.

LANGUAGE: clojure
CODE:
(github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/local.clj#L21)

----------------------------------------

TITLE: Storm Resource Access Methods
DESCRIPTION: Methods for reading and writing shared resources across different Storm components including task data, executor data, and user resources.

LANGUAGE: java
CODE:
// Task Data
TopologyContext.setTaskData(String, Object)    // write
TopologyContext.getTask(String)              // read

// Executor Data
TopologyContext.setExecutorData(String, Object) // write
TopologyContext.getExecutorData(String)        // read

// User Resources
WorkerUserContext.setResource(String, Object)   // write
WorkerTopologyContext.getResource(String)      // read
TopologyContext.getResource(String)            // read

----------------------------------------

TITLE: Storm Input Tuple JSON Format
DESCRIPTION: JSON structure representing an input tuple in Storm's multilang protocol. Includes tuple ID, component ID, stream ID, task ID, and tuple values.

LANGUAGE: json
CODE:
{
    "id": -6955786537413359385,
    "comp": 1,
    "stream": 1,
    "task": 9,
    "tuple": ["snow white and the seven dwarfs", "field2", 3]
}

----------------------------------------

TITLE: Worker Connection Management in Storm
DESCRIPTION: Worker code that handles refreshing connections and maintaining task-to-worker mappings. Called periodically or when ZooKeeper assignments change.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L123

----------------------------------------

TITLE: Implementing Task Hooks in Storm
DESCRIPTION: Task hooks can be registered either through the TopologyContext in spout/bolt initialization methods or via Storm configuration. They allow custom code execution for various Storm events by extending BaseTaskHook.

LANGUAGE: java
CODE:
TopologyContext.addTaskHook()  // Method 1: Register hook in spout/bolt

"topology.auto.task.hooks"     // Method 2: Register via configuration

----------------------------------------

TITLE: Configuring Window Parameters in Java
DESCRIPTION: Various methods for configuring window parameters such as length and sliding interval using the BaseWindowedBolt class in Storm.

LANGUAGE: java
CODE:
withWindow(Count windowLength, Count slidingInterval)
withWindow(Count windowLength)
withWindow(Count windowLength, Duration slidingInterval)
withWindow(Duration windowLength, Duration slidingInterval)
withWindow(Duration windowLength)
withWindow(Duration windowLength, Count slidingInterval)
withTumblingWindow(BaseWindowedBolt.Count count)
withTumblingWindow(BaseWindowedBolt.Duration duration)

----------------------------------------

TITLE: Implementing Task Hooks in Storm
DESCRIPTION: Task hooks can be registered either through the TopologyContext in spout/bolt initialization methods or via Storm configuration. They allow custom code execution for various Storm events by extending BaseTaskHook.

LANGUAGE: java
CODE:
TopologyContext.addTaskHook()  // Method 1: Register hook in spout/bolt

"topology.auto.task.hooks"     // Method 2: Register via configuration

----------------------------------------

TITLE: Defining submitTopology Method in Storm's Thrift API
DESCRIPTION: This Java method signature from Storm's Thrift API defines the submitTopology function used to submit a topology to Nimbus. It requires the topology name, uploaded jar location, JSON configuration, and the StormTopology object.

LANGUAGE: java
CODE:
void submitTopology(1: string name, 2: string uploadedJarLocation, 3: string jsonConf, 4: StormTopology topology) throws (1: AlreadyAliveException e, 2: InvalidTopologyException ite);

----------------------------------------

TITLE: Configuring Nimbus Seeds in Storm YAML
DESCRIPTION: This configuration specifies the machines that are candidates for master, allowing worker nodes to download topology jars and configurations. It's a mandatory setting in the storm.yaml file.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["111.222.333.44"]

----------------------------------------

TITLE: Creating ACLs for Blobs in Java
DESCRIPTION: Java code to create Access Control Lists (ACLs) for blobs in the Storm distributed cache.

LANGUAGE: java
CODE:
String stringBlobACL = "u:username:rwa";
AccessControl blobACL = BlobStoreAclHandler.parseAccessControl(stringBlobACL);
List<AccessControl> acls = new LinkedList<AccessControl>();
acls.add(blobACL); // more ACLs can be added here
SettableBlobMeta settableBlobMeta = new SettableBlobMeta(acls);
settableBlobMeta.set_replication_factor(4); // Here we can set the replication factor

----------------------------------------

TITLE: Configuring DRPC Authorization
DESCRIPTION: YAML configuration for setting up DRPC authorization in Storm

LANGUAGE: yaml
CODE:
drpc.authorizer.acl:
   "functionName1":
     "client.users":
       - "alice"
       - "bob"
     "invocation.user": "bob"

----------------------------------------

TITLE: Specifying DRPC Servers in Storm YAML
DESCRIPTION: This configuration lists the DRPC servers for the Storm cluster, allowing workers to locate them when needed.

LANGUAGE: yaml
CODE:
drpc.servers: ["111.222.333.44"]

----------------------------------------

TITLE: Defining HBaseMapper Interface in Java
DESCRIPTION: The HBaseMapper interface is the main API for interacting with HBase. It defines methods for generating row keys and column lists from Storm tuples.

LANGUAGE: java
CODE:
public interface HBaseMapper extends Serializable {
    byte[] rowKey(Tuple tuple);

    ColumnList columns(Tuple tuple);
}

----------------------------------------

TITLE: Configuring Impersonation Authorization
DESCRIPTION: YAML configuration for setting up impersonation authorization in Storm

LANGUAGE: yaml
CODE:
nimbus.impersonation.authorizer: org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer
nimbus.impersonation.acl:
    impersonating_user1:
        hosts:
            [comma separated list of hosts from which impersonating_user1 is allowed to impersonate other users]
        groups:
            [comma separated list of groups whose users impersonating_user1 is allowed to impersonate]
    impersonating_user2:
        hosts:
            [comma separated list of hosts from which impersonating_user2 is allowed to impersonate other users]
        groups:
            [comma separated list of groups whose users impersonating_user2 is allowed to impersonate]

----------------------------------------

TITLE: Configuring Redis State Provider in Storm
DESCRIPTION: JSON configuration for using Redis as a state provider in Storm. Specifies connection details, key/value classes, and serializer implementations.

LANGUAGE: json
CODE:
{
  "keyClass": "Optional fully qualified class name of the Key type.",
  "valueClass": "Optional fully qualified class name of the Value type.",
  "keySerializerClass": "Optional Key serializer implementation class.",
  "valueSerializerClass": "Optional Value Serializer implementation class.",
  "jedisPoolConfig": {
    "host": "localhost",
    "port": 6379,
    "timeout": 2000,
    "database": 0,
    "password": "xyz"
    }
}

----------------------------------------

TITLE: Implementing SimpleMongoMapper for Storm-MongoDB Integration
DESCRIPTION: This code implements the SimpleMongoMapper class, which is a general-purpose MongoMapper implementation. It maps Storm tuple fields to MongoDB document fields with the same names.

LANGUAGE: java
CODE:
public class SimpleMongoMapper implements MongoMapper {
    private String[] fields;

    @Override
    public Document toDocument(ITuple tuple) {
        Document document = new Document();
        for(String field : fields){
            document.append(field, tuple.getValueByField(field));
        }
        return document;
    }

    @Override
    public Document toDocumentByKeys(List<Object> keys) {
        Document document = new Document();
        document.append("_id", MongoUtils.getID(keys));
        return document;
    }

    public SimpleMongoMapper withFields(String... fields) {
        this.fields = fields;
        return this;
    }
}

----------------------------------------

TITLE: Implementing HTML Meta Refresh Redirect for Storm Documentation
DESCRIPTION: This HTML snippet sets up an automatic redirect to the Storm documentation page about serialization for versions prior to 0.6.0. It also includes a canonical link tag for SEO purposes.

LANGUAGE: HTML
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Serialization-(prior-to-0.6.0).html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Serialization-(prior-to-0.6.0).html" />

----------------------------------------

TITLE: Registering Metrics Consumers in YAML Configuration
DESCRIPTION: Demonstrates registering multiple metrics consumers using YAML configuration in the storm.yaml file. Specifies consumer classes and parallelism hints.

LANGUAGE: yaml
CODE:
topology.metrics.consumer.register:
  - class: "org.apache.storm.metric.LoggingMetricsConsumer"
    parallelism.hint: 1
  - class: "org.apache.storm.metric.HttpForwardingMetricsConsumer"
    parallelism.hint: 1
    argument: "http://example.com:8080/metrics/my-topology/"

----------------------------------------

TITLE: Configuring Storm Local Directory
DESCRIPTION: Configuration for specifying the local storage directory used by Storm daemons for storing state, jars, and configurations. Shows examples for both Unix and Windows paths.

LANGUAGE: yaml
CODE:
storm.local.dir: "/mnt/storm"

LANGUAGE: yaml
CODE:
storm.local.dir: "C:\\storm-local"

----------------------------------------

TITLE: Configuring EsState for Storm-Elasticsearch Trident Integration in Java
DESCRIPTION: This snippet illustrates how to set up an EsState for Trident topologies. It creates a StateFactory using EsConfig and EsTupleMapper, which is then used to create a persistent Trident state for Elasticsearch operations.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();

StateFactory factory = new EsStateFactory(esConfig, tupleMapper);
TridentState state = stream.partitionPersist(factory, esFields, new EsUpdater(), new Fields());

----------------------------------------

TITLE: Cluster Configuration Response
DESCRIPTION: Sample JSON response from /api/v1/cluster/configuration endpoint showing cluster configuration parameters

LANGUAGE: json
CODE:
{
    "dev.zookeeper.path": "/tmp/dev-storm-zookeeper",
    "topology.tick.tuple.freq.secs": null,
    "topology.builtin.metrics.bucket.size.secs": 60,
    "topology.fall.back.on.java.serialization": false,
    "topology.max.error.report.per.interval": 5,
    "zmq.linger.millis": 5000,
    "topology.skip.missing.kryo.registrations": false,
    "storm.messaging.netty.client_worker_threads": 1,
    "ui.childopts": "-Xmx768m",
    "storm.zookeeper.session.timeout": 20000,
    "nimbus.reassign": true,
    "topology.trident.batch.emit.interval.millis": 500,
    "storm.messaging.netty.flush.check.interval.ms": 10,
    "nimbus.monitor.freq.secs": 10,
    "logviewer.childopts": "-Xmx128m",
    "java.library.path": "/usr/local/lib:/opt/local/lib:/usr/lib",
    "topology.executor.send.buffer.size": 1024
}

----------------------------------------

TITLE: Initializing Fixed Batch Spout in Trident
DESCRIPTION: Creates a fixed batch spout that cycles through a set of sentences to produce a continuous stream of data for testing.

LANGUAGE: java
CODE:
FixedBatchSpout spout = new FixedBatchSpout(new Fields("sentence"), 3,
               new Values("the cow jumped over the moon"),
               new Values("the man went to the store and bought some candy"),
               new Values("four score and seven years ago"),
               new Values("how many apples can you eat"));
spout.setCycle(true);

----------------------------------------

TITLE: System Topology Creation in Storm
DESCRIPTION: Functions that create the actual topology by adding implicit streams and acker bolts. Used both by Nimbus for task creation and workers for message routing.

LANGUAGE: clojure
CODE:
system-topology!

----------------------------------------

TITLE: Implementing Trident State for Querying Data from Cassandra in Java
DESCRIPTION: Example of using Trident API to query data from Cassandra using the state API.

LANGUAGE: java
CODE:
CassandraState.Options options = new CassandraState.Options(new CassandraContext());
CQLStatementTupleMapper insertTemperatureValues = boundQuery("SELECT name FROM weather.station WHERE id = ?")
         .bind(with(field("weather_station_id").as("id")));
options.withCQLStatementTupleMapper(insertTemperatureValues);
options.withCQLResultSetValuesMapper(new TridentResultSetValuesMapper(new Fields("name")));
CassandraStateFactory selectWeatherStationStateFactory =  new CassandraStateFactory(options);
CassandraStateFactory selectWeatherStationStateFactory = getSelectWeatherStationStateFactory();
TridentState selectState = topology.newStaticState(selectWeatherStationStateFactory);
stream = stream.stateQuery(selectState, new Fields("weather_station_id"), new CassandraQuery(), new Fields("name"));

----------------------------------------

TITLE: Maven Shade Plugin Configuration for HDFS Integration
DESCRIPTION: XML configuration for maven-shade-plugin to properly package Storm topologies with HDFS dependencies

LANGUAGE: xml
CODE:
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-shade-plugin</artifactId>
    <version>1.4</version>
    <configuration>
        <createDependencyReducedPom>true</createDependencyReducedPom>
    </configuration>
    <executions>
        <execution>
            <phase>package</phase>
            <goals>
                <goal>shade</goal>
            </goals>
            <configuration>
                <transformers>
                    <transformer
                            implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                    <transformer
                            implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                        <mainClass></mainClass>
                    </transformer>
                </transformers>
            </configuration>
        </execution>
    </executions>
</plugin>

----------------------------------------

TITLE: Configuring Storm Remote Cluster Connection in YAML
DESCRIPTION: YAML configuration for specifying the Nimbus server address in the storm.yaml file. This configuration allows the Storm client to communicate with a remote cluster by defining the master node's host address.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["123.45.678.890"]

----------------------------------------

TITLE: Configuring EsPercolateBolt for Storm-Elasticsearch Percolation in Java
DESCRIPTION: Initializes an EsPercolateBolt to send percolate requests from Storm to Elasticsearch. It uses EsConfig for cluster configuration and EsTupleMapper to extract fields from input tuples.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsPercolateBolt percolateBolt = new EsPercolateBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Redirecting to Apache Storm Documentation Using HTML Meta Refresh
DESCRIPTION: This HTML snippet uses a meta refresh tag to automatically redirect the user to the current Apache Storm documentation page about guaranteeing message processing. It also includes a canonical link tag for SEO purposes.

LANGUAGE: HTML
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Guaranteeing-message-processing.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Guaranteeing-message-processing.html" />

----------------------------------------

TITLE: User Defined Function Example
DESCRIPTION: Example of creating and implementing a user defined function in Storm SQL

LANGUAGE: sql
CODE:
CREATE FUNCTION MYPLUS AS 'org.apache.storm.sql.TestUtils$MyPlus'

LANGUAGE: java
CODE:
  public class MyPlus {
    public static Integer evaluate(Integer x, Integer y) {
      return x + y;
    }
  }

----------------------------------------

TITLE: Maven Shade Plugin Configuration for HDFS Integration
DESCRIPTION: XML configuration for the Maven Shade Plugin to properly package a Storm topology with HDFS integration.

LANGUAGE: xml
CODE:
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-shade-plugin</artifactId>
    <version>1.4</version>
    <configuration>
        <createDependencyReducedPom>true</createDependencyReducedPom>
    </configuration>
    <executions>
        <execution>
            <phase>package</phase>
            <goals>
                <goal>shade</goal>
            </goals>
            <configuration>
                <transformers>
                    <transformer
                            implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                    <transformer
                            implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                        <mainClass></mainClass>
                    </transformer>
                </transformers>
            </configuration>
        </execution>
    </executions>
</plugin>

----------------------------------------

TITLE: Defining TopicSelector Interface
DESCRIPTION: Interface definition for selecting RocketMQ topics and tags based on Storm tuples, providing flexibility in message routing.

LANGUAGE: java
CODE:
public interface TopicSelector extends Serializable {
    String getTopic(ITuple tuple);
    String getTag(ITuple tuple);
}

----------------------------------------

TITLE: Chaining Map and FlatMap Operations in Trident
DESCRIPTION: Example of chaining map and flatMap operations to process a stream of sentences.

LANGUAGE: java
CODE:
mystream.flatMap(new Split()).map(new UpperCase())

----------------------------------------

TITLE: MQTT Tuple Mapper Interface
DESCRIPTION: Java interface for mapping Storm tuples to MQTT messages

LANGUAGE: java
CODE:
public interface MqttTupleMapper extends Serializable{
    MqttMessage toMessage(ITuple tuple);
}

----------------------------------------

TITLE: Storm Shell Script Execution Format
DESCRIPTION: Format showing how Storm shell executes the topology script with Nimbus connection parameters.

LANGUAGE: bash
CODE:
python3 topology.py arg1 arg2 {nimbus-host} {nimbus-port} {uploaded-jar-location}

----------------------------------------

TITLE: Implementing Storm Event Logger Interface in Java
DESCRIPTION: Core interface definition for implementing custom event loggers in Storm. Defines methods for preparation, logging events, and cleanup.

LANGUAGE: java
CODE:
/**
 * EventLogger interface for logging the event info to a sink like log file or db
 * for inspecting the events via UI for debugging.
 */
public interface IEventLogger {
    /**
    * Invoked during eventlogger bolt prepare.
    */
    void prepare(Map stormConf, Map<String, Object> arguments, TopologyContext context);

    /**
     * Invoked when the {@link EventLoggerBolt} receives a tuple from the spouts or bolts that has event logging enabled.
     *
     * @param e the event
     */
    void log(EventInfo e);

    /**
    * Invoked when the event logger bolt is cleaned up
    */
    void close();
}

----------------------------------------

TITLE: Configuring Zookeeper Servers in Storm YAML
DESCRIPTION: This snippet shows how to configure the Zookeeper servers for a Storm cluster in the storm.yaml configuration file. It specifies the IP addresses of the Zookeeper servers.

LANGUAGE: yaml
CODE:
storm.zookeeper.servers:
  - "111.222.333.444"
  - "555.666.777.888"

----------------------------------------

TITLE: Spout Message Listening in Apache Storm (Clojure)
DESCRIPTION: Implementation of message listening for spouts in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L382

----------------------------------------

TITLE: Limiting Worker Heap Size in Storm Topology
DESCRIPTION: API call to set the maximum heap size for worker processes in a Storm topology.

LANGUAGE: java
CODE:
public void setTopologyWorkerMaxHeapSize(Number size)

----------------------------------------

TITLE: Configuring MQTT Topology with Flux YAML
DESCRIPTION: Flux YAML configuration for creating an MQTT topology with a spout and logging bolt.

LANGUAGE: yaml
CODE:
name: "mqtt-topology"

components:
   ########## MQTT Spout Config ############
  - id: "mqtt-type"
    className: "org.apache.storm.mqtt.examples.CustomMessageMapper"

  - id: "mqtt-options"
    className: "org.apache.storm.mqtt.common.MqttOptions"
    properties:
      - name: "url"
        value: "tcp://localhost:1883"
      - name: "topics"
        value:
          - "/users/tgoetz/#"

# topology configuration
config:
  topology.workers: 1
  topology.max.spout.pending: 1000

# spout definitions
spouts:
  - id: "mqtt-spout"
    className: "org.apache.storm.mqtt.spout.MqttSpout"
    constructorArgs:
      - ref: "mqtt-type"
      - ref: "mqtt-options"
    parallelism: 1

# bolt definitions
bolts:
  - id: "log"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1


streams:
  - from: "mqtt-spout"
    to: "log"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Setting up EsState for Storm-Elasticsearch Trident Integration in Java
DESCRIPTION: Creates an EsState for Trident topologies to interact with Elasticsearch. It requires EsConfig and EsTupleMapper, and uses EsStateFactory to create the state.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();

StateFactory factory = new EsStateFactory(esConfig, tupleMapper);
TridentState state = stream.partitionPersist(factory, esFields, new EsUpdater(), new Fields());

----------------------------------------

TITLE: Configuring Metric Reporters in YAML
DESCRIPTION: Example configuration for setting up two metric reporters: a Graphite Reporter and a Console Reporter. The configuration includes reporting intervals, host details for Graphite, and a regex filter for the Console Reporter.

LANGUAGE: yaml
CODE:
topology.metrics.reporters:
  # Graphite Reporter
  - class: "org.apache.storm.metrics2.reporters.GraphiteStormReporter"
    report.period: 60
    report.period.units: "SECONDS"
    graphite.host: "localhost"
    graphite.port: 2003

  # Console Reporter
  - class: "org.apache.storm.metrics2.reporters.ConsoleStormReporter"
    report.period: 10
    report.period.units: "SECONDS"
    filter:
        class: "org.apache.storm.metrics2.filters.RegexFilter"
        expression: ".*my_component.*emitted.*"

----------------------------------------

TITLE: Worker Process Initialization
DESCRIPTION: Clojure code showing how worker processes are created and initialized, including connection establishment with other workers and task thread management.

LANGUAGE: clojure
CODE:
(mk-worker)
(connect-to-workers)
(monitor-topology-active-state)
(launch-task-threads)

----------------------------------------

TITLE: Implementing Task Hooks in Storm
DESCRIPTION: Task hooks can be registered either through the TopologyContext in spout/bolt initialization methods or via Storm configuration. They allow custom code execution for various Storm events by extending BaseTaskHook.

LANGUAGE: java
CODE:
TopologyContext.addTaskHook()  // Method 1: Register hook in spout/bolt

"topology.auto.task.hooks"     // Method 2: Register via configuration

----------------------------------------

TITLE: Trident Kafka Spout Example
DESCRIPTION: Example showing how to create and configure a Trident Kafka spout.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
BrokerHosts zk = new ZkHosts("localhost");
TridentKafkaConfig spoutConf = new TridentKafkaConfig(zk, "test-topic");
spoutConf.scheme = new SchemeAsMultiScheme(new StringScheme());
OpaqueTridentKafkaSpout spout = new OpaqueTridentKafkaSpout(spoutConf);

----------------------------------------

TITLE: Implementing Custom Metric in Java Bolt
DESCRIPTION: Shows how to implement a custom metric (execute count) in a Storm bolt. This includes declaring the metric, initializing and registering it in the prepare method, and incrementing it in the execute method.

LANGUAGE: java
CODE:
private transient CountMetric countMetric;

@Override
public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
	// other initialization here.
	countMetric = new CountMetric();
	context.registerMetric("execute_count", countMetric, 60);
}

public void execute(Tuple input) {
	countMetric.incr();
	// handle tuple here.	
}

----------------------------------------

TITLE: Storm JAR Command Execution
DESCRIPTION: Command to run a Storm topology jar with optional external dependencies and maven artifacts. Supports specifying additional jars and maven artifacts with transitive dependencies.

LANGUAGE: bash
CODE:
storm jar topology-jar-path class ...

----------------------------------------

TITLE: Defining Shell Bolt in Storm
DESCRIPTION: Example of defining a shell bolt that runs a Python script, showing input declarations and output field specifications.

LANGUAGE: clojure
CODE:
(shell-bolt-spec {"1" :shuffle "2" ["id"]}
                 "python"
                 "mybolt.py"
                 ["outfield1" "outfield2"]
                 :p 25)

----------------------------------------

TITLE: Creating SimpleHBaseMapper in Java
DESCRIPTION: Example of creating a SimpleHBaseMapper instance, which is a general-purpose HBaseMapper implementation for mapping Storm tuples to HBase columns.

LANGUAGE: java
CODE:
SimpleHBaseMapper mapper = new SimpleHBaseMapper() 
        .withRowKeyField("word")
        .withColumnFields(new Fields("word"))
        .withCounterFields(new Fields("count"))
        .withColumnFamily("cf");

----------------------------------------

TITLE: Storm Supervisor Resource Configuration
DESCRIPTION: YAML configuration format for specifying available node resources in the storm.yaml configuration file.

LANGUAGE: yaml
CODE:
supervisor.resources.map: {[type<String>] : [amount<Double>]}

----------------------------------------

TITLE: Implementing HTML Meta Refresh Redirect for Storm Documentation
DESCRIPTION: This HTML snippet sets up an automatic redirect to the current Storm documentation page for transactional topologies. It also includes a canonical link for search engines.

LANGUAGE: HTML
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Transactional-topologies.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Transactional-topologies.html" />

----------------------------------------

TITLE: Defining Shell Bolt in Storm
DESCRIPTION: Example of defining a shell bolt that runs a Python script, showing input declarations and output field specifications.

LANGUAGE: clojure
CODE:
(shell-bolt-spec {"1" :shuffle "2" ["id"]}
                 "python"
                 "mybolt.py"
                 ["outfield1" "outfield2"]
                 :p 25)

----------------------------------------

TITLE: Retrieving Detailed Topology Information in JSON
DESCRIPTION: Example response from the /api/v1/topology/<id> endpoint, which returns detailed information and statistics for a specific topology.

LANGUAGE: json
CODE:
{
  "name": "WordCount3",
  "id": "WordCount3-1-1402960825",
  "workersTotal": 3,
  "window": "600",
  "status": "ACTIVE",
  "tasksTotal": 28,
  "executorsTotal": 28,
  "uptime": "29m 19s",
  "uptimeSeconds": 1759,
  "msgTimeout": 30,
  "windowHint": "10m 0s",
  "schedulerDisplayResource": true,
  "topologyStats": [
    {
      "windowPretty": "10m 0s",
      "window": "600",
      "emitted": 397960,
      "transferred": 213380,
      "completeLatency": "0.000",
      "acked": 213460,
      "failed": 0
    }
  ],
  "spouts": [
    {
      "executors": 5,
      "emitted": 28880,
      "completeLatency": "0.000",
      "transferred": 28880,
      "acked": 0,
      "spoutId": "spout",
      "tasks": 5,
      "lastError": "",
      "errorLapsedSecs": null,
      "failed": 0
    }
  ],
  "bolts": [
    {
      "executors": 12,
      "emitted": 184580,
      "transferred": 0,
      "acked": 184640,
      "executeLatency": "0.048",
      "tasks": 12,
      "executed": 184620,
      "processLatency": "0.043",
      "boltId": "count",
      "lastError": "",
      "errorLapsedSecs": null,
      "capacity": "0.003",
      "failed": 0
    }
  ]
}

----------------------------------------

TITLE: Spout Tuple Transfer in Apache Storm
DESCRIPTION: Implementation of how spouts transfer tuples using the worker-provided transfer function.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L329

----------------------------------------

TITLE: Defining Kafka Topic Selector Interface
DESCRIPTION: Interface definition for selecting Kafka topics to publish messages to. Contains a single method to determine topic based on tuple content.

LANGUAGE: java
CODE:
public interface KafkaTopicSelector {
    String getTopics(Tuple/TridentTuple tuple);
}

----------------------------------------

TITLE: Redis Filter Mapper Implementation
DESCRIPTION: Implementation of RedisFilterMapper interface for filtering words using a Redis set as a blacklist.

LANGUAGE: java
CODE:
class BlacklistWordFilterMapper implements RedisFilterMapper {
    private RedisDataTypeDescription description;
    private final String setKey = "blacklist";

    public BlacklistWordFilterMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.SET, setKey);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Implementing Kafka Producer Bolt
DESCRIPTION: Example showing how to configure and use KafkaBolt to write messages to Kafka from a Storm topology. Sets up producer properties, topic selection, and tuple mapping.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();

Fields fields = new Fields("key", "message");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
            new Values("storm", "1"),
            new Values("trident", "1"),
            new Values("needs", "1"),
            new Values("javadoc", "1")
);
spout.setCycle(true);
builder.setSpout("spout", spout, 5);
//set producer properties.
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

KafkaBolt bolt = new KafkaBolt()
        .withProducerProperties(props)
        .withTopicSelector(new DefaultTopicSelector("test"))
        .withTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper());
builder.setBolt("forwardToKafka", bolt, 8).shuffleGrouping("spout");

Config conf = new Config();

StormSubmitter.submitTopology("kafkaboltTest", conf, builder.createTopology());

----------------------------------------

TITLE: Registering a Metrics Consumer in Java
DESCRIPTION: Example of how to register a metrics consumer to a topology configuration in Java code.

LANGUAGE: java
CODE:
conf.registerMetricsConsumer(org.apache.storm.metric.LoggingMetricsConsumer.class, 1);

----------------------------------------

TITLE: Implementing Bulk Operations for Custom State
DESCRIPTION: Example of implementing bulk operations for a custom State (LocationDB) to improve efficiency.

LANGUAGE: java
CODE:
public class LocationDB implements State {
    public void beginCommit(Long txid) {    
    }
    
    public void commit(Long txid) {    
    }
    
    public void setLocationsBulk(List<Long> userIds, List<String> locations) {
      // set locations in bulk
    }
    
    public List<String> bulkGetLocations(List<Long> userIds) {
      // get locations in bulk
    }
}

----------------------------------------

TITLE: Implementing RedisLookupMapper in Java
DESCRIPTION: Java class implementing RedisLookupMapper interface for word count lookup in Redis hash.

LANGUAGE: java
CODE:
class WordCountRedisLookupMapper implements RedisLookupMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountRedisLookupMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public List<Values> toTuple(ITuple input, Object value) {
        String member = getKeyFromTuple(input);
        List<Values> values = Lists.newArrayList();
        values.add(new Values(member, value));
        return values;
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("wordName", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Configuring UI/Logviewer Filter in Storm YAML
DESCRIPTION: YAML configuration for setting up authentication filters for Storm UI and logviewer

LANGUAGE: yaml
CODE:
ui.filter: "filter.class"
ui.filter.params: "param1":"value1"
logviewer.filter: "filter.class"
logviewer.filter.params: "param1":"value1"

----------------------------------------

TITLE: Debugging Nimbus JVM Shutdown in Java
DESCRIPTION: This snippet shows log output and an exception trace when the Nimbus JVM shuts down immediately after startup. The issue is related to using an incompatible version of rocksdb-jni on older CPUs lacking certain instruction sets.

LANGUAGE: plaintext
CODE:
2024-01-05 18:54:20.404 [o.a.s.v.ConfigValidation] INFO: Will use [class org.apache.storm.DaemonConfig, class org.apache.storm.Config] for validation
2024-01-05 18:54:20.556 [o.a.s.z.AclEnforcement] INFO: SECURITY IS DISABLED NO FURTHER CHECKS...
2024-01-05 18:54:20.740 [o.a.s.m.r.RocksDbStore] INFO: Opening RocksDB from <your-storm-folder>/storm_rocks, storm.metricstore.rocksdb.create_if_missing=true

LANGUAGE: plaintext
CODE:
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  EXCEPTION_ILLEGAL_INSTRUCTION (0xc000001d) at pc=0x00007ff94dc7a56d, pid=12728, tid=0x0000000000001d94
#
# JRE version: OpenJDK Runtime Environment (8.0_232) (build 1.8.0_232-09)
# Java VM: OpenJDK 64-Bit Server VM (25.232-b09 mixed mode windows-amd64 compressed oops)
# Problematic frame:
# C  [librocksdbjni4887247215762585789.dll+0x53a56d]

----------------------------------------

TITLE: Pinning Workers to NUMA Zones using CgroupManager in Linux
DESCRIPTION: This snippet demonstrates how the CgroupManager prefixes the worker launch command with the numactl command to bind workers to specific NUMA zones on Linux hosts.

LANGUAGE: bash
CODE:
numactl --cpunodebind=<numaId>> --membind=<numaId> <worker launch command>

----------------------------------------

TITLE: Local Mode Message Sending in Apache Storm (Clojure)
DESCRIPTION: In local mode, tuples are sent directly to an in-memory queue for the receiving task.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/local.clj#L21

----------------------------------------

TITLE: Configuring Skewed Streaming Top N Pattern in Storm
DESCRIPTION: This snippet shows how to set up a streaming top N pattern for skewed data in Storm. It uses partialKeyGrouping for load distribution, followed by aggregation and ranking, and finally merging the results.

LANGUAGE: java
CODE:
builder.setBolt("count", new CountObjects(), parallelism)
  .partialKeyGrouping("objects", new Fields("value"));
builder.setBolt("rank" new AggregateCountsAndRank(), parallelism)
  .fieldsGrouping("count", new Fields("key"))
builder.setBolt("merge", new MergeRanksObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Tuple Serialization in Storm
DESCRIPTION: Thread-safe implementation of tuple serialization using Kryo serializer.

LANGUAGE: java
CODE:
KryoTupleSerializer

----------------------------------------

TITLE: IConfigLoader Interface Definition in Java
DESCRIPTION: Definition of the IConfigLoader interface with a single method 'load' that retrieves the most recent configuration map.

LANGUAGE: java
CODE:
public interface IConfigLoader {
    Map<?,?> load();
};

----------------------------------------

TITLE: Configuring Isolation Scheduler in YAML
DESCRIPTION: Example YAML configuration for the Storm Isolation Scheduler. It specifies the number of isolated machines allocated to different topologies, allowing for resource isolation and multi-tenancy.

LANGUAGE: yaml
CODE:
isolation.scheduler.machines: 
    "my-topology": 8
    "tiny-topology": 1
    "some-other-topology": 3

----------------------------------------

TITLE: Implementing RocketMQ Trident State
DESCRIPTION: Configuration of RocketMQ persistent state for Trident topologies, including mapper, selector, and state factory setup for stream processing.

LANGUAGE: java
CODE:
        TupleToMessageMapper mapper = new FieldNameBasedTupleToMessageMapper("word", "count");
        TopicSelector selector = new DefaultTopicSelector(topic);

        Properties properties = new Properties();
        properties.setProperty(RocketMqConfig.NAME_SERVER_ADDR, nameserverAddr);

        RocketMqState.Options options = new RocketMqState.Options()
                .withMapper(mapper)
                .withSelector(selector)
                .withProperties(properties);

        StateFactory factory = new RocketMqStateFactory(options);

        TridentTopology topology = new TridentTopology();
        Stream stream = topology.newStream("spout1", spout);

        stream.partitionPersist(factory, fields,
                new RocketMqStateUpdater(), new Fields());

----------------------------------------

TITLE: Implementing Trident Topology with Fields Mapper
DESCRIPTION: Shows how to create a Trident topology using SolrStateFactory with fields mapping. Configures Solr connection and schema building for the 'gettingstarted' collection.

LANGUAGE: java
CODE:
    new SolrStateFactory(solrConfig, solrMapper);
    
    // zkHostString for Solr 'gettingstarted' example
    SolrConfig solrConfig = new SolrConfig("127.0.0.1:9983");
    
    /* Solr Fields Mapper used to generate 'SolrRequest' requests to update the "gettingstarted" Solr collection. The Solr index is updated using the field values of the tuple fields that match static or dynamic fields declared in the schema object build using schemaBuilder */ 
    SolrMapper solrMapper = new SolrFieldsMapper.Builder(schemaBuilder, "gettingstarted").build();

    // builds the Schema object from the JSON representation of the schema as returned by the URL http://localhost:8983/solr/gettingstarted/schema/ 
    SchemaBuilder schemaBuilder = new RestJsonSchemaBuilder("localhost", "8983", "gettingstarted")

----------------------------------------

TITLE: Submitting DRPC Topology in Java
DESCRIPTION: This code snippet shows how to submit a DRPC topology to a Storm cluster using StormSubmitter.

LANGUAGE: java
CODE:
StormSubmitter.submitTopology("exclamation-drpc", conf, builder.createRemoteTopology());

----------------------------------------

TITLE: DRPC Client Implementation Example
DESCRIPTION: Java code example showing how to create a DRPC client for production use.

LANGUAGE: java
CODE:
Config conf = new Config();
try (DRPCClient drpc = DRPCClient.getConfiguredClient(conf)) {
  //User the drpc client
  String result = drpc.execute(function, argument);
}

----------------------------------------

TITLE: Implementing Bulk Operations for Custom State
DESCRIPTION: Example of implementing bulk operations for a custom State (LocationDB) to improve efficiency.

LANGUAGE: java
CODE:
public class LocationDB implements State {
    public void beginCommit(Long txid) {    
    }
    
    public void commit(Long txid) {    
    }
    
    public void setLocationsBulk(List<Long> userIds, List<String> locations) {
      // set locations in bulk
    }
    
    public List<String> bulkGetLocations(List<Long> userIds) {
      // get locations in bulk
    }
}

----------------------------------------

TITLE: Setting Log Level Using Storm CLI
DESCRIPTION: Commands to dynamically set and clear log levels for Storm topologies using the command line interface. Supports setting specific logger levels with optional timeout values.

LANGUAGE: bash
CODE:
./bin/storm set_log_level [topology name] -l [logger name]=[LEVEL]:[TIMEOUT]

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -l ROOT=DEBUG:30

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -r ROOT

----------------------------------------

TITLE: Implementing FailedMessageRetryHandler Interface in Java
DESCRIPTION: This code snippet demonstrates the interface for FailedMessageRetryHandler, which is used to handle retries for failed messages in the Kinesis Spout. It includes methods for handling failed messages, retrieving next failed messages, and acknowledging messages.

LANGUAGE: java
CODE:
    boolean failed (KinesisMessageId messageId);
    KinesisMessageId getNextFailedMessageToRetry ();
    void failedMessageEmitted (KinesisMessageId messageId);
    void acked (KinesisMessageId messageId);

----------------------------------------

TITLE: SQL Grammar Definition
DESCRIPTION: BNF-style grammar definition for Storm SQL statements including DDL and DML commands

LANGUAGE: sql
CODE:
statement:
      setStatement
  |   resetStatement
  |   explain
  |   describe
  |   insert
  |   update
  |   merge
  |   delete
  |   query

setStatement:
      [ ALTER ( SYSTEM | SESSION ) ] SET identifier '=' expression

----------------------------------------

TITLE: Maven Dependency Configuration for Storm-Redis
DESCRIPTION: Maven dependency configuration to include storm-redis in a Storm project.

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>storm-redis</artifactId>
    <version>${storm.version}</version>
    <type>jar</type>
</dependency>

----------------------------------------

TITLE: Rebalancing Topology in Storm UI REST API
DESCRIPTION: Example JSON request body and response for the /api/v1/topology/:id/rebalance/:wait-time POST endpoint, which rebalances a topology.

LANGUAGE: json
CODE:
// Request body
{"rebalanceOptions": {"numWorkers": 2, "executors": { "spout" : "5", "split": 7, "count": 5 }}, "callback":"foo"}

// Response
{"topologyOperation":"rebalance","topologyId":"wordcount-1-1420308665","status":"success"}

----------------------------------------

TITLE: Registering a Counter Metric in Apache Storm
DESCRIPTION: Example of registering a counter metric named 'myCounter' using the TopologyContext in Apache Storm.

LANGUAGE: java
CODE:
Counter myCounter = topologyContext.registerCounter("myCounter");

----------------------------------------

TITLE: Implementing Trident Topology with Fields Mapper
DESCRIPTION: Shows how to create a Trident topology using SolrStateFactory with fields mapping. Configures Solr connection and schema building for the 'gettingstarted' collection.

LANGUAGE: java
CODE:
    new SolrStateFactory(solrConfig, solrMapper);
    
    // zkHostString for Solr 'gettingstarted' example
    SolrConfig solrConfig = new SolrConfig("127.0.0.1:9983");
    
    /* Solr Fields Mapper used to generate 'SolrRequest' requests to update the "gettingstarted" Solr collection. The Solr index is updated using the field values of the tuple fields that match static or dynamic fields declared in the schema object build using schemaBuilder */ 
    SolrMapper solrMapper = new SolrFieldsMapper.Builder(schemaBuilder, "gettingstarted").build();

    // builds the Schema object from the JSON representation of the schema as returned by the URL http://localhost:8983/solr/gettingstarted/schema/ 
    SchemaBuilder schemaBuilder = new RestJsonSchemaBuilder("localhost", "8983", "gettingstarted")

----------------------------------------

TITLE: Starting ActiveMQ Server
DESCRIPTION: Command to start the Apache ActiveMQ server for JMS messaging.

LANGUAGE: bash
CODE:
$ [ACTIVEMQ_HOME]/bin/activemq

----------------------------------------

TITLE: Implementing SimpleMongoUpdateMapper for Storm-MongoDB Updates in Java
DESCRIPTION: Implements the MongoMapper interface for updating MongoDB documents. This mapper uses the $set operator to update specified fields in MongoDB documents based on Storm tuple data.

LANGUAGE: java
CODE:
public class SimpleMongoUpdateMapper implements MongoMapper {
    private String[] fields;

    @Override
    public Document toDocument(ITuple tuple) {
        Document document = new Document();
        for(String field : fields){
            document.append(field, tuple.getValueByField(field));
        }
        return new Document("$set", document);
    }

    public SimpleMongoUpdateMapper withFields(String... fields) {
        this.fields = fields;
        return this;
    }
}

----------------------------------------

TITLE: Configuring Storm Cluster State Store
DESCRIPTION: Basic configuration to enable Pacemaker as the cluster state store in Storm.

LANGUAGE: yaml
CODE:
storm.cluster.state.store: "org.apache.storm.cluster.PaceMakerStateStorageFactory"

----------------------------------------

TITLE: Initializing EsState for Trident in Java
DESCRIPTION: Creates an EsState instance for Trident topologies to persist data in Elasticsearch. It requires an EsConfig, EsTupleMapper, and additional Trident-specific configuration.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();

StateFactory factory = new EsStateFactory(esConfig, tupleMapper);
TridentState state = stream.partitionPersist(factory, esFields, new EsUpdater(), new Fields());

----------------------------------------

TITLE: Configuring HDFS Sequence File Bolt in Java
DESCRIPTION: Example of configuring an HDFS bolt to write sequence files with compression.

LANGUAGE: java
CODE:
// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withExtension(".seq")
        .withPath("/data/");

// create sequence format instance.
DefaultSequenceFormat format = new DefaultSequenceFormat("timestamp", "sentence");

SequenceFileBolt bolt = new SequenceFileBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withSequenceFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy)
        .withCompressionType(SequenceFile.CompressionType.RECORD)
        .withCompressionCodec("deflate");

----------------------------------------

TITLE: Maven Assembly Plugin Configuration for Storm Topology
DESCRIPTION: Maven configuration to package a Storm topology as a JAR with dependencies. Configures the assembly plugin to create an executable JAR that includes all required dependencies.

LANGUAGE: xml
CODE:
  <plugin>
    <artifactId>maven-assembly-plugin</artifactId>
    <configuration>
      <descriptorRefs>  
        <descriptorRef>jar-with-dependencies</descriptorRef>
      </descriptorRefs>
      <archive>
        <manifest>
          <mainClass>com.path.to.main.Class</mainClass>
        </manifest>
      </archive>
    </configuration>
  </plugin>

----------------------------------------

TITLE: Killing a Topology in Storm (Clojure)
DESCRIPTION: The kill command triggers a transition in Nimbus that deactivates the topology for a wait time before removing it. This ensures fault tolerance and allows for graceful shutdown.

LANGUAGE: clojure
CODE:
(kill-transition)

----------------------------------------

TITLE: Implementing RecordToTupleMapper Interface in Java
DESCRIPTION: This code snippet shows the RecordToTupleMapper interface used by the Kinesis Spout to convert Kinesis records to Storm tuples. It defines two methods: getOutputFields() for specifying tuple fields and getTuple() for converting a record to a tuple.

LANGUAGE: java
CODE:
    Fields getOutputFields ();
    List<Object> getTuple (Record record);

----------------------------------------

TITLE: Creating RedisFilterBolt Instance
DESCRIPTION: Java code snippet for creating a RedisFilterBolt instance using JedisPoolConfig and BlacklistWordFilterMapper.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisFilterMapper filterMapper = new BlacklistWordFilterMapper();
RedisFilterBolt filterBolt = new RedisFilterBolt(poolConfig, filterMapper);

----------------------------------------

TITLE: Defining IEventLogger Interface in Java for Apache Storm
DESCRIPTION: This code snippet defines the IEventLogger interface used by the event logger bolt to log events in Apache Storm. It includes methods for preparation, logging events, and cleanup.

LANGUAGE: java
CODE:
public interface IEventLogger {
    void prepare(Map stormConf, Map<String, Object> arguments, TopologyContext context);
    void log(EventInfo e);
    void close();
}

----------------------------------------

TITLE: Configuring Health Check Directory in Storm YAML
DESCRIPTION: This configuration specifies the directory where health check scripts are located. These scripts are used to monitor the health of supervisor nodes.

LANGUAGE: yaml
CODE:
storm.health.check.dir: "healthchecks"

----------------------------------------

TITLE: Configuring HBase State Provider in Storm
DESCRIPTION: JSON configuration for the HBase state provider, specifying table and column family details. This configuration is set in the storm.yaml file or programmatically.

LANGUAGE: json
CODE:
{
  "keyClass": "Optional fully qualified class name of the Key type.",
  "valueClass": "Optional fully qualified class name of the Value type.",
  "keySerializerClass": "Optional Key serializer implementation class.",
  "valueSerializerClass": "Optional Value Serializer implementation class.",
  "hbaseConfigKey": "config key to load hbase configuration from storm root configuration. (similar to storm-hbase)",
  "tableName": "Pre-created table name for state.",
  "columnFamily": "Pre-created column family for state."
}

----------------------------------------

TITLE: Defining ModelRunner Interface in Java for Storm PMML Bolt
DESCRIPTION: This snippet shows the core method of the ModelRunner interface, which is responsible for computing scored tuples from raw input tuples. The method returns a map of stream IDs to lists of scored values.

LANGUAGE: java
CODE:
Map<String, List<Object>> scoredTuplePerStream(Tuple input);

----------------------------------------

TITLE: Implementing RecordToTupleMapper Interface in Java
DESCRIPTION: This code snippet shows the interface definition for RecordToTupleMapper, which is used to convert Kinesis records to Storm tuples. It includes two methods: getOutputFields() to define the tuple fields, and getTuple() to convert a Kinesis record to a tuple.

LANGUAGE: java
CODE:
    Fields getOutputFields ();
    List<Object> getTuple (Record record);

----------------------------------------

TITLE: Configuring Shell Component with Blob Store
DESCRIPTION: Example showing how to configure a Shell Spout to work with executables shipped via blob store by disabling CWD change.

LANGUAGE: java
CODE:
public MyShellSpout() {
    super("./newPython/bin/python3", "./shell_spout.py");
    changeChildCWD(false);
}

----------------------------------------

TITLE: Worker Level Metrics Configuration
DESCRIPTION: Example showing how to configure worker level metrics in storm.yaml.

LANGUAGE: yaml
CODE:
worker.metrics: 
  metricA: "aaa.bbb.ccc.ddd.MetricA"
  metricB: "aaa.bbb.ccc.ddd.MetricB"
  ...

----------------------------------------

TITLE: MQTT Tuple Mapper Interface
DESCRIPTION: Interface for converting Storm tuples to MQTT messages

LANGUAGE: java
CODE:
public interface MqttTupleMapper extends Serializable{
    MqttMessage toMessage(ITuple tuple);
}

----------------------------------------

TITLE: Applying a Custom Filter to a Trident Stream
DESCRIPTION: Example of how to apply a custom filter to a Trident stream.

LANGUAGE: java
CODE:
mystream.filter(new MyFilter())

----------------------------------------

TITLE: Storm ISpout Interface Definition
DESCRIPTION: Core interface that spouts must implement to participate in Storm's reliability mechanisms.

LANGUAGE: java
CODE:
public interface ISpout extends Serializable {
    void open(Map conf, TopologyContext context, SpoutOutputCollector collector);
    void close();
    void nextTuple();
    void ack(Object msgId);
    void fail(Object msgId);
}

----------------------------------------

TITLE: Creating a Blob in Storm Distributed Cache (Command Line)
DESCRIPTION: Example of creating a blob in the Storm distributed cache using the command line interface. This command adds a file to the cache with specified access control and replication factor.

LANGUAGE: bash
CODE:
storm blobstore create --file README.txt --acl o::rwa --replication-factor 4 key1

----------------------------------------

TITLE: Enabling Resource Aware Scheduler Configuration in Storm
DESCRIPTION: YAML configuration to enable the Resource Aware Scheduler in Storm's configuration file

LANGUAGE: yaml
CODE:
storm.scheduler: "org.apache.storm.scheduler.resource.ResourceAwareScheduler"

----------------------------------------

TITLE: Implementing HTML Meta Refresh Redirect for Apache Storm Documentation
DESCRIPTION: This HTML snippet sets up an automatic redirect to the current Apache Storm Troubleshooting documentation page. It also includes a canonical link for search engine optimization.

LANGUAGE: HTML
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Troubleshooting.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Troubleshooting.html" />

----------------------------------------

TITLE: Configuring Storm Cluster State Store for Pacemaker
DESCRIPTION: Sets the cluster state store to use the PaceMakerStateStorageFactory, enabling Pacemaker for worker heartbeats.

LANGUAGE: yaml
CODE:
storm.cluster.state.store: "org.apache.storm.cluster.PaceMakerStateStorageFactory"

----------------------------------------

TITLE: Message Sending Protocol in Apache Storm (Clojure)
DESCRIPTION: Definition of the protocol used for message sending in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/protocol.clj

----------------------------------------

TITLE: Dequeueing Items from Kestrel Queue in Java
DESCRIPTION: This method dequeues items from a Kestrel queue without removing them. It demonstrates how to retrieve messages from the queue and process them.

LANGUAGE: Java
CODE:
    private static void dequeueItems(KestrelClient kestrelClient, String queueName) throws IOException, ParseError
			 {
		for(int i=1; i<=12; i++){

			Item item = kestrelClient.dequeue(queueName);

			if(item==null){
				System.out.println("The queue (" + queueName + ") contains no items.");
			}
			else
			{
				byte[] data = item._data;

				String receivedVal = new String(data);

				System.out.println("receivedItem=" + receivedVal);
			}
		}

----------------------------------------

TITLE: Implementing SimpleQueryFilterCreator for Storm-MongoDB Updates in Java
DESCRIPTION: Implements the QueryFilterCreator interface to create simple equality filters for MongoDB queries. This implementation uses the $eq operator to match documents based on a specified field from the Storm tuple.

LANGUAGE: java
CODE:
public class SimpleQueryFilterCreator implements QueryFilterCreator {
    private String field;
    
    @Override
    public Bson createFilter(ITuple tuple) {
        return Filters.eq(field, tuple.getValueByField(field));
    }

    public SimpleQueryFilterCreator withField(String field) {
        this.field = field;
        return this;
    }
}

----------------------------------------

TITLE: Implementing Custom MqttTupleMapper in Java
DESCRIPTION: Example implementation of MqttTupleMapper that maps tuple data to MQTT messages.

LANGUAGE: java
CODE:
public class MyTupleMapper implements MqttTupleMapper {
    public MqttMessage toMessage(ITuple tuple) {
        String topic = "users/" + tuple.getStringByField("userId") + "/" + tuple.getStringByField("device");
        byte[] payload = tuple.getStringByField("message").getBytes();
        return new MqttMessage(topic, payload);
    }
}

----------------------------------------

TITLE: Configuring Supervisor Slots Ports
DESCRIPTION: Configuration for defining worker ports on supervisor nodes

LANGUAGE: yaml
CODE:
supervisor.slots.ports:
    - 6700
    - 6701
    - 6702
    - 6703

----------------------------------------

TITLE: Implementing QueryFunction for Location Lookup
DESCRIPTION: Example of a QueryFunction that retrieves locations from the custom LocationDB State.

LANGUAGE: java
CODE:
public class QueryLocation extends BaseQueryFunction<LocationDB, String> {
    public List<String> batchRetrieve(LocationDB state, List<TridentTuple> inputs) {
        List<String> ret = new ArrayList();
        for(TridentTuple input: inputs) {
            ret.add(state.getLocation(input.getLong(0)));
        }
        return ret;
    }

    public void execute(TridentTuple tuple, String location, TridentCollector collector) {
        collector.emit(new Values(location));
    }    
}

----------------------------------------

TITLE: Configuring Trident State for HDFS Integration in Java
DESCRIPTION: This example shows how to set up a Trident state implementation for writing data to HDFS, including file naming, record formatting, and rotation policies.

LANGUAGE: java
CODE:
Fields hdfsFields = new Fields("field1", "field2");

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withPath("/trident")
        .withPrefix("trident")
        .withExtension(".txt");

RecordFormat recordFormat = new DelimitedRecordFormat()
        .withFields(hdfsFields);

FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, FileSizeRotationPolicy.Units.MB);

HdfsState.Options options = new HdfsState.HdfsFileOptions()
        .withFileNameFormat(fileNameFormat)
        .withRecordFormat(recordFormat)
        .withRotationPolicy(rotationPolicy)
        .withFsUrl("hdfs://localhost:54310");

StateFactory factory = new HdfsStateFactory().withOptions(options);

TridentState state = stream
        .partitionPersist(factory, hdfsFields, new HdfsUpdater(), new Fields());

----------------------------------------

TITLE: Setting Storm Local Directory in YAML
DESCRIPTION: This snippet demonstrates how to set the local directory for Storm to store small amounts of state. It shows examples for both Unix and Windows systems.

LANGUAGE: yaml
CODE:
storm.local.dir: "/mnt/storm"

LANGUAGE: yaml
CODE:
storm.local.dir: "C:\\storm-local"

----------------------------------------

TITLE: Implementing Storm Event Logger Interface in Java
DESCRIPTION: Interface definition for Storm's event logging system. IEventLogger provides methods for preparing, logging events, and cleanup. This interface can be implemented to create custom event logging solutions.

LANGUAGE: java
CODE:
/**
 * EventLogger interface for logging the event info to a sink like log file or db
 * for inspecting the events via UI for debugging.
 */
public interface IEventLogger {
    /**
    * Invoked during eventlogger bolt prepare.
    */
    void prepare(Map stormConf, Map<String, Object> arguments, TopologyContext context);

    /**
     * Invoked when the {@link EventLoggerBolt} receives a tuple from the spouts or bolts that has event logging enabled.
     *
     * @param e the event
     */
    void log(EventInfo e);

    /**
    * Invoked when the event logger bolt is cleaned up
    */
    void close();
}

----------------------------------------

TITLE: Implementing Trident FlatMap Function
DESCRIPTION: Example of implementing a flatMap function that splits sentences into individual words in a Trident stream.

LANGUAGE: java
CODE:
public class Split extends FlatMapFunction {
  @Override
  public Iterable<Values> execute(TridentTuple input) {
    List<Values> valuesList = new ArrayList<>();
    for (String word : input.getString(0).split(" ")) {
      valuesList.add(new Values(word));
    }
    return valuesList;
  }
}

----------------------------------------

TITLE: Message Sending Protocol Implementation in Apache Storm (Clojure)
DESCRIPTION: This code defines the protocol for sending messages between workers in Apache Storm.

LANGUAGE: clojure
CODE:
(defprotocol WorkerMessageTransfer)

----------------------------------------

TITLE: Message Sending Protocol Implementation in Apache Storm (Clojure)
DESCRIPTION: This code defines the protocol for sending messages between workers in Apache Storm.

LANGUAGE: clojure
CODE:
(defprotocol WorkerMessageTransfer)

----------------------------------------

TITLE: Executing Storm SQL Command
DESCRIPTION: Basic command to compile SQL statements into Storm topology and submit to cluster.

LANGUAGE: bash
CODE:
$ bin/storm sql <sql-file> <topo-name>

----------------------------------------

TITLE: Defining User Resource Pools
DESCRIPTION: Configuration for setting up user resource pools with CPU and memory guarantees

LANGUAGE: yaml
CODE:
resource.aware.scheduler.user.pools:
    jerry:
        cpu: 1000
        memory: 8192.0
    derek:
        cpu: 10000.0
        memory: 32768
    bobby:
        cpu: 5000.0
        memory: 16384.0

----------------------------------------

TITLE: Registering Metrics Consumers in YAML Config
DESCRIPTION: Example of how to register metrics consumers in the Storm YAML configuration file.

LANGUAGE: yaml
CODE:
topology.metrics.consumer.register:
  - class: "org.apache.storm.metric.LoggingMetricsConsumer"
    parallelism.hint: 1
  - class: "org.apache.storm.metric.HttpForwardingMetricsConsumer"
    parallelism.hint: 1
    argument: "http://example.com:8080/metrics/my-topology/"

----------------------------------------

TITLE: Setting Log Level via Storm CLI
DESCRIPTION: Commands for setting and resetting log levels using Storm CLI. Allows specifying logger name, level, and optional timeout period. The command supports both setting new log levels and reverting to original values.

LANGUAGE: bash
CODE:
./bin/storm set_log_level [topology name] -l [logger name]=[LEVEL]:[TIMEOUT]

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -l ROOT=DEBUG:30

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -r ROOT

----------------------------------------

TITLE: Converting Docker Images to Squashfs for HDFS
DESCRIPTION: Command to pull Docker images, convert layers to squashfs, and push to HDFS using the docker-to-squash.py script

LANGUAGE: bash
CODE:
python3 docker-to-squash.py pull-build-push-update --hdfs-root hdfs://hostname:port/containers \
                      docker.xxx.com:4443/hadoop-user-images/storm/rhel7:20201202-232133,storm/rhel7:dev_current --log DEBUG --bootstrap

----------------------------------------

TITLE: Setting Component Memory Requirements in Storm Topology
DESCRIPTION: Example of setting memory requirements for spouts and bolts in a Storm topology using the Resource Aware Scheduler API.

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.setMemoryLoad(1024.0, 512.0);
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
            .shuffleGrouping("word").setMemoryLoad(512.0);

----------------------------------------

TITLE: Implementing a Bolt in Python
DESCRIPTION: Python implementation of a bolt that splits sentences into words.

LANGUAGE: python
CODE:
import storm

class SplitSentenceBolt(storm.BasicBolt):
    def process(self, tup):
        words = tup.values[0].split(" ")
        for word in words:
          storm.emit([word])

SplitSentenceBolt().run()

----------------------------------------

TITLE: Configuring Murmur3StreamGrouping for CassandraWriterBolt in Java
DESCRIPTION: Demonstrates how to use Murmur3StreamGrouping to optimize Cassandra writes by partitioning the stream based on row partition keys.

LANGUAGE: java
CODE:
CassandraWriterBolt bolt = new CassandraWriterBolt(
    insertInto("album")
        .values(
            with(fields("title", "year", "performer", "genre", "tracks")
            ).build()));
builder.setBolt("BOLT_WRITER", bolt, 4)
        .customGrouping("spout", new Murmur3StreamGrouping("title"))

----------------------------------------

TITLE: Defining Thrift Structures for Cluster and Nimbus Summary in Storm
DESCRIPTION: Thrift structures defining the ClusterSummary and NimbusSummary, which are used to represent cluster state including information about Nimbus servers. These structures are used in the getClusterInfo API.

LANGUAGE: thrift
CODE:
struct ClusterSummary {
  1: required list<SupervisorSummary> supervisors;
  3: required list<TopologySummary> topologies;
  4: required list<NimbusSummary> nimbuses;
}

struct NimbusSummary {
  1: required string host;
  2: required i32 port;
  3: required i32 uptime_secs;
  4: required bool isLeader;
  5: required string version;
}

----------------------------------------

TITLE: Displaying jQuery UI v1.13.2 License Text
DESCRIPTION: This snippet contains the full text of the license for jQuery UI v1.13.2. It outlines the permissions, conditions, and limitations for using, modifying, and distributing the software. It also includes specific clauses for sample code and external libraries.

LANGUAGE: plaintext
CODE:
Copyright jQuery Foundation and other contributors, https://jquery.org/

This software consists of voluntary contributions made by many
individuals. For exact contribution history, see the revision history
available at https://github.com/jquery/jquery-ui

The following license applies to all parts of this software except as
documented below:

====

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

====

Copyright and related rights for sample code are waived via CC0. Sample
code is defined as all source code contained within the demos directory.

CC0: http://creativecommons.org/publicdomain/zero/1.0/

====

All files located in the node_modules and external directories are
externally maintained libraries used by this software which have their
own licenses; we recommend you read them, as their terms may differ from
the terms above.

----------------------------------------

TITLE: Making DRPC REST API Requests
DESCRIPTION: Examples of making DRPC function calls via REST API using POST and GET methods.

LANGUAGE: no-highlight
CODE:
curl 'http://ui-daemon-host-name:8080/drpc/exclaim/hello'

----------------------------------------

TITLE: Registering User-Defined Metrics in Apache Storm
DESCRIPTION: Examples of registering various types of metrics (Timer, Histogram, Meter, Counter, Gauge) using the TopologyContext in Apache Storm.

LANGUAGE: java
CODE:
public Timer registerTimer(String name)

public Histogram registerHistogram(String name)

public Meter registerMeter(String name)

public Counter registerCounter(String name)

public Gauge registerGauge(String name, Gauge gauge)

----------------------------------------

TITLE: Displaying jQuery UI v1.13.2 License Text
DESCRIPTION: This snippet contains the full text of the license for jQuery UI v1.13.2. It outlines the permissions, conditions, and limitations for using, modifying, and distributing the software. It also includes specific clauses for sample code and external libraries.

LANGUAGE: plaintext
CODE:
Copyright jQuery Foundation and other contributors, https://jquery.org/

This software consists of voluntary contributions made by many
individuals. For exact contribution history, see the revision history
available at https://github.com/jquery/jquery-ui

The following license applies to all parts of this software except as
documented below:

====

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

====

Copyright and related rights for sample code are waived via CC0. Sample
code is defined as all source code contained within the demos directory.

CC0: http://creativecommons.org/publicdomain/zero/1.0/

====

All files located in the node_modules and external directories are
externally maintained libraries used by this software which have their
own licenses; we recommend you read them, as their terms may differ from
the terms above.

----------------------------------------

TITLE: Storm Docker Configuration Example
DESCRIPTION: This configuration example shows the necessary settings for enabling Docker support in Storm on RHEL7. It includes plugin settings, allowed Docker images, cgroup configurations, and worker-launcher path.

LANGUAGE: bash
CODE:
storm.resource.isolation.plugin.enable: true
storm.resource.isolation.plugin: "org.apache.storm.container.docker.DockerManager"
storm.oci.allowed.images: ["xxx.xxx.com:8080/storm/storm/rhel7:latest"]
storm.oci.image: "xxx.xxx.com:8080/storm/storm/rhel7:latest"
storm.oci.cgroup.root: "/storm"
storm.oci.cgroup.parent: "/sys/fs/cgroup"
storm.oci.readonly.bindmounts:
    - "/etc/storm"
storm.oci.nscd.dir: "/var/run/nscd"
supervisor.worker.launcher: "/usr/share/apache-storm-2.3.0/bin/worker-launcher"

----------------------------------------

TITLE: Configuring Digest Authentication for Pacemaker
DESCRIPTION: JAAS configuration for digest authentication in Pacemaker, specifying username and password.

LANGUAGE: java
CODE:
PacemakerDigest {
    username="some username"
    password="some password";
};

----------------------------------------

TITLE: Creating Kerberos Principals and Keytabs
DESCRIPTION: Commands for setting up Kerberos principals and keytabs for Storm components

LANGUAGE: bash
CODE:
# Zookeeper (Will need one of these for each box in the Zk ensemble)
sudo kadmin.local -q 'addprinc zookeeper/zk1.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/zk.keytab  zookeeper/zk1.example.com@STORM.EXAMPLE.COM"
# Nimbus and DRPC
sudo kadmin.local -q 'addprinc storm/storm.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm/storm.example.com@STORM.EXAMPLE.COM"
# All UI logviewer and Supervisors
sudo kadmin.local -q 'addprinc storm@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm@STORM.EXAMPLE.COM"

----------------------------------------

TITLE: Configuring Nimbus Seeds in Storm YAML
DESCRIPTION: This snippet shows how to specify the Nimbus seeds, which are the candidate master machines for worker nodes to download topology jars and configurations.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["111.222.333.44"]

----------------------------------------

TITLE: Implementing a Custom RecordTranslator for KafkaSpout
DESCRIPTION: Explains how to create a custom RecordTranslator to control how Kafka ConsumerRecords are converted to Storm tuples and emitted to specific streams. It shows the usage of KafkaTuple for stream routing.

LANGUAGE: java
CODE:
return new KafkaTuple(1, 2, 3, 4).routedTo("bar");

----------------------------------------

TITLE: Refreshing Worker Connections in Apache Storm (Clojure)
DESCRIPTION: This code snippet shows how a worker refreshes connections to other workers and maintains a mapping from task to worker. It's called periodically or when the ZooKeeper assignment changes.

LANGUAGE: Clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L123

----------------------------------------

TITLE: Configuring Automatic Task Hooks in Storm Configuration
DESCRIPTION: Shows how to configure automatic task hooks using the Storm configuration. These hooks are registered automatically in every spout or bolt, useful for integrating with custom monitoring systems.

LANGUAGE: java
CODE:
"topology.auto.task.hooks"

----------------------------------------

TITLE: Generating Javadoc for Apache Storm Release
DESCRIPTION: Maven command to generate Javadoc for a new Apache Storm release. This command aggregates Javadoc from all modules except storm-shaded-deps.

LANGUAGE: shell
CODE:
mvn javadoc:aggregate -DreportOutputDirectory=./docs/ -DdestDir=javadocs -Dnotimestamp=true -pl '!storm-shaded-deps'

----------------------------------------

TITLE: Distributed Mode Message Routing in Apache Storm Worker (Clojure)
DESCRIPTION: In distributed mode, each worker listens on a single TCP port for incoming messages and routes them in-memory to tasks.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L204

----------------------------------------

TITLE: Filtering Kafka Stream with Storm SQL
DESCRIPTION: SQL statements to filter a Kafka stream of order transactions, selecting large orders and inserting them into another Kafka stream. This example creates tables for input and output streams and defines a SELECT statement for filtering.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://localhost:2181/brokers?topic=orders'
CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) LOCATION 'kafka://localhost:2181/brokers?topic=large_orders' TBLPROPERTIES '{"producer":{"bootstrap.servers":"localhost:9092","acks":"1","key.serializer":"org.apache.org.apache.storm.kafka.IntSerializer","value.serializer":"org.apache.org.apache.storm.kafka.ByteBufferSerializer"}}'
INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Configuring Azure Event Hubs for Storm Topology
DESCRIPTION: Sample configuration properties for connecting the Storm spout to Azure Event Hubs. This includes authentication details, namespace, entity path, and other settings.

LANGUAGE: properties
CODE:
eventhubspout.username = [username: policy name in EventHubs Portal]
eventhubspout.password = [password: shared access key in EventHubs Portal]
eventhubspout.namespace = [namespace]
eventhubspout.entitypath = [entitypath]
eventhubspout.partitions.count = [partitioncount]

# if not provided, will use storm's zookeeper settings
# zookeeper.connectionstring=zookeeper0:2181,zookeeper1:2181,zookeeper2:2181

eventhubspout.checkpoint.interval = 10
eventhub.receiver.credits = 1024

----------------------------------------

TITLE: CPU Metric Data Structure
DESCRIPTION: JSON structure showing the format of CPU metrics reported by CGroupCPU. Reports user and system CPU usage in milliseconds within the CGroup.

LANGUAGE: json
CODE:
{
   "user-ms": number
   "sys-ms": number
}

----------------------------------------

TITLE: Event Hubs Configuration Properties
DESCRIPTION: Configuration properties required for connecting Storm spout to Azure Event Hubs, including authentication, namespace settings, and performance tuning parameters

LANGUAGE: properties
CODE:
eventhubspout.username = [username: policy name in EventHubs Portal]
eventhubspout.password = [password: shared access key in EventHubs Portal]
eventhubspout.namespace = [namespace]
eventhubspout.entitypath = [entitypath]
eventhubspout.partitions.count = [partitioncount]

# if not provided, will use storm's zookeeper settings
# zookeeper.connectionstring=zookeeper0:2181,zookeeper1:2181,zookeeper2:2181

eventhubspout.checkpoint.interval = 10
eventhub.receiver.credits = 1024

----------------------------------------

TITLE: Configuring DRPC Servers in YAML
DESCRIPTION: Shows how to configure DRPC server locations and Thrift transport in the storm.yaml configuration file.

LANGUAGE: yaml
CODE:
drpc.servers:
  - "drpc1.foo.com"
  - "drpc2.foo.com"

storm.thrift.transport: "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin"

----------------------------------------

TITLE: Storm SQL External Table Creation
DESCRIPTION: SQL syntax for creating external tables in Storm SQL, following Hive DDL patterns

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE table_name field_list
    [ STORED AS
      INPUTFORMAT input_format_classname
      OUTPUTFORMAT output_format_classname
    ]
    LOCATION location
    [ PARALLELISM parallelism ]
    [ TBLPROPERTIES tbl_properties ]
    [ AS select_stmt ]

----------------------------------------

TITLE: Starting Apache ActiveMQ for Storm JMS Example
DESCRIPTION: Command to start Apache ActiveMQ, which is used as the JMS provider for the example topology. This assumes ActiveMQ is installed and ACTIVEMQ_HOME is set.

LANGUAGE: bash
CODE:
$ [ACTIVEMQ_HOME]/bin/activemq

----------------------------------------

TITLE: Registering Metrics Consumer in YAML
DESCRIPTION: Example of how to register metrics consumers in Storm's YAML configuration file. Shows configuration for both LoggingMetricsConsumer and HttpForwardingMetricsConsumer with parallelism hints.

LANGUAGE: yaml
CODE:
topology.metrics.consumer.register:
  - class: "org.apache.storm.metric.LoggingMetricsConsumer"
    parallelism.hint: 1
  - class: "org.apache.storm.metric.HttpForwardingMetricsConsumer"
    parallelism.hint: 1
    argument: "http://example.com:8080/metrics/my-topology/"

----------------------------------------

TITLE: Creating a Blob via Command Line
DESCRIPTION: Example of creating a blob using the Storm command line interface, specifying a file, access control, and replication factor.

LANGUAGE: shell
CODE:
storm blobstore create --file README.txt --acl o::rwa --replication-factor 4 key1

----------------------------------------

TITLE: Executing Storm SQL Command
DESCRIPTION: Basic command to compile SQL statements into Storm topology and submit to cluster.

LANGUAGE: bash
CODE:
$ bin/storm sql <sql-file> <topo-name>

----------------------------------------

TITLE: Simple DRPC Exclamation Topology Implementation
DESCRIPTION: Example of a basic DRPC topology that appends an exclamation mark to input strings using LinearDRPCTopologyBuilder.

LANGUAGE: java
CODE:
public static class ExclaimBolt extends BaseBasicBolt {
    public void execute(Tuple tuple, BasicOutputCollector collector) {
        String input = tuple.getString(1);
        collector.emit(new Values(tuple.getValue(0), input + "!"));
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "result"));
    }
}

public static void main(String[] args) throws Exception {
    LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("exclamation");
    builder.addBolt(new ExclaimBolt(), 3);
    // ...
}

----------------------------------------

TITLE: Configuring DRPC Servers
DESCRIPTION: Configuration for specifying DRPC server locations

LANGUAGE: yaml
CODE:
drpc.servers: ["111.222.333.44"]

----------------------------------------

TITLE: Configuring RecordHiveMapper
DESCRIPTION: Examples of configuring DelimitedRecordHiveMapper with column fields and partition options.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
    or
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withTimeAsPartitionField("YYYY/MM/DD");

----------------------------------------

TITLE: Applying a FlatMap Function to a Trident Stream
DESCRIPTION: Example of applying a custom FlatMapFunction to a Trident stream.

LANGUAGE: java
CODE:
mystream.flatMap(new Split())

----------------------------------------

TITLE: Task Message Listening in Apache Storm
DESCRIPTION: Tasks listen on an in-memory ZeroMQ port for messages from the virtual port.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L201

----------------------------------------

TITLE: DRPC Server Configuration
DESCRIPTION: YAML configuration for setting up DRPC servers with transport settings.

LANGUAGE: yaml
CODE:
drpc.servers:
  - "drpc1.foo.com"
  - "drpc2.foo.com"
drpc.http.port: 8081
storm.thrift.transport: "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin"

----------------------------------------

TITLE: Registering Metrics Consumer in Java
DESCRIPTION: Example showing how to register a metrics consumer to a topology through Java configuration.

LANGUAGE: java
CODE:
conf.registerMetricsConsumer(org.apache.storm.metric.LoggingMetricsConsumer.class, 1);

----------------------------------------

TITLE: Implementing a Bolt in Java
DESCRIPTION: Shows the implementation of a simple bolt that appends '!!!' to input tuples and emits the result.

LANGUAGE: java
CODE:
public class DoubleAndTripleBolt extends BaseRichBolt {
    private OutputCollectorBase _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollectorBase collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple input) {
        int val = input.getInteger(0);        
        _collector.emit(input, new Values(val*2, val*3));
        _collector.ack(input);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("double", "triple"));
    }    
}

----------------------------------------

TITLE: Implementing MongoDB Update Mapper
DESCRIPTION: Implementation for mapping Storm tuples to MongoDB update operations

LANGUAGE: java
CODE:
public class SimpleMongoUpdateMapper extends SimpleMongoMapper implements MongoUpdateMapper {

    private String[] fields;

    @Override
    public Document toDocument(ITuple tuple) {
        Document document = new Document();
        for(String field : fields){
            document.append(field, tuple.getValueByField(field));
        }
        return new Document("$set", document);
    }

    public SimpleMongoUpdateMapper withFields(String... fields) {
        this.fields = fields;
        return this;
    }
}

----------------------------------------

TITLE: Configuring Nimbus Seed Nodes
DESCRIPTION: Specifies the master nodes (nimbus seeds) that worker nodes connect to for downloading topology configurations and jar files.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["111.222.333.44"]

----------------------------------------

TITLE: Configuring Pacemaker State Store in Storm
DESCRIPTION: Set the cluster state store to use Pacemaker in the Storm cluster configuration.

LANGUAGE: yaml
CODE:
storm.cluster.state.store: "org.apache.storm.pacemaker.pacemaker_state_factory"

----------------------------------------

TITLE: Configuring Nimbus Seed Nodes
DESCRIPTION: Specifies the master nodes (nimbus seeds) that worker nodes connect to for downloading topology configurations and jar files.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["111.222.333.44"]

----------------------------------------

TITLE: Creating User Defined Function in Storm SQL
DESCRIPTION: SQL statement to create a user defined function (UDF) in Storm SQL.

LANGUAGE: sql
CODE:
CREATE FUNCTION MYPLUS AS 'org.apache.storm.sql.TestUtils$MyPlus'

----------------------------------------

TITLE: Configuring CGroup Mount Points and Permissions
DESCRIPTION: Sample cgconfig.conf configuration file showing mount points for various cgroup subsystems and permission settings for the storm group. This establishes the basic CGroup structure needed for Storm resource management.

LANGUAGE: conf
CODE:
mount {
	cpuset	= /cgroup/cpuset;
	cpu	= /cgroup/storm_resources;
	cpuacct	= /cgroup/storm_resources;
	memory	= /cgroup/storm_resources;
	devices	= /cgroup/devices;
	freezer	= /cgroup/freezer;
	net_cls	= /cgroup/net_cls;
	blkio	= /cgroup/blkio;
}

group storm {
       perm {
               task {
                      uid = 500;
                      gid = 500;
               }
               admin {
                      uid = 500;
                      gid = 500;
               }
       }
       cpu {
       }
       memory {
       }
       cpuacct {
       }
}

----------------------------------------

TITLE: Implementing Distributed Query for Word Counts in Trident
DESCRIPTION: Demonstrates how to set up a distributed query stream to get the sum of counts for a list of words in a Trident topology.

LANGUAGE: java
CODE:
topology.newDRPCStream("words")
       .each(new Fields("args"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .stateQuery(wordCounts, new Fields("word"), new MapGet(), new Fields("count"))
       .each(new Fields("count"), new FilterNull())
       .aggregate(new Fields("count"), new Sum(), new Fields("sum"));

----------------------------------------

TITLE: Configuring Storm OpenTSDB Core Bolt
DESCRIPTION: Demonstrates how to configure and use the OpenTsdbBolt for writing data to OpenTSDB. The example shows setting up the client builder, creating the bolt with a default mapper, and configuring batch size and flush interval.

LANGUAGE: java
CODE:
OpenTsdbClient.Builder builder =  OpenTsdbClient.newBuilder(openTsdbUrl).sync(30_000).returnDetails();
final OpenTsdbBolt openTsdbBolt = new OpenTsdbBolt(builder, Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
openTsdbBolt.withBatchSize(10).withFlushInterval(2000);
topologyBuilder.setBolt("opentsdb", openTsdbBolt).shuffleGrouping("metric-gen");

----------------------------------------

TITLE: Kerberos Server Configuration
DESCRIPTION: JAAS configuration for Pacemaker Kerberos authentication on Pacemaker nodes.

LANGUAGE: java
CODE:
PacemakerServer {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/etc/keytabs/pacemaker.keytab"
   storeKey=true
   useTicketCache=false
   principal="pacemaker@MY.COMPANY.COM";
};

----------------------------------------

TITLE: Storm Topology Rebalancing via CLI
DESCRIPTION: Command line example showing how to rebalance a running Storm topology by modifying the number of worker processes and executors for specific components.

LANGUAGE: bash
CODE:
$ storm rebalance mytopology -n 5 -e blue-spout=3 -e yellow-bolt=10

----------------------------------------

TITLE: Storm Tuple Serialization
DESCRIPTION: Thread-safe Kryo-based tuple serialization implementation

LANGUAGE: java
CODE:
KryoTupleSerializer

----------------------------------------

TITLE: Secure HDFS Configuration for OCI/Squashfs Runtime
DESCRIPTION: Additional configuration settings for accessing secure HDFS with OCI/Squashfs runtime.

LANGUAGE: bash
CODE:
storm.hdfs.login.keytab: /etc/keytab
storm.hdfs.login.principal: primary/instance@REALM

----------------------------------------

TITLE: Implementing JDBC Connection Provider Interface in Java
DESCRIPTION: Interface definition for database connection providers that handle connection pooling. Includes methods for preparation, connection retrieval, and cleanup.

LANGUAGE: java
CODE:
public interface ConnectionProvider extends Serializable {
    /**
     * method must be idempotent.
     */
    void prepare();

    /**
     *
     * @return a DB connection over which the queries can be executed.
     */
    Connection getConnection();

    /**
     * called once when the system is shutting down, should be idempotent.
     */
    void cleanup();
}

----------------------------------------

TITLE: Sample Druid Beam Factory Implementation
DESCRIPTION: Implementation example of DruidBeamFactory showing configuration of Druid connection, data schema, and ingestion settings using Tranquility's DruidBeams builder.

LANGUAGE: java
CODE:
public class SampleDruidBeamFactoryImpl implements DruidBeamFactory<Map<String, Object>> {

    @Override
    public Beam<Map<String, Object>> makeBeam(Map<?, ?> conf, IMetricsContext metrics) {


        final String indexService = "druid/overlord"; // The druid.service name of the indexing service Overlord node.
        final String discoveryPath = "/druid/discovery"; // Curator service discovery path. config: druid.discovery.curator.path
        final String dataSource = "test"; //The name of the ingested datasource. Datasources can be thought of as tables.
        final List<String> dimensions = ImmutableList.of("publisher", "advertiser");
        List<AggregatorFactory> aggregators = ImmutableList.<AggregatorFactory>of(
                new CountAggregatorFactory(
                        "click"
                )
        );
        // Tranquility needs to be able to extract timestamps from your object type (in this case, Map<String, Object>).
        final Timestamper<Map<String, Object>> timestamper = new Timestamper<Map<String, Object>>()
        {
            @Override
            public DateTime timestamp(Map<String, Object> theMap)
            {
                return new DateTime(theMap.get("timestamp"));
            }
        };

        // Tranquility uses ZooKeeper (through Curator) for coordination.
        final CuratorFramework curator = CuratorFrameworkFactory
                .builder()
                .connectString((String)conf.get("druid.tranquility.zk.connect")) //take config from storm conf
                .retryPolicy(new ExponentialBackoffRetry(1000, 20, 30000))
                .build();
        curator.start();

        // The JSON serialization of your object must have a timestamp field in a format that Druid understands. By default,
        // Druid expects the field to be called "timestamp" and to be an ISO8601 timestamp.
        final TimestampSpec timestampSpec = new TimestampSpec("timestamp", "auto", null);

        // Tranquility needs to be able to serialize your object type to JSON for transmission to Druid. By default this is
        // done with Jackson. If you want to provide an alternate serializer, you can provide your own via ```objectWriter(...)```.
        // In this case, we won't provide one, so we're just using Jackson.
        final Beam<Map<String, Object>> beam = DruidBeams
                .builder(timestamper)
                .curator(curator)
                .discoveryPath(discoveryPath)
                .location(DruidLocation.create(indexService, dataSource))
                .timestampSpec(timestampSpec)
                .rollup(DruidRollup.create(DruidDimensions.specific(dimensions), aggregators, QueryGranularities.MINUTE))
                .tuning(
                        ClusteredBeamTuning
                                .builder()
                                .segmentGranularity(Granularity.HOUR)
                                .windowPeriod(new Period("PT10M"))
                                .partitions(1)
                                .replicants(1)
                                .build()
                )
                .druidBeamConfig(
                      DruidBeamConfig
                           .builder()
                           .indexRetryPeriod(new Period("PT10M"))
                           .build())
                .buildBeam();

        return beam;
    }
}

----------------------------------------

TITLE: Creating User Defined Function in Storm SQL
DESCRIPTION: SQL statement to create a user defined function (UDF) in Storm SQL.

LANGUAGE: sql
CODE:
CREATE FUNCTION MYPLUS AS 'org.apache.storm.sql.TestUtils$MyPlus'

----------------------------------------

TITLE: Task Message Listening in Apache Storm (Clojure)
DESCRIPTION: Tasks listen on an in-memory ZeroMQ port for messages from the virtual port.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L201

----------------------------------------

TITLE: Parameterized Storm Bolt in Clojure
DESCRIPTION: Example of a parameterized bolt that appends a suffix to input strings, showing how to create configurable components.

LANGUAGE: clojure
CODE:
(defbolt suffix-appender ["word"] {:params [suffix]}
  [tuple collector]
  (emit-bolt! collector [(str (.getString tuple 0) suffix)] :anchor tuple)
  )

----------------------------------------

TITLE: Configuring Nimbus Seeds in Storm
DESCRIPTION: Configuration for specifying the Nimbus master nodes that worker nodes connect to for downloading topology files

LANGUAGE: yaml
CODE:
nimbus.seeds: ["111.222.333.44"]

----------------------------------------

TITLE: Using Max and MaxBy Operations in Trident
DESCRIPTION: Example of using max and maxBy operations on a Trident stream to find maximum values.

LANGUAGE: java
CODE:
mystream.maxBy(new Fields("count"))

----------------------------------------

TITLE: Worker-Level Metrics Configuration
DESCRIPTION: Example of configuring worker-level metrics in storm.yaml configuration file.

LANGUAGE: yaml
CODE:
worker.metrics: 
  metricA: "aaa.bbb.ccc.ddd.MetricA"
  metricB: "aaa.bbb.ccc.ddd.MetricB"

----------------------------------------

TITLE: Logging Message in JSON Format for Storm Multi-Language Protocol
DESCRIPTION: This JSON structure represents a log command sent from a multi-language component in Storm. It includes the command type and the message to be logged in the worker log.

LANGUAGE: json
CODE:
{
    "command": "log",
    "msg": "hello world!"
}

----------------------------------------

TITLE: Pinning Worker to NUMA Zone with CgroupManager in Linux
DESCRIPTION: The CgroupManager prefixes the worker launch command with the numactl command to bind the worker to a specific NUMA zone's CPU cores and memory.

LANGUAGE: bash
CODE:
numactl --cpunodebind=<numaId>> --membind=<numaId> <worker launch command>

----------------------------------------

TITLE: Configuring Storm Metricstore in YAML
DESCRIPTION: YAML configuration options for setting up the Storm Metricstore, including the class implementations, RocksDB location, and retention settings.

LANGUAGE: yaml
CODE:
storm.metricstore.class: "org.apache.storm.metricstore.rocksdb.RocksDbStore"
storm.metricprocessor.class: "org.apache.storm.metricstore.NimbusMetricProcessor"
storm.metricstore.rocksdb.location: "storm_rocks"
storm.metricstore.rocksdb.create_if_missing: true
storm.metricstore.rocksdb.metadata_string_cache_capacity: 4000
storm.metricstore.rocksdb.retention_hours: 240

----------------------------------------

TITLE: Configuring ExpandUrl Bolt with Shuffle Grouping in Storm
DESCRIPTION: This snippet demonstrates how to set up an ExpandUrl bolt with shuffle grouping in a Storm topology. It's used as a contrast to show less effective caching compared to fields grouping.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .shuffleGrouping(1);

----------------------------------------

TITLE: Configuring User Resource Pools in Storm
DESCRIPTION: YAML configuration to specify resource guarantees for users in a multi-tenant Storm cluster.

LANGUAGE: yaml
CODE:
resource.aware.scheduler.user.pools:
    [UserId]
        cpu: [Amount of Guarantee CPU Resources]
        memory: [Amount of Guarantee Memory Resources]

----------------------------------------

TITLE: Task Message Listening in Apache Storm (Clojure)
DESCRIPTION: Tasks listen on an in-memory ZeroMQ port for messages from the virtual port.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L201

----------------------------------------

TITLE: Configuring HDFS Bolt in Java
DESCRIPTION: Example of how to configure and use the HDFS Bolt in a Storm topology. It demonstrates setting up record format, sync policy, rotation policy, and file naming format.

LANGUAGE: java
CODE:
// use "|" instead of "," for field delimiter
RecordFormat format = new DelimitedRecordFormat()
        .withFieldDelimiter("|");

// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withPath("/foo/");

HdfsBolt bolt = new HdfsBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withRecordFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy);

----------------------------------------

TITLE: Local Mode Message Passing in Apache Storm (Clojure)
DESCRIPTION: This code implements the local mode message passing using in-memory Java queues, allowing for easy local usage without ZeroMQ installation.

LANGUAGE: clojure
CODE:
(https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/local.clj)

----------------------------------------

TITLE: Implementing Transfer Function in Apache Storm Worker (Clojure)
DESCRIPTION: The worker provides a transfer function used by tasks to send tuples to other tasks. It serializes the tuple and puts it onto a transfer queue.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L56

----------------------------------------

TITLE: Retrieving Nimbus Summary in JSON
DESCRIPTION: Sample response from the /api/v1/nimbus/summary endpoint, showing information about Nimbus hosts.

LANGUAGE: json
CODE:
{
  "nimbuses":[
    {
      "host":"192.168.202.1",
      "port":6627,
      "nimbusLogLink":"http:\/\/192.168.202.1:8000\/log?file=nimbus.log",
      "status":"Leader",
      "version":"0.10.0-SNAPSHOT",
      "nimbusUpTime":"3m 33s",
      "nimbusUpTimeSeconds":"213"
    }
  ]
}

----------------------------------------

TITLE: Defining Storm Component Object Structure in Thrift
DESCRIPTION: Thrift union definition for ComponentObject that specifies how different types of Storm components (Java, Shell, or serialized Java) can be represented.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Configuring Storm Cluster State Store
DESCRIPTION: Basic configuration to enable Pacemaker as the cluster state store in Storm.

LANGUAGE: yaml
CODE:
storm.cluster.state.store: "org.apache.storm.cluster.PaceMakerStateStorageFactory"

----------------------------------------

TITLE: Registering Metrics Consumer in Java Configuration
DESCRIPTION: Example of how to register a metrics consumer to a Storm topology using Java configuration. This adds the LoggingMetricsConsumer with a parallelism hint of 1.

LANGUAGE: java
CODE:
conf.registerMetricsConsumer(org.apache.storm.metric.LoggingMetricsConsumer.class, 1);

----------------------------------------

TITLE: Storm SQL Query for Filtering Slow Logs
DESCRIPTION: SQL script that defines tables, creates a custom time function, and filters slow logs (response time >= 100ms).

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://apache-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
CREATE EXTERNAL TABLE APACHE_SLOW_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_RECEIVED_TIMESTAMP BIGINT, TIME_ELAPSED_MS INT) LOCATION 'kafka://apache-slow-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
CREATE FUNCTION GET_TIME AS 'org.apache.storm.sql.runtime.functions.scalar.datetime.GetTime2'
INSERT INTO APACHE_SLOW_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, GET_TIME(TIME_RECEIVED_UTC_ISOFORMAT, 'yyyy-MM-dd''T''HH:mm:ssZZ') AS TIME_RECEIVED_TIMESTAMP, TIME_US / 1000 AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (TIME_US / 1000) >= 100

----------------------------------------

TITLE: Storm SQL Query for Filtering Slow Logs
DESCRIPTION: SQL script that defines tables, creates a custom time function, and filters slow logs (response time >= 100ms).

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://apache-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
CREATE EXTERNAL TABLE APACHE_SLOW_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_RECEIVED_TIMESTAMP BIGINT, TIME_ELAPSED_MS INT) LOCATION 'kafka://apache-slow-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
CREATE FUNCTION GET_TIME AS 'org.apache.storm.sql.runtime.functions.scalar.datetime.GetTime2'
INSERT INTO APACHE_SLOW_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, GET_TIME(TIME_RECEIVED_UTC_ISOFORMAT, 'yyyy-MM-dd''T''HH:mm:ssZZ') AS TIME_RECEIVED_TIMESTAMP, TIME_US / 1000 AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (TIME_US / 1000) >= 100

----------------------------------------

TITLE: Configuring Docker Support in Storm
DESCRIPTION: Example configuration settings for enabling Docker support in Apache Storm on RHEL7. It includes essential parameters for resource isolation, image selection, and security settings.

LANGUAGE: bash
CODE:
storm.resource.isolation.plugin.enable: true
storm.resource.isolation.plugin: "org.apache.storm.container.docker.DockerManager"
storm.oci.allowed.images: ["xxx.xxx.com:8080/storm/storm/rhel7:latest"]
storm.oci.image: "xxx.xxx.com:8080/storm/storm/rhel7:latest"
storm.oci.cgroup.root: "/storm"
storm.oci.cgroup.parent: "/sys/fs/cgroup"
storm.oci.readonly.bindmounts:
    - "/etc/storm"
storm.oci.nscd.dir: "/var/run/nscd"
supervisor.worker.launcher: "/usr/share/apache-storm-2.3.0/bin/worker-launcher"

----------------------------------------

TITLE: Registering Task Hook in Java Using TopologyContext
DESCRIPTION: Demonstrates how to register a custom task hook in the open method of a spout or prepare method of a bolt using the TopologyContext. This allows for event-specific custom code execution.

LANGUAGE: java
CODE:
TopologyContext#addTaskHook

----------------------------------------

TITLE: User Defined Function Example
DESCRIPTION: Java implementation of a scalar UDF for Storm SQL

LANGUAGE: java
CODE:
  public class MyPlus {
    public static Integer evaluate(Integer x, Integer y) {
      return x + y;
    }
  }

----------------------------------------

TITLE: Implementing RedisLookupMapper for Word Count
DESCRIPTION: Java class implementing RedisLookupMapper interface for looking up word counts in Redis using a hash data structure.

LANGUAGE: java
CODE:
class WordCountRedisLookupMapper implements RedisLookupMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountRedisLookupMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public List<Values> toTuple(ITuple input, Object value) {
        String member = getKeyFromTuple(input);
        List<Values> values = Lists.newArrayList();
        values.add(new Values(member, value));
        return values;
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("wordName", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Virtual Port Implementation for Message Routing in Apache Storm (Clojure)
DESCRIPTION: This code implements the virtual port mechanism for routing messages to tasks in distributed mode of Apache Storm.

LANGUAGE: clojure
CODE:
(defn virtual-port [])

----------------------------------------

TITLE: Adding Standard and Counter Columns in Java
DESCRIPTION: Demonstrates how to add standard and counter columns to a ColumnList object for HBase operations.

LANGUAGE: java
CODE:
ColumnList cols = new ColumnList();
cols.addColumn(this.columnFamily, field.getBytes(), toBytes(tuple.getValueByField(field)));

ColumnList cols = new ColumnList();
cols.addCounter(this.columnFamily, field.getBytes(), toLong(tuple.getValueByField(field)));

----------------------------------------

TITLE: Storm JoinBolt Implementation
DESCRIPTION: Java code demonstrating how to implement the equivalent SQL join using Storm's JoinBolt with window configuration and field grouping.

LANGUAGE: java
CODE:
JoinBolt jbolt =  new JoinBolt("spout1", "key1")                   // from        spout1  
                    .join     ("spout2", "userId",  "spout1")      // inner join  spout2  on spout2.userId = spout1.key1
                    .join     ("spout3", "key3",    "spout2")      // inner join  spout3  on spout3.key3   = spout2.userId   
                    .leftJoin ("spout4", "key4",    "spout3")      // left join   spout4  on spout4.key4   = spout3.key3
                    .select  ("userId, key4, key2, spout3:key3")   // chose output fields
                    .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;

topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("spout1", new Fields("key1") )
            .fieldsGrouping("spout2", new Fields("userId") )
            .fieldsGrouping("spout3", new Fields("key3") )
            .fieldsGrouping("spout4", new Fields("key4") );

----------------------------------------

TITLE: Configuring Storm Metric Reporters
DESCRIPTION: YAML configuration example showing how to set up Graphite and Console metric reporters in storm.yaml. Includes settings for reporting period, daemon selection, and metric filtering.

LANGUAGE: yaml
CODE:
storm.metrics.reporters:
  # Graphite Reporter
  - class: "org.apache.storm.metrics2.reporters.GraphiteStormReporter"
    daemons:
        - "supervisor"
        - "nimbus"
        - "worker"
    report.period: 60
    report.period.units: "SECONDS"
    graphite.host: "localhost"
    graphite.port: 2003

  # Console Reporter
  - class: "org.apache.storm.metrics2.reporters.ConsoleStormReporter"
    daemons:
        - "worker"
    report.period: 10
    report.period.units: "SECONDS"
    filter:
        class: "org.apache.storm.metrics2.filters.RegexFilter"
        expression: ".*my_component.*emitted.*"

----------------------------------------

TITLE: Converting Docker Images to Squashfs for HDFS
DESCRIPTION: Command to pull Docker images, convert layers to squashfs, and push to HDFS using the docker-to-squash.py script.

LANGUAGE: bash
CODE:
python3 docker-to-squash.py pull-build-push-update --hdfs-root hdfs://hostname:port/containers \
                      docker.xxx.com:4443/hadoop-user-images/storm/rhel7:20201202-232133,storm/rhel7:dev_current --log DEBUG --bootstrap

----------------------------------------

TITLE: Configuring CassandraWriterBolt with Named Setters and Aliases in Java
DESCRIPTION: Creates a CassandraWriterBolt using a static bound query with named setters and aliases for field mapping.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
     async(
        boundQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (:ti, :ye, :pe, :ge, :tr);")
            .bind(
                field("ti"),as("title"),
                field("ye").as("year")),
                field("pe").as("performer")),
                field("ge").as("genre")),
                field("tr").as("tracks"))
            ).byNamedSetters()
     )
);

----------------------------------------

TITLE: Configuring Maven for Flux-Enabled Topology JAR
DESCRIPTION: Maven configuration to create a Flux-enabled topology JAR using the Maven shade plugin.

LANGUAGE: xml
CODE:
<dependencies>
    <!-- Flux include -->
    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>flux-core</artifactId>
        <version>${storm.version}</version>
    </dependency>
    <!-- Flux Wrappers include -->
    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>flux-wrappers</artifactId>
        <version>${storm.version}</version>
    </dependency>

    <!-- add user dependencies here... -->

</dependencies>
<!-- create a fat jar that includes all dependencies -->
<build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-shade-plugin</artifactId>
            <version>1.4</version>
            <configuration>
                <createDependencyReducedPom>true</createDependencyReducedPom>
            </configuration>
            <executions>
                <execution>
                    <phase>package</phase>
                    <goals>
                        <goal>shade</goal>
                    </goals>
                    <configuration>
                        <transformers>
                            <transformer
                                    implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                            <transformer
                                    implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                <mainClass>org.apache.storm.flux.Flux</mainClass>
                            </transformer>
                        </transformers>
                    </configuration>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>

----------------------------------------

TITLE: Configuring JDBC Insert Bolt with HikariCP
DESCRIPTION: Example of configuring and instantiating a JdbcInsertBolt using HikariCP connection provider and SimpleJdbcMapper.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);

String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withTableName("user")
                                    .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Enabling Kerberos Authentication in Storm YAML Config
DESCRIPTION: YAML configuration settings to enable Kerberos authentication in Storm.

LANGUAGE: yaml
CODE:
storm.thrift.transport: "org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin"
java.security.auth.login.config: "/path/to/jaas.conf"

----------------------------------------

TITLE: Configuring Kerberos Authentication for Pacemaker Client
DESCRIPTION: JAAS configuration for Kerberos authentication on Nimbus nodes to connect to Pacemaker.

LANGUAGE: java
CODE:
PacemakerClient {
    com.sun.security.auth.module.Krb5LoginModule required
    useKeyTab=true
    keyTab="/etc/keytabs/nimbus.keytab"
    storeKey=true
    useTicketCache=false
    serviceName="pacemaker"
    principal="nimbus@MY.COMPANY.COM";
};

----------------------------------------

TITLE: Using Peek Operation in Trident
DESCRIPTION: Example of using peek operation for debugging in a Trident stream processing pipeline.

LANGUAGE: java
CODE:
mystream.flatMap(new Split()).map(new UpperCase())
         .peek(new Consumer() {
                @Override
                public void accept(TridentTuple input) {
                  System.out.println(input.getString(0));
                }
         })
         .groupBy(new Fields("word"))
         .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))

----------------------------------------

TITLE: ZeroMQ Implementation for Distributed Mode in Apache Storm (Clojure)
DESCRIPTION: Implementation of the message sending protocol using ZeroMQ for distributed mode in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/zmq.clj

----------------------------------------

TITLE: Implementing RecordToTupleMapper Interface in Java
DESCRIPTION: This code snippet shows the structure of the RecordToTupleMapper interface, which is used to convert Kinesis records to Storm tuples. It includes two methods: getOutputFields for defining tuple fields and getTuple for converting a record to a tuple.

LANGUAGE: java
CODE:
    Fields getOutputFields ();
    List<Object> getTuple (Record record);

----------------------------------------

TITLE: Configuring KafkaBolt in Storm Topology
DESCRIPTION: This code example demonstrates how to configure and use KafkaBolt to write data to Kafka as part of a Storm topology.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();

Fields fields = new Fields("key", "message");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
            new Values("storm", "1"),
            new Values("trident", "1"),
            new Values("needs", "1"),
            new Values("javadoc", "1")
);
spout.setCycle(true);
builder.setSpout("spout", spout, 5);
//set producer properties.
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

KafkaBolt bolt = new KafkaBolt()
        .withProducerProperties(props)
        .withTopicSelector(new DefaultTopicSelector("test"))
        .withTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper());
builder.setBolt("forwardToKafka", bolt, 8).shuffleGrouping("spout");

Config conf = new Config();

StormSubmitter.submitTopology("kafkaboltTest", conf, builder.createTopology());

----------------------------------------

TITLE: Creating a Blob in Java
DESCRIPTION: Java code snippet demonstrating how to create a blob in Storm's distributed cache using the ClientBlobStore API. This example creates a blob with the key 'some_key' and writes data to it.

LANGUAGE: java
CODE:
AtomicOutputStream blobStream = clientBlobStore.createBlob("some_key", settableBlobMeta);
blobStream.write("Some String or input data".getBytes());
blobStream.close();

----------------------------------------

TITLE: Defining JDBC Mapper Interface
DESCRIPTION: Interface for mapping Storm tuples to database columns for JDBC operations.

LANGUAGE: java
CODE:
public interface JdbcMapper  extends Serializable {
    List<Column> getColumns(ITuple tuple);
}

----------------------------------------

TITLE: Implementing Kafka Tuple Mapping Interfaces
DESCRIPTION: Interface definitions for mapping Kafka tuples in Storm, including methods for getting keys and messages from tuples.

LANGUAGE: java
CODE:
K getKeyFromTuple(Tuple/TridentTuple tuple);
V getMessageFromTuple(Tuple/TridentTuple tuple);

----------------------------------------

TITLE: Initializing ClientBlobStore in Java
DESCRIPTION: Java code snippet showing how to initialize a ClientBlobStore object for interacting with the Storm blobstore.

LANGUAGE: java
CODE:
Config theconf = new Config();
theconf.putAll(Utils.readStormConfig());
ClientBlobStore clientBlobStore = Utils.getClientBlobStore(theconf);

----------------------------------------

TITLE: OCI Runtime Configuration Example
DESCRIPTION: Example oci-config.json file showing the configuration for launching a container with OCI/Squashfs runtime

LANGUAGE: json
CODE:
{"version":"0.1","username":"username1","containerId":"6703-1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb","pidFile":"/home/y/var/storm/workers/1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb/artifacts/container-1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb.pid",...}

----------------------------------------

TITLE: OCI Runtime Configuration Example
DESCRIPTION: Example oci-config.json file showing the configuration for launching a container with OCI/Squashfs runtime

LANGUAGE: json
CODE:
{"version":"0.1","username":"username1","containerId":"6703-1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb","pidFile":"/home/y/var/storm/workers/1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb/artifacts/container-1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb.pid",...}

----------------------------------------

TITLE: Implementing One Aggregator for Unique Counting
DESCRIPTION: Combiner aggregator that implements a simple counting mechanism, used for computing unique counts in distributed operations.

LANGUAGE: java
CODE:
public class One implements CombinerAggregator<Integer> {
   public Integer init(TridentTuple tuple) {
       return 1;
   }

   public Integer combine(Integer val1, Integer val2) {
       return 1;
   }

   public Integer zero() {
       return 1;
   }        
}

----------------------------------------

TITLE: Configuring OpenTSDB Core Bolt in Storm
DESCRIPTION: Demonstrates how to set up and configure the OpenTsdbBolt for writing data points to OpenTSDB. Shows configuration of batch size, flush interval, and tuple mapping using the default mapper.

LANGUAGE: java
CODE:
        OpenTsdbClient.Builder builder =  OpenTsdbClient.newBuilder(openTsdbUrl).sync(30_000).returnDetails();
        final OpenTsdbBolt openTsdbBolt = new OpenTsdbBolt(builder, Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
        openTsdbBolt.withBatchSize(10).withFlushInterval(2000);
        topologyBuilder.setBolt("opentsdb", openTsdbBolt).shuffleGrouping("metric-gen");

----------------------------------------

TITLE: Configuring Kerberos Authentication for Pacemaker Server in JAAS
DESCRIPTION: JAAS configuration for Kerberos authentication on Pacemaker nodes.

LANGUAGE: java
CODE:
PacemakerServer {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/etc/keytabs/pacemaker.keytab"
   storeKey=true
   useTicketCache=false
   principal="pacemaker@MY.COMPANY.COM";
};

----------------------------------------

TITLE: Storm Ack Command JSON Format
DESCRIPTION: JSON structure for acknowledging tuple processing through STDOUT. Contains command type and tuple ID.

LANGUAGE: json
CODE:
{
    "command": "ack",
    "id": 123123
}

----------------------------------------

TITLE: Defining submitTopology Method in Storm's Thrift API
DESCRIPTION: This Java method signature defines the submitTopology function in Storm's Thrift API. It takes the topology name, uploaded jar location, JSON configuration, and the StormTopology object as parameters. It can throw AlreadyAliveException and InvalidTopologyException.

LANGUAGE: java
CODE:
void submitTopology(
        1: string name,
        2: string uploadedJarLocation,
        3: string jsonConf,
        4: StormTopology topology)
        throws (
                1: AlreadyAliveException e,
                2: InvalidTopologyException ite);

----------------------------------------

TITLE: Configuring Spring JMS ApplicationContext for Storm
DESCRIPTION: Spring XML configuration that sets up JMS connection factory and message destinations (queue and topic) for Storm integration. Includes ActiveMQ namespace configuration, defines a notification queue and topic, and establishes a connection factory pointing to localhost:61616.

LANGUAGE: xml
CODE:
<?xml version="1.0" encoding="UTF-8"?>
<beans 
  xmlns="http://www.springframework.org/schema/beans" 
  xmlns:amq="http://activemq.apache.org/schema/core"
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.0.xsd
  http://activemq.apache.org/schema/core http://activemq.apache.org/schema/core/activemq-core.xsd">

    <amq:queue id="notificationQueue" physicalName="backtype.storm.contrib.example.queue" />
    
    <amq:topic id="notificationTopic" physicalName="backtype.storm.contrib.example.topic" />

    <amq:connectionFactory id="jmsConnectionFactory"
        brokerURL="tcp://localhost:61616" />
    
</beans>

----------------------------------------

TITLE: Filtering Error Logs with Storm SQL
DESCRIPTION: This Storm SQL script defines external tables for Apache logs and error logs, then filters and transforms error logs (status >= 400) into a separate Kafka topic.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://apache-logs?bootstrap-servers=localhost:9092'
CREATE EXTERNAL TABLE APACHE_ERROR_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_ELAPSED_MS INT) LOCATION 'kafka://apache-error-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO APACHE_ERROR_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, (TIME_US / 1000) AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (CAST(STATUS AS INT) / 100) >= 4

----------------------------------------

TITLE: Creating StateFactory for Custom State in Trident
DESCRIPTION: This snippet demonstrates how to create a StateFactory for a custom State implementation. The factory is responsible for creating instances of the State object within Trident tasks.

LANGUAGE: java
CODE:
public class LocationDBFactory implements StateFactory {
   public State makeState(Map conf, int partitionIndex, int numPartitions) {
      return new LocationDB();
   } 
}

----------------------------------------

TITLE: Retrieving Topology Metrics in Storm UI REST API
DESCRIPTION: Example JSON response for the /api/v1/topology/:id/metrics endpoint, which returns detailed metrics for a specific topology, including component-level statistics.

LANGUAGE: json
CODE:
{
    "window":":all-time",
    "window-hint":"All time",
    "spouts":[
        {
            "id":"spout",
            "emitted":[
                {
                    "stream_id":"__metrics",
                    "value":20
                },
                {
                    "stream_id":"default",
                    "value":17350280
                }
            ],
            "transferred":[
                {
                    "stream_id":"__metrics",
                    "value":20
                },
                {
                    "stream_id":"default",
                    "value":17350280
                }
            ],
            "acked":[
                {
                    "stream_id":"default",
                    "value":17339180
                }
            ],
            "failed":[

            ],
            "complete_ms_avg":[
                {
                    "stream_id":"default",
                    "value":"920.497"
                }
            ]
        }
    ],
    "bolts":[
        {
            "id":"count",
            "emitted":[
                {
                    "stream_id":"__metrics",
                    "value":120
                },
                {
                    "stream_id":"default",
                    "value":190748180
                }
            ],
            "transferred":[
                {
                    "stream_id":"__metrics",
                    "value":120
                },
                {
                    "stream_id":"default",
                    "value":0
                }
            ],
            "acked":[
                {
                    "component_id":"split",
                    "stream_id":"default",
                    "value":190733160
                }
            ],
            "failed":[

            ],
            "process_ms_avg":[
                {
                    "component_id":"split",
                    "stream_id":"default",
                    "value":"0.004"
                }
            ],
            "executed":[
                {
                    "component_id":"split",
                    "stream_id":"default",
                    "value":190733140
                }
            ],
            "executed_ms_avg":[
                {
                    "component_id":"split",
                    "stream_id":"default",
                    "value":"0.005"
                }
            ]
        }
    ]
}

----------------------------------------

TITLE: Implementing a Scalar User-Defined Function in Java
DESCRIPTION: Example Java class implementing a scalar user-defined function for Storm SQL.

LANGUAGE: java
CODE:
public class MyPlus {
  public static Integer evaluate(Integer x, Integer y) {
    return x + y;
  }
}

----------------------------------------

TITLE: Implementing a FlatMap Function in Trident
DESCRIPTION: Example of creating a flatMap function that splits sentences into words.

LANGUAGE: java
CODE:
public class Split extends FlatMapFunction {
  @Override
  public Iterable<Values> execute(TridentTuple input) {
    List<Values> valuesList = new ArrayList<>();
    for (String word : input.getString(0).split(" ")) {
      valuesList.add(new Values(word));
    }
    return valuesList;
  }
}

----------------------------------------

TITLE: Setting ROOT Logger to DEBUG for 30 Seconds via Storm CLI
DESCRIPTION: Shows an example of setting the ROOT logger to DEBUG level for a duration of 30 seconds using the Storm CLI. This command affects the specified topology immediately.

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -l ROOT=DEBUG:30

----------------------------------------

TITLE: Running Sample Storm Topology - Bash
DESCRIPTION: Command to run a sample topology using Flux in local mode cluster

LANGUAGE: bash
CODE:
storm jar ./examples/target/storm-mqtt-examples-*-SNAPSHOT.jar org.apache.storm.flux.Flux ./examples/src/main/flux/sample.yaml --local

----------------------------------------

TITLE: Storm Topology with KestrelSpout
DESCRIPTION: Storm topology configuration that reads from a Kestrel queue, splits sentences into words, and counts word occurrences. Uses KestrelSpout for queue consumption and implements word processing bolts.

LANGUAGE: java
CODE:
    TopologyBuilder builder = new TopologyBuilder();
    builder.setSpout("sentences", new KestrelSpout("localhost",22133,"sentence_queue",new StringScheme()));
    builder.setBolt("split", new SplitSentence(), 10)
    	        .shuffleGrouping("sentences");
    builder.setBolt("count", new WordCount(), 20)
	        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Executing Storm SQL Command
DESCRIPTION: Basic command to compile SQL statements into Storm topology and submit to cluster

LANGUAGE: bash
CODE:
$ bin/storm sql <sql-file> <topo-name>

----------------------------------------

TITLE: Implementing SimpleMongoUpdateMapper for Storm-MongoDB Integration
DESCRIPTION: Implements the SimpleMongoUpdateMapper class, which extends SimpleMongoMapper to support updating documents in MongoDB using the $set operator.

LANGUAGE: java
CODE:
public class SimpleMongoUpdateMapper extends SimpleMongoMapper implements MongoUpdateMapper {

    private String[] fields;

    @Override
    public Document toDocument(ITuple tuple) {
        Document document = new Document();
        for(String field : fields){
            document.append(field, tuple.getValueByField(field));
        }
        //$set operator: Sets the value of a field in a document.
        return new Document("$set", document);
    }

    public SimpleMongoUpdateMapper withFields(String... fields) {
        this.fields = fields;
        return this;
    }
}

----------------------------------------

TITLE: Spout Tuple Transfer in Apache Storm (Clojure)
DESCRIPTION: This code shows how spouts use the transfer function provided by the worker to transfer tuples in Apache Storm.

LANGUAGE: clojure
CODE:
(github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L329)

----------------------------------------

TITLE: Kafka Stream Filtering Example
DESCRIPTION: Complete example of filtering Kafka stream data using Storm SQL

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://orders?bootstrap-servers=localhost:9092,localhost:9093'
CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) LOCATION 'kafka://large_orders?bootstrap-servers=localhost:9092,localhost:9093' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Defining KafkaTopicSelector Interface in Java
DESCRIPTION: Interface definition for selecting the Kafka topic to publish a tuple to. Contains a method to get the topic name from a tuple.

LANGUAGE: java
CODE:
public interface KafkaTopicSelector {
    String getTopics(Tuple/TridentTuple tuple);
}

----------------------------------------

TITLE: Creating External Table for Kafka in Storm SQL
DESCRIPTION: SQL statement to create an external table representing a Kafka spout and sink. This example defines a table named FOO with an integer ID as the primary key, connecting to a Kafka topic.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Launching Docker Container for Storm Worker
DESCRIPTION: Example Docker run command used by the worker-launcher to start a container for a Storm worker. It includes various security and resource constraints.

LANGUAGE: bash
CODE:
run --name=8198e1f0-f323-4b9d-8625-e4fd640cd058 \
--user=<uid>:<gid> \
-d \
--net=host \
--read-only \
-v /sys/fs/cgroup:/sys/fs/cgroup:ro \
-v /usr/share/apache-storm-2.3.0:/usr/share/apache-storm-2.3.0:ro \
-v /<storm-local-dir>/supervisor:/<storm-local-dir>/supervisor:ro \
-v /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058:/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
-v /<workers-artifacts-dir>/workers-artifacts/word-count-1-1591895933/6703:/<workers-artifacts-dir>/workers-artifacts/word-count-1-1591895933/6703 \
-v /<storm-local-dir>/workers-users/8198e1f0-f323-4b9d-8625-e4fd640cd058:/<storm-local-dir>/workers-users/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
-v /var/run/nscd:/var/run/nscd \
-v /<storm-local-dir>/supervisor/stormdist/word-count-1-1591895933/shared_by_topology:/<storm-local-dir>/supervisor/stormdist/word-count-1-1591895933/shared_by_topology \
-v /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/tmp:/tmp \
-v /etc/storm:/etc/storm:ro \
--cgroup-parent=/storm \
--group-add <gid> \
--workdir=/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
--cidfile=/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/container.cid \
--cap-drop=ALL \
--security-opt no-new-privileges \
--security-opt seccomp=/usr/share/apache-storm-2.3.0/conf/seccomp.json \
--cpus=2.6 xxx.xxx.com:8080/storm/storm/rhel7:latest \
bash /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/storm-worker-script.sh

----------------------------------------

TITLE: Implementing ISerialization Interface in Java for Storm Custom Serialization
DESCRIPTION: This code snippet shows the ISerialization interface that needs to be implemented to create a custom serializer for Storm. It includes methods for accepting a class, serializing an object to a DataOutputStream, and deserializing from a DataInputStream.

LANGUAGE: java
CODE:
public interface ISerialization<T> {
    public boolean accept(Class c);
    public void serialize(T object, DataOutputStream stream) throws IOException;
    public T deserialize(DataInputStream stream) throws IOException;
}

----------------------------------------

TITLE: Trident Kafka Integration Example
DESCRIPTION: Example showing how to integrate Kafka with a Trident topology

LANGUAGE: java
CODE:
Fields fields = new Fields("word", "count");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
        new Values("storm", "1"),
        new Values("trident", "1"),
        new Values("needs", "1"),
        new Values("javadoc", "1")
);
spout.setCycle(true);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

//set producer properties.
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

TridentKafkaStateFactory stateFactory = new TridentKafkaStateFactory()
        .withProducerProperties(props)
        .withKafkaTopicSelector(new DefaultTopicSelector("test"))
        .withTridentTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper("word", "count"));
stream.partitionPersist(stateFactory, fields, new TridentKafkaStateUpdater(), new Fields());

Config conf = new Config();
StormSubmitter.submitTopology("kafkaTridentTest", conf, topology.build());

----------------------------------------

TITLE: Storm Spout Implementation
DESCRIPTION: Implementation of a test word spout that emits random words at fixed intervals.

LANGUAGE: java
CODE:
public void nextTuple() {
    Utils.sleep(100);
    final String[] words = new String[] {"nathan", "mike", "jackson", "golda", "bertels"};
    final Random rand = new Random();
    final String word = words[rand.nextInt(words.length)];
    _collector.emit(new Values(word));
}

----------------------------------------

TITLE: Message Transfer Protocol Implementation
DESCRIPTION: Protocol definition for message transfer between Storm workers, with separate implementations for ZeroMQ (distributed mode) and in-memory queues (local mode).

LANGUAGE: clojure
CODE:
protocol.clj, zmq.clj, local.clj

----------------------------------------

TITLE: Configuring MongoLookupBolt for Storm-MongoDB Integration
DESCRIPTION: Example of how to configure and use MongoLookupBolt for querying data from MongoDB in a Storm topology.

LANGUAGE: java
CODE:
MongoLookupMapper mapper = new SimpleMongoLookupMapper()
        .withFields("word", "count");

QueryFilterCreator filterCreator = new SimpleQueryFilterCreator()
        .withField("word");

MongoLookupBolt lookupBolt = new MongoLookupBolt(url, collectionName, filterCreator, mapper);

----------------------------------------

TITLE: Package Relocation Table in Markdown
DESCRIPTION: A table showing the mapping of original dependency packages to their relocated versions in Apache Storm 0.9.3 to avoid dependency conflicts.

LANGUAGE: markdown
CODE:
| Dependency  | Original Package | Apache Storm Package |
|:---|:---|:---|
| Apache Thrift | `org.apache.thrift` | `org.apache.thrift7` |
| Netty | `org.jboss.netty` | `org.apache.storm.netty` |
| Google Guava | `com.google.common` | `org.apache.storm.guava` |
|              | `com.google.thirdparty` | `org.apache.storm.guava.thirdparty` |
| Apache HTTPClient | `org.apache.http` | `org.apache.storm.http` |
| Apache ZooKeeper | `org.apache.zookeeper` | `org.apache.storm.zookeeper` |
| Apache Curator | `org.apache.curator` | `org.apache.storm.curator` |

----------------------------------------

TITLE: Configuring Mongo Trident State for Storm-MongoDB Integration
DESCRIPTION: Demonstrates how to configure and use Mongo Trident State for persistent storage in Trident topologies. It requires specifying the MongoDB URL, collection name, and a MongoMapper implementation.

LANGUAGE: java
CODE:
MongoMapper mapper = new SimpleMongoMapper()
        .withFields("word", "count");

MongoState.Options options = new MongoState.Options()
        .withUrl(url)
        .withCollectionName(collectionName)
        .withMapper(mapper);

StateFactory factory = new MongoStateFactory(options);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory, fields,
        new MongoStateUpdater(), new Fields());

TridentState state = topology.newStaticState(factory);
stream = stream.stateQuery(state, new Fields("word"),
        new MongoStateQuery(), new Fields("columnName", "columnValue"));
stream.each(new Fields("word", "columnValue"), new PrintFunction(), new Fields());

----------------------------------------

TITLE: CGroup Configuration for Storm in YAML
DESCRIPTION: Sample cgconfig.conf file for configuring CGroup mounts and permissions in Storm. Defines resource control groups and access permissions for CPU, memory, and other resources.

LANGUAGE: yaml
CODE:
mount {
	cpuset	= /cgroup/cpuset;
	cpu	= /cgroup/storm_resources;
	cpuacct	= /cgroup/storm_resources;
	memory	= /cgroup/storm_resources;
	devices	= /cgroup/devices;
	freezer	= /cgroup/freezer;
	net_cls	= /cgroup/net_cls;
	blkio	= /cgroup/blkio;
}

group storm {
       perm {
               task {
                      uid = 500;
                      gid = 500;
               }
               admin {
                      uid = 500;
                      gid = 500;
               }
       }
       cpu {
       }
       memory {
       }
       cpuacct {
       }
}

----------------------------------------

TITLE: Using Value Mappers in Java StreamBuilder
DESCRIPTION: Shows how to use value mappers to extract specific fields from tuples and create typed streams.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

// extract the first field from the tuple to get a Stream<String> of sentences
Stream<String> sentences = builder.newStream(new TestWordSpout(), new ValueMapper<String>(0));

// extract first three fields of the tuple emitted by the spout to produce a stream of typed tuples.
Stream<Tuple3<String, Integer, Long>> stream = builder.newStream(new TestSpout(), TupleValueMappers.of(0, 1, 2));

----------------------------------------

TITLE: Rebalancing Storm Topology
DESCRIPTION: Redistributes workers for a topology across the cluster, optionally changing parallelism.

LANGUAGE: shell
CODE:
storm rebalance topology-name [-w wait-time-secs] [-n new-num-workers] [-e component=parallelism]*

----------------------------------------

TITLE: Storm Fail Command JSON Format
DESCRIPTION: JSON structure for reporting tuple processing failure through STDOUT. Contains command type and tuple ID.

LANGUAGE: json
CODE:
{
    "command": "fail",
    "id": 123123
}

----------------------------------------

TITLE: Creating External Table for Kafka in Storm SQL
DESCRIPTION: SQL statement to create an external table representing a Kafka spout and sink. This example defines a table named FOO with an integer ID as the primary key, connecting to a Kafka topic.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Configuring Shared Memory in Storm Components
DESCRIPTION: Example showing how to configure shared memory between components in a Storm topology

LANGUAGE: java
CODE:
builder.setBolt("exclaim1", new ExclamationBolt(), 3).shuffleGrouping("word")
      .addSharedMemory(new SharedOnHeap(100, "exclaim-cache"));

----------------------------------------

TITLE: Retrieving Topology Summary in JSON (Storm UI API)
DESCRIPTION: GET request to /api/v1/topology/summary endpoint. Returns summary information for all topologies in the Storm cluster, including status, uptime, and resource usage.

LANGUAGE: json
CODE:
{
  "topologies": [
    {
      "id": "WordCount3-1-1402960825",
      "name": "WordCount3",
      "status": "ACTIVE",
      "uptime": "6m 5s",
      "uptimeSeconds": 365,
      "tasksTotal": 28,
      "workersTotal": 3,
      "executorsTotal": 28,
      "replicationCount": 1,
      "requestedMemOnHeap": 640,
      "requestedMemOffHeap": 128,
      "requestedTotalMem": 768,
      "requestedCpu": 80,
      "assignedMemOnHeap": 640,
      "assignedMemOffHeap": 128,
      "assignedTotalMem": 768,
      "assignedCpu": 80
    }
  ],
  "schedulerDisplayResource": true
}

----------------------------------------

TITLE: Logging Message in JSON Format for Storm Multi-Language Protocol
DESCRIPTION: This JSON structure represents a log command in the Storm Multi-Language Protocol. It includes the command type and the message to be logged in the worker log.

LANGUAGE: JSON
CODE:
{
    "command": "log",
    "msg": "hello world!"
}

----------------------------------------

TITLE: Storm Local Mode Configuration
DESCRIPTION: Configuration and execution of a Storm topology in local mode for testing.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setDebug(true);
conf.setNumWorkers(2);

LocalCluster cluster = new LocalCluster();
cluster.submitTopology("test", conf, builder.createTopology());
Utils.sleep(10000);
cluster.killTopology("test");
cluster.shutdown();

----------------------------------------

TITLE: Creating Kafka Topics for Apache Logs
DESCRIPTION: Command line instructions for creating Kafka topics for apache logs, error logs, and slow logs using kafka-topics utility

LANGUAGE: bash
CODE:
kafka-topics --create --topic apache-logs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
kafka-topics --create --topic apache-errorlogs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
kafka-topics --create --topic apache-slowlogs --zookeeper localhost:2181 --replication-factor 1 --partitions 5

----------------------------------------

TITLE: Implementing MqttMessageMapper Interface in Java
DESCRIPTION: Definition of the MqttMessageMapper interface used to map MQTT messages to Storm tuples.

LANGUAGE: java
CODE:
public interface MqttMessageMapper extends Serializable {

    Values toValues(MqttMessage message);

    Fields outputFields();
}

----------------------------------------

TITLE: Implementing Split Function for Word Extraction in Trident
DESCRIPTION: Defines a Split function that takes a sentence and emits individual words as tuples in the Trident stream.

LANGUAGE: java
CODE:
public class Split extends BaseFunction {
   public void execute(TridentTuple tuple, TridentCollector collector) {
       String sentence = tuple.getString(0);
       for(String word: sentence.split(" ")) {
           collector.emit(new Values(word));                
       }
   }
}

----------------------------------------

TITLE: Configuring Supervisor Slot Ports in YAML
DESCRIPTION: This configuration defines the ports available for worker processes on each machine, effectively setting the maximum number of workers per machine.

LANGUAGE: yaml
CODE:
supervisor.slots.ports:
    - 6700
    - 6701
    - 6702
    - 6703

----------------------------------------

TITLE: Insert with Named Setters and Aliases
DESCRIPTION: Demonstrates how to use named parameters and aliases in a bound query for more explicit field mapping.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
     async(
        boundQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (:ti, :ye, :pe, :ge, :tr);")
            .bind(
                field("ti").as("title"),
                field("ye").as("year")),
                field("pe").as("performer")),
                field("ge").as("genre")),
                field("tr").as("tracks"))
            ).byNamedSetters()
     )
);

----------------------------------------

TITLE: Listing Running Storm Topologies
DESCRIPTION: Displays a list of currently running topologies and their statuses.

LANGUAGE: shell
CODE:
storm list

----------------------------------------

TITLE: Initializing RocketMqSpout for Reading from RocketMQ Topic in Java
DESCRIPTION: This snippet demonstrates how to create and configure a RocketMqSpout instance for reading data from a RocketMQ topic. It sets up properties for the name server address, consumer group, and topic.

LANGUAGE: java
CODE:
Properties properties = new Properties();
properties.setProperty(SpoutConfig.NAME_SERVER_ADDR, nameserverAddr);
properties.setProperty(SpoutConfig.CONSUMER_GROUP, group);
properties.setProperty(SpoutConfig.CONSUMER_TOPIC, topic);

RocketMqSpout spout = new RocketMqSpout(properties);

----------------------------------------

TITLE: Configuring Authorization in storm.yaml
DESCRIPTION: YAML configuration for setting up authorization in Storm using SimpleACLAuthorizer

LANGUAGE: yaml
CODE:
nimbus.authorizer: "org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer"

nimbus.supervisor.users:
  - "supervisor1"
  - "supervisor2"

nimbus.admins:
  - "admin1"
  - "admin2"

logs.users:
  - "admin1"
  - "admin2"

logs.groups:
  - "admin_group"

----------------------------------------

TITLE: Configuring Pacemaker Servers in Storm
DESCRIPTION: Specifies the hosts running Pacemaker daemons in the Storm cluster configuration.

LANGUAGE: yaml
CODE:
pacemaker.servers:
    - somehost.mycompany.com
    - someotherhost.mycompany.com

----------------------------------------

TITLE: Chaining Aggregators in Trident
DESCRIPTION: Example of chaining multiple aggregators in a Trident stream.

LANGUAGE: java
CODE:
mystream.chainedAgg()
        .partitionAggregate(new Count(), new Fields("count"))
        .partitionAggregate(new Fields("b"), new Sum(), new Fields("sum"))
        .chainEnd()

----------------------------------------

TITLE: Creating a Blob using Storm CLI
DESCRIPTION: Example of creating a blob in the distributed cache using the Storm command-line interface. This command creates a blob with key 'key1' from the file 'README.txt' with read, write, and admin access for all users and a replication factor of 4.

LANGUAGE: bash
CODE:
storm blobstore create --file README.txt --acl o::rwa --replication-factor 4 key1

----------------------------------------

TITLE: Database Table Creation and Initial Data SQL
DESCRIPTION: SQL statements for creating necessary database tables and inserting initial test data.

LANGUAGE: sql
CODE:
create table if not exists user (user_id integer, user_name varchar(100), dept_name varchar(100), create_date date);
create table if not exists department (dept_id integer, dept_name varchar(100));
create table if not exists user_department (user_id integer, dept_id integer);
insert into department values (1, 'R&D');
insert into department values (2, 'Finance');
insert into department values (3, 'HR');
insert into department values (4, 'Sales');
insert into user_department values (1, 1);
insert into user_department values (2, 2);
insert into user_department values (3, 3);
insert into user_department values (4, 4);

----------------------------------------

TITLE: Starting MQTT Broker and Publisher in Bash
DESCRIPTION: Command to start an MQTT broker on port 1883 and initiate a publisher for temperature/humidity data

LANGUAGE: bash
CODE:
java -cp examples/target/storm-mqtt-examples-*-SNAPSHOT.jar org.apache.storm.mqtt.examples.MqttBrokerPublisher

----------------------------------------

TITLE: Setting DRPC Servers Configuration
DESCRIPTION: YAML configuration for specifying DRPC server addresses

LANGUAGE: yaml
CODE:
drpc.servers: ["111.222.333.44"]

----------------------------------------

TITLE: Killing Topology in JSON
DESCRIPTION: Example response from the /api/v1/topology/<id>/kill/<wait-time> POST endpoint, which kills a topology.

LANGUAGE: json
CODE:
{
  "topologyOperation":"kill",
  "topologyId":"wordcount-1-1420308665",
  "status":"success"
}

----------------------------------------

TITLE: Implementing Basic Trident Function
DESCRIPTION: Example of implementing a Trident BaseFunction that emits multiple tuples based on input value

LANGUAGE: java
CODE:
public class MyFunction extends BaseFunction {
    public void execute(TridentTuple tuple, TridentCollector collector) {
        for(int i=0; i < tuple.getInteger(0); i++) {
            collector.emit(new Values(i));
        }
    }
}

----------------------------------------

TITLE: Using Min and Max Operations in Trident
DESCRIPTION: Example of using min, minBy, max, and maxBy operations in Trident streams.

LANGUAGE: java
CODE:
mystream.minBy(new Fields("count"))

LANGUAGE: java
CODE:
vehiclesStream
        .max(new SpeedComparator())
        .each(vehicleField, new Debug("#### fastest vehicle"))
        .project(driverField)
        .each(driverField, new Debug("##### fastest driver"));

vehiclesStream
        .maxBy(Vehicle.FIELD_NAME, new EfficiencyComparator())
        .each(vehicleField, new Debug("#### most efficient vehicle"));

----------------------------------------

TITLE: Creating Implicit Topology in Storm
DESCRIPTION: The system-topology! function is used to create an implicit topology with added streams and an acker bolt for managing the acking framework. This function is used both by Nimbus when creating tasks and by workers for message routing.

LANGUAGE: clojure
CODE:
(system-topology!)

----------------------------------------

TITLE: Configuring EsConfig for Elasticsearch Connection
DESCRIPTION: Creates EsConfig instances with basic and advanced configuration options for connecting to Elasticsearch cluster.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});

// Advanced configuration
Map<String, String> additionalParameters = new HashMap<>();
additionalParameters.put("client.transport.sniff", "true");
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"}, additionalParameters);

----------------------------------------

TITLE: Using Value Mappers with Storm's Stream API in Java
DESCRIPTION: Shows how to use value mappers to extract specific fields from tuples emitted by a spout.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

// extract the first field from the tuple to get a Stream<String> of sentences
Stream<String> sentences = builder.newStream(new TestWordSpout(), new ValueMapper<String>(0));

// extract first three fields of the tuple emitted by the spout to produce a stream of typed tuples.
Stream<Tuple3<String, Integer, Long>> stream = builder.newStream(new TestSpout(), TupleValueMappers.of(0, 1, 2));

----------------------------------------

TITLE: Configuring CGroup Mount Points and Permissions in CGConfig
DESCRIPTION: Sample configuration file for setting up CGroup mount points and permissions in Storm. Defines the mount locations for various resource controllers and sets up permissions for the storm group.

LANGUAGE: conf
CODE:
mount {
	cpuset	= /cgroup/cpuset;
	cpu	= /cgroup/storm_resources;
	cpuacct	= /cgroup/storm_resources;
	memory	= /cgroup/storm_resources;
	devices	= /cgroup/devices;
	freezer	= /cgroup/freezer;
	net_cls	= /cgroup/net_cls;
	blkio	= /cgroup/blkio;
}

group storm {
       perm {
               task {
                      uid = 500;
                      gid = 500;
               }
               admin {
                      uid = 500;
                      gid = 500;
               }
       }
       cpu {
       }
       memory {
       }
       cpuacct {
       }
}

----------------------------------------

TITLE: Converting Docker Images to Squashfs using Python Script
DESCRIPTION: Example command for using docker-to-squash.py script to download Docker images, convert layers to squashfs files and upload to HDFS

LANGUAGE: bash
CODE:
python docker-to-squash.py pull-build-push-update --hdfs-root hdfs://hostname:port/containers \
                      docker.xxx.com:4443/hadoop-user-images/storm/rhel7:20201202-232133,storm/rhel7:dev_current --log DEBUG --bootstrap

----------------------------------------

TITLE: JSON Structure for Emitting Tuple in Storm Multi-Language Protocol
DESCRIPTION: This JSON structure is used to emit a tuple in the Storm multi-language protocol. It specifies the command as 'emit', includes anchor tuple ids, stream id, task id (for direct emit), and the tuple values.

LANGUAGE: JSON
CODE:
{
    "command": "emit",
    "anchors": [1231231, -234234234],
    "stream": 1,
    "task": 9,
    "tuple": ["field1", 2, 3]
}

----------------------------------------

TITLE: Configuring Secure HBase Connection
DESCRIPTION: Configuration example for setting up Kerberos authentication with HBase.

LANGUAGE: java
CODE:
Config config = new Config();
...
config.put("storm.keytab.file", "$keytab");
config.put("storm.kerberos.principal", "$principal");
StormSubmitter.submitTopology("$topologyName", config, builder.createTopology());

----------------------------------------

TITLE: Setting Log Level via Storm CLI
DESCRIPTION: Commands to dynamically set log levels for a running topology using the Storm CLI. Includes syntax for setting a log level with timeout and clearing a log level setting.

LANGUAGE: bash
CODE:
./bin/storm set_log_level [topology name] -l [logger name]=[LEVEL]:[TIMEOUT]

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -l ROOT=DEBUG:30

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -r ROOT

----------------------------------------

TITLE: Redis Filter Mapper Implementation
DESCRIPTION: Implementation of RedisFilterMapper interface for filtering words using a Redis set as a blacklist.

LANGUAGE: java
CODE:
class BlacklistWordFilterMapper implements RedisFilterMapper {
    private RedisDataTypeDescription description;
    private final String setKey = "blacklist";

    public BlacklistWordFilterMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.SET, setKey);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Creating External Table for Kafka in Storm SQL
DESCRIPTION: Shows how to create an external table in Storm SQL that represents a Kafka spout and sink.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-hosts=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Configuring Remote Cluster Access in YAML
DESCRIPTION: This YAML configuration specifies the Nimbus seed node for connecting to a remote Storm cluster. It should be placed in the '~/.storm/storm.yaml' file to enable the 'storm' client to communicate with the cluster.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["123.45.678.890"]

----------------------------------------

TITLE: Applying a Custom Function to a Trident Stream
DESCRIPTION: Example of how to apply a custom function to a Trident stream, adding a new field to the output.

LANGUAGE: java
CODE:
mystream.each(new Fields("b"), new MyFunction(), new Fields("d")))

----------------------------------------

TITLE: Initializing HiveBolt with DelimitedRecordHiveMapper (Java)
DESCRIPTION: Java code snippet demonstrating how to initialize a HiveBolt using DelimitedRecordHiveMapper and HiveOptions. This setup allows streaming tuples directly into Hive using Hive Transactions.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames));
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper);
HiveBolt hiveBolt = new HiveBolt(hiveOptions);

----------------------------------------

TITLE: Setting Health Check Timeout in Storm YAML
DESCRIPTION: This configuration sets the maximum time allowed for a health check script to run before it's marked as failed due to timeout.

LANGUAGE: yaml
CODE:
storm.health.check.timeout.ms: 5000

----------------------------------------

TITLE: Creating and Querying Kafka Stream Tables in Storm SQL
DESCRIPTION: Example showing how to create external tables linked to Kafka streams and perform filtering and aggregation operations. The query filters orders where total amount exceeds 50 and projects the ID and total into a new stream.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://...' ...

CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) 'kafka://...' ...

INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Activating Topology in JSON
DESCRIPTION: Sample response from the /api/v1/topology/<id>/activate POST endpoint, showing the result of activating a topology.

LANGUAGE: json
CODE:
{
  "topologyOperation":"activate",
  "topologyId":"wordcount-1-1420308665",
  "status":"success"
}

----------------------------------------

TITLE: Creating Custom Storm Configurations with Validation
DESCRIPTION: Illustrates how to create a custom configuration class for Storm that implements the Validated interface. This approach allows for defining new configuration options with validation annotations, useful for custom schedulers or plugins.

LANGUAGE: java
CODE:
public class CustomConfig implements Validated {
    public static final String MY_CUSTOM_CONFIG = "my.custom.config";
    
    @NotNull
    @isString
    public static final String ANOTHER_CONFIG = "another.config";
}

----------------------------------------

TITLE: Implementing RedisLookupMapper for WordCount
DESCRIPTION: Java class implementing RedisLookupMapper interface for a word count scenario. This mapper is used with RedisLookupBolt to retrieve word counts from Redis.

LANGUAGE: java
CODE:
class WordCountRedisLookupMapper implements RedisLookupMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountRedisLookupMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public List<Values> toTuple(ITuple input, Object value) {
        String member = getKeyFromTuple(input);
        List<Values> values = Lists.newArrayList();
        values.add(new Values(member, value));
        return values;
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("wordName", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Stream Builder Creation and Value Mapping in Java
DESCRIPTION: Demonstrates how to create streams using StreamBuilder and use value mappers to extract typed data from tuples.

LANGUAGE: java
CODE:
// Basic stream creation
StreamBuilder builder = new StreamBuilder();
Stream<Tuple> sentences = builder.newStream(new TestSentenceSpout());

// Using value mapper
Stream<String> sentences = builder.newStream(new TestWordSpout(), new ValueMapper<String>(0));

// Using tuple value mapper
Stream<Tuple3<String, Integer, Long>> stream = builder.newStream(new TestSpout(), TupleValueMappers.of(0, 1, 2));

----------------------------------------

TITLE: Storm Component Structure Definition
DESCRIPTION: Core Thrift definitions for Storm components including ComponentObject and ComponentCommon structs that define spout/bolt implementations and their properties.

LANGUAGE: thrift
CODE:
struct ComponentObject {
  1: binary serialized_java
  2: ShellComponent shell
  3: JavaObject java_object
}

struct ComponentCommon {
  1: map<string, StreamInfo> streams
  2: map<GlobalStreamId, Grouping> inputs
  3: i32 parallelism_hint
  4: map<string, string> json_conf
}

----------------------------------------

TITLE: Launching Storm Daemons
DESCRIPTION: Commands for starting various Storm daemon processes, including Nimbus, Supervisor, UI, and DRPC.

LANGUAGE: bash
CODE:
storm nimbus
storm supervisor
storm ui
storm drpc

----------------------------------------

TITLE: IConfigLoader Interface Definition in Java
DESCRIPTION: Definition of the IConfigLoader interface with a single method 'load' that retrieves the most recent configuration map.

LANGUAGE: java
CODE:
public interface IConfigLoader {
    Map<?,?> load();
};

----------------------------------------

TITLE: Configuring Metric Reporters in Apache Storm
DESCRIPTION: Configuration snippet showing how to configure metric reporters in Apache Storm using the storm.daemon.metrics.reporter.plugins setting. Lists the supported reporters including Console, CSV, and JMX reporters.

LANGUAGE: yaml
CODE:
storm.daemon.metrics.reporter.plugins:
  - "org.apache.storm.daemon.metrics.reporters.ConsolePreparableReporter"
  - "org.apache.storm.daemon.metrics.reporters.CsvPreparableReporter"
  - "org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter"

----------------------------------------

TITLE: Configuring HiveOptions for HiveBolt (Java)
DESCRIPTION: Java code example demonstrating how to configure HiveOptions, which is used as a constructor argument for HiveBolt. It includes settings for transactions, batch size, and idle timeout.

LANGUAGE: java
CODE:
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
                                .withTxnsPerBatch(10)
                				.withBatchSize(1000)
                	     		.withIdleTimeout(10)

----------------------------------------

TITLE: Initializing EsLookupBolt for Storm-Elasticsearch Integration in Java
DESCRIPTION: Creates an EsLookupBolt to perform get requests to Elasticsearch from Storm. It requires an EsConfig, an ElasticsearchGetRequest to convert incoming tuples to GetRequests, and an EsLookupResultOutput to handle the response.

LANGUAGE: java
CODE:
EsConfig esConfig = createEsConfig();
ElasticsearchGetRequest getRequestAdapter = createElasticsearchGetRequest();
EsLookupResultOutput output = createOutput();
EsLookupBolt lookupBolt = new EsLookupBolt(esConfig, getRequestAdapter, output);

----------------------------------------

TITLE: Registering Event Loggers in YAML Configuration
DESCRIPTION: Example of how to register multiple event loggers in the storm.yaml configuration file, including custom arguments.

LANGUAGE: yaml
CODE:
topology.event.logger.register:
  - class: "org.apache.storm.metric.FileBasedEventLogger"
  - class: "org.mycompany.MyEventLogger"
    arguments:
      endpoint: "event-logger.mycompany.org"

----------------------------------------

TITLE: Implementing Custom State Interface
DESCRIPTION: Example of implementing a custom State interface for a location database.

LANGUAGE: java
CODE:
public class LocationDB implements State {
    public void beginCommit(Long txid) {    
    }
    
    public void commit(Long txid) {    
    }
    
    public void setLocation(long userId, String location) {
      // code to access database and set location
    }
    
    public String getLocation(long userId) {
      // code to get location from database
    }
}

----------------------------------------

TITLE: Specifying Classpath Wildcards in Java
DESCRIPTION: This snippet demonstrates the use of classpath wildcards in Java 6 and later versions to include all JAR files in a directory without explicitly listing them. This technique is used by Storm to shorten classpath specifications and avoid process command length limits.

LANGUAGE: java
CODE:
foo/bar/*

----------------------------------------

TITLE: Deploying a Flux Topology
DESCRIPTION: Command line example for deploying a Flux topology using the storm jar command.

LANGUAGE: bash
CODE:
storm jar myTopology-0.1.0-SNAPSHOT.jar org.apache.storm.flux.Flux --local my_config.yaml

----------------------------------------

TITLE: Storm Shell Command Execution Format
DESCRIPTION: Format showing how Storm shell executes the topology script with Nimbus connection parameters.

LANGUAGE: bash
CODE:
python3 topology.py arg1 arg2 {nimbus-host} {nimbus-port} {uploaded-jar-location}

----------------------------------------

TITLE: Custom MQTT Message Mapper Implementation in Java
DESCRIPTION: Example implementation of MqttMessageMapper that processes topic and payload data to produce Storm tuples.

LANGUAGE: java
CODE:
public class CustomMessageMapper implements MqttMessageMapper {
    private static final Logger LOG = LoggerFactory.getLogger(CustomMessageMapper.class);


    public Values toValues(MqttMessage message) {
        String topic = message.getTopic();
        String[] topicElements = topic.split("/");
        String[] payloadElements = new String(message.getMessage()).split("/");

        return new Values(topicElements[2], topicElements[4], topicElements[3], Float.parseFloat(payloadElements[0]), 
                Float.parseFloat(payloadElements[1]));
    }

    public Fields outputFields() {
        return new Fields("user", "deviceId", "location", "temperature", "humidity");
    }
}

----------------------------------------

TITLE: Creating Custom Storm Configurations with Validation
DESCRIPTION: Illustrates how to create a custom configuration class for Storm that implements the Validated interface. This approach allows for defining new configuration options with validation annotations, useful for custom schedulers or plugins.

LANGUAGE: java
CODE:
public class CustomConfig implements Validated {
    public static final String MY_CUSTOM_CONFIG = "my.custom.config";
    
    @NotNull
    @isString
    public static final String ANOTHER_CONFIG = "another.config";
}

----------------------------------------

TITLE: Creating External Table for Kafka in Storm SQL
DESCRIPTION: SQL statement to create an external table representing a Kafka spout and sink in Storm SQL.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Retrieving Supervisor Summary in Storm UI API
DESCRIPTION: GET request to retrieve summary information for all supervisors in the Storm cluster, including host, uptime, and resource usage.

LANGUAGE: json
CODE:
{
  "supervisors": [
    {
      "id": "0b879808-2a26-442b-8f7d-23101e0c3696",
      "host": "10.11.1.7",
      "uptime": "5m 58s",
      "uptimeSeconds": 358,
      "slotsTotal": 4,
      "slotsUsed": 3,
      "totalMem": 3000,
      "totalCpu": 400,
      "usedMem": 1280,
      "usedCPU": 160
    }
  ],
  "schedulerDisplayResource": true
}

----------------------------------------

TITLE: Applying a Filter to a Trident Stream
DESCRIPTION: Example of applying a custom Filter to a Trident stream.

LANGUAGE: java
CODE:
mystream.filter(new MyFilter())

----------------------------------------

TITLE: Configuring UI/Logviewer Filter in YAML
DESCRIPTION: YAML configuration for setting up authentication filters for the Storm UI and logviewer processes.

LANGUAGE: yaml
CODE:
ui.filter: "filter.class"
ui.filter.params: "param1":"value1"
logviewer.filter: "filter.class"
logviewer.filter.params: "param1":"value1"

----------------------------------------

TITLE: Initializing HiveBolt with DelimitedRecordHiveMapper (Java)
DESCRIPTION: Java code snippet demonstrating how to initialize a HiveBolt using DelimitedRecordHiveMapper and HiveOptions. This setup allows streaming tuples directly into Hive using Hive Transactions.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames));
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper);
HiveBolt hiveBolt = new HiveBolt(hiveOptions);

----------------------------------------

TITLE: Implementing RecordToTupleMapper Interface in Java
DESCRIPTION: This code snippet shows the interface for RecordToTupleMapper, which is used to convert Kinesis records to Storm tuples. It includes methods for getting output fields and converting records to tuples.

LANGUAGE: java
CODE:
    Fields getOutputFields ();
    List<Object> getTuple (Record record);

----------------------------------------

TITLE: Running Storm SQL in Explain Mode
DESCRIPTION: Command to run Storm SQL in explain mode, which shows the query plan instead of submitting the topology. This is useful for analyzing and debugging SQL statements.

LANGUAGE: bash
CODE:
$ bin/storm sql order_filtering.sql --explain --artifacts "org.apache.storm:storm-sql-kafka:2.0.0-SNAPSHOT,org.apache.storm:storm-kafka-client:2.0.0-SNAPSHOT,org.apache.kafka:kafka-clients:1.1.0^org.slf4j:slf4j-log4j12"

----------------------------------------

TITLE: Multi-Language Storm Bolt in Python
DESCRIPTION: Example of implementing a Storm bolt in Python using the multi-lang protocol.

LANGUAGE: python
CODE:
import storm

class SplitSentenceBolt(storm.BasicBolt):
    def process(self, tup):
        words = tup.values[0].split(" ")
        for word in words:
          storm.emit([word])

SplitSentenceBolt().run()

----------------------------------------

TITLE: Importing DynamicStatementBuilder in Java for Storm Cassandra Integration
DESCRIPTION: Static import statement for the DynamicStatementBuilder class, which provides methods for building Cassandra queries in Storm bolts.

LANGUAGE: java
CODE:
import static org.apache.storm.cassandra.DynamicStatementBuilder.*

----------------------------------------

TITLE: Implementing RedisFilterMapper for Blacklist
DESCRIPTION: Java class implementing RedisFilterMapper interface for filtering words based on a blacklist stored in Redis SET.

LANGUAGE: java
CODE:
class BlacklistWordFilterMapper implements RedisFilterMapper {
    private RedisDataTypeDescription description;
    private final String setKey = "blacklist";

    public BlacklistWordFilterMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.SET, setKey);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Adding Generic Resource to Topology Component in Java
DESCRIPTION: This snippet demonstrates how to add a generic resource requirement to a spout or bolt in a Storm topology using the addResource method. It specifies the resource name and amount needed for each instance of the component.

LANGUAGE: java
CODE:
public T addResource(String resourceName, Number resourceValue)

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.addResouce("gpu.count", 1.0);

----------------------------------------

TITLE: Configuring NUMA Settings in Supervisor Config for Apache Storm
DESCRIPTION: This YAML configuration snippet shows how to set up NUMA zones in the Supervisor Config, including core assignments, generic resources, memory allocation, and port assignments for each NUMA zone.

LANGUAGE: yaml
CODE:
supervisor.numa.meta:
    "0": # Numa zone id
        numa.cores: # Cores in NUMA zone (can be determined by using the numastat command)
            - 0
            - 1
            - 2
            - 3
            - 4
            - 5
            - 12
            - 13
            - 14
            - 15
            - 16

        numa.generic.resources.map: # Generic Resources in the zone to be used for generic resource scheduling (optional)
            network.resource.units: 50.0

        numa.memory.mb: 42461 # Size of memory zone
        numa.ports: # Ports to be assigned to workers pinned to the NUMA zone (this may include ports not specified in SUPERVISOR_SLOTS_PORTS
            - 6700
            - 6701
            - 6702
            - 6703
            - 6704
            - 6705
            - 6706
            - 6707
            - 6708
            - 6709
            - 6710
            - 6711

----------------------------------------

TITLE: Creating External Table for Kafka in Storm SQL
DESCRIPTION: Example of creating an external table that specifies a Kafka spout and sink.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-hosts=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Restarting Worker in Storm UI REST API
DESCRIPTION: Example JSON response for the /api/v1/topology/:id/profiling/restartworker/:host-port endpoint, which requests to restart a worker.

LANGUAGE: json
CODE:
{
   "status": "ok",
   "id": "10.11.1.7:6701"
}

----------------------------------------

TITLE: Configuring Generic Cluster Resources in storm.yaml (YAML)
DESCRIPTION: This snippet shows how to specify the availability of generic resources on a Storm cluster node by modifying the storm.yaml configuration file. It allows administrators to define custom resource types and their amounts for each supervisor node.

LANGUAGE: yaml
CODE:
supervisor.resources.map: {[type<String>] : [amount<Double>]}

LANGUAGE: yaml
CODE:
supervisor.resources.map: {"gpu.count" : 2.0}

----------------------------------------

TITLE: HDFS Spout Configuration in Java
DESCRIPTION: Example demonstrating how to configure and instantiate an HDFS spout for reading text files

LANGUAGE: java
CODE:
// Instantiate spout to read text files
HdfsSpout textReaderSpout = new HdfsSpout().setReaderType("text")
                                           .withOutputFields(TextFileReader.defaultFields)                                      
                                           .setHdfsUri("hdfs://localhost:54310")  // required
                                           .setSourceDir("/data/in")              // required                                      
                                           .setArchiveDir("/data/done")           // required
                                           .setBadFilesDir("/data/badfiles");     // required                                      
// If using Kerberos
HashMap hdfsSettings = new HashMap();
hdfsSettings.put("hdfs.keytab.file", "/path/to/keytab");
hdfsSettings.put("hdfs.kerberos.principal","user@EXAMPLE.com");

textReaderSpout.setHdfsClientSettings(hdfsSettings);

----------------------------------------

TITLE: Custom MQTT Message Mapper Implementation
DESCRIPTION: Example implementation of MqttMessageMapper that parses topic and payload data

LANGUAGE: java
CODE:
public class CustomMessageMapper implements MqttMessageMapper {
    private static final Logger LOG = LoggerFactory.getLogger(CustomMessageMapper.class);

    public Values toValues(MqttMessage message) {
        String topic = message.getTopic();
        String[] topicElements = topic.split("/");
        String[] payloadElements = new String(message.getMessage()).split("/");

        return new Values(topicElements[2], topicElements[4], topicElements[3], Float.parseFloat(payloadElements[0]), Float.parseFloat(payloadElements[1]));
    }

    public Fields outputFields() {
        return new Fields("user", "deviceId", "location", "temperature", "humidity");
    }
}

----------------------------------------

TITLE: Redis Cluster State Configuration
DESCRIPTION: JSON configuration for Redis Cluster state provider, including serialization settings and cluster node configuration.

LANGUAGE: json
CODE:
{
   "keyClass": "Optional fully qualified class name of the Key type.",
   "valueClass": "Optional fully qualified class name of the Value type.",
   "keySerializerClass": "Optional Key serializer implementation class.",
   "valueSerializerClass": "Optional Value Serializer implementation class.",
   "jedisClusterConfig": {
     "nodes": ["localhost:7379", "localhost:7380", "localhost:7381"],
     "timeout": 2000,
     "maxRedirections": 5
   }
}

----------------------------------------

TITLE: Retrieving Supervisor Summary in Storm UI REST API
DESCRIPTION: GET request to retrieve summary information for all supervisors, including host, uptime, and resource utilization.

LANGUAGE: json
CODE:
{
  "supervisors": [
    {
      "id": "0b879808-2a26-442b-8f7d-23101e0c3696",
      "host": "10.11.1.7",
      "uptime": "5m 58s",
      "uptimeSeconds": 358,
      "slotsTotal": 4,
      "slotsUsed": 3,
      "totalMem": 3000,
      "totalCpu": 400,
      "usedMem": 1280,
      "usedCPU": 160
    }
  ],
  "schedulerDisplayResource": true
}

----------------------------------------

TITLE: Implementing TupleToKafkaMapper Interface in Java
DESCRIPTION: Interface methods for mapping a tuple to a Kafka key and message

LANGUAGE: java
CODE:
K getKeyFromTuple(Tuple/TridentTuple tuple);
V getMessageFromTuple(Tuple/TridentTuple tuple);

----------------------------------------

TITLE: Configuring User Resource Pools in YAML
DESCRIPTION: YAML configuration to specify resource guarantees for different users in the Resource Aware Scheduler.

LANGUAGE: yaml
CODE:
resource.aware.scheduler.user.pools:
    jerry:
        cpu: 1000
        memory: 8192.0
    derek:
        cpu: 10000.0
        memory: 32768
    bobby:
        cpu: 5000.0
        memory: 16384.0

----------------------------------------

TITLE: Initializing EsIndexBolt for Storm-Elasticsearch Integration
DESCRIPTION: Creates an EsIndexBolt instance to stream tuples directly into Elasticsearch using a specified cluster configuration and tuple mapper.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsIndexBolt indexBolt = new EsIndexBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Worker Process Initialization (Clojure)
DESCRIPTION: The mk-worker function initializes a worker process, connecting to other workers, monitoring topology activity, and launching task threads.

LANGUAGE: clojure
CODE:
(mk-worker conf storm-cluster-state storm-id assignment-id port worker-id)

----------------------------------------

TITLE: Pinning Workers to NUMA Zones using CgroupManager
DESCRIPTION: Command used by CgroupManager to bind worker processes to specific NUMA zone CPU cores and memory using numactl.

LANGUAGE: bash
CODE:
numactl --cpunodebind=<numaId>> --membind=<numaId> <worker launch command>

----------------------------------------

TITLE: Using Classpath Wildcards in Java
DESCRIPTION: Java 6 and later support classpath wildcards, allowing for shorter and more manageable classpath definitions. This snippet demonstrates the use of wildcards in a Java classpath.

LANGUAGE: java
CODE:
java -cp "${STORM_DIR}/lib/*:${STORM_DIR}/extlib/*" org.apache.storm.daemon.nimbus

----------------------------------------

TITLE: Implementing a FlatMap Function in Trident
DESCRIPTION: Example of a custom FlatMapFunction that splits a sentence into words.

LANGUAGE: java
CODE:
public class Split extends FlatMapFunction {
  @Override
  public Iterable<Values> execute(TridentTuple input) {
    List<Values> valuesList = new ArrayList<>();
    for (String word : input.getString(0).split(" ")) {
      valuesList.add(new Values(word));
    }
    return valuesList;
  }
}

----------------------------------------

TITLE: FailedMessageRetryHandler Interface in Java
DESCRIPTION: Interface definition for handling failed message retry logic in the Kinesis spout. Includes methods for managing failed messages and retry behavior.

LANGUAGE: java
CODE:
    boolean failed (KinesisMessageId messageId);
    KinesisMessageId getNextFailedMessageToRetry ();
    void failedMessageEmitted (KinesisMessageId messageId);
    void acked (KinesisMessageId messageId);

----------------------------------------

TITLE: Implementing Simple MongoDB Mapper
DESCRIPTION: General purpose implementation of MongoMapper that maps Storm tuple fields directly to MongoDB document fields

LANGUAGE: java
CODE:
public class SimpleMongoMapper implements MongoMapper {
    private String[] fields;

    @Override
    public Document toDocument(ITuple tuple) {
        Document document = new Document();
        for(String field : fields){
            document.append(field, tuple.getValueByField(field));
        }
        return document;
    }

    @Override
    public Document toDocumentByKeys(List<Object> keys) {
        Document document = new Document();
        document.append("_id", MongoUtils.getID(keys));
        return document;
    }

    public SimpleMongoMapper withFields(String... fields) {
        this.fields = fields;
        return this;
    }
}

----------------------------------------

TITLE: Implementing a Custom Metric in Java
DESCRIPTION: Shows how to create and register a custom CountMetric in a Storm bolt.

LANGUAGE: java
CODE:
private transient CountMetric countMetric;

@Override
public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
	// other initialization here.
	countMetric = new CountMetric();
	context.registerMetric("execute_count", countMetric, 60);
}

public void execute(Tuple input) {
	countMetric.incr();
	// handle tuple here.	
}

----------------------------------------

TITLE: Applying a Map Function to a Trident Stream
DESCRIPTION: Shows how to apply a map function to convert words to uppercase in a stream.

LANGUAGE: java
CODE:
mystream.map(new UpperCase())

----------------------------------------

TITLE: Multiple Queries from Single Tuple
DESCRIPTION: Shows how to execute multiple insert queries from a single input tuple, inserting into different tables.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
    async(
        simpleQuery("INSERT INTO titles_per_album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);")
            .with(all()),
        simpleQuery("INSERT INTO titles_per_performer (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);")
            .with(all())
    )
);

----------------------------------------

TITLE: Implementing SimpleQueryFilterCreator for Storm-MongoDB Integration
DESCRIPTION: This snippet shows the implementation of SimpleQueryFilterCreator, which creates MongoDB query filters using the $eq operator to match values equal to a specified field in the tuple.

LANGUAGE: java
CODE:
public class SimpleQueryFilterCreator implements QueryFilterCreator {
    private String field;
    
    @Override
    public Bson createFilter(ITuple tuple) {
        return Filters.eq(field, tuple.getValueByField(field));
    }

    public SimpleQueryFilterCreator withField(String field) {
        this.field = field;
        return this;
    }

}

----------------------------------------

TITLE: Registering Metrics Consumer in Java
DESCRIPTION: Example of how to register a metrics consumer in Storm topology configuration using Java code. This shows the basic configuration needed to add a LoggingMetricsConsumer.

LANGUAGE: java
CODE:
conf.registerMetricsConsumer(org.apache.storm.metric.LoggingMetricsConsumer.class, 1);

----------------------------------------

TITLE: Kafka Filtering Example
DESCRIPTION: Complete example of filtering Kafka stream using Storm SQL

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://orders?bootstrap-servers=localhost:9092,localhost:9093'
CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) LOCATION 'kafka://large_orders?bootstrap-servers=localhost:9092,localhost:9093' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Environment Variable Substitution in Flux YAML
DESCRIPTION: Example of using environment variable substitution in a Flux YAML file.

LANGUAGE: yaml
CODE:
${ENV-ZK_HOSTS}

----------------------------------------

TITLE: Defining HBaseMapper Interface in Java
DESCRIPTION: Defines the main API for interacting with HBase, including methods for generating row keys and columns from Storm tuples.

LANGUAGE: java
CODE:
public interface HBaseMapper extends Serializable {
    byte[] rowKey(Tuple tuple);

    ColumnList columns(Tuple tuple);
}

----------------------------------------

TITLE: Event Hubs Configuration Properties
DESCRIPTION: Configuration properties required for connecting Storm spout to Azure Event Hubs. Includes authentication, namespace, entity path and partition settings.

LANGUAGE: properties
CODE:
eventhubspout.username = [username: policy name in EventHubs Portal]
eventhubspout.password = [password: shared access key in EventHubs Portal]
eventhubspout.namespace = [namespace]
eventhubspout.entitypath = [entitypath]
eventhubspout.partitions.count = [partitioncount]

# if not provided, will use storm's zookeeper settings
# zookeeper.connectionstring=zookeeper0:2181,zookeeper1:2181,zookeeper2:2181

eventhubspout.checkpoint.interval = 10
eventhub.receiver.credits = 1024

----------------------------------------

TITLE: Running Storm Topology with Flux
DESCRIPTION: Command to run a sample topology using Flux in local mode to process MQTT messages

LANGUAGE: bash
CODE:
storm jar ./examples/target/storm-mqtt-examples-*-SNAPSHOT.jar org.apache.storm.flux.Flux ./examples/src/main/flux/sample.yaml --local

----------------------------------------

TITLE: Implementing RedisStoreMapper for WordCount in Java
DESCRIPTION: Java class implementing RedisStoreMapper interface for storing word count data in Redis.

LANGUAGE: java
CODE:
class WordCountStoreMapper implements RedisStoreMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountStoreMapper() {
        description = new RedisDataTypeDescription(
            RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return tuple.getStringByField("count");
    }
}

----------------------------------------

TITLE: Joining Streams in Storm's Stream API (Java)
DESCRIPTION: Shows how to join two streams based on a common key.

LANGUAGE: java
CODE:
PairStream<Long, Long> squares = â€¦ // (1, 1), (2, 4), (3, 9) ...
PairStream<Long, Long> cubes = â€¦ // (1, 1), (2, 8), (3, 27) ...

// join the sqaures and cubes stream to produce (1, [1, 1]), (2, [4, 8]), (3, [9, 27]) ...
PairStream<Long, Pair<Long, Long>> joined = squares.window(TumblingWindows.of(Duration.seconds(5))).join(cubes);

----------------------------------------

TITLE: Storm Log Command JSON Structure
DESCRIPTION: JSON structure for logging messages from a shell process to Storm's worker log.

LANGUAGE: json
CODE:
{
    "command": "log",
    "msg": "hello world!"
}

----------------------------------------

TITLE: Adding Counter Columns Example
DESCRIPTION: Example showing how to add counter columns to a ColumnList for HBase operations.

LANGUAGE: java
CODE:
ColumnList cols = new ColumnList();
cols.addCounter(this.columnFamily, field.getBytes(), toLong(tuple.getValueByField(field)));

----------------------------------------

TITLE: Configuring HiveBolt with DelimitedRecordHiveMapper (Java)
DESCRIPTION: Java code snippet demonstrating how to set up a HiveBolt using DelimitedRecordHiveMapper and HiveOptions. This configuration is used to stream tuples directly into Hive tables.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames));
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper);
HiveBolt hiveBolt = new HiveBolt(hiveOptions);

----------------------------------------

TITLE: Setting up Trident Topology with RedisState in Java
DESCRIPTION: Java code demonstrating how to set up a Trident topology using RedisState for persistence and querying.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
                                .setHost(redisHost).setPort(redisPort)
                                .build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisLookupMapper lookupMapper = new WordCountLookupMapper();
RedisState.Factory factory = new RedisState.Factory(poolConfig);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory,
                        fields,
                        new RedisStateUpdater(storeMapper).withExpire(86400000),
                        new Fields());

TridentState state = topology.newStaticState(factory);
stream = stream.stateQuery(state, new Fields("word"),
                        new RedisStateQuerier(lookupMapper),
                        new Fields("columnName","columnValue"));

----------------------------------------

TITLE: Using Preconfigured DRPC Client in Java
DESCRIPTION: This snippet shows how to use a preconfigured DRPC client, which automatically selects a host from the configured set of hosts.

LANGUAGE: java
CODE:
DRPCClient client = DRPCClient.getConfiguredClient(conf);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Defining JdbcMapper Interface in Java for Storm JDBC
DESCRIPTION: Interface definition for JdbcMapper, which defines how a storm tuple maps to a list of columns representing a row in a database. The order of the returned list is important for resolving placeholders in queries.

LANGUAGE: java
CODE:
public interface JdbcMapper  extends Serializable {
    List<Column> getColumns(ITuple tuple);
}

----------------------------------------

TITLE: Registering Metrics Consumers in YAML Configuration
DESCRIPTION: Example of registering multiple metrics consumers in the Storm YAML configuration file.

LANGUAGE: yaml
CODE:
topology.metrics.consumer.register:
  - class: "org.apache.storm.metric.LoggingMetricsConsumer"
    parallelism.hint: 1
  - class: "org.apache.storm.metric.HttpForwardingMetricsConsumer"
    parallelism.hint: 1
    argument: "http://example.com:8080/metrics/my-topology/"

----------------------------------------

TITLE: Creating Kerberos Principals and Keytabs
DESCRIPTION: Shell commands to create Kerberos principals and keytabs for Zookeeper, Nimbus, DRPC and UI components

LANGUAGE: bash
CODE:
# Zookeeper (Will need one of these for each box in the Zk ensemble)
sudo kadmin.local -q 'addprinc zookeeper/zk1.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/zk.keytab  zookeeper/zk1.example.com@STORM.EXAMPLE.COM"
# Nimbus and DRPC
sudo kadmin.local -q 'addprinc storm/storm.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm/storm.example.com@STORM.EXAMPLE.COM"
# All UI logviewer and Supervisors
sudo kadmin.local -q 'addprinc storm@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm@STORM.EXAMPLE.COM"

----------------------------------------

TITLE: Specifying DRPC Servers in Storm YAML
DESCRIPTION: This configuration lists the DRPC servers for the Storm cluster, allowing workers to locate them.

LANGUAGE: yaml
CODE:
drpc.servers: ["111.222.333.44"]

----------------------------------------

TITLE: Resetting Log Level Example
DESCRIPTION: Example of clearing a dynamic log level setting for the ROOT logger, reverting it to its original value using the Storm CLI.

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -r ROOT

----------------------------------------

TITLE: Refreshing Worker Connections in Apache Storm (Clojure)
DESCRIPTION: This snippet shows how worker connections are refreshed in Apache Storm. It manages connections to other workers and maintains a mapping from task to worker.

LANGUAGE: clojure
CODE:
refresh-connections

----------------------------------------

TITLE: Configuring HDFS Spout in Java
DESCRIPTION: Example of configuring an HDFS spout to read text files from HDFS.

LANGUAGE: java
CODE:
// Instantiate spout to read text files
HdfsSpout textReaderSpout = new HdfsSpout().setReaderType("text")
                                           .withOutputFields(TextFileReader.defaultFields)                                      
                                           .setHdfsUri("hdfs://localhost:54310")  // required
                                           .setSourceDir("/data/in")              // required                                      
                                           .setArchiveDir("/data/done")           // required
                                           .setBadFilesDir("/data/badfiles");     // required                                      
// If using Kerberos
HashMap hdfsSettings = new HashMap();
hdfsSettings.put("hdfs.keytab.file", "/path/to/keytab");
hdfsSettings.put("hdfs.kerberos.principal","user@EXAMPLE.com");

textReaderSpout.setHdfsClientSettings(hdfsSettings);

// Create topology
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("hdfsspout", textReaderSpout, SPOUT_NUM);

// Setup bolts and wire up topology
     ..snip..

// Submit topology with config
Config conf = new Config();
StormSubmitter.submitTopologyWithProgressBar("topologyName", conf, builder.createTopology());

----------------------------------------

TITLE: Setting Topology Scheduling Strategy in Java
DESCRIPTION: Java API call to set a custom scheduling strategy for a topology.

LANGUAGE: java
CODE:
public void setTopologyStrategy(Class<? extends IStrategy> clazz)

----------------------------------------

TITLE: Refreshing Worker Connections in Apache Storm
DESCRIPTION: The 'refresh-connections' function is called periodically to manage connections to other workers and maintain a mapping from task to worker.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L123

----------------------------------------

TITLE: Using Classpath Wildcards in Java 6+
DESCRIPTION: This snippet illustrates the use of classpath wildcards in Java 6 and later versions, which allows for shorter classpath specifications and avoids command length issues.

LANGUAGE: java
CODE:
foo/bar/*

----------------------------------------

TITLE: Configuring UI/Logviewer Filters in YAML
DESCRIPTION: YAML configuration for setting up authentication filters for Storm UI and logviewer components

LANGUAGE: yaml
CODE:
ui.filter: "filter.class"
ui.filter.params: "param1":"value1"
logviewer.filter: "filter.class"
logviewer.filter.params: "param1":"value1"

----------------------------------------

TITLE: Configuring File Loader in YAML
DESCRIPTION: Example configuration for the FileConfigLoader, specifying the local file path.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "file:///path/to/my/config.yaml"

----------------------------------------

TITLE: Defining MongoDB Mapper Interface
DESCRIPTION: Core interface definition for converting Storm tuples to MongoDB documents

LANGUAGE: java
CODE:
public interface MongoMapper extends Serializable {
    Document toDocument(ITuple tuple);
    Document toDocumentByKeys(List<Object> keys);
}

----------------------------------------

TITLE: Retrieving Cluster Configuration in JSON
DESCRIPTION: Example response from the /api/v1/cluster/configuration endpoint, which returns the cluster configuration as JSON.

LANGUAGE: json
CODE:
{
  "dev.zookeeper.path": "/tmp/dev-storm-zookeeper",
  "topology.tick.tuple.freq.secs": null,
  "topology.builtin.metrics.bucket.size.secs": 60,
  "topology.fall.back.on.java.serialization": false,
  "topology.max.error.report.per.interval": 5,
  "zmq.linger.millis": 5000,
  "topology.skip.missing.kryo.registrations": false,
  "storm.messaging.netty.client_worker_threads": 1,
  "ui.childopts": "-Xmx768m",
  "storm.zookeeper.session.timeout": 20000,
  "nimbus.reassign": true,
  "topology.trident.batch.emit.interval.millis": 500,
  "storm.messaging.netty.flush.check.interval.ms": 10,
  "nimbus.monitor.freq.secs": 10,
  "logviewer.childopts": "-Xmx128m",
  "java.library.path": "/usr/local/lib:/opt/local/lib:/usr/lib",
  "topology.executor.send.buffer.size": 1024
}

----------------------------------------

TITLE: Initializing RocketMqBolt for Writing to RocketMQ Topic in Java
DESCRIPTION: This code demonstrates how to create and configure a RocketMqBolt instance for writing data to a RocketMQ topic. It sets up a mapper, selector, and properties for the bolt.

LANGUAGE: java
CODE:
TupleToMessageMapper mapper = new FieldNameBasedTupleToMessageMapper("word", "count");
TopicSelector selector = new DefaultTopicSelector(topic);

properties = new Properties();
properties.setProperty(RocketMqConfig.NAME_SERVER_ADDR, nameserverAddr);

RocketMqBolt insertBolt = new RocketMqBolt()
        .withMapper(mapper)
        .withSelector(selector)
        .withProperties(properties);

----------------------------------------

TITLE: Configuring Health Check Settings
DESCRIPTION: YAML configuration for Storm's health check directory and timeout settings

LANGUAGE: yaml
CODE:
storm.health.check.dir: "healthchecks"
storm.health.check.timeout.ms: 5000

----------------------------------------

TITLE: Implementing RedisStoreMapper in Java for Word Count Storage
DESCRIPTION: This code defines a WordCountStoreMapper class that implements RedisStoreMapper. It's used to store word counts in a Redis hash. The class specifies how to map Storm tuples to Redis data for storage.

LANGUAGE: java
CODE:
class WordCountStoreMapper implements RedisStoreMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountStoreMapper() {
        description = new RedisDataTypeDescription(
            RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return tuple.getStringByField("count");
    }
}

----------------------------------------

TITLE: Storm Docker Configuration Example
DESCRIPTION: Sample configuration for enabling Docker support in Storm on RHEL7. Includes settings for resource isolation, allowed images, and cgroup setup.

LANGUAGE: bash
CODE:
storm.resource.isolation.plugin.enable: true
storm.resource.isolation.plugin: "org.apache.storm.container.docker.DockerManager"
storm.oci.allowed.images: ["xxx.xxx.com:8080/storm/storm/rhel7:latest"]
storm.oci.image: "xxx.xxx.com:8080/storm/storm/rhel7:latest"
storm.oci.cgroup.root: "/storm"
storm.oci.cgroup.parent: "/sys/fs/cgroup"
storm.oci.readonly.bindmounts:
    - "/etc/storm"
storm.oci.nscd.dir: "/var/run/nscd"
supervisor.worker.launcher: "/usr/share/apache-storm-2.3.0/bin/worker-launcher"

----------------------------------------

TITLE: Implementing Simple MongoDB Mapper
DESCRIPTION: General purpose implementation of MongoMapper that maps tuple fields directly to document fields

LANGUAGE: java
CODE:
public class SimpleMongoMapper implements MongoMapper {
    private String[] fields;

    @Override
    public Document toDocument(ITuple tuple) {
        Document document = new Document();
        for(String field : fields){
            document.append(field, tuple.getValueByField(field));
        }
        return document;
    }

    @Override
    public Document toDocumentByKeys(List<Object> keys) {
        Document document = new Document();
        document.append("_id", MongoUtils.getID(keys));
        return document;
    }

    public SimpleMongoMapper withFields(String... fields) {
        this.fields = fields;
        return this;
    }
}

----------------------------------------

TITLE: Running Storm Local Mode with Debug Configuration
DESCRIPTION: Example of launching a Storm topology in local mode with debug configuration enabled using command line parameters.

LANGUAGE: bash
CODE:
storm local topology.jar <MY_MAIN_CLASS> -c topology.debug=true

----------------------------------------

TITLE: Storm Configuration Loading Order
DESCRIPTION: Shows the preference order for how configuration values are loaded and overridden in Storm.

LANGUAGE: text
CODE:
defaults.yaml < storm.yaml < topology specific configuration < internal component specific configuration < external component specific configuration

----------------------------------------

TITLE: Configuring Kerberos Authentication for Storm UI in YAML
DESCRIPTION: Example YAML configuration for setting up Kerberos authentication for the Storm UI using the AuthenticationFilter from hadoop-auth.

LANGUAGE: yaml
CODE:
ui.filter: "org.apache.hadoop.security.authentication.server.AuthenticationFilter"
ui.filter.params:
   "type": "kerberos"
   "kerberos.principal": "HTTP/nimbus.witzend.com"
   "kerberos.keytab": "/vagrant/keytabs/http.keytab"
   "kerberos.name.rules": "RULE:[2:$1@$0]([jt]t@.*EXAMPLE.COM)s/.*/$MAPRED_USER/ RULE:[2:$1@$0]([nd]n@.*EXAMPLE.COM)s/.*/$HDFS_USER/DEFAULT"

----------------------------------------

TITLE: Synchronizing Supervisor with Assignments (Clojure)
DESCRIPTION: The synchronize-supervisor function in the Supervisor daemon synchronizes with Zookeeper assignments, downloads necessary code, and writes local assignments to the filesystem.

LANGUAGE: clojure
CODE:
(defn synchronize-supervisor [conf^String storm-id^String port^AssignmentInfo assignment-info])

----------------------------------------

TITLE: Implementing One Aggregator for Unique Counting in Trident
DESCRIPTION: Defines a One aggregator used in the URL reach computation to emit a single tuple containing the number one for each group, facilitating unique counting.

LANGUAGE: java
CODE:
public class One implements CombinerAggregator<Integer> {
   public Integer init(TridentTuple tuple) {
       return 1;
   }

   public Integer combine(Integer val1, Integer val2) {
       return 1;
   }

   public Integer zero() {
       return 1;
   }        
}

----------------------------------------

TITLE: Configuring Node Resources in Storm YAML
DESCRIPTION: Configuration to specify available CPU and memory resources on a Storm supervisor node.

LANGUAGE: yaml
CODE:
supervisor.memory.capacity.mb: 20480.0
supervisor.cpu.capacity: 100.0

----------------------------------------

TITLE: Implementing MongoDB Query Filter Creator - Java
DESCRIPTION: Implementation of QueryFilterCreator for creating MongoDB query filters using the $eq operator.

LANGUAGE: java
CODE:
public class SimpleQueryFilterCreator implements QueryFilterCreator {

    private String field;
    
    @Override
    public Bson createFilter(ITuple tuple) {
        return Filters.eq(field, tuple.getValueByField(field));
    }

    @Override
    public Bson createFilterByKeys(List<Object> keys) {
        return Filters.eq("_id", MongoUtils.getID(keys));
    }

    public SimpleQueryFilterCreator withField(String field) {
        this.field = field;
        return this;
    }
}

----------------------------------------

TITLE: Dumping Profiler in Storm UI REST API
DESCRIPTION: Example JSON response for the /api/v1/topology/:id/profiling/dumpprofile/:host-port endpoint, which requests to dump the profiler recording on a worker.

LANGUAGE: json
CODE:
{
   "status": "ok",
   "id": "10.11.1.7:6701"
}

----------------------------------------

TITLE: Configuring Artifactory Loader in YAML
DESCRIPTION: Example configuration for the ArtifactoryConfigLoader, specifying the URI and timeout settings.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "artifactory+http://artifactory.my.company.com:8000/artifactory/configurations/clusters/my_cluster/ras_pools"
scheduler.config.loader.timeout.sec: 30

----------------------------------------

TITLE: Implementing Transfer Function in Apache Storm Worker
DESCRIPTION: The worker provides a transfer function used by tasks to send tuples to other tasks. It serializes the tuple and puts it onto a transfer queue.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L56

----------------------------------------

TITLE: Configuring Storm Metricstore with YAML
DESCRIPTION: YAML configuration options for setting up the Storm Metricstore, including class implementations, RocksDB settings, and retention policies.

LANGUAGE: yaml
CODE:
storm.metricstore.class: "org.apache.storm.metricstore.rocksdb.RocksDbStore"
storm.metricprocessor.class: "org.apache.storm.metricstore.NimbusMetricProcessor"
storm.metricstore.rocksdb.location: "storm_rocks"
storm.metricstore.rocksdb.create_if_missing: true
storm.metricstore.rocksdb.metadata_string_cache_capacity: 4000
storm.metricstore.rocksdb.retention_hours: 240

----------------------------------------

TITLE: Storm Metrics Filter Interface
DESCRIPTION: Interface definition for creating custom metric filters to control which metrics get reported.

LANGUAGE: java
CODE:
public interface StormMetricsFilter extends MetricFilter {

    /**
     * Called after the filter is instantiated.
     * @param config A map of the properties from the 'filter' section of the reporter configuration.
     */
    void prepare(Map<String, Object> config);
    
   /**
    *  Returns true if the given metric should be reported.
    */
    boolean matches(String name, Metric metric);

}

----------------------------------------

TITLE: Trident Filter Implementation
DESCRIPTION: Example of a Trident Filter that checks specific integer values in tuple fields

LANGUAGE: java
CODE:
public class MyFilter extends BaseFilter {
    public boolean isKeep(TridentTuple tuple) {
        return tuple.getInteger(0) == 1 && tuple.getInteger(1) == 2;
    }
}

----------------------------------------

TITLE: Implementing Reach Calculation in Trident
DESCRIPTION: Shows how to compute the reach of a URL on demand using a DRPC topology. This example demonstrates querying multiple state sources and performing complex aggregations.

LANGUAGE: java
CODE:
TridentState urlToTweeters =
       topology.newStaticState(getUrlToTweetersState());
TridentState tweetersToFollowers =
       topology.newStaticState(getTweeterToFollowersState());

topology.newDRPCStream("reach")
       .stateQuery(urlToTweeters, new Fields("args"), new MapGet(), new Fields("tweeters"))
       .each(new Fields("tweeters"), new ExpandList(), new Fields("tweeter"))
       .shuffle()
       .stateQuery(tweetersToFollowers, new Fields("tweeter"), new MapGet(), new Fields("followers"))
       .parallelismHint(200)
       .each(new Fields("followers"), new ExpandList(), new Fields("follower"))
       .groupBy(new Fields("follower"))
       .aggregate(new One(), new Fields("one"))
       .parallelismHint(20)
       .aggregate(new Count(), new Fields("reach"));

----------------------------------------

TITLE: Defining IWindowedBolt Interface in Java
DESCRIPTION: Core interface that must be implemented by bolts requiring windowing support. Defines methods for preparation, execution and cleanup of windowed operations.

LANGUAGE: java
CODE:
public interface IWindowedBolt extends IComponent {
    void prepare(Map stormConf, TopologyContext context, OutputCollector collector);
    /**
     * Process tuples falling within the window and optionally emit 
     * new tuples based on the tuples in the input window.
     */
    void execute(TupleWindow inputWindow);
    void cleanup();
}

----------------------------------------

TITLE: Building Storm JMS Example
DESCRIPTION: Maven commands to build the storm-jms example project from source

LANGUAGE: bash
CODE:
$ cd storm-jms
$ mvn clean install

----------------------------------------

TITLE: Building Storm JMS Example
DESCRIPTION: Maven commands to build the storm-jms example project from source

LANGUAGE: bash
CODE:
$ cd storm-jms
$ mvn clean install

----------------------------------------

TITLE: Configuring RedisLookupBolt in Java
DESCRIPTION: This snippet demonstrates how to configure and create a RedisLookupBolt using a JedisPoolConfig and a RedisLookupMapper. It sets up the connection to Redis and defines how to look up data.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisLookupMapper lookupMapper = new WordCountRedisLookupMapper();
RedisLookupBolt lookupBolt = new RedisLookupBolt(poolConfig, lookupMapper);

----------------------------------------

TITLE: Creating CassandraWriterBolt with Bound Statement from Tuple Field
DESCRIPTION: Demonstrates how to create a CassandraWriterBolt using a bound statement loaded from a tuple field.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
     boundQuery(namedByField("cql"))
        .bind(all());

----------------------------------------

TITLE: Shared State Access Methods
DESCRIPTION: Methods for reading and writing shared state at different levels (task, executor, and worker) in Storm components.

LANGUAGE: java
CODE:
// Task Data Access
TopologyContext#setTaskData(String, Object)
TopologyContext#getTask(String)

// Executor Data Access
TopologyContext#setExecutorData
TopologyContext#getExecutorData(String)

// User Resources Access
WorkerUserContext#setResource(String, Object)
WorkerTopologyContext#getResouce(String)
TopologyContext#getResource(String)

----------------------------------------

TITLE: Spout Message Listening in Apache Storm (Clojure)
DESCRIPTION: Implementation of message listening for spouts in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L382

----------------------------------------

TITLE: Configuring Max Spout Pending in Storm
DESCRIPTION: Limits the number of unacknowledged tuples at the spout level, acting as a throttling mechanism that can affect throughput and latency.

LANGUAGE: yaml
CODE:
topology.max.spout.pending: 5000

----------------------------------------

TITLE: Specifying Classpath Wildcards in Java
DESCRIPTION: This snippet demonstrates the use of classpath wildcards in Java 6 and later versions to include all JAR files in a directory without explicitly listing them. This method is used by Storm to shorten classpath specifications and avoid breaching process command length limits.

LANGUAGE: java
CODE:
foo/bar/*

----------------------------------------

TITLE: Storm Metrics Reporter Configuration in YAML
DESCRIPTION: Example configuration for setting up Graphite and Console metric reporters in storm.yaml.

LANGUAGE: yaml
CODE:
storm.metrics.reporters:
  # Graphite Reporter
  - class: "org.apache.storm.metrics2.reporters.GraphiteStormReporter"
    daemons:
        - "supervisor"
        - "nimbus"
        - "worker"
    report.period: 60
    report.period.units: "SECONDS"
    graphite.host: "localhost"
    graphite.port: 2003

  # Console Reporter
  - class: "org.apache.storm.metrics2.reporters.ConsoleStormReporter"
    daemons:
        - "worker"
    report.period: 10
    report.period.units: "SECONDS"
    filter:
        class: "org.apache.storm.metrics2.filters.RegexFilter"
        expression: ".*my_component.*emitted.*"

----------------------------------------

TITLE: Custom Metric Implementation Example
DESCRIPTION: Example showing how to implement and register a custom CountMetric in a bolt to track execution count.

LANGUAGE: java
CODE:
private transient CountMetric countMetric;

LANGUAGE: java
CODE:
@Override
public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
	// other initialization here.
	countMetric = new CountMetric();
	context.registerMetric("execute_count", countMetric, 60);
}

LANGUAGE: java
CODE:
public void execute(Tuple input) {
	countMetric.incr();
	// handle tuple here.	
}

----------------------------------------

TITLE: Implementing SimpleQueryFilterCreator for Storm-MongoDB Integration
DESCRIPTION: Implements the SimpleQueryFilterCreator class, which creates MongoDB query filters using the $eq operator for matching values equal to a specified value.

LANGUAGE: java
CODE:
public class SimpleQueryFilterCreator implements QueryFilterCreator {

    private String field;
    
    @Override
    public Bson createFilter(ITuple tuple) {
        return Filters.eq(field, tuple.getValueByField(field));
    }

    @Override
    public Bson createFilterByKeys(List<Object> keys) {
        return Filters.eq("_id", MongoUtils.getID(keys));
    }

    public SimpleQueryFilterCreator withField(String field) {
        this.field = field;
        return this;
    }

}

----------------------------------------

TITLE: Configuring Artifactory Loader in YAML
DESCRIPTION: Example configuration for the ArtifactoryConfigLoader, specifying the URI and timeout.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "artifactory+http://artifactory.my.company.com:8000/artifactory/configurations/clusters/my_cluster/ras_pools"
scheduler.config.loader.timeout.sec: 30

----------------------------------------

TITLE: Registering Metrics Consumer in YAML
DESCRIPTION: Shows how to register metrics consumers in the Storm configuration YAML file with parallelism hints and arguments.

LANGUAGE: yaml
CODE:
topology.metrics.consumer.register:
  - class: "org.apache.storm.metric.LoggingMetricsConsumer"
    parallelism.hint: 1
  - class: "org.apache.storm.metric.HttpForwardingMetricsConsumer"
    parallelism.hint: 1
    argument: "http://example.com:8080/metrics/my-topology/"

----------------------------------------

TITLE: Debugging Storm Topology in IDE
DESCRIPTION: Shows how to configure local mode override for debugging Storm topologies in an IDE environment using LocalCluster.withLocalModeOverride.

LANGUAGE: java
CODE:
public static void main(final String args[]) {
    LocalCluster.withLocalModeOverride(() -> originalMain(args), 10);
}

----------------------------------------

TITLE: Running Storm SQL in Explain Mode
DESCRIPTION: Demonstrates how to use the explain mode to show the query plan for Storm SQL statements instead of submitting the topology.

LANGUAGE: bash
CODE:
$ bin/storm sql order_filtering.sql --explain --artifacts "org.apache.storm:storm-sql-kafka:2.0.0-SNAPSHOT,org.apache.storm:storm-kafka-client:2.0.0-SNAPSHOT,org.apache.kafka:kafka-clients:1.1.0^org.slf4j:slf4j-log4j12"

----------------------------------------

TITLE: Storm Metrics Filter Interface
DESCRIPTION: Interface definition for creating custom metric filters in Storm. Includes methods for preparation and metric matching.

LANGUAGE: java
CODE:
public interface StormMetricsFilter extends MetricFilter {

    /**
     * Called after the filter is instantiated.
     * @param config A map of the properties from the 'filter' section of the reporter configuration.
     */
    void prepare(Map<String, Object> config);
    
   /**
    *  Returns true if the given metric should be reported.
    */
    boolean matches(String name, Metric metric);

}

----------------------------------------

TITLE: Registering Event Logger in YAML Configuration
DESCRIPTION: Example of registering multiple event logger implementations including custom loggers with arguments in Storm YAML configuration.

LANGUAGE: yaml
CODE:
topology.event.logger.register:
  - class: "org.apache.storm.metric.FileBasedEventLogger"
  - class: "org.mycompany.MyEventLogger"
    arguments:
      endpoint: "event-logger.mycompany.org"

----------------------------------------

TITLE: Setting Storm Local Directory Configuration
DESCRIPTION: Configuration for specifying the local storage directory path for Storm daemons. Shows both Unix and Windows path examples.

LANGUAGE: yaml
CODE:
storm.local.dir: "/mnt/storm"

LANGUAGE: yaml
CODE:
storm.local.dir: "C:\\storm-local"

----------------------------------------

TITLE: Storm DRPC Metrics Table
DESCRIPTION: Markdown table listing DRPC server metrics with names, types and descriptions

LANGUAGE: markdown
CODE:
| Metric Name | Type | Description |
|-------------|------|-------------|
| drpc:HTTP-request-response-duration | timer | how long it takes to execute... |

----------------------------------------

TITLE: Configuring RedisStoreBolt in Java
DESCRIPTION: Java code snippet showing how to configure and create a RedisStoreBolt instance.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
                .setHost(host).setPort(port).build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisStoreBolt storeBolt = new RedisStoreBolt(poolConfig, storeMapper);

----------------------------------------

TITLE: Adding Storm Client Dependency in Maven POM
DESCRIPTION: This XML snippet shows how to include the Storm client library as a provided dependency in a Maven project. It specifies the groupId, artifactId, version, and scope for the Storm client dependency.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.storm</groupId>
  <artifactId>storm-client</artifactId>
  <version>{{page.version}}</version>
  <scope>provided</scope>
</dependency>

----------------------------------------

TITLE: Implementing RedisLookupMapper for WordCount in Java
DESCRIPTION: Java class implementing RedisLookupMapper interface for word count lookup operations in Redis.

LANGUAGE: java
CODE:
class WordCountRedisLookupMapper implements RedisLookupMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountRedisLookupMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public List<Values> toTuple(ITuple input, Object value) {
        String member = getKeyFromTuple(input);
        List<Values> values = Lists.newArrayList();
        values.add(new Values(member, value));
        return values;
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("wordName", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Configuring Nimbus Seeds in Storm YAML
DESCRIPTION: This snippet shows how to configure the Nimbus seeds, which are the machines that can act as the master for worker nodes to download topology jars and configurations.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["111.222.333.44"]

----------------------------------------

TITLE: Implementing Local DRPC in Storm
DESCRIPTION: Demonstrates how to set up and use DRPC (Distributed Remote Procedure Call) in local mode. Creates LocalDRPC and LocalCluster instances to simulate DRPC server and Storm cluster in-process.

LANGUAGE: java
CODE:
try (LocalDRPC drpc = new LocalDRPC();
     LocalCluster cluster = new LocalCluster();
     LocalTopology topo = cluster.submitTopology("drpc-demo", conf, builder.createLocalTopology(drpc))) {

    System.out.println("Results for 'hello':" + drpc.execute("exclamation", "hello"));
}

----------------------------------------

TITLE: Configuring Elasticsearch Connection in Storm
DESCRIPTION: Creates EsConfig instances with cluster configuration and optional additional parameters for Elasticsearch connection.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});

LANGUAGE: java
CODE:
Map<String, String> additionalParameters = new HashMap<>();
additionalParameters.put("client.transport.sniff", "true");
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"}, additionalParameters);

----------------------------------------

TITLE: Defining Thrift Structures for Cluster and Nimbus Information
DESCRIPTION: Thrift structures for representing cluster summary information, including Nimbus details, to be used in API responses.

LANGUAGE: thrift
CODE:
struct ClusterSummary {
  1: required list<SupervisorSummary> supervisors;
  3: required list<TopologySummary> topologies;
  4: required list<NimbusSummary> nimbuses;
}

struct NimbusSummary {
  1: required string host;
  2: required i32 port;
  3: required i32 uptime_secs;
  4: required bool isLeader;
  5: required string version;
}

----------------------------------------

TITLE: Maven Shade Plugin Configuration
DESCRIPTION: XML configuration for Maven Shade Plugin to build an uber jar containing all dependencies.

LANGUAGE: xml
CODE:
<plugin>
     <groupId>org.apache.maven.plugins</groupId>
     <artifactId>maven-shade-plugin</artifactId>
     <version>2.4.1</version>
     <executions>
         <execution>
             <phase>package</phase>
             <goals>
                 <goal>shade</goal>
             </goals>
             <configuration>
                 <transformers>
                     <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                         <mainClass>org.apache.storm.solr.topology.SolrJsonTopology</mainClass>
                     </transformer>
                 </transformers>
             </configuration>
         </execution>
     </executions>
</plugin>

----------------------------------------

TITLE: Creating Kafka Topics for Apache Logs
DESCRIPTION: Bash commands to create Kafka topics for storing different types of Apache logs.

LANGUAGE: bash
CODE:
kafka-topics --create --topic apache-logs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
kafka-topics --create --topic apache-errorlogs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
kafka-topics --create --topic apache-slowlogs --zookeeper localhost:2181 --replication-factor 1 --partitions 5

----------------------------------------

TITLE: Filtering Slow Logs with Storm SQL and UDF
DESCRIPTION: Storm SQL query to filter slow logs (response time >= 100ms) and convert timestamp using a custom UDF.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://apache-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
CREATE EXTERNAL TABLE APACHE_SLOW_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_RECEIVED_TIMESTAMP BIGINT, TIME_ELAPSED_MS INT) LOCATION 'kafka://apache-slow-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
CREATE FUNCTION GET_TIME AS 'org.apache.storm.sql.runtime.functions.scalar.datetime.GetTime2'
INSERT INTO APACHE_SLOW_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, GET_TIME(TIME_RECEIVED_UTC_ISOFORMAT, 'yyyy-MM-dd''T''HH:mm:ssZZ') AS TIME_RECEIVED_TIMESTAMP, TIME_US / 1000 AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (TIME_US / 1000) >= 100

----------------------------------------

TITLE: Implementing a Batch Count Bolt
DESCRIPTION: Example implementation of a BatchBolt that processes batches of tuples to compute partial counts.

LANGUAGE: java
CODE:
public static class BatchCount extends BaseBatchBolt {
    Object _id;
    BatchOutputCollector _collector;

    int _count = 0;

    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
        _collector = collector;
        _id = id;
    }

    @Override
    public void execute(Tuple tuple) {
        _count++;
    }

    @Override
    public void finishBatch() {
        _collector.emit(new Values(_id, _count));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "count"));
    }
}

----------------------------------------

TITLE: Implementing TupleToKafkaMapper Interface in Java
DESCRIPTION: This code snippet shows the interface methods required to implement TupleToKafkaMapper and TridentTupleToKafkaMapper for mapping Storm tuples to Kafka keys and messages.

LANGUAGE: java
CODE:
K getKeyFromTuple(Tuple/TridentTuple tuple);
V getMessageFromTuple(Tuple/TridentTuple tuple);

----------------------------------------

TITLE: Registering Metrics Consumers in YAML Configuration
DESCRIPTION: Demonstrates how to register multiple metrics consumers in the Storm YAML configuration file.

LANGUAGE: yaml
CODE:
topology.metrics.consumer.register:
  - class: "org.apache.storm.metric.LoggingMetricsConsumer"
    parallelism.hint: 1
  - class: "org.apache.storm.metric.HttpForwardingMetricsConsumer"
    parallelism.hint: 1
    argument: "http://example.com:8080/metrics/my-topology/"

----------------------------------------

TITLE: Defining Cluster Metrics Table in Markdown
DESCRIPTION: Markdown table defining cluster-wide metrics reported by Nimbus, including metric names, types, and descriptions.

LANGUAGE: markdown
CODE:
| Metric Name | Type | Description |
|-------------|------|-------------|
| cluster:num-nimbus-leaders | gauge | Number of nimbuses marked as a leader. This should really only ever be 1 in a healthy cluster, or 0 for a short period of time while a failover happens. |
| cluster:num-nimbuses | gauge | Number of nimbuses, leader or standby. |
| cluster:num-supervisors | gauge | Number of supervisors. |
| cluster:num-topologies | gauge | Number of topologies. |
| cluster:num-total-used-workers | gauge | Number of used workers/slots. |
| cluster:num-total-workers | gauge | Number of workers/slots. |
| cluster:total-fragmented-cpu-non-negative | gauge | Total fragmented CPU (% of core).  This is CPU that the system thinks it cannot use because other resources on the node are used up. |
| cluster:total-fragmented-memory-non-negative | gauge | Total fragmented memory (MB).  This is memory that the system thinks it cannot use because other resources on the node are used up.  |

----------------------------------------

TITLE: Defining a Spout in Clojure
DESCRIPTION: Example of defining a spout that emits random sentences using the defspout macro.

LANGUAGE: clojure
CODE:
(defspout sentence-spout ["sentence"]
  [conf context collector]
  (let [sentences ["a little brown dog"
                   "the man petted the dog"
                   "four score and seven years ago"
                   "an apple a day keeps the doctor away"]]
    (spout
     (nextTuple []
       (Thread/sleep 100)
       (emit-spout! collector [(rand-nth sentences)])         
       )
     (ack [id]
        ;; You only need to define this method for reliable spouts
        ;; (such as one that reads off of a queue like Kestrel)
        ;; This is an unreliable spout, so it does nothing here
        ))))

----------------------------------------

TITLE: Retrieving Topology Errors
DESCRIPTION: Fetches the latest errors from a running topology in JSON format.

LANGUAGE: shell
CODE:
storm get-errors topology-name

----------------------------------------

TITLE: Debugging Storm Topology in IDE
DESCRIPTION: Shows how to configure local mode override for debugging Storm topologies in an IDE environment using LocalCluster.withLocalModeOverride.

LANGUAGE: java
CODE:
public static void main(final String args[]) {
    LocalCluster.withLocalModeOverride(() -> originalMain(args), 10);
}

----------------------------------------

TITLE: Implementing Storm Trident State for Druid
DESCRIPTION: Example demonstrating Trident state implementation for Druid integration using DruidBeamFactory and event mapper with stream processing configuration.

LANGUAGE: java
CODE:
    DruidBeamFactory druidBeamFactory = new SampleDruidBeamFactoryImpl(new HashMap<String, Object>());
    ITupleDruidEventMapper<Map<String, Object>> eventMapper = new TupleDruidEventMapper<>(TupleDruidEventMapper.DEFAULT_FIELD_NAME);

    final Stream stream = tridentTopology.newStream("batch-event-gen", new SimpleBatchSpout(10));

    stream.peek(new Consumer() {
        @Override
        public void accept(TridentTuple input) {
             LOG.info("########### Received tuple: [{}]", input);
         }
    }).partitionPersist(new DruidBeamStateFactory<Map<String, Object>>(druidBeamFactory, eventMapper), new Fields("event"), new DruidBeamStateUpdater());

----------------------------------------

TITLE: Configuring Kerberos Authentication for Storm UI
DESCRIPTION: Example configuration for using Kerberos authentication with the Storm UI

LANGUAGE: yaml
CODE:
ui.filter: "org.apache.hadoop.security.authentication.server.AuthenticationFilter"
ui.filter.params:
   "type": "kerberos"
   "kerberos.principal": "HTTP/nimbus.witzend.com"
   "kerberos.keytab": "/vagrant/keytabs/http.keytab"
   "kerberos.name.rules": "RULE:[2:$1@$0]([jt]t@.*EXAMPLE.COM)s/.*/$MAPRED_USER/ RULE:[2:$1@$0]([nd]n@.*EXAMPLE.COM)s/.*/$HDFS_USER/DEFAULT"

----------------------------------------

TITLE: Configuring RedisLookupBolt in Java
DESCRIPTION: Java code snippet showing how to configure and create a RedisLookupBolt instance.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisLookupMapper lookupMapper = new WordCountRedisLookupMapper();
RedisLookupBolt lookupBolt = new RedisLookupBolt(poolConfig, lookupMapper);

----------------------------------------

TITLE: Running Storm SQL Command
DESCRIPTION: Command to compile SQL statements into Storm topology and submit to Storm cluster

LANGUAGE: bash
CODE:
$ bin/storm sql <sql-file> <topo-name>

----------------------------------------

TITLE: Basic Storm Topology Configuration
DESCRIPTION: Example showing how to set up a basic topology with spouts and bolts using the TopologyBuilder.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();        
builder.setSpout("words", new TestWordSpout(), 10);        
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
        .shuffleGrouping("words");
builder.setBolt("exclaim2", new ExclamationBolt(), 2)
        .shuffleGrouping("exclaim1");

----------------------------------------

TITLE: Configuring ExpandUrl Bolt with Shuffle Grouping in Storm
DESCRIPTION: This snippet demonstrates how to set up an ExpandUrl bolt with shuffle grouping in a Storm topology. This configuration distributes tuples randomly across bolt tasks.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .shuffleGrouping(1);

----------------------------------------

TITLE: Configuring Resource Aware Scheduler Priority Strategy
DESCRIPTION: YAML configuration to set the priority strategy class used by the Resource Aware Scheduler.

LANGUAGE: yaml
CODE:
resource.aware.scheduler.priority.strategy: "org.apache.storm.scheduler.resource.strategies.priority.DefaultSchedulingPriorityStrategy"

----------------------------------------

TITLE: Reading a Blob in Java
DESCRIPTION: Java code demonstrating how to read the contents of a blob from the Storm blobstore.

LANGUAGE: java
CODE:
String blobKey = "some_key";
InputStreamWithMeta blobInputStream = clientBlobStore.getBlob(blobKey);
BufferedReader r = new BufferedReader(new InputStreamReader(blobInputStream));
String blobContents =  r.readLine();

----------------------------------------

TITLE: Setting Storm Local Directory Configuration
DESCRIPTION: Configuration for specifying the local storage directory path for Storm daemons. Shows both Unix and Windows path examples.

LANGUAGE: yaml
CODE:
storm.local.dir: "/mnt/storm"

LANGUAGE: yaml
CODE:
storm.local.dir: "C:\\storm-local"

----------------------------------------

TITLE: IConfigLoader Interface Definition
DESCRIPTION: Core interface definition for config loaders, containing a single load() method that returns a configuration map.

LANGUAGE: java
CODE:
public interface IConfigLoader {
    Map<?,?> load();
};

----------------------------------------

TITLE: Implementing SplitSentence as BasicBolt
DESCRIPTION: Simplified bolt implementation using BasicBolt interface for automatic anchoring and acking

LANGUAGE: java
CODE:
public class SplitSentence extends BaseBasicBolt {
        public void execute(Tuple tuple, BasicOutputCollector collector) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                collector.emit(new Values(word));
            }
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Implementing a ReducerAggregator in Trident
DESCRIPTION: Example of implementing a Count ReducerAggregator.

LANGUAGE: java
CODE:
public class Count implements ReducerAggregator<Long> {
    public Long init() {
        return 0L;
    }
    
    public Long reduce(Long curr, TridentTuple tuple) {
        return curr + 1;
    }
}

----------------------------------------

TITLE: Configuring Supervisor Resources in YAML
DESCRIPTION: YAML configuration syntax for specifying available resources on Storm supervisor nodes in storm.yaml file.

LANGUAGE: yaml
CODE:
supervisor.resources.map: {[type<String>] : [amount<Double>]}

----------------------------------------

TITLE: Setting up MongoTridentState for Storm-Trident MongoDB Integration in Java
DESCRIPTION: Shows how to create a MongoDB persistent state for Trident topologies. This allows Trident to persist data in MongoDB collections. It requires configuration options including MongoDB URL, collection name, and a MongoMapper.

LANGUAGE: java
CODE:
MongoMapper mapper = new SimpleMongoMapper()
        .withFields("word", "count");

MongoState.Options options = new MongoState.Options()
        .withUrl(url)
        .withCollectionName(collectionName)
        .withMapper(mapper);

StateFactory factory = new MongoStateFactory(options);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory, fields,  new MongoStateUpdater(), new Fields());

----------------------------------------

TITLE: Configuring Zookeeper Servers in Storm YAML
DESCRIPTION: This snippet shows how to configure the Zookeeper servers for a Storm cluster in the storm.yaml configuration file. It specifies the IP addresses of the Zookeeper hosts.

LANGUAGE: yaml
CODE:
storm.zookeeper.servers:
  - "111.222.333.444"
  - "555.666.777.888"

----------------------------------------

TITLE: Redis Filter Mapper Implementation
DESCRIPTION: Implementation of RedisFilterMapper interface for filtering words using Redis Set data type.

LANGUAGE: java
CODE:
class BlacklistWordFilterMapper implements RedisFilterMapper {
    private RedisDataTypeDescription description;
    private final String setKey = "blacklist";

    public BlacklistWordFilterMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.SET, setKey);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Implementing Custom Redis Bolt in Java
DESCRIPTION: Java class extending AbstractRedisBolt to implement custom Redis lookup logic for word count totals.

LANGUAGE: java
CODE:
public static class LookupWordTotalCountBolt extends AbstractRedisBolt {
    private static final Logger LOG = LoggerFactory.getLogger(LookupWordTotalCountBolt.class);
    private static final Random RANDOM = new Random();

    public LookupWordTotalCountBolt(JedisPoolConfig config) {
        super(config);
    }

    public LookupWordTotalCountBolt(JedisClusterConfig config) {
        super(config);
    }

    @Override
    public void execute(Tuple input) {
        JedisCommands jedisCommands = null;
        try {
            jedisCommands = getInstance();
            String wordName = input.getStringByField("word");
            String countStr = jedisCommands.get(wordName);
            if (countStr != null) {
                int count = Integer.parseInt(countStr);
                this.collector.emit(new Values(wordName, count));

                // print lookup result with low probability
                if(RANDOM.nextInt(1000) > 995) {
                    LOG.info("Lookup result - word : " + wordName + " / count : " + count);
                }
            } else {
                // skip
                LOG.warn("Word not found in Redis - word : " + wordName);
            }
        } finally {
            if (jedisCommands != null) {
                returnInstance(jedisCommands);
            }
            this.collector.ack(input);
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        // wordName, count
        declarer.declare(new Fields("wordName", "count"));
    }
}

----------------------------------------

TITLE: SQL Join Example for Storm Core Stream Joining
DESCRIPTION: An SQL example demonstrating a join involving 4 tables, which serves as a reference for the equivalent Storm Core join implementation.

LANGUAGE: sql
CODE:
select  userId, key4, key2, key3
from        table1
inner join  table2  on table2.userId =  table1.key1
inner join  table3  on table3.key3   =  table2.userId
left join   table4  on table4.key4   =  table3.key3

----------------------------------------

TITLE: Bolt Message Listening in Apache Storm (Clojure)
DESCRIPTION: Implementation of message listening for bolts in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L489

----------------------------------------

TITLE: Adding Resource Usage Profiling to Storm Topology
DESCRIPTION: Configuration to enable logging of resource usage metrics for a Storm topology.

LANGUAGE: java
CODE:
//Log all storm metrics
conf.registerMetricsConsumer(backtype.storm.metric.LoggingMetricsConsumer.class);

//Add in per worker CPU measurement
Map<String, String> workerMetrics = new HashMap<String, String>();
workerMetrics.put("CPU", "org.apache.storm.metrics.sigar.CPUMetric");
conf.put(Config.TOPOLOGY_WORKER_METRICS, workerMetrics);

----------------------------------------

TITLE: Joining Named Streams from Multiple Spouts in Storm
DESCRIPTION: Java code demonstrating how to join two named streams from four different spouts using JoinBolt in Apache Storm.

LANGUAGE: java
CODE:
new JoinBolt(JoinBolt.Selector.STREAM,  "stream1", "key1") 
                             .join     ("stream2", "userId",  "stream1" )
                             .select ("userId, key1, key2")
                             .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;
                             
topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("bolt1", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt2", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt3", "stream2", new Fields("userId") )
            .fieldsGrouping("bolt4", "stream1", new Fields("key1") );

----------------------------------------

TITLE: File Loader Configuration Example
DESCRIPTION: Example configuration for loading from a local file using the file scheme.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "file:///path/to/my/config.yaml"

----------------------------------------

TITLE: Sample Storm Supervisor Summary Response
DESCRIPTION: Example JSON response showing supervisor summary information returned by the /api/v1/supervisor/summary endpoint

LANGUAGE: json
CODE:
{
    "supervisors": [
        {
            "id": "0b879808-2a26-442b-8f7d-23101e0c3696",
            "host": "10.11.1.7",
            "uptime": "5m 58s",
            "uptimeSeconds": 358,
            "slotsTotal": 4,
            "slotsUsed": 3
        }
    ],
    "schedulerDisplayResource": true
}

----------------------------------------

TITLE: Task Message Listening in Apache Storm (Clojure)
DESCRIPTION: Tasks listen on an in-memory ZeroMQ port for messages from the virtual port.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L201

----------------------------------------

TITLE: Initializing EsIndexBolt for Storm-Elasticsearch Integration
DESCRIPTION: Creates an EsIndexBolt instance to stream tuples directly into Elasticsearch using cluster configuration and tuple mapping.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsIndexBolt indexBolt = new EsIndexBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Registering Metrics Consumer in Java
DESCRIPTION: Shows how to register a metrics consumer to a topology using Java configuration.

LANGUAGE: java
CODE:
conf.registerMetricsConsumer(org.apache.storm.metric.LoggingMetricsConsumer.class, 1);

----------------------------------------

TITLE: Initializing EsLookupBolt for Elasticsearch Queries
DESCRIPTION: Creates an EsLookupBolt instance to perform get requests to Elasticsearch with custom request and output adapters.

LANGUAGE: java
CODE:
EsConfig esConfig = createEsConfig();
ElasticsearchGetRequest getRequestAdapter = createElasticsearchGetRequest();
EsLookupResultOutput output = createOutput();
EsLookupBolt lookupBolt = new EsLookupBolt(esConfig, getRequestAdapter, output);

----------------------------------------

TITLE: Configuring Health Check Directory in Storm YAML
DESCRIPTION: This configuration sets the directory for health check scripts. These scripts are used to monitor the health of supervisor nodes.

LANGUAGE: yaml
CODE:
storm.health.check.dir: "healthchecks"

----------------------------------------

TITLE: Executing Storm SQL Command
DESCRIPTION: Basic command to compile SQL statements into Storm topology and submit to cluster

LANGUAGE: bash
CODE:
$ bin/storm sql <sql-file> <topo-name>

----------------------------------------

TITLE: Implementing Worker Hooks in Storm
DESCRIPTION: Worker hooks are executed during worker startup before any bolts or spouts are initialized. They are created by extending BaseWorkerHook and registered using TopologyBuilder.

LANGUAGE: java
CODE:
TopologyBuilder.addWorkerHook()  // Register worker-level hook

----------------------------------------

TITLE: Storm JoinBolt Implementation
DESCRIPTION: Java implementation showing how to join 4 streams using JoinBolt with field grouping and window configuration.

LANGUAGE: java
CODE:
JoinBolt jbolt =  new JoinBolt("spout1", "key1")                   // from        spout1  
                    .join     ("spout2", "userId",  "spout1")      // inner join  spout2  on spout2.userId = spout1.key1
                    .join     ("spout3", "key3",    "spout2")      // inner join  spout3  on spout3.key3   = spout2.userId   
                    .leftJoin ("spout4", "key4",    "spout3")      // left join   spout4  on spout4.key4   = spout3.key3
                    .select  ("userId, key4, key2, spout3:key3")   // chose output fields
                    .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;

topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("spout1", new Fields("key1") )
            .fieldsGrouping("spout2", new Fields("userId") )
            .fieldsGrouping("spout3", new Fields("key3") )
            .fieldsGrouping("spout4", new Fields("key4") );

----------------------------------------

TITLE: Setting Storm Local Directory in YAML
DESCRIPTION: This configuration sets the local directory for Storm to store small amounts of state like jars and configs. It shows examples for both Unix and Windows systems.

LANGUAGE: yaml
CODE:
storm.local.dir: "/mnt/storm"

LANGUAGE: yaml
CODE:
storm.local.dir: "C:\\storm-local"

----------------------------------------

TITLE: Initializing EsLookupBolt for Elasticsearch Queries
DESCRIPTION: Creates an EsLookupBolt instance to perform get requests to Elasticsearch with custom request and output adapters.

LANGUAGE: java
CODE:
EsConfig esConfig = createEsConfig();
ElasticsearchGetRequest getRequestAdapter = createElasticsearchGetRequest();
EsLookupResultOutput output = createOutput();
EsLookupBolt lookupBolt = new EsLookupBolt(esConfig, getRequestAdapter, output);

----------------------------------------

TITLE: Implementing Custom Redis Bolt in Java
DESCRIPTION: Java class extending AbstractRedisBolt to implement custom Redis operations for word count lookup.

LANGUAGE: java
CODE:
public static class LookupWordTotalCountBolt extends AbstractRedisBolt {
    private static final Logger LOG = LoggerFactory.getLogger(LookupWordTotalCountBolt.class);
    private static final Random RANDOM = new Random();

    public LookupWordTotalCountBolt(JedisPoolConfig config) {
        super(config);
    }

    public LookupWordTotalCountBolt(JedisClusterConfig config) {
        super(config);
    }

    @Override
    public void execute(Tuple input) {
        JedisCommands jedisCommands = null;
        try {
            jedisCommands = getInstance();
            String wordName = input.getStringByField("word");
            String countStr = jedisCommands.get(wordName);
            if (countStr != null) {
                int count = Integer.parseInt(countStr);
                this.collector.emit(new Values(wordName, count));

                // print lookup result with low probability
                if(RANDOM.nextInt(1000) > 995) {
                    LOG.info("Lookup result - word : " + wordName + " / count : " + count);
                }
            } else {
                // skip
                LOG.warn("Word not found in Redis - word : " + wordName);
            }
        } finally {
            if (jedisCommands != null) {
                returnInstance(jedisCommands);
            }
            this.collector.ack(input);
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        // wordName, count
        declarer.declare(new Fields("wordName", "count"));
    }
}

----------------------------------------

TITLE: Configuring KafkaTridentSpout in Trident Topology
DESCRIPTION: Example of creating a KafkaTridentSpout to consume from Kafka topics in a Trident topology

LANGUAGE: java
CODE:
final TridentTopology tridentTopology = new TridentTopology();
final Stream spoutStream = tridentTopology.newStream("kafkaSpout",
    new KafkaTridentSpoutOpaque<>(KafkaSpoutConfig.builder("127.0.0.1:" + port, Pattern.compile("topic.*")).build()))
      .parallelismHint(1)

----------------------------------------

TITLE: Implementing Custom Redis Bolt in Java
DESCRIPTION: Java class extending AbstractRedisBolt to implement custom Redis operations for word count lookup.

LANGUAGE: java
CODE:
public static class LookupWordTotalCountBolt extends AbstractRedisBolt {
    private static final Logger LOG = LoggerFactory.getLogger(LookupWordTotalCountBolt.class);
    private static final Random RANDOM = new Random();

    public LookupWordTotalCountBolt(JedisPoolConfig config) {
        super(config);
    }

    public LookupWordTotalCountBolt(JedisClusterConfig config) {
        super(config);
    }

    @Override
    public void execute(Tuple input) {
        JedisCommands jedisCommands = null;
        try {
            jedisCommands = getInstance();
            String wordName = input.getStringByField("word");
            String countStr = jedisCommands.get(wordName);
            if (countStr != null) {
                int count = Integer.parseInt(countStr);
                this.collector.emit(new Values(wordName, count));

                // print lookup result with low probability
                if(RANDOM.nextInt(1000) > 995) {
                    LOG.info("Lookup result - word : " + wordName + " / count : " + count);
                }
            } else {
                // skip
                LOG.warn("Word not found in Redis - word : " + wordName);
            }
        } finally {
            if (jedisCommands != null) {
                returnInstance(jedisCommands);
            }
            this.collector.ack(input);
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        // wordName, count
        declarer.declare(new Fields("wordName", "count"));
    }
}

----------------------------------------

TITLE: Accessing User Resources in Storm Worker
DESCRIPTION: Methods for reading and writing worker-level data shared across executors, tasks, worker hooks and task hooks within a worker.

LANGUAGE: java
CODE:
WorkerUserContext#setResource(String, Object)     // write access
WorkerTopologyContext#getResouce(String)         // read access
TopologyContext#getResource(String)              // read access

----------------------------------------

TITLE: Configuring Health Check Directory in Storm YAML
DESCRIPTION: This configuration sets the directory for health check scripts. These scripts are used to monitor the health of supervisor nodes.

LANGUAGE: yaml
CODE:
storm.health.check.dir: "healthchecks"

----------------------------------------

TITLE: Adding Items to Kestrel Queue in Java
DESCRIPTION: This method adds randomly selected sentences to a Kestrel queue using KestrelClient. It demonstrates how to queue items with unique identifiers.

LANGUAGE: Java
CODE:
    private static void queueSentenceItems(KestrelClient kestrelClient, String queueName)
			throws ParseError, IOException {

		String[] sentences = new String[] {
	            "the cow jumped over the moon",
	            "an apple a day keeps the doctor away",
	            "four score and seven years ago",
	            "snow white and the seven dwarfs",
	            "i am at two with nature"};

		Random _rand = new Random();

		for(int i=1; i<=10; i++){

			String sentence = sentences[_rand.nextInt(sentences.length)];

			String val = "ID " + i + " " + sentence;

			boolean queueSucess = kestrelClient.queue(queueName, val);

			System.out.println("queueSucess=" +queueSucess+ " [" + val +"]");
		}
	}

----------------------------------------

TITLE: Implementing Simple Query Filter Creator
DESCRIPTION: Implementation for creating MongoDB query filters based on tuple fields

LANGUAGE: java
CODE:
public class SimpleQueryFilterCreator implements QueryFilterCreator {

    private String field;
    
    @Override
    public Bson createFilter(ITuple tuple) {
        return Filters.eq(field, tuple.getValueByField(field));
    }

    @Override
    public Bson createFilterByKeys(List<Object> keys) {
        return Filters.eq("_id", MongoUtils.getID(keys));
    }

    public SimpleQueryFilterCreator withField(String field) {
        this.field = field;
        return this;
    }
}

----------------------------------------

TITLE: Storm Topology with KestrelSpout
DESCRIPTION: Storm topology configuration that reads from Kestrel queue, splits sentences into words, and counts word occurrences. Uses KestrelSpout for queue consumption.

LANGUAGE: java
CODE:
    TopologyBuilder builder = new TopologyBuilder();
    builder.setSpout("sentences", new KestrelSpout("localhost",22133,"sentence_queue",new StringScheme()));
    builder.setBolt("split", new SplitSentence(), 10)
    	        .shuffleGrouping("sentences");
    builder.setBolt("count", new WordCount(), 20)
	        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: SQL Setup for Storm JDBC Example
DESCRIPTION: SQL statements for setting up the database schema and initial data for the Storm JDBC example. It includes creating tables, inserting sample data, and a select query.

LANGUAGE: sql
CODE:
create table if not exists user (user_id integer, user_name varchar(100), dept_name varchar(100), create_date date);
create table if not exists department (dept_id integer, dept_name varchar(100));
create table if not exists user_department (user_id integer, dept_id integer);
insert into department values (1, 'R&D');
insert into department values (2, 'Finance');
insert into department values (3, 'HR');
insert into department values (4, 'Sales');
insert into user_department values (1, 1);
insert into user_department values (2, 2);
insert into user_department values (3, 3);
insert into user_department values (4, 4);
select dept_name from department, user_department where department.dept_id = user_department.dept_id and user_department.user_id = ?;

----------------------------------------

TITLE: Defining a Transactional Topology in Storm
DESCRIPTION: Example of defining a transactional topology using TransactionalTopologyBuilder to compute a global count of tuples from an input stream.

LANGUAGE: java
CODE:
MemoryTransactionalSpout spout = new MemoryTransactionalSpout(DATA, new Fields("word"), PARTITION_TAKE_PER_BATCH);
TransactionalTopologyBuilder builder = new TransactionalTopologyBuilder("global-count", "spout", spout, 3);
builder.setBolt("partial-count", new BatchCount(), 5)
        .shuffleGrouping("spout");
builder.setBolt("sum", new UpdateGlobalCount())
        .globalGrouping("partial-count");

----------------------------------------

TITLE: Declaring Output Fields for CassandraWriterBolt in Java
DESCRIPTION: Shows how to declare output fields for a CassandraWriterBolt, which can be used to emit new tuples on error or to chain queries.

LANGUAGE: java
CODE:
new CassandraWriterBolt(insertInto("album").values(withFields(all()).build())
        .withResultHandler(new EmitOnDriverExceptionResultHandler());
        .withStreamOutputFields("stream_error", new Fields("message");

public static class EmitOnDriverExceptionResultHandler extends BaseExecutionResultHandler {
    @Override
    protected void onDriverException(DriverException e, OutputCollector collector, Tuple tuple) {
        LOG.error("An error occurred while executing cassandra statement", e);
        collector.emit("stream_error", new Values(e.getMessage()));
        collector.ack(tuple);
    }
}

----------------------------------------

TITLE: Simple Bolt Definition in Clojure
DESCRIPTION: Example of defining a simple bolt that splits sentences into words. Shows basic tuple processing and emission patterns.

LANGUAGE: clojure
CODE:
(defbolt split-sentence ["word"] [tuple collector]
  (let [words (.split (.getString tuple 0) " ")]
    (doseq [w words]
      (emit-bolt! collector [w] :anchor tuple))
    (ack! collector tuple)
    ))

----------------------------------------

TITLE: Defining Flux Components
DESCRIPTION: Example of defining components in YAML that can be referenced by spouts and bolts.

LANGUAGE: yaml
CODE:
components:
  - id: "stringScheme"
    className: "org.apache.storm.kafka.StringScheme"

  - id: "stringMultiScheme"
    className: "org.apache.storm.spout.SchemeAsMultiScheme"
    constructorArgs:
      - ref: "stringScheme" # component with id "stringScheme" must be declared above.

----------------------------------------

TITLE: Implementing Worker Hooks in Storm
DESCRIPTION: Worker hooks are executed during worker startup before any bolts or spouts are initialized. They are created by extending BaseWorkerHook and registered using TopologyBuilder.

LANGUAGE: java
CODE:
TopologyBuilder.addWorkerHook()  // Register worker-level hook

----------------------------------------

TITLE: Distributed Mode Message Receiving in Apache Storm Worker (Clojure)
DESCRIPTION: In distributed mode, each worker listens on a single TCP port for incoming messages and routes them in-memory to tasks.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L204

----------------------------------------

TITLE: Basic Cassandra Insert Query with Specified Fields
DESCRIPTION: Creates a CassandraWriterBolt that inserts data into an album table using specified tuple fields.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
    async(
        simpleQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);")
            .with(
                fields("title", "year", "performer", "genre", "tracks")
             )
        )
);

----------------------------------------

TITLE: Refreshing Worker Connections in Apache Storm (Clojure)
DESCRIPTION: This code snippet shows how worker connections are refreshed in Apache Storm. It's called periodically or when ZooKeeper assignments change, managing connections to other workers and maintaining a task-to-worker mapping.

LANGUAGE: clojure
CODE:
refresh-connections

----------------------------------------

TITLE: Setting Health Check Directory in Storm YAML
DESCRIPTION: This YAML configuration specifies the directory for health check scripts. These scripts are used to monitor the health of supervisor nodes.

LANGUAGE: yaml
CODE:
storm.health.check.dir: "healthchecks"

----------------------------------------

TITLE: Configuring Worker-Level Metrics in YAML
DESCRIPTION: Example of how to configure worker-level metrics in the Storm YAML configuration file. This adds two custom metrics, metricA and metricB, to be collected for all workers in the cluster.

LANGUAGE: yaml
CODE:
worker.metrics: 
  metricA: "aaa.bbb.ccc.ddd.MetricA"
  metricB: "aaa.bbb.ccc.ddd.MetricB"
  ...

----------------------------------------

TITLE: Dequeuing and Removing Items from Kestrel Queue in Java
DESCRIPTION: This method dequeues items from a Kestrel queue and then removes them using the ack method. It demonstrates how to handle item retrieval and deletion in Kestrel.

LANGUAGE: java
CODE:
    private static void dequeueAndRemoveItems(KestrelClient kestrelClient, String queueName)
    throws IOException, ParseError
		 {
			for(int i=1; i<=12; i++){

				Item item = kestrelClient.dequeue(queueName);


				if(item==null){
					System.out.println("The queue (" + queueName + ") contains no items.");
				}
				else
				{
					int itemID = item._id;


					byte[] data = item._data;

					String receivedVal = new String(data);

					kestrelClient.ack(queueName, itemID);

					System.out.println("receivedItem=" + receivedVal);
				}
			}
	}

----------------------------------------

TITLE: Basic Spout Definition in Clojure
DESCRIPTION: Example of a basic spout that emits random sentences, showing spout implementation patterns

LANGUAGE: clojure
CODE:
(defspout sentence-spout ["sentence"]
  [conf context collector]
  (let [sentences ["a little brown dog"
                   "the man petted the dog"
                   "four score and seven years ago"
                   "an apple a day keeps the doctor away"]]
    (spout
     (nextTuple []
       (Thread/sleep 100)
       (emit-spout! collector [(rand-nth sentences)])         
       )
     (ack [id]
        ))))

----------------------------------------

TITLE: Implementing RotatingMap in Java for Apache Storm's Acker
DESCRIPTION: The RotatingMap is a data structure used by the Acker to efficiently manage and expire pending tuples. It maintains multiple buckets of HashMaps, each representing a cohort of records with the same expiration time. The structure provides O(1) access and efficient time-based expiration of entries.

LANGUAGE: java
CODE:
// Internally, it holds several HashMaps ('buckets') of its own, each holding a cohort of records that will expire at the same time.

// Whenever a value is .put() to the RotatingMap, it is relocated to the nursery -- and removed from any other bucket it might have been in (effectively resetting its death clock).

// Whenever its owner calls .rotate(), the RotatingMap advances each cohort one step further towards expiration.

// If there are any key-value pairs in the former death row bucket, the RotatingMap invokes a callback (given in the constructor) for each key-value pair, letting its owner take appropriate action (eg, failing a tuple.

----------------------------------------

TITLE: Running a Non-JVM Storm Topology
DESCRIPTION: Demonstrates how to run a Storm topology written in a non-JVM language (e.g., Python) using the 'shell' command.

LANGUAGE: bash
CODE:
storm shell resources/ python3 topology.py arg1 arg2

----------------------------------------

TITLE: Setting Task-Level Data in Storm Components
DESCRIPTION: Demonstrates how to set task-level data that can be shared across components using TopologyContext.

LANGUAGE: java
CODE:
TopologyContext#setTaskData(String, Object)

----------------------------------------

TITLE: Storm Batch Processing Configuration
DESCRIPTION: Properties for configuring batch processing behavior in Storm, controlling how messages are batched for both intra-worker and inter-worker communication.

LANGUAGE: properties
CODE:
topology.producer.batch.size
topology.transfer.batch.size
topology.flush.tuple.freq.millis

----------------------------------------

TITLE: Configuring YAML Front Matter for Dynamic Worker Profiling Documentation
DESCRIPTION: YAML front matter defining metadata for the Dynamic Worker Profiling documentation page in Storm's documentation system.

LANGUAGE: yaml
CODE:
---
title: Dynamic Worker Profiling
layout: documentation
documentation: true
---

----------------------------------------

TITLE: Configuring SSL/TLS for MQTT with Flux YAML
DESCRIPTION: Flux YAML configuration for setting up SSL/TLS in an MQTT Storm topology, including keystore and truststore configuration.

LANGUAGE: yaml
CODE:
name: "mqtt-topology"

components:
   ########## MQTT Spout Config ############
  - id: "mqtt-type"
    className: "org.apache.storm.mqtt.examples.CustomMessageMapper"

  - id: "keystore-loader"
    className: "org.apache.storm.mqtt.ssl.DefaultKeyStoreLoader"
    constructorArgs:
      - "keystore.jks"
      - "truststore.jks"
    properties:
      - name: "keyPassword"
        value: "password"
      - name: "keyStorePassword"
        value: "password"
      - name: "trustStorePassword"
        value: "password"

  - id: "mqtt-options"
    className: "org.apache.storm.mqtt.common.MqttOptions"
    properties:
      - name: "url"
        value: "ssl://raspberrypi.local:8883"
      - name: "topics"
        value:
          - "/users/tgoetz/#"

# topology configuration
config:
  topology.workers: 1
  topology.max.spout.pending: 1000

# spout definitions
spouts:
  - id: "mqtt-spout"
    className: "org.apache.storm.mqtt.spout.MqttSpout"
    constructorArgs:
      - ref: "mqtt-type"
      - ref: "mqtt-options"
      - ref: "keystore-loader"
    parallelism: 1

# bolt definitions
bolts:

  - id: "log"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1


streams:

  - from: "mqtt-spout"
    to: "log"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Implementing Named Query Parameters with CassandraWriterBolt
DESCRIPTION: Shows how to use named parameters and aliases in CassandraWriterBolt queries for better readability and maintenance.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
     async(
        boundQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (:ti, :ye, :pe, :ge, :tr);")\n            .bind(
                field("ti").as("title"),
                field("ye").as("year")),
                field("pe").as("performer")),
                field("ge").as("genre")),
                field("tr").as("tracks"))
            ).byNamedSetters()
     )
);

----------------------------------------

TITLE: Implementing FailedMessageRetryHandler Interface in Java
DESCRIPTION: This code snippet demonstrates the FailedMessageRetryHandler interface used for handling failed message retries in the Kinesis Spout. It includes methods for marking messages as failed, retrieving the next failed message to retry, and acknowledging successful retries.

LANGUAGE: java
CODE:
    boolean failed (KinesisMessageId messageId);
    KinesisMessageId getNextFailedMessageToRetry ();
    void failedMessageEmitted (KinesisMessageId messageId);
    void acked (KinesisMessageId messageId);

----------------------------------------

TITLE: Configuring Storm Project Structure in Eclipse
DESCRIPTION: Instructions for setting up project directories and classpath entries in Eclipse for Storm development. Includes configuration for both Java and multilang resources.

LANGUAGE: text
CODE:
src/jvm/           # Main Java source directory
lib/              # Storm library dependencies
lib/dev/          # Development dependencies
multilang/        # Non-Java language implementations
multilang/resources/  # Resource files for multilang components

----------------------------------------

TITLE: Launching Docker Container for Storm Worker
DESCRIPTION: Example Docker run command used by the worker-launcher to start a container for a Storm worker. It sets up various mounts, security options, and resource limits.

LANGUAGE: bash
CODE:
run --name=8198e1f0-f323-4b9d-8625-e4fd640cd058 \
--user=<uid>:<gid> \
-d \
--net=host \
--read-only \
-v /sys/fs/cgroup:/sys/fs/cgroup:ro \
-v /usr/share/apache-storm-2.3.0:/usr/share/apache-storm-2.3.0:ro \
-v /<storm-local-dir>/supervisor:/<storm-local-dir>/supervisor:ro \
-v /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058:/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
-v /<workers-artifacts-dir>/workers-artifacts/word-count-1-1591895933/6703:/<workers-artifacts-dir>/workers-artifacts/word-count-1-1591895933/6703 \
-v /<storm-local-dir>/workers-users/8198e1f0-f323-4b9d-8625-e4fd640cd058:/<storm-local-dir>/workers-users/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
-v /var/run/nscd:/var/run/nscd \
-v /<storm-local-dir>/supervisor/stormdist/word-count-1-1591895933/shared_by_topology:/<storm-local-dir>/supervisor/stormdist/word-count-1-1591895933/shared_by_topology \
-v /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/tmp:/tmp \
-v /etc/storm:/etc/storm:ro \
--cgroup-parent=/storm \
--group-add <gid> \
--workdir=/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
--cidfile=/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/container.cid \
--cap-drop=ALL \
--security-opt no-new-privileges \
--security-opt seccomp=/usr/share/apache-storm-2.3.0/conf/seccomp.json \
--cpus=2.6 xxx.xxx.com:8080/storm/storm/rhel7:latest \
bash /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/storm-worker-script.sh

----------------------------------------

TITLE: Implementing FailedMessageRetryHandler Interface in Java
DESCRIPTION: This code snippet demonstrates the FailedMessageRetryHandler interface used for handling failed message retries in the Kinesis Spout. It includes methods for marking messages as failed, retrieving the next failed message to retry, and acknowledging successful retries.

LANGUAGE: java
CODE:
    boolean failed (KinesisMessageId messageId);
    KinesisMessageId getNextFailedMessageToRetry ();
    void failedMessageEmitted (KinesisMessageId messageId);
    void acked (KinesisMessageId messageId);

----------------------------------------

TITLE: Initializing Transactional Topology in Storm
DESCRIPTION: Code showing how to set up a transactional topology with a memory spout and bolts for counting global values.

LANGUAGE: java
CODE:
MemoryTransactionalSpout spout = new MemoryTransactionalSpout(DATA, new Fields("word"), PARTITION_TAKE_PER_BATCH);
TransactionalTopologyBuilder builder = new TransactionalTopologyBuilder("global-count", "spout", spout, 3);
builder.setBolt("partial-count", new BatchCount(), 5)
        .shuffleGrouping("spout");
builder.setBolt("sum", new UpdateGlobalCount())
        .globalGrouping("partial-count");

----------------------------------------

TITLE: Retrieving Topology Component Information in Storm UI API
DESCRIPTION: GET request to retrieve detailed information about a specific component in a topology, including metrics, errors, and executor stats.

LANGUAGE: json
CODE:
{
  "name": "WordCount3",
  "id": "spout",
  "componentType": "spout",
  "windowHint": "10m 0s",
  "executors": 5,
  "componentErrors": [
    {
      "errorTime": 1406006074000,
      "errorHost": "10.11.1.70",
      "errorPort": 6701,
      "errorWorkerLogLink": "http://10.11.1.7:8000/log?file=worker-6701.log",
      "errorLapsedSecs": 16,
      "error": "java.lang.RuntimeException: java.lang.StringIndexOutOfBoundsException: Some Error\n\tat org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:128)\n\tat org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99)\n\tat org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80)\n\tat backtype...more.."
    }
  ],
  "topologyId": "WordCount3-1-1402960825",
  "tasks": 5,
  "window": "600",
  "profilerActive": [
    {
      "host": "10.11.1.70",
      "port": "6701",
      "dumplink":"http:\/\/10.11.1.70:8000\/dumps\/ex-1-1452718803\/10.11.1.70%3A6701",
      "timestamp":"576328"
    }
  ],
  "profilingAndDebuggingCapable": true,
  "profileActionEnabled": true,
  "spoutSummary": [
    {
      "windowPretty": "10m 0s",
      "window": "600",
      "emitted": 28500,
      "transferred": 28460,
      "completeLatency": "0.000",
      "acked": 0,
      "failed": 0
    }
  ],
  "outputStats": [
    {
      "stream": "__metrics",
      "emitted": 40,
      "transferred": 0,
      "completeLatency": "0",
      "acked": 0,
      "failed": 0
    }
  ],
  "executorStats": [
    {
      "workerLogLink": "http://10.11.1.7:8000/log?file=worker-6701.log",
      "emitted": 5720,
      "port": 6701,
      "completeLatency": "0.000",
      "transferred": 5720,
      "host": "10.11.1.7",
      "acked": 0,
      "uptime": "43m 4s",
      "uptimeSeconds": 2584,
      "id": "[24-24]",
      "failed": 0
    }
  ]
}

----------------------------------------

TITLE: Storm SQL Basic Command Usage
DESCRIPTION: Basic command to compile and submit SQL statements as a Storm topology

LANGUAGE: bash
CODE:
$ bin/storm sql <sql-file> <topo-name>

----------------------------------------

TITLE: Redis Lookup Bolt Configuration
DESCRIPTION: Configuration and initialization of RedisLookupBolt using JedisPoolConfig.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisLookupMapper lookupMapper = new WordCountRedisLookupMapper();
RedisLookupBolt lookupBolt = new RedisLookupBolt(poolConfig, lookupMapper);

----------------------------------------

TITLE: Local Mode Message Transfer
DESCRIPTION: In-memory queue implementation for local mode message passing without ZeroMQ

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/local.clj

----------------------------------------

TITLE: Dequeuing Items from Kestrel Queue in Java
DESCRIPTION: This method demonstrates how to dequeue items from a Kestrel queue without removing them. It uses KestrelClient to retrieve items and print their contents.

LANGUAGE: java
CODE:
    private static void dequeueItems(KestrelClient kestrelClient, String queueName) throws IOException, ParseError
			 {
		for(int i=1; i<=12; i++){

			Item item = kestrelClient.dequeue(queueName);

			if(item==null){
				System.out.println("The queue (" + queueName + ") contains no items.");
			}
			else
			{
				byte[] data = item._data;

				String receivedVal = new String(data);

				System.out.println("receivedItem=" + receivedVal);
			}
		}

----------------------------------------

TITLE: Kerberos Client Authentication Configuration
DESCRIPTION: JAAS configuration for Nimbus client using Kerberos authentication with Pacemaker.

LANGUAGE: java
CODE:
PacemakerClient {\n    com.sun.security.auth.module.Krb5LoginModule required\n    useKeyTab=true\n    keyTab="/etc/keytabs/nimbus.keytab"\n    storeKey=true\n    useTicketCache=false\n    serviceName="pacemaker"\n    principal="nimbus@MY.COMPANY.COM";\n};

----------------------------------------

TITLE: Configuring UI/Logviewer Authentication Filter
DESCRIPTION: YAML configuration for setting up authentication filters for Storm UI and logviewer components using filter class and parameters

LANGUAGE: yaml
CODE:
ui.filter: "filter.class"
ui.filter.params: "param1":"value1"
logviewer.filter: "filter.class"
logviewer.filter.params: "param1":"value1"

----------------------------------------

TITLE: Thread-safe Tuple Serialization in Apache Storm (Java)
DESCRIPTION: Implementation of a thread-safe serializer for Storm tuples using Kryo serialization.

LANGUAGE: java
CODE:
https://github.com/apache/storm/blob/0.7.1/src/jvm/org/apache/storm/serialization/KryoTupleSerializer.java#L26

----------------------------------------

TITLE: Implementing a FlatMapFunction in Trident
DESCRIPTION: Example of a FlatMapFunction that splits sentences into words.

LANGUAGE: java
CODE:
public class Split extends FlatMapFunction {
  @Override
  public Iterable<Values> execute(TridentTuple input) {
    List<Values> valuesList = new ArrayList<>();
    for (String word : input.getString(0).split(" ")) {
      valuesList.add(new Values(word));
    }
    return valuesList;
  }
}

----------------------------------------

TITLE: Configuring Kerberos Authentication
DESCRIPTION: Example of setting up Kerberos authentication for secure HBase access.

LANGUAGE: java
CODE:
Config config = new Config();
...
config.put("storm.keytab.file", "$keytab");
config.put("storm.kerberos.principal", "$principal");
StormSubmitter.submitTopology("$topologyName", config, builder.createTopology());

----------------------------------------

TITLE: Storm Message Transfer Protocol
DESCRIPTION: Core messaging protocol implementation used by Storm for tuple transfer

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/protocol.clj

----------------------------------------

TITLE: DRPC Client Java Implementation
DESCRIPTION: Example of creating a DRPC client in Java for production use, showing proper client initialization and usage.

LANGUAGE: java
CODE:
Config conf = new Config();
try (DRPCClient drpc = DRPCClient.getConfiguredClient(conf)) {
  //User the drpc client
  String result = drpc.execute(function, argument);
}

----------------------------------------

TITLE: Implementing RocketMq Trident State in Java for Storm-RocketMQ Integration
DESCRIPTION: This snippet demonstrates how to create a RocketMQ persistent Trident state. It initializes the state with a TupleToMessageMapper, TopicSelector, and RocketMQ configuration properties, and shows how to use it in a Trident topology.

LANGUAGE: java
CODE:
TupleToMessageMapper mapper = new FieldNameBasedTupleToMessageMapper("word", "count");
TopicSelector selector = new DefaultTopicSelector(topic);

Properties properties = new Properties();
properties.setProperty(RocketMqConfig.NAME_SERVER_ADDR, nameserverAddr);

RocketMqState.Options options = new RocketMqState.Options()
        .withMapper(mapper)
        .withSelector(selector)
        .withProperties(properties);

StateFactory factory = new RocketMqStateFactory(options);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory, fields,
        new RocketMqStateUpdater(), new Fields());

----------------------------------------

TITLE: Configuring Storm Remote Cluster Connection in YAML
DESCRIPTION: YAML configuration for connecting to a remote Storm cluster by specifying the Nimbus seed nodes in the storm.yaml file located in the user's .storm directory.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["123.45.678.890"]

----------------------------------------

TITLE: Monitoring Storm Topology Throughput
DESCRIPTION: Interactively monitors a topology's throughput with customizable parameters.

LANGUAGE: shell
CODE:
storm monitor topology-name [-i interval-secs] [-m component-id] [-s stream-id] [-w [emitted | transferred]]

----------------------------------------

TITLE: Storm RotatingMap Implementation
DESCRIPTION: Implementation of Storm's RotatingMap data structure, which provides efficient time-based expiration of records. It maintains multiple buckets of HashMaps for different expiration cohorts, offering O(1) access time while supporting automatic expiration of entries.



----------------------------------------

TITLE: Entering OCI/Squashfs Container for Profiling
DESCRIPTION: Command to enter a running OCI/Squashfs container for profiling purposes.

LANGUAGE: bash
CODE:
sudo nsenter --target <container-pid> --pid --mount --setuid <uid> --setgid <gid>

----------------------------------------

TITLE: Initializing HBase State Provider in Java
DESCRIPTION: Example of initializing HBase state provider configuration in Java code. Sets up HBase configuration, specifies the state provider class, and provides JSON configuration for the HBase state provider.

LANGUAGE: java
CODE:
Config conf = new Config();
    Map<String, Object> hbConf = new HashMap<String, Object>();
    hbConf.put("hbase.rootdir", "file:///tmp/hbase");
    conf.put("hbase.conf", hbConf);
    conf.put("topology.state.provider",  "org.apache.storm.hbase.state.HBaseKeyValueStateProvider");
    conf.put("topology.state.provider.config", "{" +
            "   \"hbaseConfigKey\": \"hbase.conf\"," +
            "   \"tableName\": \"state\"," +
            "   \"columnFamily\": \"cf\"" +
            " }");

----------------------------------------

TITLE: Redis Cluster State Configuration
DESCRIPTION: JSON configuration for Redis Cluster state implementation, specifying cluster nodes and connection parameters.

LANGUAGE: json
CODE:
{
  "keyClass": "Optional fully qualified class name of the Key type.",
  "valueClass": "Optional fully qualified class name of the Value type.",
  "keySerializerClass": "Optional Key serializer implementation class.",
  "valueSerializerClass": "Optional Value Serializer implementation class.",
  "jedisClusterConfig": {
    "nodes": ["localhost:7379", "localhost:7380", "localhost:7381"],
    "timeout": 2000,
    "maxRedirections": 5
  }
}

----------------------------------------

TITLE: SQL Join Example for Storm Streams
DESCRIPTION: Example SQL query showing a join between 4 tables that will be recreated using Storm's JoinBolt.

LANGUAGE: sql
CODE:
select  userId, key4, key2, key3
from        table1
inner join  table2  on table2.userId =  table1.key1
inner join  table3  on table3.key3   =  table2.userId
left join   table4  on table4.key4   =  table3.key3

----------------------------------------

TITLE: Implementing Streaming Top N Pattern in Storm
DESCRIPTION: Pattern for implementing a scalable streaming top N calculation using parallel processing and merging.

LANGUAGE: java
CODE:
builder.setBolt("rank", new RankObjects(), parallelism)
  .fieldsGrouping("objects", new Fields("value"));
builder.setBolt("merge", new MergeObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Configuring RedisState for Trident Topology in Java
DESCRIPTION: Java code snippet demonstrating how to configure and use RedisState in a Trident topology.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
                                .setHost(redisHost).setPort(redisPort)
                                .build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisLookupMapper lookupMapper = new WordCountLookupMapper();
RedisState.Factory factory = new RedisState.Factory(poolConfig);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory,
                        fields,
                        new RedisStateUpdater(storeMapper).withExpire(86400000),
                        new Fields());

TridentState state = topology.newStaticState(factory);
stream = stream.stateQuery(state, new Fields("word"),
                        new RedisStateQuerier(lookupMapper),
                        new Fields("columnName","columnValue"));

----------------------------------------

TITLE: Configuring Redis Lookup Bolt
DESCRIPTION: Configuration and instantiation of RedisLookupBolt using JedisPoolConfig and lookup mapper.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisLookupMapper lookupMapper = new WordCountRedisLookupMapper();
RedisLookupBolt lookupBolt = new RedisLookupBolt(poolConfig, lookupMapper);

----------------------------------------

TITLE: Configuring RedisState for Trident Topology in Java
DESCRIPTION: Java code snippet demonstrating how to configure and use RedisState in a Trident topology.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
                                .setHost(redisHost).setPort(redisPort)
                                .build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisLookupMapper lookupMapper = new WordCountLookupMapper();
RedisState.Factory factory = new RedisState.Factory(poolConfig);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory,
                        fields,
                        new RedisStateUpdater(storeMapper).withExpire(86400000),
                        new Fields());

TridentState state = topology.newStaticState(factory);
stream = stream.stateQuery(state, new Fields("word"),
                        new RedisStateQuerier(lookupMapper),
                        new Fields("columnName","columnValue"));

----------------------------------------

TITLE: Storm Blobstore Command Example
DESCRIPTION: Example showing how to create a blob in Storm's blobstore with specific access control lists (ACLs). Creates a key named mytopo:data.tgz with custom user permissions.

LANGUAGE: bash
CODE:
storm blobstore create mytopo:data.tgz -f data.tgz -a u:alice:rwa,u:bob:rw,o::r

----------------------------------------

TITLE: Storm SQL Query for Filtering Slow Logs
DESCRIPTION: SQL script to filter slow logs from the Apache log stream, including a user-defined function for timestamp conversion.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://apache-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
CREATE EXTERNAL TABLE APACHE_SLOW_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_RECEIVED_TIMESTAMP BIGINT, TIME_ELAPSED_MS INT) LOCATION 'kafka://apache-slow-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
CREATE FUNCTION GET_TIME AS 'org.apache.storm.sql.runtime.functions.scalar.datetime.GetTime2'
INSERT INTO APACHE_SLOW_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, GET_TIME(TIME_RECEIVED_UTC_ISOFORMAT, 'yyyy-MM-dd''T''HH:mm:ssZZ') AS TIME_RECEIVED_TIMESTAMP, TIME_US / 1000 AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (TIME_US / 1000) >= 100

----------------------------------------

TITLE: Message Sending Protocol in Apache Storm (Clojure)
DESCRIPTION: This code defines the protocol used for message sending in Apache Storm.

LANGUAGE: clojure
CODE:
(github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/protocol.clj)

----------------------------------------

TITLE: Defining ConnectionProvider Interface in Java for Storm JDBC
DESCRIPTION: Interface definition for ConnectionProvider, which should be implemented by different connection pooling mechanisms. It includes methods for preparing, getting a connection, and cleanup.

LANGUAGE: java
CODE:
public interface ConnectionProvider extends Serializable {
    /**
     * method must be idempotent.
     */
    void prepare();

    /**
     *
     * @return a DB connection over which the queries can be executed.
     */
    Connection getConnection();

    /**
     * called once when the system is shutting down, should be idempotent.
     */
    void cleanup();
}

----------------------------------------

TITLE: Killing Storm Topology
DESCRIPTION: Deactivates a topology's spouts, waits for in-flight messages to complete, then shuts down workers and cleans up state.

LANGUAGE: shell
CODE:
storm kill topology-name [-w wait-time-secs]

----------------------------------------

TITLE: Specifying Nimbus Seeds in Storm YAML
DESCRIPTION: This configuration lists the machines that are candidates for master, necessary for worker nodes to download topology jars and configurations.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["111.222.333.44"]

----------------------------------------

TITLE: Implementing Storm ISerialization Interface in Java
DESCRIPTION: Interface definition for creating custom serializers in Storm. The interface includes methods for accepting types, serializing objects to binary format, and deserializing objects from binary streams.

LANGUAGE: java
CODE:
public interface ISerialization<T> {
    public boolean accept(Class c);
    public void serialize(T object, DataOutputStream stream) throws IOException;
    public T deserialize(DataInputStream stream) throws IOException;
}

----------------------------------------

TITLE: Configuring Kerberos Authentication in storm.yaml
DESCRIPTION: YAML configuration for enabling Kerberos authentication in Storm

LANGUAGE: yaml
CODE:
storm.thrift.transport: "org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin"
java.security.auth.login.config: "/path/to/jaas.conf"

nimbus.childopts: "-Xmx1024m -Djava.security.auth.login.config=/path/to/jaas.conf"
ui.childopts: "-Xmx768m -Djava.security.auth.login.config=/path/to/jaas.conf"
supervisor.childopts: "-Xmx256m -Djava.security.auth.login.config=/path/to/jaas.conf"

----------------------------------------

TITLE: Starting Profiler on Topology Worker in JSON
DESCRIPTION: Example response from the /api/v1/topology/<id>/profiling/start/<host-port>/<timeout> endpoint, which starts the profiler on a worker for a topology.

LANGUAGE: json
CODE:
{
  "status": "ok",
  "id": "10.11.1.7:6701",
  "timeout": "10",
  "dumplink": "http:\/\/10.11.1.7:8000\/dumps\/wordcount-1-1446614150\/10.11.1.7%3A6701"
}

----------------------------------------

TITLE: Displaying Storm Logo Images in Markdown
DESCRIPTION: This snippet shows how to embed three different versions of the Storm logo using Markdown image syntax. It includes a square version, a horizontal version, and a no-color horizontal version.

LANGUAGE: markdown
CODE:
![Storm Brand](/images/logocontest/rmarshall/StormLogo_Square.png)

![Storm Brand](/images/logocontest/rmarshall/StormLogo_Horizontal.png)

![Storm Brand](/images/logocontest/rmarshall/StormLogo_Horizontal_NoColour.png)

----------------------------------------

TITLE: Configuring CGroups Mount Points and Permissions in CGConfig
DESCRIPTION: Sample configuration file showing CGroup mount points and permission settings for Storm. Defines mount locations for various resource controllers and sets up a 'storm' group with specific user/group permissions.

LANGUAGE: conf
CODE:
mount {
	cpuset	= /cgroup/cpuset;
	cpu	= /cgroup/storm_resources;
	cpuacct	= /cgroup/storm_resources;
	memory	= /cgroup/storm_resources;
	devices	= /cgroup/devices;
	freezer	= /cgroup/freezer;
	net_cls	= /cgroup/net_cls;
	blkio	= /cgroup/blkio;
}

group storm {
       perm {
               task {
                      uid = 500;
                      gid = 500;
               }
               admin {
                      uid = 500;
                      gid = 500;
               }
       }
       cpu {
       }
       memory {
       }
       cpuacct {
       }
}

----------------------------------------

TITLE: Implementing Custom Task-Level Metric in Java
DESCRIPTION: Example of how to implement a custom task-level metric in a Storm bolt. This creates a CountMetric to track the number of times the bolt's execute method is called.

LANGUAGE: java
CODE:
private transient CountMetric countMetric;

@Override
public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
	// other initialization here.
	countMetric = new CountMetric();
	context.registerMetric("execute_count", countMetric, 60);
}

public void execute(Tuple input) {
	countMetric.incr();
	// handle tuple here.	
}

----------------------------------------

TITLE: Storm Shell Python Execution Format
DESCRIPTION: Format showing how storm shell executes the Python topology script with additional parameters including Nimbus host, port, and jar location.

LANGUAGE: bash
CODE:
python topology.py arg1 arg2 {nimbus-host} {nimbus-port} {uploaded-jar-location}

----------------------------------------

TITLE: Defining Input Declaration for Storm Bolt
DESCRIPTION: Example showing different ways to declare input streams for a bolt including shuffle grouping, fields grouping and global grouping.

LANGUAGE: clojure
CODE:
{["2" "1"] :shuffle
 "3" ["field1" "field2"]
 ["4" "2"] :global}

----------------------------------------

TITLE: Configuring Health Check Timeout in Storm YAML
DESCRIPTION: This snippet shows how to set the timeout for health check scripts in Storm. It defines the maximum time allowed for a health check script to run before it's marked as failed.

LANGUAGE: yaml
CODE:
storm.health.check.timeout.ms: 5000

----------------------------------------

TITLE: Configuring Flush Tuple Frequency in Storm
DESCRIPTION: Sets the frequency of flush tuple generation to ensure batched messages are processed in a timely manner, balancing throughput and latency requirements.

LANGUAGE: yaml
CODE:
topology.flush.tuple.freq.millis: 1000

----------------------------------------

TITLE: Submitting Topology to Nimbus (Java)
DESCRIPTION: StormSubmitter calls submitTopology on the Nimbus Thrift interface to submit the topology. The topology config is serialized as JSON, and the call includes the path to the uploaded jar.

LANGUAGE: java
CODE:
nimbus.submitTopology(name, uploadedJarLocation, JSONValue.toJSONString(conf), topology);

----------------------------------------

TITLE: Configuring Zookeeper Servers in Storm YAML
DESCRIPTION: This YAML configuration specifies the list of Zookeeper servers for the Storm cluster. It's a mandatory configuration in the storm.yaml file.

LANGUAGE: yaml
CODE:
storm.zookeeper.servers:
  - "111.222.333.444"
  - "555.666.777.888"

----------------------------------------

TITLE: Setting Up Tasks in Storm Worker (Clojure)
DESCRIPTION: The mk-task function sets up individual tasks within a worker. It establishes routing functions and initializes spout or bolt-specific code.

LANGUAGE: clojure
CODE:
(defn mk-task [])

----------------------------------------

TITLE: Initializing HBase State Provider in Java
DESCRIPTION: Example of initializing HBase state provider configuration in Java code. Sets up HBase configuration, specifies the state provider class, and provides JSON configuration for the HBase state provider.

LANGUAGE: java
CODE:
Config conf = new Config();
    Map<String, Object> hbConf = new HashMap<String, Object>();
    hbConf.put("hbase.rootdir", "file:///tmp/hbase");
    conf.put("hbase.conf", hbConf);
    conf.put("topology.state.provider",  "org.apache.storm.hbase.state.HBaseKeyValueStateProvider");
    conf.put("topology.state.provider.config", "{" +
            "   \"hbaseConfigKey\": \"hbase.conf\"," +
            "   \"tableName\": \"state\"," +
            "   \"columnFamily\": \"cf\"" +
            " }");

----------------------------------------

TITLE: Bolt Tuple Transfer in Apache Storm (Clojure)
DESCRIPTION: This code shows how bolts use the worker-provided transfer function to send tuples to other tasks after determining the output task IDs.

LANGUAGE: clojure
CODE:
(https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L429)

----------------------------------------

TITLE: Configuring HiveState for Trident Topology (Java)
DESCRIPTION: Java code example showing how to set up HiveState for use in a Trident topology, including configuration of DelimitedRecordHiveMapper and HiveOptions.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withTimeAsPartitionField("YYYY/MM/DD");
            
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
                                .withTxnsPerBatch(10)
                				.withBatchSize(1000)
                	     		.withIdleTimeout(10)
                	     		
StateFactory factory = new HiveStateFactory().withOptions(hiveOptions);
TridentState state = stream.partitionPersist(factory, hiveFields, new HiveUpdater(), new Fields());

----------------------------------------

TITLE: Maven Dependencies Configuration for Storm Kinesis
DESCRIPTION: XML configuration showing required Maven dependencies for implementing Storm Kinesis integration, including AWS SDK, Storm Core, and supporting libraries.

LANGUAGE: xml
CODE:
 <dependencies>
        <dependency>
            <groupId>com.amazonaws</groupId>
            <artifactId>aws-java-sdk</artifactId>
            <version>${aws-java-sdk.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-core</artifactId>
            <version>${project.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-framework</artifactId>
            <version>${curator.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>com.googlecode.json-simple</groupId>
            <artifactId>json-simple</artifactId>
        </dependency>
 </dependencies>

----------------------------------------

TITLE: Defining ComponentObject in Thrift for Storm
DESCRIPTION: This Thrift structure defines the ComponentObject union, which specifies how code for spouts or bolts can be provided in Storm topologies. It allows for serialized Java, shell components, or Java objects.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Configuring Kerberos Authentication
DESCRIPTION: Configuration setup for secure HBase access using Kerberos authentication.

LANGUAGE: java
CODE:
Config config = new Config();
...
config.put("storm.keytab.file", "$keytab");
config.put("storm.kerberos.principal", "$principal");
StormSubmitter.submitTopology("$topologyName", config, builder.createTopology());

----------------------------------------

TITLE: Configuring Event Loggers in YAML for Storm
DESCRIPTION: This YAML configuration snippet shows how to register multiple event loggers in the storm.yaml config file. It includes both the default FileBasedEventLogger and a custom MyEventLogger with additional arguments.

LANGUAGE: yaml
CODE:
topology.event.logger.register:
  - class: "org.apache.storm.metric.FileBasedEventLogger"
  - class: "org.mycompany.MyEventLogger"
    arguments:
      endpoint: "event-logger.mycompany.org"

----------------------------------------

TITLE: Configuring Kerberos Authentication
DESCRIPTION: Configuration setup for secure HBase access using Kerberos authentication.

LANGUAGE: java
CODE:
Config config = new Config();
...
config.put("storm.keytab.file", "$keytab");
config.put("storm.kerberos.principal", "$principal");
StormSubmitter.submitTopology("$topologyName", config, builder.createTopology());

----------------------------------------

TITLE: Implementing Simple MongoDB Mapper - Java
DESCRIPTION: Implementation of MongoMapper interface that maps Storm tuple fields directly to MongoDB document fields with matching names.

LANGUAGE: java
CODE:
public class SimpleMongoMapper implements MongoMapper {
    private String[] fields;

    @Override
    public Document toDocument(ITuple tuple) {
        Document document = new Document();
        for(String field : fields){
            document.append(field, tuple.getValueByField(field));
        }
        return document;
    }

    @Override
    public Document toDocumentByKeys(List<Object> keys) {
        Document document = new Document();
        document.append("_id", MongoUtils.getID(keys));
        return document;
    }

    public SimpleMongoMapper withFields(String... fields) {
        this.fields = fields;
        return this;
    }
}

----------------------------------------

TITLE: Defining Code Distribution Interface in Java for Storm Nimbus
DESCRIPTION: Java interface for distributing code across the Storm cluster, including methods for uploading, downloading, checking replication count, and cleaning up topology code.

LANGUAGE: java
CODE:
public interface ICodeDistributor {
    void prepare(Map conf);
    File upload(Path dirPath, String topologyId);
    List<File> download(Path destDirPath, String topologyid, File metafile);
    int getReplicationCount(String topologyId);
    void cleanup(String topologyid);
    void close(Map conf);
}

----------------------------------------

TITLE: Multi-Source Named Stream Join Setup
DESCRIPTION: Comprehensive example of joining named streams from multiple sources with proper bolt configuration.

LANGUAGE: java
CODE:
new JoinBolt(JoinBolt.Selector.STREAM,  "stream1", "key1") 
                             .join     ("stream2", "userId",  "stream1" )
                             .select ("userId, key1, key2")
                             .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;
                             
topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("bolt1", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt2", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt3", "stream2", new Fields("userId") )
            .fieldsGrouping("bolt4", "stream1", new Fields("key1") );

----------------------------------------

TITLE: Defining Thrift Structures for Cluster and Nimbus Summary
DESCRIPTION: Thrift struct definitions for ClusterSummary and NimbusSummary, used to represent cluster state including multiple Nimbus instances in the getClusterInfo API.

LANGUAGE: thrift
CODE:
struct ClusterSummary {
  1: required list<SupervisorSummary> supervisors;
  3: required list<TopologySummary> topologies;
  4: required list<NimbusSummary> nimbuses;
}

struct NimbusSummary {
  1: required string host;
  2: required i32 port;
  3: required i32 uptime_secs;
  4: required bool isLeader;
  5: required string version;
}

----------------------------------------

TITLE: Configuring File Loader in YAML
DESCRIPTION: Example YAML configuration for using the FileConfigLoader, specifying the file URI.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "file:///path/to/my/config.yaml"

----------------------------------------

TITLE: Configuring File Loader in YAML
DESCRIPTION: Example YAML configuration for using the FileConfigLoader, specifying the file URI.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "file:///path/to/my/config.yaml"

----------------------------------------

TITLE: Configuring Storm Metrics Reporters
DESCRIPTION: YAML configuration example showing how to set up Graphite and Console metrics reporters in Storm. Includes daemon targeting, reporting periods, and metric filtering options.

LANGUAGE: yaml
CODE:
storm.metrics.reporters:
  # Graphite Reporter
  - class: "org.apache.storm.metrics2.reporters.GraphiteStormReporter"
    daemons:
        - "supervisor"
        - "nimbus"
        - "worker"
    report.period: 60
    report.period.units: "SECONDS"
    graphite.host: "localhost"
    graphite.port: 2003

  # Console Reporter
  - class: "org.apache.storm.metrics2.reporters.ConsoleStormReporter"
    daemons:
        - "worker"
    report.period: 10
    report.period.units: "SECONDS"
    filter:
        class: "org.apache.storm.metrics2.filters.RegexFilter"
        expression: ".*my_component.*emitted.*"

----------------------------------------

TITLE: Configuring Resource Consumption for Trident Topology in Java
DESCRIPTION: This example demonstrates how to set CPU and memory resources for various operations in a Trident topology using the RAS API. It shows setting default resources, as well as specific resources for individual stream operations.

LANGUAGE: java
CODE:
TridentTopology topo = new TridentTopology();
topo.setResourceDefaults(new DefaultResourceDeclarer()
                                                      .setMemoryLoad(128)
                                                      .setCPULoad(20));
TridentState wordCounts =
    topology
        .newStream("words", feeder)
        .parallelismHint(5)
        .setCPULoad(20)
        .setMemoryLoad(512,256)
        .each( new Fields("sentence"),  new Split(), new Fields("word"))
        .setCPULoad(10)
        .setMemoryLoad(512)
        .each(new Fields("word"), new BangAdder(), new Fields("word!"))
        .parallelismHint(10)
        .setCPULoad(50)
        .setMemoryLoad(1024)
        .each(new Fields("word!"), new QMarkAdder(), new Fields("word!?"))
        .groupBy(new Fields("word!"))
        .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))
        .setCPULoad(100)
        .setMemoryLoad(2048);

----------------------------------------

TITLE: Importing DynamicStatementBuilder in Java for Storm Cassandra Integration
DESCRIPTION: Static import statement for DynamicStatementBuilder, which is used to construct Cassandra queries in Storm bolts.

LANGUAGE: java
CODE:
import static org.apache.storm.cassandra.DynamicStatementBuilder.*

----------------------------------------

TITLE: Building Exclamation Topology
DESCRIPTION: Example topology construction that demonstrates connecting spouts and bolts with shuffle grouping.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();        
builder.setSpout("words", new TestWordSpout(), 10);        
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
        .shuffleGrouping("words");
builder.setBolt("exclaim2", new ExclamationBolt(), 2)
        .shuffleGrouping("exclaim1");

----------------------------------------

TITLE: Launching Storm Supervisor Daemon
DESCRIPTION: Starts the Supervisor daemon for managing worker processes on a node.

LANGUAGE: shell
CODE:
storm supervisor

----------------------------------------

TITLE: Storm CLI Blobstore Command Example
DESCRIPTION: Example command for creating a blob in Storm's blobstore with specific access permissions. Creates a key with user-specific access controls.

LANGUAGE: bash
CODE:
storm blobstore create mytopo:data.tgz -f data.tgz -a u:alice:rwa,u:bob:rw,o::r

----------------------------------------

TITLE: Implementing a Custom Function in Trident
DESCRIPTION: Example of creating a custom Function that emits multiple tuples based on input

LANGUAGE: java
CODE:
public class MyFunction extends BaseFunction {
    public void execute(TridentTuple tuple, TridentCollector collector) {
        for(int i=0; i < tuple.getInteger(0); i++) {
            collector.emit(new Values(i));
        }
    }
}

----------------------------------------

TITLE: Initial Handshake Configuration JSON
DESCRIPTION: Example of the JSON configuration object sent during initial handshake between Storm and shell components, including topology configuration and context information.

LANGUAGE: json
CODE:
{
    "conf": {
        "topology.message.timeout.secs": 3
    },
    "pidDir": "...",
    "context": {
        "task->component": {
            "1": "example-spout",
            "2": "__acker",
            "3": "example-bolt1",
            "4": "example-bolt2"
        },
        "taskid": 3,
        "componentid": "example-bolt",
        "stream->target->grouping": {
            "default": {
                "example-bolt2": {
                    "type": "SHUFFLE"}}},
        "streams": ["default"],
        "stream->outputfields": {"default": ["word"]},
        "source->stream->grouping": {
            "example-spout": {
                "default": {
                    "type": "FIELDS",
                    "fields": ["word"]
                }
            }
        },
        "source->stream->fields": {
            "example-spout": {
                "default": ["word"]
            }
        }
    }
}

----------------------------------------

TITLE: HTML Redirect to Storm Serializers Documentation
DESCRIPTION: Meta refresh tag and canonical link implementation to redirect users to the current Storm serializers documentation page.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Serializers.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Serializers.html" />

----------------------------------------

TITLE: Compiling and Submitting SQL as a Storm Topology
DESCRIPTION: Compiles SQL statements into a Trident topology and submits it to Storm.

LANGUAGE: shell
CODE:
storm sql sql-file topology-name

----------------------------------------

TITLE: Configuring Automatic Task Hooks in Storm
DESCRIPTION: This snippet demonstrates how to configure automatic task hooks using the Storm configuration. These hooks are registered in every spout or bolt automatically.

LANGUAGE: java
CODE:
"topology.auto.task.hooks"

----------------------------------------

TITLE: SplitSentence Bolt Implementation
DESCRIPTION: Example bolt that splits sentences into words, demonstrating proper tuple anchoring and acking.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseRichBolt {
        OutputCollector _collector;
        
        public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
            _collector = collector;
        }

        public void execute(Tuple tuple) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                _collector.emit(tuple, new Values(word));
            }
            _collector.ack(tuple);
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Storm SQL Error Log Filtering Query
DESCRIPTION: SQL script that filters error logs (status >= 400) from Apache logs and writes them to a separate Kafka topic.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://apache-logs?bootstrap-servers=localhost:9092'
CREATE EXTERNAL TABLE APACHE_ERROR_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_ELAPSED_MS INT) LOCATION 'kafka://apache-error-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO APACHE_ERROR_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, (TIME_US / 1000) AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (CAST(STATUS AS INT) / 100) >= 4

----------------------------------------

TITLE: Creating External Table SQL Syntax
DESCRIPTION: SQL syntax for creating external tables in Storm SQL, following Hive DDL conventions

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE table_name field_list
    [ STORED AS
      INPUTFORMAT input_format_classname
      OUTPUTFORMAT output_format_classname
    ]
    LOCATION location
    [ PARALLELISM parallelism ]
    [ TBLPROPERTIES tbl_properties ]
    [ AS select_stmt ]

----------------------------------------

TITLE: Using Murmur3StreamGrouping for CassandraWriterBolt in Java
DESCRIPTION: Demonstrates how to use Murmur3StreamGrouping to optimize Cassandra writes by partitioning the stream based on row partition keys.

LANGUAGE: java
CODE:
CassandraWriterBolt bolt = new CassandraWriterBolt(
    insertInto("album")
        .values(
            with(fields("title", "year", "performer", "genre", "tracks")
            ).build()));
builder.setBolt("BOLT_WRITER", bolt, 4)
        .customGrouping("spout", new Murmur3StreamGrouping("title"))

----------------------------------------

TITLE: Initializing Fixed Batch Spout in Trident
DESCRIPTION: Creates a fixed batch spout that cycles through a set of sentences to produce a continuous stream of data for testing.

LANGUAGE: java
CODE:
FixedBatchSpout spout = new FixedBatchSpout(new Fields("sentence"), 3,
               new Values("the cow jumped over the moon"),
               new Values("the man went to the store and bought some candy"),
               new Values("four score and seven years ago"),
               new Values("how many apples can you eat"));
spout.setCycle(true);

----------------------------------------

TITLE: Constructing HBaseBolt in Java
DESCRIPTION: Example of creating an HBaseBolt instance for writing data to HBase from a Storm topology.

LANGUAGE: java
CODE:
HBaseBolt hbase = new HBaseBolt("WordCount", mapper);

----------------------------------------

TITLE: Implementing RedisFilterMapper for Blacklist Filtering in Java
DESCRIPTION: Java class implementing RedisFilterMapper interface for filtering blacklisted words using Redis.

LANGUAGE: java
CODE:
class BlacklistWordFilterMapper implements RedisFilterMapper {
    private RedisDataTypeDescription description;
    private final String setKey = "blacklist";

    public BlacklistWordFilterMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.SET, setKey);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Maven Dependency Configuration for Storm-Redis
DESCRIPTION: Maven dependency configuration required to use storm-redis in a project.

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>storm-redis</artifactId>
    <version>${storm.version}</version>
    <type>jar</type>
</dependency>

----------------------------------------

TITLE: Task Hook Registration via TopologyContext
DESCRIPTION: Register a task hook through the TopologyContext in a spout's open method or bolt's prepare method to handle task-level events.

LANGUAGE: java
CODE:
TopologyContext.addTaskHook()

----------------------------------------

TITLE: Named Stream Join Configuration in Storm
DESCRIPTION: Example of joining named streams from multiple spouts using JoinBolt with stream selector configuration.

LANGUAGE: java
CODE:
new JoinBolt(JoinBolt.Selector.STREAM,  "stream1", "key1") 
                             .join     ("stream2", "userId",  "stream1" )
                             .select ("userId, key1, key2")
                             .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;
                             
topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("bolt1", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt2", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt3", "stream2", new Fields("userId") )
            .fieldsGrouping("bolt4", "stream1", new Fields("key1") );

----------------------------------------

TITLE: Restarting Worker in JSON
DESCRIPTION: Sample response from the /api/v1/topology/<id>/profiling/restartworker/<host-port> endpoint, showing the result of restarting a worker.

LANGUAGE: json
CODE:
{
   "status": "ok",
   "id": "10.11.1.7:6701"
}

----------------------------------------

TITLE: Configuring RedisFilterBolt in Java
DESCRIPTION: Java code snippet showing how to configure and create a RedisFilterBolt instance.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisFilterMapper filterMapper = new BlacklistWordFilterMapper();
RedisFilterBolt filterBolt = new RedisFilterBolt(poolConfig, filterMapper);

----------------------------------------

TITLE: Configuring Maven Shade Plugin
DESCRIPTION: Maven configuration for building an uber jar containing all dependencies required for the Storm-Solr integration.

LANGUAGE: xml
CODE:
<plugin>
     <groupId>org.apache.maven.plugins</groupId>
     <artifactId>maven-shade-plugin</artifactId>
     <version>2.4.1</version>
     <executions>
         <execution>
             <phase>package</phase>
             <goals>
                 <goal>shade</goal>
             </goals>
             <configuration>
                 <transformers>
                     <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                         <mainClass>org.apache.storm.solr.topology.SolrJsonTopology</mainClass>
                     </transformer>
                 </transformers>
             </configuration>
         </execution>
     </executions>
</plugin>

----------------------------------------

TITLE: Setting Storm Local Directory Configuration
DESCRIPTION: Configuration for specifying the local storage directory for Storm daemons

LANGUAGE: yaml
CODE:
storm.local.dir: "/mnt/storm"

----------------------------------------

TITLE: Database Table Creation SQL Queries
DESCRIPTION: SQL queries for creating and populating example database tables used in the Storm JDBC integration.

LANGUAGE: sql
CODE:
create table if not exists user (user_id integer, user_name varchar(100), dept_name varchar(100), create_date date);
create table if not exists department (dept_id integer, dept_name varchar(100));
create table if not exists user_department (user_id integer, dept_id integer);
insert into department values (1, 'R&D');
insert into department values (2, 'Finance');
insert into department values (3, 'HR');
insert into department values (4, 'Sales');
insert into user_department values (1, 1);
insert into user_department values (2, 2);
insert into user_department values (3, 3);
insert into user_department values (4, 4);

----------------------------------------

TITLE: Retrieving Topology Details in Storm UI REST API
DESCRIPTION: Example JSON response for the /api/v1/topology/:id endpoint, which returns detailed information about a specific topology, including its components and statistics.

LANGUAGE: json
CODE:
{
    "name": "WordCount3",
    "id": "WordCount3-1-1402960825",
    "workersTotal": 3,
    "window": "600",
    "status": "ACTIVE",
    "tasksTotal": 28,
    "executorsTotal": 28,
    "uptime": "29m 19s",
    "uptimeSeconds": 1759,
    "msgTimeout": 30,
    "windowHint": "10m 0s",
    "schedulerDisplayResource": true,
    "topologyStats": [
        {
            "windowPretty": "10m 0s",
            "window": "600",
            "emitted": 397960,
            "transferred": 213380,
            "completeLatency": "0.000",
            "acked": 213460,
            "failed": 0
        }
    ],
    "workers":[
        {
            "topologyName":"WordCount3",
            "topologyId":"WordCount3-1-1402960825",
            "host":"192.168.10.237",
            "supervisorId":"bdfe8eff-f1d8-4bce-81f5-9d3ae1bf432e-169.254.129.212",
            "uptime":"2m 47s",
            "uptimeSeconds":167,
            "port":6707,
            "workerLogLink":"http:\/\/192.168.10.237:8000\/log?file=WordCount3-1-1402960825%2F6707%2Fworker.log",
            "componentNumTasks": {
                "spout":5
            },
            "executorsTotal":8,
            "assignedMemOnHeap":704.0,
            "assignedCpu":130.0,
            "assignedMemOffHeap":80.0
        }
    ],
    "spouts": [
        {
            "executors": 5,
            "emitted": 28880,
            "completeLatency": "0.000",
            "transferred": 28880,
            "acked": 0,
            "spoutId": "spout",
            "tasks": 5,
            "lastError": "",
            "errorLapsedSecs": null,
            "failed": 0
        }
    ],
    "bolts": [
        {
            "executors": 12,
            "emitted": 184580,
            "transferred": 0,
            "acked": 184640,
            "executeLatency": "0.048",
            "tasks": 12,
            "executed": 184620,
            "processLatency": "0.043",
            "boltId": "count",
            "lastError": "",
            "errorLapsedSecs": null,
            "capacity": "0.003",
            "failed": 0
        }
    ],
    "configuration": {
        "storm.id": "WordCount3-1-1402960825",
        "dev.zookeeper.path": "/tmp/dev-storm-zookeeper"
    },
    "replicationCount": 1
}

----------------------------------------

TITLE: Creating ACLs for Blobs in Java
DESCRIPTION: Java code example showing how to create Access Control Lists (ACLs) for blobs in Storm's distributed cache.

LANGUAGE: java
CODE:
String stringBlobACL = "u:username:rwa";
AccessControl blobACL = BlobStoreAclHandler.parseAccessControl(stringBlobACL);
List<AccessControl> acls = new LinkedList<AccessControl>();
acls.add(blobACL); // more ACLs can be added here
SettableBlobMeta settableBlobMeta = new SettableBlobMeta(acls);
settableBlobMeta.set_replication_factor(4); // Here we can set the replication factor

----------------------------------------

TITLE: Configuring Resource Aware Scheduler in YAML
DESCRIPTION: Configuration to enable the Resource Aware Scheduler in Storm's storm.yaml file.

LANGUAGE: yaml
CODE:
storm.scheduler: "org.apache.storm.scheduler.resource.ResourceAwareScheduler"

----------------------------------------

TITLE: Uploading Credentials to a Topology
DESCRIPTION: Updates the set of credentials for a running topology.

LANGUAGE: shell
CODE:
storm upload_credentials topology-name [credkey credvalue]*

----------------------------------------

TITLE: Activating a Storm Topology
DESCRIPTION: Activates the spouts of a specified topology.

LANGUAGE: bash
CODE:
storm activate topology-name

----------------------------------------

TITLE: Adding Standard HBase Columns Example
DESCRIPTION: Example showing how to add standard columns to a ColumnList for HBase operations.

LANGUAGE: java
CODE:
ColumnList cols = new ColumnList();
cols.addColumn(this.columnFamily, field.getBytes(), toBytes(tuple.getValueByField(field)));

----------------------------------------

TITLE: Creating SimpleHBaseMapper Instance in Java
DESCRIPTION: Demonstrates how to create and configure a SimpleHBaseMapper instance for mapping Storm tuples to HBase columns.

LANGUAGE: java
CODE:
SimpleHBaseMapper mapper = new SimpleHBaseMapper() 
        .withRowKeyField("word")
        .withColumnFields(new Fields("word"))
        .withCounterFields(new Fields("count"))
        .withColumnFamily("cf");

----------------------------------------

TITLE: Managing Storm Blobstore
DESCRIPTION: Performs operations on the Storm blobstore, such as listing, creating, updating, and deleting blobs.

LANGUAGE: shell
CODE:
storm blobstore cmd

----------------------------------------

TITLE: Window Configuration Methods
DESCRIPTION: Available methods for configuring different types of windows including sliding and tumbling windows with count or duration-based parameters.

LANGUAGE: java
CODE:
withWindow(Count windowLength, Count slidingInterval)
Tuple count based sliding window that slides after `slidingInterval` number of tuples.

withWindow(Count windowLength)
Tuple count based window that slides with every incoming tuple.

withWindow(Count windowLength, Duration slidingInterval)
Tuple count based sliding window that slides after `slidingInterval` time duration.

withWindow(Duration windowLength, Duration slidingInterval)
Time duration based sliding window that slides after `slidingInterval` time duration.

withWindow(Duration windowLength)
Time duration based window that slides with every incoming tuple.

withWindow(Duration windowLength, Count slidingInterval)
Time duration based sliding window configuration that slides after `slidingInterval` number of tuples.

withTumblingWindow(BaseWindowedBolt.Count count)
Count based tumbling window that tumbles after the specified count of tuples.

withTumblingWindow(BaseWindowedBolt.Duration duration)
Time duration based tumbling window that tumbles after the specified time duration.

----------------------------------------

TITLE: Configuring Metric Reporters in Apache Storm
DESCRIPTION: YAML configuration example for setting up Graphite and Console reporters in Apache Storm's metrics system.

LANGUAGE: yaml
CODE:
topology.metrics.reporters:
  # Graphite Reporter
  - class: "org.apache.storm.metrics2.reporters.GraphiteStormReporter"
    report.period: 60
    report.period.units: "SECONDS"
    graphite.host: "localhost"
    graphite.port: 2003

  # Console Reporter
  - class: "org.apache.storm.metrics2.reporters.ConsoleStormReporter"
    report.period: 10
    report.period.units: "SECONDS"
    filter:
        class: "org.apache.storm.metrics2.filters.RegexFilter"
        expression: ".*my_component.*emitted.*"

----------------------------------------

TITLE: Bolt Tuple Transfer in Apache Storm (Clojure)
DESCRIPTION: Implementation of how bolts transfer tuples using the worker-provided transfer function.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L429

----------------------------------------

TITLE: Creating External Table in Storm SQL
DESCRIPTION: SQL syntax for creating external tables in Storm SQL, following Hive DDL format

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE table_name field_list
    [ STORED AS
      INPUTFORMAT input_format_classname
      OUTPUTFORMAT output_format_classname
    ]
    LOCATION location
    [ PARALLELISM parallelism ]
    [ TBLPROPERTIES tbl_properties ]
    [ AS select_stmt ]

----------------------------------------

TITLE: Location Database State Factory
DESCRIPTION: StateFactory implementation for creating LocationDB state instances.

LANGUAGE: java
CODE:
public class LocationDBFactory implements StateFactory {
   public State makeState(Map conf, int partitionIndex, int numPartitions) {
      return new LocationDB();
   } 
}

----------------------------------------

TITLE: Defining Storm Metrics Table Format - Markdown
DESCRIPTION: Common table format used throughout the documentation to define metric names, types and descriptions

LANGUAGE: markdown
CODE:
| Metric Name | Type | Description |
|-------------|------|-------------|

----------------------------------------

TITLE: Starting Pacemaker Daemon
DESCRIPTION: Command to start the Pacemaker daemon in a Storm cluster.

LANGUAGE: bash
CODE:
$ storm pacemaker

----------------------------------------

TITLE: Storm Topology Submission Method Definition
DESCRIPTION: Thrift method definition for submitting a Storm topology. Takes topology name, jar location, JSON configuration, and topology structure as parameters. Can throw AlreadyAliveException and InvalidTopologyException.

LANGUAGE: java
CODE:
void submitTopology(
        1: string name,
        2: string uploadedJarLocation,
        3: string jsonConf,
        4: StormTopology topology)
        throws (
                1: AlreadyAliveException e,
                2: InvalidTopologyException ite);

----------------------------------------

TITLE: Configuring DelimitedRecordHiveMapper for HiveBolt (Java)
DESCRIPTION: Java code examples showing two ways to configure DelimitedRecordHiveMapper: one with column and partition fields, and another using time as a partition field.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
    or
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withTimeAsPartitionField("YYYY/MM/DD");

----------------------------------------

TITLE: Configuring URL Expansion Bolt with Shuffle Grouping in Storm
DESCRIPTION: Example showing how to set up a bolt that expands URLs using shuffle grouping, which distributes tuples randomly across bolt tasks.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .shuffleGrouping(1);

----------------------------------------

TITLE: Creating a Typed Stream with Value Mapper in Java
DESCRIPTION: This example shows how to use a ValueMapper to extract specific fields from tuples emitted by a spout, creating a typed stream.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

// extract the first field from the tuple to get a Stream<String> of sentences
Stream<String> sentences = builder.newStream(new TestWordSpout(), new ValueMapper<String>(0));

// extract first three fields of the tuple emitted by the spout to produce a stream of typed tuples.
Stream<Tuple3<String, Integer, Long>> stream = builder.newStream(new TestSpout(), TupleValueMappers.of(0, 1, 2));

----------------------------------------

TITLE: Python Script for Parsing Apache Logs to JSON
DESCRIPTION: Script that uses apache-log-parser to convert Apache log entries into JSON format with auto-incrementing IDs.

LANGUAGE: python
CODE:
import sys\nimport apache_log_parser\nimport json\n\nauto_incr_id = 1\nparser_format = '%a - - %t %D \"%r\" %s %b \"%{Referer}i\" \"%{User-Agent}i\"'\nline_parser = apache_log_parser.make_parser(parser_format)\nwhile True:\n  line = sys.stdin.readline()\n  if not line:\n    break\n  parsed_dict = line_parser(line)\n  parsed_dict['id'] = auto_incr_id\n  auto_incr_id += 1\n\n  parsed_dict = {k.upper(): v for k, v in parsed_dict.iteritems() if not k.endswith('datetimeobj')}\n  print(json.dumps(parsed_dict))

----------------------------------------

TITLE: Defining Leader Election Interface in Java for Storm Nimbus
DESCRIPTION: Java interface defining the methods required for leader election among Nimbus servers. It includes methods for joining/leaving the leader queue, checking leadership status, and getting information about current leaders and participants.

LANGUAGE: java
CODE:
public interface ILeaderElector {
    /**
     * queue up for leadership lock. The call returns immediately and the caller                     
     * must check isLeader() to perform any leadership action.
     */
    void addToLeaderLockQueue();

    /**
     * Removes the caller from the leader lock queue. If the caller is leader
     * also releases the lock.
     */
    void removeFromLeaderLockQueue();

    /**
     *
     * @return true if the caller currently has the leader lock.
     */
    boolean isLeader();

    /**
     *
     * @return the current leader's address , throws exception if noone has has    lock.
     */
    InetSocketAddress getLeaderAddress();

    /**
     * 
     * @return list of current nimbus addresses, includes leader.
     */
    List<InetSocketAddress> getAllNimbusAddresses();
}

----------------------------------------

TITLE: Adding Standard HBase Columns Example
DESCRIPTION: Example showing how to add standard columns to a ColumnList for HBase operations.

LANGUAGE: java
CODE:
ColumnList cols = new ColumnList();
cols.addColumn(this.columnFamily, field.getBytes(), toBytes(tuple.getValueByField(field)));

----------------------------------------

TITLE: Initializing EsPercolateBolt for Elasticsearch Percolation
DESCRIPTION: Sets up an EsPercolateBolt for handling percolate requests to Elasticsearch. Requires cluster configuration and tuple mapping for source, index, and type fields.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsPercolateBolt percolateBolt = new EsPercolateBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Configuring SimpleJdbcLookupMapper for Database Lookups in Java
DESCRIPTION: This code snippet shows how to configure a SimpleJdbcLookupMapper for executing select queries and mapping results to Storm tuples. It demonstrates setting up output fields and query parameter columns.

LANGUAGE: java
CODE:
Fields outputFields = new Fields("user_id", "user_name", "create_date");
List<Column> queryParamColumns = Lists.newArrayList(new Column("user_id", Types.INTEGER));
this.jdbcLookupMapper = new SimpleJdbcLookupMapper(outputFields, queryParamColumns);

----------------------------------------

TITLE: Managing Storm Blobstore
DESCRIPTION: Performs operations on the Storm blobstore, including listing, creating, updating, and deleting blobs.

LANGUAGE: bash
CODE:
storm blobstore cmd

----------------------------------------

TITLE: Chaining Map and FlatMap Operations
DESCRIPTION: Example of chaining map and flatMap operations on a Trident stream.

LANGUAGE: java
CODE:
mystream.flatMap(new Split()).map(new UpperCase())

----------------------------------------

TITLE: Storm Emit Command JSON Format
DESCRIPTION: JSON structure for emitting tuples from a shell process to Storm. Includes command type, anchor tuple IDs, stream ID, target task, and tuple values.

LANGUAGE: json
CODE:
{
    "command": "emit",
    "anchors": [1231231, -234234234],
    "stream": 1,
    "task": 9,
    "tuple": ["field1", 2, 3]
}

----------------------------------------

TITLE: Configuring Kerberos Authentication in Storm YAML
DESCRIPTION: YAML configuration for enabling Kerberos authentication in Storm, including transport plugin and JAAS config file path.

LANGUAGE: yaml
CODE:
storm.thrift.transport: "org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin"
java.security.auth.login.config: "/path/to/jaas.conf"

----------------------------------------

TITLE: Storm Plugin Configuration Service Definition
DESCRIPTION: Path specification for Storm's configuration validation service loader. This file must be included in the JAR to register custom configuration classes.

LANGUAGE: text
CODE:
META-INF/services/org.apache.storm.validation.Validated

----------------------------------------

TITLE: Dumping Heap on Topology Worker in JSON
DESCRIPTION: Example response from the /api/v1/topology/<id>/profiling/dumpheap/<host-port> endpoint, which dumps the heap (jmap) on a worker for a topology.

LANGUAGE: json
CODE:
{
  "status": "ok",
  "id": "10.11.1.7:6701"
}

----------------------------------------

TITLE: Implementing RedisFilterMapper in Java for Blacklist Filtering
DESCRIPTION: This code defines a BlacklistWordFilterMapper class that implements RedisFilterMapper. It's used to filter words based on a blacklist stored in a Redis set. The class specifies how to map Redis data for filtering.

LANGUAGE: java
CODE:
class BlacklistWordFilterMapper implements RedisFilterMapper {
    private RedisDataTypeDescription description;
    private final String setKey = "blacklist";

    public BlacklistWordFilterMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.SET, setKey);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Task Routing Map in Apache Storm (Clojure)
DESCRIPTION: Tasks have a routing map from stream ID to component ID to stream grouping function for message routing.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L198

----------------------------------------

TITLE: Creating SolrJsonMapper in Java
DESCRIPTION: This snippet demonstrates how to create a SolrJsonMapper object to update the 'gettingstarted' Solr collection with JSON content declared in the tuple field named 'JSON'.

LANGUAGE: java
CODE:
SolrMapper solrMapper = new SolrJsonMapper.Builder("gettingstarted", "JSON").build();

----------------------------------------

TITLE: Configuring Worker-Level Metrics in YAML
DESCRIPTION: Shows how to configure worker-level metrics in storm.yaml configuration file.

LANGUAGE: yaml
CODE:
worker.metrics: 
  metricA: "aaa.bbb.ccc.ddd.MetricA"
  metricB: "aaa.bbb.ccc.ddd.MetricB"
  ...

----------------------------------------

TITLE: Setting Health Check Timeout in Storm YAML
DESCRIPTION: This configuration specifies the maximum time allowed for a health check script to run before it's marked as failed due to timeout.

LANGUAGE: yaml
CODE:
storm.health.check.timeout.ms: 5000

----------------------------------------

TITLE: Defining Storm Component Object Structure in Thrift
DESCRIPTION: Thrift union definition for specifying how Storm components (spouts/bolts) can be implemented, supporting serialized Java, shell components, and Java objects.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Location State Updater Implementation
DESCRIPTION: Example showing how to implement a StateUpdater for bulk updating location data

LANGUAGE: java
CODE:
public class LocationUpdater extends BaseStateUpdater<LocationDB> {
    public void updateState(LocationDB state, List<TridentTuple> tuples, TridentCollector collector) {
        List<Long> ids = new ArrayList<Long>();
        List<String> locations = new ArrayList<String>();
        for(TridentTuple t: tuples) {
            ids.add(t.getLong(0));
            locations.add(t.getString(1));
        }
        state.setLocationsBulk(ids, locations);
    }
}

----------------------------------------

TITLE: Configuring Flush Tuple Frequency in Storm
DESCRIPTION: Sets the frequency of flush tuple generation to manage message batching and latency in low traffic scenarios.

LANGUAGE: java
CODE:
topology.flush.tuple.freq.millis

----------------------------------------

TITLE: Creating HBaseProjectionCriteria in Java
DESCRIPTION: Demonstrates how to create and configure an HBaseProjectionCriteria instance for specifying HBase column projections.

LANGUAGE: java
CODE:
HBaseProjectionCriteria projectionCriteria = new HBaseProjectionCriteria()
    .addColumn(new HBaseProjectionCriteria.ColumnMetaData("cf", "count"))
    .addColumnFamily("cf2");

----------------------------------------

TITLE: Storm Shell Command Usage
DESCRIPTION: Example command showing how to use Storm shell to package and submit a topology using Python. Takes a resources directory and Python topology file with arguments.

LANGUAGE: shell
CODE:
storm shell resources/ python topology.py arg1 arg2

----------------------------------------

TITLE: Creating JdbcTridentState for Trident Topologies in Java
DESCRIPTION: Example of creating a JdbcTridentState for use with Trident topologies. It demonstrates setting up options including connection provider, mapper, table name, and query timeout.

LANGUAGE: java
CODE:
JdbcState.Options options = new JdbcState.Options()
        .withConnectionProvider(connectionProvider)
        .withMapper(jdbcMapper)
        .withTableName("user_details")
        .withQueryTimeoutSecs(30);
JdbcStateFactory jdbcStateFactory = new JdbcStateFactory(options);

----------------------------------------

TITLE: Storm Topology Java Configuration
DESCRIPTION: Java code for configuring MQTT topology using Storm Core API

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
MqttOptions options = new MqttOptions();
options.setTopics(Arrays.asList("/users/tgoetz/#"));
options.setCleanConnection(false);
MqttSpout spout = new MqttSpout(new StringMessageMapper(), options);

MqttBolt bolt = new LogInfoBolt();

builder.setSpout("mqtt-spout", spout);
builder.setBolt("log-bolt", bolt).shuffleGrouping("mqtt-spout");

return builder.createTopology();

----------------------------------------

TITLE: GetTime2 UDF Implementation
DESCRIPTION: Java class implementing a User Defined Function to convert date strings to Unix timestamps

LANGUAGE: java
CODE:
package org.apache.storm.sql.runtime.functions.scalar.datetime;

import org.joda.time.format.DateTimeFormat;
import org.joda.time.format.DateTimeFormatter;

public class GetTime2 {
    public static Long evaluate(String dateString, String dateFormat) {
        try {
            DateTimeFormatter df = DateTimeFormat.forPattern(dateFormat).withZoneUTC();
            return df.parseDateTime(dateString).getMillis();
        } catch (Exception ex) {
            throw new RuntimeException(ex);
        }
    }
}

----------------------------------------

TITLE: Defining User Defined Function in Storm SQL
DESCRIPTION: SQL statement to create a user defined function (UDF) in Storm SQL.

LANGUAGE: sql
CODE:
CREATE FUNCTION MYPLUS AS 'org.apache.storm.sql.TestUtils$MyPlus'

----------------------------------------

TITLE: Creating CassandraWriterBolt with Multiple Queries
DESCRIPTION: Demonstrates how to create a CassandraWriterBolt that executes multiple insert queries from one input tuple.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
    async(
        simpleQuery("INSERT INTO titles_per_album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all())),
        simpleQuery("INSERT INTO titles_per_performer (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all()))
    )
);

----------------------------------------

TITLE: Creating HBaseProjectionCriteria in Java
DESCRIPTION: Example of creating an HBaseProjectionCriteria instance for specifying HBase column projections.

LANGUAGE: java
CODE:
HBaseProjectionCriteria projectionCriteria = new HBaseProjectionCriteria()
    .addColumn(new HBaseProjectionCriteria.ColumnMetaData("cf", "count"))
    .addColumnFamily("cf2");

----------------------------------------

TITLE: Initializing ZkHosts for Kafka Broker Discovery
DESCRIPTION: Code examples showing how to instantiate ZkHosts class for dynamic Kafka broker tracking via ZooKeeper

LANGUAGE: java
CODE:
public ZkHosts(String brokerZkStr, String brokerZkPath)
public ZkHosts(String brokerZkStr)

----------------------------------------

TITLE: Creating SolrJsonMapper Instance
DESCRIPTION: Example of creating a SolrJsonMapper object to update the Solr collection with JSON content from a specific tuple field.

LANGUAGE: java
CODE:
    SolrMapper solrMapper = new SolrJsonMapper.Builder("gettingstarted", "JSON").build();

----------------------------------------

TITLE: Task Hook Registration Methods
DESCRIPTION: Two methods for registering task hooks in Storm: using TopologyContext in the open/prepare methods of spouts/bolts, or through Storm configuration using topology.auto.task.hooks config.

LANGUAGE: java
CODE:
// Method 1: In spout/bolt
TopologyContext.addTaskHook()

// Method 2: In configuration
Config.TOPOLOGY_AUTO_TASK_HOOKS

----------------------------------------

TITLE: Storm Topology with KestrelSpout Configuration
DESCRIPTION: Storm topology setup that reads from Kestrel queue and processes sentences. Uses KestrelSpout for queue consumption and implements word counting functionality.

LANGUAGE: java
CODE:
    TopologyBuilder builder = new TopologyBuilder();
    builder.setSpout("sentences", new KestrelSpout("localhost",22133,"sentence_queue",new StringScheme()));
    builder.setBolt("split", new SplitSentence(), 10)
    	        .shuffleGrouping("sentences");
    builder.setBolt("count", new WordCount(), 20)
	        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Configuring Kerberos Authentication for Storm UI
DESCRIPTION: YAML configuration for setting up Kerberos authentication for the Storm UI using AuthenticationFilter

LANGUAGE: yaml
CODE:
ui.filter: "org.apache.hadoop.security.authentication.server.AuthenticationFilter"
ui.filter.params:
   "type": "kerberos"
   "kerberos.principal": "HTTP/nimbus.witzend.com"
   "kerberos.keytab": "/vagrant/keytabs/http.keytab"
   "kerberos.name.rules": "RULE:[2:$1@$0]([jt]t@.*EXAMPLE.COM)s/.*/$MAPRED_USER/ RULE:[2:$1@$0]([nd]n@.*EXAMPLE.COM)s/.*/$HDFS_USER/DEFAULT"

----------------------------------------

TITLE: Creating JdbcTridentState for Trident Topologies in Java
DESCRIPTION: Example of creating a JdbcTridentState for use with Trident topologies. It demonstrates setting up options including connection provider, mapper, table name, and query timeout.

LANGUAGE: java
CODE:
JdbcState.Options options = new JdbcState.Options()
        .withConnectionProvider(connectionProvider)
        .withMapper(jdbcMapper)
        .withTableName("user_details")
        .withQueryTimeoutSecs(30);
JdbcStateFactory jdbcStateFactory = new JdbcStateFactory(options);

----------------------------------------

TITLE: Configuring Bolt Parallelism in Storm Topology
DESCRIPTION: Demonstrates how to set the number of executors and tasks for a bolt in a Storm topology. This example configures the 'green-bolt' with 2 executors and 4 tasks, using shuffle grouping from 'blue-spout'.

LANGUAGE: java
CODE:
topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

----------------------------------------

TITLE: Storm Ack Command JSON Structure
DESCRIPTION: JSON structure for acknowledging tuple processing in Storm's multilang protocol.

LANGUAGE: json
CODE:
{
    "command": "ack",
    "id": 123123
}

----------------------------------------

TITLE: Configuring KafkaSpout with Simple Topic Subscription
DESCRIPTION: Example of creating a KafkaSpout to consume from a single Kafka topic

LANGUAGE: java
CODE:
final TopologyBuilder tp = new TopologyBuilder();
tp.setSpout("kafka_spout", new KafkaSpout<>(KafkaSpoutConfig.builder("127.0.0.1:" + port, "topic").build()), 1);
tp.setBolt("bolt", new myBolt()).shuffleGrouping("kafka_spout");

----------------------------------------

TITLE: Adding Storm Client Dependency in Maven POM
DESCRIPTION: Maven dependency configuration to include Storm client library in a project. The scope is set to 'provided' since Storm will be available in the cluster environment.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.storm</groupId>
  <artifactId>storm-client</artifactId>
  <version>{{page.version}}</version>
  <scope>provided</scope>
</dependency>

----------------------------------------

TITLE: HTML Redirect Meta Tag for Storm Documentation
DESCRIPTION: HTML meta refresh tag that automatically redirects users to the Storm documentation page about using non-JVM languages. Also includes a canonical link reference to the target page.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Using-non-JVM-languages-with-Storm.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Using-non-JVM-languages-with-Storm.html" />

----------------------------------------

TITLE: Installing ZeroMQ for Storm
DESCRIPTION: This snippet shows the process of downloading, extracting, configuring, and installing ZeroMQ 2.1.7, which is the recommended version for use with Storm.

LANGUAGE: bash
CODE:
wget http://download.zeromq.org/zeromq-2.1.7.tar.gz
tar -xzf zeromq-2.1.7.tar.gz
cd zeromq-2.1.7
./configure
make
sudo make install

----------------------------------------

TITLE: Image Tag to Manifest File Format
DESCRIPTION: Example content showing mapping between image tags and manifest file hashes

LANGUAGE: bash
CODE:
storm/rhel7:dev_current:26fd443859325d5911f3be5c5e231dddca88ee0d526456c0c92dd794148d8585#docker.xxx.com:4443/hadoop-user-images/storm/rhel7:20201202-232133

----------------------------------------

TITLE: Kafka Stream Filtering Example
DESCRIPTION: Complete example of filtering Kafka streams using Storm SQL

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://orders?bootstrap-servers=localhost:9092,localhost:9093'
CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) LOCATION 'kafka://large_orders?bootstrap-servers=localhost:9092,localhost:9093' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Storm Configuration Parameters
DESCRIPTION: Configuration properties used for performance tuning in Storm, including buffer sizes, batch processing settings, flush frequency, and wait strategy parameters.

LANGUAGE: properties
CODE:
topology.executor.receive.buffer.size
topology.transfer.buffer.size
topology.producer.batch.size
topology.transfer.batch.size
topology.flush.tuple.freq.millis
topology.spout.wait.strategy
topology.bolt.wait.strategy
topology.backpressure.wait.strategy
topology.max.spout.pending
topology.disable.loadaware.messaging
topology.stats.sample.rate

----------------------------------------

TITLE: Configuring EsConfig for Elasticsearch Connection
DESCRIPTION: Creates EsConfig instances with basic and advanced configuration options for connecting to Elasticsearch cluster.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});

// Alternative configuration with additional parameters
Map<String, String> additionalParameters = new HashMap<>();
additionalParameters.put("client.transport.sniff", "true");
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"}, additionalParameters);

----------------------------------------

TITLE: Configuring EsConfig for Elasticsearch Connection
DESCRIPTION: Creates EsConfig instances with basic and advanced configuration options for connecting to Elasticsearch cluster.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});

// Alternative configuration with additional parameters
Map<String, String> additionalParameters = new HashMap<>();
additionalParameters.put("client.transport.sniff", "true");
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"}, additionalParameters);

----------------------------------------

TITLE: Implementing User Defined Function in Storm SQL
DESCRIPTION: Example of a scalar user defined function class that implements addition functionality.

LANGUAGE: java
CODE:
  public class MyPlus {
    public static Integer evaluate(Integer x, Integer y) {
      return x + y;
    }
  }

----------------------------------------

TITLE: Stopping Profiler in Storm UI REST API
DESCRIPTION: Example JSON response for the /api/v1/topology/:id/profiling/stop/:host-port endpoint, which requests to stop the profiler on a worker.

LANGUAGE: json
CODE:
{
   "status": "ok",
   "id": "10.11.1.7:6701"
}

----------------------------------------

TITLE: Storm Acker Execute Implementation
DESCRIPTION: The execute method handles tuple tracking through XOR checksum operations. It processes init, ack, and fail events for tupletrees, maintaining checksums in a RotatingMap. When a tupletree's checksum reaches zero or fails, it notifies the spout of completion status.



----------------------------------------

TITLE: Implementing a CombinerAggregator in Trident
DESCRIPTION: Example of implementing a CombinerAggregator for counting tuples efficiently.

LANGUAGE: java
CODE:
public class Count implements CombinerAggregator<Long> {
    public Long init(TridentTuple tuple) {
        return 1L;
    }

    public Long combine(Long val1, Long val2) {
        return val1 + val2;
    }

    public Long zero() {
        return 0L;
    }
}

----------------------------------------

TITLE: Setting Storm Local Directory in YAML
DESCRIPTION: This code snippet demonstrates how to set the local directory for Storm to store small amounts of state. It provides examples for both Unix-like systems and Windows.

LANGUAGE: yaml
CODE:
storm.local.dir: "/mnt/storm"

LANGUAGE: yaml
CODE:
storm.local.dir: "C:\\storm-local"

----------------------------------------

TITLE: Worker Connection Management in Storm
DESCRIPTION: Code handling worker connection refresh and maintenance of task-to-worker mappings. Called periodically or when ZooKeeper assignments change.

LANGUAGE: clojure
CODE:
refresh-connections

----------------------------------------

TITLE: Launching Storm UI Daemon
DESCRIPTION: Starts the UI daemon to provide a web interface for the Storm cluster.

LANGUAGE: shell
CODE:
storm ui

----------------------------------------

TITLE: User-Defined Function Example
DESCRIPTION: Example implementation of a user-defined function class in Java

LANGUAGE: java
CODE:
public class MyPlus {
    public static Integer evaluate(Integer x, Integer y) {
      return x + y;
    }
  }

----------------------------------------

TITLE: Configuring Generic Cluster Resources in YAML
DESCRIPTION: This snippet shows how to specify the availability of generic resources on a Storm cluster node by modifying the storm.yaml configuration file. It defines a map of resource types and their amounts.

LANGUAGE: yaml
CODE:
supervisor.resources.map: {[type<String>] : [amount<Double>]}

LANGUAGE: yaml
CODE:
supervisor.resources.map: {"gpu.count" : 2.0}

----------------------------------------

TITLE: Implementing Named Query Parameters with Bound Statements
DESCRIPTION: Demonstrates how to use named parameters in CQL queries with bound statements, providing better query readability and maintenance.

LANGUAGE: java
CODE:
    new CassandraWriterBolt(
         async(
            boundQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (:ti, :ye, :pe, :ge, :tr);")                .bind(
                    field("ti").as("title"),
                    field("ye").as("year")),
                    field("pe").as("performer")),
                    field("ge").as("genre")),
                    field("tr").as("tracks"))
                ).byNamedSetters()
         )
    );

----------------------------------------

TITLE: Adding Counter Column Example
DESCRIPTION: Example showing how to add a counter column to a ColumnList instance.

LANGUAGE: java
CODE:
ColumnList cols = new ColumnList();
cols.addCounter(this.columnFamily, field.getBytes(), toLong(tuple.getValueByField(field)));

----------------------------------------

TITLE: SQL Setup for Storm JDBC Integration Example
DESCRIPTION: This SQL snippet shows the setup queries for creating tables and inserting initial data for the Storm JDBC integration example. It includes creating user, department, and user_department tables, and inserting sample data.

LANGUAGE: sql
CODE:
create table if not exists user (user_id integer, user_name varchar(100), dept_name varchar(100), create_date date);
create table if not exists department (dept_id integer, dept_name varchar(100));
create table if not exists user_department (user_id integer, dept_id integer);
insert into department values (1, 'R&D');
insert into department values (2, 'Finance');
insert into department values (3, 'HR');
insert into department values (4, 'Sales');
insert into user_department values (1, 1);
insert into user_department values (2, 2);
insert into user_department values (3, 3);
insert into user_department values (4, 4);
select dept_name from department, user_department where department.dept_id = user_department.dept_id and user_department.user_id = ?;

----------------------------------------

TITLE: Setting Log Level via Storm CLI
DESCRIPTION: Demonstrates how to use the Storm CLI to set a log level for a specific logger in a running topology. The command allows specifying the topology name, logger name, log level, and an optional timeout.

LANGUAGE: bash
CODE:
./bin/storm set_log_level [topology name] -l [logger name]=[LEVEL]:[TIMEOUT]

----------------------------------------

TITLE: Retrieving Topology Summary in Storm UI REST API
DESCRIPTION: GET request to retrieve summary information for all topologies, including status, uptime, and resource usage.

LANGUAGE: json
CODE:
{
  "topologies": [
    {
      "id": "WordCount3-1-1402960825",
      "name": "WordCount3",
      "status": "ACTIVE",
      "uptime": "6m 5s",
      "uptimeSeconds": 365,
      "tasksTotal": 28,
      "workersTotal": 3,
      "executorsTotal": 28,
      "replicationCount": 1,
      "requestedMemOnHeap": 640,
      "requestedMemOffHeap": 128,
      "requestedTotalMem": 768,
      "requestedCpu": 80,
      "assignedMemOnHeap": 640,
      "assignedMemOffHeap": 128,
      "assignedTotalMem": 768,
      "assignedCpu": 80
    }
  ],
  "schedulerDisplayResource": true
}

----------------------------------------

TITLE: Adding Storm Client Dependency in Maven POM
DESCRIPTION: Maven dependency configuration to include Storm client library in a project. The scope is set to 'provided' since Storm will be available in the cluster environment.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.storm</groupId>
  <artifactId>storm-client</artifactId>
  <version>{{page.version}}</version>
  <scope>provided</scope>
</dependency>

----------------------------------------

TITLE: Defining Code Distribution Interface in Java for Storm Nimbus
DESCRIPTION: Interface for distributing topology code across Storm cluster. Includes methods for uploading, downloading, and managing topology code files, supporting various distribution mechanisms like BitTorrent or HDFS.

LANGUAGE: java
CODE:
/**
 * Interface responsible to distribute code in the cluster.
 */
public interface ICodeDistributor {
    /**
     * Prepare this code distributor.
     * @param conf
     */
    void prepare(Map conf);

    /**
     * This API will perform the actual upload of the code to the distributed implementation.
     * The API should return a Meta file which should have enough information for downloader 
     * so it can download the code e.g. for bittorrent it will be a torrent file, in case of something         
     * like HDFS or s3  it might have the actual directory or paths for files to be downloaded.
     * @param dirPath local directory where all the code to be distributed exists.
     * @param topologyId the topologyId for which the meta file needs to be created.
     * @return metaFile
     */
    File upload(Path dirPath, String topologyId);

    /**
     * Given the topologyId and metafile, download the actual code and return the downloaded file's list.
     * @param topologyid
     * @param metafile 
     * @param destDirPath the folder where all the files will be downloaded.
     * @return
     */
    List<File> download(Path destDirPath, String topologyid, File metafile);

    /**
      * Given the topologyId, returns number of hosts where the code has been replicated.
      */
    int getReplicationCount(String topologyId);
    
   /**
     * Performs the cleanup.
     * @param topologyid
     */
    void cleanup(String topologyid);

    /**
     * Close this distributor.
     * @param conf
     */
    void close(Map conf);
}

----------------------------------------

TITLE: Configuring Kerberos Authentication for Pacemaker Client
DESCRIPTION: JAAS configuration for Kerberos authentication on Nimbus for Pacemaker client.

LANGUAGE: java
CODE:
PacemakerClient {
    com.sun.security.auth.module.Krb5LoginModule required
    useKeyTab=true
    keyTab="/etc/keytabs/nimbus.keytab"
    storeKey=true
    useTicketCache=false
    serviceName="pacemaker"
    principal="nimbus@MY.COMPANY.COM";
};

----------------------------------------

TITLE: Configuring SSL/TLS for MQTT in Storm using Flux YAML
DESCRIPTION: Flux YAML configuration for setting up SSL/TLS in an MQTT Storm topology.

LANGUAGE: yaml
CODE:
name: "mqtt-topology"

components:
   ########## MQTT Spout Config ############
  - id: "mqtt-type"
    className: "org.apache.storm.mqtt.examples.CustomMessageMapper"

  - id: "keystore-loader"
    className: "org.apache.storm.mqtt.ssl.DefaultKeyStoreLoader"
    constructorArgs:
      - "keystore.jks"
      - "truststore.jks"
    properties:
      - name: "keyPassword"
        value: "password"
      - name: "keyStorePassword"
        value: "password"
      - name: "trustStorePassword"
        value: "password"

  - id: "mqtt-options"
    className: "org.apache.storm.mqtt.common.MqttOptions"
    properties:
      - name: "url"
        value: "ssl://raspberrypi.local:8883"
      - name: "topics"
        value:
          - "/users/tgoetz/#"

# topology configuration
config:
  topology.workers: 1
  topology.max.spout.pending: 1000

# spout definitions
spouts:
  - id: "mqtt-spout"
    className: "org.apache.storm.mqtt.spout.MqttSpout"
    constructorArgs:
      - ref: "mqtt-type"
      - ref: "mqtt-options"
      - ref: "keystore-loader"
    parallelism: 1

# bolt definitions
bolts:

  - id: "log"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1


streams:

  - from: "mqtt-spout"
    to: "log"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Implementing a MapFunction in Trident
DESCRIPTION: Example of a MapFunction that converts words to uppercase.

LANGUAGE: java
CODE:
public class UpperCase extends MapFunction {
 @Override
 public Values execute(TridentTuple input) {
   return new Values(input.getString(0).toUpperCase());
 }
}

----------------------------------------

TITLE: Implementing a Custom Filter in Trident
DESCRIPTION: Example of creating a custom filter that keeps tuples based on specific field values.

LANGUAGE: java
CODE:
public class MyFilter extends BaseFilter {
    public boolean isKeep(TridentTuple tuple) {
        return tuple.getInteger(0) == 1 && tuple.getInteger(1) == 2;
    }
}

----------------------------------------

TITLE: Running ActiveMQ for Storm JMS Example
DESCRIPTION: Command to start Apache ActiveMQ server for the JMS example topology.

LANGUAGE: bash
CODE:
$ [ACTIVEMQ_HOME]/bin/activemq

----------------------------------------

TITLE: Registering a Metrics Consumer in Java
DESCRIPTION: Shows how to register a metrics consumer to a topology configuration in Java code.

LANGUAGE: java
CODE:
conf.registerMetricsConsumer(org.apache.storm.metric.LoggingMetricsConsumer.class, 1);

----------------------------------------

TITLE: Basic DRPC Client Configuration in Java
DESCRIPTION: Example showing how to configure and create a DRPC client with retry settings and transport configuration.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.put("storm.thrift.transport", "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin");
conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 3);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL, 10);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING, 20);
DRPCClient client = new DRPCClient(conf, "drpc-host", 3772);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Defining Storm Component Object Structure in Thrift
DESCRIPTION: Thrift union structure definition for Storm components that allows specifying different implementation types: serialized Java, shell components, or Java objects. Used for defining spouts and bolts in non-JVM languages.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Continuous Item Addition to Kestrel Queue in Java
DESCRIPTION: This program continuously adds sentence items to a Kestrel queue named 'sentence_queue' on a local Kestrel server. It runs until the user inputs a closing bracket character ']' to stop the process.

LANGUAGE: java
CODE:
    import java.io.IOException;
    import java.io.InputStream;
    import java.util.Random;

    import org.apache.storm.spout.KestrelClient;
    import org.apache.storm.spout.KestrelClient.Item;
    import org.apache.storm.spout.KestrelClient.ParseError;

    public class AddSentenceItemsToKestrel {

    	/**
    	 * @param args
    	 */
    	public static void main(String[] args) {

    		InputStream is = System.in;

			char closing_bracket = ']';

			int val = closing_bracket;

			boolean aux = true;

			try {

				KestrelClient kestrelClient = null;
				String queueName = "sentence_queue";

				while(aux){

					kestrelClient = new KestrelClient("localhost",22133);

					queueSentenceItems(kestrelClient, queueName);

					kestrelClient.close();

					Thread.sleep(1000);

					if(is.available()>0){
					 if(val==is.read())
						 aux=false;
					}
				}
			} catch (IOException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			catch (ParseError e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			} catch (InterruptedException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}

			System.out.println("end");

	    }
	}

----------------------------------------

TITLE: Setting Available Resources on Node in YAML
DESCRIPTION: YAML configuration to specify available memory and CPU resources for a Storm node.

LANGUAGE: yaml
CODE:
supervisor.memory.capacity.mb: 20480.0
supervisor.cpu.capacity: 100.0

----------------------------------------

TITLE: Registering Event Logger in Java Configuration
DESCRIPTION: Example of registering a custom event logger implementation in Storm topology configuration using Java.

LANGUAGE: java
CODE:
conf.registerEventLogger(org.apache.storm.metric.FileBasedEventLogger.class);

----------------------------------------

TITLE: SQL Join Example
DESCRIPTION: Example SQL query showing a join between 4 tables using inner and left joins. This serves as a reference for the equivalent Storm implementation.

LANGUAGE: sql
CODE:
select  userId, key4, key2, key3
from        table1
inner join  table2  on table2.userId =  table1.key1
inner join  table3  on table3.key3   =  table2.userId
left join   table4  on table4.key4   =  table3.key3

----------------------------------------

TITLE: Configuring Single Pacemaker Host (Deprecated)
DESCRIPTION: This snippet shows the deprecated way of configuring a single Pacemaker host in the Storm cluster configuration.

LANGUAGE: yaml
CODE:
pacemaker.host: single_pacemaker.mycompany.com

----------------------------------------

TITLE: Configuring Trident Topology with Fields Mapper in Java
DESCRIPTION: This snippet shows how to set up a Trident topology using a SolrFieldsMapper. It configures the mapper to update the 'gettingstarted' Solr collection using field values from tuple fields that match the schema defined by a RestJsonSchemaBuilder.

LANGUAGE: java
CODE:
new SolrStateFactory(solrConfig, solrMapper);

// zkHostString for Solr 'gettingstarted' example
SolrConfig solrConfig = new SolrConfig("127.0.0.1:9983");

/* Solr Fields Mapper used to generate 'SolrRequest' requests to update the "gettingstarted" Solr collection. The Solr index is updated using the field values of the tuple fields that match static or dynamic fields declared in the schema object build using schemaBuilder */ 
SolrMapper solrMapper = new SolrFieldsMapper.Builder(schemaBuilder, "gettingstarted").build();

// builds the Schema object from the JSON representation of the schema as returned by the URL http://localhost:8983/solr/gettingstarted/schema/ 
SchemaBuilder schemaBuilder = new RestJsonSchemaBuilder("localhost", "8983", "gettingstarted")

----------------------------------------

TITLE: Configuring EsState for Trident State Integration
DESCRIPTION: Initializes an Elasticsearch Trident state using EsConfig and EsTupleMapper. Creates a state factory and persists the stream state to Elasticsearch.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();

StateFactory factory = new EsStateFactory(esConfig, tupleMapper);
TridentState state = stream.partitionPersist(factory, esFields, new EsUpdater(), new Fields());

----------------------------------------

TITLE: Defining User-Defined Functions in Storm SQL
DESCRIPTION: Example of defining a user-defined function MYPLUS in Storm SQL using a Java class.

LANGUAGE: sql
CODE:
CREATE FUNCTION MYPLUS AS 'org.apache.storm.sql.TestUtils$MyPlus'

----------------------------------------

TITLE: Initializing Trident Topology with Persistent Aggregation
DESCRIPTION: Example of setting up a Trident topology with a persistent aggregate operation for word counting using Memcached as the state backend.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
      topology.newStream("spout1", spout)
        .each(new Fields("sentence"), new Split(), new Fields("word"))
        .groupBy(new Fields("word"))
        .persistentAggregate(MemcachedState.opaque(serverLocations), new Count(), new Fields("count"))                
        .parallelismHint(6);

----------------------------------------

TITLE: Applying a FlatMap Function to a Trident Stream
DESCRIPTION: Example of how to apply a flatMap function to a Trident stream.

LANGUAGE: java
CODE:
mystream.flatMap(new Split())

----------------------------------------

TITLE: Dequeuing and Removing Items from Kestrel
DESCRIPTION: Method to dequeue and remove items from a Kestrel queue. Processes up to 12 items, acknowledging each item after retrieval.

LANGUAGE: java
CODE:
    private static void dequeueAndRemoveItems(KestrelClient kestrelClient, String queueName)
    throws IOException, ParseError
		 {
			for(int i=1; i<=12; i++){

				Item item = kestrelClient.dequeue(queueName);


				if(item==null){
					System.out.println("The queue (" + queueName + ") contains no items.");
				}
				else
				{
					int itemID = item._id;


					byte[] data = item._data;

					String receivedVal = new String(data);

					kestrelClient.ack(queueName, itemID);

					System.out.println("receivedItem=" + receivedVal);
				}
			}
	}

----------------------------------------

TITLE: Creating SolrFieldsMapper with Custom Token
DESCRIPTION: Example demonstrating SolrFieldsMapper creation with custom multivalue field token for the 'gettingstarted' collection.

LANGUAGE: java
CODE:
    new SolrFieldsMapper.Builder(
            new RestJsonSchemaBuilder("localhost", "8983", "gettingstarted"), "gettingstarted")
                .setMultiValueFieldToken("%").build();

----------------------------------------

TITLE: Configuring Remote Storm Cluster Connection in YAML
DESCRIPTION: YAML configuration for connecting to a remote Storm cluster by specifying the Nimbus seed nodes. This configuration should be placed in the ~/.storm/storm.yaml file to enable remote topology management.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["123.45.678.890"]

----------------------------------------

TITLE: Spout Message Listening in Apache Storm (Clojure)
DESCRIPTION: This code implements the message listening functionality for spouts in Apache Storm.

LANGUAGE: clojure
CODE:
(github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L382)

----------------------------------------

TITLE: Reading Worker-Level Data in Storm Components
DESCRIPTION: Illustrates how to read worker-level data that has been shared across executors, tasks, worker hooks, and task hooks managed by the concerned worker.

LANGUAGE: java
CODE:
WorkerTopologyContext#getResouce(String)

LANGUAGE: java
CODE:
TopologyContext#getResource(String)

----------------------------------------

TITLE: Configuring ExpandUrl Bolt with Fields Grouping in Storm
DESCRIPTION: This snippet shows how to set up an ExpandUrl bolt with fields grouping in a Storm topology. This configuration ensures that tuples with the same 'url' field always go to the same bolt task, improving cache efficiency.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .fieldsGrouping("urls", new Fields("url"));

----------------------------------------

TITLE: Defining TupleToMessageMapper Interface for RocketMQ Integration in Java
DESCRIPTION: This code defines the TupleToMessageMapper interface, which is used to map Storm tuples to RocketMQ messages. It includes methods for extracting the key and value from a tuple.

LANGUAGE: java
CODE:
public interface TupleToMessageMapper extends Serializable {
    String getKeyFromTuple(ITuple tuple);
    byte[] getValueFromTuple(ITuple tuple);
}

----------------------------------------

TITLE: Toggling Load-Aware Messaging in Storm
DESCRIPTION: Enables or disables load-aware messaging for shuffle grouping, which can impact performance based on topology and deployment characteristics.

LANGUAGE: yaml
CODE:
topology.disable.loadaware.messaging: false

----------------------------------------

TITLE: Displaying jQuery and Sizzle.js License Text
DESCRIPTION: This snippet contains the full license text for jQuery v3.6.1 and the included Sizzle.js library. It outlines the permissions, conditions, and disclaimers for using and distributing the software under the MIT License.

LANGUAGE: plaintext
CODE:
jQuery v 3.6.1
Copyright OpenJS Foundation and other contributors, https://openjsf.org/

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

******************************************

The jQuery JavaScript Library v3.6.1 also includes Sizzle.js

Sizzle.js includes the following license:

Copyright JS Foundation and other contributors, https://js.foundation/

This software consists of voluntary contributions made by many
individuals. For exact contribution history, see the revision history
available at https://github.com/jquery/sizzle

The following license applies to all parts of this software except as
documented below:

====

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

====

All files located in the node_modules and external directories are
externally maintained libraries used by this software which have their
own licenses; we recommend you read them, as their terms may differ from
the terms above.

----------------------------------------

TITLE: Declaring Storm Bolt Inputs in Clojure
DESCRIPTION: Example showing different ways to declare bolt inputs including shuffle grouping, fields grouping and global grouping.

LANGUAGE: clojure
CODE:
{["2" "1"] :shuffle
 "3" ["field1" "field2"]
 ["4" "2"] :global}

----------------------------------------

TITLE: Implementing Split Function for Trident in Java
DESCRIPTION: Defines a Split function that takes a sentence and emits individual words. This function is used in the Trident topology to split input sentences into words for counting.

LANGUAGE: java
CODE:
public class Split extends BaseFunction {
   public void execute(TridentTuple tuple, TridentCollector collector) {
       String sentence = tuple.getString(0);
       for(String word: sentence.split(" ")) {
           collector.emit(new Values(word));                
       }
   }
}

----------------------------------------

TITLE: Implementing SplitSentence Bolt with Anchoring
DESCRIPTION: Example bolt implementation that splits sentences into words while maintaining tuple anchoring for reliability.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseRichBolt {
        OutputCollector _collector;
        
        public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
            _collector = collector;
        }

        public void execute(Tuple tuple) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                _collector.emit(tuple, new Values(word));
            }
            _collector.ack(tuple);
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Configuring SequenceFileBolt in Java
DESCRIPTION: Example of how to configure and use the SequenceFileBolt for writing Storm data to HDFS sequence files.

LANGUAGE: java
CODE:
// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withExtension(".seq")
        .withPath("/data/");

// create sequence format instance.
DefaultSequenceFormat format = new DefaultSequenceFormat("timestamp", "sentence");

SequenceFileBolt bolt = new SequenceFileBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withSequenceFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy)
        .withCompressionType(SequenceFile.CompressionType.RECORD)
        .withCompressionCodec("deflate");

----------------------------------------

TITLE: Implementing Split Sentence Bolt
DESCRIPTION: Simple bolt implementation that splits sentences into individual words and emits them as tuples.

LANGUAGE: clojure
CODE:
(defbolt split-sentence ["word"] [tuple collector]
  (let [words (.split (.getString tuple 0) " ")]
    (doseq [w words]
      (emit-bolt! collector [w] :anchor tuple))
    (ack! collector tuple)
    ))

----------------------------------------

TITLE: Failing Tuples in Storm Multilang Protocol
DESCRIPTION: JSON structure for marking tuples as failed in Storm's multilang protocol. Contains the command type and tuple ID to mark as failed.

LANGUAGE: json
CODE:
{
    "command": "fail",
    "id": 123123
}

----------------------------------------

TITLE: Java GetTime2 UDF Implementation
DESCRIPTION: User Defined Function implementation for converting timestamp strings to Unix timestamps

LANGUAGE: java
CODE:
package org.apache.storm.sql.runtime.functions.scalar.datetime;

public class GetTime2 {
    public static Long evaluate(String dateString, String dateFormat) {
        try {
            DateTimeFormatter df = DateTimeFormat.forPattern(dateFormat).withZoneUTC();
            return df.parseDateTime(dateString).getMillis();
        } catch (Exception ex) {
            throw new RuntimeException(ex);
        }
    }
}

----------------------------------------

TITLE: Stream Branching in Trident Topology
DESCRIPTION: Demonstrates how to create multiple conditional paths in a Trident topology by storing a Stream object in a variable and applying different filters to create branches. Shows stream splitting using each operators with different filters.

LANGUAGE: java
CODE:
Stream s = topology.each(...).groupBy(...).aggregate(...)
Stream branch1 = s.each(..., FilterA)
Stream branch2 = s.each(..., FilterB)

----------------------------------------

TITLE: Dequeueing and Removing Items from Kestrel Queue in Java
DESCRIPTION: This method dequeues items from a Kestrel queue and then removes them. It shows how to acknowledge and remove processed messages from the queue.

LANGUAGE: Java
CODE:
    private static void dequeueAndRemoveItems(KestrelClient kestrelClient, String queueName)
    throws IOException, ParseError
		 {
			for(int i=1; i<=12; i++){

				Item item = kestrelClient.dequeue(queueName);


				if(item==null){
					System.out.println("The queue (" + queueName + ") contains no items.");
				}
				else
				{
					int itemID = item._id;


					byte[] data = item._data;

					String receivedVal = new String(data);

					kestrelClient.ack(queueName, itemID);

					System.out.println("receivedItem=" + receivedVal);
				}
			}
	}

----------------------------------------

TITLE: Configuring Node Resources
DESCRIPTION: YAML configuration for specifying available CPU and memory resources on a Storm supervisor node

LANGUAGE: yaml
CODE:
supervisor.memory.capacity.mb: 20480.0
supervisor.cpu.capacity: 100.0

----------------------------------------

TITLE: Implementing Multiple Streams in Trident Topology
DESCRIPTION: Demonstrates how to create multiple conditional streams in a Trident topology using filter operations. Shows stream branching and handling of conditional paths.

LANGUAGE: java
CODE:
Stream s = topology.each(...).groupBy(...).aggregate(...)
Stream branch1 = s.each(..., FilterA)
Stream branch2 = s.each(..., FilterB)

----------------------------------------

TITLE: Failing Tuples in Storm Multi-Language Protocol (JSON)
DESCRIPTION: This JSON structure shows how to mark a tuple as failed in the Storm multi-language protocol. It includes the command type and the id of the tuple to be marked as failed.

LANGUAGE: json
CODE:
{
    "command": "fail",
    "id": 123123
}

----------------------------------------

TITLE: Leader Election Interface Implementation
DESCRIPTION: Java interface defining leader election functionality for Storm's high availability feature

LANGUAGE: java
CODE:
public interface ILeaderElector {
    void addToLeaderLockQueue();
    void removeFromLeaderLockQueue();
    boolean isLeader();
    InetSocketAddress getLeaderAddress();
    List<InetSocketAddress> getAllNimbusAddresses();
}

----------------------------------------

TITLE: Retrieving Cluster Configuration in Storm UI API
DESCRIPTION: GET request to retrieve the cluster configuration. Returns a JSON object containing various configuration settings.

LANGUAGE: json
CODE:
{
  "dev.zookeeper.path": "/tmp/dev-storm-zookeeper",
  "topology.tick.tuple.freq.secs": null,
  "topology.builtin.metrics.bucket.size.secs": 60,
  "topology.fall.back.on.java.serialization": true,
  "topology.max.error.report.per.interval": 5,
  "zmq.linger.millis": 5000,
  "topology.skip.missing.kryo.registrations": false,
  "storm.messaging.netty.client_worker_threads": 1,
  "ui.childopts": "-Xmx768m",
  "storm.zookeeper.session.timeout": 20000,
  "nimbus.reassign": true,
  "topology.trident.batch.emit.interval.millis": 500,
  "storm.messaging.netty.flush.check.interval.ms": 10,
  "nimbus.monitor.freq.secs": 10,
  "logviewer.childopts": "-Xmx128m",
  "java.library.path": "/usr/local/lib:/opt/local/lib:/usr/lib",
  "topology.executor.send.buffer.size": 1024
}

----------------------------------------

TITLE: Using Project Operation in Trident
DESCRIPTION: Example of using the project operation to keep only specific fields in a Trident stream.

LANGUAGE: java
CODE:
mystream.project(new Fields("b", "d"))

----------------------------------------

TITLE: Setting Local Directory in Storm YAML
DESCRIPTION: This code configures the local directory for Nimbus and Supervisor daemons to store state. It shows examples for both Unix and Windows systems.

LANGUAGE: yaml
CODE:
storm.local.dir: "/mnt/storm"

# For Windows:
# storm.local.dir: "C:\\storm-local"

----------------------------------------

TITLE: Configuring Automatic Task Hooks
DESCRIPTION: This snippet demonstrates how to configure automatic task hooks using the Storm configuration. These hooks are registered in every spout or bolt automatically.

LANGUAGE: Java
CODE:
"topology.auto.task.hooks"

----------------------------------------

TITLE: Storm CLI Blob Creation Command
DESCRIPTION: Example command to create a blob in Storm's blobstore with specified access controls and replication factor

LANGUAGE: bash
CODE:
storm blobstore create --file README.txt --acl o::rwa --replication-factor 4 key1

----------------------------------------

TITLE: Defining ComponentObject in Thrift for Storm
DESCRIPTION: This Thrift definition shows the ComponentObject union, which specifies how code for spouts or bolts can be defined in Storm. It includes options for serialized Java, shell components, and Java objects.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Creating Storm Topology with KestrelSpout in Java
DESCRIPTION: This code snippet demonstrates how to create a Storm topology that reads sentences from a Kestrel queue using KestrelSpout, splits them into words, and counts word occurrences. It shows the basic structure of a Storm topology with Kestrel integration.

LANGUAGE: java
CODE:
    TopologyBuilder builder = new TopologyBuilder();
    builder.setSpout("sentences", new KestrelSpout("localhost",22133,"sentence_queue",new StringScheme()));
    builder.setBolt("split", new SplitSentence(), 10)
    	        .shuffleGrouping("sentences");
    builder.setBolt("count", new WordCount(), 20)
	        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Defining IEventLogger Interface in Java
DESCRIPTION: This code snippet defines the IEventLogger interface used by the event logger bolt to log events. It includes methods for preparation, logging events, and cleanup.

LANGUAGE: java
CODE:
public interface IEventLogger {
    void prepare(Map stormConf, Map<String, Object> arguments, TopologyContext context);
    void log(EventInfo e);
    void close();
}

----------------------------------------

TITLE: SQL Join Example
DESCRIPTION: Example SQL query showing a join between 4 tables that will be recreated using Storm's JoinBolt.

LANGUAGE: sql
CODE:
select  userId, key4, key2, key3
from        table1
inner join  table2  on table2.userId =  table1.key1
inner join  table3  on table3.key3   =  table2.userId
left join   table4  on table4.key4   =  table3.key3

----------------------------------------

TITLE: Configuring Trident Topology with Fields Mapper in Java
DESCRIPTION: This code shows how to set up a Trident topology using a Fields mapper. It configures the Solr connection, creates a mapper for updating Solr fields, and builds a schema from a REST JSON source.

LANGUAGE: java
CODE:
new SolrStateFactory(solrConfig, solrMapper);

// zkHostString for Solr 'gettingstarted' example
SolrConfig solrConfig = new SolrConfig("127.0.0.1:9983");

/* Solr Fields Mapper used to generate 'SolrRequest' requests to update the "gettingstarted" Solr collection. The Solr index is updated using the field values of the tuple fields that match static or dynamic fields declared in the schema object build using schemaBuilder */ 
SolrMapper solrMapper = new SolrFieldsMapper.Builder(schemaBuilder, "gettingstarted").build();

// builds the Schema object from the JSON representation of the schema as returned by the URL http://localhost:8983/solr/gettingstarted/schema/ 
SchemaBuilder schemaBuilder = new RestJsonSchemaBuilder("localhost", "8983", "gettingstarted")

----------------------------------------

TITLE: Starting Apache ActiveMQ for JMS Integration
DESCRIPTION: Command to start Apache ActiveMQ, which is used as the JMS provider for the example.

LANGUAGE: bash
CODE:
$ [ACTIVEMQ_HOME]/bin/activemq

----------------------------------------

TITLE: Configuring Kerberos Authentication for Storm UI
DESCRIPTION: YAML configuration for setting up Kerberos authentication for the Storm UI using AuthenticationFilter from hadoop-auth.

LANGUAGE: yaml
CODE:
ui.filter: "org.apache.hadoop.security.authentication.server.AuthenticationFilter"
ui.filter.params:
   "type": "kerberos"
   "kerberos.principal": "HTTP/nimbus.witzend.com"
   "kerberos.keytab": "/vagrant/keytabs/http.keytab"
   "kerberos.name.rules": "RULE:[2:$1@$0]([jt]t@.*EXAMPLE.COM)s/.*/$MAPRED_USER/ RULE:[2:$1@$0]([nd]n@.*EXAMPLE.COM)s/.*/$HDFS_USER/DEFAULT"

----------------------------------------

TITLE: Linking to Storm JSON Serializer in Markdown
DESCRIPTION: This snippet demonstrates how to create a markdown link to the storm-json GitHub repository. It provides a brief description of the library's purpose.

LANGUAGE: markdown
CODE:
* [storm-json](https://github.com/rapportive-oss/storm-json): Simple JSON serializer for Storm

----------------------------------------

TITLE: Defining ComponentObject Structure in Thrift for Storm
DESCRIPTION: This Thrift definition shows the ComponentObject union, which specifies how code for spouts or bolts can be defined in Storm. It includes options for serialized Java, shell components, and Java objects.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Continuously Adding Sentences to Kestrel Queue in Java
DESCRIPTION: This program continuously adds sentence items to a Kestrel queue named 'sentence_queue' on a local Kestrel server. It runs until the user inputs a closing bracket character ']' to stop the process.

LANGUAGE: java
CODE:
    import java.io.IOException;
    import java.io.InputStream;
    import java.util.Random;

    import org.apache.storm.spout.KestrelClient;
    import org.apache.storm.spout.KestrelClient.Item;
    import org.apache.storm.spout.KestrelClient.ParseError;

    public class AddSentenceItemsToKestrel {

    	/**
    	 * @param args
    	 */
    	public static void main(String[] args) {

    		InputStream is = System.in;

			char closing_bracket = ']';

			int val = closing_bracket;

			boolean aux = true;

			try {

				KestrelClient kestrelClient = null;
				String queueName = "sentence_queue";

				while(aux){

					kestrelClient = new KestrelClient("localhost",22133);

					queueSentenceItems(kestrelClient, queueName);

					kestrelClient.close();

					Thread.sleep(1000);

					if(is.available()>0){
					 if(val==is.read())
						 aux=false;
					}
				}
			} catch (IOException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			catch (ParseError e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			} catch (InterruptedException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}

			System.out.println("end");

	    }
	}

----------------------------------------

TITLE: Maven Dependencies Configuration for Storm Kinesis
DESCRIPTION: Maven dependency configuration required for Storm Kinesis integration, including AWS SDK, Storm Core, Curator Framework, and JSON Simple libraries.

LANGUAGE: xml
CODE:
 <dependencies>
        <dependency>
            <groupId>com.amazonaws</groupId>
            <artifactId>aws-java-sdk</artifactId>
            <version>${aws-java-sdk.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-core</artifactId>
            <version>${project.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-framework</artifactId>
            <version>${curator.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>com.googlecode.json-simple</groupId>
            <artifactId>json-simple</artifactId>
        </dependency>
 </dependencies>

----------------------------------------

TITLE: Creating IConfigLoader Instance in Java
DESCRIPTION: Method to create an IConfigLoader instance based on the scheme of the scheduler.config.loader.uri configuration.

LANGUAGE: java
CODE:
ConfigLoaderFactoryService.createConfigLoader(Map<String, Object> conf)

----------------------------------------

TITLE: Health Check Configuration
DESCRIPTION: Configuration settings for Storm's health check mechanism, including directory path and timeout settings.

LANGUAGE: yaml
CODE:
storm.health.check.dir: "healthchecks"
storm.health.check.timeout.ms: 5000

----------------------------------------

TITLE: SQL Join Example for Stream Join Pattern
DESCRIPTION: Example SQL query showing a join pattern between 4 tables that will be replicated using Storm's JoinBolt.

LANGUAGE: sql
CODE:
select  userId, key4, key2, key3
from        table1
inner join  table2  on table2.userId =  table1.key1
inner join  table3  on table3.key3   =  table2.userId
left join   table4  on table4.key4   =  table3.key3

----------------------------------------

TITLE: Configuring Remote Cluster Access in YAML
DESCRIPTION: This YAML configuration specifies the Nimbus seed nodes for connecting to a remote Storm cluster. It should be placed in the ~/.storm/storm.yaml file to enable the storm client to communicate with the cluster.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["123.45.678.890"]

----------------------------------------

TITLE: Debugging Kryo Serialization Error in Java Storm Application
DESCRIPTION: Stack trace showing a ConcurrentModificationException that occurs when trying to emit mutable objects as output tuples in Storm. This error indicates that a bolt is modifying an object while it's being serialized for network transmission.

LANGUAGE: java
CODE:
java.lang.RuntimeException: java.util.ConcurrentModificationException
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:84)
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:55)
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:56)
	at org.apache.storm.disruptor$consume_loop_STAR_$fn__1597.invoke(disruptor.clj:67)
	at org.apache.storm.util$async_loop$fn__465.invoke(util.clj:377)
	at clojure.lang.AFn.run(AFn.java:24)
	at java.lang.Thread.run(Thread.java:679)

----------------------------------------

TITLE: Storm Shell Command Usage
DESCRIPTION: Command line syntax for using storm shell to package and submit a topology. Demonstrates how to package resources and execute a Python topology script.

LANGUAGE: bash
CODE:
storm shell resources/ python3 topology.py arg1 arg2

----------------------------------------

TITLE: Storm Emit Command JSON Format
DESCRIPTION: JSON structure for emitting tuples from a multilang bolt. Specifies anchors, stream ID, target task, and tuple values.

LANGUAGE: json
CODE:
{
    "command": "emit",
    "anchors": [1231231, -234234234],
    "stream": 1,
    "task": 9,
    "tuple": ["field1", 2, 3]
}

----------------------------------------

TITLE: Dequeuing and Removing Items from Kestrel Queue in Java
DESCRIPTION: Method to dequeue items from a Kestrel queue and remove them. It dequeues up to 12 items, acknowledges each item to remove it from the queue, and prints the received values.

LANGUAGE: java
CODE:
    private static void dequeueAndRemoveItems(KestrelClient kestrelClient, String queueName)
    throws IOException, ParseError
		 {
			for(int i=1; i<=12; i++){

				Item item = kestrelClient.dequeue(queueName);


				if(item==null){
					System.out.println("The queue (" + queueName + ") contains no items.");
				}
				else
				{
					int itemID = item._id;


					byte[] data = item._data;

					String receivedVal = new String(data);

					kestrelClient.ack(queueName, itemID);

					System.out.println("receivedItem=" + receivedVal);
				}
			}
	}

----------------------------------------

TITLE: Configuring Maven Assembly Plugin for Storm Topology JAR
DESCRIPTION: XML configuration for the Maven Assembly Plugin to package a Storm topology JAR with dependencies, excluding Storm JARs which are provided by the cluster.

LANGUAGE: xml
CODE:
<plugin>
  <artifactId>maven-assembly-plugin</artifactId>
  <configuration>
    <descriptorRefs>  
      <descriptorRef>jar-with-dependencies</descriptorRef>
    </descriptorRefs>
    <archive>
      <manifest>
        <mainClass>com.path.to.main.Class</mainClass>
      </manifest>
    </archive>
  </configuration>
</plugin>

----------------------------------------

TITLE: Configuring Automatic Task Hooks in Storm Topology
DESCRIPTION: Shows how to configure automatic task hooks using the Storm configuration. These hooks are registered automatically in every spout or bolt, useful for integrating with custom monitoring systems.

LANGUAGE: java
CODE:
"topology.auto.task.hooks"

----------------------------------------

TITLE: Using Storm DRPC Client
DESCRIPTION: Provides a simple way to send DRPC requests from the command line.

LANGUAGE: shell
CODE:
storm drpc-client [options] ([function argument]*)|(argument*)

----------------------------------------

TITLE: Creating SolrJsonMapper Instance
DESCRIPTION: Example showing how to create a SolrJsonMapper for updating a Solr collection with JSON content from tuple fields.

LANGUAGE: java
CODE:
    SolrMapper solrMapper = new SolrJsonMapper.Builder("gettingstarted", "JSON").build();

----------------------------------------

TITLE: Configuring EsState for Trident Integration
DESCRIPTION: Sets up Elasticsearch state factory for Trident streams with configuration and tuple mapping.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();

StateFactory factory = new EsStateFactory(esConfig, tupleMapper);
TridentState state = stream.partitionPersist(factory, esFields, new EsUpdater(), new Fields());

----------------------------------------

TITLE: SQL Grammar Definition in BNF
DESCRIPTION: Defines the SQL grammar supported by Storm SQL in BNF-like form, including statements, expressions, and clauses.

LANGUAGE: sql
CODE:
statement:
      setStatement
  |   resetStatement
  |   explain
  |   describe
  |   insert
  |   update
  |   merge
  |   delete
  |   query

setStatement:
      [ ALTER ( SYSTEM | SESSION ) ] SET identifier '=' expression

resetStatement:
      [ ALTER ( SYSTEM | SESSION ) ] RESET identifier
  |   [ ALTER ( SYSTEM | SESSION ) ] RESET ALL

explain:
      EXPLAIN PLAN
      [ WITH TYPE | WITH IMPLEMENTATION | WITHOUT IMPLEMENTATION ]
      [ EXCLUDING ATTRIBUTES | INCLUDING [ ALL ] ATTRIBUTES ]
      FOR ( query | insert | update | merge | delete )

describe:
      DESCRIBE DATABASE databaseName
   |  DESCRIBE CATALOG [ databaseName . ] catalogName
   |  DESCRIBE SCHEMA [ [ databaseName . ] catalogName ] . schemaName
   |  DESCRIBE [ TABLE ] [ [ [ databaseName . ] catalogName . ] schemaName . ] tableName [ columnName ]
   |  DESCRIBE [ STATEMENT ] ( query | insert | update | merge | delete )

insert:
      ( INSERT | UPSERT ) INTO tablePrimary
      [ '(' column [, column ]* ')' ]
      query

update:
      UPDATE tablePrimary
      SET assign [, assign ]*
      [ WHERE booleanExpression ]

assign:
      identifier '=' expression

merge:
      MERGE INTO tablePrimary [ [ AS ] alias ]
      USING tablePrimary
      ON booleanExpression
      [ WHEN MATCHED THEN UPDATE SET assign [, assign ]* ]
      [ WHEN NOT MATCHED THEN INSERT VALUES '(' value [ , value ]* ')' ]

delete:
      DELETE FROM tablePrimary [ [ AS ] alias ]
      [ WHERE booleanExpression ]

query:
      values
  |   WITH withItem [ , withItem ]* query
  |   {
          select
      |   selectWithoutFrom
      |   query UNION [ ALL ] query
      |   query EXCEPT query
      |   query INTERSECT query
      }
      [ ORDER BY orderItem [, orderItem ]* ]
      [ LIMIT { count | ALL } ]
      [ OFFSET start { ROW | ROWS } ]
      [ FETCH { FIRST | NEXT } [ count ] { ROW | ROWS } ]

withItem:
      name
      [ '(' column [, column ]* ')' ]
      AS '(' query ')'

orderItem:
      expression [ ASC | DESC ] [ NULLS FIRST | NULLS LAST ]

select:
      SELECT [ STREAM ] [ ALL | DISTINCT ]
          { * | projectItem [, projectItem ]* }
      FROM tableExpression
      [ WHERE booleanExpression ]
      [ GROUP BY { groupItem [, groupItem ]* } ]
      [ HAVING booleanExpression ]
      [ WINDOW windowName AS windowSpec [, windowName AS windowSpec ]* ]

selectWithoutFrom:
      SELECT [ ALL | DISTINCT ]
          { * | projectItem [, projectItem ]* }

projectItem:
      expression [ [ AS ] columnAlias ]
  |   tableAlias . *

tableExpression:
      tableReference [, tableReference ]*
  |   tableExpression [ NATURAL ] [ LEFT | RIGHT | FULL ] JOIN tableExpression [ joinCondition ]

joinCondition:
      ON booleanExpression
  |   USING '(' column [, column ]* ')'

tableReference:
      tablePrimary
      [ [ AS ] alias [ '(' columnAlias [, columnAlias ]* ')' ] ]

tablePrimary:
      [ [ catalogName . ] schemaName . ] tableName
      '(' TABLE [ [ catalogName . ] schemaName . ] tableName ')'
  |   [ LATERAL ] '(' query ')'
  |   UNNEST '(' expression ')' [ WITH ORDINALITY ]
  |   [ LATERAL ] TABLE '(' [ SPECIFIC ] functionName '(' expression [, expression ]* ')' ')'

values:
      VALUES expression [, expression ]*

groupItem:
      expression
  |   '(' ')'
  |   '(' expression [, expression ]* ')'
  |   CUBE '(' expression [, expression ]* ')'
  |   ROLLUP '(' expression [, expression ]* ')'
  |   GROUPING SETS '(' groupItem [, groupItem ]* ')'

windowRef:
      windowName
  |   windowSpec

windowSpec:
      [ windowName ]
      '('
      [ ORDER BY orderItem [, orderItem ]* ]
      [ PARTITION BY expression [, expression ]* ]
      [
          RANGE numericOrIntervalExpression { PRECEDING | FOLLOWING }
      |   ROWS numericExpression { PRECEDING | FOLLOWING }
      ]
      ')'

----------------------------------------

TITLE: Defining Bolt Component in Storm Thrift
DESCRIPTION: The Thrift definition for bolts in Storm, showing the ComponentObject and ComponentCommon structs. ComponentObject defines the bolt implementation, while ComponentCommon specifies streams, configurations, and other metadata.

LANGUAGE: Thrift
CODE:
struct ComponentObject {
  1: optional binary serialized_java
  2: optional ShellComponent shell
  3: optional JavaObject java_object
}

struct ComponentCommon {
  1: required map<string, StreamInfo> inputs
  2: required map<string, StreamInfo> outputs
  3: required i32 parallelism_hint
  4: optional string json_conf
}

----------------------------------------

TITLE: Using Min and MinBy Operations in Trident
DESCRIPTION: Example of using min and minBy operations on a Trident stream to find minimum values.

LANGUAGE: java
CODE:
mystream.minBy(new Fields("count"))

----------------------------------------

TITLE: Storm Initial Handshake Configuration JSON
DESCRIPTION: Example of the initial configuration JSON object sent to shell components containing topology configuration, PID directory, and context information.

LANGUAGE: json
CODE:
{
    "conf": {
        "topology.message.timeout.secs": 3
    },
    "pidDir": "...",
    "context": {
        "task->component": {
            "1": "example-spout",
            "2": "__acker",
            "3": "example-bolt1",
            "4": "example-bolt2"
        },
        "taskid": 3,
        "componentid": "example-bolt"
    }
}

----------------------------------------

TITLE: Configuring Remote Cluster Access in YAML
DESCRIPTION: This YAML configuration specifies the Nimbus seed nodes for connecting to a remote Storm cluster. It should be placed in the ~/.storm/storm.yaml file to enable the storm client to communicate with the cluster.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["123.45.678.890"]

----------------------------------------

TITLE: Worker Launcher Configuration
DESCRIPTION: Example configuration for the worker-launcher utility, which is used to launch Docker containers for Storm workers. Specifies group, user ID, and profiler script settings.

LANGUAGE: bash
CODE:
storm.worker-launcher.group=$(worker_launcher_group)
min.user.id=$(min_user_id)
worker.profiler.script.path=$(profiler_script_path)

----------------------------------------

TITLE: Initializing SolrUpdateBolt with JSON Mapper in Java
DESCRIPTION: This snippet demonstrates how to create a SolrUpdateBolt with a JSON mapper and count-based commit strategy for updating a Solr collection named 'gettingstarted' using JSON content from a tuple field named 'JSON'.

LANGUAGE: java
CODE:
new SolrUpdateBolt(solrConfig, solrMapper, solrCommitStgy)

// zkHostString for Solr 'gettingstarted' example
SolrConfig solrConfig = new SolrConfig("127.0.0.1:9983");

// JSON Mapper used to generate 'SolrRequest' requests to update the "gettingstarted" Solr collection with JSON content declared the tuple field with name "JSON"
SolrMapper solrMapper = new SolrJsonMapper.Builder("gettingstarted", "JSON").build(); 
 
// Acks every other five tuples. Setting to null acks every tuple
SolrCommitStrategy solrCommitStgy = new CountBasedCommit(5);

----------------------------------------

TITLE: Adding Apache Storm Dependency in Maven POM
DESCRIPTION: This XML snippet shows how to include Apache Storm as a development dependency in a Maven project's pom.xml file. It specifies the groupId, artifactId, version, and scope for the Storm client jar.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.storm</groupId>
  <artifactId>storm-client</artifactId>
  <version>{{page.version}}</version>
  <scope>provided</scope>
</dependency>

----------------------------------------

TITLE: Implementing Bulk Operations for Custom State
DESCRIPTION: Example of modifying a custom State (LocationDB) to support bulk get and set operations for improved efficiency.

LANGUAGE: java
CODE:
public class LocationDB implements State {
    public void beginCommit(Long txid) {    
    }
    
    public void commit(Long txid) {    
    }
    
    public void setLocationsBulk(List<Long> userIds, List<String> locations) {
      // set locations in bulk
    }
    
    public List<String> bulkGetLocations(List<Long> userIds) {
      // get locations in bulk
    }
}

----------------------------------------

TITLE: Configuring DRPC Servers
DESCRIPTION: Configuration for specifying DRPC server locations in the Storm cluster

LANGUAGE: yaml
CODE:
drpc.servers: ["111.222.333.44"]

----------------------------------------

TITLE: Creating Maven Shade Plugin Configuration
DESCRIPTION: XML configuration for Maven Shade plugin to create an uber jar containing all dependencies for the Storm-Solr integration.

LANGUAGE: xml
CODE:
<plugin>
     <groupId>org.apache.maven.plugins</groupId>
     <artifactId>maven-shade-plugin</artifactId>
     <version>2.4.1</version>
     <executions>
         <execution>
             <phase>package</phase>
             <goals>
                 <goal>shade</goal>
             </goals>
             <configuration>
                 <transformers>
                     <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                         <mainClass>org.apache.storm.solr.topology.SolrJsonTopology</mainClass>
                     </transformer>
                 </transformers>
             </configuration>
         </execution>
     </executions>
</plugin>

----------------------------------------

TITLE: Task Message Routing Implementation
DESCRIPTION: Implementation of task message routing logic, including stream grouping and direct stream handling.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L198

----------------------------------------

TITLE: Configuring Redis Lookup Bolt
DESCRIPTION: Configuration and initialization of RedisLookupBolt using JedisPoolConfig.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisLookupMapper lookupMapper = new WordCountRedisLookupMapper();
RedisLookupBolt lookupBolt = new RedisLookupBolt(poolConfig, lookupMapper);

----------------------------------------

TITLE: Shell Command - Storm JAR Execution Fix
DESCRIPTION: Bugfix for storm.cmd to properly handle the 'shift' command when executing 'storm jar' operations.

LANGUAGE: shell
CODE:
storm jar

----------------------------------------

TITLE: Implementing FailedMessageRetryHandler Interface in Java
DESCRIPTION: This code snippet demonstrates the structure of the FailedMessageRetryHandler interface, which is used to manage retry logic for failed messages in the Kinesis Spout. It includes methods for handling failed messages, retrieving messages for retry, and managing message state.

LANGUAGE: java
CODE:
    boolean failed (KinesisMessageId messageId);
    KinesisMessageId getNextFailedMessageToRetry ();
    void failedMessageEmitted (KinesisMessageId messageId);
    void acked (KinesisMessageId messageId);

----------------------------------------

TITLE: Initializing DRPC Client in Java
DESCRIPTION: Code showing how to configure and create a DRPC client for making remote procedure calls.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.put("storm.thrift.transport", "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin");
conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 3);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL, 10);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING, 20);
DRPCClient client = new DRPCClient(conf, "drpc-host", 3772);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Setting CPU Requirements for Storm Components
DESCRIPTION: Example of setting CPU requirements for spouts and bolts in a Storm topology.

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.setCPULoad(15.0);
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
            .shuffleGrouping("word").setCPULoad(10.0);
builder.setBolt("exclaim2", new HeavyBolt(), 1)
                .shuffleGrouping("exclaim1").setCPULoad(450.0);

----------------------------------------

TITLE: Using Storm Shell Command
DESCRIPTION: This command demonstrates how to use the 'storm shell' command to package resources, upload a jar to Nimbus, and execute a Python topology script with arguments.

LANGUAGE: bash
CODE:
storm shell resources/ python topology.py arg1 arg2

----------------------------------------

TITLE: IConfigLoader Interface Definition
DESCRIPTION: Core interface definition for config loading functionality. The load() method is called by the scheduler to retrieve the most recent configuration map.

LANGUAGE: java
CODE:
public interface IConfigLoader {
    Map<?,?> load();
};

----------------------------------------

TITLE: Creating CassandraWriterBolt with Specific Fields in Java
DESCRIPTION: Demonstrates how to create a CassandraWriterBolt that inserts data into Cassandra, specifying only certain tuple fields.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
    async(
        simpleQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);")
            .with(
                fields("title", "year", "performer", "genre", "tracks")
             )
        )
);

----------------------------------------

TITLE: Configuring DelimitedRecordHiveMapper for HiveBolt (Java)
DESCRIPTION: Java code examples showing different ways to configure DelimitedRecordHiveMapper, including mapping column fields and partition fields.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
    or
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withTimeAsPartitionField("YYYY/MM/DD");

----------------------------------------

TITLE: Configuring Health Check Directory in Storm YAML
DESCRIPTION: This YAML snippet shows how to set the directory for health check scripts in Storm configuration.

LANGUAGE: yaml
CODE:
storm.health.check.dir: "healthchecks"

----------------------------------------

TITLE: Creating EsConfig for Storm-Elasticsearch Integration in Java
DESCRIPTION: This snippet demonstrates two ways to create an EsConfig object for Elasticsearch cluster configuration. The first method uses basic settings, while the second includes additional parameters for the Elasticsearch Transport Client.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});

LANGUAGE: java
CODE:
Map<String, String> additionalParameters = new HashMap<>();
additionalParameters.put("client.transport.sniff", "true");
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"}, additionalParameters);

----------------------------------------

TITLE: Implementing Druid Beam Factory
DESCRIPTION: Sample implementation of DruidBeamFactory showing configuration of Druid connection, data source, dimensions, and aggregators using Tranquility.

LANGUAGE: java
CODE:
public class SampleDruidBeamFactoryImpl implements DruidBeamFactory<Map<String, Object>> {

    @Override
    public Beam<Map<String, Object>> makeBeam(Map<?, ?> conf, IMetricsContext metrics) {


        final String indexService = "druid/overlord"; // The druid.service name of the indexing service Overlord node.
        final String discoveryPath = "/druid/discovery"; // Curator service discovery path. config: druid.discovery.curator.path
        final String dataSource = "test"; //The name of the ingested datasource. Datasources can be thought of as tables.
        final List<String> dimensions = ImmutableList.of("publisher", "advertiser");
        List<AggregatorFactory> aggregators = ImmutableList.<AggregatorFactory>of(
                new CountAggregatorFactory(
                        "click"
                )
        );
        // Tranquility needs to be able to extract timestamps from your object type (in this case, Map<String, Object>).
        final Timestamper<Map<String, Object>> timestamper = new Timestamper<Map<String, Object>>()
        {
            @Override
            public DateTime timestamp(Map<String, Object> theMap)
            {
                return new DateTime(theMap.get("timestamp"));
            }
        };

        // Tranquility uses ZooKeeper (through Curator) for coordination.
        final CuratorFramework curator = CuratorFrameworkFactory
                .builder()
                .connectString((String)conf.get("druid.tranquility.zk.connect")) //take config from storm conf
                .retryPolicy(new ExponentialBackoffRetry(1000, 20, 30000))
                .build();
        curator.start();

        // The JSON serialization of your object must have a timestamp field in a format that Druid understands. By default,
        // Druid expects the field to be called "timestamp" and to be an ISO8601 timestamp.
        final TimestampSpec timestampSpec = new TimestampSpec("timestamp", "auto", null);

        // Tranquility needs to be able to serialize your object type to JSON for transmission to Druid. By default this is
        // done with Jackson. If you want to provide an alternate serializer, you can provide your own via ```.objectWriter(...)```.
        // In this case, we won't provide one, so we're just using Jackson.
        final Beam<Map<String, Object>> beam = DruidBeams
                .builder(timestamper)
                .curator(curator)
                .discoveryPath(discoveryPath)
                .location(DruidLocation.create(indexService, dataSource))
                .timestampSpec(timestampSpec)
                .rollup(DruidRollup.create(DruidDimensions.specific(dimensions), aggregators, QueryGranularities.MINUTE))
                .tuning(
                        ClusteredBeamTuning
                                .builder()
                                .segmentGranularity(Granularity.HOUR)
                                .windowPeriod(new Period("PT10M"))
                                .partitions(1)
                                .replicants(1)
                                .build()
                )
                .druidBeamConfig(
                      DruidBeamConfig
                           .builder()
                           .indexRetryPeriod(new Period("PT10M"))
                           .build())
                .buildBeam();

        return beam;
    }
}

----------------------------------------

TITLE: Retrieving Topology Worker Information in JSON
DESCRIPTION: Example response from the /api/v1/topology-workers/<id> endpoint, which returns worker information for a specific topology.

LANGUAGE: json
CODE:
{
  "hostPortList":[
    {
      "host":"192.168.202.2",
      "port":6701
    },
    {
      "host":"192.168.202.2",
      "port":6702
    },
    {
      "host":"192.168.202.3",
      "port":6700
    }
  ],
  "logviewerPort":8000
}

----------------------------------------

TITLE: Implementing Parallel Top-N Pattern in Storm
DESCRIPTION: Configuration for parallel processing of top-N calculations using fields grouping and global aggregation.

LANGUAGE: java
CODE:
builder.setBolt("rank", new RankObjects(), parallelism)
  .fieldsGrouping("objects", new Fields("value"));
builder.setBolt("merge", new MergeObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Invalid Stream Reference Example
DESCRIPTION: Example showing incorrect forward referencing of streams in JoinBolt configuration which is not allowed.

LANGUAGE: java
CODE:
new JoinBolt( "spout1", "key1")                 
  .join     ( "spout2", "userId",  "spout3") //not allowed. spout3 not yet introduced
  .join     ( "spout3", "key3",    "spout1")

----------------------------------------

TITLE: Creating SolrFieldsMapper with Custom Multivalue Field Token in Java
DESCRIPTION: This code demonstrates how to create a SolrFieldsMapper object to update a Solr collection, specifying a custom token for multivalue fields.

LANGUAGE: java
CODE:
new SolrFieldsMapper.Builder(
        new RestJsonSchemaBuilder("localhost", "8983", "gettingstarted"), "gettingstarted")
            .setMultiValueFieldToken("%").build();

----------------------------------------

TITLE: Python Topology Script Execution by Storm Shell
DESCRIPTION: Illustrates how Storm shell executes the Python topology script after packaging and uploading resources. It passes Nimbus host, port, and uploaded jar location as arguments to the script.

LANGUAGE: bash
CODE:
python topology.py arg1 arg2 {nimbus-host} {nimbus-port} {uploaded-jar-location}

----------------------------------------

TITLE: Configuring Spring JMS Connection with ActiveMQ
DESCRIPTION: Spring XML configuration file that sets up JMS connection factory and message destinations (queue and topic) for ActiveMQ integration. Defines connection parameters including broker URL and physical names for messaging endpoints.

LANGUAGE: xml
CODE:
<?xml version="1.0" encoding="UTF-8"?>
<beans 
  xmlns="http://www.springframework.org/schema/beans" 
  xmlns:amq="http://activemq.apache.org/schema/core"
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.0.xsd
  http://activemq.apache.org/schema/core http://activemq.apache.org/schema/core/activemq-core.xsd">

    <amq:queue id="notificationQueue" physicalName="backtype.storm.contrib.example.queue" />
    
    <amq:topic id="notificationTopic" physicalName="backtype.storm.contrib.example.topic" />

    <amq:connectionFactory id="jmsConnectionFactory"
        brokerURL="tcp://localhost:61616" />
    
</beans>

----------------------------------------

TITLE: Listing HDFS Container Resources
DESCRIPTION: Commands to list the container resources stored in HDFS after conversion.

LANGUAGE: bash
CODE:
hdfs dfs -ls /containers/*

LANGUAGE: bash
CODE:
hdfs dfs -cat /containers/image-tag-to-hash

----------------------------------------

TITLE: Storm Configuration for OCI/Squashfs Runtime
DESCRIPTION: Example Storm configuration settings for enabling and configuring the OCI/Squashfs runtime.

LANGUAGE: bash
CODE:
storm.resource.isolation.plugin: "org.apache.storm.container.oci.RuncLibContainerManager"

storm.oci.allowed.images:
    - "storm/rhel7:dev_current"
    - "storm/rhel7:dev_previous"
    - "storm/rhel7:dev_test"
storm.oci.image: "storm/rhel7:dev_current"

storm.oci.cgroup.parent: "/storm"
storm.oci.cgroup.root: "/sys/fs/cgroup"
storm.oci.image.hdfs.toplevel.dir: "hdfs://host:port/containers/"
storm.oci.image.tag.to.manifest.plugin: "org.apache.storm.container.oci.LocalOrHdfsImageTagToManifestPlugin"
storm.oci.local.or.hdfs.image.tag.to.manifest.plugin.hdfs.hash.file: "hdfs://host:port/containers/image-tag-to-hash"
storm.oci.manifest.to.resources.plugin: "org.apache.storm.container.oci.HdfsManifestToResourcesPlugin"
storm.oci.readonly.bindmounts:
    - "/home/y/lib64/storm"
    - "/etc/krb5.conf"

storm.oci.resources.localizer: "org.apache.storm.container.oci.HdfsOciResourcesLocalizer"
storm.oci.seccomp.profile: "/home/y/conf/storm/seccomp.json"

----------------------------------------

TITLE: Leader Election Interface Implementation in Java
DESCRIPTION: Interface defining the core functionality for leader election among Nimbus nodes, including methods for queue management, leadership status checks, and node address retrieval.

LANGUAGE: java
CODE:
public interface ILeaderElector {
    void addToLeaderLockQueue();
    void removeFromLeaderLockQueue();
    boolean isLeader();
    InetSocketAddress getLeaderAddress();
    List<InetSocketAddress> getAllNimbusAddresses();
}

----------------------------------------

TITLE: Configuring SSL/TLS for MQTT with Flux YAML
DESCRIPTION: Flux YAML configuration for setting up SSL/TLS in an MQTT topology.

LANGUAGE: yaml
CODE:
name: "mqtt-topology"

components:
   ########## MQTT Spout Config ############
  - id: "mqtt-type"
    className: "org.apache.storm.mqtt.examples.CustomMessageMapper"

  - id: "keystore-loader"
    className: "org.apache.storm.mqtt.ssl.DefaultKeyStoreLoader"
    constructorArgs:
      - "keystore.jks"
      - "truststore.jks"
    properties:
      - name: "keyPassword"
        value: "password"
      - name: "keyStorePassword"
        value: "password"
      - name: "trustStorePassword"
        value: "password"

  - id: "mqtt-options"
    className: "org.apache.storm.mqtt.common.MqttOptions"
    properties:
      - name: "url"
        value: "ssl://raspberrypi.local:8883"
      - name: "topics"
        value:
          - "/users/tgoetz/#"

# topology configuration
config:
  topology.workers: 1
  topology.max.spout.pending: 1000

# spout definitions
spouts:
  - id: "mqtt-spout"
    className: "org.apache.storm.mqtt.spout.MqttSpout"
    constructorArgs:
      - ref: "mqtt-type"
      - ref: "mqtt-options"
      - ref: "keystore-loader"
    parallelism: 1

# bolt definitions
bolts:

  - id: "log"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1


streams:

  - from: "mqtt-spout"
    to: "log"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Kafka Table Definition Example
DESCRIPTION: Example of creating an external table connected to Kafka stream

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Configuring Storm Druid Core Bolt
DESCRIPTION: Example showing how to configure and use the DruidBeamBolt for writing data to Druid. Demonstrates setup of DruidBeamFactory, DruidConfig, and event mapper with topology builder configuration.

LANGUAGE: java
CODE:
   DruidBeamFactory druidBeamFactory = new SampleDruidBeamFactoryImpl(new HashMap<String, Object>());
   DruidConfig druidConfig = DruidConfig.newBuilder().discardStreamId(DruidConfig.DEFAULT_DISCARD_STREAM_ID).build();
   ITupleDruidEventMapper<Map<String, Object>> eventMapper = new TupleDruidEventMapper<>(TupleDruidEventMapper.DEFAULT_FIELD_NAME);
   DruidBeamBolt<Map<String, Object>> druidBolt = new DruidBeamBolt<Map<String, Object>>(druidBeamFactory, eventMapper, druidConfig);
   topologyBuilder.setBolt("druid-bolt", druidBolt).shuffleGrouping("event-gen");
   topologyBuilder.setBolt("printer-bolt", new PrinterBolt()).shuffleGrouping("druid-bolt" , druidConfig.getDiscardStreamId());

----------------------------------------

TITLE: Configuring Multi-tenant Scheduler
DESCRIPTION: YAML configuration for enabling and configuring the multi-tenant scheduler in Storm

LANGUAGE: yaml
CODE:
storm.scheduler: "org.apache.storm.scheduler.multitenant.MultitenantScheduler"

multitenant.scheduler.user.pools: 
    "evans": 10
    "derek": 10

----------------------------------------

TITLE: Dequeuing and Removing Items from Kestrel
DESCRIPTION: Method to dequeue and remove items from a Kestrel queue. Retrieves items and acknowledges them to remove from the queue.

LANGUAGE: java
CODE:
    private static void dequeueAndRemoveItems(KestrelClient kestrelClient, String queueName)
    throws IOException, ParseError
		 {
			for(int i=1; i<=12; i++){

				Item item = kestrelClient.dequeue(queueName);


				if(item==null){
					System.out.println("The queue (" + queueName + ") contains no items.");
				}
				else
				{
					int itemID = item._id;


					byte[] data = item._data;

					String receivedVal = new String(data);

					kestrelClient.ack(queueName, itemID);

					System.out.println("receivedItem=" + receivedVal);
				}
			}
	}

----------------------------------------

TITLE: Initializing SolrUpdateBolt with JSON Mapper
DESCRIPTION: Creates a SolrUpdateBolt configured with SolrConfig, JSON mapper and count-based commit strategy for the 'gettingstarted' Solr collection.

LANGUAGE: java
CODE:
    new SolrUpdateBolt(solrConfig, solrMapper, solrCommitStgy)
    
    // zkHostString for Solr 'gettingstarted' example
    SolrConfig solrConfig = new SolrConfig("127.0.0.1:9983");
    
    // JSON Mapper used to generate 'SolrRequest' requests to update the "gettingstarted" Solr collection with JSON content declared the tuple field with name "JSON"
    SolrMapper solrMapper = new SolrJsonMapper.Builder("gettingstarted", "JSON").build(); 
     
    // Acks every other five tuples. Setting to null acks every tuple
    SolrCommitStrategy solrCommitStgy = new CountBasedCommit(5);

----------------------------------------

TITLE: Storm Set Log Level Command Example
DESCRIPTION: Examples of dynamically changing topology log levels with optional timeout periods.

LANGUAGE: bash
CODE:
./bin/storm set_log_level -l ROOT=DEBUG:30 topology-name
./bin/storm set_log_level -l com.myapp=WARN topology-name
./bin/storm set_log_level -l com.myapp=WARN -l com.myOtherLogger=ERROR:123 topology-name
./bin/storm set_log_level -r com.myOtherLogger topology-name

----------------------------------------

TITLE: Creating ACLs for Blobs in Java
DESCRIPTION: Java code demonstrating how to create Access Control Lists (ACLs) for blobs in the distributed cache.

LANGUAGE: java
CODE:
String stringBlobACL = "u:username:rwa";
AccessControl blobACL = BlobStoreAclHandler.parseAccessControl(stringBlobACL);
List<AccessControl> acls = new LinkedList<AccessControl>();
acls.add(blobACL); // more ACLs can be added here
SettableBlobMeta settableBlobMeta = new SettableBlobMeta(acls);
settableBlobMeta.set_replication_factor(4); // Here we can set the replication factor

----------------------------------------

TITLE: Configuring CGroups for Apache Storm
DESCRIPTION: Sample cgconfig.conf file for setting up CGroups in Apache Storm. It defines mount points for various subsystems and configures permissions for the 'storm' group.

LANGUAGE: bash
CODE:
mount {
	cpuset	= /cgroup/cpuset;
	cpu	= /cgroup/storm_resources;
	cpuacct	= /cgroup/storm_resources;
	memory	= /cgroup/storm_resources;
	devices	= /cgroup/devices;
	freezer	= /cgroup/freezer;
	net_cls	= /cgroup/net_cls;
	blkio	= /cgroup/blkio;
}

group storm {
       perm {
               task {
                      uid = 500;
                      gid = 500;
               }
               admin {
                      uid = 500;
                      gid = 500;
               }
       }
       cpu {
       }
       memory {
       }
       cpuacct {
       }
}

----------------------------------------

TITLE: Kafka Maven Dependency Configuration
DESCRIPTION: Example Maven dependency configuration for using Kafka 0.8.1.1 with Storm.

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka_2.10</artifactId>
    <version>0.8.1.1</version>
    <exclusions>
        <exclusion>
            <groupId>org.apache.zookeeper</groupId>
            <artifactId>zookeeper</artifactId>
        </exclusion>
        <exclusion>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
        </exclusion>
    </exclusions>
</dependency>

----------------------------------------

TITLE: Implementing OpenTSDB State in Trident Topology
DESCRIPTION: This example shows how to use OpenTSDB state in a Trident topology. It includes creating an OpenTsdbStateFactory, setting up a TridentTopology with a stream, and using partitionPersist to integrate OpenTSDB state updates into the topology.

LANGUAGE: java
CODE:
final OpenTsdbStateFactory openTsdbStateFactory =
        new OpenTsdbStateFactory(OpenTsdbClient.newBuilder(tsdbUrl),
                Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
TridentTopology tridentTopology = new TridentTopology();

final Stream stream = tridentTopology.newStream("metric-tsdb-stream", new MetricGenSpout());

stream.peek(new Consumer() {
    @Override
    public void accept(TridentTuple input) {
        LOG.info("########### Received tuple: [{}]", input);
    }
}).partitionPersist(openTsdbStateFactory, MetricGenSpout.DEFAULT_METRIC_FIELDS, new OpenTsdbStateUpdater());

----------------------------------------

TITLE: Using Persistent Aggregation in Trident Topology
DESCRIPTION: Example of using persistentAggregate to perform word count aggregation and store results in a MemoryMapState.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
      topology.newStream("spout1", spout)
        .each(new Fields("sentence"), new Split(), new Fields("word"))
        .groupBy(new Fields("word"))
        .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))

----------------------------------------

TITLE: Creating an External Table for Kafka
DESCRIPTION: Example of creating an external table that specifies a Kafka spout and sink.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-hosts=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Implementing SimpleMongoMapper for Storm-MongoDB Integration
DESCRIPTION: This snippet shows the implementation of SimpleMongoMapper, a general-purpose MongoMapper that maps Storm tuple fields to MongoDB document fields with matching names.

LANGUAGE: java
CODE:
public class SimpleMongoMapper implements MongoMapper {
    private String[] fields;

    @Override
    public Document toDocument(ITuple tuple) {
        Document document = new Document();
        for(String field : fields){
            document.append(field, tuple.getValueByField(field));
        }
        return document;
    }

    public SimpleMongoMapper withFields(String... fields) {
        this.fields = fields;
        return this;
    }
}

----------------------------------------

TITLE: Continuous Item Addition to Kestrel Queue in Java
DESCRIPTION: This program continuously adds sentence items to a Kestrel queue named 'sentence_queue' on a local Kestrel server. It runs until the user enters a closing bracket character ']' in the console.

LANGUAGE: java
CODE:
    import java.io.IOException;
    import java.io.InputStream;
    import java.util.Random;

    import org.apache.storm.spout.KestrelClient;
    import org.apache.storm.spout.KestrelClient.Item;
    import org.apache.storm.spout.KestrelClient.ParseError;

    public class AddSentenceItemsToKestrel {

    	/**
    	 * @param args
    	 */
    	public static void main(String[] args) {

    		InputStream is = System.in;

			char closing_bracket = ']';

			int val = closing_bracket;

			boolean aux = true;

			try {

				KestrelClient kestrelClient = null;
				String queueName = "sentence_queue";

				while(aux){

					kestrelClient = new KestrelClient("localhost",22133);

					queueSentenceItems(kestrelClient, queueName);

					kestrelClient.close();

					Thread.sleep(1000);

					if(is.available()>0){
					 if(val==is.read())
						 aux=false;
					}
				}
			} catch (IOException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			catch (ParseError e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			} catch (InterruptedException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}

			System.out.println("end");

	    }
	}

----------------------------------------

TITLE: Storm Shell Command Usage Example
DESCRIPTION: Example command showing how to use the storm shell utility to package and deploy resources with a Python topology.

LANGUAGE: shell
CODE:
storm shell resources/ python3 topology.py arg1 arg2

----------------------------------------

TITLE: Storm Protocol Emit Command Format
DESCRIPTION: JSON structure for emitting tuples in Storm's multilang protocol. Specifies anchors, stream ID, target task, and tuple values.

LANGUAGE: json
CODE:
{
    "command": "emit",
    "anchors": [1231231, -234234234],
    "stream": 1,
    "task": 9,
    "tuple": ["field1", 2, 3]
}

----------------------------------------

TITLE: Named Stream Join Configuration
DESCRIPTION: Example showing how to configure JoinBolt to work with named streams instead of default streams, including stream selector configuration.

LANGUAGE: java
CODE:
new JoinBolt(JoinBolt.Selector.STREAM,  "stream1", "key1") 
                             .join     ("stream2", "userId",  "stream1" )
                             .select ("userId, key1, key2")
                             .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;
                             
topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("bolt1", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt2", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt3", "stream2", new Fields("userId") )
            .fieldsGrouping("bolt4", "stream1", new Fields("key1") );

----------------------------------------

TITLE: Creating Bucketed Hive Table for Streaming
DESCRIPTION: SQL command to create a partitioned table in Hive using ORC format, which is required for using the Hive streaming API.

LANGUAGE: sql
CODE:
create table test_table ( id INT, name STRING, phone STRING, street STRING) partitioned by (city STRING, state STRING) stored as orc tblproperties ("orc.compress"="NONE");

----------------------------------------

TITLE: SQL Grammar Definition in BNF-like Form
DESCRIPTION: Defines the grammar for SQL statements supported by Storm SQL, including SELECT, INSERT, UPDATE, DELETE, and other operations.

LANGUAGE: sql
CODE:
statement:
      setStatement
  |   resetStatement
  |   explain
  |   describe
  |   insert
  |   update
  |   merge
  |   delete
  |   query

setStatement:
      [ ALTER ( SYSTEM | SESSION ) ] SET identifier '=' expression

resetStatement:
      [ ALTER ( SYSTEM | SESSION ) ] RESET identifier
  |   [ ALTER ( SYSTEM | SESSION ) ] RESET ALL

explain:
      EXPLAIN PLAN
      [ WITH TYPE | WITH IMPLEMENTATION | WITHOUT IMPLEMENTATION ]
      [ EXCLUDING ATTRIBUTES | INCLUDING [ ALL ] ATTRIBUTES ]
      FOR ( query | insert | update | merge | delete )

describe:
      DESCRIBE DATABASE databaseName
   |  DESCRIBE CATALOG [ databaseName . ] catalogName
   |  DESCRIBE SCHEMA [ [ databaseName . ] catalogName ] . schemaName
   |  DESCRIBE [ TABLE ] [ [ [ databaseName . ] catalogName . ] schemaName . ] tableName [ columnName ]
   |  DESCRIBE [ STATEMENT ] ( query | insert | update | merge | delete )

insert:
      ( INSERT | UPSERT ) INTO tablePrimary
      [ '(' column [, column ]* ')' ]
      query

update:
      UPDATE tablePrimary
      SET assign [, assign ]*
      [ WHERE booleanExpression ]

assign:
      identifier '=' expression

merge:
      MERGE INTO tablePrimary [ [ AS ] alias ]
      USING tablePrimary
      ON booleanExpression
      [ WHEN MATCHED THEN UPDATE SET assign [, assign ]* ]
      [ WHEN NOT MATCHED THEN INSERT VALUES '(' value [ , value ]* ')' ]

delete:
      DELETE FROM tablePrimary [ [ AS ] alias ]
      [ WHERE booleanExpression ]

query:
      values
  |   WITH withItem [ , withItem ]* query
  |   {
          select
      |   selectWithoutFrom
      |   query UNION [ ALL ] query
      |   query EXCEPT query
      |   query INTERSECT query
      }
      [ ORDER BY orderItem [, orderItem ]* ]
      [ LIMIT { count | ALL } ]
      [ OFFSET start { ROW | ROWS } ]
      [ FETCH { FIRST | NEXT } [ count ] { ROW | ROWS } ]

withItem:
      name
      [ '(' column [, column ]* ')' ]
      AS '(' query ')'

orderItem:
      expression [ ASC | DESC ] [ NULLS FIRST | NULLS LAST ]

select:
      SELECT [ STREAM ] [ ALL | DISTINCT ]
          { * | projectItem [, projectItem ]* }
      FROM tableExpression
      [ WHERE booleanExpression ]
      [ GROUP BY { groupItem [, groupItem ]* } ]
      [ HAVING booleanExpression ]
      [ WINDOW windowName AS windowSpec [, windowName AS windowSpec ]* ]

selectWithoutFrom:
      SELECT [ ALL | DISTINCT ]
          { * | projectItem [, projectItem ]* }

projectItem:
      expression [ [ AS ] columnAlias ]
  |   tableAlias . *

tableExpression:
      tableReference [, tableReference ]*
  |   tableExpression [ NATURAL ] [ LEFT | RIGHT | FULL ] JOIN tableExpression [ joinCondition ]

joinCondition:
      ON booleanExpression
  |   USING '(' column [, column ]* ')'

tableReference:
      tablePrimary
      [ [ AS ] alias [ '(' columnAlias [, columnAlias ]* ')' ] ]

tablePrimary:
      [ [ catalogName . ] schemaName . ] tableName
      '(' TABLE [ [ catalogName . ] schemaName . ] tableName ')'
  |   [ LATERAL ] '(' query ')'
  |   UNNEST '(' expression ')' [ WITH ORDINALITY ]
  |   [ LATERAL ] TABLE '(' [ SPECIFIC ] functionName '(' expression [, expression ]* ')' ')'

values:
      VALUES expression [, expression ]*

groupItem:
      expression
  |   '(' ')'
  |   '(' expression [, expression ]* ')'
  |   CUBE '(' expression [, expression ]* ')'
  |   ROLLUP '(' expression [, expression ]* ')'
  |   GROUPING SETS '(' groupItem [, groupItem ]* ')'

windowRef:
      windowName
  |   windowSpec

windowSpec:
      [ windowName ]
      '('
      [ ORDER BY orderItem [, orderItem ]* ]
      [ PARTITION BY expression [, expression ]* ]
      [
          RANGE numericOrIntervalExpression { PRECEDING | FOLLOWING }
      |   ROWS numericExpression { PRECEDING | FOLLOWING }
      ]
      ')'

----------------------------------------

TITLE: Configuring HdfsBolt for HDFS Integration in Java
DESCRIPTION: This snippet demonstrates how to configure an HdfsBolt to write pipe-delimited files to HDFS, with sync and rotation policies.

LANGUAGE: java
CODE:
// use "|" instead of "," for field delimiter
RecordFormat format = new DelimitedRecordFormat()
        .withFieldDelimiter("|");

// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withPath("/foo/");

HdfsBolt bolt = new HdfsBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withRecordFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy);

----------------------------------------

TITLE: User Defined Function Creation
DESCRIPTION: Example of creating a user defined function in Storm SQL

LANGUAGE: sql
CODE:
CREATE FUNCTION MYPLUS AS 'org.apache.storm.sql.TestUtils$MyPlus'

----------------------------------------

TITLE: Defining IEventLogger Interface in Java for Apache Storm
DESCRIPTION: This code snippet defines the IEventLogger interface used by the event logger bolt to log events in Apache Storm. It includes methods for preparation, logging events, and cleanup.

LANGUAGE: java
CODE:
public interface IEventLogger {
    void prepare(Map stormConf, Map<String, Object> arguments, TopologyContext context);
    void log(EventInfo e);
    void close();
}

----------------------------------------

TITLE: Implementing Transfer Function in Apache Storm Worker (Clojure)
DESCRIPTION: The worker provides a transfer function used by tasks to send tuples to other tasks. It serializes the tuple and puts it onto a transfer queue.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L56

----------------------------------------

TITLE: Configuring Skewed Streaming Top N Pattern in Storm Topology
DESCRIPTION: This snippet shows how to set up a streaming top N pattern for skewed data in a Storm topology. It uses three bolts: 'count' for partial counting, 'rank' for aggregating counts and ranking, and 'merge' for final merging. It employs partial key grouping and fields grouping for load balancing.

LANGUAGE: java
CODE:
builder.setBolt("count", new CountObjects(), parallelism)
  .partialKeyGrouping("objects", new Fields("value"));
builder.setBolt("rank" new AggregateCountsAndRank(), parallelism)
  .fieldsGrouping("count", new Fields("key"))
builder.setBolt("merge", new MergeRanksObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Setting Local Directory in Storm YAML
DESCRIPTION: This configuration specifies the local directory for Nimbus and Supervisor daemons to store state. It's a mandatory setting in the storm.yaml file.

LANGUAGE: yaml
CODE:
storm.local.dir: "/mnt/storm"

LANGUAGE: yaml
CODE:
storm.local.dir: "C:\\storm-local"

----------------------------------------

TITLE: Implementing SimpleMongoMapper for Storm-MongoDB Integration
DESCRIPTION: This snippet shows the implementation of SimpleMongoMapper, a general-purpose MongoMapper that maps Storm tuple fields to MongoDB document fields with matching names.

LANGUAGE: java
CODE:
public class SimpleMongoMapper implements MongoMapper {
    private String[] fields;

    @Override
    public Document toDocument(ITuple tuple) {
        Document document = new Document();
        for(String field : fields){
            document.append(field, tuple.getValueByField(field));
        }
        return document;
    }

    public SimpleMongoMapper withFields(String... fields) {
        this.fields = fields;
        return this;
    }
}

----------------------------------------

TITLE: Registering Task Hook in Java using TopologyContext
DESCRIPTION: This snippet demonstrates how to register a custom task hook in the open method of a spout or prepare method of a bolt using the TopologyContext. The addTaskHook method is used to add the hook to the topology.

LANGUAGE: java
CODE:
TopologyContext#addTaskHook

----------------------------------------

TITLE: Creating External Kafka Table in Storm SQL
DESCRIPTION: Example of creating an external table for a Kafka spout and sink

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-hosts=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Retrieving Cluster Configuration in JSON (Storm UI API)
DESCRIPTION: GET request to /api/v1/cluster/configuration endpoint. Returns the Storm cluster configuration as a JSON object with key-value pairs for various configuration settings.

LANGUAGE: json
CODE:
{
  "dev.zookeeper.path": "/tmp/dev-storm-zookeeper",
  "topology.tick.tuple.freq.secs": null,
  "topology.builtin.metrics.bucket.size.secs": 60,
  "topology.fall.back.on.java.serialization": false,
  "topology.max.error.report.per.interval": 5,
  "zmq.linger.millis": 5000,
  "topology.skip.missing.kryo.registrations": false,
  "storm.messaging.netty.client_worker_threads": 1,
  "ui.childopts": "-Xmx768m",
  "storm.zookeeper.session.timeout": 20000,
  "nimbus.reassign": true,
  "topology.trident.batch.emit.interval.millis": 500,
  "storm.messaging.netty.flush.check.interval.ms": 10,
  "nimbus.monitor.freq.secs": 10,
  "logviewer.childopts": "-Xmx128m",
  "java.library.path": "/usr/local/lib:/opt/local/lib:/usr/lib",
  "topology.executor.send.buffer.size": 1024
}

----------------------------------------

TITLE: MQTT Message Mapper Interface in Java
DESCRIPTION: Interface definition for mapping MQTT messages to Storm tuples

LANGUAGE: java
CODE:
public interface MqttMessageMapper extends Serializable {
    Values toValues(MqttMessage message);
    Fields outputFields();
}

----------------------------------------

TITLE: Initializing JdbcLookupBolt in Java
DESCRIPTION: This code snippet demonstrates how to initialize a JdbcLookupBolt. It shows configuration of the bolt with a connection provider, select query, lookup mapper, and query timeout.

LANGUAGE: java
CODE:
String selectSql = "select user_name from user_details where user_id = ?";
SimpleJdbcLookupMapper lookupMapper = new SimpleJdbcLookupMapper(outputFields, queryParamColumns)
JdbcLookupBolt userNameLookupBolt = new JdbcLookupBolt(connectionProvider, selectSql, lookupMapper)
        .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Running MQTT Broker and Publisher in Bash
DESCRIPTION: Command to start an MQTT broker on port 1883 and initialize a publisher for temperature/humidity data

LANGUAGE: bash
CODE:
java -cp examples/target/storm-mqtt-examples-*-SNAPSHOT.jar org.apache.storm.mqtt.examples.MqttBrokerPublisher

----------------------------------------

TITLE: Configuring Isolation Scheduler in YAML for Apache Storm
DESCRIPTION: This YAML configuration snippet demonstrates how to set up the Isolation Scheduler in Apache Storm. It specifies the number of isolated machines allocated to different topologies, allowing for resource isolation and prioritization.

LANGUAGE: yaml
CODE:
isolation.scheduler.machines: 
    "my-topology": 8
    "tiny-topology": 1
    "some-other-topology": 3

----------------------------------------

TITLE: Configuring URL Expansion with Shuffle Grouping in Storm
DESCRIPTION: Basic configuration of a Storm bolt for URL expansion using shuffle grouping for random distribution of tuples.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .shuffleGrouping(1);

----------------------------------------

TITLE: Configuring CassandraWriterBolt with Unlogged Batch Statement in Java
DESCRIPTION: Creates a CassandraWriterBolt using an unlogged batch statement to perform multiple inserts.

LANGUAGE: java
CODE:
new CassandraWriterBolt(unLoggedBatch(
        simpleQuery("INSERT INTO titles_per_album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all())),
        simpleQuery("INSERT INTO titles_per_performer (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all()))
    )
);

----------------------------------------

TITLE: Artifactory Configuration Example
DESCRIPTION: Example configuration for Artifactory-based config loader showing URI and timeout settings

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "artifactory+http://artifactory.my.company.com:8000/artifactory/configurations/clusters/my_cluster/ras_pools"
scheduler.config.loader.timeout.sec: 30

----------------------------------------

TITLE: Installing JZMQ Java Bindings for Storm
DESCRIPTION: Commands for cloning and installing JZMQ from a specific fork that is tested to work with Storm. This process includes generating configuration files, building, and installing the Java bindings for ZeroMQ.

LANGUAGE: bash
CODE:
#install jzmq
git clone https://github.com/nathanmarz/jzmq.git
cd jzmq
./autogen.sh
./configure
make
sudo make install

----------------------------------------

TITLE: Setting Component Memory Requirements in Storm Topology
DESCRIPTION: Code example showing how to set memory requirements for spouts and bolts

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.setMemoryLoad(1024.0, 512.0);
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
            .shuffleGrouping("word").setMemoryLoad(512.0);

----------------------------------------

TITLE: Emitting Tuples Asynchronously in a Bolt
DESCRIPTION: This snippet illustrates how to launch new threads in bolts for asynchronous processing. It emphasizes that OutputCollector is thread-safe and can be called at any time.

LANGUAGE: java
CODE:
public void execute(Tuple input) {
    new Thread(new Runnable() {
        public void run() {
            // Do some processing
            _collector.emit(new Values(result));
            _collector.ack(input);
        }
    }).start();
}

----------------------------------------

TITLE: Adding Dependencies to Storm Classpath
DESCRIPTION: This snippet demonstrates various methods for adding external dependencies to Storm's classpath, including command-line options, directory locations, and environment variables.

LANGUAGE: python
CODE:
# Command-line options for storm jar command
storm jar --jar external-dependency.jar --artifacts "org.example:example-artifact:1.0.0"

# Directory locations for adding dependencies
${STORM_DIR}/extlib/
${STORM_DIR}/extlib-daemon/

# Environment variables for specifying external classpaths
STORM_EXT_CLASSPATH
STORM_EXT_CLASSPATH_DAEMON

----------------------------------------

TITLE: Creating a Blob in Java
DESCRIPTION: Java code demonstrating how to create a blob in Storm's distributed cache using the ClientBlobStore API.

LANGUAGE: java
CODE:
AtomicOutputStream blobStream = clientBlobStore.createBlob("some_key", settableBlobMeta);
blobStream.write("Some String or input data".getBytes());
blobStream.close();

----------------------------------------

TITLE: Creating CassandraWriterBolt with Insert Query Using All Tuple Fields in Java
DESCRIPTION: Shows how to create a CassandraWriterBolt that inserts all fields from the input tuple into a Cassandra table.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
    async(
        simpleQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);")
            .with( all() )
        )
);

----------------------------------------

TITLE: Configuring Supervisor Slots Ports in Storm YAML
DESCRIPTION: This code snippet demonstrates how to configure the supervisor.slots.ports in storm.yaml. It defines the ports that are open for use by Storm workers on each machine.

LANGUAGE: yaml
CODE:
supervisor.slots.ports:
    - 6700
    - 6701
    - 6702
    - 6703

----------------------------------------

TITLE: Configuring Bolt Executors and Tasks in Apache Storm
DESCRIPTION: This snippet demonstrates how to set the number of executors and tasks for a bolt in Apache Storm 0.8.0. It sets 3 initial executors and 128 tasks for MyBolt, using shuffleGrouping from a spout.

LANGUAGE: java
CODE:
builder.setBolt(new MyBolt(), 3).setNumTasks(128).shuffleGrouping("spout");

----------------------------------------

TITLE: Configuring Zookeeper Servers in Storm YAML
DESCRIPTION: This YAML snippet shows how to configure the Zookeeper servers for a Storm cluster in the storm.yaml configuration file.

LANGUAGE: yaml
CODE:
storm.zookeeper.servers:
  - "111.222.333.444"
  - "555.666.777.888"

----------------------------------------

TITLE: Storm Shell Command Usage Example
DESCRIPTION: Example command showing how to use storm shell to package and submit a Python topology with resources.

LANGUAGE: bash
CODE:
storm shell resources/ python3 topology.py arg1 arg2

----------------------------------------

TITLE: Configuring RedisFilterBolt in Java
DESCRIPTION: Java code snippet showing how to configure and create a RedisFilterBolt instance.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisFilterMapper filterMapper = new BlacklistWordFilterMapper();
RedisFilterBolt filterBolt = new RedisFilterBolt(poolConfig, filterMapper);

----------------------------------------

TITLE: Configuring HiveOptions with Advanced Settings
DESCRIPTION: Detailed example of configuring HiveOptions with transaction batching, batch size, and idle timeout settings for optimized performance.

LANGUAGE: java
CODE:
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
                                .withTxnsPerBatch(10)
                				.withBatchSize(1000)
                	     		.withIdleTimeout(10)

----------------------------------------

TITLE: Storm Emit Command JSON Format
DESCRIPTION: JSON structure for emitting tuples from a multi-language bolt. Includes anchor tuples, stream ID, target task, and tuple values.

LANGUAGE: json
CODE:
{
    "command": "emit",
    "anchors": [1231231, -234234234],
    "stream": 1,
    "task": 9,
    "tuple": ["field1", 2, 3]
}

----------------------------------------

TITLE: Configuring Redis Cluster State Provider in Storm
DESCRIPTION: JSON configuration for Redis Cluster-based key-value state implementation in Storm. Specifies cluster nodes and connection details.

LANGUAGE: json
CODE:
{
  "keyClass": "Optional fully qualified class name of the Key type.",
  "valueClass": "Optional fully qualified class name of the Value type.",
  "keySerializerClass": "Optional Key serializer implementation class.",
  "valueSerializerClass": "Optional Value Serializer implementation class.",
  "jedisClusterConfig": {
    "nodes": ["localhost:7379", "localhost:7380", "localhost:7381"],
    "timeout": 2000,
    "maxRedirections": 5
  }
}

----------------------------------------

TITLE: Converting Docker Images to Squashfs for HDFS
DESCRIPTION: Command to use the docker-to-squash.py script to download Docker images, convert layers to squashfs files, and push them to HDFS.

LANGUAGE: bash
CODE:
python3 docker-to-squash.py pull-build-push-update --hdfs-root hdfs://hostname:port/containers \
                      docker.xxx.com:4443/hadoop-user-images/storm/rhel7:20201202-232133,storm/rhel7:dev_current --log DEBUG --bootstrap

----------------------------------------

TITLE: Retrieving Supervisor Summary in JSON
DESCRIPTION: Example response from the /api/v1/supervisor/summary endpoint showing summary information for all supervisors in JSON format.

LANGUAGE: json
CODE:
{
  "supervisors": [
    {
      "id": "0b879808-2a26-442b-8f7d-23101e0c3696",
      "host": "10.11.1.7",
      "uptime": "5m 58s",
      "uptimeSeconds": 358,
      "slotsTotal": 4,
      "slotsUsed": 3,
      "totalMem": 3000,
      "totalCpu": 400,
      "usedMem": 1280,
      "usedCPU": 160
    }
  ],
  "schedulerDisplayResource": true
}

----------------------------------------

TITLE: Markdown Layout Configuration
DESCRIPTION: Basic Jekyll/markdown page layout configuration

LANGUAGE: markdown
CODE:
---
layout: documentation
---

----------------------------------------

TITLE: Implementing SimpleQueryFilterCreator for Storm-MongoDB Integration
DESCRIPTION: This snippet implements the SimpleQueryFilterCreator class, which creates MongoDB query filters using the $eq operator to match values equal to a specified field.

LANGUAGE: java
CODE:
public class SimpleQueryFilterCreator implements QueryFilterCreator {

    private String field;
    
    @Override
    public Bson createFilter(ITuple tuple) {
        return Filters.eq(field, tuple.getValueByField(field));
    }

    @Override
    public Bson createFilterByKeys(List<Object> keys) {
        return Filters.eq("_id", MongoUtils.getID(keys));
    }

    public SimpleQueryFilterCreator withField(String field) {
        this.field = field;
        return this;
    }

}

----------------------------------------

TITLE: Running Storm Topology from Command Line
DESCRIPTION: Demonstrates how to package and run a Storm topology using the 'storm jar' command.

LANGUAGE: bash
CODE:
storm jar all-my-code.jar org.apache.storm.MyTopology arg1 arg2

----------------------------------------

TITLE: Storm Emit Command JSON Format
DESCRIPTION: JSON structure for emitting tuples from a multilang bolt. Includes command type, anchor tuple IDs, stream ID, target task, and tuple values.

LANGUAGE: json
CODE:
{
    "command": "emit",
    "anchors": [1231231, -234234234],
    "stream": 1,
    "task": 9,
    "tuple": ["field1", 2, 3]
}

----------------------------------------

TITLE: UDF Implementation Example
DESCRIPTION: Java implementation of a scalar user defined function

LANGUAGE: java
CODE:
  public class MyPlus {
    public static Integer evaluate(Integer x, Integer y) {
      return x + y;
    }
  }

----------------------------------------

TITLE: Configuring CGroup Mount Points and Permissions
DESCRIPTION: Sample cgconfig.conf configuration that defines mount points for various resource controllers and sets up permissions for the storm group. This configuration mounts CPU, memory and other resource controllers under specified paths and configures access permissions.

LANGUAGE: conf
CODE:
mount {
	cpuset	= /cgroup/cpuset;
	cpu	= /cgroup/storm_resources;
	cpuacct	= /cgroup/storm_resources;
	memory	= /cgroup/storm_resources;
	devices	= /cgroup/devices;
	freezer	= /cgroup/freezer;
	net_cls	= /cgroup/net_cls;
	blkio	= /cgroup/blkio;
}

group storm {
       perm {
               task {
                      uid = 500;
                      gid = 500;
               }
               admin {
                      uid = 500;
                      gid = 500;
               }
       }
       cpu {
       }
       memory {
       }
       cpuacct {
       }
}

----------------------------------------

TITLE: Configuring HDFS Spout in Java
DESCRIPTION: Example of creating and configuring an HDFS spout to read text files from a specified HDFS path.

LANGUAGE: java
CODE:
// Instantiate spout to read text files
HdfsSpout textReaderSpout = new HdfsSpout().setReaderType("text")
                                           .withOutputFields(TextFileReader.defaultFields)                                      
                                           .setHdfsUri("hdfs://localhost:54310")  // required
                                           .setSourceDir("/data/in")              // required                                      
                                           .setArchiveDir("/data/done")           // required
                                           .setBadFilesDir("/data/badfiles");     // required                                      
// If using Kerberos
HashMap hdfsSettings = new HashMap();
hdfsSettings.put("hdfs.keytab.file", "/path/to/keytab");
hdfsSettings.put("hdfs.kerberos.principal","user@EXAMPLE.com");

textReaderSpout.setHdfsClientSettings(hdfsSettings);

// Create topology
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("hdfsspout", textReaderSpout, SPOUT_NUM);

// Setup bolts and wire up topology
     ..snip..

// Submit topology with config
Config conf = new Config();
StormSubmitter.submitTopologyWithProgressBar("topologyName", conf, builder.createTopology());

----------------------------------------

TITLE: Retrieving Supervisor Summary in JSON
DESCRIPTION: Example response from the /api/v1/supervisor/summary endpoint, which returns summary information for all supervisors.

LANGUAGE: json
CODE:
{
  "supervisors": [
    {
      "id": "0b879808-2a26-442b-8f7d-23101e0c3696",
      "host": "10.11.1.7",
      "uptime": "5m 58s",
      "uptimeSeconds": 358,
      "slotsTotal": 4,
      "slotsUsed": 3,
      "totalMem": 3000,
      "totalCpu": 400,
      "usedMem": 1280,
      "usedCPU": 160
    }
  ],
  "schedulerDisplayResource": true
}

----------------------------------------

TITLE: Launching Docker Container for Storm Worker
DESCRIPTION: Example Docker run command used by the worker-launcher executable to start a container for a Storm worker. It includes various options for isolation, resource limits, and bind mounts.

LANGUAGE: bash
CODE:
run --name=8198e1f0-f323-4b9d-8625-e4fd640cd058 \
--user=<uid>:<gid> \
-d \
--net=host \
--read-only \
-v /sys/fs/cgroup:/sys/fs/cgroup:ro \
-v /usr/share/apache-storm-2.3.0:/usr/share/apache-storm-2.3.0:ro \
-v /<storm-local-dir>/supervisor:/<storm-local-dir>/supervisor:ro \
-v /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058:/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
-v /<workers-artifacts-dir>/workers-artifacts/word-count-1-1591895933/6703:/<workers-artifacts-dir>/workers-artifacts/word-count-1-1591895933/6703 \
-v /<storm-local-dir>/workers-users/8198e1f0-f323-4b9d-8625-e4fd640cd058:/<storm-local-dir>/workers-users/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
-v /var/run/nscd:/var/run/nscd \
-v /<storm-local-dir>/supervisor/stormdist/word-count-1-1591895933/shared_by_topology:/<storm-local-dir>/supervisor/stormdist/word-count-1-1591895933/shared_by_topology \
-v /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/tmp:/tmp \
-v /etc/storm:/etc/storm:ro \
--cgroup-parent=/storm \
--group-add <gid> \
--workdir=/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
--cidfile=/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/container.cid \
--cap-drop=ALL \
--security-opt no-new-privileges \
--security-opt seccomp=/usr/share/apache-storm-2.3.0/conf/seccomp.json \
--cpus=2.6 xxx.xxx.com:8080/storm/storm/rhel7:latest \
bash /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/storm-worker-script.sh

----------------------------------------

TITLE: Implementing Batch Processing Bolt in Java
DESCRIPTION: Implementation of a BatchCount bolt that processes batches of tuples and emits partial counts during the batch finish phase.

LANGUAGE: java
CODE:
public static class BatchCount extends BaseBatchBolt {
    Object _id;
    BatchOutputCollector _collector;

    int _count = 0;

    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
        _collector = collector;
        _id = id;
    }

    @Override
    public void execute(Tuple tuple) {
        _count++;
    }

    @Override
    public void finishBatch() {
        _collector.emit(new Values(_id, _count));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "count"));
    }
}

----------------------------------------

TITLE: Storm UI Metrics Table
DESCRIPTION: Markdown table showing UI daemon metrics with names, types and descriptions

LANGUAGE: markdown
CODE:
| Metric Name | Type | Description |
|-------------|------|-------------|
| ui:num-activate-topology-http-requests | meter | calls to /topology/{id}/activate |

----------------------------------------

TITLE: ILeaderElector Interface Definition
DESCRIPTION: Interface defining methods for Storm's leader election mechanism used in Nimbus HA

LANGUAGE: java
CODE:
public interface ILeaderElector {
    void addToLeaderLockQueue();
    void removeFromLeaderLockQueue();
    boolean isLeader();
    InetSocketAddress getLeaderAddress();
    List<InetSocketAddress> getAllNimbusAddresses();
}

----------------------------------------

TITLE: Creating StateFactory for Custom State in Trident
DESCRIPTION: Implementation of a StateFactory for creating instances of a custom LocationDB State object within Trident tasks.

LANGUAGE: java
CODE:
public class LocationDBFactory implements StateFactory {
   public State makeState(Map conf, int partitionIndex, int numPartitions) {
      return new LocationDB();
   } 
}

----------------------------------------

TITLE: User Defined Function Example
DESCRIPTION: Example of creating and implementing a custom scalar function in Storm SQL

LANGUAGE: sql
CODE:
CREATE FUNCTION MYPLUS AS 'org.apache.storm.sql.TestUtils$MyPlus'

LANGUAGE: java
CODE:
public class MyPlus {
    public static Integer evaluate(Integer x, Integer y) {
      return x + y;
    }
  }

----------------------------------------

TITLE: Registering Worker Hook in Java Storm Topology
DESCRIPTION: Illustrates how to register a custom worker-level hook using the TopologyBuilder's addWorkerHook method. These hooks are called during worker startup, before spouts or bolts are prepared/opened.

LANGUAGE: java
CODE:
TopologyBuilder.addWorkerHook

----------------------------------------

TITLE: Using Named Setters and Aliases with CassandraWriterBolt in Java
DESCRIPTION: Demonstrates how to use named setters and aliases when binding values to a Cassandra query. This approach provides more flexibility and readability when mapping tuple fields to query parameters.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
     async(
        boundQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (:ti, :ye, :pe, :ge, :tr);")
            .bind(
                field("ti"),as("title"),
                field("ye").as("year")),
                field("pe").as("performer")),
                field("ge").as("genre")),
                field("tr").as("tracks"))
            ).byNamedSetters()
     )
);

----------------------------------------

TITLE: Dumping Heap in JSON
DESCRIPTION: Sample response from the /api/v1/topology/<id>/profiling/dumpheap/<host-port> endpoint, showing the result of dumping heap on a worker.

LANGUAGE: json
CODE:
{
   "status": "ok",
   "id": "10.11.1.7:6701"
}

----------------------------------------

TITLE: Property Substitution in Flux YAML
DESCRIPTION: Example of using property substitution in Flux YAML files to externalize configuration values.

LANGUAGE: yaml
CODE:
  - id: "zkHosts"
    className: "org.apache.storm.kafka.ZkHosts"
    constructorArgs:
      - "${kafka.zookeeper.hosts}"

----------------------------------------

TITLE: Implementing ISerialization Interface in Java for Storm Custom Serialization
DESCRIPTION: This code snippet shows the structure of the ISerialization interface that must be implemented to create a custom serializer in Storm. It includes methods for accepting a class, serializing an object, and deserializing data.

LANGUAGE: java
CODE:
public interface ISerialization<T> {
    public boolean accept(Class c);
    public void serialize(T object, DataOutputStream stream) throws IOException;
    public T deserialize(DataInputStream stream) throws IOException;
}

----------------------------------------

TITLE: Example GET Response - Cluster Configuration API
DESCRIPTION: Sample JSON response from the /api/v1/cluster/configuration endpoint showing cluster configuration settings

LANGUAGE: json
CODE:
{
  "dev.zookeeper.path": "/tmp/dev-storm-zookeeper",
  "topology.tick.tuple.freq.secs": null,
  "topology.builtin.metrics.bucket.size.secs": 60,
  "topology.fall.back.on.java.serialization": false,
  "topology.max.error.report.per.interval": 5,
  "zmq.linger.millis": 5000
}

----------------------------------------

TITLE: Configuring Single Pacemaker Host (Deprecated)
DESCRIPTION: Set a single Pacemaker host using the deprecated configuration option.

LANGUAGE: yaml
CODE:
pacemaker.host: single_pacemaker.mycompany.com

----------------------------------------

TITLE: Configuring Storm Cluster State Store
DESCRIPTION: Configuration setting to enable Pacemaker as the cluster state store in Storm.

LANGUAGE: yaml
CODE:
storm.cluster.state.store: "org.apache.storm.cluster.PaceMakerStateStorageFactory"

----------------------------------------

TITLE: Configuring Kryo Serialization Registration in Storm YAML
DESCRIPTION: Example configuration showing how to register custom types and serializers in Storm's topology configuration. Demonstrates both automatic FieldsSerializer usage and custom serializer implementation.

LANGUAGE: yaml
CODE:
topology.kryo.register:
  - com.mycompany.CustomType1
  - com.mycompany.CustomType2: com.mycompany.serializer.CustomType2Serializer
  - com.mycompany.CustomType3

----------------------------------------

TITLE: Building Azure Event Hubs Storm Integration
DESCRIPTION: Command to build the project using Maven. This will compile the code and package it into a JAR file.

LANGUAGE: bash
CODE:
mvn clean package

----------------------------------------

TITLE: Initializing EsPercolateBolt for Elasticsearch Percolation
DESCRIPTION: Creates an EsPercolateBolt instance to handle percolate requests to Elasticsearch with specified configuration.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsPercolateBolt percolateBolt = new EsPercolateBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Implementing Storm Topology with Kinesis Spout in Java
DESCRIPTION: Demonstrates how to create and configure a Storm topology that uses KinesisSpout to consume data from Amazon Kinesis. Shows initialization of connection parameters, ZooKeeper configuration, and topology setup.

LANGUAGE: java
CODE:
public class KinesisSpoutTopology {
    public static void main (String args[]) throws InvalidTopologyException, AuthorizationException, AlreadyAliveException {
        String topologyName = args[0];
        RecordToTupleMapper recordToTupleMapper = new TestRecordToTupleMapper();
        KinesisConnectionInfo kinesisConnectionInfo = new KinesisConnectionInfo(new CredentialsProviderChain(), new ClientConfiguration(), Regions.US_WEST_2,
                1000);
        ZkInfo zkInfo = new ZkInfo("localhost:2181", "/kinesisOffsets", 20000, 15000, 10000L, 3, 2000);
        KinesisConfig kinesisConfig = new KinesisConfig(args[1], ShardIteratorType.TRIM_HORIZON,
                recordToTupleMapper, new Date(), new ExponentialBackoffRetrier(), zkInfo, kinesisConnectionInfo, 10000L);
        KinesisSpout kinesisSpout = new KinesisSpout(kinesisConfig);
        TopologyBuilder topologyBuilder = new TopologyBuilder();
        topologyBuilder.setSpout("spout", kinesisSpout, 3);
        topologyBuilder.setBolt("bolt", new KinesisBoltTest(), 1).shuffleGrouping("spout");
        Config topologyConfig = new Config();
        topologyConfig.setDebug(true);
        topologyConfig.setNumWorkers(3);
        StormSubmitter.submitTopology(topologyName, topologyConfig, topologyBuilder.createTopology());
    }
}

----------------------------------------

TITLE: Defining Leader Election Interface in Java for Storm Nimbus
DESCRIPTION: Interface for leader election among Nimbus instances, including methods for queue management, leadership status checks, and address retrieval.

LANGUAGE: java
CODE:
public interface ILeaderElector {
    void addToLeaderLockQueue();
    void removeFromLeaderLockQueue();
    boolean isLeader();
    InetSocketAddress getLeaderAddress();
    List<InetSocketAddress> getAllNimbusAddresses();
}

----------------------------------------

TITLE: Creating RedisStoreBolt Instance
DESCRIPTION: Java code snippet for creating a RedisStoreBolt instance using JedisPoolConfig and WordCountStoreMapper.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
                .setHost(host).setPort(port).build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisStoreBolt storeBolt = new RedisStoreBolt(poolConfig, storeMapper);

----------------------------------------

TITLE: Redis State Provider Configuration in JSON
DESCRIPTION: Configuration schema for Redis state provider including key/value class definitions and Redis connection settings.

LANGUAGE: json
CODE:
{
  "keyClass": "Optional fully qualified class name of the Key type.",
  "valueClass": "Optional fully qualified class name of the Value type.",
  "keySerializerClass": "Optional Key serializer implementation class.",
  "valueSerializerClass": "Optional Value Serializer implementation class.",
  "jedisPoolConfig": {
    "host": "localhost",
    "port": 6379,
    "timeout": 2000,
    "database": 0,
    "password": "xyz"
    }
}

----------------------------------------

TITLE: Using Memcached State in Trident Topology
DESCRIPTION: Shows how to modify the persistent aggregation to use Memcached instead of in-memory state. This allows for distributed state storage in a Memcached cluster.

LANGUAGE: java
CODE:
.persistentAggregate(MemcachedState.transactional(serverLocations), new Count(), new Fields("count"))        
MemcachedState.transactional()

----------------------------------------

TITLE: Example Supervisor GPU Resource Configuration
DESCRIPTION: Example YAML configuration showing how to specify GPU resources available on a Storm supervisor node.

LANGUAGE: yaml
CODE:
supervisor.resources.map: {"gpu.count" : 2.0}

----------------------------------------

TITLE: Defining Custom Metric Filter Interface in Java
DESCRIPTION: Interface definition for creating custom metric filters to determine which metrics get reported.

LANGUAGE: java
CODE:
public interface StormMetricsFilter extends MetricFilter {

    /**
     * Called after the filter is instantiated.
     * @param config A map of the properties from the 'filter' section of the reporter configuration.
     */
    void prepare(Map<String, Object> config);
    
   /**
    *  Returns true if the given metric should be reported.
    */
    boolean matches(String name, Metric metric);

}

----------------------------------------

TITLE: Retrieving Supervisor Details in JSON
DESCRIPTION: Sample response from the /api/v1/supervisor endpoint, showing detailed information about supervisors and workers.

LANGUAGE: json
CODE:
{
  "supervisors": [{ 
    "totalMem": 4096.0, 
    "host":"192.168.10.237",
    "id":"bdfe8eff-f1d8-4bce-81f5-9d3ae1bf432e",
    "uptime":"7m 8s",
    "totalCpu":400.0,
    "usedCpu":495.0,
    "usedMem":3432.0,
    "slotsUsed":2,
    "version":"0.10.1",
    "slotsTotal":4,
    "uptimeSeconds":428
  }],
  "schedulerDisplayResource":true,
  "workers":[
    {
      "topologyName":"ras",
      "topologyId":"ras-4-1460229987",
      "host":"192.168.10.237",
      "supervisorId":"bdfe8eff-f1d8-4bce-81f5-9d3ae1bf432e",
      "assignedMemOnHeap":704.0,
      "uptime":"2m 47s",
      "uptimeSeconds":167,
      "port":6707,
      "workerLogLink":"http:\/\/host:8000\/log?file=ras-4-1460229987%2F6707%2Fworker.log",
      "componentNumTasks": {
        "word":5
      },
      "executorsTotal":8,
      "assignedCpu":130.0,
      "assignedMemOffHeap":80.0
    }
  ]
}

----------------------------------------

TITLE: Retrieving Supervisor Details in JSON
DESCRIPTION: Sample response from the /api/v1/supervisor endpoint, showing detailed information about supervisors and workers.

LANGUAGE: json
CODE:
{
  "supervisors": [{ 
    "totalMem": 4096.0, 
    "host":"192.168.10.237",
    "id":"bdfe8eff-f1d8-4bce-81f5-9d3ae1bf432e",
    "uptime":"7m 8s",
    "totalCpu":400.0,
    "usedCpu":495.0,
    "usedMem":3432.0,
    "slotsUsed":2,
    "version":"0.10.1",
    "slotsTotal":4,
    "uptimeSeconds":428
  }],
  "schedulerDisplayResource":true,
  "workers":[
    {
      "topologyName":"ras",
      "topologyId":"ras-4-1460229987",
      "host":"192.168.10.237",
      "supervisorId":"bdfe8eff-f1d8-4bce-81f5-9d3ae1bf432e",
      "assignedMemOnHeap":704.0,
      "uptime":"2m 47s",
      "uptimeSeconds":167,
      "port":6707,
      "workerLogLink":"http:\/\/host:8000\/log?file=ras-4-1460229987%2F6707%2Fworker.log",
      "componentNumTasks": {
        "word":5
      },
      "executorsTotal":8,
      "assignedCpu":130.0,
      "assignedMemOffHeap":80.0
    }
  ]
}

----------------------------------------

TITLE: Configuring JoinBolt with Stream Names in Java
DESCRIPTION: Java code example showing how to configure JoinBolt to use stream names instead of component names, useful for topologies with named streams.

LANGUAGE: java
CODE:
new JoinBolt(JoinBolt.Selector.STREAM,  "stream1", "key1")
                                  .join("stream2", "key2")
    ...

----------------------------------------

TITLE: Adding External Dependencies to Storm Classpath
DESCRIPTION: This snippet demonstrates various methods for adding external dependencies to the Storm classpath, including using command-line options, specific directories, and environment variables.

LANGUAGE: shell
CODE:
storm jar --jar <path-to-jar> --artifacts <comma-separated-list-of-artifacts>

LANGUAGE: shell
CODE:
# Add dependencies to these directories
${STORM_DIR}/extlib/
${STORM_DIR}/extlib-daemon/

LANGUAGE: shell
CODE:
# Set these environment variables
STORM_EXT_CLASSPATH
STORM_EXT_CLASSPATH_DAEMON

----------------------------------------

TITLE: Debugging Storm Topology in IDE
DESCRIPTION: Shows how to override local mode execution for debugging purposes within an IDE using LocalCluster.withLocalModeOverride method.

LANGUAGE: java
CODE:
public static void main(final String args[]) {
    LocalCluster.withLocalModeOverride(() -> originalMain(args), 10);
}

----------------------------------------

TITLE: Named Stream Join Example
DESCRIPTION: Example showing how to join named streams instead of default streams using JoinBolt.Selector.STREAM.

LANGUAGE: java
CODE:
new JoinBolt(JoinBolt.Selector.STREAM,  "stream1", "key1") 
                             .join     ("stream2", "userId",  "stream1" )
                             .select ("userId, key1, key2")
                             .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;
                             
topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("bolt1", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt2", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt3", "stream2", new Fields("userId") )
            .fieldsGrouping("bolt4", "stream1", new Fields("key1") );

----------------------------------------

TITLE: Spout Tuple Transfer in Apache Storm (Clojure)
DESCRIPTION: Implementation of tuple transfer for spouts using the worker-provided transfer function.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L329

----------------------------------------

TITLE: Defining Storm Component Object Structure in Thrift
DESCRIPTION: Thrift union structure defining how component objects (spouts and bolts) can be specified in Storm. Supports serialized Java objects, shell components, and Java object specifications.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Digest Authentication JAAS Configuration
DESCRIPTION: JAAS configuration for Pacemaker digest authentication containing username and password.

LANGUAGE: java
CODE:
PacemakerDigest {
    username="some username"
    password="some password";
};

----------------------------------------

TITLE: Configuring Storm Metric Store with YAML
DESCRIPTION: YAML configuration options for Storm's metric store implementation. Defines the metric store class, processor class, RocksDB location, database creation behavior, metadata cache capacity, and retention period.

LANGUAGE: yaml
CODE:
storm.metricstore.class: "org.apache.storm.metricstore.rocksdb.RocksDbStore"
storm.metricprocessor.class: "org.apache.storm.metricstore.NimbusMetricProcessor"
storm.metricstore.rocksdb.location: "storm_rocks"
storm.metricstore.rocksdb.create_if_missing: true
storm.metricstore.rocksdb.metadata_string_cache_capacity: 4000
storm.metricstore.rocksdb.retention_hours: 240

----------------------------------------

TITLE: Creating RedisStoreBolt Instance
DESCRIPTION: Java code snippet for creating a RedisStoreBolt instance using JedisPoolConfig and WordCountStoreMapper.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
                .setHost(host).setPort(port).build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisStoreBolt storeBolt = new RedisStoreBolt(poolConfig, storeMapper);

----------------------------------------

TITLE: Implementing HiveState with Trident
DESCRIPTION: Example of implementing HiveState for Trident topologies, showing how to configure the state factory with HiveOptions and create a persistent stream.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withTimeAsPartitionField("YYYY/MM/DD");
            
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
                                .withTxnsPerBatch(10)
                				.withBatchSize(1000)
                	     		.withIdleTimeout(10)
                	     		
StateFactory factory = new HiveStateFactory().withOptions(hiveOptions);
TridentState state = stream.partitionPersist(factory, hiveFields, new HiveUpdater(), new Fields());

----------------------------------------

TITLE: Implementing a Custom Filter in Trident
DESCRIPTION: Demonstrates how to create a custom filter that keeps tuples based on specific conditions.

LANGUAGE: java
CODE:
public class MyFilter extends BaseFilter {
    public boolean isKeep(TridentTuple tuple) {
        return tuple.getInteger(0) == 1 && tuple.getInteger(1) == 2;
    }
}

----------------------------------------

TITLE: Retrieving Supervisor Details in JSON
DESCRIPTION: Example response from the /api/v1/supervisor endpoint, which returns details for a specific supervisor or all supervisors on a host.

LANGUAGE: json
CODE:
{
  "supervisors": [{ 
    "totalMem": 4096.0, 
    "host":"192.168.10.237",
    "id":"bdfe8eff-f1d8-4bce-81f5-9d3ae1bf432e",
    "uptime":"7m 8s",
    "totalCpu":400.0,
    "usedCpu":495.0,
    "usedMem":3432.0,
    "slotsUsed":2,
    "version":"0.10.1",
    "slotsTotal":4,
    "uptimeSeconds":428
  }],
  "schedulerDisplayResource":true,
  "workers":[
    {
      "topologyName":"ras",
      "topologyId":"ras-4-1460229987",
      "host":"192.168.10.237",
      "supervisorId":"bdfe8eff-f1d8-4bce-81f5-9d3ae1bf432e",
      "assignedMemOnHeap":704.0,
      "uptime":"2m 47s",
      "uptimeSeconds":167,
      "port":6707,
      "workerLogLink":"http:\/\/host:8000\/log?file=ras-4-1460229987%2F6707%2Fworker.log",
      "componentNumTasks": {
        "word":5
      },
      "executorsTotal":8,
      "assignedCpu":130.0,
      "assignedMemOffHeap":80.0
    }
  ]
}

----------------------------------------

TITLE: Configuring Storm Metric Store with YAML
DESCRIPTION: YAML configuration options for Storm's metric store implementation. Defines the metric store class, processor class, RocksDB location, database creation behavior, metadata cache capacity, and retention period.

LANGUAGE: yaml
CODE:
storm.metricstore.class: "org.apache.storm.metricstore.rocksdb.RocksDbStore"
storm.metricprocessor.class: "org.apache.storm.metricstore.NimbusMetricProcessor"
storm.metricstore.rocksdb.location: "storm_rocks"
storm.metricstore.rocksdb.create_if_missing: true
storm.metricstore.rocksdb.metadata_string_cache_capacity: 4000
storm.metricstore.rocksdb.retention_hours: 240

----------------------------------------

TITLE: Creating SolrJsonMapper in Java
DESCRIPTION: Demonstrates how to create a SolrJsonMapper object for updating a Solr collection with JSON content from a specific tuple field.

LANGUAGE: java
CODE:
SolrMapper solrMapper = new SolrJsonMapper.Builder("gettingstarted", "JSON").build();

----------------------------------------

TITLE: Defining JdbcMapper Interface for Storm-JDBC Integration
DESCRIPTION: Interface for mapping Storm tuples to database columns. The getColumns method defines how a Storm tuple maps to a list of columns representing a row in a database.

LANGUAGE: java
CODE:
public interface JdbcMapper  extends Serializable {
    List<Column> getColumns(ITuple tuple);
}

----------------------------------------

TITLE: Configuring Storm Metricstore in YAML
DESCRIPTION: YAML configuration options for setting up the Storm Metricstore, including the MetricStore class, WorkerMetricsProcessor class, RocksDB location, and various RocksDB-specific settings.

LANGUAGE: yaml
CODE:
storm.metricstore.class: "org.apache.storm.metricstore.rocksdb.RocksDbStore"
storm.metricprocessor.class: "org.apache.storm.metricstore.NimbusMetricProcessor"
storm.metricstore.rocksdb.location: "storm_rocks"
storm.metricstore.rocksdb.create_if_missing: true
storm.metricstore.rocksdb.metadata_string_cache_capacity: 4000
storm.metricstore.rocksdb.retention_hours: 240

----------------------------------------

TITLE: Dequeuing and Removing Items from Kestrel in Java
DESCRIPTION: Method to dequeue items from a Kestrel queue and remove them using acknowledgment. Processes and removes each item from the queue.

LANGUAGE: java
CODE:
    private static void dequeueAndRemoveItems(KestrelClient kestrelClient, String queueName)
    throws IOException, ParseError
		 {
			for(int i=1; i<=12; i++){

				Item item = kestrelClient.dequeue(queueName);


				if(item==null){
					System.out.println("The queue (" + queueName + ") contains no items.");
				}
				else
				{
					int itemID = item._id;


					byte[] data = item._data;

					String receivedVal = new String(data);

					kestrelClient.ack(queueName, itemID);

					System.out.println("receivedItem=" + receivedVal);
				}
			}
	}

----------------------------------------

TITLE: Chaining Aggregators in Trident
DESCRIPTION: Example of chaining Count and Sum aggregators.

LANGUAGE: java
CODE:
mystream.chainedAgg()
        .partitionAggregate(new Count(), new Fields("count"))
        .partitionAggregate(new Fields("b"), new Sum(), new Fields("sum"))
        .chainEnd()

----------------------------------------

TITLE: Defining Storm Topology in Clojure
DESCRIPTION: Example of defining a Storm topology with spouts and bolts using the Clojure DSL. Shows how to wire components together with different stream groupings and parallelism hints.

LANGUAGE: clojure
CODE:
(topology
 {"1" (spout-spec sentence-spout)
  "2" (spout-spec (sentence-spout-parameterized
                   ["the cat jumped over the door"
                    "greetings from a faraway land"])
                   :p 2)}
 {"3" (bolt-spec {"1" :shuffle "2" :shuffle}
                 split-sentence
                 :p 5)
  "4" (bolt-spec {"3" ["word"]}
                 word-count
                 :p 6)})

----------------------------------------

TITLE: Submitting Storm SQL Topology with Dependencies
DESCRIPTION: Command to submit a Storm SQL topology with required dependencies. This example includes Storm SQL Kafka integration and Kafka client libraries as artifacts.

LANGUAGE: bash
CODE:
$ bin/storm sql order_filtering.sql order_filtering --artifacts "org.apache.storm:storm-sql-kafka:2.0.0-SNAPSHOT,org.apache.storm:storm-kafka-client:2.0.0-SNAPSHOT,org.apache.kafka:kafka-clients:1.1.0^org.slf4j:slf4j-log4j12"

----------------------------------------

TITLE: CGroup CPU Usage Metrics Format in Storm
DESCRIPTION: JSON structure showing how CGroup CPU metrics are reported in Storm, including user and system CPU usage in milliseconds.

LANGUAGE: json
CODE:
{
   "user-ms": number
   "sys-ms": number
}

----------------------------------------

TITLE: Task ID Determination for Tuple Sending in Apache Storm (Clojure)
DESCRIPTION: The 'tasks-fn' returns the task IDs to send tuples to for either regular stream emit or direct stream emit.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L207

----------------------------------------

TITLE: Retrieving Nimbus Summary in JSON
DESCRIPTION: Example response from the /api/v1/nimbus/summary endpoint, which returns summary information for all Nimbus hosts.

LANGUAGE: json
CODE:
{
  "nimbuses":[
    {
      "host":"192.168.202.1",
      "port":6627,
      "nimbusLogLink":"http:\/\/192.168.202.1:8000\/log?file=nimbus.log",
      "status":"Leader",
      "version":"0.10.0-SNAPSHOT",
      "nimbusUpTime":"3m 33s",
      "nimbusUpTimeSeconds":"213"
    }
  ]
}

----------------------------------------

TITLE: Kerberos Server JAAS Configuration
DESCRIPTION: JAAS configuration for Pacemaker server using Kerberos authentication.

LANGUAGE: java
CODE:
PacemakerServer {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/etc/keytabs/pacemaker.keytab"
   storeKey=true
   useTicketCache=false
   principal="pacemaker@MY.COMPANY.COM";
};

----------------------------------------

TITLE: IConfigLoader Interface Definition
DESCRIPTION: Core interface definition for configuration loading, containing a single load() method that returns the latest configuration map.

LANGUAGE: java
CODE:
public interface IConfigLoader {
    Map<?,?> load();
};

----------------------------------------

TITLE: Continuously Adding Items to Kestrel Queue in Java
DESCRIPTION: Main program to continuously add sentence items to a Kestrel queue named 'sentence_queue' on a local Kestrel server. It runs until a closing bracket ']' is entered in the console.

LANGUAGE: java
CODE:
import java.io.IOException;
import java.io.InputStream;
import java.util.Random;

import org.apache.storm.spout.KestrelClient;
import org.apache.storm.spout.KestrelClient.Item;
import org.apache.storm.spout.KestrelClient.ParseError;

public class AddSentenceItemsToKestrel {

	/**
	 * @param args
	 */
	public static void main(String[] args) {

		InputStream is = System.in;

		char closing_bracket = ']';

		int val = closing_bracket;

		boolean aux = true;

		try {

			KestrelClient kestrelClient = null;
			String queueName = "sentence_queue";

			while(aux){

				kestrelClient = new KestrelClient("localhost",22133);

				queueSentenceItems(kestrelClient, queueName);

				kestrelClient.close();

				Thread.sleep(1000);

				if(is.available()>0){
				 if(val==is.read())
					 aux=false;
				}
			}
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		catch (ParseError e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		} catch (InterruptedException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}

		System.out.println("end");

    }
}

----------------------------------------

TITLE: Storm JAR Command with Dependencies Example
DESCRIPTION: Complete example showing how to run a Storm topology with external JARs and Maven artifacts including custom repository configuration.

LANGUAGE: bash
CODE:
./bin/storm jar example/storm-starter/storm-starter-topologies-*.jar org.apache.storm.starter.RollingTopWords blobstore-remote2 remote --jars "./external/storm-redis/storm-redis-1.1.0.jar,./external/storm-kafka/storm-kafka-1.1.0.jar" --artifacts "redis.clients:jedis:2.9.0,org.apache.kafka:kafka_2.10:0.8.2.2^org.slf4j:slf4j-log4j12" --artifactRepositories "jboss-repository^http://repository.jboss.com/maven2,HDPRepo^http://repo.hortonworks.com/content/groups/public/"

----------------------------------------

TITLE: Trident Map Function Example
DESCRIPTION: Demonstrates implementing a map function that transforms each tuple by converting strings to uppercase.

LANGUAGE: java
CODE:
public class UpperCase extends MapFunction {
 @Override
 public Values execute(TridentTuple input) {
   return new Values(input.getString(0).toUpperCase());
 }
}

----------------------------------------

TITLE: Defining Kafka Tuple Mapping Interface
DESCRIPTION: Interface definitions for mapping Storm tuples to Kafka key-value pairs

LANGUAGE: java
CODE:
K getKeyFromTuple(Tuple/TridentTuple tuple);
V getMessageFromTuple(Tuple/TridentTuple tuple);

----------------------------------------

TITLE: Configuring Kerberos Authentication for Pacemaker Server
DESCRIPTION: JAAS configuration for Kerberos authentication on Pacemaker node for Pacemaker server.

LANGUAGE: java
CODE:
PacemakerServer {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/etc/keytabs/pacemaker.keytab"
   storeKey=true
   useTicketCache=false
   principal="pacemaker@MY.COMPANY.COM";
};

----------------------------------------

TITLE: Simple Sentence Splitting Bolt in Clojure
DESCRIPTION: Implementation of a basic bolt that splits sentences into words, demonstrating tuple processing and emission.

LANGUAGE: clojure
CODE:
(defbolt split-sentence ["word"] [tuple collector]
  (let [words (.split (.getString tuple 0) " ")]
    (doseq [w words]
      (emit-bolt! collector [w] :anchor tuple))
    (ack! collector tuple)
    ))

----------------------------------------

TITLE: Configuring Maven Dependencies for Storm Kinesis Spout
DESCRIPTION: This XML snippet shows the Maven dependencies required for using the Storm Kinesis Spout, including AWS SDK, Storm client, and other necessary libraries.

LANGUAGE: xml
CODE:
 <dependencies>
        <dependency>
            <groupId>com.amazonaws</groupId>
            <artifactId>aws-java-sdk</artifactId>
            <version>${aws-java-sdk.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-client</artifactId>
            <version>${project.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-framework</artifactId>
            <version>${curator.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>com.googlecode.json-simple</groupId>
            <artifactId>json-simple</artifactId>
        </dependency>
 </dependencies>

----------------------------------------

TITLE: State Management in Storm's Stream API (Java)
DESCRIPTION: Demonstrates how to update and query state in a stream.

LANGUAGE: java
CODE:
PairStream<String, Long> wordCounts = ...
// Update the word counts in the state; here the first argument 0L is the initial value for the state and 
// the second argument is a function that adds the count to the current value in the state.
StreamState<String, Long> streamState = wordCounts.updateStateByKey(0L, (state, count) -> state + count)
streamState.toPairStream().print();

// The stream of words emitted by the QuerySpout is used as the keys to query the state.
builder.newStream(new QuerySpout(), new ValueMapper<String>(0))
// Queries the state and emits the matching (key, value) as results. 
// The stream state returned by updateStateByKey is passed as the argument to stateQuery.
.stateQuery(streamState).print();

----------------------------------------

TITLE: Configuring Maven Dependencies for Flux
DESCRIPTION: XML configuration showing how to add Flux Core and Wrappers dependencies to a Maven project for Storm topology development

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>flux-core</artifactId>
    <version>${storm.version}</version>
</dependency>
<dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>flux-wrappers</artifactId>
    <version>${storm.version}</version>
</dependency>

----------------------------------------

TITLE: Defining Supervisor Slot Ports in Storm YAML
DESCRIPTION: This YAML configuration defines the ports that Storm workers will use on each machine in the cluster.

LANGUAGE: yaml
CODE:
supervisor.slots.ports:
    - 6700
    - 6701
    - 6702
    - 6703

----------------------------------------

TITLE: Cassandra Insert Query with All Tuple Fields
DESCRIPTION: Demonstrates how to create a CassandraWriterBolt that inserts all tuple fields into an album table.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
    async(
        simpleQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all())
    )
);

----------------------------------------

TITLE: Supervisor Process Synchronization
DESCRIPTION: Code showing how Supervisor synchronizes processes by comparing desired state with actual running processes and starting/stopping workers as needed.

LANGUAGE: clojure
CODE:
synchronize-supervisor
sync-processes

----------------------------------------

TITLE: Running Event Hubs Test Client
DESCRIPTION: Command to run the included Event Hubs send client for testing purposes

LANGUAGE: bash
CODE:
java -cp .\target\eventhubs-storm-spout-{version}-jar-with-dependencies.jar com.microsoft.eventhubs.client.EventHubSendClient [username] [password] [entityPath] [partitionId] [messageSize] [messageCount]

----------------------------------------

TITLE: Storm SQL Query for Error Log Filtering
DESCRIPTION: SQL script to filter error logs (status >= 400) from Apache logs and write to a separate Kafka topic

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://apache-logs?bootstrap-servers=localhost:9092'
CREATE EXTERNAL TABLE APACHE_ERROR_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_ELAPSED_MS INT) LOCATION 'kafka://apache-error-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO APACHE_ERROR_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, (TIME_US / 1000) AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (CAST(STATUS AS INT) / 100) >= 4

----------------------------------------

TITLE: Configuring MongoDB Trident MapState for Storm Integration
DESCRIPTION: This snippet demonstrates how to configure and use MongoDB Trident MapState, which provides a map-like persistent state for Trident topologies. It includes setting up the MongoMapper, QueryFilterCreator, MongoMapState.Options, and creating a StateFactory.

LANGUAGE: java
CODE:
MongoMapper mapper = new SimpleMongoMapper()
        .withFields("word", "count");

QueryFilterCreator filterCreator = new SimpleQueryFilterCreator()
        .withField("word");

MongoMapState.Options options = new MongoMapState.Options();
options.url = url;
options.collectionName = collectionName;
options.mapper = mapper;
options.queryCreator = filterCreator;

StateFactory factory = MongoMapState.transactional(options);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

TridentState state = stream.groupBy(new Fields("word"))
        .persistentAggregate(factory, new Fields("count"), new Sum(), new Fields("sum"));

stream.stateQuery(state, new Fields("word"), new MapGet(), new Fields("sum"))
        .each(new Fields("word", "sum"), new PrintFunction(), new Fields());

----------------------------------------

TITLE: Running Storm Jar Command
DESCRIPTION: Executes a Storm topology jar file with specified class and arguments. Allows including additional jars and Maven artifacts.

LANGUAGE: bash
CODE:
storm jar topology-jar-path class ...

----------------------------------------

TITLE: Custom Exception Handling Implementation
DESCRIPTION: Shows how to implement custom exception handling for Cassandra operations using the ExecutionResultHandler interface. Includes error handling for various Cassandra-specific exceptions.

LANGUAGE: java
CODE:
public interface ExecutionResultHandler extends Serializable {
    void onQueryValidationException(QueryValidationException e, OutputCollector collector, Tuple tuple);

    void onReadTimeoutException(ReadTimeoutException e, OutputCollector collector, Tuple tuple);

    void onWriteTimeoutException(WriteTimeoutException e, OutputCollector collector, Tuple tuple);

    void onUnavailableException(UnavailableException e, OutputCollector collector, Tuple tuple);

    void onQuerySuccess(OutputCollector collector, Tuple tuple);
}

----------------------------------------

TITLE: Maven Dependencies Configuration
DESCRIPTION: Maven dependency configuration for Storm Kinesis Spout integration, including AWS SDK, Storm Core, Curator Framework, and JSON Simple dependencies.

LANGUAGE: xml
CODE:
 <dependencies>
        <dependency>
            <groupId>com.amazonaws</groupId>
            <artifactId>aws-java-sdk</artifactId>
            <version>${aws-java-sdk.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-core</artifactId>
            <version>${project.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-framework</artifactId>
            <version>${curator.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>com.googlecode.json-simple</groupId>
            <artifactId>json-simple</artifactId>
        </dependency>
 </dependencies>

----------------------------------------

TITLE: Ruby Syntax Highlighting Classes
DESCRIPTION: Specifies syntax highlighting classes for Ruby code elements including keywords, strings, substitutions, comments, YARD tags, functions, and classes.



----------------------------------------

TITLE: Configuring Storm Health Check Timeout in YAML
DESCRIPTION: This code snippet demonstrates how to set the storm.health.check.timeout.ms configuration in storm.yaml. It specifies the maximum time allowed for a health check script to run before it is marked as failed.

LANGUAGE: yaml
CODE:
storm.health.check.timeout.ms: 5000

----------------------------------------

TITLE: Configuring IsolationScheduler Machine Allocation in YAML
DESCRIPTION: YAML configuration example showing how to allocate dedicated machines to specific topologies using the IsolationScheduler. The configuration maps topology names to their allocated number of isolated machines.

LANGUAGE: yaml
CODE:
isolation.scheduler.machines: 
    "my-topology": 8
    "tiny-topology": 1
    "some-other-topology": 3

----------------------------------------

TITLE: Implementing Kafka Topic Selector
DESCRIPTION: Interface definition for selecting Kafka topics for message publishing

LANGUAGE: java
CODE:
public interface KafkaTopicSelector {
    String getTopics(Tuple/TridentTuple tuple);
}

----------------------------------------

TITLE: Dumping JStack in JSON
DESCRIPTION: Sample response from the /api/v1/topology/<id>/profiling/dumpjstack/<host-port> endpoint, showing the result of dumping jstack on a worker.

LANGUAGE: json
CODE:
{
   "status": "ok",
   "id": "10.11.1.7:6701"
}

----------------------------------------

TITLE: Configuring Pacemaker Servers
DESCRIPTION: YAML configuration for specifying Pacemaker server hosts.

LANGUAGE: yaml
CODE:
pacemaker.servers:
    - somehost.mycompany.com
    - someotherhost.mycompany.com

----------------------------------------

TITLE: Filtering Kafka Stream Example in Storm SQL
DESCRIPTION: Complete example showing how to filter large orders from a Kafka stream using Storm SQL, including table creation and data transformation.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://localhost:2181/brokers?topic=orders'
CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) LOCATION 'kafka://localhost:2181/brokers?topic=large_orders' TBLPROPERTIES '{"producer":{"bootstrap.servers":"localhost:9092","acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer","value.serializer":"org.apache.storm.kafka.ByteBufferSerializer"}}'
INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Configuring Digest Authentication for Pacemaker
DESCRIPTION: This snippet shows the JAAS configuration for digest authentication in Pacemaker. It includes the username and password for authentication.

LANGUAGE: java
CODE:
PacemakerDigest {
    username="some username"
    password="some password";
};

----------------------------------------

TITLE: Configuring Health Check Settings
DESCRIPTION: Specifies the directory for health check scripts and timeout settings for monitoring supervisor node health.

LANGUAGE: yaml
CODE:
storm.health.check.dir: "healthchecks"
storm.health.check.timeout.ms: 5000

----------------------------------------

TITLE: Creating Kafka Topics for Apache Logs
DESCRIPTION: Commands to create Kafka topics for storing different types of Apache logs with specified partitions and replication factors.

LANGUAGE: shell
CODE:
kafka-topics --create --topic apache-logs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
kafka-topics --create --topic apache-errorlogs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
kafka-topics --create --topic apache-slowlogs --zookeeper localhost:2181 --replication-factor 1 --partitions 5

----------------------------------------

TITLE: Running Storm JMS Example Topology Locally
DESCRIPTION: Maven command to execute the example Storm JMS topology locally. This runs the topology, which will connect to ActiveMQ and create the necessary JMS destinations.

LANGUAGE: bash
CODE:
$ mvn exec:java

----------------------------------------

TITLE: Bolt Message Listening in Apache Storm (Clojure)
DESCRIPTION: Implementation of message listening for bolts in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L489

----------------------------------------

TITLE: Debugging Storm Topology in IDE
DESCRIPTION: Shows how to override local mode for debugging Storm topologies within an IDE environment using LocalCluster.withLocalModeOverride method.

LANGUAGE: java
CODE:
public static void main(final String args[]) {
    LocalCluster.withLocalModeOverride(() -> originalMain(args), 10);
}

----------------------------------------

TITLE: Launching Storm DRPC Daemon
DESCRIPTION: Starts the Distributed RPC (DRPC) daemon for processing DRPC requests.

LANGUAGE: shell
CODE:
storm drpc

----------------------------------------

TITLE: Killing a Storm Topology
DESCRIPTION: Kills a specified topology, with an optional wait time before shutdown.

LANGUAGE: bash
CODE:
storm kill topology-name [-w wait-time-secs]

----------------------------------------

TITLE: Storm Component Configuration Options
DESCRIPTION: List of configuration options that can be overridden on a per-bolt/per-spout basis in Storm 0.7.0 and later.

LANGUAGE: text
CODE:
"topology.debug"
"topology.max.spout.pending"
"topology.max.task.parallelism"
"topology.kryo.register"

----------------------------------------

TITLE: Custom Exception Handling Implementation
DESCRIPTION: Shows how to implement custom exception handling for Cassandra operations using the ExecutionResultHandler interface. Includes error handling for various Cassandra-specific exceptions.

LANGUAGE: java
CODE:
public interface ExecutionResultHandler extends Serializable {
    void onQueryValidationException(QueryValidationException e, OutputCollector collector, Tuple tuple);

    void onReadTimeoutException(ReadTimeoutException e, OutputCollector collector, Tuple tuple);

    void onWriteTimeoutException(WriteTimeoutException e, OutputCollector collector, Tuple tuple);

    void onUnavailableException(UnavailableException e, OutputCollector collector, Tuple tuple);

    void onQuerySuccess(OutputCollector collector, Tuple tuple);
}

----------------------------------------

TITLE: SQL Join Example
DESCRIPTION: Example SQL query showing a join between 4 tables that will be recreated using Storm's JoinBolt.

LANGUAGE: sql
CODE:
select  userId, key4, key2, key3
from        table1
inner join  table2  on table2.userId =  table1.key1
inner join  table3  on table3.key3   =  table2.userId
left join   table4  on table4.key4   =  table3.key3

----------------------------------------

TITLE: System Topology Creation References
DESCRIPTION: Code references showing where system-topology! function is used in Nimbus for task creation and in worker for message routing.

LANGUAGE: clojure
CODE:
[system-topology!](https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/common.clj#L188)

----------------------------------------

TITLE: Partial Uniquer Implementation for Twitter Reach Calculation
DESCRIPTION: Implementation of a batch bolt that processes unique followers for the Twitter reach calculation.

LANGUAGE: java
CODE:
public class PartialUniquer extends BaseBatchBolt {
    BatchOutputCollector _collector;
    Object _id;
    Set<String> _followers = new HashSet<String>();
    
    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
        _collector = collector;
        _id = id;
    }

    @Override
    public void execute(Tuple tuple) {
        _followers.add(tuple.getString(1));
    }
    
    @Override
    public void finishBatch() {
        _collector.emit(new Values(_id, _followers.size()));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "partial-count"));
    }
}

----------------------------------------

TITLE: Kerberos Client Authentication Configuration
DESCRIPTION: JAAS configuration for Nimbus client using Kerberos authentication with Pacemaker.

LANGUAGE: java
CODE:
PacemakerClient {
    com.sun.security.auth.module.Krb5LoginModule required
    useKeyTab=true
    keyTab="/etc/keytabs/nimbus.keytab"
    storeKey=true
    useTicketCache=false
    serviceName="pacemaker"
    principal="nimbus@MY.COMPANY.COM";
};

----------------------------------------

TITLE: Configuring RedisState for Trident Topology in Java
DESCRIPTION: Java code snippet demonstrating how to configure RedisState for use in a Trident topology.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
                                .setHost(redisHost).setPort(redisPort)
                                .build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisLookupMapper lookupMapper = new WordCountLookupMapper();
RedisState.Factory factory = new RedisState.Factory(poolConfig);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory,
                        fields,
                        new RedisStateUpdater(storeMapper).withExpire(86400000),
                        new Fields());

TridentState state = topology.newStaticState(factory);
stream = stream.stateQuery(state, new Fields("word"),
                        new RedisStateQuerier(lookupMapper),
                        new Fields("columnName","columnValue"));

----------------------------------------

TITLE: Setting Topology Priority in Java
DESCRIPTION: Java API call to set the priority level of a topology.

LANGUAGE: java
CODE:
conf.setTopologyPriority(int priority)

----------------------------------------

TITLE: Implementing FailedMessageRetryHandler Interface in Java
DESCRIPTION: This code snippet defines the FailedMessageRetryHandler interface, which is used to manage the retry mechanism for failed messages in the Kinesis Spout. It includes methods for handling failed messages, retrieving the next message to retry, and acknowledging successful retries.

LANGUAGE: java
CODE:
    boolean failed (KinesisMessageId messageId);
    KinesisMessageId getNextFailedMessageToRetry ();
    void failedMessageEmitted (KinesisMessageId messageId);
    void acked (KinesisMessageId messageId);

----------------------------------------

TITLE: Image Tag to Manifest Mapping File
DESCRIPTION: Content example of image-tag-to-manifest-file mapping Docker image tags to manifest hashes

LANGUAGE: bash
CODE:
storm/rhel7:dev_current:26fd443859325d5911f3be5c5e231dddca88ee0d526456c0c92dd794148d8585#docker.xxx.com:4443/hadoop-user-images/storm/rhel7:20201202-232133

----------------------------------------

TITLE: Configuring Zookeeper Servers in Storm YAML
DESCRIPTION: Configuration block for specifying Zookeeper server addresses in storm.yaml file. This is required for Storm cluster coordination.

LANGUAGE: yaml
CODE:
storm.zookeeper.servers:
  - "111.222.333.444"
  - "555.666.777.888"

----------------------------------------

TITLE: Running Storm MQTT Example Topology with Flux
DESCRIPTION: Command to run a sample Storm topology using Flux, which creates a local mode cluster with an MQTT Spout publishing to a logging bolt.

LANGUAGE: bash
CODE:
storm jar ./examples/target/storm-mqtt-examples-*-SNAPSHOT.jar org.apache.storm.flux.Flux ./examples/src/main/flux/sample.yaml --local

----------------------------------------

TITLE: Creating HBaseProjectionCriteria in Java
DESCRIPTION: This code creates an HBaseProjectionCriteria instance to specify which columns or column families to include in HBase Get operations.

LANGUAGE: java
CODE:
HBaseProjectionCriteria projectionCriteria = new HBaseProjectionCriteria()
    .addColumn(new HBaseProjectionCriteria.ColumnMetaData("cf", "count"))
    .addColumnFamily("cf2");

----------------------------------------

TITLE: Implementing One Aggregator for Unique Counting in Trident
DESCRIPTION: Defines a combiner aggregator called 'One' used in the reach calculation topology for efficient unique counting.

LANGUAGE: java
CODE:
public class One implements CombinerAggregator<Integer> {
   public Integer init(TridentTuple tuple) {
       return 1;
   }

   public Integer combine(Integer val1, Integer val2) {
       return 1;
   }

   public Integer zero() {
       return 1;
   }        
}

----------------------------------------

TITLE: Worker Connection Management in Storm
DESCRIPTION: Code showing how worker refreshes connections to other workers and maintains task mapping. Called every task.refresh.poll.secs or when ZK assignment changes.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L123

----------------------------------------

TITLE: Dequeuing Items from Kestrel Queue in Java
DESCRIPTION: This method demonstrates how to dequeue items from a Kestrel queue without removing them. It uses KestrelClient to retrieve items and print their contents.

LANGUAGE: java
CODE:
    private static void dequeueItems(KestrelClient kestrelClient, String queueName) throws IOException, ParseError
			 {
		for(int i=1; i<=12; i++){

			Item item = kestrelClient.dequeue(queueName);

			if(item==null){
				System.out.println("The queue (" + queueName + ") contains no items.");
			}
			else
			{
				byte[] data = item._data;

				String receivedVal = new String(data);

				System.out.println("receivedItem=" + receivedVal);
			}
		}

----------------------------------------

TITLE: Defining a Parameterized Bolt in Clojure
DESCRIPTION: Example of defining a parameterized bolt that appends a suffix to input strings using the defbolt macro.

LANGUAGE: clojure
CODE:
(defbolt suffix-appender ["word"] {:params [suffix]}
  [tuple collector]
  (emit-bolt! collector [(str (.getString tuple 0) suffix)] :anchor tuple)
  )

----------------------------------------

TITLE: Defining Component Object Structure in Storm Thrift
DESCRIPTION: Thrift union structure defining how component objects (spouts and bolts) can be specified in Storm. Supports serialized Java, shell components, and Java objects.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Creating Kafka Topics for Apache Logs
DESCRIPTION: Creates Kafka topics for storing different types of Apache logs using the kafka-topics command.

LANGUAGE: bash
CODE:
kafka-topics --create --topic apache-logs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
kafka-topics --create --topic apache-errorlogs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
kafka-topics --create --topic apache-slowlogs --zookeeper localhost:2181 --replication-factor 1 --partitions 5

----------------------------------------

TITLE: Using Preconfigured DRPC Client in Java
DESCRIPTION: This code shows how to use a preconfigured DRPC client to execute a remote function call, with automatic host selection and failover.

LANGUAGE: java
CODE:
DRPCClient client = DRPCClient.getConfiguredClient(conf);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Retrieving Component Metrics Sample Response
DESCRIPTION: Example JSON response showing detailed metrics for a topology component like spouts and bolts with their statistics

LANGUAGE: json
CODE:
{
    "name": "WordCount3",
    "id": "spout",
    "componentType": "spout",
    "windowHint": "10m 0s",
    "executors": 5,
    "tasks": 5,
    "window": "600"
}

----------------------------------------

TITLE: Uploading Topology Jar in StormSubmitter (Java)
DESCRIPTION: StormSubmitter uploads the topology jar to Nimbus using its Thrift interface. It uploads the jar in 15KB chunks and finalizes the upload process.

LANGUAGE: java
CODE:
StormSubmitter.submitTopology("topology-name", conf, topology);

----------------------------------------

TITLE: Storm Configuration File Path
DESCRIPTION: Example path showing where to put configuration overrides in the Storm classpath.

LANGUAGE: text
CODE:
META-INF/services/org.apache.storm.validation.Validated

----------------------------------------

TITLE: Initializing RocketMqBolt for Writing to RocketMQ Topic in Java
DESCRIPTION: This code demonstrates how to create an instance of RocketMqBolt to write data to a RocketMQ topic. It configures the bolt with a TupleToMessageMapper, TopicSelector, and Properties object for RocketMQ configuration.

LANGUAGE: java
CODE:
TupleToMessageMapper mapper = new FieldNameBasedTupleToMessageMapper("word", "count");
TopicSelector selector = new DefaultTopicSelector(topic);

properties = new Properties();
properties.setProperty(RocketMqConfig.NAME_SERVER_ADDR, nameserverAddr);

RocketMqBolt insertBolt = new RocketMqBolt()
        .withMapper(mapper)
        .withSelector(selector)
        .withProperties(properties);

----------------------------------------

TITLE: Implementing Trident Filter
DESCRIPTION: Shows how to implement a Trident filter that selectively keeps or filters out tuples based on specific conditions.

LANGUAGE: java
CODE:
public class MyFilter extends BaseFilter {
    public boolean isKeep(TridentTuple tuple) {
        return tuple.getInteger(0) == 1 && tuple.getInteger(1) == 2;
    }
}

----------------------------------------

TITLE: Defining KafkaTopicSelector Interface in Java
DESCRIPTION: Interface definition for selecting the Kafka topic to publish a tuple to.

LANGUAGE: java
CODE:
public interface KafkaTopicSelector {
    String getTopics(Tuple/TridentTuple tuple);
}

----------------------------------------

TITLE: Configuring Resource Aware Scheduler in Storm YAML
DESCRIPTION: Basic configuration to enable the Resource Aware Scheduler in Storm's configuration file.

LANGUAGE: yaml
CODE:
storm.scheduler: "org.apache.storm.scheduler.resource.ResourceAwareScheduler"

----------------------------------------

TITLE: Redis Lookup Mapper Implementation
DESCRIPTION: Implementation of RedisLookupMapper interface for word count lookup operations using Redis hash data type.

LANGUAGE: java
CODE:
class WordCountRedisLookupMapper implements RedisLookupMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountRedisLookupMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public List<Values> toTuple(ITuple input, Object value) {
        String member = getKeyFromTuple(input);
        List<Values> values = Lists.newArrayList();
        values.add(new Values(member, value));
        return values;
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("wordName", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Defining Thrift Structures for Cluster and Nimbus Information in Storm
DESCRIPTION: Thrift struct definitions for ClusterSummary and NimbusSummary, used to represent cluster state including Nimbus instances in Storm's API responses.

LANGUAGE: thrift
CODE:
struct ClusterSummary {
  1: required list<SupervisorSummary> supervisors;
  3: required list<TopologySummary> topologies;
  4: required list<NimbusSummary> nimbuses;
}

struct NimbusSummary {
  1: required string host;
  2: required i32 port;
  3: required i32 uptime_secs;
  4: required bool isLeader;
  5: required string version;
}

----------------------------------------

TITLE: Creating Flux-Enabled Storm JAR
DESCRIPTION: Maven configuration to build a fat JAR containing Storm topology code and Flux dependencies using the Maven shade plugin.

LANGUAGE: xml
CODE:
<!-- include Flux and user dependencies in the shaded jar -->
<dependencies>
    <!-- Flux include -->
    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>flux-core</artifactId>
        <version>${storm.version}</version>
    </dependency>
    <!-- Flux Wrappers include -->
    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>flux-wrappers</artifactId>
        <version>${storm.version}</version>
    </dependency>

    <!-- add user dependencies here... -->

</dependencies>
<!-- create a fat jar that includes all dependencies -->
<build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-shade-plugin</artifactId>
            <version>1.4</version>
            <configuration>
                <createDependencyReducedPom>true</createDependencyReducedPom>
            </configuration>
            <executions>
                <execution>
                    <phase>package</phase>
                    <goals>
                        <goal>shade</goal>
                    </goals>
                    <configuration>
                        <transformers>
                            <transformer
                                    implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                            <transformer
                                    implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                <mainClass>org.apache.storm.flux.Flux</mainClass>
                            </transformer>
                        </transformers>
                    </configuration>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>

----------------------------------------

TITLE: Configuring Health Check Directory in YAML
DESCRIPTION: This snippet shows how to set the directory for health check scripts in Storm. These scripts are used to monitor the health of supervisor nodes.

LANGUAGE: yaml
CODE:
storm.health.check.dir: "healthchecks"

----------------------------------------

TITLE: Building the Storm JMS Example Topology
DESCRIPTION: Maven commands to build the Storm JMS example topology. This builds the project and installs it in the local Maven repository.

LANGUAGE: bash
CODE:
$ cd storm-jms
$ mvn clean install

----------------------------------------

TITLE: Parsing Fake Apache Logs to JSON
DESCRIPTION: This Python script parses fake Apache logs generated by Fake-Apache-Log-Generator and converts them to JSON format with an auto-incrementing ID.

LANGUAGE: python
CODE:
import sys
import apache_log_parser
import json

auto_incr_id = 1
parser_format = '%a - - %t %D "%r" %s %b "%{Referer}i" "%{User-Agent}i"'
line_parser = apache_log_parser.make_parser(parser_format)
while True:
  # we'll use pipe
  line = sys.stdin.readline()
  if not line:
    break
  parsed_dict = line_parser(line)
  parsed_dict['id'] = auto_incr_id
  auto_incr_id += 1

  # works only python 2, but I don't care cause it's just a test module :)
  parsed_dict = {k.upper(): v for k, v in parsed_dict.iteritems() if not k.endswith('datetimeobj')}
  print json.dumps(parsed_dict)

----------------------------------------

TITLE: Creating Kerberos Principals and Keytabs in Bash
DESCRIPTION: Bash commands for creating Kerberos principals and keytabs for Zookeeper, Nimbus, DRPC, UI, and Supervisors.

LANGUAGE: bash
CODE:
# Zookeeper (Will need one of these for each box in teh Zk ensamble)
sudo kadmin.local -q 'addprinc zookeeper/zk1.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/zk.keytab  zookeeper/zk1.example.com@STORM.EXAMPLE.COM"
# Nimbus and DRPC
sudo kadmin.local -q 'addprinc storm/storm.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm/storm.example.com@STORM.EXAMPLE.COM"
# All UI logviewer and Supervisors
sudo kadmin.local -q 'addprinc storm@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm@STORM.EXAMPLE.COM"

----------------------------------------

TITLE: Dequeuing and Removing Items from Kestrel Queue in Java
DESCRIPTION: Method to dequeue items from a Kestrel queue and remove them. It dequeues up to 12 items, acknowledges each item to remove it from the queue, and prints the received values.

LANGUAGE: java
CODE:
private static void dequeueAndRemoveItems(KestrelClient kestrelClient, String queueName)
    throws IOException, ParseError
		 {
			for(int i=1; i<=12; i++){

				Item item = kestrelClient.dequeue(queueName);


				if(item==null){
					System.out.println("The queue (" + queueName + ") contains no items.");
				}
				else
				{
					int itemID = item._id;


					byte[] data = item._data;

					String receivedVal = new String(data);

					kestrelClient.ack(queueName, itemID);

					System.out.println("receivedItem=" + receivedVal);
				}
			}
	}

----------------------------------------

TITLE: Defining JDBC Mapper Interface in Java
DESCRIPTION: Interface for mapping Storm tuples to database columns. Used for inserting data into database tables.

LANGUAGE: java
CODE:
public interface JdbcMapper  extends Serializable {
    List<Column> getColumns(ITuple tuple);
}

----------------------------------------

TITLE: Thread-safe Tuple Serialization
DESCRIPTION: Java implementation of thread-safe tuple serialization using Kryo.

LANGUAGE: java
CODE:
https://github.com/apache/storm/blob/0.7.1/src/jvm/org/apache/storm/serialization/KryoTupleSerializer.java#L26

----------------------------------------

TITLE: Python Syntax Highlighting Classes
DESCRIPTION: Defines syntax highlighting classes for Python code elements including keywords, built-ins, numbers, strings, comments, decorators, functions, and classes.



----------------------------------------

TITLE: jQuery UI MIT License Text
DESCRIPTION: The complete license text for jQuery UI v1.12.1, including MIT license terms, CC0 waiver for sample code, and notes about external library licenses

LANGUAGE: text
CODE:
Copyright jQuery Foundation and other contributors, https://jquery.org/

This software consists of voluntary contributions made by many
individuals. For exact contribution history, see the revision history
available at https://github.com/jquery/jquery-ui

The following license applies to all parts of this software except as
documented below:

====

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

====

Copyright and related rights for sample code are waived via CC0. Sample
code is defined as all source code contained within the demos directory.

CC0: http://creativecommons.org/publicdomain/zero/1.0/

====

All files located in the node_modules and external directories are
externally maintained libraries used by this software which have their
own licenses; we recommend you read them, as their terms may differ from
the terms above.

----------------------------------------

TITLE: Virtual Port Implementation for Message Routing in Apache Storm (Clojure)
DESCRIPTION: This code implements the virtual port concept in Storm, which receives [task id, message] pairs and routes them to the actual tasks.

LANGUAGE: Clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/zilch/virtual_port.clj

----------------------------------------

TITLE: Configuring MongoInsertBolt for Storm-MongoDB Integration in Java
DESCRIPTION: Demonstrates how to configure and create a MongoInsertBolt, which is used to insert data from Storm tuples into MongoDB collections. It requires a MongoDB URI, collection name, and a MongoMapper implementation.

LANGUAGE: java
CODE:
String url = "mongodb://127.0.0.1:27017/test";
String collectionName = "wordcount";

MongoMapper mapper = new SimpleMongoMapper()
        .withFields("word", "count");

MongoInsertBolt insertBolt = new MongoInsertBolt(url, collectionName, mapper);

----------------------------------------

TITLE: Running Redis State Migration Tool
DESCRIPTION: Command to run the Redis state migration tool, which migrates state from Base64-encoded strings to binary format for improved performance. This is needed when upgrading from Storm 1.1.0 or earlier.

LANGUAGE: bash
CODE:
<storm-installation-dir>/bin/storm jar target/storm-redis-examples-*.jar org.apache.storm.redis.tools.Base64ToBinaryStateMigrationUtil [options]

----------------------------------------

TITLE: Implementing DRPC Topology with LinearDRPCTopologyBuilder in Java
DESCRIPTION: Shows how to create a simple DRPC topology that appends '!' to the input argument using LinearDRPCTopologyBuilder.

LANGUAGE: java
CODE:
public static class ExclaimBolt extends BaseBasicBolt {
    public void execute(Tuple tuple, BasicOutputCollector collector) {
        String input = tuple.getString(1);
        collector.emit(new Values(tuple.getValue(0), input + "!"));
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "result"));
    }
}

public static void main(String[] args) throws Exception {
    LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("exclamation");
    builder.addBolt(new ExclaimBolt(), 3);
    // ...
}

----------------------------------------

TITLE: Configuring NUMA Settings in Storm Supervisor
DESCRIPTION: YAML configuration for setting up NUMA zones in Storm supervisor, including core assignments, memory allocation, ports, and generic resources mapping.

LANGUAGE: yaml
CODE:
supervisor.numa.meta:
    "0": # Numa zone id
        numa.cores: # Cores in NUMA zone (can be determined by using the numastat command)
            - 0
            - 1
            - 2
            - 3
            - 4
            - 5
            - 12
            - 13
            - 14
            - 15
            - 16

        numa.generic.resources.map: # Generic Resources in the zone to be used for generic resource scheduling (optional)
            network.resource.units: 50.0

        numa.memory.mb: 42461 # Size of memory zone
        numa.ports: # Ports to be assigned to workers pinned to the NUMA zone (this may include ports not specified in SUPERVISOR_SLOTS_PORTS
            - 6700
            - 6701
            - 6702
            - 6703
            - 6704
            - 6705
            - 6706
            - 6707
            - 6708
            - 6709
            - 6710
            - 6711

----------------------------------------

TITLE: Configuring MongoInsertBolt for Storm-MongoDB Integration in Java
DESCRIPTION: Demonstrates how to configure and create a MongoInsertBolt, which is used to insert data from Storm tuples into MongoDB collections. It requires a MongoDB URI, collection name, and a MongoMapper implementation.

LANGUAGE: java
CODE:
String url = "mongodb://127.0.0.1:27017/test";
String collectionName = "wordcount";

MongoMapper mapper = new SimpleMongoMapper()
        .withFields("word", "count");

MongoInsertBolt insertBolt = new MongoInsertBolt(url, collectionName, mapper);

----------------------------------------

TITLE: Bolt Message Listening in Apache Storm (Clojure)
DESCRIPTION: This code implements the message listening functionality for bolts in Apache Storm.

LANGUAGE: clojure
CODE:
(github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L489)

----------------------------------------

TITLE: GetTime2 UDF Implementation
DESCRIPTION: Java implementation of a User Defined Function to convert date strings to Unix timestamps.

LANGUAGE: java
CODE:
package org.apache.storm.sql.runtime.functions.scalar.datetime;

import org.joda.time.format.DateTimeFormat;
import org.joda.time.format.DateTimeFormatter;

public class GetTime2 {
    public static Long evaluate(String dateString, String dateFormat) {
        try {
            DateTimeFormatter df = DateTimeFormat.forPattern(dateFormat).withZoneUTC();
            return df.parseDateTime(dateString).getMillis();
        } catch (Exception ex) {
            throw new RuntimeException(ex);
        }
    }
}

----------------------------------------

TITLE: Launching OCI Container with runc
DESCRIPTION: Command used by worker-launcher to start the OCI container using runc.

LANGUAGE: bash
CODE:
/usr/bin/runc run -d \
              --pid-file /home/y/var/storm/workers/1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb/artifacts/container-1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb.pid \
              -b /home/y/var/storm/workers/1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb \
              6703-1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb

----------------------------------------

TITLE: Managing Storm Heartbeats
DESCRIPTION: Lists or retrieves heartbeat data from the ClusterState.

LANGUAGE: shell
CODE:
storm heartbeats [cmd]

----------------------------------------

TITLE: Displaying Cluster Metrics Table in Markdown
DESCRIPTION: This code snippet shows how to create a table in Markdown format to display cluster metrics. It includes columns for Metric Name, Type, and Description.

LANGUAGE: markdown
CODE:
| Metric Name | Type | Description |
|-------------|------|-------------|
| cluster:num-nimbus-leaders | gauge | Number of nimbuses marked as a leader. This should really only ever be 1 in a healthy cluster, or 0 for a short period of time while a failover happens. |
| cluster:num-nimbuses | gauge | Number of nimbuses, leader or standby. |
| cluster:num-supervisors | gauge | Number of supervisors. |
| cluster:num-topologies | gauge | Number of topologies. |
| cluster:num-total-used-workers | gauge | Number of used workers/slots. |
| cluster:num-total-workers | gauge | Number of workers/slots. |

----------------------------------------

TITLE: Defining a Parameterized Spout in Clojure
DESCRIPTION: Example of defining a parameterized spout that emits random sentences from a provided list using the defspout macro.

LANGUAGE: clojure
CODE:
(defspout sentence-spout-parameterized ["word"] {:params [sentences] :prepare false}
  [collector]
  (Thread/sleep 500)
  (emit-spout! collector [(rand-nth sentences)]))

----------------------------------------

TITLE: Configuring Redis Cluster State Provider in Storm YAML
DESCRIPTION: JSON configuration for the Redis Cluster state provider, specifying cluster nodes and connection details. This configuration is set in the storm.yaml file or programmatically.

LANGUAGE: json
CODE:
{
  "keyClass": "Optional fully qualified class name of the Key type.",
  "valueClass": "Optional fully qualified class name of the Value type.",
  "keySerializerClass": "Optional Key serializer implementation class.",
  "valueSerializerClass": "Optional Value Serializer implementation class.",
  "jedisClusterConfig": {
    "nodes": ["localhost:7379", "localhost:7380", "localhost:7381"],
    "timeout": 2000,
    "maxRedirections": 5
  }
}

----------------------------------------

TITLE: Dequeuing and Removing Items from Kestrel Queue in Java
DESCRIPTION: This method dequeues items from a Kestrel queue and then removes them using the ack() method. It demonstrates the complete process of retrieving and acknowledging items in Kestrel.

LANGUAGE: java
CODE:
    private static void dequeueAndRemoveItems(KestrelClient kestrelClient, String queueName)
    throws IOException, ParseError
		 {
			for(int i=1; i<=12; i++){

				Item item = kestrelClient.dequeue(queueName);


				if(item==null){
					System.out.println("The queue (" + queueName + ") contains no items.");
				}
				else
				{
					int itemID = item._id;


					byte[] data = item._data;

					String receivedVal = new String(data);

					kestrelClient.ack(queueName, itemID);

					System.out.println("receivedItem=" + receivedVal);
				}
			}
	}

----------------------------------------

TITLE: Launching Docker Container for Storm Worker
DESCRIPTION: Example Docker run command executed by worker-launcher to start a Storm worker container with security constraints, resource limits, and mounted volumes.

LANGUAGE: bash
CODE:
run --name=8198e1f0-f323-4b9d-8625-e4fd640cd058 \
--user=<uid>:<gid> \
-d \
--net=host \
--read-only \
-v /sys/fs/cgroup:/sys/fs/cgroup:ro \
-v /usr/share/apache-storm-2.3.0:/usr/share/apache-storm-2.3.0:ro \
-v /<storm-local-dir>/supervisor:/<storm-local-dir>/supervisor:ro \
-v /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058:/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
-v /<workers-artifacts-dir>/workers-artifacts/word-count-1-1591895933/6703:/<workers-artifacts-dir>/workers-artifacts/word-count-1-1591895933/6703 \
-v /<storm-local-dir>/workers-users/8198e1f0-f323-4b9d-8625-e4fd640cd058:/<storm-local-dir>/workers-users/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
-v /var/run/nscd:/var/run/nscd \
-v /<storm-local-dir>/supervisor/stormdist/word-count-1-1591895933/shared_by_topology:/<storm-local-dir>/supervisor/stormdist/word-count-1-1591895933/shared_by_topology \
-v /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/tmp:/tmp \
-v /etc/storm:/etc/storm:ro \
--cgroup-parent=/storm \
--group-add <gid> \
--workdir=/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
--cidfile=/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/container.cid \
--cap-drop=ALL \
--security-opt no-new-privileges \
--security-opt seccomp=/usr/share/apache-storm-2.3.0/conf/seccomp.json \
--cpus=2.6 xxx.xxx.com:8080/storm/storm/rhel7:latest \
bash /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/storm-worker-script.sh

----------------------------------------

TITLE: Adding Storm-Redis Maven Dependency
DESCRIPTION: Maven dependency configuration for including storm-redis in a project.

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>storm-redis</artifactId>
    <version>${storm.version}</version>
    <type>jar</type>
</dependency>

----------------------------------------

TITLE: Registering Metrics Consumer in Java
DESCRIPTION: Java code to register metrics consumers for profiling resource usage.

LANGUAGE: java
CODE:
conf.registerMetricsConsumer(backtype.storm.metric.LoggingMetricsConsumer.class);

Map<String, String> workerMetrics = new HashMap<String, String>();
workerMetrics.put("CPU", "org.apache.storm.metrics.sigar.CPUMetric");
conf.put(Config.TOPOLOGY_WORKER_METRICS, workerMetrics);

----------------------------------------

TITLE: Creating DRPC Client in Java
DESCRIPTION: Shows how to create a DRPC client in Java for production use, demonstrating proper resource management with try-with-resources.

LANGUAGE: java
CODE:
Config conf = new Config();
try (DRPCClient drpc = DRPCClient.getConfiguredClient(conf)) {
  //User the drpc client
  String result = drpc.execute(function, argument);
}

----------------------------------------

TITLE: Listing HDFS Container Resources
DESCRIPTION: Example HDFS commands to list the converted container resources stored in HDFS.

LANGUAGE: bash
CODE:
-bash-4.2$ hdfs dfs -ls /containers/*
Found 1 items
-r--r--r--   3 hdfsqa hadoop       7877 2020-12-04 14:29 /containers/config/ef1ff2c7167a1a6cd01e106f51b84a4d400611ba971c53cbc28de7919515ca4e
-r--r--r--   3 hdfsqa hadoop        160 2020-12-04 14:30 /containers/image-tag-to-hash
Found 7 items
-r--r--r--   3 hdfsqa hadoop   84697088 2020-12-04 14:28 /containers/layers/152ee1d2cccea9dfe6393d2bdf9d077b67616b2b417b25eb74fc5ffaadcb96f5.sqsh
-r--r--r--   3 hdfsqa hadoop  545267712 2020-12-04 14:28 /containers/layers/18ee671016a1bf3ecab07395d93c2cbecd352d59c497a1551e2074d64e1098d9.sqsh
-r--r--r--   3 hdfsqa hadoop   12906496 2020-10-06 15:24 /containers/layers/1b73e9433ecca0a6bb152bd7525f2b7c233484d51c24f8a6ba483d5cfd3035dc.sqsh
-r--r--r--   3 hdfsqa hadoop       4096 2020-12-04 14:29 /containers/layers/344224962010c03c9ca1f11a9bff0dfcc296ac46d0a55e4ff30a0ad13b9817af.sqsh
-r--r--r--   3 hdfsqa hadoop   26091520 2020-10-06 15:22 /containers/layers/3692c3483ef6516fba685b316448e8aaf0fc10bb66818116edc8e5e6800076c7.sqsh
-r--r--r--   3 hdfsqa hadoop       4096 2020-12-04 14:29 /containers/layers/8710a3d72f75b45c48ab6b9b67eb6d77caea3dac91a0c30e0831f591cba4887e.sqsh
-r--r--r--   3 hdfsqa hadoop  121122816 2020-10-06 15:23 /containers/layers/ea067172a7138f035d89a5c378db6d66c1581d98b0497b21f256e04c3d2b5303.sqsh
Found 1 items
-r--r--r--   3 hdfsqa hadoop       1793 2020-12-04 14:29 /containers/manifests/26fd443859325d5911f3be5c5e231dddca88ee0d526456c0c92dd794148d8585

----------------------------------------

TITLE: Storm Log Command JSON Format
DESCRIPTION: JSON structure for logging messages from a multi-language bolt to the worker log.

LANGUAGE: json
CODE:
{
    "command": "log",
    "msg": "hello world!"
}

----------------------------------------

TITLE: Retrieving Topology History in Storm UI REST API
DESCRIPTION: Example JSON response for the /api/v1/history/summary endpoint, which returns a list of all running topologies' IDs submitted by the current user.

LANGUAGE: json
CODE:
{
    "topo-history":[
        "wc6-1-1446571009",
        "wc8-2-1446587178"
     ]
}

----------------------------------------

TITLE: Spout Emit JSON Command Structure
DESCRIPTION: JSON structure for emitting tuples from a spout, including optional fields for stream ID, task ID and tuple values.

LANGUAGE: json
CODE:
{
	"command": "emit",
	"id": "1231231",
	"stream": "1",
	"task": 9,
	"tuple": ["field1", 2, 3]
}

----------------------------------------

TITLE: Implementing a Bolt in Java
DESCRIPTION: Full implementation of a Java bolt that appends '!!!' to input tuples.

LANGUAGE: java
CODE:
public static class ExclamationBolt implements IRichBolt {
    OutputCollector _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple tuple) {
        _collector.emit(tuple, new Values(tuple.getString(0) + "!!!"));
        _collector.ack(tuple);
    }

    @Override
    public void cleanup() {
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word"));
    }
    
    @Override
    public Map<String, Object> getComponentConfiguration() {
        return null;
    }
}

----------------------------------------

TITLE: Adding Generic Resources to Storm Components in Java
DESCRIPTION: This snippet demonstrates how to add generic resource requirements to a Storm topology component using the addResource method. It specifies the resource name and value for a spout.

LANGUAGE: java
CODE:
public T addResource(String resourceName, Number resourceValue)

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.addResouce("gpu.count", 1.0);

----------------------------------------

TITLE: Using Trident API for Querying Data from Cassandra in Java
DESCRIPTION: Demonstrates how to use the Trident API to query data from Cassandra using the CassandraState and CassandraStateFactory classes.

LANGUAGE: java
CODE:
CassandraState.Options options = new CassandraState.Options(new CassandraContext());
CQLStatementTupleMapper insertTemperatureValues = boundQuery("SELECT name FROM weather.station WHERE id = ?")
         .bind(with(field("weather_station_id").as("id")));
options.withCQLStatementTupleMapper(insertTemperatureValues);
options.withCQLResultSetValuesMapper(new TridentResultSetValuesMapper(new Fields("name")));
CassandraStateFactory selectWeatherStationStateFactory =  new CassandraStateFactory(options);
CassandraStateFactory selectWeatherStationStateFactory = getSelectWeatherStationStateFactory();
TridentState selectState = topology.newStaticState(selectWeatherStationStateFactory);
stream = stream.stateQuery(selectState, new Fields("weather_station_id"), new CassandraQuery(), new Fields("name"));

----------------------------------------

TITLE: Defining External Table for Kafka Data Source
DESCRIPTION: Example of creating an external table that specifies a Kafka spout and sink using the CREATE EXTERNAL TABLE statement.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-hosts=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Using Storm Shell Command for Topology Submission
DESCRIPTION: This command demonstrates how to use the 'storm shell' command to package resources, upload them to Nimbus, and execute a Python topology script with arguments.

LANGUAGE: bash
CODE:
storm shell resources/ python topology.py arg1 arg2

----------------------------------------

TITLE: Setting Component Memory and CPU Requirements
DESCRIPTION: Example code showing how to set memory and CPU requirements for Storm topology components.

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.setMemoryLoad(1024.0, 512.0);
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
            .shuffleGrouping("word").setMemoryLoad(512.0);

----------------------------------------

TITLE: Setting Component Memory and CPU Requirements
DESCRIPTION: Example code showing how to set memory and CPU requirements for Storm topology components.

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.setMemoryLoad(1024.0, 512.0);
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
            .shuffleGrouping("word").setMemoryLoad(512.0);

----------------------------------------

TITLE: Python Log Parser Script
DESCRIPTION: Script to parse Apache logs into JSON format with auto-incrementing IDs

LANGUAGE: python
CODE:
import sys
import apache_log_parser
import json

auto_incr_id = 1
parser_format = '%a - - %t %D "%r" %s %b "%{Referer}i" "%{User-Agent}i"'
line_parser = apache_log_parser.make_parser(parser_format)
while True:
  line = sys.stdin.readline()
  if not line:
    break
  parsed_dict = line_parser(line)
  parsed_dict['id'] = auto_incr_id
  auto_incr_id += 1

  parsed_dict = {k.upper(): v for k, v in parsed_dict.iteritems() if not k.endswith('datetimeobj')}
  print json.dumps(parsed_dict)

----------------------------------------

TITLE: Running the Storm JMS Example Topology Locally
DESCRIPTION: Maven command to execute the example topology locally. This runs the topology, which will connect to ActiveMQ and create the necessary JMS destinations.

LANGUAGE: bash
CODE:
$ mvn exec:java

----------------------------------------

TITLE: Debugging Kryo ConcurrentModificationException in Java
DESCRIPTION: This stack trace demonstrates a common error that occurs when emitting mutable objects as output tuples in Storm. The error is caused by modifying an object while it's being serialized for network transmission.

LANGUAGE: java
CODE:
java.lang.RuntimeException: java.util.ConcurrentModificationException
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:84)
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:55)
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:56)
	at org.apache.storm.disruptor$consume_loop_STAR_$fn__1597.invoke(disruptor.clj:67)
	at org.apache.storm.util$async_loop$fn__465.invoke(util.clj:377)
	at clojure.lang.AFn.run(AFn.java:24)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.util.ConcurrentModificationException
	at java.util.LinkedHashMap$LinkedHashIterator.nextEntry(LinkedHashMap.java:390)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:409)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:408)
	at java.util.HashMap.writeObject(HashMap.java:1016)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:959)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1480)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at org.apache.storm.serialization.SerializableSerializer.write(SerializableSerializer.java:21)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:554)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:77)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:472)
	at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:27)

----------------------------------------

TITLE: Configuring Childopts for Nimbus, UI, and Supervisor in YAML
DESCRIPTION: Example YAML configuration for setting Java system properties in the child process options for Nimbus, UI, and Supervisor components.

LANGUAGE: yaml
CODE:
nimbus.childopts: "-Xmx1024m -Djava.security.auth.login.config=/path/to/jaas.conf"
ui.childopts: "-Xmx768m -Djava.security.auth.login.config=/path/to/jaas.conf"
supervisor.childopts: "-Xmx256m -Djava.security.auth.login.config=/path/to/jaas.conf"

----------------------------------------

TITLE: Retrieving Cluster Configuration in JSON
DESCRIPTION: Sample response from the /api/v1/cluster/configuration endpoint, showing a subset of the cluster configuration parameters.

LANGUAGE: json
CODE:
{
  "dev.zookeeper.path": "/tmp/dev-storm-zookeeper",
  "topology.tick.tuple.freq.secs": null,
  "topology.builtin.metrics.bucket.size.secs": 60,
  "topology.fall.back.on.java.serialization": false,
  "topology.max.error.report.per.interval": 5,
  "zmq.linger.millis": 5000,
  "topology.skip.missing.kryo.registrations": false,
  "storm.messaging.netty.client_worker_threads": 1,
  "ui.childopts": "-Xmx768m",
  "storm.zookeeper.session.timeout": 20000,
  "nimbus.reassign": true,
  "topology.trident.batch.emit.interval.millis": 500,
  "storm.messaging.netty.flush.check.interval.ms": 10,
  "nimbus.monitor.freq.secs": 10,
  "logviewer.childopts": "-Xmx128m",
  "java.library.path": "/usr/local/lib:/opt/local/lib:/usr/lib",
  "topology.executor.send.buffer.size": 1024
}

----------------------------------------

TITLE: Converting Docker Images to Squashfs with HDFS Storage
DESCRIPTION: Example command to pull Docker images, convert layers to squashfs, and push to HDFS using the docker-to-squash.py script.

LANGUAGE: bash
CODE:
python docker-to-squash.py pull-build-push-update --hdfs-root hdfs://hostname:port/containers \
                      docker.xxx.com:4443/hadoop-user-images/storm/rhel7:20201202-232133,storm/rhel7:dev_current --log DEBUG --bootstrap

----------------------------------------

TITLE: Defining a Transactional Topology in Storm
DESCRIPTION: Example of using TransactionalTopologyBuilder to create a topology that computes a global count of tuples from an input stream. It demonstrates setting up the spout and bolts with appropriate groupings.

LANGUAGE: java
CODE:
MemoryTransactionalSpout spout = new MemoryTransactionalSpout(DATA, new Fields("word"), PARTITION_TAKE_PER_BATCH);
TransactionalTopologyBuilder builder = new TransactionalTopologyBuilder("global-count", "spout", spout, 3);
builder.setBolt("partial-count", new BatchCount(), 5)
        .shuffleGrouping("spout");
builder.setBolt("sum", new UpdateGlobalCount())
        .globalGrouping("partial-count");

----------------------------------------

TITLE: Defining TupleToKafkaMapper and KafkaTopicSelector Interfaces for Kafka Integration
DESCRIPTION: Interfaces for mapping Storm tuples to Kafka messages and selecting Kafka topics. The TupleToKafkaMapper interface defines methods for getting the key and message from a tuple. The KafkaTopicSelector interface defines a method for getting the topic to publish to based on a tuple.

LANGUAGE: java
CODE:
K getKeyFromTuple(Tuple/TridentTuple tuple);
V getMessageFromTuple(Tuple/TridentTuple tuple);

LANGUAGE: java
CODE:
public interface KafkaTopicSelector {
    String getTopics(Tuple/TridentTuple tuple);
}

----------------------------------------

TITLE: Configuring Mongo Trident State for Storm-MongoDB Integration
DESCRIPTION: Example of how to configure and use Mongo Trident State in a Trident topology for persistent state management.

LANGUAGE: java
CODE:
MongoMapper mapper = new SimpleMongoMapper()
        .withFields("word", "count");

MongoState.Options options = new MongoState.Options()
        .withUrl(url)
        .withCollectionName(collectionName)
        .withMapper(mapper);

StateFactory factory = new MongoStateFactory(options);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory, fields,
        new MongoStateUpdater(), new Fields());

TridentState state = topology.newStaticState(factory);
stream = stream.stateQuery(state, new Fields("word"),
        new MongoStateQuery(), new Fields("columnName", "columnValue"));
stream.each(new Fields("word", "columnValue"), new PrintFunction(), new Fields());

----------------------------------------

TITLE: Running the Storm JMS Example Topology Locally
DESCRIPTION: Maven command to execute the example topology locally. This runs the topology and connects it to ActiveMQ.

LANGUAGE: bash
CODE:
$ mvn exec:java

----------------------------------------

TITLE: Named Streams Join Configuration
DESCRIPTION: Example showing how to configure JoinBolt to use named streams instead of component names

LANGUAGE: java
CODE:
new JoinBolt(JoinBolt.Selector.STREAM,  "stream1", "key1")
                             .join     ("stream2", "userId",  "stream1" )
                             .select ("userId, key1, key2")
                             .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;
                             
topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("bolt1", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt2", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt3", "stream2", new Fields("userId") )
            .fieldsGrouping("bolt4", "stream1", new Fields("key1") );

----------------------------------------

TITLE: Spout Tuple Transfer in Apache Storm (Clojure)
DESCRIPTION: Implementation of tuple transfer for spouts using the worker-provided transfer function.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L329

----------------------------------------

TITLE: Including Flux Dependencies with Maven
DESCRIPTION: Maven configuration to include Flux and create a fat JAR with all dependencies.

LANGUAGE: xml
CODE:
<dependencies>
    <!-- Flux include -->
    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>flux-core</artifactId>
        <version>${storm.version}</version>
    </dependency>
    <!-- Flux Wrappers include -->
    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>flux-wrappers</artifactId>
        <version>${storm.version}</version>
    </dependency>

    <!-- add user dependencies here... -->

</dependencies>
<!-- create a fat jar that includes all dependencies -->
<build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-shade-plugin</artifactId>
            <version>1.4</version>
            <configuration>
                <createDependencyReducedPom>true</createDependencyReducedPom>
            </configuration>
            <executions>
                <execution>
                    <phase>package</phase>
                    <goals>
                        <goal>shade</goal>
                    </goals>
                    <configuration>
                        <transformers>
                            <transformer
                                    implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                            <transformer
                                    implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                <mainClass>org.apache.storm.flux.Flux</mainClass>
                            </transformer>
                        </transformers>
                    </configuration>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>

----------------------------------------

TITLE: JDBC Mapper Interface Definition
DESCRIPTION: Interface that defines how Storm tuples map to database columns. The order of returned columns is significant for query placeholder resolution.

LANGUAGE: java
CODE:
public interface JdbcMapper  extends Serializable {
    List<Column> getColumns(ITuple tuple);
}

----------------------------------------

TITLE: Storm Task Message Routing
DESCRIPTION: Implementation of message routing and task management for both bolts and spouts

LANGUAGE: clojure
CODE:
daemon/task.clj

----------------------------------------

TITLE: Storm Topology Submission Method Definition
DESCRIPTION: Thrift API method definition for submitting a Storm topology. Takes topology name, jar location, JSON configuration, and topology structure as parameters. Can throw AlreadyAliveException and InvalidTopologyException.

LANGUAGE: java
CODE:
void submitTopology(1: string name, 2: string uploadedJarLocation, 3: string jsonConf, 4: StormTopology topology) throws (1: AlreadyAliveException e, 2: InvalidTopologyException ite);

----------------------------------------

TITLE: Implementing IRichBolt Interface in Java
DESCRIPTION: Example of the IRichBolt interface in Java, which is the main interface for implementing bolts in Storm. It includes methods for processing tuples, declaring output fields, and lifecycle management.

LANGUAGE: Java
CODE:
public interface IRichBolt extends IBolt, IComponent {
  void prepare(Map stormConf, TopologyContext context, OutputCollector collector);
  void execute(Tuple input);
  void cleanup();
}

----------------------------------------

TITLE: Applying a Map Function to a Trident Stream
DESCRIPTION: Example of applying a custom MapFunction to a Trident stream.

LANGUAGE: java
CODE:
mystream.map(new UpperCase())

----------------------------------------

TITLE: Defining a Bolt in Java
DESCRIPTION: Implementation of a simple bolt that appends '!!!' to input tuples.

LANGUAGE: java
CODE:
public class DoubleAndTripleBolt extends BaseRichBolt {
    private OutputCollectorBase _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollectorBase collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple input) {
        int val = input.getInteger(0);        
        _collector.emit(input, new Values(val*2, val*3));
        _collector.ack(input);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("double", "triple"));
    }    
}

----------------------------------------

TITLE: Implementing QueryFunction for State Querying in Trident
DESCRIPTION: Example of implementing a QueryFunction (QueryLocation) to query a custom State object (LocationDB) in a Trident topology.

LANGUAGE: java
CODE:
public class QueryLocation extends BaseQueryFunction<LocationDB, String> {
    public List<String> batchRetrieve(LocationDB state, List<TridentTuple> inputs) {
        List<String> ret = new ArrayList();
        for(TridentTuple input: inputs) {
            ret.add(state.getLocation(input.getLong(0)));
        }
        return ret;
    }

    public void execute(TridentTuple tuple, String location, TridentCollector collector) {
        collector.emit(new Values(location));
    }    
}

----------------------------------------

TITLE: Configuring Spouts in Flux YAML
DESCRIPTION: Example of configuring a Kafka spout using Flux's YAML configuration.

LANGUAGE: yaml
CODE:
spouts:
  - id: "kafka-spout"
    className: "org.apache.storm.kafka.KafkaSpout"
    constructorArgs:
      - ref: "spoutConfig"

----------------------------------------

TITLE: Dequeueing and Removing Items from Kestrel Queue in Java
DESCRIPTION: Method to dequeue items from a Kestrel queue and remove them. It uses KestrelClient to fetch items, print their contents, and then acknowledge them to remove from the queue.

LANGUAGE: java
CODE:
private static void dequeueAndRemoveItems(KestrelClient kestrelClient, String queueName)
    throws IOException, ParseError
		 {
			for(int i=1; i<=12; i++){

				Item item = kestrelClient.dequeue(queueName);


				if(item==null){
					System.out.println("The queue (" + queueName + ") contains no items.");
				}
				else
				{
					int itemID = item._id;


					byte[] data = item._data;

					String receivedVal = new String(data);

					kestrelClient.ack(queueName, itemID);

					System.out.println("receivedItem=" + receivedVal);
				}
			}
	}

----------------------------------------

TITLE: Configuring Storm Bolt with JSON Mapper
DESCRIPTION: Example of setting up a SolrUpdateBolt with JSON mapping and count-based commit strategy. Uses a SolrConfig pointing to local Solr instance and configures JSON mapping for the 'gettingstarted' collection.

LANGUAGE: java
CODE:
    new SolrUpdateBolt(solrConfig, solrMapper, solrCommitStgy)
    
    // zkHostString for Solr 'gettingstarted' example
    SolrConfig solrConfig = new SolrConfig("127.0.0.1:9983");
    
    // JSON Mapper used to generate 'SolrRequest' requests to update the "gettingstarted" Solr collection with JSON content declared the tuple field with name "JSON"
    SolrMapper solrMapper = new SolrJsonMapper.Builder("gettingstarted", "JSON").build(); 
     
    // Acks every other five tuples. Setting to null acks every tuple
    SolrCommitStrategy solrCommitStgy = new CountBasedCommit(5);

----------------------------------------

TITLE: Configuring User Resource Pools in YAML
DESCRIPTION: YAML configuration to specify resource guarantees for users in the Resource Aware Scheduler.

LANGUAGE: yaml
CODE:
resource.aware.scheduler.user.pools:
    jerry:
        cpu: 1000
        memory: 8192.0
    derek:
        cpu: 10000.0
        memory: 32768
    bobby:
        cpu: 5000.0
        memory: 16384.0

----------------------------------------

TITLE: Invalid Stream Reference Example
DESCRIPTION: Example showing incorrect forward referencing of streams in JoinBolt configuration

LANGUAGE: java
CODE:
new JoinBolt( "spout1", "key1")                 
  .join     ( "spout2", "userId",  "spout3") //not allowed. spout3 not yet introduced
  .join     ( "spout3", "key3",    "spout1")

----------------------------------------

TITLE: Initializing HiveBolt with DelimitedRecordHiveMapper (Java)
DESCRIPTION: Java code snippet demonstrating how to initialize a HiveBolt using a DelimitedRecordHiveMapper and HiveOptions. This setup allows streaming tuples directly into Hive.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames));
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper);
HiveBolt hiveBolt = new HiveBolt(hiveOptions);

----------------------------------------

TITLE: Custom Time Conversion UDF Implementation
DESCRIPTION: Java class implementing a User Defined Function for converting datetime strings to Unix timestamps.

LANGUAGE: java
CODE:
package org.apache.storm.sql.runtime.functions.scalar.datetime;

import org.joda.time.format.DateTimeFormat;
import org.joda.time.format.DateTimeFormatter;

public class GetTime2 {
    public static Long evaluate(String dateString, String dateFormat) {
        try {
            DateTimeFormatter df = DateTimeFormat.forPattern(dateFormat).withZoneUTC();
            return df.parseDateTime(dateString).getMillis();
        } catch (Exception ex) {
            throw new RuntimeException(ex);
        }
    }
}

----------------------------------------

TITLE: Worker Launcher Configuration
DESCRIPTION: Configuration file template for worker-launcher showing required settings for group permissions, user IDs and profiler script path.

LANGUAGE: bash
CODE:
storm.worker-launcher.group=$(worker_launcher_group)
min.user.id=$(min_user_id)
worker.profiler.script.path=$(profiler_script_path)

----------------------------------------

TITLE: Using Storm Shell Command for Topology Submission
DESCRIPTION: This command demonstrates how to use the 'storm shell' command to package resources, upload them to Nimbus, and execute a Python topology script with arguments.

LANGUAGE: bash
CODE:
storm shell resources/ python topology.py arg1 arg2

----------------------------------------

TITLE: Configuring DRPC Servers in YAML
DESCRIPTION: This YAML configuration specifies the locations of DRPC servers, HTTP port, and Thrift transport plugin for a Storm cluster.

LANGUAGE: yaml
CODE:
drpc.servers:
  - "drpc1.foo.com"
  - "drpc2.foo.com"
drpc.http.port: 8081
storm.thrift.transport: "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin"

----------------------------------------

TITLE: Storm Fail Command JSON Format
DESCRIPTION: JSON structure for reporting tuple processing failure in Storm.

LANGUAGE: json
CODE:
{
    "command": "fail",
    "id": 123123
}

----------------------------------------

TITLE: Batch Processing Bolt Implementation in Java
DESCRIPTION: Shows implementation of a batch processing bolt for handling partial uniqueness in DRPC calculations.

LANGUAGE: java
CODE:
public class PartialUniquer extends BaseBatchBolt {
    BatchOutputCollector _collector;
    Object _id;
    Set<String> _followers = new HashSet<String>();
    
    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
        _collector = collector;
        _id = id;
    }

    @Override
    public void execute(Tuple tuple) {
        _followers.add(tuple.getString(1));
    }
    
    @Override
    public void finishBatch() {
        _collector.emit(new Values(_id, _followers.size()));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "partial-count"));
    }
}

----------------------------------------

TITLE: Scoring Method Interface for PMML ModelRunner
DESCRIPTION: Interface method definition for computing scored tuples from raw input data. Returns a map where keys are stream IDs and values are lists of predicted scores.

LANGUAGE: java
CODE:
Map<String, List<Object>> scoredTuplePerStream(Tuple input);

----------------------------------------

TITLE: Configuring CGroups for Apache Storm
DESCRIPTION: Sample cgconfig.conf file for setting up CGroups in Apache Storm. It defines the mount points for various subsystems and specifies permissions for the 'storm' group.

LANGUAGE: bash
CODE:
mount {
	cpuset	= /cgroup/cpuset;
	cpu	= /cgroup/storm_resources;
	cpuacct	= /cgroup/storm_resources;
	memory	= /cgroup/storm_resources;
	devices	= /cgroup/devices;
	freezer	= /cgroup/freezer;
	net_cls	= /cgroup/net_cls;
	blkio	= /cgroup/blkio;
}

group storm {
       perm {
               task {
                      uid = 500;
                      gid = 500;
               }
               admin {
                      uid = 500;
                      gid = 500;
               }
       }
       cpu {
       }
       memory {
       }
       cpuacct {
       }
}

----------------------------------------

TITLE: Specifying Topology Scheduling Strategy in Storm
DESCRIPTION: API call to set a custom scheduling strategy class for a Storm topology.

LANGUAGE: java
CODE:
public void setTopologyStrategy(Class<? extends IStrategy> clazz)

----------------------------------------

TITLE: File-based Config Example
DESCRIPTION: YAML configuration example for file-based config loader using local file system path.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "file:///path/to/my/config.yaml"

----------------------------------------

TITLE: Stream Value Mapping Examples
DESCRIPTION: Shows how to use value mappers to extract fields from tuples and create typed streams.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

// extract the first field from the tuple to get a Stream<String> of sentences
Stream<String> sentences = builder.newStream(new TestWordSpout(), new ValueMapper<String>(0));

// extract first three fields of the tuple emitted by the spout to produce a stream of typed tuples.
Stream<Tuple3<String, Integer, Long>> stream = builder.newStream(new TestSpout(), TupleValueMappers.of(0, 1, 2));

----------------------------------------

TITLE: Running WordCountTopology in Eclipse
DESCRIPTION: After setting up the Storm project in Eclipse, you can test it by running the WordCountTopology.java file. This will emit messages to the console for 10 seconds.

LANGUAGE: Java
CODE:
WordCountTopology.java

----------------------------------------

TITLE: Continuously Adding Items to Kestrel Queue in Java
DESCRIPTION: This program continuously adds sentence items to a Kestrel queue named 'sentence_queue' on a local Kestrel server. It runs until the user enters a closing bracket character ']' in the console.

LANGUAGE: java
CODE:
    import java.io.IOException;
    import java.io.InputStream;
    import java.util.Random;

    import org.apache.storm.spout.KestrelClient;
    import org.apache.storm.spout.KestrelClient.Item;
    import org.apache.storm.spout.KestrelClient.ParseError;

    public class AddSentenceItemsToKestrel {

    	/**
    	 * @param args
    	 */
    	public static void main(String[] args) {

    		InputStream is = System.in;

			char closing_bracket = ']';

			int val = closing_bracket;

			boolean aux = true;

			try {

				KestrelClient kestrelClient = null;
				String queueName = "sentence_queue";

				while(aux){

					kestrelClient = new KestrelClient("localhost",22133);

					queueSentenceItems(kestrelClient, queueName);

					kestrelClient.close();

					Thread.sleep(1000);

					if(is.available()>0){
					 if(val==is.read())
						 aux=false;
					}
				}
			} catch (IOException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			catch (ParseError e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			} catch (InterruptedException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}

			System.out.println("end");

	    }
	}

----------------------------------------

TITLE: Spout Definition in Clojure
DESCRIPTION: Example of defining a spout that emits random sentences. Shows basic spout implementation with nextTuple and ack methods.

LANGUAGE: clojure
CODE:
(defspout sentence-spout ["sentence"]
  [conf context collector]
  (let [sentences ["a little brown dog"
                   "the man petted the dog"
                   "four score and seven years ago"
                   "an apple a day keeps the doctor away"]]
    (spout
     (nextTuple []
       (Thread/sleep 100)
       (emit-spout! collector [(rand-nth sentences)])         
       )
     (ack [id]
        ))))

----------------------------------------

TITLE: Configuring Storm OpenTSDB Core Bolt
DESCRIPTION: Demonstrates how to configure and use the OpenTsdbBolt for writing time series data to OpenTSDB. The example shows setup with batch processing and flush interval configuration.

LANGUAGE: java
CODE:
        OpenTsdbClient.Builder builder =  OpenTsdbClient.newBuilder(openTsdbUrl).sync(30_000).returnDetails();
        final OpenTsdbBolt openTsdbBolt = new OpenTsdbBolt(builder, Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
        openTsdbBolt.withBatchSize(10).withFlushInterval(2000);
        topologyBuilder.setBolt("opentsdb", openTsdbBolt).shuffleGrouping("metric-gen");

----------------------------------------

TITLE: Configuring MongoUpdateBolt for Storm-MongoDB Integration
DESCRIPTION: Demonstrates how to configure and use the MongoUpdateBolt for updating data in MongoDB. It requires specifying the MongoDB URL, collection name, a QueryFilterCreator, and a MongoUpdateMapper implementation.

LANGUAGE: java
CODE:
MongoUpdateMapper mapper = new SimpleMongoUpdateMapper()
        .withFields("word", "count");

QueryFilterCreator updateQueryCreator = new SimpleQueryFilterCreator()
        .withField("word");

MongoUpdateBolt updateBolt = new MongoUpdateBolt(url, collectionName, updateQueryCreator, mapper);

//if a new document should be inserted if there are no matches to the query filter
//updateBolt.withUpsert(true);

//whether find all documents according to the query filter
//updateBolt.withMany(true);

----------------------------------------

TITLE: Configuring Flux Maven Dependency
DESCRIPTION: XML snippet showing how to include Flux as a Maven dependency in a Storm project.

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>flux-core</artifactId>
    <version>${storm.version}</version>
</dependency>

----------------------------------------

TITLE: Defining IRichBolt Interface in Java
DESCRIPTION: Java interface definition for IRichBolt, which extends IBolt and adds the declareOutputFields method. This interface is part of Storm's strategy to provide default implementations and allow output field declarations.

LANGUAGE: Java
CODE:
public interface IRichBolt extends IBolt {
    void declareOutputFields(OutputFieldsDeclarer declarer);
    Map<String, Object> getComponentConfiguration();
}

----------------------------------------

TITLE: Initializing EsPercolateBolt for Elasticsearch Percolation
DESCRIPTION: Creates an EsPercolateBolt instance to stream percolate requests to Elasticsearch using specified configuration and tuple mapping.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsPercolateBolt percolateBolt = new EsPercolateBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Defining ModelRunner Interface in Java for Storm PMML Bolt
DESCRIPTION: This snippet shows the core method of the ModelRunner interface used in the Storm PMML Bolt. The method 'scoredTuplePerStream' takes a Tuple as input and returns a Map of stream IDs to lists of Objects representing the scored values for each stream.

LANGUAGE: java
CODE:
Map<String, List<Object>> scoredTuplePerStream(Tuple input);

----------------------------------------

TITLE: Defining a Multi-Language Bolt in Java
DESCRIPTION: Java code to define a bolt that uses a Python script for processing.

LANGUAGE: java
CODE:
public static class SplitSentence extends ShellBolt implements IRichBolt {
    public SplitSentence() {
        super("python", "splitsentence.py");
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word"));
    }
}

----------------------------------------

TITLE: Tuple Counting Bolt Implementation in Java
DESCRIPTION: Example bolt implementation that counts received tuples using a registered counter metric.

LANGUAGE: java
CODE:
public class TupleCountingBolt extends BaseRichBolt {
    private Counter tupleCounter;
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        this.tupleCounter = context.registerCounter("tupleCount");
    }

    @Override
    public void execute(Tuple input) {
        this.tupleCounter.inc();
    }
}

----------------------------------------

TITLE: CGroup CPU Metrics Format
DESCRIPTION: Format specification for CGroup CPU metrics reporting showing user and system CPU usage in milliseconds.

LANGUAGE: json
CODE:
   "CGroupCPU.user-ms": number
   "CGroupCPU.sys-ms": number

----------------------------------------

TITLE: Implementing PartialUniquer Bolt for DRPC Reach Calculation in Java
DESCRIPTION: This code defines the PartialUniquer bolt used in the reach calculation DRPC topology. It implements IBatchBolt to process batches of follower tuples and emit partial counts.

LANGUAGE: java
CODE:
public class PartialUniquer extends BaseBatchBolt {
    BatchOutputCollector _collector;
    Object _id;
    Set<String> _followers = new HashSet<String>();
    
    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
        _collector = collector;
        _id = id;
    }

    @Override
    public void execute(Tuple tuple) {
        _followers.add(tuple.getString(1));
    }
    
    @Override
    public void finishBatch() {
        _collector.emit(new Values(_id, _followers.size()));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "partial-count"));
    }
}

----------------------------------------

TITLE: Configuring File Loader in YAML
DESCRIPTION: Example YAML configuration for using the FileConfigLoader, specifying the file URI.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "file:///path/to/my/config.yaml"

----------------------------------------

TITLE: Retrieving Cluster Configuration Sample Response
DESCRIPTION: Example JSON response showing cluster configuration parameters like zookeeper paths, topology settings and system properties

LANGUAGE: json
CODE:
{
    "dev.zookeeper.path": "/tmp/dev-storm-zookeeper",
    "topology.tick.tuple.freq.secs": null,
    "topology.builtin.metrics.bucket.size.secs": 60,
    "topology.fall.back.on.java.serialization": false,
    "topology.max.error.report.per.interval": 5,
    "zmq.linger.millis": 5000
}

----------------------------------------

TITLE: Executing Python Topology Script
DESCRIPTION: This command shows how Storm executes the Python topology script after packaging and uploading resources. It includes Nimbus host, port, and uploaded jar location as arguments.

LANGUAGE: bash
CODE:
python topology.py arg1 arg2 {nimbus-host} {nimbus-port} {uploaded-jar-location}

----------------------------------------

TITLE: Setting Shared Memory for Storm Components
DESCRIPTION: Example of adding shared memory requirements to Storm topology components.

LANGUAGE: java
CODE:
builder.setBolt("exclaim1", new ExclamationBolt(), 3).shuffleGrouping("word")
      .addSharedMemory(new SharedOnHeap(100, "exclaim-cache"));

builder.setBolt("lookup", new LookupBolt(), 3).shuffleGrouping("spout")
      .addSharedMemory(new SharedOffHeapWithinNode(500, "static-lookup"));

----------------------------------------

TITLE: Running Event Hubs Test Client
DESCRIPTION: Command to execute the included Event Hubs send client for testing purposes, with parameters for authentication, routing, and message specifications

LANGUAGE: bash
CODE:
java -cp .\target\eventhubs-storm-spout-{version}-jar-with-dependencies.jar com.microsoft.eventhubs.client.EventHubSendClient [username] [password] [entityPath] [partitionId] [messageSize] [messageCount]

----------------------------------------

TITLE: JAAS Configuration for Digest Authentication
DESCRIPTION: Java Authentication and Authorization Service (JAAS) configuration for Pacemaker digest authentication.

LANGUAGE: java
CODE:
PacemakerDigest {
    username="some username"
    password="some password";
};

----------------------------------------

TITLE: Setting Shared Memory for Components in Java
DESCRIPTION: Java API call to set shared memory for bolts in a Storm topology.

LANGUAGE: java
CODE:
builder.setBolt("exclaim1", new ExclamationBolt(), 3).shuffleGrouping("word")
      .addSharedMemory(new SharedOnHeap(100, "exclaim-cache"));

----------------------------------------

TITLE: Initializing EsLookupBolt for Elasticsearch Queries
DESCRIPTION: Creates an EsLookupBolt for performing get requests to Elasticsearch. Requires EsConfig, ElasticsearchGetRequest adapter, and EsLookupResultOutput for handling responses.

LANGUAGE: java
CODE:
EsConfig esConfig = createEsConfig();
ElasticsearchGetRequest getRequestAdapter = createElasticsearchGetRequest();
EsLookupResultOutput output = createOutput();
EsLookupBolt lookupBolt = new EsLookupBolt(esConfig, getRequestAdapter, output);

----------------------------------------

TITLE: Storm JMS Debug Output
DESCRIPTION: Sample debug output showing JMS message processing, tuple flow, and acknowledgments in the Storm topology.

LANGUAGE: log
CODE:
DEBUG (backtype.storm.contrib.jms.bolt.JmsBolt:183) - Connecting JMS..
DEBUG (backtype.storm.contrib.jms.spout.JmsSpout:213) - sending tuple: ActiveMQTextMessage {commandId = 5, responseRequired = true, messageId = ID:budreau.home-51286-1321074044423-2:4:1:1:1, originalDestination = null, originalTransactionId = null, producerId = ID:budreau.home-51286-1321074044423-2:4:1:1, destination = queue://backtype.storm.contrib.example.queue, transactionId = null, expiration = 0, timestamp = 1321735055910, arrival = 0, brokerInTime = 1321735055910, brokerOutTime = 1321735055921, correlationId = , replyTo = null, persistent = true, type = , priority = 0, groupID = null, groupSequence = 0, targetConsumerId = null, compressed = false, userID = null, content = null, marshalledProperties = org.apache.activemq.util.ByteSequence@6c27ca12, dataStructure = null, redeliveryCounter = 0, size = 0, properties = {secret=880412b7-de71-45dd-8a80-8132589ccd22}, readOnlyProperties = true, readOnlyBody = true, droppable = false, text = Hello storm-jms!}

----------------------------------------

TITLE: Monitoring Topology in Nimbus
DESCRIPTION: Nimbus monitors topologies using a finite state machine approach, periodically calling the reassign-topology function to check and update assignments as needed.

LANGUAGE: clojure
CODE:
(reassign-topology)

----------------------------------------

TITLE: Configuring Storm Metricstore in YAML
DESCRIPTION: YAML configuration options for setting up the Storm Metricstore, including class implementations, RocksDB location, and retention settings.

LANGUAGE: yaml
CODE:
storm.metricstore.class: "org.apache.storm.metricstore.rocksdb.RocksDbStore"
storm.metricprocessor.class: "org.apache.storm.metricstore.NimbusMetricProcessor"
storm.metricstore.rocksdb.location: "storm_rocks"
storm.metricstore.rocksdb.create_if_missing: true
storm.metricstore.rocksdb.metadata_string_cache_capacity: 4000
storm.metricstore.rocksdb.retention_hours: 240

----------------------------------------

TITLE: Storm Topology Submission Thrift Definition
DESCRIPTION: Thrift method definition for submitting a Storm topology to Nimbus, including parameters for name, jar location, configuration, and topology structure.

LANGUAGE: thrift
CODE:
void submitTopology(1: string name, 2: string uploadedJarLocation, 3: string jsonConf, 4: StormTopology topology)
    throws (1: AlreadyAliveException e, 2: InvalidTopologyException ite);

----------------------------------------

TITLE: Implementing ISerialization Interface in Java for Storm Custom Serialization
DESCRIPTION: This code snippet shows the ISerialization interface that must be implemented to create a custom serializer in Storm. It includes methods for accepting a class, serializing an object to a DataOutputStream, and deserializing from a DataInputStream.

LANGUAGE: java
CODE:
public interface ISerialization<T> {
    public boolean accept(Class c);
    public void serialize(T object, DataOutputStream stream) throws IOException;
    public T deserialize(DataInputStream stream) throws IOException;
}

----------------------------------------

TITLE: Configuring File Loader in YAML
DESCRIPTION: Example configuration for the FileConfigLoader, specifying the file URI.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "file:///path/to/my/config.yaml"

----------------------------------------

TITLE: Defining ConnectionProvider Interface in Java
DESCRIPTION: Interface for implementing different connection pooling mechanisms in Storm JDBC integration. It includes methods for preparing, getting connections, and cleanup.

LANGUAGE: java
CODE:
public interface ConnectionProvider extends Serializable {
    /**
     * method must be idempotent.
     */
    void prepare();

    /**
     *
     * @return a DB connection over which the queries can be executed.
     */
    Connection getConnection();

    /**
     * called once when the system is shutting down, should be idempotent.
     */
    void cleanup();
}

----------------------------------------

TITLE: Dequeuing and Removing Items from Kestrel Queue in Java
DESCRIPTION: This method dequeues items from a Kestrel queue and then removes them. It attempts to dequeue 12 items, prints them if available, and acknowledges their removal.

LANGUAGE: java
CODE:
    private static void dequeueAndRemoveItems(KestrelClient kestrelClient, String queueName)
    throws IOException, ParseError
		 {
			for(int i=1; i<=12; i++){

				Item item = kestrelClient.dequeue(queueName);


				if(item==null){
					System.out.println("The queue (" + queueName + ") contains no items.");
				}
				else
				{
					int itemID = item._id;


					byte[] data = item._data;

					String receivedVal = new String(data);

					kestrelClient.ack(queueName, itemID);

					System.out.println("receivedItem=" + receivedVal);
				}
			}
	}

----------------------------------------

TITLE: Launching Docker Container for Storm Worker
DESCRIPTION: Example Docker run command used by worker-launcher to start a container for a Storm worker. It includes various security and resource isolation options.

LANGUAGE: bash
CODE:
run --name=8198e1f0-f323-4b9d-8625-e4fd640cd058 \
--user=<uid>:<gid> \
-d \
--net=host \
--read-only \
-v /sys/fs/cgroup:/sys/fs/cgroup:ro \
-v /usr/share/apache-storm-2.3.0:/usr/share/apache-storm-2.3.0:ro \
-v /<storm-local-dir>/supervisor:/<storm-local-dir>/supervisor:ro \
-v /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058:/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
-v /<workers-artifacts-dir>/workers-artifacts/word-count-1-1591895933/6703:/<workers-artifacts-dir>/workers-artifacts/word-count-1-1591895933/6703 \
-v /<storm-local-dir>/workers-users/8198e1f0-f323-4b9d-8625-e4fd640cd058:/<storm-local-dir>/workers-users/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
-v /var/run/nscd:/var/run/nscd \
-v /<storm-local-dir>/supervisor/stormdist/word-count-1-1591895933/shared_by_topology:/<storm-local-dir>/supervisor/stormdist/word-count-1-1591895933/shared_by_topology \
-v /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/tmp:/tmp \
-v /etc/storm:/etc/storm:ro \
--cgroup-parent=/storm \
--group-add <gid> \
--workdir=/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
--cidfile=/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/container.cid \
--cap-drop=ALL \
--security-opt no-new-privileges \
--security-opt seccomp=/usr/share/apache-storm-2.3.0/conf/seccomp.json \
--cpus=2.6 xxx.xxx.com:8080/storm/storm/rhel7:latest \
bash /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/storm-worker-script.sh

----------------------------------------

TITLE: FailedMessageRetryHandler Interface Methods in Java
DESCRIPTION: Interface methods for handling failed message retry logic in the Kinesis spout. Includes methods for managing failed messages, retry scheduling, and acknowledgment.

LANGUAGE: java
CODE:
    boolean failed (KinesisMessageId messageId);
    KinesisMessageId getNextFailedMessageToRetry ();
    void failedMessageEmitted (KinesisMessageId messageId);
    void acked (KinesisMessageId messageId);

----------------------------------------

TITLE: Creating JdbcTridentState for Inserts in Java
DESCRIPTION: This code snippet demonstrates how to create a JdbcTridentState for use with Trident topologies, configuring it with a connection provider, mapper, table name, and query timeout.

LANGUAGE: java
CODE:
JdbcState.Options options = new JdbcState.Options()
        .withConnectionProvider(connectionProvider)
        .withMapper(jdbcMapper)
        .withTableName("user_details")
        .withQueryTimeoutSecs(30);
JdbcStateFactory jdbcStateFactory = new JdbcStateFactory(options);

----------------------------------------

TITLE: Creating Basic Trident Stream with Storm Spout
DESCRIPTION: Demonstrates how to create a new stream in a Trident topology using a regular Storm IRichSpout. This creates a non-transactional spout that requires a unique identifier for metadata storage in Zookeeper.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
topology.newStream("myspoutid", new MyRichSpout());

----------------------------------------

TITLE: Storm Configuration JSON Structure
DESCRIPTION: Example JSON structure showing the initial configuration and topology context sent to shell components.

LANGUAGE: json
CODE:
{
    "conf": {
        "topology.message.timeout.secs": 3
    },
    "pidDir": "...",
    "context": {
        "task->component": {
            "1": "example-spout",
            "2": "__acker",
            "3": "example-bolt1",
            "4": "example-bolt2"
        },
        "taskid": 3,
        "componentid": "example-bolt"
    }
}

----------------------------------------

TITLE: HTML Meta Redirect for Storm Documentation
DESCRIPTION: HTML meta refresh tag and canonical link that redirects users to the Storm multi-language protocol documentation page for versions 0.7.0 and below.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Storm-multi-language-protocol-(versions-0.7.0-and-below).html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Storm-multi-language-protocol-(versions-0.7.0-and-below).html" />

----------------------------------------

TITLE: Complex DRPC Example - Twitter URL Reach
DESCRIPTION: Implementation of a complex DRPC topology that calculates the reach of a URL on Twitter by processing follower relationships.

LANGUAGE: java
CODE:
LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("reach");
builder.addBolt(new GetTweeters(), 3);
builder.addBolt(new GetFollowers(), 12)
        .shuffleGrouping();
builder.addBolt(new PartialUniquer(), 6)
        .fieldsGrouping(new Fields("id", "follower"));
builder.addBolt(new CountAggregator(), 2)
        .fieldsGrouping(new Fields("id"));

----------------------------------------

TITLE: Storm JAR Command with Dependencies
DESCRIPTION: Complete example showing how to run a Storm topology with external jars, maven artifacts, and custom repositories.

LANGUAGE: bash
CODE:
./bin/storm jar example/storm-starter/storm-starter-topologies-*.jar org.apache.storm.starter.RollingTopWords blobstore-remote2 remote --jars "./external/storm-redis/storm-redis-1.1.0.jar,./external/storm-kafka/storm-kafka-1.1.0.jar" --artifacts "redis.clients:jedis:2.9.0,org.apache.kafka:kafka_2.10:0.8.2.2^org.slf4j:slf4j-log4j12" --artifactRepositories "jboss-repository^http://repository.jboss.com/maven2,HDPRepo^http://repo.hortonworks.com/content/groups/public/"

----------------------------------------

TITLE: RecordToTupleMapper Interface Methods
DESCRIPTION: Interface methods for converting Kinesis records to Storm tuples. Includes getOutputFields for defining tuple structure and getTuple for converting records to tuples.

LANGUAGE: java
CODE:
    Fields getOutputFields ();
    List<Object> getTuple (Record record);

----------------------------------------

TITLE: Continuous Kestrel Queue Producer in Java
DESCRIPTION: Complete program to continuously add sentence items to a Kestrel queue. Runs until user enters closing bracket character, with error handling and connection management.

LANGUAGE: java
CODE:
    import java.io.IOException;
    import java.io.InputStream;
    import java.util.Random;

    import org.apache.storm.spout.KestrelClient;
    import org.apache.storm.spout.KestrelClient.Item;
    import org.apache.storm.spout.KestrelClient.ParseError;

    public class AddSentenceItemsToKestrel {

    	/**
    	 * @param args
    	 */
    	public static void main(String[] args) {

    		InputStream is = System.in;

			char closing_bracket = ']';

			int val = closing_bracket;

			boolean aux = true;

			try {

				KestrelClient kestrelClient = null;
				String queueName = "sentence_queue";

				while(aux){

					kestrelClient = new KestrelClient("localhost",22133);

					queueSentenceItems(kestrelClient, queueName);

					kestrelClient.close();

					Thread.sleep(1000);

					if(is.available()>0){
					 if(val==is.read())
						 aux=false;
					}
				}
			} catch (IOException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			catch (ParseError e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			} catch (InterruptedException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}

			System.out.println("end");

	    }
	}

----------------------------------------

TITLE: Configuring and Creating JdbcInsertBolt in Java
DESCRIPTION: This code snippet demonstrates how to configure and create a JdbcInsertBolt using HikariCP for connection pooling and SimpleJdbcMapper for mapping tuples to database rows.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);

String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

JdbcInsertBolt userPersistanceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withTableName("user")
                                    .withQueryTimeoutSecs(30);
                                    Or
JdbcInsertBolt userPersistanceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withInsertQuery("insert into user values (?,?)")
                                    .withQueryTimeoutSecs(30);                                    

----------------------------------------

TITLE: Launching Docker Container for Storm Worker
DESCRIPTION: Example Docker run command used by worker-launcher to start a container for a Storm worker. It includes various security and resource isolation options.

LANGUAGE: bash
CODE:
run --name=8198e1f0-f323-4b9d-8625-e4fd640cd058 \
--user=<uid>:<gid> \
-d \
--net=host \
--read-only \
-v /sys/fs/cgroup:/sys/fs/cgroup:ro \
-v /usr/share/apache-storm-2.3.0:/usr/share/apache-storm-2.3.0:ro \
-v /<storm-local-dir>/supervisor:/<storm-local-dir>/supervisor:ro \
-v /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058:/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
-v /<workers-artifacts-dir>/workers-artifacts/word-count-1-1591895933/6703:/<workers-artifacts-dir>/workers-artifacts/word-count-1-1591895933/6703 \
-v /<storm-local-dir>/workers-users/8198e1f0-f323-4b9d-8625-e4fd640cd058:/<storm-local-dir>/workers-users/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
-v /var/run/nscd:/var/run/nscd \
-v /<storm-local-dir>/supervisor/stormdist/word-count-1-1591895933/shared_by_topology:/<storm-local-dir>/supervisor/stormdist/word-count-1-1591895933/shared_by_topology \
-v /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/tmp:/tmp \
-v /etc/storm:/etc/storm:ro \
--cgroup-parent=/storm \
--group-add <gid> \
--workdir=/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
--cidfile=/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/container.cid \
--cap-drop=ALL \
--security-opt no-new-privileges \
--security-opt seccomp=/usr/share/apache-storm-2.3.0/conf/seccomp.json \
--cpus=2.6 xxx.xxx.com:8080/storm/storm/rhel7:latest \
bash /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/storm-worker-script.sh

----------------------------------------

TITLE: Implementing FlatMapFunction for Sentence Splitting
DESCRIPTION: Shows how to use FlatMapFunction to split sentences into words, creating a one-to-many transformation.

LANGUAGE: java
CODE:
public class Split extends FlatMapFunction {
  @Override
  public Iterable<Values> execute(TridentTuple input) {
    List<Values> valuesList = new ArrayList<>();
    for (String word : input.getString(0).split(" ")) {
      valuesList.add(new Values(word));
    }
    return valuesList;
  }
}

----------------------------------------

TITLE: Storm Log Command JSON Format
DESCRIPTION: JSON structure for logging messages from a multilang bolt to the worker log.

LANGUAGE: json
CODE:
{
    "command": "log",
    "msg": "hello world!"
}

----------------------------------------

TITLE: Running ActiveMQ Server
DESCRIPTION: Command to start the Apache ActiveMQ server for JMS messaging.

LANGUAGE: shell
CODE:
$ [ACTIVEMQ_HOME]/bin/activemq

----------------------------------------

TITLE: Kerberos Client Authentication Configuration
DESCRIPTION: JAAS configuration for Nimbus client Kerberos authentication with Pacemaker.

LANGUAGE: java
CODE:
PacemakerClient {\n    com.sun.security.auth.module.Krb5LoginModule required\n    useKeyTab=true\n    keyTab="/etc/keytabs/nimbus.keytab"\n    storeKey=true\n    useTicketCache=false\n    serviceName="pacemaker"\n    principal="nimbus@MY.COMPANY.COM";\n};

----------------------------------------

TITLE: Configuring Automatic Task Hooks in Storm
DESCRIPTION: Shows how to configure automatic task hooks using the Storm configuration. These hooks are registered in every spout or bolt.

LANGUAGE: java
CODE:
"topology.auto.task.hooks"

----------------------------------------

TITLE: Configuring Shell Component with Blob Store in Java
DESCRIPTION: Example showing how to configure a ShellSpout to work with executables and scripts from the blob store by setting the working directory and command path.

LANGUAGE: java
CODE:
public MyShellSpout() {
    super("./newPython/bin/python", "./shell_spout.py");
    changeChildCWD(false);
}

----------------------------------------

TITLE: Creating and Querying Kafka Streams with Storm SQL
DESCRIPTION: Example showing how to create external tables mapped to Kafka streams and perform filtering and aggregation operations. The query filters orders where total value exceeds 50 and projects the results into a new stream.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://...' ...

CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) 'kafka://...' ...

INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Configuring CGroups for Apache Storm in YAML
DESCRIPTION: Sample YAML configuration for CGroups in Apache Storm. This configures mount points for various subsystems and sets up a 'storm' group with specific permissions.

LANGUAGE: yaml
CODE:
mount {
	cpuset	= /cgroup/cpuset;
	cpu	= /cgroup/storm_resources;
	cpuacct	= /cgroup/storm_resources;
	memory	= /cgroup/storm_resources;
	devices	= /cgroup/devices;
	freezer	= /cgroup/freezer;
	net_cls	= /cgroup/net_cls;
	blkio	= /cgroup/blkio;
}

group storm {
       perm {
               task {
                      uid = 500;
                      gid = 500;
               }
               admin {
                      uid = 500;
                      gid = 500;
               }
       }
       cpu {
       }
       memory {
       }
       cpuacct {
       }
}

----------------------------------------

TITLE: Named Streams Join Example
DESCRIPTION: Example showing how to join named streams instead of default streams using JoinBolt.

LANGUAGE: java
CODE:
new JoinBolt(JoinBolt.Selector.STREAM,  "stream1", "key1") 
                             .join     ("stream2", "userId",  "stream1" )
                             .select ("userId, key1, key2")
                             .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;
                             
topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("bolt1", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt2", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt3", "stream2", new Fields("userId") )
            .fieldsGrouping("bolt4", "stream1", new Fields("key1") );

----------------------------------------

TITLE: Storm Spout Definition in Clojure
DESCRIPTION: Example of defining a spout that emits random sentences, showing basic spout implementation patterns.

LANGUAGE: clojure
CODE:
(defspout sentence-spout ["sentence"]
  [conf context collector]
  (let [sentences ["a little brown dog"
                   "the man petted the dog"
                   "four score and seven years ago"
                   "an apple a day keeps the doctor away"]]
    (spout
     (nextTuple []
       (Thread/sleep 100)
       (emit-spout! collector [(rand-nth sentences)])         
       )
     (ack [id]
        ))))

----------------------------------------

TITLE: Initializing EsPercolateBolt for Elasticsearch Percolation
DESCRIPTION: Creates an EsPercolateBolt instance to handle percolate requests to Elasticsearch with specified configuration and mapping.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsPercolateBolt percolateBolt = new EsPercolateBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Running Example Topology
DESCRIPTION: Maven command to execute the example Storm topology locally.

LANGUAGE: bash
CODE:
$ mvn exec:java

----------------------------------------

TITLE: Implementing Storm DruidBeamBolt Configuration
DESCRIPTION: Demonstrates how to configure and set up a DruidBeamBolt in a Storm topology. Shows bolt creation with DruidBeamFactory, DruidConfig, and event mapper configuration.

LANGUAGE: java
CODE:
   DruidBeamFactory druidBeamFactory = new SampleDruidBeamFactoryImpl(new HashMap<String, Object>());
   DruidConfig druidConfig = DruidConfig.newBuilder().discardStreamId(DruidConfig.DEFAULT_DISCARD_STREAM_ID).build();
   ITupleDruidEventMapper<Map<String, Object>> eventMapper = new TupleDruidEventMapper<>(TupleDruidEventMapper.DEFAULT_FIELD_NAME);
   DruidBeamBolt<Map<String, Object>> druidBolt = new DruidBeamBolt<Map<String, Object>>(druidBeamFactory, eventMapper, druidConfig);
   topologyBuilder.setBolt("druid-bolt", druidBolt).shuffleGrouping("event-gen");
   topologyBuilder.setBolt("printer-bolt", new PrinterBolt()).shuffleGrouping("druid-bolt" , druidConfig.getDiscardStreamId());

----------------------------------------

TITLE: Monitoring Topology in Nimbus
DESCRIPTION: Nimbus monitors topologies using a finite state machine approach, periodically calling the reassign-topology function to check and update assignments as needed.

LANGUAGE: clojure
CODE:
(reassign-topology)

----------------------------------------

TITLE: Retrieving Topology Summary in JSON
DESCRIPTION: Sample response from the /api/v1/topology/summary endpoint, showing summary information for all topologies.

LANGUAGE: json
CODE:
{
  "topologies": [
    {
      "id": "WordCount3-1-1402960825",
      "name": "WordCount3",
      "status": "ACTIVE",
      "uptime": "6m 5s",
      "uptimeSeconds": 365,
      "tasksTotal": 28,
      "workersTotal": 3,
      "executorsTotal": 28,
      "replicationCount": 1,
      "requestedMemOnHeap": 640,
      "requestedMemOffHeap": 128,
      "requestedTotalMem": 768,
      "requestedCpu": 80,
      "assignedMemOnHeap": 640,
      "assignedMemOffHeap": 128,
      "assignedTotalMem": 768,
      "assignedCpu": 80
    }
  ],
  "schedulerDisplayResource": true
}

----------------------------------------

TITLE: Storm JMS Debug Output
DESCRIPTION: Example debug output showing the message flow through the topology, including JMS message handling, tuple processing, and acknowledgments.

LANGUAGE: log
CODE:
DEBUG (backtype.storm.contrib.jms.bolt.JmsBolt:183) - Connecting JMS..
DEBUG (backtype.storm.contrib.jms.spout.JmsSpout:213) - sending tuple: ActiveMQTextMessage {commandId = 5, responseRequired = true, messageId = ID:budreau.home-51286-1321074044423-2:4:1:1:1, originalDestination = null, originalTransactionId = null, producerId = ID:budreau.home-51286-1321074044423-2:4:1:1, destination = queue://backtype.storm.contrib.example.queue, transactionId = null, expiration = 0, timestamp = 1321735055910, arrival = 0, brokerInTime = 1321735055910, brokerOutTime = 1321735055921, correlationId = , replyTo = null, persistent = true, type = , priority = 0, groupID = null, groupSequence = 0, targetConsumerId = null, compressed = false, userID = null, content = null, marshalledProperties = org.apache.activemq.util.ByteSequence@6c27ca12, dataStructure = null, redeliveryCounter = 0, size = 0, properties = {secret=880412b7-de71-45dd-8a80-8132589ccd22}, readOnlyProperties = true, readOnlyBody = true, droppable = false, text = Hello storm-jms!}

----------------------------------------

TITLE: Configuring Storm Cluster State Store
DESCRIPTION: Basic configuration to enable Pacemaker as the cluster state store in Storm.

LANGUAGE: yaml
CODE:
storm.cluster.state.store: "org.apache.storm.cluster.PaceMakerStateStorageFactory"

----------------------------------------

TITLE: Implementing RedisLookupMapper for WordCount
DESCRIPTION: Java class implementing RedisLookupMapper interface for a word count scenario, using Redis HASH data type.

LANGUAGE: java
CODE:
class WordCountRedisLookupMapper implements RedisLookupMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountRedisLookupMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public List<Values> toTuple(ITuple input, Object value) {
        String member = getKeyFromTuple(input);
        List<Values> values = Lists.newArrayList();
        values.add(new Values(member, value));
        return values;
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("wordName", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Configuring Storm Remote Cluster Connection in YAML
DESCRIPTION: YAML configuration for specifying the Nimbus master node address in ~/.storm/storm.yaml to enable remote cluster connectivity. This configuration is required for submitting and managing topologies on a remote Storm cluster.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["123.45.678.890"]

----------------------------------------

TITLE: Initializing Basic Trident Stream with Storm Spout
DESCRIPTION: Shows how to create a new Trident stream using a regular Storm IRichSpout. The spout requires a unique identifier that will be used for storing metadata in Zookeeper.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
topology.newStream("myspoutid", new MyRichSpout());

----------------------------------------

TITLE: Stateful Word Count Bolt Definition
DESCRIPTION: A prepared bolt implementation that maintains state to count words, showing how to use closures for state management.

LANGUAGE: clojure
CODE:
(defbolt word-count ["word" "count"] {:prepare true}
  [conf context collector]
  (let [counts (atom {})]
    (bolt
     (execute [tuple]
       (let [word (.getString tuple 0)]
         (swap! counts (partial merge-with +) {word 1})
         (emit-bolt! collector [word (@counts word)] :anchor tuple)
         (ack! collector tuple)
         )))))

----------------------------------------

TITLE: Thrift Structures for Cluster Information
DESCRIPTION: Thrift struct definitions for cluster summary information including supervisor, topology and nimbus details.

LANGUAGE: thrift
CODE:
struct ClusterSummary {
  1: required list<SupervisorSummary> supervisors;
  3: required list<TopologySummary> topologies;
  4: required list<NimbusSummary> nimbuses;
}

struct NimbusSummary {
  1: required string host;
  2: required i32 port;
  3: required i32 uptime_secs;
  4: required bool isLeader;
  5: required string version;
}

----------------------------------------

TITLE: Thread-safe Tuple Serialization in Apache Storm (Java)
DESCRIPTION: Implementation of a thread-safe serializer for tuples in Apache Storm.

LANGUAGE: java
CODE:
https://github.com/apache/storm/blob/0.7.1/src/jvm/org/apache/storm/serialization/KryoTupleSerializer.java#L26

----------------------------------------

TITLE: Implementing Generic Resources for Storm Spout
DESCRIPTION: Example showing how to add GPU resource requirements to a Storm Spout component using the addResource API.

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.addResouce("gpu.count", 1.0);

----------------------------------------

TITLE: Using Max and MaxBy Operations in Trident
DESCRIPTION: Example of using maxBy operation to find maximum values in each partition.

LANGUAGE: java
CODE:
mystream.maxBy(new Fields("count"))

----------------------------------------

TITLE: Running Storm JMS Example Topology Locally
DESCRIPTION: Maven command to execute the example Storm topology locally. This runs the topology, which will connect to ActiveMQ and create the necessary JMS destinations.

LANGUAGE: bash
CODE:
$ mvn exec:java

----------------------------------------

TITLE: Configuring Resource Aware Scheduler in Storm
DESCRIPTION: Basic YAML configuration to enable the Resource Aware Scheduler in Storm's configuration file

LANGUAGE: yaml
CODE:
storm.scheduler: "org.apache.storm.scheduler.resource.ResourceAwareScheduler"

----------------------------------------

TITLE: Configuring Authorization in storm.yaml
DESCRIPTION: YAML configuration for setting up authorization in Storm using SimpleACLAuthorizer.

LANGUAGE: yaml
CODE:
nimbus.authorizer: "org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer"

multitenant.scheduler.user.pools: 
    "evans": 10
    "derek": 10

supervisor.run.worker.as.user: true

nimbus.impersonation.authorizer: org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer
nimbus.impersonation.acl:
    impersonating_user1:
        hosts:
            [comma separated list of hosts from which impersonating_user1 is allowed to impersonate other users]
        groups:
            [comma separated list of groups whose users impersonating_user1 is allowed to impersonate]
    impersonating_user2:
        hosts:
            [comma separated list of hosts from which impersonating_user2 is allowed to impersonate other users]
        groups:
            [comma separated list of groups whose users impersonating_user2 is allowed to impersonate]

----------------------------------------

TITLE: JAAS Configuration for Storm and Zookeeper
DESCRIPTION: Example JAAS configuration file for Storm nodes, including sections for StormServer, StormClient, Client, and Server.

LANGUAGE: yaml
CODE:
StormServer {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/storm.keytab"
   storeKey=true
   useTicketCache=false
   principal="storm/storm.example.com@STORM.EXAMPLE.COM";
};
StormClient {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/storm.keytab"
   storeKey=true
   useTicketCache=false
   serviceName="storm"
   principal="storm@STORM.EXAMPLE.COM";
};
Client {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/storm.keytab"
   storeKey=true
   useTicketCache=false
   serviceName="zookeeper"
   principal="storm@STORM.EXAMPLE.COM";
};
Server {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/zk.keytab"
   storeKey=true
   useTicketCache=false
   serviceName="zookeeper"
   principal="zookeeper/zk1.example.com@STORM.EXAMPLE.COM";
};

----------------------------------------

TITLE: Setting Log Level Using Storm CLI
DESCRIPTION: Commands to dynamically set and clear log levels for Storm topologies using the command line interface. Supports setting specific logger levels with optional timeout values and reverting back to original settings.

LANGUAGE: bash
CODE:
./bin/storm set_log_level [topology name] -l [logger name]=[LEVEL]:[TIMEOUT]

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -l ROOT=DEBUG:30

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -r ROOT

----------------------------------------

TITLE: Configuring Kerberos Authentication in Storm YAML
DESCRIPTION: YAML configuration for enabling Kerberos authentication in Storm, including settings for the Thrift transport and JAAS configuration file.

LANGUAGE: yaml
CODE:
storm.thrift.transport: "org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin"
java.security.auth.login.config: "/path/to/jaas.conf"

nimbus.childopts: "-Xmx1024m -Djava.security.auth.login.config=/path/to/jaas.conf"
ui.childopts: "-Xmx768m -Djava.security.auth.login.config=/path/to/jaas.conf"
supervisor.childopts: "-Xmx256m -Djava.security.auth.login.config=/path/to/jaas.conf"

----------------------------------------

TITLE: Bolt Tuple Transfer in Apache Storm (Clojure)
DESCRIPTION: This code shows how bolts use the transfer function provided by the worker to transfer tuples in Apache Storm.

LANGUAGE: clojure
CODE:
(github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L429)

----------------------------------------

TITLE: Using StateUpdater in Trident Topology
DESCRIPTION: Example of incorporating a custom StateUpdater (LocationUpdater) into a Trident topology for updating location data.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
TridentState locations = 
    topology.newStream("locations", locationsSpout)
        .partitionPersist(new LocationDBFactory(), new Fields("userid", "location"), new LocationUpdater())

----------------------------------------

TITLE: Storm Shell Command Usage Example
DESCRIPTION: Example showing how to use the storm shell command to package and submit a Python topology with resources.

LANGUAGE: bash
CODE:
storm shell resources/ python topology.py arg1 arg2

----------------------------------------

TITLE: Logging Message in Storm Multi-Language Protocol
DESCRIPTION: This JSON structure represents a log command in the Storm multi-language protocol. It includes the command type and the message to be logged in the worker log.

LANGUAGE: json
CODE:
{
    "command": "log",
    "msg": "hello world!"
}

----------------------------------------

TITLE: Logging Message in Storm Multi-Language Protocol
DESCRIPTION: This JSON structure represents a log command in the Storm multi-language protocol. It includes the command type and the message to be logged in the worker log.

LANGUAGE: json
CODE:
{
    "command": "log",
    "msg": "hello world!"
}

----------------------------------------

TITLE: Configuring OpenTSDB Core Bolt in Storm
DESCRIPTION: Demonstrates how to set up and configure OpenTsdbBolt for writing time series data to OpenTSDB. The example shows creating an OpenTsdbClient builder, configuring the bolt with batch size and flush interval, and adding it to the topology builder.

LANGUAGE: java
CODE:
OpenTsdbClient.Builder builder =  OpenTsdbClient.newBuilder(openTsdbUrl).sync(30_000).returnDetails();
final OpenTsdbBolt openTsdbBolt = new OpenTsdbBolt(builder, Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
openTsdbBolt.withBatchSize(10).withFlushInterval(2000);
topologyBuilder.setBolt("opentsdb", openTsdbBolt).shuffleGrouping("metric-gen");

----------------------------------------

TITLE: Converting Docker Images to Squashfs Format
DESCRIPTION: Example command using docker-to-squash.py script to download Docker images, convert layers to squashfs files and place them in HDFS.

LANGUAGE: bash
CODE:
python3 docker-to-squash.py pull-build-push-update --hdfs-root hdfs://hostname:port/containers \
                      docker.xxx.com:4443/hadoop-user-images/storm/rhel7:20201202-232133,storm/rhel7:dev_current --log DEBUG --bootstrap

----------------------------------------

TITLE: Specifying Custom Scheduling Strategy
DESCRIPTION: API call to set a custom scheduling strategy for a Storm topology.

LANGUAGE: java
CODE:
conf.setTopologyStrategy(org.apache.storm.scheduler.resource.strategies.scheduling.DefaultResourceAwareStrategy.class);

----------------------------------------

TITLE: Storm SQL Query for Filtering Error Logs
DESCRIPTION: SQL script to filter error logs from Apache logs and store them in a separate Kafka topic.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://localhost:2181/brokers?topic=apache-logs'
CREATE EXTERNAL TABLE APACHE_ERROR_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_ELAPSED_MS INT) LOCATION 'kafka://localhost:2181/brokers?topic=apache-error-logs' TBLPROPERTIES '{"producer":{"bootstrap.servers":"localhost:9092","acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer","value.serializer":"org.apache.storm.kafka.ByteBufferSerializer"}}'
INSERT INTO APACHE_ERROR_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, (TIME_US / 1000) AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (CAST(STATUS AS INT) / 100) >= 4

----------------------------------------

TITLE: Input Declaration Example for Storm Bolt
DESCRIPTION: Shows different ways to declare bolt inputs including shuffle grouping, fields grouping and global grouping.

LANGUAGE: clojure
CODE:
{["2" "1"] :shuffle
 "3" ["field1" "field2"]
 ["4" "2"] :global}

----------------------------------------

TITLE: Configuring HiveBolt with DelimitedRecordHiveMapper (Java)
DESCRIPTION: Java code snippet demonstrating how to configure a HiveBolt using DelimitedRecordHiveMapper and HiveOptions. This setup allows streaming tuples directly into Hive tables.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames));
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper);
HiveBolt hiveBolt = new HiveBolt(hiveOptions);

----------------------------------------

TITLE: Configuring Generic Cluster Resources in YAML
DESCRIPTION: This snippet shows how to specify node resource availability in the storm.yaml configuration file. It demonstrates setting the GPU count for a supervisor node.

LANGUAGE: yaml
CODE:
supervisor.resources.map: {[type<String>] : [amount<Double>]}

LANGUAGE: yaml
CODE:
supervisor.resources.map: {"gpu.count" : 2.0}

----------------------------------------

TITLE: HTML Meta Redirect to Storm Documentation
DESCRIPTION: HTML meta refresh directive that automatically redirects visitors to the Storm Trident API documentation page, along with a canonical link reference.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Trident-API-Overview.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Trident-API-Overview.html" />

----------------------------------------

TITLE: Configuring StaticHosts for Kafka Broker Mapping
DESCRIPTION: Example showing how to create static broker-partition mapping using GlobalPartitionInformation

LANGUAGE: java
CODE:
Broker brokerForPartition0 = new Broker("localhost");
Broker brokerForPartition1 = new Broker("localhost", 9092);
Broker brokerForPartition2 = new Broker("localhost:9092");
GlobalPartitionInformation partitionInfo = new GlobalPartitionInformation();
partitionInfo.addPartition(0, brokerForPartition0);
partitionInfo.addPartition(1, brokerForPartition1);
partitionInfo.addPartition(2, brokerForPartition2);
StaticHosts hosts = new StaticHosts(partitionInfo);

----------------------------------------

TITLE: Implementing SimpleMongoMapper for Storm-MongoDB Integration in Java
DESCRIPTION: Implements the MongoMapper interface with SimpleMongoMapper, which maps Storm tuple fields directly to MongoDB document fields. This class is used for straightforward mappings between Storm tuples and MongoDB documents.

LANGUAGE: java
CODE:
public class SimpleMongoMapper implements MongoMapper {
    private String[] fields;

    @Override
    public Document toDocument(ITuple tuple) {
        Document document = new Document();
        for(String field : fields){
            document.append(field, tuple.getValueByField(field));
        }
        return document;
    }

    public SimpleMongoMapper withFields(String... fields) {
        this.fields = fields;
        return this;
    }
}

----------------------------------------

TITLE: Applying Map Operation on a Stream in Java
DESCRIPTION: Demonstrates the use of the map operation to transform each element of a stream using a given function.

LANGUAGE: java
CODE:
Stream<String> words = ...
Stream<Integer> wordLengths = words.map(String::length);

----------------------------------------

TITLE: Creating and Querying Kafka Streams with Storm SQL
DESCRIPTION: Example showing how to create external tables mapped to Kafka streams and perform filtering and projection operations. The query calculates total order values and filters for large orders exceeding 50 units.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://...' ...

CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) 'kafka://...' ...

INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Storm OCI Runtime Configuration
DESCRIPTION: Example Storm configuration settings for enabling and configuring the OCI/Squashfs runtime.

LANGUAGE: bash
CODE:
storm.resource.isolation.plugin: "org.apache.storm.container.oci.RuncLibContainerManager"

storm.oci.allowed.images:
    - "storm/rhel7:dev_current"
    - "storm/rhel7:dev_previous"
    - "storm/rhel7:dev_test"
storm.oci.image: "storm/rhel7:dev_current"

storm.oci.cgroup.parent: "/storm"
storm.oci.cgroup.root: "/sys/fs/cgroup"
storm.oci.image.hdfs.toplevel.dir: "hdfs://host:port/containers/"
storm.oci.image.tag.to.manifest.plugin: "org.apache.storm.container.oci.LocalOrHdfsImageTagToManifestPlugin"
storm.oci.local.or.hdfs.image.tag.to.manifest.plugin.hdfs.hash.file: "hdfs://host:port/containers/image-tag-to-hash"
storm.oci.manifest.to.resources.plugin: "org.apache.storm.container.oci.HdfsManifestToResourcesPlugin"
storm.oci.readonly.bindmounts:
    - "/home/y/lib64/storm"
    - "/etc/krb5.conf"

storm.oci.resources.localizer: "org.apache.storm.container.oci.HdfsOciResourcesLocalizer"
storm.oci.seccomp.profile: "/home/y/conf/storm/seccomp.json"

----------------------------------------

TITLE: Storm Shell Component I/O Format
DESCRIPTION: JSON structures for various commands and responses between Storm and shell components including emit, ack, fail, and log operations.

LANGUAGE: json
CODE:
{
    "command": "emit",
    "id": "1231231",
    "stream": "1",
    "task": 9,
    "tuple": ["field1", 2, 3]
}

----------------------------------------

TITLE: Basic Trident State Query Example
DESCRIPTION: Example showing how to query state using Trident's StateQuery interface with a LocationDB implementation

LANGUAGE: java
CODE:
public class QueryLocation extends BaseQueryFunction<LocationDB, String> {
    public List<String> batchRetrieve(LocationDB state, List<TridentTuple> inputs) {
        List<String> ret = new ArrayList();
        for(TridentTuple input: inputs) {
            ret.add(state.getLocation(input.getLong(0)));
        }
        return ret;
    }

    public void execute(TridentTuple tuple, String location, TridentCollector collector) {
        collector.emit(new Values(location));
    }    
}

----------------------------------------

TITLE: Applying a Filter to a Trident Stream
DESCRIPTION: Example of applying the MyFilter to a stream.

LANGUAGE: java
CODE:
mystream.filter(new MyFilter())

----------------------------------------

TITLE: Setting Log Level via Storm CLI
DESCRIPTION: Command to set a log level for a specific logger in a Storm topology using the CLI. It allows specifying the topology name, logger name, log level, and an optional timeout.

LANGUAGE: bash
CODE:
./bin/storm set_log_level [topology name] -l [logger name]=[LEVEL]:[TIMEOUT]

----------------------------------------

TITLE: Killing Topology in Storm UI REST API
DESCRIPTION: Example JSON response for the /api/v1/topology/:id/kill/:wait-time POST endpoint, which kills a topology.

LANGUAGE: json
CODE:
{"topologyOperation":"kill","topologyId":"wordcount-1-1420308665","status":"success"}

----------------------------------------

TITLE: Creating a Flux-Enabled Storm Topology JAR
DESCRIPTION: Maven shade plugin configuration to create a fat JAR containing Flux and topology dependencies

LANGUAGE: xml
CODE:
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-shade-plugin</artifactId>
    <version>1.4</version>
    <configuration>
        <createDependencyReducedPom>true</createDependencyReducedPom>
    </configuration>
    <executions>
        <execution>
            <phase>package</phase>
            <goals>
                <goal>shade</goal>
            </goals>
            <configuration>
                <transformers>
                    <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                    <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                        <mainClass>org.apache.storm.flux.Flux</mainClass>
                    </transformer>
                </transformers>
            </configuration>
        </execution>
    </executions>
</plugin>

----------------------------------------

TITLE: Storm Ack Command JSON Format
DESCRIPTION: JSON structure for acknowledging tuple processing completion in Storm.

LANGUAGE: json
CODE:
{
    "command": "ack",
    "id": 123123
}

----------------------------------------

TITLE: Specifying Classpath Wildcard in Java 6+
DESCRIPTION: This snippet demonstrates the use of classpath wildcards in Java 6 and later versions to include all JAR files in a directory without explicitly listing them. This approach is used by Storm to shorten classpath declarations and avoid process command length limits.

LANGUAGE: java
CODE:
foo/bar/*

----------------------------------------

TITLE: Merging Streams in Trident
DESCRIPTION: Example of merging multiple streams into one.

LANGUAGE: java
CODE:
topology.merge(stream1, stream2, stream3);

----------------------------------------

TITLE: Creating a ClientBlobStore in Java
DESCRIPTION: Initializes a ClientBlobStore object to interact with Storm's blobstore

LANGUAGE: java
CODE:
Config theconf = new Config();
theconf.putAll(Utils.readStormConfig());
ClientBlobStore clientBlobStore = Utils.getClientBlobStore(theconf);

----------------------------------------

TITLE: Executing Python topology script with Nimbus connection details
DESCRIPTION: Demonstrates how Storm shell executes the Python topology script, passing Nimbus host, port, and uploaded jar location as arguments. This allows the script to connect to Nimbus and submit the topology.

LANGUAGE: bash
CODE:
python3 topology.py arg1 arg2 {nimbus-host} {nimbus-port} {uploaded-jar-location}

----------------------------------------

TITLE: Storm NUMA Configuration YAML
DESCRIPTION: YAML configuration for setting up NUMA zones in Storm supervisor, including core allocation, memory size, port assignments, and optional generic resource mapping.

LANGUAGE: yaml
CODE:
supervisor.numa.meta:
    "0": # Numa zone id
        numa.cores: # Cores in NUMA zone (can be determined by using the numastat command)
            - 0
            - 1
            - 2
            - 3
            - 4
            - 5
            - 12
            - 13
            - 14
            - 15
            - 16

        numa.generic.resources.map: # Generic Resources in the zone to be used for generic resource scheduling (optional)
            network.resource.units: 50.0

        numa.memory.mb: 42461 # Size of memory zone
        numa.ports: # Ports to be assigned to workers pinned to the NUMA zone (this may include ports not specified in SUPERVISOR_SLOTS_PORTS
            - 6700
            - 6701
            - 6702
            - 6703
            - 6704
            - 6705
            - 6706
            - 6707
            - 6708
            - 6709
            - 6710
            - 6711

----------------------------------------

TITLE: Enabling Resource Aware Scheduler in Storm Config
DESCRIPTION: Configuration to enable the Resource Aware Scheduler in Storm's configuration file.

LANGUAGE: yaml
CODE:
storm.scheduler: "org.apache.storm.scheduler.resource.ResourceAwareScheduler"

----------------------------------------

TITLE: Deactivating a Storm Topology's Spouts
DESCRIPTION: Deactivates the spouts of a specified topology to pause data processing.

LANGUAGE: shell
CODE:
storm deactivate topology-name

----------------------------------------

TITLE: Thread-safe Serializer Implementation in Apache Storm (Java)
DESCRIPTION: Implementation of a thread-safe serializer used for tuple serialization in Apache Storm.

LANGUAGE: java
CODE:
https://github.com/apache/storm/blob/0.7.1/src/jvm/org/apache/storm/serialization/KryoTupleSerializer.java#L26

----------------------------------------

TITLE: Thread-safe Tuple Serialization in Apache Storm (Java)
DESCRIPTION: Implementation of a thread-safe serializer for tuples in Apache Storm.

LANGUAGE: java
CODE:
https://github.com/apache/storm/blob/0.7.1/src/jvm/org/apache/storm/serialization/KryoTupleSerializer.java#L26

----------------------------------------

TITLE: Retrieving Cluster Summary in JSON
DESCRIPTION: Example response from the /api/v1/cluster/summary endpoint, which returns summary information about the cluster.

LANGUAGE: json
CODE:
{
  "stormVersion": "0.9.2-incubating-SNAPSHOT",
  "supervisors": 1,
  "slotsTotal": 4,
  "slotsUsed": 3,
  "slotsFree": 1,
  "executorsTotal": 28,
  "tasksTotal": 28,
  "schedulerDisplayResource": true,
  "totalMem": 4096.0,
  "totalCpu": 400.0,
  "availMem": 1024.0,
  "availCPU": 250.0,
  "memAssignedPercentUtil": 75.0,
  "cpuAssignedPercentUtil": 37.5
}

----------------------------------------

TITLE: Retrieving Cluster Summary in JSON
DESCRIPTION: Example response from the /api/v1/cluster/summary endpoint, which returns summary information about the cluster.

LANGUAGE: json
CODE:
{
  "stormVersion": "0.9.2-incubating-SNAPSHOT",
  "supervisors": 1,
  "slotsTotal": 4,
  "slotsUsed": 3,
  "slotsFree": 1,
  "executorsTotal": 28,
  "tasksTotal": 28,
  "schedulerDisplayResource": true,
  "totalMem": 4096.0,
  "totalCpu": 400.0,
  "availMem": 1024.0,
  "availCPU": 250.0,
  "memAssignedPercentUtil": 75.0,
  "cpuAssignedPercentUtil": 37.5
}

----------------------------------------

TITLE: Building the Storm JMS Example Topology
DESCRIPTION: Maven commands to build the Storm JMS example topology. These commands should be executed from the storm-jms directory.

LANGUAGE: bash
CODE:
$ cd storm-jms
$ mvn clean install

----------------------------------------

TITLE: Event Hubs Configuration Properties
DESCRIPTION: Configuration properties required for connecting Storm spout to Azure Event Hubs, including authentication, namespace, entity path and partition settings

LANGUAGE: properties
CODE:
eventhubspout.username = [username: policy name in EventHubs Portal]
eventhubspout.password = [password: shared access key in EventHubs Portal]
eventhubspout.namespace = [namespace]
eventhubspout.entitypath = [entitypath]
eventhubspout.partitions.count = [partitioncount]

# if not provided, will use storm's zookeeper settings
# zookeeper.connectionstring=zookeeper0:2181,zookeeper1:2181,zookeeper2:2181

eventhubspout.checkpoint.interval = 10
eventhub.receiver.credits = 1024

----------------------------------------

TITLE: Implementing Query Filter Creator
DESCRIPTION: Simple implementation for creating MongoDB query filters based on tuple fields.

LANGUAGE: java
CODE:
public class SimpleQueryFilterCreator implements QueryFilterCreator {

    private String field;
    
    @Override
    public Bson createFilter(ITuple tuple) {
        return Filters.eq(field, tuple.getValueByField(field));
    }

    @Override
    public Bson createFilterByKeys(List<Object> keys) {
        return Filters.eq("_id", MongoUtils.getID(keys));
    }

    public SimpleQueryFilterCreator withField(String field) {
        this.field = field;
        return this;
    }
}

----------------------------------------

TITLE: Executing Python topology script with Nimbus connection details
DESCRIPTION: Demonstrates how Storm shell executes the Python topology script, passing Nimbus host, port, and uploaded jar location as arguments. This allows the script to connect to Nimbus and submit the topology.

LANGUAGE: bash
CODE:
python3 topology.py arg1 arg2 {nimbus-host} {nimbus-port} {uploaded-jar-location}

----------------------------------------

TITLE: Setting Log Level Using Storm CLI
DESCRIPTION: Commands for setting and clearing log levels using the Storm CLI. The command allows specifying a logger name, log level, and optional timeout duration.

LANGUAGE: bash
CODE:
./bin/storm set_log_level [topology name] -l [logger name]=[LEVEL]:[TIMEOUT]

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -l ROOT=DEBUG:30

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -r ROOT

----------------------------------------

TITLE: Initializing Regular Storm Spout in Trident Topology
DESCRIPTION: Creates a new stream in a Trident topology using a regular Storm IRichSpout. The spout requires a unique identifier that is used to store metadata in Zookeeper.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
topology.newStream("myspoutid", new MyRichSpout());

----------------------------------------

TITLE: Implementing Simple MongoDB Mapper
DESCRIPTION: Basic implementation of MongoMapper that maps tuple fields directly to document fields

LANGUAGE: java
CODE:
public class SimpleMongoMapper implements MongoMapper {
    private String[] fields;

    @Override
    public Document toDocument(ITuple tuple) {
        Document document = new Document();
        for(String field : fields){
            document.append(field, tuple.getValueByField(field));
        }
        return document;
    }

    @Override
    public Document toDocumentByKeys(List<Object> keys) {
        Document document = new Document();
        document.append("_id", MongoUtils.getID(keys));
        return document;
    }

    public SimpleMongoMapper withFields(String... fields) {
        this.fields = fields;
        return this;
    }
}

----------------------------------------

TITLE: Declaring Maven Dependency for Storm-Redis
DESCRIPTION: XML snippet showing how to include storm-redis as a Maven dependency in a project.

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>storm-redis</artifactId>
    <version>${storm.version}</version>
    <type>jar</type>
</dependency>

----------------------------------------

TITLE: Defining Code Distribution Interface in Java
DESCRIPTION: Interface responsible for distributing topology code across the Storm cluster, supporting various distribution mechanisms like BitTorrent or filesystem-based approaches.

LANGUAGE: java
CODE:
public interface ICodeDistributor {
    void prepare(Map conf);
    File upload(Path dirPath, String topologyId);
    List<File> download(Path destDirPath, String topologyid, File metafile);
    int getReplicationCount(String topologyId);
    void cleanup(String topologyid);
    void close(Map conf);
}

----------------------------------------

TITLE: Synchronizing Supervisor with Assignments (Clojure)
DESCRIPTION: The synchronize-supervisor function in the Supervisor daemon downloads code from Nimbus for assigned topologies and writes local assignments to the filesystem.

LANGUAGE: clojure
CODE:
synchronize-supervisor

----------------------------------------

TITLE: Implementing a Parameterized Bolt in Clojure
DESCRIPTION: Example of a parameterized bolt that appends a suffix to input strings, demonstrating runtime configuration.

LANGUAGE: clojure
CODE:
(defbolt suffix-appender ["word"] {:params [suffix]}
  [tuple collector]
  (emit-bolt! collector [(str (.getString tuple 0) suffix)] :anchor tuple)
  )

----------------------------------------

TITLE: Implementing One Aggregator for Unique Counting in Java
DESCRIPTION: This class defines a combiner aggregator that emits a single tuple containing the number one for each group. It's used in the reach calculation to prepare for the final unique count.

LANGUAGE: java
CODE:
public class One implements CombinerAggregator<Integer> {
   public Integer init(TridentTuple tuple) {
       return 1;
   }

   public Integer combine(Integer val1, Integer val2) {
       return 1;
   }

   public Integer zero() {
       return 1;
   }        
}

----------------------------------------

TITLE: Container Manifest JSON Configuration
DESCRIPTION: Example manifest JSON showing container configuration including layers, digests and media types

LANGUAGE: json
CODE:
{
  "schemaVersion": 2,
  "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
  "config": {
    "mediaType": "application/vnd.docker.container.image.v1+json",
    "size": 7877,
    "digest": "sha256:ef1ff2c7167a1a6cd01e106f51b84a4d400611ba971c53cbc28de7919515ca4e"
  },
  "layers": [
    {
      "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
      "size": 26858854,
      "digest": "sha256:3692c3483ef6516fba685b316448e8aaf0fc10bb66818116edc8e5e6800076c7"
    }
  ]
}

----------------------------------------

TITLE: Applying a Map Function to a Trident Stream
DESCRIPTION: Example of how to apply a map function to a Trident stream.

LANGUAGE: java
CODE:
mystream.map(new UpperCase())

----------------------------------------

TITLE: Initializing FixedBatchSpout in Java for Trident
DESCRIPTION: Creates a FixedBatchSpout that cycles through a set of sentences to produce a stream of data for Trident processing.

LANGUAGE: java
CODE:
FixedBatchSpout spout = new FixedBatchSpout(new Fields("sentence"), 3,
               new Values("the cow jumped over the moon"),
               new Values("the man went to the store and bought some candy"),
               new Values("four score and seven years ago"),
               new Values("how many apples can you eat"));
spout.setCycle(true);

----------------------------------------

TITLE: HTML Meta Redirect to Storm Documentation
DESCRIPTION: HTML meta tags that perform a client-side redirect to Storm's documentation page and establish the canonical URL reference.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/DSLs-and-multilang-adapters.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/DSLs-and-multilang-adapters.html" />

----------------------------------------

TITLE: Configuring NUMA Settings in Storm Supervisor
DESCRIPTION: YAML configuration for setting up NUMA zones in Storm supervisor, including core assignments, memory allocation, port mappings, and generic resource specifications.

LANGUAGE: yaml
CODE:
supervisor.numa.meta:
    "0": # Numa zone id
        numa.cores: # Cores in NUMA zone (can be determined by using the numastat command)
            - 0
            - 1
            - 2
            - 3
            - 4
            - 5
            - 12
            - 13
            - 14
            - 15
            - 16

        numa.generic.resources.map: # Generic Resources in the zone to be used for generic resource scheduling (optional)
            network.resource.units: 50.0

        numa.memory.mb: 42461 # Size of memory zone
        numa.ports: # Ports to be assigned to workers pinned to the NUMA zone (this may include ports not specified in SUPERVISOR_SLOTS_PORTS
            - 6700
            - 6701
            - 6702
            - 6703
            - 6704
            - 6705
            - 6706
            - 6707
            - 6708
            - 6709
            - 6710
            - 6711

----------------------------------------

TITLE: Configuring Kerberos Authentication in Storm YAML
DESCRIPTION: YAML configuration for enabling Kerberos authentication in Storm, including SASL transport and Java security settings.

LANGUAGE: yaml
CODE:
storm.thrift.transport: "org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin"
java.security.auth.login.config: "/path/to/jaas.conf"

nimbus.childopts: "-Xmx1024m -Djava.security.auth.login.config=/path/to/jaas.conf"
ui.childopts: "-Xmx768m -Djava.security.auth.login.config=/path/to/jaas.conf"
supervisor.childopts: "-Xmx256m -Djava.security.auth.login.config=/path/to/jaas.conf"

----------------------------------------

TITLE: Configuring IsolationScheduler in YAML
DESCRIPTION: Example configuration for the IsolationScheduler in Storm's YAML file. It specifies the number of isolated machines allocated to different topologies.

LANGUAGE: yaml
CODE:
isolation.scheduler.machines: 
    "my-topology": 8
    "tiny-topology": 1
    "some-other-topology": 3

----------------------------------------

TITLE: Configuring Kerberos Authentication for Pacemaker Client
DESCRIPTION: JAAS configuration for Kerberos authentication on Nimbus node for Pacemaker client.

LANGUAGE: java
CODE:
PacemakerClient {
    com.sun.security.auth.module.Krb5LoginModule required
    useKeyTab=true
    keyTab="/etc/keytabs/nimbus.keytab"
    storeKey=true
    useTicketCache=false
    serviceName="pacemaker"
    principal="nimbus@MY.COMPANY.COM";
};

----------------------------------------

TITLE: Task Routing Map in Apache Storm (Clojure)
DESCRIPTION: This code defines the routing map used by tasks to determine message routing based on stream id, component id, and stream grouping function.

LANGUAGE: clojure
CODE:
(github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L198)

----------------------------------------

TITLE: Configuring MongoLookupBolt for Storm-MongoDB Integration
DESCRIPTION: Demonstrates how to configure and use the MongoLookupBolt for querying data from MongoDB. It requires specifying the MongoDB URL, collection name, a QueryFilterCreator, and a MongoLookupMapper implementation.

LANGUAGE: java
CODE:
MongoLookupMapper mapper = new SimpleMongoLookupMapper()
        .withFields("word", "count");

QueryFilterCreator filterCreator = new SimpleQueryFilterCreator()
        .withField("word");

MongoLookupBolt lookupBolt = new MongoLookupBolt(url, collectionName, filterCreator, mapper);

----------------------------------------

TITLE: Implementing ISpout Interface in Java for Storm
DESCRIPTION: This code snippet shows the ISpout interface that spouts must implement in Storm. It includes methods for lifecycle management, tuple emission, and handling acks and fails.

LANGUAGE: java
CODE:
public interface ISpout extends Serializable {
    void open(Map conf, TopologyContext context, SpoutOutputCollector collector);
    void close();
    void nextTuple();
    void ack(Object msgId);
    void fail(Object msgId);
}

----------------------------------------

TITLE: Configuring Pacemaker Servers
DESCRIPTION: YAML configuration for specifying Pacemaker server hosts.

LANGUAGE: yaml
CODE:
pacemaker.servers:
    - somehost.mycompany.com
    - someotherhost.mycompany.com

----------------------------------------

TITLE: Configuring Generic Cluster Resources in YAML
DESCRIPTION: This YAML configuration snippet shows how to specify the availability of generic resources on a Storm supervisor node. It defines the resource type and amount in the storm.yaml file.

LANGUAGE: yaml
CODE:
supervisor.resources.map: {[type<String>] : [amount<Double>]}

LANGUAGE: yaml
CODE:
supervisor.resources.map: {"gpu.count" : 2.0}

----------------------------------------

TITLE: Implementing User Defined Function in Java
DESCRIPTION: Java class implementing a scalar user defined function for Storm SQL.

LANGUAGE: java
CODE:
  public class MyPlus {
    public static Integer evaluate(Integer x, Integer y) {
      return x + y;
    }
  }

----------------------------------------

TITLE: Task Output Determination in Apache Storm (Clojure)
DESCRIPTION: This code implements the 'tasks-fn' that returns the task ids to send tuples to for either regular stream emit or direct stream emit.

LANGUAGE: clojure
CODE:
(github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L207)

----------------------------------------

TITLE: Storm Shell Python Execution Format
DESCRIPTION: Format showing how Storm shell executes the Python topology script with additional system arguments for Nimbus connection and jar location.

LANGUAGE: shell
CODE:
python topology.py arg1 arg2 {nimbus-host} {nimbus-port} {uploaded-jar-location}

----------------------------------------

TITLE: Clearing Dynamic Log Level for ROOT Logger
DESCRIPTION: This command clears the dynamic log level setting for the ROOT logger, reverting it to its original value in the 'my_topology' Storm topology.

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -r ROOT

----------------------------------------

TITLE: Node Resource Configuration in Storm YAML
DESCRIPTION: Configuration for specifying available memory and CPU resources on a node

LANGUAGE: yaml
CODE:
supervisor.memory.capacity.mb: 20480.0
supervisor.cpu.capacity: 100.0

----------------------------------------

TITLE: Initializing EsIndexBolt for Elasticsearch in Java
DESCRIPTION: Creates an EsIndexBolt to stream tuples directly into Elasticsearch. Requires an EsConfig for cluster configuration and an EsTupleMapper to extract necessary fields from input tuples.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsIndexBolt indexBolt = new EsIndexBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Creating IConfigLoader Instance in Java
DESCRIPTION: Method to create an IConfigLoader instance based on the scheme of the scheduler.config.loader.uri configuration.

LANGUAGE: java
CODE:
ConfigLoaderFactoryService.createConfigLoader(Map<String, Object> conf)

----------------------------------------

TITLE: Implementing HTML Redirect and Canonical Link for Apache Storm Documentation
DESCRIPTION: This HTML snippet sets up an automatic redirect to the current Apache Storm Trident state documentation page and establishes a canonical URL for search engines. The meta refresh tag triggers an immediate redirect, while the link tag indicates the preferred URL for indexing.

LANGUAGE: HTML
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Trident-state.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Trident-state.html" />

----------------------------------------

TITLE: Implementing StateUpdater for Custom State in Trident
DESCRIPTION: Implementation of a StateUpdater to update location information in a custom LocationDB state.

LANGUAGE: java
CODE:
public class LocationUpdater extends BaseStateUpdater<LocationDB> {
    public void updateState(LocationDB state, List<TridentTuple> tuples, TridentCollector collector) {
        List<Long> ids = new ArrayList<Long>();
        List<String> locations = new ArrayList<String>();
        for(TridentTuple t: tuples) {
            ids.add(t.getLong(0));
            locations.add(t.getString(1));
        }
        state.setLocationsBulk(ids, locations);
    }
}

----------------------------------------

TITLE: Chaining Map and FlatMap Operations in Trident
DESCRIPTION: Example of chaining flatMap and map operations to convert sentences to uppercase words.

LANGUAGE: java
CODE:
mystream.flatMap(new Split()).map(new UpperCase())

----------------------------------------

TITLE: Registering Event Loggers in YAML Configuration
DESCRIPTION: This code snippet demonstrates how to register multiple event logger implementations in the storm.yaml configuration file.

LANGUAGE: yaml
CODE:
topology.event.logger.register:
  - class: "org.apache.storm.metric.FileBasedEventLogger"
  - class: "org.mycompany.MyEventLogger"
    arguments:
      endpoint: "event-logger.mycompany.org"

----------------------------------------

TITLE: Configuring JdbcTridentState for Lookups in Java
DESCRIPTION: Shows how to configure a JdbcTridentState for lookup operations in Trident topologies. It demonstrates specifying the connection provider, lookup mapper, and select query.

LANGUAGE: java
CODE:
JdbcState.Options options = new JdbcState.Options()
        .withConnectionProvider(connectionProvider)
        .withJdbcLookupMapper(new SimpleJdbcLookupMapper(new Fields("user_name"), Lists.newArrayList(new Column("user_id", Types.INTEGER))))
        .withSelectQuery("select user_name from user_details where user_id = ?");
        .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Configuring Kerberos Authentication for Pacemaker Server
DESCRIPTION: This snippet shows the JAAS configuration for Kerberos authentication on the Pacemaker server. It includes settings for the keytab and principal.

LANGUAGE: java
CODE:
PacemakerServer {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/etc/keytabs/pacemaker.keytab"
   storeKey=true
   useTicketCache=false
   principal="pacemaker@MY.COMPANY.COM";
};

----------------------------------------

TITLE: Chaining Map and FlatMap Operations in Trident
DESCRIPTION: Example of chaining flatMap and map operations to convert sentences to uppercase words.

LANGUAGE: java
CODE:
mystream.flatMap(new Split()).map(new UpperCase())

----------------------------------------

TITLE: Rebalancing a Storm Topology
DESCRIPTION: Redistributes workers for a topology and optionally changes parallelism, with customizable wait time and worker/executor counts.

LANGUAGE: shell
CODE:
storm rebalance topology-name [-w wait-time-secs] [-n new-num-workers] [-e component=parallelism]*

----------------------------------------

TITLE: Implementing Sentence Spout in Clojure
DESCRIPTION: Example spout implementation that emits random sentences from a predefined list.

LANGUAGE: clojure
CODE:
(defspout sentence-spout ["sentence"]
  [conf context collector]
  (let [sentences ["a little brown dog"
                   "the man petted the dog"
                   "four score and seven years ago"
                   "an apple a day keeps the doctor away"]]
    (spout
     (nextTuple []
       (Thread/sleep 100)
       (emit-spout! collector [(rand-nth sentences)])         
       )
     (ack [id]
        ))))

----------------------------------------

TITLE: Launching Storm Nimbus Daemon
DESCRIPTION: Starts the Nimbus daemon for Storm cluster management.

LANGUAGE: shell
CODE:
storm nimbus

----------------------------------------

TITLE: Configuring MongoUpdateBolt for Storm-MongoDB Integration in Java
DESCRIPTION: Demonstrates how to configure and create a MongoUpdateBolt, which is used to update data in MongoDB collections based on Storm tuples. It requires a MongoDB URI, collection name, a QueryFilterCreator, and a MongoMapper implementation.

LANGUAGE: java
CODE:
MongoMapper mapper = new SimpleMongoUpdateMapper()
        .withFields("word", "count");

QueryFilterCreator updateQueryCreator = new SimpleQueryFilterCreator()
        .withField("word");

MongoUpdateBolt updateBolt = new MongoUpdateBolt(url, collectionName, updateQueryCreator, mapper);

//if a new document should be inserted if there are no matches to the query filter
//updateBolt.withUpsert(true);

----------------------------------------

TITLE: Clearing Dynamic Log Level for ROOT Logger
DESCRIPTION: This command clears the dynamic log level setting for the ROOT logger, reverting it to its original value in the 'my_topology' Storm topology.

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -r ROOT

----------------------------------------

TITLE: Configuring Worker-Level Metrics in YAML
DESCRIPTION: Example showing how to configure custom worker-level metrics in storm.yaml configuration file.

LANGUAGE: yaml
CODE:
worker.metrics: 
  metricA: "aaa.bbb.ccc.ddd.MetricA"
  metricB: "aaa.bbb.ccc.ddd.MetricB"
  ...

----------------------------------------

TITLE: Defining ConnectionProvider Interface in Java
DESCRIPTION: This code snippet defines the ConnectionProvider interface, which should be implemented by different connection pooling mechanisms. It includes methods for preparing, getting a connection, and cleaning up.

LANGUAGE: java
CODE:
public interface ConnectionProvider extends Serializable {
    /**
     * method must be idempotent.
     */
    void prepare();

    /**
     *
     * @return a DB connection over which the queries can be executed.
     */
    Connection getConnection();

    /**
     * called once when the system is shutting down, should be idempotent.
     */
    void cleanup();
}

----------------------------------------

TITLE: Artifactory Configuration Example
DESCRIPTION: Example configuration for loading from an Artifactory server with timeout settings.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "artifactory+http://artifactory.my.company.com:8000/artifactory/configurations/clusters/my_cluster/ras_pools"
scheduler.config.loader.timeout.sec: 30

----------------------------------------

TITLE: Implementing SimpleMongoLookupMapper for Storm-MongoDB Integration
DESCRIPTION: This code implements the SimpleMongoLookupMapper class, which converts MongoDB documents to lists of Storm values and declares output fields.

LANGUAGE: java
CODE:
public class SimpleMongoLookupMapper implements MongoLookupMapper {

    private String[] fields;

    @Override
    public List<Values> toTuple(ITuple input, Document doc) {
        Values values = new Values();

        for(String field : fields) {
            if(input.contains(field)) {
                values.add(input.getValueByField(field));
            } else {
                values.add(doc.get(field));
            }
        }
        List<Values> result = new ArrayList<Values>();
        result.add(values);
        return result;
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields(fields));
    }

    public SimpleMongoLookupMapper withFields(String... fields) {
        this.fields = fields;
        return this;
    }

}

----------------------------------------

TITLE: Storm Metrics Filter Interface
DESCRIPTION: Interface definition for creating custom metric filters in Storm. Implements methods for preparation and metric matching.

LANGUAGE: java
CODE:
public interface StormMetricsFilter extends MetricFilter {

    /**
     * Called after the filter is instantiated.
     * @param config A map of the properties from the 'filter' section of the reporter configuration.
     */
    void prepare(Map<String, Object> config);
    
   /**
    *  Returns true if the given metric should be reported.
    */
    boolean matches(String name, Metric metric);

}

----------------------------------------

TITLE: Creating External Table in Storm SQL
DESCRIPTION: SQL syntax for creating external tables to specify data sources, following Hive DDL pattern.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE table_name field_list
    [ STORED AS
      INPUTFORMAT input_format_classname
      OUTPUTFORMAT output_format_classname
    ]
    LOCATION location
    [ PARALLELISM parallelism ]
    [ TBLPROPERTIES tbl_properties ]
    [ AS select_stmt ]

----------------------------------------

TITLE: Setting Topology Scheduling Strategy in Java
DESCRIPTION: Java API call to set the scheduling strategy for a Storm topology.

LANGUAGE: java
CODE:
conf.setTopologyStrategy(org.apache.storm.scheduler.resource.strategies.scheduling.DefaultResourceAwareStrategy.class);

----------------------------------------

TITLE: Creating External Table in Storm SQL
DESCRIPTION: SQL syntax for creating external tables to specify data sources, following Hive DDL pattern.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE table_name field_list
    [ STORED AS
      INPUTFORMAT input_format_classname
      OUTPUTFORMAT output_format_classname
    ]
    LOCATION location
    [ PARALLELISM parallelism ]
    [ TBLPROPERTIES tbl_properties ]
    [ AS select_stmt ]

----------------------------------------

TITLE: Defining Spouts and Bolts in Flux YAML
DESCRIPTION: Examples of defining spouts and bolts in Flux YAML configuration.

LANGUAGE: yaml
CODE:
spouts:
  - id: "sentence-spout"
    className: "org.apache.storm.flux.wrappers.spouts.FluxShellSpout"
    # shell spout constructor takes 2 arguments: String[], String[]
    constructorArgs:
      # command line
      - ["node", "randomsentence.js"]
      # output fields
      - ["word"]
    parallelism: 1

bolts:
  - id: "splitsentence"
    className: "org.apache.storm.flux.wrappers.bolts.FluxShellBolt"
    constructorArgs:
      # command line
      - ["python", "splitsentence.py"]
      # output fields
      - ["word"]
    parallelism: 1

  - id: "log"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1

  - id: "count"
    className: "org.apache.storm.testing.TestWordCounter"
    parallelism: 1

----------------------------------------

TITLE: Using Murmur3StreamGrouping for Optimized Cassandra Writes
DESCRIPTION: Shows how to use Murmur3StreamGrouping to optimize Cassandra writes by partitioning the stream based on row partition keys.

LANGUAGE: java
CODE:
CassandraWriterBolt bolt = new CassandraWriterBolt(
    insertInto("album")
        .values(
            with(fields("title", "year", "performer", "genre", "tracks")
            ).build()));
builder.setBolt("BOLT_WRITER", bolt, 4)
        .customGrouping("spout", new Murmur3StreamGrouping("title"))

----------------------------------------

TITLE: Named Stream Join Configuration
DESCRIPTION: Examples showing how to join named streams instead of default streams using JoinBolt.

LANGUAGE: java
CODE:
new JoinBolt(JoinBolt.Selector.STREAM,  "stream1", "key1")
                                  .join("stream2", "key2")
    ...

LANGUAGE: java
CODE:
new JoinBolt(JoinBolt.Selector.STREAM,  "stream1", "key1") 
                             .join     ("stream2", "userId",  "stream1" )
                             .select ("userId, key1, key2")
                             .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;
                             
topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("bolt1", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt2", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt3", "stream2", new Fields("userId") )
            .fieldsGrouping("bolt4", "stream1", new Fields("key1") );

----------------------------------------

TITLE: Storm Topology Submission Method Definition
DESCRIPTION: Thrift method definition for submitting a topology to Storm, including parameters for name, jar location, configuration, and topology structure.

LANGUAGE: thrift
CODE:
void submitTopology(1: string name, 2: string uploadedJarLocation, 3: string jsonConf, 4: StormTopology topology)
    throws (1: AlreadyAliveException e, 2: InvalidTopologyException ite);

----------------------------------------

TITLE: Creating CassandraWriterBolt with Logged Batch Statement in Java
DESCRIPTION: Shows how to create a CassandraWriterBolt that uses a logged batch statement to execute multiple queries atomically.

LANGUAGE: java
CODE:
new CassandraWriterBolt(loggedBatch(
        simpleQuery("INSERT INTO titles_per_album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all())),
        simpleQuery("INSERT INTO titles_per_performer (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all()))
    )
);

----------------------------------------

TITLE: Stopping Profiler on Topology Worker in JSON
DESCRIPTION: Example response from the /api/v1/topology/<id>/profiling/stop/<host-port> endpoint, which stops the profiler on a worker for a topology.

LANGUAGE: json
CODE:
{
  "status": "ok",
  "id": "10.11.1.7:6701"
}

----------------------------------------

TITLE: Maven Shade Plugin Configuration for Storm-HDFS
DESCRIPTION: Maven configuration for properly packaging a Storm topology with HDFS dependencies using the maven-shade-plugin.

LANGUAGE: xml
CODE:
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-shade-plugin</artifactId>
    <version>1.4</version>
    <configuration>
        <createDependencyReducedPom>true</createDependencyReducedPom>
    </configuration>
    <executions>
        <execution>
            <phase>package</phase>
            <goals>
                <goal>shade</goal>
            </goals>
            <configuration>
                <transformers>
                    <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                    <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                        <mainClass></mainClass>
                    </transformer>
                </transformers>
            </configuration>
        </execution>
    </executions>
</plugin>

----------------------------------------

TITLE: Maven Dependencies Configuration
DESCRIPTION: Maven dependency configuration for Storm Kinesis integration, including AWS SDK, Storm Core, Curator Framework, and JSON Simple libraries.

LANGUAGE: xml
CODE:
 <dependencies>
        <dependency>
            <groupId>com.amazonaws</groupId>
            <artifactId>aws-java-sdk</artifactId>
            <version>${aws-java-sdk.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-core</artifactId>
            <version>${project.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-framework</artifactId>
            <version>${curator.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>com.googlecode.json-simple</groupId>
            <artifactId>json-simple</artifactId>
        </dependency>
 </dependencies>

----------------------------------------

TITLE: Activating Topology in JSON
DESCRIPTION: Example response from the /api/v1/topology/<id>/activate POST endpoint, which activates a topology.

LANGUAGE: json
CODE:
{
  "topologyOperation":"activate",
  "topologyId":"wordcount-1-1420308665",
  "status":"success"
}

----------------------------------------

TITLE: Implementing DRPC Query Stream in Java
DESCRIPTION: Creates a distributed query stream for processing word count queries in parallel.

LANGUAGE: java
CODE:
topology.newDRPCStream("words")
       .each(new Fields("args"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .stateQuery(wordCounts, new Fields("word"), new MapGet(), new Fields("count"))
       .each(new Fields("count"), new FilterNull())
       .aggregate(new Fields("count"), new Sum(), new Fields("sum"));

----------------------------------------

TITLE: Applying a Filter to a Trident Stream
DESCRIPTION: Example of applying a Filter to a Trident stream.

LANGUAGE: java
CODE:
mystream.filter(new MyFilter())

----------------------------------------

TITLE: Storm Protocol Ack Command Format
DESCRIPTION: JSON structure for acknowledging tuple processing in Storm's multilang protocol. Contains command type and tuple ID.

LANGUAGE: json
CODE:
{
    "command": "ack",
    "id": 123123
}

----------------------------------------

TITLE: Storm Nimbus Metrics Table Schema
DESCRIPTION: Table structure defining Nimbus-specific metrics that track nimbus operations and state.

LANGUAGE: markdown
CODE:
| Metric Name | Type | Description |
|-------------|------|-------------|
| nimbus:files-upload-duration-ms | timer | Time it takes to upload a file... |

----------------------------------------

TITLE: Configuring Shell Component with Blob Store in Java
DESCRIPTION: Example showing how to configure a ShellSpout to work with executables and scripts shipped via the blob store dist cache by modifying the child working directory.

LANGUAGE: java
CODE:
public MyShellSpout() {
    super("./newPython/bin/python", "./shell_spout.py");
    changeChildCWD(false);
}

----------------------------------------

TITLE: Running a Flux Topology
DESCRIPTION: Command to run a Flux topology using the storm jar command.

LANGUAGE: bash
CODE:
storm jar myTopology-0.1.0-SNAPSHOT.jar org.apache.storm.flux.Flux --local my_config.yaml

----------------------------------------

TITLE: Configuring Shell Component with Blob Store in Java
DESCRIPTION: Example showing how to configure a ShellSpout to work with executables and scripts shipped via the blob store dist cache by modifying the child working directory.

LANGUAGE: java
CODE:
public MyShellSpout() {
    super("./newPython/bin/python", "./shell_spout.py");
    changeChildCWD(false);
}

----------------------------------------

TITLE: Defining Leader Election Interface in Java
DESCRIPTION: Interface definition for the leader election process in Storm Nimbus. It includes methods for joining and leaving the leader queue, checking leadership status, and getting information about current leaders and nimbus addresses.

LANGUAGE: java
CODE:
public interface ILeaderElector {
    /**
     * queue up for leadership lock. The call returns immediately and the caller                     
     * must check isLeader() to perform any leadership action.
     */
    void addToLeaderLockQueue();

    /**
     * Removes the caller from the leader lock queue. If the caller is leader
     * also releases the lock.
     */
    void removeFromLeaderLockQueue();

    /**
     *
     * @return true if the caller currently has the leader lock.
     */
    boolean isLeader();

    /**
     *
     * @return the current leader's address , throws exception if noone has has    lock.
     */
    InetSocketAddress getLeaderAddress();

    /**
     * 
     * @return list of current nimbus addresses, includes leader.
     */
    List<InetSocketAddress> getAllNimbusAddresses();
}

----------------------------------------

TITLE: File-based Configuration Example
DESCRIPTION: Example configuration for loading from a local file system.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "file:///path/to/my/config.yaml"

----------------------------------------

TITLE: Implementing HiveState with Trident
DESCRIPTION: Example of implementing HiveState for Trident topologies with custom mapping and configuration options.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withTimeAsPartitionField("YYYY/MM/DD");
            
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
                                .withTxnsPerBatch(10)
                				.withBatchSize(1000)
                	     		.withIdleTimeout(10)
                	     		
StateFactory factory = new HiveStateFactory().withOptions(hiveOptions);
TridentState state = stream.partitionPersist(factory, hiveFields, new HiveUpdater(), new Fields());

----------------------------------------

TITLE: Configuring Kerberos Authentication for Pacemaker Server
DESCRIPTION: JAAS configuration for Kerberos authentication on Pacemaker node for Pacemaker server.

LANGUAGE: java
CODE:
PacemakerServer {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/etc/keytabs/pacemaker.keytab"
   storeKey=true
   useTicketCache=false
   principal="pacemaker@MY.COMPANY.COM";
};

----------------------------------------

TITLE: Worker-Launcher Configuration
DESCRIPTION: Example configuration for the worker-launcher executable, which is used to launch Docker containers and run Docker and nsenter commands.

LANGUAGE: properties
CODE:
storm.worker-launcher.group=$(worker_launcher_group)
min.user.id=$(min_user_id)
worker.profiler.script.path=$(profiler_script_path)

----------------------------------------

TITLE: Defining submitTopology Method in Java for Storm
DESCRIPTION: This Java method signature shows the Thrift API for submitting a topology to Nimbus. It requires the topology name, uploaded jar location, JSON configuration, and the StormTopology object.

LANGUAGE: java
CODE:
void submitTopology(
        1: string name,
        2: string uploadedJarLocation,
        3: string jsonConf,
        4: StormTopology topology)
        throws (
                1: AlreadyAliveException e,
                2: InvalidTopologyException ite);

----------------------------------------

TITLE: Kerberos Server Authentication Configuration
DESCRIPTION: JAAS configuration for Pacemaker server authentication using Kerberos.

LANGUAGE: java
CODE:
PacemakerServer {\n   com.sun.security.auth.module.Krb5LoginModule required\n   useKeyTab=true\n   keyTab="/etc/keytabs/pacemaker.keytab"\n   storeKey=true\n   useTicketCache=false\n   principal="pacemaker@MY.COMPANY.COM";\n};

----------------------------------------

TITLE: Submitting Storm Topology
DESCRIPTION: Command to submit the sample topology to Storm using the Event Hubs spout

LANGUAGE: bash
CODE:
storm jar {jarfile} com.microsoft.eventhubs.samples.EventCount {topologyname} {spoutconffile}

----------------------------------------

TITLE: DRPC Server Configuration
DESCRIPTION: YAML configuration for setting up DRPC servers including server locations and transport settings.

LANGUAGE: yaml
CODE:
drpc.servers:
  - "drpc1.foo.com"
  - "drpc2.foo.com"
drpc.http.port: 8081
storm.thrift.transport: "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin"

----------------------------------------

TITLE: Defining QueryFilterCreator Interface for Storm-MongoDB Integration
DESCRIPTION: This snippet defines the QueryFilterCreator interface, which is the main API for creating MongoDB query filters based on Storm tuples.

LANGUAGE: java
CODE:
public interface QueryFilterCreator extends Serializable {
    Bson createFilter(ITuple tuple);
}

----------------------------------------

TITLE: Implementing Skewed Streaming Top N Pattern in Storm
DESCRIPTION: Enhanced pattern for handling skewed data in streaming top N calculations using partial key grouping.

LANGUAGE: java
CODE:
builder.setBolt("count", new CountObjects(), parallelism)
  .partialKeyGrouping("objects", new Fields("value"));
builder.setBolt("rank" new AggregateCountsAndRank(), parallelism)
  .fieldsGrouping("count", new Fields("key"))
builder.setBolt("merge", new MergeRanksObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Configuring Pacemaker State Storage in Storm
DESCRIPTION: Sets the cluster state store to use the PaceMakerStateStorageFactory in the Storm configuration.

LANGUAGE: yaml
CODE:
storm.cluster.state.store: "org.apache.storm.cluster.PaceMakerStateStorageFactory"

----------------------------------------

TITLE: Implementing StateUpdater for Location Database Updates
DESCRIPTION: Example of implementing a StateUpdater to update user locations in the custom LocationDB State in batches.

LANGUAGE: java
CODE:
public class LocationUpdater extends BaseStateUpdater<LocationDB> {
    public void updateState(LocationDB state, List<TridentTuple> tuples, TridentCollector collector) {
        List<Long> ids = new ArrayList<Long>();
        List<String> locations = new ArrayList<String>();
        for(TridentTuple t: tuples) {
            ids.add(t.getLong(0));
            locations.add(t.getString(1));
        }
        state.setLocationsBulk(ids, locations);
    }
}

----------------------------------------

TITLE: Configuring YAML Front Matter for Storm Documentation
DESCRIPTION: YAML front matter for the Storm documentation page, specifying the title, layout, and documentation flag.

LANGUAGE: yaml
CODE:
---
title: Performance Tuning
layout: documentation
documentation: true
---

----------------------------------------

TITLE: Setting Log Level via Storm CLI
DESCRIPTION: Command to set a log level for a specific logger in a Storm topology using the CLI. It allows specifying the topology name, logger name, log level, and an optional timeout.

LANGUAGE: bash
CODE:
./bin/storm set_log_level [topology name] -l [logger name]=[LEVEL]:[TIMEOUT]

----------------------------------------

TITLE: Applying a FlatMapFunction to a Trident Stream
DESCRIPTION: Example of applying the Split FlatMapFunction to a stream.

LANGUAGE: java
CODE:
mystream.flatMap(new Split())

----------------------------------------

TITLE: Configuring EsState for Trident Integration
DESCRIPTION: Sets up an Elasticsearch state factory for Trident streams using specified configuration and mapper.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();

StateFactory factory = new EsStateFactory(esConfig, tupleMapper);
TridentState state = stream.partitionPersist(factory, esFields, new EsUpdater(), new Fields());

----------------------------------------

TITLE: Defining a Parameterized Spout in Clojure
DESCRIPTION: Example of an unprepared, parameterized spout that emits random sentences from a provided list. It demonstrates using :params and :prepare false options.

LANGUAGE: clojure
CODE:
(defspout sentence-spout-parameterized ["word"] {:params [sentences] :prepare false}
  [collector]
  (Thread/sleep 500)
  (emit-spout! collector [(rand-nth sentences)]))

----------------------------------------

TITLE: Redirecting to Storm Documentation on Kestrel Integration
DESCRIPTION: This HTML snippet uses a meta refresh tag to automatically redirect the user to the current Storm documentation page about Kestrel and Storm integration. It also includes a canonical link tag for search engine optimization.

LANGUAGE: HTML
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Kestrel-and-Storm.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Kestrel-and-Storm.html" />

----------------------------------------

TITLE: Worker-Launcher Configuration
DESCRIPTION: Example configuration for the worker-launcher executable, which is used to launch Docker containers and run Docker and nsenter commands.

LANGUAGE: properties
CODE:
storm.worker-launcher.group=$(worker_launcher_group)
min.user.id=$(min_user_id)
worker.profiler.script.path=$(profiler_script_path)

----------------------------------------

TITLE: Topology Submission with Blob Mapping
DESCRIPTION: Command to submit a topology with blob mapping configuration that specifies local file names and compression settings.

LANGUAGE: bash
CODE:
storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar org.apache.storm.starter.clj.word_count test_topo -c topology.blobstore.map='{"key1":{"localname":"blob_file", "uncompress":false},"key2":{}}'

----------------------------------------

TITLE: Dequeuing Items from Kestrel in Java
DESCRIPTION: Method to dequeue items from a Kestrel queue without removing them. Retrieves and displays queue items while keeping them in the queue.

LANGUAGE: java
CODE:
    private static void dequeueItems(KestrelClient kestrelClient, String queueName) throws IOException, ParseError
			 {
		for(int i=1; i<=12; i++){

			Item item = kestrelClient.dequeue(queueName);

			if(item==null){
				System.out.println("The queue (" + queueName + ") contains no items.");
			}
			else
			{
				byte[] data = item._data;

				String receivedVal = new String(data);

				System.out.println("receivedItem=" + receivedVal);
			}
		}

----------------------------------------

TITLE: Basic Bolt Implementation Using IBasicBolt
DESCRIPTION: Simplified bolt implementation using IBasicBolt interface which handles anchoring and acknowledgment automatically

LANGUAGE: java
CODE:
public class SplitSentence extends BaseBasicBolt {
        public void execute(Tuple tuple, BasicOutputCollector collector) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                collector.emit(new Values(word));
            }
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Storm Supervisor Metrics Table Schema
DESCRIPTION: Table structure defining metrics for supervisor processes that manage worker lifecycles.

LANGUAGE: markdown
CODE:
| Metric Name | Type | Description |
|-------------|------|-------------|
| supervisor:blob-cache-update-duration | timer | how long it takes to update... |

----------------------------------------

TITLE: Task Message Listening in Apache Storm (Clojure)
DESCRIPTION: Tasks listen on an in-memory ZeroMQ port for messages from the virtual port.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L201

----------------------------------------

TITLE: SQL Setup for Storm-JDBC Integration Example
DESCRIPTION: This SQL snippet shows the setup queries required for the example topologies. It includes creating tables, inserting initial data, and a sample select query.

LANGUAGE: sql
CODE:
create table if not exists user (user_id integer, user_name varchar(100), dept_name varchar(100), create_date date);
create table if not exists department (dept_id integer, dept_name varchar(100));
create table if not exists user_department (user_id integer, dept_id integer);
insert into department values (1, 'R&D');
insert into department values (2, 'Finance');
insert into department values (3, 'HR');
insert into department values (4, 'Sales');
insert into user_department values (1, 1);
insert into user_department values (2, 2);
insert into user_department values (3, 3);
insert into user_department values (4, 4);
select dept_name from department, user_department where department.dept_id = user_department.dept_id and user_department.user_id = ?;

----------------------------------------

TITLE: Deactivating Topology in JSON
DESCRIPTION: Example response from the /api/v1/topology/<id>/deactivate POST endpoint, which deactivates a topology.

LANGUAGE: json
CODE:
{
  "topologyOperation":"deactivate",
  "topologyId":"wordcount-1-1420308665",
  "status":"success"
}

----------------------------------------

TITLE: Displaying Storm Command Help
DESCRIPTION: Shows help information for a specific Storm command or lists all available commands.

LANGUAGE: shell
CODE:
storm help [command]

----------------------------------------

TITLE: Defining a Simple Bolt in Clojure
DESCRIPTION: Example of defining a simple bolt using defbolt that splits a sentence into words. It demonstrates how to emit and ack tuples.

LANGUAGE: clojure
CODE:
(defbolt split-sentence ["word"] [tuple collector]
  (let [words (.split (.getString tuple 0) " ")]
    (doseq [w words]
      (emit-bolt! collector [w] :anchor tuple))
    (ack! collector tuple)
    ))

----------------------------------------

TITLE: Continuous Item Addition to Kestrel
DESCRIPTION: Main program for continuously adding sentence items to a Kestrel queue. Runs until a closing bracket character is entered in the console.

LANGUAGE: java
CODE:
    import java.io.IOException;
    import java.io.InputStream;
    import java.util.Random;

    import org.apache.storm.spout.KestrelClient;
    import org.apache.storm.spout.KestrelClient.Item;
    import org.apache.storm.spout.KestrelClient.ParseError;

    public class AddSentenceItemsToKestrel {

    	/**
    	 * @param args
    	 */
    	public static void main(String[] args) {

    		InputStream is = System.in;

			char closing_bracket = ']';

			int val = closing_bracket;

			boolean aux = true;

			try {

				KestrelClient kestrelClient = null;
				String queueName = "sentence_queue";

				while(aux){

					kestrelClient = new KestrelClient("localhost",22133);

					queueSentenceItems(kestrelClient, queueName);

					kestrelClient.close();

					Thread.sleep(1000);

					if(is.available()>0){
					 if(val==is.read())
						 aux=false;
					}
				}
			} catch (IOException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			catch (ParseError e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			} catch (InterruptedException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}

			System.out.println("end");

	    }
	}

----------------------------------------

TITLE: Creating Kerberos Principals and Keytabs
DESCRIPTION: Bash commands for creating Kerberos principals and keytabs for Zookeeper, Nimbus, DRPC, UI, and Supervisors

LANGUAGE: bash
CODE:
# Zookeeper (Will need one of these for each box in the Zk ensemble)
sudo kadmin.local -q 'addprinc zookeeper/zk1.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/zk.keytab  zookeeper/zk1.example.com@STORM.EXAMPLE.COM"
# Nimbus and DRPC
sudo kadmin.local -q 'addprinc storm/storm.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm/storm.example.com@STORM.EXAMPLE.COM"
# All UI logviewer and Supervisors
sudo kadmin.local -q 'addprinc storm@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm@STORM.EXAMPLE.COM"

----------------------------------------

TITLE: Creating Custom Configuration Class for Storm Plugins
DESCRIPTION: Illustrates how to create a custom configuration class for Storm plugins or custom schedulers. This class implements the Validated interface and uses annotations for config validation.

LANGUAGE: java
CODE:
public class CustomConfig implements Validated {
    @isPositiveNumber
    public static final String CUSTOM_PARAM = "custom.param";
    
    // Other custom configurations
}

----------------------------------------

TITLE: Defining User-Defined Function in Storm SQL
DESCRIPTION: Example of creating a user-defined function named MYPLUS using a custom Java class.

LANGUAGE: sql
CODE:
CREATE FUNCTION MYPLUS AS 'org.apache.storm.sql.TestUtils$MyPlus'

----------------------------------------

TITLE: Implementing Custom Task-Level Metrics
DESCRIPTION: Example showing how to implement and register a custom CountMetric at the task level in a bolt.

LANGUAGE: java
CODE:
private transient CountMetric countMetric;

LANGUAGE: java
CODE:
@Override
public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
	// other initialization here.
	countMetric = new CountMetric();
	context.registerMetric("execute_count", countMetric, 60);
}

LANGUAGE: java
CODE:
public void execute(Tuple input) {
	countMetric.incr();
	// handle tuple here.	
}

----------------------------------------

TITLE: Defining Leader Election Interface in Java for Storm Nimbus
DESCRIPTION: Java interface defining methods for leader election, including adding/removing from the leader lock queue, checking leadership status, and retrieving leader and nimbus addresses.

LANGUAGE: java
CODE:
public interface ILeaderElector {
    void addToLeaderLockQueue();
    void removeFromLeaderLockQueue();
    boolean isLeader();
    InetSocketAddress getLeaderAddress();
    List<InetSocketAddress> getAllNimbusAddresses();
}

----------------------------------------

TITLE: Configuring SimpleJdbcLookupMapper in Java
DESCRIPTION: This code snippet shows how to configure SimpleJdbcLookupMapper for use with JdbcLookupBolt, specifying output fields and query parameter columns.

LANGUAGE: java
CODE:
Fields outputFields = new Fields("user_id", "user_name", "create_date");
List<Column> queryParamColumns = Lists.newArrayList(new Column("user_id", Types.INTEGER));
this.jdbcLookupMapper = new SimpleJdbcLookupMapper(outputFields, queryParamColumns);

----------------------------------------

TITLE: Adding Generic Resources to Storm Components in Java
DESCRIPTION: This snippet demonstrates how to add generic resource requirements to a Storm topology component using the addResource method. It specifies the resource name and value for a spout declaration.

LANGUAGE: java
CODE:
public T addResource(String resourceName, Number resourceValue)

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.addResouce("gpu.count", 1.0);

----------------------------------------

TITLE: Retrieving Cluster Configuration in Storm UI REST API
DESCRIPTION: GET request to retrieve the cluster configuration. Returns a JSON object with configuration key-value pairs.

LANGUAGE: json
CODE:
{
  "dev.zookeeper.path": "/tmp/dev-storm-zookeeper",
  "topology.tick.tuple.freq.secs": null,
  "topology.builtin.metrics.bucket.size.secs": 60,
  "topology.fall.back.on.java.serialization": false,
  "topology.max.error.report.per.interval": 5,
  "zmq.linger.millis": 5000,
  "topology.skip.missing.kryo.registrations": false,
  "storm.messaging.netty.client_worker_threads": 1,
  "ui.childopts": "-Xmx768m",
  "storm.zookeeper.session.timeout": 20000,
  "nimbus.reassign": true,
  "topology.trident.batch.emit.interval.millis": 500,
  "storm.messaging.netty.flush.check.interval.ms": 10,
  "nimbus.monitor.freq.secs": 10,
  "logviewer.childopts": "-Xmx128m",
  "java.library.path": "/usr/local/lib:/opt/local/lib:/usr/lib",
  "topology.executor.send.buffer.size": 1024
}

----------------------------------------

TITLE: Configuring Trident with Kafka State
DESCRIPTION: Example showing how to use Kafka state factory with Trident in Storm to write data to Kafka. Demonstrates Trident stream creation and state configuration.

LANGUAGE: java
CODE:
Fields fields = new Fields("word", "count");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
        new Values("storm", "1"),
        new Values("trident", "1"),
        new Values("needs", "1"),
        new Values("javadoc", "1")
);
spout.setCycle(true);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

//set producer properties.
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

TridentKafkaStateFactory stateFactory = new TridentKafkaStateFactory()
        .withProducerProperties(props)
        .withKafkaTopicSelector(new DefaultTopicSelector("test"))
        .withTridentTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper("word", "count"));
stream.partitionPersist(stateFactory, fields, new TridentKafkaStateUpdater(), new Fields());

Config conf = new Config();
StormSubmitter.submitTopology("kafkaTridentTest", conf, topology.build());

----------------------------------------

TITLE: Submitting Topology with Blob Mapping
DESCRIPTION: Example of submitting a Storm topology with blob mapping configuration.

LANGUAGE: bash
CODE:
storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar org.apache.storm.starter.clj.word_count test_topo -c topology.blobstore.map='{"key1":{"localname":"blob_file", "uncompress":false, "workerRestart":true},"key2":{}}'

----------------------------------------

TITLE: Defining External Tables and Inserting Data in Storm SQL
DESCRIPTION: This SQL snippet demonstrates how to create external tables representing Kafka streams and perform a filtered insert operation. It showcases table creation with location specifications and a SELECT query with filtering and projection.

LANGUAGE: SQL
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://localhost:2181/brokers?topic=orders' ...

CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) LOCATION 'kafka://localhost:2181/brokers?topic=large_orders' ...

INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Initializing Elasticsearch Index Bolt in Storm
DESCRIPTION: Creates an EsIndexBolt instance for streaming tuples into Elasticsearch using cluster configuration and tuple mapping.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsIndexBolt indexBolt = new EsIndexBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Storm Log Command JSON Format
DESCRIPTION: JSON structure for logging messages from a multilang bolt to the worker log. Contains command type and message text.

LANGUAGE: json
CODE:
{
    "command": "log",
    "msg": "hello world!"
}

----------------------------------------

TITLE: Continuous Kestrel Queue Producer
DESCRIPTION: Main program that continuously adds sentences to a Kestrel queue until stopped. Connects to a local Kestrel server and adds items with 1-second intervals.

LANGUAGE: java
CODE:
    import java.io.IOException;
    import java.io.InputStream;
    import java.util.Random;

    import org.apache.storm.spout.KestrelClient;
    import org.apache.storm.spout.KestrelClient.Item;
    import org.apache.storm.spout.KestrelClient.ParseError;

    public class AddSentenceItemsToKestrel {

    	/**
    	 * @param args
    	 */
    	public static void main(String[] args) {

    		InputStream is = System.in;

			char closing_bracket = ']';

			int val = closing_bracket;

			boolean aux = true;

			try {

				KestrelClient kestrelClient = null;
				String queueName = "sentence_queue";

				while(aux){

					kestrelClient = new KestrelClient("localhost",22133);

					queueSentenceItems(kestrelClient, queueName);

					kestrelClient.close();

					Thread.sleep(1000);

					if(is.available()>0){
					 if(val==is.read())
						 aux=false;
					}
				}
			} catch (IOException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			catch (ParseError e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			} catch (InterruptedException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}

			System.out.println("end");

	    }
	}

----------------------------------------

TITLE: Database Table Creation SQL
DESCRIPTION: SQL statements for creating required database tables and inserting initial data for testing.

LANGUAGE: sql
CODE:
create table if not exists user (user_id integer, user_name varchar(100), dept_name varchar(100), create_date date);
create table if not exists department (dept_id integer, dept_name varchar(100));
create table if not exists user_department (user_id integer, dept_id integer);
insert into department values (1, 'R&D');
insert into department values (2, 'Finance');
insert into department values (3, 'HR');
insert into department values (4, 'Sales');
insert into user_department values (1, 1);
insert into user_department values (2, 2);
insert into user_department values (3, 3);
insert into user_department values (4, 4);

----------------------------------------

TITLE: Installing JZMQ Java Bindings
DESCRIPTION: Commands for cloning and installing JZMQ from a specific fork that is tested to work with Storm. JZMQ provides Java bindings for ZeroMQ.

LANGUAGE: bash
CODE:
#install jzmq
git clone https://github.com/nathanmarz/jzmq.git
cd jzmq
./autogen.sh
./configure
make
sudo make install

----------------------------------------

TITLE: Thrift Definition of submitTopology Method for Storm
DESCRIPTION: This Thrift method definition shows the structure of the submitTopology method used to submit a topology to Storm. It includes parameters for topology name, jar location, configuration, and the topology itself.

LANGUAGE: thrift
CODE:
void submitTopology(1: string name, 2: string uploadedJarLocation, 3: string jsonConf, 4: StormTopology topology)
    throws (1: AlreadyAliveException e, 2: InvalidTopologyException ite);

----------------------------------------

TITLE: Setting Log Levels for Storm Topology
DESCRIPTION: Dynamically changes log levels for a running topology, with options for specific loggers and timeouts.

LANGUAGE: shell
CODE:
storm set_log_level -l [logger name]=[log level][:optional timeout] -r [logger name] topology-name

----------------------------------------

TITLE: Implementing ISerialization Interface in Java for Custom Storm Serializers
DESCRIPTION: This code snippet shows the ISerialization interface that must be implemented to create a custom serializer in Storm. It includes methods for accepting a class type, serializing an object to a DataOutputStream, and deserializing an object from a DataInputStream.

LANGUAGE: java
CODE:
public interface ISerialization<T> {
    public boolean accept(Class c);
    public void serialize(T object, DataOutputStream stream) throws IOException;
    public T deserialize(DataInputStream stream) throws IOException;
}

----------------------------------------

TITLE: FlatMap Function for String Splitting
DESCRIPTION: Implementation of a flatMap function that splits sentences into individual words in a Trident stream.

LANGUAGE: java
CODE:
public class Split extends FlatMapFunction {
  @Override
  public Iterable<Values> execute(TridentTuple input) {
    List<Values> valuesList = new ArrayList<>();
    for (String word : input.getString(0).split(" ")) {
      valuesList.add(new Values(word));
    }
    return valuesList;
  }
}

----------------------------------------

TITLE: Worker Process Initialization
DESCRIPTION: Implementation of worker process startup and task management through mk-worker function.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L67

----------------------------------------

TITLE: Creating IConfigLoader Instance in Java
DESCRIPTION: Method to create an IConfigLoader instance based on the scheme of the scheduler.config.loader.uri.

LANGUAGE: java
CODE:
ConfigLoaderFactoryService.createConfigLoader(Map<String, Object> conf)

----------------------------------------

TITLE: Configuring HBase State Provider in Storm Config
DESCRIPTION: Java code example showing how to configure the HBase state provider in Storm's Config object, including HBase configuration and state provider settings.

LANGUAGE: java
CODE:
Config conf = new Config();
    Map<String, Object> hbConf = new HashMap<String, Object>();
    hbConf.put("hbase.rootdir", "file:///tmp/hbase");
    conf.put("hbase.conf", hbConf);
    conf.put("topology.state.provider",  "org.apache.storm.hbase.state.HBaseKeyValueStateProvider");
    conf.put("topology.state.provider.config", "{" +
            "   \"hbaseConfigKey\": \"hbase.conf\"," +
            "   \"tableName\": \"state\"," +
            "   \"columnFamily\": \"cf\"" +
            " }");

----------------------------------------

TITLE: Retrieving Topology Component Details in Storm UI REST API
DESCRIPTION: Example JSON response for the /api/v1/topology/:id/component/:component endpoint, which returns detailed information about a specific component in a topology.

LANGUAGE: json
CODE:
{
    "name": "WordCount3",
    "id": "spout",
    "componentType": "spout",
    "windowHint": "10m 0s",
    "executors": 5,
    "componentErrors":[
        {
            "errorTime": 1406006074000,
            "errorHost": "10.11.1.70",
            "errorPort": 6701,
            "errorWorkerLogLink": "http://10.11.1.7:8000/log?file=worker-6701.log",
            "errorLapsedSecs": 16,
            "error": "java.lang.RuntimeException: java.lang.StringIndexOutOfBoundsException: Some Error\n\tat org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:128)\n\tat org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99)\n\tat org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80)\n\tat backtype...more.."
        }
    ],
    "topologyId": "WordCount3-1-1402960825",
    "tasks": 5,
    "window": "600",
    "spoutSummary": [
        {
            "windowPretty": "10m 0s",
            "window": "600",
            "emitted": 28500,
            "transferred": 28460,
            "completeLatency": "0.000",
            "acked": 0,
            "failed": 0
        }
    ],
    "outputStats": [
        {
            "stream": "default",
            "emitted": 28460,
            "transferred": 28460,
            "completeLatency": "0",
            "acked": 0,
            "failed": 0
        }
    ],
    "executorStats": [
        {
            "workerLogLink": "http://10.11.1.7:8000/log?file=worker-6701.log",
            "emitted": 5720,
            "port": 6701,
            "completeLatency": "0.000",
            "transferred": 5720,
            "host": "10.11.1.7",
            "acked": 0,
            "uptime": "43m 4s",
            "uptimeSeconds": 2584,
            "id": "[24-24]",
            "failed": 0
        }
    ]
}

----------------------------------------

TITLE: DRPC Client Example in Java
DESCRIPTION: Example code showing how to create a DRPC client and execute requests in production. Uses DRPCClient class with proper configuration and resource cleanup.

LANGUAGE: java
CODE:
Config conf = new Config();
try (DRPCClient drpc = DRPCClient.getConfiguredClient(conf)) {
  //User the drpc client
  String result = drpc.execute(function, argument);
}

----------------------------------------

TITLE: Assigning Tasks to Machines in Nimbus (Clojure)
DESCRIPTION: Nimbus uses the mk-assignment function to assign tasks to machines. This creates an Assignment record containing master code directory, task to node+port mapping, node to host mapping, and task start times.

LANGUAGE: clojure
CODE:
(defn mk-assignment [topology-id])

----------------------------------------

TITLE: Registering Task Hook in Java using TopologyContext
DESCRIPTION: This snippet demonstrates how to register a custom task hook in the open method of a spout or prepare method of a bolt using the TopologyContext. The method addTaskHook is used to add the custom hook.

LANGUAGE: java
CODE:
TopologyContext#addTaskHook

----------------------------------------

TITLE: Configuring Pacemaker State Store in Storm
DESCRIPTION: This snippet shows how to configure Storm to use the Pacemaker state store for cluster state management. It sets the cluster state store to use the Pacemaker state factory.

LANGUAGE: yaml
CODE:
storm.cluster.state.store: "org.apache.storm.pacemaker.pacemaker_state_factory"

----------------------------------------

TITLE: Storm Thrift submitTopology Method Definition
DESCRIPTION: Thrift method definition for submitting a topology to Storm, including parameters for name, jar location, configuration and topology structure.

LANGUAGE: thrift
CODE:
void submitTopology(1: string name, 2: string uploadedJarLocation, 3: string jsonConf, 4: StormTopology topology)
    throws (1: AlreadyAliveException e, 2: InvalidTopologyException ite);

----------------------------------------

TITLE: Storm Docker Support Configuration Example
DESCRIPTION: Example configuration for enabling Docker support in Apache Storm on RHEL7. It includes settings for resource isolation, allowed Docker images, cgroup configuration, and worker launcher path.

LANGUAGE: bash
CODE:
storm.resource.isolation.plugin.enable: true
storm.resource.isolation.plugin: "org.apache.storm.container.docker.DockerManager"
storm.oci.allowed.images: ["xxx.xxx.com:8080/storm/storm/rhel7:latest"]
storm.oci.image: "xxx.xxx.com:8080/storm/storm/rhel7:latest"
storm.oci.cgroup.root: "/storm"
storm.oci.cgroup.parent: "/sys/fs/cgroup"
storm.oci.readonly.bindmounts:
    - "/etc/storm"
storm.oci.nscd.dir: "/var/run/nscd"
supervisor.worker.launcher: "/usr/share/apache-storm-2.3.0/bin/worker-launcher"

----------------------------------------

TITLE: MQTT Message Mapper Interface Implementation
DESCRIPTION: Java interface defining how MQTT messages are mapped to Storm tuples

LANGUAGE: java
CODE:
public interface MqttMessageMapper extends Serializable {
    Values toValues(MqttMessage message);
    Fields outputFields();
}

----------------------------------------

TITLE: Configuring Worker-Level Metrics in YAML
DESCRIPTION: Example of registering worker-level metrics in storm.yaml configuration.

LANGUAGE: yaml
CODE:
worker.metrics: 
  metricA: "aaa.bbb.ccc.ddd.MetricA"
  metricB: "aaa.bbb.ccc.ddd.MetricB"
  ...

----------------------------------------

TITLE: Configuring Pacemaker Servers in Storm
DESCRIPTION: This snippet demonstrates how to configure multiple Pacemaker servers in the Storm cluster configuration. It lists the hostnames of the Pacemaker servers.

LANGUAGE: yaml
CODE:
pacemaker.servers:
    - somehost.mycompany.com
    - someotherhost.mycompany.com

----------------------------------------

TITLE: Trident FlatMap Function Example
DESCRIPTION: Example of implementing a Trident FlatMapFunction for splitting sentences into words

LANGUAGE: java
CODE:
public class Split extends FlatMapFunction {
  @Override
  public Iterable<Values> execute(TridentTuple input) {
    List<Values> valuesList = new ArrayList<>();
    for (String word : input.getString(0).split(" ")) {
      valuesList.add(new Values(word));
    }
    return valuesList;
  }
}

----------------------------------------

TITLE: Storm Fail Command JSON Structure
DESCRIPTION: JSON structure for indicating tuple processing failure in Storm's multilang protocol. Contains command type and tuple ID.

LANGUAGE: json
CODE:
{
    "command": "fail",
    "id": 123123
}

----------------------------------------

TITLE: Implementing Map and FlatMap Operations in Trident
DESCRIPTION: Examples of using map and flatMap for one-to-one and one-to-many transformations

LANGUAGE: java
CODE:
public class UpperCase extends MapFunction {
 @Override
 public Values execute(TridentTuple input) {
   return new Values(input.getString(0).toUpperCase());
 }
}

LANGUAGE: java
CODE:
public class Split extends FlatMapFunction {
  @Override
  public Iterable<Values> execute(TridentTuple input) {
    List<Values> valuesList = new ArrayList<>();
    for (String word : input.getString(0).split(" ")) {
      valuesList.add(new Values(word));
    }
    return valuesList;
  }
}

----------------------------------------

TITLE: Defining submitTopology Method in Thrift for Storm
DESCRIPTION: This Thrift definition shows the submitTopology method used to submit a topology to Storm. It specifies the required parameters and potential exceptions.

LANGUAGE: thrift
CODE:
void submitTopology(1: string name, 2: string uploadedJarLocation, 3: string jsonConf, 4: StormTopology topology)
    throws (1: AlreadyAliveException e, 2: InvalidTopologyException ite);

----------------------------------------

TITLE: Defining Split Sentence Bolt in Clojure
DESCRIPTION: Implementation of a simple bolt that splits input sentences into individual words. Shows basic bolt definition and tuple processing.

LANGUAGE: clojure
CODE:
(defbolt split-sentence ["word"] [tuple collector]
  (let [words (.split (.getString tuple 0) " ")]
    (doseq [w words]
      (emit-bolt! collector [w] :anchor tuple))
    (ack! collector tuple)
    ))

----------------------------------------

TITLE: HTML Meta Redirect to Storm Documentation
DESCRIPTION: HTML code that performs an automatic redirect to the Storm documentation page about defining non-JVM language DSLs, along with a canonical URL link.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Defining-a-non-jvm-language-dsl-for-storm.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Defining-a-non-jvm-language-dsl-for-storm.html" />

----------------------------------------

TITLE: Implementing a Global Count Update Bolt
DESCRIPTION: Example implementation of a committer bolt that updates a global count in the database with exactly-once semantics.

LANGUAGE: java
CODE:
public static class UpdateGlobalCount extends BaseTransactionalBolt implements ICommitter {
    TransactionAttempt _attempt;
    BatchOutputCollector _collector;

    int _sum = 0;

    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, TransactionAttempt attempt) {
        _collector = collector;
        _attempt = attempt;
    }

    @Override
    public void execute(Tuple tuple) {
        _sum+=tuple.getInteger(1);
    }

    @Override
    public void finishBatch() {
        Value val = DATABASE.get(GLOBAL_COUNT_KEY);
        Value newval;
        if(val == null || !val.txid.equals(_attempt.getTransactionId())) {
            newval = new Value();
            newval.txid = _attempt.getTransactionId();
            if(val==null) {
                newval.count = _sum;
            } else {
                newval.count = _sum + val.count;
            }
            DATABASE.put(GLOBAL_COUNT_KEY, newval);
        } else {
            newval = val;
        }
        _collector.emit(new Values(_attempt, newval.count));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "sum"));
    }
}

----------------------------------------

TITLE: Implementing Trident State for Druid
DESCRIPTION: Example of implementing Trident state integration with Druid using DruidBeamStateFactory and event mapper.

LANGUAGE: java
CODE:
    DruidBeamFactory druidBeamFactory = new SampleDruidBeamFactoryImpl(new HashMap<String, Object>());
    ITupleDruidEventMapper<Map<String, Object>> eventMapper = new TupleDruidEventMapper<>(TupleDruidEventMapper.DEFAULT_FIELD_NAME);

    final Stream stream = tridentTopology.newStream("batch-event-gen", new SimpleBatchSpout(10));

    stream.peek(new Consumer() {
        @Override
        public void accept(TridentTuple input) {
             LOG.info("########### Received tuple: [{}]", input);
         }
    }).partitionPersist(new DruidBeamStateFactory<Map<String, Object>>(druidBeamFactory, eventMapper), new Fields("event"), new DruidBeamStateUpdater());

----------------------------------------

TITLE: Defining Leader Election Interface in Java for Storm Nimbus
DESCRIPTION: Interface defining methods for leader election, including adding/removing from leader lock queue, checking leadership status, and getting leader and nimbus addresses.

LANGUAGE: java
CODE:
public interface ILeaderElector {
    void addToLeaderLockQueue();
    void removeFromLeaderLockQueue();
    boolean isLeader();
    InetSocketAddress getLeaderAddress();
    List<InetSocketAddress> getAllNimbusAddresses();
}

----------------------------------------

TITLE: Storm Docker Support Configuration Example
DESCRIPTION: Sample configuration for enabling Docker support in Storm on RHEL7. Includes essential settings for resource isolation, Docker image selection, and cgroup management.

LANGUAGE: yaml
CODE:
storm.resource.isolation.plugin.enable: true
storm.resource.isolation.plugin: "org.apache.storm.container.docker.DockerManager"
storm.oci.allowed.images: ["xxx.xxx.com:8080/storm/storm/rhel7:latest"]
storm.oci.image: "xxx.xxx.com:8080/storm/storm/rhel7:latest"
storm.oci.cgroup.root: "/storm"
storm.oci.cgroup.parent: "/sys/fs/cgroup"
storm.oci.readonly.bindmounts:
    - "/etc/storm"
storm.oci.nscd.dir: "/var/run/nscd"
supervisor.worker.launcher: "/usr/share/apache-storm-2.3.0/bin/worker-launcher"

----------------------------------------

TITLE: Storm Topology Building and Deployment
DESCRIPTION: Bash command for building and deploying a Flux-enabled Storm topology JAR

LANGUAGE: bash
CODE:
storm jar myTopology-0.1.0-SNAPSHOT.jar org.apache.storm.flux.Flux --local my_config.yaml

----------------------------------------

TITLE: Running Storm Local Mode via Command Line
DESCRIPTION: Example of running a Storm topology in local mode with debug configuration enabled using the command line interface.

LANGUAGE: bash
CODE:
storm local topology.jar <MY_MAIN_CLASS> -c topology.debug=true

----------------------------------------

TITLE: Implementing ISerialization Interface in Java for Storm Custom Serialization
DESCRIPTION: This code snippet shows the ISerialization interface that needs to be implemented to create a custom serializer in Storm. It includes methods for accepting a class, serializing an object to a DataOutputStream, and deserializing an object from a DataInputStream.

LANGUAGE: java
CODE:
public interface ISerialization<T> {
    public boolean accept(Class c);
    public void serialize(T object, DataOutputStream stream) throws IOException;
    public T deserialize(DataInputStream stream) throws IOException;
}

----------------------------------------

TITLE: Configuring Kerberos Authentication for Storm UI in YAML
DESCRIPTION: YAML configuration for setting up Kerberos authentication for the Storm UI using AuthenticationFilter from hadoop-auth.

LANGUAGE: yaml
CODE:
ui.filter: "org.apache.hadoop.security.authentication.server.AuthenticationFilter"
ui.filter.params:
   "type": "kerberos"
   "kerberos.principal": "HTTP/nimbus.witzend.com"
   "kerberos.keytab": "/vagrant/keytabs/http.keytab"
   "kerberos.name.rules": "RULE:[2:$1@$0]([jt]t@.*EXAMPLE.COM)s/.*/$MAPRED_USER/ RULE:[2:$1@$0]([nd]n@.*EXAMPLE.COM)s/.*/$HDFS_USER/DEFAULT"

----------------------------------------

TITLE: Jekyll Post Front Matter Configuration
DESCRIPTION: Jekyll front matter configuration block defining the post layout and title for a logo contest entry.

LANGUAGE: markdown
CODE:
---
layout: post
title: Logo Entry No. 11 - Jennifer Lee
---

----------------------------------------

TITLE: Storm Shell Command Usage Example
DESCRIPTION: Example command showing how to use the storm shell utility to package and submit a topology using Python.

LANGUAGE: bash
CODE:
storm shell resources/ python topology.py arg1 arg2

----------------------------------------

TITLE: Setting Log Level Using Storm CLI
DESCRIPTION: Command to set a log level for a specific logger in a Storm topology with an optional timeout. This example sets the ROOT logger to DEBUG level for 30 seconds.

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -l ROOT=DEBUG:30

----------------------------------------

TITLE: Spout Message Listening in Apache Storm (Clojure)
DESCRIPTION: Implementation of how spouts listen for incoming messages in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L382

----------------------------------------

TITLE: Configuring Storm Topology with Multiple Components
DESCRIPTION: Illustrates the configuration of a Storm topology with multiple components (spouts and bolts). It sets the number of worker processes, executors, and tasks for different components and submits the topology.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(2); // use two worker processes

topologyBuilder.setSpout("blue-spout", new BlueSpout(), 2); // set parallelism hint to 2

topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

topologyBuilder.setBolt("yellow-bolt", new YellowBolt(), 6)
               .shuffleGrouping("green-bolt");

StormSubmitter.submitTopology(
        "mytopology",
        conf,
        topologyBuilder.createTopology()
    );

----------------------------------------

TITLE: Creating SimpleHBaseMapper Instance in Java
DESCRIPTION: This code creates a SimpleHBaseMapper instance that maps Storm tuple fields to HBase row keys, standard columns, and counter columns within a specified column family.

LANGUAGE: java
CODE:
SimpleHBaseMapper mapper = new SimpleHBaseMapper() 
        .withRowKeyField("word")
        .withColumnFields(new Fields("word"))
        .withCounterFields(new Fields("count"))
        .withColumnFamily("cf");

----------------------------------------

TITLE: Defining Code Distribution Interface in Java for Storm Nimbus
DESCRIPTION: Java interface for distributing topology code across Nimbus instances. Includes methods for uploading, downloading, checking replication status, and cleaning up topology code.

LANGUAGE: java
CODE:
public interface ICodeDistributor {
    void prepare(Map conf);
    File upload(Path dirPath, String topologyId);
    List<File> download(Path destDirPath, String topologyid, File metafile);
    int getReplicationCount(String topologyId);
    void cleanup(String topologyid);
    void close(Map conf);
}

----------------------------------------

TITLE: Registering Task Hook in Java using TopologyContext
DESCRIPTION: This snippet demonstrates how to register a custom task hook in the open method of a spout or prepare method of a bolt using the TopologyContext. The method addTaskHook is used to add the custom hook.

LANGUAGE: java
CODE:
TopologyContext#addTaskHook

----------------------------------------

TITLE: Storm SQL Error Log Filtering Query
DESCRIPTION: SQL script that defines tables for Apache logs and filters error logs (status >= 400) into a separate Kafka topic.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://apache-logs?bootstrap-servers=localhost:9092'
CREATE EXTERNAL TABLE APACHE_ERROR_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_ELAPSED_MS INT) LOCATION 'kafka://apache-error-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO APACHE_ERROR_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, (TIME_US / 1000) AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (CAST(STATUS AS INT) / 100) >= 4

----------------------------------------

TITLE: Implementing Acker Execute Method in Clojure for Storm
DESCRIPTION: The execute method of Storm's acker bolt, defined within mk-acker-bolt. It processes new tupletrees, handles acks and fails, and manages the pending tuple ledger using a RotatingMap.

LANGUAGE: clojure
CODE:
execute()

On a tick tuple, just advance pending tupletree checksums towards death and return. Otherwise, update or create the record for this tupletree:

* on init: initialize with the given checksum value, and record the spout's id for later.
* on ack:  xor the partial checksum into the existing checksum value
* on fail: just mark it as failed

Next, put the record into the RotatingMap (thus resetting is countdown to expiry) and take action:

* if the total checksum is zero, the tupletree is complete: remove it from the pending collection and notify the spout of success
* if the tupletree has failed, it is also complete:   remove it from the pending collection and notify the spout of failure

Finally, pass on an ack of our own.

----------------------------------------

TITLE: Setting up EsLookupBolt for Elasticsearch Queries
DESCRIPTION: Configures an EsLookupBolt for performing get requests to Elasticsearch. Requires configuration, request adapter, and output formatting.

LANGUAGE: java
CODE:
EsConfig esConfig = createEsConfig();
ElasticsearchGetRequest getRequestAdapter = createElasticsearchGetRequest();
EsLookupResultOutput output = createOutput();
EsLookupBolt lookupBolt = new EsLookupBolt(esConfig, getRequestAdapter, output);

----------------------------------------

TITLE: Initializing EsPercolateBolt in Java
DESCRIPTION: Creates an EsPercolateBolt instance for streaming tuples into Elasticsearch as percolate requests. It requires an EsConfig and EsTupleMapper to extract necessary fields from input tuples.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsPercolateBolt percolateBolt = new EsPercolateBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Configuring HiveOptions for HiveBolt (Java)
DESCRIPTION: Java code example demonstrating how to create and configure HiveOptions for use with HiveBolt, including settings for transaction batches, batch size, and idle timeout.

LANGUAGE: java
CODE:
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
                                .withTxnsPerBatch(10)
                				.withBatchSize(1000)
                	     		.withIdleTimeout(10)

----------------------------------------

TITLE: Invalid Stream Reference Example
DESCRIPTION: Example showing incorrect forward referencing of streams in JoinBolt configuration

LANGUAGE: java
CODE:
new JoinBolt( "spout1", "key1")                 
  .join     ( "spout2", "userId",  "spout3") //not allowed. spout3 not yet introduced
  .join     ( "spout3", "key3",    "spout1")

----------------------------------------

TITLE: Implementing OpenTSDB State in Trident Topology
DESCRIPTION: This example shows how to use OpenTSDB state in a Trident topology. It creates an OpenTsdbStateFactory, sets up a TridentTopology with a metric generation spout, and persists the data using OpenTsdbStateUpdater.

LANGUAGE: java
CODE:
final OpenTsdbStateFactory openTsdbStateFactory =
        new OpenTsdbStateFactory(OpenTsdbClient.newBuilder(tsdbUrl),
                Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
TridentTopology tridentTopology = new TridentTopology();

final Stream stream = tridentTopology.newStream("metric-tsdb-stream", new MetricGenSpout());

stream.peek(new Consumer() {
    @Override
    public void accept(TridentTuple input) {
        LOG.info("########### Received tuple: [{}]", input);
    }
}).partitionPersist(openTsdbStateFactory, MetricGenSpout.DEFAULT_METRIC_FIELDS, new OpenTsdbStateUpdater());

----------------------------------------

TITLE: Implementing Transactional State for Word Count
DESCRIPTION: Example of using Trident to implement a fault-tolerant word count topology with persistent state stored in Memcached.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
      topology.newStream("spout1", spout)
        .each(new Fields("sentence"), new Split(), new Fields("word"))
        .groupBy(new Fields("word"))
        .persistentAggregate(MemcachedState.opaque(serverLocations), new Count(), new Fields("count"))                
        .parallelismHint(6);

----------------------------------------

TITLE: IConfigLoader Interface Definition in Java
DESCRIPTION: Interface definition for IConfigLoader, containing a single method 'load' that retrieves the most recent configuration map.

LANGUAGE: java
CODE:
public interface IConfigLoader {
    Map<?,?> load();
};

----------------------------------------

TITLE: Implementing StateUpdater for Custom State
DESCRIPTION: Example of implementing a StateUpdater to update a custom State (LocationDB) with new location information.

LANGUAGE: java
CODE:
public class LocationUpdater extends BaseStateUpdater<LocationDB> {
    public void updateState(LocationDB state, List<TridentTuple> tuples, TridentCollector collector) {
        List<Long> ids = new ArrayList<Long>();
        List<String> locations = new ArrayList<String>();
        for(TridentTuple t: tuples) {
            ids.add(t.getLong(0));
            locations.add(t.getString(1));
        }
        state.setLocationsBulk(ids, locations);
    }
}

----------------------------------------

TITLE: Implementing Reach Calculation in Java
DESCRIPTION: Computes the reach of a URL by tracking unique followers across Twitter users who shared the URL.

LANGUAGE: java
CODE:
TridentState urlToTweeters =
       topology.newStaticState(getUrlToTweetersState());
TridentState tweetersToFollowers =
       topology.newStaticState(getTweeterToFollowersState());

topology.newDRPCStream("reach")
       .stateQuery(urlToTweeters, new Fields("args"), new MapGet(), new Fields("tweeters"))
       .each(new Fields("tweeters"), new ExpandList(), new Fields("tweeter"))
       .shuffle()
       .stateQuery(tweetersToFollowers, new Fields("tweeter"), new MapGet(), new Fields("followers"))
       .parallelismHint(200)
       .each(new Fields("followers"), new ExpandList(), new Fields("follower"))
       .groupBy(new Fields("follower"))
       .aggregate(new One(), new Fields("one"))
       .parallelismHint(20)
       .aggregate(new Count(), new Fields("reach"));

----------------------------------------

TITLE: Synchronizing Supervisor with Assignments (Clojure)
DESCRIPTION: The synchronize-supervisor function is called when assignments in ZooKeeper change. It downloads code from Nimbus for assigned topologies and writes local assignments to the filesystem.

LANGUAGE: clojure
CODE:
(defn synchronize-supervisor [])

----------------------------------------

TITLE: Forcing IPv4 Stack in Java for Storm
DESCRIPTION: Add this Java option to the supervisor child options to force IPv4 stack usage, which can resolve communication issues between nodes in Storm.

LANGUAGE: java
CODE:
-Djava.net.preferIPv4Stack=true

----------------------------------------

TITLE: Configuring Redis State Provider in Storm
DESCRIPTION: This JSON configuration snippet shows how to set up the Redis state provider for Storm. It includes options for specifying the key and value classes, serializers, and Redis connection details.

LANGUAGE: json
CODE:
{
  "keyClass": "Optional fully qualified class name of the Key type.",
  "valueClass": "Optional fully qualified class name of the Value type.",
  "keySerializerClass": "Optional Key serializer implementation class.",
  "valueSerializerClass": "Optional Value Serializer implementation class.",
  "jedisPoolConfig": {
    "host": "localhost",
    "port": 6379,
    "timeout": 2000,
    "database": 0,
    "password": "xyz"
    }
}

----------------------------------------

TITLE: Implementing a Bolt in Python
DESCRIPTION: Python implementation of a bolt that splits sentences into words.

LANGUAGE: python
CODE:
import storm

class SplitSentenceBolt(storm.BasicBolt):
    def process(self, tup):
        words = tup.values[0].split(" ")
        for word in words:
          storm.emit([word])

SplitSentenceBolt().run()

----------------------------------------

TITLE: Storm Assignment Record Definition
DESCRIPTION: Clojure code defining the structure of a Storm topology assignment record, which includes task mappings, node information, and timing data.

LANGUAGE: clojure
CODE:
master-code-dir
task->node+port
node->host
task->start-time-secs

----------------------------------------

TITLE: Uploading Topology Jar in StormSubmitter (Java)
DESCRIPTION: StormSubmitter uploads the topology jar to Nimbus if it hasn't been uploaded before. This is done through Nimbus's Thrift interface, uploading 15 kilobytes at a time.

LANGUAGE: java
CODE:
StormSubmitter.submitTopology()

----------------------------------------

TITLE: Clearing Log Level Using Storm CLI
DESCRIPTION: Command to clear a previously set dynamic log level for a specific logger in a Storm topology. This example resets the ROOT logger to its original log level.

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -r ROOT

----------------------------------------

TITLE: Registering Custom Serializers in Storm Topology Configuration
DESCRIPTION: Example YAML configuration for registering custom serializers in Storm. It demonstrates how to register classes for default serialization and how to specify custom serializer implementations.

LANGUAGE: yaml
CODE:
topology.kryo.register:
  - com.mycompany.CustomType1
  - com.mycompany.CustomType2: com.mycompany.serializer.CustomType2Serializer
  - com.mycompany.CustomType3

----------------------------------------

TITLE: Launching Docker Container for Storm Worker
DESCRIPTION: Example Docker run command used by the worker-launcher executable to start a container for a Storm worker. It includes various options for security, resource constraints, and volume mounts.

LANGUAGE: bash
CODE:
run --name=8198e1f0-f323-4b9d-8625-e4fd640cd058 \
--user=<uid>:<gid> \
-d \
--net=host \
--read-only \
-v /sys/fs/cgroup:/sys/fs/cgroup:ro \
-v /usr/share/apache-storm-2.3.0:/usr/share/apache-storm-2.3.0:ro \
-v /<storm-local-dir>/supervisor:/<storm-local-dir>/supervisor:ro \
-v /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058:/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
-v /<workers-artifacts-dir>/workers-artifacts/word-count-1-1591895933/6703:/<workers-artifacts-dir>/workers-artifacts/word-count-1-1591895933/6703 \
-v /<storm-local-dir>/workers-users/8198e1f0-f323-4b9d-8625-e4fd640cd058:/<storm-local-dir>/workers-users/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
-v /var/run/nscd:/var/run/nscd \
-v /<storm-local-dir>/supervisor/stormdist/word-count-1-1591895933/shared_by_topology:/<storm-local-dir>/supervisor/stormdist/word-count-1-1591895933/shared_by_topology \
-v /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/tmp:/tmp \
-v /etc/storm:/etc/storm:ro \
--cgroup-parent=/storm \
--group-add <gid> \
--workdir=/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058 \
--cidfile=/<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/container.cid \
--cap-drop=ALL \
--security-opt no-new-privileges \
--security-opt seccomp=/usr/share/apache-storm-2.3.0/conf/seccomp.json \
--cpus=2.6 xxx.xxx.com:8080/storm/storm/rhel7:latest \
bash /<storm-local-dir>/workers/8198e1f0-f323-4b9d-8625-e4fd640cd058/storm-worker-script.sh

----------------------------------------

TITLE: Setting Health Check Directory in Storm YAML
DESCRIPTION: This configuration specifies the directory for health check scripts. These scripts are used to monitor the health of supervisor nodes.

LANGUAGE: yaml
CODE:
storm.health.check.dir: "healthchecks"

----------------------------------------

TITLE: Storm Topology Submission Method Definition
DESCRIPTION: Thrift method definition for submitting a topology to Storm, including required parameters and possible exceptions.

LANGUAGE: java
CODE:
void submitTopology(1: string name, 2: string uploadedJarLocation, 3: string jsonConf, 4: StormTopology topology) throws (1: AlreadyAliveException e, 2: InvalidTopologyException ite);

----------------------------------------

TITLE: Killing Topology
DESCRIPTION: The kill_topology command calls Nimbus's kill method, which applies a 'kill' transition to the topology, deactivating it for a wait time before final shutdown.

LANGUAGE: clojure
CODE:
(kill-transition)

----------------------------------------

TITLE: Defining a Multilingual Bolt in Java
DESCRIPTION: Example of defining a bolt that uses a Python script for processing.

LANGUAGE: java
CODE:
public static class SplitSentence extends ShellBolt implements IRichBolt {
    public SplitSentence() {
        super("python3", "splitsentence.py");
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word"));
    }
}

----------------------------------------

TITLE: Creating Kafka Topics for Apache Logs
DESCRIPTION: Creates Kafka topics for raw logs, error logs, and slow logs using the kafka-topics command line tool.

LANGUAGE: bash
CODE:
kafka-topics --create --topic apache-logs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
kafka-topics --create --topic apache-errorlogs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
kafka-topics --create --topic apache-slowlogs --zookeeper localhost:2181 --replication-factor 1 --partitions 5

----------------------------------------

TITLE: Trident FlatMap Function Example
DESCRIPTION: Implementation of a flatMap function that splits sentences into words

LANGUAGE: java
CODE:
public class Split extends FlatMapFunction {
  @Override
  public Iterable<Values> execute(TridentTuple input) {
    List<Values> valuesList = new ArrayList<>();
    for (String word : input.getString(0).split(" ")) {
      valuesList.add(new Values(word));
    }
    return valuesList;
  }
}

----------------------------------------

TITLE: Running Storm Topology JAR
DESCRIPTION: Command to execute a Storm topology by packaging code and dependencies into a JAR file and submitting to the cluster.

LANGUAGE: bash
CODE:
storm jar all-my-code.jar org.apache.storm.MyTopology arg1 arg2

----------------------------------------

TITLE: Retrieving Cluster Summary in Storm UI REST API
DESCRIPTION: Example JSON response for the /api/v1/cluster/summary endpoint, which returns summary information about the cluster such as number of supervisors, topologies, and resource utilization.

LANGUAGE: json
CODE:
{
 "stormVersion": "0.9.2-incubating-SNAPSHOT",
 "supervisors": 1,
 "slotsTotal": 4,
 "slotsUsed": 3,
 "slotsFree": 1,
 "executorsTotal": 28,
 "tasksTotal": 28,
 "schedulerDisplayResource": true,
 "totalMem": 4096.0,
 "totalCpu": 400.0,
 "availMem": 1024.0,
 "availCPU": 250.0,
 "memAssignedPercentUtil": 75.0,
 "cpuAssignedPercentUtil": 37.5
}

----------------------------------------

TITLE: Initializing Trident Topology with Persistent Aggregation
DESCRIPTION: Example of creating a Trident topology that computes word counts and stores them persistently using MemcachedState.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
      topology.newStream("spout1", spout)
        .each(new Fields("sentence"), new Split(), new Fields("word"))
        .groupBy(new Fields("word"))
        .persistentAggregate(MemcachedState.opaque(serverLocations), new Count(), new Fields("count"))                
        .parallelismHint(6);

----------------------------------------

TITLE: Maven Shade Plugin Configuration
DESCRIPTION: XML configuration for Maven Shade Plugin to build an uber jar with all dependencies for the Storm-Solr integration.

LANGUAGE: xml
CODE:
<plugin>
     <groupId>org.apache.maven.plugins</groupId>
     <artifactId>maven-shade-plugin</artifactId>
     <version>2.4.1</version>
     <executions>
         <execution>
             <phase>package</phase>
             <goals>
                 <goal>shade</goal>
             </goals>
             <configuration>
                 <transformers>
                     <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                         <mainClass>org.apache.storm.solr.topology.SolrJsonTopology</mainClass>
                     </transformer>
                 </transformers>
             </configuration>
         </execution>
     </executions>
</plugin>

----------------------------------------

TITLE: Submitting Storm SQL Topology with Dependencies
DESCRIPTION: Command to submit a Storm SQL topology with necessary dependencies. It includes the SQL file, topology name, and artifacts for Storm SQL Kafka integration and Kafka clients.

LANGUAGE: bash
CODE:
$ bin/storm sql order_filtering.sql order_filtering --artifacts "org.apache.storm:storm-sql-kafka:2.0.0-SNAPSHOT,org.apache.storm:storm-kafka-client:2.0.0-SNAPSHOT,org.apache.kafka:kafka-clients:1.1.0^org.slf4j:slf4j-log4j12"

----------------------------------------

TITLE: Downloading Storm Kafka Client 1.1.0 JAR
DESCRIPTION: Specific example URL for downloading the Storm Kafka Client 1.1.0 JAR from Maven Central.

LANGUAGE: text
CODE:
https://repo1.maven.org/maven2/org/apache/storm/storm-kafka-client/1.1.0/storm-kafka-client-1.1.0.jar

----------------------------------------

TITLE: Implementing Bulk Operations in Custom State
DESCRIPTION: Enhanced version of the LocationDB state implementation with bulk get and set operations for improved efficiency.

LANGUAGE: java
CODE:
public class LocationDB implements State {
    public void beginCommit(Long txid) {    
    }
    
    public void commit(Long txid) {    
    }
    
    public void setLocationsBulk(List<Long> userIds, List<String> locations) {
      // set locations in bulk
    }
    
    public List<String> bulkGetLocations(List<Long> userIds) {
      // get locations in bulk
    }
}

----------------------------------------

TITLE: Configuring Storm Druid Core Bolt
DESCRIPTION: Example showing how to configure and use the DruidBeamBolt for sending data to Druid. Demonstrates setting up the bolt with a beam factory, configuration, and event mapper.

LANGUAGE: java
CODE:
   DruidBeamFactory druidBeamFactory = new SampleDruidBeamFactoryImpl(new HashMap<String, Object>());
   DruidConfig druidConfig = DruidConfig.newBuilder().discardStreamId(DruidConfig.DEFAULT_DISCARD_STREAM_ID).build();
   ITupleDruidEventMapper<Map<String, Object>> eventMapper = new TupleDruidEventMapper<>(TupleDruidEventMapper.DEFAULT_FIELD_NAME);
   DruidBeamBolt<Map<String, Object>> druidBolt = new DruidBeamBolt<Map<String, Object>>(druidBeamFactory, eventMapper, druidConfig);
   topologyBuilder.setBolt("druid-bolt", druidBolt).shuffleGrouping("event-gen");
   topologyBuilder.setBolt("printer-bolt", new PrinterBolt()).shuffleGrouping("druid-bolt" , druidConfig.getDiscardStreamId());

----------------------------------------

TITLE: Window Configuration APIs
DESCRIPTION: Various window configuration methods supported by Storm including count-based and time-based windows with different sliding intervals.

LANGUAGE: java
CODE:
withWindow(Count windowLength, Count slidingInterval)
Tuple count based sliding window that slides after `slidingInterval` number of tuples.

withWindow(Count windowLength)
Tuple count based window that slides with every incoming tuple.

withWindow(Count windowLength, Duration slidingInterval)
Tuple count based sliding window that slides after `slidingInterval` time duration.

withWindow(Duration windowLength, Duration slidingInterval)
Time duration based sliding window that slides after `slidingInterval` time duration.

withWindow(Duration windowLength)
Time duration based window that slides with every incoming tuple.

withWindow(Duration windowLength, Count slidingInterval)
Time duration based sliding window configuration that slides after `slidingInterval` number of tuples.

withTumblingWindow(BaseWindowedBolt.Count count)
Count based tumbling window that tumbles after the specified count of tuples.

withTumblingWindow(BaseWindowedBolt.Duration duration)
Time duration based tumbling window that tumbles after the specified time duration.

----------------------------------------

TITLE: Creating MQTT Topology with Java API
DESCRIPTION: Java code for creating a Storm topology with an MQTT spout and logging bolt using the Storm Core Java API.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
MqttOptions options = new MqttOptions();
options.setTopics(Arrays.asList("/users/tgoetz/#"));
options.setCleanConnection(false);
MqttSpout spout = new MqttSpout(new StringMessageMapper(), options);

MqttBolt bolt = new LogInfoBolt();

builder.setSpout("mqtt-spout", spout);
builder.setBolt("log-bolt", bolt).shuffleGrouping("mqtt-spout");

return builder.createTopology();

----------------------------------------

TITLE: Implementing Storm Topology with Kinesis Spout in Java
DESCRIPTION: Sample code demonstrating how to create a Storm topology that uses KinesisSpout to consume data from Kinesis Streams. Shows configuration of connection parameters, ZooKeeper settings, and topology setup.

LANGUAGE: java
CODE:
public class KinesisSpoutTopology {
    public static void main (String args[]) throws InvalidTopologyException, AuthorizationException, AlreadyAliveException {
        String topologyName = args[0];
        RecordToTupleMapper recordToTupleMapper = new TestRecordToTupleMapper();
        KinesisConnectionInfo kinesisConnectionInfo = new KinesisConnectionInfo(new CredentialsProviderChain(), new ClientConfiguration(), Regions.US_WEST_2,
                1000);
        ZkInfo zkInfo = new ZkInfo("localhost:2181", "/kinesisOffsets", 20000, 15000, 10000L, 3, 2000);
        KinesisConfig kinesisConfig = new KinesisConfig(args[1], ShardIteratorType.TRIM_HORIZON,
                recordToTupleMapper, new Date(), new ExponentialBackoffRetrier(), zkInfo, kinesisConnectionInfo, 10000L);
        KinesisSpout kinesisSpout = new KinesisSpout(kinesisConfig);
        TopologyBuilder topologyBuilder = new TopologyBuilder();
        topologyBuilder.setSpout("spout", kinesisSpout, 3);
        topologyBuilder.setBolt("bolt", new KinesisBoltTest(), 1).shuffleGrouping("spout");
        Config topologyConfig = new Config();
        topologyConfig.setDebug(true);
        topologyConfig.setNumWorkers(3);
        StormSubmitter.submitTopology(topologyName, topologyConfig, topologyBuilder.createTopology());
    }
}

----------------------------------------

TITLE: Configuring Health Check Settings
DESCRIPTION: Configuration for setting up supervisor health check directory and timeout parameters.

LANGUAGE: yaml
CODE:
storm.health.check.dir: "healthchecks"
storm.health.check.timeout.ms: 5000

----------------------------------------

TITLE: Registering Event Logger in Java for Apache Storm Topology
DESCRIPTION: This code snippet demonstrates how to register an event logger to an Apache Storm topology using Java configuration. It uses the FileBasedEventLogger class as an example.

LANGUAGE: java
CODE:
conf.registerEventLogger(org.apache.storm.metric.FileBasedEventLogger.class);

----------------------------------------

TITLE: Implementing a Bolt in Python
DESCRIPTION: Demonstrates how to implement a bolt in Python that splits sentences into words.

LANGUAGE: python
CODE:
import storm

class SplitSentenceBolt(storm.BasicBolt):
    def process(self, tup):
        words = tup.values[0].split(" ")
        for word in words:
          storm.emit([word])

SplitSentenceBolt().run()

----------------------------------------

TITLE: Creating and Setting Up Worker Process (Clojure)
DESCRIPTION: The mk-worker function sets up a worker process, including connecting to other workers, monitoring topology activity, and launching task threads.

LANGUAGE: clojure
CODE:
(defn mk-worker [conf shared-mq-context storm-id port worker-id])

----------------------------------------

TITLE: ZeroMQ Implementation for Distributed Mode in Apache Storm (Clojure)
DESCRIPTION: Implementation of the message sending protocol using ZeroMQ for distributed mode in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/zmq.clj

----------------------------------------

TITLE: Configuring Metrics Consumers in YAML
DESCRIPTION: Example showing how to register metrics consumers through the storm.yaml configuration file with options for class name, parallelism hint and arguments

LANGUAGE: yaml
CODE:
topology.metrics.consumer.register:
  - class: "org.apache.storm.metric.LoggingMetricsConsumer"
    parallelism.hint: 1
  - class: "org.apache.storm.metric.HttpForwardingMetricsConsumer"
    parallelism.hint: 1
    argument: "http://example.com:8080/metrics/my-topology/"

----------------------------------------

TITLE: Configuring User/Group Access Control in Storm YAML
DESCRIPTION: YAML configuration for restricting access to Storm cluster based on specific users or groups.

LANGUAGE: yaml
CODE:
nimbus.users: 
   - "testuser"

# or

nimbus.groups: 
   - "storm"

----------------------------------------

TITLE: Defining ComponentObject Struct in Thrift for Storm
DESCRIPTION: This Thrift struct defines the ComponentObject union, which is used to specify the code for spouts and bolts in Storm topologies. It allows for serialized Java objects, shell components, or Java objects.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Storm Thrift API submitTopology Definition
DESCRIPTION: Thrift method definition for submitting a topology to Storm, including parameters for name, jar location, configuration, and topology structure.

LANGUAGE: thrift
CODE:
void submitTopology(1: string name, 2: string uploadedJarLocation, 3: string jsonConf, 4: StormTopology topology)
    throws (1: AlreadyAliveException e, 2: InvalidTopologyException ite);

----------------------------------------

TITLE: Configuring Resource Aware Scheduler in YAML
DESCRIPTION: YAML configuration to enable the Resource Aware Scheduler in Storm.

LANGUAGE: yaml
CODE:
storm.scheduler: "org.apache.storm.scheduler.resource.ResourceAwareScheduler"

----------------------------------------

TITLE: Bolt Message Listening in Apache Storm (Clojure)
DESCRIPTION: Implementation of how bolts listen for incoming messages in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L489

----------------------------------------

TITLE: Core Storm Component Structure Definition
DESCRIPTION: Thrift definition showing how Storm components (spouts and bolts) are structured with ComponentObject and ComponentCommon. ComponentObject defines the implementation while ComponentCommon defines streams, configurations and parallelism.

LANGUAGE: thrift
CODE:
struct ComponentObject {
  1: binary serialized_java
  2: ShellComponent shell
  3: JavaObject java_object
}

struct ComponentCommon {
  1: map<string, StreamInfo> streams
  2: map<GlobalStreamId, Grouping> inputs
  3: i32 parallelism_hint
  4: map<string, string> json_conf
}

----------------------------------------

TITLE: Digest Authentication JAAS Configuration
DESCRIPTION: JAAS configuration for Pacemaker digest authentication method.

LANGUAGE: java
CODE:
PacemakerDigest {
    username="some username"
    password="some password";
};

----------------------------------------

TITLE: Flux YAML Configuration for MQTT Topology
DESCRIPTION: Complete Flux YAML configuration for setting up an MQTT topology with spout and bolt

LANGUAGE: yaml
CODE:
name: "mqtt-topology"

components:
   ########## MQTT Spout Config ############
  - id: "mqtt-type"
    className: "org.apache.storm.mqtt.examples.CustomMessageMapper"

  - id: "mqtt-options"
    className: "org.apache.storm.mqtt.common.MqttOptions"
    properties:
      - name: "url"
        value: "tcp://localhost:1883"
      - name: "topics"
        value:
          - "/users/tgoetz/#"

# topology configuration
config:
  topology.workers: 1
  topology.max.spout.pending: 1000

# spout definitions
spouts:
  - id: "mqtt-spout"
    className: "org.apache.storm.mqtt.spout.MqttSpout"
    constructorArgs:
      - ref: "mqtt-type"
      - ref: "mqtt-options"
    parallelism: 1

# bolt definitions
bolts:
  - id: "log"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1


streams:
  - from: "mqtt-spout"
    to: "log"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Configuring UI/Logviewer Filters in YAML
DESCRIPTION: YAML configuration for setting up authentication filters for the Storm UI and logviewer

LANGUAGE: yaml
CODE:
ui.filter: "filter.class"
ui.filter.params: "param1":"value1"
logviewer.filter: "filter.class"
logviewer.filter.params: "param1":"value1"

----------------------------------------

TITLE: Filtering Error Logs with Storm SQL
DESCRIPTION: Storm SQL script to filter error logs from Apache logs and store them in a separate Kafka topic.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://apache-logs?bootstrap-servers=localhost:9092'
CREATE EXTERNAL TABLE APACHE_ERROR_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_ELAPSED_MS INT) LOCATION 'kafka://apache-error-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO APACHE_ERROR_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, (TIME_US / 1000) AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (CAST(STATUS AS INT) / 100) >= 4

----------------------------------------

TITLE: Storm Blobstore ACL Example
DESCRIPTION: Example of creating a blob in Storm's blobstore with specific access control lists (ACLs).

LANGUAGE: bash
CODE:
storm blobstore create mytopo:data.tgz -f data.tgz -a u:alice:rwa,u:bob:rw,o::r

----------------------------------------

TITLE: Parameterized Bolt Definition in Clojure
DESCRIPTION: Example of creating a parameterized bolt that appends a suffix to input strings. Demonstrates how to create configurable bolt components.

LANGUAGE: clojure
CODE:
(defbolt suffix-appender ["word"] {:params [suffix]}
  [tuple collector]
  (emit-bolt! collector [(str (.getString tuple 0) suffix)] :anchor tuple)
  )

----------------------------------------

TITLE: Profiling Container Processes
DESCRIPTION: Command to enter the container namespace for profiling processes.

LANGUAGE: bash
CODE:
sudo nsenter --target <container-pid> --pid --mount --setuid <uid> --setgid <gid>

----------------------------------------

TITLE: Configuring Resource Aware Scheduler in Storm YAML
DESCRIPTION: Basic configuration to enable the Resource Aware Scheduler in Storm's configuration file.

LANGUAGE: yaml
CODE:
storm.scheduler: "org.apache.storm.scheduler.resource.ResourceAwareScheduler"

----------------------------------------

TITLE: Configuring EsState for Trident State Integration
DESCRIPTION: Sets up Elasticsearch Trident state integration using EsConfig and EsTupleMapper for persistent storage.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();

StateFactory factory = new EsStateFactory(esConfig, tupleMapper);
TridentState state = stream.partitionPersist(factory, esFields, new EsUpdater(), new Fields());

----------------------------------------

TITLE: Configuring NUMA in Supervisor Config for Apache Storm
DESCRIPTION: YAML configuration for setting up NUMA zones in the Supervisor Config, including core allocation, generic resources, memory size, and port assignments.

LANGUAGE: yaml
CODE:
supervisor.numa.meta:
    "0": # Numa zone id
        numa.cores: # Cores in NUMA zone (can be determined by using the numastat command)
            - 0
            - 1
            - 2
            - 3
            - 4
            - 5
            - 12
            - 13
            - 14
            - 15
            - 16

        numa.generic.resources.map: # Generic Resources in the zone to be used for generic resource scheduling (optional)
            network.resource.units: 50.0

        numa.memory.mb: 42461 # Size of memory zone
        numa.ports: # Ports to be assigned to workers pinned to the NUMA zone (this may include ports not specified in SUPERVISOR_SLOTS_PORTS
            - 6700
            - 6701
            - 6702
            - 6703
            - 6704
            - 6705
            - 6706
            - 6707
            - 6708
            - 6709
            - 6710
            - 6711

----------------------------------------

TITLE: Implementing a Python Bolt for Storm
DESCRIPTION: Python implementation of a SplitSentence bolt that splits input sentences into words.

LANGUAGE: python
CODE:
import storm

class SplitSentenceBolt(storm.BasicBolt):
    def process(self, tup):
        words = tup.values[0].split(" ")
        for word in words:
          storm.emit([word])

SplitSentenceBolt().run()

----------------------------------------

TITLE: Storm Assignment Record Definition
DESCRIPTION: Defines the structure for task assignments including master code directory, task-to-node mapping, node-to-host mapping, and task start times.

LANGUAGE: clojure
CODE:
Assignment record {
  master-code-dir
  task->node+port
  node->host
  task->start-time-secs
}

----------------------------------------

TITLE: Running Azure Event Hubs Send Client for Testing
DESCRIPTION: Command to execute the included Event Hubs send client for testing purposes. Allows specifying username, password, entity path, partition ID, message size, and count.

LANGUAGE: bash
CODE:
java -cp .\target\eventhubs-storm-spout-{version}-jar-with-dependencies.jar com.microsoft.eventhubs.client.EventHubSendClient
[username] [password] [entityPath] [partitionId] [messageSize] [messageCount]

----------------------------------------

TITLE: Querying Custom State in Trident Topology
DESCRIPTION: Demonstrates how to use a custom QueryFunction to query a State object in a Trident topology.

LANGUAGE: Java
CODE:
TridentTopology topology = new TridentTopology();
TridentState locations = topology.newStaticState(new LocationDBFactory());
topology.newStream("myspout", spout)
        .stateQuery(locations, new Fields("userid"), new QueryLocation(), new Fields("location"))

----------------------------------------

TITLE: Configuring Digest Authentication for Pacemaker
DESCRIPTION: JAAS configuration for digest authentication in Pacemaker.

LANGUAGE: java
CODE:
PacemakerDigest {
    username="some username"
    password="some password";
};

----------------------------------------

TITLE: Implementing OpenTSDB Trident State
DESCRIPTION: Shows how to implement OpenTSDB integration using Storm's Trident abstraction. The example includes creating a state factory, setting up a Trident topology with a metric stream, and configuring state persistence with a custom updater.

LANGUAGE: java
CODE:
final OpenTsdbStateFactory openTsdbStateFactory =
        new OpenTsdbStateFactory(OpenTsdbClient.newBuilder(tsdbUrl),
                Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
TridentTopology tridentTopology = new TridentTopology();

final Stream stream = tridentTopology.newStream("metric-tsdb-stream", new MetricGenSpout());

stream.peek(new Consumer() {
    @Override
    public void accept(TridentTuple input) {
        LOG.info("########### Received tuple: [{}]", input);
    }
}).partitionPersist(openTsdbStateFactory, MetricGenSpout.DEFAULT_METRIC_FIELDS, new OpenTsdbStateUpdater());

----------------------------------------

TITLE: Implementing a Multi-Language Bolt in Java
DESCRIPTION: This snippet shows how to define a bolt that uses a non-JVM language (Python in this case). It demonstrates the use of ShellBolt to execute a Python script as part of the Storm topology.

LANGUAGE: Java
CODE:
public static class SplitSentence extends ShellBolt implements IRichBolt {
    public SplitSentence() {
        super("python3", "splitsentence.py");
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word"));
    }
}

----------------------------------------

TITLE: Applying a Map Function to a Trident Stream
DESCRIPTION: Example of applying a Map Function to a Trident stream.

LANGUAGE: java
CODE:
mystream.map(new UpperCase())

----------------------------------------

TITLE: Spout Message Listening in Apache Storm (Clojure)
DESCRIPTION: Implementation of message listening for spouts in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L382

----------------------------------------

TITLE: Setting Debug Level Example in Storm CLI
DESCRIPTION: Example of setting the ROOT logger to DEBUG level for 30 seconds using the Storm CLI command.

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -l ROOT=DEBUG:30

----------------------------------------

TITLE: RecordToTupleMapper Interface Definition in Java
DESCRIPTION: Interface definition for mapping Kinesis records to Storm tuples. It includes methods for defining output fields and converting records to tuples.

LANGUAGE: java
CODE:
    Fields getOutputFields ();
    List<Object> getTuple (Record record);

----------------------------------------

TITLE: Configuring Event Loggers in YAML
DESCRIPTION: YAML configuration example showing how to register multiple event loggers with custom arguments in storm.yaml.

LANGUAGE: yaml
CODE:
topology.event.logger.register:
  - class: "org.apache.storm.metric.FileBasedEventLogger"
  - class: "org.mycompany.MyEventLogger"
    arguments:
      endpoint: "event-logger.mycompany.org"

----------------------------------------

TITLE: Running Storm SQL Command
DESCRIPTION: Command to compile SQL statements into a Storm topology and submit it to the Storm cluster.

LANGUAGE: bash
CODE:
$ bin/storm sql <sql-file> <topo-name>

----------------------------------------

TITLE: Record to Tuple Mapper Interface in Java
DESCRIPTION: Interface definition for converting Kinesis records to Storm tuples. Includes methods for defining output fields and tuple conversion.

LANGUAGE: java
CODE:
    Fields getOutputFields ();
    List<Object> getTuple (Record record);

----------------------------------------

TITLE: Setting Up Tasks
DESCRIPTION: The mk-task function sets up individual tasks, including routing functions for tuple distribution and specific spout or bolt code.

LANGUAGE: clojure
CODE:
(mk-task)

----------------------------------------

TITLE: Complex DRPC Reach Calculation Implementation
DESCRIPTION: Implementation of a complex Twitter URL reach calculation using multiple bolts and parallel processing.

LANGUAGE: java
CODE:
public class PartialUniquer extends BaseBatchBolt {
    BatchOutputCollector _collector;
    Object _id;
    Set<String> _followers = new HashSet<String>();
    
    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
        _collector = collector;
        _id = id;
    }

    @Override
    public void execute(Tuple tuple) {
        _followers.add(tuple.getString(1));
    }
    
    @Override
    public void finishBatch() {
        _collector.emit(new Values(_id, _followers.size()));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "partial-count"));
    }
}

----------------------------------------

TITLE: User-Defined Function for Timestamp Conversion
DESCRIPTION: Java implementation of a user-defined function to convert string timestamps to Unix timestamps.

LANGUAGE: java
CODE:
package org.apache.storm.sql.runtime.functions.scalar.datetime;

import org.joda.time.format.DateTimeFormat;
import org.joda.time.format.DateTimeFormatter;

public class GetTime2 {
    public static Long evaluate(String dateString, String dateFormat) {
        try {
            DateTimeFormatter df = DateTimeFormat.forPattern(dateFormat).withZoneUTC();
            return df.parseDateTime(dateString).getMillis();
        } catch (Exception ex) {
            throw new RuntimeException(ex);
        }
    }
}

----------------------------------------

TITLE: Killing Topology through Nimbus (Clojure)
DESCRIPTION: The kill command triggers a transition in Nimbus that changes the topology status to 'killed' and schedules its removal after a wait time. This allows for graceful shutdown of the topology.

LANGUAGE: clojure
CODE:
(transition! [:kill storm-name wait-time] topologies)

----------------------------------------

TITLE: Java Topology Builder Example - Java
DESCRIPTION: Example of creating an MQTT topology using Storm's Java API

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
MqttOptions options = new MqttOptions();
options.setTopics(Arrays.asList("/users/tgoetz/#"));
options.setCleanConnection(false);
MqttSpout spout = new MqttSpout(new StringMessageMapper(), options);

MqttBolt bolt = new LogInfoBolt();

builder.setSpout("mqtt-spout", spout);
builder.setBolt("log-bolt", bolt).shuffleGrouping("mqtt-spout");

return builder.createTopology();

----------------------------------------

TITLE: Storm Input Tuple JSON Format
DESCRIPTION: JSON structure representing an input tuple received by a shell process from Storm. Includes tuple ID, component ID, stream ID, task ID, and tuple values.

LANGUAGE: json
CODE:
{
    "id": -6955786537413359385,
    "comp": 1,
    "stream": 1,
    "task": 9,
    "tuple": ["snow white and the seven dwarfs", "field2", 3]
}

----------------------------------------

TITLE: Initializing HiveBolt with Basic Configuration
DESCRIPTION: Basic setup of HiveBolt using DelimitedRecordHiveMapper to map tuple fields to Hive table columns.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames));
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper);
HiveBolt hiveBolt = new HiveBolt(hiveOptions);

----------------------------------------

TITLE: Implementing QueryFunction for State Querying in Trident
DESCRIPTION: This example shows how to implement a QueryFunction for querying a custom State (LocationDB). It includes methods for batch retrieval and execution of individual tuples.

LANGUAGE: java
CODE:
public class QueryLocation extends BaseQueryFunction<LocationDB, String> {
    public List<String> batchRetrieve(LocationDB state, List<TridentTuple> inputs) {
        List<String> ret = new ArrayList();
        for(TridentTuple input: inputs) {
            ret.add(state.getLocation(input.getLong(0)));
        }
        return ret;
    }

    public void execute(TridentTuple tuple, String location, TridentCollector collector) {
        collector.emit(new Values(location));
    }    
}

----------------------------------------

TITLE: Configuring Zookeeper Servers in Storm YAML
DESCRIPTION: This snippet shows how to configure the Zookeeper servers for a Storm cluster in the storm.yaml configuration file. It specifies the IP addresses of the Zookeeper servers.

LANGUAGE: yaml
CODE:
storm.zookeeper.servers:
  - "111.222.333.444"
  - "555.666.777.888"

----------------------------------------

TITLE: Filtering Error Logs with Storm SQL
DESCRIPTION: SQL script to filter error logs from Apache logs and store them in a separate Kafka topic.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://apache-logs?bootstrap-servers=localhost:9092'
CREATE EXTERNAL TABLE APACHE_ERROR_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_ELAPSED_MS INT) LOCATION 'kafka://apache-error-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO APACHE_ERROR_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, (TIME_US / 1000) AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (CAST(STATUS AS INT) / 100) >= 4

----------------------------------------

TITLE: Running Storm SQL Command
DESCRIPTION: Command to compile SQL statements into a Storm topology and submit it to the Storm cluster.

LANGUAGE: bash
CODE:
$ bin/storm sql <sql-file> <topo-name>

----------------------------------------

TITLE: Adding Items to Kestrel Queue in Java
DESCRIPTION: This method adds random sentences to a Kestrel queue using KestrelClient. It selects sentences from a predefined array and queues them with unique IDs.

LANGUAGE: java
CODE:
    private static void queueSentenceItems(KestrelClient kestrelClient, String queueName)
			throws ParseError, IOException {

		String[] sentences = new String[] {
	            "the cow jumped over the moon",
	            "an apple a day keeps the doctor away",
	            "four score and seven years ago",
	            "snow white and the seven dwarfs",
	            "i am at two with nature"};

		Random _rand = new Random();

		for(int i=1; i<=10; i++){

			String sentence = sentences[_rand.nextInt(sentences.length)];

			String val = "ID " + i + " " + sentence;

			boolean queueSucess = kestrelClient.queue(queueName, val);

			System.out.println("queueSucess=" +queueSucess+ " [" + val +"]");
		}
	}

----------------------------------------

TITLE: Setting Health Check Timeout in Storm YAML
DESCRIPTION: This configuration sets the maximum time allowed for a health check script to run before it is considered failed due to timeout.

LANGUAGE: yaml
CODE:
storm.health.check.timeout.ms: 5000

----------------------------------------

TITLE: Storm Input Tuple JSON Format
DESCRIPTION: JSON structure representing an input tuple received by a shell process from Storm. Includes tuple ID, component ID, stream ID, task ID, and tuple values.

LANGUAGE: json
CODE:
{
    "id": -6955786537413359385,
    "comp": 1,
    "stream": 1,
    "task": 9,
    "tuple": ["snow white and the seven dwarfs", "field2", 3]
}

----------------------------------------

TITLE: Storm Message Transfer Protocol
DESCRIPTION: Protocol definition for message transfer between workers

LANGUAGE: clojure
CODE:
messaging/protocol.clj

----------------------------------------

TITLE: Setting Local Directory for Storm in YAML
DESCRIPTION: This configuration specifies the local directory for Storm to store small amounts of state. It's required for Nimbus and Supervisor daemons.

LANGUAGE: yaml
CODE:
storm.local.dir: "/mnt/storm"

----------------------------------------

TITLE: Running Storm MQTT Example Topology with Flux
DESCRIPTION: Command to run a sample Storm topology using Flux, which starts a local mode cluster and topology with an MQTT Spout publishing to a logging bolt.

LANGUAGE: bash
CODE:
storm jar ./examples/target/storm-mqtt-examples-*-SNAPSHOT.jar org.apache.storm.flux.Flux ./examples/src/main/flux/sample.yaml --local

----------------------------------------

TITLE: Defining Code Distribution Interface in Java
DESCRIPTION: Interface for distributing topology code across Nimbus instances, including methods for uploading, downloading, and managing code replication.

LANGUAGE: java
CODE:
public interface ICodeDistributor {
    void prepare(Map conf);
    File upload(Path dirPath, String topologyId);
    List<File> download(Path destDirPath, String topologyid, File metafile);
    int getReplicationCount(String topologyId);
    void cleanup(String topologyid);
    void close(Map conf);
}

----------------------------------------

TITLE: Refreshing Worker Connections in Apache Storm (Clojure)
DESCRIPTION: The 'refresh-connections' function is called periodically to manage connections to other workers and maintain a mapping from task to worker.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L123

----------------------------------------

TITLE: Thrift Data Structures for Cluster Information
DESCRIPTION: Thrift struct definitions for cluster summary information including supervisor, topology and nimbus details.

LANGUAGE: thrift
CODE:
struct ClusterSummary {
  1: required list<SupervisorSummary> supervisors;
  3: required list<TopologySummary> topologies;
  4: required list<NimbusSummary> nimbuses;
}

struct NimbusSummary {
  1: required string host;
  2: required i32 port;
  3: required i32 uptime_secs;
  4: required bool isLeader;
  5: required string version;
}

----------------------------------------

TITLE: Configuring Windows Storm Local Directory
DESCRIPTION: Windows-specific configuration for Storm local directory path

LANGUAGE: yaml
CODE:
storm.local.dir: "C:\\storm-local"

----------------------------------------

TITLE: Storm Jar Upload Process
DESCRIPTION: Implementation of the jar upload process in Nimbus using Thrift interface. Handles file upload in 15KB chunks and manages the upload path.

LANGUAGE: thrift
CODE:
beginFileUpload
uploadChunk
finishFileUpload

----------------------------------------

TITLE: Creating and Querying Kafka Stream Tables in Storm SQL
DESCRIPTION: Example showing how to create external tables representing Kafka streams and perform filtering and projection operations. The query filters orders where total amount exceeds 50 and writes results to a separate stream.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://...' ...

CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) 'kafka://...' ...

INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Implementing the ISpout Interface in Java
DESCRIPTION: This code snippet shows the ISpout interface that spouts must implement in Storm. It includes methods for opening and closing the spout, emitting tuples, and handling acks and fails.

LANGUAGE: java
CODE:
public interface ISpout extends Serializable {
    void open(Map conf, TopologyContext context, SpoutOutputCollector collector);
    void close();
    void nextTuple();
    void ack(Object msgId);
    void fail(Object msgId);
}

----------------------------------------

TITLE: Implementing Custom Metric in Java
DESCRIPTION: Example of how to implement and register a custom CountMetric in a Storm bolt.

LANGUAGE: java
CODE:
private transient CountMetric countMetric;

@Override
public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
	// other initialization here.
	countMetric = new CountMetric();
	context.registerMetric("execute_count", countMetric, 60);
}

public void execute(Tuple input) {
	countMetric.incr();
	// handle tuple here.	
}

----------------------------------------

TITLE: Running Storm SQL Topology
DESCRIPTION: Compiles SQL statements into a Trident topology and submits it to Storm, with options for including additional JARs and artifacts.

LANGUAGE: bash
CODE:
storm sql sql-file topology-name [--jars "jar1,jar2"] [--artifacts "artifact1,artifact2"] [--artifactRepositories "repo1^url1,repo2^url2"]

----------------------------------------

TITLE: Setting Storm Local Directory in YAML
DESCRIPTION: This code snippet demonstrates how to set the storm.local.dir configuration in storm.yaml. It specifies the directory where Nimbus and Supervisor daemons store small amounts of state.

LANGUAGE: yaml
CODE:
storm.local.dir: "/mnt/storm"

LANGUAGE: yaml
CODE:
storm.local.dir: "C:\\storm-local"

----------------------------------------

TITLE: Registering Custom Metrics in Apache Storm Java API
DESCRIPTION: Example of registering custom metrics using the TopologyContext class in Apache Storm. Shows method signatures for registering various metric types like Timer, Histogram, Meter, Counter, and Gauge.

LANGUAGE: java
CODE:
public Timer registerTimer(String name)

public Histogram registerHistogram(String name)

public Meter registerMeter(String name)

public Counter registerCounter(String name)

public Gauge registerGauge(String name, Gauge gauge)

----------------------------------------

TITLE: Implementing Skewed Top-N Pattern with Partial Key Grouping
DESCRIPTION: Enhanced top-N calculation pattern that handles data skew using partial key grouping and additional aggregation layer.

LANGUAGE: java
CODE:
builder.setBolt("count", new CountObjects(), parallelism)
  .partialKeyGrouping("objects", new Fields("value"));
builder.setBolt("rank" new AggregateCountsAndRank(), parallelism)
  .fieldsGrouping("count", new Fields("key"))
builder.setBolt("merge", new MergeRanksObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Storm Shell Command Execution Format
DESCRIPTION: Format showing how Storm shell executes the topology script with Nimbus connection parameters.

LANGUAGE: bash
CODE:
python topology.py arg1 arg2 {nimbus-host} {nimbus-port} {uploaded-jar-location}

----------------------------------------

TITLE: GNU General Public License v3 Text
DESCRIPTION: The complete text of the GNU General Public License version 3, providing terms for free software distribution

LANGUAGE: text
CODE:
                    GNU GENERAL PUBLIC LICENSE
                       Version 3, 29 June 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

[Full GPL v3 text omitted for brevity]

----------------------------------------

TITLE: Creating a Simple Storm Topology in Java
DESCRIPTION: Java code to create a topology with a spout and two bolts using TopologyBuilder.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();        
builder.setSpout("words", new TestWordSpout(), 10);        
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
        .shuffleGrouping("words");
builder.setBolt("exclaim2", new ExclamationBolt(), 2)
        .shuffleGrouping("exclaim1");

----------------------------------------

TITLE: Defining ModelRunner Interface in Java for Storm PMML Integration
DESCRIPTION: This snippet shows the core method of the ModelRunner interface used in Storm PMML integration. The method 'scoredTuplePerStream' takes a Tuple as input and returns a Map of stream IDs to lists of scored objects.

LANGUAGE: java
CODE:
Map<String, List<Object>> scoredTuplePerStream(Tuple input);

----------------------------------------

TITLE: Registering Metrics API Methods in Java
DESCRIPTION: Core API methods available in TopologyContext for registering different types of metrics including Timer, Histogram, Meter, Counter, and Gauge.

LANGUAGE: java
CODE:
public Timer registerTimer(String name)

public Histogram registerHistogram(String name)

public Meter registerMeter(String name)

public Counter registerCounter(String name)

public Gauge registerGauge(String name, Gauge gauge)

----------------------------------------

TITLE: Redis Cluster State Configuration
DESCRIPTION: JSON configuration for Redis Cluster state implementation including serialization settings and cluster node configuration

LANGUAGE: json
CODE:
{
   "keyClass": "Optional fully qualified class name of the Key type.",
   "valueClass": "Optional fully qualified class name of the Value type.",
   "keySerializerClass": "Optional Key serializer implementation class.",
   "valueSerializerClass": "Optional Value Serializer implementation class.",
   "jedisClusterConfig": {
     "nodes": ["localhost:7379", "localhost:7380", "localhost:7381"],
     "timeout": 2000,
     "maxRedirections": 5
   }
 }

----------------------------------------

TITLE: Named Streams Join Implementation
DESCRIPTION: Example of joining named streams from multiple spouts using JoinBolt with stream selector configuration.

LANGUAGE: java
CODE:
new JoinBolt(JoinBolt.Selector.STREAM,  "stream1", "key1") 
                             .join     ("stream2", "userId",  "stream1" )
                             .select ("userId, key1, key2")
                             .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;
                             
topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("bolt1", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt2", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt3", "stream2", new Fields("userId") )
            .fieldsGrouping("bolt4", "stream1", new Fields("key1") );

----------------------------------------

TITLE: Configuring CassandraWriterBolt with All Fields Insert Query in Java
DESCRIPTION: Creates a CassandraWriterBolt that inserts all fields from the input tuple into a Cassandra table.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
    async(
        simpleQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);")
            .with( all() )
        )
);

----------------------------------------

TITLE: Sentence Spout Implementation in Clojure
DESCRIPTION: Example of a basic spout that emits random sentences from a predefined list.

LANGUAGE: clojure
CODE:
(defspout sentence-spout ["sentence"]
  [conf context collector]
  (let [sentences ["a little brown dog"
                   "the man petted the dog"
                   "four score and seven years ago"
                   "an apple a day keeps the doctor away"]]
    (spout
     (nextTuple []
       (Thread/sleep 100)
       (emit-spout! collector [(rand-nth sentences)])         
       )
     (ack [id]
        ))))

----------------------------------------

TITLE: Failing Tuple in JSON Format for Storm Multi-Language Protocol
DESCRIPTION: This JSON structure represents a fail command sent by a shell process in Storm's multi-language protocol. It includes the command type and the id of the tuple being marked as failed.

LANGUAGE: JSON
CODE:
{
    "command": "fail",
    "id": 123123
}

----------------------------------------

TITLE: Local Mode Message Passing in Apache Storm (Clojure)
DESCRIPTION: Implementation of message passing using in-memory Java queues for local mode in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/local.clj

----------------------------------------

TITLE: Configuring Pacemaker Servers
DESCRIPTION: YAML configuration for specifying Pacemaker server hosts.

LANGUAGE: yaml
CODE:
pacemaker.servers:\n    - somehost.mycompany.com\n    - someotherhost.mycompany.com

----------------------------------------

TITLE: Simplified DRPC Client Creation
DESCRIPTION: Demonstrates using a preconfigured DRPC client with automatic host selection and failover capability.

LANGUAGE: java
CODE:
DRPCClient client = DRPCClient.getConfiguredClient(conf);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Failed Message Retry Handler Interface
DESCRIPTION: Interface for handling failed message retry logic in the Kinesis spout. Includes methods for tracking failed messages, scheduling retries, and managing message acknowledgments.

LANGUAGE: java
CODE:
    boolean failed (KinesisMessageId messageId);
    KinesisMessageId getNextFailedMessageToRetry ();
    void failedMessageEmitted (KinesisMessageId messageId);
    void acked (KinesisMessageId messageId);

----------------------------------------

TITLE: Refreshing Worker Connections in Apache Storm (Clojure)
DESCRIPTION: This code snippet shows how a worker refreshes its connections to other workers and maintains a mapping from task to worker. It's called periodically or when the ZooKeeper assignment changes.

LANGUAGE: clojure
CODE:
refresh-connections

----------------------------------------

TITLE: Creating Kafka Topics for Apache Logs
DESCRIPTION: Creates Kafka topics for Apache logs, error logs, and slow logs using the kafka-topics command.

LANGUAGE: bash
CODE:
kafka-topics --create --topic apache-logs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
kafka-topics --create --topic apache-errorlogs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
kafka-topics --create --topic apache-slowlogs --zookeeper localhost:2181 --replication-factor 1 --partitions 5

----------------------------------------

TITLE: Custom MQTT Tuple Mapper Implementation - Java
DESCRIPTION: Example implementation of MqttTupleMapper converting tuple data to MQTT messages

LANGUAGE: java
CODE:
public class MyTupleMapper implements MqttTupleMapper {
    public MqttMessage toMessage(ITuple tuple) {
        String topic = "users/" + tuple.getStringByField("userId") + "/" + tuple.getStringByField("device");
        byte[] payload = tuple.getStringByField("message").getBytes();
        return new MqttMessage(topic, payload);
    }
}

----------------------------------------

TITLE: Implementing Storm Bolt with Output Fields
DESCRIPTION: Java example showing a Storm bolt implementation that processes input tuples and emits double and triple values.

LANGUAGE: java
CODE:
public class DoubleAndTripleBolt extends BaseRichBolt {
    private OutputCollectorBase _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollectorBase collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple input) {
        int val = input.getInteger(0);        
        _collector.emit(input, new Values(val*2, val*3));
        _collector.ack(input);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("double", "triple"));
    }    
}

----------------------------------------

TITLE: Storm SQL Query for Filtering Slow Logs with UDF
DESCRIPTION: SQL script to filter slow logs from Apache logs, using a user-defined function to convert timestamps.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://localhost:2181/brokers?topic=apachelogs' TBLPROPERTIES '{"producer":{"bootstrap.servers":"localhost:9092","acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer","value.serializer":"org.apache.storm.kafka.ByteBufferSerializer"}}'
CREATE EXTERNAL TABLE APACHE_SLOW_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_RECEIVED_TIMESTAMP BIGINT, TIME_ELAPSED_MS INT) LOCATION 'kafka://localhost:2181/brokers?topic=apacheslowlogs' TBLPROPERTIES '{"producer":{"bootstrap.servers":"localhost:9092","acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer","value.serializer":"org.apache.storm.kafka.ByteBufferSerializer"}}'
CREATE FUNCTION GET_TIME AS 'org.apache.storm.sql.runtime.functions.scalar.datetime.GetTime2'
INSERT INTO APACHE_SLOW_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, GET_TIME(TIME_RECEIVED_UTC_ISOFORMAT, 'yyyy-MM-dd''T''HH:mm:ssZZ') AS TIME_RECEIVED_TIMESTAMP, TIME_US / 1000 AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (TIME_US / 1000) >= 100

----------------------------------------

TITLE: Code Distribution Interface Implementation in Java
DESCRIPTION: Interface for managing code distribution across the Storm cluster, handling upload, download, replication tracking and cleanup operations.

LANGUAGE: java
CODE:
public interface ICodeDistributor {
    void prepare(Map conf);
    File upload(Path dirPath, String topologyId);
    List<File> download(Path destDirPath, String topologyid, File metafile);
    int getReplicationCount(String topologyId);
    void cleanup(String topologyid);
    void close(Map conf);
}

----------------------------------------

TITLE: Configuring ExpandUrl Bolt with Shuffle Grouping in Storm
DESCRIPTION: This snippet demonstrates how to set up an ExpandUrl bolt with shuffle grouping in a Storm topology. This approach doesn't optimize for caching efficiency.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .shuffleGrouping(1);

----------------------------------------

TITLE: Code Distribution Interface Implementation in Java
DESCRIPTION: Interface for managing code distribution across the Storm cluster, handling upload, download, replication tracking and cleanup operations.

LANGUAGE: java
CODE:
public interface ICodeDistributor {
    void prepare(Map conf);
    File upload(Path dirPath, String topologyId);
    List<File> download(Path destDirPath, String topologyid, File metafile);
    int getReplicationCount(String topologyId);
    void cleanup(String topologyid);
    void close(Map conf);
}

----------------------------------------

TITLE: Storm Topology File Upload in StormSubmitter
DESCRIPTION: Code showing how StormSubmitter uploads topology jar files to Nimbus using Thrift interface. Files are uploaded in 15KB chunks to Nimbus's inbox directory.

LANGUAGE: thrift
CODE:
beginFileUpload
uploadChunk
finishFileUpload

----------------------------------------

TITLE: Flux Configuration for MQTT Topology - YAML
DESCRIPTION: YAML configuration for creating an MQTT topology using Flux

LANGUAGE: yaml
CODE:
name: "mqtt-topology"

components:
   ########## MQTT Spout Config ############
  - id: "mqtt-type"
    className: "org.apache.storm.mqtt.examples.CustomMessageMapper"

  - id: "mqtt-options"
    className: "org.apache.storm.mqtt.common.MqttOptions"
    properties:
      - name: "url"
        value: "tcp://localhost:1883"
      - name: "topics"
        value:
          - "/users/tgoetz/#"

# topology configuration
config:
  topology.workers: 1
  topology.max.spout.pending: 1000

# spout definitions
spouts:
  - id: "mqtt-spout"
    className: "org.apache.storm.mqtt.spout.MqttSpout"
    constructorArgs:
      - ref: "mqtt-type"
      - ref: "mqtt-options"
    parallelism: 1

# bolt definitions
bolts:
  - id: "log"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1


streams:
  - from: "mqtt-spout"
    to: "log"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Implementing a Bolt in Python
DESCRIPTION: Shows the implementation of a simple bolt in Python that splits sentences into words.

LANGUAGE: python
CODE:
import storm

class SplitSentenceBolt(storm.BasicBolt):
    def process(self, tup):
        words = tup.values[0].split(" ")
        for word in words:
          storm.emit([word])

SplitSentenceBolt().run()

----------------------------------------

TITLE: Running Storm Topology Locally
DESCRIPTION: Runs a Storm topology in local mode for 30 seconds, using an embedded version of Storm daemons within the same process.

LANGUAGE: shell
CODE:
storm local topology-jar-path class ...

----------------------------------------

TITLE: Submitting Storm SQL Topology with Dependencies
DESCRIPTION: Shows how to submit a Storm SQL topology with required dependencies using the 'storm sql' command.

LANGUAGE: bash
CODE:
$ bin/storm sql order_filtering.sql order_filtering --artifacts "org.apache.storm:storm-sql-kafka:2.0.0-SNAPSHOT,org.apache.storm:storm-kafka-client:2.0.0-SNAPSHOT,org.apache.kafka:kafka-clients:1.1.0^org.slf4j:slf4j-log4j12"

----------------------------------------

TITLE: DRPC Server Configuration in YAML
DESCRIPTION: YAML configuration for setting up DRPC servers including server locations and transport security settings.

LANGUAGE: yaml
CODE:
drpc.servers:
  - "drpc1.foo.com"
  - "drpc2.foo.com"
drpc.http.port: 8081
storm.thrift.transport: "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin"

----------------------------------------

TITLE: Accessing Executor Data in Storm
DESCRIPTION: Methods for reading and writing executor-level data shared across tasks and task hooks within an executor.

LANGUAGE: java
CODE:
TopologyContext#setExecutorData  // write access
TopologyContext#getExecutorData(String)  // read access

----------------------------------------

TITLE: Spout Tuple Transfer in Apache Storm (Clojure)
DESCRIPTION: This code demonstrates how spouts use the worker-provided transfer function to send tuples to other tasks after determining the output task IDs.

LANGUAGE: clojure
CODE:
(https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L329)

----------------------------------------

TITLE: Custom Metric Implementation Example
DESCRIPTION: Example showing how to implement and register a custom CountMetric in a Storm bolt.

LANGUAGE: java
CODE:
private transient CountMetric countMetric;

LANGUAGE: java
CODE:
@Override
public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
	// other initialization here.
	countMetric = new CountMetric();
	context.registerMetric("execute_count", countMetric, 60);
}

LANGUAGE: java
CODE:
public void execute(Tuple input) {
	countMetric.incr();
	// handle tuple here.	
}

----------------------------------------

TITLE: Creating Kerberos Principals and Keytabs
DESCRIPTION: Bash commands for creating Kerberos service principals and keytabs for Zookeeper, Nimbus, DRPC and other Storm components

LANGUAGE: bash
CODE:
# Zookeeper (Will need one of these for each box in the Zk ensemble)
sudo kadmin.local -q 'addprinc zookeeper/zk1.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/zk.keytab  zookeeper/zk1.example.com@STORM.EXAMPLE.COM"
# Nimbus and DRPC
sudo kadmin.local -q 'addprinc storm/storm.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm/storm.example.com@STORM.EXAMPLE.COM"
# All UI logviewer and Supervisors
sudo kadmin.local -q 'addprinc storm@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm@STORM.EXAMPLE.COM"

----------------------------------------

TITLE: IConfigLoader Interface Definition in Java
DESCRIPTION: Interface definition for IConfigLoader, containing a single method 'load' that retrieves the most recent configuration map.

LANGUAGE: java
CODE:
public interface IConfigLoader {
    Map<?,?> load();
};

----------------------------------------

TITLE: Trident State Implementation
DESCRIPTION: Example of using the Trident API to write data to HDFS, demonstrating both regular file and sequence file implementations.

LANGUAGE: java
CODE:
Fields hdfsFields = new Fields("field1", "field2");

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withPath("/trident")
        .withPrefix("trident")
        .withExtension(".txt");

RecordFormat recordFormat = new DelimitedRecordFormat()
        .withFields(hdfsFields);

FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, FileSizeRotationPolicy.Units.MB);

HdfsState.Options options = new HdfsState.HdfsFileOptions()
        .withFileNameFormat(fileNameFormat)
        .withRecordFormat(recordFormat)
        .withRotationPolicy(rotationPolicy)
        .withFsUrl("hdfs://localhost:54310");

StateFactory factory = new HdfsStateFactory().withOptions(options);

TridentState state = stream
        .partitionPersist(factory, hdfsFields, new HdfsUpdater(), new Fields());

----------------------------------------

TITLE: Configuring HDFS AvroGenericRecordBolt in Java
DESCRIPTION: Example of configuring an AvroGenericRecordBolt to write Avro objects directly to HDFS with specific formats and policies.

LANGUAGE: java
CODE:
// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withExtension(".avro")
        .withPath("/data/");

// create sequence format instance.
DefaultSequenceFormat format = new DefaultSequenceFormat("timestamp", "sentence");

AvroGenericRecordBolt bolt = new AvroGenericRecordBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy);

----------------------------------------

TITLE: Executing NUMA Control Commands in CgroupManager
DESCRIPTION: Command syntax for binding worker processes to specific NUMA zones using numactl, which controls both CPU and memory allocation.

LANGUAGE: bash
CODE:
numactl --cpunodebind=<numaId>> --membind=<numaId> <worker launch command>

----------------------------------------

TITLE: Adding Storm Dependency to Maven POM
DESCRIPTION: This XML snippet shows how to add Apache Storm as a provided dependency in a Maven project's pom.xml file. It specifies the groupId, artifactId, and version, with the scope set to 'provided'.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.storm</groupId>
  <artifactId>storm-client</artifactId>
  <version>{{page.version}}</version>
  <scope>provided</scope>
</dependency>

----------------------------------------

TITLE: IConfigLoader Interface Definition in Java
DESCRIPTION: Interface definition for IConfigLoader, containing a single method 'load' that retrieves the most recent configuration map.

LANGUAGE: java
CODE:
public interface IConfigLoader {
    Map<?,?> load();
};

----------------------------------------

TITLE: Spout Tuple Transfer in Apache Storm (Clojure)
DESCRIPTION: This code demonstrates how spouts use the worker-provided transfer function to send tuples to other tasks after determining the output task IDs.

LANGUAGE: clojure
CODE:
(https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L329)

----------------------------------------

TITLE: OCI/Squashfs Runtime Configuration
DESCRIPTION: Example configuration settings for using the OCI/Squashfs runtime in Storm.

LANGUAGE: bash
CODE:
storm.resource.isolation.plugin: "org.apache.storm.container.oci.RuncLibContainerManager"

storm.oci.allowed.images:
    - "storm/rhel7:dev_current"
    - "storm/rhel7:dev_previous"
    - "storm/rhel7:dev_test"
storm.oci.image: "storm/rhel7:dev_current"

storm.oci.cgroup.parent: "/storm"
storm.oci.cgroup.root: "/sys/fs/cgroup"
storm.oci.image.hdfs.toplevel.dir: "hdfs://host:port/containers/"
storm.oci.image.tag.to.manifest.plugin: "org.apache.storm.container.oci.LocalOrHdfsImageTagToManifestPlugin"
storm.oci.local.or.hdfs.image.tag.to.manifest.plugin.hdfs.hash.file: "hdfs://host:port/containers/image-tag-to-hash"
storm.oci.manifest.to.resources.plugin: "org.apache.storm.container.oci.HdfsManifestToResourcesPlugin"
storm.oci.readonly.bindmounts:
    - "/home/y/lib64/storm"
    - "/etc/krb5.conf"

storm.oci.resources.localizer: "org.apache.storm.container.oci.HdfsOciResourcesLocalizer"
storm.oci.seccomp.profile: "/home/y/conf/storm/seccomp.json"

----------------------------------------

TITLE: jQuery and Sizzle.js MIT License Text
DESCRIPTION: Complete MIT license text covering both jQuery v3.6.1 and the included Sizzle.js library. Includes copyright notices, permission grants, conditions, warranty disclaimers and liability limitations for both components.

LANGUAGE: text
CODE:
jQuery v 3.6.1
Copyright OpenJS Foundation and other contributors, https://openjsf.org/

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

******************************************

The jQuery JavaScript Library v3.6.1 also includes Sizzle.js

Sizzle.js includes the following license:

Copyright JS Foundation and other contributors, https://js.foundation/

This software consists of voluntary contributions made by many
individuals. For exact contribution history, see the revision history
available at https://github.com/jquery/sizzle

The following license applies to all parts of this software except as
documented below:

====

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

====

All files located in the node_modules and external directories are
externally maintained libraries used by this software which have their
own licenses; we recommend you read them, as their terms may differ from
the terms above.

*********************

----------------------------------------

TITLE: Starting MQTT Broker and Publisher in Java
DESCRIPTION: Command to start an MQTT broker on port 1883 and a publisher that sends random temperature/humidity values to an MQTT topic.

LANGUAGE: bash
CODE:
java -cp examples/target/storm-mqtt-examples-*-SNAPSHOT.jar org.apache.storm.mqtt.examples.MqttBrokerPublisher

----------------------------------------

TITLE: Installing JZMQ Java Bindings
DESCRIPTION: Commands to clone and install JZMQ (Java bindings for ZeroMQ) from a specific fork that is tested to work with Storm.

LANGUAGE: bash
CODE:
#install jzmq
git clone https://github.com/nathanmarz/jzmq.git
cd jzmq
./autogen.sh
./configure
make
sudo make install

----------------------------------------

TITLE: Initializing Fixed Batch Spout in Trident
DESCRIPTION: Creates a FixedBatchSpout that cycles through a set of predefined sentences to produce an input stream.

LANGUAGE: java
CODE:
FixedBatchSpout spout = new FixedBatchSpout(new Fields("sentence"), 3,
               new Values("the cow jumped over the moon"),
               new Values("the man went to the store and bought some candy"),
               new Values("four score and seven years ago"),
               new Values("how many apples can you eat"));
spout.setCycle(true);

----------------------------------------

TITLE: Code Distribution Interface Implementation in Java
DESCRIPTION: Interface for managing code distribution across the Storm cluster, handling upload, download, replication tracking and cleanup operations.

LANGUAGE: java
CODE:
public interface ICodeDistributor {
    void prepare(Map conf);
    File upload(Path dirPath, String topologyId);
    List<File> download(Path destDirPath, String topologyid, File metafile);
    int getReplicationCount(String topologyId);
    void cleanup(String topologyid);
    void close(Map conf);
}

----------------------------------------

TITLE: Creating a Typed Stream with ValueMapper in Java
DESCRIPTION: Shows how to create a typed stream using a ValueMapper to extract specific fields from tuples emitted by a spout.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

// extract the first field from the tuple to get a Stream<String> of sentences
Stream<String> sentences = builder.newStream(new TestWordSpout(), new ValueMapper<String>(0));

----------------------------------------

TITLE: Dequeuing Items from Kestrel
DESCRIPTION: Method to retrieve items from a Kestrel queue without removing them. Attempts to dequeue 12 items and prints them if available.

LANGUAGE: java
CODE:
    private static void dequeueItems(KestrelClient kestrelClient, String queueName) throws IOException, ParseError
			 {
		for(int i=1; i<=12; i++){

			Item item = kestrelClient.dequeue(queueName);

			if(item==null){
				System.out.println("The queue (" + queueName + ") contains no items.");
			}
			else
			{
				byte[] data = item._data;

				String receivedVal = new String(data);

				System.out.println("receivedItem=" + receivedVal);
			}
		}

----------------------------------------

TITLE: Assigning Tasks in Nimbus
DESCRIPTION: Nimbus uses the mk-assignment function to assign tasks to machines. It creates an Assignment record containing task-to-node mappings, node-to-host mappings, and task start times.

LANGUAGE: clojure
CODE:
(mk-assignment)

----------------------------------------

TITLE: Executing Storm SQL Command
DESCRIPTION: Command to compile SQL statements into a Storm topology and submit it to the Storm cluster. The sql-file contains SQL statements and topo-name is the name of the topology.

LANGUAGE: bash
CODE:
$ bin/storm sql <sql-file> <topo-name>

----------------------------------------

TITLE: Creating Fat JAR with Maven Shade Plugin
DESCRIPTION: Maven configuration for creating an uber jar containing Flux and topology dependencies using the shade plugin

LANGUAGE: xml
CODE:
<!-- create a fat jar that includes all dependencies -->
<build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-shade-plugin</artifactId>
            <version>1.4</version>
            <configuration>
                <createDependencyReducedPom>true</createDependencyReducedPom>
            </configuration>
            <executions>
                <execution>
                    <phase>package</phase>
                    <goals>
                        <goal>shade</goal>
                    </goals>
                    <configuration>
                        <transformers>
                            <transformer
                                    implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                            <transformer
                                    implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                <mainClass>org.apache.storm.flux.Flux</mainClass>
                            </transformer>
                        </transformers>
                    </configuration>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>

----------------------------------------

TITLE: Setting Supervisor Slot Ports in Storm YAML
DESCRIPTION: This configuration defines the ports available for worker processes on each machine. It determines how many workers can run on a machine.

LANGUAGE: yaml
CODE:
supervisor.slots.ports:
    - 6700
    - 6701
    - 6702
    - 6703

----------------------------------------

TITLE: Creating a Blob in Storm's Blobstore
DESCRIPTION: Shows how to create a new blob in Storm's blobstore with specific access controls using the 'blobstore' command.

LANGUAGE: bash
CODE:
storm blobstore create mytopo:data.tgz -f data.tgz -a u:alice:rwa,u:bob:rw,o::r

----------------------------------------

TITLE: Window Configuration Methods in Java
DESCRIPTION: Available methods for configuring different types of windows including sliding and tumbling windows with count or duration-based parameters.

LANGUAGE: java
CODE:
withWindow(Count windowLength, Count slidingInterval)
Tuple count based sliding window that slides after `slidingInterval` number of tuples.

withWindow(Count windowLength)
Tuple count based window that slides with every incoming tuple.

withWindow(Count windowLength, Duration slidingInterval)
Tuple count based sliding window that slides after `slidingInterval` time duration.

withWindow(Duration windowLength, Duration slidingInterval)
Time duration based sliding window that slides after `slidingInterval` time duration.

withWindow(Duration windowLength)
Time duration based window that slides with every incoming tuple.

withWindow(Duration windowLength, Count slidingInterval)
Time duration based sliding window configuration that slides after `slidingInterval` number of tuples.

withTumblingWindow(BaseWindowedBolt.Count count)
Count based tumbling window that tumbles after the specified count of tuples.

withTumblingWindow(BaseWindowedBolt.Duration duration)
Time duration based tumbling window that tumbles after the specified time duration.

----------------------------------------

TITLE: Configuring DRPC Servers
DESCRIPTION: Configuration for specifying DRPC server locations for worker nodes to connect to.

LANGUAGE: yaml
CODE:
drpc.servers: ["111.222.333.44"]

----------------------------------------

TITLE: Submitting DRPC Topology in Java
DESCRIPTION: This code snippet shows how to submit a DRPC topology to a Storm cluster using StormSubmitter.

LANGUAGE: java
CODE:
StormSubmitter.submitTopology("exclamation-drpc", conf, builder.createRemoteTopology());

----------------------------------------

TITLE: Implementing Redis Filter Mapper
DESCRIPTION: Example implementation of RedisFilterMapper interface for filtering words based on a blacklist stored in Redis.

LANGUAGE: java
CODE:
class BlacklistWordFilterMapper implements RedisFilterMapper {
    private RedisDataTypeDescription description;
    private final String setKey = "blacklist";

    public BlacklistWordFilterMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.SET, setKey);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Enabling Resource Aware Scheduler Configuration
DESCRIPTION: YAML configuration to enable the Resource Aware Scheduler in Storm by setting the scheduler class.

LANGUAGE: yaml
CODE:
storm.scheduler: "org.apache.storm.scheduler.resource.ResourceAwareScheduler"

----------------------------------------

TITLE: Creating CassandraWriterBolt with Logged Batch Statement in Java
DESCRIPTION: Shows how to create a CassandraWriterBolt that uses a logged batch statement to execute multiple queries atomically.

LANGUAGE: java
CODE:
new CassandraWriterBolt(loggedBatch(
        simpleQuery("INSERT INTO titles_per_album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all())),
        simpleQuery("INSERT INTO titles_per_performer (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all()))
    )
);

----------------------------------------

TITLE: Implementing Storm Bolt with Output Fields
DESCRIPTION: Example implementation of a Storm bolt that processes incoming tuples by doubling and tripling values

LANGUAGE: java
CODE:
public class DoubleAndTripleBolt extends BaseRichBolt {
    private OutputCollectorBase _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollectorBase collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple input) {
        int val = input.getInteger(0);        
        _collector.emit(input, new Values(val*2, val*3));
        _collector.ack(input);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("double", "triple"));
    }    
}

----------------------------------------

TITLE: Configuring Pacemaker State Storage in Storm
DESCRIPTION: Sets the cluster state store to use the PaceMakerStateStorageFactory for handling worker heartbeats through Pacemaker.

LANGUAGE: yaml
CODE:
storm.cluster.state.store: "org.apache.storm.cluster.PaceMakerStateStorageFactory"

----------------------------------------

TITLE: Compiling SQL to Storm Topology
DESCRIPTION: Compiles SQL statements into a Trident topology and submits it to Storm. Supports additional JAR and artifact options.

LANGUAGE: shell
CODE:
storm sql sql-file topology-name

----------------------------------------

TITLE: Sequence File Bolt Configuration
DESCRIPTION: Example of configuring a SequenceFileBolt to write Storm data to HDFS sequence files with compression and custom formatting.

LANGUAGE: java
CODE:
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withExtension(".seq")
        .withPath("/data/");

DefaultSequenceFormat format = new DefaultSequenceFormat("timestamp", "sentence");

SequenceFileBolt bolt = new SequenceFileBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withSequenceFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy)
        .withCompressionType(SequenceFile.CompressionType.RECORD)
        .withCompressionCodec("deflate");

----------------------------------------

TITLE: Storm Supervisor Metrics Table
DESCRIPTION: Markdown table defining Supervisor metrics with names, types and descriptions

LANGUAGE: markdown
CODE:
| Metric Name | Type | Description |
|-------------|------|-------------|
| supervisor:blob-cache-update-duration | timer | how long it takes to update... |

----------------------------------------

TITLE: Using StateUpdater in Trident Topology
DESCRIPTION: Shows how to use a custom StateUpdater in a Trident topology to update a persistent state.

LANGUAGE: Java
CODE:
TridentTopology topology = new TridentTopology();
TridentState locations = 
    topology.newStream("locations", locationsSpout)
        .partitionPersist(new LocationDBFactory(), new Fields("userid", "location"), new LocationUpdater())

----------------------------------------

TITLE: Implementing an Update Global Count Bolt in Apache Storm
DESCRIPTION: Definition of an UpdateGlobalCount bolt that updates a global count in a database as part of a transactional topology, ensuring exactly-once semantics.

LANGUAGE: java
CODE:
public static class UpdateGlobalCount extends BaseTransactionalBolt implements ICommitter {
    TransactionAttempt _attempt;
    BatchOutputCollector _collector;

    int _sum = 0;

    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, TransactionAttempt attempt) {
        _collector = collector;
        _attempt = attempt;
    }

    @Override
    public void execute(Tuple tuple) {
        _sum+=tuple.getInteger(1);
    }

    @Override
    public void finishBatch() {
        Value val = DATABASE.get(GLOBAL_COUNT_KEY);
        Value newval;
        if(val == null || !val.txid.equals(_attempt.getTransactionId())) {
            newval = new Value();
            newval.txid = _attempt.getTransactionId();
            if(val==null) {
                newval.count = _sum;
            } else {
                newval.count = _sum + val.count;
            }
            DATABASE.put(GLOBAL_COUNT_KEY, newval);
        } else {
            newval = val;
        }
        _collector.emit(new Values(_attempt, newval.count));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "sum"));
    }
}

----------------------------------------

TITLE: Local Mode Implementation using Java Queues in Apache Storm (Clojure)
DESCRIPTION: This code implements the message sending functionality for local mode using in-memory Java queues in Apache Storm, allowing for easy local usage without ZeroMQ installation.

LANGUAGE: clojure
CODE:
(github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/local.clj)

----------------------------------------

TITLE: Setting Topology Priority in Storm
DESCRIPTION: API call to set the priority level of a Storm topology for scheduling.

LANGUAGE: java
CODE:
conf.setTopologyPriority(int priority)

----------------------------------------

TITLE: Defining TupleToMessageMapper Interface
DESCRIPTION: Interface definition for mapping Storm tuples to RocketMQ messages, specifying methods for extracting message keys and values from tuples.

LANGUAGE: java
CODE:
public interface TupleToMessageMapper extends Serializable {
    String getKeyFromTuple(ITuple tuple);
    byte[] getValueFromTuple(ITuple tuple);
}

----------------------------------------

TITLE: Stopping Profiler in JSON
DESCRIPTION: Sample response from the /api/v1/topology/<id>/profiling/stop/<host-port> endpoint, showing the result of stopping a profiler on a worker.

LANGUAGE: json
CODE:
{
   "status": "ok",
   "id": "10.11.1.7:6701"
}

----------------------------------------

TITLE: Record to Tuple Mapper Interface
DESCRIPTION: Interface definition for mapping Kinesis records to Storm tuples. Includes methods for defining output fields and converting records to tuples.

LANGUAGE: java
CODE:
    Fields getOutputFields ();
    List<Object> getTuple (Record record);

----------------------------------------

TITLE: Executing DRPC Client Call in Java
DESCRIPTION: Demonstrates how to configure and execute a DRPC client call to compute the 'reach' function for a given URL.

LANGUAGE: java
CODE:
Config conf = new Config();
        conf.put("storm.thrift.transport", "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin");
        conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 3);
        conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL, 10);
        conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING, 20);
DRPCClient client = new DRPCClient(conf, "drpc-host", 3772);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Supervisor Synchronization Process (Clojure)
DESCRIPTION: The Supervisor runs two background functions: synchronize-supervisor to download code and update local assignments, and sync-processes to start/stop worker processes as necessary.

LANGUAGE: clojure
CODE:
(synchronize-supervisor conf storm-cluster-state sync-id)

----------------------------------------

TITLE: Cassandra Bound Query with Named Setters
DESCRIPTION: Demonstrates using bound queries with named parameters and field aliases for more explicit mapping.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
     async(
        boundQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (:ti, :ye, :pe, :ge, :tr);")
            .bind(
                field("ti").as("title"),
                field("ye").as("year")),
                field("pe").as("performer")),
                field("ge").as("genre")),
                field("tr").as("tracks"))
            ).byNamedSetters()
     )
);

----------------------------------------

TITLE: Defining TupleToMessageMapper Interface
DESCRIPTION: Interface definition for mapping Storm tuples to RocketMQ messages, requiring implementations for extracting message keys and values from tuples.

LANGUAGE: java
CODE:
public interface TupleToMessageMapper extends Serializable {
    String getKeyFromTuple(ITuple tuple);
    byte[] getValueFromTuple(ITuple tuple);
}

----------------------------------------

TITLE: Persistent Word Count Aggregation
DESCRIPTION: Example demonstrating persistent aggregation using MemcachedState for word counting

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
      topology.newStream("spout1", spout)
        .each(new Fields("sentence"), new Split(), new Fields("word"))
        .groupBy(new Fields("word"))
        .persistentAggregate(MemcachedState.opaque(serverLocations), new Count(), new Fields("count"))                
        .parallelismHint(6);

----------------------------------------

TITLE: Subscribing to Streams in a Bolt
DESCRIPTION: Demonstrates how to subscribe to specific streams from other components in a Storm topology using InputDeclarer.

LANGUAGE: java
CODE:
builder.setBolt("myBolt", new MyBolt())
    .shuffleGrouping("spout1", "stream1")
    .fieldsGrouping("bolt1", "stream2", new Fields("userId"));

----------------------------------------

TITLE: HTML/XML Syntax Highlighting Classes
DESCRIPTION: Defines syntax highlighting classes for HTML/XML markup including tags, attributes, values, comments, processing instructions, and CDATA sections.



----------------------------------------

TITLE: Configuring DRPC Servers in YAML
DESCRIPTION: This YAML configuration specifies the locations of DRPC servers, the HTTP port, and the Thrift transport plugin for Storm cluster setup.

LANGUAGE: yaml
CODE:
drpc.servers:
  - "drpc1.foo.com"
  - "drpc2.foo.com"
drpc.http.port: 8081
storm.thrift.transport: "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin"

----------------------------------------

TITLE: Creating and Transforming Streams in Java
DESCRIPTION: Demonstrates creating a StreamBuilder, generating a stream of sentences from a spout, transforming it into a stream of words, and printing the result.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

// a stream of sentences obtained from a source spout
Stream<String> sentences = builder.newStream(new RandomSentenceSpout()).map(tuple -> tuple.getString(0));

// a stream of words obtained by transforming (splitting) the stream of sentences
Stream<String> words = sentences.flatMap(s -> Arrays.asList(s.split(" ")));

// output operation that prints the words to console
words.forEach(w -> System.out.println(w));

----------------------------------------

TITLE: Adding Storm-Redis Maven Dependency
DESCRIPTION: XML snippet for including the storm-redis library as a Maven dependency in a project.

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>storm-redis</artifactId>
    <version>${storm.version}</version>
    <type>jar</type>
</dependency>

----------------------------------------

TITLE: Thrift API Definition for Submitting Storm Topology
DESCRIPTION: This Thrift API definition shows the submitTopology method used to submit a Storm topology. It includes parameters for the topology name, uploaded jar location, JSON configuration, and the StormTopology object.

LANGUAGE: java
CODE:
void submitTopology(1: string name, 2: string uploadedJarLocation, 3: string jsonConf, 4: StormTopology topology) throws (1: AlreadyAliveException e, 2: InvalidTopologyException ite);

----------------------------------------

TITLE: Defining Nimbus Metrics Table in Markdown
DESCRIPTION: Markdown table defining metrics specific to Nimbus instances, including metric names, types, and descriptions.

LANGUAGE: markdown
CODE:
| Metric Name | Type | Description |
|-------------|------|-------------|
| nimbus:files-upload-duration-ms | timer | Time it takes to upload a file from start to finish (Not Blobs, but this may change) |
| nimbus:longest-scheduling-time-ms | gauge | Longest time ever taken so far to schedule. This includes the current scheduling run, which is intended to detect if scheduling is stuck for some reason. |
| nimbus:mkAssignments-Errors | meter | tracks exceptions from mkAssignments |
| nimbus:num-activate-calls | meter | calls to the activate thrift method. |

----------------------------------------

TITLE: Running Storm JMS Example
DESCRIPTION: Maven command to execute the example topology locally.

LANGUAGE: shell
CODE:
$ mvn exec:java

----------------------------------------

TITLE: Local Mode Implementation using Java Queues in Apache Storm (Clojure)
DESCRIPTION: This code implements the message sending functionality for local mode using in-memory Java queues in Apache Storm, allowing for easy local usage without ZeroMQ installation.

LANGUAGE: clojure
CODE:
(github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/local.clj)

----------------------------------------

TITLE: Tuple Transfer Queue Implementation
DESCRIPTION: Implementation of the transfer function used by tasks to send tuples. Handles serialization and queuing of tuples for transfer.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L56

----------------------------------------

TITLE: Installing JZMQ from GitHub Fork
DESCRIPTION: Commands to clone a specific JZMQ fork from GitHub, navigate to the directory, and build and install it. This process involves using git to clone the repository, then running autogen.sh, configure, make, and make install commands.

LANGUAGE: bash
CODE:
#install jzmq
git clone https://github.com/nathanmarz/jzmq.git
cd jzmq
./autogen.sh
./configure
make
sudo make install

----------------------------------------

TITLE: Defining YAML Front Matter for Jekyll Documentation Page
DESCRIPTION: YAML front matter block defining metadata for a Jekyll-based documentation page about non-Java language support in Apache Storm.

LANGUAGE: YAML
CODE:
---
title: Support for Non-Java Languages
layout: documentation
documentation: true
---

----------------------------------------

TITLE: Code Distribution Interface Implementation in Java
DESCRIPTION: Interface responsible for distributing topology code across the cluster. Supports various distribution mechanisms including file system based and BitTorrent approaches.

LANGUAGE: java
CODE:
public interface ICodeDistributor {
    void prepare(Map conf);
    File upload(Path dirPath, String topologyId);
    List<File> download(Path destDirPath, String topologyid, File metafile);
    int getReplicationCount(String topologyId);
    void cleanup(String topologyid);
    void close(Map conf);
}

----------------------------------------

TITLE: Creating IConfigLoader Instance in Java
DESCRIPTION: Method for creating an IConfigLoader instance based on the scheme of the scheduler.config.loader.uri.

LANGUAGE: java
CODE:
ConfigLoaderFactoryService.createConfigLoader(Map<String, Object> conf)

----------------------------------------

TITLE: Registering Event Logger in Java for Storm Topology
DESCRIPTION: This code snippet shows how to register an event logger to a Storm topology using Java configuration. It demonstrates adding the FileBasedEventLogger to the topology's configuration.

LANGUAGE: java
CODE:
conf.registerEventLogger(org.apache.storm.metric.FileBasedEventLogger.class);

----------------------------------------

TITLE: Converting Docker Images to Squashfs for HDFS Storage
DESCRIPTION: Command to pull Docker images, convert layers to squashfs files, and push them to HDFS using the docker-to-squash.py script.

LANGUAGE: bash
CODE:
python3 docker-to-squash.py pull-build-push-update --hdfs-root hdfs://hostname:port/containers \
                      docker.xxx.com:4443/hadoop-user-images/storm/rhel7:20201202-232133,storm/rhel7:dev_current --log DEBUG --bootstrap

----------------------------------------

TITLE: Running WordCountTopology in Eclipse
DESCRIPTION: This snippet demonstrates how to run the WordCountTopology.java file in Eclipse to test the Storm project setup. When executed, it will emit messages to the console for 10 seconds.

LANGUAGE: Java
CODE:
WordCountTopology.java

----------------------------------------

TITLE: Parsing Apache Logs to JSON
DESCRIPTION: Python script to parse fake Apache logs and convert them to JSON format with an auto-incrementing ID.

LANGUAGE: python
CODE:
import sys
import apache_log_parser
import json

auto_incr_id = 1
parser_format = '%a - - %t %D "%r" %s %b "%{Referer}i" "%{User-Agent}i"'
line_parser = apache_log_parser.make_parser(parser_format)
while True:
  # we'll use pipe
  line = sys.stdin.readline()
  if not line:
    break
  parsed_dict = line_parser(line)
  parsed_dict['id'] = auto_incr_id
  auto_incr_id += 1

  # works only python 2, but I don't care cause it's just a test module :)
  parsed_dict = {k.upper(): v for k, v in parsed_dict.iteritems() if not k.endswith('datetimeobj')}
  print json.dumps(parsed_dict)

----------------------------------------

TITLE: Killing Workers on a Supervisor
DESCRIPTION: Terminates all worker processes running on the local supervisor node.

LANGUAGE: shell
CODE:
storm kill_workers

----------------------------------------

TITLE: Optimized QueryFunction for Bulk Location Lookup
DESCRIPTION: Improved QueryLocation function that uses bulk operations for efficient database access.

LANGUAGE: java
CODE:
public class QueryLocation extends BaseQueryFunction<LocationDB, String> {
    public List<String> batchRetrieve(LocationDB state, List<TridentTuple> inputs) {
        List<Long> userIds = new ArrayList<Long>();
        for(TridentTuple input: inputs) {
            userIds.add(input.getLong(0));
        }
        return state.bulkGetLocations(userIds);
    }

    public void execute(TridentTuple tuple, String location, TridentCollector collector) {
        collector.emit(new Values(location));
    }    
}

----------------------------------------

TITLE: Implementing a FlatMap Function for Sentence Splitting
DESCRIPTION: Demonstrates how to create a flatMap function that splits sentences into words.

LANGUAGE: java
CODE:
public class Split extends FlatMapFunction {
  @Override
  public Iterable<Values> execute(TridentTuple input) {
    List<Values> valuesList = new ArrayList<>();
    for (String word : input.getString(0).split(" ")) {
      valuesList.add(new Values(word));
    }
    return valuesList;
  }
}

----------------------------------------

TITLE: Defining submitTopology Method in Java for Storm
DESCRIPTION: This Java code defines the submitTopology method in Storm's Thrift API, used to submit a topology to Nimbus. It specifies the required parameters and potential exceptions.

LANGUAGE: java
CODE:
void submitTopology(1: string name, 2: string uploadedJarLocation, 3: string jsonConf, 4: StormTopology topology) throws (1: AlreadyAliveException e, 2: InvalidTopologyException ite);

----------------------------------------

TITLE: Configuring Bolt Parallelism in Storm Topology (Java)
DESCRIPTION: Demonstrates how to set the number of executors and tasks for a bolt in a Storm topology. This snippet configures the 'green-bolt' to run with 2 executors and 4 tasks, using shuffle grouping from 'blue-spout'.

LANGUAGE: java
CODE:
topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

----------------------------------------

TITLE: Implementing Trident State for Druid
DESCRIPTION: Example of implementing Trident state integration with Druid, showing stream creation and state persistence configuration.

LANGUAGE: java
CODE:
    DruidBeamFactory druidBeamFactory = new SampleDruidBeamFactoryImpl(new HashMap<String, Object>());
    ITupleDruidEventMapper<Map<String, Object>> eventMapper = new TupleDruidEventMapper<>(TupleDruidEventMapper.DEFAULT_FIELD_NAME);

    final Stream stream = tridentTopology.newStream("batch-event-gen", new SimpleBatchSpout(10));

    stream.peek(new Consumer() {
        @Override
        public void accept(TridentTuple input) {
             LOG.info("########### Received tuple: [{}]", input);
         }
    }).partitionPersist(new DruidBeamStateFactory<Map<String, Object>>(druidBeamFactory, eventMapper), new Fields("event"), new DruidBeamStateUpdater());

----------------------------------------

TITLE: HTML Redirect to Storm FAQ Documentation
DESCRIPTION: Implementation of an HTML meta refresh redirect to the Apache Storm FAQ page with a canonical link reference for SEO purposes.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/FAQ.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/FAQ.html" />

----------------------------------------

TITLE: Setting up EsState for Elasticsearch in Java Trident
DESCRIPTION: Configures EsState for use with Trident in Storm. Requires an EsConfig for cluster configuration, an EsTupleMapper, and sets up a StateFactory for persistence.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();

StateFactory factory = new EsStateFactory(esConfig, tupleMapper);
TridentState state = stream.partitionPersist(factory, esFields, new EsUpdater(), new Fields());

----------------------------------------

TITLE: Creating EsConfig for Elasticsearch Cluster Configuration in Java
DESCRIPTION: These snippets show two ways to create an EsConfig object for configuring the Elasticsearch cluster connection. The first uses basic settings, while the second includes additional parameters for advanced configuration.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});

LANGUAGE: java
CODE:
Map<String, String> additionalParameters = new HashMap<>();
additionalParameters.put("client.transport.sniff", "true");
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"}, additionalParameters);

----------------------------------------

TITLE: Configuring ExpandUrl Bolt with Shuffle Grouping in Storm
DESCRIPTION: This snippet demonstrates how to set up an ExpandUrl bolt with shuffle grouping in a Storm topology. This configuration distributes tuples randomly across all tasks of the bolt.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .shuffleGrouping(1);

----------------------------------------

TITLE: Task Routing Map Implementation in Apache Storm (Clojure)
DESCRIPTION: Tasks maintain a routing map from stream ID to component ID to stream grouping function.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L198

----------------------------------------

TITLE: Storm Integration Links in Markdown
DESCRIPTION: Markdown formatted list of links to various Storm DSL implementations and adapters, including Clojure, Scala, JRuby, Esper, Perl, and F# integrations.

LANGUAGE: markdown
CODE:
* [Clojure DSL](Clojure-DSL.html)
* [Scala DSL](https://github.com/velvia/ScalaStorm)
* [JRuby DSL](https://github.com/colinsurprenant/redstorm)
* [Storm/Esper integration](https://github.com/tomdz/storm-esper): Streaming SQL on top of Storm
* [io-storm](https://github.com/dan-blanchard/io-storm): Perl multilang adapter
* [FsShelter](https://github.com/Prolucid/FsShelter): F# DSL and runtime with protobuf multilang

----------------------------------------

TITLE: Configuring Generic Cluster Resources in YAML
DESCRIPTION: This snippet shows how to specify node resource availability in the storm.yaml configuration file. It demonstrates setting the GPU count for a supervisor node.

LANGUAGE: yaml
CODE:
supervisor.resources.map: {[type<String>] : [amount<Double>]}

LANGUAGE: yaml
CODE:
supervisor.resources.map: {"gpu.count" : 2.0}

----------------------------------------

TITLE: Synchronizing Supervisor with Assignments (Clojure)
DESCRIPTION: The synchronize-supervisor function in Supervisor downloads code from Nimbus for assigned topologies and writes local assignments. It's called on ZooKeeper assignment changes and every 10 seconds.

LANGUAGE: clojure
CODE:
(synchronize-supervisor)

----------------------------------------

TITLE: Installing ZeroMQ for Apache Storm
DESCRIPTION: Commands to download, extract, configure, and install ZeroMQ 2.1.7, which is the recommended version for use with Apache Storm. This process requires root privileges for the final installation step.

LANGUAGE: bash
CODE:
wget http://download.zeromq.org/zeromq-2.1.7.tar.gz
tar -xzf zeromq-2.1.7.tar.gz
cd zeromq-2.1.7
./configure
make
sudo make install

----------------------------------------

TITLE: Implementing Custom Serialization Interface in Java for Storm
DESCRIPTION: This code snippet shows the ISerialization interface that must be implemented to create custom serializers in Storm. It includes methods for accepting a class, serializing an object to a binary format, and deserializing from a binary format.

LANGUAGE: java
CODE:
public interface ISerialization<T> {
    public boolean accept(Class c);
    public void serialize(T object, DataOutputStream stream) throws IOException;
    public T deserialize(DataInputStream stream) throws IOException;
}

----------------------------------------

TITLE: Initializing Trident Topology with Regular Storm Spout
DESCRIPTION: Example showing how to create a new stream in a Trident topology using a regular Storm IRichSpout. The spout requires a unique identifier that will be used for storing metadata in Zookeeper.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
topology.newStream("myspoutid", new MyRichSpout());

----------------------------------------

TITLE: Submitting Topology with Distributed Cache Configuration
DESCRIPTION: Example of submitting a Storm topology with distributed cache configuration. This command starts the 'word_count' topology and configures it to access two cached files with keys 'key1' and 'key2'.

LANGUAGE: bash
CODE:
storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar org.apache.storm.starter.clj.word_count test_topo -c topology.blobstore.map='{"key1":{"localname":"blob_file", "uncompress":false},"key2":{}}'

----------------------------------------

TITLE: Launching OCI/Squashfs Container with runc
DESCRIPTION: Command used by worker-launcher to launch the OCI/Squashfs container using runc.

LANGUAGE: bash
CODE:
/usr/bin/runc run -d \
              --pid-file /home/y/var/storm/workers/1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb/artifacts/container-1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb.pid \
              -b /home/y/var/storm/workers/1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb \
              6703-1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb

----------------------------------------

TITLE: Registering Metrics Consumer in Java
DESCRIPTION: Example showing how to register a metrics consumer programmatically in a Storm topology configuration

LANGUAGE: java
CODE:
conf.registerMetricsConsumer(org.apache.storm.metric.LoggingMetricsConsumer.class, 1);

----------------------------------------

TITLE: Configuring UI Authentication Filter in YAML
DESCRIPTION: Example YAML configuration for setting up an authentication filter for the Storm UI.

LANGUAGE: yaml
CODE:
ui.filter: "filter.class"
ui.filter.params: "param1":"value1"

----------------------------------------

TITLE: Initializing a Storm Topology with KestrelSpout in Java
DESCRIPTION: This snippet demonstrates how to set up a basic Storm topology using KestrelSpout for sentence processing. It includes a spout for reading sentences from a Kestrel queue and two bolts for splitting sentences and counting words.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("sentences", new KestrelSpout("kestrel.backtype.com",
                                               22133,
                                               "sentence_queue",
                                               new StringScheme()));
builder.setBolt("split", new SplitSentence(), 10)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 20)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Implementing Batch Processing Bolt
DESCRIPTION: Implementation of a BatchCount bolt that processes batches of tuples and emits partial counts for aggregation.

LANGUAGE: java
CODE:
public static class BatchCount extends BaseBatchBolt {
    Object _id;
    BatchOutputCollector _collector;

    int _count = 0;

    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
        _collector = collector;
        _id = id;
    }

    @Override
    public void execute(Tuple tuple) {
        _count++;
    }

    @Override
    public void finishBatch() {
        _collector.emit(new Values(_id, _count));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "count"));
    }
}

----------------------------------------

TITLE: File Config Loader Example
DESCRIPTION: Configuration example for loading from a local file system.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "file:///path/to/my/config.yaml"

----------------------------------------

TITLE: Multi-Anchored Tuple Emission in Storm
DESCRIPTION: Example showing how to emit tuples anchored to multiple parent tuples

LANGUAGE: java
CODE:
List<Tuple> anchors = new ArrayList<Tuple>();
anchors.add(tuple1);
anchors.add(tuple2);
_collector.emit(anchors, new Values(1, 2, 3));

----------------------------------------

TITLE: Setting Up Database Schema in SQL
DESCRIPTION: This SQL script sets up the necessary tables and inserts initial data for the example Storm-JDBC integration topology.

LANGUAGE: sql
CODE:
create table if not exists user (user_id integer, user_name varchar(100), dept_name varchar(100), create_date date);
create table if not exists department (dept_id integer, dept_name varchar(100));
create table if not exists user_department (user_id integer, dept_id integer);
insert into department values (1, 'R&D');
insert into department values (2, 'Finance');
insert into department values (3, 'HR');
insert into department values (4, 'Sales');
insert into user_department values (1, 1);
insert into user_department values (2, 2);
insert into user_department values (3, 3);
insert into user_department values (4, 4);
select dept_name from department, user_department where department.dept_id = user_department.dept_id and user_department.user_id = ?;

----------------------------------------

TITLE: Initializing DRPC Client in Java
DESCRIPTION: Code showing how to configure and create a DRPC client with custom transport and retry settings.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.put("storm.thrift.transport", "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin");
conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 3);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL, 10);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING, 20);
DRPCClient client = new DRPCClient(conf, "drpc-host", 3772);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Specifying Classpath Wildcards in Java 6+
DESCRIPTION: This snippet demonstrates the use of classpath wildcards in Java 6 and later versions to include all JAR files in a directory without explicitly listing them. This approach is used by Storm to shorten classpath declarations and avoid process command length issues.

LANGUAGE: java
CODE:
foo/bar/*

----------------------------------------

TITLE: Defining a Storm Topology in Clojure
DESCRIPTION: Example of defining a Storm topology using the Clojure DSL, including spout and bolt specifications with different groupings and parallelism hints.

LANGUAGE: clojure
CODE:
(topology
 {"1" (spout-spec sentence-spout)
  "2" (spout-spec (sentence-spout-parameterized
                   ["the cat jumped over the door"
                    "greetings from a faraway land"])
                   :p 2)}
 {"3" (bolt-spec {"1" :shuffle "2" :shuffle}
                 split-sentence
                 :p 5)
  "4" (bolt-spec {"3" ["word"]}
                 word-count
                 :p 6)})

----------------------------------------

TITLE: Storm Ack Command JSON Format
DESCRIPTION: JSON structure for acknowledging tuple processing in Storm. Includes command type and tuple ID to acknowledge.

LANGUAGE: json
CODE:
{
    "command": "ack",
    "id": 123123
}

----------------------------------------

TITLE: Configuring MongoDB Insert Bolt
DESCRIPTION: Example of configuring a MongoDB insert bolt with connection URL and mapper

LANGUAGE: java
CODE:
String url = "mongodb://127.0.0.1:27017/test";
String collectionName = "wordcount";

MongoMapper mapper = new SimpleMongoMapper()
        .withFields("word", "count");

MongoInsertBolt insertBolt = new MongoInsertBolt(url, collectionName, mapper);

----------------------------------------

TITLE: Configuring Windows Local Directory for Storm in YAML
DESCRIPTION: This configuration sets the local directory for Storm on Windows systems. It's an alternative to the Unix-style path configuration.

LANGUAGE: yaml
CODE:
storm.local.dir: "C:\\storm-local"

----------------------------------------

TITLE: Storm SQL Query for Filtering Error Logs
DESCRIPTION: SQL script to filter error logs (status >= 400) from Apache logs and store them in a separate Kafka topic.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://apache-logs?bootstrap-servers=localhost:9092'
CREATE EXTERNAL TABLE APACHE_ERROR_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_ELAPSED_MS INT) LOCATION 'kafka://apache-error-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO APACHE_ERROR_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, (TIME_US / 1000) AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (CAST(STATUS AS INT) / 100) >= 4

----------------------------------------

TITLE: Configuring Digest Authentication for Pacemaker
DESCRIPTION: JAAS configuration for digest authentication in Pacemaker.

LANGUAGE: java
CODE:
PacemakerDigest {
    username="some username"
    password="some password";
};

----------------------------------------

TITLE: Configuring URL Expansion with Shuffle Grouping in Storm
DESCRIPTION: Demonstrates setting up a bolt for URL expansion using shuffle grouping, which distributes tuples randomly across bolt tasks. This configuration is less cache-efficient as the same URL may be processed by different tasks.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .shuffleGrouping(1);

----------------------------------------

TITLE: Creating a Bucketed Hive Table for Streaming (SQL)
DESCRIPTION: SQL command to create a bucketed Hive table with ORC format, which is required for using the Hive Streaming API.

LANGUAGE: sql
CODE:
create table test_table ( id INT, name STRING, phone STRING, street STRING) partitioned by (city STRING, state STRING) stored as orc tblproperties ("orc.compress"="NONE");

----------------------------------------

TITLE: Killing a Storm Topology
DESCRIPTION: Terminates a running Storm topology with optional wait time before shutdown.

LANGUAGE: bash
CODE:
storm kill topology-name [-w wait-time-secs]

----------------------------------------

TITLE: Maven Shade Plugin Configuration
DESCRIPTION: XML configuration for Maven Shade Plugin to create an uber jar containing all dependencies for the Storm-Solr integration.

LANGUAGE: xml
CODE:
<plugin>
     <groupId>org.apache.maven.plugins</groupId>
     <artifactId>maven-shade-plugin</artifactId>
     <version>2.4.1</version>
     <executions>
         <execution>
             <phase>package</phase>
             <goals>
                 <goal>shade</goal>
             </goals>
             <configuration>
                 <transformers>
                     <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                         <mainClass>org.apache.storm.solr.topology.SolrJsonTopology</mainClass>
                     </transformer>
                 </transformers>
             </configuration>
         </execution>
     </executions>
</plugin>

----------------------------------------

TITLE: Debugging Kryo ConcurrentModificationException in Java
DESCRIPTION: This stack trace demonstrates a common error that occurs when emitting mutable objects as output tuples in Storm. The exception is caused by modifying an object while it's being serialized for network transmission.

LANGUAGE: java
CODE:
java.lang.RuntimeException: java.util.ConcurrentModificationException
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:84)
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:55)
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:56)
	at org.apache.storm.disruptor$consume_loop_STAR_$fn__1597.invoke(disruptor.clj:67)
	at org.apache.storm.util$async_loop$fn__465.invoke(util.clj:377)
	at clojure.lang.AFn.run(AFn.java:24)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.util.ConcurrentModificationException
	at java.util.LinkedHashMap$LinkedHashIterator.nextEntry(LinkedHashMap.java:390)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:409)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:408)
	at java.util.HashMap.writeObject(HashMap.java:1016)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:959)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1480)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at org.apache.storm.serialization.SerializableSerializer.write(SerializableSerializer.java:21)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:554)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:77)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:472)
	at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:27)

----------------------------------------

TITLE: Initializing EsPercolateBolt for Elasticsearch Percolation in Java
DESCRIPTION: This code shows how to create an EsPercolateBolt for sending percolate requests to Elasticsearch. It requires an EsConfig for cluster configuration and an EsTupleMapper for mapping tuples to percolate requests.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsPercolateBolt percolateBolt = new EsPercolateBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Configuring Storm MetricStore with YAML
DESCRIPTION: Configuration options for Storm's metric storage system, including MetricStore class implementation, RocksDB location, retention settings, and cache capacity parameters.

LANGUAGE: yaml
CODE:
storm.metricstore.class: "org.apache.storm.metricstore.rocksdb.RocksDbStore"
storm.metricprocessor.class: "org.apache.storm.metricstore.NimbusMetricProcessor"
storm.metricstore.rocksdb.location: "storm_rocks"
storm.metricstore.rocksdb.create_if_missing: true
storm.metricstore.rocksdb.metadata_string_cache_capacity: 4000
storm.metricstore.rocksdb.retention_hours: 240

----------------------------------------

TITLE: Configuring Storm MetricStore with YAML
DESCRIPTION: Configuration options for Storm's metric storage system, including MetricStore class implementation, RocksDB location, retention settings, and cache capacity parameters.

LANGUAGE: yaml
CODE:
storm.metricstore.class: "org.apache.storm.metricstore.rocksdb.RocksDbStore"
storm.metricprocessor.class: "org.apache.storm.metricstore.NimbusMetricProcessor"
storm.metricstore.rocksdb.location: "storm_rocks"
storm.metricstore.rocksdb.create_if_missing: true
storm.metricstore.rocksdb.metadata_string_cache_capacity: 4000
storm.metricstore.rocksdb.retention_hours: 240

----------------------------------------

TITLE: Implementing DRPC Topology with LinearDRPCTopologyBuilder in Java
DESCRIPTION: This example demonstrates how to implement a simple DRPC topology using LinearDRPCTopologyBuilder. It creates a topology that appends '!' to the input argument.

LANGUAGE: java
CODE:
public static class ExclaimBolt extends BaseBasicBolt {
    public void execute(Tuple tuple, BasicOutputCollector collector) {
        String input = tuple.getString(1);
        collector.emit(new Values(tuple.getValue(0), input + "!"));
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "result"));
    }
}

public static void main(String[] args) throws Exception {
    LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("exclamation");
    builder.addBolt(new ExclaimBolt(), 3);
    // ...
}

----------------------------------------

TITLE: Configuring Spring ApplicationContext for JMS Integration with Storm
DESCRIPTION: This XML configuration defines a Spring applicationContext.xml file for JMS integration with Storm. It sets up a queue, a topic, and a connection factory for ActiveMQ. The configuration includes necessary namespace declarations and schema locations for Spring and ActiveMQ.

LANGUAGE: XML
CODE:
<?xml version="1.0" encoding="UTF-8"?>
<beans 
  xmlns="http://www.springframework.org/schema/beans" 
  xmlns:amq="http://activemq.apache.org/schema/core"
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.0.xsd
  http://activemq.apache.org/schema/core http://activemq.apache.org/schema/core/activemq-core.xsd">

	<amq:queue id="notificationQueue" physicalName="backtype.storm.contrib.example.queue" />
	
	<amq:topic id="notificationTopic" physicalName="backtype.storm.contrib.example.topic" />

	<amq:connectionFactory id="jmsConnectionFactory"
		brokerURL="tcp://localhost:61616" />
	
</beans>

----------------------------------------

TITLE: External Table Definition
DESCRIPTION: Example of creating an external table that connects to Kafka

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-hosts=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Implementing DRPC Topology with LinearDRPCTopologyBuilder in Java
DESCRIPTION: This example demonstrates how to implement a simple DRPC topology using LinearDRPCTopologyBuilder. It creates a topology that appends '!' to the input argument.

LANGUAGE: java
CODE:
public static class ExclaimBolt extends BaseBasicBolt {
    public void execute(Tuple tuple, BasicOutputCollector collector) {
        String input = tuple.getString(1);
        collector.emit(new Values(tuple.getValue(0), input + "!"));
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "result"));
    }
}

public static void main(String[] args) throws Exception {
    LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("exclamation");
    builder.addBolt(new ExclaimBolt(), 3);
    // ...
}

----------------------------------------

TITLE: Defining Leader Election Interface in Java for Storm Nimbus
DESCRIPTION: Java interface defining methods for leader election, including joining/leaving the leader queue, checking leadership status, and getting leader and nimbus addresses.

LANGUAGE: java
CODE:
public interface ILeaderElector {
    /**
     * queue up for leadership lock. The call returns immediately and the caller                     
     * must check isLeader() to perform any leadership action.
     */
    void addToLeaderLockQueue();

    /**
     * Removes the caller from the leader lock queue. If the caller is leader
     * also releases the lock.
     */
    void removeFromLeaderLockQueue();

    /**
     *
     * @return true if the caller currently has the leader lock.
     */
    boolean isLeader();

    /**
     *
     * @return the current leader's address , throws exception if noone has has    lock.
     */
    InetSocketAddress getLeaderAddress();

    /**
     * 
     * @return list of current nimbus addresses, includes leader.
     */
    List<InetSocketAddress> getAllNimbusAddresses();
}

----------------------------------------

TITLE: Implementing Aggregators in Trident
DESCRIPTION: Shows how to implement different types of aggregators: CombinerAggregator, ReducerAggregator, and Aggregator.

LANGUAGE: java
CODE:
public class Count implements CombinerAggregator<Long> {
    public Long init(TridentTuple tuple) {
        return 1L;
    }

    public Long combine(Long val1, Long val2) {
        return val1 + val2;
    }

    public Long zero() {
        return 0L;
    }
}

public class Count implements ReducerAggregator<Long> {
    public Long init() {
        return 0L;
    }
    
    public Long reduce(Long curr, TridentTuple tuple) {
        return curr + 1;
    }
}

public class CountAgg extends BaseAggregator<CountState> {
    static class CountState {
        long count = 0;
    }

    public CountState init(Object batchId, TridentCollector collector) {
        return new CountState();
    }

    public void aggregate(CountState state, TridentTuple tuple, TridentCollector collector) {
        state.count+=1;
    }

    public void complete(CountState state, TridentCollector collector) {
        collector.emit(new Values(state.count));
    }
}

----------------------------------------

TITLE: Retrieving Cluster Summary in JSON
DESCRIPTION: Sample response from the /api/v1/cluster/summary endpoint, showing overall cluster statistics and resource utilization.

LANGUAGE: json
CODE:
{
  "stormVersion": "0.9.2-incubating-SNAPSHOT",
  "supervisors": 1,
  "slotsTotal": 4,
  "slotsUsed": 3,
  "slotsFree": 1,
  "executorsTotal": 28,
  "tasksTotal": 28,
  "schedulerDisplayResource": true,
  "totalMem": 4096.0,
  "totalCpu": 400.0,
  "availMem": 1024.0,
  "availCPU": 250.0,
  "memAssignedPercentUtil": 75.0,
  "cpuAssignedPercentUtil": 37.5
}

----------------------------------------

TITLE: Defining Storm Topology in Clojure
DESCRIPTION: Example of defining a Storm topology with spouts and bolts using the topology function. Shows how to wire components together with different stream groupings and parallelism hints.

LANGUAGE: clojure
CODE:
(topology
 {"1" (spout-spec sentence-spout)
  "2" (spout-spec (sentence-spout-parameterized
                   ["the cat jumped over the door"
                    "greetings from a faraway land"])
                   :p 2)}
 {"3" (bolt-spec {"1" :shuffle "2" :shuffle}
                 split-sentence
                 :p 5)
  "4" (bolt-spec {"3" ["word"]}
                 word-count
                 :p 6)})

----------------------------------------

TITLE: Creating Kafka Topics for Apache Logs
DESCRIPTION: Bash commands to create Kafka topics for storing different types of Apache logs.

LANGUAGE: bash
CODE:
kafka-topics --create --topic apache-logs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
kafka-topics --create --topic apache-errorlogs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
kafka-topics --create --topic apache-slowlogs --zookeeper localhost:2181 --replication-factor 1 --partitions 5

----------------------------------------

TITLE: Trident Aggregator Implementation
DESCRIPTION: Example of implementing a count aggregator in Trident

LANGUAGE: java
CODE:
public class CountAgg extends BaseAggregator<CountState> {
    static class CountState {
        long count = 0;
    }

    public CountState init(Object batchId, TridentCollector collector) {
        return new CountState();
    }

    public void aggregate(CountState state, TridentTuple tuple, TridentCollector collector) {
        state.count+=1;
    }

    public void complete(CountState state, TridentCollector collector) {
        collector.emit(new Values(state.count));
    }
}

----------------------------------------

TITLE: Installing Flux Dependencies with Maven
DESCRIPTION: Maven dependency configuration for including Flux core and wrappers in a Storm topology project

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>flux-core</artifactId>
    <version>${storm.version}</version>
</dependency>

----------------------------------------

TITLE: Configuring Static Kafka Broker Hosts
DESCRIPTION: Example of setting up static broker-partition mappings using StaticHosts and GlobalPartitionInformation classes.

LANGUAGE: java
CODE:
Broker brokerForPartition0 = new Broker("localhost");
Broker brokerForPartition1 = new Broker("localhost", 9092);
Broker brokerForPartition2 = new Broker("localhost:9092");
GlobalPartitionInformation partitionInfo = new GlobalPartitionInformation();
partitionInfo.addPartition(0, brokerForPartition0);
partitionInfo.addPartition(1, brokerForPartition1);
partitionInfo.addPartition(2, brokerForPartition2);
StaticHosts hosts = new StaticHosts(partitionInfo);

----------------------------------------

TITLE: Bolt Input Tuple JSON Structure
DESCRIPTION: JSON structure received by bolts containing tuple information including ID, component, stream, task, and values.

LANGUAGE: json
CODE:
{
	"id": "-6955786537413359385",
	"comp": "1",
	"stream": "1",
	"task": 9,
	"tuple": ["snow white and the seven dwarfs", "field2", 3]
}

----------------------------------------

TITLE: Configuring Supervisor Slots and Ports
DESCRIPTION: Configuration for defining the number of worker slots and their corresponding ports on each supervisor node.

LANGUAGE: yaml
CODE:
supervisor.slots.ports:
    - 6700
    - 6701
    - 6702
    - 6703

----------------------------------------

TITLE: Applying a Custom Function to a Trident Stream
DESCRIPTION: Shows how to apply a custom function to a Trident stream, adding a new field to the output.

LANGUAGE: java
CODE:
mystream.each(new Fields("b"), new MyFunction(), new Fields("d")))

----------------------------------------

TITLE: Virtual Port Implementation in Apache Storm
DESCRIPTION: Implementation of the virtual port used for routing messages to tasks in distributed mode.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/zilch/virtual_port.clj

----------------------------------------

TITLE: Configuring UI/Logviewer Filter in YAML
DESCRIPTION: YAML configuration for setting up authentication filters for Storm UI and logviewer components.

LANGUAGE: yaml
CODE:
ui.filter: "filter.class"
ui.filter.params: "param1":"value1"
logviewer.filter: "filter.class"
logviewer.filter.params: "param1":"value1"

----------------------------------------

TITLE: Trident FlatMap Function Example
DESCRIPTION: Example of a flatMap function that splits sentences into words

LANGUAGE: java
CODE:
public class Split extends FlatMapFunction {
  @Override
  public Iterable<Values> execute(TridentTuple input) {
    List<Values> valuesList = new ArrayList<>();
    for (String word : input.getString(0).split(" ")) {
      valuesList.add(new Values(word));
    }
    return valuesList;
  }
}

----------------------------------------

TITLE: Defining Storm DSLs and Multi-Language Adapters in Markdown
DESCRIPTION: This Markdown snippet lists various DSLs and multi-language adapters for Apache Storm, including links to their respective implementations or documentation pages. It covers Clojure, Scala, JRuby, Esper integration, Perl, and F# adapters.

LANGUAGE: Markdown
CODE:
---
title: Storm DSLs and Multi-Lang Adapters
layout: documentation
documentation: true
---
* [Clojure DSL](Clojure-DSL.html)
* [Scala DSL](https://github.com/velvia/ScalaStorm)
* [JRuby DSL](https://github.com/colinsurprenant/redstorm)
* [Storm/Esper integration](https://github.com/tomdz/storm-esper): Streaming SQL on top of Storm
* [io-storm](https://github.com/dan-blanchard/io-storm): Perl multilang adapter
* [FsShelter](https://github.com/Prolucid/FsShelter): F# DSL and runtime with protobuf multilang

----------------------------------------

TITLE: Implementing SplitSentence Bolt with Anchoring
DESCRIPTION: Implementation of a bolt that splits sentences into words while maintaining tuple anchoring for reliability.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseRichBolt {
        OutputCollector _collector;
        
        public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
            _collector = collector;
        }

        public void execute(Tuple tuple) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                _collector.emit(tuple, new Values(word));
            }
            _collector.ack(tuple);
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Implementing FailedMessageRetryHandler Interface in Java
DESCRIPTION: This code snippet demonstrates the FailedMessageRetryHandler interface, which is used to manage retry logic for failed messages in the Kinesis spout. It includes methods for handling failed messages, retrieving the next message to retry, and acknowledging successful retries.

LANGUAGE: java
CODE:
    boolean failed (KinesisMessageId messageId);
    KinesisMessageId getNextFailedMessageToRetry ();
    void failedMessageEmitted (KinesisMessageId messageId);
    void acked (KinesisMessageId messageId);

----------------------------------------

TITLE: Task ID Determination for Tuple Sending in Apache Storm (Clojure)
DESCRIPTION: This code determines the task IDs to send tuples to for both regular and direct stream emits in Storm.

LANGUAGE: Clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L207

----------------------------------------

TITLE: Java-based Storm Topology Configuration
DESCRIPTION: Example of creating an MQTT-enabled Storm topology using the Java API

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
MqttOptions options = new MqttOptions();
options.setTopics(Arrays.asList("/users/tgoetz/#"));
options.setCleanConnection(false);
MqttSpout spout = new MqttSpout(new StringMessageMapper(), options);

MqttBolt bolt = new LogInfoBolt();

builder.setSpout("mqtt-spout", spout);
builder.setBolt("log-bolt", bolt).shuffleGrouping("mqtt-spout");

return builder.createTopology();

----------------------------------------

TITLE: Implementing Global Count Update Bolt in Java
DESCRIPTION: Example committer bolt that maintains exactly-once counting semantics by updating a global count in the database.

LANGUAGE: java
CODE:
public static class UpdateGlobalCount extends BaseTransactionalBolt implements ICommitter {
    TransactionAttempt _attempt;
    BatchOutputCollector _collector;

    int _sum = 0;

    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, TransactionAttempt attempt) {
        _collector = collector;
        _attempt = attempt;
    }

    @Override
    public void execute(Tuple tuple) {
        _sum+=tuple.getInteger(1);
    }

    @Override
    public void finishBatch() {
        Value val = DATABASE.get(GLOBAL_COUNT_KEY);
        Value newval;
        if(val == null || !val.txid.equals(_attempt.getTransactionId())) {
            newval = new Value();
            newval.txid = _attempt.getTransactionId();
            if(val==null) {
                newval.count = _sum;
            } else {
                newval.count = _sum + val.count;
            }
            DATABASE.put(GLOBAL_COUNT_KEY, newval);
        } else {
            newval = val;
        }
        _collector.emit(new Values(_attempt, newval.count));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "sum"));
    }
}

----------------------------------------

TITLE: Component-Specific Configuration Override Options in Storm
DESCRIPTION: Lists the configuration parameters that can be overridden on a per-bolt/per-spout basis in Storm 0.7.0 and later.

LANGUAGE: text
CODE:
1. "topology.debug"
2. "topology.max.spout.pending"
3. "topology.max.task.parallelism"
4. "topology.kryo.register"

----------------------------------------

TITLE: MIT License Text for JSZip
DESCRIPTION: The MIT License text granting permission to use, modify and distribute JSZip under permissive terms

LANGUAGE: text
CODE:
Copyright (c) 2009-2016 Stuart Knightley, David Duponchel, Franz Buchinger, AntÃ³nio Afonso

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

----------------------------------------

TITLE: Component-Specific Configuration Override Options in Storm
DESCRIPTION: Lists the configuration parameters that can be overridden on a per-bolt/per-spout basis in Storm 0.7.0 and later.

LANGUAGE: text
CODE:
1. "topology.debug"
2. "topology.max.spout.pending"
3. "topology.max.task.parallelism"
4. "topology.kryo.register"

----------------------------------------

TITLE: Chaining Map and FlatMap Operations in Trident
DESCRIPTION: Example of chaining map and flatMap operations on a Trident stream.

LANGUAGE: java
CODE:
mystream.flatMap(new Split()).map(new UpperCase())

----------------------------------------

TITLE: Creating CassandraWriterBolt with Static Bound Query in Java
DESCRIPTION: Shows how to create a CassandraWriterBolt using a static bound query that binds all tuple fields.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
     async(
        boundQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);")
            .bind(all());
     )
);

----------------------------------------

TITLE: Configuring NUMA Process Binding with CgroupManager in Linux
DESCRIPTION: Command used by CgroupManager to bind worker processes to specific NUMA zones using numactl, which controls CPU and memory binding.

LANGUAGE: bash
CODE:
numactl --cpunodebind=<numaId>> --membind=<numaId> <worker launch command>

----------------------------------------

TITLE: Storm Nimbus submitTopology Method Definition
DESCRIPTION: Thrift API method signature for submitting a topology to Storm. Takes topology name, jar location, JSON configuration, and topology structure as parameters.

LANGUAGE: java
CODE:
void submitTopology(1: string name, 2: string uploadedJarLocation, 3: string jsonConf, 4: StormTopology topology) throws (1: AlreadyAliveException e, 2: InvalidTopologyException ite);

----------------------------------------

TITLE: Configuring Pacemaker Servers in Storm
DESCRIPTION: Specify the hosts running Pacemaker daemons in the Storm cluster configuration.

LANGUAGE: yaml
CODE:
pacemaker.servers:
    - somehost.mycompany.com
    - someotherhost.mycompany.com

----------------------------------------

TITLE: Creating Core KafkaSpout Configuration
DESCRIPTION: Example showing how to configure and initialize a core KafkaSpout instance

LANGUAGE: java
CODE:
BrokerHosts hosts = new ZkHosts(zkConnString);
SpoutConfig spoutConfig = new SpoutConfig(hosts, topicName, "/" + topicName, UUID.randomUUID().toString());
spoutConfig.scheme = new SchemeAsMultiScheme(new StringScheme());
KafkaSpout kafkaSpout = new KafkaSpout(spoutConfig);

----------------------------------------

TITLE: Implementing Batch Count Bolt in Storm
DESCRIPTION: Implementation of a BatchCount bolt that processes batches of tuples and emits partial counts.

LANGUAGE: java
CODE:
public static class BatchCount extends BaseBatchBolt {
    Object _id;
    BatchOutputCollector _collector;

    int _count = 0;

    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
        _collector = collector;
        _id = id;
    }

    @Override
    public void execute(Tuple tuple) {
        _count++;
    }

    @Override
    public void finishBatch() {
        _collector.emit(new Values(_id, _count));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "count"));
    }
}

----------------------------------------

TITLE: Storm Metrics Filter Interface in Java
DESCRIPTION: Interface definition for creating custom metric filters to control which metrics get reported.

LANGUAGE: java
CODE:
public interface StormMetricsFilter extends MetricFilter {

    /**
     * Called after the filter is instantiated.
     * @param config A map of the properties from the 'filter' section of the reporter configuration.
     */
    void prepare(Map<String, Object> config);
    
   /**
    *  Returns true if the given metric should be reported.
    */
    boolean matches(String name, Metric metric);

}

----------------------------------------

TITLE: Creating Custom Configurations for Storm Components
DESCRIPTION: Explains how to create custom configuration classes for Storm components like bolts, spouts, and plugins. These classes should implement the Validated interface and use annotations for config validation.

LANGUAGE: java
CODE:
public class CustomConfig implements Validated {
    public static final String MY_CUSTOM_CONFIG = "my.custom.config";
    
    @isString
    public static final String ANOTHER_CONFIG = "another.config";
}

// In META-INF/services/org.apache.storm.validation.Validated file:
// com.example.CustomConfig

----------------------------------------

TITLE: Creating Bucketed Hive Table for Streaming (SQL)
DESCRIPTION: SQL command to create a bucketed Hive table with ORC format, which is required for using the Hive streaming API.

LANGUAGE: sql
CODE:
create table test_table ( id INT, name STRING, phone STRING, street STRING) partitioned by (city STRING, state STRING) stored as orc tblproperties ("orc.compress"="NONE");

----------------------------------------

TITLE: Defining TupleToMessageMapper Interface in Java for Storm-RocketMQ Integration
DESCRIPTION: This snippet shows the TupleToMessageMapper interface, which is used to map Storm tuples to RocketMQ messages. It defines methods for extracting the key and value from a tuple.

LANGUAGE: java
CODE:
public interface TupleToMessageMapper extends Serializable {
    String getKeyFromTuple(ITuple tuple);
    byte[] getValueFromTuple(ITuple tuple);
}

----------------------------------------

TITLE: Adding Apache Storm Dependency in Maven POM
DESCRIPTION: This XML snippet shows how to include Apache Storm as a development dependency in a Maven project's pom.xml file. It specifies the groupId, artifactId, version, and scope for the storm-client artifact.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.storm</groupId>
  <artifactId>storm-client</artifactId>
  <version>{{page.version}}</version>
  <scope>provided</scope>
</dependency>

----------------------------------------

TITLE: Task Hook Registration Methods
DESCRIPTION: Two methods for registering task hooks in Storm: using TopologyContext in spout/bolt methods or through Storm configuration.

LANGUAGE: java
CODE:
TopologyContext#addTaskHook
topology.auto.task.hooks

----------------------------------------

TITLE: IConfigLoader Interface Definition
DESCRIPTION: Core interface definition for configuration loading, containing a single method 'load()' that returns a configuration map.

LANGUAGE: java
CODE:
public interface IConfigLoader {
    Map<?,?> load();
};

----------------------------------------

TITLE: Configuring Kerberos Authentication for Pacemaker Server
DESCRIPTION: JAAS configuration for Kerberos authentication on Pacemaker nodes.

LANGUAGE: java
CODE:
PacemakerServer {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/etc/keytabs/pacemaker.keytab"
   storeKey=true
   useTicketCache=false
   principal="pacemaker@MY.COMPANY.COM";
};

----------------------------------------

TITLE: Using Storm Shell Command for Topology Submission
DESCRIPTION: This command demonstrates how to use the 'storm shell' command to package resources, upload them to Nimbus, and execute a Python topology script with arguments.

LANGUAGE: bash
CODE:
storm shell resources/ python3 topology.py arg1 arg2

----------------------------------------

TITLE: Configuring Maven Dependencies for Storm Kinesis Spout
DESCRIPTION: This XML snippet shows the required Maven dependencies for implementing the Storm Kinesis Spout. It includes AWS SDK, Storm Client, Apache Curator, and JSON Simple libraries. The AWS SDK version used for testing is 1.10.77.

LANGUAGE: xml
CODE:
 <dependencies>
        <dependency>
            <groupId>com.amazonaws</groupId>
            <artifactId>aws-java-sdk</artifactId>
            <version>${aws-java-sdk.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-client</artifactId>
            <version>${project.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-framework</artifactId>
            <version>${curator.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>com.googlecode.json-simple</groupId>
            <artifactId>json-simple</artifactId>
        </dependency>
 </dependencies>

----------------------------------------

TITLE: Uploading Storm Topology Jar
DESCRIPTION: Code showing how StormSubmitter uploads the topology jar to Nimbus through Thrift interface, uploading 15KB chunks at a time.

LANGUAGE: thrift
CODE:
beginFileUpload
uploadChunk
finishFileUpload

----------------------------------------

TITLE: Configuring Redis State Provider in Storm YAML
DESCRIPTION: JSON configuration for the Redis state provider, specifying key/value classes, serializers, and Redis connection details. This is set in the storm.yaml configuration file.

LANGUAGE: json
CODE:
{
  "keyClass": "Optional fully qualified class name of the Key type.",
  "valueClass": "Optional fully qualified class name of the Value type.",
  "keySerializerClass": "Optional Key serializer implementation class.",
  "valueSerializerClass": "Optional Value Serializer implementation class.",
  "jedisPoolConfig": {
    "host": "localhost",
    "port": 6379,
    "timeout": 2000,
    "database": 0,
    "password": "xyz"
    }
}

----------------------------------------

TITLE: Configuring RocketMQ Trident State
DESCRIPTION: Shows how to set up a persistent Trident state for RocketMQ, including configuration of mapper, selector, and state factory initialization.

LANGUAGE: java
CODE:
        TupleToMessageMapper mapper = new FieldNameBasedTupleToMessageMapper("word", "count");
        TopicSelector selector = new DefaultTopicSelector(topic);

        Properties properties = new Properties();
        properties.setProperty(RocketMqConfig.NAME_SERVER_ADDR, nameserverAddr);

        RocketMqState.Options options = new RocketMqState.Options()
                .withMapper(mapper)
                .withSelector(selector)
                .withProperties(properties);

        StateFactory factory = new RocketMqStateFactory(options);

        TridentTopology topology = new TridentTopology();
        Stream stream = topology.newStream("spout1", spout);

        stream.partitionPersist(factory, fields,
                new RocketMqStateUpdater(), new Fields());

----------------------------------------

TITLE: Defining a Word Count Topology in Java
DESCRIPTION: This code snippet demonstrates how to define a more complex Storm topology for word counting. It shows the use of different stream groupings (shuffle and fields) to connect spouts and bolts.

LANGUAGE: Java
CODE:
TopologyBuilder builder = new TopologyBuilder();
        
builder.setSpout("sentences", new RandomSentenceSpout(), 5);        
builder.setBolt("split", new SplitSentence(), 8)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 12)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Defining a Word Count Topology in Java
DESCRIPTION: This code snippet demonstrates how to define a more complex Storm topology for word counting. It shows the use of different stream groupings (shuffle and fields) to connect spouts and bolts.

LANGUAGE: Java
CODE:
TopologyBuilder builder = new TopologyBuilder();
        
builder.setSpout("sentences", new RandomSentenceSpout(), 5);        
builder.setBolt("split", new SplitSentence(), 8)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 12)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Initializing Regular Storm Spout in Trident Topology
DESCRIPTION: Demonstrates how to create a stream using a regular Storm IRichSpout in a Trident topology. The spout requires a unique identifier that will be used for storing metadata in Zookeeper.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
topology.newStream("myspoutid", new MyRichSpout());

----------------------------------------

TITLE: Maven Dependencies Configuration for Storm Kinesis
DESCRIPTION: Maven dependency configuration required for Storm Kinesis integration, including AWS SDK, Storm Client, Curator Framework, and JSON Simple dependencies.

LANGUAGE: xml
CODE:
 <dependencies>
        <dependency>
            <groupId>com.amazonaws</groupId>
            <artifactId>aws-java-sdk</artifactId>
            <version>${aws-java-sdk.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-client</artifactId>
            <version>${project.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-framework</artifactId>
            <version>${curator.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>com.googlecode.json-simple</groupId>
            <artifactId>json-simple</artifactId>
        </dependency>
 </dependencies>

----------------------------------------

TITLE: Defining Storm Topology in Clojure
DESCRIPTION: Example topology definition showing how to wire spouts and bolts with specified parallelism and stream groupings

LANGUAGE: clojure
CODE:
(topology
 {"1" (spout-spec sentence-spout)
  "2" (spout-spec (sentence-spout-parameterized
                   ["the cat jumped over the door"
                    "greetings from a faraway land"])
                   :p 2)}
 {"3" (bolt-spec {"1" :shuffle "2" :shuffle}
                 split-sentence
                 :p 5)
  "4" (bolt-spec {"3" ["word"]}
                 word-count
                 :p 6)})

----------------------------------------

TITLE: Monitoring Table Format - Markdown
DESCRIPTION: Example format used throughout document to describe metrics in tabular form with name, type and description columns

LANGUAGE: markdown
CODE:
| Metric Name | Type | Description |
|-------------|------|-------------|

----------------------------------------

TITLE: Configuring Storm Metric Reporters in YAML
DESCRIPTION: Example YAML configuration for setting up Graphite and Console metric reporters in Storm. It shows how to specify reporter classes, daemons, reporting intervals, and metric filters.

LANGUAGE: yaml
CODE:
storm.metrics.reporters:
  # Graphite Reporter
  - class: "org.apache.storm.metrics2.reporters.GraphiteStormReporter"
    daemons:
        - "supervisor"
        - "nimbus"
        - "worker"
    report.period: 60
    report.period.units: "SECONDS"
    graphite.host: "localhost"
    graphite.port: 2003

  # Console Reporter
  - class: "org.apache.storm.metrics2.reporters.ConsoleStormReporter"
    daemons:
        - "worker"
    report.period: 10
    report.period.units: "SECONDS"
    filter:
        class: "org.apache.storm.metrics2.filters.RegexFilter"
        expression: ".*my_component.*emitted.*"

----------------------------------------

TITLE: Implementing Reach Computation using Trident DRPC
DESCRIPTION: Demonstrates a pure DRPC topology that computes the reach of a URL on demand, showcasing Trident's ability to parallelize complex computations across a cluster.

LANGUAGE: java
CODE:
TridentState urlToTweeters =
       topology.newStaticState(getUrlToTweetersState());
TridentState tweetersToFollowers =
       topology.newStaticState(getTweeterToFollowersState());

topology.newDRPCStream("reach")
       .stateQuery(urlToTweeters, new Fields("args"), new MapGet(), new Fields("tweeters"))
       .each(new Fields("tweeters"), new ExpandList(), new Fields("tweeter"))
       .shuffle()
       .stateQuery(tweetersToFollowers, new Fields("tweeter"), new MapGet(), new Fields("followers"))
       .parallelismHint(200)
       .each(new Fields("followers"), new ExpandList(), new Fields("follower"))
       .groupBy(new Fields("follower"))
       .aggregate(new One(), new Fields("one"))
       .parallelismHint(20)
       .aggregate(new Count(), new Fields("reach"));

----------------------------------------

TITLE: ZeroMQ Implementation for Distributed Mode in Apache Storm (Clojure)
DESCRIPTION: Implementation of the message sending protocol using ZeroMQ for distributed mode in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/zmq.clj

----------------------------------------

TITLE: Creating a Flux-Enabled Topology JAR with Maven
DESCRIPTION: Maven configuration for creating a fat JAR that includes Flux and all dependencies for a Storm topology.

LANGUAGE: xml
CODE:
<dependencies>
    <!-- Flux include -->
    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>flux-core</artifactId>
        <version>${storm.version}</version>
    </dependency>
    <!-- Flux Wrappers include -->
    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>flux-wrappers</artifactId>
        <version>${storm.version}</version>
    </dependency>

    <!-- add user dependencies here... -->

</dependencies>
<!-- create a fat jar that includes all dependencies -->
<build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-shade-plugin</artifactId>
            <version>1.4</version>
            <configuration>
                <createDependencyReducedPom>true</createDependencyReducedPom>
            </configuration>
            <executions>
                <execution>
                    <phase>package</phase>
                    <goals>
                        <goal>shade</goal>
                    </goals>
                    <configuration>
                        <transformers>
                            <transformer
                                    implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                            <transformer
                                    implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                <mainClass>org.apache.storm.flux.Flux</mainClass>
                            </transformer>
                        </transformers>
                    </configuration>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>

----------------------------------------

TITLE: Configuring KafkaTridentSpout in a Trident Topology
DESCRIPTION: Example of how to use KafkaTridentSpoutOpaque to create a Trident stream that consumes from Kafka topics matching a pattern.

LANGUAGE: java
CODE:
final TridentTopology tridentTopology = new TridentTopology();
final Stream spoutStream = tridentTopology.newStream("kafkaSpout",
    new KafkaTridentSpoutOpaque<>(KafkaSpoutConfig.builder("127.0.0.1:" + port, Pattern.compile("topic.*")).build()))
      .parallelismHint(1)

----------------------------------------

TITLE: Creating External Table in Storm SQL
DESCRIPTION: SQL syntax for creating external tables to specify data sources, following Hive DDL format

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE table_name field_list
    [ STORED AS
      INPUTFORMAT input_format_classname
      OUTPUTFORMAT output_format_classname
    ]
    LOCATION location
    [ PARALLELISM parallelism ]
    [ TBLPROPERTIES tbl_properties ]
    [ AS select_stmt ]

----------------------------------------

TITLE: Bolt Tuple Transfer in Apache Storm
DESCRIPTION: Implementation of how bolts transfer tuples using the worker-provided transfer function.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L429

----------------------------------------

TITLE: Storm Shell Command Usage Example
DESCRIPTION: Example command showing how to use the storm shell utility to package and deploy resources with a Python topology script.

LANGUAGE: shell
CODE:
storm shell resources/ python topology.py arg1 arg2

----------------------------------------

TITLE: Setting Storm Local Directory in YAML
DESCRIPTION: This code configures the local directory for Storm to store small amounts of state. It shows examples for both Unix and Windows systems.

LANGUAGE: yaml
CODE:
storm.local.dir: "/mnt/storm"

# For Windows:
# storm.local.dir: "C:\\storm-local"

----------------------------------------

TITLE: Implementing Page Redirect and Canonical Link in HTML
DESCRIPTION: This HTML snippet sets up an automatic redirect to the Apache Storm tutorial page and specifies the canonical URL for search engines. The meta refresh tag causes an immediate redirect, while the link tag indicates the preferred URL for indexing.

LANGUAGE: HTML
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Tutorial.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Tutorial.html" />

----------------------------------------

TITLE: Running Storm Local Mode via Command Line
DESCRIPTION: Example of launching a Storm topology in local mode with debug configuration enabled using the command line interface.

LANGUAGE: bash
CODE:
storm local topology.jar <MY_MAIN_CLASS> -c topology.debug=true

----------------------------------------

TITLE: Installing JZMQ for Storm
DESCRIPTION: This snippet demonstrates the process of cloning a specific fork of JZMQ from GitHub, which is tested to work with Storm. It includes commands for cloning the repository, running the autogen script, configuring, and installing JZMQ.

LANGUAGE: bash
CODE:
#install jzmq
git clone https://github.com/nathanmarz/jzmq.git
cd jzmq
./autogen.sh
./configure
make
sudo make install

----------------------------------------

TITLE: Implementing URL Reach Computation in Trident
DESCRIPTION: Demonstrates a DRPC topology that computes the reach of a URL on Twitter, involving multiple state queries and complex aggregations.

LANGUAGE: java
CODE:
TridentState urlToTweeters =
       topology.newStaticState(getUrlToTweetersState());
TridentState tweetersToFollowers =
       topology.newStaticState(getTweeterToFollowersState());

topology.newDRPCStream("reach")
       .stateQuery(urlToTweeters, new Fields("args"), new MapGet(), new Fields("tweeters"))
       .each(new Fields("tweeters"), new ExpandList(), new Fields("tweeter"))
       .shuffle()
       .stateQuery(tweetersToFollowers, new Fields("tweeter"), new MapGet(), new Fields("followers"))
       .parallelismHint(200)
       .each(new Fields("followers"), new ExpandList(), new Fields("follower"))
       .groupBy(new Fields("follower"))
       .aggregate(new One(), new Fields("one"))
       .parallelismHint(20)
       .aggregate(new Count(), new Fields("reach"));

----------------------------------------

TITLE: Message Draining and Sending in Apache Storm Worker (Clojure)
DESCRIPTION: The worker uses a single thread to drain the transfer queue and send messages to other workers.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L185

----------------------------------------

TITLE: Maven Shade Plugin Configuration for HDFS Support
DESCRIPTION: Maven shade plugin configuration required for proper JAR packaging with HDFS support

LANGUAGE: xml
CODE:
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-shade-plugin</artifactId>
    <version>1.4</version>
    <configuration>
        <createDependencyReducedPom>true</createDependencyReducedPom>
    </configuration>
    <executions>
        <execution>
            <phase>package</phase>
            <goals>
                <goal>shade</goal>
            </goals>
            <configuration>
                <transformers>
                    <transformer
                            implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                    <transformer
                            implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                        <mainClass></mainClass>
                    </transformer>
                </transformers>
            </configuration>
        </execution>
    </executions>
</plugin>

----------------------------------------

TITLE: Implementing Update Global Count Bolt in Storm
DESCRIPTION: Implementation of UpdateGlobalCount bolt that handles the commit phase and updates global counts in the database

LANGUAGE: java
CODE:
public static class UpdateGlobalCount extends BaseTransactionalBolt implements ICommitter {
    TransactionAttempt _attempt;
    BatchOutputCollector _collector;

    int _sum = 0;

    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, TransactionAttempt attempt) {
        _collector = collector;
        _attempt = attempt;
    }

    @Override
    public void execute(Tuple tuple) {
        _sum+=tuple.getInteger(1);
    }

    @Override
    public void finishBatch() {
        Value val = DATABASE.get(GLOBAL_COUNT_KEY);
        Value newval;
        if(val == null || !val.txid.equals(_attempt.getTransactionId())) {
            newval = new Value();
            newval.txid = _attempt.getTransactionId();
            if(val==null) {
                newval.count = _sum;
            } else {
                newval.count = _sum + val.count;
            }
            DATABASE.put(GLOBAL_COUNT_KEY, newval);
        } else {
            newval = val;
        }
        _collector.emit(new Values(_attempt, newval.count));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "sum"));
    }
}

----------------------------------------

TITLE: Implementing HTML Meta Refresh Redirect for Storm Documentation
DESCRIPTION: This HTML snippet sets up an automatic redirect to the current Storm fault tolerance documentation page. It also includes a canonical link for search engines.

LANGUAGE: HTML
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Fault-tolerance.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Fault-tolerance.html" />

----------------------------------------

TITLE: Updating Current Release Symlink for Apache Storm Documentation
DESCRIPTION: Shell command to update the 'current' symlink in the releases directory to point to the latest release. This should be run from the storm-site/releases directory.

LANGUAGE: shell
CODE:
ln -f -n -s ${release_name} current

----------------------------------------

TITLE: Using Peek Operation in Trident
DESCRIPTION: Example of using the peek operation for debugging in a Trident stream.

LANGUAGE: java
CODE:
mystream.flatMap(new Split()).map(new UpperCase())
         .peek(new Consumer() {
                @Override
                public void accept(TridentTuple input) {
                  System.out.println(input.getString(0));
                }
         })
         .groupBy(new Fields("word"))
         .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))

----------------------------------------

TITLE: Flux YAML Configuration
DESCRIPTION: YAML configuration for setting up MQTT topology using Flux

LANGUAGE: yaml
CODE:
name: "mqtt-topology"

components:
   ########## MQTT Spout Config ############
  - id: "mqtt-type"
    className: "org.apache.storm.mqtt.examples.CustomMessageMapper"

  - id: "mqtt-options"
    className: "org.apache.storm.mqtt.common.MqttOptions"
    properties:
      - name: "url"
        value: "tcp://localhost:1883"
      - name: "topics"
        value:
          - "/users/tgoetz/#"

config:
  topology.workers: 1
  topology.max.spout.pending: 1000

spouts:
  - id: "mqtt-spout"
    className: "org.apache.storm.mqtt.spout.MqttSpout"
    constructorArgs:
      - ref: "mqtt-type"
      - ref: "mqtt-options"
    parallelism: 1

bolts:
  - id: "log"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1

streams:
  - from: "mqtt-spout"
    to: "log"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Overriding Component Configurations in Java Storm Topologies
DESCRIPTION: Demonstrates two methods for specifying component-specific configurations in Storm topologies using Java. The first method overrides the getComponentConfiguration method internally, while the second uses the TopologyBuilder API to set configurations externally.

LANGUAGE: java
CODE:
// Method 1: Internal override
@Override
public Map<String, Object> getComponentConfiguration() {
    return componentSpecificConfig;
}

// Method 2: External configuration
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("spout", new MySpout())
       .addConfiguration("topology.debug", true)
       .addConfigurations(componentSpecificConfig);

----------------------------------------

TITLE: Managing Storm Topology Lifecycle
DESCRIPTION: Commands for controlling the lifecycle of a Storm topology, including killing, activating, deactivating, and rebalancing.

LANGUAGE: bash
CODE:
storm kill topology-name [-w wait-time-secs]
storm activate topology-name
storm deactivate topology-name
storm rebalance topology-name [-w wait-time-secs] [-n new-num-workers] [-e component=parallelism]*

----------------------------------------

TITLE: Building Storm Azure Event Hubs Integration
DESCRIPTION: Maven command to build the project package

LANGUAGE: bash
CODE:
mvn clean package

----------------------------------------

TITLE: Adding Generic Resources API Method
DESCRIPTION: API method for specifying resource requirements for Storm topology components. Takes a resource name and value as parameters to define resource allocation for spouts or bolts.

LANGUAGE: java
CODE:
public T addResource(String resourceName, Number resourceValue)

----------------------------------------

TITLE: Defining a Storm Topology in Clojure
DESCRIPTION: Example of defining a Storm topology using the Clojure DSL, specifying spouts and bolts with their configurations.

LANGUAGE: clojure
CODE:
(topology
 {"1" (spout-spec sentence-spout)
  "2" (spout-spec (sentence-spout-parameterized
                   ["the cat jumped over the door"
                    "greetings from a faraway land"])
                   :p 2)}
 {"3" (bolt-spec {"1" :shuffle "2" :shuffle}
                 split-sentence
                 :p 5)
  "4" (bolt-spec {"3" ["word"]}
                 word-count
                 :p 6)})

----------------------------------------

TITLE: Initial Handshake Configuration JSON
DESCRIPTION: Example JSON configuration sent to shell components during initial handshake, containing topology configuration, PID directory and topology context information.

LANGUAGE: json
CODE:
{
    "conf": {
        "topology.message.timeout.secs": 3
    },
    "pidDir": "...",
    "context": {
        "task->component": {
            "1": "example-spout",
            "2": "__acker",
            "3": "example-bolt1",
            "4": "example-bolt2"
        },
        "taskid": 3,
        "componentid": "example-bolt",
        "stream->target->grouping": {
            "default": {
                "example-bolt2": {
                    "type": "SHUFFLE"}}},
        "streams": ["default"],
        "stream->outputfields": {"default": ["word"]},
        "source->stream->grouping": {
            "example-spout": {
                "default": {
                    "type": "FIELDS",
                    "fields": ["word"]
                }
            }
        },
        "source->stream->fields": {
            "example-spout": {
                "default": ["word"]
            }
        }
    }
}

----------------------------------------

TITLE: Configuring CassandraWriterBolt with Multiple Insert Queries in Java
DESCRIPTION: Creates a CassandraWriterBolt that performs multiple insert queries from a single input tuple.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
    async(
        simpleQuery("INSERT INTO titles_per_album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all())),
        simpleQuery("INSERT INTO titles_per_performer (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all()))
    )
);

----------------------------------------

TITLE: Storm Topology Submission Thrift Definition
DESCRIPTION: Thrift method definition for submitting a topology to Storm, including parameters for name, jar location, configuration, and topology structure.

LANGUAGE: thrift
CODE:
void submitTopology(1: string name, 2: string uploadedJarLocation, 3: string jsonConf, 4: StormTopology topology)
    throws (1: AlreadyAliveException e, 2: InvalidTopologyException ite);

----------------------------------------

TITLE: Implementing Transactional Committer Bolt in Java
DESCRIPTION: Implementation of UpdateGlobalCount bolt that commits global counts transactionally to a database with exactly-once semantics.

LANGUAGE: java
CODE:
public static class UpdateGlobalCount extends BaseTransactionalBolt implements ICommitter {
    TransactionAttempt _attempt;
    BatchOutputCollector _collector;

    int _sum = 0;

    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, TransactionAttempt attempt) {
        _collector = collector;
        _attempt = attempt;
    }

    @Override
    public void execute(Tuple tuple) {
        _sum+=tuple.getInteger(1);
    }

    @Override
    public void finishBatch() {
        Value val = DATABASE.get(GLOBAL_COUNT_KEY);
        Value newval;
        if(val == null || !val.txid.equals(_attempt.getTransactionId())) {
            newval = new Value();
            newval.txid = _attempt.getTransactionId();
            if(val==null) {
                newval.count = _sum;
            } else {
                newval.count = _sum + val.count;
            }
            DATABASE.put(GLOBAL_COUNT_KEY, newval);
        } else {
            newval = val;
        }
        _collector.emit(new Values(_attempt, newval.count));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "sum"));
    }
}

----------------------------------------

TITLE: Running Base64 to Binary State Migration Tool in Shell
DESCRIPTION: Command to run the migration tool for converting Redis state from Base64-encoded string to binary format when upgrading from Storm 1.1.0 or earlier.

LANGUAGE: shell
CODE:
<storm-installation-dir>/bin/storm jar target/storm-redis-examples-*.jar org.apache.storm.redis.tools.Base64ToBinaryStateMigrationUtil [options]

----------------------------------------

TITLE: Configuring ShellSpout with Blob Store
DESCRIPTION: Example of configuring a ShellSpout to work with executables and scripts shipped in the blob store dist cache by changing the child working directory.

LANGUAGE: java
CODE:
public MyShellSpout() {
    super("./newPython/bin/python", "./shell_spout.py");
    changeChildCWD(false);
}

----------------------------------------

TITLE: Bolt Message Listening in Apache Storm (Clojure)
DESCRIPTION: This code snippet shows how bolts listen for incoming messages in Storm.

LANGUAGE: Clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L489

----------------------------------------

TITLE: Storm YAML Resource Configuration Example
DESCRIPTION: Example of configuring GPU resources in the storm.yaml file for a supervisor node.

LANGUAGE: yaml
CODE:
supervisor.resources.map: {"gpu.count" : 2.0}

----------------------------------------

TITLE: Multi-Anchoring Tuples in Storm
DESCRIPTION: This snippet demonstrates how to anchor an output tuple to multiple input tuples in Storm, useful for streaming joins or aggregations.

LANGUAGE: java
CODE:
List<Tuple> anchors = new ArrayList<Tuple>();
anchors.add(tuple1);
anchors.add(tuple2);
_collector.emit(anchors, new Values(1, 2, 3));

----------------------------------------

TITLE: Multi-Anchoring Tuples in Storm
DESCRIPTION: This snippet demonstrates how to anchor an output tuple to multiple input tuples in Storm, useful for streaming joins or aggregations.

LANGUAGE: java
CODE:
List<Tuple> anchors = new ArrayList<Tuple>();
anchors.add(tuple1);
anchors.add(tuple2);
_collector.emit(anchors, new Values(1, 2, 3));

----------------------------------------

TITLE: SQL Grammar for Storm SQL
DESCRIPTION: The core SQL grammar supported by Storm SQL, defined in BNF-like form. Includes statements for querying, inserting, updating, and deleting data.

LANGUAGE: sql
CODE:
statement:
      setStatement
  |   resetStatement
  |   explain
  |   describe
  |   insert
  |   update
  |   merge
  |   delete
  |   query

setStatement:
      [ ALTER ( SYSTEM | SESSION ) ] SET identifier '=' expression

resetStatement:
      [ ALTER ( SYSTEM | SESSION ) ] RESET identifier
  |   [ ALTER ( SYSTEM | SESSION ) ] RESET ALL

explain:
      EXPLAIN PLAN
      [ WITH TYPE | WITH IMPLEMENTATION | WITHOUT IMPLEMENTATION ]
      [ EXCLUDING ATTRIBUTES | INCLUDING [ ALL ] ATTRIBUTES ]
      FOR ( query | insert | update | merge | delete )

describe:
      DESCRIBE DATABASE databaseName
   |  DESCRIBE CATALOG [ databaseName . ] catalogName
   |  DESCRIBE SCHEMA [ [ databaseName . ] catalogName ] . schemaName
   |  DESCRIBE [ TABLE ] [ [ [ databaseName . ] catalogName . ] schemaName . ] tableName [ columnName ]
   |  DESCRIBE [ STATEMENT ] ( query | insert | update | merge | delete )

insert:
      ( INSERT | UPSERT ) INTO tablePrimary
      [ '(' column [, column ]* ')' ]
      query

update:
      UPDATE tablePrimary
      SET assign [, assign ]*
      [ WHERE booleanExpression ]

assign:
      identifier '=' expression

merge:
      MERGE INTO tablePrimary [ [ AS ] alias ]
      USING tablePrimary
      ON booleanExpression
      [ WHEN MATCHED THEN UPDATE SET assign [, assign ]* ]
      [ WHEN NOT MATCHED THEN INSERT VALUES '(' value [ , value ]* ')' ]

delete:
      DELETE FROM tablePrimary [ [ AS ] alias ]
      [ WHERE booleanExpression ]

query:
      values
  |   WITH withItem [ , withItem ]* query
  |   {
          select
      |   selectWithoutFrom
      |   query UNION [ ALL ] query
      |   query EXCEPT query
      |   query INTERSECT query
      }
      [ ORDER BY orderItem [, orderItem ]* ]
      [ LIMIT { count | ALL } ]
      [ OFFSET start { ROW | ROWS } ]
      [ FETCH { FIRST | NEXT } [ count ] { ROW | ROWS } ]

withItem:
      name
      [ '(' column [, column ]* ')' ]
      AS '(' query ')'

orderItem:
      expression [ ASC | DESC ] [ NULLS FIRST | NULLS LAST ]

select:
      SELECT [ STREAM ] [ ALL | DISTINCT ]
          { * | projectItem [, projectItem ]* }
      FROM tableExpression
      [ WHERE booleanExpression ]
      [ GROUP BY { groupItem [, groupItem ]* } ]
      [ HAVING booleanExpression ]
      [ WINDOW windowName AS windowSpec [, windowName AS windowSpec ]* ]

selectWithoutFrom:
      SELECT [ ALL | DISTINCT ]
          { * | projectItem [, projectItem ]* }

projectItem:
      expression [ [ AS ] columnAlias ]
  |   tableAlias . *

tableExpression:
      tableReference [, tableReference ]*
  |   tableExpression [ NATURAL ] [ LEFT | RIGHT | FULL ] JOIN tableExpression [ joinCondition ]

joinCondition:
      ON booleanExpression
  |   USING '(' column [, column ]* ')'

tableReference:
      tablePrimary
      [ [ AS ] alias [ '(' columnAlias [, columnAlias ]* ')' ] ]

tablePrimary:
      [ [ catalogName . ] schemaName . ] tableName
      '(' TABLE [ [ catalogName . ] schemaName . ] tableName ')'
  |   [ LATERAL ] '(' query ')'
  |   UNNEST '(' expression ')' [ WITH ORDINALITY ]
  |   [ LATERAL ] TABLE '(' [ SPECIFIC ] functionName '(' expression [, expression ]* ')' ')'

values:
      VALUES expression [, expression ]*

groupItem:
      expression
  |   '(' ')'
  |   '(' expression [, expression ]* ')'
  |   CUBE '(' expression [, expression ]* ')'
  |   ROLLUP '(' expression [, expression ]* ')'
  |   GROUPING SETS '(' groupItem [, groupItem ]* ')'

windowRef:
      windowName
  |   windowSpec

windowSpec:
      [ windowName ]
      '('
      [ ORDER BY orderItem [, orderItem ]* ]
      [ PARTITION BY expression [, expression ]* ]
      [
          RANGE numericOrIntervalExpression { PRECEDING | FOLLOWING }
      |   ROWS numericExpression { PRECEDING | FOLLOWING }
      ]
      ')'

----------------------------------------

TITLE: Defining JDBC Connection Provider Interface in Java
DESCRIPTION: Interface definition for database connection pooling in Storm JDBC integration. Includes methods for preparation, connection retrieval, and cleanup.

LANGUAGE: java
CODE:
public interface ConnectionProvider extends Serializable {
    /**
     * method must be idempotent.
     */
    void prepare();

    /**
     *
     * @return a DB connection over which the queries can be executed.
     */
    Connection getConnection();

    /**
     * called once when the system is shutting down, should be idempotent.
     */
    void cleanup();
}

----------------------------------------

TITLE: Setting Log Level via Storm CLI
DESCRIPTION: Commands for setting and resetting log levels using the Storm CLI. Allows specifying logger name, level, and optional timeout duration. The syntax supports both setting new levels and reverting to original values.

LANGUAGE: bash
CODE:
./bin/storm set_log_level [topology name] -l [logger name]=[LEVEL]:[TIMEOUT]

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -l ROOT=DEBUG:30

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -r ROOT

----------------------------------------

TITLE: Storm CLI Blob Creation Command
DESCRIPTION: Example command for creating a blob in Storm's distributed cache via command line

LANGUAGE: bash
CODE:
storm blobstore create --file README.txt --acl o::rwa --replication-factor 4 key1

----------------------------------------

TITLE: Implementing Multiple Streams in Trident Topology
DESCRIPTION: Demonstrates how to create multiple conditional streams in a Trident topology using stream variables and filters. Shows branch creation and stream splitting patterns.

LANGUAGE: java
CODE:
Stream s = topology.each(...).groupBy(...).aggregate(...)
Stream branch1 = s.each(..., FilterA)
Stream branch2 = s.each(..., FilterB)

----------------------------------------

TITLE: Implementing a Batch Count Bolt in Java
DESCRIPTION: Example implementation of a BatchBolt that counts tuples in a batch and emits the partial count.

LANGUAGE: java
CODE:
public static class BatchCount extends BaseBatchBolt {
    Object _id;
    BatchOutputCollector _collector;

    int _count = 0;

    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
        _collector = collector;
        _id = id;
    }

    @Override
    public void execute(Tuple tuple) {
        _count++;
    }

    @Override
    public void finishBatch() {
        _collector.emit(new Values(_id, _count));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "count"));
    }
}

----------------------------------------

TITLE: Registering Custom Counter Metric in Java
DESCRIPTION: Demonstrates how to register a custom counter metric using the TopologyContext in a Storm topology. The metric name is expanded with additional information for unique identification.

LANGUAGE: java
CODE:
Counter myCounter = topologyContext.registerCounter("myCounter");

----------------------------------------

TITLE: Declaring Maven Dependency for Storm-Redis in XML
DESCRIPTION: Shows how to include storm-redis as a Maven dependency in a project's pom.xml file.

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>storm-redis</artifactId>
    <version>${storm.version}</version>
    <type>jar</type>
</dependency>

----------------------------------------

TITLE: SQL Grammar Specification
DESCRIPTION: BNF-style grammar definition for Storm SQL, including statements for DDL and DML operations

LANGUAGE: sql
CODE:
statement:
      setStatement
  |   resetStatement
  |   explain
  |   describe
  |   insert
  |   update
  |   merge
  |   delete
  |   query

setStatement:
      [ ALTER ( SYSTEM | SESSION ) ] SET identifier '=' expression

----------------------------------------

TITLE: Code Distribution Interface Implementation in Java
DESCRIPTION: Interface defining methods for distributing topology code across the Storm cluster, supporting various storage backends and replication strategies.

LANGUAGE: java
CODE:
public interface ICodeDistributor {
    void prepare(Map conf);
    File upload(Path dirPath, String topologyId);
    List<File> download(Path destDirPath, String topologyid, File metafile);
    int getReplicationCount(String topologyId);
    void cleanup(String topologyid);
    void close(Map conf);
}

----------------------------------------

TITLE: Maven Dependency Configuration for Storm Kafka
DESCRIPTION: Maven coordinates for including the storm-kafka module as a dependency. Note that the Kafka dependency is marked as 'provided' to allow users to choose their preferred Scala version.

LANGUAGE: xml
CODE:
groupId: org.apache.storm
artifactId: storm-kafka
version: 0.9.2-incubating

----------------------------------------

TITLE: Initializing SimpleJdbcLookupMapper in Java for Storm JDBC
DESCRIPTION: Example of initializing a SimpleJdbcLookupMapper for use with JdbcLookupBolt. It shows setting up output fields and query parameter columns.

LANGUAGE: java
CODE:
Fields outputFields = new Fields("user_id", "user_name", "create_date");
List<Column> queryParamColumns = Lists.newArrayList(new Column("user_id", Types.INTEGER));
this.jdbcLookupMapper = new SimpleJdbcLookupMapper(outputFields, queryParamColumns);

----------------------------------------

TITLE: Configuring Stateful Windowing in Storm Topology
DESCRIPTION: Example of setting up a stateful windowed bolt in a Storm topology, including persistence configuration and specifying the maximum number of events to keep in memory.

LANGUAGE: java
CODE:
topologyBuilder.setBolt("mybolt",
                   new MyStatefulPersistentWindowedBolt()
                   .withWindow(...) // windowing configuarations
                   .withPersistence() // persist the window state
                   .withMaxEventsInMemory(25000), // max number of events to be cached in memory
                    parallelism)
               .shuffleGrouping("spout");

// use redis for state persistence
conf.put(Config.TOPOLOGY_STATE_PROVIDER, "org.apache.storm.redis.state.RedisKeyValueStateProvider");

----------------------------------------

TITLE: Mounting Container Layers
DESCRIPTION: Example of how container layers are mounted using loop devices

LANGUAGE: bash
CODE:
-bash-4.2$ cat /proc/mounts
...
/dev/loop3 /run/worker-launcher/layers/f7452c2657900c53da1a4f7e430485a267b89c7717466ee61ffefba85f690226/mnt squashfs ro,relatime 0 0
/dev/loop4 /run/worker-launcher/layers/8156da43228752c7364b71dabba6aef6bd1cc081e9ea59cf92ea0f79fd8a50b6/mnt squashfs ro,relatime 0 0
/dev/loop5 /run/worker-launcher/layers/c7c9b1d6df043edf307c49d75c7d2bc3df72f8dcaf7d17b733c97022387902e6/mnt squashfs ro,relatime 0 0
/dev/loop6 /run/worker-launcher/layers/f0d08d5707855b02def8ac622a6c60203b380e31c6c237e5b691f5856594a3e7/mnt squashfs ro,relatime 0 0
/dev/loop11 /run/worker-launcher/layers/34b0bc9c446a9be565fb50b04db1e9d1c1c4d14a22a885a7aba6981748b6635e/mnt squashfs ro,relatime 0 0
/dev/loop12 /run/worker-launcher/layers/0ba001c025aa172a7d630914c75c1772228606f622e2c9d46a8fedf10774623e/mnt squashfs ro,relatime 0 0
/dev/loop13 /run/worker-launcher/layers/a5e4e615565081e04eaf4c5ab5b20d37de271db704fc781c7b1e07c5dcdf96e5/mnt squashfs ro,relatime 0 0
...

----------------------------------------

TITLE: Storm Leader Election Interface
DESCRIPTION: Java interface definition for Storm's leader election functionality used in high availability.

LANGUAGE: java
CODE:
public interface ILeaderElector {
    void addToLeaderLockQueue();
    void removeFromLeaderLockQueue();
    boolean isLeader();
    InetSocketAddress getLeaderAddress();
    List<InetSocketAddress> getAllNimbusAddresses();
}

----------------------------------------

TITLE: Implementing Distributed Word Count Query in Trident
DESCRIPTION: Shows how to implement a low-latency distributed query for word counts using Trident, including splitting input, querying state, and aggregating results.

LANGUAGE: java
CODE:
topology.newDRPCStream("words")
       .each(new Fields("args"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .stateQuery(wordCounts, new Fields("word"), new MapGet(), new Fields("count"))
       .each(new Fields("count"), new FilterNull())
       .aggregate(new Fields("count"), new Sum(), new Fields("sum"));

----------------------------------------

TITLE: Reading Executor-Level Data in Storm Components
DESCRIPTION: Demonstrates how to read executor-level data that has been shared across tasks and task hooks managed by the concerned executor.

LANGUAGE: java
CODE:
TopologyContext#getExecutorData(String)

----------------------------------------

TITLE: Defining ComponentObject Struct in Thrift for Storm
DESCRIPTION: This Thrift struct defines the ComponentObject union, which specifies how code for spouts and bolts can be provided in Storm topologies. It allows for serialized Java, shell components, or Java objects.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Basic Trident Word Count with Persistent State
DESCRIPTION: Example showing how to implement a basic word count topology with persistent state storage using Trident.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
      topology.newStream("spout1", spout)
        .each(new Fields("sentence"), new Split(), new Fields("word"))
        .groupBy(new Fields("word"))
        .persistentAggregate(MemcachedState.opaque(serverLocations), new Count(), new Fields("count"))                
        .parallelismHint(6);

----------------------------------------

TITLE: Dequeuing and Removing Items from Kestrel
DESCRIPTION: Method to dequeue and remove items from a Kestrel queue. Retrieves items, acknowledges them to remove from queue, and displays their content.

LANGUAGE: java
CODE:
    private static void dequeueAndRemoveItems(KestrelClient kestrelClient, String queueName)
    throws IOException, ParseError
		 {
			for(int i=1; i<=12; i++){

				Item item = kestrelClient.dequeue(queueName);


				if(item==null){
					System.out.println("The queue (" + queueName + ") contains no items.");
				}
				else
				{
					int itemID = item._id;


					byte[] data = item._data;

					String receivedVal = new String(data);

					kestrelClient.ack(queueName, itemID);

					System.out.println("receivedItem=" + receivedVal);
				}
			}
	}

----------------------------------------

TITLE: Configuring a Kafka Spout in Flux
DESCRIPTION: YAML configuration for setting up a Kafka spout using Flux.

LANGUAGE: yaml
CODE:
components:
  - id: "stringScheme"
    className: "org.apache.storm.kafka.StringScheme"

  - id: "stringMultiScheme"
    className: "org.apache.storm.spout.SchemeAsMultiScheme"
    constructorArgs:
      - ref: "stringScheme"

  - id: "zkHosts"
    className: "org.apache.storm.kafka.ZkHosts"
    constructorArgs:
      - "localhost:2181"

  - id: "spoutConfig"
    className: "org.apache.storm.kafka.SpoutConfig"
    constructorArgs:
      # brokerHosts
      - ref: "zkHosts"
      # topic
      - "myKafkaTopic"
      # zkRoot
      - "/kafkaSpout"
      # id
      - "myId"
    properties:
      - name: "ignoreZkOffsets"
        value: true
      - name: "scheme"
        ref: "stringMultiScheme"

config:
  topology.workers: 1

# spout definitions
spouts:
  - id: "kafka-spout"
    className: "org.apache.storm.kafka.KafkaSpout"
    constructorArgs:
      - ref: "spoutConfig"

----------------------------------------

TITLE: Configuring Child Options for Storm Daemons in YAML
DESCRIPTION: YAML configuration for setting child options for Nimbus, UI, and Supervisor processes, including JAAS config.

LANGUAGE: yaml
CODE:
nimbus.childopts: "-Xmx1024m -Djava.security.auth.login.config=/path/to/jaas.conf"
ui.childopts: "-Xmx768m -Djava.security.auth.login.config=/path/to/jaas.conf"
supervisor.childopts: "-Xmx256m -Djava.security.auth.login.config=/path/to/jaas.conf"

----------------------------------------

TITLE: Building Storm JMS Example
DESCRIPTION: Maven commands to build the storm-jms example project from source.

LANGUAGE: shell
CODE:
$ cd storm-jms
$ mvn clean install

----------------------------------------

TITLE: Direct Tuple Sending in Local Mode for Apache Storm (Clojure)
DESCRIPTION: In local mode, tuples are sent directly to an in-memory queue for the receiving task.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/local.clj#L21

----------------------------------------

TITLE: Virtual Port Implementation in Apache Storm (Clojure)
DESCRIPTION: Implementation of the virtual port mechanism for routing messages to tasks in distributed mode.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/zilch/virtual_port.clj

----------------------------------------

TITLE: Implementing ISerialization Interface in Java for Storm Custom Serialization
DESCRIPTION: This snippet shows the ISerialization interface that must be implemented to create a custom serializer in Storm. It includes methods for accepting a class type, serializing an object to a binary format, and deserializing an object from a binary format.

LANGUAGE: java
CODE:
public interface ISerialization<T> {
    public boolean accept(Class c);
    public void serialize(T object, DataOutputStream stream) throws IOException;
    public T deserialize(DataInputStream stream) throws IOException;
}

----------------------------------------

TITLE: Creating Kerberos Principals and Keytabs
DESCRIPTION: Bash commands for creating Kerberos principals and keytabs for Zookeeper, Nimbus, DRPC, UI, and Supervisors.

LANGUAGE: bash
CODE:
# Zookeeper (Will need one of these for each box in the Zk ensemble)
sudo kadmin.local -q 'addprinc zookeeper/zk1.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/zk.keytab  zookeeper/zk1.example.com@STORM.EXAMPLE.COM"
# Nimbus and DRPC
sudo kadmin.local -q 'addprinc storm/storm.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm/storm.example.com@STORM.EXAMPLE.COM"
# All UI logviewer and Supervisors
sudo kadmin.local -q 'addprinc storm@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm@STORM.EXAMPLE.COM"

----------------------------------------

TITLE: Managing Storm Topology Lifecycle
DESCRIPTION: Commands for controlling the lifecycle of a Storm topology, including killing, activating, deactivating, and rebalancing.

LANGUAGE: bash
CODE:
storm kill topology-name [-w wait-time-secs]
storm activate topology-name
storm deactivate topology-name
storm rebalance topology-name [-w wait-time-secs] [-n new-num-workers] [-e component=parallelism]*

----------------------------------------

TITLE: Defining MQTT Tuple Mapper Interface in Java
DESCRIPTION: Interface definition for mapping Storm tuples to MQTT messages, used when publishing messages with the MQTT bolt or Trident function.

LANGUAGE: java
CODE:
public interface MqttTupleMapper extends Serializable{

    MqttMessage toMessage(ITuple tuple);

}

----------------------------------------

TITLE: Setting DEBUG Log Level for ROOT Logger
DESCRIPTION: Example of setting the ROOT logger to DEBUG level for 30 seconds in a Storm topology named 'my_topology' using the CLI.

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -l ROOT=DEBUG:30

----------------------------------------

TITLE: Local Mode Message Sending in Apache Storm (Clojure)
DESCRIPTION: In local mode, tuples are sent directly to an in-memory queue for the receiving task.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/local.clj#L21

----------------------------------------

TITLE: Running Storm Topology with Flux in Java
DESCRIPTION: Command to run a sample Storm topology using Flux, which starts a local mode cluster and topology with an MQTT Spout publishing to a logging bolt.

LANGUAGE: bash
CODE:
storm jar ./examples/target/storm-mqtt-examples-*-SNAPSHOT.jar org.apache.storm.flux.Flux ./examples/src/main/flux/sample.yaml --local

----------------------------------------

TITLE: Defining YAML Frontmatter for Storm Documentation Page in Markdown
DESCRIPTION: This snippet defines the YAML frontmatter for a documentation page about non-Java language support in Storm. It specifies the title, layout, and documentation flag.

LANGUAGE: Markdown
CODE:
---
title: Support for Non-Java Languages
layout: documentation
documentation: true
---

----------------------------------------

TITLE: Using Memcached for Persistent Aggregation in Trident
DESCRIPTION: Shows how to replace in-memory state with Memcached for persistent aggregation in a Trident topology.

LANGUAGE: java
CODE:
.persistentAggregate(MemcachedState.transactional(serverLocations), new Count(), new Fields("count"))        
MemcachedState.transactional()

----------------------------------------

TITLE: Emitting a Tuple with Message ID in Storm Spout
DESCRIPTION: This snippet demonstrates how to emit a tuple from a Storm spout using SpoutOutputCollector, including a message ID for tracking.

LANGUAGE: java
CODE:
_collector.emit(new Values("field1", "field2", 3) , msgId);

----------------------------------------

TITLE: Configuring Automatic Task Hooks in Storm Configuration
DESCRIPTION: This snippet shows how to configure automatic task hooks using the Storm configuration. The 'topology.auto.task.hooks' config is used to specify hooks that will be automatically registered in every spout or bolt.

LANGUAGE: java
CODE:
"topology.auto.task.hooks"

----------------------------------------

TITLE: Monitoring Storm Topology Throughput
DESCRIPTION: Interactively monitors a topology's throughput, with options to specify components, streams, and metrics.

LANGUAGE: shell
CODE:
storm monitor topology-name [-i interval-secs] [-m component-id] [-s stream-id] [-w [emitted | transferred]]

----------------------------------------

TITLE: Defining Storm Component Object Structure in Thrift
DESCRIPTION: Thrift union structure defining how component objects can be represented in Storm, supporting Java serialization, shell components, and Java objects.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Displaying Logo Contest Entry Title Using Jekyll Variable
DESCRIPTION: This snippet renders the page title as a main heading using a Jekyll liquid template variable. It demonstrates how to dynamically insert the title defined in the front matter.

LANGUAGE: markdown
CODE:
{{ page.title }}
================

----------------------------------------

TITLE: Creating a Blob in Storm Distributed Cache
DESCRIPTION: Command to create a blob in the Storm distributed cache with specified file, ACL, and replication factor.

LANGUAGE: bash
CODE:
storm blobstore create --file README.txt --acl o::rwa --replication-factor 4 key1

----------------------------------------

TITLE: Implementing Reach Calculation for URLs in Trident
DESCRIPTION: Shows how to create a DRPC topology that computes the reach of a URL on Twitter using Trident.

LANGUAGE: java
CODE:
TridentState urlToTweeters =
       topology.newStaticState(getUrlToTweetersState());
TridentState tweetersToFollowers =
       topology.newStaticState(getTweeterToFollowersState());

topology.newDRPCStream("reach")
       .stateQuery(urlToTweeters, new Fields("args"), new MapGet(), new Fields("tweeters"))
       .each(new Fields("tweeters"), new ExpandList(), new Fields("tweeter"))
       .shuffle()
       .stateQuery(tweetersToFollowers, new Fields("tweeter"), new MapGet(), new Fields("followers"))
       .parallelismHint(200)
       .each(new Fields("followers"), new ExpandList(), new Fields("follower"))
       .groupBy(new Fields("follower"))
       .aggregate(new One(), new Fields("one"))
       .parallelismHint(20)
       .aggregate(new Count(), new Fields("reach"));

----------------------------------------

TITLE: Configuring MQTT Topology with Flux YAML
DESCRIPTION: Flux YAML configuration for creating a Storm topology with an MQTT spout and logging bolt, including MQTT options and custom message mapper.

LANGUAGE: yaml
CODE:
name: "mqtt-topology"

components:
   ########## MQTT Spout Config ############
  - id: "mqtt-type"
    className: "org.apache.storm.mqtt.examples.CustomMessageMapper"

  - id: "mqtt-options"
    className: "org.apache.storm.mqtt.common.MqttOptions"
    properties:
      - name: "url"
        value: "tcp://localhost:1883"
      - name: "topics"
        value:
          - "/users/tgoetz/#"

# topology configuration
config:
  topology.workers: 1
  topology.max.spout.pending: 1000

# spout definitions
spouts:
  - id: "mqtt-spout"
    className: "org.apache.storm.mqtt.spout.MqttSpout"
    constructorArgs:
      - ref: "mqtt-type"
      - ref: "mqtt-options"
    parallelism: 1

# bolt definitions
bolts:
  - id: "log"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1


streams:
  - from: "mqtt-spout"
    to: "log"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Setting Health Check Timeout in Storm YAML
DESCRIPTION: This configuration sets the timeout for health check scripts in Storm. It defines how long a script can run before it's marked as failed due to timeout.

LANGUAGE: yaml
CODE:
storm.health.check.timeout.ms: 5000

----------------------------------------

TITLE: Submitting Topology with Blob Mapping
DESCRIPTION: Example of submitting a Storm topology with blob mapping configuration, specifying local file names and compression settings.

LANGUAGE: shell
CODE:
storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar org.apache.storm.starter.clj.word_count test_topo -c topology.blobstore.map='{"key1":{"localname":"blob_file", "uncompress":false},"key2":{}}'

----------------------------------------

TITLE: Registering Custom Counter in Storm Topology
DESCRIPTION: Example of registering and using a custom counter metric in a Storm topology. The counter's name will be expanded with topology, host, and component information.

LANGUAGE: java
CODE:
Counter myCounter = topologyContext.registerCounter("myCounter");

----------------------------------------

TITLE: Creating a Blob in Storm's Distributed Cache
DESCRIPTION: Command-line example of creating a blob in Storm's distributed cache with specific access control and replication settings.

LANGUAGE: bash
CODE:
storm blobstore create --file README.txt --acl o::rwa --replication-factor 4 key1

----------------------------------------

TITLE: Task ID Resolution for Tuple Emission in Apache Storm (Clojure)
DESCRIPTION: The 'tasks-fn' returns the task IDs to send tuples to for either regular stream emit or direct stream emit.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L207

----------------------------------------

TITLE: Clearing Dynamic Log Level Setting via Storm CLI
DESCRIPTION: Command to clear a previously set dynamic log level for the ROOT logger in a Storm topology, reverting it to its original value.

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -r ROOT

----------------------------------------

TITLE: Configuring and Using KafkaSpout in a Storm Topology
DESCRIPTION: Shows how to create a simple KafkaSpout to consume events from a Kafka topic and send them to a bolt in a Storm topology. It demonstrates basic KafkaSpoutConfig setup and topology construction.

LANGUAGE: java
CODE:
final TopologyBuilder tp = new TopologyBuilder();
tp.setSpout("kafka_spout", new KafkaSpout<>(KafkaSpoutConfig.builder("127.0.0.1:" + port, "topic").build()), 1);
tp.setBolt("bolt", new myBolt()).shuffleGrouping("kafka_spout");

----------------------------------------

TITLE: Using Storm Shell Command for Topology Submission
DESCRIPTION: Example of using the 'storm shell' command to package resources, upload a jar to Nimbus, and execute a Python topology script with arguments.

LANGUAGE: bash
CODE:
storm shell resources/ python topology.py arg1 arg2

----------------------------------------

TITLE: Implementing QueryFunction for State Queries
DESCRIPTION: Example of implementing a QueryFunction to query a custom State object.

LANGUAGE: java
CODE:
public class QueryLocation extends BaseQueryFunction<LocationDB, String> {
    public List<String> batchRetrieve(LocationDB state, List<TridentTuple> inputs) {
        List<String> ret = new ArrayList();
        for(TridentTuple input: inputs) {
            ret.add(state.getLocation(input.getLong(0)));
        }
        return ret;
    }

    public void execute(TridentTuple tuple, String location, TridentCollector collector) {
        collector.emit(new Values(location));
    }    
}

----------------------------------------

TITLE: Creating External Tables and Querying with Storm SQL
DESCRIPTION: This SQL snippet demonstrates creating external tables for Kafka streams and performing a SELECT query with filtering and projection. It shows how to define input and output data sources and execute a basic SQL operation on streaming data.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://localhost:2181/brokers?topic=orders' ...

CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) LOCATION 'kafka://localhost:2181/brokers?topic=large_orders' ...

INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Implementing Split Function for Word Tokenization
DESCRIPTION: Defines a Split function that takes a sentence and emits individual words as separate tuples.

LANGUAGE: java
CODE:
public class Split extends BaseFunction {
   public void execute(TridentTuple tuple, TridentCollector collector) {
       String sentence = tuple.getString(0);
       for(String word: sentence.split(" ")) {
           collector.emit(new Values(word));                
       }
   }
}

----------------------------------------

TITLE: Setting Up Database Tables in SQL
DESCRIPTION: SQL queries for setting up the necessary database tables and inserting initial data for the Storm JDBC integration example.

LANGUAGE: sql
CODE:
create table if not exists user (user_id integer, user_name varchar(100), dept_name varchar(100), create_date date);
create table if not exists department (dept_id integer, dept_name varchar(100));
create table if not exists user_department (user_id integer, dept_id integer);
insert into department values (1, 'R&D');
insert into department values (2, 'Finance');
insert into department values (3, 'HR');
insert into department values (4, 'Sales');
insert into user_department values (1, 1);
insert into user_department values (2, 2);
insert into user_department values (3, 3);
insert into user_department values (4, 4);
select dept_name from department, user_department where department.dept_id = user_department.dept_id and user_department.user_id = ?;

----------------------------------------

TITLE: Defining User Defined Function in Storm SQL
DESCRIPTION: Demonstrates how to create a user-defined function (UDF) using the CREATE FUNCTION statement and provides an example implementation of a scalar function.

LANGUAGE: sql
CODE:
CREATE FUNCTION MYPLUS AS 'org.apache.storm.sql.TestUtils$MyPlus'

LANGUAGE: java
CODE:
public class MyPlus {
  public static Integer evaluate(Integer x, Integer y) {
    return x + y;
  }
}

----------------------------------------

TITLE: Defining User Defined Function in Storm SQL
DESCRIPTION: Demonstrates how to create a user-defined function (UDF) using the CREATE FUNCTION statement and provides an example implementation of a scalar function.

LANGUAGE: sql
CODE:
CREATE FUNCTION MYPLUS AS 'org.apache.storm.sql.TestUtils$MyPlus'

LANGUAGE: java
CODE:
public class MyPlus {
  public static Integer evaluate(Integer x, Integer y) {
    return x + y;
  }
}

----------------------------------------

TITLE: Implementing Efficient QueryFunction with Bulk Operations
DESCRIPTION: Example of updating the QueryLocation function to use bulk operations for improved efficiency when querying the LocationDB state.

LANGUAGE: java
CODE:
public class QueryLocation extends BaseQueryFunction<LocationDB, String> {
    public List<String> batchRetrieve(LocationDB state, List<TridentTuple> inputs) {
        List<Long> userIds = new ArrayList<Long>();
        for(TridentTuple input: inputs) {
            userIds.add(input.getLong(0));
        }
        return state.bulkGetLocations(userIds);
    }

    public void execute(TridentTuple tuple, String location, TridentCollector collector) {
        collector.emit(new Values(location));
    }    
}

----------------------------------------

TITLE: Creating Implicit Topology in Storm (Clojure)
DESCRIPTION: The system-topology! function is used to create an implicit topology with added streams and an acker bolt for the acking framework. This is used both by Nimbus when creating tasks and by workers for message routing.

LANGUAGE: clojure
CODE:
(system-topology!)

----------------------------------------

TITLE: Complete Storm Bolt Implementation
DESCRIPTION: Full implementation of an ExclamationBolt that appends '!!!' to input tuples.

LANGUAGE: java
CODE:
public static class ExclamationBolt implements IRichBolt {
    OutputCollector _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple tuple) {
        _collector.emit(tuple, new Values(tuple.getString(0) + "!!!"));
        _collector.ack(tuple);
    }

    @Override
    public void cleanup() {
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word"));
    }
    
    @Override
    public Map<String, Object> getComponentConfiguration() {
        return null;
    }
}

----------------------------------------

TITLE: Configuring and Using KafkaSpout in a Storm Topology
DESCRIPTION: Shows how to create a simple KafkaSpout to consume events from a Kafka topic and send them to a bolt in a Storm topology. It demonstrates basic KafkaSpoutConfig setup and topology construction.

LANGUAGE: java
CODE:
final TopologyBuilder tp = new TopologyBuilder();
tp.setSpout("kafka_spout", new KafkaSpout<>(KafkaSpoutConfig.builder("127.0.0.1:" + port, "topic").build()), 1);
tp.setBolt("bolt", new myBolt()).shuffleGrouping("kafka_spout");

----------------------------------------

TITLE: Continuous Item Addition to Kestrel
DESCRIPTION: Complete program to continuously add sentence items to a Kestrel queue. Runs until user enters closing bracket character.

LANGUAGE: java
CODE:
    import java.io.IOException;
    import java.io.InputStream;
    import java.util.Random;

    import org.apache.storm.spout.KestrelClient;
    import org.apache.storm.spout.KestrelClient.Item;
    import org.apache.storm.spout.KestrelClient.ParseError;

    public class AddSentenceItemsToKestrel {

    	/**
    	 * @param args
    	 */
    	public static void main(String[] args) {

    		InputStream is = System.in;

			char closing_bracket = ']';

			int val = closing_bracket;

			boolean aux = true;

			try {

				KestrelClient kestrelClient = null;
				String queueName = "sentence_queue";

				while(aux){

					kestrelClient = new KestrelClient("localhost",22133);

					queueSentenceItems(kestrelClient, queueName);

					kestrelClient.close();

					Thread.sleep(1000);

					if(is.available()>0){
					 if(val==is.read())
						 aux=false;
					}
				}
			} catch (IOException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			catch (ParseError e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			} catch (InterruptedException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}

			System.out.println("end");

	    }
	}

----------------------------------------

TITLE: Basic Flux YAML Topology
DESCRIPTION: Example YAML configuration demonstrating a basic word count topology using Flux

LANGUAGE: yaml
CODE:
name: "yaml-topology"
config:
  topology.workers: 1

spouts:
  - id: "spout-1"
    className: "org.apache.storm.testing.TestWordSpout"
    parallelism: 1

bolts:
  - id: "bolt-1"
    className: "org.apache.storm.testing.TestWordCounter"
    parallelism: 1
  - id: "bolt-2"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1

streams:
  - name: "spout-1 --> bolt-1"
    from: "spout-1"
    to: "bolt-1"
    grouping:
      type: FIELDS
      args: ["word"]

  - name: "bolt-1 --> bolt2"
    from: "bolt-1"
    to: "bolt-2"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Creating JdbcTridentState for Lookups in Java
DESCRIPTION: This code snippet shows how to create a JdbcTridentState for lookups in Trident topologies, configuring it with a connection provider, lookup mapper, select query, and query timeout.

LANGUAGE: java
CODE:
JdbcState.Options options = new JdbcState.Options()
        .withConnectionProvider(connectionProvider)
        .withJdbcLookupMapper(new SimpleJdbcLookupMapper(new Fields("user_name"), Lists.newArrayList(new Column("user_id", Types.INTEGER))))
        .withSelectQuery("select user_name from user_details where user_id = ?");
        .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Registering Custom Counter in Storm Topology
DESCRIPTION: Example showing how to register and use a custom counter metric in a Storm topology. The metric name will be automatically expanded with topology, host, and component information.

LANGUAGE: java
CODE:
Counter myCounter = topologyContext.registerCounter("myCounter");

----------------------------------------

TITLE: Registering Custom Counter in Storm Topology
DESCRIPTION: Example showing how to register and use a custom counter metric in a Storm topology. The metric name will be automatically expanded with topology, host, and component information.

LANGUAGE: java
CODE:
Counter myCounter = topologyContext.registerCounter("myCounter");

----------------------------------------

TITLE: Configuring Resources in Trident Topology
DESCRIPTION: Demonstrates how to set CPU and memory resources for different operations in a Trident topology. Shows setting default resources, operation-specific resources, and how resources are combined within topology boundaries. Memory can be specified for both on-heap and off-heap usage.

LANGUAGE: java
CODE:
    TridentTopology topo = new TridentTopology();
    topo.setResourceDefaults(new DefaultResourceDeclarer();
                                                          .setMemoryLoad(128)
                                                          .setCPULoad(20));
    TridentState wordCounts =
        topology
            .newStream("words", feeder)
            .parallelismHint(5)
            .setCPULoad(20)
            .setMemoryLoad(512,256)
            .each( new Fields("sentence"),  new Split(), new Fields("word"))
            .setCPULoad(10)
            .setMemoryLoad(512)
            .each(new Fields("word"), new BangAdder(), new Fields("word!"))
            .parallelismHint(10)
            .setCPULoad(50)
            .setMemoryLoad(1024)
            .each(new Fields("word!"), new QMarkAdder(), new Fields("word!?"))
            .groupBy(new Fields("word!"))
            .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))
            .setCPULoad(100)
            .setMemoryLoad(2048);

----------------------------------------

TITLE: Configuring Health Check Directory in Storm YAML
DESCRIPTION: This snippet shows how to configure the directory for health check scripts in Storm. These scripts are used to monitor the health of supervisor nodes.

LANGUAGE: yaml
CODE:
storm.health.check.dir: "healthchecks"

----------------------------------------

TITLE: Implementing One Aggregator for Unique Counting
DESCRIPTION: Defines a combiner aggregator that emits the value one for each group, used in the reach calculation example.

LANGUAGE: java
CODE:
public class One implements CombinerAggregator<Integer> {
   public Integer init(TridentTuple tuple) {
       return 1;
   }

   public Integer combine(Integer val1, Integer val2) {
       return 1;
   }

   public Integer zero() {
       return 1;
   }        
}

----------------------------------------

TITLE: Adding Generic Resources API Method
DESCRIPTION: API method for specifying resource requirements for topology components like Spouts or Bolts. Takes a resource name and value as parameters.

LANGUAGE: java
CODE:
public T addResource(String resourceName, Number resourceValue)

----------------------------------------

TITLE: Storm BlobStore Command Line Creation
DESCRIPTION: Command to create a blob in Storm's blobstore with specified access controls and replication factor

LANGUAGE: bash
CODE:
storm blobstore create --file README.txt --acl o::rwa --replication-factor 4 key1

----------------------------------------

TITLE: Configuring Kerberos Authentication for Pacemaker Client
DESCRIPTION: JAAS configuration for Kerberos authentication on Nimbus node for Pacemaker client.

LANGUAGE: java
CODE:
PacemakerClient {
    com.sun.security.auth.module.Krb5LoginModule required
    useKeyTab=true
    keyTab="/etc/keytabs/nimbus.keytab"
    storeKey=true
    useTicketCache=false
    serviceName="pacemaker"
    principal="nimbus@MY.COMPANY.COM";
};

----------------------------------------

TITLE: Nimbus Assignment Generation
DESCRIPTION: Clojure function to generate task assignments for topology execution across the cluster.

LANGUAGE: clojure
CODE:
[code](https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/nimbus.clj#L458)

----------------------------------------

TITLE: Queuing Sentences to Kestrel
DESCRIPTION: Method to add random sentences to a Kestrel queue using KestrelClient. Randomly selects from 5 predefined sentences and adds them with sequential IDs.

LANGUAGE: java
CODE:
    private static void queueSentenceItems(KestrelClient kestrelClient, String queueName)
			throws ParseError, IOException {

		String[] sentences = new String[] {
	            "the cow jumped over the moon",
	            "an apple a day keeps the doctor away",
	            "four score and seven years ago",
	            "snow white and the seven dwarfs",
	            "i am at two with nature"};

		Random _rand = new Random();

		for(int i=1; i<=10; i++){

			String sentence = sentences[_rand.nextInt(sentences.length)];

			String val = "ID " + i + " " + sentence;

			boolean queueSucess = kestrelClient.queue(queueName, val);

			System.out.println("queueSucess=" +queueSucess+ " [" + val +"]");
		}
	}

----------------------------------------

TITLE: MQTT Tuple Mapper Interface - Java
DESCRIPTION: Interface definition for mapping Storm tuples to MQTT messages

LANGUAGE: java
CODE:
public interface MqttTupleMapper extends Serializable{
    MqttMessage toMessage(ITuple tuple);
}

----------------------------------------

TITLE: Reading a Blob from Storm Distributed Cache (Java)
DESCRIPTION: Example of reading a blob from the Storm distributed cache using Java. This snippet demonstrates how to get an input stream for a blob and read its contents.

LANGUAGE: java
CODE:
String blobKey = "some_key";
InputStreamWithMeta blobInputStream = clientBlobStore.getBlob(blobKey);
BufferedReader r = new BufferedReader(new InputStreamReader(blobInputStream));
String blobContents =  r.readLine();

----------------------------------------

TITLE: Using persistentAggregate for Word Count
DESCRIPTION: Example of using persistentAggregate in a Trident topology to compute and store word counts in a MemoryMapState.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
      topology.newStream("spout1", spout)
        .each(new Fields("sentence"), new Split(), new Fields("word"))
        .groupBy(new Fields("word"))
        .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))

----------------------------------------

TITLE: Implementing Custom State Interface
DESCRIPTION: Example of a custom State implementation for a location database, showing basic methods for committing transactions and accessing data.

LANGUAGE: java
CODE:
public class LocationDB implements State {
    public void beginCommit(Long txid) {    
    }
    
    public void commit(Long txid) {    
    }
    
    public void setLocation(long userId, String location) {
      // code to access database and set location
    }
    
    public String getLocation(long userId) {
      // code to get location from database
    }
}

----------------------------------------

TITLE: Storm Input Tuple JSON Format
DESCRIPTION: JSON structure representing an input tuple in Storm's multilang protocol. Contains tuple ID, component ID, stream ID, task ID, and tuple values.

LANGUAGE: json
CODE:
{
    "id": -6955786537413359385,
    "comp": 1,
    "stream": 1,
    "task": 9,
    "tuple": ["snow white and the seven dwarfs", "field2", 3]
}

----------------------------------------

TITLE: Defining Shell Bolt Specification
DESCRIPTION: Example showing how to define a bolt implemented in a non-JVM language using shell-bolt-spec.

LANGUAGE: clojure
CODE:
(shell-bolt-spec {"1" :shuffle "2" ["id"]}
                 "python3"
                 "mybolt.py"
                 ["outfield1" "outfield2"]
                 :p 25)

----------------------------------------

TITLE: Storm Blobstore Command Line Creation
DESCRIPTION: Example command for creating a blob in the blobstore with read/write/admin access and replication factor

LANGUAGE: bash
CODE:
storm blobstore create --file README.txt --acl o::rwa --replication-factor 4 key1

----------------------------------------

TITLE: Maven Dependencies Configuration
DESCRIPTION: Required Maven dependencies for Storm Kinesis integration, including AWS SDK, Storm client, Curator Framework, and JSON Simple library.

LANGUAGE: xml
CODE:
 <dependencies>
        <dependency>
            <groupId>com.amazonaws</groupId>
            <artifactId>aws-java-sdk</artifactId>
            <version>${aws-java-sdk.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-client</artifactId>
            <version>${project.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-framework</artifactId>
            <version>${curator.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>com.googlecode.json-simple</groupId>
            <artifactId>json-simple</artifactId>
        </dependency>
 </dependencies>

----------------------------------------

TITLE: Defining Thrift Structures for Storm Cluster Information
DESCRIPTION: Thrift structures for representing cluster summary information in Storm, including supervisor, topology, and nimbus details. Used for API responses to provide cluster state.

LANGUAGE: thrift
CODE:
struct ClusterSummary {
  1: required list<SupervisorSummary> supervisors;
  3: required list<TopologySummary> topologies;
  4: required list<NimbusSummary> nimbuses;
}

struct NimbusSummary {
  1: required string host;
  2: required i32 port;
  3: required i32 uptime_secs;
  4: required bool isLeader;
  5: required string version;
}

----------------------------------------

TITLE: Implementing One Aggregator for Unique Counting in Trident
DESCRIPTION: Defines a combiner aggregator called 'One' that is used in the reach calculation topology to efficiently count unique followers.

LANGUAGE: java
CODE:
public class One implements CombinerAggregator<Integer> {
   public Integer init(TridentTuple tuple) {
       return 1;
   }

   public Integer combine(Integer val1, Integer val2) {
       return 1;
   }

   public Integer zero() {
       return 1;
   }        
}

----------------------------------------

TITLE: Python Storm Bolt Implementation
DESCRIPTION: Example of implementing a Storm bolt in Python that splits sentences into words.

LANGUAGE: python
CODE:
import storm

class SplitSentenceBolt(storm.BasicBolt):
    def process(self, tup):
        words = tup.values[0].split(" ")
        for word in words:
          storm.emit([word])

SplitSentenceBolt().run()

----------------------------------------

TITLE: Accessing Storm Unit Testing API in Java
DESCRIPTION: Example of how to use Apache Storm's newly exposed unit testing facilities in Java. This API allows for easy setup and teardown of local clusters, running fixed sets of tuples through topologies, and using time simulation for testing.

LANGUAGE: Java
CODE:
https://github.com/xumingming/storm-lib/blob/master/src/jvm/storm/TestingApiDemo.java

----------------------------------------

TITLE: Creating Storm Topology with KestrelSpout in Java
DESCRIPTION: This code snippet demonstrates how to create a Storm topology that reads sentences from a Kestrel queue, splits them into words, and counts word occurrences. It uses KestrelSpout to consume data from Kestrel.

LANGUAGE: java
CODE:
    TopologyBuilder builder = new TopologyBuilder();
    builder.setSpout("sentences", new KestrelSpout("localhost",22133,"sentence_queue",new StringScheme()));
    builder.setBolt("split", new SplitSentence(), 10)
    	        .shuffleGrouping("sentences");
    builder.setBolt("count", new WordCount(), 20)
	        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Configuring Windowing Operations on a Stream in Java
DESCRIPTION: Shows various ways to configure windowing operations on a stream, including sliding and tumbling windows based on time or count.

LANGUAGE: java
CODE:
// time based sliding window
stream.window(SlidingWindows.of(Duration.minutes(10), Duration.minutes(1)));

// count based sliding window
stream.window(SlidingWindows.of(Count.(10), Count.of(2)));

// tumbling window
stream.window(TumblingWindows.of(Duration.seconds(10));

// specifying timestamp field for event time based processing and a late tuple stream.
stream.window(TumblingWindows.of(Duration.seconds(10)
                     .withTimestampField("ts")
                     .withLateTupleStream("late_events"));

----------------------------------------

TITLE: Storm Topology Submission Method Definition
DESCRIPTION: Thrift method definition for submitting a topology to Storm. Includes parameters for topology name, jar location, configuration, and topology structure with possible exceptions.

LANGUAGE: java
CODE:
void submitTopology(
        1: string name,
        2: string uploadedJarLocation,
        3: string jsonConf,
        4: StormTopology topology)
        throws (
                1: AlreadyAliveException e,
                2: InvalidTopologyException ite);

----------------------------------------

TITLE: Uploading Topology Jar in StormSubmitter (Java)
DESCRIPTION: StormSubmitter uploads the topology jar to Nimbus using its Thrift interface. It uploads the jar in 15KB chunks and returns a path in Nimbus's inbox.

LANGUAGE: java
CODE:
String uploadedJarLocation = submitJar(conf, localJar);

----------------------------------------

TITLE: Registering Event Logger in Java Configuration
DESCRIPTION: This code snippet shows how to register an event logger implementation in the topology configuration using Java.

LANGUAGE: java
CODE:
conf.registerEventLogger(org.apache.storm.metric.FileBasedEventLogger.class);

----------------------------------------

TITLE: Storm Metrics Filter Interface in Java
DESCRIPTION: Interface definition for creating custom metric filters to control which metrics get reported.

LANGUAGE: java
CODE:
public interface StormMetricsFilter extends MetricFilter {
    /**
     * Called after the filter is instantiated.
     * @param config A map of the properties from the 'filter' section of the reporter configuration.
     */
    void prepare(Map<String, Object> config);
    
   /**
    *  Returns true if the given metric should be reported.
    */
    boolean matches(String name, Metric metric);
}

----------------------------------------

TITLE: Setting Memory Requirements for Components in Java
DESCRIPTION: Java API calls to set memory requirements for spouts and bolts in a Storm topology.

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.setMemoryLoad(1024.0, 512.0);
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
            .shuffleGrouping("word").setMemoryLoad(512.0);

----------------------------------------

TITLE: HTML Meta Refresh Redirect for Storm Documentation
DESCRIPTION: HTML code that automatically redirects visitors to the Storm native dependencies installation documentation page on the Apache Storm website. Includes a canonical link tag for SEO purposes.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Installing-native-dependencies.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Installing-native-dependencies.html" />

----------------------------------------

TITLE: Configuring Zookeeper Servers in Storm YAML
DESCRIPTION: Required configuration for specifying Zookeeper cluster hosts in storm.yaml file

LANGUAGE: yaml
CODE:
storm.zookeeper.servers:
  - "111.222.333.444"
  - "555.666.777.888"

----------------------------------------

TITLE: Storm SQL Query for Filtering Error Logs
DESCRIPTION: SQL statements to create external tables and filter error logs (status >= 400) from Apache logs stream.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://apache-logs?bootstrap-servers=localhost:9092'\nCREATE EXTERNAL TABLE APACHE_ERROR_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_ELAPSED_MS INT) LOCATION 'kafka://apache-error-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{\"producer\":{\"acks\":\"1\",\"key.serializer\":\"org.apache.storm.kafka.IntSerializer\"}}'\nINSERT INTO APACHE_ERROR_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, (TIME_US / 1000) AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (CAST(STATUS AS INT) / 100) >= 4

----------------------------------------

TITLE: Configuring Storm Topology Parallelism (Java)
DESCRIPTION: Shows how to configure parallelism for a complete Storm topology. It sets the number of worker processes, configures parallelism for spouts and bolts, and submits the topology. The example includes setting executors and tasks for different components.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(2); // use two worker processes

topologyBuilder.setSpout("blue-spout", new BlueSpout(), 2); // set parallelism hint to 2

topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

topologyBuilder.setBolt("yellow-bolt", new YellowBolt(), 6)
               .shuffleGrouping("green-bolt");

StormSubmitter.submitTopology(
        "mytopology",
        conf,
        topologyBuilder.createTopology()
    );

----------------------------------------

TITLE: Overriding Component Configurations in Java Storm API
DESCRIPTION: Demonstrates two methods for specifying component-specific configurations in Storm topologies using the Java API. The first method overrides the getComponentConfiguration method, while the second uses the TopologyBuilder's setSpout and setBolt methods.

LANGUAGE: java
CODE:
// Method 1: Override getComponentConfiguration
@Override
public Map<String, Object> getComponentConfiguration() {
    return new HashMap<String, Object>() {
        {
            put("topology.debug", true);
        }
    };
}

// Method 2: Use TopologyBuilder methods
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("spout", new MySpout())
       .addConfiguration("topology.max.spout.pending", 100);
builder.setBolt("bolt", new MyBolt())
       .addConfiguration("topology.max.task.parallelism", 2);

----------------------------------------

TITLE: Configuring Jekyll Page Layout and Title for Logo Contest Entry
DESCRIPTION: This snippet sets up the Jekyll page layout and title for a blog post showcasing a logo contest entry. It uses YAML front matter to define the post's metadata.

LANGUAGE: markdown
CODE:
---
layout: post
title: Logo Entry No. 6 - Alec Bartos
---

----------------------------------------

TITLE: Executing Python Topology Script with Nimbus Information
DESCRIPTION: Example of how Storm executes the Python topology script after uploading the jar, providing Nimbus host, port, and jar location as arguments.

LANGUAGE: bash
CODE:
python topology.py arg1 arg2 {nimbus-host} {nimbus-port} {uploaded-jar-location}

----------------------------------------

TITLE: Named Stream Join Configuration
DESCRIPTION: Examples showing how to configure JoinBolt using named streams instead of component names.

LANGUAGE: java
CODE:
new JoinBolt(JoinBolt.Selector.STREAM,  "stream1", "key1")
                                  .join("stream2", "key2")
    ...

LANGUAGE: java
CODE:
new JoinBolt(JoinBolt.Selector.STREAM,  "stream1", "key1") 
                             .join     ("stream2", "userId",  "stream1" )
                             .select ("userId, key1, key2")
                             .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;
                             
topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("bolt1", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt2", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt3", "stream2", new Fields("userId") )
            .fieldsGrouping("bolt4", "stream1", new Fields("key1") );

----------------------------------------

TITLE: IWindowedBolt Interface Definition in Java
DESCRIPTION: Core interface that must be implemented by bolts requiring windowing support. Defines methods for preparation, window execution, and cleanup.

LANGUAGE: java
CODE:
public interface IWindowedBolt extends IComponent {
    void prepare(Map stormConf, TopologyContext context, OutputCollector collector);
    /**
     * Process tuples falling within the window and optionally emit 
     * new tuples based on the tuples in the input window.
     */
    void execute(TupleWindow inputWindow);
    void cleanup();
}

----------------------------------------

TITLE: Configuring SimpleJdbcMapper in Java
DESCRIPTION: Example of configuring SimpleJdbcMapper with explicit column schema. It shows how to set up the mapper for specific columns and data types.

LANGUAGE: java
CODE:
List<Column> columnSchema = Lists.newArrayList(
    new Column("user_id", java.sql.Types.INTEGER),
    new Column("user_name", java.sql.Types.VARCHAR),
    new Column("dept_name", java.sql.Types.VARCHAR));
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(columnSchema);

----------------------------------------

TITLE: Configuring Zookeeper Servers in Storm YAML
DESCRIPTION: Required configuration for specifying Zookeeper cluster hosts in storm.yaml file

LANGUAGE: yaml
CODE:
storm.zookeeper.servers:
  - "111.222.333.444"
  - "555.666.777.888"

----------------------------------------

TITLE: Configuring YAML Front Matter for Storm Documentation
DESCRIPTION: YAML front matter defining the title, layout, and documentation status for the Storm performance tuning guide.

LANGUAGE: yaml
CODE:
---
title: Performance Tuning
layout: documentation
documentation: true
---

----------------------------------------

TITLE: Configuring ArtifactoryConfigLoader in YAML
DESCRIPTION: Example YAML configuration for using ArtifactoryConfigLoader, specifying the URI and timeout.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "artifactory+http://artifactory.my.company.com:8000/artifactory/configurations/clusters/my_cluster/ras_pools"
scheduler.config.loader.timeout.sec: 30

----------------------------------------

TITLE: Custom MQTT Tuple Mapper Implementation
DESCRIPTION: Sample implementation of MqttTupleMapper that converts tuple data to MQTT messages

LANGUAGE: java
CODE:
public class MyTupleMapper implements MqttTupleMapper {
    public MqttMessage toMessage(ITuple tuple) {
        String topic = "users/" + tuple.getStringByField("userId") + "/" + tuple.getStringByField("device");
        byte[] payload = tuple.getStringByField("message").getBytes();
        return new MqttMessage(topic, payload);
    }
}

----------------------------------------

TITLE: Running WordCountTopology in Eclipse
DESCRIPTION: This Java code snippet represents the entry point for running the WordCountTopology in Eclipse. It demonstrates how to execute a Storm topology in a local development environment.

LANGUAGE: Java
CODE:
WordCountTopology.java

----------------------------------------

TITLE: Creating a Stream with Regular Storm Spout in Trident Topology
DESCRIPTION: This snippet demonstrates how to create a stream using a regular Storm IRichSpout in a Trident topology. It initializes a TridentTopology and adds a new stream with a unique identifier and a custom spout.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
topology.newStream("myspoutid", new MyRichSpout());

----------------------------------------

TITLE: Setting Executor-Level Data in Storm Components
DESCRIPTION: Illustrates how to set executor-level data that can be shared across tasks and task hooks managed by the concerned executor.

LANGUAGE: java
CODE:
TopologyContext#setExecutorData

----------------------------------------

TITLE: Registering Worker Hook in Storm Topology
DESCRIPTION: This snippet shows how to register a custom worker hook in a Storm topology using the TopologyBuilder method.

LANGUAGE: java
CODE:
TopologyBuilder.addWorkerHook

----------------------------------------

TITLE: Configuring DelimitedRecordHiveMapper for HiveBolt (Java)
DESCRIPTION: Java code examples showing how to configure DelimitedRecordHiveMapper with column fields and partition fields for use with HiveBolt.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
    or
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withTimeAsPartitionField("YYYY/MM/DD");

----------------------------------------

TITLE: Configuring OpenTSDB Bolt in Storm Topology
DESCRIPTION: This snippet demonstrates how to set up and configure an OpenTSDB bolt in a Storm topology. It shows the creation of an OpenTsdbClient, configuration of the OpenTsdbBolt with batch size and flush interval, and adding it to the topology builder.

LANGUAGE: java
CODE:
OpenTsdbClient.Builder builder =  OpenTsdbClient.newBuilder(openTsdbUrl).sync(30_000).returnDetails();
final OpenTsdbBolt openTsdbBolt = new OpenTsdbBolt(builder, Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
openTsdbBolt.withBatchSize(10).withFlushInterval(2000);
topologyBuilder.setBolt("opentsdb", openTsdbBolt).shuffleGrouping("metric-gen");

----------------------------------------

TITLE: Creating Kerberos Principals for Storm Services
DESCRIPTION: Bash commands for creating Kerberos principals and keytabs for Zookeeper, Nimbus, DRPC and other Storm services

LANGUAGE: bash
CODE:
# Zookeeper (Will need one of these for each box in the Zk ensemble)
sudo kadmin.local -q 'addprinc zookeeper/zk1.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/zk.keytab  zookeeper/zk1.example.com@STORM.EXAMPLE.COM"
# Nimbus and DRPC
sudo kadmin.local -q 'addprinc storm/storm.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm/storm.example.com@STORM.EXAMPLE.COM"
# All UI logviewer and Supervisors
sudo kadmin.local -q 'addprinc storm@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm@STORM.EXAMPLE.COM"

----------------------------------------

TITLE: HTML Meta Redirect to Storm Local Mode Documentation
DESCRIPTION: HTML meta refresh directive that automatically redirects the browser to the Storm local mode documentation page. Also includes a canonical link reference for SEO purposes.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Local-mode.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Local-mode.html" />

----------------------------------------

TITLE: Configuring Nimbus Seeds in Storm YAML
DESCRIPTION: This YAML configuration specifies the Nimbus seed nodes for worker nodes to download topology jars and configurations. It's a mandatory configuration in the storm.yaml file.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["111.222.333.44"]

----------------------------------------

TITLE: HTML Redirect to Storm Eclipse Setup Guide
DESCRIPTION: Meta refresh redirect tag and canonical link element that redirects users to the Apache Storm documentation page for Eclipse project setup.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Setting-up-a-Storm-project-in-Eclipse.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Setting-up-a-Storm-project-in-Eclipse.html" />

----------------------------------------

TITLE: Sliding Window Bolt Implementation in Java
DESCRIPTION: Example implementation of a sliding window bolt showing topology setup with window configuration and bolt execution logic.

LANGUAGE: java
CODE:
public class SlidingWindowBolt extends BaseWindowedBolt {
	private OutputCollector collector;
	
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
    	this.collector = collector;
    }
	
    @Override
    public void execute(TupleWindow inputWindow) {
	  for(Tuple tuple: inputWindow.get()) {
	    // do the windowing computation
		...
	  }
	  // emit the results
	  collector.emit(new Values(computedValue));
    }
}

public static void main(String[] args) {
    TopologyBuilder builder = new TopologyBuilder();
     builder.setSpout("spout", new RandomSentenceSpout(), 1);
     builder.setBolt("slidingwindowbolt", 
                     new SlidingWindowBolt().withWindow(new Count(30), new Count(10)),
                     1).shuffleGrouping("spout");
    Config conf = new Config();
    conf.setDebug(true);
    conf.setNumWorkers(1);

    StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());
	
}

----------------------------------------

TITLE: Storm Acker Execute Operation Flow
DESCRIPTION: The core execution logic of Storm's acker bolt that handles tuple tree tracking. It processes tick tuples for advancing checksums, manages tuple initialization, acknowledgments, and failures, and coordinates with spouts for completion notification.



----------------------------------------

TITLE: Receiving Tuple in JSON Format for Storm Multi-Language Protocol
DESCRIPTION: This JSON structure represents a tuple received by a multi-language component in Storm. It includes the tuple's id, component id, stream id, task id, and the actual tuple values.

LANGUAGE: json
CODE:
{
    "id": -6955786537413359385,
    "comp": 1,
    "stream": 1,
    "task": 9,
    "tuple": ["snow white and the seven dwarfs", "field2", 3]
}

----------------------------------------

TITLE: Running Event Hubs Test Client
DESCRIPTION: Command to run the included Event Hubs send client for testing purposes. Allows sending messages to specific partitions or all partitions.

LANGUAGE: bash
CODE:
java -cp .\target\eventhubs-storm-spout-{version}-jar-with-dependencies.jar com.microsoft.eventhubs.client.EventHubSendClient [username] [password] [entityPath] [partitionId] [messageSize] [messageCount]

----------------------------------------

TITLE: Running Storm SQL in Explain Mode
DESCRIPTION: Command to run Storm SQL in explain mode to show the query plan instead of submitting the topology.

LANGUAGE: bash
CODE:
$ bin/storm sql order_filtering.sql --explain --artifacts "org.apache.storm:storm-sql-kafka:2.0.0-SNAPSHOT,org.apache.storm:storm-kafka-client:2.0.0-SNAPSHOT,org.apache.kafka:kafka-clients:1.1.0^org.slf4j:slf4j-log4j12"

----------------------------------------

TITLE: Running Storm SQL Command
DESCRIPTION: Command to compile SQL statements into a Storm topology and submit it to the Storm cluster. The sql-file contains SQL statements to execute, and topo-name is the name of the topology.

LANGUAGE: bash
CODE:
$ bin/storm sql <sql-file> <topo-name>

----------------------------------------

TITLE: Debugging Nimbus JVM Shutdown in Java
DESCRIPTION: This log output shows the Nimbus JVM shutting down immediately after startup, potentially due to an incompatibility with the RocksDB library on older CPU architectures.

LANGUAGE: java
CODE:
2024-01-05 18:54:20.404 [o.a.s.v.ConfigValidation] INFO: Will use [class org.apache.storm.DaemonConfig, class org.apache.storm.Config] for validation
2024-01-05 18:54:20.556 [o.a.s.z.AclEnforcement] INFO: SECURITY IS DISABLED NO FURTHER CHECKS...
2024-01-05 18:54:20.740 [o.a.s.m.r.RocksDbStore] INFO: Opening RocksDB from <your-storm-folder>/storm_rocks, storm.metricstore.rocksdb.create_if_missing=true

----------------------------------------

TITLE: Basic Storm Topology Configuration
DESCRIPTION: Example showing how to set up a basic Storm topology with spouts and bolts for word counting

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("sentences", new KestrelSpout("kestrel.backtype.com",
                                               22133,
                                               "sentence_queue",
                                               new StringScheme()));
builder.setBolt("split", new SplitSentence(), 10)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 20)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Basic Word Count Topology YAML Configuration
DESCRIPTION: YAML configuration for a simple word count topology using JavaScript spout, Python bolt, and Java bolt

LANGUAGE: yaml
CODE:
name: "shell-topology"
config:
  topology.workers: 1

spouts:
  - id: "sentence-spout"
    className: "org.apache.storm.flux.wrappers.spouts.FluxShellSpout"
    constructorArgs:
      - ["node", "randomsentence.js"]
      - ["word"]
    parallelism: 1

bolts:
  - id: "splitsentence"
    className: "org.apache.storm.flux.wrappers.bolts.FluxShellBolt"
    constructorArgs:
      - ["python", "splitsentence.py"]
      - ["word"]
    parallelism: 1

  - id: "log"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1

  - id: "count"
    className: "org.apache.storm.testing.TestWordCounter"
    parallelism: 1

streams:
  - name: "spout --> split"
    from: "sentence-spout"
    to: "splitsentence"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Implementing a Map Function for Uppercase Conversion
DESCRIPTION: Demonstrates how to create a map function that converts strings to uppercase.

LANGUAGE: java
CODE:
public class UpperCase extends MapFunction {
 @Override
 public Values execute(TridentTuple input) {
   return new Values(input.getString(0).toUpperCase());
 }
}

----------------------------------------

TITLE: Deploying Storm Topology
DESCRIPTION: Command to deploy the sample topology to Storm cluster using the Event Hubs spout

LANGUAGE: bash
CODE:
storm jar {jarfile} com.microsoft.eventhubs.samples.EventCount {topologyname} {spoutconffile}

----------------------------------------

TITLE: Configuring Remote Cluster Access in YAML
DESCRIPTION: This YAML configuration specifies the Nimbus seed node for connecting to a remote Storm cluster. It should be placed in the ~/.storm/storm.yaml file to enable the storm client to communicate with the cluster.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["123.45.678.890"]

----------------------------------------

TITLE: Implementing Redis Lookup Mapper
DESCRIPTION: Example implementation of RedisLookupMapper interface for word count lookups in Redis hash storage.

LANGUAGE: java
CODE:
class WordCountRedisLookupMapper implements RedisLookupMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountRedisLookupMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public List<Values> toTuple(ITuple input, Object value) {
        String member = getKeyFromTuple(input);
        List<Values> values = Lists.newArrayList();
        values.add(new Values(member, value));
        return values;
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("wordName", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Running Storm Topology with Flux
DESCRIPTION: Command to run a sample Storm topology using Flux, which sets up a local mode cluster and topology with an MQTT Spout publishing to a logging bolt.

LANGUAGE: bash
CODE:
storm jar ./examples/target/storm-mqtt-examples-*-SNAPSHOT.jar org.apache.storm.flux.Flux ./examples/src/main/flux/sample.yaml --local

----------------------------------------

TITLE: Storm Shell Python Execution Format
DESCRIPTION: Format showing how Storm shell executes the Python topology script with additional runtime parameters for Nimbus connection.

LANGUAGE: bash
CODE:
python topology.py arg1 arg2 {nimbus-host} {nimbus-port} {uploaded-jar-location}

----------------------------------------

TITLE: Building the Storm JMS Example Topology
DESCRIPTION: Maven commands to build the Storm JMS example topology. This compiles the project and installs it in the local Maven repository.

LANGUAGE: bash
CODE:
$ cd storm-jms
$ mvn clean install

----------------------------------------

TITLE: Creating RocketMQ Persistent Trident State for Storm Topology in Java
DESCRIPTION: This snippet illustrates how to create a RocketMQ persistent Trident state for use in Storm Trident topologies. It configures the state with a TupleToMessageMapper, TopicSelector, and Properties object, and then uses it in a Trident topology.

LANGUAGE: java
CODE:
TupleToMessageMapper mapper = new FieldNameBasedTupleToMessageMapper("word", "count");
TopicSelector selector = new DefaultTopicSelector(topic);

Properties properties = new Properties();
properties.setProperty(RocketMqConfig.NAME_SERVER_ADDR, nameserverAddr);

RocketMqState.Options options = new RocketMqState.Options()
        .withMapper(mapper)
        .withSelector(selector)
        .withProperties(properties);

StateFactory factory = new RocketMqStateFactory(options);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory, fields,
        new RocketMqStateUpdater(), new Fields());

----------------------------------------

TITLE: Creating RocketMQ Persistent Trident State for Storm Topology in Java
DESCRIPTION: This snippet illustrates how to create a RocketMQ persistent Trident state for use in Storm Trident topologies. It configures the state with a TupleToMessageMapper, TopicSelector, and Properties object, and then uses it in a Trident topology.

LANGUAGE: java
CODE:
TupleToMessageMapper mapper = new FieldNameBasedTupleToMessageMapper("word", "count");
TopicSelector selector = new DefaultTopicSelector(topic);

Properties properties = new Properties();
properties.setProperty(RocketMqConfig.NAME_SERVER_ADDR, nameserverAddr);

RocketMqState.Options options = new RocketMqState.Options()
        .withMapper(mapper)
        .withSelector(selector)
        .withProperties(properties);

StateFactory factory = new RocketMqStateFactory(options);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory, fields,
        new RocketMqStateUpdater(), new Fields());

----------------------------------------

TITLE: HTML Meta Redirect to Storm Documentation
DESCRIPTION: HTML meta refresh tag and canonical link that redirects users to the current Storm concepts documentation page on the Apache Storm website.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Concepts.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Concepts.html" />

----------------------------------------

TITLE: Linking to Storm JSON GitHub Repository in Markdown
DESCRIPTION: This snippet creates a Markdown link to the GitHub repository for the storm-json library. It provides a brief description of the library as a simple JSON serializer for Storm.

LANGUAGE: markdown
CODE:
* [storm-json](https://github.com/rapportive-oss/storm-json): Simple JSON serializer for Storm

----------------------------------------

TITLE: Configuring Trident Kafka State in Storm Topology
DESCRIPTION: Example of setting up a TridentKafkaState to write to Kafka from a Trident topology

LANGUAGE: java
CODE:
Fields fields = new Fields("word", "count");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
        new Values("storm", "1"),
        new Values("trident", "1"),
        new Values("needs", "1"),
        new Values("javadoc", "1")
);
spout.setCycle(true);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

//set producer properties.
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

TridentKafkaStateFactory stateFactory = new TridentKafkaStateFactory()
        .withProducerProperties(props)
        .withKafkaTopicSelector(new DefaultTopicSelector("test"))
        .withTridentTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper("word", "count"));
stream.partitionPersist(stateFactory, fields, new TridentKafkaStateUpdater(), new Fields());

Config conf = new Config();
StormSubmitter.submitTopology("kafkaTridentTest", conf, topology.build());

----------------------------------------

TITLE: Configuring URL Expansion with Fields Grouping in Storm
DESCRIPTION: Example showing URL expansion bolt configuration using fields grouping to optimize caching by ensuring same URLs are processed by same tasks.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .fieldsGrouping("urls", new Fields("url"));

----------------------------------------

TITLE: Implementing a ReducerAggregator in Trident
DESCRIPTION: Example of implementing a ReducerAggregator for counting tuples.

LANGUAGE: java
CODE:
public class Count implements ReducerAggregator<Long> {
    public Long init() {
        return 0L;
    }
    
    public Long reduce(Long curr, TridentTuple tuple) {
        return curr + 1;
    }
}

----------------------------------------

TITLE: Configuring Pacemaker Servers in Storm
DESCRIPTION: Specifies the hostnames of the Pacemaker servers in the Storm cluster configuration.

LANGUAGE: yaml
CODE:
pacemaker.servers:
    - somehost.mycompany.com
    - someotherhost.mycompany.com

----------------------------------------

TITLE: Storm Shell Python Script Execution Format
DESCRIPTION: Format showing how Storm shell executes the Python topology script with additional system arguments for Nimbus connection details and jar location.

LANGUAGE: bash
CODE:
python3 topology.py arg1 arg2 {nimbus-host} {nimbus-port} {uploaded-jar-location}

----------------------------------------

TITLE: Artifactory Loader Configuration Example
DESCRIPTION: Example configuration for loading from an Artifactory server using the artifactory+http scheme.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "artifactory+http://artifactory.my.company.com:8000/artifactory/configurations/clusters/my_cluster/ras_pools"
scheduler.config.loader.timeout.sec: 30

----------------------------------------

TITLE: Implementing an Aggregator in Trident
DESCRIPTION: Example of implementing a general Aggregator for counting tuples.

LANGUAGE: java
CODE:
public class CountAgg extends BaseAggregator<CountState> {
    static class CountState {
        long count = 0;
    }

    public CountState init(Object batchId, TridentCollector collector) {
        return new CountState();
    }

    public void aggregate(CountState state, TridentTuple tuple, TridentCollector collector) {
        state.count+=1;
    }

    public void complete(CountState state, TridentCollector collector) {
        collector.emit(new Values(state.count));
    }
}

----------------------------------------

TITLE: Listing Spout Implementations in Markdown
DESCRIPTION: A markdown-formatted list of GitHub repositories containing spout implementations for Apache Storm. Each entry includes a brief description of the spout's purpose and functionality.

LANGUAGE: markdown
CODE:
* [storm-kestrel](https://github.com/nathanmarz/storm-kestrel): Adapter to use Kestrel as a spout
* [storm-amqp-spout](https://github.com/rapportive-oss/storm-amqp-spout): Adapter to use AMQP source as a spout
* [storm-jms](https://github.com/ptgoetz/storm-jms): Adapter to use a JMS source as a spout
* [storm-redis-pubsub](https://github.com/sorenmacbeth/storm-redis-pubsub): A spout that subscribes to a Redis pubsub stream
* [storm-beanstalkd-spout](https://github.com/haitaoyao/storm-beanstalkd-spout): A spout that subscribes to a beanstalkd queue

----------------------------------------

TITLE: Using Min and MinBy Operations in Trident
DESCRIPTION: Example of using minBy operation to find minimum values in each partition.

LANGUAGE: java
CODE:
mystream.minBy(new Fields("count"))

----------------------------------------

TITLE: External Table Definition Example
DESCRIPTION: Example showing how to create an external table connecting to a Kafka source/sink

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-hosts=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Defining Code Distribution Interface in Java for Storm Nimbus
DESCRIPTION: Interface for distributing topology code across Nimbus instances, including methods for uploading, downloading, and managing distributed code.

LANGUAGE: java
CODE:
public interface ICodeDistributor {
    void prepare(Map conf);
    File upload(Path dirPath, String topologyId);
    List<File> download(Path destDirPath, String topologyid, File metafile);
    int getReplicationCount(String topologyId);
    void cleanup(String topologyid);
    void close(Map conf);
}

----------------------------------------

TITLE: Initializing Transactional Topology in Java
DESCRIPTION: Example of creating a transactional topology for global counting using TransactionalTopologyBuilder with a memory spout and batch processing bolts.

LANGUAGE: java
CODE:
MemoryTransactionalSpout spout = new MemoryTransactionalSpout(DATA, new Fields("word"), PARTITION_TAKE_PER_BATCH);
TransactionalTopologyBuilder builder = new TransactionalTopologyBuilder("global-count", "spout", spout, 3);
builder.setBolt("partial-count", new BatchCount(), 5)
        .shuffleGrouping("spout");
builder.setBolt("sum", new UpdateGlobalCount())
        .globalGrouping("partial-count");

----------------------------------------

TITLE: Configuring Digest Authentication for Pacemaker
DESCRIPTION: JAAS configuration for digest authentication in Pacemaker, specifying username and password.

LANGUAGE: java
CODE:
PacemakerDigest {
    username="some username"
    password="some password";
};

----------------------------------------

TITLE: HTML Redirect to Storm Spout Documentation
DESCRIPTION: HTML meta refresh redirect and canonical link implementation that sends users to the Apache Storm spout implementations documentation page.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Spout-implementations.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Spout-implementations.html" />

----------------------------------------

TITLE: Setting Health Check Timeout in Storm YAML
DESCRIPTION: This configuration sets the maximum time allowed for a health check script to run before it is considered failed due to timeout.

LANGUAGE: yaml
CODE:
storm.health.check.timeout.ms: 5000

----------------------------------------

TITLE: Defining Jekyll Front Matter for Storm Logo Entry Post
DESCRIPTION: This snippet sets up the Jekyll front matter for a blog post about a logo contest entry. It specifies the layout and title of the page.

LANGUAGE: markdown
CODE:
---
layout: post
title: Logo Entry No. 4 - Richard Brownlie-Marshall
---

----------------------------------------

TITLE: Implementing Storm Topology with Kinesis Spout
DESCRIPTION: Example of creating a Storm topology that uses KinesisSpout to consume data from Kinesis Streams. Shows configuration of connection parameters, ZooKeeper settings, and topology setup.

LANGUAGE: java
CODE:
public class KinesisSpoutTopology {
    public static void main (String args[]) throws InvalidTopologyException, AuthorizationException, AlreadyAliveException {
        String topologyName = args[0];
        RecordToTupleMapper recordToTupleMapper = new TestRecordToTupleMapper();
        KinesisConnectionInfo kinesisConnectionInfo = new KinesisConnectionInfo(new CredentialsProviderChain(), new ClientConfiguration(), Regions.US_WEST_2,
                1000);
        ZkInfo zkInfo = new ZkInfo("localhost:2181", "/kinesisOffsets", 20000, 15000, 10000L, 3, 2000);
        KinesisConfig kinesisConfig = new KinesisConfig(args[1], ShardIteratorType.TRIM_HORIZON,
                recordToTupleMapper, new Date(), new ExponentialBackoffRetrier(), zkInfo, kinesisConnectionInfo, 10000L);
        KinesisSpout kinesisSpout = new KinesisSpout(kinesisConfig);
        TopologyBuilder topologyBuilder = new TopologyBuilder();
        topologyBuilder.setSpout("spout", kinesisSpout, 3);
        topologyBuilder.setBolt("bolt", new KinesisBoltTest(), 1).shuffleGrouping("spout");
        Config topologyConfig = new Config();
        topologyConfig.setDebug(true);
        topologyConfig.setNumWorkers(3);
        StormSubmitter.submitTopology(topologyName, topologyConfig, topologyBuilder.createTopology());
    }
}

----------------------------------------

TITLE: Implementing Storm Topology with Kinesis Spout in Java
DESCRIPTION: This code snippet demonstrates how to create a Storm topology that uses the KinesisSpout to consume data from Amazon Kinesis Streams. It shows the configuration of various components such as RecordToTupleMapper, KinesisConnectionInfo, ZkInfo, and KinesisConfig.

LANGUAGE: java
CODE:
public class KinesisSpoutTopology {
    public static void main (String args[]) throws InvalidTopologyException, AuthorizationException, AlreadyAliveException {
        String topologyName = args[0];
        RecordToTupleMapper recordToTupleMapper = new TestRecordToTupleMapper();
        KinesisConnectionInfo kinesisConnectionInfo = new KinesisConnectionInfo(new CredentialsProviderChain(), new ClientConfiguration(), Regions.US_WEST_2,
                1000);
        ZkInfo zkInfo = new ZkInfo("localhost:2181", "/kinesisOffsets", 20000, 15000, 10000L, 3, 2000);
        KinesisConfig kinesisConfig = new KinesisConfig(args[1], ShardIteratorType.TRIM_HORIZON,
                recordToTupleMapper, new Date(), new ExponentialBackoffRetrier(), zkInfo, kinesisConnectionInfo, 10000L);
        KinesisSpout kinesisSpout = new KinesisSpout(kinesisConfig);
        TopologyBuilder topologyBuilder = new TopologyBuilder();
        topologyBuilder.setSpout("spout", kinesisSpout, 3);
        topologyBuilder.setBolt("bolt", new KinesisBoltTest(), 1).shuffleGrouping("spout");
        Config topologyConfig = new Config();
        topologyConfig.setDebug(true);
        topologyConfig.setNumWorkers(3);
        StormSubmitter.submitTopology(topologyName, topologyConfig, topologyBuilder.createTopology());
    }
}

----------------------------------------

TITLE: Specifying Pacemaker Servers in Storm Configuration
DESCRIPTION: Defines the hostnames of Pacemaker servers in the Storm cluster configuration.

LANGUAGE: yaml
CODE:
pacemaker.servers:
    - somehost.mycompany.com
    - someotherhost.mycompany.com

----------------------------------------

TITLE: Registering Worker Hooks in Storm
DESCRIPTION: Worker hooks can be registered using TopologyBuilder and are called during worker startup. They provide access to worker-level resources and lifecycle management.

LANGUAGE: java
CODE:
// Register worker hook
TopologyBuilder.addWorkerHook()

// Set worker-level resource
WorkerUserContext.setResource(String, Object)

// Access resource in spouts/bolts
TopologyContext.getResource(String)

----------------------------------------

TITLE: Configuring Artifactory Loader in YAML
DESCRIPTION: Example YAML configuration for using the ArtifactoryConfigLoader, specifying the URI and timeout.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "artifactory+http://artifactory.my.company.com:8000/artifactory/configurations/clusters/my_cluster/ras_pools"
scheduler.config.loader.timeout.sec: 30

----------------------------------------

TITLE: Storm Jar Command Implementation
DESCRIPTION: Command line implementation that sets storm.jar environment variable for use by StormSubmitter.

LANGUAGE: shell
CODE:
[code](https://github.com/apache/storm/blob/0.7.1/bin/storm#L101)

----------------------------------------

TITLE: Submitting Storm SQL Topology with Dependencies
DESCRIPTION: Command to submit a Storm SQL topology with required dependencies for Kafka integration.

LANGUAGE: bash
CODE:
$ bin/storm sql order_filtering.sql order_filtering --artifacts "org.apache.storm:storm-sql-kafka:2.0.0-SNAPSHOT,org.apache.storm:storm-kafka-client:2.0.0-SNAPSHOT,org.apache.kafka:kafka-clients:1.1.0^org.slf4j:slf4j-log4j12"

----------------------------------------

TITLE: Implementing Storm Trident State for OpenTSDB
DESCRIPTION: Shows how to implement OpenTSDB state management using Storm's Trident abstraction. Creates a Trident topology with OpenTSDB state factory and configures stream processing with logging capabilities.

LANGUAGE: java
CODE:
final OpenTsdbStateFactory openTsdbStateFactory =
        new OpenTsdbStateFactory(OpenTsdbClient.newBuilder(tsdbUrl),
                Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
TridentTopology tridentTopology = new TridentTopology();

final Stream stream = tridentTopology.newStream("metric-tsdb-stream", new MetricGenSpout());

stream.peek(new Consumer() {
    @Override
    public void accept(TridentTuple input) {
        LOG.info("########### Received tuple: [{}]", input);
    }
}).partitionPersist(openTsdbStateFactory, MetricGenSpout.DEFAULT_METRIC_FIELDS, new OpenTsdbStateUpdater());

----------------------------------------

TITLE: Installing Jekyll and Bundler for Apache Storm Documentation
DESCRIPTION: Commands to install Jekyll and Bundler, which are required for generating the Apache Storm website. This assumes Ruby is already installed on the system.

LANGUAGE: shell
CODE:
gem install jekyll bundler

----------------------------------------

TITLE: Creating CassandraWriterBolt with Named Setters and Aliases
DESCRIPTION: Demonstrates how to create a CassandraWriterBolt using a static bound query with named setters and aliases for field mapping.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
     async(
        boundQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (:ti, :ye, :pe, :ge, :tr);")
            .bind(
                field("ti"),as("title"),
                field("ye").as("year")),
                field("pe").as("performer")),
                field("ge").as("genre")),
                field("tr").as("tracks"))
            ).byNamedSetters()
     )
);

----------------------------------------

TITLE: Kafka Maven Dependency Configuration
DESCRIPTION: XML configuration showing how to include Kafka dependency in a Maven project

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka_2.10</artifactId>
    <version>0.8.1.1</version>
    <exclusions>
        <exclusion>
            <groupId>org.apache.zookeeper</groupId>
            <artifactId>zookeeper</artifactId>
        </exclusion>
        <exclusion>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
        </exclusion>
    </exclusions>
</dependency>

----------------------------------------

TITLE: Defining IEventLogger Interface in Java
DESCRIPTION: Defines the IEventLogger interface used by the event logger bolt to log events. This interface includes methods for preparation, logging events, and cleanup.

LANGUAGE: java
CODE:
public interface IEventLogger {
    void prepare(Map stormConf, Map<String, Object> arguments, TopologyContext context);
    void log(EventInfo e);
    void close();
}

----------------------------------------

TITLE: Setting DRPC Server Configuration
DESCRIPTION: Defines the DRPC server endpoints for distributed remote procedure calls in the Storm cluster.

LANGUAGE: yaml
CODE:
drpc.servers: ["111.222.333.44"]

----------------------------------------

TITLE: Storm Shell Python Execution Format
DESCRIPTION: Format showing how Storm shell executes the Python topology script with additional Nimbus connection parameters.

LANGUAGE: shell
CODE:
python topology.py arg1 arg2 {nimbus-host} {nimbus-port} {uploaded-jar-location}

----------------------------------------

TITLE: Initializing Trident Stream with Regular Storm Spout
DESCRIPTION: Demonstrates how to create a new stream in a Trident topology using a regular Storm IRichSpout. The spout requires a unique identifier that will be used for storing metadata in Zookeeper.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
topology.newStream("myspoutid", new MyRichSpout());

----------------------------------------

TITLE: Implementing User-Defined Function in Java
DESCRIPTION: Example Java class implementation for a scalar user-defined function.

LANGUAGE: java
CODE:
public class MyPlus {
  public static Integer evaluate(Integer x, Integer y) {
    return x + y;
  }
}

----------------------------------------

TITLE: Configuring Streaming Top N Pattern in Storm
DESCRIPTION: This code demonstrates how to set up a streaming top N pattern in Storm. It uses a RankObjects bolt with fields grouping for parallel processing, followed by a MergeObjects bolt with global grouping to combine results.

LANGUAGE: java
CODE:
builder.setBolt("rank", new RankObjects(), parallelism)
  .fieldsGrouping("objects", new Fields("value"));
builder.setBolt("merge", new MergeObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Setting Log Level via Storm CLI
DESCRIPTION: Command to set a specific log level for a logger in a running Storm topology using the CLI. It allows specifying the topology name, logger name, log level, and an optional timeout.

LANGUAGE: bash
CODE:
./bin/storm set_log_level [topology name] -l [logger name]=[LEVEL]:[TIMEOUT]

----------------------------------------

TITLE: Implementing Split Function in Java
DESCRIPTION: Defines a Split function that separates sentences into individual words for processing.

LANGUAGE: java
CODE:
public class Split extends BaseFunction {
   public void execute(TridentTuple tuple, TridentCollector collector) {
       String sentence = tuple.getString(0);
       for(String word: sentence.split(" ")) {
           collector.emit(new Values(word));                
       }
   }
}

----------------------------------------

TITLE: Creating Kafka Topics for Apache Logs
DESCRIPTION: Commands to create Kafka topics for storing different types of Apache logs using kafka-topics utility.

LANGUAGE: bash
CODE:
kafka-topics --create --topic apache-logs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
kafka-topics --create --topic apache-errorlogs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
kafka-topics --create --topic apache-slowlogs --zookeeper localhost:2181 --replication-factor 1 --partitions 5

----------------------------------------

TITLE: Configuring MQTT Topology with Flux in YAML
DESCRIPTION: Flux YAML configuration for creating an MQTT topology with a spout and logging bolt.

LANGUAGE: yaml
CODE:
name: "mqtt-topology"

components:
   ########## MQTT Spout Config ############
  - id: "mqtt-type"
    className: "org.apache.storm.mqtt.examples.CustomMessageMapper"

  - id: "mqtt-options"
    className: "org.apache.storm.mqtt.common.MqttOptions"
    properties:
      - name: "url"
        value: "tcp://localhost:1883"
      - name: "topics"
        value:
          - "/users/tgoetz/#"

# topology configuration
config:
  topology.workers: 1
  topology.max.spout.pending: 1000

# spout definitions
spouts:
  - id: "mqtt-spout"
    className: "org.apache.storm.mqtt.spout.MqttSpout"
    constructorArgs:
      - ref: "mqtt-type"
      - ref: "mqtt-options"
    parallelism: 1

# bolt definitions
bolts:
  - id: "log"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1


streams:
  - from: "mqtt-spout"
    to: "log"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Implementing Storm Trident State for OpenTSDB
DESCRIPTION: Shows how to implement OpenTSDB state management using Storm's Trident abstraction. Creates a Trident topology with OpenTSDB state factory and configures stream processing with logging capabilities.

LANGUAGE: java
CODE:
final OpenTsdbStateFactory openTsdbStateFactory =
        new OpenTsdbStateFactory(OpenTsdbClient.newBuilder(tsdbUrl),
                Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
TridentTopology tridentTopology = new TridentTopology();

final Stream stream = tridentTopology.newStream("metric-tsdb-stream", new MetricGenSpout());

stream.peek(new Consumer() {
    @Override
    public void accept(TridentTuple input) {
        LOG.info("########### Received tuple: [{}]", input);
    }
}).partitionPersist(openTsdbStateFactory, MetricGenSpout.DEFAULT_METRIC_FIELDS, new OpenTsdbStateUpdater());

----------------------------------------

TITLE: Defining JdbcLookupMapper Interface for Database Lookups in Java
DESCRIPTION: Interface for executing select queries against a database using JDBC. It includes methods for declaring output fields, getting columns for placeholders, and mapping database rows to Storm tuples.

LANGUAGE: java
CODE:
void declareOutputFields(OutputFieldsDeclarer declarer);
List<Column> getColumns(ITuple tuple);
List<Values> toTuple(ITuple input, List<Column> columns);

----------------------------------------

TITLE: Setting CPU Requirements for Storm Components
DESCRIPTION: Java API call to set CPU requirements for Storm spouts and bolts.

LANGUAGE: java
CODE:
public T setCPULoad(Double amount)

----------------------------------------

TITLE: Implementing Message Sending Protocol in Apache Storm (Clojure)
DESCRIPTION: Definition of the protocol used for message sending in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/protocol.clj

----------------------------------------

TITLE: Implementing Message Sending Protocol in Apache Storm (Clojure)
DESCRIPTION: Definition of the protocol used for message sending in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/protocol.clj

----------------------------------------

TITLE: Configuring Storm Cluster State Store for Pacemaker
DESCRIPTION: Basic configuration to enable Pacemaker as the cluster state store in Storm.

LANGUAGE: yaml
CODE:
storm.cluster.state.store: "org.apache.storm.cluster.PaceMakerStateStorageFactory"

----------------------------------------

TITLE: Creating StateFactory for Custom State
DESCRIPTION: Implementation of a StateFactory for creating instances of the custom LocationDB State object.

LANGUAGE: java
CODE:
public class LocationDBFactory implements StateFactory {
   public State makeState(Map conf, int partitionIndex, int numPartitions) {
      return new LocationDB();
   } 
}

----------------------------------------

TITLE: Maven Dependencies Configuration
DESCRIPTION: Maven dependency configuration for Storm Kinesis Spout including AWS SDK, Storm Core, Curator Framework, and JSON Simple libraries.

LANGUAGE: xml
CODE:
 <dependencies>
        <dependency>
            <groupId>com.amazonaws</groupId>
            <artifactId>aws-java-sdk</artifactId>
            <version>${aws-java-sdk.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-core</artifactId>
            <version>${project.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-framework</artifactId>
            <version>${curator.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>com.googlecode.json-simple</groupId>
            <artifactId>json-simple</artifactId>
        </dependency>
 </dependencies>

----------------------------------------

TITLE: Storm Topology Submission Method Definition
DESCRIPTION: Thrift method definition for submitting a topology to Storm. Takes topology name, jar location, JSON configuration, and topology structure as parameters.

LANGUAGE: java
CODE:
void submitTopology(1: string name, 2: string uploadedJarLocation, 3: string jsonConf, 4: StormTopology topology) throws (1: AlreadyAliveException e, 2: InvalidTopologyException ite);

----------------------------------------

TITLE: Starting Pacemaker Daemon
DESCRIPTION: Command to start the Pacemaker daemon in a Storm cluster.

LANGUAGE: bash
CODE:
$ storm pacemaker

----------------------------------------

TITLE: Configuring Storm Local Directory on Windows
DESCRIPTION: This shows how to set the Storm local directory configuration on Windows systems using backslashes in the path.

LANGUAGE: yaml
CODE:
storm.local.dir: "C:\\storm-local"

----------------------------------------

TITLE: Implementing Transfer Function in Apache Storm Worker (Clojure)
DESCRIPTION: The transfer function in the worker is responsible for serializing tuples and putting them onto a transfer queue for sending to other tasks.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L56

----------------------------------------

TITLE: Dequeuing Items from Kestrel in Java
DESCRIPTION: Method to dequeue items from a Kestrel queue without removing them. Retrieves and displays queue items while preserving them in the queue.

LANGUAGE: java
CODE:
    private static void dequeueItems(KestrelClient kestrelClient, String queueName) throws IOException, ParseError
			 {
		for(int i=1; i<=12; i++){

			Item item = kestrelClient.dequeue(queueName);

			if(item==null){
				System.out.println("The queue (" + queueName + ") contains no items.");
			}
			else
			{
				byte[] data = item._data;

				String receivedVal = new String(data);

				System.out.println("receivedItem=" + receivedVal);
			}
		}

----------------------------------------

TITLE: Acknowledging Tuples in Storm Multi-Language Protocol (JSON)
DESCRIPTION: This JSON structure demonstrates how to acknowledge (ack) a tuple in the Storm multi-language protocol. It includes the command type and the id of the tuple to be acknowledged.

LANGUAGE: json
CODE:
{
    "command": "ack",
    "id": 123123
}

----------------------------------------

TITLE: Defining Bolt Component in Thrift
DESCRIPTION: Thrift definition for a bolt component in Storm, containing ComponentObject and ComponentCommon structs. ComponentObject defines the bolt implementation, while ComponentCommon specifies streams, parallelism, and configuration.

LANGUAGE: Thrift
CODE:
struct Bolt {
  1: ComponentObject bolt_object,
  2: ComponentCommon common
}

----------------------------------------

TITLE: Storm Cluster Metrics Table Structure
DESCRIPTION: Markdown table structure defining cluster metrics from Nimbus with metric names, types and descriptions

LANGUAGE: markdown
CODE:
| Metric Name | Type | Description |
|-------------|------|-------------|
| cluster:num-nimbus-leaders | gauge | Number of nimbuses marked as a leader... |

----------------------------------------

TITLE: Performing Storm Admin Operations
DESCRIPTION: Provides access to administrative operations for debugging or fixing a Storm cluster.

LANGUAGE: shell
CODE:
storm admin <cmd> [options]

----------------------------------------

TITLE: Creating External Table in Storm SQL
DESCRIPTION: SQL syntax for creating an external table in Storm SQL, which represents a data source. This example defines a Kafka spout and sink.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Running ActiveMQ Server
DESCRIPTION: Command to start the Apache ActiveMQ server for JMS message handling.

LANGUAGE: shell
CODE:
$ [ACTIVEMQ_HOME]/bin/activemq

----------------------------------------

TITLE: FailedMessageRetryHandler Interface Methods
DESCRIPTION: Interface methods for handling failed message retry logic in the Kinesis spout. Includes methods for tracking failed messages, scheduling retries, and managing message acknowledgments.

LANGUAGE: java
CODE:
    boolean failed (KinesisMessageId messageId);
    KinesisMessageId getNextFailedMessageToRetry ();
    void failedMessageEmitted (KinesisMessageId messageId);
    void acked (KinesisMessageId messageId);

----------------------------------------

TITLE: Refreshing Worker Connections in Apache Storm (Clojure)
DESCRIPTION: The 'refresh-connections' function is called periodically to manage connections to other workers and maintain a mapping from task to worker.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L123

----------------------------------------

TITLE: Configuring Shell Component Working Directory in Java
DESCRIPTION: Code example showing how to configure a ShellSpout to work with executables shipped via blob store by changing the child process working directory.

LANGUAGE: java
CODE:
public MyShellSpout() {
    super("./newPython/bin/python", "./shell_spout.py");
    changeChildCWD(false);
}

----------------------------------------

TITLE: Storm Configuration Service Loading
DESCRIPTION: Example path for the ServiceLoader configuration file required when implementing custom Storm configurations.

LANGUAGE: text
CODE:
META-INF/services/org.apache.storm.validation.Validated

----------------------------------------

TITLE: Creating Kafka Topics for Apache Logs
DESCRIPTION: Commands to create Kafka topics for storing different types of Apache logs with specified partitions and replication factor.

LANGUAGE: bash
CODE:
kafka-topics --create --topic apache-logs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
kafka-topics --create --topic apache-errorlogs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
kafka-topics --create --topic apache-slowlogs --zookeeper localhost:2181 --replication-factor 1 --partitions 5

----------------------------------------

TITLE: Adding Storm Client Dependency in Maven POM
DESCRIPTION: This XML snippet shows how to add the Storm client as a provided dependency in a Maven project's pom.xml file. It specifies the groupId, artifactId, version, and scope for the Storm client dependency.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.storm</groupId>
  <artifactId>storm-client</artifactId>
  <version>{{page.version}}</version>
  <scope>provided</scope>
</dependency>

----------------------------------------

TITLE: Implementing Joins with JoinBolt in Storm Core
DESCRIPTION: Java code snippet demonstrating how to use JoinBolt to join multiple streams in Storm Core, equivalent to the SQL example. It shows the setup of JoinBolt, stream joining, field selection, and window configuration.

LANGUAGE: java
CODE:
JoinBolt jbolt =  new JoinBolt("spout1", "key1")                   // from        spout1  
                    .join     ("spout2", "userId",  "spout1")      // inner join  spout2  on spout2.userId = spout1.key1
                    .join     ("spout3", "key3",    "spout2")      // inner join  spout3  on spout3.key3   = spout2.userId   
                    .leftJoin ("spout4", "key4",    "spout3")      // left join   spout4  on spout4.key4   = spout3.key3
                    .select  ("userId, key4, key2, spout3:key3")   // chose output fields
                    .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;

topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("spout1", new Fields("key1") )
            .fieldsGrouping("spout2", new Fields("userId") )
            .fieldsGrouping("spout3", new Fields("key3") )
            .fieldsGrouping("spout4", new Fields("key4") );

----------------------------------------

TITLE: Worker Transfer Function for Tuple Sending in Apache Storm (Clojure)
DESCRIPTION: This code implements the transfer function used by tasks to send tuples to other tasks. It serializes the tuple and puts it onto a transfer queue, with a single queue per worker.

LANGUAGE: Clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L56

----------------------------------------

TITLE: Creating MQTT Topology with Storm Core Java API
DESCRIPTION: Java code for creating a Storm topology with an MQTT spout and logging bolt using the Storm Core Java API, equivalent to the Flux YAML configuration.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
MqttOptions options = new MqttOptions();
options.setTopics(Arrays.asList("/users/tgoetz/#"));
options.setCleanConnection(false);
MqttSpout spout = new MqttSpout(new StringMessageMapper(), options);

MqttBolt bolt = new LogInfoBolt();

builder.setSpout("mqtt-spout", spout);
builder.setBolt("log-bolt", bolt).shuffleGrouping("mqtt-spout");

return builder.createTopology();

----------------------------------------

TITLE: Configuring MongoUpdateBolt for Storm-MongoDB Integration
DESCRIPTION: This snippet demonstrates how to configure a MongoUpdateBolt, which is used to update data in a MongoDB collection. It specifies the MongoDB URL, collection name, QueryFilterCreator, and MongoMapper implementations.

LANGUAGE: java
CODE:
MongoMapper mapper = new SimpleMongoUpdateMapper()
        .withFields("word", "count");

QueryFilterCreator updateQueryCreator = new SimpleQueryFilterCreator()
        .withField("word");

MongoUpdateBolt updateBolt = new MongoUpdateBolt(url, collectionName, updateQueryCreator, mapper);

//if a new document should be inserted if there are no matches to the query filter
//updateBolt.withUpsert(true);

----------------------------------------

TITLE: Defining HBaseValueMapper Interface in Java
DESCRIPTION: The HBaseValueMapper interface for transforming HBase lookup results into Storm Values.

LANGUAGE: java
CODE:
public interface HBaseValueMapper extends Serializable {
    public List<Values> toTuples(Result result) throws Exception;
    void declareOutputFields(OutputFieldsDeclarer declarer);
}

----------------------------------------

TITLE: Cluster Resource Configuration Example - YAML
DESCRIPTION: Example of specifying GPU resources available on a Storm supervisor node in the storm.yaml configuration file.

LANGUAGE: yaml
CODE:
supervisor.resources.map: {"gpu.count" : 2.0}

----------------------------------------

TITLE: Configuring Metric Reporters in YAML for Apache Storm
DESCRIPTION: This YAML configuration example sets up two metric reporters for Apache Storm: a Graphite Reporter and a Console Reporter. It demonstrates how to specify the reporter class, reporting period, and additional parameters specific to each reporter type.

LANGUAGE: yaml
CODE:
topology.metrics.reporters:
  # Graphite Reporter
  - class: "org.apache.storm.metrics2.reporters.GraphiteStormReporter"
    report.period: 60
    report.period.units: "SECONDS"
    graphite.host: "localhost"
    graphite.port: 2003

  # Console Reporter
  - class: "org.apache.storm.metrics2.reporters.ConsoleStormReporter"
    report.period: 10
    report.period.units: "SECONDS"
    filter:
        class: "org.apache.storm.metrics2.filters.RegexFilter"
        expression: ".*my_component.*emitted.*"

----------------------------------------

TITLE: Implementing Parallel Top N Processing in Storm
DESCRIPTION: Demonstrates a scalable approach to computing top N items by partitioning the stream and performing parallel calculations before final merging. Uses fields grouping for initial partitioning and global grouping for final aggregation.

LANGUAGE: java
CODE:
builder.setBolt("rank", new RankObjects(), parallelism)
  .fieldsGrouping("objects", new Fields("value"));
builder.setBolt("merge", new MergeObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Using Storm Jar Command with Additional Dependencies
DESCRIPTION: Example of using the 'storm jar' command with additional JAR dependencies and Maven artifacts. It demonstrates how to include external JARs and specify Maven artifacts with exclusions.

LANGUAGE: shell
CODE:
./bin/storm jar example/storm-starter/storm-starter-topologies-*.jar org.apache.storm.starter.RollingTopWords blobstore-remote2 remote --jars "./external/storm-redis/storm-redis-1.1.0.jar,./external/storm-kafka-client/storm-kafka-client-1.1.0.jar" --artifacts "redis.clients:jedis:2.9.0,org.apache.kafka:kafka-clients:1.0.0^org.slf4j:slf4j-api" --artifactRepositories "jboss-repository^http://repository.jboss.com/maven2,HDPRepo^http://repo.hortonworks.com/content/groups/public/"

----------------------------------------

TITLE: Configuring Storm OpenTSDB Core Bolt
DESCRIPTION: Demonstrates how to configure and set up an OpenTSDB bolt in a Storm topology. Creates an OpenTSDB client with sync settings and configures the bolt with batch processing parameters.

LANGUAGE: java
CODE:
OpenTsdbClient.Builder builder =  OpenTsdbClient.newBuilder(openTsdbUrl).sync(30_000).returnDetails();
final OpenTsdbBolt openTsdbBolt = new OpenTsdbBolt(builder, Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
openTsdbBolt.withBatchSize(10).withFlushInterval(2000);
topologyBuilder.setBolt("opentsdb", openTsdbBolt).shuffleGrouping("metric-gen");

----------------------------------------

TITLE: Named Streams Join Example
DESCRIPTION: Example demonstrating how to join named streams instead of default streams using JoinBolt.

LANGUAGE: java
CODE:
new JoinBolt(JoinBolt.Selector.STREAM,  "stream1", "key1") 
                             .join     ("stream2", "userId",  "stream1" )
                             .select ("userId, key1, key2")
                             .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;
                             
topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("bolt1", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt2", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt3", "stream2", new Fields("userId") )
            .fieldsGrouping("bolt4", "stream1", new Fields("key1") );

----------------------------------------

TITLE: Setting CPU Requirements for Storm Components
DESCRIPTION: API call to set CPU requirements for Storm spouts and bolts.

LANGUAGE: java
CODE:
public T setCPULoad(Double amount)

----------------------------------------

TITLE: Launching Storm with Additional Dependencies in Python
DESCRIPTION: The storm.py launcher script provides options for including additional dependencies in the classpath. This snippet demonstrates the use of --jar and --artifacts options for the storm jar command.

LANGUAGE: python
CODE:
storm jar --jar additional_dependency.jar --artifacts "org.example:example-artifact:1.0.0" topology.jar

----------------------------------------

TITLE: Logging Message in JSON Format for Storm Multi-Language Protocol
DESCRIPTION: This JSON structure represents a log command in the Storm multi-language protocol. It includes the command type and the message to be logged in the worker log.

LANGUAGE: JSON
CODE:
{
    "command": "log",
    "msg": "hello world!"
}

----------------------------------------

TITLE: Implementing Split Function for Word Processing
DESCRIPTION: A custom function that splits sentences into individual words, creating separate tuples for each word.

LANGUAGE: java
CODE:
public class Split extends BaseFunction {
   public void execute(TridentTuple tuple, TridentCollector collector) {
       String sentence = tuple.getString(0);
       for(String word: sentence.split(" ")) {
           collector.emit(new Values(word));                
       }
   }
}

----------------------------------------

TITLE: Converting Docker Images for HDFS Storage
DESCRIPTION: Example command using docker-to-squash.py script to download Docker images, convert layers to squashfs files and upload to HDFS

LANGUAGE: bash
CODE:
python3 docker-to-squash.py pull-build-push-update --hdfs-root hdfs://hostname:port/containers \
                      docker.xxx.com:4443/hadoop-user-images/storm/rhel7:20201202-232133,storm/rhel7:dev_current --log DEBUG --bootstrap

----------------------------------------

TITLE: Submitting Storm Topology
DESCRIPTION: Command to submit the sample topology to Storm using the compiled jar file

LANGUAGE: bash
CODE:
storm jar {jarfile} com.microsoft.eventhubs.samples.EventCount {topologyname} {spoutconffile}

----------------------------------------

TITLE: Implementing Custom Metrics in Storm Bolt
DESCRIPTION: Example showing how to implement and register a custom metric (CountMetric) in a Storm bolt to track execution count

LANGUAGE: java
CODE:
private transient CountMetric countMetric;

@Override
public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
	// other initialization here.
	countMetric = new CountMetric();
	context.registerMetric("execute_count", countMetric, 60);
}

LANGUAGE: java
CODE:
public void execute(Tuple input) {
	countMetric.incr();
	// handle tuple here.	
}

----------------------------------------

TITLE: Defining HBaseValueMapper Interface in Java
DESCRIPTION: The HBaseValueMapper interface for transforming HBase lookup results into Storm Values.

LANGUAGE: java
CODE:
public interface HBaseValueMapper extends Serializable {
    public List<Values> toTuples(Result result) throws Exception;
    void declareOutputFields(OutputFieldsDeclarer declarer);
}

----------------------------------------

TITLE: Starting the Pacemaker Daemon
DESCRIPTION: Command to start the Pacemaker daemon in a Storm cluster.

LANGUAGE: bash
CODE:
$ storm pacemaker

----------------------------------------

TITLE: Creating a Stream with a Regular Storm Spout in Trident Topology
DESCRIPTION: Demonstrates how to create a stream using a regular Storm IRichSpout in a Trident topology. This code initializes a TridentTopology and adds a new stream with a unique identifier and a custom RichSpout implementation.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
topology.newStream("myspoutid", new MyRichSpout());

----------------------------------------

TITLE: Configuring Trident Topology with RedisState
DESCRIPTION: This Java code snippet demonstrates how to configure a Trident topology using RedisState for single Redis instance. It sets up the Redis connection, creates state factories, and configures state updates and queries.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
                                .setHost(redisHost).setPort(redisPort)
                                .build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisLookupMapper lookupMapper = new WordCountLookupMapper();
RedisState.Factory factory = new RedisState.Factory(poolConfig);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory,
                        fields,
                        new RedisStateUpdater(storeMapper).withExpire(86400000),
                        new Fields());

TridentState state = topology.newStaticState(factory);
stream = stream.stateQuery(state, new Fields("word"),
                        new RedisStateQuerier(lookupMapper),
                        new Fields("columnName","columnValue"));

----------------------------------------

TITLE: Converting Docker Images for HDFS Storage
DESCRIPTION: Example command using docker-to-squash.py script to download Docker images, convert layers to squashfs files and upload to HDFS

LANGUAGE: bash
CODE:
python3 docker-to-squash.py pull-build-push-update --hdfs-root hdfs://hostname:port/containers \
                      docker.xxx.com:4443/hadoop-user-images/storm/rhel7:20201202-232133,storm/rhel7:dev_current --log DEBUG --bootstrap

----------------------------------------

TITLE: Implementing Storm Topology with Kinesis Spout in Java
DESCRIPTION: This code snippet demonstrates how to create a Storm topology that uses the KinesisSpout to consume data from Amazon Kinesis Streams. It shows the configuration of various components such as RecordToTupleMapper, KinesisConnectionInfo, ZkInfo, and KinesisConfig.

LANGUAGE: java
CODE:
public class KinesisSpoutTopology {
    public static void main (String args[]) throws InvalidTopologyException, AuthorizationException, AlreadyAliveException {
        String topologyName = args[0];
        RecordToTupleMapper recordToTupleMapper = new TestRecordToTupleMapper();
        KinesisConnectionInfo kinesisConnectionInfo = new KinesisConnectionInfo(new CredentialsProviderChain(), new ClientConfiguration(), Regions.US_WEST_2,
                1000);
        ZkInfo zkInfo = new ZkInfo("localhost:2181", "/kinesisOffsets", 20000, 15000, 10000L, 3, 2000);
        KinesisConfig kinesisConfig = new KinesisConfig(args[1], ShardIteratorType.TRIM_HORIZON,
                recordToTupleMapper, new Date(), new ExponentialBackoffRetrier(), zkInfo, kinesisConnectionInfo, 10000L);
        KinesisSpout kinesisSpout = new KinesisSpout(kinesisConfig);
        TopologyBuilder topologyBuilder = new TopologyBuilder();
        topologyBuilder.setSpout("spout", kinesisSpout, 3);
        topologyBuilder.setBolt("bolt", new KinesisBoltTest(), 1).shuffleGrouping("spout");
        Config topologyConfig = new Config();
        topologyConfig.setDebug(true);
        topologyConfig.setNumWorkers(3);
        StormSubmitter.submitTopology(topologyName, topologyConfig, topologyBuilder.createTopology());
    }
}

----------------------------------------

TITLE: Registering Task Hook in Spout or Bolt in Java
DESCRIPTION: This snippet shows how to register a custom task hook in the open method of a spout or prepare method of a bolt using the TopologyContext method.

LANGUAGE: java
CODE:
TopologyContext.addTaskHook()

----------------------------------------

TITLE: Filtering Kafka Stream Example in Storm SQL
DESCRIPTION: SQL statements to filter large orders from a Kafka stream of transactions and insert them into another Kafka stream.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://orders?bootstrap-servers=localhost:9092,localhost:9093'
CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) LOCATION 'kafka://large_orders?bootstrap-servers=localhost:9092,localhost:9093' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Updating Thrift API for Nimbus HA
DESCRIPTION: Thrift struct definitions for including Nimbus information in cluster summaries. Adds NimbusSummary to ClusterSummary to provide details about all Nimbus instances.

LANGUAGE: thrift
CODE:
struct ClusterSummary {
  1: required list<SupervisorSummary> supervisors;
  3: required list<TopologySummary> topologies;
  4: required list<NimbusSummary> nimbuses;
}

struct NimbusSummary {
  1: required string host;
  2: required i32 port;
  3: required i32 uptime_secs;
  4: required bool isLeader;
  5: required string version;
}

----------------------------------------

TITLE: Adding Items to Kestrel Queue in Java
DESCRIPTION: Method to add random sentences to a Kestrel queue using KestrelClient. It selects sentences from a predefined array and queues them with unique IDs.

LANGUAGE: java
CODE:
private static void queueSentenceItems(KestrelClient kestrelClient, String queueName)
			throws ParseError, IOException {

		String[] sentences = new String[] {
	            "the cow jumped over the moon",
	            "an apple a day keeps the doctor away",
	            "four score and seven years ago",
	            "snow white and the seven dwarfs",
	            "i am at two with nature"};

		Random _rand = new Random();

		for(int i=1; i<=10; i++){

			String sentence = sentences[_rand.nextInt(sentences.length)];

			String val = "ID " + i + " " + sentence;

			boolean queueSucess = kestrelClient.queue(queueName, val);

			System.out.println("queueSucess=" +queueSucess+ " [" + val +"]");
		}
	}

----------------------------------------

TITLE: Worker Process Initialization in Clojure
DESCRIPTION: Sets up worker processes, establishes connections with other workers, and monitors topology activation state.

LANGUAGE: clojure
CODE:
mk-worker

----------------------------------------

TITLE: Worker Message Sending Thread in Apache Storm (Clojure)
DESCRIPTION: This code snippet shows the implementation of a single thread in the worker that drains the transfer queue and sends messages to other workers.

LANGUAGE: clojure
CODE:
(github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L185)

----------------------------------------

TITLE: Creating Multiple Streams in Trident Topology
DESCRIPTION: Demonstrates how to create conditional paths and multiple streams in a Trident topology using the Stream object. Shows stream branching with filters and subsequent operations.

LANGUAGE: java
CODE:
Stream s = topology.each(...).groupBy(...).aggregate(...)
Stream branch1 = s.each(..., FilterA)
Stream branch2 = s.each(..., FilterB)

----------------------------------------

TITLE: Configuring Health Check Timeout in Storm YAML
DESCRIPTION: This YAML configuration sets the timeout for health check scripts in milliseconds. It determines how long to wait before marking a health check as failed.

LANGUAGE: yaml
CODE:
storm.health.check.timeout.ms: 5000

----------------------------------------

TITLE: UDF Implementation Example
DESCRIPTION: Java implementation of a scalar user defined function

LANGUAGE: java
CODE:
  public class MyPlus {
    public static Integer evaluate(Integer x, Integer y) {
      return x + y;
    }
  }

----------------------------------------

TITLE: Implementing Gauge Metrics in Clojure
DESCRIPTION: Example of implementing a gauge metric to get instantaneous measurements. This specific example shows how to track the number of supervisors in a Storm cluster.

LANGUAGE: clojure
CODE:
(defgauge nimbus:num-supervisors
     (fn [] (.size (.supervisors (:storm-cluster-state nimbus) nil))))

----------------------------------------

TITLE: Overriding Local Mode in Java for IDE Debugging
DESCRIPTION: Demonstrates how to modify code to run within a LocalCluster for easier debugging in an IDE environment.

LANGUAGE: java
CODE:
public static void main(final String args[]) {
    LocalCluster.withLocalModeOverride(() -> originalMain(args), 10);
}

----------------------------------------

TITLE: Defining IRichBolt Interface in Java
DESCRIPTION: Java interface definition for IRichBolt, which extends IBolt and adds the declareOutputFields method. This interface is part of Storm's strategy to provide both Thrift and Java-friendly APIs.

LANGUAGE: Java
CODE:
public interface IRichBolt extends IBolt {
    void declareOutputFields(OutputFieldsDeclarer declarer);
    // ... other methods
}

----------------------------------------

TITLE: Configuring Metric Reporters in YAML
DESCRIPTION: Example configuration in storm.yaml for setting up Graphite and Console reporters with specific reporting intervals and filters.

LANGUAGE: yaml
CODE:
storm.metrics.reporters:
  # Graphite Reporter
  - class: "org.apache.storm.metrics2.reporters.GraphiteStormReporter"
    daemons:
        - "supervisor"
        - "nimbus"
        - "worker"
    report.period: 60
    report.period.units: "SECONDS"
    graphite.host: "localhost"
    graphite.port: 2003

  # Console Reporter
  - class: "org.apache.storm.metrics2.reporters.ConsoleStormReporter"
    daemons:
        - "worker"
    report.period: 10
    report.period.units: "SECONDS"
    filter:
        class: "org.apache.storm.metrics2.filters.RegexFilter"
        expression: ".*my_component.*emitted.*"

----------------------------------------

TITLE: Setting CPU Requirements in Storm Topology
DESCRIPTION: Example of setting CPU requirements for components in a Storm topology using points-based system.

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.setCPULoad(15.0);
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
            .shuffleGrouping("word").setCPULoad(10.0);
builder.setBolt("exclaim2", new HeavyBolt(), 1)
                .shuffleGrouping("exclaim1").setCPULoad(450.0);

----------------------------------------

TITLE: Storm Input Tuple JSON Format
DESCRIPTION: JSON structure representing an input tuple received by a shell process from Storm. Includes tuple ID, component ID, stream ID, task ID, and tuple values.

LANGUAGE: json
CODE:
{
    "id": -6955786537413359385,
    "comp": 1,
    "stream": 1,
    "task": 9,
    "tuple": ["snow white and the seven dwarfs", "field2", 3]
}

----------------------------------------

TITLE: Implementing Local DRPC in Storm
DESCRIPTION: Demonstrates how to set up and use DRPC (Distributed Remote Procedure Call) in local mode, including cluster initialization, topology submission, and DRPC execution.

LANGUAGE: java
CODE:
try (LocalDRPC drpc = new LocalDRPC();
     LocalCluster cluster = new LocalCluster();
     LocalTopology topo = cluster.submitTopology("drpc-demo", conf, builder.createLocalTopology(drpc))) {

    System.out.println("Results for 'hello':" + drpc.execute("exclamation", "hello"));
}

----------------------------------------

TITLE: SQL Grammar Definition in BNF Form
DESCRIPTION: Defines the SQL grammar supported by Storm SQL in Backus-Naur Form (BNF). Includes statements like SELECT, INSERT, UPDATE, DELETE, etc.

LANGUAGE: sql
CODE:
statement:
      setStatement
  |   resetStatement
  |   explain
  |   describe
  |   insert
  |   update
  |   merge
  |   delete
  |   query

setStatement:
      [ ALTER ( SYSTEM | SESSION ) ] SET identifier '=' expression

resetStatement:
      [ ALTER ( SYSTEM | SESSION ) ] RESET identifier
  |   [ ALTER ( SYSTEM | SESSION ) ] RESET ALL

explain:
      EXPLAIN PLAN
      [ WITH TYPE | WITH IMPLEMENTATION | WITHOUT IMPLEMENTATION ]
      [ EXCLUDING ATTRIBUTES | INCLUDING [ ALL ] ATTRIBUTES ]
      FOR ( query | insert | update | merge | delete )

describe:
      DESCRIBE DATABASE databaseName
   |  DESCRIBE CATALOG [ databaseName . ] catalogName
   |  DESCRIBE SCHEMA [ [ databaseName . ] catalogName ] . schemaName
   |  DESCRIBE [ TABLE ] [ [ [ databaseName . ] catalogName . ] schemaName . ] tableName [ columnName ]
   |  DESCRIBE [ STATEMENT ] ( query | insert | update | merge | delete )

insert:
      ( INSERT | UPSERT ) INTO tablePrimary
      [ '(' column [, column ]* ')' ]
      query

update:
      UPDATE tablePrimary
      SET assign [, assign ]*
      [ WHERE booleanExpression ]

assign:
      identifier '=' expression

merge:
      MERGE INTO tablePrimary [ [ AS ] alias ]
      USING tablePrimary
      ON booleanExpression
      [ WHEN MATCHED THEN UPDATE SET assign [, assign ]* ]
      [ WHEN NOT MATCHED THEN INSERT VALUES '(' value [ , value ]* ')' ]

delete:
      DELETE FROM tablePrimary [ [ AS ] alias ]
      [ WHERE booleanExpression ]

query:
      values
  |   WITH withItem [ , withItem ]* query
  |   {
          select
      |   selectWithoutFrom
      |   query UNION [ ALL ] query
      |   query EXCEPT query
      |   query INTERSECT query
      }
      [ ORDER BY orderItem [, orderItem ]* ]
      [ LIMIT { count | ALL } ]
      [ OFFSET start { ROW | ROWS } ]
      [ FETCH { FIRST | NEXT } [ count ] { ROW | ROWS } ]

withItem:
      name
      [ '(' column [, column ]* ')' ]
      AS '(' query ')'

orderItem:
      expression [ ASC | DESC ] [ NULLS FIRST | NULLS LAST ]

select:
      SELECT [ STREAM ] [ ALL | DISTINCT ]
          { * | projectItem [, projectItem ]* }
      FROM tableExpression
      [ WHERE booleanExpression ]
      [ GROUP BY { groupItem [, groupItem ]* } ]
      [ HAVING booleanExpression ]
      [ WINDOW windowName AS windowSpec [, windowName AS windowSpec ]* ]

selectWithoutFrom:
      SELECT [ ALL | DISTINCT ]
          { * | projectItem [, projectItem ]* }

projectItem:
      expression [ [ AS ] columnAlias ]
  |   tableAlias . *

tableExpression:
      tableReference [, tableReference ]*
  |   tableExpression [ NATURAL ] [ LEFT | RIGHT | FULL ] JOIN tableExpression [ joinCondition ]

joinCondition:
      ON booleanExpression
  |   USING '(' column [, column ]* ')'

tableReference:
      tablePrimary
      [ [ AS ] alias [ '(' columnAlias [, columnAlias ]* ')' ] ]

tablePrimary:
      [ [ catalogName . ] schemaName . ] tableName
      '(' TABLE [ [ catalogName . ] schemaName . ] tableName ')'
  |   [ LATERAL ] '(' query ')'
  |   UNNEST '(' expression ')' [ WITH ORDINALITY ]
  |   [ LATERAL ] TABLE '(' [ SPECIFIC ] functionName '(' expression [, expression ]* ')' ')'

values:
      VALUES expression [, expression ]*

groupItem:
      expression
  |   '(' ')'
  |   '(' expression [, expression ]* ')'
  |   CUBE '(' expression [, expression ]* ')'
  |   ROLLUP '(' expression [, expression ]* ')'
  |   GROUPING SETS '(' groupItem [, groupItem ]* ')'

windowRef:
      windowName
  |   windowSpec

windowSpec:
      [ windowName ]
      '('
      [ ORDER BY orderItem [, orderItem ]* ]
      [ PARTITION BY expression [, expression ]* ]
      [
          RANGE numericOrIntervalExpression { PRECEDING | FOLLOWING }
      |   ROWS numericExpression { PRECEDING | FOLLOWING }
      ]
      ')'

----------------------------------------

TITLE: Running DRPC in Local Mode with Storm in Java
DESCRIPTION: This example shows how to run a DRPC (Distributed Remote Procedure Call) topology in local mode using Storm. It demonstrates creating a LocalDRPC object, submitting a topology, and executing DRPC calls.

LANGUAGE: java
CODE:
try (LocalDRPC drpc = new LocalDRPC();
     LocalCluster cluster = new LocalCluster();
     LocalTopology topo = cluster.submitTopology("drpc-demo", conf, builder.createLocalTopology(drpc))) {

    System.out.println("Results for 'hello':" + drpc.execute("exclamation", "hello"));
}

----------------------------------------

TITLE: Configuring Kerberos Authentication for Pacemaker Server
DESCRIPTION: JAAS configuration for Kerberos authentication on Pacemaker server.

LANGUAGE: java
CODE:
PacemakerServer {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/etc/keytabs/pacemaker.keytab"
   storeKey=true
   useTicketCache=false
   principal="pacemaker@MY.COMPANY.COM";
};

----------------------------------------

TITLE: Receiving Tuple in JSON Format for Storm Multi-Language Protocol
DESCRIPTION: This JSON structure represents a tuple received by a script in the Storm multi-language protocol. It includes the tuple's id, component id, stream id, task id, and the actual tuple values.

LANGUAGE: JSON
CODE:
{
    "id": -6955786537413359385,
    "comp": 1,
    "stream": 1,
    "task": 9,
    "tuple": ["snow white and the seven dwarfs", "field2", 3]
}

----------------------------------------

TITLE: Updating ClusterSummary Thrift Structure for Storm Nimbus HA
DESCRIPTION: Thrift structure definitions for ClusterSummary and NimbusSummary, adding nimbus information to support high availability.

LANGUAGE: thrift
CODE:
struct ClusterSummary {
  1: required list<SupervisorSummary> supervisors;
  3: required list<TopologySummary> topologies;
  4: required list<NimbusSummary> nimbuses;
}

struct NimbusSummary {
  1: required string host;
  2: required i32 port;
  3: required i32 uptime_secs;
  4: required bool isLeader;
  5: required string version;
}

----------------------------------------

TITLE: Receiving Tuple in JSON Format for Storm Multi-Language Protocol
DESCRIPTION: This JSON structure represents a tuple received by a script in the Storm multi-language protocol. It includes the tuple's id, component id, stream id, task id, and the actual tuple values.

LANGUAGE: JSON
CODE:
{
    "id": -6955786537413359385,
    "comp": 1,
    "stream": 1,
    "task": 9,
    "tuple": ["snow white and the seven dwarfs", "field2", 3]
}

----------------------------------------

TITLE: FailedMessageRetryHandler Interface Implementation in Java
DESCRIPTION: Interface definition for handling failed message retry logic in Kinesis spout. Includes methods for managing failed messages, retry scheduling, and acknowledgment handling.

LANGUAGE: java
CODE:
    boolean failed (KinesisMessageId messageId);
    KinesisMessageId getNextFailedMessageToRetry ();
    void failedMessageEmitted (KinesisMessageId messageId);
    void acked (KinesisMessageId messageId);

----------------------------------------

TITLE: Emitting a Tuple with a Message ID in Storm
DESCRIPTION: This snippet demonstrates how to emit a tuple from a spout with a message ID for tracking purposes. The message ID is used by Storm to identify the tuple for acking or failing.

LANGUAGE: java
CODE:
_collector.emit(new Values("field1", "field2", 3) , msgId);

----------------------------------------

TITLE: Configuring User Resource Pools in YAML
DESCRIPTION: Configuration format for specifying resource guarantees for users in user-resource-pools.yaml.

LANGUAGE: yaml
CODE:
resource.aware.scheduler.user.pools:
    [UserId]
        cpu: [Amount of Guarantee CPU Resources]
        memory: [Amount of Guarantee Memory Resources]

----------------------------------------

TITLE: Running a Storm Topology Locally
DESCRIPTION: Runs a topology in local mode for testing, using an embedded version of Storm daemons within the same process.

LANGUAGE: shell
CODE:
storm local topology-jar-path class ...

----------------------------------------

TITLE: Setting Topology Priority in Storm Config
DESCRIPTION: API call to set the priority of a Storm topology for scheduling.

LANGUAGE: java
CODE:
conf.setTopologyPriority(int priority)

----------------------------------------

TITLE: Dequeuing and Removing Items from Kestrel
DESCRIPTION: Method to dequeue and remove items from a Kestrel queue. Retrieves items and acknowledges them to remove them from the queue.

LANGUAGE: java
CODE:
    private static void dequeueAndRemoveItems(KestrelClient kestrelClient, String queueName)
    throws IOException, ParseError
		 {
			for(int i=1; i<=12; i++){

				Item item = kestrelClient.dequeue(queueName);


				if(item==null){
					System.out.println("The queue (" + queueName + ") contains no items.");
				}
				else
				{
					int itemID = item._id;


					byte[] data = item._data;

					String receivedVal = new String(data);

					kestrelClient.ack(queueName, itemID);

					System.out.println("receivedItem=" + receivedVal);
				}
			}
	}

----------------------------------------

TITLE: Adding Items to Kestrel Queue in Java
DESCRIPTION: Method to add random sentences to a Kestrel queue using KestrelClient. It selects sentences from a predefined array and queues them with unique IDs.

LANGUAGE: java
CODE:
private static void queueSentenceItems(KestrelClient kestrelClient, String queueName)
			throws ParseError, IOException {

		String[] sentences = new String[] {
	            "the cow jumped over the moon",
	            "an apple a day keeps the doctor away",
	            "four score and seven years ago",
	            "snow white and the seven dwarfs",
	            "i am at two with nature"};

		Random _rand = new Random();

		for(int i=1; i<=10; i++){

			String sentence = sentences[_rand.nextInt(sentences.length)];

			String val = "ID " + i + " " + sentence;

			boolean queueSucess = kestrelClient.queue(queueName, val);

			System.out.println("queueSucess=" +queueSucess+ " [" + val +"]");
		}
	}

----------------------------------------

TITLE: Implementing DruidBeamFactory
DESCRIPTION: Sample implementation of DruidBeamFactory showing configuration of Druid connection, data schema, and indexing parameters using Tranquility.

LANGUAGE: java
CODE:
public class SampleDruidBeamFactoryImpl implements DruidBeamFactory<Map<String, Object>> {

    @Override
    public Beam<Map<String, Object>> makeBeam(Map<?, ?> conf, IMetricsContext metrics) {


        final String indexService = "druid/overlord"; // The druid.service name of the indexing service Overlord node.
        final String discoveryPath = "/druid/discovery"; // Curator service discovery path. config: druid.discovery.curator.path
        final String dataSource = "test"; //The name of the ingested datasource. Datasources can be thought of as tables.
        final List<String> dimensions = ImmutableList.of("publisher", "advertiser");
        List<AggregatorFactory> aggregators = ImmutableList.<AggregatorFactory>of(
                new CountAggregatorFactory(
                        "click"
                )
        );
        // Tranquility needs to be able to extract timestamps from your object type (in this case, Map<String, Object>).
        final Timestamper<Map<String, Object>> timestamper = new Timestamper<Map<String, Object>>()
        {
            @Override
            public DateTime timestamp(Map<String, Object> theMap)
            {
                return new DateTime(theMap.get("timestamp"));
            }
        };

        // Tranquility uses ZooKeeper (through Curator) for coordination.
        final CuratorFramework curator = CuratorFrameworkFactory
                .builder()
                .connectString((String)conf.get("druid.tranquility.zk.connect")) //take config from storm conf
                .retryPolicy(new ExponentialBackoffRetry(1000, 20, 30000))
                .build();
        curator.start();

        // The JSON serialization of your object must have a timestamp field in a format that Druid understands. By default,
        // Druid expects the field to be called "timestamp" and to be an ISO8601 timestamp.
        final TimestampSpec timestampSpec = new TimestampSpec("timestamp", "auto", null);

        // Tranquility needs to be able to serialize your object type to JSON for transmission to Druid. By default this is
        // done with Jackson. If you want to provide an alternate serializer, you can provide your own via ```.objectWriter(...)```.
        // In this case, we won't provide one, so we're just using Jackson.
        final Beam<Map<String, Object>> beam = DruidBeams
                .builder(timestamper)
                .curator(curator)
                .discoveryPath(discoveryPath)
                .location(DruidLocation.create(indexService, dataSource))
                .timestampSpec(timestampSpec)
                .rollup(DruidRollup.create(DruidDimensions.specific(dimensions), aggregators, QueryGranularities.MINUTE))
                .tuning(
                        ClusteredBeamTuning
                                .builder()
                                .segmentGranularity(Granularity.HOUR)
                                .windowPeriod(new Period("PT10M"))
                                .partitions(1)
                                .replicants(1)
                                .build()
                )
                .druidBeamConfig(
                      DruidBeamConfig
                           .builder()
                           .indexRetryPeriod(new Period("PT10M"))
                           .build())
                .buildBeam();

        return beam;
    }
}

----------------------------------------

TITLE: Configuring Maven Dependencies for Storm Kinesis Integration
DESCRIPTION: This XML snippet shows the necessary Maven dependencies for integrating Storm with Amazon Kinesis. It includes dependencies for AWS SDK, Storm client, Apache Curator, and JSON Simple library. The AWS SDK version used is 1.10.77.

LANGUAGE: xml
CODE:
 <dependencies>
        <dependency>
            <groupId>com.amazonaws</groupId>
            <artifactId>aws-java-sdk</artifactId>
            <version>${aws-java-sdk.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-client</artifactId>
            <version>${project.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-framework</artifactId>
            <version>${curator.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>com.googlecode.json-simple</groupId>
            <artifactId>json-simple</artifactId>
        </dependency>
 </dependencies>

----------------------------------------

TITLE: Setting Topology Log Levels
DESCRIPTION: Dynamically changes log levels for a running topology with optional timeout.

LANGUAGE: shell
CODE:
storm set_log_level -l [logger name]=[log level][:optional timeout] -r [logger name] topology-name

----------------------------------------

TITLE: Kerberos Server JAAS Configuration
DESCRIPTION: JAAS configuration for Pacemaker server using Kerberos authentication.

LANGUAGE: java
CODE:
PacemakerServer {\n   com.sun.security.auth.module.Krb5LoginModule required\n   useKeyTab=true\n   keyTab="/etc/keytabs/pacemaker.keytab"\n   storeKey=true\n   useTicketCache=false\n   principal="pacemaker@MY.COMPANY.COM";\n};

----------------------------------------

TITLE: Implementing Storm Kinesis Spout Topology in Java
DESCRIPTION: Example topology implementation showing how to create and configure a KinesisSpout with necessary connection and configuration parameters. The topology sets up spout tasks to consume data from Kinesis shards.

LANGUAGE: java
CODE:
public class KinesisSpoutTopology {
    public static void main (String args[]) throws InvalidTopologyException, AuthorizationException, AlreadyAliveException {
        String topologyName = args[0];
        RecordToTupleMapper recordToTupleMapper = new TestRecordToTupleMapper();
        KinesisConnectionInfo kinesisConnectionInfo = new KinesisConnectionInfo(new CredentialsProviderChain(), new ClientConfiguration(), Regions.US_WEST_2,
                1000);
        ZkInfo zkInfo = new ZkInfo("localhost:2181", "/kinesisOffsets", 20000, 15000, 10000L, 3, 2000);
        KinesisConfig kinesisConfig = new KinesisConfig(args[1], ShardIteratorType.TRIM_HORIZON,
                recordToTupleMapper, new Date(), new ExponentialBackoffRetrier(), zkInfo, kinesisConnectionInfo, 10000L);
        KinesisSpout kinesisSpout = new KinesisSpout(kinesisConfig);
        TopologyBuilder topologyBuilder = new TopologyBuilder();
        topologyBuilder.setSpout("spout", kinesisSpout, 3);
        topologyBuilder.setBolt("bolt", new KinesisBoltTest(), 1).shuffleGrouping("spout");
        Config topologyConfig = new Config();
        topologyConfig.setDebug(true);
        topologyConfig.setNumWorkers(3);
        StormSubmitter.submitTopology(topologyName, topologyConfig, topologyBuilder.createTopology());
    }
}

----------------------------------------

TITLE: Setting Storm Health Check Directory in YAML
DESCRIPTION: This snippet shows how to configure the storm.health.check.dir in storm.yaml. It specifies the directory where administrator-supplied health check scripts are located.

LANGUAGE: yaml
CODE:
storm.health.check.dir: "healthchecks"

----------------------------------------

TITLE: Implementing Storm Core Bolt for Druid Integration
DESCRIPTION: Example showing how to configure and use the DruidBeamBolt for sending data to Druid. Demonstrates setting up the bolt with a beam factory, configuration, and event mapper.

LANGUAGE: java
CODE:
   DruidBeamFactory druidBeamFactory = new SampleDruidBeamFactoryImpl(new HashMap<String, Object>());
   DruidConfig druidConfig = DruidConfig.newBuilder().discardStreamId(DruidConfig.DEFAULT_DISCARD_STREAM_ID).build();
   ITupleDruidEventMapper<Map<String, Object>> eventMapper = new TupleDruidEventMapper<>(TupleDruidEventMapper.DEFAULT_FIELD_NAME);
   DruidBeamBolt<Map<String, Object>> druidBolt = new DruidBeamBolt<Map<String, Object>>(druidBeamFactory, eventMapper, druidConfig);
   topologyBuilder.setBolt("druid-bolt", druidBolt).shuffleGrouping("event-gen");
   topologyBuilder.setBolt("printer-bolt", new PrinterBolt()).shuffleGrouping("druid-bolt" , druidConfig.getDiscardStreamId());

----------------------------------------

TITLE: Distributed Mode Message Receiving in Apache Storm
DESCRIPTION: In distributed mode, each worker listens on a single TCP port for incoming messages and routes them in-memory to tasks.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L204

----------------------------------------

TITLE: Maven Assembly Plugin Configuration
DESCRIPTION: Maven configuration for packaging a Storm topology into a JAR with dependencies. Uses the maven-assembly-plugin to create a standalone JAR that includes all required dependencies except Storm itself.

LANGUAGE: xml
CODE:
  <plugin>
    <artifactId>maven-assembly-plugin</artifactId>
    <configuration>
      <descriptorRefs>  
        <descriptorRef>jar-with-dependencies</descriptorRef>
      </descriptorRefs>
      <archive>
        <manifest>
          <mainClass>com.path.to.main.Class</mainClass>
        </manifest>
      </archive>
    </configuration>
  </plugin>

----------------------------------------

TITLE: Submitting a Storm Topology with Blobstore Mapping
DESCRIPTION: Example of submitting a Storm topology with configuration to map blobstore keys to local files.

LANGUAGE: bash
CODE:
storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar org.apache.storm.starter.clj.word_count test_topo -c topology.blobstore.map='{"key1":{"localname":"blob_file", "uncompress":false, "workerRestart":true},"key2":{}}'

----------------------------------------

TITLE: Using Preconfigured DRPC Client
DESCRIPTION: Simplified way to create a DRPC client using preconfigured settings.

LANGUAGE: java
CODE:
DRPCClient client = DRPCClient.getConfiguredClient(conf);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Configuring Kerberos Authentication for Pacemaker Client
DESCRIPTION: JAAS configuration for Kerberos authentication on Nimbus for Pacemaker client.

LANGUAGE: java
CODE:
PacemakerClient {
    com.sun.security.auth.module.Krb5LoginModule required
    useKeyTab=true
    keyTab="/etc/keytabs/nimbus.keytab"
    storeKey=true
    useTicketCache=false
    serviceName="pacemaker"
    principal="nimbus@MY.COMPANY.COM";
};

----------------------------------------

TITLE: Defining Leader Election Interface in Java
DESCRIPTION: Defines the ILeaderElector interface for implementing leader election among Nimbus instances. Includes methods for joining/leaving the election queue, checking leadership status, and getting addresses of Nimbus nodes.

LANGUAGE: java
CODE:
public interface ILeaderElector {
    void addToLeaderLockQueue();
    void removeFromLeaderLockQueue();
    boolean isLeader();
    InetSocketAddress getLeaderAddress();
    List<InetSocketAddress> getAllNimbusAddresses();
}

----------------------------------------

TITLE: Storm Spout Implementation in Clojure
DESCRIPTION: Example of a basic spout implementation that emits random sentences using defspout macro.

LANGUAGE: clojure
CODE:
(defspout sentence-spout ["sentence"]
  [conf context collector]
  (let [sentences ["a little brown dog"
                   "the man petted the dog"
                   "four score and seven years ago"
                   "an apple a day keeps the doctor away"]]
    (spout
     (nextTuple []
       (Thread/sleep 100)
       (emit-spout! collector [(rand-nth sentences)])         
       )
     (ack [id]
        ))))

----------------------------------------

TITLE: Thread-safe Tuple Serialization in Apache Storm (Java)
DESCRIPTION: This Java code implements a thread-safe serializer for Storm tuples, ensuring safe concurrent access during the serialization process.

LANGUAGE: Java
CODE:
https://github.com/apache/storm/blob/0.7.1/src/jvm/org/apache/storm/serialization/KryoTupleSerializer.java#L26

----------------------------------------

TITLE: Adding Storm Client Dependency in Maven POM
DESCRIPTION: This XML snippet shows how to include the Storm client library as a provided dependency in a Maven project's pom.xml file. It specifies the groupId, artifactId, and version, with the scope set to 'provided'.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.storm</groupId>
  <artifactId>storm-client</artifactId>
  <version>{{page.version}}</version>
  <scope>provided</scope>
</dependency>

----------------------------------------

TITLE: Implementing RedisFilterMapper in Java
DESCRIPTION: Java class implementing RedisFilterMapper interface for filtering words based on a Redis set.

LANGUAGE: java
CODE:
class BlacklistWordFilterMapper implements RedisFilterMapper {
    private RedisDataTypeDescription description;
    private final String setKey = "blacklist";

    public BlacklistWordFilterMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.SET, setKey);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Adding Generic Resources API - Java
DESCRIPTION: API method for specifying generic resource requirements for Storm topology components. The method takes a resource name and value as parameters to define resource requirements for spouts or bolts.

LANGUAGE: java
CODE:
public T addResource(String resourceName, Number resourceValue)

----------------------------------------

TITLE: Storm Topology Rebalance Command
DESCRIPTION: CLI command example for rebalancing a running Storm topology, showing how to modify the number of worker processes and executors without restart.

LANGUAGE: bash
CODE:
storm rebalance mytopology -n 5 -e blue-spout=3 -e yellow-bolt=10

----------------------------------------

TITLE: Activating Storm Topology Spouts
DESCRIPTION: Activates the spouts of a specified topology.

LANGUAGE: shell
CODE:
storm activate topology-name

----------------------------------------

TITLE: Normalizing Topology Configuration in Nimbus (Clojure)
DESCRIPTION: Nimbus normalizes the topology configuration to ensure consistent serialization registrations across all tasks, which is critical for correct serialization.

LANGUAGE: clojure
CODE:
// Normalization code
(normalize-topology-config)

----------------------------------------

TITLE: Configuring HDFS Spout in Java
DESCRIPTION: Example of how to configure an HDFS Spout to read text files from HDFS.

LANGUAGE: java
CODE:
// Instantiate spout to read text files
HdfsSpout textReaderSpout = new HdfsSpout().setReaderType("text")
                                           .withOutputFields(TextFileReader.defaultFields)                                      
                                           .setHdfsUri("hdfs://localhost:54310")  // required
                                           .setSourceDir("/data/in")              // required                                      
                                           .setArchiveDir("/data/done")           // required
                                           .setBadFilesDir("/data/badfiles");     // required                                      
// If using Kerberos
HashMap hdfsSettings = new HashMap();
hdfsSettings.put("hdfs.keytab.file", "/path/to/keytab");
hdfsSettings.put("hdfs.kerberos.principal","user@EXAMPLE.com");

textReaderSpout.setHdfsClientSettings(hdfsSettings);

// Create topology
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("hdfsspout", textReaderSpout, SPOUT_NUM);

// Setup bolts and wire up topology
     ..snip..

// Submit topology with config
Config conf = new Config();
StormSubmitter.submitTopologyWithProgressBar("topologyName", conf, builder.createTopology());

----------------------------------------

TITLE: Configuring HDFS Spout in Java
DESCRIPTION: Example of how to configure an HDFS Spout to read text files from HDFS.

LANGUAGE: java
CODE:
// Instantiate spout to read text files
HdfsSpout textReaderSpout = new HdfsSpout().setReaderType("text")
                                           .withOutputFields(TextFileReader.defaultFields)                                      
                                           .setHdfsUri("hdfs://localhost:54310")  // required
                                           .setSourceDir("/data/in")              // required                                      
                                           .setArchiveDir("/data/done")           // required
                                           .setBadFilesDir("/data/badfiles");     // required                                      
// If using Kerberos
HashMap hdfsSettings = new HashMap();
hdfsSettings.put("hdfs.keytab.file", "/path/to/keytab");
hdfsSettings.put("hdfs.kerberos.principal","user@EXAMPLE.com");

textReaderSpout.setHdfsClientSettings(hdfsSettings);

// Create topology
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("hdfsspout", textReaderSpout, SPOUT_NUM);

// Setup bolts and wire up topology
     ..snip..

// Submit topology with config
Config conf = new Config();
StormSubmitter.submitTopologyWithProgressBar("topologyName", conf, builder.createTopology());

----------------------------------------

TITLE: Creating Implicit Topology in Storm (Clojure)
DESCRIPTION: The system-topology! function is used to create an implicit topology with added streams and an acker bolt for managing the acking framework. This is used both by Nimbus when creating tasks and by workers for message routing.

LANGUAGE: clojure
CODE:
(system-topology!)

----------------------------------------

TITLE: Launching Container with runc
DESCRIPTION: Command used to launch the container using runc

LANGUAGE: bash
CODE:
/usr/bin/runc run -d \
              --pid-file /home/y/var/storm/workers/1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb/artifacts/container-1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb.pid \
              -b /home/y/var/storm/workers/1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb \
              6703-1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb

----------------------------------------

TITLE: Reading a Blob in Java
DESCRIPTION: Reads the contents of a blob from Storm's blobstore

LANGUAGE: java
CODE:
String blobKey = "some_key";
InputStreamWithMeta blobInputStream = clientBlobStore.getBlob(blobKey);
BufferedReader r = new BufferedReader(new InputStreamReader(blobInputStream));
String blobContents =  r.readLine();

----------------------------------------

TITLE: Storm Topology Rebalance Command
DESCRIPTION: CLI command example for rebalancing a running Storm topology, showing how to modify the number of worker processes and executors without restart.

LANGUAGE: bash
CODE:
storm rebalance mytopology -n 5 -e blue-spout=3 -e yellow-bolt=10

----------------------------------------

TITLE: Storm Fail Command JSON Structure
DESCRIPTION: JSON structure for reporting tuple processing failures in Storm's multilang protocol.

LANGUAGE: json
CODE:
{
    "command": "fail",
    "id": 123123
}

----------------------------------------

TITLE: Storm Jar Upload Process
DESCRIPTION: Thrift interface definition for uploading topology jars to Nimbus in chunks

LANGUAGE: thrift
CODE:
beginFileUpload
uploadChunk
finishFileUpload

----------------------------------------

TITLE: Registering Custom Serializers in Storm Topology Configuration
DESCRIPTION: This YAML snippet demonstrates how to register custom serializers for specific classes in a Storm topology configuration. It shows both default FieldsSerializer usage and custom serializer implementation.

LANGUAGE: yaml
CODE:
topology.kryo.register:
  - com.mycompany.CustomType1
  - com.mycompany.CustomType2: com.mycompany.serializer.CustomType2Serializer
  - com.mycompany.CustomType3

----------------------------------------

TITLE: Configuring DelimitedRecordHiveMapper for HiveBolt (Java)
DESCRIPTION: Java code examples showing different ways to configure DelimitedRecordHiveMapper, which maps Tuple field names to Hive table column names. It demonstrates setting column fields, partition fields, and using time as a partition field.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
    or
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withTimeAsPartitionField("YYYY/MM/DD");

----------------------------------------

TITLE: Creating StateFactory for Custom State
DESCRIPTION: Example of implementing a StateFactory to create instances of a custom State object (LocationDB) within Trident tasks.

LANGUAGE: Java
CODE:
public class LocationDBFactory implements StateFactory {
   public State makeState(Map conf, int partitionIndex, int numPartitions) {
      return new LocationDB();
   } 
}

----------------------------------------

TITLE: Storm Metrics Filter Interface
DESCRIPTION: Interface definition for creating custom metric filters in Storm. Shows the required methods for implementing a custom filter that determines which metrics get reported.

LANGUAGE: java
CODE:
public interface StormMetricsFilter extends MetricFilter {

    /**
     * Called after the filter is instantiated.
     * @param config A map of the properties from the 'filter' section of the reporter configuration.
     */
    void prepare(Map<String, Object> config);
    
   /**
    *  Returns true if the given metric should be reported.
    */
    boolean matches(String name, Metric metric);

}

----------------------------------------

TITLE: Implementing Trident State for Druid
DESCRIPTION: Shows how to implement Trident state integration with Druid using DruidBeamStateFactory and event mapper configuration.

LANGUAGE: java
CODE:
    DruidBeamFactory druidBeamFactory = new SampleDruidBeamFactoryImpl(new HashMap<String, Object>());
    ITupleDruidEventMapper<Map<String, Object>> eventMapper = new TupleDruidEventMapper<>(TupleDruidEventMapper.DEFAULT_FIELD_NAME);

    final Stream stream = tridentTopology.newStream("batch-event-gen", new SimpleBatchSpout(10));

    stream.peek(new Consumer() {
        @Override
        public void accept(TridentTuple input) {
             LOG.info("########### Received tuple: [{}]", input);
         }
    }).partitionPersist(new DruidBeamStateFactory<Map<String, Object>>(druidBeamFactory, eventMapper), new Fields("event"), new DruidBeamStateUpdater());

----------------------------------------

TITLE: Implementing Word Count Bolt in Clojure
DESCRIPTION: Prepared bolt implementation that maintains state to count word occurrences using an atom to store counts.

LANGUAGE: clojure
CODE:
(defbolt word-count ["word" "count"] {:prepare true}
  [conf context collector]
  (let [counts (atom {})]
    (bolt
     (execute [tuple]
       (let [word (.getString tuple 0)]
         (swap! counts (partial merge-with +) {word 1})
         (emit-bolt! collector [word (@counts word)] :anchor tuple)
         (ack! collector tuple)
         )))))

----------------------------------------

TITLE: Dequeuing Items from Kestrel Queue in Java
DESCRIPTION: This method dequeues items from a Kestrel queue without removing them. It attempts to dequeue 12 items and prints them if available.

LANGUAGE: java
CODE:
    private static void dequeueItems(KestrelClient kestrelClient, String queueName) throws IOException, ParseError
			 {
		for(int i=1; i<=12; i++){

			Item item = kestrelClient.dequeue(queueName);

			if(item==null){
				System.out.println("The queue (" + queueName + ") contains no items.");
			}
			else
			{
				byte[] data = item._data;

				String receivedVal = new String(data);

				System.out.println("receivedItem=" + receivedVal);
			}
		}

----------------------------------------

TITLE: Storm BasicBolt Implementation
DESCRIPTION: Simplified implementation of SplitSentence using BasicBolt interface for automatic anchoring and acking.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseBasicBolt {
        public void execute(Tuple tuple, BasicOutputCollector collector) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                collector.emit(new Values(word));
            }
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Emitting a Tuple with Message ID in Storm Spout
DESCRIPTION: This code snippet demonstrates how to emit a tuple from a Storm spout using the SpoutOutputCollector. It includes a message ID for tracking and reliability purposes.

LANGUAGE: java
CODE:
_collector.emit(new Values("field1", "field2", 3) , msgId);

----------------------------------------

TITLE: Configuring Resource Aware Scheduler in Storm YAML
DESCRIPTION: Basic configuration to enable the Resource Aware Scheduler in Storm's configuration file

LANGUAGE: yaml
CODE:
storm.scheduler: "org.apache.storm.scheduler.resource.ResourceAwareScheduler"

----------------------------------------

TITLE: Installing ZeroMQ 2.1.7 for Storm
DESCRIPTION: This snippet shows the process of downloading, extracting, configuring, and installing ZeroMQ 2.1.7, which is the recommended version for use with Storm. It includes commands for downloading the source, extracting it, and running the configure and make scripts.

LANGUAGE: bash
CODE:
wget http://download.zeromq.org/zeromq-2.1.7.tar.gz
tar -xzf zeromq-2.1.7.tar.gz
cd zeromq-2.1.7
./configure
make
sudo make install

----------------------------------------

TITLE: Implementing Sliding Window Bolt in Java
DESCRIPTION: Example implementation of a sliding window bolt showing window configuration, tuple processing, and topology setup.

LANGUAGE: java
CODE:
public class SlidingWindowBolt extends BaseWindowedBolt {
	private OutputCollector collector;
	
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
    	this.collector = collector;
    }
	
    @Override
    public void execute(TupleWindow inputWindow) {
	  for(Tuple tuple: inputWindow.get()) {
	    // do the windowing computation
		...
	  }
	  // emit the results
	  collector.emit(new Values(computedValue));
    }
}

public static void main(String[] args) {
    TopologyBuilder builder = new TopologyBuilder();
     builder.setSpout("spout", new RandomSentenceSpout(), 1);
     builder.setBolt("slidingwindowbolt", 
                     new SlidingWindowBolt().withWindow(new Count(30), new Count(10)),
                     1).shuffleGrouping("spout");
    Config conf = new Config();
    conf.setDebug(true);
    conf.setNumWorkers(1);

    StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());
	
}

----------------------------------------

TITLE: IConfigLoader Interface Definition in Java
DESCRIPTION: Definition of the IConfigLoader interface with a single method 'load' that retrieves the most recent configuration map.

LANGUAGE: java
CODE:
public interface IConfigLoader {
    Map<?,?> load();
};

----------------------------------------

TITLE: Displaying jQuery UI v1.12.1 License
DESCRIPTION: This snippet contains the full text of the jQuery UI license, which is based on the MIT license. It includes copyright information, terms of use, disclaimer of warranty, and additional notes on sample code and external libraries.

LANGUAGE: plaintext
CODE:
Copyright jQuery Foundation and other contributors, https://jquery.org/

This software consists of voluntary contributions made by many
individuals. For exact contribution history, see the revision history
available at https://github.com/jquery/jquery-ui

The following license applies to all parts of this software except as
documented below:

====

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

====

Copyright and related rights for sample code are waived via CC0. Sample
code is defined as all source code contained within the demos directory.

CC0: http://creativecommons.org/publicdomain/zero/1.0/

====

All files located in the node_modules and external directories are
externally maintained libraries used by this software which have their
own licenses; we recommend you read them, as their terms may differ from
the terms above.

----------------------------------------

TITLE: Initializing DRPC Client in Java
DESCRIPTION: This snippet demonstrates how to initialize a DRPC client with configuration settings for connecting to a DRPC server. It includes retry settings and transport plugin configuration.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.put("storm.thrift.transport", "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin");
conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 3);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL, 10);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING, 20);
DRPCClient client = new DRPCClient(conf, "drpc-host", 3772);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Configuring Kryo Serialization Registration in Storm YAML
DESCRIPTION: Example YAML configuration showing how to register custom serializers for Storm using Kryo. Demonstrates both default FieldsSerializer registration and custom serializer implementation registration.

LANGUAGE: yaml
CODE:
topology.kryo.register:
  - com.mycompany.CustomType1
  - com.mycompany.CustomType2: com.mycompany.serializer.CustomType2Serializer
  - com.mycompany.CustomType3

----------------------------------------

TITLE: Configuring Resource Consumption for Trident Topology in Java
DESCRIPTION: This code snippet demonstrates how to set resource defaults and specify CPU and memory loads for different operations in a Trident topology. It shows the usage of setResourceDefaults, setCPULoad, setMemoryLoad, and parallelismHint methods on various stream operations.

LANGUAGE: java
CODE:
    TridentTopology topo = new TridentTopology();
    topo.setResourceDefaults(new DefaultResourceDeclarer();
                                                          .setMemoryLoad(128)
                                                          .setCPULoad(20));
    TridentState wordCounts =
        topology
            .newStream("words", feeder)
            .parallelismHint(5)
            .setCPULoad(20)
            .setMemoryLoad(512,256)
            .each( new Fields("sentence"),  new Split(), new Fields("word"))
            .setCPULoad(10)
            .setMemoryLoad(512)
            .each(new Fields("word"), new BangAdder(), new Fields("word!"))
            .parallelismHint(10)
            .setCPULoad(50)
            .setMemoryLoad(1024)
            .each(new Fields("word!"), new QMarkAdder(), new Fields("word!?"))
            .groupBy(new Fields("word!"))
            .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))
            .setCPULoad(100)
            .setMemoryLoad(2048);

----------------------------------------

TITLE: Configuring Pacemaker State Storage in Storm
DESCRIPTION: Sets the cluster state store to use Pacemaker for handling worker heartbeats.

LANGUAGE: yaml
CODE:
storm.cluster.state.store: "org.apache.storm.cluster.PaceMakerStateStorageFactory"

----------------------------------------

TITLE: Creating a Stream with Regular Storm Spout in Trident Topology
DESCRIPTION: This snippet demonstrates how to create a stream using a regular Storm IRichSpout in a Trident topology. It shows the initialization of a TridentTopology object and the creation of a new stream with a unique identifier and a custom spout.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
topology.newStream("myspoutid", new MyRichSpout());

----------------------------------------

TITLE: Defining a Prepared Bolt in Clojure
DESCRIPTION: Example of defining a prepared bolt that maintains state for word counting using the defbolt macro.

LANGUAGE: clojure
CODE:
(defbolt word-count ["word" "count"] {:prepare true}
  [conf context collector]
  (let [counts (atom {})]
    (bolt
     (execute [tuple]
       (let [word (.getString tuple 0)]
         (swap! counts (partial merge-with +) {word 1})
         (emit-bolt! collector [word (@counts word)] :anchor tuple)
         (ack! collector tuple)
         )))))

----------------------------------------

TITLE: Registering Task Hook in Spout or Bolt
DESCRIPTION: This snippet shows how to register a custom task hook in the open method of a spout or prepare method of a bolt using the TopologyContext.

LANGUAGE: Java
CODE:
TopologyContext#addTaskHook

----------------------------------------

TITLE: Task Message Listening in Apache Storm (Clojure)
DESCRIPTION: This code shows how tasks listen on an in-memory ZeroMQ port for messages from the virtual port in Storm.

LANGUAGE: Clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L201

----------------------------------------

TITLE: Configuring SimpleJdbcMapper with Explicit Column Schema in Java
DESCRIPTION: This code snippet shows how to configure SimpleJdbcMapper with an explicit column schema, which is necessary when using a custom insert query or when the tuple only contains a subset of the table's columns.

LANGUAGE: java
CODE:
List<Column> columnSchema = Lists.newArrayList(
    new Column("user_id", java.sql.Types.INTEGER),
    new Column("user_name", java.sql.Types.VARCHAR),
    new Column("dept_name", java.sql.Types.VARCHAR));
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(columnSchema);

----------------------------------------

TITLE: Configuring EsConfig for Storm-Elasticsearch Integration in Java
DESCRIPTION: These snippets show two ways to initialize EsConfig: one with basic cluster configuration, and another with additional parameters. EsConfig is used to configure the Elasticsearch connection for Storm components.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});

LANGUAGE: java
CODE:
Map<String, String> additionalParameters = new HashMap<>();
additionalParameters.put("client.transport.sniff", "true");
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"}, additionalParameters);

----------------------------------------

TITLE: Defining Markdown Layout for Storm JSON Documentation
DESCRIPTION: This snippet sets the layout for the documentation page using YAML front matter in Markdown. It specifies that the 'documentation' layout should be used for this page.

LANGUAGE: markdown
CODE:
---
layout: documentation
---

----------------------------------------

TITLE: Submitting Topology to Nimbus
DESCRIPTION: StormSubmitter calls the submitTopology method on the Nimbus Thrift interface to submit the topology. The topology config is serialized using JSON for language-agnostic DSL support.

LANGUAGE: java
CODE:
nimbus.submitTopology(topologyName, uploadedJarLocation, JSONValue.toJSONString(conf), topology)

----------------------------------------

TITLE: Configuring Elasticsearch Trident State
DESCRIPTION: Establishes a Trident state for Elasticsearch operations. Uses EsStateFactory to create persistent state with specified configuration and mapping.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();

StateFactory factory = new EsStateFactory(esConfig, tupleMapper);
TridentState state = stream.partitionPersist(factory, esFields, new EsUpdater(), new Fields());

----------------------------------------

TITLE: User Resources Access Methods
DESCRIPTION: Methods for reading and writing worker-level shared resources in Storm components.

LANGUAGE: java
CODE:
WorkerUserContext#setResource(String, Object)
WorkerTopologyContext#getResouce(String)
TopologyContext#getResource(String)

----------------------------------------

TITLE: Implementing Batch Count Bolt
DESCRIPTION: BatchBolt implementation that counts tuples within a transaction batch and emits the partial count.

LANGUAGE: java
CODE:
public static class BatchCount extends BaseBatchBolt {
    Object _id;
    BatchOutputCollector _collector;

    int _count = 0;

    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
        _collector = collector;
        _id = id;
    }

    @Override
    public void execute(Tuple tuple) {
        _count++;
    }

    @Override
    public void finishBatch() {
        _collector.emit(new Values(_id, _count));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "count"));
    }
}

----------------------------------------

TITLE: Storm Shell Command Usage Example
DESCRIPTION: Example showing how to use the 'storm shell' command to package and submit a topology using Python resources.

LANGUAGE: bash
CODE:
storm shell resources/ python topology.py arg1 arg2

----------------------------------------

TITLE: Retrieving Cluster Summary Sample Response
DESCRIPTION: Example JSON response showing overview metrics of the Storm cluster like number of supervisors, slots and resource utilization

LANGUAGE: json
CODE:
{
    "stormVersion": "0.9.2-incubating-SNAPSHOT",
    "supervisors": 1,
    "slotsTotal": 4,
    "slotsUsed": 3,
    "slotsFree": 1,
    "executorsTotal": 28,
    "tasksTotal": 28
}

----------------------------------------

TITLE: Running Storm Topology with JAR File
DESCRIPTION: Executes a Storm topology by running the main method of a specified class, using the provided JAR file and optional dependencies.

LANGUAGE: bash
CODE:
storm jar topology-jar-path class ... [--jars "jar1,jar2"] [--artifacts "artifact1,artifact2"] [--artifactRepositories "repo1^url1,repo2^url2"]

----------------------------------------

TITLE: Configuring UI/Logviewer Filter in YAML
DESCRIPTION: Example YAML configuration for setting up authentication filters for the Storm UI and logviewer

LANGUAGE: yaml
CODE:
ui.filter: "filter.class"
ui.filter.params: "param1":"value1"
logviewer.filter: "filter.class"
logviewer.filter.params: "param1":"value1"

----------------------------------------

TITLE: Implementing Trident Filter
DESCRIPTION: Example of implementing a Trident BaseFilter that filters tuples based on field values

LANGUAGE: java
CODE:
public class MyFilter extends BaseFilter {
    public boolean isKeep(TridentTuple tuple) {
        return tuple.getInteger(0) == 1 && tuple.getInteger(1) == 2;
    }
}

----------------------------------------

TITLE: Defining HBaseMapper Interface in Java
DESCRIPTION: Core interface definition for mapping Storm tuples to HBase operations, including methods for generating row keys and column lists.

LANGUAGE: java
CODE:
public interface HBaseMapper extends Serializable {
    byte[] rowKey(Tuple tuple);

    ColumnList columns(Tuple tuple);
}

----------------------------------------

TITLE: Creating a Bucketed Hive Table for Streaming (SQL)
DESCRIPTION: SQL command to create a partitioned Hive table with ORC format, suitable for use with the Hive Streaming API.

LANGUAGE: sql
CODE:
create table test_table ( id INT, name STRING, phone STRING, street STRING) partitioned by (city STRING, state STRING) stored as orc tblproperties ("orc.compress"="NONE");

----------------------------------------

TITLE: Configuring Supervisor Slots Ports
DESCRIPTION: Configuration for defining the ports available for worker processes on supervisor nodes

LANGUAGE: yaml
CODE:
supervisor.slots.ports:
    - 6700
    - 6701
    - 6702
    - 6703

----------------------------------------

TITLE: Registering User-Defined Counter in Storm Topology
DESCRIPTION: Example of registering a custom counter metric in a Storm topology using the TopologyContext. The metric name will be automatically expanded with additional information for unique identification.

LANGUAGE: java
CODE:
Counter myCounter = topologyContext.registerCounter("myCounter");

----------------------------------------

TITLE: Packaging Flux with Maven
DESCRIPTION: XML configuration for including Flux as a Maven dependency and creating a fat JAR with all dependencies using the Maven shade plugin.

LANGUAGE: xml
CODE:
<dependencies>
    <!-- Flux include -->
    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>flux-core</artifactId>
        <version>${storm.version}</version>
    </dependency>
    <!-- Flux Wrappers include -->
    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>flux-wrappers</artifactId>
        <version>${storm.version}</version>
    </dependency>

    <!-- add user dependencies here... -->

</dependencies>
<!-- create a fat jar that includes all dependencies -->
<build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-shade-plugin</artifactId>
            <version>1.4</version>
            <configuration>
                <createDependencyReducedPom>true</createDependencyReducedPom>
            </configuration>
            <executions>
                <execution>
                    <phase>package</phase>
                    <goals>
                        <goal>shade</goal>
                    </goals>
                    <configuration>
                        <transformers>
                            <transformer
                                    implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                            <transformer
                                    implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                <mainClass>org.apache.storm.flux.Flux</mainClass>
                            </transformer>
                        </transformers>
                    </configuration>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>

----------------------------------------

TITLE: Packaging Flux with Maven
DESCRIPTION: XML configuration for including Flux as a Maven dependency and creating a fat JAR with all dependencies using the Maven shade plugin.

LANGUAGE: xml
CODE:
<dependencies>
    <!-- Flux include -->
    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>flux-core</artifactId>
        <version>${storm.version}</version>
    </dependency>
    <!-- Flux Wrappers include -->
    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>flux-wrappers</artifactId>
        <version>${storm.version}</version>
    </dependency>

    <!-- add user dependencies here... -->

</dependencies>
<!-- create a fat jar that includes all dependencies -->
<build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-shade-plugin</artifactId>
            <version>1.4</version>
            <configuration>
                <createDependencyReducedPom>true</createDependencyReducedPom>
            </configuration>
            <executions>
                <execution>
                    <phase>package</phase>
                    <goals>
                        <goal>shade</goal>
                    </goals>
                    <configuration>
                        <transformers>
                            <transformer
                                    implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                            <transformer
                                    implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                <mainClass>org.apache.storm.flux.Flux</mainClass>
                            </transformer>
                        </transformers>
                    </configuration>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>

----------------------------------------

TITLE: Building Storm-EventHubs Integration
DESCRIPTION: Maven command to build the project package

LANGUAGE: bash
CODE:
mvn clean package

----------------------------------------

TITLE: Implementing MqttTupleMapper Interface in Java
DESCRIPTION: Definition of the MqttTupleMapper interface used to convert Storm tuples to MQTT messages for publishing.

LANGUAGE: java
CODE:
public interface MqttTupleMapper extends Serializable{

    MqttMessage toMessage(ITuple tuple);

}

----------------------------------------

TITLE: Defining a Parameterized Spout in Clojure
DESCRIPTION: Example of defining an unprepared, parameterized spout that emits random sentences using the defspout macro.

LANGUAGE: clojure
CODE:
(defspout sentence-spout-parameterized ["word"] {:params [sentences] :prepare false}
  [collector]
  (Thread/sleep 500)
  (emit-spout! collector [(rand-nth sentences)]))

----------------------------------------

TITLE: Pako v1.0 License Text
DESCRIPTION: Combined MIT and zlib license terms for Pako software, granting permissions for use, modification, and distribution while specifying conditions and disclaimers. Includes copyright notices for original authors and contributors.

LANGUAGE: text
CODE:
Copyright (C) 2014-2017 by Vitaly Puzrin and Andrei Tuputcyn

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
(C) 1995-2013 Jean-loup Gailly and Mark Adler
(C) 2014-2017 Vitaly Puzrin and Andrey Tupitsin

This software is provided 'as-is', without any express or implied
warranty. In no event will the authors be held liable for any damages
arising from the use of this software.

Permission is granted to anyone to use this software for any purpose,
including commercial applications, and to alter it and redistribute it
freely, subject to the following restrictions:

1. The origin of this software must not be misrepresented; you must not
claim that you wrote the original software. If you use this software
in a product, an acknowledgment in the product documentation would be
appreciated but is not required.
2. Altered source versions must be plainly marked as such, and must not be
 misrepresented as being the original software.
3. This notice may not be removed or altered from any source distribution.

----------------------------------------

TITLE: Deploying Storm Topology with Maven Shade Plugin
DESCRIPTION: Maven configuration for creating a fat JAR containing Flux dependencies and user code required for deploying Storm topologies

LANGUAGE: xml
CODE:
<!-- include Flux and user dependencies in the shaded jar -->
<dependencies>
    <!-- Flux include -->
    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>flux-core</artifactId>
        <version>${storm.version}</version>
    </dependency>
    <!-- Flux Wrappers include -->
    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>flux-wrappers</artifactId>
        <version>${storm.version}</version>
    </dependency>
    <!-- add user dependencies here... -->
</dependencies>
<!-- create a fat jar that includes all dependencies -->
<build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-shade-plugin</artifactId>
            <version>1.4</version>
            <configuration>
                <createDependencyReducedPom>true</createDependencyReducedPom>
            </configuration>
            <executions>
                <execution>
                    <phase>package</phase>
                    <goals>
                        <goal>shade</goal>
                    </goals>
                    <configuration>
                        <transformers>
                            <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                            <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                <mainClass>org.apache.storm.flux.Flux</mainClass>
                            </transformer>
                        </transformers>
                    </configuration>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>

----------------------------------------

TITLE: Setting Node Resource Availability
DESCRIPTION: YAML configuration to specify available CPU and memory resources on a node

LANGUAGE: yaml
CODE:
supervisor.memory.capacity.mb: 20480.0
supervisor.cpu.capacity: 100.0

----------------------------------------

TITLE: Worker Launcher Configuration
DESCRIPTION: Example configuration for the worker-launcher executable, which is used to launch Docker containers and run Docker commands. It specifies group settings and user ID restrictions.

LANGUAGE: bash
CODE:
storm.worker-launcher.group=$(worker_launcher_group)
min.user.id=$(min_user_id)
worker.profiler.script.path=$(profiler_script_path)

----------------------------------------

TITLE: Sliding Window Bolt Implementation Example
DESCRIPTION: Example implementation of a sliding window bolt including topology setup and window configuration. Shows how to process windowed tuples and emit results.

LANGUAGE: java
CODE:
public class SlidingWindowBolt extends BaseWindowedBolt {
	private OutputCollector collector;
	
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
    	this.collector = collector;
    }
	
    @Override
    public void execute(TupleWindow inputWindow) {
	  for(Tuple tuple: inputWindow.get()) {
	    // do the windowing computation
		...
	  }
	  // emit the results
	  collector.emit(new Values(computedValue));
    }
}

public static void main(String[] args) {
    TopologyBuilder builder = new TopologyBuilder();
     builder.setSpout("spout", new RandomSentenceSpout(), 1);
     builder.setBolt("slidingwindowbolt", 
                     new SlidingWindowBolt().withWindow(new Count(30), new Count(10)),
                     1).shuffleGrouping("spout");
    Config conf = new Config();
    conf.setDebug(true);
    conf.setNumWorkers(1);

    StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());
	
}

----------------------------------------

TITLE: Markdown Link Reference for Storm JSON Library
DESCRIPTION: A markdown link reference pointing to the storm-json GitHub repository which provides simple JSON serialization functionality for Storm applications.

LANGUAGE: markdown
CODE:
* [storm-json](https://github.com/rapportive-oss/storm-json): Simple JSON serializer for Storm

----------------------------------------

TITLE: Configuring HiveOptions
DESCRIPTION: Example of configuring HiveOptions with transaction batch size and timeout settings.

LANGUAGE: java
CODE:
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
                                .withTxnsPerBatch(10)
                				.withBatchSize(1000)
                	     		.withIdleTimeout(10)

----------------------------------------

TITLE: Implementing a Tuple Counting Bolt in Apache Storm
DESCRIPTION: Example of a bolt that counts the number of tuples received using a registered Counter metric in Apache Storm.

LANGUAGE: java
CODE:
public class TupleCountingBolt extends BaseRichBolt {
    private Counter tupleCounter;
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        this.tupleCounter = context.registerCounter("tupleCount");
    }

    @Override
    public void execute(Tuple input) {
        this.tupleCounter.inc();
    }
}

----------------------------------------

TITLE: Starting Pacemaker Daemon
DESCRIPTION: Command to start the Pacemaker daemon in a Storm cluster.

LANGUAGE: bash
CODE:
$ storm pacemaker

----------------------------------------

TITLE: Creating a Blob in Java
DESCRIPTION: Java code showing how to create a blob in the distributed cache using the ClientBlobStore API.

LANGUAGE: java
CODE:
AtomicOutputStream blobStream = clientBlobStore.createBlob("some_key", settableBlobMeta);
blobStream.write("Some String or input data".getBytes());
blobStream.close();

----------------------------------------

TITLE: Redirecting to Storm Contribution Guidelines using HTML meta tags
DESCRIPTION: This HTML snippet uses a meta refresh tag to automatically redirect the user to the current Storm contribution guidelines page. It also includes a canonical link for search engines.

LANGUAGE: HTML
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Contributing-to-Storm.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Contributing-to-Storm.html" />

----------------------------------------

TITLE: Implementing a Trident Map Function
DESCRIPTION: Example of implementing a Trident Map Function for one-to-one transformations.

LANGUAGE: java
CODE:
public class UpperCase extends MapFunction {
 @Override
 public Values execute(TridentTuple input) {
   return new Values(input.getString(0).toUpperCase());
 }
}

----------------------------------------

TITLE: Storm Input Declaration Example in Clojure
DESCRIPTION: Demonstrates various ways to declare inputs for Storm bolts including shuffle grouping, fields grouping, and global grouping.

LANGUAGE: clojure
CODE:
{["2" "1"] :shuffle
 "3" ["field1" "field2"]
 ["4" "2"] :global}

----------------------------------------

TITLE: Storm Local Mode Messaging
DESCRIPTION: In-memory queue implementation for local mode message passing

LANGUAGE: clojure
CODE:
messaging/local.clj

----------------------------------------

TITLE: RotatingMap Implementation for Tuple Expiration in Java
DESCRIPTION: The RotatingMap is a data structure used by the Acker to efficiently manage and expire pending tuples. It maintains multiple buckets of tuples, rotating them towards expiration on each tick. This implementation ensures O(1) access time while allowing for time-based expiration of entries.

LANGUAGE: java
CODE:
// Pseudo-code representation of RotatingMap
class RotatingMap<K, V> {
    private List<Map<K, V>> buckets;
    private ExpiredCallback<K, V> callback;

    public void put(K key, V value) {
        // Move or add to nursery bucket
    }

    public void rotate() {
        Map<K, V> expiredBucket = buckets.remove(0);
        buckets.add(new HashMap<>());
        for (Entry<K, V> entry : expiredBucket.entrySet()) {
            callback.expire(entry.getKey(), entry.getValue());
        }
    }
}

----------------------------------------

TITLE: Database Schema Setup SQL
DESCRIPTION: SQL script for creating necessary database tables and inserting initial data for testing the Storm JDBC integration.

LANGUAGE: sql
CODE:
create table if not exists user (user_id integer, user_name varchar(100), dept_name varchar(100), create_date date);
create table if not exists department (dept_id integer, dept_name varchar(100));
create table if not exists user_department (user_id integer, dept_id integer);
insert into department values (1, 'R&D');
insert into department values (2, 'Finance');
insert into department values (3, 'HR');
insert into department values (4, 'Sales');
insert into user_department values (1, 1);
insert into user_department values (2, 2);
insert into user_department values (3, 3);
insert into user_department values (4, 4);

----------------------------------------

TITLE: Configuring Storm Topology with KestrelSpout in Java
DESCRIPTION: Code snippet demonstrating how to set up a Storm topology using KestrelSpout to read sentences from a Kestrel queue, split them into words, and count word occurrences.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("sentences", new KestrelSpout("localhost",22133,"sentence_queue",new StringScheme()));
builder.setBolt("split", new SplitSentence(), 10)
	        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 20)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Displaying Pako Combined MIT and zlib Licenses
DESCRIPTION: Combined license text for Pako software showing both MIT license terms and zlib license terms. Includes copyright notices for Vitaly Puzrin, Andrei Tuputcyn, Jean-loup Gailly and Mark Adler.

LANGUAGE: plaintext
CODE:
Copyright (C) 2014-2017 by Vitaly Puzrin and Andrei Tuputcyn

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
(C) 1995-2013 Jean-loup Gailly and Mark Adler
(C) 2014-2017 Vitaly Puzrin and Andrey Tupitsin

This software is provided 'as-is', without any express or implied
warranty. In no event will the authors be held liable for any damages
arising from the use of this software.

Permission is granted to anyone to use this software for any purpose,
including commercial applications, and to alter it and redistribute it
freely, subject to the following restrictions:

1. The origin of this software must not be misrepresented; you must not
claim that you wrote the original software. If you use this software
in a product, an acknowledgment in the product documentation would be
appreciated but is not required.
2. Altered source versions must be plainly marked as such, and must not be
 misrepresented as being the original software.
3. This notice may not be removed or altered from any source distribution.

----------------------------------------

TITLE: Implementing Code Distribution Interface in Java
DESCRIPTION: Defines the ICodeDistributor interface for managing topology code distribution across Nimbus instances. Includes methods for uploading, downloading, checking replication status, and cleaning up topology code.

LANGUAGE: java
CODE:
public interface ICodeDistributor {
    void prepare(Map conf);
    File upload(Path dirPath, String topologyId);
    List<File> download(Path destDirPath, String topologyid, File metafile);
    int getReplicationCount(String topologyId);
    void cleanup(String topologyid);
    void close(Map conf);
}

----------------------------------------

TITLE: Configuring Transactional Topology
DESCRIPTION: Example code showing how to build a transactional topology using TransactionalTopologyBuilder. Sets up spouts and bolts for a global counting topology.

LANGUAGE: java
CODE:
MemoryTransactionalSpout spout = new MemoryTransactionalSpout(DATA, new Fields("word"), PARTITION_TAKE_PER_BATCH);
TransactionalTopologyBuilder builder = new TransactionalTopologyBuilder("global-count", "spout", spout, 3);
builder.setBolt("partial-count", new BatchCount(), 5)
        .shuffleGrouping("spout");
builder.setBolt("sum", new UpdateGlobalCount())
        .globalGrouping("partial-count");

----------------------------------------

TITLE: Configuring Transactional Topology in Storm
DESCRIPTION: Example code showing how to set up a transactional topology using TransactionalTopologyBuilder with spouts and bolts for counting tuples globally.

LANGUAGE: java
CODE:
MemoryTransactionalSpout spout = new MemoryTransactionalSpout(DATA, new Fields("word"), PARTITION_TAKE_PER_BATCH);
TransactionalTopologyBuilder builder = new TransactionalTopologyBuilder("global-count", "spout", spout, 3);
builder.setBolt("partial-count", new BatchCount(), 5)
        .shuffleGrouping("spout");
builder.setBolt("sum", new UpdateGlobalCount())
        .globalGrouping("partial-count");

----------------------------------------

TITLE: Creating External Table for Kafka in Storm SQL
DESCRIPTION: SQL statement to create an external table representing a Kafka spout and sink. This example defines a table named FOO with an integer ID as the primary key, connecting to a Kafka topic at localhost:9092.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Initializing ClientBlobStore in Java
DESCRIPTION: Java code to initialize a ClientBlobStore object for interacting with the Storm distributed cache.

LANGUAGE: java
CODE:
Config theconf = new Config();
theconf.putAll(Utils.readStormConfig());
ClientBlobStore clientBlobStore = Utils.getClientBlobStore(theconf);

----------------------------------------

TITLE: Configuring FileConfigLoader in YAML
DESCRIPTION: Example YAML configuration for using FileConfigLoader, specifying the file URI.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "file:///path/to/my/config.yaml"

----------------------------------------

TITLE: Configuring Digest Authentication for Pacemaker
DESCRIPTION: JAAS configuration for digest authentication in Pacemaker, specifying username and password.

LANGUAGE: java
CODE:
PacemakerDigest {
    username="some username"
    password="some password";
};

----------------------------------------

TITLE: Building Storm JMS Example
DESCRIPTION: Maven commands to build the storm-jms example project from source.

LANGUAGE: bash
CODE:
$ cd storm-jms
$ mvn clean install

----------------------------------------

TITLE: Executing DRPC Query in Trident
DESCRIPTION: Shows how to use a DRPCClient to execute a distributed query on a Trident topology, demonstrating the RPC-like interface for querying.

LANGUAGE: java
CODE:
DRPCClient client = new DRPCClient("drpc.server.location", 3772);
System.out.println(client.execute("words", "cat dog the man");
// prints the JSON-encoded result, e.g.: "[[5078]]"

----------------------------------------

TITLE: Creating EsConfig for Storm-Elasticsearch Integration in Java
DESCRIPTION: These snippets show two ways to create an EsConfig object for configuring the Elasticsearch cluster connection. The first method uses basic settings, while the second allows for additional parameters to be specified.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});

LANGUAGE: java
CODE:
Map<String, String> additionalParameters = new HashMap<>();
additionalParameters.put("client.transport.sniff", "true");
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"}, additionalParameters);

----------------------------------------

TITLE: Storm SQL Slow Log Filtering with UDF
DESCRIPTION: SQL script demonstrating UDF usage to filter slow logs and add timestamp conversion

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://apache-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
CREATE EXTERNAL TABLE APACHE_SLOW_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_RECEIVED_TIMESTAMP BIGINT, TIME_ELAPSED_MS INT) LOCATION 'kafka://apache-slow-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
CREATE FUNCTION GET_TIME AS 'org.apache.storm.sql.runtime.functions.scalar.datetime.GetTime2'
INSERT INTO APACHE_SLOW_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, GET_TIME(TIME_RECEIVED_UTC_ISOFORMAT, 'yyyy-MM-dd''T''HH:mm:ssZZ') AS TIME_RECEIVED_TIMESTAMP, TIME_US / 1000 AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (TIME_US / 1000) >= 100

----------------------------------------

TITLE: Storm JMX Configuration in YAML
DESCRIPTION: YAML configuration for enabling JMX metrics collection in various Storm daemons including Nimbus, UI, LogViewer, DRPC, and Supervisor.

LANGUAGE: yaml
CODE:
nimbus.childopts: "-Xmx1024m -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=3333  -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false"

ui.childopts: "-Xmx768m -Dcom.sun.management.jmxremote.port=3334 -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false"

logviewer.childopts: "-Xmx128m -Dcom.sun.management.jmxremote.port=3335 -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false"

drpc.childopts: "-Xmx768m -Dcom.sun.management.jmxremote.port=3336 -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false"

supervisor.childopts: "-Xmx256m -Dcom.sun.management.jmxremote.port=3337 -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false"

----------------------------------------

TITLE: Storm Topology Submission Method Definition
DESCRIPTION: Thrift method definition for submitting a topology to Storm, showing required parameters and possible exceptions.

LANGUAGE: thrift
CODE:
void submitTopology(1: string name, 2: string uploadedJarLocation, 3: string jsonConf, 4: StormTopology topology)
    throws (1: AlreadyAliveException e, 2: InvalidTopologyException ite);

----------------------------------------

TITLE: Constructing HBaseBolt in Java
DESCRIPTION: Example of how to construct an HBaseBolt, which is used to write data to HBase from a Storm topology.

LANGUAGE: java
CODE:
HBaseBolt hbase = new HBaseBolt("WordCount", mapper);

----------------------------------------

TITLE: Implementing a FlatMapFunction for String Splitting
DESCRIPTION: Shows how to create a FlatMapFunction that splits input sentences into individual words.

LANGUAGE: java
CODE:
public class Split extends FlatMapFunction {
  @Override
  public Iterable<Values> execute(TridentTuple input) {
    List<Values> valuesList = new ArrayList<>();
    for (String word : input.getString(0).split(" ")) {
      valuesList.add(new Values(word));
    }
    return valuesList;
  }
}

----------------------------------------

TITLE: Defining Thrift Structures for Cluster Information
DESCRIPTION: Thrift structure definitions for ClusterSummary and NimbusSummary, which are used to return information about the Storm cluster, including details about nimbus hosts.

LANGUAGE: thrift
CODE:
struct ClusterSummary {
  1: required list<SupervisorSummary> supervisors;
  3: required list<TopologySummary> topologies;
  4: required list<NimbusSummary> nimbuses;
}

struct NimbusSummary {
  1: required string host;
  2: required i32 port;
  3: required i32 uptime_secs;
  4: required bool isLeader;
  5: required string version;
}

----------------------------------------

TITLE: Creating External Table for Kafka Data Source
DESCRIPTION: Example of creating an external table for a Kafka data source with a Kafka spout and sink.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://localhost:2181/brokers?topic=test' TBLPROPERTIES '{"producer":{"bootstrap.servers":"localhost:9092","acks":"1","key.serializer":"org.apache.org.apache.storm.kafka.IntSerializer","value.serializer":"org.apache.org.apache.storm.kafka.ByteBufferSerializer"}}'

----------------------------------------

TITLE: Creating a LocalCluster in Java
DESCRIPTION: This snippet demonstrates how to create an in-process Storm cluster using the LocalCluster class. It uses a try-with-resources block to ensure proper cleanup.

LANGUAGE: java
CODE:
import org.apache.storm.LocalCluster;

...

try (LocalCluster cluster = new LocalCluster()) {
    //Interact with the cluster...
}

----------------------------------------

TITLE: Configuring Automatic Task Hooks in Storm Configuration
DESCRIPTION: This snippet shows how to configure automatic task hooks using the Storm configuration. The 'topology.auto.task.hooks' config is used to specify hooks that will be automatically registered in every spout or bolt.

LANGUAGE: java
CODE:
"topology.auto.task.hooks"

----------------------------------------

TITLE: Creating RedisLookupBolt Instance
DESCRIPTION: Java code snippet showing how to create a RedisLookupBolt instance using JedisPoolConfig and RedisLookupMapper.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisLookupMapper lookupMapper = new WordCountRedisLookupMapper();
RedisLookupBolt lookupBolt = new RedisLookupBolt(poolConfig, lookupMapper);

----------------------------------------

TITLE: Language Syntax Definition Format
DESCRIPTION: Standard format used to define syntax highlighting classes for each programming language. Each entry includes the language name and its corresponding class identifiers.

LANGUAGE: text
CODE:
language_name ("identifier"):
  class_name     description

----------------------------------------

TITLE: Subscribing to Streams in a Bolt using InputDeclarer
DESCRIPTION: This snippet shows how to subscribe to specific streams of another component when declaring a bolt's input streams. It includes an example of using syntactic sugar for subscribing to the default stream.

LANGUAGE: java
CODE:
declarer.shuffleGrouping("1", "myStream");

// Subscribing to the default stream
declarer.shuffleGrouping("1");
// Equivalent to:
declarer.shuffleGrouping("1", DEFAULT_STREAM_ID);

----------------------------------------

TITLE: Implementing HTML Meta Refresh and Canonical Link for Apache Storm Documentation
DESCRIPTION: This HTML snippet sets up an automatic redirect to the current Apache Storm Distributed RPC documentation page and specifies the canonical URL for search engines.

LANGUAGE: HTML
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Distributed-RPC.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Distributed-RPC.html" />

----------------------------------------

TITLE: Configuring Resource Consumption for Trident Topology in Java
DESCRIPTION: This snippet demonstrates how to set resource defaults and specify CPU and memory consumption for various operations in a Trident topology. It shows the usage of setResourceDefaults, setCPULoad, setMemoryLoad, and parallelismHint methods on Trident streams and operations.

LANGUAGE: java
CODE:
    TridentTopology topo = new TridentTopology();
    topo.setResourceDefaults(new DefaultResourceDeclarer();
                                                          .setMemoryLoad(128)
                                                          .setCPULoad(20));
    TridentState wordCounts =
        topology
            .newStream("words", feeder)
            .parallelismHint(5)
            .setCPULoad(20)
            .setMemoryLoad(512,256)
            .each( new Fields("sentence"),  new Split(), new Fields("word"))
            .setCPULoad(10)
            .setMemoryLoad(512)
            .each(new Fields("word"), new BangAdder(), new Fields("word!"))
            .parallelismHint(10)
            .setCPULoad(50)
            .setMemoryLoad(1024)
            .each(new Fields("word!"), new QMarkAdder(), new Fields("word!?"))
            .groupBy(new Fields("word!"))
            .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))
            .setCPULoad(100)
            .setMemoryLoad(2048);

----------------------------------------

TITLE: Storm Spout Interface Definition
DESCRIPTION: Core interface definition for Storm spouts showing the essential methods required for reliable message processing.

LANGUAGE: java
CODE:
public interface ISpout extends Serializable {
    void open(Map conf, TopologyContext context, SpoutOutputCollector collector);
    void close();
    void nextTuple();
    void ack(Object msgId);
    void fail(Object msgId);
}

----------------------------------------

TITLE: HTML Meta Redirect to Storm Documentation
DESCRIPTION: HTML meta refresh tag and canonical link element that redirects visitors to the official Apache Storm documentation page about support for non-Java languages.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Support-for-non-java-languages.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Support-for-non-java-languages.html" />

----------------------------------------

TITLE: GetTime2 UDF Implementation
DESCRIPTION: Java implementation of GetTime2 UDF for timestamp conversion

LANGUAGE: java
CODE:
package org.apache.storm.sql.runtime.functions.scalar.datetime;

import org.joda.time.format.DateTimeFormat;
import org.joda.time.format.DateTimeFormatter;

public class GetTime2 {
    public static Long evaluate(String dateString, String dateFormat) {
        try {
            DateTimeFormatter df = DateTimeFormat.forPattern(dateFormat).withZoneUTC();
            return df.parseDateTime(dateString).getMillis();
        } catch (Exception ex) {
            throw new RuntimeException(ex);
        }
    }
}

----------------------------------------

TITLE: Draining Transfer Queue in Apache Storm Worker
DESCRIPTION: The worker uses a single thread to drain the transfer queue and send messages to other workers.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L185

----------------------------------------

TITLE: Configuring Trident Topology with Fields Mapper
DESCRIPTION: Demonstrates setting up a Trident topology using SolrStateFactory with a Fields mapper for the 'gettingstarted' collection using REST-based schema builder.

LANGUAGE: java
CODE:
    new SolrStateFactory(solrConfig, solrMapper);
    
    // zkHostString for Solr 'gettingstarted' example
    SolrConfig solrConfig = new SolrConfig("127.0.0.1:9983");
    
    /* Solr Fields Mapper used to generate 'SolrRequest' requests to update the "gettingstarted" Solr collection. The Solr index is updated using the field values of the tuple fields that match static or dynamic fields declared in the schema object build using schemaBuilder */ 
    SolrMapper solrMapper = new SolrFieldsMapper.Builder(schemaBuilder, "gettingstarted").build();

    // builds the Schema object from the JSON representation of the schema as returned by the URL http://localhost:8983/solr/gettingstarted/schema/ 
    SchemaBuilder schemaBuilder = new RestJsonSchemaBuilder("localhost", "8983", "gettingstarted")

----------------------------------------

TITLE: Running the Storm JMS Example Topology Locally
DESCRIPTION: Maven command to execute the example topology locally. This runs the topology and connects it to the ActiveMQ instance.

LANGUAGE: bash
CODE:
$ mvn exec:java

----------------------------------------

TITLE: Configuring Storm Remote Cluster Connection in YAML
DESCRIPTION: YAML configuration for specifying the Nimbus seed nodes in the ~/.storm/storm.yaml file to enable remote cluster connectivity. This configuration is required for submitting and managing topologies on a remote Storm cluster.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["123.45.678.890"]

----------------------------------------

TITLE: Configuring HDFS SequenceFileBolt in Java
DESCRIPTION: Example of configuring a SequenceFileBolt to write Storm data to HDFS sequence files with specific formats and policies.

LANGUAGE: java
CODE:
// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withExtension(".seq")
        .withPath("/data/");

// create sequence format instance.
DefaultSequenceFormat format = new DefaultSequenceFormat("timestamp", "sentence");

SequenceFileBolt bolt = new SequenceFileBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withSequenceFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy)
        .withCompressionType(SequenceFile.CompressionType.RECORD)
        .withCompressionCodec("deflate");

----------------------------------------

TITLE: Multi-Anchoring Tuples in Storm
DESCRIPTION: This snippet shows how to anchor an output tuple to multiple input tuples in Storm. This is useful for streaming joins or aggregations where a failure in the output should trigger replay of multiple input tuples.

LANGUAGE: java
CODE:
List<Tuple> anchors = new ArrayList<Tuple>();
anchors.add(tuple1);
anchors.add(tuple2);
_collector.emit(anchors, new Values(1, 2, 3));

----------------------------------------

TITLE: Complex Reach Topology Implementation
DESCRIPTION: Implementation of a complex DRPC topology for calculating Twitter URL reach.

LANGUAGE: java
CODE:
LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("reach");
builder.addBolt(new GetTweeters(), 3);
builder.addBolt(new GetFollowers(), 12)
        .shuffleGrouping();
builder.addBolt(new PartialUniquer(), 6)
        .fieldsGrouping(new Fields("id", "follower"));
builder.addBolt(new CountAggregator(), 2)
        .fieldsGrouping(new Fields("id"));

----------------------------------------

TITLE: Defining JDBC Connection Provider Interface in Java
DESCRIPTION: Interface definition for database connection pooling providers with methods for preparation, connection retrieval, and cleanup.

LANGUAGE: java
CODE:
public interface ConnectionProvider extends Serializable {
    /**
     * method must be idempotent.
     */
    void prepare();

    /**
     *
     * @return a DB connection over which the queries can be executed.
     */
    Connection getConnection();

    /**
     * called once when the system is shutting down, should be idempotent.
     */
    void cleanup();
}

----------------------------------------

TITLE: Configuring Nimbus Seeds in Storm YAML
DESCRIPTION: This YAML snippet shows how to set the nimbus.seeds configuration, which specifies the machines that are candidates for the master node.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["111.222.333.44"]

----------------------------------------

TITLE: Configuring Storm MetricStore with YAML
DESCRIPTION: Configuration options for Storm's metric store including class implementations, RocksDB location, retention settings, and cache capacity parameters.

LANGUAGE: yaml
CODE:
storm.metricstore.class: "org.apache.storm.metricstore.rocksdb.RocksDbStore"
storm.metricprocessor.class: "org.apache.storm.metricstore.NimbusMetricProcessor"
storm.metricstore.rocksdb.location: "storm_rocks"
storm.metricstore.rocksdb.create_if_missing: true
storm.metricstore.rocksdb.metadata_string_cache_capacity: 4000
storm.metricstore.rocksdb.retention_hours: 240

----------------------------------------

TITLE: Storm Jar Command Implementation
DESCRIPTION: Shows how the storm jar command sets up environment variables for StormSubmitter.

LANGUAGE: bash
CODE:
https://github.com/apache/storm/blob/0.7.1/bin/storm#L101

----------------------------------------

TITLE: Jekyll Page Layout with Liquid Template
DESCRIPTION: Basic Jekyll page layout combining YAML front matter and Liquid templating to display a logo contest entry page with an image.

LANGUAGE: markdown
CODE:
---
layout: post
title: Logo Entry No. 9 - Jennifer Lee
---

{{ page.title }}
================

![Storm Brand](/images/logocontest/jlee1/storm_logo.jpg)

----------------------------------------

TITLE: Maven Dependencies for Flux
DESCRIPTION: Maven XML configuration to include Flux dependencies in a Storm project

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>flux-core</artifactId>
    <version>${storm.version}</version>
</dependency>

----------------------------------------

TITLE: Implementing Reach Calculation in Trident
DESCRIPTION: Creates a DRPC topology that computes the reach of a URL by analyzing tweet and follower data with parallel processing.

LANGUAGE: java
CODE:
TridentState urlToTweeters =
       topology.newStaticState(getUrlToTweetersState());
TridentState tweetersToFollowers =
       topology.newStaticState(getTweeterToFollowersState());

topology.newDRPCStream("reach")
       .stateQuery(urlToTweeters, new Fields("args"), new MapGet(), new Fields("tweeters"))
       .each(new Fields("tweeters"), new ExpandList(), new Fields("tweeter"))
       .shuffle()
       .stateQuery(tweetersToFollowers, new Fields("tweeter"), new MapGet(), new Fields("followers"))
       .parallelismHint(200)
       .each(new Fields("followers"), new ExpandList(), new Fields("follower"))
       .groupBy(new Fields("follower"))
       .aggregate(new One(), new Fields("one"))
       .parallelismHint(20)
       .aggregate(new Count(), new Fields("reach"));

----------------------------------------

TITLE: Configuring CassandraWriterBolt with Bound Statement from Storm Configuration in Java
DESCRIPTION: Creates a CassandraWriterBolt using a bound statement loaded from Storm configuration.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
     boundQuery(named("insertIntoAlbum"))
        .bind(all());

----------------------------------------

TITLE: Implementing MapFunction for Uppercase Conversion
DESCRIPTION: Demonstrates how to use MapFunction to transform stream elements, converting words to uppercase.

LANGUAGE: java
CODE:
public class UpperCase extends MapFunction {
 @Override
 public Values execute(TridentTuple input) {
   return new Values(input.getString(0).toUpperCase());
 }
}

----------------------------------------

TITLE: Storm Component Structure Definition in Thrift
DESCRIPTION: Defines the core data structures for Storm topologies using Thrift, including ComponentObject for bolt implementation and ComponentCommon for stream configurations and component properties.

LANGUAGE: thrift
CODE:
struct ComponentObject {
  1: binary serialized_java
  2: ShellComponent shell
  3: JavaObject java_object
}

struct ComponentCommon {
  1: map<string, StreamInfo> streams
  2: map<GlobalStreamId, Grouping> inputs
  3: i32 parallelism_hint
  4: map<string, string> json_conf
}

----------------------------------------

TITLE: Defining TupleToKafkaMapper Interface in Java
DESCRIPTION: Defines the TupleToKafkaMapper interface with methods for mapping a tuple to a Kafka key and message.

LANGUAGE: java
CODE:
K getKeyFromTuple(Tuple/TridentTuple tuple);
V getMessageFromTuple(Tuple/TridentTuple tuple);

----------------------------------------

TITLE: Defining TupleToKafkaMapper Interface in Java
DESCRIPTION: Defines the TupleToKafkaMapper interface with methods for mapping a tuple to a Kafka key and message.

LANGUAGE: java
CODE:
K getKeyFromTuple(Tuple/TridentTuple tuple);
V getMessageFromTuple(Tuple/TridentTuple tuple);

----------------------------------------

TITLE: Merging Streams in Trident
DESCRIPTION: Example of merging multiple Trident streams into a single stream.

LANGUAGE: java
CODE:
topology.merge(stream1, stream2, stream3);

----------------------------------------

TITLE: Storm SQL Command Syntax
DESCRIPTION: Command for compiling SQL statements into a Trident topology and submitting to Storm.

LANGUAGE: bash
CODE:
storm sql sql-file topology-name

----------------------------------------

TITLE: Storm Component Object Thrift Definition
DESCRIPTION: Thrift union definition for Storm component objects that specifies how different component types (Java and Shell) are structured in the system.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Configuring CGroup Mount Points and Permissions in YAML
DESCRIPTION: Sample cgconfig.conf configuration defining mount points for various resource controllers and permissions for the storm group. Sets up basic resource management structure for CPU, memory, and other subsystems.

LANGUAGE: yaml
CODE:
mount {
	cpuset	= /cgroup/cpuset;
	cpu	= /cgroup/storm_resources;
	cpuacct	= /cgroup/storm_resources;
	memory	= /cgroup/storm_resources;
	devices	= /cgroup/devices;
	freezer	= /cgroup/freezer;
	net_cls	= /cgroup/net_cls;
	blkio	= /cgroup/blkio;
}

group storm {
       perm {
               task {
                      uid = 500;
                      gid = 500;
               }
               admin {
                      uid = 500;
                      gid = 500;
               }
       }
       cpu {
       }
       memory {
       }
       cpuacct {
       }
}

----------------------------------------

TITLE: Setting Max Spout Pending in Storm
DESCRIPTION: Limits the number of unacknowledged tuples at the spout level, affecting throughput and latency.

LANGUAGE: java
CODE:
topology.max.spout.pending

----------------------------------------

TITLE: Building Azure Event Hubs Storm Integration
DESCRIPTION: Command to build the project using Maven, generating the necessary JAR files for deployment.

LANGUAGE: bash
CODE:
mvn clean package

----------------------------------------

TITLE: Implementing Custom Task-Level Metrics
DESCRIPTION: Example showing how to implement custom task-level metrics in a Storm bolt using CountMetric.

LANGUAGE: java
CODE:
private transient CountMetric countMetric;

@Override
public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
	// other initialization here.
	countMetric = new CountMetric();
	context.registerMetric("execute_count", countMetric, 60);
}

public void execute(Tuple input) {
	countMetric.incr();
	// handle tuple here.	
}

----------------------------------------

TITLE: Implementing Trident State for Druid Integration
DESCRIPTION: Example demonstrating how to set up Trident state integration with Druid using DruidBeamStateFactory and event mapper.

LANGUAGE: java
CODE:
    DruidBeamFactory druidBeamFactory = new SampleDruidBeamFactoryImpl(new HashMap<String, Object>());
    ITupleDruidEventMapper<Map<String, Object>> eventMapper = new TupleDruidEventMapper<>(TupleDruidEventMapper.DEFAULT_FIELD_NAME);

    final Stream stream = tridentTopology.newStream("batch-event-gen", new SimpleBatchSpout(10));

    stream.peek(new Consumer() {
        @Override
        public void accept(TridentTuple input) {
             LOG.info("########### Received tuple: [{}]", input);
         }
    }).partitionPersist(new DruidBeamStateFactory<Map<String, Object>>(druidBeamFactory, eventMapper), new Fields("event"), new DruidBeamStateUpdater());

----------------------------------------

TITLE: Configuring Remote Storm Cluster Access in YAML
DESCRIPTION: This YAML configuration specifies the Nimbus seed node IP address for connecting to a remote Storm cluster. It should be placed in the ~/.storm/storm.yaml file to enable the storm client to communicate with the cluster.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["123.45.678.890"]

----------------------------------------

TITLE: Implementing HTML Meta Refresh Redirect for Apache Storm Powered By Page
DESCRIPTION: This HTML snippet sets up an immediate page redirect to the Apache Storm Powered By page using a meta refresh directive. It also includes a canonical link for search engine optimization.

LANGUAGE: HTML
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Powered-By.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Powered-By.html" />

----------------------------------------

TITLE: HTML Redirect for Storm Topology Documentation
DESCRIPTION: HTML meta refresh redirect and canonical link to the Apache Storm topology lifecycle documentation page.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Lifecycle-of-a-topology.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Lifecycle-of-a-topology.html" />

----------------------------------------

TITLE: Starting Worker Process in Storm (Clojure)
DESCRIPTION: The mk-worker function starts a worker process. It connects to other workers, monitors topology activity, and launches tasks as threads within the worker.

LANGUAGE: clojure
CODE:
(defn mk-worker [])

----------------------------------------

TITLE: Declaring Multiple Streams in a Spout (Java)
DESCRIPTION: Example of how to declare multiple streams in a Storm spout using the OutputFieldsDeclarer and emit to specific streams using SpoutOutputCollector.

LANGUAGE: java
CODE:
declareStream("stream1", new Fields("field1", "field2"));
declareStream("stream2", new Fields("field3", "field4"));

// Emitting to a specific stream
collector.emit("stream1", new Values(value1, value2));

----------------------------------------

TITLE: Apache Log Parser Script
DESCRIPTION: Python script to parse Apache logs into JSON format with auto-incrementing IDs

LANGUAGE: python
CODE:
import sys
import apache_log_parser
import json

auto_incr_id = 1
parser_format = '%a - - %t %D "%r" %s %b "%{Referer}i" "%{User-Agent}i"'
line_parser = apache_log_parser.make_parser(parser_format)
while True:
  line = sys.stdin.readline()
  if not line:
    break
  parsed_dict = line_parser(line)
  parsed_dict['id'] = auto_incr_id
  auto_incr_id += 1

  parsed_dict = {k.upper(): v for k, v in parsed_dict.iteritems() if not k.endswith('datetimeobj')}
  print(json.dumps(parsed_dict))

----------------------------------------

TITLE: Bolt Tuple Transfer in Apache Storm (Clojure)
DESCRIPTION: This code shows how bolts use the worker-provided transfer function to send tuples in Storm.

LANGUAGE: Clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L429

----------------------------------------

TITLE: Initializing SolrStateFactory with Fields Mapper in Java for Trident
DESCRIPTION: This snippet shows how to create a SolrStateFactory with a Fields mapper for a Trident topology. It uses a RestJsonSchemaBuilder to construct the schema for the 'gettingstarted' Solr collection.

LANGUAGE: java
CODE:
new SolrStateFactory(solrConfig, solrMapper);

// zkHostString for Solr 'gettingstarted' example
SolrConfig solrConfig = new SolrConfig("127.0.0.1:9983");

/* Solr Fields Mapper used to generate 'SolrRequest' requests to update the "gettingstarted" Solr collection. The Solr index is updated using the field values of the tuple fields that match static or dynamic fields declared in the schema object build using schemaBuilder */ 
SolrMapper solrMapper = new SolrFieldsMapper.Builder(schemaBuilder, "gettingstarted").build();

// builds the Schema object from the JSON representation of the schema as returned by the URL http://localhost:8983/solr/gettingstarted/schema/ 
SchemaBuilder schemaBuilder = new RestJsonSchemaBuilder("localhost", "8983", "gettingstarted")

----------------------------------------

TITLE: Deactivating Storm Topology Spouts
DESCRIPTION: Deactivates the spouts of a specified topology.

LANGUAGE: shell
CODE:
storm deactivate topology-name

----------------------------------------

TITLE: Defining TopicSelector Interface for RocketMQ Integration in Java
DESCRIPTION: This snippet shows the TopicSelector interface, which is used to select the topic and tags for RocketMQ messages based on Storm tuples.

LANGUAGE: java
CODE:
public interface TopicSelector extends Serializable {
    String getTopic(ITuple tuple);
    String getTag(ITuple tuple);
}

----------------------------------------

TITLE: Storm Component Object Thrift Definition
DESCRIPTION: Thrift definition for the ComponentObject union that specifies how Storm components (spouts/bolts) can be implemented, including options for serialized Java, shell components, and Java objects.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Configuring Storm Bolt Parallelism in Java
DESCRIPTION: Example showing how to configure a Storm bolt with specific executor and task counts. Sets up a GreenBolt with 2 executors and 4 tasks using shuffle grouping.

LANGUAGE: java
CODE:
topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

----------------------------------------

TITLE: HTML Redirect to Storm Production Cluster Documentation
DESCRIPTION: HTML meta refresh redirect and canonical link to the current Apache Storm documentation page about running topologies on production clusters.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Running-topologies-on-a-production-cluster.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Running-topologies-on-a-production-cluster.html" />

----------------------------------------

TITLE: Diagnosing Kryo ConcurrentModificationException in Storm
DESCRIPTION: This stack trace illustrates a common error that occurs when emitting mutable objects as output tuples in Storm. The exception is thrown during serialization, indicating that an object is being modified while being serialized for network transmission.

LANGUAGE: java
CODE:
java.lang.RuntimeException: java.util.ConcurrentModificationException
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:84)
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:55)
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:56)
	at org.apache.storm.disruptor$consume_loop_STAR_$fn__1597.invoke(disruptor.clj:67)
	at org.apache.storm.util$async_loop$fn__465.invoke(util.clj:377)
	at clojure.lang.AFn.run(AFn.java:24)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.util.ConcurrentModificationException
	at java.util.LinkedHashMap$LinkedHashIterator.nextEntry(LinkedHashMap.java:390)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:409)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:408)
	at java.util.HashMap.writeObject(HashMap.java:1016)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:959)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1480)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at org.apache.storm.serialization.SerializableSerializer.write(SerializableSerializer.java:21)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:554)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:77)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:472)
	at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:27)

----------------------------------------

TITLE: Setting Up RocketMq Trident State for Storm Topology in Java
DESCRIPTION: This snippet shows how to create a RocketMQ persistent trident state for use in Storm trident topologies. It configures the state with a mapper, selector, and properties, and demonstrates how to use it in a topology.

LANGUAGE: java
CODE:
TupleToMessageMapper mapper = new FieldNameBasedTupleToMessageMapper("word", "count");
TopicSelector selector = new DefaultTopicSelector(topic);

Properties properties = new Properties();
properties.setProperty(RocketMqConfig.NAME_SERVER_ADDR, nameserverAddr);

RocketMqState.Options options = new RocketMqState.Options()
        .withMapper(mapper)
        .withSelector(selector)
        .withProperties(properties);

StateFactory factory = new RocketMqStateFactory(options);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory, fields,
        new RocketMqStateUpdater(), new Fields());

----------------------------------------

TITLE: Implementing a Basic Bolt
DESCRIPTION: Example of implementing the IBasicBolt interface for simple tuple processing and automatic acking in Storm.

LANGUAGE: java
CODE:
public class MyBasicBolt extends BaseBasicBolt {
    @Override
    public void execute(Tuple input, BasicOutputCollector collector) {
        String value = input.getString(0);
        collector.emit(new Values(value.toUpperCase()));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("uppercased"));
    }
}

----------------------------------------

TITLE: Applying a Custom Filter in Trident
DESCRIPTION: Shows how to implement a custom Filter to selectively keep tuples based on specific conditions.

LANGUAGE: java
CODE:
public class MyFilter extends BaseFilter {
    public boolean isKeep(TridentTuple tuple) {
        return tuple.getInteger(0) == 1 && tuple.getInteger(1) == 2;
    }
}

----------------------------------------

TITLE: Message Routing in Apache Storm Tasks (Clojure)
DESCRIPTION: This code implements the message routing logic in Apache Storm tasks, handling both direct and regular streams.

LANGUAGE: clojure
CODE:
(defn mk-tasks-fn [storm-conf task-data])

----------------------------------------

TITLE: Adding Items to Kestrel Queue in Java
DESCRIPTION: This method adds randomly selected sentences to a Kestrel queue using KestrelClient. It demonstrates how to queue items in Kestrel from a Java application.

LANGUAGE: java
CODE:
    private static void queueSentenceItems(KestrelClient kestrelClient, String queueName)
			throws ParseError, IOException {

		String[] sentences = new String[] {
	            "the cow jumped over the moon",
	            "an apple a day keeps the doctor away",
	            "four score and seven years ago",
	            "snow white and the seven dwarfs",
	            "i am at two with nature"};

		Random _rand = new Random();

		for(int i=1; i<=10; i++){

			String sentence = sentences[_rand.nextInt(sentences.length)];

			String val = "ID " + i + " " + sentence;

			boolean queueSucess = kestrelClient.queue(queueName, val);

			System.out.println("queueSucess=" +queueSucess+ " [" + val +"]");
		}
	}

----------------------------------------

TITLE: Implementing Double and Triple Storm Bolt
DESCRIPTION: Example bolt implementation that takes a number and emits its double and triple values as a tuple.

LANGUAGE: java
CODE:
public class DoubleAndTripleBolt extends BaseRichBolt {
    private OutputCollectorBase _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollectorBase collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple input) {
        int val = input.getInteger(0);        
        _collector.emit(input, new Values(val*2, val*3));
        _collector.ack(input);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("double", "triple"));
    }    
}

----------------------------------------

TITLE: Storm Component Structure in Thrift
DESCRIPTION: The ComponentObject struct in storm.thrift defines three possible implementations for bolts: serialized Java objects, shell components for non-JVM languages, and JavaObject structures for class instantiation.

LANGUAGE: thrift
CODE:
ComponentObject {
  1: binary serialized_java
  2: ShellComponent shell
  3: JavaObject java_object
}

----------------------------------------

TITLE: Submitting Topology to Nimbus (Java)
DESCRIPTION: StormSubmitter calls the submitTopology method on the Nimbus Thrift interface to submit the topology. The topology config is serialized using JSON for language-agnostic DSL support.

LANGUAGE: java
CODE:
StormSubmitter.submitTopology()

----------------------------------------

TITLE: Submitting Topology to Nimbus (Java)
DESCRIPTION: StormSubmitter calls the submitTopology method on the Nimbus Thrift interface to submit the topology. The topology config is serialized using JSON for language-agnostic DSL support.

LANGUAGE: java
CODE:
StormSubmitter.submitTopology()

----------------------------------------

TITLE: Initializing a Fixed Batch Spout in Java for Trident
DESCRIPTION: Creates a FixedBatchSpout that cycles through a set of sentences to produce a stream of data for processing in a Trident topology.

LANGUAGE: java
CODE:
FixedBatchSpout spout = new FixedBatchSpout(new Fields("sentence"), 3,
               new Values("the cow jumped over the moon"),
               new Values("the man went to the store and bought some candy"),
               new Values("four score and seven years ago"),
               new Values("how many apples can you eat"));
spout.setCycle(true);

----------------------------------------

TITLE: Declaring Output Fields for CassandraWriterBolt
DESCRIPTION: Demonstrates how to declare output fields and streams for CassandraWriterBolt, useful for emitting new tuples on error or chaining queries.

LANGUAGE: java
CODE:
new CassandraWriterBolt(insertInto("album").values(withFields(all()).build())
        .withResultHandler(new EmitOnDriverExceptionResultHandler());
        .withStreamOutputFields("stream_error", new Fields("message");

public static class EmitOnDriverExceptionResultHandler extends BaseExecutionResultHandler {
    @Override
    protected void onDriverException(DriverException e, OutputCollector collector, Tuple tuple) {
        LOG.error("An error occurred while executing cassandra statement", e);
        collector.emit("stream_error", new Values(e.getMessage()));
        collector.ack(tuple);
    }
}

----------------------------------------

TITLE: Artifactory Config Example
DESCRIPTION: YAML configuration example for Artifactory-based config loader with timeout setting.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "artifactory+http://artifactory.my.company.com:8000/artifactory/configurations/clusters/my_cluster/ras_pools"
scheduler.config.loader.timeout.sec: 30

----------------------------------------

TITLE: Continuous Item Addition to Kestrel
DESCRIPTION: Main program that continuously adds sentences to a Kestrel queue until stopped. Connects to a local Kestrel server and adds items with a 1-second delay between batches.

LANGUAGE: java
CODE:
    import java.io.IOException;
    import java.io.InputStream;
    import java.util.Random;

    import org.apache.storm.spout.KestrelClient;
    import org.apache.storm.spout.KestrelClient.Item;
    import org.apache.storm.spout.KestrelClient.ParseError;

    public class AddSentenceItemsToKestrel {

    	/**
    	 * @param args
    	 */
    	public static void main(String[] args) {

    		InputStream is = System.in;

			char closing_bracket = ']';

			int val = closing_bracket;

			boolean aux = true;

			try {

				KestrelClient kestrelClient = null;
				String queueName = "sentence_queue";

				while(aux){

					kestrelClient = new KestrelClient("localhost",22133);

					queueSentenceItems(kestrelClient, queueName);

					kestrelClient.close();

					Thread.sleep(1000);

					if(is.available()>0){
					 if(val==is.read())
						 aux=false;
					}
				}
			} catch (IOException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			catch (ParseError e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			} catch (InterruptedException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}

			System.out.println("end");

	    }
	}

----------------------------------------

TITLE: Kryo Tuple Serialization
DESCRIPTION: Thread-safe serialization implementation for Storm tuples using Kryo

LANGUAGE: java
CODE:
https://github.com/apache/storm/blob/0.7.1/src/jvm/org/apache/storm/serialization/KryoTupleSerializer.java#L26

----------------------------------------

TITLE: Configuring Basic CGroup Settings in cgconfig.conf
DESCRIPTION: Sample configuration file that defines CGroup mount points and permissions for Storm resource management. Sets up CPU, memory, and other resource control groups with appropriate user/group permissions.

LANGUAGE: conf
CODE:
mount {
	cpuset	= /cgroup/cpuset;
	cpu	= /cgroup/storm_resources;
	cpuacct	= /cgroup/storm_resources;
	memory	= /cgroup/storm_resources;
	devices	= /cgroup/devices;
	freezer	= /cgroup/freezer;
	net_cls	= /cgroup/net_cls;
	blkio	= /cgroup/blkio;
}

group storm {
       perm {
               task {
                      uid = 500;
                      gid = 500;
               }
               admin {
                      uid = 500;
                      gid = 500;
               }
       }
       cpu {
       }
       memory {
       }
       cpuacct {
       }
}

----------------------------------------

TITLE: Setting ROOT Logger to DEBUG for 30 Seconds
DESCRIPTION: Example command to set the ROOT logger to DEBUG level for 30 seconds in a Storm topology named 'my_topology' using the CLI.

LANGUAGE: bash
CODE:
./bin/storm set_log_level my_topology -l ROOT=DEBUG:30

----------------------------------------

TITLE: Configuring SimpleJdbcMapper in Java
DESCRIPTION: Shows how to configure a SimpleJdbcMapper for transforming Storm tuples to database rows. It includes examples for automatic column detection and explicit column schema definition.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);
String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

LANGUAGE: java
CODE:
List<Column> columnSchema = Lists.newArrayList(
    new Column("user_id", java.sql.Types.INTEGER),
    new Column("user_name", java.sql.Types.VARCHAR));
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(columnSchema);

----------------------------------------

TITLE: Configuring SimpleJdbcMapper in Java
DESCRIPTION: Shows how to configure a SimpleJdbcMapper for transforming Storm tuples to database rows. It includes examples for automatic column detection and explicit column schema definition.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);
String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

LANGUAGE: java
CODE:
List<Column> columnSchema = Lists.newArrayList(
    new Column("user_id", java.sql.Types.INTEGER),
    new Column("user_name", java.sql.Types.VARCHAR));
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(columnSchema);

----------------------------------------

TITLE: Defining Topology in Clojure DSL
DESCRIPTION: Example of using Clojure's domain-specific language (DSL) to define a Storm topology. This snippet demonstrates how to create spouts, bolts, and connect them together using the Clojure DSL.

LANGUAGE: Clojure
CODE:
(defspout sentence-spout ["sentence"]
  [conf context collector]
  (let [sentences ["a" "b" "c"]]
    (spout
      (nextTuple []
        (emit-spout! collector [(rand-nth sentences)]))
      (ack [id])
      (fail [id]))))

(defbolt word-split ["word"] ["sentence"]
  [tuple collector]
  (let [words (. (tuple getString 0) split " ")]
    (doseq [w words]
      (emit-bolt! collector [w] :anchor tuple))
    (ack! collector tuple)))

(defn create-topology []
  (topology
    {"1" (spout-spec sentence-spout)
     "2" (spout-spec sentence-spout)}
    {"3" (bolt-spec {"1" :shuffle "2" :shuffle}
                    word-split
                    :p 5)}))

----------------------------------------

TITLE: Redirecting to Apache Storm Acking Framework Documentation
DESCRIPTION: This HTML snippet uses a meta refresh tag to automatically redirect the user to the current Apache Storm documentation page for the Acking framework implementation. It also includes a canonical link for search engines.

LANGUAGE: HTML
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Acking-framework-implementation.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Acking-framework-implementation.html" />

----------------------------------------

TITLE: Assigning Tasks to Machines in Nimbus (Clojure)
DESCRIPTION: Nimbus uses the mk-assignment function to assign tasks to machines. This creates an Assignment record containing task-to-node mappings, node-to-host mappings, and task start times.

LANGUAGE: clojure
CODE:
(mk-assignment)

----------------------------------------

TITLE: Configuring Isolation Scheduler in Storm YAML
DESCRIPTION: Configuration example for the new Isolation Scheduler feature in Storm 0.8.2. This YAML configuration specifies how many dedicated machines should be allocated to specific topologies for isolation purposes.

LANGUAGE: yaml
CODE:
storm.scheduler: "backtype.storm.scheduler.IsolationScheduler"
isolation.scheduler.machines:
  "production-topology-1": 5
  "production-topology-2": 3

----------------------------------------

TITLE: Retrieving Nimbus Summary in JSON
DESCRIPTION: Example response from the /api/v1/nimbus/summary endpoint showing summary information for all Nimbus hosts in JSON format.

LANGUAGE: json
CODE:
{
  "nimbuses":[
    {
      "host":"192.168.202.1",
      "port":6627,
      "nimbusLogLink":"http:\/\/192.168.202.1:8000\/log?file=nimbus.log",
      "status":"Leader",
      "version":"0.10.0-SNAPSHOT",
      "nimbusUpTime":"3m 33s",
      "nimbusUpTimeSeconds":"213"
    }
  ]
}

----------------------------------------

TITLE: System Topology Creation in Nimbus
DESCRIPTION: Function to create the actual topology with implicit streams and acker bolt, used by Nimbus when creating tasks.

LANGUAGE: clojure
CODE:
[system-topology!](https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/common.clj#L188)

----------------------------------------

TITLE: Configuring Resource Aware Scheduler in Storm YAML
DESCRIPTION: Basic configuration to enable the Resource Aware Scheduler in Storm's configuration file.

LANGUAGE: yaml
CODE:
storm.scheduler: "org.apache.storm.scheduler.resource.ResourceAwareScheduler"

----------------------------------------

TITLE: Using Storm Shell Command for Topology Submission
DESCRIPTION: This command demonstrates how to use the 'storm shell' command to submit a topology. It packages resources into a jar, uploads it to Nimbus, and calls the specified Python script with additional arguments.

LANGUAGE: bash
CODE:
storm shell resources/ python topology.py arg1 arg2

----------------------------------------

TITLE: Node Resource Configuration
DESCRIPTION: YAML configuration for specifying available CPU and memory resources on a Storm node.

LANGUAGE: yaml
CODE:
supervisor.memory.capacity.mb: 20480.0
supervisor.cpu.capacity: 100.0

----------------------------------------

TITLE: Configuring Redis Cluster State Provider in Storm YAML
DESCRIPTION: JSON configuration for the Redis Cluster state provider, specifying key/value classes, serializers, and Redis Cluster connection details. This is set in the storm.yaml configuration file.

LANGUAGE: json
CODE:
{
  "keyClass": "Optional fully qualified class name of the Key type.",
  "valueClass": "Optional fully qualified class name of the Value type.",
  "keySerializerClass": "Optional Key serializer implementation class.",
  "valueSerializerClass": "Optional Value Serializer implementation class.",
  "jedisClusterConfig": {
    "nodes": ["localhost:7379", "localhost:7380", "localhost:7381"],
    "timeout": 2000,
    "maxRedirections": 5
  }
}

----------------------------------------

TITLE: Markdown Table of Logo Contest Voting Results
DESCRIPTION: A markdown table showing the voting results for each logo entry, including counts from both PPMC members and the community. The table lists all 11 entries with their creators and respective vote counts.

LANGUAGE: markdown
CODE:
| Entry                        | PPMC | Community |
|:-----------------------------|-----:|----------:|
|1 - Patricia Forrest          | 2    | 23        |
|2 - Samuel QuiÃ±ones           | 3    | 6         |
|3- Shaan Shiv Suleman         | 0    | 7         |
|4 - Richard Brownlie-Marshall | 0    | 6         |
|5 - Ziba Sayari               | 0    | 9         |
|6 - Alec Bartos               | 3    | 32        |
|7 - Calum Boustead            | 0    | 0         |
|8 - Stefano Asili             | 0    | 2         |
|9 - Jennifer Lee              | 9    | 63        |
|10 - Jennifer Lee             | 18   | 64        |
|11 - Jennifer Lee             | 0    | 18        |

----------------------------------------

TITLE: Defining ComponentObject Struct in Thrift for Storm
DESCRIPTION: This Thrift code defines the ComponentObject struct used in Storm to specify the code for spouts and bolts. It allows for serialized Java, shell components, or Java objects.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Storm SQL Command
DESCRIPTION: Command to compile SQL statements into a Trident topology and submit to Storm.

LANGUAGE: bash
CODE:
storm sql sql-file topology-name

----------------------------------------

TITLE: Storm Topology Submission Method Definition
DESCRIPTION: Thrift method definition for submitting a Storm topology, including parameters for name, jar location, configuration, and topology structure.

LANGUAGE: java
CODE:
void submitTopology(1: string name, 2: string uploadedJarLocation, 3: string jsonConf, 4: StormTopology topology) throws (1: AlreadyAliveException e, 2: InvalidTopologyException ite);

----------------------------------------

TITLE: Aggregator Implementation for Counting
DESCRIPTION: Example of implementing a count aggregator using the Aggregator interface.

LANGUAGE: java
CODE:
public class CountAgg extends BaseAggregator<CountState> {
    static class CountState {
        long count = 0;
    }

    public CountState init(Object batchId, TridentCollector collector) {
        return new CountState();
    }

    public void aggregate(CountState state, TridentTuple tuple, TridentCollector collector) {
        state.count+=1;
    }

    public void complete(CountState state, TridentCollector collector) {
        collector.emit(new Values(state.count));
    }
}

----------------------------------------

TITLE: Dequeuing Items from Kestrel
DESCRIPTION: Method to retrieve items from a Kestrel queue without removing them. Attempts to dequeue 12 items and prints them if available.

LANGUAGE: java
CODE:
    private static void dequeueItems(KestrelClient kestrelClient, String queueName) throws IOException, ParseError
			 {
		for(int i=1; i<=12; i++){

			Item item = kestrelClient.dequeue(queueName);

			if(item==null){
				System.out.println("The queue (" + queueName + ") contains no items.");
			}
			else
			{
				byte[] data = item._data;

				String receivedVal = new String(data);

				System.out.println("receivedItem=" + receivedVal);
			}
		}

----------------------------------------

TITLE: Configuring HDFS Bolt in Java
DESCRIPTION: Example of how to configure and use the HdfsBolt to write pipe-delimited files to HDFS with specific sync and rotation policies.

LANGUAGE: java
CODE:
// use "|" instead of "," for field delimiter
RecordFormat format = new DelimitedRecordFormat()
        .withFieldDelimiter("|");

// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withPath("/foo/");

HdfsBolt bolt = new HdfsBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withRecordFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy);

----------------------------------------

TITLE: Using Classpath Wildcards in Java 6+ for Storm Commands
DESCRIPTION: This snippet demonstrates the use of classpath wildcards in Java 6 and later versions to include all JAR files in a directory. This approach is used by Storm to shorten command lengths and avoid breaching the Linux Kernel process table limit.

LANGUAGE: java
CODE:
foo/bar/*

----------------------------------------

TITLE: Virtual Port Implementation for Distributed Mode in Apache Storm (Clojure)
DESCRIPTION: Implementation of the virtual port used for routing messages to tasks in distributed mode.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/zilch/virtual_port.clj

----------------------------------------

TITLE: Storm Topology Submission with Blobstore Mapping
DESCRIPTION: Example command for submitting a Storm topology with blobstore configuration mapping

LANGUAGE: bash
CODE:
storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar org.apache.storm.starter.clj.word_count test_topo -c topology.blobstore.map='{"key1":{"localname":"blob_file", "uncompress":false, "workerRestart":true},"key2":{}}'

----------------------------------------

TITLE: Registering Custom Serializers in Storm Topology Configuration
DESCRIPTION: This YAML configuration snippet demonstrates how to register custom serializers for specific types in a Storm topology. It shows both default FieldsSerializer usage and custom serializer implementation.

LANGUAGE: yaml
CODE:
topology.kryo.register:
  - com.mycompany.CustomType1
  - com.mycompany.CustomType2: com.mycompany.serializer.CustomType2Serializer
  - com.mycompany.CustomType3

----------------------------------------

TITLE: Defining Code Distribution Interface in Java for Storm Nimbus
DESCRIPTION: Java interface defining methods for distributing and managing topology code across Nimbus servers. It includes methods for uploading, downloading, and cleaning up code, as well as checking replication status.

LANGUAGE: java
CODE:
public interface ICodeDistributor {
    /**
     * Prepare this code distributor.
     * @param conf
     */
    void prepare(Map conf);

    /**
     * This API will perform the actual upload of the code to the distributed implementation.
     * The API should return a Meta file which should have enough information for downloader 
     * so it can download the code e.g. for bittorrent it will be a torrent file, in case of something         
     * like HDFS or s3  it might have the actual directory or paths for files to be downloaded.
     * @param dirPath local directory where all the code to be distributed exists.
     * @param topologyId the topologyId for which the meta file needs to be created.
     * @return metaFile
     */
    File upload(Path dirPath, String topologyId);

    /**
     * Given the topologyId and metafile, download the actual code and return the downloaded file's list.
     * @param topologyid
     * @param metafile 
     * @param destDirPath the folder where all the files will be downloaded.
     * @return
     */
    List<File> download(Path destDirPath, String topologyid, File metafile);

    /**
      * Given the topologyId, returns number of hosts where the code has been replicated.
      */
    int getReplicationCount(String topologyId);
    
   /**
     * Performs the cleanup.
     * @param topologyid
     */
    void cleanup(String topologyid);

    /**
     * Close this distributor.
     * @param conf
     */
    void close(Map conf);
}

----------------------------------------

TITLE: Configuring Storm Bolt with JSON Mapper
DESCRIPTION: Demonstrates setting up a SolrUpdateBolt with JSON mapping and count-based commit strategy. Uses zkHostString for Solr configuration and defines JSON mapping for the 'gettingstarted' collection.

LANGUAGE: java
CODE:
    new SolrUpdateBolt(solrConfig, solrMapper, solrCommitStgy)
    
    // zkHostString for Solr 'gettingstarted' example
    SolrConfig solrConfig = new SolrConfig("127.0.0.1:9983");
    
    // JSON Mapper used to generate 'SolrRequest' requests to update the "gettingstarted" Solr collection with JSON content declared the tuple field with name "JSON"
    SolrMapper solrMapper = new SolrJsonMapper.Builder("gettingstarted", "JSON").build(); 
     
    // Acks every other five tuples. Setting to null acks every tuple
    SolrCommitStrategy solrCommitStgy = new CountBasedCommit(5);

----------------------------------------

TITLE: JSON Structure for Failing Tuple in Storm Multi-Language Protocol
DESCRIPTION: This JSON structure is used to mark a tuple as failed in the Storm multi-language protocol. It specifies the command as 'fail' and includes the id of the tuple to be marked as failed.

LANGUAGE: JSON
CODE:
{
    "command": "fail",
    "id": 123123
}

----------------------------------------

TITLE: Viewing Image Tag to Manifest Hash Mapping
DESCRIPTION: Command to view the contents of the image-tag-to-hash file in HDFS.

LANGUAGE: bash
CODE:
-bash-4.2$ hdfs dfs -cat /containers/image-tag-to-hash
storm/rhel7:dev_current:26fd443859325d5911f3be5c5e231dddca88ee0d526456c0c92dd794148d8585#docker.xxx.com:4443/hadoop-user-images/storm/rhel7:20201202-232133

----------------------------------------

TITLE: Storm Shell Bolt Specification in Clojure
DESCRIPTION: Example of defining a shell bolt that runs a Python script, showing input declarations and output specifications.

LANGUAGE: clojure
CODE:
(shell-bolt-spec {"1" :shuffle "2" ["id"]}
                 "python3"
                 "mybolt.py"
                 ["outfield1" "outfield2"]
                 :p 25)

----------------------------------------

TITLE: Storm Project Maven POM Reference
DESCRIPTION: Example reference to the Maven POM configuration file used for setting up a Storm project with proper dependencies. Located at examples/storm-starter/pom.xml.

LANGUAGE: xml
CODE:
{{page.git-blob-base}}/examples/storm-starter/pom.xml

----------------------------------------

TITLE: Worker Transfer Function Implementation in Apache Storm (Clojure)
DESCRIPTION: This code implements the transfer function used by tasks to send tuples to other tasks. It serializes the tuple and adds it to a transfer queue, which is then processed by a separate thread.

LANGUAGE: clojure
CODE:
(github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L56)

----------------------------------------

TITLE: Implementing OpenTSDB TridentState in Storm Topology
DESCRIPTION: This example illustrates the usage of OpenTSDB TridentState in a Storm topology. It shows the creation of an OpenTsdbStateFactory, setting up a TridentTopology, and configuring a stream with OpenTSDB state persistence.

LANGUAGE: java
CODE:
final OpenTsdbStateFactory openTsdbStateFactory =
        new OpenTsdbStateFactory(OpenTsdbClient.newBuilder(tsdbUrl),
                Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
TridentTopology tridentTopology = new TridentTopology();

final Stream stream = tridentTopology.newStream("metric-tsdb-stream", new MetricGenSpout());

stream.peek(new Consumer() {
    @Override
    public void accept(TridentTuple input) {
        LOG.info("########### Received tuple: [{}]", input);
    }
}).partitionPersist(openTsdbStateFactory, MetricGenSpout.DEFAULT_METRIC_FIELDS, new OpenTsdbStateUpdater());

----------------------------------------

TITLE: Defining the ISpout Interface in Java
DESCRIPTION: This code snippet shows the ISpout interface definition in Storm, which spouts must implement. It includes methods for initialization, tuple emission, and handling acknowledgments or failures.

LANGUAGE: java
CODE:
public interface ISpout extends Serializable {
    void open(Map conf, TopologyContext context, SpoutOutputCollector collector);
    void close();
    void nextTuple();
    void ack(Object msgId);
    void fail(Object msgId);
}

----------------------------------------

TITLE: Storm Topology Submission Method Definition
DESCRIPTION: Thrift method definition for submitting a topology to Storm, including parameters for name, jar location, configuration, and topology structure.

LANGUAGE: thrift
CODE:
void submitTopology(1: string name, 2: string uploadedJarLocation, 3: string jsonConf, 4: StormTopology topology)
    throws (1: AlreadyAliveException e, 2: InvalidTopologyException ite);

----------------------------------------

TITLE: Displaying jQuery and Sizzle.js License Text
DESCRIPTION: This snippet contains the full text of the MIT License for jQuery v3.6.1 and Sizzle.js. It outlines the permissions, conditions, and limitations for using, modifying, and distributing the software.

LANGUAGE: plaintext
CODE:
jQuery v 3.6.1
Copyright OpenJS Foundation and other contributors, https://openjsf.org/

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

******************************************

The jQuery JavaScript Library v3.6.1 also includes Sizzle.js

Sizzle.js includes the following license:

Copyright JS Foundation and other contributors, https://js.foundation/

This software consists of voluntary contributions made by many
individuals. For exact contribution history, see the revision history
available at https://github.com/jquery/sizzle

The following license applies to all parts of this software except as
documented below:

====

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

====

All files located in the node_modules and external directories are
externally maintained libraries used by this software which have their
own licenses; we recommend you read them, as their terms may differ from
the terms above.

----------------------------------------

TITLE: Selecting Wait Strategies for Storm Components
DESCRIPTION: Chooses wait strategies for spouts, bolts, and backpressure scenarios to conserve CPU usage during low traffic or queue full situations.

LANGUAGE: yaml
CODE:
topology.spout.wait.strategy: "org.apache.storm.policy.WaitStrategyProgressive"
topology.bolt.wait.strategy: "org.apache.storm.policy.WaitStrategyPark"
topology.backpressure.wait.strategy: "org.apache.storm.policy.WaitStrategyProgressive"

----------------------------------------

TITLE: Implementing HTML Redirect for Storm Documentation
DESCRIPTION: Sets up an automatic redirect to the current Storm implementation documentation page using HTML meta tags. Also includes a canonical link for search engines.

LANGUAGE: HTML
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Implementation-docs.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Implementation-docs.html" />

----------------------------------------

TITLE: Adding Apache Storm Dependency in Maven POM
DESCRIPTION: This XML snippet shows how to add Apache Storm as a development dependency in a Maven project's pom.xml file. It specifies the groupId, artifactId, version, and scope for the Storm client dependency.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.storm</groupId>
  <artifactId>storm-client</artifactId>
  <version>{{page.version}}</version>
  <scope>provided</scope>
</dependency>

----------------------------------------

TITLE: Shutting Down Storm LocalCluster in Java
DESCRIPTION: Demonstrates how to properly shutdown a local Storm cluster instance.

LANGUAGE: java
CODE:
cluster.shutdown();

----------------------------------------

TITLE: Configuring DRPC Servers in Storm YAML
DESCRIPTION: This YAML configuration specifies the list of DRPC servers for the Storm cluster. It's required if you want to set up DRPC servers.

LANGUAGE: yaml
CODE:
drpc.servers: ["111.222.333.44"]

----------------------------------------

TITLE: Enabling Resource Aware Scheduler in Storm Configuration
DESCRIPTION: Sets the scheduler class to use the Resource Aware Scheduler in the Storm configuration file.

LANGUAGE: yaml
CODE:
storm.scheduler: "org.apache.storm.scheduler.resource.ResourceAwareScheduler"

----------------------------------------

TITLE: Adding Storm-Redis Dependency in Maven
DESCRIPTION: This XML snippet shows how to add the storm-redis dependency to a Maven project. It specifies the groupId, artifactId, and version, which should be set to match the Storm version being used.

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>storm-redis</artifactId>
    <version>${storm.version}</version>
    <type>jar</type>
</dependency>

----------------------------------------

TITLE: Initializing JdbcInsertBolt in Java
DESCRIPTION: This code snippet demonstrates how to initialize a JdbcInsertBolt using a ConnectionProvider and JdbcMapper. It shows two ways to configure the bolt: using a table name or an insert query.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);

String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

JdbcInsertBolt userPersistanceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withTableName("user")
                                    .withQueryTimeoutSecs(30);
                                    Or
JdbcInsertBolt userPersistanceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withInsertQuery("insert into user values (?,?)")
                                    .withQueryTimeoutSecs(30);   

----------------------------------------

TITLE: Submitting Topology with Blobstore Mapping
DESCRIPTION: Command to submit a Storm topology with blobstore mapping configuration.

LANGUAGE: bash
CODE:
storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar org.apache.storm.starter.clj.word_count test_topo -c topology.blobstore.map='{"key1":{"localname":"blob_file", "uncompress":false},"key2":{}}'

----------------------------------------

TITLE: Implementing RedisStoreMapper in Java
DESCRIPTION: Java class implementing RedisStoreMapper interface for storing word counts in Redis hash.

LANGUAGE: java
CODE:
class WordCountStoreMapper implements RedisStoreMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountStoreMapper() {
        description = new RedisDataTypeDescription(
            RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return tuple.getStringByField("count");
    }
}

----------------------------------------

TITLE: Creating Trident Kafka Spout Configuration
DESCRIPTION: Example showing how to configure and initialize a Trident Kafka spout

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
BrokerHosts zk = new ZkHosts("localhost");
TridentKafkaConfig spoutConf = new TridentKafkaConfig(zk, "test-topic");
spoutConf.scheme = new SchemeAsMultiScheme(new StringScheme());
OpaqueTridentKafkaSpout spout = new OpaqueTridentKafkaSpout(spoutConf);

----------------------------------------

TITLE: Starting Pacemaker Daemon
DESCRIPTION: Command to start the Pacemaker daemon in Storm.

LANGUAGE: bash
CODE:
$ storm pacemaker

----------------------------------------

TITLE: Distributed Mode Message Receiving in Apache Storm (Clojure)
DESCRIPTION: This code implements the virtual port functionality in distributed mode, where each worker listens on a single TCP port for incoming messages and routes them to tasks.

LANGUAGE: clojure
CODE:
(github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L204)

----------------------------------------

TITLE: Leader Election Interface Definition
DESCRIPTION: Java interface definition for Storm's leader election mechanism

LANGUAGE: java
CODE:
public interface ILeaderElector {
    void addToLeaderLockQueue();
    void removeFromLeaderLockQueue();
    boolean isLeader();
    InetSocketAddress getLeaderAddress();
    List<InetSocketAddress> getAllNimbusAddresses();
}

----------------------------------------

TITLE: Configuring URL Expansion with Fields Grouping in Storm
DESCRIPTION: Example showing URL expansion bolt configuration using fields grouping to ensure the same URLs are processed by the same task for efficient caching.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .fieldsGrouping("urls", new Fields("url"));

----------------------------------------

TITLE: Defining Leader Election Interface in Java
DESCRIPTION: Interface for implementing leader election among Nimbus instances. Includes methods for joining/leaving the leader queue, checking leadership status, and getting leader/participant addresses.

LANGUAGE: java
CODE:
public interface ILeaderElector {
    void addToLeaderLockQueue();
    void removeFromLeaderLockQueue();
    boolean isLeader();
    InetSocketAddress getLeaderAddress();
    List<InetSocketAddress> getAllNimbusAddresses();
}

----------------------------------------

TITLE: Retrieving Supervisor Details in Storm UI REST API
DESCRIPTION: Example JSON response for the /api/v1/supervisor endpoint, which returns detailed information about supervisors and workers in the cluster.

LANGUAGE: json
CODE:
{
    "supervisors": [{ 
        "totalMem": 4096.0, 
        "host":"192.168.10.237",
        "id":"bdfe8eff-f1d8-4bce-81f5-9d3ae1bf432e-169.254.129.212",
        "uptime":"7m 8s",
        "totalCpu":400.0,
        "usedCpu":495.0,
        "usedMem":3432.0,
        "slotsUsed":2,
        "version":"0.10.1",
        "slotsTotal":4,
        "uptimeSeconds":428
    }],
    "schedulerDisplayResource":true,
    "workers":[{
        "topologyName":"ras",
        "topologyId":"ras-4-1460229987",
        "host":"192.168.10.237",
        "supervisorId":"bdfe8eff-f1d8-4bce-81f5-9d3ae1bf432e-169.254.129.212",
        "assignedMemOnHeap":704.0,
        "uptime":"2m 47s",
        "uptimeSeconds":167,
        "port":6707,
        "workerLogLink":"http:\/\/192.168.10.237:8000\/log?file=ras-4-1460229987%2F6707%2Fworker.log",
        "componentNumTasks": {
            "word":5
        },
        "executorsTotal":8,
        "assignedCpu":130.0,
        "assignedMemOffHeap":80.0
    }]
}

----------------------------------------

TITLE: Configuring Isolation Scheduler in Storm YAML
DESCRIPTION: YAML configuration example showing how to allocate dedicated machines to specific topologies using the Isolation Scheduler. This configuration maps topology names to their allocated number of isolated machines.

LANGUAGE: yaml
CODE:
isolation.scheduler.machines: 
    "my-topology": 8
    "tiny-topology": 1
    "some-other-topology": 3

----------------------------------------

TITLE: Viewing OCI Image Config JSON
DESCRIPTION: JSON content of an OCI image config file stored in HDFS (some content omitted).

LANGUAGE: json
CODE:
{
  "architecture": "amd64",
  "config": {
    "Hostname": "",
    "Domainname": "",
    "User": "root",
    "AttachStdin": false,
    "AttachStdout": false,
    "AttachStderr": false,
    "Tty": false,
    "OpenStdin": false,
    "StdinOnce": false,
    "Env": [
      "X_SCLS=rh-git218",
      "LD_LIBRARY_PATH=/opt/rh/httpd24/root/usr/lib64",
      "PATH=/opt/rh/rh-git218/root/usr/bin:/home/y/bin64:/home/y/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/y/share/yjava_jdk/java/bin",
      "PERL5LIB=/opt/rh/rh-git218/root/usr/share/perl5/vendor_perl",
      "LANG=en_US.UTF-8",
      "LANGUAGE=en_US:en",
      "LC_ALL=en_US.UTF-8",
      "JAVA_HOME=/home/y/share/yjava_jdk/java"
    ],
    "Cmd": [
      "/bin/bash"
    ],
    "Image": "sha256:6977cd0735c96d14248e834f775373e40230c134b70f10163c05ce6c6c8873ca",
    "Volumes": null,
    "WorkingDir": "",
    "Entrypoint": null,
    "OnBuild": null,
    "Labels": {
      "name": "xxxxx"
    }
  },
  "container": "344ff1084dea3e0501a0d426e52c43cd589d6b29f33ab0915b7be8906b9aec41",
  "container_config": {
    "Hostname": "344ff1084dea",
    "Domainname": "",
    "User": "root",
    "AttachStdin": false,
    "AttachStdout": false,
    "AttachStderr": false,
    "Tty": false,
    "OpenStdin": false,
    "StdinOnce": false,
    "Env": [
      "X_SCLS=rh-git218",
      "LD_LIBRARY_PATH=/opt/rh/httpd24/root/usr/lib64",
      "PATH=/opt/rh/rh-git218/root/usr/bin:/home/y/bin64:/home/y/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/y/share/yjava_jdk/java/bin",
      "PERL5LIB=/opt/rh/rh-git218/root/usr/share/perl5/vendor_perl",
      "LANG=en_US.UTF-8",
      "LANGUAGE=en_US:en",
      "LC_ALL=en_US.UTF-8",
      "JAVA_HOME=/home/y/share/yjava_jdk/java"
    ],
    "Cmd": [
      "/bin/sh",
      "-c"
    ],
    "Image": "sha256:6977cd0735c96d14248e834f775373e40230c134b70f10163c05ce6c6c8873ca",
    "Volumes": null,
    "WorkingDir": "",
    "Entrypoint": null,
    "OnBuild": null,
    "Labels": {
      "name": "xxxxx"
    }
  },
  "created": "2020-12-02T23:25:47.354704574Z",
  "docker_version": "19.03.8",
  "history": [
    {
      "created": "2020-02-18T21:43:36.934503462Z",
      "created_by": "/bin/sh"
    },
    {
      "created": "2020-02-18T21:45:05.729764427Z",
      "created_by": "/bin/sh"
    },
    {
      "created": "2020-02-18T21:46:36.638896031Z",
      "created_by": "/bin/sh"
    },
    {
      "created": "2020-12-02T23:21:54.595662813Z",
      "created_by": "/bin/sh -c #(nop)  USER root",
      "empty_layer": true
    },
    {
      "created": "2020-12-02T23:25:45.822235539Z",
      "created_by": "/bin/sh -c /opt/python/bin/pip3.6 install --no-cache-dir numpy scipy pandas requests setuptools scikit-learn matplotlib"
    },
    {
      "created": "2020-12-02T23:25:46.708884538Z",
      "created_by": "/bin/sh -c #(nop)  ENV JAVA_HOME=/home/y/share/yjava_jdk/java",
      "empty_layer": true
    },
    {
      "created": "2020-12-02T23:25:46.770226108Z",
      "created_by": "/bin/sh -c #(nop)  ENV PATH=/opt/rh/rh-git218/root/usr/bin:/home/y/bin64:/home/y/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/y/share/yjava_jdk/java/bin",
      "empty_layer": true
    },
    {
      "created": "2020-12-02T23:25:46.837263533Z",
      "created_by": "/bin/sh -c #(nop) COPY file:33283617fbd796b25e53eaf4d26012eea1f610ff9acc0706f11281e86be440dc in /etc/krb5.conf "
    },
    {
      "created": "2020-12-02T23:25:47.237515768Z",
      "created_by": "/bin/sh -c echo '7.7.4' > /etc/hadoop-dockerfile-version"
    }
  ],
  "os": "linux",
  "rootfs": {
    "type": "layers",
    "diff_ids": [
      "sha256:9f627fdb0292afbe5e2eb96edc1b3a5d3a8f468e3acf1d29f1509509285c7341",
      "sha256:83d2667f9458eaf719588a96bb63f2520bd377d29d52f6dbd4ff13c819c08037",
      "sha256:fcba5f49eef4f3d77d3e73e499a1a4e1914b3f20d903625d27c0aa3ab82f41a3",
      "sha256:3bd4567d0726f5d6560b548bc0c0400e868f6a27067887a36edd7e8ceafff96c",
      "sha256:ad56900a1f10e6ef96f17c7e8019384540ab1b34ccce6bda06675473b08d787e",
      "sha256:ac0a645609f957ab9c4a8a62f8646e99f09a74ada54ed2eaca204c6e183c9ae8",
      "sha256:9bf10102fc145156f4081c2cacdbadab5816dce4f88eb02881ab739239d316e6"
    ]
  }
}

----------------------------------------

TITLE: Stream Builder and Value Mapper Usage
DESCRIPTION: Examples demonstrating StreamBuilder initialization and using value mappers to extract typed data from tuples

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();
Stream<String> sentences = builder.newStream(new TestWordSpout(), new ValueMapper<String>(0));

// extract first three fields of the tuple emitted by the spout to produce a stream of typed tuples.
Stream<Tuple3<String, Integer, Long>> stream = builder.newStream(new TestSpout(), TupleValueMappers.of(0, 1, 2));

----------------------------------------

TITLE: Installing JZMQ for Storm
DESCRIPTION: This snippet demonstrates how to clone a specific fork of JZMQ, configure it, and install it. This fork is tested to work with Storm and is recommended to prevent regressions.

LANGUAGE: bash
CODE:
#install jzmq
git clone https://github.com/nathanmarz/jzmq.git
cd jzmq
./autogen.sh
./configure
make
sudo make install

----------------------------------------

TITLE: Task Message Listening in Apache Storm (Clojure)
DESCRIPTION: Tasks listen on an in-memory ZeroMQ port for messages from the virtual port.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L201

----------------------------------------

TITLE: Implementing Simple MongoDB Update Mapper
DESCRIPTION: Implementation for mapping tuple fields to MongoDB update operations using $set operator

LANGUAGE: java
CODE:
public class SimpleMongoUpdateMapper extends SimpleMongoMapper implements MongoUpdateMapper {

    private String[] fields;

    @Override
    public Document toDocument(ITuple tuple) {
        Document document = new Document();
        for(String field : fields){
            document.append(field, tuple.getValueByField(field));
        }
        return new Document("$set", document);
    }

    public SimpleMongoUpdateMapper withFields(String... fields) {
        this.fields = fields;
        return this;
    }
}

----------------------------------------

TITLE: Defining a Storm Topology with Spout and Bolts in Java
DESCRIPTION: This code snippet demonstrates how to create a Storm topology with a KestrelSpout for reading sentences, a SplitSentence bolt for splitting sentences into words, and a WordCount bolt for counting word occurrences.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("sentences", new KestrelSpout("kestrel.backtype.com",
                                               22133,
                                               "sentence_queue",
                                               new StringScheme()));
builder.setBolt("split", new SplitSentence(), 10)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 20)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Queuing Sentences in Kestrel
DESCRIPTION: Method to add random sentences to a Kestrel queue using KestrelClient. Randomly selects from five predefined sentences and adds them with sequential IDs.

LANGUAGE: java
CODE:
    private static void queueSentenceItems(KestrelClient kestrelClient, String queueName)
			throws ParseError, IOException {

		String[] sentences = new String[] {
	            "the cow jumped over the moon",
	            "an apple a day keeps the doctor away",
	            "four score and seven years ago",
	            "snow white and the seven dwarfs",
	            "i am at two with nature"};

		Random _rand = new Random();

		for(int i=1; i<=10; i++){

			String sentence = sentences[_rand.nextInt(sentences.length)];

			String val = "ID " + i + " " + sentence;

			boolean queueSucess = kestrelClient.queue(queueName, val);

			System.out.println("queueSucess=" +queueSucess+ " [" + val +"]");
		}
	}

----------------------------------------

TITLE: Defining a Transactional Topology in Java
DESCRIPTION: Example of defining a transactional topology that computes a global count of tuples from an input stream using TransactionalTopologyBuilder.

LANGUAGE: java
CODE:
MemoryTransactionalSpout spout = new MemoryTransactionalSpout(DATA, new Fields("word"), PARTITION_TAKE_PER_BATCH);
TransactionalTopologyBuilder builder = new TransactionalTopologyBuilder("global-count", "spout", spout, 3);
builder.setBolt("partial-count", new BatchCount(), 5)
        .shuffleGrouping("spout");
builder.setBolt("sum", new UpdateGlobalCount())
        .globalGrouping("partial-count");

----------------------------------------

TITLE: Displaying jQuery v3.6.1 and Sizzle.js License Text
DESCRIPTION: This snippet contains the full text of the jQuery v3.6.1 license, which follows the MIT License format. It also includes the license for Sizzle.js, which is bundled with jQuery. The licenses grant permissions for use, modification, and distribution while disclaiming warranties and liabilities.

LANGUAGE: plaintext
CODE:
jQuery v 3.6.1
Copyright OpenJS Foundation and other contributors, https://openjsf.org/

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

******************************************

The jQuery JavaScript Library v3.6.1 also includes Sizzle.js

Sizzle.js includes the following license:

Copyright JS Foundation and other contributors, https://js.foundation/

This software consists of voluntary contributions made by many
individuals. For exact contribution history, see the revision history
available at https://github.com/jquery/sizzle

The following license applies to all parts of this software except as
documented below:

====

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

====

All files located in the node_modules and external directories are
externally maintained libraries used by this software which have their
own licenses; we recommend you read them, as their terms may differ from
the terms above.

----------------------------------------

TITLE: Storm Acker Execute Implementation
DESCRIPTION: The core execution logic of Storm's acker bolt. It processes tick tuples, updates tuple tree checksums, and manages tuple completion tracking. The acker uses XOR operations to maintain checksums for tracking tuple processing completion.



----------------------------------------

TITLE: Displaying jQuery UI v1.13.2 License Text
DESCRIPTION: This snippet shows the full license text for jQuery UI v1.13.2. It includes the MIT license, copyright notice, permissions, warranty disclaimer, and liability limitations. It also mentions CC0 waiver for sample code and separate licensing for external libraries.

LANGUAGE: plaintext
CODE:
Copyright jQuery Foundation and other contributors, https://jquery.org/

This software consists of voluntary contributions made by many
individuals. For exact contribution history, see the revision history
available at https://github.com/jquery/jquery-ui

The following license applies to all parts of this software except as
documented below:

====

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

====

Copyright and related rights for sample code are waived via CC0. Sample
code is defined as all source code contained within the demos directory.

CC0: http://creativecommons.org/publicdomain/zero/1.0/

====

All files located in the node_modules and external directories are
externally maintained libraries used by this software which have their
own licenses; we recommend you read them, as their terms may differ from
the terms above.

----------------------------------------

TITLE: Configuring CGroups Mount Points and Permissions
DESCRIPTION: Sample cgconfig.conf configuration showing mount points for various resources (CPU, memory, devices, etc.) and permission settings for the storm group. This configuration enables Storm to manage worker resources through CGroups.

LANGUAGE: conf
CODE:
mount {
	cpuset	= /cgroup/cpuset;
	cpu	= /cgroup/storm_resources;
	cpuacct	= /cgroup/storm_resources;
	memory	= /cgroup/storm_resources;
	devices	= /cgroup/devices;
	freezer	= /cgroup/freezer;
	net_cls	= /cgroup/net_cls;
	blkio	= /cgroup/blkio;
}

group storm {
       perm {
               task {
                      uid = 500;
                      gid = 500;
               }
               admin {
                      uid = 500;
                      gid = 500;
               }
       }
       cpu {
       }
       memory {
       }
       cpuacct {
       }
}

----------------------------------------

TITLE: Installing ZeroMQ 2.1.7 for Storm
DESCRIPTION: Commands for downloading, extracting, and installing ZeroMQ 2.1.7, which is the recommended version for Storm. This involves configuring, building, and installing the ZeroMQ library system-wide.

LANGUAGE: bash
CODE:
wget http://download.zeromq.org/zeromq-2.1.7.tar.gz
tar -xzf zeromq-2.1.7.tar.gz
cd zeromq-2.1.7
./configure
make
sudo make install

----------------------------------------

TITLE: Task Message Routing Implementation
DESCRIPTION: Code handling message routing in tasks, including stream grouping functions and task ID determination for tuple emission.

LANGUAGE: clojure
CODE:
tasks-fn

----------------------------------------

TITLE: Implementing Streaming Top N Pattern in Storm
DESCRIPTION: This code demonstrates the Streaming Top N pattern in Storm. It uses a RankObjects bolt with fields grouping for parallel processing, followed by a MergeObjects bolt with global grouping to combine results.

LANGUAGE: java
CODE:
builder.setBolt("rank", new RankObjects(), parallelism)
  .fieldsGrouping("objects", new Fields("value"));
builder.setBolt("merge", new MergeObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Setting Component Memory Requirements
DESCRIPTION: Java API examples showing how to set memory requirements for Storm topology components

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.setMemoryLoad(1024.0, 512.0);
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
            .shuffleGrouping("word").setMemoryLoad(512.0);

----------------------------------------

TITLE: Defining JdbcMapper Interface in Java
DESCRIPTION: This code snippet defines the JdbcMapper interface, which is the main API for inserting data into a table using JDBC. It includes a method for mapping a Storm tuple to a list of columns representing a row in a database.

LANGUAGE: java
CODE:
public interface JdbcMapper  extends Serializable {
    List<Column> getColumns(ITuple tuple);
}

----------------------------------------

TITLE: Defining JdbcMapper Interface in Java
DESCRIPTION: This code snippet defines the JdbcMapper interface, which is the main API for inserting data into a table using JDBC. It includes a method for mapping a Storm tuple to a list of columns representing a row in a database.

LANGUAGE: java
CODE:
public interface JdbcMapper  extends Serializable {
    List<Column> getColumns(ITuple tuple);
}

----------------------------------------

TITLE: Configuring SolrUpdateBolt with JSON Mapper in Java
DESCRIPTION: This snippet demonstrates how to configure a SolrUpdateBolt using a SolrJsonMapper and CountBasedCommit strategy. It sets up the bolt to index JSON content from a tuple field named 'JSON' into the 'gettingstarted' Solr collection.

LANGUAGE: java
CODE:
new SolrUpdateBolt(solrConfig, solrMapper, solrCommitStgy)

// zkHostString for Solr 'gettingstarted' example
SolrConfig solrConfig = new SolrConfig("127.0.0.1:9983");

// JSON Mapper used to generate 'SolrRequest' requests to update the "gettingstarted" Solr collection with JSON content declared the tuple field with name "JSON"
SolrMapper solrMapper = new SolrJsonMapper.Builder("gettingstarted", "JSON").build(); 
 
// Acks every other five tuples. Setting to null acks every tuple
SolrCommitStrategy solrCommitStgy = new CountBasedCommit(5);          

----------------------------------------

TITLE: Defining MongoMapper Interface for Storm-MongoDB Integration in Java
DESCRIPTION: Defines the MongoMapper interface used to convert Storm tuples to MongoDB documents. This interface is crucial for inserting data into MongoDB collections from Storm topologies.

LANGUAGE: java
CODE:
public interface MongoMapper extends Serializable {
    Document toDocument(ITuple tuple);
}

----------------------------------------

TITLE: Defining Bolt Component in Thrift
DESCRIPTION: Thrift definition for a bolt component in Storm, including ComponentObject and ComponentCommon structs. ComponentObject specifies the implementation, while ComponentCommon defines streams, configurations, and other metadata.

LANGUAGE: Thrift
CODE:
struct ComponentObject {
  1: binary serialized_java
  2: ShellComponent shell
  3: JavaObject java_object
}

struct ComponentCommon {
  1: map<string, StreamInfo> streams
  2: map<GlobalStreamId, Grouping> inputs
  3: map<string, string> component_specific_config
  4: i32 parallelism_hint
}

----------------------------------------

TITLE: Maven Dependencies Configuration for Storm Kinesis
DESCRIPTION: XML configuration for required Maven dependencies including AWS SDK, Storm client, Curator framework, and JSON Simple library.

LANGUAGE: xml
CODE:
 <dependencies>
        <dependency>
            <groupId>com.amazonaws</groupId>
            <artifactId>aws-java-sdk</artifactId>
            <version>${aws-java-sdk.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-client</artifactId>
            <version>${project.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-framework</artifactId>
            <version>${curator.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>com.googlecode.json-simple</groupId>
            <artifactId>json-simple</artifactId>
        </dependency>
 </dependencies>

----------------------------------------

TITLE: Setting Local Directory in Storm YAML
DESCRIPTION: This YAML configuration sets the local directory for Nimbus and Supervisor daemons to store small amounts of state. It's a mandatory configuration in the storm.yaml file.

LANGUAGE: yaml
CODE:
storm.local.dir: "/mnt/storm"

LANGUAGE: yaml
CODE:
storm.local.dir: "C:\\storm-local"

----------------------------------------

TITLE: Implementing a Tuple Counting Bolt in Apache Storm Java
DESCRIPTION: Example of a simple bolt implementation that reports the running total of tuples received. Uses the Counter metric to increment the count for each tuple processed.

LANGUAGE: java
CODE:
public class TupleCountingBolt extends BaseRichBolt {
    private Counter tupleCounter;
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        this.tupleCounter = context.registerCounter("tupleCount");
    }

    @Override
    public void execute(Tuple input) {
        this.tupleCounter.inc();
    }
}

----------------------------------------

TITLE: Implementing Gauge Metrics in Clojure for Storm
DESCRIPTION: This snippet shows how to implement a gauge metric in Clojure to measure the instantaneous value of a metric. In this case, it's used to get the number of supervisors in the Storm cluster.

LANGUAGE: clojure
CODE:
(defgauge nimbus:num-supervisors
     (fn [] (.size (.supervisors (:storm-cluster-state nimbus) nil))))

----------------------------------------

TITLE: Declaring Multiple Streams in a Spout
DESCRIPTION: Example of how to declare multiple output streams in a Storm spout using the OutputFieldsDeclarer and emit to specific streams using SpoutOutputCollector.

LANGUAGE: java
CODE:
declareStream(OutputFieldsDeclarer declarer) {
    declarer.declareStream("stream1", new Fields("field1", "field2"));
    declarer.declareStream("stream2", new Fields("field3", "field4"));
}

nextTuple() {
    collector.emit("stream1", new Values(value1, value2));
    collector.emit("stream2", new Values(value3, value4));
}

----------------------------------------

TITLE: Creating Blob via Storm CLI
DESCRIPTION: Example of creating a blob in the Storm distributed cache using the command line interface.

LANGUAGE: shell
CODE:
storm blobstore create --file README.txt --acl o::rwa --replication-factor 4 key1

----------------------------------------

TITLE: Registering Event Logger in Java Configuration
DESCRIPTION: Demonstrates how to register an event logger implementation in the topology configuration using Java code.

LANGUAGE: java
CODE:
conf.registerEventLogger(org.apache.storm.metric.FileBasedEventLogger.class);

----------------------------------------

TITLE: Defining Nimbus Summary Thrift Struct for Storm API
DESCRIPTION: Thrift struct definition for NimbusSummary, containing information about each Nimbus instance including host, port, uptime, leadership status, and version.

LANGUAGE: thrift
CODE:
struct NimbusSummary {
  1: required string host;
  2: required i32 port;
  3: required i32 uptime_secs;
  4: required bool isLeader;
  5: required string version;
}

----------------------------------------

TITLE: Kerberos Server Authentication Configuration
DESCRIPTION: JAAS configuration for Pacemaker server using Kerberos authentication.

LANGUAGE: java
CODE:
PacemakerServer {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/etc/keytabs/pacemaker.keytab"
   storeKey=true
   useTicketCache=false
   principal="pacemaker@MY.COMPANY.COM";
};

----------------------------------------

TITLE: Queuing Sentences to Kestrel in Java
DESCRIPTION: Method to add random sentences to a Kestrel queue using KestrelClient. Selects sentences randomly from a predefined array and queues them with unique IDs.

LANGUAGE: java
CODE:
    private static void queueSentenceItems(KestrelClient kestrelClient, String queueName)
			throws ParseError, IOException {

		String[] sentences = new String[] {
	            "the cow jumped over the moon",
	            "an apple a day keeps the doctor away",
	            "four score and seven years ago",
	            "snow white and the seven dwarfs",
	            "i am at two with nature"};

		Random _rand = new Random();

		for(int i=1; i<=10; i++){

			String sentence = sentences[_rand.nextInt(sentences.length)];

			String val = "ID " + i + " " + sentence;

			boolean queueSucess = kestrelClient.queue(queueName, val);

			System.out.println("queueSucess=" +queueSucess+ " [" + val +"]");
		}
	}

----------------------------------------

TITLE: Example Config File Content
DESCRIPTION: Partial JSON content of an example config file stored in HDFS.

LANGUAGE: json
CODE:
{
  "architecture": "amd64",
  "config": {
    "Hostname": "",
    "Domainname": "",
    "User": "root",
    "AttachStdin": false,
    "AttachStdout": false,
    "AttachStderr": false,
    "Tty": false,
    "OpenStdin": false,
    "StdinOnce": false,
    "Env": [
      "X_SCLS=rh-git218",
      "LD_LIBRARY_PATH=/opt/rh/httpd24/root/usr/lib64",
      "PATH=/opt/rh/rh-git218/root/usr/bin:/home/y/bin64:/home/y/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/y/share/yjava_jdk/java/bin",
      "PERL5LIB=/opt/rh/rh-git218/root/usr/share/perl5/vendor_perl",
      "LANG=en_US.UTF-8",
      "LANGUAGE=en_US:en",
      "LC_ALL=en_US.UTF-8",
      "JAVA_HOME=/home/y/share/yjava_jdk/java"
    ],
    "Cmd": [
      "/bin/bash"
    ],
    "Image": "sha256:6977cd0735c96d14248e834f775373e40230c134b70f10163c05ce6c6c8873ca",
    "Volumes": null,
    "WorkingDir": "",
    "Entrypoint": null,
    "OnBuild": null,
    "Labels": {
      "name": "xxxxx"
    }
  },
  "container": "344ff1084dea3e0501a0d426e52c43cd589d6b29f33ab0915b7be8906b9aec41",
  "container_config": {
    "Hostname": "344ff1084dea",
    "Domainname": "",
    "User": "root",
    "AttachStdin": false,
    "AttachStdout": false,
    "AttachStderr": false,
    "Tty": false,
    "OpenStdin": false,
    "StdinOnce": false,
    "Env": [
      "X_SCLS=rh-git218",
      "LD_LIBRARY_PATH=/opt/rh/httpd24/root/usr/lib64",
      "PATH=/opt/rh/rh-git218/root/usr/bin:/home/y/bin64:/home/y/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/y/share/yjava_jdk/java/bin",
      "PERL5LIB=/opt/rh/rh-git218/root/usr/share/perl5/vendor_perl",
      "LANG=en_US.UTF-8",
      "LANGUAGE=en_US:en",
      "LC_ALL=en_US.UTF-8",
      "JAVA_HOME=/home/y/share/yjava_jdk/java"
    ],
    "Cmd": [
      "/bin/sh",
      "-c"
    ],
    "Image": "sha256:6977cd0735c96d14248e834f775373e40230c134b70f10163c05ce6c6c8873ca",
    "Volumes": null,
    "WorkingDir": "",
    "Entrypoint": null,
    "OnBuild": null,
    "Labels": {
      "name": "xxxxx"
    }
  },
  "created": "2020-12-02T23:25:47.354704574Z",
  "docker_version": "19.03.8",
  "history": [
    {
      "created": "2020-02-18T21:43:36.934503462Z",
      "created_by": "/bin/sh"
    },
    {
      "created": "2020-02-18T21:45:05.729764427Z",
      "created_by": "/bin/sh"
    },
    {
      "created": "2020-02-18T21:46:36.638896031Z",
      "created_by": "/bin/sh"
    },
    {
      "created": "2020-12-02T23:21:54.595662813Z",
      "created_by": "/bin/sh -c #(nop)  USER root",
      "empty_layer": true
    },
    {
      "created": "2020-12-02T23:25:45.822235539Z",
      "created_by": "/bin/sh -c /opt/python/bin/pip3.6 install --no-cache-dir numpy scipy pandas requests setuptools scikit-learn matplotlib"
    },
    {
      "created": "2020-12-02T23:25:46.708884538Z",
      "created_by": "/bin/sh -c #(nop)  ENV JAVA_HOME=/home/y/share/yjava_jdk/java",
      "empty_layer": true
    },
    {
      "created": "2020-12-02T23:25:46.770226108Z",
      "created_by": "/bin/sh -c #(nop)  ENV PATH=/opt/rh/rh-git218/root/usr/bin:/home/y/bin64:/home/y/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/y/share/yjava_jdk/java/bin",
      "empty_layer": true
    },
    {
      "created": "2020-12-02T23:25:46.837263533Z",
      "created_by": "/bin/sh -c #(nop) COPY file:33283617fbd796b25e53eaf4d26012eea1f610ff9acc0706f11281e86be440dc in /etc/krb5.conf "
    },
    {
      "created": "2020-12-02T23:25:47.237515768Z",
      "created_by": "/bin/sh -c echo '7.7.4' > /etc/hadoop-dockerfile-version"
    }
  ],
  "os": "linux",
  "rootfs": {
    "type": "layers",
    "diff_ids": [
      "sha256:9f627fdb0292afbe5e2eb96edc1b3a5d3a8f468e3acf1d29f1509509285c7341",
      "sha256:83d2667f9458eaf719588a96bb63f2520bd377d29d52f6dbd4ff13c819c08037",
      "sha256:fcba5f49eef4f3d77d3e73e499a1a4e1914b3f20d903625d27c0aa3ab82f41a3",
      "sha256:3bd4567d0726f5d6560b548bc0c0400e868f6a27067887a36edd7e8ceafff96c",
      "sha256:ad56900a1f10e6ef96f17c7e8019384540ab1b34ccce6bda06675473b08d787e",
      "sha256:ac0a645609f957ab9c4a8a62f8646e99f09a74ada54ed2eaca204c6e183c9ae8",
      "sha256:9bf10102fc145156f4081c2cacdbadab5816dce4f88eb02881ab739239d316e6"
    ]
  }
}

----------------------------------------

TITLE: jQuery UI MIT License and Terms
DESCRIPTION: Complete license text for jQuery UI including MIT license terms, CC0 waiver for sample code, and notes about external dependencies.

LANGUAGE: text
CODE:
Copyright jQuery Foundation and other contributors, https://jquery.org/

This software consists of voluntary contributions made by many
individuals. For exact contribution history, see the revision history
available at https://github.com/jquery/jquery-ui

The following license applies to all parts of this software except as
documented below:

====

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

====

Copyright and related rights for sample code are waived via CC0. Sample
code is defined as all source code contained within the demos directory.

CC0: http://creativecommons.org/publicdomain/zero/1.0/

====

All files located in the node_modules and external directories are
externally maintained libraries used by this software which have their
own licenses; we recommend you read them, as their terms may differ from
the terms above.

----------------------------------------

TITLE: Thread-safe Tuple Serialization in Apache Storm (Java)
DESCRIPTION: This Java code implements a thread-safe serializer for tuples in Apache Storm.

LANGUAGE: java
CODE:
public class KryoTupleSerializer

----------------------------------------

TITLE: Initializing FixedBatchSpout in Java for Trident
DESCRIPTION: Creates a FixedBatchSpout that cycles through a set of sentences to produce a sentence stream. This spout is used as an input source for the Trident topology.

LANGUAGE: java
CODE:
FixedBatchSpout spout = new FixedBatchSpout(new Fields("sentence"), 3,
               new Values("the cow jumped over the moon"),
               new Values("the man went to the store and bought some candy"),
               new Values("four score and seven years ago"),
               new Values("how many apples can you eat"));
spout.setCycle(true);

----------------------------------------

TITLE: Task Routing Map in Apache Storm (Clojure)
DESCRIPTION: Tasks have a routing map from stream id to component id to stream grouping function.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L198

----------------------------------------

TITLE: Configuring Storm Metrics Reporters
DESCRIPTION: YAML configuration example showing how to set up Graphite and Console reporters for Storm metrics, including reporting periods and metric filtering.

LANGUAGE: yaml
CODE:
topology.metrics.reporters:
  # Graphite Reporter
  - class: "org.apache.storm.metrics2.reporters.GraphiteStormReporter"
    report.period: 60
    report.period.units: "SECONDS"
    graphite.host: "localhost"
    graphite.port: 2003

  # Console Reporter
  - class: "org.apache.storm.metrics2.reporters.ConsoleStormReporter"
    report.period: 10
    report.period.units: "SECONDS"
    filter:
        class: "org.apache.storm.metrics2.filters.RegexFilter"
        expression: ".*my_component.*emitted.*"

----------------------------------------

TITLE: Initializing Trident State for Word Count
DESCRIPTION: Example of using Trident to compute word counts and store them in a Memcached state. Demonstrates the use of persistentAggregate for updating state.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
      topology.newStream("spout1", spout)
        .each(new Fields("sentence"), new Split(), new Fields("word"))
        .groupBy(new Fields("word"))
        .persistentAggregate(MemcachedState.opaque(serverLocations), new Count(), new Fields("count"))                
        .parallelismHint(6);

----------------------------------------

TITLE: Creating Database Tables and Inserting Data in SQL
DESCRIPTION: SQL statements for creating tables and inserting initial data for the example topology. It includes creating user, department, and user_department tables.

LANGUAGE: sql
CODE:
create table if not exists user (user_id integer, user_name varchar(100), dept_name varchar(100), create_date date);
create table if not exists department (dept_id integer, dept_name varchar(100));
create table if not exists user_department (user_id integer, dept_id integer);
insert into department values (1, 'R&D');
insert into department values (2, 'Finance');
insert into department values (3, 'HR');
insert into department values (4, 'Sales');
insert into user_department values (1, 1);
insert into user_department values (2, 2);
insert into user_department values (3, 3);
insert into user_department values (4, 4);
select dept_name from department, user_department where department.dept_id = user_department.dept_id and user_department.user_id = ?;

----------------------------------------

TITLE: Storm Shell Python Execution Format
DESCRIPTION: Format showing how Storm shell executes the Python topology script with Nimbus connection parameters.

LANGUAGE: bash
CODE:
python topology.py arg1 arg2 {nimbus-host} {nimbus-port} {uploaded-jar-location}

----------------------------------------

TITLE: Debugging Kryo Serialization Error in Java Storm
DESCRIPTION: Error stack trace showing a ConcurrentModificationException that occurs when mutable objects are emitted as output tuples in Storm. This highlights the need for immutable objects when emitting to the output collector.

LANGUAGE: java
CODE:
java.lang.RuntimeException: java.util.ConcurrentModificationException
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:84)
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:55)
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:56)
	at org.apache.storm.disruptor$consume_loop_STAR_$fn__1597.invoke(disruptor.clj:67)
	at org.apache.storm.util$async_loop$fn__465.invoke(util.clj:377)
	at clojure.lang.AFn.run(AFn.java:24)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.util.ConcurrentModificationException
	at java.util.LinkedHashMap$LinkedHashIterator.nextEntry(LinkedHashMap.java:390)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:409)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:408)
	at java.util.HashMap.writeObject(HashMap.java:1016)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:959)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1480)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at org.apache.storm.serialization.SerializableSerializer.write(SerializableSerializer.java:21)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:554)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:77)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:472)
	at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:27)

----------------------------------------

TITLE: Creating a Bucketed Hive Table for Streaming (SQL)
DESCRIPTION: Example SQL command to create a bucketed table with ORC format, which is required for using the Hive Streaming API.

LANGUAGE: sql
CODE:
create table test_table ( id INT, name STRING, phone STRING, street STRING) partitioned by (city STRING, state STRING) stored as orc tblproperties ("orc.compress"="NONE");

----------------------------------------

TITLE: Installing ZeroMQ for Storm
DESCRIPTION: This snippet shows the process of downloading, extracting, configuring, and installing ZeroMQ 2.1.7, which is the recommended version for use with Storm.

LANGUAGE: bash
CODE:
wget http://download.zeromq.org/zeromq-2.1.7.tar.gz
tar -xzf zeromq-2.1.7.tar.gz
cd zeromq-2.1.7
./configure
make
sudo make install

----------------------------------------

TITLE: ZeroMQ Implementation for Distributed Mode in Apache Storm
DESCRIPTION: Implementation of the message sending protocol using ZeroMQ for distributed mode in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/zmq.clj

----------------------------------------

TITLE: Retrieving Topology Component Metrics
DESCRIPTION: Sample response showing detailed metrics for a topology component including spout/bolt statistics and executor information.

LANGUAGE: json
CODE:
{
    "name": "WordCount3",
    "id": "spout",
    "componentType": "spout",
    "executors": 5,
    "tasks": 5,
    "window": "600"
}

----------------------------------------

TITLE: Configuring Zookeeper Authentication in Storm YAML
DESCRIPTION: Storm YAML configuration for Zookeeper authentication

LANGUAGE: yaml
CODE:
storm.zookeeper.auth.scheme: "digest"
storm.zookeeper.auth.payload: "your-payload-here"

storm.zookeeper.topology.auth.scheme: "digest"
storm.zookeeper.topology.auth.payload: "your-topology-payload-here"

----------------------------------------

TITLE: Defining ComponentObject Structure in Thrift for Storm
DESCRIPTION: This Thrift structure defines the ComponentObject union, which is used to specify the code for spouts and bolts in Storm topologies. It allows for serialized Java objects, shell components, or Java objects.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Implementing SimpleHBaseMapper
DESCRIPTION: Example of configuring SimpleHBaseMapper for mapping tuple fields to HBase columns and counters.

LANGUAGE: java
CODE:
SimpleHBaseMapper mapper = new SimpleHBaseMapper() 
        .withRowKeyField("word")
        .withColumnFields(new Fields("word"))
        .withCounterFields(new Fields("count"))
        .withColumnFamily("cf");

----------------------------------------

TITLE: Installing ZeroMQ 2.1.7
DESCRIPTION: Commands to download, extract, and install ZeroMQ 2.1.7 from source. This is the recommended version for use with Storm.

LANGUAGE: bash
CODE:
wget http://download.zeromq.org/zeromq-2.1.7.tar.gz
tar -xzf zeromq-2.1.7.tar.gz
cd zeromq-2.1.7
./configure
make
sudo make install

----------------------------------------

TITLE: Configuring Artifactory Loader in YAML
DESCRIPTION: Example YAML configuration for using the ArtifactoryConfigLoader, specifying the URI and timeout.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "artifactory+http://artifactory.my.company.com:8000/artifactory/configurations/clusters/my_cluster/ras_pools"
scheduler.config.loader.timeout.sec: 30

----------------------------------------

TITLE: Message Sending Protocol in Apache Storm (Clojure)
DESCRIPTION: Definition of the protocol used for message sending in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/protocol.clj

----------------------------------------

TITLE: Implementing RecordToTupleMapper Interface in Java
DESCRIPTION: This code snippet shows the interface for RecordToTupleMapper, which is used to convert Kinesis records to Storm tuples. It includes methods for getting output fields and converting records to tuples.

LANGUAGE: java
CODE:
    Fields getOutputFields ();
    List<Object> getTuple (Record record);

----------------------------------------

TITLE: Implementing RecordToTupleMapper Interface in Java
DESCRIPTION: This code snippet shows the interface for RecordToTupleMapper, which is used to convert Kinesis records to Storm tuples. It includes methods for getting output fields and converting records to tuples.

LANGUAGE: java
CODE:
    Fields getOutputFields ();
    List<Object> getTuple (Record record);

----------------------------------------

TITLE: Implementing StateUpdater for Location Updates
DESCRIPTION: Example of a StateUpdater that updates the LocationDB with new location information in bulk.

LANGUAGE: java
CODE:
public class LocationUpdater extends BaseStateUpdater<LocationDB> {
    public void updateState(LocationDB state, List<TridentTuple> tuples, TridentCollector collector) {
        List<Long> ids = new ArrayList<Long>();
        List<String> locations = new ArrayList<String>();
        for(TridentTuple t: tuples) {
            ids.add(t.getLong(0));
            locations.add(t.getString(1));
        }
        state.setLocationsBulk(ids, locations);
    }
}

----------------------------------------

TITLE: Storm Log Command JSON Format
DESCRIPTION: JSON structure for logging messages from a shell process to the worker log through STDOUT. Contains command type and message text.

LANGUAGE: json
CODE:
{
    "command": "log",
    "msg": "hello world!"
}

----------------------------------------

TITLE: DRPC Server Configuration in YAML
DESCRIPTION: YAML configuration for setting up DRPC servers and transport settings.

LANGUAGE: yaml
CODE:
drpc.servers:
  - "drpc1.foo.com"
  - "drpc2.foo.com"
drpc.http.port: 8081
storm.thrift.transport: "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin"

----------------------------------------

TITLE: Storm Logviewer Metrics Table
DESCRIPTION: Markdown table showing Logviewer process metrics with names, types and descriptions

LANGUAGE: markdown
CODE:
| Metric Name | Type | Description |
|-------------|------|-------------|
| logviewer:cleanup-routine-duration-ms | timer | how long it takes to run... |

----------------------------------------

TITLE: Kerberos Server Authentication Configuration
DESCRIPTION: JAAS configuration for Pacemaker server Kerberos authentication.

LANGUAGE: java
CODE:
PacemakerServer {\n   com.sun.security.auth.module.Krb5LoginModule required\n   useKeyTab=true\n   keyTab="/etc/keytabs/pacemaker.keytab"\n   storeKey=true\n   useTicketCache=false\n   principal="pacemaker@MY.COMPANY.COM";\n};

----------------------------------------

TITLE: Storm Topology with KestrelSpout Configuration in Java
DESCRIPTION: Storm topology configuration that uses KestrelSpout to read sentences from a Kestrel queue. Sets up a processing pipeline for sentence splitting and word counting.

LANGUAGE: java
CODE:
    TopologyBuilder builder = new TopologyBuilder();
    builder.setSpout("sentences", new KestrelSpout("localhost",22133,"sentence_queue",new StringScheme()));
    builder.setBolt("split", new SplitSentence(), 10)
    	        .shuffleGrouping("sentences");
    builder.setBolt("count", new WordCount(), 20)
	        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Displaying jQuery and Sizzle.js License Text
DESCRIPTION: This snippet contains the full text of the MIT License for jQuery v3.6.1 and Sizzle.js. It outlines the permissions, conditions, and limitations for using, modifying, and distributing the software.

LANGUAGE: plaintext
CODE:
jQuery v 3.6.1
Copyright OpenJS Foundation and other contributors, https://openjsf.org/

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

******************************************

The jQuery JavaScript Library v3.6.1 also includes Sizzle.js

Sizzle.js includes the following license:

Copyright JS Foundation and other contributors, https://js.foundation/

This software consists of voluntary contributions made by many
individuals. For exact contribution history, see the revision history
available at https://github.com/jquery/sizzle

The following license applies to all parts of this software except as
documented below:

====

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

====

All files located in the node_modules and external directories are
externally maintained libraries used by this software which have their
own licenses; we recommend you read them, as their terms may differ from
the terms above.

----------------------------------------

TITLE: Initial Handshake Configuration JSON
DESCRIPTION: JSON structure showing the initial handshake configuration sent to shell components, including topology configuration, PID directory, and context information.

LANGUAGE: json
CODE:
{
    "conf": {
        "topology.message.timeout.secs": 3
    },
    "pidDir": "...",
    "context": {
        "task->component": {
            "1": "example-spout",
            "2": "__acker",
            "3": "example-bolt1",
            "4": "example-bolt2"
        },
        "taskid": 3,
        "componentid": "example-bolt",
        "stream->target->grouping": {
            "default": {
                "example-bolt2": {
                    "type": "SHUFFLE"}}},
        "streams": ["default"],
        "stream->outputfields": {"default": ["word"]},
        "source->stream->grouping": {
            "example-spout": {
                "default": {
                    "type": "FIELDS",
                    "fields": ["word"]
                }
            }
        },
        "source->stream->fields": {
            "example-spout": {
                "default": ["word"]
            }
        }
    }
}

----------------------------------------

TITLE: Maven Shade Plugin Configuration for Flux
DESCRIPTION: XML configuration for creating a fat jar with Flux dependencies using Maven shade plugin

LANGUAGE: xml
CODE:
<build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-shade-plugin</artifactId>
            <version>1.4</version>
            <configuration>
                <createDependencyReducedPom>true</createDependencyReducedPom>
            </configuration>
            <executions>
                <execution>
                    <phase>package</phase>
                    <goals>
                        <goal>shade</goal>
                    </goals>
                    <configuration>
                        <transformers>
                            <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                            <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                <mainClass>org.apache.storm.flux.Flux</mainClass>
                            </transformer>
                        </transformers>
                    </configuration>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>

----------------------------------------

TITLE: Implementing User-Defined Function in Java
DESCRIPTION: Example implementation of a scalar user-defined function in Java.

LANGUAGE: java
CODE:
public class MyPlus {
  public static Integer evaluate(Integer x, Integer y) {
    return x + y;
  }
}

----------------------------------------

TITLE: Bolt Message Transfer in Apache Storm (Clojure)
DESCRIPTION: Implementation of the transfer function for bolts to send tuples using the worker's transfer function.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L429

----------------------------------------

TITLE: Deploying a Flux Topology
DESCRIPTION: Example command to deploy a Flux topology using the storm jar command.

LANGUAGE: bash
CODE:
storm jar myTopology-0.1.0-SNAPSHOT.jar org.apache.storm.flux.Flux --local my_config.yaml

----------------------------------------

TITLE: Running ActiveMQ for Storm JMS Example
DESCRIPTION: Command to start Apache ActiveMQ, which is used as the JMS provider for the example topology. This should be run from the ActiveMQ installation directory.

LANGUAGE: bash
CODE:
$ [ACTIVEMQ_HOME]/bin/activemq

----------------------------------------

TITLE: Storm Topology Rebalancing Command
DESCRIPTION: CLI command example for rebalancing a running Storm topology, showing how to modify the number of worker processes and executors without restarting the cluster.

LANGUAGE: bash
CODE:
$ storm rebalance mytopology -n 5 -e blue-spout=3 -e yellow-bolt=10

----------------------------------------

TITLE: Building Storm JMS Example Topology with Maven
DESCRIPTION: Maven commands to build the Storm JMS example topology project. This compiles the source code and prepares the project for execution.

LANGUAGE: bash
CODE:
$ cd storm-jms
$ mvn clean install

----------------------------------------

TITLE: Configuring Storm Topology Parallelism (Java)
DESCRIPTION: This code snippet shows how to configure parallelism for a complete Storm topology. It sets up worker processes, spouts, and bolts with different parallelism settings, including executor and task configurations.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(2); // use two worker processes

topologyBuilder.setSpout("blue-spout", new BlueSpout(), 2); // set parallelism hint to 2

topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

topologyBuilder.setBolt("yellow-bolt", new YellowBolt(), 6)
               .shuffleGrouping("green-bolt");

StormSubmitter.submitTopology(
        "mytopology",
        conf,
        topologyBuilder.createTopology()
    );

----------------------------------------

TITLE: Configuring Kryo Serialization Registration in Storm YAML
DESCRIPTION: Example YAML configuration showing how to register custom types and serializers with Storm's Kryo serialization system. Demonstrates both default FieldsSerializer usage and custom serializer implementation.

LANGUAGE: yaml
CODE:
topology.kryo.register:
  - com.mycompany.CustomType1
  - com.mycompany.CustomType2: com.mycompany.serializer.CustomType2Serializer
  - com.mycompany.CustomType3

----------------------------------------

TITLE: Defining Storm Topology in Clojure
DESCRIPTION: Example of defining a Storm topology with spouts and bolts using the topology function. Shows how to wire components together and specify parallelism hints.

LANGUAGE: clojure
CODE:
(topology
 {"1" (spout-spec sentence-spout)
  "2" (spout-spec (sentence-spout-parameterized
                   ["the cat jumped over the door"
                    "greetings from a faraway land"])
                   :p 2)}
 {"3" (bolt-spec {"1" :shuffle "2" :shuffle}
                 split-sentence
                 :p 5)
  "4" (bolt-spec {"3" ["word"]}
                 word-count
                 :p 6)})

----------------------------------------

TITLE: Configuring Spring JMS Connection with ActiveMQ
DESCRIPTION: Spring XML configuration that sets up JMS connection factory and defines ActiveMQ queue and topic destinations. The configuration includes a connection factory pointing to localhost:61616 and defines both a notification queue and topic with specific physical names.

LANGUAGE: xml
CODE:
<?xml version="1.0" encoding="UTF-8"?>
<beans 
  xmlns="http://www.springframework.org/schema/beans" 
  xmlns:amq="http://activemq.apache.org/schema/core"
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.0.xsd
  http://activemq.apache.org/schema/core http://activemq.apache.org/schema/core/activemq-core.xsd">

    <amq:queue id="notificationQueue" physicalName="backtype.storm.contrib.example.queue" />
    
    <amq:topic id="notificationTopic" physicalName="backtype.storm.contrib.example.topic" />

    <amq:connectionFactory id="jmsConnectionFactory"
        brokerURL="tcp://localhost:61616" />
    
</beans>

----------------------------------------

TITLE: Implementing Custom Serializer Interface in Java for Storm
DESCRIPTION: This code snippet shows the ISerialization interface that must be implemented to create a custom serializer in Storm. It includes methods for accepting a class, serializing an object to a DataOutputStream, and deserializing from a DataInputStream.

LANGUAGE: java
CODE:
public interface ISerialization<T> {
    public boolean accept(Class c);
    public void serialize(T object, DataOutputStream stream) throws IOException;
    public T deserialize(DataInputStream stream) throws IOException;
}

----------------------------------------

TITLE: User-Defined Function for Timestamp Conversion
DESCRIPTION: Java implementation of a user-defined function to convert string timestamps to Unix timestamps.

LANGUAGE: java
CODE:
package org.apache.storm.sql.runtime.functions.scalar.datetime;

import org.joda.time.format.DateTimeFormat;
import org.joda.time.format.DateTimeFormatter;

public class GetTime2 {
    public static Long evaluate(String dateString, String dateFormat) {
        try {
            DateTimeFormatter df = DateTimeFormat.forPattern(dateFormat).withZoneUTC();
            return df.parseDateTime(dateString).getMillis();
        } catch (Exception ex) {
            throw new RuntimeException(ex);
        }
    }
}

----------------------------------------

TITLE: Registering Custom Serializers in Storm Topology Configuration
DESCRIPTION: This YAML snippet demonstrates how to register custom serializers for specific classes in a Storm topology configuration. It shows both default FieldsSerializer usage and custom serializer implementation.

LANGUAGE: yaml
CODE:
topology.kryo.register:
  - com.mycompany.CustomType1
  - com.mycompany.CustomType2: com.mycompany.serializer.CustomType2Serializer
  - com.mycompany.CustomType3

----------------------------------------

TITLE: Configuring EsConfig in Java
DESCRIPTION: Creates an EsConfig instance for configuring Elasticsearch connection. It can be initialized with cluster name and nodes, or with additional parameters for the Elasticsearch Transport Client.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});

LANGUAGE: java
CODE:
Map<String, String> additionalParameters = new HashMap<>();
additionalParameters.put("client.transport.sniff", "true");
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"}, additionalParameters);

----------------------------------------

TITLE: Configuring EsConfig in Java
DESCRIPTION: Creates an EsConfig instance for configuring Elasticsearch connection. It can be initialized with cluster name and nodes, or with additional parameters for the Elasticsearch Transport Client.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});

LANGUAGE: java
CODE:
Map<String, String> additionalParameters = new HashMap<>();
additionalParameters.put("client.transport.sniff", "true");
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"}, additionalParameters);

----------------------------------------

TITLE: Distributed Mode Message Receiving in Apache Storm (Clojure)
DESCRIPTION: This code snippet shows how workers listen on a TCP port (virtual port) for incoming messages and route them to tasks in distributed mode.

LANGUAGE: clojure
CODE:
(https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L204)

----------------------------------------

TITLE: Configuring Kerberos Authentication for Pacemaker Client in JAAS
DESCRIPTION: JAAS configuration for Kerberos authentication on Nimbus nodes to connect to Pacemaker.

LANGUAGE: java
CODE:
PacemakerClient {
    com.sun.security.auth.module.Krb5LoginModule required
    useKeyTab=true
    keyTab="/etc/keytabs/nimbus.keytab"
    storeKey=true
    useTicketCache=false
    serviceName="pacemaker"
    principal="nimbus@MY.COMPANY.COM";
};

----------------------------------------

TITLE: Creating a Bucketed Hive Table for Streaming (SQL)
DESCRIPTION: Example SQL query to create a bucketed Hive table with ORC format, which is required for using the Hive streaming API.

LANGUAGE: sql
CODE:
create table test_table ( id INT, name STRING, phone STRING, street STRING) partitioned by (city STRING, state STRING) stored as orc tblproperties ("orc.compress"="NONE");

----------------------------------------

TITLE: Implementing KafkaTopicSelector Interface in Java
DESCRIPTION: This code snippet shows the interface method required to implement KafkaTopicSelector for selecting the Kafka topic to publish a tuple to.

LANGUAGE: java
CODE:
public interface KafkaTopicSelector {
    String getTopics(Tuple/TridentTuple tuple);
}

----------------------------------------

TITLE: Storm Input Tuple JSON Format
DESCRIPTION: JSON structure representing an input tuple received by a shell process from Storm. Includes tuple ID, component ID, stream ID, task ID, and tuple values.

LANGUAGE: json
CODE:
{
    "id": -6955786537413359385,
    "comp": 1,
    "stream": 1,
    "task": 9,
    "tuple": ["snow white and the seven dwarfs", "field2", 3]
}

----------------------------------------

TITLE: Storm Protocol Fail Command Format
DESCRIPTION: JSON structure for indicating tuple processing failure in Storm's multilang protocol. Contains command type and tuple ID.

LANGUAGE: json
CODE:
{
    "command": "fail",
    "id": 123123
}

----------------------------------------

TITLE: Event Hubs Configuration Properties
DESCRIPTION: Configuration properties for connecting Storm spout to Azure Event Hubs, including authentication, namespace settings, and performance tuning parameters

LANGUAGE: properties
CODE:
eventhubspout.username = [username: policy name in EventHubs Portal]
eventhubspout.password = [password: shared access key in EventHubs Portal]
eventhubspout.namespace = [namespace]
eventhubspout.entitypath = [entitypath]
eventhubspout.partitions.count = [partitioncount]

# if not provided, will use storm's zookeeper settings
# zookeeper.connectionstring=zookeeper0:2181,zookeeper1:2181,zookeeper2:2181

eventhubspout.checkpoint.interval = 10
eventhub.receiver.credits = 1024

----------------------------------------

TITLE: Configuring KafkaBolt in Storm Topology
DESCRIPTION: Example showing how to configure and use KafkaBolt in a Storm topology to write data to Kafka. Includes producer properties configuration and topology builder setup.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();

Fields fields = new Fields("key", "message");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
            new Values("storm", "1"),
            new Values("trident", "1"),
            new Values("needs", "1"),
            new Values("javadoc", "1")
);
spout.setCycle(true);
builder.setSpout("spout", spout, 5);
//set producer properties.
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

KafkaBolt bolt = new KafkaBolt()
        .withProducerProperties(props)
        .withTopicSelector(new DefaultTopicSelector("test"))
        .withTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper());
builder.setBolt("forwardToKafka", bolt, 8).shuffleGrouping("spout");

Config conf = new Config();

StormSubmitter.submitTopology("kafkaboltTest", conf, builder.createTopology());

----------------------------------------

TITLE: Implementing Reach Calculation with Trident DRPC in Java
DESCRIPTION: Demonstrates a pure DRPC topology that computes the reach of a URL on demand. It queries two databases, processes the data, and calculates the unique count of followers for a given URL.

LANGUAGE: java
CODE:
TridentState urlToTweeters =
       topology.newStaticState(getUrlToTweetersState());
TridentState tweetersToFollowers =
       topology.newStaticState(getTweeterToFollowersState());

topology.newDRPCStream("reach")
       .stateQuery(urlToTweeters, new Fields("args"), new MapGet(), new Fields("tweeters"))
       .each(new Fields("tweeters"), new ExpandList(), new Fields("tweeter"))
       .shuffle()
       .stateQuery(tweetersToFollowers, new Fields("tweeter"), new MapGet(), new Fields("followers"))
       .parallelismHint(200)
       .each(new Fields("followers"), new ExpandList(), new Fields("follower"))
       .groupBy(new Fields("follower"))
       .aggregate(new One(), new Fields("one"))
       .parallelismHint(20)
       .aggregate(new Count(), new Fields("reach"));

----------------------------------------

TITLE: Launching Storm DRPC Client in Java
DESCRIPTION: Example of creating and using a DRPC client in Java. This snippet demonstrates how to configure and execute a DRPC request.

LANGUAGE: java
CODE:
Config conf = new Config();
try (DRPCClient drpc = DRPCClient.getConfiguredClient(conf)) {
  //User the drpc client
  String result = drpc.execute(function, argument);
}

----------------------------------------

TITLE: Configuring Kerberos Authentication for Pacemaker Server
DESCRIPTION: JAAS configuration for Kerberos authentication on Pacemaker nodes.

LANGUAGE: java
CODE:
PacemakerServer {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/etc/keytabs/pacemaker.keytab"
   storeKey=true
   useTicketCache=false
   principal="pacemaker@MY.COMPANY.COM";
};

----------------------------------------

TITLE: Configuring Storm Project Structure in Eclipse
DESCRIPTION: Project structure setup showing key directories and classpath requirements for Storm development. Includes configuration for both Java and multilang implementations.

LANGUAGE: plaintext
CODE:
project/
  src/jvm/           # Java source files
  lib/              # Storm dependency jars
  lib/dev/          # Development dependency jars
  multilang/        # Non-Java implementations
    resources/      # Resource files for multilang

----------------------------------------

TITLE: jQuery and Sizzle.js MIT License Text
DESCRIPTION: Complete license documentation for jQuery v3.6.1 and Sizzle.js, including copyright notices, permissions, conditions, warranty disclaimers, and liability limitations under MIT license terms.

LANGUAGE: text
CODE:
jQuery v 3.6.1
Copyright OpenJS Foundation and other contributors, https://openjsf.org/

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

******************************************

The jQuery JavaScript Library v3.6.1 also includes Sizzle.js

Sizzle.js includes the following license:

Copyright JS Foundation and other contributors, https://js.foundation/

This software consists of voluntary contributions made by many
individuals. For exact contribution history, see the revision history
available at https://github.com/jquery/sizzle

The following license applies to all parts of this software except as
documented below:

====

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

====

All files located in the node_modules and external directories are
externally maintained libraries used by this software which have their
own licenses; we recommend you read them, as their terms may differ from
the terms above.

*********************

----------------------------------------

TITLE: Using Trident API for Querying Data from Cassandra
DESCRIPTION: Shows how to use the Trident API to query data from Cassandra using the state API.

LANGUAGE: java
CODE:
CassandraState.Options options = new CassandraState.Options(new CassandraContext());
CQLStatementTupleMapper insertTemperatureValues = boundQuery("SELECT name FROM weather.station WHERE id = ?")
         .bind(with(field("weather_station_id").as("id")));
options.withCQLStatementTupleMapper(insertTemperatureValues);
options.withCQLResultSetValuesMapper(new TridentResultSetValuesMapper(new Fields("name")));
CassandraStateFactory selectWeatherStationStateFactory =  new CassandraStateFactory(options);
CassandraStateFactory selectWeatherStationStateFactory = getSelectWeatherStationStateFactory();
TridentState selectState = topology.newStaticState(selectWeatherStationStateFactory);
stream = stream.stateQuery(selectState, new Fields("weather_station_id"), new CassandraQuery(), new Fields("name"));

----------------------------------------

TITLE: User-Defined Function for Timestamp Conversion
DESCRIPTION: Java implementation of a user-defined function to convert string timestamps to Unix timestamps.

LANGUAGE: java
CODE:
package org.apache.storm.sql.runtime.functions.scalar.datetime;

import org.joda.time.format.DateTimeFormat;
import org.joda.time.format.DateTimeFormatter;

public class GetTime2 {
    public static Long evaluate(String dateString, String dateFormat) {
        try {
            DateTimeFormatter df = DateTimeFormat.forPattern(dateFormat).withZoneUTC();
            return df.parseDateTime(dateString).getMillis();
        } catch (Exception ex) {
            throw new RuntimeException(ex);
        }
    }
}

----------------------------------------

TITLE: Creating External Kafka Table in Storm SQL
DESCRIPTION: Example of creating an external table that specifies a Kafka spout and sink with serialization properties.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://localhost:2181/brokers?topic=test' TBLPROPERTIES '{"producer":{"bootstrap.servers":"localhost:9092","acks":"1","key.serializer":"org.apache.org.apache.storm.kafka.IntSerializer","value.serializer":"org.apache.org.apache.storm.kafka.ByteBufferSerializer"}}'

----------------------------------------

TITLE: Maven Assembly Plugin Configuration for Storm Topology JAR
DESCRIPTION: Maven configuration to package a Storm topology and its dependencies into a single JAR file, excluding Storm's core JARs which are provided by the cluster.

LANGUAGE: xml
CODE:
  <plugin>
    <artifactId>maven-assembly-plugin</artifactId>
    <configuration>
      <descriptorRefs>  
        <descriptorRef>jar-with-dependencies</descriptorRef>
      </descriptorRefs>
      <archive>
        <manifest>
          <mainClass>com.path.to.main.Class</mainClass>
        </manifest>
      </archive>
    </configuration>
  </plugin>

----------------------------------------

TITLE: Configuring and Executing DRPC Client in Java
DESCRIPTION: Example showing how to configure and execute a DRPC client to compute results for a 'reach' function.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.put("storm.thrift.transport", "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin");
conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 3);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL, 10);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING, 20);
DRPCClient client = new DRPCClient(conf, "drpc-host", 3772);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Installing JZMQ from Fork
DESCRIPTION: Commands to clone and install JZMQ (Java bindings for ZeroMQ) from a specific fork that is tested to work with Storm. The installation process includes cloning the repository, generating configuration files, building and installing the package.

LANGUAGE: bash
CODE:
#install jzmq
git clone https://github.com/nathanmarz/jzmq.git
cd jzmq
./autogen.sh
./configure
make
sudo make install

----------------------------------------

TITLE: Configuring Spring JMS Connection with ActiveMQ
DESCRIPTION: Spring XML configuration that defines ActiveMQ queue, topic, and connection factory beans. Sets up a notification queue and topic with specific physical names, and configures a JMS connection factory pointing to localhost.

LANGUAGE: xml
CODE:
<?xml version="1.0" encoding="UTF-8"?>
<beans 
  xmlns="http://www.springframework.org/schema/beans" 
  xmlns:amq="http://activemq.apache.org/schema/core"
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.0.xsd
  http://activemq.apache.org/schema/core http://activemq.apache.org/schema/core/activemq-core.xsd">

    <amq:queue id="notificationQueue" physicalName="backtype.storm.contrib.example.queue" />
    
    <amq:topic id="notificationTopic" physicalName="backtype.storm.contrib.example.topic" />

    <amq:connectionFactory id="jmsConnectionFactory"
        brokerURL="tcp://localhost:61616" />
    
</beans>

----------------------------------------

TITLE: Configuring SSL/TLS for MQTT in Java
DESCRIPTION: Java code to configure SSL/TLS for MQTT connections using DefaultKeyStoreLoader.

LANGUAGE: java
CODE:
DefaultKeyStoreLoader ksl = new DefaultKeyStoreLoader("/path/to/keystore.jks", "/path/to/truststore.jks");
ksl.setKeyStorePassword("password");
ksl.setTrustStorePassword("password");
//...

----------------------------------------

TITLE: Storm Ack Command JSON Format
DESCRIPTION: JSON structure for acknowledging processed tuples in Storm's multilang protocol.

LANGUAGE: json
CODE:
{
    "command": "ack",
    "id": 123123
}

----------------------------------------

TITLE: Defining a Spout in Clojure
DESCRIPTION: Example of defining a spout that emits random sentences using the defspout macro.

LANGUAGE: clojure
CODE:
(defspout sentence-spout ["sentence"]
  [conf context collector]
  (let [sentences ["a little brown dog"
                   "the man petted the dog"
                   "four score and seven years ago"
                   "an apple a day keeps the doctor away"]]
    (spout
     (nextTuple []
       (Thread/sleep 100)
       (emit-spout! collector [(rand-nth sentences)])         
       )
     (ack [id]
        ;; You only need to define this method for reliable spouts
        ;; (such as one that reads off of a queue like Kestrel)
        ;; This is an unreliable spout, so it does nothing here
        ))))

----------------------------------------

TITLE: Running Storm JMS Example Topology
DESCRIPTION: Command to execute the example topology using Maven from the examples directory.

LANGUAGE: bash
CODE:
$ mvn exec:java

----------------------------------------

TITLE: Storm Emit Command JSON Structure
DESCRIPTION: JSON structure for emitting tuples from a multilang bolt. Specifies anchors, stream ID, target task, and tuple values.

LANGUAGE: json
CODE:
{
    "command": "emit",
    "anchors": [1231231, -234234234],
    "stream": 1,
    "task": 9,
    "tuple": ["field1", 2, 3]
}

----------------------------------------

TITLE: Registering Custom Serializers in Storm Topology Configuration (YAML)
DESCRIPTION: This YAML snippet demonstrates how to register custom serializers for specific classes in a Storm topology configuration. It shows both automatic serialization using FieldsSerializer and custom serializer implementation.

LANGUAGE: yaml
CODE:
topology.kryo.register:
  - com.mycompany.CustomType1
  - com.mycompany.CustomType2: com.mycompany.serializer.CustomType2Serializer
  - com.mycompany.CustomType3

----------------------------------------

TITLE: ZeroMQ-based Message Transfer in Apache Storm (Clojure)
DESCRIPTION: This implementation uses ZeroMQ for message transfer in distributed mode of Apache Storm.

LANGUAGE: clojure
CODE:
(deftype ZMQTransfer [])

----------------------------------------

TITLE: ZeroMQ-based Message Transfer in Apache Storm (Clojure)
DESCRIPTION: This implementation uses ZeroMQ for message transfer in distributed mode of Apache Storm.

LANGUAGE: clojure
CODE:
(deftype ZMQTransfer [])

----------------------------------------

TITLE: Creating ACLs for Blobs in Java
DESCRIPTION: Creates Access Control Lists (ACLs) to be used when creating blobs in Storm's blobstore

LANGUAGE: java
CODE:
String stringBlobACL = "u:username:rwa";
AccessControl blobACL = BlobStoreAclHandler.parseAccessControl(stringBlobACL);
List<AccessControl> acls = new LinkedList<AccessControl>();
acls.add(blobACL); // more ACLs can be added here
SettableBlobMeta settableBlobMeta = new SettableBlobMeta(acls);
settableBlobMeta.set_replication_factor(4); // Here we can set the replication factor

----------------------------------------

TITLE: Defining TupleToKafkaMapper and TridentTupleToKafkaMapper Interfaces in Java
DESCRIPTION: Interface definitions for mapping Storm tuples to Kafka keys and messages.

LANGUAGE: java
CODE:
K getKeyFromTuple(Tuple/TridentTuple tuple);
V getMessageFromTuple(Tuple/TridentTuple tuple);

----------------------------------------

TITLE: Creating Kerberos Principals and Keytabs
DESCRIPTION: Bash commands for creating Kerberos principals and keytabs for Zookeeper, Nimbus, DRPC, UI and Supervisor nodes

LANGUAGE: bash
CODE:
# Zookeeper (Will need one of these for each box in the Zk ensemble)
sudo kadmin.local -q 'addprinc zookeeper/zk1.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/zk.keytab  zookeeper/zk1.example.com@STORM.EXAMPLE.COM"
# Nimbus and DRPC
sudo kadmin.local -q 'addprinc storm/storm.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm/storm.example.com@STORM.EXAMPLE.COM"
# All UI logviewer and Supervisors
sudo kadmin.local -q 'addprinc storm@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm@STORM.EXAMPLE.COM"

----------------------------------------

TITLE: Direct Tuple Sending in Local Mode for Apache Storm (Clojure)
DESCRIPTION: In local mode, tuples are sent directly to an in-memory queue for the receiving task.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/local.clj#L21

----------------------------------------

TITLE: Installing ZeroMQ 2.1.7
DESCRIPTION: Commands to download, extract and install ZeroMQ version 2.1.7, which is the recommended version for Storm. This process includes configuring and building ZeroMQ from source.

LANGUAGE: bash
CODE:
wget http://download.zeromq.org/zeromq-2.1.7.tar.gz
tar -xzf zeromq-2.1.7.tar.gz
cd zeromq-2.1.7
./configure
make
sudo make install

----------------------------------------

TITLE: Configuring KafkaSpout in a Storm Topology
DESCRIPTION: Example of how to configure and use KafkaSpout to read data from Kafka into a Storm topology. It sets up a KafkaSpout using KafkaSpoutConfig and connects it to a bolt.

LANGUAGE: java
CODE:
final TopologyBuilder tp = new TopologyBuilder();
tp.setSpout("kafka_spout", new KafkaSpout<>(KafkaSpoutConfig.builder("127.0.0.1:" + port, "topic").build()), 1);
tp.setBolt("bolt", new myBolt()).shuffleGrouping("kafka_spout");

----------------------------------------

TITLE: Defining ComponentObject Struct in Thrift for Storm
DESCRIPTION: This Thrift struct defines the ComponentObject union, which is used to specify the code for spouts and bolts in Storm topologies. It allows for serialized Java objects, shell components, or Java objects.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Implementing DRPC Query for Word Counts in Trident
DESCRIPTION: Demonstrates how to set up a distributed query to get the sum of counts for a list of words using Trident's DRPC functionality.

LANGUAGE: java
CODE:
topology.newDRPCStream("words")
       .each(new Fields("args"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .stateQuery(wordCounts, new Fields("word"), new MapGet(), new Fields("count"))
       .each(new Fields("count"), new FilterNull())
       .aggregate(new Fields("count"), new Sum(), new Fields("sum"));

----------------------------------------

TITLE: Python Script for Parsing Apache Logs to JSON
DESCRIPTION: Python script that parses Apache log entries and converts them to JSON format with auto-incrementing IDs.

LANGUAGE: python
CODE:
import sys
import apache_log_parser
import json

auto_incr_id = 1
parser_format = '%a - - %t %D "%r" %s %b "%{Referer}i" "%{User-Agent}i"'
line_parser = apache_log_parser.make_parser(parser_format)
while True:
  # we'll use pipe
  line = sys.stdin.readline()
  if not line:
    break
  parsed_dict = line_parser(line)
  parsed_dict['id'] = auto_incr_id
  auto_incr_id += 1

  parsed_dict = {k.upper(): v for k, v in parsed_dict.iteritems() if not k.endswith('datetimeobj')}
  print(json.dumps(parsed_dict))

----------------------------------------

TITLE: Digest Authentication JAAS Configuration
DESCRIPTION: JAAS configuration structure for Pacemaker digest authentication.

LANGUAGE: java
CODE:
PacemakerDigest {\n    username="some username"\n    password="some password";\n};

----------------------------------------

TITLE: Continuous Item Addition to Kestrel Queue in Java
DESCRIPTION: Main program to continuously add sentence items to a Kestrel queue named 'sentence_queue' on a local server. It runs until a closing bracket ']' is entered in the console.

LANGUAGE: java
CODE:
    import java.io.IOException;
    import java.io.InputStream;
    import java.util.Random;

    import org.apache.storm.spout.KestrelClient;
    import org.apache.storm.spout.KestrelClient.Item;
    import org.apache.storm.spout.KestrelClient.ParseError;

    public class AddSentenceItemsToKestrel {

    	/**
    	 * @param args
    	 */
    	public static void main(String[] args) {

    		InputStream is = System.in;

			char closing_bracket = ']';

			int val = closing_bracket;

			boolean aux = true;

			try {

				KestrelClient kestrelClient = null;
				String queueName = "sentence_queue";

				while(aux){

					kestrelClient = new KestrelClient("localhost",22133);

					queueSentenceItems(kestrelClient, queueName);

					kestrelClient.close();

					Thread.sleep(1000);

					if(is.available()>0){
					 if(val==is.read())
						 aux=false;
					}
				}
			} catch (IOException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			catch (ParseError e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			} catch (InterruptedException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}

			System.out.println("end");

	    }
	}

----------------------------------------

TITLE: HTML Redirect to Storm Development Setup Documentation
DESCRIPTION: HTML meta refresh directive and canonical link that redirects users to the current Storm development environment setup documentation page.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Setting-up-development-environment.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Setting-up-development-environment.html" />

----------------------------------------

TITLE: Running Event Hub Send Client
DESCRIPTION: Command to run the included test client for sending messages to Event Hubs

LANGUAGE: bash
CODE:
java -cp .\target\eventhubs-storm-spout-{version}-jar-with-dependencies.jar com.microsoft.eventhubs.client.EventHubSendClient [username] [password] [entityPath] [partitionId] [messageSize] [messageCount]

----------------------------------------

TITLE: Configuring Spring XML for Storm JMS Integration
DESCRIPTION: Spring applicationContext.xml configuration that defines JMS destinations (queue and topic) and an ActiveMQ connection factory. Sets up connection to a local ActiveMQ broker and establishes message destinations for Storm integration.

LANGUAGE: xml
CODE:
<?xml version="1.0" encoding="UTF-8"?>
<beans 
  xmlns="http://www.springframework.org/schema/beans" 
  xmlns:amq="http://activemq.apache.org/schema/core"
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.0.xsd
  http://activemq.apache.org/schema/core http://activemq.apache.org/schema/core/activemq-core.xsd">

    <amq:queue id="notificationQueue" physicalName="backtype.storm.contrib.example.queue" />
    
    <amq:topic id="notificationTopic" physicalName="backtype.storm.contrib.example.topic" />

    <amq:connectionFactory id="jmsConnectionFactory"
        brokerURL="tcp://localhost:61616" />
    
</beans>

----------------------------------------

TITLE: Logging Messages in Storm Multi-Language Protocol (JSON)
DESCRIPTION: This JSON structure demonstrates how to log messages in the Storm multi-language protocol. It includes the command type and the message to be logged in the worker log.

LANGUAGE: json
CODE:
{
    "command": "log",
    "msg": "hello world!"
}

----------------------------------------

TITLE: Shutting Down Storm LocalCluster in Java
DESCRIPTION: Demonstrates how to properly shutdown a local Storm cluster when testing is complete. This ensures cleanup of all resources used by the in-process cluster.

LANGUAGE: java
CODE:
cluster.shutdown();

----------------------------------------

TITLE: Modifying Fake Apache Log Generator Script
DESCRIPTION: Python code snippet to modify the Fake-Apache-Log-Generator script to include elapsed time in microseconds.

LANGUAGE: python
CODE:
elapsed_us = random.randint(1 * 1000,1000 * 1000) # 1 ms to 1 sec
seconds=random.randint(30,300)
increment = datetime.timedelta(seconds=seconds)
otime += increment

ip = faker.ipv4()
dt = otime.strftime('%d/%b/%Y:%H:%M:%S')
tz = datetime.datetime.now(pytz.timezone('US/Pacific')).strftime('%z')
vrb = numpy.random.choice(verb,p=[0.6,0.1,0.1,0.2])

uri = random.choice(resources)
if uri.find("apps")>0:
        uri += `random.randint(1000,10000)`

resp = numpy.random.choice(response,p=[0.9,0.04,0.02,0.04])
byt = int(random.gauss(5000,50))
referer = faker.uri()
useragent = numpy.random.choice(ualist,p=[0.5,0.3,0.1,0.05,0.05] )()
f.write('%s - - [%s %s] %s "%s %s HTTP/1.0" %s %s "%s" "%s"\n' % (ip,dt,tz,elapsed_us,vrb,uri,resp,byt,referer,useragent))

log_lines = log_lines - 1
flag = False if log_lines == 0 else True

----------------------------------------

TITLE: Registering Metrics Consumer in YAML
DESCRIPTION: Example showing how to register metrics consumers in the storm.yaml config file, including configuration for multiple consumers with arguments.

LANGUAGE: yaml
CODE:
topology.metrics.consumer.register:
  - class: "org.apache.storm.metric.LoggingMetricsConsumer"
    parallelism.hint: 1
  - class: "org.apache.storm.metric.HttpForwardingMetricsConsumer"
    parallelism.hint: 1
    argument: "http://example.com:8080/metrics/my-topology/"

----------------------------------------

TITLE: Storm JMS Debug Output
DESCRIPTION: Example debug output showing the flow of messages through the JMS topology, including spout and bolt processing, message acknowledgment, and transaction handling.

LANGUAGE: log
CODE:
DEBUG (backtype.storm.contrib.jms.bolt.JmsBolt:183) - Connecting JMS..
DEBUG (backtype.storm.contrib.jms.spout.JmsSpout:213) - sending tuple: ActiveMQTextMessage {commandId = 5, responseRequired = true, messageId = ID:budreau.home-51286-1321074044423-2:4:1:1:1, originalDestination = null, originalTransactionId = null, producerId = ID:budreau.home-51286-1321074044423-2:4:1:1, destination = queue://backtype.storm.contrib.example.queue, transactionId = null, expiration = 0, timestamp = 1321735055910, arrival = 0, brokerInTime = 1321735055910, brokerOutTime = 1321735055921, correlationId = , replyTo = null, persistent = true, type = , priority = 0, groupID = null, groupSequence = 0, targetConsumerId = null, compressed = false, userID = null, content = null, marshalledProperties = org.apache.activemq.util.ByteSequence@6c27ca12, dataStructure = null, redeliveryCounter = 0, size = 0, properties = {secret=880412b7-de71-45dd-8a80-8132589ccd22}, readOnlyProperties = true, readOnlyBody = true, droppable = false, text = Hello storm-jms!}

----------------------------------------

TITLE: Implementing Custom Redis Bolt for Word Count Lookup
DESCRIPTION: Java class extending AbstractRedisBolt to implement custom logic for looking up word counts in Redis. This example demonstrates how to use AbstractRedisBolt for scenarios not covered by the pre-implemented bolts.

LANGUAGE: java
CODE:
public static class LookupWordTotalCountBolt extends AbstractRedisBolt {
    private static final Logger LOG = LoggerFactory.getLogger(LookupWordTotalCountBolt.class);
    private static final Random RANDOM = new Random();

    public LookupWordTotalCountBolt(JedisPoolConfig config) {
        super(config);
    }

    public LookupWordTotalCountBolt(JedisClusterConfig config) {
        super(config);
    }

    @Override
    public void execute(Tuple input) {
        JedisCommands jedisCommands = null;
        try {
            jedisCommands = getInstance();
            String wordName = input.getStringByField("word");
            String countStr = jedisCommands.get(wordName);
            if (countStr != null) {
                int count = Integer.parseInt(countStr);
                this.collector.emit(new Values(wordName, count));

                // print lookup result with low probability
                if(RANDOM.nextInt(1000) > 995) {
                    LOG.info("Lookup result - word : " + wordName + " / count : " + count);
                }
            } else {
                // skip
                LOG.warn("Word not found in Redis - word : " + wordName);
            }
        } finally {
            if (jedisCommands != null) {
                returnInstance(jedisCommands);
            }
            this.collector.ack(input);
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        // wordName, count
        declarer.declare(new Fields("wordName", "count"));
    }
}

----------------------------------------

TITLE: JSON Structure for Incoming Tuple in Storm Multi-Language Protocol
DESCRIPTION: This JSON structure represents an incoming tuple in the Storm multi-language protocol. It includes the tuple's id, component id, stream id, task id, and the actual tuple values.

LANGUAGE: JSON
CODE:
{
    "id": -6955786537413359385,
    "comp": 1,
    "stream": 1,
    "task": 9,
    "tuple": ["snow white and the seven dwarfs", "field2", 3]
}

----------------------------------------

TITLE: Implementing StateUpdater for Bulk Updates
DESCRIPTION: This snippet shows how to implement a StateUpdater to perform bulk updates on the LocationDB state, efficiently updating multiple user locations at once.

LANGUAGE: java
CODE:
public class LocationUpdater extends BaseStateUpdater<LocationDB> {
    public void updateState(LocationDB state, List<TridentTuple> tuples, TridentCollector collector) {
        List<Long> ids = new ArrayList<Long>();
        List<String> locations = new ArrayList<String>();
        for(TridentTuple t: tuples) {
            ids.add(t.getLong(0));
            locations.add(t.getString(1));
        }
        state.setLocationsBulk(ids, locations);
    }
}

----------------------------------------

TITLE: Implementing a Spout in Clojure
DESCRIPTION: Example of a spout implementation that emits random sentences, demonstrating the defspout macro and spout functionality.

LANGUAGE: clojure
CODE:
(defspout sentence-spout ["sentence"]
  [conf context collector]
  (let [sentences ["a little brown dog"
                   "the man petted the dog"
                   "four score and seven years ago"
                   "an apple a day keeps the doctor away"]]
    (spout
     (nextTuple []
       (Thread/sleep 100)
       (emit-spout! collector [(rand-nth sentences)])         
       )
     (ack [id]
        ;; You only need to define this method for reliable spouts
        ;; (such as one that reads off of a queue like Kestrel)
        ;; This is an unreliable spout, so it does nothing here
        ))))

----------------------------------------

TITLE: Implementing a Spout in Clojure
DESCRIPTION: Example of a spout implementation that emits random sentences, demonstrating the defspout macro and spout functionality.

LANGUAGE: clojure
CODE:
(defspout sentence-spout ["sentence"]
  [conf context collector]
  (let [sentences ["a little brown dog"
                   "the man petted the dog"
                   "four score and seven years ago"
                   "an apple a day keeps the doctor away"]]
    (spout
     (nextTuple []
       (Thread/sleep 100)
       (emit-spout! collector [(rand-nth sentences)])         
       )
     (ack [id]
        ;; You only need to define this method for reliable spouts
        ;; (such as one that reads off of a queue like Kestrel)
        ;; This is an unreliable spout, so it does nothing here
        ))))

----------------------------------------

TITLE: Storm Thrift Component Structure
DESCRIPTION: The core Thrift definition for Storm components including ComponentObject for implementation and ComponentCommon for configuration. Defines how spouts and bolts are structured and configured.

LANGUAGE: markdown
CODE:
1. ComponentObject (implementation):\n- Serialized Java object (IBolt)\n- ShellComponent for non-JVM languages\n- JavaObject for class instantiation\n\n2. ComponentCommon (configuration):\n- Stream emissions and metadata\n- Stream consumption mappings\n- Parallelism settings\n- Component-specific configuration

----------------------------------------

TITLE: Custom Redis Bolt Implementation
DESCRIPTION: Example of extending AbstractRedisBolt for custom Redis operations, demonstrating word count total lookup.

LANGUAGE: java
CODE:
public static class LookupWordTotalCountBolt extends AbstractRedisBolt {
    private static final Logger LOG = LoggerFactory.getLogger(LookupWordTotalCountBolt.class);
    private static final Random RANDOM = new Random();

    public LookupWordTotalCountBolt(JedisPoolConfig config) {
        super(config);
    }

    public LookupWordTotalCountBolt(JedisClusterConfig config) {
        super(config);
    }

    @Override
    public void execute(Tuple input) {
        JedisCommands jedisCommands = null;
        try {
            jedisCommands = getInstance();
            String wordName = input.getStringByField("word");
            String countStr = jedisCommands.get(wordName);
            if (countStr != null) {
                int count = Integer.parseInt(countStr);
                this.collector.emit(new Values(wordName, count));

                if(RANDOM.nextInt(1000) > 995) {
                    LOG.info("Lookup result - word : " + wordName + " / count : " + count);
                }
            } else {
                LOG.warn("Word not found in Redis - word : " + wordName);
            }
        } finally {
            if (jedisCommands != null) {
                returnInstance(jedisCommands);
            }
            this.collector.ack(input);
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("wordName", "count"));
    }
}

----------------------------------------

TITLE: Implementing a FlatMap Function in Trident
DESCRIPTION: Example of a FlatMap Function that splits sentences into words.

LANGUAGE: java
CODE:
public class Split extends FlatMapFunction {
  @Override
  public Iterable<Values> execute(TridentTuple input) {
    List<Values> valuesList = new ArrayList<>();
    for (String word : input.getString(0).split(" ")) {
      valuesList.add(new Values(word));
    }
    return valuesList;
  }
}

----------------------------------------

TITLE: Building the Storm JMS Example Topology
DESCRIPTION: Maven commands to build the Storm JMS example topology project.

LANGUAGE: bash
CODE:
$ cd storm-jms
$ mvn clean install

----------------------------------------

TITLE: Configuring Health Check Directory in Storm YAML
DESCRIPTION: This code snippet demonstrates how to set the directory for health check scripts in the Storm configuration file.

LANGUAGE: yaml
CODE:
storm.health.check.dir: "healthchecks"

----------------------------------------

TITLE: Acknowledging Tuple in JSON Format for Storm Multi-Language Protocol
DESCRIPTION: This JSON structure represents an ack command sent by a shell process in Storm's multi-language protocol. It includes the command type and the id of the tuple being acknowledged.

LANGUAGE: JSON
CODE:
{
    "command": "ack",
    "id": 123123
}

----------------------------------------

TITLE: Configuring Nimbus Seeds in Storm YAML
DESCRIPTION: This snippet shows how to configure the Nimbus seeds, which are the candidate master machines that worker nodes need to know for downloading topology jars and configs.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["111.222.333.44"]

----------------------------------------

TITLE: Shell Bolt Specification in Clojure
DESCRIPTION: Example of defining a shell bolt that runs a Python script, showing input declarations and output specifications.

LANGUAGE: clojure
CODE:
(shell-bolt-spec {"1" :shuffle "2" ["id"]}
                 "python3"
                 "mybolt.py"
                 ["outfield1" "outfield2"]
                 :p 25)

----------------------------------------

TITLE: Implementing Split Function for Word Processing
DESCRIPTION: Defines a Split function that takes a sentence and emits individual words as separate tuples.

LANGUAGE: java
CODE:
public class Split extends BaseFunction {
   public void execute(TridentTuple tuple, TridentCollector collector) {
       String sentence = tuple.getString(0);
       for(String word: sentence.split(" ")) {
           collector.emit(new Values(word));                
       }
   }
}

----------------------------------------

TITLE: Configuring Kerberos Authentication in Java
DESCRIPTION: Configuration setup for secure HBase access using Kerberos authentication.

LANGUAGE: java
CODE:
Config config = new Config();
...
config.put("storm.keytab.file", "$keytab");
config.put("storm.kerberos.principal", "$principle");
StormSubmitter.submitTopology("$topologyName", config, builder.createTopology());

----------------------------------------

TITLE: Starting Pacemaker Daemon
DESCRIPTION: This command demonstrates how to start the Pacemaker daemon using the Storm command-line interface.

LANGUAGE: bash
CODE:
$ storm pacemaker

----------------------------------------

TITLE: SQL Basic Grammar Definition
DESCRIPTION: BNF-style grammar definition for basic SQL statements including SELECT, INSERT, UPDATE, DELETE and various clauses.

LANGUAGE: sql
CODE:
statement:
      setStatement
  |   resetStatement
  |   explain
  |   describe
  |   insert
  |   update
  |   merge
  |   delete
  |   query

----------------------------------------

TITLE: Configuring HTML Meta Refresh and Canonical Link
DESCRIPTION: Sets up an automatic page redirect to Apache Storm's main documentation page using meta refresh tag with 0 second delay. Also includes a canonical link tag to indicate the preferred URL.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=https://storm.apache.org/index.html">
<link rel="canonical" href="https://storm.apache.org/index.html" />

----------------------------------------

TITLE: Debugging Kryo ConcurrentModificationException in Java
DESCRIPTION: This stack trace demonstrates a runtime error caused by emitting mutable objects as output tuples in Storm. The issue occurs when a bolt modifies an object while it's being serialized for network transmission.

LANGUAGE: java
CODE:
java.lang.RuntimeException: java.util.ConcurrentModificationException
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:84)
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:55)
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:56)
	at org.apache.storm.disruptor$consume_loop_STAR_$fn__1597.invoke(disruptor.clj:67)
	at org.apache.storm.util$async_loop$fn__465.invoke(util.clj:377)
	at clojure.lang.AFn.run(AFn.java:24)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.util.ConcurrentModificationException
	at java.util.LinkedHashMap$LinkedHashIterator.nextEntry(LinkedHashMap.java:390)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:409)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:408)
	at java.util.HashMap.writeObject(HashMap.java:1016)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:959)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1480)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at org.apache.storm.serialization.SerializableSerializer.write(SerializableSerializer.java:21)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:554)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:77)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:472)
	at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:27)

----------------------------------------

TITLE: Storm Spout Interface Definition
DESCRIPTION: Core interface definition for Storm spouts showing the essential methods for message processing and reliability.

LANGUAGE: java
CODE:
public interface ISpout extends Serializable {
    void open(Map conf, TopologyContext context, SpoutOutputCollector collector);
    void close();
    void nextTuple();
    void ack(Object msgId);
    void fail(Object msgId);
}

----------------------------------------

TITLE: Defining Flux Components in YAML
DESCRIPTION: Example of defining named object instances (components) in Flux YAML that can be referenced by spouts and bolts.

LANGUAGE: yaml
CODE:
components:
  - id: "stringScheme"
    className: "org.apache.storm.kafka.StringScheme"

  - id: "stringMultiScheme"
    className: "org.apache.storm.spout.SchemeAsMultiScheme"
    constructorArgs:
      - ref: "stringScheme" # component with id "stringScheme" must be declared above.

----------------------------------------

TITLE: Storm WordCount Example
DESCRIPTION: Reference to WordCountTopology.java as a test implementation for verifying Storm setup.

LANGUAGE: java
CODE:
WordCountTopology.java

----------------------------------------

TITLE: Implementing Complex DRPC Topology for URL Reach Calculation in Java
DESCRIPTION: This code defines a more complex DRPC topology for computing the reach of a URL on Twitter using LinearDRPCTopologyBuilder.

LANGUAGE: java
CODE:
LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("reach");
builder.addBolt(new GetTweeters(), 3);
builder.addBolt(new GetFollowers(), 12)
        .shuffleGrouping();
builder.addBolt(new PartialUniquer(), 6)
        .fieldsGrouping(new Fields("id", "follower"));
builder.addBolt(new CountAggregator(), 2)
        .fieldsGrouping(new Fields("id"));

----------------------------------------

TITLE: Defining MongoDB Mapper Interface - Java
DESCRIPTION: Interface definition for converting Storm tuples to MongoDB documents. Includes methods for basic document conversion and key-based document creation.

LANGUAGE: java
CODE:
public interface MongoMapper extends Serializable {
    Document toDocument(ITuple tuple);
    Document toDocumentByKeys(List<Object> keys);
}

----------------------------------------

TITLE: Configuring Mongo Trident MapState for Storm-MongoDB Integration
DESCRIPTION: Example of how to configure and use Mongo Trident MapState in a Trident topology for key-value state management.

LANGUAGE: java
CODE:
MongoMapper mapper = new SimpleMongoMapper()
        .withFields("word", "count");

QueryFilterCreator filterCreator = new SimpleQueryFilterCreator()
        .withField("word");

MongoMapState.Options options = new MongoMapState.Options();
options.url = url;
options.collectionName = collectionName;
options.mapper = mapper;
options.queryCreator = filterCreator;

StateFactory factory = MongoMapState.transactional(options);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

TridentState state = stream.groupBy(new Fields("word"))
        .persistentAggregate(factory, new Fields("count"), new Sum(), new Fields("sum"));

stream.stateQuery(state, new Fields("word"), new MapGet(), new Fields("sum"))
        .each(new Fields("word", "sum"), new PrintFunction(), new Fields());

----------------------------------------

TITLE: Installing ZeroMQ for Storm
DESCRIPTION: This snippet shows the process of downloading, extracting, configuring, and installing ZeroMQ 2.1.7, which is the recommended version for use with Storm. It includes commands for downloading the source, extracting it, and running the installation process.

LANGUAGE: bash
CODE:
wget http://download.zeromq.org/zeromq-2.1.7.tar.gz
tar -xzf zeromq-2.1.7.tar.gz
cd zeromq-2.1.7
./configure
make
sudo make install

----------------------------------------

TITLE: Submitting Storm Topology to Cluster
DESCRIPTION: Example showing how to configure and submit a Storm topology to a production cluster using StormSubmitter. Sets the number of workers and maximum pending spout configurations.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(20);
conf.setMaxSpoutPending(5000);
StormSubmitter.submitTopology("mytopology", conf, topology);

----------------------------------------

TITLE: Configuring JdbcLookupBolt in Java
DESCRIPTION: Example of configuring JdbcLookupBolt with connection provider, lookup mapper, and select query. It demonstrates setting up the bolt for database lookups with a specified query timeout.

LANGUAGE: java
CODE:
String selectSql = "select user_name from user_details where user_id = ?";
SimpleJdbcLookupMapper lookupMapper = new SimpleJdbcLookupMapper(outputFields, queryParamColumns)
JdbcLookupBolt userNameLookupBolt = new JdbcLookupBolt(connectionProvider, selectSql, lookupMapper)
        .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Using StateUpdater in Trident Topology
DESCRIPTION: Example of using a custom StateUpdater to persist location updates in a Trident topology.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();
TridentState locations = 
    topology.newStream("locations", locationsSpout)
        .partitionPersist(new LocationDBFactory(), new Fields("userid", "location"), new LocationUpdater())

----------------------------------------

TITLE: Storm Fail Command JSON Format
DESCRIPTION: JSON structure for marking tuples as failed in Storm's multilang protocol.

LANGUAGE: json
CODE:
{
    "command": "fail",
    "id": 123123
}

----------------------------------------

TITLE: Implementing Word Count Bolt
DESCRIPTION: Prepared bolt implementation that maintains state to count word occurrences in a stream.

LANGUAGE: clojure
CODE:
(defbolt word-count ["word" "count"] {:prepare true}
  [conf context collector]
  (let [counts (atom {})]
    (bolt
     (execute [tuple]
       (let [word (.getString tuple 0)]
         (swap! counts (partial merge-with +) {word 1})
         (emit-bolt! collector [word (@counts word)] :anchor tuple)
         (ack! collector tuple)
         )))))

----------------------------------------

TITLE: Receiving Tuple Format in Storm Multilang Protocol
DESCRIPTION: JSON structure representing an incoming tuple in Storm's multilang protocol. Contains tuple ID, component ID, stream ID, task ID, and the actual tuple values.

LANGUAGE: json
CODE:
{
    "id": -6955786537413359385,
    "comp": 1,
    "stream": 1,
    "task": 9,
    "tuple": ["snow white and the seven dwarfs", "field2", 3]
}

----------------------------------------

TITLE: RecordToTupleMapper Interface Definition in Java
DESCRIPTION: Interface definition for converting Kinesis records to Storm tuples. Includes two methods: getOutputFields for defining tuple structure and getTuple for actual record conversion.

LANGUAGE: java
CODE:
    Fields getOutputFields ();
    List<Object> getTuple (Record record);

----------------------------------------

TITLE: Updating ClusterSummary Thrift Structure
DESCRIPTION: Modifies the ClusterSummary Thrift structure to include information about Nimbus instances, adding a list of NimbusSummary objects to support multiple Nimbus nodes.

LANGUAGE: thrift
CODE:
struct ClusterSummary {
  1: required list<SupervisorSummary> supervisors;
  3: required list<TopologySummary> topologies;
  4: required list<NimbusSummary> nimbuses;
}

struct NimbusSummary {
  1: required string host;
  2: required i32 port;
  3: required i32 uptime_secs;
  4: required bool isLeader;
  5: required string version;
}

----------------------------------------

TITLE: Debugging Kryo ConcurrentModificationException in Java
DESCRIPTION: This stack trace demonstrates a runtime error caused by emitting mutable objects as output tuples in Apache Storm. The error occurs during serialization when the bolt modifies an object being sent over the network.

LANGUAGE: java
CODE:
java.lang.RuntimeException: java.util.ConcurrentModificationException
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:84)
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:55)
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:56)
	at org.apache.storm.disruptor$consume_loop_STAR_$fn__1597.invoke(disruptor.clj:67)
	at org.apache.storm.util$async_loop$fn__465.invoke(util.clj:377)
	at clojure.lang.AFn.run(AFn.java:24)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.util.ConcurrentModificationException
	at java.util.LinkedHashMap$LinkedHashIterator.nextEntry(LinkedHashMap.java:390)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:409)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:408)
	at java.util.HashMap.writeObject(HashMap.java:1016)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:959)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1480)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at org.apache.storm.serialization.SerializableSerializer.write(SerializableSerializer.java:21)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:554)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:77)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:472)
	at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:27)

----------------------------------------

TITLE: Aggregation Operations in Storm's Stream API (Java)
DESCRIPTION: Demonstrates various aggregation operations like sum, count, and reduce on streams.

LANGUAGE: java
CODE:
Stream<Long> numbers = â€¦
// aggregate the numbers and produce a stream of last 10 sec sums.
Stream<Long> sums = numbers.window(TumblingWindows.of(Duration.seconds(10))).aggregate(new Sum());

// the last 10 sec sums computed using reduce
Stream<Long> sums = numbers.window(...).reduce((x, y) -> x + y);

Stream<String> words = ...                                              // a windowed stream of words
Stream<String, Long> wordCounts = words.mapToPair(w -> Pair.of(w,1)     // convert to a stream of (word, 1) pairs
                                       .aggregateByKey(new Count<>());  // compute counts per word

----------------------------------------

TITLE: Using Classpath Wildcards in Java 6+
DESCRIPTION: Illustrates the use of classpath wildcards introduced in Java 6, which allows Storm to shorten classpath specifications and avoid process command length limitations.

LANGUAGE: java
CODE:
java -cp "foo/bar/*:other/path/*" com.example.MainClass

----------------------------------------

TITLE: Configuring Kerberos Authentication in Storm YAML
DESCRIPTION: YAML configuration for enabling Kerberos authentication in Storm

LANGUAGE: yaml
CODE:
storm.thrift.transport: "org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin"
java.security.auth.login.config: "/path/to/jaas.conf"

nimbus.childopts: "-Xmx1024m -Djava.security.auth.login.config=/path/to/jaas.conf"
ui.childopts: "-Xmx768m -Djava.security.auth.login.config=/path/to/jaas.conf"
supervisor.childopts: "-Xmx256m -Djava.security.auth.login.config=/path/to/jaas.conf"

----------------------------------------

TITLE: Implementing Split Sentence Bolt in Clojure
DESCRIPTION: Simple bolt implementation that splits input sentences into individual words and emits them as separate tuples.

LANGUAGE: clojure
CODE:
(defbolt split-sentence ["word"] [tuple collector]
  (let [words (.split (.getString tuple 0) " ")]
    (doseq [w words]
      (emit-bolt! collector [w] :anchor tuple))
    (ack! collector tuple)
    ))

----------------------------------------

TITLE: Using Aggregate Operation in Trident
DESCRIPTION: Example of using aggregate to get a global count for a batch.

LANGUAGE: java
CODE:
mystream.aggregate(new Count(), new Fields("count"))

----------------------------------------

TITLE: Configuring Resource Usage Profiling in Java
DESCRIPTION: Java configuration to enable resource usage profiling in a Storm topology.

LANGUAGE: java
CODE:
//Log all storm metrics
conf.registerMetricsConsumer(backtype.storm.metric.LoggingMetricsConsumer.class);

//Add in per worker CPU measurement
Map<String, String> workerMetrics = new HashMap<String, String>();
workerMetrics.put("CPU", "org.apache.storm.metrics.sigar.CPUMetric");
conf.put(Config.TOPOLOGY_WORKER_METRICS, workerMetrics);

----------------------------------------

TITLE: Adding Items to Kestrel Queue in Java
DESCRIPTION: Method to add random sentences to a Kestrel queue using KestrelClient. It selects sentences from a predefined array and queues them with unique IDs.

LANGUAGE: java
CODE:
    private static void queueSentenceItems(KestrelClient kestrelClient, String queueName)
			throws ParseError, IOException {

		String[] sentences = new String[] {
	            "the cow jumped over the moon",
	            "an apple a day keeps the doctor away",
	            "four score and seven years ago",
	            "snow white and the seven dwarfs",
	            "i am at two with nature"};

		Random _rand = new Random();

		for(int i=1; i<=10; i++){

			String sentence = sentences[_rand.nextInt(sentences.length)];

			String val = "ID " + i + " " + sentence;

			boolean queueSucess = kestrelClient.queue(queueName, val);

			System.out.println("queueSucess=" +queueSucess+ " [" + val +"]");
		}
	}

----------------------------------------

TITLE: Setting Topology Priority in Java
DESCRIPTION: Java API call to set the priority of a Storm topology.

LANGUAGE: java
CODE:
conf.setTopologyPriority(int priority)

----------------------------------------

TITLE: Using Memcached for Persistent Aggregation in Trident
DESCRIPTION: Shows how to modify the word count topology to use Memcached for persistent storage of word counts instead of in-memory storage.

LANGUAGE: java
CODE:
.persistentAggregate(MemcachedState.transactional(serverLocations), new Count(), new Fields("count"))        
MemcachedState.transactional()

----------------------------------------

TITLE: Adding Columns to ColumnList in Java
DESCRIPTION: Examples of how to add standard and counter columns to a ColumnList object, which defines what will be written to an HBase row.

LANGUAGE: java
CODE:
ColumnList cols = new ColumnList();
cols.addColumn(this.columnFamily, field.getBytes(), toBytes(tuple.getValueByField(field)));

LANGUAGE: java
CODE:
ColumnList cols = new ColumnList();
cols.addCounter(this.columnFamily, field.getBytes(), toLong(tuple.getValueByField(field)));

----------------------------------------

TITLE: Using HBaseWindowStoreFactory for Windowing
DESCRIPTION: Demonstrates how to use HBaseWindowStoreFactory for storing window state in HBase.

LANGUAGE: java
CODE:
HBaseWindowsStoreFactory windowStoreFactory = new HBaseWindowsStoreFactory(new HashMap<String, Object>(), "window-state", "cf".getBytes("UTF-8"), "tuples".getBytes("UTF-8"));
FixedBatchSpout spout = new FixedBatchSpout(new Fields("sentence"), 3, new Values("the cow jumped over the moon"),
        new Values("the man went to the store and bought some candy"), new Values("four score and seven years ago"),
        new Values("how many apples can you eat"), new Values("to be or not to be the person"));
spout.setCycle(true);

TridentTopology topology = new TridentTopology();

Stream stream = topology.newStream("spout1", spout).parallelismHint(16).each(new Fields("sentence"),
        new Split(), new Fields("word"))
        .window(TumblingCountWindow.of(1000), windowStoreFactory, new Fields("word"), new CountAsAggregator(), new Fields("count"))
        .peek(new Consumer() {
            @Override
            public void accept(TridentTuple input) {
                LOG.info("Received tuple: [{}]", input);
            }
        });

StormTopology stormTopology =  topology.build();

----------------------------------------

TITLE: Using Storm Shell Command for Topology Submission
DESCRIPTION: Example of using the 'storm shell' command to package resources, upload a jar to Nimbus, and execute a Python topology script. This command simplifies the process of submitting a topology implemented in a non-JVM language.

LANGUAGE: bash
CODE:
storm shell resources/ python topology.py arg1 arg2

----------------------------------------

TITLE: Storm Configuration Property Example (YAML)
DESCRIPTION: Example of configuration properties that can be overridden at the component level in Storm, including debug settings, spout pending limits, parallelism, and Kryo registration.

LANGUAGE: yaml
CODE:
topology.debug: true
topology.max.spout.pending: 100
topology.max.task.parallelism: 10
topology.kryo.register:
  - serialization.class

----------------------------------------

TITLE: Implementing a Bolt in Java using BaseRichBolt
DESCRIPTION: A more concise implementation of a bolt in Java using the BaseRichBolt class. It demonstrates how to simplify bolt code by extending a base class that provides default implementations.

LANGUAGE: java
CODE:
public static class ExclamationBolt extends BaseRichBolt {
    OutputCollector _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple tuple) {
        _collector.emit(tuple, new Values(tuple.getString(0) + "!!!"));
        _collector.ack(tuple);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word"));
    }    
}

----------------------------------------

TITLE: Configuring MQTT Topology with Flux YAML
DESCRIPTION: Flux YAML configuration for creating a Storm topology with an MQTT spout and logging bolt.

LANGUAGE: yaml
CODE:
name: "mqtt-topology"

components:
   ########## MQTT Spout Config ############
  - id: "mqtt-type"
    className: "org.apache.storm.mqtt.examples.CustomMessageMapper"

  - id: "mqtt-options"
    className: "org.apache.storm.mqtt.common.MqttOptions"
    properties:
      - name: "url"
        value: "tcp://localhost:1883"
      - name: "topics"
        value:
          - "/users/tgoetz/#"

# topology configuration
config:
  topology.workers: 1
  topology.max.spout.pending: 1000

# spout definitions
spouts:
  - id: "mqtt-spout"
    className: "org.apache.storm.mqtt.spout.MqttSpout"
    constructorArgs:
      - ref: "mqtt-type"
      - ref: "mqtt-options"
    parallelism: 1

# bolt definitions
bolts:
  - id: "log"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1


streams:
  - from: "mqtt-spout"
    to: "log"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Uploading Storm Topology Jar
DESCRIPTION: Code showing how StormSubmitter uploads the topology jar to Nimbus via Thrift interface. The upload process is done in chunks of 15KB.

LANGUAGE: thrift
CODE:
string beginFileUpload()
uploadChunk(string location, binary chunk)
void finishFileUpload(string location)

----------------------------------------

TITLE: Submitting Storm Topology with Configuration in Java
DESCRIPTION: Demonstrates how to use StormSubmitter to submit a topology to the cluster with specific configurations for number of workers and maximum pending spouts.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(20);
conf.setMaxSpoutPending(5000);
StormSubmitter.submitTopology("mytopology", conf, topology);

----------------------------------------

TITLE: Configuring RedisFilterBolt in Java
DESCRIPTION: This snippet shows how to configure and create a RedisFilterBolt using a JedisPoolConfig and a RedisFilterMapper. It sets up the connection to Redis and defines how to filter data.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisFilterMapper filterMapper = new BlacklistWordFilterMapper();
RedisFilterBolt filterBolt = new RedisFilterBolt(poolConfig, filterMapper);

----------------------------------------

TITLE: Implementing DRPC Stream for Word Count Queries in Trident
DESCRIPTION: Shows how to implement a DRPC stream in a Trident topology to handle queries for word counts. This stream processes incoming requests, queries the persistent state, and aggregates the results.

LANGUAGE: java
CODE:
topology.newDRPCStream("words")
       .each(new Fields("args"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .stateQuery(wordCounts, new Fields("word"), new MapGet(), new Fields("count"))
       .each(new Fields("count"), new FilterNull())
       .aggregate(new Fields("count"), new Sum(), new Fields("sum"));

----------------------------------------

TITLE: Invalid Join Order Example
DESCRIPTION: Example showing invalid forward referencing of stream names in JoinBolt configuration.

LANGUAGE: java
CODE:
new JoinBolt( "spout1", "key1")                 
  .join     ( "spout2", "userId",  "spout3") //not allowed. spout3 not yet introduced
  .join     ( "spout3", "key3",    "spout1")

----------------------------------------

TITLE: Initializing HBase State Provider in Java
DESCRIPTION: Example of initializing the HBase state provider configuration in Storm topology code, including HBase connection details and state provider settings.

LANGUAGE: java
CODE:
Config conf = new Config();
    Map<String, Object> hbConf = new HashMap<String, Object>();
    hbConf.put("hbase.rootdir", "file:///tmp/hbase");
    conf.put("hbase.conf", hbConf);
    conf.put("topology.state.provider",  "org.apache.storm.hbase.state.HBaseKeyValueStateProvider");
    conf.put("topology.state.provider.config", "{" +
            "   \"hbaseConfigKey\": \"hbase.conf\"," +
            "   \"tableName\": \"state\"," +
            "   \"columnFamily\": \"cf\"" +
            " }");

----------------------------------------

TITLE: Configuring Maven Shade Plugin for Storm Topology Packaging
DESCRIPTION: This XML configuration for the Maven Shade Plugin ensures proper packaging of a Storm topology JAR, including merging of JAR manifest entries for Hadoop client URL scheme resolution.

LANGUAGE: xml
CODE:
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-shade-plugin</artifactId>
    <version>1.4</version>
    <configuration>
        <createDependencyReducedPom>true</createDependencyReducedPom>
    </configuration>
    <executions>
        <execution>
            <phase>package</phase>
            <goals>
                <goal>shade</goal>
            </goals>
            <configuration>
                <transformers>
                    <transformer
                            implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                    <transformer
                            implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                        <mainClass></mainClass>
                    </transformer>
                </transformers>
            </configuration>
        </execution>
    </executions>
</plugin>

----------------------------------------

TITLE: Declaring Multiple Streams in Storm Spouts
DESCRIPTION: Example of how to declare multiple streams in a Storm spout using the OutputFieldsDeclarer and emit to specific streams using SpoutOutputCollector.

LANGUAGE: java
CODE:
declareStream(OutputFieldsDeclarer declarer) {
    declarer.declareStream("stream1", new Fields("field1", "field2"));
    declarer.declareStream("stream2", new Fields("field3", "field4"));
}

nextTuple() {
    // Emit to stream1
    collector.emit("stream1", new Values(value1, value2));
    // Emit to stream2
    collector.emit("stream2", new Values(value3, value4));
}

----------------------------------------

TITLE: Defining SQL Grammar in BNF-like Form
DESCRIPTION: Defines the SQL grammar supported by Storm SQL in a BNF-like form, covering statements, expressions, queries, and more.

LANGUAGE: sql
CODE:
statement:
      setStatement
  |   resetStatement
  |   explain
  |   describe
  |   insert
  |   update
  |   merge
  |   delete
  |   query

setStatement:
      [ ALTER ( SYSTEM | SESSION ) ] SET identifier '=' expression

resetStatement:
      [ ALTER ( SYSTEM | SESSION ) ] RESET identifier
  |   [ ALTER ( SYSTEM | SESSION ) ] RESET ALL

explain:
      EXPLAIN PLAN
      [ WITH TYPE | WITH IMPLEMENTATION | WITHOUT IMPLEMENTATION ]
      [ EXCLUDING ATTRIBUTES | INCLUDING [ ALL ] ATTRIBUTES ]
      FOR ( query | insert | update | merge | delete )

// ... additional grammar definitions ...

----------------------------------------

TITLE: Adding Counter Column
DESCRIPTION: Example of adding a counter column to a ColumnList for HBase operations

LANGUAGE: java
CODE:
ColumnList cols = new ColumnList();
cols.addCounter(this.columnFamily, field.getBytes(), toLong(tuple.getValueByField(field)));

----------------------------------------

TITLE: Worker Connection Management in Storm
DESCRIPTION: Code showing how workers refresh connections to other workers and maintain task mappings

LANGUAGE: clojure
CODE:
(refresh-connections)

----------------------------------------

TITLE: Defining a Simple Bolt in Clojure
DESCRIPTION: Example of defining a simple bolt that splits sentences into words using the defbolt macro.

LANGUAGE: clojure
CODE:
(defbolt split-sentence ["word"] [tuple collector]
  (let [words (.split (.getString tuple 0) " ")]
    (doseq [w words]
      (emit-bolt! collector [w] :anchor tuple))
    (ack! collector tuple)
    ))

----------------------------------------

TITLE: Storm Topology Submission API Definition
DESCRIPTION: Thrift API definition for submitting a topology to Storm. Includes parameters for topology name, jar location, configuration, and topology structure, along with possible exceptions.

LANGUAGE: java
CODE:
void submitTopology(
        1: string name,
        2: string uploadedJarLocation,
        3: string jsonConf,
        4: StormTopology topology)
        throws (
                1: AlreadyAliveException e,
                2: InvalidTopologyException ite);

----------------------------------------

TITLE: Configuring Isolation Scheduler in YAML
DESCRIPTION: This YAML configuration snippet demonstrates how to allocate isolated machines to specific topologies using the Isolation Scheduler in Storm. It maps topology names to the number of dedicated machines they should receive.

LANGUAGE: yaml
CODE:
isolation.scheduler.machines: 
    "my-topology": 8
    "tiny-topology": 1
    "some-other-topology": 3

----------------------------------------

TITLE: Python Script for Parsing Apache Logs to JSON
DESCRIPTION: Python script that parses Apache log entries and converts them to JSON format with auto-incrementing IDs.

LANGUAGE: python
CODE:
import sys
import apache_log_parser
import json

auto_incr_id = 1
parser_format = '%a - - %t %D "%r" %s %b "%{Referer}i" "%{User-Agent}i"'
line_parser = apache_log_parser.make_parser(parser_format)
while True:
  line = sys.stdin.readline()
  if not line:
    break
  parsed_dict = line_parser(line)
  parsed_dict['id'] = auto_incr_id
  auto_incr_id += 1

  parsed_dict = {k.upper(): v for k, v in parsed_dict.iteritems() if not k.endswith('datetimeobj')}
  print(json.dumps(parsed_dict))

----------------------------------------

TITLE: Configuring Nimbus Seeds in Storm YAML
DESCRIPTION: This snippet sets up the nimbus.seeds configuration, which specifies the machines that are candidates for the master node. It's used by worker nodes to download topology jars and configurations.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["111.222.333.44"]

----------------------------------------

TITLE: Simple Sentence Splitting Bolt in Storm
DESCRIPTION: Implementation of a basic bolt that splits sentences into words, demonstrating tuple processing and emission.

LANGUAGE: clojure
CODE:
(defbolt split-sentence ["word"] [tuple collector]
  (let [words (.split (.getString tuple 0) " ")]
    (doseq [w words]
      (emit-bolt! collector [w] :anchor tuple))
    (ack! collector tuple)
    ))

----------------------------------------

TITLE: Registering Worker Hook in Java Storm Topology
DESCRIPTION: Illustrates how to register a custom worker-level hook using the TopologyBuilder. These hooks are called during worker startup, before any bolts or spouts are prepared or opened.

LANGUAGE: java
CODE:
TopologyBuilder.addWorkerHook

----------------------------------------

TITLE: Configuring RedisClusterState for Trident Topology in Java
DESCRIPTION: This code demonstrates how to set up RedisClusterState for a Trident topology. It configures the Redis cluster connection, creates mappers for storing and looking up data, and sets up the topology to use Redis Cluster for state management.

LANGUAGE: java
CODE:
Set<InetSocketAddress> nodes = new HashSet<InetSocketAddress>();
for (String hostPort : redisHostPort.split(",")) {
    String[] host_port = hostPort.split(":");
    nodes.add(new InetSocketAddress(host_port[0], Integer.valueOf(host_port[1])));
}
JedisClusterConfig clusterConfig = new JedisClusterConfig.Builder().setNodes(nodes)
                                .build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisLookupMapper lookupMapper = new WordCountLookupMapper();
RedisClusterState.Factory factory = new RedisClusterState.Factory(clusterConfig);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

stream.partitionPersist(factory,
                        fields,
                        new RedisClusterStateUpdater(storeMapper).withExpire(86400000),
                        new Fields());

TridentState state = topology.newStaticState(factory);
stream = stream.stateQuery(state, new Fields("word"),
                        new RedisClusterStateQuerier(lookupMapper),
                        new Fields("columnName","columnValue"));

----------------------------------------

TITLE: Python Script Execution Format
DESCRIPTION: Format showing how Storm shell executes the Python topology script with additional parameters for Nimbus connection and jar location.

LANGUAGE: bash
CODE:
python3 topology.py arg1 arg2 {nimbus-host} {nimbus-port} {uploaded-jar-location}

----------------------------------------

TITLE: Storm Emit Command JSON Structure
DESCRIPTION: JSON format for emit commands used by shell components to emit tuples, including optional fields for stream ID, task targeting, and tuple values.

LANGUAGE: json
CODE:
{
    "command": "emit",
    "id": "1231231",
    "stream": "1",
    "task": 9,
    "tuple": ["field1", 2, 3]
}

----------------------------------------

TITLE: Registering User-Defined Metrics in Apache Storm
DESCRIPTION: Example of registering a custom counter metric in a Storm topology using the TopologyContext. This demonstrates how to create and use a named counter that will be uniquely identified across the cluster.

LANGUAGE: java
CODE:
Counter myCounter = topologyContext.registerCounter("myCounter");

----------------------------------------

TITLE: Creating MQTT Topology with Storm Core Java API
DESCRIPTION: Java code to create an MQTT topology using the Storm Core API, equivalent to the Flux YAML configuration.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
MqttOptions options = new MqttOptions();
options.setTopics(Arrays.asList("/users/tgoetz/#"));
options.setCleanConnection(false);
MqttSpout spout = new MqttSpout(new StringMessageMapper(), options);

MqttBolt bolt = new LogInfoBolt();

builder.setSpout("mqtt-spout", spout);
builder.setBolt("log-bolt", bolt).shuffleGrouping("mqtt-spout");

return builder.createTopology();

----------------------------------------

TITLE: jQuery and Sizzle.js License Text
DESCRIPTION: The MIT license text for jQuery v3.6.1 and Sizzle.js, including copyright notices, permissions, conditions, and warranty disclaimers. Covers both the main jQuery library and the included Sizzle.js component.

LANGUAGE: text
CODE:
jQuery v 3.6.1
Copyright OpenJS Foundation and other contributors, https://openjsf.org/

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

******************************************

The jQuery JavaScript Library v3.6.1 also includes Sizzle.js

Sizzle.js includes the following license:

Copyright JS Foundation and other contributors, https://js.foundation/

This software consists of voluntary contributions made by many
individuals. For exact contribution history, see the revision history
available at https://github.com/jquery/sizzle

The following license applies to all parts of this software except as
documented below:

====

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

====

All files located in the node_modules and external directories are
externally maintained libraries used by this software which have their
own licenses; we recommend you read them, as their terms may differ from
the terms above.

*********************

----------------------------------------

TITLE: Storm Shell Script Execution Format
DESCRIPTION: Format showing how Storm shell executes the topology script with additional parameters including Nimbus host, port, and jar location. These parameters are appended to the user-provided arguments.

LANGUAGE: bash
CODE:
python topology.py arg1 arg2 {nimbus-host} {nimbus-port} {uploaded-jar-location}

----------------------------------------

TITLE: Adding Storm-Redis Dependency in Maven
DESCRIPTION: XML snippet showing how to add the storm-redis dependency to a Maven project.

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>storm-redis</artifactId>
    <version>${storm.version}</version>
    <type>jar</type>
</dependency>

----------------------------------------

TITLE: Configuring Memcached State Storage in Trident
DESCRIPTION: Shows how to configure Trident to store state in Memcached instead of in-memory storage.

LANGUAGE: java
CODE:
.persistentAggregate(MemcachedState.transactional(serverLocations), new Count(), new Fields("count"))        
MemcachedState.transactional()

----------------------------------------

TITLE: Joining Streams in Trident
DESCRIPTION: Example of joining two Trident streams based on specific fields.

LANGUAGE: java
CODE:
topology.join(stream1, new Fields("key"), stream2, new Fields("x"), new Fields("key", "a", "b", "c"));

----------------------------------------

TITLE: Listing Storm DSLs and Multi-Language Adapters in Markdown
DESCRIPTION: This markdown snippet lists various DSLs and multi-language adapters for Apache Storm, including links to their respective repositories or documentation pages.

LANGUAGE: markdown
CODE:
* [Clojure DSL](Clojure-DSL.html)
* [Scala DSL](https://github.com/velvia/ScalaStorm)
* [JRuby DSL](https://github.com/colinsurprenant/redstorm)
* [Storm/Esper integration](https://github.com/tomdz/storm-esper): Streaming SQL on top of Storm
* [io-storm](https://github.com/dan-blanchard/io-storm): Perl multilang adapter
* [FsShelter](https://github.com/Prolucid/FsShelter): F# DSL and runtime with protobuf multilang

----------------------------------------

TITLE: Storm Protocol Tuple Input Format
DESCRIPTION: JSON structure representing an input tuple in Storm's multilang protocol. Contains tuple ID, component ID, stream ID, task ID, and tuple values.

LANGUAGE: json
CODE:
{
    "id": -6955786537413359385,
    "comp": 1,
    "stream": 1,
    "task": 9,
    "tuple": ["snow white and the seven dwarfs", "field2", 3]
}

----------------------------------------

TITLE: Spout Message Listening in Apache Storm (Clojure)
DESCRIPTION: This code snippet shows how spouts listen for incoming messages in Storm.

LANGUAGE: Clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L382

----------------------------------------

TITLE: Adding Storm Client Dependency in Maven POM
DESCRIPTION: XML configuration to add Storm as a development dependency in a Maven project's pom.xml file. The scope is set to 'provided' since Storm will be available on the cluster at runtime.

LANGUAGE: xml
CODE:
<dependency>
  <groupId>org.apache.storm</groupId>
  <artifactId>storm-client</artifactId>
  <version>{{page.version}}</version>
  <scope>provided</scope>
</dependency>

----------------------------------------

TITLE: Transferring Tuples in Apache Storm Worker (Clojure)
DESCRIPTION: This code defines the transfer function used by tasks to send tuples to other tasks. It serializes the tuple and adds it to a transfer queue.

LANGUAGE: clojure
CODE:
(https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L56)

----------------------------------------

TITLE: Defining Code Distribution Interface in Java for Storm Nimbus
DESCRIPTION: Interface for distributing code across the Storm cluster, including methods for uploading, downloading, checking replication count, and cleanup.

LANGUAGE: java
CODE:
public interface ICodeDistributor {
    void prepare(Map conf);
    File upload(Path dirPath, String topologyId);
    List<File> download(Path destDirPath, String topologyid, File metafile);
    int getReplicationCount(String topologyId);
    void cleanup(String topologyid);
    void close(Map conf);
}

----------------------------------------

TITLE: Deploying Storm Topology Command
DESCRIPTION: Command line example showing how to deploy a Storm topology jar with arguments.

LANGUAGE: bash
CODE:
storm jar all-my-code.jar org.apache.storm.MyTopology arg1 arg2

----------------------------------------

TITLE: Killing a Topology in Storm (Clojure)
DESCRIPTION: The kill process involves changing the topology status to 'killed' and scheduling a 'remove' event after a wait time. This allows for graceful shutdown of processing.

LANGUAGE: clojure
CODE:
kill-transition

----------------------------------------

TITLE: Executing Python Topology Script with Storm Shell
DESCRIPTION: This command shows how Storm shell executes the Python topology script after uploading the jar. It passes additional arguments including Nimbus host, port, and the uploaded jar location.

LANGUAGE: bash
CODE:
python topology.py arg1 arg2 {nimbus-host} {nimbus-port} {uploaded-jar-location}

----------------------------------------

TITLE: Worker Launcher Configuration Example
DESCRIPTION: Example configuration for the worker-launcher executable, which is used to launch Docker containers and run Docker and nsenter commands. It specifies group settings and user ID restrictions.

LANGUAGE: bash
CODE:
storm.worker-launcher.group=$(worker_launcher_group)
min.user.id=$(min_user_id)
worker.profiler.script.path=$(profiler_script_path)

----------------------------------------

TITLE: Parsing Apache Logs to JSON
DESCRIPTION: Python script to parse fake Apache logs and convert them to JSON format with an auto-incrementing ID.

LANGUAGE: python
CODE:
import sys
import apache_log_parser
import json

auto_incr_id = 1
parser_format = '%a - - %t %D "%r" %s %b "%{Referer}i" "%{User-Agent}i"'
line_parser = apache_log_parser.make_parser(parser_format)
while True:
  line = sys.stdin.readline()
  if not line:
    break
  parsed_dict = line_parser(line)
  parsed_dict['id'] = auto_incr_id
  auto_incr_id += 1

  parsed_dict = {k.upper(): v for k, v in parsed_dict.iteritems() if not k.endswith('datetimeobj')}
  print json.dumps(parsed_dict)

----------------------------------------

TITLE: Using Peek Operation in Trident
DESCRIPTION: Example of using peek to print uppercase words before grouping.

LANGUAGE: java
CODE:
mystream.flatMap(new Split()).map(new UpperCase())
         .peek(new Consumer() {
                @Override
                public void accept(TridentTuple input) {
                  System.out.println(input.getString(0));
                }
         })
         .groupBy(new Fields("word"))
         .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))

----------------------------------------

TITLE: Running Storm SQL in Explain Mode
DESCRIPTION: Command to run Storm SQL in explain mode, which shows the query plan instead of submitting the topology. This is useful for analyzing and debugging SQL statements.

LANGUAGE: bash
CODE:
$ bin/storm sql order_filtering.sql --explain --artifacts "org.apache.storm:storm-sql-kafka:2.0.0-SNAPSHOT,org.apache.storm:storm-kafka-client:2.0.0-SNAPSHOT,org.apache.kafka:kafka-clients:1.1.0^org.slf4j:slf4j-log4j12"

----------------------------------------

TITLE: Adding Items to Kestrel Queue in Java
DESCRIPTION: Method to add random sentences to a Kestrel queue using KestrelClient. Randomly selects from five predefined sentences and adds them with sequential IDs.

LANGUAGE: java
CODE:
    private static void queueSentenceItems(KestrelClient kestrelClient, String queueName)
			throws ParseError, IOException {

		String[] sentences = new String[] {
	            "the cow jumped over the moon",
	            "an apple a day keeps the doctor away",
	            "four score and seven years ago",
	            "snow white and the seven dwarfs",
	            "i am at two with nature"};

		Random _rand = new Random();

		for(int i=1; i<=10; i++){

			String sentence = sentences[_rand.nextInt(sentences.length)];

			String val = "ID " + i + " " + sentence;

			boolean queueSucess = kestrelClient.queue(queueName, val);

			System.out.println("queueSucess=" +queueSucess+ " [" + val +"]");
		}
	}

----------------------------------------

TITLE: Creating JdbcLookupBolt in Java
DESCRIPTION: This code snippet demonstrates how to create a JdbcLookupBolt, configuring it with a connection provider, SQL select query, lookup mapper, and query timeout.

LANGUAGE: java
CODE:
String selectSql = "select user_name from user_details where user_id = ?";
SimpleJdbcLookupMapper lookupMapper = new SimpleJdbcLookupMapper(outputFields, queryParamColumns)
JdbcLookupBolt userNameLookupBolt = new JdbcLookupBolt(connectionProvider, selectSql, lookupMapper)
        .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Storm Topology Submission Method Definition
DESCRIPTION: Thrift method definition for submitting a topology to Storm. Includes parameters for topology name, jar location, configuration, and topology structure with exception handling.

LANGUAGE: java
CODE:
void submitTopology(
        1: string name,
        2: string uploadedJarLocation,
        3: string jsonConf,
        4: StormTopology topology)
        throws (
                1: AlreadyAliveException e,
                2: InvalidTopologyException ite);

----------------------------------------

TITLE: Thread-safe Serializer Implementation in Apache Storm
DESCRIPTION: Implementation of a thread-safe serializer used for tuple serialization in Apache Storm.

LANGUAGE: java
CODE:
https://github.com/apache/storm/blob/0.7.1/src/jvm/org/apache/storm/serialization/KryoTupleSerializer.java#L26

----------------------------------------

TITLE: Implementing HTML Meta Refresh and Canonical Link for Apache Storm Documentation
DESCRIPTION: This HTML snippet sets up a meta refresh to automatically redirect the user to the current Apache Storm documentation page on common patterns. It also includes a canonical link to improve SEO and indicate the preferred URL for the content.

LANGUAGE: HTML
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Common-patterns.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Common-patterns.html" />

----------------------------------------

TITLE: Creating Kerberos Principals and Keytabs
DESCRIPTION: Bash commands for creating Kerberos principals and generating keytabs for Zookeeper, Nimbus, DRPC and other Storm components.

LANGUAGE: bash
CODE:
# Zookeeper (Will need one of these for each box in the Zk ensemble)
sudo kadmin.local -q 'addprinc zookeeper/zk1.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/zk.keytab  zookeeper/zk1.example.com@STORM.EXAMPLE.COM"
# Nimbus and DRPC
sudo kadmin.local -q 'addprinc storm/storm.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm/storm.example.com@STORM.EXAMPLE.COM"
# All UI logviewer and Supervisors
sudo kadmin.local -q 'addprinc storm@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm@STORM.EXAMPLE.COM"

----------------------------------------

TITLE: Storm Wait Strategy Configuration
DESCRIPTION: Configuration properties for defining wait strategies to optimize CPU usage in different scenarios including spout wait, bolt wait, and backpressure situations.

LANGUAGE: properties
CODE:
topology.spout.wait.strategy
topology.bolt.wait.strategy
topology.backpressure.wait.strategy

----------------------------------------

TITLE: Implementing Storm Kinesis Spout Topology in Java
DESCRIPTION: Example topology implementation showing how to configure and use KinesisSpout with Storm. Demonstrates setup of connection parameters, ZooKeeper configuration, and topology building with spouts and bolts.

LANGUAGE: java
CODE:
public class KinesisSpoutTopology {
    public static void main (String args[]) throws InvalidTopologyException, AuthorizationException, AlreadyAliveException {
        String topologyName = args[0];
        RecordToTupleMapper recordToTupleMapper = new TestRecordToTupleMapper();
        KinesisConnectionInfo kinesisConnectionInfo = new KinesisConnectionInfo(new CredentialsProviderChain(), new ClientConfiguration(), Regions.US_WEST_2,
                1000);
        ZkInfo zkInfo = new ZkInfo("localhost:2181", "/kinesisOffsets", 20000, 15000, 10000L, 3, 2000);
        KinesisConfig kinesisConfig = new KinesisConfig(args[1], ShardIteratorType.TRIM_HORIZON,
                recordToTupleMapper, new Date(), new ExponentialBackoffRetrier(), zkInfo, kinesisConnectionInfo, 10000L);
        KinesisSpout kinesisSpout = new KinesisSpout(kinesisConfig);
        TopologyBuilder topologyBuilder = new TopologyBuilder();
        topologyBuilder.setSpout("spout", kinesisSpout, 3);
        topologyBuilder.setBolt("bolt", new KinesisBoltTest(), 1).shuffleGrouping("spout");
        Config topologyConfig = new Config();
        topologyConfig.setDebug(true);
        topologyConfig.setNumWorkers(3);
        StormSubmitter.submitTopology(topologyName, topologyConfig, topologyBuilder.createTopology());
    }
}

----------------------------------------

TITLE: Digest Authentication JAAS Configuration
DESCRIPTION: JAAS configuration file structure for Pacemaker digest authentication with username and password.

LANGUAGE: java
CODE:
PacemakerDigest {\n    username="some username"\n    password="some password";\n};

----------------------------------------

TITLE: Initializing Transactional Topology in Java
DESCRIPTION: Example of creating a transactional topology builder with a memory spout and configuring bolts for global counting.

LANGUAGE: java
CODE:
MemoryTransactionalSpout spout = new MemoryTransactionalSpout(DATA, new Fields("word"), PARTITION_TAKE_PER_BATCH);
TransactionalTopologyBuilder builder = new TransactionalTopologyBuilder("global-count", "spout", spout, 3);
builder.setBolt("partial-count", new BatchCount(), 5)
        .shuffleGrouping("spout");
builder.setBolt("sum", new UpdateGlobalCount())
        .globalGrouping("partial-count");

----------------------------------------

TITLE: Installing JZMQ for Storm
DESCRIPTION: This snippet demonstrates the process of cloning a specific JZMQ fork, configuring, and installing it. This version is tested to work with Storm and prevents potential regressions.

LANGUAGE: bash
CODE:
#install jzmq
git clone https://github.com/nathanmarz/jzmq.git
cd jzmq
./autogen.sh
./configure
make
sudo make install

----------------------------------------

TITLE: Implementing Custom Metric in Java Bolt
DESCRIPTION: Shows how to implement a custom metric (execution count) in a Storm bolt. Demonstrates declaring, initializing, registering, and incrementing the metric.

LANGUAGE: java
CODE:
private transient CountMetric countMetric;

@Override
public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
	// other initialization here.
	countMetric = new CountMetric();
	context.registerMetric("execute_count", countMetric, 60);
}

public void execute(Tuple input) {
	countMetric.incr();
	// handle tuple here.	
}

----------------------------------------

TITLE: Implementing Custom Metric in Java Bolt
DESCRIPTION: Shows how to implement a custom metric (execution count) in a Storm bolt. Demonstrates declaring, initializing, registering, and incrementing the metric.

LANGUAGE: java
CODE:
private transient CountMetric countMetric;

@Override
public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
	// other initialization here.
	countMetric = new CountMetric();
	context.registerMetric("execute_count", countMetric, 60);
}

public void execute(Tuple input) {
	countMetric.incr();
	// handle tuple here.	
}

----------------------------------------

TITLE: Chaining Map and FlatMap Operations in Trident
DESCRIPTION: Shows how to chain flatMap and map operations to split sentences and convert words to uppercase.

LANGUAGE: java
CODE:
mystream.flatMap(new Split()).map(new UpperCase())

----------------------------------------

TITLE: Listing Storm Classpath
DESCRIPTION: Displays the classpath used by the Storm client for running commands.

LANGUAGE: shell
CODE:
storm classpath

----------------------------------------

TITLE: Running Storm SQL for Error Log Filtering
DESCRIPTION: Command to run the Storm SQL topology for filtering error logs from Apache log stream.

LANGUAGE: bash
CODE:
$STORM_DIR/bin/storm sql apache_log_error_filtering.sql apache_log_error_filtering --artifacts "org.apache.storm:storm-sql-kafka:2.0.0-SNAPSHOT,org.apache.storm:storm-kafka-client:2.0.0-SNAPSHOT,org.apache.kafka:kafka-clients:1.1.0^org.slf4j:slf4j-log4j12"

----------------------------------------

TITLE: Using persistentAggregate for Word Count
DESCRIPTION: Example of using persistentAggregate in a Trident topology to implement a streaming word count with persistent state.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
      topology.newStream("spout1", spout)
        .each(new Fields("sentence"), new Split(), new Fields("word"))
        .groupBy(new Fields("word"))
        .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))

----------------------------------------

TITLE: Basic Trident Function Implementation
DESCRIPTION: Example of implementing a basic Trident function that emits multiple tuples based on input value

LANGUAGE: java
CODE:
public class MyFunction extends BaseFunction {
    public void execute(TridentTuple tuple, TridentCollector collector) {
        for(int i=0; i < tuple.getInteger(0); i++) {
            collector.emit(new Values(i));
        }
    }
}

----------------------------------------

TITLE: Understanding Storm's Acker Execution Flow
DESCRIPTION: The Acker bolt processes tuple acknowledgments by maintaining a checksum-based tracking system. It handles three main cases: initialization of new tuple trees, processing of acks, and handling of failures. The bolt uses XOR operations to track tuple completion and manages tuple expiration through a RotatingMap structure.



----------------------------------------

TITLE: Implementing Sentence Spout in Clojure
DESCRIPTION: Example spout implementation that emits random sentences from a predefined list at regular intervals.

LANGUAGE: clojure
CODE:
(defspout sentence-spout ["sentence"]
  [conf context collector]
  (let [sentences ["a little brown dog"
                   "the man petted the dog"
                   "four score and seven years ago"
                   "an apple a day keeps the doctor away"]]
    (spout
     (nextTuple []
       (Thread/sleep 100)
       (emit-spout! collector [(rand-nth sentences)])         
       )
     (ack [id]
        ))))

----------------------------------------

TITLE: Docker Manifest Configuration
DESCRIPTION: Example Docker manifest JSON configuration specifying image layers and metadata

LANGUAGE: json
CODE:
{
  "schemaVersion": 2,
  "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
  "config": {
    "mediaType": "application/vnd.docker.container.image.v1+json",
    "size": 7877,
    "digest": "sha256:ef1ff2c7167a1a6cd01e106f51b84a4d400611ba971c53cbc28de7919515ca4e"
  },
  "layers": [
    {
      "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
      "size": 26858854,
      "digest": "sha256:3692c3483ef6516fba685b316448e8aaf0fc10bb66818116edc8e5e6800076c7"
    }
  ]
}

----------------------------------------

TITLE: Defining SQL Grammar in BNF-like Form
DESCRIPTION: Defines the SQL grammar supported by Storm SQL in a BNF-like form, including statements, queries, expressions, and clauses.

LANGUAGE: sql
CODE:
statement:
      setStatement
  |   resetStatement
  |   explain
  |   describe
  |   insert
  |   update
  |   merge
  |   delete
  |   query

setStatement:
      [ ALTER ( SYSTEM | SESSION ) ] SET identifier '=' expression

resetStatement:
      [ ALTER ( SYSTEM | SESSION ) ] RESET identifier
  |   [ ALTER ( SYSTEM | SESSION ) ] RESET ALL

explain:
      EXPLAIN PLAN
      [ WITH TYPE | WITH IMPLEMENTATION | WITHOUT IMPLEMENTATION ]
      [ EXCLUDING ATTRIBUTES | INCLUDING [ ALL ] ATTRIBUTES ]
      FOR ( query | insert | update | merge | delete )

describe:
      DESCRIBE DATABASE databaseName
   |  DESCRIBE CATALOG [ databaseName . ] catalogName
   |  DESCRIBE SCHEMA [ [ databaseName . ] catalogName ] . schemaName
   |  DESCRIBE [ TABLE ] [ [ [ databaseName . ] catalogName . ] schemaName . ] tableName [ columnName ]
   |  DESCRIBE [ STATEMENT ] ( query | insert | update | merge | delete )

insert:
      ( INSERT | UPSERT ) INTO tablePrimary
      [ '(' column [, column ]* ')' ]
      query

update:
      UPDATE tablePrimary
      SET assign [, assign ]*
      [ WHERE booleanExpression ]

assign:
      identifier '=' expression

merge:
      MERGE INTO tablePrimary [ [ AS ] alias ]
      USING tablePrimary
      ON booleanExpression
      [ WHEN MATCHED THEN UPDATE SET assign [, assign ]* ]
      [ WHEN NOT MATCHED THEN INSERT VALUES '(' value [ , value ]* ')' ]

delete:
      DELETE FROM tablePrimary [ [ AS ] alias ]
      [ WHERE booleanExpression ]

query:
      values
  |   WITH withItem [ , withItem ]* query
  |   {
          select
      |   selectWithoutFrom
      |   query UNION [ ALL ] query
      |   query EXCEPT query
      |   query INTERSECT query
      }
      [ ORDER BY orderItem [, orderItem ]* ]
      [ LIMIT { count | ALL } ]
      [ OFFSET start { ROW | ROWS } ]
      [ FETCH { FIRST | NEXT } [ count ] { ROW | ROWS } ]

withItem:
      name
      [ '(' column [, column ]* ')' ]
      AS '(' query ')'

orderItem:
      expression [ ASC | DESC ] [ NULLS FIRST | NULLS LAST ]

select:
      SELECT [ STREAM ] [ ALL | DISTINCT ]
          { * | projectItem [, projectItem ]* }
      FROM tableExpression
      [ WHERE booleanExpression ]
      [ GROUP BY { groupItem [, groupItem ]* } ]
      [ HAVING booleanExpression ]
      [ WINDOW windowName AS windowSpec [, windowName AS windowSpec ]* ]

selectWithoutFrom:
      SELECT [ ALL | DISTINCT ]
          { * | projectItem [, projectItem ]* }

projectItem:
      expression [ [ AS ] columnAlias ]
  |   tableAlias . *

tableExpression:
      tableReference [, tableReference ]*
  |   tableExpression [ NATURAL ] [ LEFT | RIGHT | FULL ] JOIN tableExpression [ joinCondition ]

joinCondition:
      ON booleanExpression
  |   USING '(' column [, column ]* ')'

tableReference:
      tablePrimary
      [ [ AS ] alias [ '(' columnAlias [, columnAlias ]* ')' ] ]

tablePrimary:
      [ [ catalogName . ] schemaName . ] tableName
      '(' TABLE [ [ catalogName . ] schemaName . ] tableName ')'
  |   [ LATERAL ] '(' query ')'
  |   UNNEST '(' expression ')' [ WITH ORDINALITY ]
  |   [ LATERAL ] TABLE '(' [ SPECIFIC ] functionName '(' expression [, expression ]* ')' ')'

values:
      VALUES expression [, expression ]*

groupItem:
      expression
  |   '(' ')'
  |   '(' expression [, expression ]* ')'
  |   CUBE '(' expression [, expression ]* ')'
  |   ROLLUP '(' expression [, expression ]* ')'
  |   GROUPING SETS '(' groupItem [, groupItem ]* ')'

windowRef:
      windowName
  |   windowSpec

windowSpec:
      [ windowName ]
      '('
      [ ORDER BY orderItem [, orderItem ]* ]
      [ PARTITION BY expression [, expression ]* ]
      [
          RANGE numericOrIntervalExpression { PRECEDING | FOLLOWING }
      |   ROWS numericExpression { PRECEDING | FOLLOWING }
      ]
      ')'

----------------------------------------

TITLE: Filtering Slow Logs with Storm SQL and UDF
DESCRIPTION: Storm SQL script to filter slow logs from Apache logs, using a user-defined function to convert timestamps.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://apache-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
CREATE EXTERNAL TABLE APACHE_SLOW_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_RECEIVED_TIMESTAMP BIGINT, TIME_ELAPSED_MS INT) LOCATION 'kafka://apache-slow-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
CREATE FUNCTION GET_TIME AS 'org.apache.storm.sql.runtime.functions.scalar.datetime.GetTime2'
INSERT INTO APACHE_SLOW_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, GET_TIME(TIME_RECEIVED_UTC_ISOFORMAT, 'yyyy-MM-dd''T''HH:mm:ssZZ') AS TIME_RECEIVED_TIMESTAMP, TIME_US / 1000 AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (TIME_US / 1000) >= 100

----------------------------------------

TITLE: Configuring ZooKeeper Authentication in Storm YAML
DESCRIPTION: YAML configuration for setting up ZooKeeper authentication in Storm cluster.

LANGUAGE: yaml
CODE:
storm.zookeeper.auth.scheme: "digest"
storm.zookeeper.auth.payload: "username:password"

# Topology ZooKeeper authentication
storm.zookeeper.topology.auth.scheme: "digest"
storm.zookeeper.topology.auth.payload: "topology_username:topology_password"

----------------------------------------

TITLE: Aggregation Operations in Java Stream API
DESCRIPTION: Shows how to perform global and key-based aggregations using aggregate, reduce, aggregateByKey, and reduceByKey operations.

LANGUAGE: java
CODE:
Stream<Long> numbers = ...
// aggregate the numbers and produce a stream of last 10 sec sums.
Stream<Long> sums = numbers.window(TumblingWindows.of(Duration.seconds(10))).aggregate(new Sum());

// the last 10 sec sums computed using reduce
Stream<Long> sums = numbers.window(...).reduce((x, y) -> x + y);

Stream<String> words = ...
Stream<String, Long> wordCounts = words.mapToPair(w -> Pair.of(w,1)     // convert to a stream of (word, 1) pairs
                                       .aggregateByKey(new Count<>());  // compute counts per word

----------------------------------------

TITLE: Aggregation Operations in Java Stream API
DESCRIPTION: Shows how to perform global and key-based aggregations using aggregate, reduce, aggregateByKey, and reduceByKey operations.

LANGUAGE: java
CODE:
Stream<Long> numbers = ...
// aggregate the numbers and produce a stream of last 10 sec sums.
Stream<Long> sums = numbers.window(TumblingWindows.of(Duration.seconds(10))).aggregate(new Sum());

// the last 10 sec sums computed using reduce
Stream<Long> sums = numbers.window(...).reduce((x, y) -> x + y);

Stream<String> words = ...
Stream<String, Long> wordCounts = words.mapToPair(w -> Pair.of(w,1)     // convert to a stream of (word, 1) pairs
                                       .aggregateByKey(new Count<>());  // compute counts per word

----------------------------------------

TITLE: Submitting Storm SQL Topology with Dependencies
DESCRIPTION: Command to submit a Storm SQL topology with necessary dependencies. This example includes Kafka-related artifacts and submits SQL statements from a file named order_filtering.sql.

LANGUAGE: bash
CODE:
$ bin/storm sql order_filtering.sql order_filtering --artifacts "org.apache.storm:storm-sql-kafka:2.0.0-SNAPSHOT,org.apache.storm:storm-kafka:2.0.0-SNAPSHOT,org.apache.kafka:kafka_2.10:0.8.2.2^org.slf4j:slf4j-log4j12,org.apache.kafka:kafka-clients:0.8.2.2"

----------------------------------------

TITLE: Defining Storm Topology in Clojure
DESCRIPTION: Example of defining a Storm topology with spouts and bolts using the Clojure DSL. Shows how to wire components together and specify parallelism hints.

LANGUAGE: clojure
CODE:
(topology
 {"1" (spout-spec sentence-spout)
  "2" (spout-spec (sentence-spout-parameterized
                   ["the cat jumped over the door"
                    "greetings from a faraway land"])
                   :p 2)}
 {"3" (bolt-spec {"1" :shuffle "2" :shuffle}
                 split-sentence
                 :p 5)
  "4" (bolt-spec {"3" ["word"]}
                 word-count
                 :p 6)})

----------------------------------------

TITLE: Redis State Provider Configuration
DESCRIPTION: JSON configuration for Redis-based state persistence, including key/value class definitions and Redis connection parameters.

LANGUAGE: json
CODE:
{
  "keyClass": "Optional fully qualified class name of the Key type.",
  "valueClass": "Optional fully qualified class name of the Value type.",
  "keySerializerClass": "Optional Key serializer implementation class.",
  "valueSerializerClass": "Optional Value Serializer implementation class.",
  "jedisPoolConfig": {
    "host": "localhost",
    "port": 6379,
    "timeout": 2000,
    "database": 0,
    "password": "xyz"
    }
}

----------------------------------------

TITLE: Storm Assignment Record Definition
DESCRIPTION: Clojure code defining the structure of a Storm topology assignment, including master code directory, task mappings, and node configurations.

LANGUAGE: clojure
CODE:
master-code-dir
task->node+port
node->host
task->start-time-secs

----------------------------------------

TITLE: HBase State Provider Configuration Example
DESCRIPTION: Java code example showing how to initialize HBase state provider with configuration parameters.

LANGUAGE: java
CODE:
Config conf = new Config();
    Map<String, Object> hbConf = new HashMap<String, Object>();
    hbConf.put("hbase.rootdir", "file:///tmp/hbase");
    conf.put("hbase.conf", hbConf);
    conf.put("topology.state.provider",  "org.apache.storm.hbase.state.HBaseKeyValueStateProvider");
    conf.put("topology.state.provider.config", "{" +
            "   \"hbaseConfigKey\": \"hbase.conf\"," +
            "   \"tableName\": \"state\"," +
            "   \"columnFamily\": \"cf\"" +
            " }");

----------------------------------------

TITLE: Joining Streams Based on Stream Names in Storm Core
DESCRIPTION: Java code example showing how to configure JoinBolt to use stream names instead of source component names for joining streams in Apache Storm.

LANGUAGE: java
CODE:
new JoinBolt(JoinBolt.Selector.STREAM,  "stream1", "key1")
                             .join     ("stream2", "userId",  "stream1" )
                             .select ("userId, key1, key2")
                             .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;
                             
topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("bolt1", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt2", "stream1", new Fields("key1") )
            .fieldsGrouping("bolt3", "stream2", new Fields("userId") )
            .fieldsGrouping("bolt4", "stream1", new Fields("key1") );

----------------------------------------

TITLE: Configuring URL Expansion Bolt with Fields Grouping in Storm
DESCRIPTION: Example showing how to set up a bolt that expands URLs using fields grouping, which ensures the same URL always goes to the same task for better cache efficiency.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .fieldsGrouping("urls", new Fields("url"));

----------------------------------------

TITLE: Defining ExecutionResultHandler Interface in Java for Cassandra Query Handling
DESCRIPTION: Interface definition for handling various Cassandra query execution results in Storm bolts.

LANGUAGE: java
CODE:
public interface ExecutionResultHandler extends Serializable {
    void onQueryValidationException(QueryValidationException e, OutputCollector collector, Tuple tuple);

    void onReadTimeoutException(ReadTimeoutException e, OutputCollector collector, Tuple tuple);

    void onWriteTimeoutException(WriteTimeoutException e, OutputCollector collector, Tuple tuple);

    void onUnavailableException(UnavailableException e, OutputCollector collector, Tuple tuple);

    void onQuerySuccess(OutputCollector collector, Tuple tuple);
}

----------------------------------------

TITLE: Supervisor Synchronization
DESCRIPTION: Functions for synchronizing supervisor state and managing worker processes

LANGUAGE: clojure
CODE:
synchronize-supervisor
sync-processes

----------------------------------------

TITLE: Registering Event Logger in Java for Apache Storm Topology
DESCRIPTION: This snippet demonstrates how to register an event logger to a Storm topology using Java configuration. It uses the FileBasedEventLogger class as an example.

LANGUAGE: java
CODE:
conf.registerEventLogger(org.apache.storm.metric.FileBasedEventLogger.class);

----------------------------------------

TITLE: Implementing a Custom Metric in Java
DESCRIPTION: Example of creating and registering a custom CountMetric in a Storm bolt.

LANGUAGE: java
CODE:
private transient CountMetric countMetric;

@Override
public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
	// other initialization here.
	countMetric = new CountMetric();
	context.registerMetric("execute_count", countMetric, 60);
}

public void execute(Tuple input) {
	countMetric.incr();
	// handle tuple here.	
}

----------------------------------------

TITLE: Implementing a Custom Metric in Java
DESCRIPTION: Example of creating and registering a custom CountMetric in a Storm bolt.

LANGUAGE: java
CODE:
private transient CountMetric countMetric;

@Override
public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
	// other initialization here.
	countMetric = new CountMetric();
	context.registerMetric("execute_count", countMetric, 60);
}

public void execute(Tuple input) {
	countMetric.incr();
	// handle tuple here.	
}

----------------------------------------

TITLE: Implementing Custom Task-Level Metrics
DESCRIPTION: Example of implementing a custom metric at the task level to count bolt executions using CountMetric.

LANGUAGE: java
CODE:
private transient CountMetric countMetric;

LANGUAGE: java
CODE:
@Override
public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
	// other initialization here.
	countMetric = new CountMetric();
	context.registerMetric("execute_count", countMetric, 60);
}

LANGUAGE: java
CODE:
public void execute(Tuple input) {
	countMetric.incr();
	// handle tuple here.	
}

----------------------------------------

TITLE: Displaying jQuery UI v1.12.1 License Text
DESCRIPTION: This snippet presents the complete license text for jQuery UI v1.12.1. It outlines the permissions granted, conditions of use, warranty disclaimers, and special considerations for sample code and external libraries.

LANGUAGE: plaintext
CODE:
Copyright jQuery Foundation and other contributors, https://jquery.org/

This software consists of voluntary contributions made by many
individuals. For exact contribution history, see the revision history
available at https://github.com/jquery/jquery-ui

The following license applies to all parts of this software except as
documented below:

====

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

====

Copyright and related rights for sample code are waived via CC0. Sample
code is defined as all source code contained within the demos directory.

CC0: http://creativecommons.org/publicdomain/zero/1.0/

====

All files located in the node_modules and external directories are
externally maintained libraries used by this software which have their
own licenses; we recommend you read them, as their terms may differ from
the terms above.

----------------------------------------

TITLE: Implementing Custom Serialization Interface in Java
DESCRIPTION: Definition of the ISerialization interface used to create custom serializers in Storm. The interface includes methods for accepting types, serializing objects to binary format, and deserializing data streams back into objects.

LANGUAGE: java
CODE:
public interface ISerialization<T> {
    public boolean accept(Class c);
    public void serialize(T object, DataOutputStream stream) throws IOException;
    public T deserialize(DataInputStream stream) throws IOException;
}

----------------------------------------

TITLE: Spout Message Listening in Apache Storm
DESCRIPTION: Implementation of how spouts listen for incoming messages in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/task.clj#L382

----------------------------------------

TITLE: Launching Storm Log Viewer
DESCRIPTION: Starts the log viewer daemon to provide a web interface for viewing Storm log files.

LANGUAGE: shell
CODE:
storm logviewer

----------------------------------------

TITLE: Implementing Storm ISerialization Interface in Java
DESCRIPTION: Interface definition for creating custom serializers in Storm. The interface requires implementing accept() to determine serialization compatibility, serialize() to write objects to binary format, and deserialize() to read objects from binary format.

LANGUAGE: java
CODE:
public interface ISerialization<T> {
    public boolean accept(Class c);
    public void serialize(T object, DataOutputStream stream) throws IOException;
    public T deserialize(DataInputStream stream) throws IOException;
}

----------------------------------------

TITLE: Defining Storm DSLs and Multi-Language Adapters in Markdown
DESCRIPTION: This Markdown snippet lists various DSLs and adapters for Apache Storm, including links to their respective repositories or documentation pages. It covers implementations in Clojure, Scala, JRuby, F#, as well as integrations with Esper for streaming SQL and a Perl multilang adapter.

LANGUAGE: Markdown
CODE:
---
title: Storm DSLs and Multi-Lang Adapters
layout: documentation
documentation: true
---
* [Clojure DSL](Clojure-DSL.html)
* [Scala DSL](https://github.com/velvia/ScalaStorm)
* [JRuby DSL](https://github.com/colinsurprenant/redstorm)
* [Storm/Esper integration](https://github.com/tomdz/storm-esper): Streaming SQL on top of Storm
* [io-storm](https://github.com/dan-blanchard/io-storm): Perl multilang adapter
* [FsShelter](https://github.com/Prolucid/FsShelter): F# DSL and runtime with protobuf multilang

----------------------------------------

TITLE: Retrieving Remote Storm Configuration Value
DESCRIPTION: Prints the value of a specified configuration parameter from the cluster's Storm configs.

LANGUAGE: shell
CODE:
storm remoteconfvalue conf-name

----------------------------------------

TITLE: Configuring URL Expansion with Shuffle Grouping in Storm
DESCRIPTION: Example showing URL expansion bolt configuration using shuffle grouping for distributed processing.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .shuffleGrouping(1);

----------------------------------------

TITLE: Storm Task Parallelism Configuration
DESCRIPTION: Shows how to configure the number of worker processes for a Storm topology using the Config.TOPOLOGY_WORKERS setting. This determines how tasks are distributed across JVM instances.

LANGUAGE: java
CODE:
Config.TOPOLOGY_WORKERS

----------------------------------------

TITLE: Java Interface for Rich Bolt
DESCRIPTION: The Java interface for implementing a Rich Bolt in Storm. This interface extends IBolt and adds the declareOutputFields method for specifying output streams.

LANGUAGE: Java
CODE:
public interface IRichBolt extends IBolt {
  void declareOutputFields(OutputFieldsDeclarer declarer);
  Map<String, Object> getComponentConfiguration();
}

----------------------------------------

TITLE: Using Peek for Debugging in Trident
DESCRIPTION: Example of using peek to inspect tuples as they flow through the stream

LANGUAGE: java
CODE:
mystream.flatMap(new Split()).map(new UpperCase())
         .peek(new Consumer() {
                @Override
                public void accept(TridentTuple input) {
                  System.out.println(input.getString(0));
                }
         })
         .groupBy(new Fields("word"))
         .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))

----------------------------------------

TITLE: Running Storm SQL Command
DESCRIPTION: Command to compile SQL statements into a Trident topology and submit it to the Storm cluster. The sql-file contains SQL statements to execute and topo-name is the name of the topology.

LANGUAGE: bash
CODE:
$ bin/storm sql <sql-file> <topo-name>

----------------------------------------

TITLE: Stateful Prepared Bolt in Clojure
DESCRIPTION: Implementation of a word counting bolt that maintains state using a prepared bolt with closure.

LANGUAGE: clojure
CODE:
(defbolt word-count ["word" "count"] {:prepare true}
  [conf context collector]
  (let [counts (atom {})]
    (bolt
     (execute [tuple]
       (let [word (.getString tuple 0)]
         (swap! counts (partial merge-with +) {word 1})
         (emit-bolt! collector [word (@counts word)] :anchor tuple)
         (ack! collector tuple)
         )))))

----------------------------------------

TITLE: Configuring Isolation Scheduler in Storm YAML
DESCRIPTION: YAML configuration example showing how to allocate dedicated machines to specific topologies using the isolation scheduler. The configuration maps topology names to their allocated machine count for isolation purposes.

LANGUAGE: yaml
CODE:
isolation.scheduler.machines: 
    "my-topology": 8
    "tiny-topology": 1
    "some-other-topology": 3

----------------------------------------

TITLE: Debugging Storm Topology in IDE
DESCRIPTION: Shows how to set up local mode override for debugging Storm topologies within an IDE environment.

LANGUAGE: java
CODE:
public static void main(final String args[]) {
    LocalCluster.withLocalModeOverride(() -> originalMain(args), 10);
}

----------------------------------------

TITLE: CGroup CPU Statistics Output Format
DESCRIPTION: Example structure of CGroup CPU statistics metrics reported by the system. Shows the format of bandwidth statistics including period count, throttled count, and throttled time measurements.

LANGUAGE: text
CODE:
   "CGroupCPU.user-ms": number
   "CGroupCPU.sys-ms": number

----------------------------------------

TITLE: Implementing a CombinerAggregator in Trident
DESCRIPTION: Example of implementing a Count CombinerAggregator.

LANGUAGE: java
CODE:
public class Count implements CombinerAggregator<Long> {
    public Long init(TridentTuple tuple) {
        return 1L;
    }

    public Long combine(Long val1, Long val2) {
        return val1 + val2;
    }

    public Long zero() {
        return 0L;
    }
}

----------------------------------------

TITLE: DRPC Server Configuration in YAML
DESCRIPTION: YAML configuration for setting up DRPC servers in a Storm cluster, including server locations and transport settings.

LANGUAGE: yaml
CODE:
drpc.servers:
  - "drpc1.foo.com"
  - "drpc2.foo.com"
drpc.http.port: 8081
storm.thrift.transport: "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin"

----------------------------------------

TITLE: Implementing Multiple Query Insert from Single Tuple
DESCRIPTION: Demonstrates how to insert data into multiple tables from a single input tuple using CassandraWriterBolt.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
    async(
        simpleQuery("INSERT INTO titles_per_album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all())),
        simpleQuery("INSERT INTO titles_per_performer (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all()))
    )
);

----------------------------------------

TITLE: Storm SplitSentence Bolt Implementation
DESCRIPTION: Example bolt implementation that splits sentences into words with reliability guarantees.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseRichBolt {
        OutputCollector _collector;
        
        public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
            _collector = collector;
        }

        public void execute(Tuple tuple) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                _collector.emit(tuple, new Values(word));
            }
            _collector.ack(tuple);
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Basic JoinBolt Configuration
DESCRIPTION: Example showing how to configure JoinBolt to join multiple streams with field grouping and window configuration.

LANGUAGE: java
CODE:
JoinBolt jbolt =  new JoinBolt("spout1", "key1")                   // from        spout1  
                    .join     ("spout2", "userId",  "spout1")      // inner join  spout2  on spout2.userId = spout1.key1
                    .join     ("spout3", "key3",    "spout2")      // inner join  spout3  on spout3.key3   = spout2.userId   
                    .leftJoin ("spout4", "key4",    "spout3")      // left join   spout4  on spout4.key4   = spout3.key3
                    .select  ("userId, key4, key2, spout3:key3")   // chose output fields
                    .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;

topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("spout1", new Fields("key1") )
            .fieldsGrouping("spout2", new Fields("userId") )
            .fieldsGrouping("spout3", new Fields("key3") )
            .fieldsGrouping("spout4", new Fields("key4") );

----------------------------------------

TITLE: Custom MQTT Message Mapper Implementation
DESCRIPTION: Implementation of MqttMessageMapper that parses topic and payload data into tuple values

LANGUAGE: java
CODE:
public class CustomMessageMapper implements MqttMessageMapper {
    private static final Logger LOG = LoggerFactory.getLogger(CustomMessageMapper.class);

    public Values toValues(MqttMessage message) {
        String topic = message.getTopic();
        String[] topicElements = topic.split("/");
        String[] payloadElements = new String(message.getMessage()).split("/");

        return new Values(topicElements[2], topicElements[4], topicElements[3], Float.parseFloat(payloadElements[0]), Float.parseFloat(payloadElements[1]));
    }

    public Fields outputFields() {
        return new Fields("user", "deviceId", "location", "temperature", "humidity");
    }
}

----------------------------------------

TITLE: Custom MQTT Message Mapper Implementation
DESCRIPTION: Implementation of MqttMessageMapper that parses topic and payload data into tuple values

LANGUAGE: java
CODE:
public class CustomMessageMapper implements MqttMessageMapper {
    private static final Logger LOG = LoggerFactory.getLogger(CustomMessageMapper.class);

    public Values toValues(MqttMessage message) {
        String topic = message.getTopic();
        String[] topicElements = topic.split("/");
        String[] payloadElements = new String(message.getMessage()).split("/");

        return new Values(topicElements[2], topicElements[4], topicElements[3], Float.parseFloat(payloadElements[0]), Float.parseFloat(payloadElements[1]));
    }

    public Fields outputFields() {
        return new Fields("user", "deviceId", "location", "temperature", "humidity");
    }
}

----------------------------------------

TITLE: Registering Event Logger in Java for Apache Storm Topology
DESCRIPTION: This code snippet shows how to register an event logger to an Apache Storm topology using the topology configuration object.

LANGUAGE: java
CODE:
conf.registerEventLogger(org.apache.storm.metric.FileBasedEventLogger.class);

----------------------------------------

TITLE: Installing JZMQ from GitHub for Storm
DESCRIPTION: This snippet demonstrates how to clone a specific fork of JZMQ from GitHub and install it. The process includes cloning the repository, running the autogen script, configuring, and installing. This fork is tested to work with Storm to prevent potential regressions.

LANGUAGE: bash
CODE:
#install jzmq
git clone https://github.com/nathanmarz/jzmq.git
cd jzmq
./autogen.sh
./configure
make
sudo make install

----------------------------------------

TITLE: Configuring Resource Aware Scheduler in YAML
DESCRIPTION: YAML configuration to enable the Resource Aware Scheduler in Storm's configuration file.

LANGUAGE: yaml
CODE:
storm.scheduler: "org.apache.storm.scheduler.resource.ResourceAwareScheduler"

----------------------------------------

TITLE: Generic Resource Usage Example - Java
DESCRIPTION: Example showing how to add GPU resource requirements to a Storm spout using the addResource API.

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.addResouce("gpu.count", 1.0);

----------------------------------------

TITLE: Deactivating a Storm Topology
DESCRIPTION: Deactivates the spouts of a specified topology.

LANGUAGE: bash
CODE:
storm deactivate topology-name

----------------------------------------

TITLE: Registering Event Logger in YAML Configuration
DESCRIPTION: Example of registering multiple event loggers including a custom implementation using Storm's YAML configuration.

LANGUAGE: yaml
CODE:
topology.event.logger.register:
  - class: "org.apache.storm.metric.FileBasedEventLogger"
  - class: "org.mycompany.MyEventLogger"
    arguments:
      endpoint: "event-logger.mycompany.org"

----------------------------------------

TITLE: Implementing Persistent Word Count Topology in Java
DESCRIPTION: Provides a complete example of a Storm topology that counts words and persists the results to HBase.

LANGUAGE: java
CODE:
public class PersistentWordCount {
    private static final String WORD_SPOUT = "WORD_SPOUT";
    private static final String COUNT_BOLT = "COUNT_BOLT";
    private static final String HBASE_BOLT = "HBASE_BOLT";


    public static void main(String[] args) throws Exception {
        Config config = new Config();

        WordSpout spout = new WordSpout();
        WordCounter bolt = new WordCounter();

        SimpleHBaseMapper mapper = new SimpleHBaseMapper()
                .withRowKeyField("word")
                .withColumnFields(new Fields("word"))
                .withCounterFields(new Fields("count"))
                .withColumnFamily("cf");

        HBaseBolt hbase = new HBaseBolt("WordCount", mapper);


        // wordSpout ==> countBolt ==> HBaseBolt
        TopologyBuilder builder = new TopologyBuilder();

        builder.setSpout(WORD_SPOUT, spout, 1);
        builder.setBolt(COUNT_BOLT, bolt, 1).shuffleGrouping(WORD_SPOUT);
        builder.setBolt(HBASE_BOLT, hbase, 1).fieldsGrouping(COUNT_BOLT, new Fields("word"));


        if (args.length == 0) {
            LocalCluster cluster = new LocalCluster();
            cluster.submitTopology("test", config, builder.createTopology());
            Thread.sleep(10000);
            cluster.killTopology("test");
            cluster.shutdown();
            System.exit(0);
        } else {
            config.setNumWorkers(3);
            StormSubmitter.submitTopology(args[0], config, builder.createTopology());
        }
    }
}

----------------------------------------

TITLE: Configuring OpenTSDB Bolt in Storm Topology (Java)
DESCRIPTION: This snippet demonstrates how to set up and configure an OpenTSDB bolt in a Storm topology. It creates an OpenTsdbClient, configures the OpenTsdbBolt with batch size and flush interval, and adds it to the topology builder.

LANGUAGE: java
CODE:
OpenTsdbClient.Builder builder =  OpenTsdbClient.newBuilder(openTsdbUrl).sync(30_000).returnDetails();
final OpenTsdbBolt openTsdbBolt = new OpenTsdbBolt(builder, Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
openTsdbBolt.withBatchSize(10).withFlushInterval(2000);
topologyBuilder.setBolt("opentsdb", openTsdbBolt).shuffleGrouping("metric-gen");

----------------------------------------

TITLE: Defining Code Distribution Interface in Java
DESCRIPTION: Interface for distributing topology code across Nimbus instances. Includes methods for uploading, downloading, checking replication status, and cleaning up topology code.

LANGUAGE: java
CODE:
public interface ICodeDistributor {
    void prepare(Map conf);
    File upload(Path dirPath, String topologyId);
    List<File> download(Path destDirPath, String topologyid, File metafile);
    int getReplicationCount(String topologyId);
    void cleanup(String topologyid);
    void close(Map conf);
}

----------------------------------------

TITLE: Defining Storm Component Object Structure in Thrift
DESCRIPTION: Thrift union structure defining how component objects can be represented in Storm, supporting serialized Java, shell components, and Java objects.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Setting Storm Local Directory in YAML
DESCRIPTION: This YAML configuration sets the local directory for Storm to store small amounts of state on each machine in the cluster.

LANGUAGE: yaml
CODE:
storm.local.dir: "/mnt/storm"

LANGUAGE: yaml
CODE:
storm.local.dir: "C:\\storm-local"

----------------------------------------

TITLE: Mounting Container Filesystem
DESCRIPTION: Example of how the container filesystem is mounted using overlayfs

LANGUAGE: bash
CODE:
-bash-4.2$ mount
...
overlay on /run/worker-launcher/6703-1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb/rootfs type overlay (rw,relatime,lowerdir=/run/worker-launcher/layers/a5e4e615565081e04eaf4c5ab5b20d37de271db704fc781c7b1e07c5dcdf96e5/mnt:/run/worker-launcher/layers/0ba001c025aa172a7d630914c75c1772228606f622e2c9d46a8fedf10774623e/mnt:/run/worker-launcher/layers/34b0bc9c446a9be565fb50b04db1e9d1c1c4d14a22a885a7aba6981748b6635e/mnt:/run/worker-launcher/layers/f0d08d5707855b02def8ac622a6c60203b380e31c6c237e5b691f5856594a3e7/mnt:/run/worker-launcher/layers/c7c9b1d6df043edf307c49d75c7d2bc3df72f8dcaf7d17b733c97022387902e6/mnt:/run/worker-launcher/layers/8156da43228752c7364b71dabba6aef6bd1cc081e9ea59cf92ea0f79fd8a50b6/mnt:/run/worker-launcher/layers/f7452c2657900c53da1a4f7e430485a267b89c7717466ee61ffefba85f690226/mnt,upperdir=/run/worker-launcher/6703-1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb/upper,workdir=/run/worker-launcher/6703-1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb/work)
...

----------------------------------------

TITLE: Implementing Simple DRPC Topology with ExclaimBolt
DESCRIPTION: Example of a basic DRPC topology that appends an exclamation mark to input strings.

LANGUAGE: java
CODE:
public static class ExclaimBolt extends BaseBasicBolt {
    public void execute(Tuple tuple, BasicOutputCollector collector) {
        String input = tuple.getString(1);
        collector.emit(new Values(tuple.getValue(0), input + "!"));
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "result"));
    }
}

public static void main(String[] args) throws Exception {
    LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("exclamation");
    builder.addBolt(new ExclaimBolt(), 3);
    // ...
}

----------------------------------------

TITLE: Redis Cluster State Provider Configuration
DESCRIPTION: JSON configuration for Redis Cluster-based state persistence, specifying cluster nodes and connection parameters.

LANGUAGE: json
CODE:
{
   "keyClass": "Optional fully qualified class name of the Key type.",
   "valueClass": "Optional fully qualified class name of the Value type.",
   "keySerializerClass": "Optional Key serializer implementation class.",
   "valueSerializerClass": "Optional Value Serializer implementation class.",
   "jedisClusterConfig": {
     "nodes": ["localhost:7379", "localhost:7380", "localhost:7381"],
     "timeout": 2000,
     "maxRedirections": 5
   }
 }

----------------------------------------

TITLE: Configuring OpenTSDB Bolt in Storm Topology (Java)
DESCRIPTION: This snippet demonstrates how to set up and configure an OpenTSDB bolt in a Storm topology. It creates an OpenTsdbClient, configures the OpenTsdbBolt with batch size and flush interval, and adds it to the topology builder.

LANGUAGE: java
CODE:
OpenTsdbClient.Builder builder =  OpenTsdbClient.newBuilder(openTsdbUrl).sync(30_000).returnDetails();
final OpenTsdbBolt openTsdbBolt = new OpenTsdbBolt(builder, Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
openTsdbBolt.withBatchSize(10).withFlushInterval(2000);
topologyBuilder.setBolt("opentsdb", openTsdbBolt).shuffleGrouping("metric-gen");

----------------------------------------

TITLE: Prepared Bolt with State in Clojure
DESCRIPTION: Example of a stateful bolt implementation that maintains word counts. Shows how to use prepared bolts to maintain state between tuple processing.

LANGUAGE: clojure
CODE:
(defbolt word-count ["word" "count"] {:prepare true}
  [conf context collector]
  (let [counts (atom {})]
    (bolt
     (execute [tuple]
       (let [word (.getString tuple 0)]
         (swap! counts (partial merge-with +) {word 1})
         (emit-bolt! collector [word (@counts word)] :anchor tuple)
         (ack! collector tuple)
         )))))

----------------------------------------

TITLE: Storm Topology Submission API Definition
DESCRIPTION: Thrift API definition for submitting a topology to Storm, including parameters for name, jar location, configuration, and topology structure.

LANGUAGE: thrift
CODE:
void submitTopology(1: string name, 2: string uploadedJarLocation, 3: string jsonConf, 4: StormTopology topology)
    throws (1: AlreadyAliveException e, 2: InvalidTopologyException ite);

----------------------------------------

TITLE: Configuring DRPC Authorization ACL in YAML
DESCRIPTION: Example YAML configuration for setting up access control lists for DRPC operations.

LANGUAGE: yaml
CODE:
drpc.authorizer.acl:
   "functionName1":
     "client.users":
       - "alice"
       - "bob"
     "invocation.user": "bob"

----------------------------------------

TITLE: Configuring Pacemaker Servers
DESCRIPTION: YAML configuration specifying the hosts running Pacemaker daemons.

LANGUAGE: yaml
CODE:
pacemaker.servers:\n    - somehost.mycompany.com\n    - someotherhost.mycompany.com

----------------------------------------

TITLE: Implementing Multiple Query Insert from Single Tuple
DESCRIPTION: Demonstrates how to insert data into multiple tables from a single input tuple using CassandraWriterBolt.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
    async(
        simpleQuery("INSERT INTO titles_per_album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all())),
        simpleQuery("INSERT INTO titles_per_performer (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all()))
    )
);

----------------------------------------

TITLE: Implementing PartialUniquer Bolt for DRPC in Java
DESCRIPTION: Shows the implementation of a PartialUniquer bolt used in the Twitter reach DRPC topology to process batches of follower tuples.

LANGUAGE: java
CODE:
public class PartialUniquer extends BaseBatchBolt {
    BatchOutputCollector _collector;
    Object _id;
    Set<String> _followers = new HashSet<String>();
    
    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
        _collector = collector;
        _id = id;
    }

    @Override
    public void execute(Tuple tuple) {
        _followers.add(tuple.getString(1));
    }
    
    @Override
    public void finishBatch() {
        _collector.emit(new Values(_id, _followers.size()));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "partial-count"));
    }
}

----------------------------------------

TITLE: Retrieving Nimbus Summary in Storm UI REST API
DESCRIPTION: Example JSON response for the /api/v1/nimbus/summary endpoint, which returns summary information for all Nimbus hosts in the cluster.

LANGUAGE: json
CODE:
{
    "nimbuses":[
        {
            "host":"192.168.202.1",
            "port":6627,
            "nimbusLogLink":"http:\/\/192.168.202.1:8000\/log?file=nimbus.log",
            "status":"Leader",
            "version":"0.10.0-SNAPSHOT",
            "nimbusUpTime":"3m 33s",
            "nimbusUpTimeSeconds":"213"
        }
    ]
}

----------------------------------------

TITLE: Creating External Tables in Storm SQL
DESCRIPTION: Example of creating an external table in Storm SQL to specify a Kafka spout and sink.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE FOO (ID INT PRIMARY KEY) LOCATION 'kafka://test?bootstrap-hosts=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'

----------------------------------------

TITLE: Defining Storm Component Object Structure in Thrift
DESCRIPTION: Thrift union definition for ComponentObject that specifies how Storm components (spouts/bolts) can be implemented either as serialized Java objects, shell components, or Java object references.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Local Mode Implementation using Java Queues in Apache Storm (Clojure)
DESCRIPTION: Implementation of the message sending protocol using in-memory Java queues for local mode in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/local.clj

----------------------------------------

TITLE: Local Mode Implementation using Java Queues in Apache Storm (Clojure)
DESCRIPTION: Implementation of the message sending protocol using in-memory Java queues for local mode in Apache Storm.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/local.clj

----------------------------------------

TITLE: ZeroMQ Implementation for Distributed Mode in Apache Storm (Clojure)
DESCRIPTION: This code implements the distributed mode message passing using ZeroMQ in Storm.

LANGUAGE: Clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/zmq.clj

----------------------------------------

TITLE: Registering a Metrics Consumer in Java
DESCRIPTION: Example of how to register a metrics consumer to a Storm topology configuration in Java code.

LANGUAGE: java
CODE:
conf.registerMetricsConsumer(org.apache.storm.metric.LoggingMetricsConsumer.class, 1);

----------------------------------------

TITLE: Using Static Factory Methods in Flux
DESCRIPTION: YAML and Java code demonstrating how to use static factory methods with Flux.

LANGUAGE: java
CODE:
public class TestBolt extends BaseBasicBolt {
  public static TestBolt newInstance(Duration triggerTime) {
    return new TestBolt(triggerTime);
  }
}

public class Duration {
  public static Duration ofSeconds(long seconds) {
    return new Duration(seconds);
  }
}

LANGUAGE: yaml
CODE:
components:
  - id: "time"
    className: "java.time.Duration"
    factory: "ofSeconds"

bolts:
  - id: "testBolt"
    className: "org.apache.storm.flux.test.TestBolt"
    factory: "newInstance"
    factoryArgs:
      - ref: "time"

----------------------------------------

TITLE: HTML Redirect to Storm Configuration Documentation
DESCRIPTION: HTML meta refresh tag and canonical link element that redirects users to the current Storm configuration documentation page at storm.apache.org.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Configuration.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Configuration.html" />

----------------------------------------

TITLE: Implementing One Aggregator for Unique Counting in Trident
DESCRIPTION: Defines a 'One' aggregator used in the reach calculation topology to emit a single tuple containing the number one for each group, facilitating unique counting.

LANGUAGE: java
CODE:
public class One implements CombinerAggregator<Integer> {
   public Integer init(TridentTuple tuple) {
       return 1;
   }

   public Integer combine(Integer val1, Integer val2) {
       return 1;
   }

   public Integer zero() {
       return 1;
   }        
}

----------------------------------------

TITLE: Configuring ExpandUrl Bolt with Fields Grouping in Storm
DESCRIPTION: This snippet shows how to configure an ExpandUrl bolt with fields grouping in a Storm topology. This approach ensures that the same URL always goes to the same task, improving cache efficiency by reducing duplication across tasks.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .fieldsGrouping("urls", new Fields("url"));

----------------------------------------

TITLE: Sample REST API Call with curl
DESCRIPTION: Example of making a REST API call to Storm using curl to request cluster configuration

LANGUAGE: no-highlight
CODE:
# Request the cluster configuration.
# Note: We assume ui.port is configured to the default value of 8080.
$ curl http://<ui-host>:8080/api/v1/cluster/configuration

----------------------------------------

TITLE: Custom MQTT Message Mapper Implementation in Java
DESCRIPTION: Example implementation of MqttMessageMapper that converts MQTT messages with specific topic and payload formats into Storm tuples.

LANGUAGE: java
CODE:
public class CustomMessageMapper implements MqttMessageMapper {
    private static final Logger LOG = LoggerFactory.getLogger(CustomMessageMapper.class);


    public Values toValues(MqttMessage message) {
        String topic = message.getTopic();
        String[] topicElements = topic.split("/");
        String[] payloadElements = new String(message.getMessage()).split("/");

        return new Values(topicElements[2], topicElements[4], topicElements[3], Float.parseFloat(payloadElements[0]), 
                Float.parseFloat(payloadElements[1]));
    }

    public Fields outputFields() {
        return new Fields("user", "deviceId", "location", "temperature", "humidity");
    }
}

----------------------------------------

TITLE: Dequeuing Items from Kestrel Queue in Java
DESCRIPTION: This method dequeues items from a Kestrel queue without removing them. It iterates 12 times, attempting to dequeue an item each time and printing the received value if successful.

LANGUAGE: java
CODE:
    private static void dequeueItems(KestrelClient kestrelClient, String queueName) throws IOException, ParseError
			 {
		for(int i=1; i<=12; i++){

			Item item = kestrelClient.dequeue(queueName);

			if(item==null){
				System.out.println("The queue (" + queueName + ") contains no items.");
			}
			else
			{
				byte[] data = item._data;

				String receivedVal = new String(data);

				System.out.println("receivedItem=" + receivedVal);
			}
		}

----------------------------------------

TITLE: Creating Worker Process
DESCRIPTION: The mk-worker function sets up a worker process, connecting to other workers, monitoring topology activity, and launching task threads.

LANGUAGE: clojure
CODE:
(mk-worker)

----------------------------------------

TITLE: Parameterized Storm Spout in Clojure
DESCRIPTION: Implementation of a parameterized spout that can be configured with custom sentences at runtime.

LANGUAGE: clojure
CODE:
(defspout sentence-spout-parameterized ["word"] {:params [sentences] :prepare false}
  [collector]
  (Thread/sleep 500)
  (emit-spout! collector [(rand-nth sentences)]))

----------------------------------------

TITLE: Configuring Isolation Scheduler in YAML
DESCRIPTION: Example YAML configuration for the Isolation Scheduler, specifying the number of isolated machines for different topologies. This configuration allows administrators to allocate dedicated resources to specific topologies for improved multi-tenancy and resource management.

LANGUAGE: yaml
CODE:
isolation.scheduler.machines: 
    "my-topology": 8
    "tiny-topology": 1
    "some-other-topology": 3

----------------------------------------

TITLE: Defining Connection Provider Interface in Java
DESCRIPTION: Interface defining the contract for database connection providers that handle connection pooling. Includes methods for preparation, getting connections, and cleanup.

LANGUAGE: java
CODE:
public interface ConnectionProvider extends Serializable {
    /**
     * method must be idempotent.
     */
    void prepare();

    /**
     *
     * @return a DB connection over which the queries can be executed.
     */
    Connection getConnection();

    /**
     * called once when the system is shutting down, should be idempotent.
     */
    void cleanup();
}

----------------------------------------

TITLE: Joining Streams in Trident
DESCRIPTION: Example of joining two streams based on specific fields.

LANGUAGE: java
CODE:
topology.join(stream1, new Fields("key"), stream2, new Fields("x"), new Fields("key", "a", "b", "c"));

----------------------------------------

TITLE: Configuring Storm Metricstore with YAML
DESCRIPTION: YAML configuration options for setting up the Storm Metricstore, including the MetricStore class, WorkerMetricsProcessor class, RocksDB location, database creation, metadata cache capacity, and metric retention period.

LANGUAGE: yaml
CODE:
storm.metricstore.class: "org.apache.storm.metricstore.rocksdb.RocksDbStore"
storm.metricprocessor.class: "org.apache.storm.metricstore.NimbusMetricProcessor"
storm.metricstore.rocksdb.location: "storm_rocks"
storm.metricstore.rocksdb.create_if_missing: true
storm.metricstore.rocksdb.metadata_string_cache_capacity: 4000
storm.metricstore.rocksdb.retention_hours: 240

----------------------------------------

TITLE: Storm Component Structure Definition
DESCRIPTION: Thrift definition that outlines the core structure of Storm components (spouts and bolts), including ComponentObject and ComponentCommon structs that define implementation and configuration details.

LANGUAGE: thrift
CODE:
struct ComponentObject {
  1: binary serialized_java
  2: ShellComponent shell
  3: JavaObject java_object
}

struct ComponentCommon {
  1: map<string, StreamInfo> streams
  2: map<GlobalStreamId, Grouping> inputs
  3: i32 parallelism_hint
  4: map<string, string> json_conf
}

----------------------------------------

TITLE: Starting Apache ActiveMQ for Storm JMS Example
DESCRIPTION: Command to start Apache ActiveMQ, which is used as the JMS provider for the example topology. This assumes ActiveMQ is installed and ACTIVEMQ_HOME is set.

LANGUAGE: bash
CODE:
$ [ACTIVEMQ_HOME]/bin/activemq

----------------------------------------

TITLE: Initializing SimpleJdbcMapper in Java for Storm JDBC
DESCRIPTION: Example of creating a SimpleJdbcMapper instance for transforming storm tuples to database rows. It shows setting up the HikariCP configuration and creating the mapper with a table name and connection provider.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);
String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

----------------------------------------

TITLE: Defining IWindowedBolt Interface for Storm Windowing
DESCRIPTION: Interface definition for bolts that require windowing support in Storm. It includes methods for preparation, execution of windowed operations, and cleanup.

LANGUAGE: java
CODE:
public interface IWindowedBolt extends IComponent {
    void prepare(Map stormConf, TopologyContext context, OutputCollector collector);
    /**
     * Process tuples falling within the window and optionally emit 
     * new tuples based on the tuples in the input window.
     */
    void execute(TupleWindow inputWindow);
    void cleanup();
}

----------------------------------------

TITLE: Defining IWindowedBolt Interface for Storm Windowing
DESCRIPTION: Interface definition for bolts that require windowing support in Storm. It includes methods for preparation, execution of windowed operations, and cleanup.

LANGUAGE: java
CODE:
public interface IWindowedBolt extends IComponent {
    void prepare(Map stormConf, TopologyContext context, OutputCollector collector);
    /**
     * Process tuples falling within the window and optionally emit 
     * new tuples based on the tuples in the input window.
     */
    void execute(TupleWindow inputWindow);
    void cleanup();
}

----------------------------------------

TITLE: Creating a Simple KafkaSpout Configuration
DESCRIPTION: Example of how to create a basic KafkaSpout configuration to consume messages from a Kafka topic and emit them into a Storm topology.

LANGUAGE: java
CODE:
final TopologyBuilder tp = new TopologyBuilder();
tp.setSpout("kafka_spout", new KafkaSpout<>(KafkaSpoutConfig.builder("127.0.0.1:" + port, "topic").build()), 1);
tp.setBolt("bolt", new myBolt()).shuffleGrouping("kafka_spout");

----------------------------------------

TITLE: Storm Component Configuration Override Keys
DESCRIPTION: Lists the configuration keys that can be overridden on a per-bolt/per-spout basis in Storm 0.7.0+

LANGUAGE: text
CODE:
1. "topology.debug"
2. "topology.max.spout.pending"
3. "topology.max.task.parallelism"
4. "topology.kryo.register"

----------------------------------------

TITLE: Creating ConfigLoader Instance
DESCRIPTION: Factory method to create an IConfigLoader instance based on the scheme of the scheduler.config.loader.uri

LANGUAGE: java
CODE:
ConfigLoaderFactoryService.createConfigLoader(Map<String, Object> conf)

----------------------------------------

TITLE: Python Split Sentence Bolt Implementation
DESCRIPTION: Example of implementing a Storm bolt in Python that splits sentences into words.

LANGUAGE: python
CODE:
import storm

class SplitSentenceBolt(storm.BasicBolt):
    def process(self, tup):
        words = tup.values[0].split(" ")
        for word in words:
          storm.emit([word])

SplitSentenceBolt().run()

----------------------------------------

TITLE: Configuring URL Expansion with Shuffle Grouping in Storm
DESCRIPTION: Demonstrates URL expansion bolt configuration using shuffle grouping, which distributes tuples randomly across tasks. This approach is less cache-efficient as the same URL may be processed by different tasks.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .shuffleGrouping(1);

----------------------------------------

TITLE: Error Response in Storm UI REST API
DESCRIPTION: Example JSON error response returned by the API in case of internal server errors.

LANGUAGE: json
CODE:
{
  "error": "Internal Server Error",
  "errorMessage": "java.lang.NullPointerException\n\tat clojure.core$name.invoke(core.clj:1505)\n\tat org.apache.storm.ui.core$component_page.invoke(core.clj:752)\n\tat org.apache.storm.ui.core$fn__7766.invoke(core.clj:782)\n\tat compojure.core$make_route$fn__5755.invoke(core.clj:93)\n\tat compojure.core$if_route$fn__5743.invoke(core.clj:39)\n\tat compojure.core$if_method$fn__5736.invoke(core.clj:24)\n\tat compojure.core$routing$fn__5761.invoke(core.clj:106)\n\tat clojure.core$some.invoke(core.clj:2443)\n\tat compojure.core$routing.doInvoke(core.clj:106)\n\tat clojure.lang.RestFn.applyTo(RestFn.java:139)\n\tat clojure.core$apply.invoke(core.clj:619)\n\tat compojure.core$routes$fn__5765.invoke(core.clj:111)\n\tat ring.middleware.reload$wrap_reload$fn__6880.invoke(reload.clj:14)\n\tat org.apache.storm.ui.core$catch_errors$fn__7800.invoke(core.clj:836)\n\tat ring.middleware.keyword_params$wrap_keyword_params$fn__6319.invoke(keyword_params.clj:27)\n\tat ring.middleware.nested_params$wrap_nested_params$fn__6358.invoke(nested_params.clj:65)\n\tat ring.middleware.params$wrap_params$fn__6291.invoke(params.clj:55)\n\tat ring.middleware.multipart_params$wrap_multipart_params$fn__6386.invoke(multipart_params.clj:103)\n\tat ring.middleware.flash$wrap_flash$fn__6675.invoke(flash.clj:14)\n\tat ring.middleware.session$wrap_session$fn__6664.invoke(session.clj:43)\n\tat ring.middleware.cookies$wrap_cookies$fn__6595.invoke(cookies.clj:160)\n\tat ring.adapter.jetty$proxy_handler$fn__6112.invoke(jetty.clj:16)\n\tat ring.adapter.jetty.proxy$org.mortbay.jetty.handler.AbstractHandler$0.handle(Unknown Source)\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n\tat org.mortbay.jetty.Server.handle(Server.java:326)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n\tat org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n\tat org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n"
}

----------------------------------------

TITLE: Configuring Maven Shade Plugin for HDFS Integration
DESCRIPTION: XML configuration for the Maven Shade Plugin to properly package a Storm topology with HDFS integration.

LANGUAGE: xml
CODE:
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-shade-plugin</artifactId>
    <version>1.4</version>
    <configuration>
        <createDependencyReducedPom>true</createDependencyReducedPom>
    </configuration>
    <executions>
        <execution>
            <phase>package</phase>
            <goals>
                <goal>shade</goal>
            </goals>
            <configuration>
                <transformers>
                    <transformer
                            implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                    <transformer
                            implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                        <mainClass></mainClass>
                    </transformer>
                </transformers>
            </configuration>
        </execution>
    </executions>
</plugin>

----------------------------------------

TITLE: ISpout Interface Definition
DESCRIPTION: The core interface that spouts implement in Storm, showing the essential methods for message processing and reliability.

LANGUAGE: java
CODE:
public interface ISpout extends Serializable {
    void open(Map conf, TopologyContext context, SpoutOutputCollector collector);
    void close();
    void nextTuple();
    void ack(Object msgId);
    void fail(Object msgId);
}

----------------------------------------

TITLE: Implementing MqttTupleMapper Interface in Java
DESCRIPTION: Definition of the MqttTupleMapper interface used to map Storm tuple data to MQTT messages.

LANGUAGE: java
CODE:
public interface MqttTupleMapper extends Serializable{

    MqttMessage toMessage(ITuple tuple);

}

----------------------------------------

TITLE: Configuring and Using KafkaTridentState in a Trident Topology
DESCRIPTION: Example of how to configure KafkaTridentState with producer properties, topic selection, and tuple mapping, then use it in a Trident topology to write data to Kafka.

LANGUAGE: java
CODE:
Fields fields = new Fields("word", "count");
FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
        new Values("storm", "1"),
        new Values("trident", "1"),
        new Values("needs", "1"),
        new Values("javadoc", "1")
);
spout.setCycle(true);

TridentTopology topology = new TridentTopology();
Stream stream = topology.newStream("spout1", spout);

//set producer properties.
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "1");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

TridentKafkaStateFactory stateFactory = new TridentKafkaStateFactory()
        .withProducerProperties(props)
        .withKafkaTopicSelector(new DefaultTopicSelector("test"))
        .withTridentTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper("word", "count"));
stream.partitionPersist(stateFactory, fields, new TridentKafkaStateUpdater(), new Fields());

Config conf = new Config();
StormSubmitter.submitTopology("kafkaTridentTest", conf, topology.build());

----------------------------------------

TITLE: Emitting Tuple in JSON Format for Storm Multi-Language Protocol
DESCRIPTION: This JSON structure represents an emit command in the Storm Multi-Language Protocol. It includes the command type, anchor tuple ids, stream id, task id (for direct emit), and the tuple values to be emitted.

LANGUAGE: JSON
CODE:
{
    "command": "emit",
    "anchors": [1231231, -234234234],
    "stream": 1,
    "task": 9,
    "tuple": ["field1", 2, 3]
}

----------------------------------------

TITLE: Retrieving Owner Resources in JSON
DESCRIPTION: Sample response from the /api/v1/owner-resources endpoint, showing resource usage aggregated by topology owner.

LANGUAGE: json
CODE:
{
  "owners": [
    {
      "totalReqOnHeapMem": 896,
      "owner": "ownerA",
      "totalExecutors": 7,
      "cpuGuaranteeRemaining": 30,
      "totalReqMem": 896,
      "cpuGuarantee": 100,
      "isolatedNodes": "N/A",
      "memoryGuarantee": 4000,
      "memoryGuaranteeRemaining": 3104,
      "totalTasks": 7,
      "totalMemoryUsage": 896,
      "totalReqOffHeapMem": 0,
      "totalReqCpu": 70,
      "totalWorkers": 2,
      "totalCpuUsage": 70,
      "totalAssignedOffHeapMem": 0,
      "totalAssignedOnHeapMem": 896,
      "totalTopologies": 1
    }
  ],
  "schedulerDisplayResource": true
}

----------------------------------------

TITLE: Nimbus Topology Monitoring (Clojure)
DESCRIPTION: Nimbus monitors topologies using a finite state machine approach, periodically calling reassign-topology to check heartbeats and reassign workers if necessary.

LANGUAGE: clojure
CODE:
(defn monitor-topologies []
  (doseq [topology (get-active-topologies)]
    (reassign-topology topology)))

(defn reassign-topology [topology]
  (let [assignments (mk-assignments topology)]
    (update-zk-assignments assignments)))

----------------------------------------

TITLE: Configuring Trident State for HDFS in Java
DESCRIPTION: Example of how to configure a Trident State implementation for writing data to HDFS.

LANGUAGE: java
CODE:
Fields hdfsFields = new Fields("field1", "field2");

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withPath("/trident")
        .withPrefix("trident")
        .withExtension(".txt");

RecordFormat recordFormat = new DelimitedRecordFormat()
        .withFields(hdfsFields);

FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, FileSizeRotationPolicy.Units.MB);

HdfsState.Options options = new HdfsState.HdfsFileOptions()
        .withFileNameFormat(fileNameFormat)
        .withRecordFormat(recordFormat)
        .withRotationPolicy(rotationPolicy)
        .withFsUrl("hdfs://localhost:54310");

StateFactory factory = new HdfsStateFactory().withOptions(options);

TridentState state = stream
        .partitionPersist(factory, hdfsFields, new HdfsUpdater(), new Fields());

----------------------------------------

TITLE: Creating RedisFilterBolt in Java
DESCRIPTION: Shows how to create a RedisFilterBolt using a JedisPoolConfig and a custom RedisFilterMapper.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisFilterMapper filterMapper = new BlacklistWordFilterMapper();
RedisFilterBolt filterBolt = new RedisFilterBolt(poolConfig, filterMapper);

----------------------------------------

TITLE: Creating RedisFilterBolt in Java
DESCRIPTION: Shows how to create a RedisFilterBolt using a JedisPoolConfig and a custom RedisFilterMapper.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisFilterMapper filterMapper = new BlacklistWordFilterMapper();
RedisFilterBolt filterBolt = new RedisFilterBolt(poolConfig, filterMapper);

----------------------------------------

TITLE: Storm Nimbus Metrics Table
DESCRIPTION: Markdown table structure showing Nimbus-specific metrics with names, types and descriptions

LANGUAGE: markdown
CODE:
| Metric Name | Type | Description |
|-------------|------|-------------|
| nimbus:files-upload-duration-ms | timer | Time it takes to upload a file... |

----------------------------------------

TITLE: Storm Package Structure Overview
DESCRIPTION: A comprehensive list of Storm's main Java packages and Clojure namespaces with their purposes. Each package contains specific functionality like coordination, DRPC, serialization, etc.

LANGUAGE: text
CODE:
org.apache.storm.coordination - Batch processing coordination
org.apache.storm.drpc - DRPC implementation
org.apache.storm.generated - Generated Thrift code
org.apache.storm.grouping - Custom stream groupings
org.apache.storm.hooks - Event hook interfaces
org.apache.storm.serialization - Tuple serialization
org.apache.storm.spout - Spout definitions
org.apache.storm.task - Bolt definitions and contexts
org.apache.storm.topology - Java API and builder
org.apache.storm.tuple - Tuple data model
org.apache.storm.utils - Utilities and data structures
org.apache.storm.daemon.* - Core daemon implementations

----------------------------------------

TITLE: Initializing HiveBolt with Basic Configuration
DESCRIPTION: Basic setup of HiveBolt using DelimitedRecordHiveMapper to map tuple fields to Hive columns.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames));
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper);
HiveBolt hiveBolt = new HiveBolt(hiveOptions);

----------------------------------------

TITLE: OCI Runtime Configuration JSON
DESCRIPTION: Example configuration file (oci-config.json) defining container settings, mounts, resources and security parameters.

LANGUAGE: json
CODE:
{"version": "0.1","username": "username1","containerId": "6703-1a23ca4b-6062-4d08-8ac3-b09e7d35e7cb",...}

----------------------------------------

TITLE: Configuring HDFS Bolt in Java
DESCRIPTION: Example of configuring an HdfsBolt to write pipe-delimited files to HDFS with sync and rotation policies. The bolt is configured to sync after 1000 tuples and rotate files at 5MB size.

LANGUAGE: java
CODE:
RecordFormat format = new DelimitedRecordFormat()
        .withFieldDelimiter("|");

SyncPolicy syncPolicy = new CountSyncPolicy(1000);

FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withPath("/foo/");

HdfsBolt bolt = new HdfsBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withRecordFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy);

----------------------------------------

TITLE: Using Trident API for Inserting Data into Cassandra in Java
DESCRIPTION: Shows how to use the Trident API to insert data into Cassandra using the CassandraState and CassandraStateFactory classes.

LANGUAGE: java
CODE:
CassandraState.Options options = new CassandraState.Options(new CassandraContext());
CQLStatementTupleMapper insertTemperatureValues = boundQuery(
        "INSERT INTO weather.temperature(weather_station_id, weather_station_name, event_time, temperature) VALUES(?, ?, ?, ?)")
        .bind(with(field("weather_station_id"), field("name").as("weather_station_name"), field("event_time").now(), field("temperature")));
options.withCQLStatementTupleMapper(insertTemperatureValues);
CassandraStateFactory insertValuesStateFactory =  new CassandraStateFactory(options);
TridentState selectState = topology.newStaticState(selectWeatherStationStateFactory);
stream = stream.stateQuery(selectState, new Fields("weather_station_id"), new CassandraQuery(), new Fields("name"));
stream = stream.each(new Fields("name"), new PrintFunction(), new Fields("name_x"));
stream.partitionPersist(insertValuesStateFactory, new Fields("weather_station_id", "name", "event_time", "temperature"), new CassandraStateUpdater(), new Fields());

----------------------------------------

TITLE: Listing Spout Implementations in Markdown
DESCRIPTION: A markdown-formatted list of links to various spout implementations for Apache Storm, including brief descriptions of each spout's purpose.

LANGUAGE: markdown
CODE:
* [storm-kestrel](https://github.com/nathanmarz/storm-kestrel): Adapter to use Kestrel as a spout
* [storm-amqp-spout](https://github.com/rapportive-oss/storm-amqp-spout): Adapter to use AMQP source as a spout
* [storm-jms](https://github.com/ptgoetz/storm-jms): Adapter to use a JMS source as a spout
* [storm-redis-pubsub](https://github.com/sorenmacbeth/storm-redis-pubsub): A spout that subscribes to a Redis pubsub stream
* [storm-beanstalkd-spout](https://github.com/haitaoyao/storm-beanstalkd-spout): A spout that subscribes to a beanstalkd queue

----------------------------------------

TITLE: Configuring Storm Topology for Secure HBase in Java
DESCRIPTION: Example of how to configure a Storm topology to work with secure HBase using Kerberos authentication.

LANGUAGE: java
CODE:
Config config = new Config();
...
config.put("storm.keytab.file", "$keytab");
config.put("storm.kerberos.principal", "$principal");
StormSubmitter.submitTopology("$topologyName", config, builder.createTopology());

----------------------------------------

TITLE: Configuring HDFS Sequence File Bolt in Java
DESCRIPTION: Example of how to configure and use the SequenceFileBolt to write sequence files to HDFS with compression and custom formats.

LANGUAGE: java
CODE:
// sync the filesystem after every 1k tuples
SyncPolicy syncPolicy = new CountSyncPolicy(1000);

// rotate files when they reach 5MB
FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);

FileNameFormat fileNameFormat = new DefaultFileNameFormat()
        .withExtension(".seq")
        .withPath("/data/");

// create sequence format instance.
DefaultSequenceFormat format = new DefaultSequenceFormat("timestamp", "sentence");

SequenceFileBolt bolt = new SequenceFileBolt()
        .withFsUrl("hdfs://localhost:54310")
        .withFileNameFormat(fileNameFormat)
        .withSequenceFormat(format)
        .withRotationPolicy(rotationPolicy)
        .withSyncPolicy(syncPolicy)
        .withCompressionType(SequenceFile.CompressionType.RECORD)
        .withCompressionCodec("deflate");

----------------------------------------

TITLE: Implementing Skewed Top N Processing in Storm
DESCRIPTION: Shows an optimized approach for handling skewed data in top N calculations using partial key grouping. This distributes load for each key between two downstream bolts and includes an aggregation layer.

LANGUAGE: java
CODE:
builder.setBolt("count", new CountObjects(), parallelism)
  .partialKeyGrouping("objects", new Fields("value"));
builder.setBolt("rank" new AggregateCountsAndRank(), parallelism)
  .fieldsGrouping("count", new Fields("key"))
builder.setBolt("merge", new MergeRanksObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: RotatingMap Implementation for Tuple Tracking
DESCRIPTION: Implementation of a time-based expiration system using the RotatingMap data structure. The map maintains multiple buckets of records that expire at different times, providing O(1) access time while efficiently managing tuple timeouts and expirations.



----------------------------------------

TITLE: Configuring JAAS for Storm and Zookeeper
DESCRIPTION: Example JAAS configuration file for Storm and Zookeeper authentication

LANGUAGE: yaml
CODE:
StormServer {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/storm.keytab"
   storeKey=true
   useTicketCache=false
   principal="storm/storm.example.com@STORM.EXAMPLE.COM";
};
StormClient {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/storm.keytab"
   storeKey=true
   useTicketCache=false
   serviceName="storm"
   principal="storm@STORM.EXAMPLE.COM";
};
Client {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/storm.keytab"
   storeKey=true
   useTicketCache=false
   serviceName="zookeeper"
   principal="storm@STORM.EXAMPLE.COM";
};
Server {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/keytabs/zk.keytab"
   storeKey=true
   useTicketCache=false
   serviceName="zookeeper"
   principal="zookeeper/zk1.example.com@STORM.EXAMPLE.COM";
};

----------------------------------------

TITLE: Implementing Druid Beam Factory
DESCRIPTION: Sample implementation of DruidBeamFactory showing configuration of Druid connection, data source settings, dimensions, aggregators, and beam tuning parameters.

LANGUAGE: java
CODE:
public class SampleDruidBeamFactoryImpl implements DruidBeamFactory<Map<String, Object>> {

    @Override
    public Beam<Map<String, Object>> makeBeam(Map<?, ?> conf, IMetricsContext metrics) {


        final String indexService = "druid/overlord"; // The druid.service name of the indexing service Overlord node.
        final String discoveryPath = "/druid/discovery"; // Curator service discovery path. config: druid.discovery.curator.path
        final String dataSource = "test"; //The name of the ingested datasource. Datasources can be thought of as tables.
        final List<String> dimensions = ImmutableList.of("publisher", "advertiser");
        List<AggregatorFactory> aggregators = ImmutableList.<AggregatorFactory>of(
                new CountAggregatorFactory(
                        "click"
                )
        );
        // Tranquility needs to be able to extract timestamps from your object type (in this case, Map<String, Object>).
        final Timestamper<Map<String, Object>> timestamper = new Timestamper<Map<String, Object>>()
        {
            @Override
            public DateTime timestamp(Map<String, Object> theMap)
            {
                return new DateTime(theMap.get("timestamp"));
            }
        };

        // Tranquility uses ZooKeeper (through Curator) for coordination.
        final CuratorFramework curator = CuratorFrameworkFactory
                .builder()
                .connectString((String)conf.get("druid.tranquility.zk.connect")) //take config from storm conf
                .retryPolicy(new ExponentialBackoffRetry(1000, 20, 30000))
                .build();
        curator.start();

        // The JSON serialization of your object must have a timestamp field in a format that Druid understands. By default,
        // Druid expects the field to be called "timestamp" and to be an ISO8601 timestamp.
        final TimestampSpec timestampSpec = new TimestampSpec("timestamp", "auto", null);

        // Tranquility needs to be able to serialize your object type to JSON for transmission to Druid. By default this is
        // done with Jackson. If you want to provide an alternate serializer, you can provide your own via ```.objectWriter(...)```.
        // In this case, we won't provide one, so we're just using Jackson.
        final Beam<Map<String, Object>> beam = DruidBeams
                .builder(timestamper)
                .curator(curator)
                .discoveryPath(discoveryPath)
                .location(DruidLocation.create(indexService, dataSource))
                .timestampSpec(timestampSpec)
                .rollup(DruidRollup.create(DruidDimensions.specific(dimensions), aggregators, QueryGranularities.MINUTE))
                .tuning(
                        ClusteredBeamTuning
                                .builder()
                                .segmentGranularity(Granularity.HOUR)
                                .windowPeriod(new Period("PT10M"))
                                .partitions(1)
                                .replicants(1)
                                .build()
                )
                .druidBeamConfig(
                      DruidBeamConfig
                           .builder()
                           .indexRetryPeriod(new Period("PT10M"))
                           .build())
                .buildBeam();

        return beam;
    }
}

----------------------------------------

TITLE: Creating Blobs via Command Line
DESCRIPTION: Example command showing how to create a blob in Storm's distributed cache with README.txt file and specified access permissions.

LANGUAGE: bash
CODE:
storm blobstore create --file README.txt --acl o::rwa --replication-factor 4 key1

----------------------------------------

TITLE: Storm Topology Submission Method Definition
DESCRIPTION: Thrift method definition for submitting a topology to Storm, including required parameters and possible exceptions.

LANGUAGE: thrift
CODE:
void submitTopology(1: string name, 2: string uploadedJarLocation, 3: string jsonConf, 4: StormTopology topology)
    throws (1: AlreadyAliveException e, 2: InvalidTopologyException ite);

----------------------------------------

TITLE: Configuring Pacemaker Servers
DESCRIPTION: YAML configuration for specifying Pacemaker server hosts.

LANGUAGE: yaml
CODE:
pacemaker.servers:
    - somehost.mycompany.com
    - someotherhost.mycompany.com

----------------------------------------

TITLE: Retrieving Cluster Configuration in JSON
DESCRIPTION: Example response from the /api/v1/cluster/configuration endpoint showing cluster configuration settings in JSON format.

LANGUAGE: json
CODE:
{
  "dev.zookeeper.path": "/tmp/dev-storm-zookeeper",
  "topology.tick.tuple.freq.secs": null,
  "topology.builtin.metrics.bucket.size.secs": 60,
  "topology.fall.back.on.java.serialization": false,
  "topology.max.error.report.per.interval": 5,
  "zmq.linger.millis": 5000,
  "topology.skip.missing.kryo.registrations": false,
  "storm.messaging.netty.client_worker_threads": 1,
  "ui.childopts": "-Xmx768m",
  "storm.zookeeper.session.timeout": 20000,
  "nimbus.reassign": true,
  "topology.trident.batch.emit.interval.millis": 500,
  "storm.messaging.netty.flush.check.interval.ms": 10,
  "nimbus.monitor.freq.secs": 10,
  "logviewer.childopts": "-Xmx128m",
  "java.library.path": "/usr/local/lib:/opt/local/lib:/usr/lib",
  "topology.executor.send.buffer.size": 1024
}

----------------------------------------

TITLE: Implementing MongoDB Update Mapper - Java
DESCRIPTION: Implementation of MongoUpdateMapper interface for updating MongoDB documents using $set operator.

LANGUAGE: java
CODE:
public class SimpleMongoUpdateMapper extends SimpleMongoMapper implements MongoUpdateMapper {

    private String[] fields;

    @Override
    public Document toDocument(ITuple tuple) {
        Document document = new Document();
        for(String field : fields){
            document.append(field, tuple.getValueByField(field));
        }
        return new Document("$set", document);
    }

    public SimpleMongoUpdateMapper withFields(String... fields) {
        this.fields = fields;
        return this;
    }
}

----------------------------------------

TITLE: Installing JZMQ for Apache Storm
DESCRIPTION: This snippet demonstrates the process of installing JZMQ, the Java bindings for ZeroMQ, from a specific fork that is tested to work with Storm. It involves cloning the repository, running the autogen script, configuring, and installing.

LANGUAGE: bash
CODE:
#install jzmq
git clone https://github.com/nathanmarz/jzmq.git
cd jzmq
./autogen.sh
./configure
make
sudo make install

----------------------------------------

TITLE: Configuring URL Expansion with Shuffle Grouping in Storm
DESCRIPTION: Demonstrates setting up a bolt for URL expansion using shuffle grouping, which distributes tuples randomly across bolt tasks. This approach is less cache-efficient as the same URL may be processed by different tasks.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .shuffleGrouping(1);

----------------------------------------

TITLE: Registering Event Logger in Java Configuration
DESCRIPTION: Example of registering a FileBasedEventLogger in Storm topology configuration using Java code.

LANGUAGE: java
CODE:
conf.registerEventLogger(org.apache.storm.metric.FileBasedEventLogger.class);

----------------------------------------

TITLE: Storm Docker Configuration Example
DESCRIPTION: Sample configuration for enabling Docker support in Storm on RHEL7. Includes settings for resource isolation plugin, allowed Docker images, cgroups, and worker launcher.

LANGUAGE: bash
CODE:
storm.resource.isolation.plugin.enable: true
storm.resource.isolation.plugin: "org.apache.storm.container.docker.DockerManager"
storm.oci.allowed.images: ["xxx.xxx.com:8080/storm/storm/rhel7:latest"]
storm.oci.image: "xxx.xxx.com:8080/storm/storm/rhel7:latest"
storm.oci.cgroup.root: "/storm"
storm.oci.cgroup.parent: "/sys/fs/cgroup"
storm.oci.readonly.bindmounts:
    - "/etc/storm"
storm.oci.nscd.dir: "/var/run/nscd"
supervisor.worker.launcher: "/usr/share/apache-storm-2.3.0/bin/worker-launcher"

----------------------------------------

TITLE: Storm BlobStore Creation Command
DESCRIPTION: CLI command to create a blob in the Storm blobstore with specified access controls and replication factor.

LANGUAGE: bash
CODE:
storm blobstore create --file README.txt --acl o::rwa --replication-factor 4 key1

----------------------------------------

TITLE: Configuring CassandraWriterBolt with Bound Statement from Tuple Field in Java
DESCRIPTION: Creates a CassandraWriterBolt using a bound statement loaded from a tuple field.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
     boundQuery(namedByField("cql"))
        .bind(all());

----------------------------------------

TITLE: ZeroMQ Implementation for Distributed Mode in Apache Storm (Clojure)
DESCRIPTION: The messaging implementation for distributed mode in Storm using ZeroMQ.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/zmq.clj

----------------------------------------

TITLE: Defining a Transactional Topology in Apache Storm
DESCRIPTION: This code snippet demonstrates how to define a transactional topology using TransactionalTopologyBuilder for computing a global count of tuples from an input stream.

LANGUAGE: java
CODE:
MemoryTransactionalSpout spout = new MemoryTransactionalSpout(DATA, new Fields("word"), PARTITION_TAKE_PER_BATCH);
TransactionalTopologyBuilder builder = new TransactionalTopologyBuilder("global-count", "spout", spout, 3);
builder.setBolt("partial-count", new BatchCount(), 5)
        .shuffleGrouping("spout");
builder.setBolt("sum", new UpdateGlobalCount())
        .globalGrouping("partial-count");

----------------------------------------

TITLE: Windowing Operations in Java Stream API
DESCRIPTION: Demonstrates how to apply different windowing configurations to streams, including sliding and tumbling windows based on time or count.

LANGUAGE: java
CODE:
// time based sliding window
stream.window(SlidingWindows.of(Duration.minutes(10), Duration.minutes(1)));

// count based sliding window
stream.window(SlidingWindows.of(Count.(10), Count.of(2)));

// tumbling window
stream.window(TumblingWindows.of(Duration.seconds(10));

// specifying timestamp field for event time based processing and a late tuple stream.
stream.window(TumblingWindows.of(Duration.seconds(10)
                     .withTimestampField("ts")
                     .withLateTupleStream("late_events"));

----------------------------------------

TITLE: Simplified DRPC Client Usage
DESCRIPTION: Simplified way to create and use a preconfigured DRPC client with automatic host selection.

LANGUAGE: java
CODE:
DRPCClient client = DRPCClient.getConfiguredClient(conf);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Window Configuration Methods in BaseWindowedBolt
DESCRIPTION: Available methods for configuring different types of windows including sliding and tumbling windows based on count or duration.

LANGUAGE: java
CODE:
withWindow(Count windowLength, Count slidingInterval)
withWindow(Count windowLength)
withWindow(Count windowLength, Duration slidingInterval)
withWindow(Duration windowLength, Duration slidingInterval)
withWindow(Duration windowLength)
withWindow(Duration windowLength, Count slidingInterval)
withTumblingWindow(BaseWindowedBolt.Count count)
withTumblingWindow(BaseWindowedBolt.Duration duration)

----------------------------------------

TITLE: Initializing HBase State Provider in Java
DESCRIPTION: Example of programmatically initializing the HBase state provider in Storm, including setting HBase configuration and state provider details.

LANGUAGE: java
CODE:
Config conf = new Config();
    Map<String, Object> hbConf = new HashMap<String, Object>();
    hbConf.put("hbase.rootdir", "file:///tmp/hbase");
    conf.put("hbase.conf", hbConf);
    conf.put("topology.state.provider",  "org.apache.storm.hbase.state.HBaseKeyValueStateProvider");
    conf.put("topology.state.provider.config", "{" +
            "   \"hbaseConfigKey\": \"hbase.conf\"," +
            "   \"tableName\": \"state\"," +
            "   \"columnFamily\": \"cf\"" +
            " }");

----------------------------------------

TITLE: Maven Assembly Plugin Configuration for Storm Topology
DESCRIPTION: Maven configuration snippet for packaging a Storm topology into a JAR with dependencies using the Maven Assembly Plugin. Configures the manifest to specify the main class.

LANGUAGE: xml
CODE:
  <plugin>
    <artifactId>maven-assembly-plugin</artifactId>
    <configuration>
      <descriptorRefs>  
        <descriptorRef>jar-with-dependencies</descriptorRef>
      </descriptorRefs>
      <archive>
        <manifest>
          <mainClass>com.path.to.main.Class</mainClass>
        </manifest>
      </archive>
    </configuration>
  </plugin>

----------------------------------------

TITLE: Defining Kafka Topic Selector Interface
DESCRIPTION: Interface definition for selecting which Kafka topic a tuple should be published to. Contains a single method that returns the target topic name based on a tuple.

LANGUAGE: java
CODE:
public interface KafkaTopicSelector {
    String getTopics(Tuple/TridentTuple tuple);
}

----------------------------------------

TITLE: ILeaderElector Interface Definition
DESCRIPTION: Interface for leader election functionality in Storm's HA implementation.

LANGUAGE: java
CODE:
public interface ILeaderElector {
    void addToLeaderLockQueue();
    void removeFromLeaderLockQueue();
    boolean isLeader();
    InetSocketAddress getLeaderAddress();
    List<InetSocketAddress> getAllNimbusAddresses();
}

----------------------------------------

TITLE: Configuring Maven POM for Storm Project
DESCRIPTION: This XML snippet shows an example of how to set up a pom.xml file for a Storm project. It includes the necessary dependencies and configurations for Storm development.

LANGUAGE: XML
CODE:
<pom.xml>

----------------------------------------

TITLE: Dequeuing Items from Kestrel Queue in Java
DESCRIPTION: Method to dequeue items from a Kestrel queue without removing them. It attempts to dequeue 12 items and prints each received item or a message if the queue is empty.

LANGUAGE: java
CODE:
    private static void dequeueItems(KestrelClient kestrelClient, String queueName) throws IOException, ParseError
			 {
		for(int i=1; i<=12; i++){

			Item item = kestrelClient.dequeue(queueName);

			if(item==null){
				System.out.println("The queue (" + queueName + ") contains no items.");
			}
			else
			{
				byte[] data = item._data;

				String receivedVal = new String(data);

				System.out.println("receivedItem=" + receivedVal);
			}
		}

----------------------------------------

TITLE: Listing Blob Keys in Java
DESCRIPTION: Retrieves a list of all blob keys in Storm's blobstore

LANGUAGE: java
CODE:
Iterator <String> stringIterator = clientBlobStore.listKeys();

----------------------------------------

TITLE: User Defined Function Example
DESCRIPTION: Example of creating and implementing a custom SQL function in Java

LANGUAGE: sql
CODE:
CREATE FUNCTION MYPLUS AS 'org.apache.storm.sql.TestUtils$MyPlus'

LANGUAGE: java
CODE:
  public class MyPlus {
    public static Integer evaluate(Integer x, Integer y) {
      return x + y;
    }
  }

----------------------------------------

TITLE: Implementing RedisLookupMapper in Java
DESCRIPTION: Java class implementation of RedisLookupMapper for word count lookup in Redis hash.

LANGUAGE: java
CODE:
class WordCountRedisLookupMapper implements RedisLookupMapper {
    private RedisDataTypeDescription description;
    private final String hashKey = "wordCount";

    public WordCountRedisLookupMapper() {
        description = new RedisDataTypeDescription(
                RedisDataTypeDescription.RedisDataType.HASH, hashKey);
    }

    @Override
    public List<Values> toTuple(ITuple input, Object value) {
        String member = getKeyFromTuple(input);
        List<Values> values = Lists.newArrayList();
        values.add(new Values(member, value));
        return values;
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("wordName", "count"));
    }

    @Override
    public RedisDataTypeDescription getDataTypeDescription() {
        return description;
    }

    @Override
    public String getKeyFromTuple(ITuple tuple) {
        return tuple.getStringByField("word");
    }

    @Override
    public String getValueFromTuple(ITuple tuple) {
        return null;
    }
}

----------------------------------------

TITLE: Implementing a Tuple Counting Bolt in Java for Apache Storm
DESCRIPTION: This example demonstrates a simple bolt implementation that reports the running total of tuples received. It uses the Counter metric to increment a count each time a tuple is processed.

LANGUAGE: java
CODE:
public class TupleCountingBolt extends BaseRichBolt {
    private Counter tupleCounter;
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        this.tupleCounter = context.registerCounter("tupleCount");
    }

    @Override
    public void execute(Tuple input) {
        this.tupleCounter.inc();
    }
}

----------------------------------------

TITLE: Configuring Storm Topology with Murmur3StreamGrouping for Cassandra Writes in Java
DESCRIPTION: Sets up a Storm topology using Murmur3StreamGrouping to optimize Cassandra writes by partitioning the stream based on row partition keys.

LANGUAGE: java
CODE:
CassandraWriterBolt bolt = new CassandraWriterBolt(
    insertInto("album")
        .values(
            with(fields("title", "year", "performer", "genre", "tracks")
            ).build()));
builder.setBolt("BOLT_WRITER", bolt, 4)
        .customGrouping("spout", new Murmur3StreamGrouping("title"))

----------------------------------------

TITLE: Registering Event Logger in Java Configuration
DESCRIPTION: Example of how to register an event logger to a topology using Java configuration.

LANGUAGE: java
CODE:
conf.registerEventLogger(org.apache.storm.metric.FileBasedEventLogger.class);

----------------------------------------

TITLE: Initializing HiveState for Trident Topology (Java)
DESCRIPTION: Java code demonstrating how to set up HiveState for use in a Trident topology, including mapper and options configuration.

LANGUAGE: java
CODE:
DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withTimeAsPartitionField("YYYY/MM/DD");
            
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
                                .withTxnsPerBatch(10)
                				.withBatchSize(1000)
                	     		.withIdleTimeout(10)
                	     		
StateFactory factory = new HiveStateFactory().withOptions(hiveOptions);
TridentState state = stream.partitionPersist(factory, hiveFields, new HiveUpdater(), new Fields());

----------------------------------------

TITLE: Implementing PartialUniquer Batch Bolt
DESCRIPTION: Implementation of a batch bolt for processing unique followers in the reach calculation.

LANGUAGE: java
CODE:
public class PartialUniquer extends BaseBatchBolt {
    BatchOutputCollector _collector;
    Object _id;
    Set<String> _followers = new HashSet<String>();
    
    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
        _collector = collector;
        _id = id;
    }

    @Override
    public void execute(Tuple tuple) {
        _followers.add(tuple.getString(1));
    }
    
    @Override
    public void finishBatch() {
        _collector.emit(new Values(_id, _followers.size()));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "partial-count"));
    }
}

----------------------------------------

TITLE: Deleting a Blob in Java
DESCRIPTION: Deletes a blob from Storm's blobstore

LANGUAGE: java
CODE:
String blobKey = "some_key";
clientBlobStore.deleteBlob(blobKey);

----------------------------------------

TITLE: Registering Task Hook in Storm Spout or Bolt
DESCRIPTION: This snippet shows how to register a custom task hook in the open method of a spout or prepare method of a bolt using the TopologyContext method.

LANGUAGE: java
CODE:
TopologyContext#addTaskHook

----------------------------------------

TITLE: Retrieving Topology Metrics in JSON
DESCRIPTION: Example response from the /api/v1/topology/<id>/metrics endpoint, which returns detailed metrics for a specific topology.

LANGUAGE: json
CODE:
{
  "window":":all-time",
  "window-hint":"All time",
  "spouts":[
    {
      "id":"spout",
      "emitted":[
        {
          "stream_id":"__metrics",
          "value":20
        },
        {
          "stream_id":"default",
          "value":17350280
        }
      ],
      "transferred":[
        {
          "stream_id":"__metrics",
          "value":20
        },
        {
          "stream_id":"default",
          "value":17350280
        }
      ],
      "acked":[
        {
          "stream_id":"default",
          "value":17339180
        }
      ],
      "failed":[

      ],
      "complete_ms_avg":[
        {
          "stream_id":"default",
          "value":"920.497"
        }
      ]
    }
  ],
  "bolts":[
    {
      "id":"count",
      "emitted":[
        {
          "stream_id":"__metrics",
          "value":120
        },
        {
          "stream_id":"default",
          "value":190748180
        }
      ],
      "transferred":[
        {
          "stream_id":"__metrics",
          "value":120
        },
        {
          "stream_id":"default",
          "value":0
        }
      ],
      "acked":[
        {
          "component_id":"split",
          "stream_id":"default",
          "value":190733160
        }
      ],
      "failed":[

      ],
      "process_ms_avg":[
        {
          "component_id":"split",
          "stream_id":"default",
          "value":"0.004"
        }
      ],
      "executed":[
        {
          "component_id":"split",
          "stream_id":"default",
          "value":190733140
        }
      ],
      "executed_ms_avg":[
        {
          "component_id":"split",
          "stream_id":"default",
          "value":"0.005"
        }
      ]
    }
  ]
}

----------------------------------------

TITLE: Implementing an Update Global Count Bolt in Java
DESCRIPTION: Example implementation of a committer bolt that updates a global count in a database, ensuring exactly-once semantics.

LANGUAGE: java
CODE:
public static class UpdateGlobalCount extends BaseTransactionalBolt implements ICommitter {
    TransactionAttempt _attempt;
    BatchOutputCollector _collector;

    int _sum = 0;

    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, TransactionAttempt attempt) {
        _collector = collector;
        _attempt = attempt;
    }

    @Override
    public void execute(Tuple tuple) {
        _sum+=tuple.getInteger(1);
    }

    @Override
    public void finishBatch() {
        Value val = DATABASE.get(GLOBAL_COUNT_KEY);
        Value newval;
        if(val == null || !val.txid.equals(_attempt.getTransactionId())) {
            newval = new Value();
            newval.txid = _attempt.getTransactionId();
            if(val==null) {
                newval.count = _sum;
            } else {
                newval.count = _sum + val.count;
            }
            DATABASE.put(GLOBAL_COUNT_KEY, newval);
        } else {
            newval = val;
        }
        _collector.emit(new Values(_attempt, newval.count));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "sum"));
    }
}

----------------------------------------

TITLE: Configuring Timestamp Field for Storm Windowing
DESCRIPTION: This code snippet shows how to specify a field in the tuple that represents the timestamp for windowing calculations. It also demonstrates how to set a maximum time lag for out-of-order tuples.

LANGUAGE: java
CODE:
/**
* Specify a field in the tuple that represents the timestamp as a long value. If this
* field is not present in the incoming tuple, an {@link IllegalArgumentException} will be thrown.
*
* @param fieldName the name of the field that contains the timestamp
*/
public BaseWindowedBolt withTimestampField(String fieldName)

/**
* Specify the maximum time lag of the tuple timestamp in milliseconds. It means that the tuple timestamps
* cannot be out of order by more than this amount.
*
* @param duration the max lag duration
*/
public BaseWindowedBolt withLag(Duration duration)

----------------------------------------

TITLE: Setting Up Task in Worker (Clojure)
DESCRIPTION: The mk-task function sets up individual tasks within a worker, including routing functions and spout/bolt-specific code initialization.

LANGUAGE: clojure
CODE:
(mk-task)

----------------------------------------

TITLE: Defining ComponentObject in Storm's Thrift Structure
DESCRIPTION: This Thrift structure defines the ComponentObject union, which specifies how code for spouts or bolts can be represented in Storm topologies. It allows for serialized Java, shell components, or Java objects.

LANGUAGE: thrift
CODE:
union ComponentObject {
  1: binary serialized_java;
  2: ShellComponent shell;
  3: JavaObject java_object;
}

----------------------------------------

TITLE: Maven Dependency Configuration for Storm Server Testing
DESCRIPTION: Maven dependency configuration for including the storm-server artifact needed for LocalCluster testing. Uses test scope since it's only needed for testing purposes.

LANGUAGE: xml
CODE:
<groupId>org.apache.storm</groupId>
<artifactId>storm-server</artifactId>
<version>2.0.0</version>
<scope>test</scope>

----------------------------------------

TITLE: Maven Dependencies Configuration
DESCRIPTION: Maven dependency configuration for Storm Kinesis integration, including AWS SDK, Storm Core, Apache Curator, and JSON Simple dependencies.

LANGUAGE: xml
CODE:
 <dependencies>
        <dependency>
            <groupId>com.amazonaws</groupId>
            <artifactId>aws-java-sdk</artifactId>
            <version>${aws-java-sdk.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-core</artifactId>
            <version>${project.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-framework</artifactId>
            <version>${curator.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>com.googlecode.json-simple</groupId>
            <artifactId>json-simple</artifactId>
        </dependency>
 </dependencies>

----------------------------------------

TITLE: Implementing Custom Serialization Interface in Java
DESCRIPTION: Interface definition for implementing custom serializers in Storm. The interface includes methods for accepting types, serializing objects to binary format, and deserializing objects from binary format.

LANGUAGE: java
CODE:
public interface ISerialization<T> {
    public boolean accept(Class c);
    public void serialize(T object, DataOutputStream stream) throws IOException;
    public T deserialize(DataInputStream stream) throws IOException;
}

----------------------------------------

TITLE: Using Preconfigured DRPC Client in Java
DESCRIPTION: This code demonstrates how to use a preconfigured DRPC client, which selects a host randomly from the configured set and handles failover.

LANGUAGE: java
CODE:
DRPCClient client = DRPCClient.getConfiguredClient(conf);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Configuring Nimbus Seeds in Storm YAML
DESCRIPTION: This snippet shows how to configure the Nimbus seeds, which are the candidate master machines for worker nodes to download topology jars and configurations.

LANGUAGE: yaml
CODE:
nimbus.seeds: ["111.222.333.44"]

----------------------------------------

TITLE: Word Count Stream Example
DESCRIPTION: Complete example showing how to build a word count topology using Stream API with windowing and aggregation.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

builder
   // A stream of random sentences with two partitions
   .newStream(new RandomSentenceSpout(), new ValueMapper<String>(0), 2)
   // a two seconds tumbling window
   .window(TumblingWindows.of(Duration.seconds(2)))
   // split the sentences to words
   .flatMap(s -> Arrays.asList(s.split(" ")))
   // create a stream of (word, 1) pairs
   .mapToPair(w -> Pair.of(w, 1))
   // compute the word counts in the last two second window
   .countByKey()
   // print the results to stdout
   .print();

----------------------------------------

TITLE: Running ActiveMQ Server
DESCRIPTION: Command to start the Apache ActiveMQ server, which is required for the JMS integration example.

LANGUAGE: bash
CODE:
$ [ACTIVEMQ_HOME]/bin/activemq

----------------------------------------

TITLE: Configuring HikariCP Connection Pool with JDBC Insert Bolt
DESCRIPTION: Example showing how to configure a HikariCP connection pool and set up a JDBC Insert Bolt for database operations.

LANGUAGE: java
CODE:
Map hikariConfigMap = Maps.newHashMap();
hikariConfigMap.put("dataSourceClassName","com.mysql.jdbc.jdbc2.optional.MysqlDataSource");
hikariConfigMap.put("dataSource.url", "jdbc:mysql://localhost/test");
hikariConfigMap.put("dataSource.user","root");
hikariConfigMap.put("dataSource.password","password");
ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);

String tableName = "user_details";
JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

JdbcInsertBolt userPersistenceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                                    .withTableName("user")
                                    .withQueryTimeoutSecs(30);

----------------------------------------

TITLE: Configuring CassandraWriterBolt with Logged Batch Statement in Java
DESCRIPTION: Creates a CassandraWriterBolt using a logged batch statement to perform multiple inserts.

LANGUAGE: java
CODE:
new CassandraWriterBolt(loggedBatch(
        simpleQuery("INSERT INTO titles_per_album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all())),
        simpleQuery("INSERT INTO titles_per_performer (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all()))
    )
);

----------------------------------------

TITLE: Dequeuing and Removing Items from Kestrel in Java
DESCRIPTION: Method that dequeues items from a Kestrel queue and removes them using acknowledgment. Processes items and acknowledges them to remove from queue.

LANGUAGE: java
CODE:
    private static void dequeueAndRemoveItems(KestrelClient kestrelClient, String queueName)
    throws IOException, ParseError
		 {
			for(int i=1; i<=12; i++){

				Item item = kestrelClient.dequeue(queueName);


				if(item==null){
					System.out.println("The queue (" + queueName + ") contains no items.");
				}
				else
				{
					int itemID = item._id;


					byte[] data = item._data;

					String receivedVal = new String(data);

					kestrelClient.ack(queueName, itemID);

					System.out.println("receivedItem=" + receivedVal);
				}
			}
	}

----------------------------------------

TITLE: Apache Log to JSON Parser Script
DESCRIPTION: Python script that parses Apache logs into JSON format with auto-incrementing IDs.

LANGUAGE: python
CODE:
import sys
import apache_log_parser
import json

auto_incr_id = 1
parser_format = '%a - - %t %D "%r" %s %b "%{Referer}i" "%{User-Agent}i"'
line_parser = apache_log_parser.make_parser(parser_format)
while True:
  line = sys.stdin.readline()
  if not line:
    break
  parsed_dict = line_parser(line)
  parsed_dict['id'] = auto_incr_id
  auto_incr_id += 1

  parsed_dict = {k.upper(): v for k, v in parsed_dict.iteritems() if not k.endswith('datetimeobj')}
  print json.dumps(parsed_dict)

----------------------------------------

TITLE: Redirecting to Storm Maven Documentation using HTML meta refresh
DESCRIPTION: This HTML snippet uses a meta refresh tag to automatically redirect the user to the current Storm Maven documentation page. It also includes a canonical link for search engine optimization.

LANGUAGE: HTML
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Maven.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Maven.html" />

----------------------------------------

TITLE: Creating CassandraWriterBolt with Batch Statements
DESCRIPTION: Shows how to create a CassandraWriterBolt using logged and unlogged batch statements for multiple inserts.

LANGUAGE: java
CODE:
// Logged
new CassandraWriterBolt(loggedBatch(
        simpleQuery("INSERT INTO titles_per_album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all())),
        simpleQuery("INSERT INTO titles_per_performer (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all()))
    )
);
// UnLogged
new CassandraWriterBolt(unLoggedBatch(
        simpleQuery("INSERT INTO titles_per_album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all())),
        simpleQuery("INSERT INTO titles_per_performer (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all()))
    )
);

----------------------------------------

TITLE: Storm Component Configuration Override Example (Java)
DESCRIPTION: Example showing two methods for overriding component configurations in Storm topologies: internally by overriding getComponentConfiguration method, and externally using TopologyBuilder methods setSpout/setBolt.

LANGUAGE: java
CODE:
// Method 1: Internal override
@Override
public Map<String, Object> getComponentConfiguration() {
    // Return component-specific configuration map
}

// Method 2: External override
topologyBuilder.setSpout("spout")
    .addConfiguration("topology.debug", true);

----------------------------------------

TITLE: Task Data Access Methods
DESCRIPTION: Methods for reading and writing task-level shared data in Storm components.

LANGUAGE: java
CODE:
TopologyContext#setTaskData(String, Object)
TopologyContext#getTask(String)

----------------------------------------

TITLE: Retrieving Local Storm Configuration Value
DESCRIPTION: Prints the value of a specified configuration parameter from the local Storm configs.

LANGUAGE: shell
CODE:
storm localconfvalue conf-name

----------------------------------------

TITLE: Implementing a Batch Count Bolt in Java
DESCRIPTION: Example of a BatchBolt implementation that counts tuples in a batch. This bolt is part of the global count topology.

LANGUAGE: java
CODE:
public static class BatchCount extends BaseBatchBolt {
    Object _id;
    BatchOutputCollector _collector;

    int _count = 0;

    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
        _collector = collector;
        _id = id;
    }

    @Override
    public void execute(Tuple tuple) {
        _count++;
    }

    @Override
    public void finishBatch() {
        _collector.emit(new Values(_id, _count));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "count"));
    }
}

----------------------------------------

TITLE: Implementing a Stateful Persistent Windowed Bolt in Storm
DESCRIPTION: Example of a stateful persistent windowed bolt that uses window checkpointing to save its state. It demonstrates state initialization, tuple processing, and state updates.

LANGUAGE: java
CODE:
public class MyStatefulPersistentWindowedBolt extends BaseStatefulWindowedBolt<K, V> {
  private KeyValueState<K, V> state;
  
  @Override
  public void initState(KeyValueState<K, V> state) {
    this.state = state;
   // ...
   // restore the state from the last saved state.
   // ...
  }
  
  @Override
  public void execute(TupleWindow window) {      
    // iterate over tuples in the current window
    Iterator<Tuple> it = window.getIter();
    while (it.hasNext()) {
        // compute some result based on the tuples in window
    }
    
    // possibly update any state to be maintained across windows
    state.put(STATE_KEY, updatedValue);
    
    // emit the results downstream
    collector.emit(new Values(result));
  }
}

----------------------------------------

TITLE: Defining IEventLogger Interface in Java for Apache Storm
DESCRIPTION: This code snippet defines the IEventLogger interface used by the event logger bolt to log events in Apache Storm. It includes methods for preparation, logging events, and cleanup.

LANGUAGE: java
CODE:
public interface IEventLogger {
    void prepare(Map stormConf, Map<String, Object> arguments, TopologyContext context);
    void log(EventInfo e);
    void close();
}

----------------------------------------

TITLE: Storm RotatingMap Implementation
DESCRIPTION: The RotatingMap is a specialized HashMap implementation used for time-based expiration of tuples. It maintains multiple buckets of records that progress towards expiration, with O(1) access time. The structure automatically handles tuple relocation and expiration through a rotation mechanism triggered by system tick streams.



----------------------------------------

TITLE: Configuring URL Expansion with Fields Grouping in Storm
DESCRIPTION: Shows URL expansion bolt configuration using fields grouping, ensuring the same URL always goes to the same task. This approach optimizes cache efficiency by preventing cache duplication across tasks.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .fieldsGrouping("urls", new Fields("url"));

----------------------------------------

TITLE: Kryo Serialization Error Stack Trace Example
DESCRIPTION: Example stack trace showing a ConcurrentModificationException that occurs when mutable objects are emitted as output tuples in Storm. This happens when a bolt modifies an object during network serialization.

LANGUAGE: java
CODE:
java.lang.RuntimeException: java.util.ConcurrentModificationException
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:84)
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:55)
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:56)
	at org.apache.storm.disruptor$consume_loop_STAR_$fn__1597.invoke(disruptor.clj:67)
	at org.apache.storm.util$async_loop$fn__465.invoke(util.clj:377)
	at clojure.lang.AFn.run(AFn.java:24)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.util.ConcurrentModificationException
	at java.util.LinkedHashMap$LinkedHashIterator.nextEntry(LinkedHashMap.java:390)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:409)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:408)
	at java.util.HashMap.writeObject(HashMap.java:1016)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:959)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1480)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at org.apache.storm.serialization.SerializableSerializer.write(SerializableSerializer.java:21)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:554)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:77)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:472)
	at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:27)

----------------------------------------

TITLE: Sample Topology Metrics Response
DESCRIPTION: Example JSON response from the /api/v1/topology/:id/metrics endpoint showing detailed metrics for a topology's components

LANGUAGE: json
CODE:
{
    "window":":all-time",
    "window-hint":"All time",
    "spouts":[
        {
            "id":"spout",
            "emitted":[
                {
                    "stream_id":"__metrics",
                    "value":20
                },
                {
                    "stream_id":"default",
                    "value":17350280
                }
            ],
            "transferred":[
                {
                    "stream_id":"__metrics",
                    "value":20
                },
                {
                    "stream_id":"default",
                    "value":17350280
                }
            ]
        }
    ]
}

----------------------------------------

TITLE: Batch Processing in DRPC with PartialUniquer
DESCRIPTION: Implementation of a batch bolt that processes groups of tuples to calculate unique follower counts in the Twitter reach example.

LANGUAGE: java
CODE:
public class PartialUniquer extends BaseBatchBolt {
    BatchOutputCollector _collector;
    Object _id;
    Set<String> _followers = new HashSet<String>();
    
    @Override
    public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
        _collector = collector;
        _id = id;
    }

    @Override
    public void execute(Tuple tuple) {
        _followers.add(tuple.getString(1));
    }
    
    @Override
    public void finishBatch() {
        _collector.emit(new Values(_id, _followers.size()));
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id", "partial-count"));
    }
}

----------------------------------------

TITLE: Storm Topology Rebalancing Command
DESCRIPTION: CLI command example for rebalancing a running Storm topology, showing how to modify worker processes and executor counts without restart.

LANGUAGE: bash
CODE:
storm rebalance mytopology -n 5 -e blue-spout=3 -e yellow-bolt=10

----------------------------------------

TITLE: GetTime2 UDF Implementation
DESCRIPTION: Java implementation of a User Defined Function to convert date strings to Unix timestamps using Joda Time.

LANGUAGE: java
CODE:
package org.apache.storm.sql.runtime.functions.scalar.datetime;

import org.joda.time.format.DateTimeFormat;
import org.joda.time.format.DateTimeFormatter;

public class GetTime2 {
    public static Long evaluate(String dateString, String dateFormat) {
        try {
            DateTimeFormatter df = DateTimeFormat.forPattern(dateFormat).withZoneUTC();
            return df.parseDateTime(dateString).getMillis();
        } catch (Exception ex) {
            throw new RuntimeException(ex);
        }
    }
}

----------------------------------------

TITLE: Configuring Maven Dependencies for Flux
DESCRIPTION: XML snippet showing how to add Flux as a Maven dependency in a project's pom.xml file.

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>flux-core</artifactId>
    <version>${storm.version}</version>
</dependency>

----------------------------------------

TITLE: Configuring Kerberos Authentication for Storm UI
DESCRIPTION: YAML configuration for setting up Kerberos authentication for Storm UI using hadoop-auth.

LANGUAGE: yaml
CODE:
ui.filter: "org.apache.hadoop.security.authentication.server.AuthenticationFilter"
ui.filter.params:
   "type": "kerberos"
   "kerberos.principal": "HTTP/nimbus.witzend.com"
   "kerberos.keytab": "/vagrant/keytabs/http.keytab"
   "kerberos.name.rules": "RULE:[2:$1@$0]([jt]t@.*EXAMPLE.COM)s/.*/$MAPRED_USER/ RULE:[2:$1@$0]([nd]n@.*EXAMPLE.COM)s/.*/$HDFS_USER/DEFAULT"

----------------------------------------

TITLE: Configuring Multi-tenant Scheduler in YAML
DESCRIPTION: YAML configuration for enabling the multi-tenant scheduler in Storm and setting user resource pools.

LANGUAGE: yaml
CODE:
storm.scheduler: "org.apache.storm.scheduler.multitenant.MultitenantScheduler"

multitenant.scheduler.user.pools: 
    "evans": 10
    "derek": 10

----------------------------------------

TITLE: Storm Acker Execute Operation
DESCRIPTION: The acker bolt's execute method processes tuple acknowledgments by managing checksums and notifying spouts of tuple tree completion. It handles initialization of new tuple trees, processes acknowledgments, and manages failure cases using a RotatingMap data structure.



----------------------------------------

TITLE: Dequeuing Items from Kestrel Queue in Java
DESCRIPTION: Method to dequeue items from a Kestrel queue without removing them. It attempts to dequeue 12 items and prints each received item or a message if the queue is empty.

LANGUAGE: java
CODE:
private static void dequeueItems(KestrelClient kestrelClient, String queueName) throws IOException, ParseError
			 {
		for(int i=1; i<=12; i++){

			Item item = kestrelClient.dequeue(queueName);

			if(item==null){
				System.out.println("The queue (" + queueName + ") contains no items.");
			}
			else
			{
				byte[] data = item._data;

				String receivedVal = new String(data);

				System.out.println("receivedItem=" + receivedVal);
			}
		}

----------------------------------------

TITLE: Starting Apache ActiveMQ for Storm JMS Example
DESCRIPTION: Command to start Apache ActiveMQ, which is used as the JMS provider for the example topology.

LANGUAGE: bash
CODE:
$ [ACTIVEMQ_HOME]/bin/activemq

----------------------------------------

TITLE: Implementing HTML Meta Refresh Redirect for Storm Documentation
DESCRIPTION: This HTML snippet sets up an automatic redirect to the current Storm Multilang protocol documentation page. It also includes a canonical link for search engine optimization.

LANGUAGE: HTML
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Multilang-protocol.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Multilang-protocol.html" />

----------------------------------------

TITLE: Submitting a Storm Topology with External Dependencies
DESCRIPTION: Demonstrates how to submit a Storm topology jar with additional external jars and Maven artifacts. This command runs the main method of a specified class, configuring the classpath to include Storm jars and external dependencies.

LANGUAGE: bash
CODE:
./bin/storm jar example/storm-starter/storm-starter-topologies-*.jar org.apache.storm.starter.RollingTopWords blobstore-remote2 remote --jars "./external/storm-redis/storm-redis-1.1.0.jar,./external/storm-kafka-client/storm-kafka-client-1.1.0.jar" --artifacts "redis.clients:jedis:2.9.0,org.apache.kafka:kafka-clients:1.0.0^org.slf4j:slf4j-api" --artifactRepositories "jboss-repository^http://repository.jboss.com/maven2,HDPRepo^http://repo.hortonworks.com/content/groups/public/"

----------------------------------------

TITLE: Submitting a Storm Topology with External Dependencies
DESCRIPTION: Demonstrates how to submit a Storm topology jar with additional external jars and Maven artifacts. This command runs the main method of a specified class, configuring the classpath to include Storm jars and external dependencies.

LANGUAGE: bash
CODE:
./bin/storm jar example/storm-starter/storm-starter-topologies-*.jar org.apache.storm.starter.RollingTopWords blobstore-remote2 remote --jars "./external/storm-redis/storm-redis-1.1.0.jar,./external/storm-kafka-client/storm-kafka-client-1.1.0.jar" --artifacts "redis.clients:jedis:2.9.0,org.apache.kafka:kafka-clients:1.0.0^org.slf4j:slf4j-api" --artifactRepositories "jboss-repository^http://repository.jboss.com/maven2,HDPRepo^http://repo.hortonworks.com/content/groups/public/"

----------------------------------------

TITLE: Defining JdbcMapper Interface in Java
DESCRIPTION: This code snippet defines the JdbcMapper interface, which is the main API for inserting data into a table using JDBC. It includes a method for getting columns from a tuple.

LANGUAGE: java
CODE:
public interface JdbcMapper  extends Serializable {
    List<Column> getColumns(ITuple tuple);
}

----------------------------------------

TITLE: Running a Storm Topology JAR
DESCRIPTION: Executes the main method of a specified class with given arguments, configuring the process for topology submission.

LANGUAGE: shell
CODE:
storm jar topology-jar-path class ...

----------------------------------------

TITLE: Configuring HiveOptions for HiveBolt (Java)
DESCRIPTION: Java code snippet showing how to create and configure HiveOptions for use with HiveBolt, including transaction settings and batch size.

LANGUAGE: java
CODE:
HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
                                .withTxnsPerBatch(10)
                				.withBatchSize(1000)
                	     		.withIdleTimeout(10)

----------------------------------------

TITLE: Maven Dependency Configuration for Storm Client
DESCRIPTION: Maven dependency configuration for including the storm-client artifact in topology projects. Uses scope 'provided' since the dependency will be available on the cluster.

LANGUAGE: xml
CODE:
<groupId>org.apache.storm</groupId>
<artifactId>storm-client</artifactId>
<version>2.0.0</version>
<scope>provided</scope>

----------------------------------------

TITLE: Stateful Windowed Bolt Example - Java
DESCRIPTION: Implementation of a stateful windowed bolt that maintains state across windows and supports persistence.

LANGUAGE: java
CODE:
public class MyStatefulPersistentWindowedBolt extends BaseStatefulWindowedBolt<K, V> {
  private KeyValueState<K, V> state;
  
  @Override
  public void initState(KeyValueState<K, V> state) {
    this.state = state;
   // ...
   // restore the state from the last saved state.
   // ...
  }
  
  @Override
  public void execute(TupleWindow window) {      
    // iterate over tuples in the current window
    Iterator<Tuple> it = window.getIter();
    while (it.hasNext()) {
        // compute some result based on the tuples in window
    }
    
    // possibly update any state to be maintained across windows
    state.put(STATE_KEY, updatedValue);
    
    // emit the results downstream
    collector.emit(new Values(result));
  }
}

----------------------------------------

TITLE: Debugging Kryo ConcurrentModificationException in Java
DESCRIPTION: This stack trace demonstrates a common error that occurs when emitting mutable objects as output tuples in Storm. The exception is thrown during object serialization, indicating that the bolt is modifying an object while it's being serialized for network transmission.

LANGUAGE: java
CODE:
java.lang.RuntimeException: java.util.ConcurrentModificationException
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:84)
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:55)
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:56)
	at org.apache.storm.disruptor$consume_loop_STAR_$fn__1597.invoke(disruptor.clj:67)
	at org.apache.storm.util$async_loop$fn__465.invoke(util.clj:377)
	at clojure.lang.AFn.run(AFn.java:24)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.util.ConcurrentModificationException
	at java.util.LinkedHashMap$LinkedHashIterator.nextEntry(LinkedHashMap.java:390)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:409)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:408)
	at java.util.HashMap.writeObject(HashMap.java:1016)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:959)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1480)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at org.apache.storm.serialization.SerializableSerializer.write(SerializableSerializer.java:21)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:554)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:77)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:472)
	at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:27)

----------------------------------------

TITLE: Defining Storm Metric Reporter Configuration in YAML
DESCRIPTION: Example YAML configuration for setting up metric reporters in Storm. This shows how to configure the storm.daemon.metrics.reporter.plugins setting.

LANGUAGE: yaml
CODE:
storm.daemon.metrics.reporter.plugins:
  - "org.apache.storm.daemon.metrics.reporters.ConsolePreparableReporter"
  - "org.apache.storm.daemon.metrics.reporters.CsvPreparableReporter"
  - "org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter"

----------------------------------------

TITLE: Initializing Fixed Batch Spout in Java for Trident
DESCRIPTION: This code snippet creates a FixedBatchSpout that cycles through a set of predefined sentences to produce a continuous stream of data for processing in a Trident topology.

LANGUAGE: java
CODE:
FixedBatchSpout spout = new FixedBatchSpout(new Fields("sentence"), 3,
               new Values("the cow jumped over the moon"),
               new Values("the man went to the store and bought some candy"),
               new Values("four score and seven years ago"),
               new Values("how many apples can you eat"));
spout.setCycle(true);

----------------------------------------

TITLE: Creating RedisLookupBolt Instance
DESCRIPTION: Java code snippet for creating a RedisLookupBolt instance using JedisPoolConfig and the custom WordCountRedisLookupMapper.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
        .setHost(host).setPort(port).build();
RedisLookupMapper lookupMapper = new WordCountRedisLookupMapper();
RedisLookupBolt lookupBolt = new RedisLookupBolt(poolConfig, lookupMapper);

----------------------------------------

TITLE: Initializing Fixed Batch Spout in Java for Trident
DESCRIPTION: This code snippet creates a FixedBatchSpout that cycles through a set of predefined sentences to produce a continuous stream of data for processing in a Trident topology.

LANGUAGE: java
CODE:
FixedBatchSpout spout = new FixedBatchSpout(new Fields("sentence"), 3,
               new Values("the cow jumped over the moon"),
               new Values("the man went to the store and bought some candy"),
               new Values("four score and seven years ago"),
               new Values("how many apples can you eat"));
spout.setCycle(true);

----------------------------------------

TITLE: Multiple Cassandra Inserts from Single Tuple
DESCRIPTION: Shows how to perform multiple insert queries from a single input tuple into different tables.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
    async(
        simpleQuery("INSERT INTO titles_per_album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all())),
        simpleQuery("INSERT INTO titles_per_performer (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);").with(all()))
    )
);

----------------------------------------

TITLE: Implementing Split Function for Word Tokenization in Trident
DESCRIPTION: Defines a Split function that takes a sentence and emits individual words, used in the word count Trident topology.

LANGUAGE: java
CODE:
public class Split extends BaseFunction {
   public void execute(TridentTuple tuple, TridentCollector collector) {
       String sentence = tuple.getString(0);
       for(String word: sentence.split(" ")) {
           collector.emit(new Values(word));                
       }
   }
}

----------------------------------------

TITLE: Thrift Structures for Cluster Information
DESCRIPTION: Thrift definitions for cluster summary structures including supervisor, topology, and nimbus information for API responses.

LANGUAGE: thrift
CODE:
struct ClusterSummary {
  1: required list<SupervisorSummary> supervisors;
  3: required list<TopologySummary> topologies;
  4: required list<NimbusSummary> nimbuses;
}

struct NimbusSummary {
  1: required string host;
  2: required i32 port;
  3: required i32 uptime_secs;
  4: required bool isLeader;
  5: required string version;
}

----------------------------------------

TITLE: Implementing Reach Calculation with Trident DRPC in Java
DESCRIPTION: This topology computes the reach of a URL on demand using Trident DRPC. It fetches tweeters and their followers from external databases, then calculates the unique count of followers exposed to the URL.

LANGUAGE: java
CODE:
TridentState urlToTweeters =
       topology.newStaticState(getUrlToTweetersState());
TridentState tweetersToFollowers =
       topology.newStaticState(getTweeterToFollowersState());

topology.newDRPCStream("reach")
       .stateQuery(urlToTweeters, new Fields("args"), new MapGet(), new Fields("tweeters"))
       .each(new Fields("tweeters"), new ExpandList(), new Fields("tweeter"))
       .shuffle()
       .stateQuery(tweetersToFollowers, new Fields("tweeter"), new MapGet(), new Fields("followers"))
       .parallelismHint(200)
       .each(new Fields("followers"), new ExpandList(), new Fields("follower"))
       .groupBy(new Fields("follower"))
       .aggregate(new One(), new Fields("one"))
       .parallelismHint(20)
       .aggregate(new Count(), new Fields("reach"));

----------------------------------------

TITLE: Implementing Storm Event Logger Interface in Java
DESCRIPTION: Definition of IEventLogger interface used for logging topology events. Includes methods for preparation, logging events, and cleanup.

LANGUAGE: java
CODE:
/**
 * EventLogger interface for logging the event info to a sink like log file or db
 * for inspecting the events via UI for debugging.
 */
public interface IEventLogger {
    /**
    * Invoked during eventlogger bolt prepare.
    */
    void prepare(Map stormConf, Map<String, Object> arguments, TopologyContext context);

    /**
     * Invoked when the {@link EventLoggerBolt} receives a tuple from the spouts or bolts that has event logging enabled.
     *
     * @param e the event
     */
    void log(EventInfo e);

    /**
    * Invoked when the event logger bolt is cleaned up
    */
    void close();
}

----------------------------------------

TITLE: Configuring OpenTSDB Bolt in Storm Topology
DESCRIPTION: This snippet demonstrates how to set up and configure an OpenTSDB bolt in a Storm topology. It shows the creation of an OpenTsdbClient, configuration of the OpenTsdbBolt, and its integration into the topology builder.

LANGUAGE: java
CODE:
OpenTsdbClient.Builder builder =  OpenTsdbClient.newBuilder(openTsdbUrl).sync(30_000).returnDetails();
final OpenTsdbBolt openTsdbBolt = new OpenTsdbBolt(builder, Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
openTsdbBolt.withBatchSize(10).withFlushInterval(2000);
topologyBuilder.setBolt("opentsdb", openTsdbBolt).shuffleGrouping("metric-gen");

----------------------------------------

TITLE: Configuring Generic Resources in Storm YAML
DESCRIPTION: Configuration syntax for specifying available node resources in the storm.yaml configuration file.

LANGUAGE: yaml
CODE:
supervisor.resources.map: {[type<String>] : [amount<Double>]}

----------------------------------------

TITLE: Join Operations in Java Stream API
DESCRIPTION: Demonstrates how to perform join operations between two streams based on keys.

LANGUAGE: java
CODE:
PairStream<Long, Long> squares = ... // (1, 1), (2, 4), (3, 9) ...
PairStream<Long, Long> cubes = ... // (1, 1), (2, 8), (3, 27) ...

// join the sqaures and cubes stream to produce (1, [1, 1]), (2, [4, 8]), (3, [9, 27]) ...
PairStream<Long, Pair<Long, Long>> joined = squares.window(TumblingWindows.of(Duration.seconds(5))).join(cubes);

----------------------------------------

TITLE: Configuring Storm Topology with KestrelSpout in Java
DESCRIPTION: Example of setting up a Storm topology that uses KestrelSpout to read sentences from a Kestrel queue, split them into words, and count word occurrences. It demonstrates the basic structure of a Storm topology with Kestrel integration.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("sentences", new KestrelSpout("localhost",22133,"sentence_queue",new StringScheme()));
builder.setBolt("split", new SplitSentence(), 10)
	        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 20)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Setting Health Check Directory in Storm YAML
DESCRIPTION: This snippet demonstrates how to configure the directory for health check scripts in Storm. These scripts are used to monitor the health of supervisor nodes.

LANGUAGE: yaml
CODE:
storm.health.check.dir: "healthchecks"

----------------------------------------

TITLE: Generating and Serving Apache Storm Website Locally
DESCRIPTION: Command to generate the Apache Storm website and start a local server for preview. The -w option enables automatic regeneration when files are changed.

LANGUAGE: shell
CODE:
bundle exec jekyll serve -w

----------------------------------------

TITLE: Initializing EsIndexBolt for Storm-Elasticsearch Integration in Java
DESCRIPTION: Creates an EsIndexBolt to stream tuples from Storm directly into Elasticsearch. It requires an EsConfig for cluster configuration and an EsTupleMapper to extract necessary fields from input tuples.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});
EsTupleMapper tupleMapper = new DefaultEsTupleMapper();
EsIndexBolt indexBolt = new EsIndexBolt(esConfig, tupleMapper);

----------------------------------------

TITLE: Configuring Bolt with Fields Grouping in Storm Topology
DESCRIPTION: This snippet shows how to set up a bolt named 'expand' with fields grouping in a Storm topology. It uses the ExpandUrl class, specifies a parallelism hint, and groups by the 'url' field from the 'urls' component.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .fieldsGrouping("urls", new Fields("url"));

----------------------------------------

TITLE: Storm Package Compatibility Configuration
DESCRIPTION: YAML configuration required for backward compatibility when running older Storm topologies on version 1.0.0+. This transformer allows code compiled with backtype.storm packages to run on newer versions using org.apache.storm packages.

LANGUAGE: yaml
CODE:
client.jartransformer.class: "org.apache.storm.hack.StormShadeTransformer"

----------------------------------------

TITLE: Registering Metrics Consumer in Java
DESCRIPTION: Shows how to register a metrics consumer via Java configuration to monitor topology metrics.

LANGUAGE: java
CODE:
conf.registerMetricsConsumer(org.apache.storm.metric.LoggingMetricsConsumer.class, 1);

----------------------------------------

TITLE: Configuring Maven Assembly Plugin for Storm Topology JAR Creation
DESCRIPTION: This XML snippet shows how to configure the Maven Assembly Plugin in a pom.xml file to create a JAR with dependencies for a Storm topology. It specifies the main class and uses the jar-with-dependencies descriptor.

LANGUAGE: xml
CODE:
  <plugin>
    <artifactId>maven-assembly-plugin</artifactId>
    <configuration>
      <descriptorRefs>  
        <descriptorRef>jar-with-dependencies</descriptorRef>
      </descriptorRefs>
      <archive>
        <manifest>
          <mainClass>com.path.to.main.Class</mainClass>
        </manifest>
      </archive>
    </configuration>
  </plugin>

----------------------------------------

TITLE: Creating a Bucketed Hive Table for Streaming (SQL)
DESCRIPTION: SQL command to create a bucketed Hive table with ORC format, which is required for using the Hive Streaming API.

LANGUAGE: sql
CODE:
create table test_table ( id INT, name STRING, phone STRING, street STRING) partitioned by (city STRING, state STRING) stored as orc tblproperties ("orc.compress"="NONE");

----------------------------------------

TITLE: HTML meta redirect to Storm documentation
DESCRIPTION: HTML meta tags that handle automatic redirection to the Storm command line client documentation and establish canonical URL.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Command-line-client.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Command-line-client.html" />

----------------------------------------

TITLE: Setting Up Database Tables in SQL
DESCRIPTION: This SQL snippet shows the creation of tables and insertion of sample data for the example Storm/Trident JDBC integration. It includes creating user, department, and user_department tables, as well as inserting some initial data.

LANGUAGE: sql
CODE:
create table if not exists user (user_id integer, user_name varchar(100), dept_name varchar(100), create_date date);
create table if not exists department (dept_id integer, dept_name varchar(100));
create table if not exists user_department (user_id integer, dept_id integer);
insert into department values (1, 'R&D');
insert into department values (2, 'Finance');
insert into department values (3, 'HR');
insert into department values (4, 'Sales');
insert into user_department values (1, 1);
insert into user_department values (2, 2);
insert into user_department values (3, 3);
insert into user_department values (4, 4);
select dept_name from department, user_department where department.dept_id = user_department.dept_id and user_department.user_id = ?;

----------------------------------------

TITLE: Defining a Parameterized Spout in Clojure
DESCRIPTION: Example of defining an unprepared, parameterized spout that emits random sentences from a provided list.

LANGUAGE: clojure
CODE:
(defspout sentence-spout-parameterized ["word"] {:params [sentences] :prepare false}
  [collector]
  (Thread/sleep 500)
  (emit-spout! collector [(rand-nth sentences)]))

----------------------------------------

TITLE: Thread-safe Tuple Serialization in Apache Storm (Java)
DESCRIPTION: This code snippet refers to the thread-safe serializer used in Apache Storm for tuple serialization.

LANGUAGE: java
CODE:
(https://github.com/apache/storm/blob/0.7.1/src/jvm/org/apache/storm/serialization/KryoTupleSerializer.java#L26)

----------------------------------------

TITLE: Custom Location Database State Implementation
DESCRIPTION: Example of implementing a custom State interface for a location database with bulk operations

LANGUAGE: java
CODE:
public class LocationDB implements State {
    public void beginCommit(Long txid) {    
    }
    
    public void commit(Long txid) {    
    }
    
    public void setLocationsBulk(List<Long> userIds, List<String> locations) {
      // set locations in bulk
    }
    
    public List<String> bulkGetLocations(List<Long> userIds) {
      // get locations in bulk
    }
}

----------------------------------------

TITLE: Location Database StateFactory Implementation
DESCRIPTION: Factory class for creating LocationDB state instances within Trident tasks.

LANGUAGE: java
CODE:
public class LocationDBFactory implements StateFactory {
   public State makeState(Map conf, int partitionIndex, int numPartitions) {
      return new LocationDB();
   } 
}

----------------------------------------

TITLE: Implementing Streaming Word Count in Trident
DESCRIPTION: Demonstrates how to create a Trident topology that computes streaming word count from an input stream of sentences. It splits sentences into words, groups by word, and persistently aggregates the count.

LANGUAGE: java
CODE:
TridentTopology topology = new TridentTopology();        
TridentState wordCounts =
     topology.newStream("spout1", spout)
       .each(new Fields("sentence"), new Split(), new Fields("word"))
       .groupBy(new Fields("word"))
       .persistentAggregate(new MemoryMapState.Factory(), new Count(), new Fields("count"))                
       .parallelismHint(6);

----------------------------------------

TITLE: Configuring RedisStoreBolt in Java
DESCRIPTION: This snippet demonstrates how to configure and create a RedisStoreBolt using a JedisPoolConfig and a RedisStoreMapper. It sets up the connection to Redis and defines how to store data.

LANGUAGE: java
CODE:
JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
                .setHost(host).setPort(port).build();
RedisStoreMapper storeMapper = new WordCountStoreMapper();
RedisStoreBolt storeBolt = new RedisStoreBolt(poolConfig, storeMapper);

----------------------------------------

TITLE: Debugging Kryo ConcurrentModificationException in Java
DESCRIPTION: This snippet shows a stack trace for a ConcurrentModificationException that occurs when emitting mutable objects as output tuples in Storm. The error is caused by modifying an object while it's being serialized for network transmission.

LANGUAGE: java
CODE:
java.lang.RuntimeException: java.util.ConcurrentModificationException
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:84)
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:55)
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:56)
	at org.apache.storm.disruptor$consume_loop_STAR_$fn__1597.invoke(disruptor.clj:67)
	at org.apache.storm.util$async_loop$fn__465.invoke(util.clj:377)
	at clojure.lang.AFn.run(AFn.java:24)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.util.ConcurrentModificationException
	at java.util.LinkedHashMap$LinkedHashIterator.nextEntry(LinkedHashMap.java:390)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:409)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:408)
	at java.util.HashMap.writeObject(HashMap.java:1016)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:959)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1480)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at org.apache.storm.serialization.SerializableSerializer.write(SerializableSerializer.java:21)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:554)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:77)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:472)
	at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:27)

----------------------------------------

TITLE: Deploying a Flux Topology
DESCRIPTION: Command line example for deploying a Flux topology using the storm jar command.

LANGUAGE: bash
CODE:
storm jar myTopology-0.1.0-SNAPSHOT.jar org.apache.storm.flux.Flux --local my_config.yaml

----------------------------------------

TITLE: Debugging Kryo ConcurrentModificationException in Java
DESCRIPTION: This stack trace demonstrates a common runtime error in Storm caused by emitting mutable objects as output tuples. The issue occurs when a bolt modifies an object while it's being serialized for network transmission.

LANGUAGE: java
CODE:
java.lang.RuntimeException: java.util.ConcurrentModificationException
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:84)
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:55)
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:56)
	at org.apache.storm.disruptor$consume_loop_STAR_$fn__1597.invoke(disruptor.clj:67)
	at org.apache.storm.util$async_loop$fn__465.invoke(util.clj:377)
	at clojure.lang.AFn.run(AFn.java:24)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.util.ConcurrentModificationException
	at java.util.LinkedHashMap$LinkedHashIterator.nextEntry(LinkedHashMap.java:390)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:409)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:408)
	at java.util.HashMap.writeObject(HashMap.java:1016)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:959)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1480)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at org.apache.storm.serialization.SerializableSerializer.write(SerializableSerializer.java:21)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:554)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:77)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:472)
	at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:27)

----------------------------------------

TITLE: Implementing Sliding Window Bolt in Java
DESCRIPTION: Example implementation of a sliding window bolt showing topology setup and window configuration. Demonstrates how to process windowed tuples and emit results.

LANGUAGE: java
CODE:
public class SlidingWindowBolt extends BaseWindowedBolt {
	private OutputCollector collector;
	
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
    	this.collector = collector;
    }
	
    @Override
    public void execute(TupleWindow inputWindow) {
	  for(Tuple tuple: inputWindow.get()) {
	    // do the windowing computation
		...
	  }
	  // emit the results
	  collector.emit(new Values(computedValue));
    }
}

public static void main(String[] args) {
    TopologyBuilder builder = new TopologyBuilder();
     builder.setSpout("spout", new RandomSentenceSpout(), 1);
     builder.setBolt("slidingwindowbolt", 
                     new SlidingWindowBolt().withWindow(new Count(30), new Count(10)),
                     1).shuffleGrouping("spout");
    Config conf = new Config();
    conf.setDebug(true);
    conf.setNumWorkers(1);

    StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());
	
}

----------------------------------------

TITLE: Starting Metrics Reporters in Clojure for Storm
DESCRIPTION: This snippet demonstrates how to start a JMX metrics reporter in Clojure. This is necessary to collect and expose the metrics for monitoring tools.

LANGUAGE: clojure
CODE:
(defn start-metrics-reporters []
    (jmx/start (jmx/reporter {})))

----------------------------------------

TITLE: BasicBolt Implementation of SplitSentence
DESCRIPTION: Simplified implementation of the sentence splitting bolt using BasicBolt interface which handles anchoring and acking automatically.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseBasicBolt {
        public void execute(Tuple tuple, BasicOutputCollector collector) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                collector.emit(new Values(word));
            }
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Registering Custom Counter in Storm Bolt
DESCRIPTION: Example of a simple bolt implementation that counts received tuples using Storm's metrics API. The bolt registers a counter metric and increments it for each tuple processed.

LANGUAGE: java
CODE:
public class TupleCountingBolt extends BaseRichBolt {
    private Counter tupleCounter;
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        this.tupleCounter = context.registerCounter("tupleCount");
    }

    @Override
    public void execute(Tuple input) {
        this.tupleCounter.inc();
    }
}

----------------------------------------

TITLE: Preconfigured DRPC Client Usage
DESCRIPTION: Simplified example of using a preconfigured DRPC client that automatically handles host selection and failover.

LANGUAGE: java
CODE:
DRPCClient client = DRPCClient.getConfiguredClient(conf);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Overriding Local Mode in Java for Storm Topology Debugging
DESCRIPTION: Shows how to modify the main method to run a Storm topology in local mode within an IDE for debugging purposes. It uses the LocalCluster.withLocalModeOverride method to wrap the original main method.

LANGUAGE: java
CODE:
public static void main(final String args[]) {
    LocalCluster.withLocalModeOverride(() -> originalMain(args), 10);
}

----------------------------------------

TITLE: Configuring Maven Assembly Plugin for Storm Topology JAR Creation
DESCRIPTION: This XML configuration for the Maven Assembly Plugin packages a Storm topology into a JAR file with dependencies. It specifies the main class and uses the jar-with-dependencies descriptor.

LANGUAGE: xml
CODE:
<plugin>
  <artifactId>maven-assembly-plugin</artifactId>
  <configuration>
    <descriptorRefs>  
      <descriptorRef>jar-with-dependencies</descriptorRef>
    </descriptorRefs>
    <archive>
      <manifest>
        <mainClass>com.path.to.main.Class</mainClass>
      </manifest>
    </archive>
  </configuration>
</plugin>

----------------------------------------

TITLE: Applying a Function to a Trident Stream
DESCRIPTION: Example of applying a Function to a Trident stream, adding a new field to the output.

LANGUAGE: java
CODE:
mystream.each(new Fields("b"), new MyFunction(), new Fields("d")))

----------------------------------------

TITLE: Storm Docker Configuration Example
DESCRIPTION: Example configuration settings for enabling Docker support in Storm on RHEL7, including resource isolation plugin, image settings, and bind mounts.

LANGUAGE: bash
CODE:
storm.resource.isolation.plugin.enable: true
storm.resource.isolation.plugin: "org.apache.storm.container.docker.DockerManager"
storm.oci.allowed.images: ["xxx.xxx.com:8080/storm/storm/rhel7:latest"]
storm.oci.image: "xxx.xxx.com:8080/storm/storm/rhel7:latest"
storm.oci.cgroup.root: "/storm"
storm.oci.cgroup.parent: "/sys/fs/cgroup"
storm.oci.readonly.bindmounts:
    - "/etc/storm"
storm.oci.nscd.dir: "/var/run/nscd"
supervisor.worker.launcher: "/usr/share/apache-storm-2.3.0/bin/worker-launcher"

----------------------------------------

TITLE: Invalid Stream Reference Example
DESCRIPTION: Example showing incorrect forward referencing of streams in JoinBolt configuration.

LANGUAGE: java
CODE:
new JoinBolt( "spout1", "key1")                 
  .join     ( "spout2", "userId",  "spout3") //not allowed. spout3 not yet introduced
  .join     ( "spout3", "key3",    "spout1")

----------------------------------------

TITLE: Deploying Flux Topology
DESCRIPTION: Command to deploy a Flux topology using storm jar command

LANGUAGE: bash
CODE:
storm jar myTopology-0.1.0-SNAPSHOT.jar org.apache.storm.flux.Flux --local my_config.yaml

----------------------------------------

TITLE: Markdown Page Layout Definition
DESCRIPTION: Front matter and page structure definition for a Jekyll blog post displaying a logo contest entry

LANGUAGE: markdown
CODE:
---
layout: post
title: Logo Entry No. 5 - Ziba Sayari
---

{{ page.title }}
================

![Storm Brand](/images/logocontest/zsayari/storm_logo.png)

----------------------------------------

TITLE: Implementing StateUpdater for Bulk Updates
DESCRIPTION: Example of a StateUpdater implementation that performs bulk updates on the LocationDB state.

LANGUAGE: java
CODE:
public class LocationUpdater extends BaseStateUpdater<LocationDB> {
    public void updateState(LocationDB state, List<TridentTuple> tuples, TridentCollector collector) {
        List<Long> ids = new ArrayList<Long>();
        List<String> locations = new ArrayList<String>();
        for(TridentTuple t: tuples) {
            ids.add(t.getLong(0));
            locations.add(t.getString(1));
        }
        state.setLocationsBulk(ids, locations);
    }
}

----------------------------------------

TITLE: Configuring Scheduling Priority Strategy in YAML
DESCRIPTION: YAML configuration to set the scheduling priority strategy.

LANGUAGE: yaml
CODE:
resource.aware.scheduler.priority.strategy: "org.apache.storm.scheduler.resource.strategies.priority.DefaultSchedulingPriorityStrategy"

----------------------------------------

TITLE: Implementing FlatMap Transform
DESCRIPTION: Example of implementing a flatMap transform that splits sentences into words.

LANGUAGE: java
CODE:
public class Split extends FlatMapFunction {
  @Override
  public Iterable<Values> execute(TridentTuple input) {
    List<Values> valuesList = new ArrayList<>();
    for (String word : input.getString(0).split(" ")) {
      valuesList.add(new Values(word));
    }
    return valuesList;
  }
}

----------------------------------------

TITLE: Failing Tuple in JSON Format for Storm Multi-Language Protocol
DESCRIPTION: This JSON structure represents a fail command in the Storm multi-language protocol. It includes the command type and the id of the tuple to be marked as failed.

LANGUAGE: JSON
CODE:
{
    "command": "fail",
    "id": 123123
}

----------------------------------------

TITLE: Implementing a Sliding Window in Trident
DESCRIPTION: Shows how to create a sliding window based on tuple count or duration.

LANGUAGE: java
CODE:
public Stream slidingWindow(int windowCount, int slideCount, WindowsStoreFactory windowStoreFactory,
                                      Fields inputFields, Aggregator aggregator, Fields functionFields);

public Stream slidingWindow(BaseWindowedBolt.Duration windowDuration, BaseWindowedBolt.Duration slidingInterval,
                                    WindowsStoreFactory windowStoreFactory, Fields inputFields, Aggregator aggregator, Fields functionFields);

----------------------------------------

TITLE: Storm DSL Links in Markdown
DESCRIPTION: Markdown formatted list of links to various Storm DSL implementations and language adapters, including references to GitHub repositories and documentation.

LANGUAGE: markdown
CODE:
* [Scala DSL](https://github.com/velvia/ScalaStorm)
* [JRuby DSL](https://github.com/colinsurprenant/redstorm)
* [Clojure DSL](Clojure-DSL.html)
* [Storm/Esper integration](https://github.com/tomdz/storm-esper): Streaming SQL on top of Storm
* [io-storm](https://github.com/dan-blanchard/io-storm): Perl multilang adapter

----------------------------------------

TITLE: Building the Azure Event Hubs Storm Integration
DESCRIPTION: Command to build the project using Maven, generating the necessary JAR files for deployment.

LANGUAGE: bash
CODE:
mvn clean package

----------------------------------------

TITLE: Deploying and Running a Flux Topology
DESCRIPTION: Command line example for deploying and running a Flux topology using the 'storm jar' command.

LANGUAGE: bash
CODE:
storm jar myTopology-0.1.0-SNAPSHOT.jar org.apache.storm.flux.Flux --local my_config.yaml

----------------------------------------

TITLE: Creating Storm Topology with Word Count Example
DESCRIPTION: Example showing how to build a Storm topology that reads sentences from Kestrel queue, splits them into words, and counts occurrences.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("sentences", new KestrelSpout("kestrel.backtype.com",
                                               22133,
                                               "sentence_queue",
                                               new StringScheme()));
builder.setBolt("split", new SplitSentence(), 10)
        .shuffleGrouping("sentences");
builder.setBolt("count", new WordCount(), 20)
        .fieldsGrouping("split", new Fields("word"));

----------------------------------------

TITLE: Configuring Pacemaker Servers in Storm
DESCRIPTION: Specifies the hosts running Pacemaker daemons in the Storm cluster configuration.

LANGUAGE: yaml
CODE:
pacemaker.servers:
    - somehost.mycompany.com
    - someotherhost.mycompany.com

----------------------------------------

TITLE: Setting Memory and CPU Requirements for Storm Components
DESCRIPTION: Example showing how to set memory and CPU requirements for spouts and bolts in a Storm topology using the Java API.

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.setMemoryLoad(1024.0, 512.0);
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
            .shuffleGrouping("word").setMemoryLoad(512.0);

s1.setCPULoad(15.0);
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
            .shuffleGrouping("word").setCPULoad(10.0);
builder.setBolt("exclaim2", new HeavyBolt(), 1)
                .shuffleGrouping("exclaim1").setCPULoad(450.0);

----------------------------------------

TITLE: Configuring KafkaSpout with Wildcard Topics
DESCRIPTION: Example of setting up a KafkaSpout to consume from multiple Kafka topics using a wildcard pattern.

LANGUAGE: java
CODE:
final TopologyBuilder tp = new TopologyBuilder();
tp.setSpout("kafka_spout", new KafkaSpout<>(KafkaSpoutConfig.builder("127.0.0.1:" + port, Pattern.compile("topic.*")).build()), 1);
tp.setBolt("bolt", new myBolt()).shuffleGrouping("kafka_spout");

----------------------------------------

TITLE: Creating Kerberos Principals and Keytabs
DESCRIPTION: Bash commands for creating Kerberos principals and keytabs for Zookeeper, Nimbus, DRPC, UI, and Supervisors

LANGUAGE: bash
CODE:
# Zookeeper (Will need one of these for each box in the Zk ensemble)
sudo kadmin.local -q 'addprinc zookeeper/zk1.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/zk.keytab  zookeeper/zk1.example.com@STORM.EXAMPLE.COM"
# Nimbus and DRPC
sudo kadmin.local -q 'addprinc storm/storm.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm/storm.example.com@STORM.EXAMPLE.COM"
# All UI logviewer and Supervisors
sudo kadmin.local -q 'addprinc storm@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm@STORM.EXAMPLE.COM"

----------------------------------------

TITLE: Initializing Worker Process (Clojure)
DESCRIPTION: The mk-worker function initializes a worker process, connecting to other workers, monitoring topology activity, and launching task threads.

LANGUAGE: clojure
CODE:
(mk-worker)

----------------------------------------

TITLE: Deploying a Flux Topology
DESCRIPTION: Bash command showing how to deploy a Flux topology using the storm jar command.

LANGUAGE: bash
CODE:
storm jar myTopology-0.1.0-SNAPSHOT.jar org.apache.storm.flux.Flux --local my_config.yaml

----------------------------------------

TITLE: Initializing ClientBlobStore in Java
DESCRIPTION: Java code snippet showing how to initialize a ClientBlobStore object for interacting with the distributed cache API.

LANGUAGE: java
CODE:
Config theconf = new Config();
theconf.putAll(Utils.readStormConfig());
ClientBlobStore clientBlobStore = Utils.getClientBlobStore(theconf);

----------------------------------------

TITLE: Setting Supervisor Slot Ports in Storm YAML
DESCRIPTION: This snippet demonstrates how to configure the ports for worker processes on each machine. It defines which ports are open for use by Storm workers.

LANGUAGE: yaml
CODE:
supervisor.slots.ports:
    - 6700
    - 6701
    - 6702
    - 6703

----------------------------------------

TITLE: Storm JoinBolt Implementation
DESCRIPTION: Java implementation of JoinBolt configuration that recreates the SQL join example, including window configuration and fields grouping setup

LANGUAGE: java
CODE:
JoinBolt jbolt =  new JoinBolt("spout1", "key1")                   // from        spout1  
                    .join     ("spout2", "userId",  "spout1")      // inner join  spout2  on spout2.userId = spout1.key1
                    .join     ("spout3", "key3",    "spout2")      // inner join  spout3  on spout3.key3   = spout2.userId   
                    .leftJoin ("spout4", "key4",    "spout3")      // left join   spout4  on spout4.key4   = spout3.key3
                    .select  ("userId, key4, key2, spout3:key3")   // chose output fields
                    .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;

topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("spout1", new Fields("key1") )
            .fieldsGrouping("spout2", new Fields("userId") )
            .fieldsGrouping("spout3", new Fields("key3") )
            .fieldsGrouping("spout4", new Fields("key4") );

----------------------------------------

TITLE: Chaining Map and FlatMap Operations in Trident
DESCRIPTION: Example of chaining Map and FlatMap operations in a Trident stream.

LANGUAGE: java
CODE:
mystream.flatMap(new Split()).map(new UpperCase())

----------------------------------------

TITLE: Displaying jQuery UI License Text
DESCRIPTION: This snippet contains the full text of the jQuery UI license, which is based on the MIT License. It includes copyright information, permissions granted, conditions, warranty disclaimers, and liability limitations. It also specifies different terms for sample code and external libraries.

LANGUAGE: plaintext
CODE:
Copyright jQuery Foundation and other contributors, https://jquery.org/

This software consists of voluntary contributions made by many
individuals. For exact contribution history, see the revision history
available at https://github.com/jquery/jquery-ui

The following license applies to all parts of this software except as
documented below:

====

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

====

Copyright and related rights for sample code are waived via CC0. Sample
code is defined as all source code contained within the demos directory.

CC0: http://creativecommons.org/publicdomain/zero/1.0/

====

All files located in the node_modules and external directories are
externally maintained libraries used by this software which have their
own licenses; we recommend you read them, as their terms may differ from
the terms above.

----------------------------------------

TITLE: Storm JoinBolt Implementation
DESCRIPTION: Implementation of the equivalent SQL join using Storm's JoinBolt, demonstrating stream joining with window configuration and fields grouping.

LANGUAGE: java
CODE:
JoinBolt jbolt =  new JoinBolt("spout1", "key1")                   // from        spout1  
                    .join     ("spout2", "userId",  "spout1")      // inner join  spout2  on spout2.userId = spout1.key1
                    .join     ("spout3", "key3",    "spout2")      // inner join  spout3  on spout3.key3   = spout2.userId   
                    .leftJoin ("spout4", "key4",    "spout3")      // left join   spout4  on spout4.key4   = spout3.key3
                    .select  ("userId, key4, key2, spout3:key3")   // chose output fields
                    .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;

topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("spout1", new Fields("key1") )
            .fieldsGrouping("spout2", new Fields("userId") )
            .fieldsGrouping("spout3", new Fields("key3") )
            .fieldsGrouping("spout4", new Fields("key4") );

----------------------------------------

TITLE: Storm Protocol Log Command Format
DESCRIPTION: JSON structure for logging messages in Storm's multilang protocol. Contains command type and message content.

LANGUAGE: json
CODE:
{
    "command": "log",
    "msg": "hello world!"
}

----------------------------------------

TITLE: Storm Protocol Log Command Format
DESCRIPTION: JSON structure for logging messages in Storm's multilang protocol. Contains command type and message content.

LANGUAGE: json
CODE:
{
    "command": "log",
    "msg": "hello world!"
}

----------------------------------------

TITLE: Storm Protocol Log Command Format
DESCRIPTION: JSON structure for logging messages in Storm's multilang protocol. Contains command type and message content.

LANGUAGE: json
CODE:
{
    "command": "log",
    "msg": "hello world!"
}

----------------------------------------

TITLE: Storm JoinBolt Configuration
DESCRIPTION: Example of configuring JoinBolt to join multiple streams with field grouping and window settings. Demonstrates inner and left joins equivalent to the SQL example.

LANGUAGE: java
CODE:
JoinBolt jbolt =  new JoinBolt("spout1", "key1")                   // from        spout1  
                    .join     ("spout2", "userId",  "spout1")      // inner join  spout2  on spout2.userId = spout1.key1
                    .join     ("spout3", "key3",    "spout2")      // inner join  spout3  on spout3.key3   = spout2.userId   
                    .leftJoin ("spout4", "key4",    "spout3")      // left join   spout4  on spout4.key4   = spout3.key3
                    .select  ("userId, key4, key2, spout3:key3")   // chose output fields
                    .withTumblingWindow( new Duration(10, TimeUnit.MINUTES) ) ;

topoBuilder.setBolt("joiner", jbolt, 1)
            .fieldsGrouping("spout1", new Fields("key1") )
            .fieldsGrouping("spout2", new Fields("userId") )
            .fieldsGrouping("spout3", new Fields("key3") )
            .fieldsGrouping("spout4", new Fields("key4") );

----------------------------------------

TITLE: Implementing a Trident Filter
DESCRIPTION: Example of implementing a Trident Filter to selectively keep or discard tuples.

LANGUAGE: java
CODE:
public class MyFilter extends BaseFilter {
    public boolean isKeep(TridentTuple tuple) {
        return tuple.getInteger(0) == 1 && tuple.getInteger(1) == 2;
    }
}

----------------------------------------

TITLE: Storm Topology Rebalance Command
DESCRIPTION: CLI command example for rebalancing a running Storm topology. Shows how to modify the number of worker processes and executors for specific components.

LANGUAGE: bash
CODE:
storm rebalance mytopology -n 5 -e blue-spout=3 -e yellow-bolt=10

----------------------------------------

TITLE: Submitting Storm Topology Configuration
DESCRIPTION: Demonstrates how to configure and submit a Storm topology using StormSubmitter with worker and spout settings.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(20);
conf.setMaxSpoutPending(5000);
StormSubmitter.submitTopology("mytopology", conf, topology);

----------------------------------------

TITLE: Submitting DRPC Topology to Storm Cluster in Java
DESCRIPTION: This code snippet shows how to submit a DRPC topology to a Storm cluster using StormSubmitter.

LANGUAGE: java
CODE:
StormSubmitter.submitTopology("exclamation-drpc", conf, builder.createRemoteTopology());

----------------------------------------

TITLE: Implementing Sliding Window Bolt
DESCRIPTION: Example implementation of a sliding window bolt extending BaseWindowedBolt with main topology setup and configuration.

LANGUAGE: java
CODE:
public class SlidingWindowBolt extends BaseWindowedBolt {
	private OutputCollector collector;
	
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
    	this.collector = collector;
    }
	
    @Override
    public void execute(TupleWindow inputWindow) {
	  for(Tuple tuple: inputWindow.get()) {
	    // do the windowing computation
		...
	  }
	  // emit the results
	  collector.emit(new Values(computedValue));
    }
}

public static void main(String[] args) {
    TopologyBuilder builder = new TopologyBuilder();
     builder.setSpout("spout", new RandomSentenceSpout(), 1);
     builder.setBolt("slidingwindowbolt", 
                     new SlidingWindowBolt().withWindow(new Count(30), new Count(10)),
                     1).shuffleGrouping("spout");
    Config conf = new Config();
    conf.setDebug(true);
    conf.setNumWorkers(1);

    StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());
	
}

----------------------------------------

TITLE: Configuring HTML Meta Redirect to Storm Documentation
DESCRIPTION: Sets up an automatic page redirect to the Storm project's Rationale documentation page using meta refresh tag and canonical link.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Rationale.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Rationale.html" />

----------------------------------------

TITLE: Setting CPU Requirements for Components in Java
DESCRIPTION: Java API calls to set CPU requirements for spouts and bolts in a Storm topology.

LANGUAGE: java
CODE:
SpoutDeclarer s1 = builder.setSpout("word", new TestWordSpout(), 10);
s1.setCPULoad(15.0);
builder.setBolt("exclaim1", new ExclamationBolt(), 3)
            .shuffleGrouping("word").setCPULoad(10.0);
builder.setBolt("exclaim2", new HeavyBolt(), 1)
                .shuffleGrouping("exclaim1").setCPULoad(450.0);

----------------------------------------

TITLE: OCI/Squashfs Runtime Configuration
DESCRIPTION: Storm configuration settings required to enable and configure the OCI/Squashfs runtime.

LANGUAGE: yaml
CODE:
storm.resource.isolation.plugin: "org.apache.storm.container.oci.RuncLibContainerManager"

storm.oci.allowed.images:
    - "storm/rhel7:dev_current"
    - "storm/rhel7:dev_previous"
    - "storm/rhel7:dev_test"
storm.oci.image: "storm/rhel7:dev_current"

storm.oci.cgroup.parent: "/storm"
storm.oci.cgroup.root: "/sys/fs/cgroup"
storm.oci.image.hdfs.toplevel.dir: "hdfs://host:port/containers/"
storm.oci.image.tag.to.manifest.plugin: "org.apache.storm.container.oci.LocalOrHdfsImageTagToManifestPlugin"
storm.oci.local.or.hdfs.image.tag.to.manifest.plugin.hdfs.hash.file: "hdfs://host:port/containers/image-tag-to-hash"
storm.oci.manifest.to.resources.plugin: "org.apache.storm.container.oci.HdfsManifestToResourcesPlugin"
storm.oci.readonly.bindmounts:
    - "/home/y/lib64/storm"
    - "/etc/krb5.conf"

storm.oci.resources.localizer: "org.apache.storm.container.oci.HdfsOciResourcesLocalizer"
storm.oci.seccomp.profile: "/home/y/conf/storm/seccomp.json"

----------------------------------------

TITLE: Running a Storm Topology
DESCRIPTION: Demonstrates how to package and run a Storm topology using the command line.

LANGUAGE: bash
CODE:
storm jar all-my-code.jar org.apache.storm.MyTopology arg1 arg2

----------------------------------------

TITLE: Emitting Tuples with Direct Grouping
DESCRIPTION: Shows how to emit tuples to a direct stream, specifying the target task ID when using OutputCollector in a bolt.

LANGUAGE: java
CODE:
int targetTaskId = getTargetTaskId();
collector.emitDirect(targetTaskId, "directStream", new Values(value1, value2));

----------------------------------------

TITLE: Creating Storm Blob Store ACLs
DESCRIPTION: Example showing how to create Access Control Lists (ACLs) for blob storage in Storm with user permissions

LANGUAGE: java
CODE:
String stringBlobACL = "u:username:rwa";
AccessControl blobACL = BlobStoreAclHandler.parseAccessControl(stringBlobACL);
List<AccessControl> acls = new LinkedList<AccessControl>();
acls.add(blobACL); // more ACLs can be added here
SettableBlobMeta settableBlobMeta = new SettableBlobMeta(acls);
settableBlobMeta.set_replication_factor(4); // Here we can set the replication factor

----------------------------------------

TITLE: Topology Configuration Example - Java
DESCRIPTION: Example demonstrating how to set the number of worker processes for a Storm topology using the TOPOLOGY_WORKERS configuration parameter.

LANGUAGE: java
CODE:
Config.TOPOLOGY_WORKERS

----------------------------------------

TITLE: Configuring IsolationScheduler in YAML
DESCRIPTION: Example YAML configuration for the IsolationScheduler in Storm. This snippet demonstrates how to allocate specific numbers of isolated machines to different topologies using the 'isolation.scheduler.machines' setting.

LANGUAGE: yaml
CODE:
isolation.scheduler.machines: 
    "my-topology": 8
    "tiny-topology": 1
    "some-other-topology": 3

----------------------------------------

TITLE: Named Stream Join Configuration
DESCRIPTION: Example showing how to configure JoinBolt to use named streams instead of default streams.

LANGUAGE: java
CODE:
new JoinBolt(JoinBolt.Selector.STREAM,  "stream1", "key1")
                                  .join("stream2", "key2")
    ...

----------------------------------------

TITLE: Configuring EsConfig for Storm-Elasticsearch Integration in Java
DESCRIPTION: These snippets show two ways to create an EsConfig object for configuring Elasticsearch cluster connection in Storm components. The first uses basic settings, while the second includes additional parameters.

LANGUAGE: java
CODE:
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"});

LANGUAGE: java
CODE:
Map<String, String> additionalParameters = new HashMap<>();
additionalParameters.put("client.transport.sniff", "true");
EsConfig esConfig = new EsConfig(clusterName, new String[]{"localhost:9300"}, additionalParameters);

----------------------------------------

TITLE: Applying a Function to a Trident Stream
DESCRIPTION: Example of applying the MyFunction to a stream with fields ["a", "b", "c"].

LANGUAGE: java
CODE:
mystream.each(new Fields("b"), new MyFunction(), new Fields("d")))

----------------------------------------

TITLE: Configuring Storm Bolt Parallelism
DESCRIPTION: Example showing how to configure a Storm bolt with specific executor and task settings. Sets up a GreenBolt with 2 executors and 4 tasks, using shuffle grouping from a blue-spout.

LANGUAGE: java
CODE:
topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

----------------------------------------

TITLE: Creating Trident Topology with Fields Mapper
DESCRIPTION: Demonstrates setting up a Trident topology using SolrFieldsMapper for updating Solr collections. Includes configuration for schema building and field mapping.

LANGUAGE: java
CODE:
    new SolrStateFactory(solrConfig, solrMapper);
    
    // zkHostString for Solr 'gettingstarted' example
    SolrConfig solrConfig = new SolrConfig("127.0.0.1:9983");
    
    /* Solr Fields Mapper used to generate 'SolrRequest' requests to update the "gettingstarted" Solr collection. The Solr index is updated using the field values of the tuple fields that match static or dynamic fields declared in the schema object build using schemaBuilder */ 
    SolrMapper solrMapper = new SolrFieldsMapper.Builder(schemaBuilder, "gettingstarted").build();

    // builds the Schema object from the JSON representation of the schema as returned by the URL http://localhost:8983/solr/gettingstarted/schema/ 
    SchemaBuilder schemaBuilder = new RestJsonSchemaBuilder("localhost", "8983", "gettingstarted")

----------------------------------------

TITLE: Stateful Persistent Windowed Bolt Implementation
DESCRIPTION: Example implementation of a persistent windowed bolt that uses window checkpointing to save state across windows.

LANGUAGE: java
CODE:
public class MyStatefulPersistentWindowedBolt extends BaseStatefulWindowedBolt<K, V> {
  private KeyValueState<K, V> state;
  
  @Override
  public void initState(KeyValueState<K, V> state) {
    this.state = state;
   // ...
   // restore the state from the last saved state.
   // ...
  }
  
  @Override
  public void execute(TupleWindow window) {      
    // iterate over tuples in the current window
    Iterator<Tuple> it = window.getIter();
    while (it.hasNext()) {
        // compute some result based on the tuples in window
    }
    
    // possibly update any state to be maintained across windows
    state.put(STATE_KEY, updatedValue);
    
    // emit the results downstream
    collector.emit(new Values(result));
  }
}

----------------------------------------

TITLE: Creating Kerberos Principals and Keytabs
DESCRIPTION: Bash commands to create Kerberos principals and keytabs for Zookeeper, Nimbus, DRPC and Storm components

LANGUAGE: bash
CODE:
# Zookeeper (Will need one of these for each box in the Zk ensemble)
sudo kadmin.local -q 'addprinc zookeeper/zk1.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/zk.keytab  zookeeper/zk1.example.com@STORM.EXAMPLE.COM"
# Nimbus and DRPC
sudo kadmin.local -q 'addprinc storm/storm.example.com@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm/storm.example.com@STORM.EXAMPLE.COM"
# All UI logviewer and Supervisors
sudo kadmin.local -q 'addprinc storm@STORM.EXAMPLE.COM'
sudo kadmin.local -q "ktadd -k /tmp/storm.keytab storm@STORM.EXAMPLE.COM"

----------------------------------------

TITLE: Updating Python Storm Script Compatibility
DESCRIPTION: Update to storm.py script to ensure compatibility with Python 3 environments while maintaining existing functionality.



----------------------------------------

TITLE: Implementing MQTT Tuple Mapper in Java
DESCRIPTION: Example implementation of MqttTupleMapper interface to convert Storm tuples to MQTT messages for publishing.

LANGUAGE: java
CODE:
public class MyTupleMapper implements MqttTupleMapper {
    public MqttMessage toMessage(ITuple tuple) {
        String topic = "users/" + tuple.getStringByField("userId") + "/" + tuple.getStringByField("device");
        byte[] payload = tuple.getStringByField("message").getBytes();
        return new MqttMessage(topic, payload);
    }
}

----------------------------------------

TITLE: Registering Event Loggers in YAML Configuration
DESCRIPTION: Shows how to register multiple event logger implementations in the storm.yaml configuration file, including passing arguments to custom loggers.

LANGUAGE: yaml
CODE:
topology.event.logger.register:
  - class: "org.apache.storm.metric.FileBasedEventLogger"
  - class: "org.mycompany.MyEventLogger"
    arguments:
      endpoint: "event-logger.mycompany.org"

----------------------------------------

TITLE: Event Hubs Configuration Properties
DESCRIPTION: Configuration properties for connecting Storm spout to Azure Event Hubs, including authentication, namespace settings, and performance tuning parameters

LANGUAGE: properties
CODE:
eventhubspout.username = [username: policy name in EventHubs Portal]
eventhubspout.password = [password: shared access key in EventHubs Portal]
eventhubspout.namespace = [namespace]
eventhubspout.entitypath = [entitypath]
eventhubspout.partitions.count = [partitioncount]

# if not provided, will use storm's zookeeper settings
# zookeeper.connectionstring=zookeeper0:2181,zookeeper1:2181,zookeeper2:2181

eventhubspout.checkpoint.interval = 10
eventhub.receiver.credits = 1024

----------------------------------------

TITLE: Configuring Stream Groupings in Storm Topology
DESCRIPTION: This example demonstrates how to set up different stream groupings when building a Storm topology using TopologyBuilder.

LANGUAGE: java
CODE:
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("spout", new MySpout());
builder.setBolt("bolt1", new MyBolt1()).shuffleGrouping("spout");
builder.setBolt("bolt2", new MyBolt2()).fieldsGrouping("bolt1", new Fields("user_id"));
builder.setBolt("bolt3", new MyBolt3()).allGrouping("bolt1");

----------------------------------------

TITLE: Implementing a Bolt in Java with BaseRichBolt
DESCRIPTION: Demonstrates a more concise implementation of a bolt using BaseRichBolt.

LANGUAGE: java
CODE:
public static class ExclamationBolt extends BaseRichBolt {
    OutputCollector _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple tuple) {
        _collector.emit(tuple, new Values(tuple.getString(0) + "!!!"));
        _collector.ack(tuple);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word"));
    }    
}

----------------------------------------

TITLE: Installing JZMQ from Fork
DESCRIPTION: Commands to clone and install JZMQ from a specific fork that is tested to work with Storm. This process includes cloning the repository, generating configuration files, and installing the Java bindings.

LANGUAGE: bash
CODE:
#install jzmq
git clone https://github.com/nathanmarz/jzmq.git
cd jzmq
./autogen.sh
./configure
make
sudo make install

----------------------------------------

TITLE: Multi-Anchored Tuple Emission
DESCRIPTION: Example showing how to emit a tuple anchored to multiple parent tuples for join or aggregation operations.

LANGUAGE: java
CODE:
List<Tuple> anchors = new ArrayList<Tuple>();
anchors.add(tuple1);
anchors.add(tuple2);
_collector.emit(anchors, new Values(1, 2, 3));

----------------------------------------

TITLE: Configuring Storm Cluster State Store
DESCRIPTION: Basic configuration to enable Pacemaker as the cluster state store in Storm.

LANGUAGE: yaml
CODE:
storm.cluster.state.store: "org.apache.storm.cluster.PaceMakerStateStorageFactory"

----------------------------------------

TITLE: Implementing Storm BasicBolt
DESCRIPTION: Simplified implementation of a sentence splitting bolt using BasicBolt interface which handles anchoring and acking automatically.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseBasicBolt {
        public void execute(Tuple tuple, BasicOutputCollector collector) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                collector.emit(new Values(word));
            }
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Starting MQTT Broker and Publisher in Java
DESCRIPTION: Command to start an MQTT broker on port 1883 and a publisher that sends random temperature/humidity values to an MQTT topic.

LANGUAGE: bash
CODE:
java -cp examples/target/storm-mqtt-examples-*-SNAPSHOT.jar org.apache.storm.mqtt.examples.MqttBrokerPublisher

----------------------------------------

TITLE: Running Storm PMML Example Command
DESCRIPTION: Command line instruction for executing the PMML example topology. Requires Storm installation, example JAR file, PMML model file, and input data CSV file.

LANGUAGE: java
CODE:
STORM-HOME/bin/storm jar STORM-HOME/examples/storm-pmml-examples/storm-pmml-examples-2.0.0-SNAPSHOT.jar org.apache.storm.pmml.JpmmlRunnerTestTopology jpmmlTopology PMMLModel.xml RawInputData.csv

----------------------------------------

TITLE: Running Storm PMML Example Command
DESCRIPTION: Command line instruction for executing the PMML example topology. Requires Storm installation, example JAR file, PMML model file, and input data CSV file.

LANGUAGE: java
CODE:
STORM-HOME/bin/storm jar STORM-HOME/examples/storm-pmml-examples/storm-pmml-examples-2.0.0-SNAPSHOT.jar org.apache.storm.pmml.JpmmlRunnerTestTopology jpmmlTopology PMMLModel.xml RawInputData.csv

----------------------------------------

TITLE: Configuring Maven Shade Plugin for Uber Jar in XML
DESCRIPTION: This XML snippet shows how to configure the Maven Shade Plugin in the pom.xml file to create an uber jar with all dependencies for the Storm Solr integration project.

LANGUAGE: xml
CODE:
<plugin>
     <groupId>org.apache.maven.plugins</groupId>
     <artifactId>maven-shade-plugin</artifactId>
     <version>2.4.1</version>
     <executions>
         <execution>
             <phase>package</phase>
             <goals>
                 <goal>shade</goal>
             </goals>
             <configuration>
                 <transformers>
                     <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                         <mainClass>org.apache.storm.solr.topology.SolrJsonTopology</mainClass>
                     </transformer>
                 </transformers>
             </configuration>
         </execution>
     </executions>
</plugin>

----------------------------------------

TITLE: Implementing Skew-Aware Top N Processing in Storm
DESCRIPTION: Shows an advanced implementation of top N processing that handles data skew using partialKeyGrouping. This pattern distributes load for each key across multiple bolts and includes an aggregation layer.

LANGUAGE: java
CODE:
builder.setBolt("count", new CountObjects(), parallelism)
  .partialKeyGrouping("objects", new Fields("value"));
builder.setBolt("rank" new AggregateCountsAndRank(), parallelism)
  .fieldsGrouping("count", new Fields("key"))
builder.setBolt("merge", new MergeRanksObjects())
  .globalGrouping("rank");

----------------------------------------

TITLE: Configuring Netty Transport in Apache Storm
DESCRIPTION: YAML configuration for enabling and customizing the Netty transport system in Apache Storm. Includes settings for worker threads, buffer size, and retry parameters.

LANGUAGE: yaml
CODE:
storm.messaging.transport: "backtype.storm.messaging.netty.Context"
storm.messaging.netty.server_worker_threads: 1
storm.messaging.netty.client_worker_threads: 1
storm.messaging.netty.buffer_size: 5242880
storm.messaging.netty.max_retries: 100
storm.messaging.netty.max_wait_ms: 1000
storm.messaging.netty.min_wait_ms: 100

----------------------------------------

TITLE: Implementing Local DRPC in Storm
DESCRIPTION: Demonstrates how to run DRPC (Distributed Remote Procedure Call) in local mode using LocalDRPC and LocalCluster. Shows topology submission and execution of DRPC requests.

LANGUAGE: java
CODE:
try (LocalDRPC drpc = new LocalDRPC();
     LocalCluster cluster = new LocalCluster();
     LocalTopology topo = cluster.submitTopology("drpc-demo", conf, builder.createLocalTopology(drpc))) {

    System.out.println("Results for 'hello':" + drpc.execute("exclamation", "hello"));
}

----------------------------------------

TITLE: Running Storm Topology Command
DESCRIPTION: Command to submit the sample topology to Storm using the Event Hubs spout

LANGUAGE: bash
CODE:
storm jar {jarfile} com.microsoft.eventhubs.samples.EventCount {topologyname} {spoutconffile}

----------------------------------------

TITLE: Dumping Heap in Storm UI REST API
DESCRIPTION: Example JSON response for the /api/v1/topology/:id/profiling/dumpheap/:host-port endpoint, which requests to dump the heap (jmap) on a worker.

LANGUAGE: json
CODE:
{
   "status": "ok",
   "id": "10.11.1.7:6701"
}

----------------------------------------

TITLE: Defining a Multi-Language Bolt in Java
DESCRIPTION: Java code showing how to define a bolt that uses a Python script for processing.

LANGUAGE: java
CODE:
public static class SplitSentence extends ShellBolt implements IRichBolt {
    public SplitSentence() {
        super("python3", "splitsentence.py");
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word"));
    }
}

----------------------------------------

TITLE: Registering Metrics Consumer in YAML
DESCRIPTION: Example of registering multiple metrics consumers using YAML configuration in storm.yaml file.

LANGUAGE: yaml
CODE:
topology.metrics.consumer.register:
  - class: "org.apache.storm.metric.LoggingMetricsConsumer"
    parallelism.hint: 1
  - class: "org.apache.storm.metric.HttpForwardingMetricsConsumer"
    parallelism.hint: 1
    argument: "http://example.com:8080/metrics/my-topology/"

----------------------------------------

TITLE: Debugging Nimbus JVM Shutdown in Storm
DESCRIPTION: This log snippet shows the Nimbus process shutting down immediately after startup due to an illegal instruction exception. This is likely caused by using a version of rocksdb-jni that is incompatible with older CPUs.

LANGUAGE: plaintext
CODE:
2024-01-05 18:54:20.404 [o.a.s.v.ConfigValidation] INFO: Will use [class org.apache.storm.DaemonConfig, class org.apache.storm.Config] for validation
2024-01-05 18:54:20.556 [o.a.s.z.AclEnforcement] INFO: SECURITY IS DISABLED NO FURTHER CHECKS...
2024-01-05 18:54:20.740 [o.a.s.m.r.RocksDbStore] INFO: Opening RocksDB from <your-storm-folder>/storm_rocks, storm.metricstore.rocksdb.create_if_missing=true

LANGUAGE: plaintext
CODE:
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  EXCEPTION_ILLEGAL_INSTRUCTION (0xc000001d) at pc=0x00007ff94dc7a56d, pid=12728, tid=0x0000000000001d94
#
# JRE version: OpenJDK Runtime Environment (8.0_232) (build 1.8.0_232-09)
# Java VM: OpenJDK 64-Bit Server VM (25.232-b09 mixed mode windows-amd64 compressed oops)
# Problematic frame:
# C  [librocksdbjni4887247215762585789.dll+0x53a56d]

----------------------------------------

TITLE: Sample Cluster Summary Response
DESCRIPTION: Example JSON response from the /api/v1/cluster/summary endpoint showing overall cluster metrics and resource utilization

LANGUAGE: json
CODE:
{
    "stormVersion": "0.9.2-incubating-SNAPSHOT",
    "supervisors": 1,
    "slotsTotal": 4,
    "slotsUsed": 3,
    "slotsFree": 1,
    "executorsTotal": 28,
    "tasksTotal": 28,
    "schedulerDisplayResource": true,
    "totalMem": 4096.0,
    "totalCpu": 400.0,
    "availMem": 1024.0,
    "availCPU": 250.0,
    "memAssignedPercentUtil": 75.0,
    "cpuAssignedPercentUtil": 37.5
}

----------------------------------------

TITLE: Configuring Pacemaker Servers in Storm
DESCRIPTION: Specifies the hostnames of Pacemaker servers in Storm's configuration.

LANGUAGE: yaml
CODE:
pacemaker.servers:
    - somehost.mycompany.com
    - someotherhost.mycompany.com

----------------------------------------

TITLE: Defining MQTT Message Mapper Interface in Java
DESCRIPTION: Interface definition for mapping MQTT messages to Storm tuples, including methods for converting messages to values and specifying output fields.

LANGUAGE: java
CODE:
public interface MqttMessageMapper extends Serializable {

    Values toValues(MqttMessage message);

    Fields outputFields();
}

----------------------------------------

TITLE: Defining MQTT Message Mapper Interface in Java
DESCRIPTION: Interface definition for mapping MQTT messages to Storm tuples, including methods for converting messages to values and specifying output fields.

LANGUAGE: java
CODE:
public interface MqttMessageMapper extends Serializable {

    Values toValues(MqttMessage message);

    Fields outputFields();
}

----------------------------------------

TITLE: Storm SQL Error Log Filtering
DESCRIPTION: SQL script to filter error logs (status >= 400) from Apache logs and store them in a separate Kafka topic with transformed fields.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://apache-logs?bootstrap-servers=localhost:9092'
CREATE EXTERNAL TABLE APACHE_ERROR_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_ELAPSED_MS INT) LOCATION 'kafka://apache-error-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO APACHE_ERROR_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, (TIME_US / 1000) AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (CAST(STATUS AS INT) / 100) >= 4

----------------------------------------

TITLE: Setting Log Levels for Storm Topology
DESCRIPTION: Illustrates how to dynamically change log levels for a running Storm topology using the 'set_log_level' command.

LANGUAGE: bash
CODE:
./bin/storm set_log_level -l ROOT=DEBUG:30 topology-name

----------------------------------------

TITLE: Worker Hook Registration
DESCRIPTION: Register worker-level hooks that execute during worker startup before spout/bolt initialization using TopologyBuilder.

LANGUAGE: java
CODE:
TopologyBuilder.addWorkerHook()

----------------------------------------

TITLE: Setting Log Level via Storm CLI
DESCRIPTION: Command line syntax for setting a logger level with optional timeout using the Storm CLI. The command allows specifying the topology name, logger name, log level, and timeout duration.

LANGUAGE: bash
CODE:
./bin/storm set_log_level [topology name] -l [logger name]=[LEVEL]:[TIMEOUT]

----------------------------------------

TITLE: Configuring Spout Wait Strategy in Apache Storm
DESCRIPTION: Demonstration of how to configure the new pluggable spout wait strategy in Apache Storm 0.8.1. This feature allows customization of spout behavior when reaching max pending limit or when nothing is emitted from nextTuple.

LANGUAGE: Java
CODE:
TOPOLOGY_SPOUT_WAIT_STRATEGY

----------------------------------------

TITLE: Configuring Worker-Level Metrics in YAML
DESCRIPTION: Example of how to configure worker-level metrics in the Storm YAML configuration file.

LANGUAGE: yaml
CODE:
worker.metrics: 
  metricA: "aaa.bbb.ccc.ddd.MetricA"
  metricB: "aaa.bbb.ccc.ddd.MetricB"
  ...

----------------------------------------

TITLE: Submitting Storm Topology
DESCRIPTION: Command to submit the sample topology to Storm using the configured Event Hubs spout

LANGUAGE: bash
CODE:
storm jar {jarfile} com.microsoft.eventhubs.samples.EventCount {topologyname} {spoutconffile}

----------------------------------------

TITLE: Storm SQL Error Log Filtering
DESCRIPTION: SQL script to filter error logs (status >= 400) from Apache logs and store them in a separate Kafka topic with transformed fields.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://apache-logs?bootstrap-servers=localhost:9092'
CREATE EXTERNAL TABLE APACHE_ERROR_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_ELAPSED_MS INT) LOCATION 'kafka://apache-error-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO APACHE_ERROR_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, (TIME_US / 1000) AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (CAST(STATUS AS INT) / 100) >= 4

----------------------------------------

TITLE: Implementing Double and Triple Bolt
DESCRIPTION: Example bolt implementation that takes input values and emits their double and triple values as output tuples.

LANGUAGE: java
CODE:
public class DoubleAndTripleBolt extends BaseRichBolt {
    private OutputCollectorBase _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollectorBase collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple input) {
        int val = input.getInteger(0);        
        _collector.emit(input, new Values(val*2, val*3));
        _collector.ack(input);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("double", "triple"));
    }    
}

----------------------------------------

TITLE: Configuring URL Expansion with Shuffle Grouping in Storm
DESCRIPTION: Example showing URL expansion bolt configuration using shuffle grouping for distributed processing of URLs.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .shuffleGrouping(1);

----------------------------------------

TITLE: Debugging Kryo Serialization Error in Java
DESCRIPTION: Stack trace showing a ConcurrentModificationException that occurs when mutable objects are emitted as output tuples in Storm. This demonstrates the serialization error that happens when bolts modify objects during network transmission.

LANGUAGE: java
CODE:
java.lang.RuntimeException: java.util.ConcurrentModificationException
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:84)
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:55)
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:56)
	at org.apache.storm.disruptor$consume_loop_STAR_$fn__1597.invoke(disruptor.clj:67)
	at org.apache.storm.util$async_loop$fn__465.invoke(util.clj:377)
	at clojure.lang.AFn.run(AFn.java:24)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.util.ConcurrentModificationException
	at java.util.LinkedHashMap$LinkedHashIterator.nextEntry(LinkedHashMap.java:390)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:409)
	at java.util.LinkedHashMap$EntryIterator.next(LinkedHashMap.java:408)
	at java.util.HashMap.writeObject(HashMap.java:1016)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:959)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1480)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at org.apache.storm.serialization.SerializableSerializer.write(SerializableSerializer.java:21)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:554)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:77)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:472)
	at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:27)

----------------------------------------

TITLE: Configuring Pacemaker Servers
DESCRIPTION: YAML configuration for specifying Pacemaker server hosts.

LANGUAGE: yaml
CODE:
pacemaker.servers:
    - somehost.mycompany.com
    - someotherhost.mycompany.com

----------------------------------------

TITLE: Configuring SSL/TLS for MQTT with Flux YAML
DESCRIPTION: Flux YAML configuration for setting up SSL/TLS connections for MQTT, including keystore and truststore configuration.

LANGUAGE: yaml
CODE:
name: "mqtt-topology"

components:
   ########## MQTT Spout Config ############
  - id: "mqtt-type"
    className: "org.apache.storm.mqtt.examples.CustomMessageMapper"

  - id: "keystore-loader"
    className: "org.apache.storm.mqtt.ssl.DefaultKeyStoreLoader"
    constructorArgs:
      - "keystore.jks"
      - "truststore.jks"
    properties:
      - name: "keyPassword"
        value: "password"
      - name: "keyStorePassword"
        value: "password"
      - name: "trustStorePassword"
        value: "password"

  - id: "mqtt-options"
    className: "org.apache.storm.mqtt.common.MqttOptions"
    properties:
      - name: "url"
        value: "ssl://raspberrypi.local:8883"
      - name: "topics"
        value:
          - "/users/tgoetz/#"

# topology configuration
config:
  topology.workers: 1
  topology.max.spout.pending: 1000

# spout definitions
spouts:
  - id: "mqtt-spout"
    className: "org.apache.storm.mqtt.spout.MqttSpout"
    constructorArgs:
      - ref: "mqtt-type"
      - ref: "mqtt-options"
      - ref: "keystore-loader"
    parallelism: 1

# bolt definitions
bolts:

  - id: "log"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1


streams:

  - from: "mqtt-spout"
    to: "log"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Configuring SSL/TLS for MQTT with Flux YAML
DESCRIPTION: Flux YAML configuration for setting up SSL/TLS connections for MQTT, including keystore and truststore configuration.

LANGUAGE: yaml
CODE:
name: "mqtt-topology"

components:
   ########## MQTT Spout Config ############
  - id: "mqtt-type"
    className: "org.apache.storm.mqtt.examples.CustomMessageMapper"

  - id: "keystore-loader"
    className: "org.apache.storm.mqtt.ssl.DefaultKeyStoreLoader"
    constructorArgs:
      - "keystore.jks"
      - "truststore.jks"
    properties:
      - name: "keyPassword"
        value: "password"
      - name: "keyStorePassword"
        value: "password"
      - name: "trustStorePassword"
        value: "password"

  - id: "mqtt-options"
    className: "org.apache.storm.mqtt.common.MqttOptions"
    properties:
      - name: "url"
        value: "ssl://raspberrypi.local:8883"
      - name: "topics"
        value:
          - "/users/tgoetz/#"

# topology configuration
config:
  topology.workers: 1
  topology.max.spout.pending: 1000

# spout definitions
spouts:
  - id: "mqtt-spout"
    className: "org.apache.storm.mqtt.spout.MqttSpout"
    constructorArgs:
      - ref: "mqtt-type"
      - ref: "mqtt-options"
      - ref: "keystore-loader"
    parallelism: 1

# bolt definitions
bolts:

  - id: "log"
    className: "org.apache.storm.flux.wrappers.bolts.LogInfoBolt"
    parallelism: 1


streams:

  - from: "mqtt-spout"
    to: "log"
    grouping:
      type: SHUFFLE

----------------------------------------

TITLE: Configuring DRPC Servers in YAML
DESCRIPTION: This YAML configuration specifies the locations of DRPC servers, the HTTP port, and the Thrift transport plugin to use for DRPC communication.

LANGUAGE: yaml
CODE:
drpc.servers:
  - "drpc1.foo.com"
  - "drpc2.foo.com"
drpc.http.port: 8081
storm.thrift.transport: "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin"

----------------------------------------

TITLE: jQuery UI MIT License Text
DESCRIPTION: The full MIT license text for jQuery UI, including copyright notice, permissions, warranty disclaimer, and special provisions for sample code and external libraries.

LANGUAGE: text
CODE:
Copyright jQuery Foundation and other contributors, https://jquery.org/

This software consists of voluntary contributions made by many
individuals. For exact contribution history, see the revision history
available at https://github.com/jquery/jquery-ui

The following license applies to all parts of this software except as
documented below:

====

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

====

Copyright and related rights for sample code are waived via CC0. Sample
code is defined as all source code contained within the demos directory.

CC0: http://creativecommons.org/publicdomain/zero/1.0/

====

All files located in the node_modules and external directories are
externally maintained libraries used by this software which have their
own licenses; we recommend you read them, as their terms may differ from
the terms above.

----------------------------------------

TITLE: Defining Shell Bolt in Clojure
DESCRIPTION: Example of defining a shell bolt that runs a Python script, showing input declaration and output field specification.

LANGUAGE: clojure
CODE:
(shell-bolt-spec {"1" :shuffle "2" ["id"]}
                 "python"
                 "mybolt.py"
                 ["outfield1" "outfield2"]
                 :p 25)

----------------------------------------

TITLE: Storm Tuple Transfer Function
DESCRIPTION: Implementation of worker's transfer function that serializes tuples and places them on a transfer queue for processing.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/daemon/worker.clj#L56

----------------------------------------

TITLE: Storm Shell Command Usage Example
DESCRIPTION: Example command for using storm shell to package and submit a Python topology with resources.

LANGUAGE: bash
CODE:
storm shell resources/ python3 topology.py arg1 arg2

----------------------------------------

TITLE: Deploying Storm Topology
DESCRIPTION: Command to submit Storm topology using the Event Hubs spout

LANGUAGE: bash
CODE:
storm jar {jarfile} com.microsoft.eventhubs.samples.EventCount {topologyname} {spoutconffile}

----------------------------------------

TITLE: Invalid Stream Reference Example
DESCRIPTION: Example showing incorrect forward referencing of streams in JoinBolt configuration.

LANGUAGE: java
CODE:
new JoinBolt( "spout1", "key1")                 
  .join     ( "spout2", "userId",  "spout3") //not allowed. spout3 not yet introduced
  .join     ( "spout3", "key3",    "spout1")

----------------------------------------

TITLE: Defining Shell Bolt in Clojure
DESCRIPTION: Example of defining a shell bolt that runs a Python script, showing input declarations and output field specifications.

LANGUAGE: clojure
CODE:
(shell-bolt-spec {"1" :shuffle "2" ["id"]}
                 "python3"
                 "mybolt.py"
                 ["outfield1" "outfield2"]
                 :p 25)

----------------------------------------

TITLE: HTML Meta Redirect to Storm Hooks Documentation
DESCRIPTION: HTML meta tags that perform an immediate redirect to the current Storm documentation page for Hooks and establish canonical URL reference.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Hooks.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Hooks.html" />

----------------------------------------

TITLE: Accessing Shared Resources in Storm Components
DESCRIPTION: Storm provides multiple levels of resource sharing: task level, executor level, and worker level. Each level has specific access methods for reading and writing data.

LANGUAGE: java
CODE:
// Task level data access
TopologyContext.setTaskData(String, Object)    // write
TopologyContext.getTask(String)               // read

// Executor level data access
TopologyContext.setExecutorData(String, Object) // write
TopologyContext.getExecutorData(String)        // read

// Worker level data access
WorkerUserContext.setResource(String, Object)   // write
TopologyContext.getResource(String)            // read

----------------------------------------

TITLE: Configuring Maven Dependency for Storm Redis
DESCRIPTION: Maven dependency configuration required to use storm-redis in a Storm project.

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>storm-redis</artifactId>
    <version>${storm.version}</version>
    <type>jar</type>
</dependency>

----------------------------------------

TITLE: Implementing MongoDB Update Mapper
DESCRIPTION: Implementation for updating MongoDB documents using $set operator

LANGUAGE: java
CODE:
public class SimpleMongoUpdateMapper extends SimpleMongoMapper implements MongoUpdateMapper {

    private String[] fields;

    @Override
    public Document toDocument(ITuple tuple) {
        Document document = new Document();
        for(String field : fields){
            document.append(field, tuple.getValueByField(field));
        }
        return new Document("$set", document);
    }

    public SimpleMongoUpdateMapper withFields(String... fields) {
        this.fields = fields;
        return this;
    }
}

----------------------------------------

TITLE: Submitting Storm Topology with Configuration
DESCRIPTION: Example showing how to configure and submit a Storm topology using StormSubmitter. Sets the number of workers and maximum pending spouts before submitting the topology to the cluster.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(20);
conf.setMaxSpoutPending(5000);
StormSubmitter.submitTopology("mytopology", conf, topology);

----------------------------------------

TITLE: Updating Blob ACLs in Java
DESCRIPTION: Updates the Access Control Lists (ACLs) for an existing blob in Storm's blobstore

LANGUAGE: java
CODE:
String blobKey = "some_key";
AccessControl updateAcl = BlobStoreAclHandler.parseAccessControl("u:USER:--a");
List<AccessControl> updateAcls = new LinkedList<AccessControl>();
updateAcls.add(updateAcl);
SettableBlobMeta modifiedSettableBlobMeta = new SettableBlobMeta(updateAcls);
clientBlobStore.setBlobMeta(blobKey, modifiedSettableBlobMeta);

//Now set write only
updateAcl = BlobStoreAclHandler.parseAccessControl("u:USER:-w-");
updateAcls = new LinkedList<AccessControl>();
updateAcls.add(updateAcl);
modifiedSettableBlobMeta = new SettableBlobMeta(updateAcls);
clientBlobStore.setBlobMeta(blobKey, modifiedSettableBlobMeta);

----------------------------------------

TITLE: Defining External Tables in Storm SQL
DESCRIPTION: Shows how to specify external data sources using the CREATE EXTERNAL TABLE statement, which follows Hive DDL syntax.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE table_name field_list
    [ STORED AS
      INPUTFORMAT input_format_classname
      OUTPUTFORMAT output_format_classname
    ]
    LOCATION location
    [ TBLPROPERTIES tbl_properties ]
    [ AS select_stmt ]

----------------------------------------

TITLE: Configuring Node Resources in YAML
DESCRIPTION: YAML configuration to specify available memory and CPU resources on a node.

LANGUAGE: yaml
CODE:
supervisor.memory.capacity.mb: [amount<Double>]
supervisor.cpu.capacity: [amount<Double>]

----------------------------------------

TITLE: Configuring Bolt with Shuffle Grouping in Storm Topology
DESCRIPTION: This snippet demonstrates how to set up a bolt named 'expand' with shuffle grouping in a Storm topology. It uses the ExpandUrl class and specifies a parallelism hint.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .shuffleGrouping(1);

----------------------------------------

TITLE: YAML Frontmatter for Dynamic Worker Profiling Documentation
DESCRIPTION: YAML frontmatter defining metadata for the Dynamic Worker Profiling documentation page, including title, layout, and documentation flag.

LANGUAGE: yaml
CODE:
---
title: Dynamic Worker Profiling
layout: documentation
documentation: true
---

----------------------------------------

TITLE: Retrieving Topology History in JSON
DESCRIPTION: Sample response from the /api/v1/history/summary endpoint, showing IDs of running topologies.

LANGUAGE: json
CODE:
{
  "topo-history":[
    "wc6-1-1446571009",
    "wc8-2-1446587178"
  ]
}

----------------------------------------

TITLE: Defining TupleToMessageMapper Interface in Java for Storm-RocketMQ
DESCRIPTION: This code defines the TupleToMessageMapper interface, which is crucial for mapping Storm tuples to RocketMQ messages. It includes methods for extracting key and value from a tuple.

LANGUAGE: java
CODE:
public interface TupleToMessageMapper extends Serializable {
    String getKeyFromTuple(ITuple tuple);
    byte[] getValueFromTuple(ITuple tuple);
}

----------------------------------------

TITLE: Configuring HBase State Provider in Java
DESCRIPTION: Example of initializing HBase state provider configuration in Java code, including HBase configuration and state provider settings.

LANGUAGE: java
CODE:
Config conf = new Config();
    Map<String, Object> hbConf = new HashMap<String, Object>();
    hbConf.put("hbase.rootdir", "file:///tmp/hbase");
    conf.put("hbase.conf", hbConf);
    conf.put("topology.state.provider",  "org.apache.storm.hbase.state.HBaseKeyValueStateProvider");
    conf.put("topology.state.provider.config", "{" +
            "   \"hbaseConfigKey\": \"hbase.conf\"," +
            "   \"tableName\": \"state\"," +
            "   \"columnFamily\": \"cf\"" +
            " }");

----------------------------------------

TITLE: Using Project Operation in Trident
DESCRIPTION: Example of using project to keep only specific fields in a stream.

LANGUAGE: java
CODE:
mystream.project(new Fields("b", "d"))

----------------------------------------

TITLE: Running MQTT Broker and Publisher - Bash
DESCRIPTION: Command to start an MQTT broker on port 1883 and initiate a publisher for temperature/humidity data

LANGUAGE: bash
CODE:
java -cp examples/target/storm-mqtt-examples-*-SNAPSHOT.jar org.apache.storm.mqtt.examples.MqttBrokerPublisher

----------------------------------------

TITLE: Emitting Tuple in JSON Format for Storm Multi-Language Protocol
DESCRIPTION: This JSON structure represents an emit command sent from a multi-language component in Storm. It includes the command type, anchor tuple ids, stream id, target task (for direct emit), and the tuple values to be emitted.

LANGUAGE: json
CODE:
{
    "command": "emit",
    "anchors": [1231231, -234234234],
    "stream": 1,
    "task": 9,
    "tuple": ["field1", 2, 3]
}

----------------------------------------

TITLE: Filtering Kafka Stream with Storm SQL
DESCRIPTION: SQL statements to filter orders from a Kafka stream based on transaction value and insert significant orders into another Kafka stream.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://orders?bootstrap-servers=localhost:9092,localhost:9093'
CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) LOCATION 'kafka://large_orders?bootstrap-servers=localhost:9092,localhost:9093' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Filtering Kafka Stream with Storm SQL
DESCRIPTION: SQL statements to filter orders from a Kafka stream based on transaction value and insert significant orders into another Kafka stream.

LANGUAGE: sql
CODE:
CREATE EXTERNAL TABLE ORDERS (ID INT PRIMARY KEY, UNIT_PRICE INT, QUANTITY INT) LOCATION 'kafka://orders?bootstrap-servers=localhost:9092,localhost:9093'
CREATE EXTERNAL TABLE LARGE_ORDERS (ID INT PRIMARY KEY, TOTAL INT) LOCATION 'kafka://large_orders?bootstrap-servers=localhost:9092,localhost:9093' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO LARGE_ORDERS SELECT ID, UNIT_PRICE * QUANTITY AS TOTAL FROM ORDERS WHERE UNIT_PRICE * QUANTITY > 50

----------------------------------------

TITLE: Configuring Zookeeper Servers in Storm YAML
DESCRIPTION: This snippet shows how to specify the Zookeeper servers for a Storm cluster in the storm.yaml configuration file. It lists the IP addresses of the Zookeeper hosts.

LANGUAGE: yaml
CODE:
storm.zookeeper.servers:
  - "111.222.333.444"
  - "555.666.777.888"

----------------------------------------

TITLE: Implementing a Simple Bolt in Clojure
DESCRIPTION: Example of a simple bolt implementation using defbolt to split sentences into words.

LANGUAGE: clojure
CODE:
(defbolt split-sentence ["word"] [tuple collector]
  (let [words (.split (.getString tuple 0) " ")]
    (doseq [w words]
      (emit-bolt! collector [w] :anchor tuple))
    (ack! collector tuple)
    ))

----------------------------------------

TITLE: Basic DRPC Client Configuration in Java
DESCRIPTION: Example of configuring and using a DRPC client to make remote procedure calls. Shows both manual configuration and using a preconfigured client.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.put("storm.thrift.transport", "org.apache.storm.security.auth.plain.PlainSaslTransportPlugin");
conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 3);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL, 10);
conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING, 20);
DRPCClient client = new DRPCClient(conf, "drpc-host", 3772);
String result = client.execute("reach", "http://twitter.com");

----------------------------------------

TITLE: Executor Data Access Methods
DESCRIPTION: Methods for reading and writing executor-level shared data in Storm components.

LANGUAGE: java
CODE:
TopologyContext#setExecutorData
TopologyContext#getExecutorData(String)

----------------------------------------

TITLE: Implementing a Simple Bolt in Clojure
DESCRIPTION: Example of a simple bolt implementation using defbolt to split sentences into words.

LANGUAGE: clojure
CODE:
(defbolt split-sentence ["word"] [tuple collector]
  (let [words (.split (.getString tuple 0) " ")]
    (doseq [w words]
      (emit-bolt! collector [w] :anchor tuple))
    (ack! collector tuple)
    ))

----------------------------------------

TITLE: Storm RotatingMap Implementation
DESCRIPTION: The RotatingMap is a specialized HashMap implementation used to efficiently manage time-based expiration of tuples. It maintains multiple buckets of records that advance towards expiration when rotated, providing O(1) access time. The map automatically moves entries to the newest bucket on updates, effectively resetting their expiration timer.



----------------------------------------

TITLE: Retrieving Topology Component Details in Storm UI REST API
DESCRIPTION: GET request to retrieve detailed metrics and executor information for a specific component in a topology.

LANGUAGE: json
CODE:
{
  "name": "WordCount3",
  "id": "spout",
  "componentType": "spout",
  "windowHint": "10m 0s",
  "executors": 5,
  "componentErrors":[
    {
      "errorTime": 1406006074000,
      "errorHost": "10.11.1.70",
      "errorPort": 6701,
      "errorWorkerLogLink": "http://10.11.1.7:8000/log?file=worker-6701.log",
      "errorLapsedSecs": 16,
      "error": "java.lang.RuntimeException: java.lang.StringIndexOutOfBoundsException: Some Error\n\tat org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:128)\n\tat org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99)\n\tat org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80)\n\tat backtype...more.."
    }
  ],
  "topologyId": "WordCount3-1-1402960825",
  "tasks": 5,
  "window": "600",
  "profilerActive": [
    {
      "host": "10.11.1.70",
      "port": "6701",
      "dumplink":"http:\/\/10.11.1.70:8000\/dumps\/ex-1-1452718803\/10.11.1.70%3A6701",
      "timestamp":"576328"
    }
  ],
  "profilingAndDebuggingCapable": true,
  "profileActionEnabled": true,
  "spoutSummary": [
    {
      "windowPretty": "10m 0s",
      "window": "600",
      "emitted": 28500,
      "transferred": 28460,
      "completeLatency": "0.000",
      "acked": 0,
      "failed": 0
    }
  ],
  "outputStats": [
    {
      "stream": "__metrics",
      "emitted": 40,
      "transferred": 0,
      "completeLatency": "0",
      "acked": 0,
      "failed": 0
    }
  ],
  "executorStats": [
    {
      "workerLogLink": "http://10.11.1.7:8000/log?file=worker-6701.log",
      "emitted": 5720,
      "port": 6701,
      "completeLatency": "0.000",
      "transferred": 5720,
      "host": "10.11.1.7",
      "acked": 0,
      "uptime": "43m 4s",
      "uptimeSeconds": 2584,
      "id": "[24-24]",
      "failed": 0
    }
  ]
}

----------------------------------------

TITLE: Configuring Event Loggers in YAML for Apache Storm
DESCRIPTION: This YAML configuration snippet demonstrates how to register multiple event loggers in the storm.yaml config file, including custom event loggers with additional arguments.

LANGUAGE: yaml
CODE:
topology.event.logger.register:
  - class: "org.apache.storm.metric.FileBasedEventLogger"
  - class: "org.mycompany.MyEventLogger"
    arguments:
      endpoint: "event-logger.mycompany.org"

----------------------------------------

TITLE: Artifactory Configuration Example
DESCRIPTION: Example configuration for loading from Artifactory server with timeout setting.

LANGUAGE: yaml
CODE:
scheduler.config.loader.uri: "artifactory+http://artifactory.my.company.com:8000/artifactory/configurations/clusters/my_cluster/ras_pools"
scheduler.config.loader.timeout.sec: 30

----------------------------------------

TITLE: Storm Log Command JSON Format
DESCRIPTION: JSON structure for logging messages from a shell process to Storm worker log. Includes command type and message to log.

LANGUAGE: json
CODE:
{
    "command": "log",
    "msg": "hello world!"
}

----------------------------------------

TITLE: Configuring Isolation Scheduler Machines in Storm YAML
DESCRIPTION: YAML configuration example showing how to allocate dedicated machines to specific topologies using the isolation scheduler. Maps topology names to their allocated machine count for resource isolation.

LANGUAGE: yaml
CODE:
isolation.scheduler.machines: 
    "my-topology": 8
    "tiny-topology": 1
    "some-other-topology": 3

----------------------------------------

TITLE: Redirecting to Storm Cluster Setup Documentation in HTML
DESCRIPTION: This HTML snippet uses a meta refresh tag to redirect the user to the current Storm cluster setup documentation page. It also includes a canonical link for search engines.

LANGUAGE: HTML
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Setting-up-a-Storm-cluster.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Setting-up-a-Storm-cluster.html" />

----------------------------------------

TITLE: Parsing Apache Logs to JSON
DESCRIPTION: Python script to parse Apache log lines into JSON format, adding an auto-incrementing ID field.

LANGUAGE: python
CODE:
import sys
import apache_log_parser
import json

auto_incr_id = 1
parser_format = '%a - - %t %D "%r" %s %b "%{Referer}i" "%{User-Agent}i"'
line_parser = apache_log_parser.make_parser(parser_format)
while True:
  # we'll use pipe
  line = sys.stdin.readline()
  if not line:
    break
  parsed_dict = line_parser(line)
  parsed_dict['id'] = auto_incr_id
  auto_incr_id += 1

  parsed_dict = {k.upper(): v for k, v in parsed_dict.iteritems() if not k.endswith('datetimeobj')}
  print(json.dumps(parsed_dict))

----------------------------------------

TITLE: Configuring URL Expansion with Fields Grouping in Storm
DESCRIPTION: Optimized configuration of a Storm bolt for URL expansion using fields grouping to ensure the same URLs are processed by the same task, improving cache efficiency.

LANGUAGE: java
CODE:
builder.setBolt("expand", new ExpandUrl(), parallelism)
  .fieldsGrouping("urls", new Fields("url"));

----------------------------------------

TITLE: Dequeueing Items from Kestrel Queue in Java
DESCRIPTION: Method to dequeue items from a Kestrel queue without removing them. It uses KestrelClient to fetch items and print their contents.

LANGUAGE: java
CODE:
private static void dequeueItems(KestrelClient kestrelClient, String queueName) throws IOException, ParseError
			 {
		for(int i=1; i<=12; i++){

			Item item = kestrelClient.dequeue(queueName);

			if(item==null){
				System.out.println("The queue (" + queueName + ") contains no items.");
			}
			else
			{
				byte[] data = item._data;

				String receivedVal = new String(data);

				System.out.println("receivedItem=" + receivedVal);
			}
		}

----------------------------------------

TITLE: Submitting a Topology with Blob Mapping
DESCRIPTION: Example of submitting a Storm topology with blob mapping configuration. This command submits a topology and specifies two blob keys with their local file mappings and compression settings.

LANGUAGE: bash
CODE:
storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar org.apache.storm.starter.clj.word_count test_topo -c topology.blobstore.map='{"key1":{"localname":"blob_file", "uncompress":false, "workerRestart":true},"key2":{}}'

----------------------------------------

TITLE: Word Count Example using Stream API in Java
DESCRIPTION: This example demonstrates a complete word count topology using the Stream API, including windowing, transformations, and aggregations.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

builder
   // A stream of random sentences with two partitions
   .newStream(new RandomSentenceSpout(), new ValueMapper<String>(0), 2)
   // a two seconds tumbling window
   .window(TumblingWindows.of(Duration.seconds(2)))
   // split the sentences to words
   .flatMap(s -> Arrays.asList(s.split(" ")))
   // create a stream of (word, 1) pairs
   .mapToPair(w -> Pair.of(w, 1))
   // compute the word counts in the last two second window
   .countByKey()
   // print the results to stdout
   .print();

----------------------------------------

TITLE: Word Count Example using Stream API in Java
DESCRIPTION: This example demonstrates a complete word count topology using the Stream API, including windowing, transformations, and aggregations.

LANGUAGE: java
CODE:
StreamBuilder builder = new StreamBuilder();

builder
   // A stream of random sentences with two partitions
   .newStream(new RandomSentenceSpout(), new ValueMapper<String>(0), 2)
   // a two seconds tumbling window
   .window(TumblingWindows.of(Duration.seconds(2)))
   // split the sentences to words
   .flatMap(s -> Arrays.asList(s.split(" ")))
   // create a stream of (word, 1) pairs
   .mapToPair(w -> Pair.of(w, 1))
   // compute the word counts in the last two second window
   .countByKey()
   // print the results to stdout
   .print();

----------------------------------------

TITLE: Killing Topology in JSON
DESCRIPTION: Sample response from the /api/v1/topology/<id>/kill/<wait-time> POST endpoint, showing the result of killing a topology.

LANGUAGE: json
CODE:
{
  "topologyOperation":"kill",
  "topologyId":"wordcount-1-1420308665",
  "status":"success"
}

----------------------------------------

TITLE: HTML Meta Refresh Redirect to Storm Documentation
DESCRIPTION: HTML meta tags that implement an immediate page redirect to the Trident spouts documentation page and specify the canonical URL for search engines.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Trident-spouts.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Trident-spouts.html" />

----------------------------------------

TITLE: Running Storm PMML Examples from Command Line
DESCRIPTION: This snippet demonstrates the command to run the bundled PMML examples in Storm. It specifies the Storm jar, the main class to run, and required arguments including the topology name, PMML model file, and input data file.

LANGUAGE: java
CODE:
STORM-HOME/bin/storm jar STORM-HOME/examples/storm-pmml-examples/storm-pmml-examples-2.0.0-SNAPSHOT.jar 
org.apache.storm.pmml.JpmmlRunnerTestTopology jpmmlTopology PMMLModel.xml RawInputData.csv

----------------------------------------

TITLE: Implementing a Spout in Java
DESCRIPTION: Example of a simple spout that emits random words.

LANGUAGE: java
CODE:
public void nextTuple() {
    Utils.sleep(100);
    final String[] words = new String[] {"nathan", "mike", "jackson", "golda", "bertels"};
    final Random rand = new Random();
    final String word = words[rand.nextInt(words.length)];
    _collector.emit(new Values(word));
}

----------------------------------------

TITLE: Implementing Reach Calculation DRPC Topology in Trident
DESCRIPTION: Defines a Trident topology for computing the reach of a URL on demand, utilizing two external state sources and complex aggregations.

LANGUAGE: java
CODE:
TridentState urlToTweeters =
       topology.newStaticState(getUrlToTweetersState());
TridentState tweetersToFollowers =
       topology.newStaticState(getTweeterToFollowersState());

topology.newDRPCStream("reach")
       .stateQuery(urlToTweeters, new Fields("args"), new MapGet(), new Fields("tweeters"))
       .each(new Fields("tweeters"), new ExpandList(), new Fields("tweeter"))
       .shuffle()
       .stateQuery(tweetersToFollowers, new Fields("tweeter"), new MapGet(), new Fields("followers"))
       .parallelismHint(200)
       .each(new Fields("followers"), new ExpandList(), new Fields("follower"))
       .groupBy(new Fields("follower"))
       .aggregate(new One(), new Fields("one"))
       .parallelismHint(20)
       .aggregate(new Count(), new Fields("reach"));

----------------------------------------

TITLE: Queueing Sentences to Kestrel
DESCRIPTION: Method to add random sentences to a Kestrel queue. Takes a KestrelClient and queue name as parameters, selecting sentences from a predefined array and adding them with sequential IDs.

LANGUAGE: java
CODE:
    private static void queueSentenceItems(KestrelClient kestrelClient, String queueName)
			throws ParseError, IOException {

		String[] sentences = new String[] {
	            "the cow jumped over the moon",
	            "an apple a day keeps the doctor away",
	            "four score and seven years ago",
	            "snow white and the seven dwarfs",
	            "i am at two with nature"};

		Random _rand = new Random();

		for(int i=1; i<=10; i++){

			String sentence = sentences[_rand.nextInt(sentences.length)];

			String val = "ID " + i + " " + sentence;

			boolean queueSucess = kestrelClient.queue(queueName, val);

			System.out.println("queueSucess=" +queueSucess+ " [" + val +"]");
		}
	}

----------------------------------------

TITLE: Installing JZMQ from a Specific Fork for Apache Storm
DESCRIPTION: This snippet shows the process of cloning a specific JZMQ fork, configuring, and installing it. This fork is tested to work with Storm and is recommended to prevent regressions. Root privileges are required for the final installation step.

LANGUAGE: bash
CODE:
#install jzmq
git clone https://github.com/nathanmarz/jzmq.git
cd jzmq
./autogen.sh
./configure
make
sudo make install

----------------------------------------

TITLE: Implementing Reach Calculation DRPC Topology in Trident
DESCRIPTION: Defines a Trident topology for computing the reach of a URL on demand, utilizing two external state sources and complex aggregations.

LANGUAGE: java
CODE:
TridentState urlToTweeters =
       topology.newStaticState(getUrlToTweetersState());
TridentState tweetersToFollowers =
       topology.newStaticState(getTweeterToFollowersState());

topology.newDRPCStream("reach")
       .stateQuery(urlToTweeters, new Fields("args"), new MapGet(), new Fields("tweeters"))
       .each(new Fields("tweeters"), new ExpandList(), new Fields("tweeter"))
       .shuffle()
       .stateQuery(tweetersToFollowers, new Fields("tweeter"), new MapGet(), new Fields("followers"))
       .parallelismHint(200)
       .each(new Fields("followers"), new ExpandList(), new Fields("follower"))
       .groupBy(new Fields("follower"))
       .aggregate(new One(), new Fields("one"))
       .parallelismHint(20)
       .aggregate(new Count(), new Fields("reach"));

----------------------------------------

TITLE: Sample Druid Beam Factory Implementation
DESCRIPTION: Detailed implementation of a DruidBeamFactory showing configuration of Druid connection parameters, dimensions, aggregators, and beam settings using Tranquility.

LANGUAGE: java
CODE:
public class SampleDruidBeamFactoryImpl implements DruidBeamFactory<Map<String, Object>> {

    @Override
    public Beam<Map<String, Object>> makeBeam(Map<?, ?> conf, IMetricsContext metrics) {


        final String indexService = "druid/overlord"; // The druid.service name of the indexing service Overlord node.
        final String discoveryPath = "/druid/discovery"; // Curator service discovery path. config: druid.discovery.curator.path
        final String dataSource = "test"; //The name of the ingested datasource. Datasources can be thought of as tables.
        final List<String> dimensions = ImmutableList.of("publisher", "advertiser");
        List<AggregatorFactory> aggregators = ImmutableList.<AggregatorFactory>of(
                new CountAggregatorFactory(
                        "click"
                )
        );
        // Tranquility needs to be able to extract timestamps from your object type (in this case, Map<String, Object>).
        final Timestamper<Map<String, Object>> timestamper = new Timestamper<Map<String, Object>>()
        {
            @Override
            public DateTime timestamp(Map<String, Object> theMap)
            {
                return new DateTime(theMap.get("timestamp"));
            }
        };

        // Tranquility uses ZooKeeper (through Curator) for coordination.
        final CuratorFramework curator = CuratorFrameworkFactory
                .builder()
                .connectString((String)conf.get("druid.tranquility.zk.connect")) //take config from storm conf
                .retryPolicy(new ExponentialBackoffRetry(1000, 20, 30000))
                .build();
        curator.start();

        // The JSON serialization of your object must have a timestamp field in a format that Druid understands. By default,
        // Druid expects the field to be called "timestamp" and to be an ISO8601 timestamp.
        final TimestampSpec timestampSpec = new TimestampSpec("timestamp", "auto", null);

        // Tranquility needs to be able to serialize your object type to JSON for transmission to Druid. By default this is
        // done with Jackson. If you want to provide an alternate serializer, you can provide your own via ```.objectWriter(...)```.
        // In this case, we won't provide one, so we're just using Jackson.
        final Beam<Map<String, Object>> beam = DruidBeams
                .builder(timestamper)
                .curator(curator)
                .discoveryPath(discoveryPath)
                .location(DruidLocation.create(indexService, dataSource))
                .timestampSpec(timestampSpec)
                .rollup(DruidRollup.create(DruidDimensions.specific(dimensions), aggregators, QueryGranularities.MINUTE))
                .tuning(
                        ClusteredBeamTuning
                                .builder()
                                .segmentGranularity(Granularity.HOUR)
                                .windowPeriod(new Period("PT10M"))
                                .partitions(1)
                                .replicants(1)
                                .build()
                )
                .druidBeamConfig(
                      DruidBeamConfig
                           .builder()
                           .indexRetryPeriod(new Period("PT10M"))
                           .build())
                .buildBeam();

        return beam;
    }
}

----------------------------------------

TITLE: Initializing HBase State Provider in Java
DESCRIPTION: This Java code snippet demonstrates how to initialize an HBase state provider programmatically. It sets up the necessary configuration options and state provider details.

LANGUAGE: java
CODE:
Config conf = new Config();
    Map<String, Object> hbConf = new HashMap<String, Object>();
    hbConf.put("hbase.rootdir", "file:///tmp/hbase");
    conf.put("hbase.conf", hbConf);
    conf.put("topology.state.provider",  "org.apache.storm.hbase.state.HBaseKeyValueStateProvider");
    conf.put("topology.state.provider.config", "{" +
            "   \"hbaseConfigKey\": \"hbase.conf\"," +
            "   \"tableName\": \"state\"," +
            "   \"columnFamily\": \"cf\"" +
            " }");

----------------------------------------

TITLE: Configuring Maven Shade Plugin for Uber Jar in XML
DESCRIPTION: XML configuration for the Maven Shade Plugin to create an uber jar containing all dependencies for the Storm Solr integration project.

LANGUAGE: xml
CODE:
<plugin>
     <groupId>org.apache.maven.plugins</groupId>
     <artifactId>maven-shade-plugin</artifactId>
     <version>2.4.1</version>
     <executions>
         <execution>
             <phase>package</phase>
             <goals>
                 <goal>shade</goal>
             </goals>
             <configuration>
                 <transformers>
                     <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                         <mainClass>org.apache.storm.solr.topology.SolrJsonTopology</mainClass>
                     </transformer>
                 </transformers>
             </configuration>
         </execution>
     </executions>
</plugin>

----------------------------------------

TITLE: Apache Storm 1.0.1 Changelog
DESCRIPTION: Comprehensive list of bug fixes, improvements and changes included in the Apache Storm 1.0.1 release. Key updates include performance improvements to internal messaging, spout data emission rates, and inter-task communication enhancements.

LANGUAGE: text
CODE:
STORM-1741: remove unconditional setting of JAVA_HOME from storm-env.sh
STORM-1739: update the minor JAVA version dependency in 0.10.0 and above
STORM-1727: document 1.0 package renaming and how to use the migration tool
STORM-1733: Flush stdout and stderr before calling "os.execvp" to prevent log loss.
STORM-1729: Get rid of reflections while recording stats
STORM-1731: Avoid looking up debug / backpressure enable flags within critical path
STORM-1535: Make sure hdfs key tab login happens only once for multiple bolts/executors.
STORM-1725: Kafka Spout New Consumer API - KafkaSpoutRetryExponential Backoff method should use HashMap instead of TreeMap not to throw Exception
STORM-1544: Document Debug/Sampling of Topologies
STORM-1679: add storm Scheduler documents
STORM-1704: When logviewer_search.html opens daemon file, next search always show no result
STORM-1714: StatefulBolts ends up as normal bolts while using TopologyBuilder.setBolt without parallelism
STORM-1683: only check non-system streams by default
STORM-1680: Provide configuration to set min fetch size in KafkaSpout
STORM-1649: Optimize Kryo instaces creation in HBaseWindowsStore
STORM-1696: status not sync if zk fails in backpressure
STORM-1693: Move stats cleanup to executor shutdown
STORM-1585: Add DDL support for UDFs in storm-sql
STORM-1681: Bug in scheduling cyclic topologies when scheduling with RAS
STORM-1706: Add RELEASE and storm-env.sh to storm-diet assembly
STORM-1613: Upgraded HBase version to 1.1.0
STORM-1687: divide by zero in stats

----------------------------------------

TITLE: Implementing Storm Bolt with Output Fields
DESCRIPTION: Example of a Storm bolt that processes input tuples and emits double and triple values with declared output fields.

LANGUAGE: java
CODE:
public class DoubleAndTripleBolt extends BaseRichBolt {
    private OutputCollectorBase _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollectorBase collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple input) {
        int val = input.getInteger(0);        
        _collector.emit(input, new Values(val*2, val*3));
        _collector.ack(input);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("double", "triple"));
    }    
}

----------------------------------------

TITLE: Implementing Storm Bolt with Output Fields
DESCRIPTION: Example of a Storm bolt that processes input tuples and emits double and triple values with declared output fields.

LANGUAGE: java
CODE:
public class DoubleAndTripleBolt extends BaseRichBolt {
    private OutputCollectorBase _collector;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollectorBase collector) {
        _collector = collector;
    }

    @Override
    public void execute(Tuple input) {
        int val = input.getInteger(0);        
        _collector.emit(input, new Values(val*2, val*3));
        _collector.ack(input);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("double", "triple"));
    }    
}

----------------------------------------

TITLE: Displaying jQuery and Sizzle.js License Text
DESCRIPTION: This snippet contains the full text of the MIT license for jQuery v3.6.1 and Sizzle.js. It outlines the permissions granted to users, including rights to use, modify, and distribute the software, as well as the disclaimer of warranty and limitation of liability.

LANGUAGE: plaintext
CODE:
jQuery v 3.6.1
Copyright OpenJS Foundation and other contributors, https://openjsf.org/

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

******************************************

The jQuery JavaScript Library v3.6.1 also includes Sizzle.js

Sizzle.js includes the following license:

Copyright JS Foundation and other contributors, https://js.foundation/

This software consists of voluntary contributions made by many
individuals. For exact contribution history, see the revision history
available at https://github.com/jquery/sizzle

The following license applies to all parts of this software except as
documented below:

====

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

====

All files located in the node_modules and external directories are
externally maintained libraries used by this software which have their
own licenses; we recommend you read them, as their terms may differ from
the terms above.

----------------------------------------

TITLE: Embedding Storm Logo Images in Markdown
DESCRIPTION: These snippets embed multiple images of the Storm logo designs using Markdown image syntax. They reference local image files stored in the project's image directory.

LANGUAGE: markdown
CODE:
![Storm Brand](/images/logocontest/abartos/storm_logo.png)

![Storm Brand](/images/logocontest/abartos/storm_logo2.png)

![Storm Brand](/images/logocontest/abartos/storm_logo3.png)

![Storm Brand](/images/logocontest/abartos/stationery_mockup.jpg)

----------------------------------------

TITLE: Configuring HBase State Provider in JSON
DESCRIPTION: This JSON configuration snippet shows how to set up an HBase state provider for Storm. It includes options for key and value classes, serializers, and HBase-specific configuration.

LANGUAGE: json
CODE:
{
  "keyClass": "Optional fully qualified class name of the Key type.",
  "valueClass": "Optional fully qualified class name of the Value type.",
  "keySerializerClass": "Optional Key serializer implementation class.",
  "valueSerializerClass": "Optional Value Serializer implementation class.",
  "hbaseConfigKey": "config key to load hbase configuration from storm root configuration. (similar to storm-hbase)",
  "tableName": "Pre-created table name for state.",
  "columnFamily": "Pre-created column family for state."
}

----------------------------------------

TITLE: Displaying jQuery and Sizzle.js License Text
DESCRIPTION: This snippet contains the full text of the MIT license for jQuery v3.6.1 and Sizzle.js. It outlines the permissions granted to users, including rights to use, modify, and distribute the software, as well as the disclaimer of warranty and limitation of liability.

LANGUAGE: plaintext
CODE:
jQuery v 3.6.1
Copyright OpenJS Foundation and other contributors, https://openjsf.org/

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

******************************************

The jQuery JavaScript Library v3.6.1 also includes Sizzle.js

Sizzle.js includes the following license:

Copyright JS Foundation and other contributors, https://js.foundation/

This software consists of voluntary contributions made by many
individuals. For exact contribution history, see the revision history
available at https://github.com/jquery/sizzle

The following license applies to all parts of this software except as
documented below:

====

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

====

All files located in the node_modules and external directories are
externally maintained libraries used by this software which have their
own licenses; we recommend you read them, as their terms may differ from
the terms above.

----------------------------------------

TITLE: Storm Component Common Configuration
DESCRIPTION: The ComponentCommon struct defines metadata for Storm components including stream emissions, consumption patterns, parallelism, and component-specific configuration.

LANGUAGE: thrift
CODE:
ComponentCommon {
  1: map<string, StreamInfo> streams
  2: map<GlobalStreamId, Grouping> inputs
  3: i32 parallelism_hint
  4: map<string, string> json_conf
}

----------------------------------------

TITLE: HTML Redirect to Storm Project Ideas Documentation
DESCRIPTION: Implements a meta refresh redirect to the current Storm project ideas documentation page and sets the canonical URL for SEO purposes.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Project-ideas.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Project-ideas.html" />

----------------------------------------

TITLE: Complete Storm Topology Configuration
DESCRIPTION: Full example of configuring a Storm topology with multiple components. Shows configuration of worker processes, spouts, and bolts with different parallelism settings.

LANGUAGE: java
CODE:
Config conf = new Config();
conf.setNumWorkers(2); // use two worker processes

topologyBuilder.setSpout("blue-spout", new BlueSpout(), 2); // set parallelism hint to 2

topologyBuilder.setBolt("green-bolt", new GreenBolt(), 2)
               .setNumTasks(4)
               .shuffleGrouping("blue-spout");

topologyBuilder.setBolt("yellow-bolt", new YellowBolt(), 6)
               .shuffleGrouping("green-bolt");

StormSubmitter.submitTopology(
        "mytopology",
        conf,
        topologyBuilder.createTopology()
    );

----------------------------------------

TITLE: Running Non-JVM Language Topologies
DESCRIPTION: Facilitates running topologies written in non-JVM languages by constructing and uploading JARs to Nimbus.

LANGUAGE: shell
CODE:
storm shell resourcesdir command args

----------------------------------------

TITLE: Configuring YAML Front Matter for Storm Documentation
DESCRIPTION: YAML front matter specifying the title, layout, and documentation status for the Storm performance tuning page.

LANGUAGE: yaml
CODE:
---
title: Performance Tuning
layout: documentation
documentation: true
---

----------------------------------------

TITLE: jQuery UI MIT License Text
DESCRIPTION: The official MIT license text for jQuery UI, including copyright notice, permissions, warranty disclaimers, and special provisions for sample code (CC0) and external libraries

LANGUAGE: text
CODE:
Copyright jQuery Foundation and other contributors, https://jquery.org/

This software consists of voluntary contributions made by many
individuals. For exact contribution history, see the revision history
available at https://github.com/jquery/jquery-ui

The following license applies to all parts of this software except as
documented below:

====

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

====

Copyright and related rights for sample code are waived via CC0. Sample
code is defined as all source code contained within the demos directory.

CC0: http://creativecommons.org/publicdomain/zero/1.0/

====

All files located in the node_modules and external directories are
externally maintained libraries used by this software which have their
own licenses; we recommend you read them, as their terms may differ from
the terms above.

----------------------------------------

TITLE: Local Mode Messaging Implementation in Apache Storm (Clojure)
DESCRIPTION: Implementation of messaging for local mode using in-memory Java queues, allowing easy local usage without ZeroMQ.

LANGUAGE: clojure
CODE:
https://github.com/apache/storm/blob/0.7.1/src/clj/org/apache/storm/messaging/local.clj

----------------------------------------

TITLE: Implementing HTML META Redirect to Storm Documentation
DESCRIPTION: HTML code that performs an automatic redirect to the Apache Storm documentation page about codebase structure. Also includes a canonical link reference.

LANGUAGE: html
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Structure-of-the-codebase.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Structure-of-the-codebase.html" />

----------------------------------------

TITLE: Creating CassandraWriterBolt with Static Bound Query in Java
DESCRIPTION: Shows how to create a CassandraWriterBolt using a static bound query, which can be more efficient for repeated executions.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
     async(
        boundQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);")
            .bind(all());
     )
);

----------------------------------------

TITLE: Creating CassandraWriterBolt with Static Bound Query in Java
DESCRIPTION: Shows how to create a CassandraWriterBolt using a static bound query, which can be more efficient for repeated executions.

LANGUAGE: java
CODE:
new CassandraWriterBolt(
     async(
        boundQuery("INSERT INTO album (title,year,performer,genre,tracks) VALUES (?, ?, ?, ?, ?);")
            .bind(all());
     )
);

----------------------------------------

TITLE: Configuring YAML Front Matter for Dynamic Worker Profiling Documentation
DESCRIPTION: This YAML snippet defines the front matter for the documentation page, specifying the title, layout, and documentation status.

LANGUAGE: yaml
CODE:
---
title: Dynamic Worker Profiling
layout: documentation
documentation: true
---

----------------------------------------

TITLE: Implementing HTML Meta Refresh and Canonical Link for Apache Storm Documentation
DESCRIPTION: This HTML snippet sets up an automatic redirect to the Apache Storm Clojure DSL documentation page and specifies the canonical URL for search engines. The meta refresh tag triggers an immediate redirect, while the canonical link helps with SEO.

LANGUAGE: HTML
CODE:
<meta http-equiv="refresh" content="0; url=http://storm.apache.org/releases/current/Clojure-DSL.html">
<link rel="canonical" href="https://storm.apache.org/releases/current/Clojure-DSL.html" />

----------------------------------------

TITLE: Adding Storm-Redis Maven Dependency
DESCRIPTION: XML snippet for including the storm-redis dependency in a Maven project. This is required to use the Storm-Redis integration in your Storm topology.

LANGUAGE: xml
CODE:
<dependency>
    <groupId>org.apache.storm</groupId>
    <artifactId>storm-redis</artifactId>
    <version>${storm.version}</version>
    <type>jar</type>
</dependency>

----------------------------------------

TITLE: Configuring JMS Destinations and Connection Factory with Spring XML
DESCRIPTION: This XML configuration defines JMS queue and topic destinations, as well as a connection factory for ActiveMQ. It uses Spring's bean configuration syntax and ActiveMQ-specific XML namespaces to set up the JMS resources.

LANGUAGE: xml
CODE:
<?xml version="1.0" encoding="UTF-8"?>
<beans 
  xmlns="http://www.springframework.org/schema/beans" 
  xmlns:amq="http://activemq.apache.org/schema/core"
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.0.xsd
  http://activemq.apache.org/schema/core http://activemq.apache.org/schema/core/activemq-core.xsd">

	<amq:queue id="notificationQueue" physicalName="backtype.storm.contrib.example.queue" />
	
	<amq:topic id="notificationTopic" physicalName="backtype.storm.contrib.example.topic" />

	<amq:connectionFactory id="jmsConnectionFactory"
		brokerURL="tcp://localhost:61616" />
	
</beans>

----------------------------------------

TITLE: Registering Task Hook in Java Storm Spout/Bolt
DESCRIPTION: Demonstrates how to register a custom task hook in the open method of a spout or prepare method of a bolt using the TopologyContext method.

LANGUAGE: java
CODE:
TopologyContext#addTaskHook

----------------------------------------

TITLE: Registering Task Hook in Java Storm Spout/Bolt
DESCRIPTION: Demonstrates how to register a custom task hook in the open method of a spout or prepare method of a bolt using the TopologyContext method.

LANGUAGE: java
CODE:
TopologyContext#addTaskHook

----------------------------------------

TITLE: Property Substitution in Flux YAML
DESCRIPTION: Example of using property substitution in a Flux YAML file with an external properties file.

LANGUAGE: yaml
CODE:
  - id: "zkHosts"
    className: "org.apache.storm.kafka.ZkHosts"
    constructorArgs:
      - "${kafka.zookeeper.hosts}"

----------------------------------------

TITLE: Property Substitution in Flux YAML
DESCRIPTION: Example of using property substitution in a Flux YAML file with an external properties file.

LANGUAGE: yaml
CODE:
  - id: "zkHosts"
    className: "org.apache.storm.kafka.ZkHosts"
    constructorArgs:
      - "${kafka.zookeeper.hosts}"

----------------------------------------

TITLE: Defining Supervisor Slot Ports in Storm YAML
DESCRIPTION: This configuration defines the ports that Storm can use for worker processes on each machine. It determines how many workers can run on a single machine.

LANGUAGE: yaml
CODE:
supervisor.slots.ports:
    - 6700
    - 6701
    - 6702
    - 6703

----------------------------------------

TITLE: Defining Supervisor Slot Ports in Storm YAML
DESCRIPTION: This configuration defines the ports that Storm can use for worker processes on each machine. It determines how many workers can run on a single machine.

LANGUAGE: yaml
CODE:
supervisor.slots.ports:
    - 6700
    - 6701
    - 6702
    - 6703

----------------------------------------

TITLE: Implementing OpenTSDB State in Trident Topology
DESCRIPTION: This code snippet shows how to integrate OpenTSDB state into a Trident topology. It creates an OpenTsdbStateFactory, sets up a TridentTopology with a metric generation spout, and uses partitionPersist to write data to OpenTSDB using the OpenTsdbStateUpdater.

LANGUAGE: java
CODE:
final OpenTsdbStateFactory openTsdbStateFactory =
        new OpenTsdbStateFactory(OpenTsdbClient.newBuilder(tsdbUrl),
                Collections.singletonList(TupleOpenTsdbDatapointMapper.DEFAULT_MAPPER));
TridentTopology tridentTopology = new TridentTopology();

final Stream stream = tridentTopology.newStream("metric-tsdb-stream", new MetricGenSpout());

stream.peek(new Consumer() {
    @Override
    public void accept(TridentTuple input) {
        LOG.info("########### Received tuple: [{}]", input);
    }
}).partitionPersist(openTsdbStateFactory, MetricGenSpout.DEFAULT_METRIC_FIELDS, new OpenTsdbStateUpdater());

----------------------------------------

TITLE: Defining Components in Flux YAML
DESCRIPTION: Example of defining components, including constructor arguments and properties, in Flux YAML.

LANGUAGE: yaml
CODE:
components:
  - id: "stringScheme"
    className: "org.apache.storm.kafka.StringScheme"

  - id: "stringMultiScheme"
    className: "org.apache.storm.spout.SchemeAsMultiScheme"
    constructorArgs:
      - ref: "stringScheme"

  - id: "zkHosts"
    className: "org.apache.storm.kafka.ZkHosts"
    constructorArgs:
      - "localhost:2181"

  - id: "spoutConfig"
    className: "org.apache.storm.kafka.SpoutConfig"
    constructorArgs:
      - ref: "zkHosts"
      - "myKafkaTopic"
      - "/kafkaSpout"
      - "myId"
    properties:
      - name: "ignoreZkOffsets"
        value: true
      - name: "scheme"
        ref: "stringMultiScheme"

----------------------------------------

TITLE: Retrieving Topology Component Details in JSON
DESCRIPTION: Example response from the /api/v1/topology/<id>/component/<component> endpoint showing detailed metrics for a specific topology component in JSON format.

LANGUAGE: json
CODE:
{
  "name": "WordCount3",
  "id": "spout",
  "componentType": "spout",
  "windowHint": "10m 0s",
  "executors": 5,
  "componentErrors":[{
    "errorTime": 1406006074000,
    "errorHost": "10.11.1.70",
    "errorPort": 6701,
    "errorWorkerLogLink": "http://10.11.1.7:8000/log?file=worker-6701.log",
    "errorLapsedSecs": 16,
    "error": "java.lang.RuntimeException: java.lang.StringIndexOutOfBoundsException: Some Error\n\tat org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:128)\n\tat org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99)\n\tat org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80)\n\tat backtype...more.."
  }],
  "topologyId": "WordCount3-1-1402960825",
  "tasks": 5,
  "window": "600",
  "spoutSummary": [
    {
      "windowPretty": "10m 0s",
      "window": "600",
      "emitted": 28500,
      "transferred": 28460,
      "completeLatency": "0.000",
      "acked": 0,
      "failed": 0
    }
  ],
  "outputStats": [
    {
      "stream": "default",
      "emitted": 28460,
      "transferred": 28460,
      "completeLatency": "0",
      "acked": 0,
      "failed": 0
    }
  ]
}

----------------------------------------

TITLE: Retrieving Topology Component Details in JSON
DESCRIPTION: Example response from the /api/v1/topology/<id>/component/<component> endpoint showing detailed metrics for a specific topology component in JSON format.

LANGUAGE: json
CODE:
{
  "name": "WordCount3",
  "id": "spout",
  "componentType": "spout",
  "windowHint": "10m 0s",
  "executors": 5,
  "componentErrors":[{
    "errorTime": 1406006074000,
    "errorHost": "10.11.1.70",
    "errorPort": 6701,
    "errorWorkerLogLink": "http://10.11.1.7:8000/log?file=worker-6701.log",
    "errorLapsedSecs": 16,
    "error": "java.lang.RuntimeException: java.lang.StringIndexOutOfBoundsException: Some Error\n\tat org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:128)\n\tat org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99)\n\tat org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80)\n\tat backtype...more.."
  }],
  "topologyId": "WordCount3-1-1402960825",
  "tasks": 5,
  "window": "600",
  "spoutSummary": [
    {
      "windowPretty": "10m 0s",
      "window": "600",
      "emitted": 28500,
      "transferred": 28460,
      "completeLatency": "0.000",
      "acked": 0,
      "failed": 0
    }
  ],
  "outputStats": [
    {
      "stream": "default",
      "emitted": 28460,
      "transferred": 28460,
      "completeLatency": "0",
      "acked": 0,
      "failed": 0
    }
  ]
}

----------------------------------------

TITLE: Configuring Node Resources in Storm YAML
DESCRIPTION: Configuration for specifying available CPU and memory resources on a node.

LANGUAGE: yaml
CODE:
supervisor.memory.capacity.mb: 20480.0
supervisor.cpu.capacity: 100.0

----------------------------------------

TITLE: Implementing SplitSentence as BasicBolt in Storm
DESCRIPTION: This code shows how to implement the SplitSentence functionality using Storm's BasicBolt interface, which simplifies the implementation by automatically handling anchoring and acking.

LANGUAGE: java
CODE:
public class SplitSentence extends BaseBasicBolt {
        public void execute(Tuple tuple, BasicOutputCollector collector) {
            String sentence = tuple.getString(0);
            for(String word: sentence.split(" ")) {
                collector.emit(new Values(word));
            }
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declare(new Fields("word"));
        }        
    }

----------------------------------------

TITLE: Persistent Word Count Topology Implementation
DESCRIPTION: Complete example of a Storm topology that implements persistent word counting using HBase for storage.

LANGUAGE: java
CODE:
public class PersistentWordCount {
    private static final String WORD_SPOUT = "WORD_SPOUT";
    private static final String COUNT_BOLT = "COUNT_BOLT";
    private static final String HBASE_BOLT = "HBASE_BOLT";


    public static void main(String[] args) throws Exception {
        Config config = new Config();

        Map<String, Object> hbConf = new HashMap<String, Object>();
        if(args.length > 0){
            hbConf.put("hbase.rootdir", args[0]);
        }
        config.put("hbase.conf", hbConf);

        WordSpout spout = new WordSpout();
        WordCounter bolt = new WordCounter();

        SimpleHBaseMapper mapper = new SimpleHBaseMapper()
                .withRowKeyField("word")
                .withColumnFields(new Fields("word"))
                .withCounterFields(new Fields("count"))
                .withColumnFamily("cf");

        HBaseBolt hbase = new HBaseBolt("WordCount", mapper)
                .withConfigKey("hbase.conf");


        TopologyBuilder builder = new TopologyBuilder();

        builder.setSpout(WORD_SPOUT, spout, 1);
        builder.setBolt(COUNT_BOLT, bolt, 1).shuffleGrouping(WORD_SPOUT);
        builder.setBolt(HBASE_BOLT, hbase, 1).fieldsGrouping(COUNT_BOLT, new Fields("word"));

        String topoName = "test";
        if (args.length > 1) {
            topoName = args[1];
        }
        if (args.length == 4) {
            System.out.println("hdfs url: " + args[0] + ", keytab file: " + args[2] + 
                ", principal name: " + args[3] + ", toplogy name: " + topoName);
            hbConf.put(HBaseSecurityUtil.STORM_KEYTAB_FILE_KEY, args[2]);
            hbConf.put(HBaseSecurityUtil.STORM_USER_NAME_KEY, args[3]);
        } else if (args.length == 3 || args.length > 4) {
            System.out.println("Usage: PersistentWordCount <hbase.rootdir> [topology name] [keytab file] [principal name]");
            return;
        }
        config.setNumWorkers(3);
        StormSubmitter.submitTopology(topoName, config, builder.createTopology());
    }
}

----------------------------------------

TITLE: Persistent Word Count Topology Implementation
DESCRIPTION: Complete example of a Storm topology that implements persistent word counting using HBase for storage.

LANGUAGE: java
CODE:
public class PersistentWordCount {
    private static final String WORD_SPOUT = "WORD_SPOUT";
    private static final String COUNT_BOLT = "COUNT_BOLT";
    private static final String HBASE_BOLT = "HBASE_BOLT";


    public static void main(String[] args) throws Exception {
        Config config = new Config();

        Map<String, Object> hbConf = new HashMap<String, Object>();
        if(args.length > 0){
            hbConf.put("hbase.rootdir", args[0]);
        }
        config.put("hbase.conf", hbConf);

        WordSpout spout = new WordSpout();
        WordCounter bolt = new WordCounter();

        SimpleHBaseMapper mapper = new SimpleHBaseMapper()
                .withRowKeyField("word")
                .withColumnFields(new Fields("word"))
                .withCounterFields(new Fields("count"))
                .withColumnFamily("cf");

        HBaseBolt hbase = new HBaseBolt("WordCount", mapper)
                .withConfigKey("hbase.conf");


        TopologyBuilder builder = new TopologyBuilder();

        builder.setSpout(WORD_SPOUT, spout, 1);
        builder.setBolt(COUNT_BOLT, bolt, 1).shuffleGrouping(WORD_SPOUT);
        builder.setBolt(HBASE_BOLT, hbase, 1).fieldsGrouping(COUNT_BOLT, new Fields("word"));

        String topoName = "test";
        if (args.length > 1) {
            topoName = args[1];
        }
        if (args.length == 4) {
            System.out.println("hdfs url: " + args[0] + ", keytab file: " + args[2] + 
                ", principal name: " + args[3] + ", toplogy name: " + topoName);
            hbConf.put(HBaseSecurityUtil.STORM_KEYTAB_FILE_KEY, args[2]);
            hbConf.put(HBaseSecurityUtil.STORM_USER_NAME_KEY, args[3]);
        } else if (args.length == 3 || args.length > 4) {
            System.out.println("Usage: PersistentWordCount <hbase.rootdir> [topology name] [keytab file] [principal name]");
            return;
        }
        config.setNumWorkers(3);
        StormSubmitter.submitTopology(topoName, config, builder.createTopology());
    }
}

----------------------------------------

TITLE: Running DRPC in Local Mode with Java
DESCRIPTION: This example shows how to set up and run a DRPC topology in local mode using LocalDRPC, LocalCluster, and LocalTopology. It demonstrates submitting a topology and executing DRPC calls.

LANGUAGE: java
CODE:
try (LocalDRPC drpc = new LocalDRPC();
     LocalCluster cluster = new LocalCluster();
     LocalTopology topo = cluster.submitTopology("drpc-demo", conf, builder.createLocalTopology(drpc))) {

    System.out.println("Results for 'hello':" + drpc.execute("exclamation", "hello"));
}

----------------------------------------

TITLE: Implementing a Spout in Java
DESCRIPTION: This code shows the implementation of the nextTuple() method in a TestWordSpout class. It demonstrates how to emit random words as tuples in a Storm spout.

LANGUAGE: Java
CODE:
public void nextTuple() {
    Utils.sleep(100);
    final String[] words = new String[] {"nathan", "mike", "jackson", "golda", "bertels"};
    final Random rand = new Random();
    final String word = words[rand.nextInt(words.length)];
    _collector.emit(new Values(word));
}

----------------------------------------

TITLE: Implementing a Spout in Java
DESCRIPTION: This code shows the implementation of the nextTuple() method in a TestWordSpout class. It demonstrates how to emit random words as tuples in a Storm spout.

LANGUAGE: Java
CODE:
public void nextTuple() {
    Utils.sleep(100);
    final String[] words = new String[] {"nathan", "mike", "jackson", "golda", "bertels"};
    final Random rand = new Random();
    final String word = words[rand.nextInt(words.length)];
    _collector.emit(new Values(word));
}

----------------------------------------

TITLE: Shared State Access Methods
DESCRIPTION: Methods for accessing shared state at different levels (task, executor, and worker) including write and read operations.

LANGUAGE: java
CODE:
// Task Data Access
topologyContext.setTaskData(String, Object)    // write
topologyContext.getTaskData(String)          // read

// Executor Data Access
topologyContext.setExecutorData(String, Object) // write
topologyContext.getExecutorData(String)        // read

// User Resources Access
workerUserContext.setResource(String, Object)   // write
topologyContext.getResource(String)            // read

----------------------------------------

TITLE: Storm Shell Command Usage
DESCRIPTION: Command line syntax for submitting a Storm topology using the shell command. Demonstrates packaging resources and executing a Python topology script.

LANGUAGE: bash
CODE:
storm shell resources/ python3 topology.py arg1 arg2

----------------------------------------

TITLE: Parsing JSON-encoded Tuple in Storm Multi-Language Protocol
DESCRIPTION: This JSON structure represents an incoming tuple in the Storm multi-language protocol. It includes the tuple's id, component id, stream id, task id, and the actual tuple values.

LANGUAGE: json
CODE:
{
    "id": -6955786537413359385,
    "comp": 1,
    "stream": 1,
    "task": 9,
    "tuple": ["snow white and the seven dwarfs", "field2", 3]
}

----------------------------------------

TITLE: Parsing JSON-encoded Tuple in Storm Multi-Language Protocol
DESCRIPTION: This JSON structure represents an incoming tuple in the Storm multi-language protocol. It includes the tuple's id, component id, stream id, task id, and the actual tuple values.

LANGUAGE: json
CODE:
{
    "id": -6955786537413359385,
    "comp": 1,
    "stream": 1,
    "task": 9,
    "tuple": ["snow white and the seven dwarfs", "field2", 3]
}

----------------------------------------

TITLE: Initializing Local Storm Cluster in Java
DESCRIPTION: Creates an in-process Storm cluster using LocalCluster class for testing topologies. The cluster is automatically closed when exiting the try block.

LANGUAGE: java
CODE:
import org.apache.storm.LocalCluster;

...

try (LocalCluster cluster = new LocalCluster()) {
    //Interact with the cluster...
}

----------------------------------------

TITLE: Assignment Record Structure
DESCRIPTION: Definition of the assignment record containing task mappings, node information, and timing data

LANGUAGE: clojure
CODE:
master-code-dir
task->node+port
node->host
task->start-time-secs

----------------------------------------

TITLE: Leader Election Interface Implementation in Java
DESCRIPTION: Interface defining the leader election mechanism for Nimbus servers with methods for managing leadership queue, status checks and address lookups.

LANGUAGE: java
CODE:
public interface ILeaderElector {
    void addToLeaderLockQueue();
    void removeFromLeaderLockQueue();
    boolean isLeader();
    InetSocketAddress getLeaderAddress();
    List<InetSocketAddress> getAllNimbusAddresses();
}

----------------------------------------

TITLE: Retrieving Supervisor Summary in Storm UI REST API
DESCRIPTION: Example JSON response for the /api/v1/supervisor/summary endpoint, which returns summary information for all supervisors in the cluster.

LANGUAGE: json
CODE:
{
    "supervisors": [
        {
            "id": "0b879808-2a26-442b-8f7d-23101e0c3696",
            "host": "10.11.1.7",
            "uptime": "5m 58s",
            "uptimeSeconds": 358,
            "slotsTotal": 4,
            "slotsUsed": 3,
            "totalMem": 3000,
            "totalCpu": 400,
            "usedMem": 1280,
            "usedCPU": 160
        }
    ],
    "schedulerDisplayResource": true
}

----------------------------------------

TITLE: Configuring YAML Frontmatter for Storm Documentation
DESCRIPTION: YAML frontmatter defining the title, layout, and documentation status for the Storm performance tuning guide.

LANGUAGE: yaml
CODE:
---
title: Performance Tuning
layout: documentation
documentation: true
---

----------------------------------------

TITLE: Building Event Hubs Storm Integration
DESCRIPTION: Maven command to build the project package

LANGUAGE: bash
CODE:
mvn clean package

----------------------------------------

TITLE: Running Example Topology
DESCRIPTION: Maven command to execute the example topology locally.

LANGUAGE: bash
CODE:
$ mvn exec:java