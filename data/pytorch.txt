TITLE: Initializing DistributedDataParallel with torch.distributed.run in PyTorch
DESCRIPTION: This code demonstrates how to use PyTorch Elastic to simplify DDP initialization and job launching. It shows how to create a DDP-ready script that can be run with torchrun.

LANGUAGE: python
CODE:
import torch
import torch.distributed as dist
import torch.nn as nn
import torch.optim as optim

from torch.nn.parallel import DistributedDataParallel as DDP

class ToyModel(nn.Module):
    def __init__(self):
        super(ToyModel, self).__init__()
        self.net1 = nn.Linear(10, 10)
        self.relu = nn.ReLU()
        self.net2 = nn.Linear(10, 5)

    def forward(self, x):
        return self.net2(self.relu(self.net1(x)))


def demo_basic():
    torch.cuda.set_device(int(os.environ["LOCAL_RANK"]))
    dist.init_process_group("nccl")
    rank = dist.get_rank()
    print(f"Start running basic DDP example on rank {rank}.")
    # create model and move it to GPU with id rank
    device_id = rank % torch.cuda.device_count()
    model = ToyModel().to(device_id)
    ddp_model = DDP(model, device_ids=[device_id])
    loss_fn = nn.MSELoss()
    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)

    optimizer.zero_grad()
    outputs = ddp_model(torch.randn(20, 10))
    labels = torch.randn(20, 5).to(device_id)
    loss_fn(outputs, labels).backward()
    optimizer.step()
    dist.destroy_process_group()
    print(f"Finished running basic DDP example on rank {rank}.")

if __name__ == "__main__":
    demo_basic()

----------------------------------------

TITLE: Importing PyTorch and Essential Modules
DESCRIPTION: Shows how to import PyTorch and its main modules for neural network development, including autograd, nn, functional, and optim. Also includes imports for vision tasks and distributed training.

LANGUAGE: python
CODE:
import torch
from torch.utils.data import Dataset, DataLoader

import torch.autograd as autograd
from torch import Tensor
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from torchvision import datasets, models, transforms

import torch.distributed as dist
from torch.multiprocessing import Process

----------------------------------------

TITLE: PyTorch Autograd Implementation
DESCRIPTION: Demonstrates automatic differentiation using PyTorch's autograd package for polynomial fitting, eliminating need for manual backward pass implementation.

LANGUAGE: python
CODE:
# Code not directly visible in provided text

----------------------------------------

TITLE: Defining a Neural Network Model for MNIST Classification
DESCRIPTION: This class defines a simple convolutional neural network for classifying MNIST digits. It includes convolutional layers, dropout, and fully connected layers.

LANGUAGE: python
CODE:
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output

----------------------------------------

TITLE: GPU Usage in PyTorch
DESCRIPTION: Shows how to check for CUDA availability and move tensors and models between CPU and GPU. Includes device-agnostic code for flexibility in different environments.

LANGUAGE: python
CODE:
torch.cuda.is_available
x = x.cuda()
x = x.cpu()

if not args.disable_cuda and torch.cuda.is_available():
    args.device = torch.device('cuda')
else:
    args.device = torch.device('cpu')

net.to(device)
x = x.to(device)

----------------------------------------

TITLE: Defining Neural Network Layers in PyTorch
DESCRIPTION: Illustrates how to create various neural network layers using PyTorch's nn module, including fully connected, convolutional, pooling, normalization, recurrent, and dropout layers.

LANGUAGE: python
CODE:
nn.Linear(m,n)
nn.ConvXd(m,n,s)
nn.MaxPoolXd(s)
nn.BatchNormXd
nn.RNN/LSTM/GRU
nn.Dropout(p=0.5, inplace=False)
nn.Dropout2d(p=0.5, inplace=False)
nn.Embedding(num_embeddings, embedding_dim)

----------------------------------------

TITLE: Basic PyTorch Imports
DESCRIPTION: Essential imports for general PyTorch usage and dataset handling

LANGUAGE: python
CODE:
import torch
from torch.utils.data import Dataset, DataLoader

----------------------------------------

TITLE: Implementing ZeroRedundancyOptimizer with DistributedDataParallel in PyTorch
DESCRIPTION: Complete implementation showing how to use ZeroRedundancyOptimizer with DDP for distributed training. The code demonstrates memory optimization by sharding optimizer states across processes, including memory usage tracking and comparison with standard Adam optimizer.

LANGUAGE: python
CODE:
import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import torch.nn as nn
import torch.optim as optim
from torch.distributed.optim import ZeroRedundancyOptimizer
from torch.nn.parallel import DistributedDataParallel as DDP

def print_peak_memory(prefix, device):
    if device == 0:
        print(f"{prefix}: {torch.cuda.max_memory_allocated(device) // 1e6}MB ")

def example(rank, world_size, use_zero):
    torch.manual_seed(0)
    torch.cuda.manual_seed(0)
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '29500'
    # create default process group
    dist.init_process_group("gloo", rank=rank, world_size=world_size)

    # create local model
    model = nn.Sequential(*[nn.Linear(2000, 2000).to(rank) for _ in range(20)])
    print_peak_memory("Max memory allocated after creating local model", rank)

    # construct DDP model
    ddp_model = DDP(model, device_ids=[rank])
    print_peak_memory("Max memory allocated after creating DDP", rank)

    # define loss function and optimizer
    loss_fn = nn.MSELoss()
    if use_zero:
        optimizer = ZeroRedundancyOptimizer(
            ddp_model.parameters(),
            optimizer_class=torch.optim.Adam,
            lr=0.01
        )
    else:
        optimizer = torch.optim.Adam(ddp_model.parameters(), lr=0.01)

    # forward pass
    outputs = ddp_model(torch.randn(20, 2000).to(rank))
    labels = torch.randn(20, 2000).to(rank)
    # backward pass
    loss_fn(outputs, labels).backward()

    # update parameters
    print_peak_memory("Max memory allocated before optimizer step()", rank)
    optimizer.step()
    print_peak_memory("Max memory allocated after optimizer step()", rank)

    print(f"params sum is: {sum(model.parameters()).sum()}")


def main():
    world_size = 2
    print("=== Using ZeroRedundancyOptimizer ===")
    mp.spawn(example,
        args=(world_size, True),
        nprocs=world_size,
        join=True)

    print("=== Not Using ZeroRedundancyOptimizer ===")
    mp.spawn(example,
        args=(world_size, False),
        nprocs=world_size,
        join=True)

if __name__=="__main__":
    main()

----------------------------------------

TITLE: Creating a Basic DistributedDataParallel Model in PyTorch
DESCRIPTION: This snippet shows how to create a toy model, wrap it with DDP, and perform a basic training step. It demonstrates the core functionality of DDP for distributed training.

LANGUAGE: python
CODE:
class ToyModel(nn.Module):
    def __init__(self):
        super(ToyModel, self).__init__()
        self.net1 = nn.Linear(10, 10)
        self.relu = nn.ReLU()
        self.net2 = nn.Linear(10, 5)

    def forward(self, x):
        return self.net2(self.relu(self.net1(x)))


def demo_basic(rank, world_size):
    print(f"Running basic DDP example on rank {rank}.")
    setup(rank, world_size)

    # create model and move it to GPU with id rank
    model = ToyModel().to(rank)
    ddp_model = DDP(model, device_ids=[rank])

    loss_fn = nn.MSELoss()
    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)

    optimizer.zero_grad()
    outputs = ddp_model(torch.randn(20, 10))
    labels = torch.randn(20, 5).to(rank)
    loss_fn(outputs, labels).backward()
    optimizer.step()

    cleanup()
    print(f"Finished running basic DDP example on rank {rank}.")


def run_demo(demo_fn, world_size):
    mp.spawn(demo_fn,
             args=(world_size,),
             nprocs=world_size,
             join=True)

----------------------------------------

TITLE: Configuring Optimizers and Learning Rate Schedulers in PyTorch
DESCRIPTION: Shows how to create and use optimizers and learning rate schedulers in PyTorch, including popular options like SGD, Adam, and various learning rate adjustment strategies.

LANGUAGE: python
CODE:
opt = optim.x(model.parameters(), ...)
opt.step()
opt.zero_grad()

scheduler = optim.X(optimizer,...)
scheduler.step()

----------------------------------------

TITLE: Initializing Distributed Environment in PyTorch
DESCRIPTION: Sets up the distributed environment by initializing the process group and spawning multiple processes. Uses the 'gloo' backend and sets environment variables for coordination.

LANGUAGE: python
CODE:
"""run.py:"""
#!/usr/bin/env python
import os
import sys
import torch
import torch.distributed as dist
import torch.multiprocessing as mp

def run(rank, size):
    """ Distributed function to be implemented later. """
    pass

def init_process(rank, size, fn, backend='gloo'):
    """ Initialize the distributed environment. """
    os.environ['MASTER_ADDR'] = '127.0.0.1'
    os.environ['MASTER_PORT'] = '29500'
    dist.init_process_group(backend, rank=rank, world_size=size)
    fn(rank, size)


if __name__ == "__main__":
    world_size = 2
    processes = []
    if "google.colab" in sys.modules:
        print("Running in Google Colab")
        mp.get_context("spawn")
    else:
        mp.set_start_method("spawn")
    for rank in range(world_size):
        p = mp.Process(target=init_process, args=(rank, world_size, run))
        p.start()
        processes.append(p)

    for p in processes:
        p.join()

----------------------------------------

TITLE: Creating and Manipulating PyTorch Tensors
DESCRIPTION: Demonstrates various ways to create PyTorch tensors, including random tensors, ones, zeros, and from lists. Also shows tensor operations like cloning, reshaping, and dimension manipulation.

LANGUAGE: python
CODE:
x = torch.randn(*size)
x = torch.[ones|zeros](*size)
x = torch.tensor(L)
y = x.clone()
with torch.no_grad():
requires_grad=True

x.size()
x = torch.cat(tensor_seq, dim=0)
y = x.view(a,b,...)
y = x.view(-1,a)
y = x.transpose(a,b)
y = x.permute(*dims)
y = x.unsqueeze(dim)
y = x.squeeze()

----------------------------------------

TITLE: Implementing Distributed Synchronous SGD in PyTorch
DESCRIPTION: Demonstrates distributed synchronous SGD training by partitioning the dataset, running forward-backward passes, and averaging gradients across processes.

LANGUAGE: python
CODE:
""" Distributed Synchronous SGD Example """
def run(rank, size):
    torch.manual_seed(1234)
    train_set, bsz = partition_dataset()
    model = Net()
    optimizer = optim.SGD(model.parameters(),
                          lr=0.01, momentum=0.5)

    num_batches = ceil(len(train_set.dataset) / float(bsz))
    for epoch in range(10):
        epoch_loss = 0.0
        for data, target in train_set:
            optimizer.zero_grad()
            output = model(data)
            loss = F.nll_loss(output, target)
            epoch_loss += loss.item()
            loss.backward()
            average_gradients(model)
            optimizer.step()
        print('Rank ', dist.get_rank(), ', epoch ',
              epoch, ': ', epoch_loss / num_batches)

----------------------------------------

TITLE: Complete Flask Application for PyTorch Model Serving
DESCRIPTION: Full implementation of a Flask web service that serves predictions from a pre-trained DenseNet model, including request handling, image processing, and response formatting.

LANGUAGE: python
CODE:
import io
import json
import os

import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
from flask import Flask, jsonify, request


app = Flask(__name__)
model = models.densenet121(pretrained=True)               # Trained on 1000 classes from ImageNet
model.eval()                                              # Turns off autograd 



img_class_map = None
mapping_file_path = 'index_to_name.json'                  # Human-readable names for Imagenet classes
if os.path.isfile(mapping_file_path):
    with open (mapping_file_path) as f:
        img_class_map = json.load(f)



# Transform input into the form our model expects
def transform_image(infile):
    input_transforms = [transforms.Resize(255),           # We use multiple TorchVision transforms to ready the image
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],       # Standard normalization for ImageNet model input
            [0.229, 0.224, 0.225])]
    my_transforms = transforms.Compose(input_transforms)
    image = Image.open(infile)                            # Open the image file
    timg = my_transforms(image)                           # Transform PIL image to appropriately-shaped PyTorch tensor
    timg.unsqueeze_(0)                                    # PyTorch models expect batched input; create a batch of 1
    return timg


# Get a prediction
def get_prediction(input_tensor):
    outputs = model.forward(input_tensor)                 # Get likelihoods for all ImageNet classes
    _, y_hat = outputs.max(1)                             # Extract the most likely class
    prediction = y_hat.item()                             # Extract the int value from the PyTorch tensor
    return prediction

# Make the prediction human-readable
def render_prediction(prediction_idx):
    stridx = str(prediction_idx)
    class_name = 'Unknown'
    if img_class_map is not None:
        if stridx in img_class_map is not None:
            class_name = img_class_map[stridx][1]

    return prediction_idx, class_name


@app.route('/', methods=['GET'])
def root():
    return jsonify({'msg' : 'Try POSTing to the /predict endpoint with an RGB image attachment'})


@app.route('/predict', methods=['POST'])
def predict():
    if request.method == 'POST':
        file = request.files['file']
        if file is not None:
            input_tensor = transform_image(file)
            prediction_idx = get_prediction(input_tensor)
            class_id, class_name = render_prediction(prediction_idx)
            return jsonify({'class_id': class_id, 'class_name': class_name})


if __name__ == '__main__':
    app.run()

----------------------------------------

TITLE: Implementing Post-Training Static Quantization in PyTorch
DESCRIPTION: Demonstrates how to apply static quantization to a PyTorch model, converting both weights and activations to 8-bit integers beforehand for improved inference speed.

LANGUAGE: python
CODE:
backend = "qnnpack"
model.qconfig = torch.quantization.get_default_qconfig(backend)
torch.backends.quantized.engine = backend
model_static_quantized = torch.quantization.prepare(model, inplace=False)
model_static_quantized = torch.quantization.convert(model_static_quantized, inplace=False)

----------------------------------------

TITLE: PyTorch Tensor Implementation
DESCRIPTION: Implementation of polynomial fitting using PyTorch Tensors, showing basic tensor operations and GPU capability. Manual implementation of forward and backward passes.

LANGUAGE: python
CODE:
# Code not directly visible in provided text

----------------------------------------

TITLE: Implementing Hybrid Sharding Data Parallel (HSDP) with DeviceMesh in Python
DESCRIPTION: This code snippet demonstrates how to use DeviceMesh to implement Hybrid Sharding Data Parallel (HSDP), which performs FSDP within a host and DDP across hosts. It shows the creation of a 2D device mesh and its application to a toy model using FSDP.

LANGUAGE: python
CODE:
import torch
import torch.nn as nn

from torch.distributed.device_mesh import init_device_mesh
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy


class ToyModel(nn.Module):
    def __init__(self):
        super(ToyModel, self).__init__()
        self.net1 = nn.Linear(10, 10)
        self.relu = nn.ReLU()
        self.net2 = nn.Linear(10, 5)

    def forward(self, x):
        return self.net2(self.relu(self.net1(x)))


# HSDP: MeshShape(2, 4)
mesh_2d = init_device_mesh("cuda", (2, 4))
model = FSDP(
    ToyModel(), device_mesh=mesh_2d, sharding_strategy=ShardingStrategy.HYBRID_SHARD
)

----------------------------------------

TITLE: Applying Post-Training Dynamic Quantization in PyTorch
DESCRIPTION: Shows how to apply dynamic quantization to a PyTorch model, converting weights to 8-bit integers while keeping activations in floating-point until computation.

LANGUAGE: python
CODE:
model_dynamic_quantized = torch.quantization.quantize_dynamic(
    model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8
)

----------------------------------------

TITLE: Defining Neural Network Architecture for Fashion-MNIST
DESCRIPTION: Implements a CNN model architecture with convolutional layers, max pooling, and fully connected layers for classifying Fashion-MNIST images.

LANGUAGE: python
CODE:
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 4 * 4, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 4 * 4)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

net = Net()

----------------------------------------

TITLE: Setting Up Quantization-Aware Training in PyTorch
DESCRIPTION: Explains how to prepare a model for quantization-aware training by adding QuantStub and DeQuantStub, and shows the process of quantization-aware training.

LANGUAGE: python
CODE:
self.quant = torch.quantization.QuantStub()
self.dequant = torch.quantization.DeQuantStub()

# In the forward method:
# x = self.quant(x)
# ...
# x = self.dequant(x)

model.qconfig = torch.quantization.get_default_qat_qconfig(backend)
model_qat = torch.quantization.prepare_qat(model, inplace=False)
# quantization aware training goes here
model_qat = torch.quantization.convert(model_qat.eval(), inplace=False)

----------------------------------------

TITLE: Setting up DistributedDataParallel Environment in Python
DESCRIPTION: This code snippet demonstrates how to set up the environment for DistributedDataParallel, including importing necessary modules and defining setup and cleanup functions.

LANGUAGE: python
CODE:
import os
import sys
import tempfile
import torch
import torch.distributed as dist
import torch.nn as nn
import torch.optim as optim
import torch.multiprocessing as mp

from torch.nn.parallel import DistributedDataParallel as DDP

def setup(rank, world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'

    # initialize the process group
    dist.init_process_group("gloo", rank=rank, world_size=world_size)

def cleanup():
    dist.destroy_process_group()

----------------------------------------

TITLE: Deep Learning Components
DESCRIPTION: Neural network layers, loss functions, and activation functions

LANGUAGE: python
CODE:
nn.Linear(m,n)
nn.ConvXd(m,n,s)
nn.MaxPoolXd(s)
nn.BatchNorm
nn.RNN/LSTM/GRU
nn.Dropout(p=0.5, inplace=False)
nn.Dropout2d(p=0.5, inplace=False)
nn.Embedding(num_embeddings, embedding_dim)

----------------------------------------

TITLE: Saving FSDP-wrapped Model with DCP
DESCRIPTION: This snippet demonstrates how to create a toy model, wrap it with FSDP, and save it using DCP. It includes the AppState class for managing model and optimizer state, and the process for initializing and saving the distributed checkpoint.

LANGUAGE: python
CODE:
import os

import torch
import torch.distributed as dist
import torch.distributed.checkpoint as dcp
import torch.multiprocessing as mp
import torch.nn as nn

from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.checkpoint.state_dict import get_state_dict, set_state_dict
from torch.distributed.checkpoint.stateful import Stateful
from torch.distributed.fsdp.fully_sharded_data_parallel import StateDictType

CHECKPOINT_DIR = "checkpoint"


class AppState(Stateful):
    def __init__(self, model, optimizer=None):
        self.model = model
        self.optimizer = optimizer

    def state_dict(self):
        model_state_dict, optimizer_state_dict = get_state_dict(self.model, self.optimizer)
        return {
            "model": model_state_dict,
            "optim": optimizer_state_dict
        }

    def load_state_dict(self, state_dict):
        set_state_dict(
            self.model,
            self.optimizer,
            model_state_dict=state_dict["model"],
            optim_state_dict=state_dict["optim"]
        )

class ToyModel(nn.Module):
    def __init__(self):
        super(ToyModel, self).__init__()
        self.net1 = nn.Linear(16, 16)
        self.relu = nn.ReLU()
        self.net2 = nn.Linear(16, 8)

    def forward(self, x):
        return self.net2(self.relu(self.net1(x)))


def setup(rank, world_size):
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "12355 "

    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)


def cleanup():
    dist.destroy_process_group()


def run_fsdp_checkpoint_save_example(rank, world_size):
    print(f"Running basic FSDP checkpoint saving example on rank {rank}.")
    setup(rank, world_size)

    model = ToyModel().to(rank)
    model = FSDP(model)

    loss_fn = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

    optimizer.zero_grad()
    model(torch.rand(8, 16, device="cuda")).sum().backward()
    optimizer.step()

    state_dict = { "app": AppState(model, optimizer) }
    dcp.save(state_dict, checkpoint_id=CHECKPOINT_DIR)

    cleanup()


if __name__ == "__main__":
    world_size = torch.cuda.device_count()
    print(f"Running fsdp checkpoint example on {world_size} devices.")
    mp.spawn(
        run_fsdp_checkpoint_save_example,
        args=(world_size,),
        nprocs=world_size,
        join=True,
    )

----------------------------------------

TITLE: Configuring PyTorch Dependencies for Python Projects
DESCRIPTION: This requirements file specifies the exact versions of packages needed for PyTorch-related projects. It includes libraries for machine learning, data processing, visualization, and documentation generation. Some packages are conditionally included based on the operating system.

LANGUAGE: Text
CODE:
# --extra-index-url https://download.pytorch.org/whl/cu117/index.html # Use this to run/publish tutorials against the latest binaries during the RC stage. Comment out after the release. Each release verify the correct cuda version.
# Refer to ./jenkins/build.sh for tutorial build instructions

sphinx==5.0.0
sphinx-gallery==0.11.1
sphinx_design
docutils==0.16
sphinx-copybutton
sphinx_sitemap==2.6.0
pypandoc==1.12
pandocfilters
markdown
tqdm==4.66.1
numpy==1.24.4
matplotlib
librosa
torch==2.6
torchvision
torchdata
networkx
PyHamcrest
bs4
awscliv2==2.1.1
flask
spacy==3.4.1
ray[tune]==2.7.2
tensorboard
jinja2==3.1.3
pytorch-lightning
torchx
torchrl==0.6.0
tensordict==0.6.0
ax-platform>=0.4.0
nbformat>=5.9.2
datasets
transformers
torchmultimodal-nightly # needs to be updated to stable as soon as it's avaialable
onnx
onnxscript
onnxruntime
evaluate
accelerate>=0.20.1

importlib-metadata==6.8.0

# PyTorch Theme
-e git+https://github.com/pytorch/pytorch_sphinx_theme.git#egg=pytorch_sphinx_theme

ipython

sphinxcontrib.katex
# to run examples
boto3
pandas
requests
scikit-image
scipy==1.11.1
numba==0.57.1
pillow==10.2.0
wget
gym==0.26.2
gym-super-mario-bros==7.4.0
pyopengl
gymnasium[mujoco]==0.27.0
timm
iopath
pygame==2.6.0
pycocotools
semilearn==0.3.2
torchao==0.5.0
segment_anything==1.0
torchrec==1.0.0; platform_system == "Linux"
fbgemm-gpu==1.1.0; platform_system == "Linux"

----------------------------------------

TITLE: Setting Up Distributed Training Environment for FSDP
DESCRIPTION: These functions initialize the distributed training environment and clean up after training. They set up the process group for distributed communication.

LANGUAGE: python
CODE:
def setup(rank, world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'

    # initialize the process group
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def cleanup():
    dist.destroy_process_group()

----------------------------------------

TITLE: Neural Network Module Implementation
DESCRIPTION: Uses PyTorch's nn package to implement the polynomial model with built-in layers and loss functions.

LANGUAGE: python
CODE:
# Code not directly visible in provided text

----------------------------------------

TITLE: Importing DDP-related modules in PyTorch
DESCRIPTION: Import necessary modules for distributed training, including torch.multiprocessing, DistributedSampler, DistributedDataParallel, and process group initialization functions.

LANGUAGE: python
CODE:
import torch
import torch.nn.functional as F
from utils import MyTrainDataset

import torch.multiprocessing as mp
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group
import os

----------------------------------------

TITLE: Optimized DCP Checkpointing with Pinned Memory
DESCRIPTION: Enhanced implementation using pinned memory buffers for improved performance. Demonstrates how to use StorageWriter with cached state dict for efficient memory copying between checkpointing steps.

LANGUAGE: python
CODE:
import os

import torch
import torch.distributed as dist
import torch.distributed.checkpoint as dcp
import torch.multiprocessing as mp
import torch.nn as nn

from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.checkpoint.state_dict import get_state_dict, set_state_dict
from torch.distributed.checkpoint.stateful import Stateful
from torch.distributed.fsdp.fully_sharded_data_parallel import StateDictType
from torch.distributed.checkpoint import StorageWriter

CHECKPOINT_DIR = "checkpoint"


class AppState(Stateful):
    def __init__(self, model, optimizer=None):
        self.model = model
        self.optimizer = optimizer

    def state_dict(self):
        model_state_dict, optimizer_state_dict = get_state_dict(model, optimizer)
        return {
            "model": model_state_dict,
            "optim": optimizer_state_dict
        }

    def load_state_dict(self, state_dict):
        set_state_dict(
            self.model,
            self.optimizer,
            model_state_dict=state_dict["model"],
            optim_state_dict=state_dict["optim"]
        )

class ToyModel(nn.Module):
    def __init__(self):
        super(ToyModel, self).__init__()
        self.net1 = nn.Linear(16, 16)
        self.relu = nn.ReLU()
        self.net2 = nn.Linear(16, 8)

    def forward(self, x):
        return self.net2(self.relu(self.net1(x)))


def setup(rank, world_size):
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "12355 "
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)


def cleanup():
    dist.destroy_process_group()


def run_fsdp_checkpoint_save_example(rank, world_size):
    print(f"Running basic FSDP checkpoint saving example on rank {rank}.")
    setup(rank, world_size)

    model = ToyModel().to(rank)
    model = FSDP(model)

    loss_fn = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

    writer = StorageWriter(cached_state_dict=True)
    checkpoint_future = None
    for step in range(10):
        optimizer.zero_grad()
        model(torch.rand(8, 16, device="cuda")).sum().backward()
        optimizer.step()

        state_dict = { "app": AppState(model, optimizer) }
        if checkpoint_future is not None:
            checkpoint_future.result()
        dcp.async_save(state_dict, storage_writer=writer, checkpoint_id=f"{CHECKPOINT_DIR}_step{step}")

    cleanup()


if __name__ == "__main__":
    world_size = torch.cuda.device_count()
    print(f"Running fsdp checkpoint example on {world_size} devices.")
    mp.spawn(
        run_fsdp_checkpoint_save_example,
        args=(world_size,),
        nprocs=world_size,
        join=True,
    )

----------------------------------------

TITLE: Basic Compiled Autograd Usage with torch.compile
DESCRIPTION: Demonstrates the basic setup and usage of Compiled Autograd with torch.compile, including model initialization and training loop configuration.

LANGUAGE: python
CODE:
model = Model()
x = torch.randn(10)

torch._dynamo.config.compiled_autograd = True
@torch.compile
def train(model, x):
   loss = model(x).sum()
   loss.backward()

train(model, x)

----------------------------------------

TITLE: Implementing Neural Network with Numpy
DESCRIPTION: Basic implementation of a third-order polynomial network using numpy arrays for forward and backward passes. Demonstrates fundamental concepts before introducing PyTorch.

LANGUAGE: python
CODE:
# Code not directly visible in provided text

----------------------------------------

TITLE: Training ResNet50 with BF16 using IPEX Backend
DESCRIPTION: Shows how to implement BFloat16 training using IPEX backend. Includes data preparation, model optimization with BF16 dtype, and training loop implementation with automatic mixed precision.

LANGUAGE: python
CODE:
import torch
import torchvision

LR = 0.001
DOWNLOAD = True
DATA = 'datasets/cifar10/'

transform = torchvision.transforms.Compose([
  torchvision.transforms.Resize((224, 224)),
  torchvision.transforms.ToTensor(),
  torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
train_dataset = torchvision.datasets.CIFAR10(
  root=DATA,
  train=True,
  transform=transform,
  download=DOWNLOAD,
)
train_loader = torch.utils.data.DataLoader(
  dataset=train_dataset,
  batch_size=128
)

model = torchvision.models.resnet50()
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum=0.9)
model.train()

import intel_extension_for_pytorch as ipex

# Invoke the following API optionally, to apply frontend optimizations
model, optimizer = ipex.optimize(model, dtype=torch.bfloat16, optimizer=optimizer)

compile_model = torch.compile(model, backend="ipex")

with torch.cpu.amp.autocast():
    for batch_idx, (data, target) in enumerate(train_loader):
        optimizer.zero_grad()
        output = compile_model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

----------------------------------------

TITLE: Custom nn.Module Implementation
DESCRIPTION: Shows how to create custom neural network modules by subclassing nn.Module for the polynomial model.

LANGUAGE: python
CODE:
# Code not directly visible in provided text

----------------------------------------

TITLE: Implementing FSDP Main Function with Auto-Wrap Policy
DESCRIPTION: This function sets up the FSDP training environment, including data loading, model initialization with FSDP wrapping, and the training loop. It demonstrates how to use auto_wrap_policy for efficient model sharding.

LANGUAGE: python
CODE:
def fsdp_main(rank, world_size, args):
    setup(rank, world_size)

    transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])

    dataset1 = datasets.MNIST('../data', train=True, download=True,
                        transform=transform)
    dataset2 = datasets.MNIST('../data', train=False,
                        transform=transform)

    sampler1 = DistributedSampler(dataset1, rank=rank, num_replicas=world_size, shuffle=True)
    sampler2 = DistributedSampler(dataset2, rank=rank, num_replicas=world_size)

    train_kwargs = {'batch_size': args.batch_size, 'sampler': sampler1}
    test_kwargs = {'batch_size': args.test_batch_size, 'sampler': sampler2}
    cuda_kwargs = {'num_workers': 2,
                    'pin_memory': True,
                    'shuffle': False}
    train_kwargs.update(cuda_kwargs)
    test_kwargs.update(cuda_kwargs)

    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)
    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)
    my_auto_wrap_policy = functools.partial(
        size_based_auto_wrap_policy, min_num_params=100
    )
    torch.cuda.set_device(rank)
    
    model = Net().to(rank)

    model = FSDP(model, auto_wrap_policy=my_auto_wrap_policy)

    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)

    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)
    for epoch in range(1, args.epochs + 1):
        train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=sampler1)
        test(model, rank, world_size, test_loader)
        scheduler.step()

    if args.save_model:
        dist.barrier()
        states = model.state_dict()
        if rank == 0:
            torch.save(states, "mnist_cnn.pt")
    
    cleanup()

----------------------------------------

TITLE: Implementing Custom Square Function with Double Backward in PyTorch
DESCRIPTION: This snippet demonstrates a simple custom autograd function that squares its input and supports double backward. It saves the input tensor for backward computation and uses gradcheck and gradgradcheck for verification.

LANGUAGE: python
CODE:
import torch

class Square(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x):
        # Because we are saving one of the inputs use `save_for_backward`
        # Save non-tensors and non-inputs/non-outputs directly on ctx
        ctx.save_for_backward(x)
        return x**2

    @staticmethod
    def backward(ctx, grad_out):
        # A function support double backward automatically if autograd
        # is able to record the computations performed in backward
        x, = ctx.saved_tensors
        return grad_out * 2 * x

# Use double precision because finite differencing method magnifies errors
x = torch.rand(3, 3, requires_grad=True, dtype=torch.double)
torch.autograd.gradcheck(Square.apply, x)
# Use gradcheck to verify second-order derivatives
torch.autograd.gradgradcheck(Square.apply, x)

----------------------------------------

TITLE: Working with Datasets and DataLoaders in PyTorch
DESCRIPTION: Demonstrates how to use PyTorch's data utilities, including Dataset classes and DataLoader for efficient data handling and batch processing during model training.

LANGUAGE: python
CODE:
Dataset
TensorDataset
Concat Dataset

DataLoader(dataset, batch_size=1, ...)
sampler.Sampler(dataset,...)
sampler.XSampler

----------------------------------------

TITLE: Converting PyTorch Model to TorchScript via Tracing
DESCRIPTION: Demonstrates how to convert a PyTorch model to TorchScript using tracing by evaluating it once with example inputs. Uses ResNet18 as an example.

LANGUAGE: python
CODE:
import torch
import torchvision

# An instance of your model.
model = torchvision.models.resnet18()

# An example input you would normally provide to your model's forward() method.
example = torch.rand(1, 3, 224, 224)

# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.
traced_script_module = torch.jit.trace(model, example)

----------------------------------------

TITLE: Implementing Quantization-Aware Training in PyTorch
DESCRIPTION: Shows how to perform quantization-aware training, including model preparation, training loop modifications, and evaluation of the quantized model.

LANGUAGE: python
CODE:
qat_model = load_model(saved_model_dir + float_model_file)
qat_model.fuse_model(is_qat=True)

optimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)
qat_model.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')

torch.ao.quantization.prepare_qat(qat_model, inplace=True)

num_train_batches = 20

for nepoch in range(8):
    train_one_epoch(qat_model, criterion, optimizer, data_loader, torch.device('cpu'), num_train_batches)
    if nepoch > 3:
        qat_model.apply(torch.ao.quantization.disable_observer)
    if nepoch > 2:
        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)

    quantized_model = torch.ao.quantization.convert(qat_model.eval(), inplace=False)
    quantized_model.eval()
    top1, top5 = evaluate(quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches)
    print('Epoch %d :Evaluation accuracy on %d images, %2.2f'%(nepoch, num_eval_batches * eval_batch_size, top1.avg))

----------------------------------------

TITLE: Training Loop with TensorBoard Logging
DESCRIPTION: Implements the training loop with integrated TensorBoard logging for loss tracking and prediction visualization.

LANGUAGE: python
CODE:
running_loss = 0.0
for epoch in range(1):
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if i % 1000 == 999:
            writer.add_scalar('training loss',
                            running_loss / 1000,
                            epoch * len(trainloader) + i)
            writer.add_figure('predictions vs. actuals',
                            plot_classes_preds(net, inputs, labels),
                            global_step=epoch * len(trainloader) + i)
            running_loss = 0.0

----------------------------------------

TITLE: Compiling and Benchmarking Adam Optimizer in PyTorch
DESCRIPTION: Sets up the Adam optimizer, compiles the step function, and benchmarks the performance of both eager and compiled versions.

LANGUAGE: python
CODE:
# exit cleanly if we are on a device that doesn't support torch.compile
if torch.cuda.get_device_capability() < (7, 0):
    print("Exiting because torch.compile is not supported on this device.")
    import sys
    sys.exit(0)


opt = torch.optim.Adam(model.parameters(), lr=0.01)


@torch.compile(fullgraph=False)
def fn():
    opt.step()


# Let's define a helpful benchmarking function:
import torch.utils.benchmark as benchmark


def benchmark_torch_function_in_microseconds(f, *args, **kwargs):
    t0 = benchmark.Timer(
        stmt="f(*args, **kwargs)", globals={"args": args, "kwargs": kwargs, "f": f}
    )
    return t0.blocked_autorange().mean * 1e6


# Warmup runs to compile the function
for _ in range(5):
    fn()

eager_runtime = benchmark_torch_function_in_microseconds(opt.step)
compiled_runtime = benchmark_torch_function_in_microseconds(fn)

assert eager_runtime > compiled_runtime

print(f"eager runtime: {eager_runtime}us")
print(f"compiled runtime: {compiled_runtime}us")

----------------------------------------

TITLE: Initializing Basic Neural Network Model in PyTorch
DESCRIPTION: Defines a simple neural network model with a single linear layer that processes 10-dimensional input vectors.

LANGUAGE: python
CODE:
import torch

class Model(torch.nn.Module):
   def __init__(self):
      super().__init__()
      self.linear = torch.nn.Linear(10, 10)

   def forward(self, x):
      return self.linear(x)

----------------------------------------

TITLE: Implementing Transformer Model in Python
DESCRIPTION: Defines a basic transformer model architecture with embedding layers, transformer decoder layers, layer normalization, and output projection. Uses dataclass for model configuration.

LANGUAGE: python
CODE:
import torch
import torch.nn as nn
from dataclasses import dataclass

@dataclass
class ModelArgs:
   dim: int = 512
   n_layers: int = 8
   n_heads: int = 8
   vocab_size: int = 10000

class Transformer(nn.Module):
   def __init__(self, model_args: ModelArgs):
      super().__init__()

      self.tok_embeddings = nn.Embedding(model_args.vocab_size, model_args.dim)

      # Using a ModuleDict lets us delete layers witout affecting names,
      # ensuring checkpoints will correctly save and load.
      self.layers = torch.nn.ModuleDict()
      for layer_id in range(model_args.n_layers):
            self.layers[str(layer_id)] = nn.TransformerDecoderLayer(model_args.dim, model_args.n_heads)

      self.norm = nn.LayerNorm(model_args.dim)
      self.output = nn.Linear(model_args.dim, model_args.vocab_size)

   def forward(self, tokens: torch.Tensor):
      # Handling layers being 'None' at runtime enables easy pipeline splitting
      h = self.tok_embeddings(tokens) if self.tok_embeddings else tokens

      for layer in self.layers.values():
            h = layer(h, h)

      h = self.norm(h) if self.norm else h
      output = self.output(h).clone() if self.output else h
      return output

----------------------------------------

TITLE: Loading TorchScript Model in C++
DESCRIPTION: Minimal C++ application that loads and executes a serialized TorchScript model using LibTorch.

LANGUAGE: cpp
CODE:
#include <torch/script.h>
#include <iostream>
#include <memory>

int main(int argc, const char* argv[]) {
  if (argc != 2) {
    std::cerr << "usage: example-app <path-to-exported-script-module>\n";
    return -1;
  }

  torch::jit::script::Module module;
  try {
    module = torch::jit::load(argv[1]);
  }
  catch (const c10::Error& e) {
    std::cerr << "error loading the model\n";
    return -1;
  }

  std::cout << "ok\n";
}

----------------------------------------

TITLE: Basic PyTorch C++ Application
DESCRIPTION: A minimal C++ application that demonstrates tensor creation using the PyTorch C++ frontend.

LANGUAGE: cpp
CODE:
#include <torch/torch.h>
#include <iostream>

int main() {
  torch::Tensor tensor = torch::eye(3);
  std::cout << tensor << std::endl;
}

----------------------------------------

TITLE: Implementing LLTM Forward Pass in Python/PyTorch
DESCRIPTION: Pure Python implementation of the LLTM forward pass using PyTorch operations, subclassing torch.nn.Module.

LANGUAGE: python
CODE:
class LLTM(torch.nn.Module):
    def __init__(self, input_features, state_size):
        super(LLTM, self).__init__()
        self.input_features = input_features
        self.state_size = state_size
        self.weights = torch.nn.Parameter(
            torch.empty(3 * state_size, input_features + state_size))
        self.bias = torch.nn.Parameter(torch.empty(3 * state_size))
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.state_size)
        for weight in self.parameters():
            weight.data.uniform_(-stdv, +stdv)

    def forward(self, input, state):
        old_h, old_cell = state
        X = torch.cat([old_h, input], dim=1)
        gate_weights = F.linear(X, self.weights, self.bias)
        gates = gate_weights.chunk(3, dim=1)
        input_gate = torch.sigmoid(gates[0])
        output_gate = torch.sigmoid(gates[1])
        candidate_cell = F.elu(gates[2])
        new_cell = old_cell + candidate_cell * input_gate
        new_h = torch.tanh(new_cell) * output_gate
        return new_h, new_cell

----------------------------------------

TITLE: Fusing and Preparing PyTorch Models
DESCRIPTION: Demonstrates how to create both fused and non-fused versions of a model, applying quantization configurations and optimization for mobile deployment.

LANGUAGE: python
CODE:
model = AnnotatedConvBnReLUModel()
print(model)

def prepare_save(model, fused):
    model.qconfig = torch.quantization.get_default_qconfig('qnnpack')
    torch.quantization.prepare(model, inplace=True)
    torch.quantization.convert(model, inplace=True)
    torchscript_model = torch.jit.script(model)
    torchscript_model_optimized = optimize_for_mobile(torchscript_model)
    torch.jit.save(torchscript_model_optimized, "model.pt" if not fused else "model_fused.pt")

prepare_save(model, False)

model = AnnotatedConvBnReLUModel()
model_fused = torch.quantization.fuse_modules(model, [['bn', 'relu']], inplace=False)
print(model_fused)

prepare_save(model_fused, True)

----------------------------------------

TITLE: Image Preprocessing for PyTorch Model
DESCRIPTION: Transforms input images to the format required by the PyTorch model, including resizing, cropping, tensor conversion, and normalization for ImageNet models.

LANGUAGE: python
CODE:
def transform_image(infile):
    input_transforms = [transforms.Resize(255),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],
            [0.229, 0.224, 0.225])]
    my_transforms = transforms.Compose(input_transforms)
    image = Image.open(infile)
    timg = my_transforms(image)
    timg.unsqueeze_(0)
    return timg

----------------------------------------

TITLE: Preparing Model for Quantization-Aware Training
DESCRIPTION: This code prepares the exported model for quantization-aware training by inserting fake quantizes and performing QAT fusions.

LANGUAGE: python
CODE:
prepared_model = prepare_qat_pt2e(exported_model, quantizer)
print(prepared_model)

----------------------------------------

TITLE: Installing PyTorch Dependencies in Google Colab
DESCRIPTION: Commands to uninstall existing PyTorch packages and install the latest versions of PyTorch and related libraries.

LANGUAGE: python
CODE:
!pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata
!pip3 install torch torchaudio torchvision torchtext torchdata

----------------------------------------

TITLE: Using Pre-trained Quantized MobileNet v2 in PyTorch
DESCRIPTION: Demonstrates how to load a pre-trained quantized MobileNet v2 model using torchvision and compare its size with a non-quantized version.

LANGUAGE: python
CODE:
import torchvision
model_quantized = torchvision.models.quantization.mobilenet_v2(pretrained=True, quantize=True)

model = torchvision.models.mobilenet_v2(pretrained=True)

import os
import torch

def print_model_size(mdl):
    torch.save(mdl.state_dict(), "tmp.pt")
    print("%.2f MB" %(os.path.getsize("tmp.pt")/1e6))
    os.remove('tmp.pt')

print_model_size(model)
print_model_size(model_quantized)

----------------------------------------

TITLE: Setting up PyTorch C++/CUDA Extension
DESCRIPTION: Setup script for building the CUDA extension using setuptools and CUDAExtension.

LANGUAGE: python
CODE:
from setuptools import setup
from torch.utils.cpp_extension import BuildExtension, CUDAExtension

setup(
    name='lltm',
    ext_modules=[
        CUDAExtension('lltm_cuda', [
            'lltm_cuda.cpp',
            'lltm_cuda_kernel.cu',
        ])
    ],
    cmdclass={
        'build_ext': BuildExtension
    })

----------------------------------------

TITLE: Preparing a Quantizable Model for Fine-tuning in PyTorch
DESCRIPTION: Prepares a pre-trained ResNet-18 model for quantization-aware training by fusing modules and setting up quantization configurations.

LANGUAGE: python
CODE:
model = models.resnet18(pretrained=True, progress=True, quantize=False)
num_ftrs = model.fc.in_features

model.train()
model.fuse_model()
model_ft = create_combined_model(model)
model_ft[0].qconfig = torch.quantization.default_qat_qconfig
model_ft = torch.quantization.prepare_qat(model_ft, inplace=True)

----------------------------------------

TITLE: Implementing Batch-Updating Parameter Server Class
DESCRIPTION: Implementation of a parameter server class that handles batch updates from multiple trainers using async_execution decorator. The server accumulates gradients and updates model parameters when batch size is reached.

LANGUAGE: python
CODE:
class BatchUpdateParameterServer(object):
    def __init__(self, batch_update_size=batch_update_size):
        self.model = torchvision.models.resnet50(num_classes=num_classes)
        self.lock = threading.Lock()
        self.future_model = torch.futures.Future()
        self.batch_update_size = batch_update_size
        self.curr_update_size = 0
        self.optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)
        for p in self.model.parameters():
            p.grad = torch.zeros_like(p)

    def get_model(self):
        return self.model

    @staticmethod
    @rpc.functions.async_execution
    def update_and_fetch_model(ps_rref, grads):
        # Using the RRef to retrieve the local PS instance
        self = ps_rref.local_value()
        with self.lock:
            self.curr_update_size += 1
            # accumulate gradients into .grad field
            for p, g in zip(self.model.parameters(), grads):
                p.grad += g

            fut = self.future_model

            if self.curr_update_size >= self.batch_update_size:
                # update the model
                for p in self.model.parameters():
                    p.grad /= self.batch_update_size
                self.curr_update_size = 0
                self.optimizer.step()
                self.optimizer.zero_grad()
                fut.set_result(self.model)
                self.future_model = torch.futures.Future()

        return fut

----------------------------------------

TITLE: Saving and Loading Compile Cache Artifacts in PyTorch
DESCRIPTION: This snippet demonstrates how to compile a function, execute it, save the cache artifacts, and later load them to pre-populate the torch.compile caches. It uses torch.compiler.save_cache_artifacts() and torch.compiler.load_cache_artifacts() functions.

LANGUAGE: python
CODE:
@torch.compile
def fn(x, y):
    return x.sin() @ y

a = torch.rand(100, 100, dtype=dtype, device=device)
b = torch.rand(100, 100, dtype=dtype, device=device)

result = fn(a, b)

artifacts = torch.compiler.save_cache_artifacts()

assert artifacts is not None
artifact_bytes, cache_info = artifacts

# Now, potentially store artifact_bytes in a database
# You can use cache_info for logging

LANGUAGE: python
CODE:
# Potentially download/fetch the artifacts from the database
torch.compiler.load_cache_artifacts(artifact_bytes)

----------------------------------------

TITLE: Installing Intel Neural Compressor
DESCRIPTION: Commands for installing Intel Neural Compressor through pip or conda. Supports Python versions 3.6-3.9.

LANGUAGE: bash
CODE:
# install stable version from pip
pip install neural-compressor

# install nightly version from pip
pip install -i https://test.pypi.org/simple/ neural-compressor

# install stable version from from conda
conda install neural-compressor -c conda-forge -c intel

----------------------------------------

TITLE: Implementing Ring-Allreduce in PyTorch
DESCRIPTION: Demonstrates a custom implementation of ring-allreduce using point-to-point communication primitives.

LANGUAGE: python
CODE:
""" Implementation of a ring-reduce with addition. """
def allreduce(send, recv):
   rank = dist.get_rank()
   size = dist.get_world_size()
   send_buff = send.clone()
   recv_buff = send.clone()
   accum = send.clone()

   left = ((rank - 1) + size) % size
   right = (rank + 1) % size

   for i in range(size - 1):
       if i % 2 == 0:
           # Send send_buff
           send_req = dist.isend(send_buff, right)
           dist.recv(recv_buff, left)
           accum[:] += recv_buff[:]
       else:
           # Send recv_buff
           send_req = dist.isend(recv_buff, right)
           dist.recv(send_buff, left)
           accum[:] += send_buff[:]
       send_req.wait()
   recv[:] = accum[:]

----------------------------------------

TITLE: Exporting PyTorch Model to TorchScript
DESCRIPTION: Demonstrates how to export a pretrained ResNet18 model to TorchScript format and verify its output matches the original model. Uses torchvision and torch libraries.

LANGUAGE: python
CODE:
import torch
import torch.nn.functional as F
import torchvision.models as models

r18 = models.resnet18(pretrained=True)       # We now have an instance of the pretrained model
r18_scripted = torch.jit.script(r18)         # *** This is the TorchScript export
dummy_input = torch.rand(1, 3, 224, 224)     # We should run a quick test

----------------------------------------

TITLE: Implementing Quantization-Aware Training
DESCRIPTION: Example of quantization-aware training implementation including data loading, model training, and quantization process.

LANGUAGE: python
CODE:
model.eval()

from torchvision import datasets, transforms
train_loader = torch.utils.data.DataLoader(
    datasets.MNIST('./data', train=True, download=True,
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                   ])),
    batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(
    datasets.MNIST('./data', train=False, download=True,
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                   ])),
    batch_size=1)

import torch.optim as optim
optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.1)

def training_func(model):
    model.train()
    for epoch in range(1, 3):
        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad()
            output = model(data)
            loss = F.nll_loss(output, target)
            loss.backward()
            optimizer.step()
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                  epoch, batch_idx * len(data), len(train_loader.dataset),
                  100. * batch_idx / len(train_loader), loss.item()))

from neural_compressor.experimental import Quantization
quantizer = Quantization("./conf.yaml")
quantizer.model = model
quantizer.q_func = training_func
quantizer.eval_dataloader = test_loader
q_model = quantizer()
q_model.save('./output')

----------------------------------------

TITLE: Implementing PyTorch Model with Fusion-Compatible Modules
DESCRIPTION: Defines a PyTorch model class that includes Conv2d, BatchNorm2d, and ReLU layers that can be fused together. The model also includes quantization stubs for later optimization.

LANGUAGE: python
CODE:
import torch
from torch.utils.mobile_optimizer import optimize_for_mobile

class AnnotatedConvBnReLUModel(torch.nn.Module):
    def __init__(self):
        super(AnnotatedConvBnReLUModel, self).__init__()
        self.conv = torch.nn.Conv2d(3, 5, 3, bias=False).to(dtype=torch.float)
        self.bn = torch.nn.BatchNorm2d(5).to(dtype=torch.float)
        self.relu = torch.nn.ReLU(inplace=True)
        self.quant = torch.quantization.QuantStub()
        self.dequant = torch.quantization.DeQuantStub()

    def forward(self, x):
        x = x.contiguous(memory_format=torch.channels_last)
        x = self.quant(x)
        x = self.conv(x)
        x = self.bn(x)
        x = self.relu(x)
        x = self.dequant(x)
        return x

----------------------------------------

TITLE: Training with Float32 on CPU using Intel® Extension for PyTorch*
DESCRIPTION: This snippet demonstrates how to use Intel® Extension for PyTorch* for training a ResNet50 model on CPU with Float32 precision. It includes data loading, model creation, and the training loop with optimization.

LANGUAGE: python
CODE:
import torch
import torchvision
import intel_extension_for_pytorch as ipex

LR = 0.001
DOWNLOAD = True
DATA = 'datasets/cifar10/'

transform = torchvision.transforms.Compose([
    torchvision.transforms.Resize((224, 224)),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
train_dataset = torchvision.datasets.CIFAR10(
        root=DATA,
        train=True,
        transform=transform,
        download=DOWNLOAD,
)
train_loader = torch.utils.data.DataLoader(
        dataset=train_dataset,
        batch_size=128
)

model = torchvision.models.resnet50()
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum=0.9)
model.train()
model, optimizer = ipex.optimize(model, optimizer=optimizer)

for batch_idx, (data, target) in enumerate(train_loader):
    optimizer.zero_grad()
    output = model(data)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()
    print(batch_idx)
torch.save({
     'model_state_dict': model.state_dict(),
     'optimizer_state_dict': optimizer.state_dict(),
     }, 'checkpoint.pth')

----------------------------------------

TITLE: Importing Required Packages for FSDP in PyTorch
DESCRIPTION: This code snippet imports the necessary packages for using FSDP in PyTorch, including torch, torchvision, and specific FSDP-related modules.

LANGUAGE: python
CODE:
import os
import argparse
import functools
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms

from torch.optim.lr_scheduler import StepLR

import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp.fully_sharded_data_parallel import (
    CPUOffload,
    BackwardPrefetch,
)
from torch.distributed.fsdp.wrap import (
    size_based_auto_wrap_policy,
    enable_wrap,
    wrap,
)

----------------------------------------

TITLE: Model Calibration and Conversion
DESCRIPTION: Shows the process of calibrating the prepared model using sample data and converting it to a quantized model.

LANGUAGE: python
CODE:
def calibrate(model, data_loader):
    model.eval()
    with torch.no_grad():
        for image, target in data_loader:
            model(image)
calibrate(prepared_model, data_loader_test)

quantized_model = convert_pt2e(prepared_model)

----------------------------------------

TITLE: Basic Autograd Operations in C++
DESCRIPTION: Demonstrates fundamental autograd operations including tensor creation with gradients, basic operations, and backpropagation

LANGUAGE: cpp
CODE:
auto x = torch::ones({2, 2}, torch::requires_grad());
auto y = x + 2;
auto z = y * y * 3;
auto out = z.mean();
out.backward();

----------------------------------------

TITLE: Defining MNIST ConvNet Model in Python
DESCRIPTION: Defines a convolutional neural network model for MNIST digit classification that can utilize multiple GPUs.

LANGUAGE: python
CODE:
class Net(nn.Module):
    def __init__(self, num_gpus=0):
        super(Net, self).__init__()
        print(f"Using {num_gpus} GPUs to train")
        self.num_gpus = num_gpus
        device = torch.device(
            "cuda:0" if torch.cuda.is_available() and self.num_gpus > 0 else "cpu")
        print(f"Putting first 2 convs on {str(device)}")
        # Put conv layers on the first cuda device, or CPU if no cuda device
        self.conv1 = nn.Conv2d(1, 32, 3, 1).to(device)
        self.conv2 = nn.Conv2d(32, 64, 3, 1).to(device)
        # Put rest of the network on the 2nd cuda device, if there is one
        if "cuda" in str(device) and num_gpus > 1:
            device = torch.device("cuda:1")

        print(f"Putting rest of layers on {str(device)}")
        self.dropout1 = nn.Dropout2d(0.25).to(device)
        self.dropout2 = nn.Dropout2d(0.5).to(device)
        self.fc1 = nn.Linear(9216, 128).to(device)
        self.fc2 = nn.Linear(128, 10).to(device)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.max_pool2d(x, 2)

        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        # Move tensor to next device if necessary
        next_device = next(self.fc1.parameters()).device
        x = x.to(next_device)

        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output

----------------------------------------

TITLE: Implementing Custom C++ Class Registration
DESCRIPTION: Example of implementing a custom C++ class MyStackClass that inherits from torch::CustomClassHolder and registering it with TorchScript using the TORCH_LIBRARY macro.

LANGUAGE: cpp
CODE:
TORCH_LIBRARY(my_classes, m) {
  m.class_<MyStackClass>("MyStackClass")
      .def(torch::init<std::vector<std::string>>())
      .def("pop", &MyStackClass::pop)
      .def("push", &MyStackClass::push)
      .def("top", &MyStackClass::top);
}

----------------------------------------

TITLE: Defining HybridModel with Remote Embedding and DDP in PyTorch
DESCRIPTION: This class defines a hybrid model that combines a remote embedding module with a local FC layer wrapped in DistributedDataParallel. The forward method performs embedding lookup on the parameter server and passes the result through the FC layer.

LANGUAGE: Python
CODE:
class HybridModel(nn.Module):
    def __init__(self, remote_emb_module, device):
        super().__init__()
        self.remote_emb_module = remote_emb_module
        self.fc = DDP(nn.Linear(16, 8).cuda(device), device_ids=[device])

    def forward(self, indices, offsets):
        emb_lookup = self.remote_emb_module(indices, offsets)
        return self.fc(emb_lookup.cuda(self.fc.device))

----------------------------------------

TITLE: Using Custom Operator in TorchScript
DESCRIPTION: Python code demonstrating how to use the custom warp_perspective operator in a TorchScript function.

LANGUAGE: Python
CODE:
@torch.jit.script
def compute(x, y):
  if bool(x[0][0] == 42):
      z = 5
  else:
      z = 10
  x = torch.ops.my_ops.warp_perspective(x, torch.eye(3))
  return x.matmul(y) + z

compute.save("example.pt")

----------------------------------------

TITLE: Implementing Custom Sinh Function with Intermediate Results in PyTorch
DESCRIPTION: This snippet demonstrates a custom autograd function for the hyperbolic sine (sinh) that supports double backward. It saves intermediate results as additional outputs to ensure proper gradient computation.

LANGUAGE: python
CODE:
class Sinh(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x):
        expx = torch.exp(x)
        expnegx = torch.exp(-x)
        ctx.save_for_backward(expx, expnegx)
        # In order to be able to save the intermediate results, a trick is to
        # include them as our outputs, so that the backward graph is constructed
        return (expx - expnegx) / 2, expx, expnegx

    @staticmethod
    def backward(ctx, grad_out, _grad_out_exp, _grad_out_negexp):
        expx, expnegx = ctx.saved_tensors
        grad_input = grad_out * (expx + expnegx) / 2
        # We cannot skip accumulating these even though we won't use the outputs
        # directly. They will be used later in the second backward.
        grad_input += _grad_out_exp * expx
        grad_input -= _grad_out_negexp * expnegx
        return grad_input

def sinh(x):
    # Create a wrapper that only returns the first output
    return Sinh.apply(x)[0]

x = torch.rand(3, 3, requires_grad=True, dtype=torch.double)
torch.autograd.gradcheck(sinh, x)
torch.autograd.gradgradcheck(sinh, x)

----------------------------------------

TITLE: Initializing FX Graph Mode Quantization Pipeline in PyTorch
DESCRIPTION: Sets up the basic quantization pipeline using the FX graph mode API, including preparing the model, calibrating, and converting to a quantized version.

LANGUAGE: python
CODE:
import torch
from torch.ao.quantization import get_default_qconfig
from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx
from torch.ao.quantization import QConfigMapping
float_model.eval()
# The old 'fbgemm' is still available but 'x86' is the recommended default.
qconfig = get_default_qconfig("x86") 
qconfig_mapping = QConfigMapping().set_global(qconfig)
def calibrate(model, data_loader):
    model.eval()
    with torch.no_grad():
        for image, target in data_loader:
            model(image)
example_inputs = (next(iter(data_loader))[0]) # get an example input
prepared_model = prepare_fx(float_model, qconfig_mapping, example_inputs)  # fuse modules and insert observers
calibrate(prepared_model, data_loader_test)  # run calibration on sample data
quantized_model = convert_fx(prepared_model)  # convert the calibrated model to a quantized model

----------------------------------------

TITLE: Using CommDebugMode with PyTorch DistributedTensor
DESCRIPTION: This snippet demonstrates how to use CommDebugMode to track collective operations in a distributed training environment. It shows how to wrap the model execution, print debug information, log to a file, and generate a JSON dump for visualization.

LANGUAGE: python
CODE:
# The model used in this example is a MLPModule applying Tensor Parallel
comm_mode = CommDebugMode()
    with comm_mode:
        output = model(inp)

# print the operation level collective tracing information
print(comm_mode.generate_comm_debug_tracing_table(noise_level=0))

# log the operation level collective tracing information to a file
comm_mode.log_comm_debug_tracing_table_to_file(
    noise_level=1, file_name="transformer_operation_log.txt"
)

# dump the operation level collective tracing information to json file,
# used in the visual browser below
comm_mode.generate_json_dump(noise_level=2)

----------------------------------------

TITLE: Quantizing BERT Model with Graph Mode
DESCRIPTION: Applies dynamic quantization to the traced BERT model using the one-line quantize_dynamic_jit API.

LANGUAGE: python
CODE:
quantized_model = quantize_dynamic_jit(traced_model, qconfig_dict)

----------------------------------------

TITLE: CPU Implementation of Custom Operator
DESCRIPTION: C++ implementation of multiply-add operation for CPU backend

LANGUAGE: cpp
CODE:
at::Tensor mymuladd_cpu(at::Tensor a, const at::Tensor& b, double c) {
  TORCH_CHECK(a.sizes() == b.sizes());
  TORCH_CHECK(a.dtype() == at::kFloat);
  TORCH_CHECK(b.dtype() == at::kFloat);
  TORCH_INTERNAL_ASSERT(a.device().type() == at::DeviceType::CPU);
  TORCH_INTERNAL_ASSERT(b.device().type() == at::DeviceType::CPU);
  at::Tensor a_contig = a.contiguous();
  at::Tensor b_contig = b.contiguous();
  at::Tensor result = torch::empty(a_contig.sizes(), a_contig.options());
  const float* a_ptr = a_contig.data_ptr<float>();
  const float* b_ptr = b_contig.data_ptr<float>();
  float* result_ptr = result.data_ptr<float>();
  for (int64_t i = 0; i < result.numel(); i++) {
    result_ptr[i] = a_ptr[i] * b_ptr[i] + c;
  }
  return result;
}

----------------------------------------

TITLE: Implementing Dataset Partitioning for Distributed Training in PyTorch
DESCRIPTION: Creates helper classes to partition a dataset across multiple processes for distributed training.

LANGUAGE: python
CODE:
""" Dataset partitioning helper """
class Partition(object):

    def __init__(self, data, index):
        self.data = data
        self.index = index

    def __len__(self):
        return len(self.index)

    def __getitem__(self, index):
        data_idx = self.index[index]
        return self.data[data_idx]


class DataPartitioner(object):

    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):
        self.data = data
        self.partitions = []
        rng = Random()  # from random import Random
        rng.seed(seed)
        data_len = len(data)
        indexes = [x for x in range(0, data_len)]
        rng.shuffle(indexes)

        for frac in sizes:
            part_len = int(frac * data_len)
            self.partitions.append(indexes[0:part_len])
            indexes = indexes[part_len:]

    def use(self, partition):
        return Partition(self.data, self.partitions[partition])

----------------------------------------

TITLE: Configuring XNNPACKQuantizer for QAT
DESCRIPTION: This snippet shows how to import and configure the XNNPACKQuantizer for quantization-aware training, setting up symmetric quantization.

LANGUAGE: python
CODE:
from torch.ao.quantization.quantizer.xnnpack_quantizer import (
    XNNPACKQuantizer,
    get_symmetric_quantization_config,
)
quantizer = XNNPACKQuantizer()
quantizer.set_global(get_symmetric_quantization_config(is_qat=True))

----------------------------------------

TITLE: Training Loop for Distributed RNN using Distributed Autograd and Optimizer
DESCRIPTION: Implements a training loop for the distributed RNN model using distributed autograd for backpropagation and a distributed optimizer for parameter updates.

LANGUAGE: python
CODE:
def run_trainer():
    batch = 5
    ntoken = 10
    ninp = 2
    nhid = 3
    nindices = 3
    nlayers = 4
    hidden = (
        torch.randn(nlayers, nindices, nhid),
        torch.randn(nlayers, nindices, nhid)
    )

    model = rnn.RNNModel('ps', ntoken, ninp, nhid, nlayers)

    opt = DistributedOptimizer(
        optim.SGD,
        model.parameter_rrefs(),
        lr=0.05,
    )

    criterion = torch.nn.CrossEntropyLoss()

    def get_next_batch():
        for _ in range(5):
            data = torch.LongTensor(batch, nindices) % ntoken
            target = torch.LongTensor(batch, ntoken) % nindices
            yield data, target

    for epoch in range(10):
        for data, target in get_next_batch():
            with dist_autograd.context() as context_id:
                hidden[0].detach_()
                hidden[1].detach_()
                output, hidden = model(data, hidden)
                loss = criterion(output, target)
                dist_autograd.backward(context_id, [loss])
                opt.step(context_id)
        print(f"Training epoch {epoch}")

----------------------------------------

TITLE: Implementing Distributed Neural Network with CUDA RPC in PyTorch
DESCRIPTION: Complete implementation showing how to use CUDA RPC for direct device-to-device tensor transfer in a distributed neural network setting. The code demonstrates both CPU and CUDA RPC modes with performance measurements, using TensorPipe as the backend.

LANGUAGE: python
CODE:
import torch
import torch.distributed.autograd as autograd
import torch.distributed.rpc as rpc
import torch.multiprocessing as mp
import torch.nn as nn

import os
import time


class MyModule(nn.Module):
    def __init__(self, device, comm_mode):
        super().__init__()
        self.device = device
        self.linear = nn.Linear(1000, 1000).to(device)
        self.comm_mode = comm_mode

    def forward(self, x):
        # x.to() is a no-op if x is already on self.device
        y = self.linear(x.to(self.device))
        return y.cpu() if self.comm_mode == "cpu" else y

    def parameter_rrefs(self):
        return [rpc.RRef(p) for p in self.parameters()]


def measure(comm_mode):
    # local module on "worker0/cuda:0"
    lm = MyModule("cuda:0", comm_mode)
    # remote module on "worker1/cuda:1"
    rm = rpc.remote("worker1", MyModule, args=("cuda:1", comm_mode))
    # prepare random inputs
    x = torch.randn(1000, 1000).cuda(0)

    tik = time.time()
    for _ in range(10):
        with autograd.context() as ctx:
            y = rm.rpc_sync().forward(lm(x))
            autograd.backward(ctx, [y.sum()])
    # synchronize on "cuda:0" to make sure that all pending CUDA ops are
    # included in the measurements
    torch.cuda.current_stream("cuda:0").synchronize()
    tok = time.time()
    print(f"{comm_mode} RPC total execution time: {tok - tik}")


def run_worker(rank):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '29500'
    options = rpc.TensorPipeRpcBackendOptions(num_worker_threads=128)

    if rank == 0:
        options.set_device_map("worker1", {0: 1})
        rpc.init_rpc(
            f"worker{rank}",
            rank=rank,
            world_size=2,
            rpc_backend_options=options
        )
        measure(comm_mode="cpu")
        measure(comm_mode="cuda")
    else:
        rpc.init_rpc(
            f"worker{rank}",
            rank=rank,
            world_size=2,
            rpc_backend_options=options
        )

    # block until all rpcs finish
    rpc.shutdown()


if __name__=="__main__":
    world_size = 2
    mp.spawn(run_worker, nprocs=world_size, join=True)

----------------------------------------

TITLE: Implementing QAT Training Loop
DESCRIPTION: This snippet outlines the training loop for quantization-aware training, including disabling observers and freezing batch normalization after certain epochs.

LANGUAGE: python
CODE:
num_epochs = 10
num_train_batches = 20
num_eval_batches = 20
num_observer_update_epochs = 4
num_batch_norm_update_epochs = 3
num_epochs_between_evals = 2

for nepoch in range(num_epochs):
    train_one_epoch(prepared_model, criterion, optimizer, data_loader, "cuda", num_train_batches)

    if epoch >= num_observer_update_epochs:
        print("Disabling observer for subseq epochs, epoch = ", epoch)
        prepared_model.apply(torch.ao.quantization.disable_observer)
    if epoch >= num_batch_norm_update_epochs:
        print("Freezing BN for subseq epochs, epoch = ", epoch)
        for n in prepared_model.graph.nodes:
            if n.target in [
                torch.ops.aten._native_batch_norm_legit.default,
                torch.ops.aten.cudnn_batch_norm.default,
            ]:
                new_args = list(n.args)
                new_args[5] = False
                n.args = new_args
        prepared_model.recompile()

    if (nepoch + 1) % num_epochs_between_evals == 0:
        prepared_model_copy = copy.deepcopy(prepared_model)
        quantized_model = convert_pt2e(prepared_model_copy)
        top1, top5 = evaluate(quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches)
        print('Epoch %d: Evaluation accuracy on %d images, %2.2f' % (nepoch, num_eval_batches * eval_batch_size, top1.avg))

----------------------------------------

TITLE: Handling Graph Breaks with Compiled Autograd
DESCRIPTION: Demonstrates how Compiled Autograd handles graph breaks in the forward pass without breaking the backward graph.

LANGUAGE: python
CODE:
@torch.compile(backend="aot_eager")
def fn(x):
   # 1st graph
   temp = x + 10
   torch._dynamo.graph_break()
   # 2nd graph
   temp = temp + 10
   torch._dynamo.graph_break()
   # 3rd graph
   return temp.sum()

x = torch.randn(10, 10, requires_grad=True)
torch._dynamo.utils.counters.clear()
loss = fn(x)

# 1. base torch.compile 
loss.backward(retain_graph=True)
assert(torch._dynamo.utils.counters["stats"]["unique_graphs"] == 3)
torch._dynamo.utils.counters.clear()

# 2. torch.compile with compiled autograd
with torch._dynamo.compiled_autograd.enable(torch.compile(backend="aot_eager")):
   loss.backward()

# single graph for the backward
assert(torch._dynamo.utils.counters["stats"]["unique_graphs"] == 1)

----------------------------------------

TITLE: Implementing Parameter Server Class in Python
DESCRIPTION: Defines a ParameterServer class that hosts the model and responds to requests from trainers. It includes methods for forward pass and retrieving parameter references.

LANGUAGE: python
CODE:
class ParameterServer(nn.Module):
    def __init__(self, num_gpus=0):
        super().__init__()
        model = Net(num_gpus=num_gpus)
        self.model = model
        self.input_device = torch.device(
            "cuda:0" if torch.cuda.is_available() and num_gpus > 0 else "cpu")

    def forward(self, inp):
        inp = inp.to(self.input_device)
        out = self.model(inp)
        # This output is forwarded over RPC, which as of 1.5.0 only accepts CPU tensors.
        # Tensors must be moved in and out of GPU memory due to this.
        out = out.to("cpu")
        return out

    def get_dist_gradients(self, cid):
        grads = dist_autograd.get_gradients(cid)
        # This output is forwarded over RPC, which as of 1.5.0 only accepts CPU tensors.
        # Tensors must be moved in and out of GPU memory due to this.
        cpu_grads = {}
        for k, v in grads.items():
            k_cpu, v_cpu = k.to("cpu"), v.to("cpu")
            cpu_grads[k_cpu] = v_cpu
        return cpu_grads

    def get_param_rrefs(self):
        param_rrefs = [rpc.RRef(param) for param in self.model.parameters()]
        return param_rrefs

----------------------------------------

TITLE: Helper Functions and Dataset Preparation
DESCRIPTION: Defines utility functions for model evaluation, accuracy measurement, and data loading. Also sets up the ImageNet dataset and loads a pretrained ResNet18 model.

LANGUAGE: python
CODE:
import os
import sys
import time
import numpy as np

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

import torchvision
from torchvision import datasets
from torchvision.models.resnet import resnet18
import torchvision.transforms as transforms

# Helper class definitions and functions...

----------------------------------------

TITLE: Implementing Conv2d-ReLU Fusion Function
DESCRIPTION: Defines a fusion function for combining Conv2d and ReLU operations into a single ConvReLU2d module.

LANGUAGE: python
CODE:
def fuse_conv2d_relu(is_qat, conv, relu):
    """Return a fused ConvReLU2d from individual conv and relu modules."""
    return torch.ao.nn.intrinsic.ConvReLU2d(conv, relu)

----------------------------------------

TITLE: Real-Time Inference Loop with PyTorch on Raspberry Pi
DESCRIPTION: Implements a continuous loop for real-time inference using the quantized MobileNetV2 model, including frame capture, preprocessing, and performance logging.

LANGUAGE: python
CODE:
import time

import torch
import numpy as np
from torchvision import models, transforms

import cv2
from PIL import Image

torch.backends.quantized.engine = 'qnnpack'

cap = cv2.VideoCapture(0, cv2.CAP_V4L2)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 224)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 224)
cap.set(cv2.CAP_PROP_FPS, 36)

preprocess = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

net = models.quantization.mobilenet_v2(pretrained=True, quantize=True)
# jit model to take it from ~20fps to ~30fps
net = torch.jit.script(net)

started = time.time()
last_logged = time.time()
frame_count = 0

with torch.no_grad():
    while True:
        # read frame
        ret, image = cap.read()
        if not ret:
            raise RuntimeError("failed to read frame")

        # convert opencv output from BGR to RGB
        image = image[:, :, [2, 1, 0]]
        permuted = image

        # preprocess
        input_tensor = preprocess(image)

        # create a mini-batch as expected by the model
        input_batch = input_tensor.unsqueeze(0)

        # run model
        output = net(input_batch)
        # do something with output ...

        # log model performance
        frame_count += 1
        now = time.time()
        if now - last_logged > 1:
            print(f"{frame_count / (now-last_logged)} fps")
            last_logged = now
            frame_count = 0

----------------------------------------

TITLE: Skipping Initialization Using torch.nn.utils.skip_init
DESCRIPTION: This snippet shows how to use the skip_init function to avoid wasted computation during module construction and perform custom initialization directly.

LANGUAGE: python
CODE:
from torch import nn
from torch.nn.utils import skip_init

m = skip_init(nn.Linear, 10, 5)

# Example: Do custom, non-default parameter initialization.
nn.init.orthogonal_(m.weight)

----------------------------------------

TITLE: Creating and Activating Conda Environment for TorchInductor
DESCRIPTION: Commands to create a custom Conda environment for TorchInductor and activate it.

LANGUAGE: sh
CODE:
conda create -n inductor_cpu_windows python=3.10 -y 
conda activate inductor_cpu_windows

----------------------------------------

TITLE: Basic Model Input Bundling in PyTorch
DESCRIPTION: Demonstrates creating a simple neural network model and converting it to TorchScript with bundled inputs using the forward method.

LANGUAGE: python
CODE:
import torch
import torch.jit
import torch.utils
import torch.utils.bundled_inputs

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.lin = nn.Linear(10, 1)

    def forward(self, x):
        return self.lin(x)

model = Net()
scripted_module = torch.jit.script(model)

----------------------------------------

TITLE: Implementing a Custom PyTorch Module with Initialization Skipping Support
DESCRIPTION: This example demonstrates how to create a custom PyTorch module that supports the device kwarg and adheres to the requirements for skipping initialization.

LANGUAGE: python
CODE:
import torch
from torch import nn

class MyModule(torch.nn.Module):
  def __init__(self, foo, bar, device=None):
    super().__init__()

    # ==== Case 1: Module creates parameters directly. ====
    # Pass device along to any created parameters.
    self.param1 = nn.Parameter(torch.empty((foo, bar), device=device))
    self.register_parameter('param2', nn.Parameter(torch.empty(bar, device=device)))

    # To ensure support for the meta device, avoid using ops except those in
    # torch.nn.init on parameters in your module's constructor.
    with torch.no_grad():
        nn.init.kaiming_uniform_(self.param1)
        nn.init.uniform_(self.param2)


    # ==== Case 2: Module creates submodules. ====
    # Pass device along recursively. All submodules will need to support
    # them as well; this is the case for all torch.nn provided modules.
    self.fc = nn.Linear(bar, 5, device=device)

    # This also works with containers.
    self.linears = nn.Sequential(
        nn.Linear(5, 5, device=device),
        nn.Linear(5, 1, device=device)
    )


    # ==== Case 3: Module creates buffers. ====
    # Pass device along during buffer tensor creation.
    self.register_buffer('some_buffer', torch.ones(7, device=device))

...

----------------------------------------

TITLE: Basic Model Structure Before Traceability Modification
DESCRIPTION: Example showing a model class with mixed traceable and non-traceable code before modification for quantization.

LANGUAGE: python
CODE:
class M(nn.Module):
    def forward(self, x):
        x = non_traceable_code_1(x)
        x = traceable_code(x)
        x = non_traceable_code_2(x)
        return x

----------------------------------------

TITLE: Activating Max-Autotune Mode in PyTorch
DESCRIPTION: This snippet demonstrates how to activate the max-autotune mode in PyTorch using torch.compile. It includes setting up a simple neural network with a linear layer and ReLU activation, and compiling it with the max-autotune mode.

LANGUAGE: python
CODE:
import torch
from torch._inductor import config
config.trace.log_autotuning_results = True # enable the log of autotuning results

class M(torch.nn.Module):
    def __init__(
        self,
        in_features,
        out_features,
        bias,
        **kwargs,
    ):
        super().__init__()
        self.linear = torch.nn.Linear(
            in_features,
            out_features,
            bias,
            **kwargs,
        )
        self.relu = torch.nn.ReLU()

    def forward(self, x):
        x = self.linear(x)
        x = self.relu(x)
        return x

amp_enabled = True
batch_size = 64
in_features = 16
out_features = 32
bias = True

x = torch.randn(batch_size, in_features)
model = M(in_features, out_features, bias)

with torch.no_grad(), torch.cpu.amp.autocast(enabled=amp_enabled):
    compiled = torch.compile(model, mode="max-autotune") # turn on "max-autotune" mode
    y = compiled(x)

----------------------------------------

TITLE: Registering Custom Kernels for PrivateUse1 Backend
DESCRIPTION: Example of registering custom operator implementations for the PrivateUse1 backend using TORCH_LIBRARY_IMPL.

LANGUAGE: cpp
CODE:
TORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {
  m.impl(<schema_my_op1>, &my_op1);
  m.impl(<schema_my_op2>, &my_op2);
  m.impl(<schema_my_op2_backward>, &my_op2_backward);
}

----------------------------------------

TITLE: Registering Device Guard for New Backend in C++
DESCRIPTION: Illustrates how to implement and register a device guard for a new backend by inheriting from DeviceGuardImplInterface.

LANGUAGE: cpp
CODE:
struct CustomGuardImpl final : public c10::impl::DeviceGuardImplInterface {
  // Implementation of guard in new backend
}

C10_REGISTER_GUARD_IMPL(PrivateUse1, CustomGuardImpl);

----------------------------------------

TITLE: Enabling Inductor C++ Wrapper in PyTorch
DESCRIPTION: Configuration code to enable the Inductor C++ wrapper feature, which is currently in prototype stage.

LANGUAGE: python
CODE:
import torch._inductor.config as config
config.cpp_wrapper = True

----------------------------------------

TITLE: Implementing Observer for Distributed Reinforcement Learning using RPC
DESCRIPTION: Defines an Observer class that creates a gym environment and runs episodes based on commands from a remote agent using RPC calls.

LANGUAGE: python
CODE:
import argparse
import gym
import torch.distributed.rpc as rpc

parser = argparse.ArgumentParser(
    description="RPC Reinforcement Learning Example",
    formatter_class=argparse.ArgumentDefaultsHelpFormatter,
)

parser.add_argument('--world_size', default=2, type=int, metavar='W',
                    help='number of workers')
parser.add_argument('--log_interval', type=int, default=10, metavar='N',
                    help='interval between training status logs')
parser.add_argument('--gamma', type=float, default=0.99, metavar='G',
                    help='how much to value future rewards')
parser.add_argument('--seed', type=int, default=1, metavar='S',
                    help='random seed  for reproducibility')
args = parser.parse_args()

class Observer:

    def __init__(self):
        self.id = rpc.get_worker_info().id
        self.env = gym.make('CartPole-v1')
        self.env.seed(args.seed)

    def run_episode(self, agent_rref):
        state, ep_reward = self.env.reset(), 0
        for _ in range(10000):
            # send the state to the agent to get an action
            action = agent_rref.rpc_sync().select_action(self.id, state)

            # apply the action to the environment, and get the reward
            state, reward, done, _ = self.env.step(action)

            # report the reward to the agent for training purpose
            agent_rref.rpc_sync().report_reward(self.id, reward)

            # finishes after the number of self.env._max_episode_steps
            if done:
                break

----------------------------------------

TITLE: Importing PyTorch Quantization Dependencies
DESCRIPTION: Imports required PyTorch modules and classes for quantization, including observers, QConfig mappings, and backend configuration components.

LANGUAGE: python
CODE:
import torch
from torch.ao.quantization import (
    default_weight_observer,
    get_default_qconfig_mapping,
    MinMaxObserver,
    QConfig,
    QConfigMapping,
)
from torch.ao.quantization.backend_config import (
    BackendConfig,
    BackendPatternConfig,
    DTypeConfig,
    DTypeWithConstraints,
    ObservationType,
)
from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx

----------------------------------------

TITLE: Implementing Bidirectional LSTM with Parallelism in TorchScript
DESCRIPTION: This snippet shows how to parallelize forward and backward layers in a Bidirectional LSTM using fork() and wait(). It demonstrates overlapping the execution of forward and backward LSTM cells.

LANGUAGE: python
CODE:
class BidirectionalRecurrentLSTM(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.cell_f = torch.nn.LSTM(input_size=C, hidden_size=C)
        self.cell_b = torch.nn.LSTM(input_size=C, hidden_size=C)

    def forward(self, x : torch.Tensor) -> torch.Tensor:
        # Forward layer - fork() so this can run in parallel to the backward
        # layer
        future_f = torch.jit.fork(self.cell_f, x)

        # Backward layer. Flip input in the time dimension (dim 0), apply the
        # layer, then flip the outputs in the time dimension
        x_rev = torch.flip(x, dims=[0])
        output_b, _ = self.cell_b(torch.flip(x, dims=[0]))
        output_b_rev = torch.flip(output_b, dims=[0])

        # Retrieve the output from the forward layer. Note this needs to happen
        # *after* the stuff we want to parallelize with
        output_f, _ = torch.jit.wait(future_f)

        return torch.cat((output_f, output_b_rev), dim=2)

----------------------------------------

TITLE: Exposing the Extension Python APIs in C++
DESCRIPTION: Adds methods to expose the BackendDummy constructor to Python and register it as a valid backend with PyTorch's distributed module.

LANGUAGE: cpp
CODE:
// file name: dummy.hpp
class BackendDummy : public Backend {
    ...
    <Step 1 code>
    ...

    static c10::intrusive_ptr<Backend> createBackendDummy(
        const c10::intrusive_ptr<::c10d::Store>& store,
        int rank,
        int size,
        const std::chrono::duration<float>& timeout);

    static void BackendDummyConstructor() __attribute__((constructor)) {
        py::object module = py::module::import("torch.distributed");
        py::object register_backend =
            module.attr("Backend").attr("register_backend");
        // torch.distributed.Backend.register_backend will add `dummy` as a
        // new valid backend.
        register_backend("dummy", py::cpp_function(createBackendDummy));
    }
}

LANGUAGE: cpp
CODE:
// file name: dummy.cpp
c10::intrusive_ptr<Backend> BackendDummy::createBackendDummy(
        const c10::intrusive_ptr<::c10d::Store>& /* unused */,
        int rank,
        int size,
        const std::chrono::duration<float>& /* unused */) {
    return c10::make_intrusive<BackendDummy>(rank, size);
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("createBackendDummy", &BackendDummy::createBackendDummy);
}

----------------------------------------

TITLE: Implementing QHM Parameter Update Function
DESCRIPTION: Defines a TorchScript-compatible function that handles the core computation logic for the QHM optimizer, separating it from state management for better threading support.

LANGUAGE: python
CODE:
def qhm_update(params: List[Tensor],
                dp_list: List[Tensor],
                momentum_buffer_list: List[Tensor],
                lr: float,
                nu: float,
                weight_decay: float,
                weight_decay_type: str,
                momentum: float):

    for p, d_p, momentum_buffer in zip(params, dp_list, momentum_buffer_list):
        if weight_decay != 0:
            if weight_decay_type == "grad":
                d_p.add_(weight_decay, p)
            elif weight_decay_type == "direct":
                p.mul_(1.0 - lr * weight_decay)
            else:
                raise ValueError("Invalid weight decay type provided")

        momentum_buffer.mul_(momentum).add_(1.0 - momentum, d_p)

        p.data.add_(-lr * nu, momentum_buffer)
        p.data.add_(-lr * (1.0 - nu), d_p)

----------------------------------------

TITLE: Specifying Quantization Configuration for BERT
DESCRIPTION: Defines the quantization configuration using per-channel dynamic quantization for the entire model.

LANGUAGE: python
CODE:
qconfig_dict = {'': per_channel_dynamic_qconfig}

----------------------------------------

TITLE: Building MNIST Example with CMake for PyTorch C++
DESCRIPTION: These commands create a build directory, configure CMake with the path to LibTorch, and compile the project. The CMAKE_PREFIX_PATH should be set to the location of the LibTorch distribution.

LANGUAGE: shell
CODE:
$ cd mnist
$ mkdir build
$ cd build
$ cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..
$ make

----------------------------------------

TITLE: Defining Autoload Function for PyTorch Extension
DESCRIPTION: Example of defining an autoload function in the __init__.py file of a PyTorch extension package. This function is called when the extension is automatically loaded.

LANGUAGE: python
CODE:
def _autoload():
    print("Check things are working with `torch.foo.is_available()`.")

----------------------------------------

TITLE: Deploying Model to Vertex AI Endpoint
DESCRIPTION: Python code to deploy the created model to a Vertex AI endpoint with specified compute resources (NVIDIA Tesla P100 GPU and n1-standard-8 machine)

LANGUAGE: shell
CODE:
endpoint = aiplatform.Endpoint.create(display_name=ENDPOINT_DISPLAY_NAME)

model.deploy(
    endpoint=endpoint,
    deployed_model_display_name=MODEL_DISPLAY_NAME,
    machine_type="n1-standard-8",
    accelerator_type="NVIDIA_TESLA_P100",
    accelerator_count=1,
    traffic_percentage=100,
    deploy_request_timeout=1200,
    sync=True,
)

----------------------------------------

TITLE: Setting up 2D Parallel Pattern with DeviceMesh in Python
DESCRIPTION: This code snippet shows how to use DeviceMesh to set up a 2D parallel pattern with just two lines of code. It demonstrates the simplification of the process compared to the manual setup.

LANGUAGE: python
CODE:
from torch.distributed.device_mesh import init_device_mesh
mesh_2d = init_device_mesh("cuda", (2, 4), mesh_dim_names=("replicate", "shard"))

# Users can access the underlying process group thru `get_group` API.
replicate_group = mesh_2d.get_group(mesh_dim="replicate")
shard_group = mesh_2d.get_group(mesh_dim="shard")

----------------------------------------

TITLE: Installing Vulkan SDK on macOS
DESCRIPTION: Commands to install Vulkan SDK on macOS, including setting up the environment and running the installation script.

LANGUAGE: bash
CODE:
cd $VULKAN_SDK_ROOT
source setup-env.sh
sudo python install_vulkan.py

----------------------------------------

TITLE: Building PyTorch Android Benchmark Tool
DESCRIPTION: Commands for building the PyTorch Android benchmark tool from source, which is used to compare performance between fused and non-fused models.

LANGUAGE: bash
CODE:
git clone --recursive https://github.com/pytorch/pytorch
cd pytorch
git submodule update --init --recursive
BUILD_PYTORCH_MOBILE=1 ANDROID_ABI=arm64-v8a ./scripts/build_android.sh -DBUILD_BINARY=ON

----------------------------------------

TITLE: Creating a TensorImpl on PrivateUse1 Backend
DESCRIPTION: Example of creating a TensorImpl for a custom backend using the PrivateUse1 dispatch key.

LANGUAGE: cpp
CODE:
/* Example TensorImpl constructor */
TensorImpl(
    Storage&& storage,
    DispatchKeySet ks,
    const caffe2::TypeMeta data_type);

// To create a TensorImpl on PrivateUse1 backend, pass in the following ks to TensorImpl creation.
DispatchKeySet ks = c10::DispatchKeySet{c10::DispatchKey::PrivateUse1, c10::DispatchKey::AutogradPrivateUse1};

----------------------------------------

TITLE: Installing numactl on CentOS
DESCRIPTION: Command to install the numactl tool on CentOS for NUMA policy control.

LANGUAGE: bash
CODE:
$ yum install numactl

----------------------------------------

TITLE: Fine-tuning and Quantizing a PyTorch Model
DESCRIPTION: Demonstrates the process of fine-tuning a quantizable model and then converting it to a fully quantized version for inference.

LANGUAGE: python
CODE:
for param in model_ft.parameters():
    param.requires_grad = True

model_ft.to(device)

criterion = nn.CrossEntropyLoss()
optimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.1)
exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.3)

model_ft_tuned = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,
                             num_epochs=25, device=device)

model_ft_tuned.cpu()
model_quantized_and_trained = convert(model_ft_tuned, inplace=False)

----------------------------------------

TITLE: Using Environment Variable to Select Legacy TCPStore Backend in Python
DESCRIPTION: This code shows how to use the USE_LIBUV environment variable to specify the use of the old TCPStore backend when initializing a ProcessGroup.

LANGUAGE: python
CODE:
import os

import torch
import torch.distributed as dist

addr = "localhost"
port = 23456
os.environ["USE_LIBUV"] = "0"
dist.init_process_group(
    backend="cpu:gloo,cuda:nccl",
    rank=0,
    world_size=1,
    init_method=f"tcp://{addr}:{port}",
)
dist.destroy_process_group()

----------------------------------------

TITLE: Building PyTorch Android AAR with Vulkan Backend
DESCRIPTION: Command to build PyTorch Android AAR files with Vulkan backend support.

LANGUAGE: bash
CODE:
cd $PYTORCH_ROOT
USE_VULKAN=1 sh ./scripts/build_pytorch_android.sh

----------------------------------------

TITLE: Creating and Slicing 3D DeviceMesh for Custom Parallel Solutions in Python
DESCRIPTION: This code snippet illustrates how to create a 3D DeviceMesh and slice it into child meshes for different parallelism solutions. It demonstrates the flexibility of DeviceMesh in handling complex custom parallel training compositions.

LANGUAGE: python
CODE:
from torch.distributed.device_mesh import init_device_mesh
mesh_3d = init_device_mesh("cuda", (2, 2, 2), mesh_dim_names=("replicate", "shard", "tp"))

# Users can slice child meshes from the parent mesh.
hsdp_mesh = mesh_3d["replicate", "shard"]
tp_mesh = mesh_3d["tp"]

# Users can access the underlying process group thru `get_group` API.
replicate_group = hsdp_mesh["replicate"].get_group()
shard_group = hsdp_mesh["shard"].get_group()
tp_group = tp_mesh.get_group()

----------------------------------------

TITLE: Using Vulkan Backend in C++ API
DESCRIPTION: C++ code snippet demonstrating how to use the Vulkan backend for tensor operations and model inference.

LANGUAGE: cpp
CODE:
at::is_vulkan_available()
auto tensor = at::rand({1, 2, 2, 3}, at::device(at::kCPU).dtype(at::kFloat));
auto tensor_vulkan = t.vulkan();
auto module = torch::jit::load("$PATH");
auto tensor_output_vulkan = module.forward(inputs).toTensor();
auto tensor_output = tensor_output.cpu();

----------------------------------------

TITLE: Installing JeMalloc on Ubuntu
DESCRIPTION: Command to install JeMalloc on Ubuntu for optimized memory allocation.

LANGUAGE: bash
CODE:
$ apt-get install libjemalloc2

----------------------------------------

TITLE: Comparing FX Graph Mode and Eager Mode Quantization in PyTorch
DESCRIPTION: Compares the performance of FX graph mode quantization with eager mode quantization and the baseline float model.

LANGUAGE: python
CODE:
scripted_float_model_file = "resnet18_scripted.pth"

print("Size of baseline model")
print_size_of_model(float_model)

top1, top5 = evaluate(float_model, criterion, data_loader_test)
print("Baseline Float Model Evaluation accuracy: %2.2f, %2.2f"%(top1.avg, top5.avg))
torch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file)

print("Size of Fx graph mode quantized model")
print_size_of_model(quantized_model)
top1, top5 = evaluate(quantized_model, criterion, data_loader_test)
print("FX graph mode quantized model Evaluation accuracy on test dataset: %2.2f, %2.2f"%(top1.avg, top5.avg))

from torchvision.models.quantization.resnet import resnet18
eager_quantized_model = resnet18(pretrained=True, quantize=True).eval()
print("Size of eager mode quantized model")
eager_quantized_model = torch.jit.script(eager_quantized_model)
print_size_of_model(eager_quantized_model)
top1, top5 = evaluate(eager_quantized_model, criterion, data_loader_test)
print("eager mode quantized model Evaluation accuracy on test dataset: %2.2f, %2.2f"%(top1.avg, top5.avg))
eager_mode_model_file = "resnet18_eager_mode_quantized.pth"
torch.jit.save(eager_quantized_model, saved_model_dir + eager_mode_model_file)

----------------------------------------

TITLE: Defining Quantization Specifications
DESCRIPTION: Demonstrates how to create QuantizationSpec objects with specific parameters for activation quantization.

LANGUAGE: python
CODE:
act_quantization_spec = QuantizationSpec(
    dtype=torch.int8,
    quant_min=-128,
    quant_max=127,
    qscheme=torch.per_tensor_affine,
    is_dynamic=False,
    observer_or_fake_quant_ctr=HistogramObserver.with_args(eps=2**-12),
)

input_act_qspec = act_quantization_spec
output_act_qspec = act_quantization_spec

----------------------------------------

TITLE: Defining Operator Schema
DESCRIPTION: Registers the schema for a custom operator using TORCH_LIBRARY without providing implementation.

LANGUAGE: cpp
CODE:
TORCH_LIBRARY(myops, m) {
  m.def("myadd(Tensor self, Tensor other) -> Tensor");
}

----------------------------------------

TITLE: Implementing TorchScript-Compatible Functional QHM Optimizer
DESCRIPTION: Creates a TorchScript-compatible optimizer class that manages optimizer states and interfaces with the update function. This class follows TorchScript conventions and doesn't inherit from torch.optim.Optimizer.

LANGUAGE: python
CODE:
@torch.jit.script
class FunctionalQHM(object):
    def __init__(self,
                params: List[Tensor],
                lr: float,
                momentum: float,
                nu: float,
                weight_decay: float = 0.0,
                weight_decay_type: str = "grad"):
        if lr < 0.0:
            raise ValueError("Invalid learning rate: {}".format(lr))
        if momentum < 0.0:
            raise ValueError("Invalid momentum value: {}".format(momentum))
        if weight_decay < 0.0:
            raise ValueError("Invalid weight_decay value: {}".format(weight_decay))
        if weight_decay_type not in ("grad", "direct"):
            raise ValueError("Invalid weight_decay_type value: {}".format(weight_decay_type))

        self.defaults = {
            "lr": lr,
            "momentum": momentum,
            "nu": nu,
            "weight_decay": weight_decay,
        }
        self.weight_decay_type = weight_decay_type

        self.param_group = {"params": params}

        self.state = torch.jit.annotate(Dict[torch.Tensor, Dict[str, torch.Tensor]], {})

    def step(self, gradients: List[Optional[Tensor]]):
        params = self.param_group['params']
        params_with_grad = []
        grads = []
        momentum_buffer_list: List[Tensor] = []

        if len(params) != len(gradients):
            raise ValueError(
                "the gradients passed in does not equal to the size of the parameters!"
                + f"Params length: {len(params)}. "
                + f"Gradients length: {len(gradients)}"
            )

        for param, gradient in zip(self.param_group['params'], gradients):
            if gradient is not None:
                params_with_grad.append(param)
                grads.append(gradient)
                state = self.state[param]
                state['momentum_buffer'] = torch.zeros_like(param, memory_format=torch.preserve_format)
                momentum_buffer_list.append(state['momentum_buffer'])

        with torch.no_grad():
            qhm_update(params_with_grad,
                    grads,
                    momentum_buffer_list,
                    self.defaults['lr'],
                    self.defaults['nu'],
                    self.defaults['weight_decay'],
                    self.weight_decay_type,
                    self.defaults['momentum'])

----------------------------------------

TITLE: Installing PyTorch for AWS Graviton
DESCRIPTION: Command to install PyTorch, which natively supports AWS Graviton3 optimizations starting from version 2.0.

LANGUAGE: bash
CODE:
python3 -m pip install torch

----------------------------------------

TITLE: Activating MSVC in Windows Command Line
DESCRIPTION: Command to activate Microsoft Visual C++ (MSVC) in the Windows command line environment.

LANGUAGE: sh
CODE:
"C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Auxiliary/Build/vcvars64.bat"

----------------------------------------

TITLE: Benchmarking Quantized Model Performance in PyTorch
DESCRIPTION: Demonstrates how to measure and compare the inference speed of the original floating-point model and the quantized model.

LANGUAGE: python
CODE:
def run_benchmark(model_file, img_loader):
    elapsed = 0
    model = torch.jit.load(model_file)
    model.eval()
    num_batches = 5
    for i, (images, target) in enumerate(img_loader):
        if i < num_batches:
            start = time.time()
            output = model(images)
            end = time.time()
            elapsed = elapsed + (end-start)
        else:
            break
    num_images = images.size()[0] * num_batches
    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000))
    return elapsed

run_benchmark(saved_model_dir + scripted_float_model_file, data_loader_test)
run_benchmark(saved_model_dir + scripted_quantized_model_file, data_loader_test)

----------------------------------------

TITLE: Preprocessing Images for MobileNetV2
DESCRIPTION: Apply image preprocessing transforms to prepare the input for the MobileNetV2 model.

LANGUAGE: python
CODE:
from torchvision import transforms

preprocess = transforms.Compose([
    # convert the frame to a CHW torch tensor for training
    transforms.ToTensor(),
    # normalize the colors to the range that mobilenet_v2/3 expect
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
input_tensor = preprocess(image)
# The model can handle multiple images simultaneously so we need to add an
# empty dimension for the batch.
# [3, 224, 224] -> [1, 3, 224, 224]
input_batch = input_tensor.unsqueeze(0)

----------------------------------------

TITLE: Installing Required Dependencies
DESCRIPTION: Commands for installing required Python packages including sklearn and transformers.

LANGUAGE: shell
CODE:
pip install sklearn
pip install transformers==4.29.2

----------------------------------------

TITLE: Saving Pretrained MobileNetV2 Model for Vulkan
DESCRIPTION: Python script to save a pretrained MobileNetV2 model and optimize it for Vulkan backend execution.

LANGUAGE: python
CODE:
import torch
import torchvision
from torch.utils.mobile_optimizer import optimize_for_mobile

model = torchvision.models.mobilenet_v2(pretrained=True)
model.eval()
script_model = torch.jit.script(model)
script_model_vulkan = optimize_for_mobile(script_model, backend='vulkan')
torch.jit.save(script_model_vulkan, "mobilenet2-vulkan.pt")

----------------------------------------

TITLE: Installing PyTorch Dependencies
DESCRIPTION: Commands for installing the latest nightly build of PyTorch with CUDA support.

LANGUAGE: shell
CODE:
yes y | pip uninstall torch torchvision
yes y | pip install --pre torch -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html

----------------------------------------

TITLE: Setting LLVM Compiler for Windows Inductor
DESCRIPTION: Command to set the LLVM Compiler as the Windows Inductor Compiler using the CXX environment variable.

LANGUAGE: sh
CODE:
set CXX=clang-cl

----------------------------------------

TITLE: Implementing LeNet Model in PyTorch
DESCRIPTION: Definition of LeNet neural network model using PyTorch, including model architecture and weight loading.

LANGUAGE: python
CODE:
import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc1_drop = nn.Dropout()
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.reshape(-1, 320)
        x = F.relu(self.fc1(x))
        x = self.fc1_drop(x)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

model = Net()
model.load_state_dict(torch.load('./lenet_mnist_model.pth', weights_only=True))

----------------------------------------

TITLE: Running Single-Instance Inference with 1 Thread on 1 CPU Core
DESCRIPTION: Command to run single-instance inference using the torch.backends.xeon.run_cpu script, allocating 1 thread on 1 CPU core (Core #0).

LANGUAGE: bash
CODE:
$ python -m torch.backends.xeon.run_cpu --ninstances 1 --ncores-per-instance 1 <program.py> [program_args]

----------------------------------------

TITLE: Importing Required Modules
DESCRIPTION: Python code for importing necessary modules and setting up logging configuration.

LANGUAGE: python
CODE:
import logging
import numpy as np
import os
import random
import sys
import time
import torch

from argparse import Namespace
from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,
                                  TensorDataset)
from tqdm import tqdm
from transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,)
from transformers import glue_compute_metrics as compute_metrics
from transformers import glue_output_modes as output_modes
from transformers import glue_processors as processors
from transformers import glue_convert_examples_to_features as convert_examples_to_features

# Setup logging
logger = logging.getLogger(__name__)
logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',
                    datefmt = '%m/%d/%Y %H:%M:%S',
                    level = logging.WARN)

----------------------------------------

TITLE: Running Single-Instance Inference on a Single CPU Node
DESCRIPTION: Command to run single-instance inference on a single CPU node (NUMA socket) using the torch.backends.xeon.run_cpu script.

LANGUAGE: bash
CODE:
$ python -m torch.backends.xeon.run_cpu --node-id 0 <program.py> [program_args]

----------------------------------------

TITLE: Launch Script for PyTorch ITT Profiling (Bash)
DESCRIPTION: This bash script sets up the environment and runs the PyTorch sample code for ITT profiling. It's designed to be used with Intel VTune Profiler.

LANGUAGE: bash
CODE:
#!/bin/bash

# Retrieve the directory path where the path contains both the sample.py and launch.sh so that this bash script can be invoked from any directory
BASEFOLDER=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
<Activate a Python environment>
cd ${BASEFOLDER}
python sample.py

----------------------------------------

TITLE: Git Commands for Tutorial Submission
DESCRIPTION: Git commands for committing and pushing tutorial changes to create a pull request.

LANGUAGE: bash
CODE:
git add -A
git commit -m "Add <mytutorial>"
git push --set-upstream mybranch

----------------------------------------

TITLE: Inference with ResNet50 FP32 using IPEX Backend
DESCRIPTION: Demonstrates FP32 inference using IPEX backend with a pre-trained ResNet50 model. Shows model optimization and compilation for inference without weights prepack.

LANGUAGE: python
CODE:
import torch
import torchvision.models as models

model = models.resnet50(weights='ResNet50_Weights.DEFAULT')
model.eval()
data = torch.rand(1, 3, 224, 224)

import intel_extension_for_pytorch as ipex

# Invoke the following API optionally, to apply frontend optimizations
model = ipex.optimize(model, weights_prepack=False)

compile_model = torch.compile(model, backend="ipex")

with torch.no_grad():
    compile_model(data)

----------------------------------------

TITLE: Tensor Creation and Manipulation
DESCRIPTION: Common operations for creating and manipulating PyTorch tensors

LANGUAGE: python
CODE:
torch.randn(*size)
torch.[ones|zeros](*size)
torch.Tensor(L)
x.clone()
with torch.no_grad():
requires_grad=True

----------------------------------------

TITLE: Autotuning Result Output for Max-Autotune Mode
DESCRIPTION: This shell output shows the autotuning result when running the max-autotune mode. It compares the performance of C++ template and ATen kernel implementations.

LANGUAGE: shell
CODE:
AUTOTUNE linear_unary(64x16, 32x16, 32)
cpp_packed_gemm_0 0.2142 ms 100.0% 
_linear_pointwise 0.2441 ms 87.7% 

----------------------------------------

TITLE: Neural Network API Imports
DESCRIPTION: Imports for neural network components including autograd, nn modules, and optimizers

LANGUAGE: python
CODE:
import torch.autograd as autograd
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.jit import script, trace

----------------------------------------

TITLE: Adding Tutorial to Table of Contents
DESCRIPTION: Example of how to add a tutorial to the toctree in index.rst to include it in the documentation hierarchy.

LANGUAGE: restructuredtext
CODE:
.. toctree::
   :maxdepth: 2
   :includehidden:
   :hidden:
   :caption: Image and Video

   intermediate/torchvision_tutorial
   beginner/my-new-tutorial

----------------------------------------

TITLE: Manual Model Partitioning
DESCRIPTION: Implements manual model splitting for pipeline parallelism by selectively deleting layers based on stage index.

LANGUAGE: python
CODE:
def manual_model_split(model) -> PipelineStage:
   if stage_index == 0:
      # prepare the first stage model
      for i in range(4, 8):
            del model.layers[str(i)]
      model.norm = None
      model.output = None

   elif stage_index == 1:
      # prepare the second stage model
      for i in range(4):
            del model.layers[str(i)]
      model.tok_embeddings = None

   stage = PipelineStage(
      model,
      stage_index,
      num_stages,
      device,
   )
   return stage

----------------------------------------

TITLE: Setting Up Entry Point for PyTorch Extension
DESCRIPTION: Configuration in setup.py to define an entry point for a PyTorch extension, enabling autoloading functionality.

LANGUAGE: python
CODE:
setup(
    name="torch_foo",
    version="1.0",
    entry_points={
        "torch.backends": [
            "torch_foo = torch_foo:_autoload",
        ],
    }
)

----------------------------------------

TITLE: Optimizing Thread Usage for Performance
DESCRIPTION: Set the number of threads used by PyTorch to reduce latency spikes at the cost of a small performance penalty.

LANGUAGE: python
CODE:
torch.set_num_threads(2)

----------------------------------------

TITLE: Implementing Autoload Function for Habana Frameworks
DESCRIPTION: Implementation of the __autoload function in habana_frameworks/__init__.py to handle autoloading and prevent circular imports.

LANGUAGE: python
CODE:
import os

is_loaded = False  # A member variable of habana_frameworks module to track if our module has been imported

def __autoload():
    # This is an entrypoint for pytorch autoload mechanism
    # If the following condition is true, that means our backend has already been loaded, either explicitly
    # or by the autoload mechanism and importing it again should be skipped to avoid circular imports
    global is_loaded
    if is_loaded:
        return
    import habana_frameworks.torch

----------------------------------------

TITLE: Markdown Table - H1 2023 Docathon Leaderboard
DESCRIPTION: Tabular display of contributor rankings including GitHub usernames, points earned, and links to their merged pull requests for the H1 2023 PyTorch documentation campaign.

LANGUAGE: markdown
CODE:
| Author | Points | PR |
|--- | --- | ---|
| JoseLuisC99 | 22 | https://github.com/pytorch/tutorials/pull/2468, [...] |
[Additional rows omitted for brevity]

----------------------------------------

TITLE: Helper Functions for SQuAD Dataset Processing
DESCRIPTION: Utility functions for preprocessing the SQuAD dataset, including tokenization and answer span extraction.

LANGUAGE: python
CODE:
def preprocess_validation_function(examples, tokenizer):
    inputs = tokenizer(
        [q.strip() for q in examples["question"]],
        examples["context"],
        max_length=384,
        truncation="only_second",
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )
    sample_map = inputs.pop("overflow_to_sample_mapping")
    example_ids = []

    for i in range(len(inputs["input_ids"])):
        sample_idx = sample_map[i]
        example_ids.append(examples["id"][sample_idx])
        sequence_ids = inputs.sequence_ids(i)
        offset = inputs["offset_mapping"][i]
        inputs["offset_mapping"][i] = [
            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)
        ]

    inputs["example_id"] = example_ids
    return inputs

----------------------------------------

TITLE: Advanced Model Input Bundling with Multiple Methods
DESCRIPTION: Shows how to bundle inputs for multiple model methods beyond just 'forward', including custom exported functions with dictionary inputs.

LANGUAGE: python
CODE:
import torch
import torch.jit
import torch.utils
import torch.utils.bundled_inputs
from typing import Dict

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.lin = nn.Linear(10, 1)

    def forward(self, x):
        return self.lin(x)

    @torch.jit.export
    def foo(self, x: Dict[String, int]):
        return x['a'] + x['b']


model = Net()
scripted_module = torch.jit.script(model)

----------------------------------------

TITLE: Preventing Circular Imports in Habana Frameworks
DESCRIPTION: Code snippet from habana_frameworks/torch/__init__.py to prevent circular imports by updating the global state.

LANGUAGE: python
CODE:
import os

# This is to prevent torch autoload mechanism from causing circular imports
import habana_frameworks

habana_frameworks.is_loaded = True

----------------------------------------

TITLE: Environment Variables for PyTorch Compiler Caching
DESCRIPTION: List of environment variables used to configure PyTorch compiler caching behavior, including FX graph caching, autograd caching, remote caching via Redis, and cache directory settings.

LANGUAGE: env
CODE:
TORCHINDUCTOR_FX_GRAPH_CACHE=1
TORCHINDUCTOR_AUTOGRAD_CACHE=1
TORCHINDUCTOR_CACHE_DIR=/path/to/cache
TORCHINDUCTOR_FX_GRAPH_REMOTE_CACHE=1
TORCHINDUCTOR_REDIS_HOST=localhost
TORCHINDUCTOR_REDIS_PORT=6379
TORCHINDUCTOR_AUTOGRAD_REMOTE_CACHE=1
TORCHINDUCTOR_AUTOTUNE_REMOTE_CACHE=1
TORCHINDUCTOR_FORCE_DISABLE_CACHES=1

----------------------------------------

TITLE: Tensor Preallocation for CUDA Graphs
DESCRIPTION: Modified implementation with preallocated tensors for reuse in the training loop, preparing for CUDA Graph implementation.

LANGUAGE: cpp
CODE:
torch::TensorOptions FloatCUDA =
    torch::TensorOptions(device).dtype(torch::kFloat);
torch::TensorOptions LongCUDA =
    torch::TensorOptions(device).dtype(torch::kLong);

torch::Tensor data = torch::zeros({kTrainBatchSize, 1, 28, 28}, FloatCUDA);
torch::Tensor targets = torch::zeros({kTrainBatchSize}, LongCUDA);
torch::Tensor output = torch::zeros({1}, FloatCUDA);
torch::Tensor loss = torch::zeros({1}, FloatCUDA);

for (auto& batch : data_loader) {
  data.copy_(batch.data);
  targets.copy_(batch.target);
  training_step(model, optimizer, data, targets, output, loss);
}

----------------------------------------

TITLE: Implementing 2:4 Sparsity with Weight Norm Sparsifier
DESCRIPTION: Configuration and application of magnitude-based pruning to achieve 2:4 sparsity pattern.

LANGUAGE: python
CODE:
sparsifier = WeightNormSparsifier(
    sparsity_level=1.0,
    sparse_block_shape=(1, 4),
    zeros_per_block=2
)

sparse_config = [
    {"tensor_fqn": f"{fqn}.weight"}
    for fqn, module in model.named_modules()
    if isinstance(module, nn.Linear) and "layer" in fqn
]

----------------------------------------

TITLE: Implementing All-Reduce Collective Communication in PyTorch
DESCRIPTION: Demonstrates collective communication using all_reduce to sum tensors across all processes in a group.

LANGUAGE: python
CODE:
""" All-Reduce example."""
def run(rank, size):
    """ Simple collective communication. """
    group = dist.new_group([0, 1])
    tensor = torch.ones(1)
    dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group)
    print('Rank ', rank, ' has data ', tensor[0])

----------------------------------------

TITLE: HTML Meta Redirect for PyTorch Tutorial
DESCRIPTION: HTML meta refresh tag that redirects users to the updated PyTorch tensor tutorial page after a 3-second delay.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html'" />

----------------------------------------

TITLE: Setting up Linear Model in PyTorch
DESCRIPTION: Creates a sequential model with 10 linear layers and performs a forward and backward pass.

LANGUAGE: python
CODE:
import torch

model = torch.nn.Sequential(
    *[torch.nn.Linear(1024, 1024, False, device="cuda") for _ in range(10)]
)
input = torch.rand(1024, device="cuda")
output = model(input)
output.sum().backward()

----------------------------------------

TITLE: HTML Meta Refresh Redirect to PyTorch Tutorials
DESCRIPTION: HTML meta tag implementation that automatically redirects the user to the main PyTorch tutorials page after a 3 second delay.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials'" />

----------------------------------------

TITLE: CUDA Graph Capture Implementation
DESCRIPTION: Setup and execution of CUDA Graph capture using a dedicated stream.

LANGUAGE: cpp
CODE:
at::cuda::CUDAGraph graph;
at::cuda::CUDAStream captureStream = at::cuda::getStreamFromPool();
at::cuda::setCurrentCUDAStream(captureStream);

graph.capture_begin();
training_step(model, optimizer, data, targets, output, loss);
graph.capture_end();

----------------------------------------

TITLE: Implementing Example Model Class
DESCRIPTION: Defines a sample PyTorch model with linear, convolution, batch normalization, ReLU, and sigmoid layers for testing quantization.

LANGUAGE: python
CODE:
class MyModel(torch.nn.Module):
    def __init__(self, use_bn: bool):
        super().__init__()
        self.linear = torch.nn.Linear(10, 3)
        self.conv = torch.nn.Conv2d(3, 3, 3)
        self.bn = torch.nn.BatchNorm2d(3)
        self.relu = torch.nn.ReLU()
        self.sigmoid = torch.nn.Sigmoid()
        self.use_bn = use_bn

    def forward(self, x):
        x = self.linear(x)
        x = self.conv(x)
        if self.use_bn:
            x = self.bn(x)
        x = self.relu(x)
        x = self.sigmoid(x)
        return x

----------------------------------------

TITLE: Retrieving Memory Bandwidth and Queue Length Summaries in Python
DESCRIPTION: Code to get summaries of memory bandwidth and queue length counters using the TraceAnalysis object.

LANGUAGE: python
CODE:
# generate summary
mem_bw_summary = analyzer.get_memory_bw_summary()
queue_len_summary = analyzer.get_queue_length_summary()

# get time series
mem_bw_series = analyzer.get_memory_bw_time_series()
queue_len_series = analyzer.get_queue_length_series()

----------------------------------------

TITLE: HTML Meta Redirect to ExecuTorch Documentation
DESCRIPTION: HTML meta refresh tag that redirects users to the ExecuTorch stable documentation page after a 3 second delay.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/executorch/stable/index.html'" />

----------------------------------------

TITLE: Installing numactl on Ubuntu
DESCRIPTION: Command to install the numactl tool on Ubuntu for NUMA policy control.

LANGUAGE: bash
CODE:
$ apt-get install numactl

----------------------------------------

TITLE: HTML Meta Refresh Redirect to PyTorch Tutorials
DESCRIPTION: HTML meta tag implementation that automatically redirects the user to the PyTorch tutorials homepage after a 2 second delay.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="2; url='https://pytorch.org/tutorials'" />

----------------------------------------

TITLE: CMake Configuration for LibTorch
DESCRIPTION: CMake configuration file for building a C++ application with LibTorch dependencies.

LANGUAGE: cmake
CODE:
cmake_minimum_required(VERSION 3.0 FATAL_ERROR)
project(custom_ops)

find_package(Torch REQUIRED)

add_executable(example-app example-app.cpp)
target_link_libraries(example-app "${TORCH_LIBRARIES}")
set_property(TARGET example-app PROPERTY CXX_STANDARD 17)

----------------------------------------

TITLE: Implementing Distributed RNN Model using RPC and RRef
DESCRIPTION: Defines an RNN model that uses RPC to distribute its components (embedding table, LSTM, decoder) across different workers.

LANGUAGE: python
CODE:
class RNNModel(nn.Module):
    def __init__(self, ps, ntoken, ninp, nhid, nlayers, dropout=0.5):
        super(RNNModel, self).__init__()

        self.emb_table_rref = rpc.remote(ps, EmbeddingTable, args=(ntoken, ninp, dropout))
        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)
        self.decoder_rref = rpc.remote(ps, Decoder, args=(ntoken, nhid, dropout))

    def forward(self, input, hidden):
        emb = _remote_method(EmbeddingTable.forward, self.emb_table_rref, input)
        output, hidden = self.rnn(emb, hidden)
        decoded = _remote_method(Decoder.forward, self.decoder_rref, output)
        return decoded, hidden

    def parameter_rrefs(self):
        remote_params = []
        remote_params.extend(_remote_method(_parameter_rrefs, self.emb_table_rref))
        remote_params.extend(_parameter_rrefs(self.rnn))
        remote_params.extend(_remote_method(_parameter_rrefs, self.decoder_rref))
        return remote_params

----------------------------------------

TITLE: Configuring Quantization Parameters
DESCRIPTION: YAML configuration for accuracy-driven quantization, specifying model framework, evaluation metrics, and accuracy criteria.

LANGUAGE: yaml
CODE:
model:
    name: LeNet
    framework: pytorch_fx

evaluation:
    accuracy:
        metric:
            topk: 1

tuning:
  accuracy_criterion:
    relative: 0.01

----------------------------------------

TITLE: HTML Redirect Meta Tag
DESCRIPTION: HTML meta refresh tag that redirects users to an updated PyTorch pipelining tutorial after 3 seconds

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/intermediate/pipelining_tutorial.html'" />

----------------------------------------

TITLE: Installing JeMalloc on CentOS
DESCRIPTION: Command to install JeMalloc on CentOS for optimized memory allocation.

LANGUAGE: bash
CODE:
$ yum install jemalloc

----------------------------------------

TITLE: HTML Meta Refresh Redirect to ExecuTorch Documentation
DESCRIPTION: HTML meta tag that implements a 3-second redirect to the ExecuTorch stable documentation page.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/executorch/stable/index.html'" />

----------------------------------------

TITLE: Implementing Forward and Backward Ops for PrivateUse1
DESCRIPTION: Example of implementing forward and backward operations for a custom backend and registering them.

LANGUAGE: cpp
CODE:
Tensor my_op2_backward(const Tensor& self, const Tensor& other) {
  // call your backend-specific APIs to implement my_op2_backward so that
  // it matches PyTorch's native behavior
}

// Note backward kernel is still registered to PrivateUse1 instead of AutogradPrivateUse1.
// PyTorch will wrap your backward kernel with proper autograd setup and then link to it in
// my_op2's AutogradPrivateUse1 kernel.
TORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {
  m.impl(<schema_my_op2>, &my_op2);
  m.impl(<schema_my_op2_backward>, &my_op2_backward);
}

----------------------------------------

TITLE: Parallelizing Multiple Tasks with Loops in TorchScript
DESCRIPTION: This example demonstrates how to use fork() and wait() in a loop to launch and wait for multiple parallel tasks. It creates 100 instances of a function execution and sums the results.

LANGUAGE: python
CODE:
import torch
from typing import List

def foo(x):
    return torch.neg(x)

@torch.jit.script
def example(x):
    futures : List[torch.jit.Future[torch.Tensor]] = []
    for _ in range(100):
        futures.append(torch.jit.fork(foo, x))

    results = []
    for future in futures:
        results.append(torch.jit.wait(future))

    return torch.sum(torch.stack(results))

print(example(torch.ones([])))

----------------------------------------

TITLE: Displaying Help for torch.backends.xeon.run_cpu Script
DESCRIPTION: Command to display the help message and usage guidance for the torch.backends.xeon.run_cpu script.

LANGUAGE: bash
CODE:
$ python -m torch.backends.xeon.run_cpu –h

----------------------------------------

TITLE: HTML Meta Redirect to PyTorch Quantization Tutorial
DESCRIPTION: HTML meta refresh tag that automatically redirects users to the new tutorial location after 3 seconds delay.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html'" />

----------------------------------------

TITLE: Installing TCMalloc via conda
DESCRIPTION: Command to install TCMalloc (Google Perftools) using conda for optimized memory allocation.

LANGUAGE: bash
CODE:
$ conda install conda-forge::gperftools

----------------------------------------

TITLE: Redirecting to Latest Parallelism APIs in HTML
DESCRIPTION: This HTML meta tag redirects the user to the latest parallelism APIs documentation after a 3-second delay.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/beginner/dist_overview.html#parallelism-apis'" />

----------------------------------------

TITLE: Registering Backend Module in Python
DESCRIPTION: Shows how to register a new backend module to PyTorch, allowing users to access backend-specific APIs.

LANGUAGE: python
CODE:
torch._register_device_module('npu', torch_npu.npu)

----------------------------------------

TITLE: Applying Loss Parallel
DESCRIPTION: Enable Loss Parallel by marking the output layouts of the last linear projection layer and using the loss_parallel context manager.

LANGUAGE: python
CODE:
model = parallelize_module(
    model,
    tp_mesh,
    {
        "tok_embeddings": RowwiseParallel(
            input_layouts=Replicate(),
            output_layouts=Shard(1),
        ),
        "norm": SequenceParallel(),
        "output": ColwiseParallel(
            input_layouts=Shard(1),
            use_local_output=False,
        ),
    },
)

import torch.nn.functional as F
from torch.distributed.tensor.parallel import loss_parallel

pred = model(input_ids)
with loss_parallel():
    loss = F.cross_entropy(pred.flatten(0, 1), labels.flatten(0, 1))
    loss.backward()

----------------------------------------

TITLE: Offloading Smaller Shapes to OpenBLAS
DESCRIPTION: Setting a threshold for MKLDNN backend selection to improve performance for smaller batch dimensions.

LANGUAGE: bash
CODE:
export TORCH_MKLDNN_MATMUL_MIN_DIM=64

----------------------------------------

TITLE: HTML Meta Refresh Redirect to PyTorch Parallelism Docs
DESCRIPTION: HTML meta refresh tag that automatically redirects users to the updated PyTorch parallelism APIs documentation after 3 seconds.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/beginner/dist_overview.html#parallelism-apis'" />

----------------------------------------

TITLE: Installing JeMalloc via conda
DESCRIPTION: Command to install JeMalloc using conda for optimized memory allocation.

LANGUAGE: bash
CODE:
$ conda install conda-forge::jemalloc

----------------------------------------

TITLE: Adding Tutorial Card in reStructuredText
DESCRIPTION: Example of how to add a tutorial card to index.rst using reStructuredText format. This defines the tutorial's metadata including title, description, image, link and tags.

LANGUAGE: restructuredtext
CODE:
.. customcarditem::
   :header: Learn the Basics
   :card_description: A step-by-step guide to building a complete ML workflow with PyTorch.
   :image: _static/img/thumbnails/cropped/60-min-blitz.png
   :link: beginner/basics/intro.html
   :tags: Getting-Started

----------------------------------------

TITLE: Renaming PrivateUse1 Backend in Python and C++
DESCRIPTION: Demonstrates how to rename the PrivateUse1 backend to a custom name for better user experience.

LANGUAGE: python
CODE:
torch.rename_privateuse1_backend("npu")

LANGUAGE: cpp
CODE:
c10::register_privateuse1_backend("npu")

----------------------------------------

TITLE: Combining Tensor Parallel with Fully Sharded Data Parallel
DESCRIPTION: Demonstrate how to combine Tensor Parallel and Fully Sharded Data Parallel using a 2-D DeviceMesh for efficient large-scale training.

LANGUAGE: python
CODE:
from torch.distributed.device_mesh import init_device_mesh
from torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel, parallelize_module
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP

mesh_2d = init_device_mesh("cuda", (8, 8))
tp_mesh = mesh_2d["tp"] # a submesh that connects intra-host devices
dp_mesh = mesh_2d["dp"] # a submesh that connects inter-host devices

model = Model(...)

tp_plan = {...}

# apply Tensor Parallel intra-host on tp_mesh
model_tp = parallelize_module(model, tp_mesh, tp_plan)
# apply FSDP inter-host on dp_mesh
model_2d = FSDP(model_tp, device_mesh=dp_mesh, use_orig_params=True, ...)

----------------------------------------

TITLE: CommDebugMode Output Example for MLPModule
DESCRIPTION: This snippet shows an example of the output generated by CommDebugMode for an MLPModule at noise level 0. It displays the module-level collective counts, specifically the all_reduce operations in the forward pass.

LANGUAGE: python
CODE:
Expected Output:
    Global
      FORWARD PASS
        *c10d_functional.all_reduce: 1
        MLPModule
          FORWARD PASS
            *c10d_functional.all_reduce: 1
            MLPModule.net1
            MLPModule.relu
            MLPModule.net2
              FORWARD PASS
                *c10d_functional.all_reduce: 1

----------------------------------------

TITLE: Markdown Table - H1 2024 Docathon Leaderboard
DESCRIPTION: Tabular display of contributor rankings including GitHub usernames, points earned, and links to their merged pull requests for the H1 2024 PyTorch documentation campaign.

LANGUAGE: markdown
CODE:
| Author | Points | PR |
|--- | --- | ---|
| ahoblitz | 34 | https://github.com/pytorch/pytorch/pull/128566, [...] |
[Additional rows omitted for brevity]

----------------------------------------

TITLE: Running Multi-Instance Inference
DESCRIPTION: Command to run multi-instance inference with 8 instances, each using 14 cores, on a 112-core CPU using the torch.backends.xeon.run_cpu script.

LANGUAGE: bash
CODE:
$ python -m torch.backends.xeon.run_cpu --ninstances 8 --ncores-per-instance 14 <program.py> [program_args]

----------------------------------------

TITLE: Sample Custom Operation Function Definition in Python
DESCRIPTION: Definition of the custom multiply-add operation showing expected inputs and output

LANGUAGE: python
CODE:
def mymuladd(a: Tensor, b: Tensor, c: float):
    return a * b + c

----------------------------------------

TITLE: Setting Up Trainer with HybridModel and DistributedOptimizer in PyTorch
DESCRIPTION: This function sets up the trainer by creating the HybridModel, collecting parameter RRefs, initializing the DistributedOptimizer, and defining the loss function. It prepares the trainer for distributed training.

LANGUAGE: Python
CODE:
def _setup_trainer(rank, remote_emb_module):
    # Create HybridModel, retrieve list of RRefs for embedding parameters.
    model = HybridModel(remote_emb_module, rank)
    emb_param_rrefs = rpc.rpc_sync(
        remote_emb_module.owner(),
        remote_emb_module.remote_parameters,
    )

    # Retrieve RRefs for local parameters.
    param_rrefs = emb_param_rrefs + [
        RRef(param) for param in model.fc.parameters()
    ]

    # Create DistributedOptimizer.
    opt = DistributedOptimizer(
        torch.optim.SGD,
        param_rrefs,
        lr=0.05,
    )

    # Setup loss function.
    loss_fn = nn.CrossEntropyLoss()
    return model, opt, loss_fn

----------------------------------------

TITLE: Implementing HTML Redirection for PyTorch Tacotron2 Tutorial
DESCRIPTION: This HTML snippet creates an automatic redirection to the new location of the Tacotron2 text-to-speech tutorial on the PyTorch Audio documentation site. It uses a meta tag with an HTTP-EQUIV attribute to refresh the page after 3 seconds.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/audio/stable/tutorials/tacotron2_pipeline_tutorial.html'" />

----------------------------------------

TITLE: Building Tutorial Documentation Locally
DESCRIPTION: Commands for building and testing tutorial documentation locally using make.

LANGUAGE: bash
CODE:
make html-noplot

----------------------------------------

TITLE: HTML Meta Redirect to New Tutorial
DESCRIPTION: HTML meta refresh tag that automatically redirects users to the new tutorial location after 3 seconds

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/beginner/saving_loading_models.html'" />

----------------------------------------

TITLE: Using Autoloaded PyTorch Extension
DESCRIPTION: Demonstration of how to use an autoloaded PyTorch extension without explicitly importing it.

LANGUAGE: python
CODE:
>>> import torch
Check things are working with `torch.foo.is_available()`.
>>> torch.foo.is_available()
True

----------------------------------------

TITLE: Implementing HTML Redirect for PyTorch Audio Tutorial
DESCRIPTION: This HTML meta tag implements an automatic redirect to the new location of the Forced Alignment with Wav2Vec2 tutorial on the PyTorch Audio website. The redirect occurs after a 3-second delay.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/audio/stable/tutorials/forced_alignment_tutorial.html'" />

----------------------------------------

TITLE: Initializing Workers and Running Training in PyTorch
DESCRIPTION: This function initializes RPC for all workers, creates a remote module on the parameter server, and kicks off training on the trainers. It handles different initialization procedures for trainers, master, and parameter server roles.

LANGUAGE: Python
CODE:
def run_worker(rank, world_size):
    if rank in [0, 1]:
        run_trainer(rank, world_size)
    elif rank == 2:
        run_master()
    elif rank == 3:
        run_parameter_server()
    else:
        raise RuntimeError(f"Invalid rank {rank}")


def run_master():
    # Initialize RPC.
    rpc.init_rpc(
        name="master",
        rank=2,
        world_size=4,
        rpc_backend_options=rpc.TensorPipeRpcBackendOptions(
            init_method="tcp://localhost:29500"
        ),
    )

    # Create remote embedding module.
    remote_emb_module = RemoteModule(
        "ps",
        nn.EmbeddingBag,
        args=(NUM_EMBEDDINGS, EMBEDDING_DIM),
        kwargs={"mode": "sum"},
    )

    # Run training loop on trainers.
    futs = []
    for trainer_rank in [0, 1]:
        trainer_name = f"trainer{trainer_rank}"
        futs.append(
            rpc.rpc_async(trainer_name, _run_trainer, args=(remote_emb_module,))
        )

    # Wait for all training to finish.
    for fut in futs:
        fut.wait()

    # Shutdown.
    rpc.shutdown()

----------------------------------------

TITLE: Initializing and Re-initializing PyTorch Linear Module
DESCRIPTION: This snippet demonstrates the traditional approach of initializing a PyTorch Linear module with default parameters and then re-initializing it with a custom initialization scheme.

LANGUAGE: python
CODE:
from torch import nn

# Initializes weight from the default distribution: uniform(-1/sqrt(10), 1/sqrt(10)).
m = nn.Linear(10, 5)

# Re-initialize weight from a different distribution.
nn.init.orthogonal_(m.weight)

----------------------------------------

TITLE: Building Single Tutorial with Environment Variable in Bash
DESCRIPTION: Demonstrates how to build a single tutorial using the GALLERY_PATTERN environment variable to specify the tutorial file name. This allows for faster, targeted builds of individual tutorials.

LANGUAGE: bash
CODE:
GALLERY_PATTERN="neural_style_transfer_tutorial.py" make html

LANGUAGE: bash
CODE:
GALLERY_PATTERN="neural_style_transfer_tutorial.py" sphinx-build . _build

----------------------------------------

TITLE: Retrieving Flight Recorder Data via API in Python
DESCRIPTION: This snippet demonstrates how to retrieve Flight Recorder data using a Python API call and how to unpickle and print the retrieved data.

LANGUAGE: python
CODE:
torch._C._distributed_c10d._dump_nccl_trace(includeCollectives=True, includeStackTraces=True, onlyActive=False)

LANGUAGE: python
CODE:
t = pickle.loads(torch._C._distributed_c10d._dump_nccl_trace())
print(t)

----------------------------------------

TITLE: Using PyTorch Library API
DESCRIPTION: Reference to PyTorch library documentation for operator registration

LANGUAGE: Python
CODE:
torch.library

----------------------------------------

TITLE: Installing TorchRec and Dependencies in Python
DESCRIPTION: Installs PyTorch with CUDA support, TorchRec, and other necessary dependencies for the tutorial.

LANGUAGE: bash
CODE:
!sudo conda install pytorch cudatoolkit=11.3 -c pytorch-nightly -y
!pip3 install torchrec-nightly
!pip3 install multiprocess

----------------------------------------

TITLE: Implementing Agent for Distributed Reinforcement Learning using RPC and RRef
DESCRIPTION: Defines an Agent class that manages multiple remote observers, selects actions, records rewards, and performs training using RPC and RRef.

LANGUAGE: python
CODE:
import gym
import numpy as np

import torch
import torch.distributed.rpc as rpc
import torch.optim as optim
from torch.distributed.rpc import RRef, rpc_async, remote
from torch.distributions import Categorical

class Agent:
    def __init__(self, world_size):
        self.ob_rrefs = []
        self.agent_rref = RRef(self)
        self.rewards = {}
        self.saved_log_probs = {}
        self.policy = Policy()
        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)
        self.eps = np.finfo(np.float32).eps.item()
        self.running_reward = 0
        self.reward_threshold = gym.make('CartPole-v1').spec.reward_threshold
        for ob_rank in range(1, world_size):
            ob_info = rpc.get_worker_info(OBSERVER_NAME.format(ob_rank))
            self.ob_rrefs.append(remote(ob_info, Observer))
            self.rewards[ob_info.id] = []
            self.saved_log_probs[ob_info.id] = []

    def select_action(self, ob_id, state):
        state = torch.from_numpy(state).float().unsqueeze(0)
        probs = self.policy(state)
        m = Categorical(probs)
        action = m.sample()
        self.saved_log_probs[ob_id].append(m.log_prob(action))
        return action.item()

    def report_reward(self, ob_id, reward):
        self.rewards[ob_id].append(reward)

    def run_episode(self):
        futs = []
        for ob_rref in self.ob_rrefs:
            futs.append(
                rpc_async(
                    ob_rref.owner(),
                    ob_rref.rpc_sync().run_episode,
                    args=(self.agent_rref,)
                )
            )
        for fut in futs:
            fut.wait()

    def finish_episode(self):
        # Training code omitted for brevity
        pass

----------------------------------------

TITLE: Using Vulkan Backend in Android Java API
DESCRIPTION: Java code snippet showing how to load and run a PyTorch model on the Vulkan backend in an Android application.

LANGUAGE: java
CODE:
import org.pytorch.Device;
Module module = Module.load("$PATH", Device.VULKAN)
FloatBuffer buffer = Tensor.allocateFloatBuffer(1 * 3 * 224 * 224);
Tensor inputTensor = Tensor.fromBlob(buffer, new int[]{1, 3, 224, 224});
Tensor outputTensor = mModule.forward(IValue.from(inputTensor)).toTensor();

----------------------------------------

TITLE: C++ Library Registration
DESCRIPTION: Macro for registering custom operators in C++

LANGUAGE: C++
CODE:
TORCH_LIBRARY

----------------------------------------

TITLE: Generating CUDA Kernel Launch Statistics in Python
DESCRIPTION: Code to generate statistics about CUDA kernel launches using the TraceAnalysis object.

LANGUAGE: python
CODE:
analyzer = TraceAnalysis(trace_dir="/path/to/trace/dir")
kernel_info_df = analyzer.get_cuda_kernel_launch_stats()

----------------------------------------

TITLE: Creating Vertex AI Model
DESCRIPTION: Python code using Vertex AI SDK to create a model in the Model Registry using pre-built PyTorch container and uploaded artifacts

LANGUAGE: shell
CODE:
from google.cloud import aiplatform as vertexai
PYTORCH_PREDICTION_IMAGE_URI = (
    "us-docker.pkg.dev/vertex-ai/prediction/pytorch-gpu.1-12:latest"
)
MODEL_DISPLAY_NAME = "stable_diffusion_1_5-unique"
MODEL_DESCRIPTION = "stable_diffusion_1_5 container"

vertexai.init(project='your_project', location='us-central1', staging_bucket=BUCKET_NAME)

model = aiplatform.Model.upload(
    display_name=MODEL_DISPLAY_NAME,
    description=MODEL_DESCRIPTION,
    serving_container_image_uri=PYTORCH_PREDICTION_IMAGE_URI,
    artifact_uri=BUCKET_URI,
)

----------------------------------------

TITLE: Constructing EmbeddingBagCollection in TorchRec
DESCRIPTION: Creates an EmbeddingBagCollection with large and small embedding tables, and defines sharding constraints.

LANGUAGE: python
CODE:
from torchrec.distributed.planner.types import ParameterConstraints
from torchrec.distributed.embedding_types import EmbeddingComputeKernel
from torchrec.distributed.types import ShardingType
from typing import Dict

# ... (code for creating large_tables and small_tables)

def gen_constraints(sharding_type: ShardingType = ShardingType.TABLE_WISE) -> Dict[str, ParameterConstraints]:
    # ... (constraint generation code)

ebc = torchrec.EmbeddingBagCollection(
    device="cuda",
    tables=large_tables + small_tables
)

----------------------------------------

TITLE: Custom MulConstant Autograd Function
DESCRIPTION: Implements a custom autograd function for scalar multiplication with non-tensor parameters

LANGUAGE: cpp
CODE:
class MulConstant : public Function<MulConstant> {
 public:
  static torch::Tensor forward(AutogradContext *ctx, torch::Tensor tensor, double constant) {
    ctx->saved_data["constant"] = constant;
    return tensor * constant;
  }

  static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) {
    return {grad_outputs[0] * ctx->saved_data["constant"].toDouble(), torch::Tensor()};
  }
};

----------------------------------------

TITLE: Implementing HistoEncoder Model Wrapper
DESCRIPTION: Creates a wrapper class for the HistoEncoder model to make it compatible with TIAToolbox's ModelABC interface for feature extraction.

LANGUAGE: python
CODE:
class HistoEncWrapper(ModelABC):
    """Wrapper for HistoEnc model that conforms to tiatoolbox ModelABC interface."""

    def __init__(self: HistoEncWrapper, encoder) -> None:
        super().__init__()
        self.feat_extract = encoder

    def forward(self: HistoEncWrapper, imgs: torch.Tensor) -> torch.Tensor:
        """Pass input data through the model.

        Args:
            imgs (torch.Tensor):
                Model input.

        """
        out = F.extract_features(self.feat_extract, imgs, num_blocks=2, avg_pool=True)
        return out

    @staticmethod
    def infer_batch(
        model: nn.Module,
        batch_data: torch.Tensor,
        *,
        on_gpu: bool,
    ) -> list[np.ndarray]:
        """Run inference on an input batch.

        Contains logic for forward operation as well as i/o aggregation.

        Args:
            model (nn.Module):
                PyTorch defined model.
            batch_data (torch.Tensor):
                A batch of data generated by
                `torch.utils.data.DataLoader`.
            on_gpu (bool):
                Whether to run inference on a GPU.

        """
        img_patches_device = batch_data.to('cuda') if on_gpu else batch_data
        model.eval()
        # Do not compute the gradient (not training)
        with torch.inference_mode():
            output = model(img_patches_device)
        return [output.cpu().numpy()]

----------------------------------------

TITLE: Initializing TraceAnalysis in Python
DESCRIPTION: Code to import and initialize the TraceAnalysis class from HTA with a specified trace directory.

LANGUAGE: python
CODE:
from hta.trace_analysis import TraceAnalysis
trace_dir = "/path/to/folder/with/traces"
analyzer = TraceAnalysis(trace_dir=trace_dir)

----------------------------------------

TITLE: Analyzing Flight Recorder Dumps with torchfrtrace
DESCRIPTION: This command shows how to use the torchfrtrace tool to analyze the Flight Recorder dump files generated by the example script.

LANGUAGE: shell
CODE:
torchfrtrace --prefix "trace_" /tmp/

----------------------------------------

TITLE: Demonstrating Basic Fork and Wait Syntax in TorchScript
DESCRIPTION: This snippet shows how to use torch.jit.fork() and torch.jit.wait() to parallelize function execution in TorchScript. It demonstrates forking a task, performing normal execution, and waiting for the parallel task to complete.

LANGUAGE: python
CODE:
import torch

def foo(x):
    return torch.neg(x)

@torch.jit.script
def example(x):
    # Call `foo` using parallelism:
    # First, we "fork" off a task. This task will run `foo` with argument `x`
    future = torch.jit.fork(foo, x)

    # Call `foo` normally
    x_normal = foo(x)

    # Second, we "wait" on the task. Since the task may be running in
    # parallel, we have to "wait" for its result to become available.
    # Notice that by having lines of code between the "fork()" and "wait()"
    # call for a given Future, we can overlap computations so that they
    # run in parallel.
    x_parallel = torch.jit.wait(future)

    return x_normal, x_parallel

print(example(torch.ones(1))) # (-1., -1.)

----------------------------------------

TITLE: Enabling bfloat16 Fast Math Kernels
DESCRIPTION: Setting an environment variable to enable fast math GEMM kernels using bfloat16 precision for improved performance.

LANGUAGE: bash
CODE:
export DNNL_DEFAULT_FPMATH_MODE=BF16

----------------------------------------

TITLE: Disabling Autoloading to Prevent Circular Imports
DESCRIPTION: Example of disabling autoloading using an environment variable to prevent circular imports in Torch NPU.

LANGUAGE: python
CODE:
# Disable autoloading before running 'import torch'
os.environ['TORCH_DEVICE_BACKEND_AUTOLOAD'] = '0'

import torch

----------------------------------------

TITLE: Implementing Custom Generator for New Backend in C++
DESCRIPTION: Shows how to create a custom generator for a new backend by inheriting from GeneratorImpl and registering it with REGISTER_GENERATOR_PRIVATEUSE1.

LANGUAGE: cpp
CODE:
struct CustomGeneratorImpl : public c10::GeneratorImpl {
  // Implementation of generator in new backend
}

at::Generator make_custom_generator(c10::DeviceIndex device_index) {
  return at::make_generator<CustomGeneratorImpl>(device_index);
}

REGISTER_GENERATOR_PRIVATEUSE1(make_cumstom_generator)

----------------------------------------

TITLE: Generating Trace with Augmented Counters in Python
DESCRIPTION: Code to generate a new trace file with memory bandwidth and queue length counters using the TraceAnalysis object.

LANGUAGE: python
CODE:
analyzer = TraceAnalysis(trace_dir = "/path/to/trace/folder")
analyzer.generate_trace_with_counters()

----------------------------------------

TITLE: Generating Backend-Specific Methods and Properties in Python
DESCRIPTION: Shows how to automatically generate methods and properties for the new backend in various PyTorch modules.

LANGUAGE: python
CODE:
torch.rename_privateuse1_backend("npu")
unsupported_dtype = [torch.quint8]
torch.utils.generate_methods_for_privateuse1_backend(for_tensor=True, for_module=True, for_storage=True, unsupported_dtype=unsupported_dtype)

----------------------------------------

TITLE: Parallelizing Ensemble Models in TorchScript
DESCRIPTION: This example shows how to parallelize multiple models in an ensemble using fork() and wait(). It demonstrates launching tasks for each model and collecting results in parallel.

LANGUAGE: python
CODE:
class LSTMEnsemble(torch.nn.Module):
    def __init__(self, n_models):
        super().__init__()
        self.n_models = n_models
        self.models = torch.nn.ModuleList([
            BidirectionalRecurrentLSTM() for _ in range(self.n_models)])

    def forward(self, x : torch.Tensor) -> torch.Tensor:
        futures = [torch.jit.fork(model, x) for model in self.models]
        results = [torch.jit.wait(fut) for fut in futures]
        return torch.stack(results).sum(dim=0)

----------------------------------------

TITLE: Profiling Inference Performance
DESCRIPTION: Using PyTorch's profiler to measure the inference performance with a larger batch size (256).

LANGUAGE: python
CODE:
X = torch.rand(256, 64, 64, device=device)

with torch.set_grad_enabled(False):
    for _ in range(50):
        model(X) #Warmup
    with profile(activities=[ProfilerActivity.CPU]) as prof:
        with record_function("mymodel_inference"):
            for _ in range(100):
                model(X)

print(prof.key_averages().table(sort_by="self_cpu_time_total"))

----------------------------------------

TITLE: Setting Intel Compiler for Windows Inductor
DESCRIPTION: Command to set the Intel Compiler as the Windows Inductor Compiler using the CXX environment variable.

LANGUAGE: sh
CODE:
set CXX=icx-cl

----------------------------------------

TITLE: Installing TCMalloc on Ubuntu
DESCRIPTION: Command to install TCMalloc (Google Perftools) on Ubuntu for optimized memory allocation.

LANGUAGE: bash
CODE:
$ apt-get install google-perftools

----------------------------------------

TITLE: Running ResNet50 Inference in PyTorch
DESCRIPTION: Example code for running ResNet50 inference on a dummy tensor in PyTorch, used to demonstrate performance optimization techniques.

LANGUAGE: python
CODE:
import torch
import torchvision.models as models
import time

model = models.resnet50(pretrained=False)
model.eval()
data = torch.rand(1, 3, 224, 224)

# warm up
for _ in range(100):
    model(data)

start = time.time()
for _ in range(100):
    model(data)
end = time.time()
print('Inference took {:.2f} ms in average'.format((end-start)/100*1000))

----------------------------------------

TITLE: Tracer-Based Model Splitting
DESCRIPTION: Implements automatic model splitting using the pipeline API's tracer-based functionality with split specifications.

LANGUAGE: python
CODE:
def tracer_model_split(model, example_input_microbatch) -> PipelineStage:
   pipe = pipeline(
      module=model,
      mb_args=(example_input_microbatch,),
      split_spec={
         "layers.4": SplitPoint.BEGINNING,
      }
   )
   stage = pipe.build_stage(stage_index, device, pp_group)
   return stage

----------------------------------------

TITLE: CMake Build Configuration
DESCRIPTION: CMake configuration for building the custom C++ class as a shared library that can be loaded by PyTorch.

LANGUAGE: cmake
CODE:
cmake_minimum_required(VERSION 3.1 FATAL_ERROR)
project(custom_class)

find_package(Torch REQUIRED)
torch_library(custom_class class.cpp)

----------------------------------------

TITLE: Importing Libraries and Capturing FX Graph for PyTorch 2 Export Quantization
DESCRIPTION: This snippet shows the necessary imports and the process of capturing an FX graph from an eager PyTorch model using the capture_pre_autograd_graph function.

LANGUAGE: python
CODE:
import torch
import torchvision.models as models
import copy
from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e
import torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq
from torch.ao.quantization.quantizer.x86_inductor_quantizer import X86InductorQuantizer
from torch._export import capture_pre_autograd_graph

# Create the Eager Model
model_name = "resnet18"
model = models.__dict__[model_name](pretrained=True)

# Set the model to eval mode
model = model.eval()

# Create the data, using the dummy data here as an example
traced_bs = 50
x = torch.randn(traced_bs, 3, 224, 224).contiguous(memory_format=torch.channels_last)
example_inputs = (x,)

# Capture the FX Graph to be quantized
with torch.no_grad():
     exported_model = capture_pre_autograd_graph(
         model,
         example_inputs
     )

----------------------------------------

TITLE: Visualizing Top 10 Operator Duration Changes in PyTorch Traces
DESCRIPTION: Shows how to identify and visualize the top 10 operators with the largest changes in duration, excluding ProfilerStep entries that might overshadow other operators.

LANGUAGE: python
CODE:
df = compare_traces_output.sort_values(by="diff_duration", ascending=False)
# The duration differerence can be overshadowed by the "ProfilerStep",
# so we can filter it out to show the trend of other operators.
df = df.loc[~df.index.str.startswith("ProfilerStep")].head(10)
TraceDiff.visualize_duration_diff(df)

----------------------------------------

TITLE: Main Pipeline Execution
DESCRIPTION: Sets up and executes the pipeline schedule with model initialization, data preparation, and loss function definition.

LANGUAGE: python
CODE:
if __name__ == "__main__":
   init_distributed()
   num_microbatches = 4
   model_args = ModelArgs()
   model = Transformer(model_args)

   # Dummy data
   x = torch.ones(32, 500, dtype=torch.long)
   y = torch.randint(0, model_args.vocab_size, (32, 500), dtype=torch.long)
   example_input_microbatch = x.chunk(num_microbatches)[0]

   # Option 1: Manual model splitting
   stage = manual_model_split(model)

   # Option 2: Tracer model splitting
   # stage = tracer_model_split(model, example_input_microbatch)

   model.to(device)
   x = x.to(device)
   y = y.to(device)

   def tokenwise_loss_fn(outputs, targets):
      loss_fn = nn.CrossEntropyLoss()
      outputs = outputs.reshape(-1, model_args.vocab_size)
      targets = targets.reshape(-1)
      return loss_fn(outputs, targets)

   schedule = ScheduleGPipe(stage, n_microbatches=num_microbatches, loss_fn=tokenwise_loss_fn)

   if rank == 0:
      schedule.step(x)
   elif rank == 1:
      losses = []
      output = schedule.step(target=y, losses=losses)
      print(f"losses: {losses}")
   dist.destroy_process_group()

----------------------------------------

TITLE: Visualizing Top 10 Operator Frequency Changes in PyTorch Traces
DESCRIPTION: Demonstrates how to sort and visualize the top 10 operators with the largest increase in frequency using the compare_traces output and TraceDiff visualization method.

LANGUAGE: python
CODE:
df = compare_traces_output.sort_values(by="diff_counts", ascending=False).head(10)
TraceDiff.visualize_counts_diff(df)

----------------------------------------

TITLE: C++ Inference Example
DESCRIPTION: Example showing how to load and run a TorchScript model containing custom classes in C++.

LANGUAGE: cpp
CODE:
torch::jit::Module module = torch::jit::load("foo.pt");
module.run();

----------------------------------------

TITLE: Building PyTorch Extension for Custom Backend
DESCRIPTION: Example setup.py script for building a C++ extension to add a custom backend to PyTorch.

LANGUAGE: python
CODE:
from setuptools import setup
from torch.utils.cpp_extension import BuildExtension, CppExtension

setup(
    name='torch_xla',
    ext_modules=[
        CppExtension(
            '_XLAC',
            torch_xla_sources,
            include_dirs=include_dirs,
            extra_compile_args=extra_compile_args,
            library_dirs=library_dirs,
            extra_link_args=extra_link_args + \
                [make_relative_rpath('torch_xla/lib')],
        ),
    ],
    cmdclass={
        'build_ext': Build,  # Build is a derived class of BuildExtension
    }
    # more configs...
)

----------------------------------------

TITLE: Global Configuration Setup
DESCRIPTION: Configuration setup for model parameters, paths and other global settings.

LANGUAGE: python
CODE:
configs = Namespace()

configs.output_dir = "./MRPC/"
configs.data_dir = "./glue_data/MRPC"
configs.model_name_or_path = "bert-base-uncased"
configs.max_seq_length = 128

configs.task_name = "MRPC".lower()
configs.processor = processors[configs.task_name]()
configs.output_mode = output_modes[configs.task_name]
configs.label_list = configs.processor.get_labels()
configs.model_type = "bert".lower()
configs.do_lower_case = True

configs.device = "cpu"
configs.per_gpu_eval_batch_size = 8
configs.n_gpu = 0
configs.local_rank = -1
configs.overwrite_cache = False

----------------------------------------

TITLE: ResNet50 Inference Benchmarking with PyTorch
DESCRIPTION: Script to measure average inference time of ResNet50 model using PyTorch with ITT profiling enabled

LANGUAGE: python
CODE:
import torch
import torchvision.models as models
import time

model = models.resnet50(pretrained=False)
model.eval()
batch_size = 32
data = torch.rand(batch_size, 3, 224, 224)

# warm up
for _ in range(100):
    model(data)

# measure 
# Intel® VTune Profiler's ITT context manager
with torch.autograd.profiler.emit_itt():
    start = time.time()
    for i in range(100):
   # Intel® VTune Profiler's ITT to annotate each step
        torch.profiler.itt.range_push('step_{}'.format(i))
        model(data)
        torch.profiler.itt.range_pop()
    end = time.time()

print('Inference took {:.2f} ms in average'.format((end-start)/100*1000))

----------------------------------------

TITLE: Implementing Blocking Point-to-Point Communication in PyTorch
DESCRIPTION: Demonstrates blocking point-to-point communication between two processes using send and recv functions. Process 0 sends a tensor to process 1.

LANGUAGE: python
CODE:
"""Blocking point-to-point communication."""

def run(rank, size):
    tensor = torch.zeros(1)
    if rank == 0:
        tensor += 1
        # Send the tensor to process 1
        dist.send(tensor=tensor, dst=1)
    else:
        # Receive tensor from process 0
        dist.recv(tensor=tensor, src=0)
    print('Rank ', rank, ' has data ', tensor[0])

----------------------------------------

TITLE: Vector-Jacobian Product Example
DESCRIPTION: Shows how to compute vector-Jacobian products using autograd in C++

LANGUAGE: cpp
CODE:
x = torch::randn(3, torch::requires_grad());
y = x * 2;
while (y.norm().item<double>() < 1000) {
  y = y * 2;
}
auto v = torch::tensor({0.1, 1.0, 0.0001}, torch::kFloat);
y.backward(v);

----------------------------------------

TITLE: Registering Custom Operator with TorchScript
DESCRIPTION: C++ code to register the custom warp_perspective operator with the TorchScript runtime using the TORCH_LIBRARY macro.

LANGUAGE: C++
CODE:
TORCH_LIBRARY(my_ops, m) {
  m.def("warp_perspective", &warp_perspective);
}

----------------------------------------

TITLE: Training Step Function Implementation
DESCRIPTION: Encapsulated training step function containing forward pass, loss calculation, backward pass, and optimizer update.

LANGUAGE: cpp
CODE:
void training_step(
    Net& model,
    torch::optim::Optimizer& optimizer,
    torch::Tensor& data,
    torch::Tensor& targets,
    torch::Tensor& output,
    torch::Tensor& loss) {
  optimizer.zero_grad();
  output = model.forward(data);
  loss = torch::nll_loss(output, targets);
  loss.backward();
  optimizer.step();
}

----------------------------------------

TITLE: Setting Environment Variables for NCCL Debugging in PyTorch
DESCRIPTION: These commands set environment variables to enable verbose logging for NCCL (NVIDIA Collective Communications Library) and specify the network interface for the distributed backend in PyTorch.

LANGUAGE: bash
CODE:
export NCCL_DEBUG=INFO
export NCCL_SOCKET_IFNAME=eth0

----------------------------------------

TITLE: Converting Trained Model to Quantized Model
DESCRIPTION: This snippet shows how to convert the trained quantization-aware model to a fully quantized model and move it to evaluation mode for inference.

LANGUAGE: python
CODE:
quantized_model = convert_pt2e(prepared_model)

# move certain ops like dropout to eval mode, equivalent to `m.eval()`
torch.ao.quantization.move_exported_model_to_eval(m)

print(quantized_model)

top1, top5 = evaluate(quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches)
print('Final evaluation accuracy on %d images, %2.2f' % (num_eval_batches * eval_batch_size, top1.avg))

----------------------------------------

TITLE: Measuring TCPStore Initialization Time in Python
DESCRIPTION: This code snippet demonstrates how to measure the initialization time of TCPStore using environment variables for configuration. It uses the perf_counter function to calculate the elapsed time.

LANGUAGE: python
CODE:
import logging
import os

from time import perf_counter

import torch
import torch.distributed as dist

logger: logging.Logger = logging.getLogger(__name__)

# Env var are preset when launching the benchmark
env_rank = os.environ.get("RANK", 0)
env_world_size = os.environ.get("WORLD_SIZE", 1)
env_master_addr = os.environ.get("MASTER_ADDR", "localhost")
env_master_port = os.environ.get("MASTER_PORT", "23456")

start = perf_counter()
tcp_store = dist.TCPStore(
    env_master_addr,
    int(env_master_port),
    world_size=int(env_world_size),
    is_master=(int(env_rank) == 0),
)
end = perf_counter()
time_elapsed = end - start
logger.info(
    f"Complete TCPStore init with rank={env_rank}, world_size={env_world_size} in {time_elapsed} seconds."
)

----------------------------------------

TITLE: Quantization Configuration and Execution
DESCRIPTION: Example of configuring and executing the quantization process with custom module handling.

LANGUAGE: python
CODE:
qconfig_mapping = QConfigMapping.set_global(qconfig)

prepare_custom_config_dict = {
    "non_traceable_module_name": "non_traceable_submodule",
    "non_traceable_module_class": [MNonTraceable],
}
model_prepared = prepare_fx(
    model_fp32,
    qconfig_mapping,
    example_inputs,
    prepare_custom_config_dict=prepare_custom_config_dict,
)

----------------------------------------

TITLE: CMake Configuration for Custom Operator
DESCRIPTION: CMake configuration for building the custom operator into a shared library.

LANGUAGE: CMake
CODE:
find_package(OpenCV REQUIRED)
add_library(warp_perspective SHARED op.cpp)
target_compile_features(warp_perspective PRIVATE cxx_range_for)
target_link_libraries(warp_perspective PRIVATE "${TORCH_LIBRARIES}")
target_link_libraries(warp_perspective PRIVATE opencv_core opencv_photo)

----------------------------------------

TITLE: CUDA Graph Warmup Implementation
DESCRIPTION: Warmup iterations implementation to prepare CUDA cache and libraries before graph capture.

LANGUAGE: cpp
CODE:
at::cuda::CUDAStream warmupStream = at::cuda::getStreamFromPool();
at::cuda::setCurrentCUDAStream(warmupStream);
for (int iter = 0; iter < num_warmup_iters; iter++) {
  training_step(model, optimizer, data, targets, output, loss);
}

----------------------------------------

TITLE: Configuring Raspberry Pi for Camera Support
DESCRIPTION: Edit the /boot/config.txt file to enable camera features and set appropriate memory allocation for camera processing.

LANGUAGE: toml
CODE:
# This enables the extended features such as the camera.
start_x=1

# This needs to be at least 128M for the camera processing, if it's bigger you can just leave it as is.
gpu_mem=128

# You need to commment/remove the existing camera_auto_detect line since this causes issues with OpenCV/V4L2 capture.
#camera_auto_detect=1

----------------------------------------

TITLE: Inflatable Arguments Implementation in PyTorch
DESCRIPTION: Demonstrates the creation and usage of inflatable arguments for compressing model inputs, including custom inflation functions and handling complex input types.

LANGUAGE: python
CODE:
def create_example(*size, dtype=None):
    """Generate a tuple of 2 random tensors both of the specified size"""

    deflated_input = (torch.zeros(1, dtype=dtype).expand(*size), torch.zeros(1, dtype=dtype).expand(*size))

    return torch.utils.bundled_inputs.InflatableArg(
        value=stub,
        fmt="(torch.randn_like({0}[0]), torch.randn_like({0}[1]))",
    )

----------------------------------------

TITLE: Basic Asynchronous DCP Checkpointing Implementation
DESCRIPTION: Demonstrates basic asynchronous checkpointing with FSDP model using DCP. Includes AppState wrapper for managing model and optimizer state, and implementation of a simple training loop with async checkpoint saves.

LANGUAGE: python
CODE:
import os

import torch
import torch.distributed as dist
import torch.distributed.checkpoint as dcp
import torch.multiprocessing as mp
import torch.nn as nn

from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.checkpoint.state_dict import get_state_dict, set_state_dict
from torch.distributed.checkpoint.stateful import Stateful
from torch.distributed.fsdp.fully_sharded_data_parallel import StateDictType

CHECKPOINT_DIR = "checkpoint"


class AppState(Stateful):
    def __init__(self, model, optimizer=None):
        self.model = model
        self.optimizer = optimizer

    def state_dict(self):
        model_state_dict, optimizer_state_dict = get_state_dict(model, optimizer)
        return {
            "model": model_state_dict,
            "optim": optimizer_state_dict
        }

    def load_state_dict(self, state_dict):
        set_state_dict(
            self.model,
            self.optimizer,
            model_state_dict=state_dict["model"],
            optim_state_dict=state_dict["optim"]
        )

class ToyModel(nn.Module):
    def __init__(self):
        super(ToyModel, self).__init__()
        self.net1 = nn.Linear(16, 16)
        self.relu = nn.ReLU()
        self.net2 = nn.Linear(16, 8)

    def forward(self, x):
        return self.net2(self.relu(self.net1(x)))


def setup(rank, world_size):
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "12355 "
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)


def cleanup():
    dist.destroy_process_group()


def run_fsdp_checkpoint_save_example(rank, world_size):
    print(f"Running basic FSDP checkpoint saving example on rank {rank}.")
    setup(rank, world_size)

    model = ToyModel().to(rank)
    model = FSDP(model)

    loss_fn = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

    checkpoint_future = None
    for step in range(10):
        optimizer.zero_grad()
        model(torch.rand(8, 16, device="cuda")).sum().backward()
        optimizer.step()

        if checkpoint_future is not None:
            checkpoint_future.result()

        state_dict = { "app": AppState(model, optimizer) }
        checkpoint_future = dcp.async_save(state_dict, checkpoint_id=f"{CHECKPOINT_DIR}_step{step}")

    cleanup()


if __name__ == "__main__":
    world_size = torch.cuda.device_count()
    print(f"Running async checkpoint example on {world_size} devices.")
    mp.spawn(
        run_fsdp_checkpoint_save_example,
        args=(world_size,),
        nprocs=world_size,
        join=True,
    )

----------------------------------------

TITLE: Configuring PyTorch MNIST Project Build with CMake
DESCRIPTION: Sets up a CMake build environment for a PyTorch MNIST project. Configures C++17, finds required dependencies (PyTorch and Threads), downloads MNIST dataset, and sets up executable compilation with proper linking. Includes special handling for MSVC to copy DLL files.

LANGUAGE: cmake
CODE:
cmake_minimum_required(VERSION 3.18 FATAL_ERROR)
project(mnist)
set(CMAKE_CXX_STANDARD 17)

find_package(Torch REQUIRED)
find_package(Threads REQUIRED)

option(DOWNLOAD_MNIST "Download the MNIST dataset from the internet" ON)
if (DOWNLOAD_MNIST)
  message(STATUS "Downloading MNIST dataset")
  execute_process(
    COMMAND python ${CMAKE_CURRENT_LIST_DIR}/../tools/download_mnist.py
      -d ${CMAKE_BINARY_DIR}/data
    ERROR_VARIABLE DOWNLOAD_ERROR)
  if (DOWNLOAD_ERROR)
    message(FATAL_ERROR "Error downloading MNIST dataset: ${DOWNLOAD_ERROR}")
  endif()
endif()

add_executable(mnist mnist.cpp)
target_compile_features(mnist PUBLIC cxx_range_for)
target_link_libraries(mnist ${TORCH_LIBRARIES} ${CMAKE_THREAD_LIBS_INIT})

if (MSVC)
  file(GLOB TORCH_DLLS "${TORCH_INSTALL_PREFIX}/lib/*.dll")
  add_custom_command(TARGET mnist
                     POST_BUILD
                     COMMAND ${CMAKE_COMMAND} -E copy_if_different
                     ${TORCH_DLLS}
                     $<TARGET_FILE_DIR:mnist>)
endif (MSVC)

----------------------------------------

TITLE: JIT Compilation of Custom Operator in Python
DESCRIPTION: Python code demonstrating how to use PyTorch's JIT compilation feature to compile and load the custom operator.

LANGUAGE: Python
CODE:
import torch.utils.cpp_extension

torch.utils.cpp_extension.load(
    name="warp_perspective",
    sources=["op.cpp"],
    extra_ldflags=["-lopencv_core", "-lopencv_imgproc"],
    is_python_module=False,
    verbose=True
)

print(torch.ops.my_ops.warp_perspective)

----------------------------------------

TITLE: Version Number Specification
DESCRIPTION: Specifies the version number 3.8, which could represent a Python version requirement or other versioning information.

LANGUAGE: plaintext
CODE:
3.8

----------------------------------------

TITLE: HTML Meta Refresh Redirect for PyTorch Audio Tutorial
DESCRIPTION: HTML meta tag that automatically redirects users to the new tutorial location after 3 seconds

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/audio/stable/tutorials/audio_data_augmentation_tutorial.html'" />

----------------------------------------

TITLE: Defining Quantization Constraints
DESCRIPTION: Sets up DTypeConfig with specific constraints for quantization, including dtype restrictions and scale ranges.

LANGUAGE: python
CODE:
quint8_with_constraints = DTypeWithConstraints(
    dtype=torch.quint8,
    quant_min_lower_bound=0,
    quant_max_upper_bound=255,
    scale_min_lower_bound=2 ** -12,
)

weighted_int8_dtype_config = DTypeConfig(
    input_dtype=quint8_with_constraints,
    output_dtype=quint8_with_constraints,
    weight_dtype=torch.qint8,
    bias_dtype=torch.float)

----------------------------------------

TITLE: CUDA Implementation of Custom Operator
DESCRIPTION: CUDA kernel and wrapper implementation of multiply-add operation

LANGUAGE: cpp
CODE:
__global__ void muladd_kernel(int numel, const float* a, const float* b, float c, float* result) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < numel) result[idx] = a[idx] * b[idx] + c;
}

at::Tensor mymuladd_cuda(const at::Tensor& a, const at::Tensor& b, double c) {
  TORCH_CHECK(a.sizes() == b.sizes());
  TORCH_CHECK(a.dtype() == at::kFloat);
  TORCH_CHECK(b.dtype() == at::kFloat);
  TORCH_INTERNAL_ASSERT(a.device().type() == at::DeviceType::CUDA);
  TORCH_INTERNAL_ASSERT(b.device().type() == at::DeviceType::CUDA);
  at::Tensor a_contig = a.contiguous();
  at::Tensor b_contig = b.contiguous();
  at::Tensor result = torch::empty(a_contig.sizes(), a_contig.options());
  const float* a_ptr = a_contig.data_ptr<float>();
  const float* b_ptr = b_contig.data_ptr<float>();
  float* result_ptr = result.data_ptr<float>();

  int numel = a_contig.numel();
  muladd_kernel<<<(numel+255)/256, 256>>>(numel, a_ptr, b_ptr, c, result_ptr);
  return result;
}

----------------------------------------

TITLE: Training Function for PyTorch Models
DESCRIPTION: Defines a function to train a PyTorch model, including learning rate scheduling and saving the best model. Handles both training and validation phases.

LANGUAGE: python
CODE:
def train_model(model, criterion, optimizer, scheduler, num_epochs=25, device='cpu'):
    since = time.time()

    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch, num_epochs - 1))
        print('-' * 10)

        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
            else:
                model.eval()

            running_loss = 0.0
            running_corrects = 0

            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                optimizer.zero_grad()

                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)
            if phase == 'train':
                scheduler.step()

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]

            print('{} Loss: {:.4f} Acc: {:.4f}'.format(
                phase, epoch_loss, epoch_acc))

            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())

        print()

    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(
        time_elapsed // 60, time_elapsed % 60))
    print('Best val Acc: {:4f}'.format(best_acc))

    model.load_state_dict(best_model_wts)
    return model

----------------------------------------

TITLE: Torchscript and JIT Operations
DESCRIPTION: Core functions for model tracing and scripting in PyTorch

LANGUAGE: python
CODE:
torch.jit.trace()         # takes your module or function and an example data input
@script                   # decorator used to indicate data-dependent control flow

----------------------------------------

TITLE: CUDA Kernel Implementation for LLTM Forward Pass
DESCRIPTION: CUDA kernel implementation of the LLTM forward pass using PackedTensorAccessor for improved readability and efficiency.

LANGUAGE: cpp
CODE:
template <typename scalar_t>
__global__ void lltm_cuda_forward_kernel(
    const torch::PackedTensorAccessor32<scalar_t,3,torch::RestrictPtrTraits> gates,
    const torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits> old_cell,
    torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits> new_h,
    torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits> new_cell,
    torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits> input_gate,
    torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits> output_gate,
    torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits> candidate_cell) {
    const int n = blockIdx.y;
    const int c = blockIdx.x * blockDim.x + threadIdx.x;
    if (c < gates.size(2)){
        input_gate[n][c] = sigmoid(gates[n][0][c]);
        output_gate[n][c] = sigmoid(gates[n][1][c]);
        candidate_cell[n][c] = elu(gates[n][2][c]);
        new_cell[n][c] = old_cell[n][c] + candidate_cell[n][c] * input_gate[n][c];
        new_h[n][c] = tanh(new_cell[n][c]) * output_gate[n][c];
    }
}

----------------------------------------

TITLE: HTML Meta Refresh Redirect for PyTorch Audio Tutorial
DESCRIPTION: An HTML meta refresh tag that automatically redirects users to the new Audio I/O tutorial location after 3 seconds.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/audio/stable/tutorials/audio_io_tutorial.html'" />

----------------------------------------

TITLE: Implementing Custom Warp Perspective Operator in C++
DESCRIPTION: C++ implementation of a custom warp_perspective operator using OpenCV. It takes an image tensor and warp matrix as inputs, applies perspective transformation, and returns the output tensor.

LANGUAGE: C++
CODE:
#include <opencv2/opencv.hpp>
#include <torch/script.h>

torch::Tensor warp_perspective(torch::Tensor image, torch::Tensor warp) {
  cv::Mat image_mat(/*rows=*/image.size(0),
                    /*cols=*/image.size(1),
                    /*type=*/CV_32FC1,
                    /*data=*/image.data<float>());
  cv::Mat warp_mat(/*rows=*/warp.size(0),
                   /*cols=*/warp.size(1),
                   /*type=*/CV_32FC1,
                   /*data=*/warp.data<float>());

  cv::Mat output_mat;
  cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{64, 64});

  torch::Tensor output =
    torch::from_blob(output_mat.ptr<float>(), /*sizes=*/{64, 64});
  return output.clone();
}

TORCH_LIBRARY(my_ops, m) {
  m.def("warp_perspective", &warp_perspective);
}

----------------------------------------

TITLE: ONNX Model Export and Loading
DESCRIPTION: Functions for exporting and loading ONNX format models

LANGUAGE: python
CODE:
torch.onnx.export(model, dummy data, xxxx.proto)
model = onnx.load("alexnet.proto")
onnx.checker.check_model(model)
onnx.helper.printable_graph(model.graph)

----------------------------------------

TITLE: CMake Configuration for PyTorch C++
DESCRIPTION: CMake configuration file for building a PyTorch C++ project with the required dependencies.

LANGUAGE: cmake
CODE:
cmake_minimum_required(VERSION 3.0 FATAL_ERROR)
project(dcgan)

find_package(Torch REQUIRED)

add_executable(dcgan dcgan.cpp)
target_link_libraries(dcgan "${TORCH_LIBRARIES}")
set_property(TARGET dcgan PROPERTY CXX_STANDARD 14)

----------------------------------------

TITLE: Implementing HTML Meta Refresh for PyTorch Audio Tutorial Redirect
DESCRIPTION: This HTML snippet creates an automatic redirect to the new location of the PyTorch Audio Resampling tutorial. It uses the meta http-equiv attribute to refresh the page after 3 seconds, directing users to the updated tutorial URL.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/audio/stable/tutorials/audio_resampling_tutorial.html'" />

----------------------------------------

TITLE: GPU Implementation - C++ Wrapper
DESCRIPTION: Generated C++ wrapper code for GPU showing CUDA integration and kernel launching using C++ APIs.

LANGUAGE: cpp
CODE:
std::vector<at::Tensor> inductor_entry_cpp(const std::vector<at::Tensor>& args) {
    at::Tensor arg0_1 = args[0];
    at::Tensor constant0 = args[1];

    at::cuda::CUDAGuard device_guard(0);
    auto buf0 = at::empty_strided({19L, }, {1L, }, at::TensorOptions(c10::Device(at::kCUDA, 0)).dtype(at::kFloat));
    if (triton_poi_fused_add_lift_fresh_0 == nullptr) {
        triton_poi_fused_add_lift_fresh_0 = loadKernel("/tmp/torchinductor_user/mm/cmm6xjgijjffxjku4akv55eyzibirvw6bti6uqmfnruujm5cvvmw.cubin", "triton_poi_fused_add_lift_fresh_0_0d1d2d3");
    }
    CUdeviceptr var_0 = reinterpret_cast<CUdeviceptr>(constant0.data_ptr());
    CUdeviceptr var_1 = reinterpret_cast<CUdeviceptr>(arg0_1.data_ptr());
    CUdeviceptr var_2 = reinterpret_cast<CUdeviceptr>(buf0.data_ptr());
    auto var_3 = 19;
    void* kernel_args_var_0[] = {&var_0, &var_1, &var_2, &var_3};
    cudaStream_t stream0 = at::cuda::getCurrentCUDAStream(0);
    launchKernel(triton_poi_fused_add_lift_fresh_0, 1, 1, 1, 1, 0, kernel_args_var_0, stream0);
    arg0_1.reset();
    return {buf0};
}

----------------------------------------

TITLE: Converting PyTorch Model to TorchScript via Annotation
DESCRIPTION: Shows how to directly write a model in TorchScript and annotate it for compilation, useful when the model contains control flow dependent on input.

LANGUAGE: python
CODE:
class MyModule(torch.nn.Module):
    def __init__(self, N, M):
        super(MyModule, self).__init__()
        self.weight = torch.nn.Parameter(torch.rand(N, M))

    def forward(self, input):
        if input.sum() > 0:
          output = self.weight.mv(input)
        else:
          output = self.weight + input
        return output

my_module = MyModule(10,20)
sm = torch.jit.script(my_module)

----------------------------------------

TITLE: CPU Implementation - Python Wrapper
DESCRIPTION: Default Python wrapper implementation for CPU showing how Inductor generates Python code to invoke kernels.

LANGUAGE: python
CODE:
def call(args):
    arg0_1, = args
    args.clear()
    assert_size_stride(arg0_1, (1, ), (1, ))
    buf0 = empty_strided((19, ), (1, ), device='cpu', dtype=torch.float32)
    cpp_fused_add_lift_fresh_0(c_void_p(constant0.data_ptr()), c_void_p(arg0_1.data_ptr()), c_void_p(buf0.data_ptr()))
    del arg0_1
    return (buf0, )

----------------------------------------

TITLE: Vision and Distributed Training Imports
DESCRIPTION: Imports for computer vision tasks and distributed training capabilities

LANGUAGE: python
CODE:
from torchvision import datasets, models, transforms
import torchvision.transforms as transforms
import torch.distributed as dist
from multiprocessing import Process

----------------------------------------

TITLE: Implementing Non-Blocking Point-to-Point Communication in PyTorch
DESCRIPTION: Shows non-blocking point-to-point communication using isend and irecv functions. Processes continue execution and use wait() to ensure communication completion.

LANGUAGE: python
CODE:
"""Non-blocking point-to-point communication."""

def run(rank, size):
    tensor = torch.zeros(1)
    req = None
    if rank == 0:
        tensor += 1
        # Send the tensor to process 1
        req = dist.isend(tensor=tensor, dst=1)
        print('Rank 0 started sending')
    else:
        # Receive tensor from process 0
        req = dist.irecv(tensor=tensor, src=0)
        print('Rank 1 started receiving')
    req.wait()
    print('Rank ', rank, ' has data ', tensor[0])

----------------------------------------

TITLE: Redirecting to PyTorch Tutorials Homepage using HTML Meta Tag
DESCRIPTION: This HTML snippet uses a meta tag to automatically redirect the user to the PyTorch tutorials homepage after a 3-second delay. It's used to handle navigation away from the deprecated tutorial page.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/'" />

----------------------------------------

TITLE: Calibrating Quantized Model in PyTorch
DESCRIPTION: Runs calibration on the prepared model using a sample of the dataset to collect statistics for quantization parameters.

LANGUAGE: python
CODE:
def calibrate(model, data_loader):
    model.eval()
    with torch.no_grad():
        for image, target in data_loader:
            model(image)
calibrate(prepared_model, data_loader_test)  # run calibration on sample data

----------------------------------------

TITLE: Reading and Processing WSI Patch Data
DESCRIPTION: Loads patch data from the Kather100k dataset and creates lists of patches with corresponding labels for tissue classification.

LANGUAGE: python
CODE:
# Read the patch data and create a list of patches and a list of corresponding labels
dataset_path = global_save_dir / "kather100k-validation-sample"

# Set the path to the dataset
image_ext = ".tif"  # file extension of each image

# Obtain the mapping between the label ID and the class name
label_dict = {
    "BACK": 0, # Background (empty glass region)
    "NORM": 1, # Normal colon mucosa
    "DEB": 2,  # Debris
    "TUM": 3,  # Colorectal adenocarcinoma epithelium
    "ADI": 4,  # Adipose
    "MUC": 5,  # Mucus
    "MUS": 6,  # Smooth muscle
    "STR": 7,  # Cancer-associated stroma
    "LYM": 8,  # Lymphocytes
}

class_names = list(label_dict.keys())
class_labels = list(label_dict.values())

----------------------------------------

TITLE: Using the TorchScript-enabled Distributed Optimizer
DESCRIPTION: Demonstrates how to use the custom distributed optimizer in practice with remote parameters.

LANGUAGE: python
CODE:
remote_params_list = [...]
dist_optim = DistributedOptimizer(
    QHM, remote_params_list, *args, **kwargs
)

----------------------------------------

TITLE: HTML Meta Redirect for PyTorch Audio Tutorial
DESCRIPTION: HTML meta refresh tag that automatically redirects users to the new tutorial location after 3 seconds.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/audio/stable/tutorials/audio_feature_extractions_tutorial.html'" />

----------------------------------------

TITLE: Initializing TCPStore with Legacy Backend in Python
DESCRIPTION: This code snippet demonstrates how to initialize TCPStore with a listen_fd using the legacy backend by setting use_libuv=False. It also shows the default behavior which raises a NotImplementedError with the libuv backend.

LANGUAGE: python
CODE:
import socket

import torch
import torch.distributed as dist

listen_sock: socket.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
listen_sock.bind(("localhost", 0))
addr, port, *_ = listen_sock.getsockname()
listen_fd = listen_sock.detach()

tcpstore = dist.TCPStore(addr, port, 1, True, master_listen_fd=listen_fd)  # expect NotImplementedError
tcpstore = dist.TCPStore(addr, port, 1, True, master_listen_fd=listen_fd, use_libuv=False)  # OK. Use legacy backend

----------------------------------------

TITLE: Implementing HTML Redirection for PyTorch Tutorial
DESCRIPTION: This HTML snippet creates a meta tag for automatic page redirection. It redirects the user to the new location of the PyTorch 2.0 quantization tutorial after a 3-second delay.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/prototype/pt2e_quant_x86_inductor.html'" />

----------------------------------------

TITLE: Building the Custom Extension with setup.py
DESCRIPTION: Creates a setup.py file to build the C++ extension using PyTorch's cpp_extension module. Supports both CPU and CUDA builds.

LANGUAGE: python
CODE:
# file name: setup.py
import os
import sys
import torch
from setuptools import setup
from torch.utils import cpp_extension

sources = ["src/dummy.cpp"]
include_dirs = [f"{os.path.dirname(os.path.abspath(__file__))}/include/"]

if torch.cuda.is_available():
    module = cpp_extension.CUDAExtension(
        name = "dummy_collectives",
        sources = sources,
        include_dirs = include_dirs,
    )
else:
    module = cpp_extension.CppExtension(
        name = "dummy_collectives",
        sources = sources,
        include_dirs = include_dirs,
    )

setup(
    name = "Dummy-Collectives",
    version = "0.0.1",
    ext_modules = [module],
    cmdclass={'build_ext': cpp_extension.BuildExtension}
)

----------------------------------------

TITLE: Markdown Documentation - PyTorch Tutorial Guidelines
DESCRIPTION: Structured markdown documentation describing the complete process of submitting, maintaining, and managing PyTorch tutorials, including acceptance criteria, submission steps, and maintenance requirements.

LANGUAGE: markdown
CODE:
# PyTorch Tutorial Submission Policy

This policy outlines the criteria and process for submitting new
tutorials to the PyTorch community.
Our goal is to ensure that all tutorials are of high quality,
relevant, and up-to-date, supporting both the growth of the PyTorch
users and the evolution of the PyTorch framework itself. By following
these guidelines, contributors can help us maintain a robust and
informative educational environment.

## Acceptance Criteria For New Tutorials

We accept new tutorials that adhere to one of the following use cases:

* **Demonstrate New PyTorch Features:** Tutorials that support new features
  for upcoming PyTorch releases are typically authored by the engineers who
  are developing these features. These tutorials are crucial for showcasing
  the latest advancements in PyTorch. We typically do not require more than
  one tutorial per feature.

* **Tutorials showcasing PyTorch usage with other tools and libraries:** We
  accept community-contributed tutorials that illustrate innovative uses of
  PyTorch alongside other open-source projects, models, and tools. Please
  ensure that your tutorial remains neutral and does not promote or endorse
  proprietary technologies over others.

----------------------------------------

TITLE: Creating Tensor Parallel Plan for FeedForward Layer
DESCRIPTION: Define a parallelization plan for the FeedForward layer using ColwiseParallel and RowwiseParallel styles.

LANGUAGE: python
CODE:
from torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel, parallelize_module

layer_tp_plan = {
    "feed_foward.w1": ColwiseParallel(),
    "feed_forward.w2": RowwiseParallel(),
    "feed_forward.w3": ColwiseParallel(),
}

----------------------------------------

TITLE: Implementing HTML Meta Redirect for PyTorch Tutorial
DESCRIPTION: This HTML snippet creates a meta refresh tag to automatically redirect the user to the main PyTorch tutorials page after 3 seconds. It's used to handle the deprecation of the 'Fast Transformer Inference with Better Transformer' tutorial.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/'" />

----------------------------------------

TITLE: Running Inference in Throughput Mode
DESCRIPTION: Command to run inference in throughput mode, where all cores in each CPU node set up an instance, using the torch.backends.xeon.run_cpu script.

LANGUAGE: bash
CODE:
$ python -m torch.backends.xeon.run_cpu --throughput-mode <program.py> [program_args]

----------------------------------------

TITLE: Implementing TrainerNet Class in Python
DESCRIPTION: Defines a TrainerNet class that represents the network trained by each trainer. It uses RPC to communicate with the parameter server for forward passes and parameter retrieval.

LANGUAGE: python
CODE:
class TrainerNet(nn.Module):
    def __init__(self, num_gpus=0):
        super().__init__()
        self.num_gpus = num_gpus
        self.param_server_rref = rpc.remote(
            "parameter_server", get_parameter_server, args=(num_gpus,))

    def get_global_param_rrefs(self):
        remote_params = remote_method(
            ParameterServer.get_param_rrefs,
            self.param_server_rref)
        return remote_params

    def forward(self, x):
        model_output = remote_method(
            ParameterServer.forward, self.param_server_rref, x)
        return model_output

----------------------------------------

TITLE: CPU Implementation - C++ Wrapper
DESCRIPTION: Generated C++ wrapper code for CPU showing the optimized implementation using C++ APIs.

LANGUAGE: cpp
CODE:
std::vector<at::Tensor> inductor_entry_cpp(const std::vector<at::Tensor>& args) {
    at::Tensor arg0_1 = args[0];
    at::Tensor constant0 = args[1];
    auto buf0 = at::empty_strided({19L, }, {1L, }, at::device(at::kCPU).dtype(at::kFloat));
    cpp_fused_add_lift_fresh_0((long*)(constant0.data_ptr()), (float*)(arg0_1.data_ptr()), (float*)(buf0.data_ptr()));
    arg0_1.reset();
    return {buf0};
}

----------------------------------------

TITLE: Computing and Displaying Top Predictions
DESCRIPTION: Process the model output to compute probabilities and display the top 10 predicted classes using ImageNet labels.

LANGUAGE: python
CODE:
top = list(enumerate(output[0].softmax(dim=0)))
top.sort(key=lambda x: x[1], reverse=True)
for idx, val in top[:10]:
    print(f"{val.item()*100:.2f}% {classes[idx]}")

----------------------------------------

TITLE: Saving and Loading Snapshots in PyTorch Distributed Training
DESCRIPTION: These methods demonstrate how to save and load snapshots in a PyTorch distributed training setup. The snapshot includes the model state and the number of epochs run.

LANGUAGE: python
CODE:
def _save_snapshot(self, epoch):
    snapshot = {}
    snapshot["MODEL_STATE"] = self.model.module.state_dict()
    snapshot["EPOCHS_RUN"] = epoch
    torch.save(snapshot, "snapshot.pt")
    print(f"Epoch {epoch} | Training snapshot saved at snapshot.pt")

def _load_snapshot(self, snapshot_path):
    snapshot = torch.load(snapshot_path)
    self.model.load_state_dict(snapshot["MODEL_STATE"])
    self.epochs_run = snapshot["EPOCHS_RUN"]
    print(f"Resuming training from snapshot at Epoch {self.epochs_run}")

----------------------------------------

TITLE: Example PyTorch Function for Demonstration
DESCRIPTION: Sample PyTorch function that creates a tensor and performs addition operation, used to demonstrate the difference between Python and C++ wrappers.

LANGUAGE: python
CODE:
import torch

def fn(x):
    return torch.tensor(list(range(2, 40, 2)), device=x.device) + x

x = torch.randn(1)
opt_fn = torch.compile()(fn)
y = opt_fn(x)

----------------------------------------

TITLE: Quantization-Aware Training with X86InductorQuantizer in PyTorch 2
DESCRIPTION: This code demonstrates the process of quantization-aware training using X86InductorQuantizer, including model preparation, conversion, and lowering into Inductor.

LANGUAGE: python
CODE:
import torch
from torch._export import capture_pre_autograd_graph
from torch.ao.quantization.quantize_pt2e import (
  prepare_qat_pt2e,
  convert_pt2e,
)
import torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq
from torch.ao.quantization.quantizer.x86_inductor_quantizer import X86InductorQuantizer

class M(torch.nn.Module):
   def __init__(self):
      super().__init__()
      self.linear = torch.nn.Linear(1024, 1000)

   def forward(self, x):
      return self.linear(x)

example_inputs = (torch.randn(1, 1024),)
m = M()

exported_model = capture_pre_autograd_graph(m, example_inputs)

quantizer = X86InductorQuantizer()
quantizer.set_global(xiq.get_default_x86_inductor_quantization_config(is_qat=True))
prepared_model = prepare_qat_pt2e(exported_model, quantizer)

# train omitted

converted_model = convert_pt2e(prepared_model)

torch.ao.quantization.move_exported_model_to_eval(converted_model)

with torch.no_grad():
  optimized_model = torch.compile(converted_model)
  _ = optimized_model(*example_inputs)

----------------------------------------

TITLE: Resuming Training from Snapshot in PyTorch
DESCRIPTION: This code snippet shows how to resume training from a snapshot in a PyTorch training loop. It starts the epoch range from the last saved epoch.

LANGUAGE: python
CODE:
def train(self, max_epochs: int):
    for epoch in range(self.epochs_run, max_epochs):
        self._run_epoch(epoch)

----------------------------------------

TITLE: Registering Custom Distributed Optimizer
DESCRIPTION: Shows how to register the custom functional optimizer implementation with DistributedOptimizer for automatic conversion during distributed training.

LANGUAGE: python
CODE:
from torch.distributed.optim import DistributedOptimizer

DistributedOptimizer.functional_optim_map[QHM] = FunctionalQHM

----------------------------------------

TITLE: Saving and Loading QAT Model Checkpoints
DESCRIPTION: This code demonstrates how to save and load model checkpoints during quantization-aware training for resuming training or performing inference later.

LANGUAGE: python
CODE:
# Saving checkpoint
checkpoint_path = "/path/to/my/checkpoint_%s.pth" % nepoch
torch.save(prepared_model.state_dict(), "checkpoint_path")

# Loading checkpoint
from torch._export import capture_pre_autograd_graph
from torch.ao.quantization.quantizer.xnnpack_quantizer import (
    XNNPACKQuantizer,
    get_symmetric_quantization_config,
)
from torchvision.models.resnet import resnet18

example_inputs = (torch.rand(2, 3, 224, 224),)
float_model = resnet18(pretrained=False)
exported_model = capture_pre_autograd_graph(float_model, example_inputs)
quantizer = XNNPACKQuantizer()
quantizer.set_global(get_symmetric_quantization_config(is_qat=True))
prepared_model = prepare_qat_pt2e(exported_model, quantizer)
prepared_model.load_state_dict(torch.load(checkpoint_path))

----------------------------------------

TITLE: Setting up distributed process group in PyTorch
DESCRIPTION: Initialize the distributed process group using NCCL backend, setting the device for each process and defining the world size and rank.

LANGUAGE: python
CODE:
def ddp_setup(rank: int, world_size: int):
    """
    Args:
        rank: Unique identifier of each process
       world_size: Total number of processes
    """
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "12355"
    torch.cuda.set_device(rank)
    init_process_group(backend="nccl", rank=rank, world_size=world_size)

----------------------------------------

TITLE: Adding Entry Point for Torch NPU
DESCRIPTION: Example of adding an entry point for the Torch NPU package in its setup.py file.

LANGUAGE: python
CODE:
setup(
    name="torch_npu",
    version="2.5",
+   entry_points={
+       'torch.backends': [
+           'torch_npu = torch_npu:_autoload',
+       ],
+   }
)

----------------------------------------

TITLE: Importing PyTorch 2 Export QAT Dependencies
DESCRIPTION: This snippet shows the necessary imports for PyTorch 2 Export Quantization-Aware Training, including torch.export and quantization modules.

LANGUAGE: python
CODE:
import torch
from torch._export import capture_pre_autograd_graph
from torch.ao.quantization.quantize_pt2e import (
  prepare_qat_pt2e,
  convert_pt2e,
)
from torch.ao.quantization.quantizer.xnnpack_quantizer import (
  XNNPACKQuantizer,
  get_symmetric_quantization_config,
)

----------------------------------------

TITLE: Distributing input data with DistributedSampler in PyTorch
DESCRIPTION: Use DistributedSampler to distribute the dataset across processes and create a DataLoader with the sampler.

LANGUAGE: python
CODE:
train_data = torch.utils.data.DataLoader(
    dataset=train_dataset,
    batch_size=32,
    shuffle=False,  # We don't shuffle
    sampler=DistributedSampler(train_dataset), # Use the Distributed Sampler here.
)

----------------------------------------

TITLE: Exporting Model with torch.export
DESCRIPTION: This code demonstrates how to export a PyTorch model using torch.export for quantization-aware training. It includes examples for both static and dynamic shapes.

LANGUAGE: python
CODE:
example_inputs = (torch.rand(2, 3, 224, 224),)
# for pytorch 2.5+
exported_model = torch.export.export_for_training(float_model, example_inputs).module()

# Dynamic shapes example
dynamic_shapes = tuple(
  {0: torch.export.Dim("dim")} if i == 0 else None
  for i in range(len(example_inputs))
)
exported_model = torch.export.export_for_training(float_model, example_inputs, dynamic_shapes=dynamic_shapes).module()

----------------------------------------

TITLE: Running distributed training job in PyTorch
DESCRIPTION: Set up the main function to accept rank and world_size arguments, initialize the process group, and use mp.spawn to launch multiple processes.

LANGUAGE: python
CODE:
def main(rank, world_size, total_epochs, save_every):
   ddp_setup(rank, world_size)
   dataset, model, optimizer = load_train_objs()
   train_data = prepare_dataloader(dataset, batch_size=32)
   trainer = Trainer(model, train_data, optimizer, rank, save_every)
   trainer.train(total_epochs)
   destroy_process_group()

if __name__ == "__main__":
   import sys
   total_epochs = int(sys.argv[1])
   save_every = int(sys.argv[2])
   world_size = torch.cuda.device_count()
   mp.spawn(main, args=(world_size, total_epochs, save_every,), nprocs=world_size)

----------------------------------------

TITLE: Calculating Idle Time Breakdown in Python
DESCRIPTION: Code to generate an idle time breakdown for GPUs using the TraceAnalysis object.

LANGUAGE: python
CODE:
analyzer = TraceAnalysis(trace_dir = "/path/to/trace/folder")
idle_time_df = analyzer.get_idle_time_breakdown()

----------------------------------------

TITLE: Adding Entry Point for Habana Frameworks
DESCRIPTION: Example of adding an entry point for the Habana Frameworks package in its setup.py file.

LANGUAGE: python
CODE:
setup(
    name="habana_frameworks",
    version="2.5",
+   entry_points={
+       'torch.backends': [
+           "device_backend = habana_frameworks:__autoload",
+       ],
+   }
)

----------------------------------------

TITLE: Applying Tensor Parallel to TransformerBlock
DESCRIPTION: Apply the Tensor Parallel plan to each TransformerBlock in the model, adjusting attention layers for local number of heads.

LANGUAGE: python
CODE:
for layer_id, transformer_block in enumerate(model.layers):
    layer_tp_plan = {...}  # i.e. the plan we just generated

    # Adjust attention module to use the local number of heads
    attn_layer = transformer_block.attention
    attn_layer.n_heads = attn_layer.n_heads // tp_mesh.size()
    attn_layer.n_kv_heads = attn_layer.n_kv_heads // tp_mesh.size()

    parallelize_module(
        module=transformer_block,
        device_mesh=tp_mesh,
        parallelize_plan=layer_tp_plan,
    )

----------------------------------------

TITLE: Generating Kernel Breakdown in Python
DESCRIPTION: Code to generate a breakdown of kernel types and their metrics using the TraceAnalysis object.

LANGUAGE: python
CODE:
analyzer = TraceAnalysis(trace_dir = "/path/to/trace/folder")
kernel_type_metrics_df, kernel_metrics_df = analyzer.get_gpu_kernel_breakdown()

----------------------------------------

TITLE: Setting Up Distributed Environment for PyTorch
DESCRIPTION: Configures the environment for torch distributed communication backend.

LANGUAGE: python
CODE:
import os
import torch
import torchrec

os.environ["MASTER_ADDR"] = "localhost"
os.environ["MASTER_PORT"] = "29500"

----------------------------------------

TITLE: Configuring Video Capture with OpenCV
DESCRIPTION: Set up video capture using OpenCV to stream video frames at 224x224 resolution and 36 fps.

LANGUAGE: python
CODE:
import cv2
from PIL import Image

cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 224)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 224)
cap.set(cv2.CAP_PROP_FPS, 36)

----------------------------------------

TITLE: Redirecting to PyTorch Tutorials Using HTML Meta Tag
DESCRIPTION: This HTML snippet sets up a meta refresh tag to redirect the user to the main PyTorch tutorials page after 3 seconds. It's used to handle the deprecation of the T5-Base model tutorial.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/'" />

----------------------------------------

TITLE: Building PyTorch with Vulkan Backend on macOS
DESCRIPTION: Command to build PyTorch with Vulkan backend support on macOS, including necessary environment variables and compiler settings.

LANGUAGE: bash
CODE:
cd PYTORCH_ROOT
USE_VULKAN=1 USE_VULKAN_SHADERC_RUNTIME=1 USE_VULKAN_WRAPPER=0 MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install

----------------------------------------

TITLE: Converting OpenCV BGR Output to RGB
DESCRIPTION: Process the captured frame to convert it from BGR to RGB format.

LANGUAGE: python
CODE:
ret, image = cap.read()
# convert opencv output from BGR to RGB
image = image[:, :, [2, 1, 0]]

----------------------------------------

TITLE: Implementing Store-Based Barrier for TCPStore Performance Measurement
DESCRIPTION: This code implements a store-based barrier function to measure the performance of TCPStore operations. It includes error handling and logging for timeout scenarios.

LANGUAGE: python
CODE:
import logging
import os
import time

from datetime import timedelta
from time import perf_counter

import torch
import torch.distributed as dist

DistStoreError = torch._C._DistStoreError
logger: logging.Logger = logging.getLogger(__name__)

def store_based_barrier(
    rank,
    store,
    group_name,
    rendezvous_count,
    timeout=dist.constants.default_pg_timeout,
    logging_interval=timedelta(seconds=10),
):
    store_key = f"store_based_barrier_key:{group_name}"
    store.add(store_key, 1)

    world_size = rendezvous_count
    worker_count = store.add(store_key, 0)

    last_worker_key = f"{store_key}:last_worker"
    if worker_count == world_size:
        store.set(last_worker_key, "1")

    start = time.time()
    while True:
        try:
            store.wait([last_worker_key], logging_interval)
            break
        except RuntimeError as e:
            worker_count = store.add(store_key, 0)
            logger.info(
                "Waiting in store based barrier to initialize process group for "
                "rank: %s, key: %s (world_size=%s, num_workers_joined=%s, timeout=%s)"
                "error: %s",
                rank,
                store_key,
                world_size,
                worker_count,
                timeout,
                e,
            )

            if timedelta(seconds=(time.time() - start)) > timeout:
                raise DistStoreError(
                    "Timed out initializing process group in store based barrier on "
                    "rank {}, for key: {} (world_size={}, num_workers_joined={}, timeout={})".format(
                        rank, store_key, world_size, worker_count, timeout
                    )
                )

    logger.info(
        "Rank %s: Completed store-based barrier for key:%s with %s nodes.",
        rank,
        store_key,
        world_size,
    )

# Env var are preset when launching the benchmark
env_rank = os.environ.get("RANK", 0)
env_world_size = os.environ.get("WORLD_SIZE", 1)
env_master_addr = os.environ.get("MASTER_ADDR", "localhost")
env_master_port = os.environ.get("MASTER_PORT", "23456")

tcp_store = dist.TCPStore(
    env_master_addr,
    int(env_master_port),
    world_size=int(env_world_size),
    is_master=(int(env_rank) == 0),
)

# sync workers
store_based_barrier(int(env_rank), tcp_store, "tcpstore_test", int(env_world_size))

number_runs = 10
start = perf_counter()
for _ in range(number_runs):
    store_based_barrier(
        int(env_rank), tcp_store, "tcpstore_test", int(env_world_size)
    )
end = perf_counter()
time_elapsed = end - start
logger.info(
    f"Complete {number_runs} TCPStore barrier runs with rank={env_rank}, world_size={env_world_size} in {time_elapsed} seconds."
)

----------------------------------------

TITLE: Configuring CMake Build for PyTorch with Custom Class
DESCRIPTION: Sets up a CMake build configuration for a PyTorch project that includes custom class implementation. Specifies minimum CMake version, project name, finds PyTorch package, and configures linking against LibTorch and custom class library. Sets C++ standard to 14.

LANGUAGE: cmake
CODE:
cmake_minimum_required(VERSION 3.1 FATAL_ERROR)
project(infer)

find_package(Torch REQUIRED)

add_subdirectory(custom_class_project)

# Define our library target
add_executable(infer infer.cpp)
set(CMAKE_CXX_STANDARD 14)
# Link against LibTorch
target_link_libraries(infer "${TORCH_LIBRARIES}")
# This is where we link in our libcustom_class code, making our
# custom class available in our binary.
target_link_libraries(infer -Wl,--no-as-needed custom_class)

----------------------------------------

TITLE: Model Training Implementation
DESCRIPTION: Main training loop implementation with FSDP wrapping for T5 model including training and validation functions.

LANGUAGE: python
CODE:
def train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=None):
    model.train()
    local_rank = int(os.environ['LOCAL_RANK'])
    fsdp_loss = torch.zeros(2).to(local_rank)
    # ... training loop implementation

----------------------------------------

TITLE: Initializing DeviceMesh for Tensor Parallel
DESCRIPTION: Initialize a DeviceMesh that connects 8 GPUs within a host for Tensor Parallel operations.

LANGUAGE: python
CODE:
from torch.distributed.device_mesh import init_device_mesh

tp_mesh = init_device_mesh("cuda", (8,))

----------------------------------------

TITLE: Loading Custom Operator in Python
DESCRIPTION: Python code to load the custom operator shared library and access the registered function.

LANGUAGE: Python
CODE:
import torch
torch.ops.load_library("libwarp_perspective.so")
print(torch.ops.my_ops.warp_perspective)

----------------------------------------

TITLE: Package Imports for FSDP Training
DESCRIPTION: Required package imports for implementing FSDP training including PyTorch core libraries, transformers, and distributed training components.

LANGUAGE: python
CODE:
import os
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from transformers import AutoTokenizer, GPT2TokenizerFast
from transformers import T5Tokenizer, T5ForConditionalGeneration
# ... additional imports

----------------------------------------

TITLE: Applying Sequence Parallel to LayerNorm/RMSNorm
DESCRIPTION: Adjust the Tensor Parallel plan to enable Sequence Parallel on the RMSNorm layers within a TransformerBlock.

LANGUAGE: python
CODE:
from torch.distributed.tensor.parallel import PrepareModuleInput, SequenceParallel

layer_tp_plan = {
    "attention_norm": SequenceParallel(),
    "attention": PrepareModuleInput(
        input_layouts=(Shard(1),),
        desired_input_layouts=(Replicate(),),
    ),
    "attention.wq": ColwiseParallel(),
    "attention.wk": ColwiseParallel(),
    "attention.wv": ColwiseParallel(),
    "attention.wo": RowwiseParallel(output_layouts=Shard(1)),
    "ffn_norm": SequenceParallel(),
    "feed_forward": PrepareModuleInput(
        input_layouts=(Shard(1),),
        desired_input_layouts=(Replicate(),),
    ),
    "feed_forward.w1": ColwiseParallel(),
    "feed_forward.w2": RowwiseParallel(output_layouts=Shard(1)),
    "feed_forward.w3": ColwiseParallel(),
}

----------------------------------------

TITLE: Dynamic Quantization Application
DESCRIPTION: Code to apply dynamic quantization to the BERT model, converting linear layers to INT8.

LANGUAGE: python
CODE:
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

----------------------------------------

TITLE: Using TorchInductor to Compile and Execute a PyTorch Function
DESCRIPTION: Python code demonstrating how to use TorchInductor to compile and execute a simple PyTorch function on Windows CPU.

LANGUAGE: python
CODE:
import torch
def foo(x, y):
    a = torch.sin(x)
    b = torch.cos(y)
    return a + b
opt_foo1 = torch.compile(foo)
print(opt_foo1(torch.randn(10, 10), torch.randn(10, 10)))

----------------------------------------

TITLE: Partitioning MNIST Dataset for Distributed Training in PyTorch
DESCRIPTION: Uses the dataset partitioning helpers to split the MNIST dataset across processes for distributed training.

LANGUAGE: python
CODE:
""" Partitioning MNIST """
def partition_dataset():
    dataset = datasets.MNIST('./data', train=True, download=True,
                             transform=transforms.Compose([
                                 transforms.ToTensor(),
                                 transforms.Normalize((0.1307,), (0.3081,))
                             ]))
    size = dist.get_world_size()
    bsz = 128 // size
    partition_sizes = [1.0 / size for _ in range(size)]
    partition = DataPartitioner(dataset, partition_sizes)
    partition = partition.use(dist.get_rank())
    train_set = torch.utils.data.DataLoader(partition,
                                         batch_size=bsz,
                                         shuffle=True)
    return train_set, bsz

----------------------------------------

TITLE: Configuring CMake Build for PyTorch Custom Operations
DESCRIPTION: Sets up a CMake build configuration for a PyTorch project. Requires minimum CMake 3.0, finds PyTorch package dependencies, and configures a transformer timeseries executable with C++14 standard support.

LANGUAGE: cmake
CODE:
cmake_minimum_required(VERSION 3.0 FATAL_ERROR)
project(custom_ops)

find_package(Torch REQUIRED)

add_executable(transformer_ts transformer_timeseries.cpp)
target_link_libraries(transformer_ts "${TORCH_LIBRARIES}")
set_property(TARGET transformer_ts PROPERTY CXX_STANDARD 14)

----------------------------------------

TITLE: Activating Conda in Windows Command Line
DESCRIPTION: Command to activate Conda in the Windows command line environment.

LANGUAGE: sh
CODE:
"C:/ProgramData/miniforge3/Scripts/activate.bat"

----------------------------------------

TITLE: Redirecting to Updated Parallelism Tutorial using HTML Meta Refresh
DESCRIPTION: This HTML snippet creates an automatic redirect to the latest parallelism APIs tutorial page after a 3-second delay.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/intermediate/pipelining_tutorial.html'" />

----------------------------------------

TITLE: Installing TCMalloc on CentOS
DESCRIPTION: Command to install TCMalloc (Google Perftools) on CentOS for optimized memory allocation.

LANGUAGE: bash
CODE:
$ yum install gperftools

----------------------------------------

TITLE: Uploading Model Artifacts to GCS
DESCRIPTION: Commands to copy model artifacts to Google Cloud Storage bucket for centralized storage

LANGUAGE: shell
CODE:
BUCKET_NAME = "your-bucket-name-unique"  # @param {type:"string"}
BUCKET_URI = f"gs://{BUCKET_NAME}/"

# Will copy the artifacts into the bucket
!gsutil cp -r model_artifacts $BUCKET_URI

----------------------------------------

TITLE: Initializing Distributed Environment
DESCRIPTION: Sets up the distributed training environment with process group initialization and global variables for pipeline parallelism configuration.

LANGUAGE: python
CODE:
import os
import torch.distributed as dist
from torch.distributed.pipelining import pipeline, SplitPoint, PipelineStage, ScheduleGPipe

global rank, device, pp_group, stage_index, num_stages
def init_distributed():
   global rank, device, pp_group, stage_index, num_stages
   rank = int(os.environ["LOCAL_RANK"])
   world_size = int(os.environ["WORLD_SIZE"])
   device = torch.device(f"cuda:{rank}") if torch.cuda.is_available() else torch.device("cpu")
   dist.init_process_group()

   # This group can be a sub-group in the N-D parallel case
   pp_group = dist.new_group()
   stage_index = rank
   num_stages = world_size

----------------------------------------

TITLE: Installing Intel OpenMP Runtime Library via conda
DESCRIPTION: Command to install the Intel OpenMP Runtime Library using conda.

LANGUAGE: bash
CODE:
$ conda install mkl

----------------------------------------

TITLE: Creating TorchServe Model Archive
DESCRIPTION: Commands to package a custom TorchServe handler into a model archive (MAR) file using torch-model-archiver

LANGUAGE: shell
CODE:
!torch-model-archiver \
-f \
--model-name <your_model_name> \
--version 1.0 \
 --handler model_artifacts/handler.py \
--export-path model_artifacts

----------------------------------------

TITLE: Implementing Backend and Work Subclasses in C++
DESCRIPTION: Defines the BackendDummy class inheriting from Backend and WorkDummy class inheriting from Work. Implements dummy versions of allgather and allreduce operations.

LANGUAGE: cpp
CODE:
// file name: dummy.hpp
#include <torch/python.h>

#include <torch/csrc/distributed/c10d/Backend.hpp>
#include <torch/csrc/distributed/c10d/Work.hpp>
#include <torch/csrc/distributed/c10d/Store.hpp>
#include <torch/csrc/distributed/c10d/Types.hpp>
#include <torch/csrc/distributed/c10d/Utils.hpp>

#include <pybind11/chrono.h>

namespace c10d {

class BackendDummy : public Backend {
  public:
    BackendDummy(int rank, int size);

    c10::intrusive_ptr<Work> allgather(
        std::vector<std::vector<at::Tensor>>& outputTensors,
        std::vector<at::Tensor>& inputTensors,
        const AllgatherOptions& opts = AllgatherOptions()) override;

    c10::intrusive_ptr<Work> allreduce(
        std::vector<at::Tensor>& tensors,
        const AllreduceOptions& opts = AllreduceOptions()) override;

    // The collective communication APIs without a custom implementation
    // will error out if invoked by application code.
};

class WorkDummy : public Work {
  public:
    WorkDummy(
      OpType opType,
      c10::intrusive_ptr<c10::ivalue::Future> future) // future of the output
      : Work(
          -1, // rank, only used by recvAnySource, irrelevant in this demo
          opType),
      future_(std::move(future)) {}
    bool isCompleted() override;
    bool isSuccess() const override;
    bool wait(std::chrono::milliseconds timeout = kUnsetTimeout) override;
    virtual c10::intrusive_ptr<c10::ivalue::Future> getFuture() override;

  private:
    c10::intrusive_ptr<c10::ivalue::Future> future_;
};
} // namespace c10d

LANGUAGE: cpp
CODE:
// file name: dummy.cpp
#include "dummy.hpp"

namespace c10d {

// This is a dummy allgather that sets all output tensors to zero
// Modify the implementation to conduct real communication asynchronously
c10::intrusive_ptr<Work> BackendDummy::allgather(
        std::vector<std::vector<at::Tensor>>& outputTensors,
        std::vector<at::Tensor>& inputTensors,
        const AllgatherOptions& /* unused */) {
    for (auto& outputTensorVec : outputTensors) {
        for (auto& outputTensor : outputTensorVec) {
            outputTensor.zero_();
        }
    }

    auto future = c10::make_intrusive<c10::ivalue::Future>(
        c10::ListType::create(c10::ListType::create(c10::TensorType::get())));
    future->markCompleted(c10::IValue(outputTensors));
    return c10::make_intrusive<WorkDummy>(OpType::ALLGATHER, std::move(future));
}

// This is a dummy allreduce that sets all output tensors to zero
// Modify the implementation to conduct real communication asynchronously
c10::intrusive_ptr<Work> BackendDummy::allreduce(
        std::vector<at::Tensor>& tensors,
        const AllreduceOptions& opts) {
    for (auto& tensor : tensors) {
        tensor.zero_();
    }

    auto future = c10::make_intrusive<c10::ivalue::Future>(
        c10::ListType::create(c10::TensorType::get()));
    future->markCompleted(c10::IValue(tensors));
    return c10::make_intrusive<WorkDummy>(OpType::ALLGATHER, std::move(future));
}
} // namespace c10d

----------------------------------------

TITLE: Installing Intel OpenMP Runtime Library via pip
DESCRIPTION: Command to install the Intel OpenMP Runtime Library using pip.

LANGUAGE: bash
CODE:
$ pip install intel-openmp

----------------------------------------

TITLE: Running Performance Benchmarks on Android
DESCRIPTION: Commands to deploy and test both fused and non-fused models on an Android device using the benchmark tool.

LANGUAGE: bash
CODE:
adb push build_android/bin/speed_benchmark_torch /data/local/tmp
adb push model.pt /data/local/tmp
adb push model_fused.pt /data/local/tmp
adb shell "/data/local/tmp/speed_benchmark_torch --model=/data/local/tmp/model.pt" --input_dims="1,3,224,224" --input_type="float"
adb shell "/data/local/tmp/speed_benchmark_torch --model=/data/local/tmp/model_fused.pt" --input_dims="1,3,224,224" --input_type="float"

----------------------------------------

TITLE: Installing PyTorch and OpenCV on Raspberry Pi
DESCRIPTION: Use pip to install PyTorch, torchvision, torchaudio, OpenCV, and upgrade numpy on the Raspberry Pi.

LANGUAGE: shell
CODE:
$ pip install torch torchvision torchaudio
$ pip install opencv-python
$ pip install numpy --upgrade

----------------------------------------

TITLE: Installing taskset on CentOS
DESCRIPTION: Command to install the taskset utility on CentOS for setting CPU affinity.

LANGUAGE: bash
CODE:
$ yum install util-linux

----------------------------------------

TITLE: Importing Required Libraries for PyTorch Flask Service
DESCRIPTION: Essential imports for running a PyTorch model with Flask, including torchvision for models and transforms, PIL for image handling, and Flask components.

LANGUAGE: python
CODE:
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
from flask import Flask, jsonify, request

----------------------------------------

TITLE: Launching Distributed Reinforcement Learning using RPC
DESCRIPTION: Sets up and launches multiple processes for distributed reinforcement learning using RPC, with one agent and multiple observers.

LANGUAGE: python
CODE:
import os
from itertools import count

import torch.multiprocessing as mp

AGENT_NAME = "agent"
OBSERVER_NAME="obs{}"

def run_worker(rank, world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '29500'
    if rank == 0:
        rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size)
        agent = Agent(world_size)
        for i_episode in count(1):
            agent.run_episode()
            last_reward = agent.finish_episode()
            if agent.running_reward > agent.reward_threshold:
                print(f"Solved! Running reward is now {agent.running_reward}!")
                break
    else:
        rpc.init_rpc(OBSERVER_NAME.format(rank), rank=rank, world_size=world_size)
    rpc.shutdown()

mp.spawn(
    run_worker,
    args=(args.world_size, ),
    nprocs=args.world_size,
    join=True
)

----------------------------------------

TITLE: Installing taskset on Ubuntu
DESCRIPTION: Command to install the taskset utility on Ubuntu for setting CPU affinity.

LANGUAGE: bash
CODE:
$ apt-get install util-linux

----------------------------------------

TITLE: Building PyTorch with Vulkan Backend on Linux
DESCRIPTION: Command to build PyTorch with Vulkan backend support on Linux, including necessary environment variables.

LANGUAGE: bash
CODE:
cd PYTORCH_ROOT
USE_VULKAN=1 USE_VULKAN_SHADERC_RUNTIME=1 USE_VULKAN_WRAPPER=0 python setup.py install

----------------------------------------

TITLE: HTML Meta Refresh Redirect for Speech Recognition Tutorial
DESCRIPTION: An HTML meta refresh tag that automatically redirects users to the new tutorial location after 3 seconds.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/audio/stable/tutorials/speech_recognition_pipeline_tutorial.html'" />

----------------------------------------

TITLE: Launching Distributed Training with torchrun in PyTorch
DESCRIPTION: This command demonstrates how to launch a distributed training job using torchrun in PyTorch. It specifies the number of processes per node and passes arguments to the script.

LANGUAGE: shell
CODE:
torchrun --standalone --nproc_per_node=4 multigpu_torchrun.py 50 10

----------------------------------------

TITLE: Debugging Quantized BERT Model
DESCRIPTION: Enables debug mode when quantizing to allow inspection of model attributes and emulate quantized operations in floating point.

LANGUAGE: python
CODE:
quantized_model_debug = quantize_dynamic_jit(traced_model, qconfig_dict, debug=True)

----------------------------------------

TITLE: Importing Required Libraries for WSI Classification
DESCRIPTION: Imports essential Python libraries for data processing, visualization, WSI handling with TIAToolbox, and deep learning with PyTorch.

LANGUAGE: python
CODE:
"""Import modules required to run the Jupyter notebook."""
from __future__ import annotations

# Configure logging
import logging
import warnings
if logging.getLogger().hasHandlers():
    logging.getLogger().handlers.clear()
warnings.filterwarnings("ignore", message=".*The 'nopython' keyword.*")

# Downloading data and files
import shutil
from pathlib import Path
from zipfile import ZipFile

# Data processing and visualization
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib import cm
import PIL
import contextlib
import io
from sklearn.metrics import accuracy_score, confusion_matrix

# TIAToolbox for WSI loading and processing
from tiatoolbox import logger
from tiatoolbox.models.architecture import vanilla
from tiatoolbox.models.engine.patch_predictor import (
    IOPatchPredictorConfig,
    PatchPredictor,
)
from tiatoolbox.utils.misc import download_data, grab_files_from_dir
from tiatoolbox.utils.visualization import overlay_prediction_mask
from tiatoolbox.wsicore.wsireader import WSIReader

# Torch-related
import torch
from torchvision import transforms

----------------------------------------

TITLE: Setting Up Quantized MobileNetV2 Model
DESCRIPTION: Configure PyTorch to use the QNNPACK engine and load a pre-trained, quantized MobileNetV2 model.

LANGUAGE: python
CODE:
import torch
torch.backends.quantized.engine = 'qnnpack'

from torchvision import models
net = models.quantization.mobilenet_v2(pretrained=True, quantize=True)

# JIT compile the model for better performance
net = torch.jit.script(net)

----------------------------------------

TITLE: Non-Traceable Module Implementation
DESCRIPTION: Example of implementing a non-traceable module as a separate class that can be skipped during quantization.

LANGUAGE: python
CODE:
class FP32NonTraceable(nn.Module):
    def forward(self, x):
        x = non_traceable_code(x)
        return x

class M(nn.Module):
    def __init__(self):
        self.non_traceable_submodule = FP32NonTraceable(...)
    def forward(self, x):
        x = self.traceable_code_1(x)
        x = self.non_traceable_submodule(x)
        x = self.traceable_code_2(x)
        return x

----------------------------------------

TITLE: Configuring Backend Patterns
DESCRIPTION: Creates BackendPatternConfig objects for linear and conv-relu operations, specifying observation types, dtype configs, and module mappings.

LANGUAGE: python
CODE:
linear_config = BackendPatternConfig() \
    .set_pattern(torch.nn.Linear) \
    .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT) \
    .add_dtype_config(weighted_int8_dtype_config) \
    .set_root_module(torch.nn.Linear) \
    .set_qat_module(torch.nn.qat.Linear) \
    .set_reference_quantized_module(torch.ao.nn.quantized.reference.Linear)

conv_relu_config = BackendPatternConfig() \
    .set_pattern((torch.nn.Conv2d, torch.nn.ReLU)) \
    .set_fused_module(torch.ao.nn.intrinsic.ConvReLU2d) \
    .set_fuser_method(fuse_conv2d_relu)

fused_conv_relu_config = BackendPatternConfig() \
    .set_pattern(torch.ao.nn.intrinsic.ConvReLU2d) \
    .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT) \
    .add_dtype_config(weighted_int8_dtype_config) \
    .set_root_module(torch.nn.Conv2d) \
    .set_qat_module(torch.ao.nn.intrinsic.qat.ConvReLU2d) \
    .set_reference_quantized_module(torch.ao.nn.quantized.reference.Conv2d)

backend_config = BackendConfig("my_backend") \
    .set_backend_pattern_config(linear_config) \
    .set_backend_pattern_config(conv_relu_config) \
    .set_backend_pattern_config(fused_conv_relu_config)

----------------------------------------

TITLE: Verifying PyTorch Installation
DESCRIPTION: Check if PyTorch is correctly installed by printing its version using Python.

LANGUAGE: shell
CODE:
$ python -c "import torch; print(torch.__version__)"

----------------------------------------

TITLE: Refactored Model with Traceable Submodule
DESCRIPTION: Modified version of the model that isolates traceable code into a separate submodule for quantization.

LANGUAGE: python
CODE:
class FP32Traceable(nn.Module):
    def forward(self, x):
        x = traceable_code(x)
        return x

class M(nn.Module):
    def __init__(self):
        self.traceable_submodule = FP32Traceable(...)
    def forward(self, x):
        x = self.traceable_code_1(x)
        x = self.traceable_submodule(x)
        x = self.traceable_code_2(x)
        return x

----------------------------------------

TITLE: Setting up QConfig Mapping
DESCRIPTION: Creates QConfig mapping that satisfies backend constraints by configuring observers and quantization parameters for different module types.

LANGUAGE: python
CODE:
activation_observer = MinMaxObserver.with_args(quant_min=0, quant_max=127, eps=2 ** -12)
qconfig = QConfig(activation=activation_observer, weight=default_weight_observer)

qconfig_mapping = QConfigMapping() \
    .set_object_type(torch.nn.Linear, qconfig) \
    .set_object_type(torch.nn.Conv2d, qconfig) \
    .set_object_type(torch.nn.BatchNorm2d, qconfig) \
    .set_object_type(torch.nn.ReLU, qconfig)

----------------------------------------

TITLE: Running the End-to-End Example with torchrun
DESCRIPTION: This command demonstrates how to run the Flight Recorder example script using torchrun for distributed execution.

LANGUAGE: shell
CODE:
torchrun --nnodes=1 --nproc_per_node=2 crash.py

----------------------------------------

TITLE: Converting to Quantized Model in PyTorch
DESCRIPTION: Converts the calibrated model to a fully quantized model using the convert_fx function.

LANGUAGE: python
CODE:
quantized_model = convert_fx(prepared_model)
print(quantized_model)

----------------------------------------

TITLE: Preparing Model for Post-Training Static Quantization in PyTorch
DESCRIPTION: Prepares the model for quantization by inserting observers and folding BatchNorm modules into previous Conv2d modules.

LANGUAGE: python
CODE:
prepared_model = prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)
print(prepared_model.graph)

----------------------------------------

TITLE: End-to-End Example of Using Flight Recorder in Python
DESCRIPTION: This Python script demonstrates how to use Flight Recorder in a distributed training scenario. It intentionally creates a mismatch in collectives to show how Flight Recorder captures the issue.

LANGUAGE: python
CODE:
import torch
import torch.distributed as dist
import os
from datetime import timedelta

local_rank = int(os.environ["LOCAL_RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size <= 8, "world size must be less than or equal to 8"
os.environ["TORCH_NCCL_DEBUG_INFO_TEMP_FILE"] = "/tmp/trace_"
os.environ["TORCH_NCCL_DUMP_ON_TIMEOUT"] = "1"
os.environ["TORCH_NCCL_TRACE_BUFFER_SIZE"] = "2000"
device = torch.device(f"cuda:{local_rank}")
print(f"{local_rank=} {world_size=} master addr: {os.environ['MASTER_ADDR']} master port: {os.environ['MASTER_PORT']} {device=}")

# Initialize the process group with a small timeout so that jobs fail quickly
dist.init_process_group("nccl", world_size=world_size, rank=local_rank, timeout=timedelta(seconds=1))

a = torch.full((3, 4), float(local_rank), device=device)
# Write some collectives to populate Flight Recorder data
for i in range(2):
  print(f"calling allreduce on {local_rank=}")
  f = dist.all_reduce(a)

# rank0 is doing an additional collective
if local_rank == 0:
  print("rank0 is doing an allreduce on tensor b, but other ranks forgot")
  b = torch.full((4,5), float(local_rank), device=device)
  f = dist.all_reduce(b)

for i in range(2):
  print(f"calling allreduce on {local_rank=}")
  f = dist.all_reduce(a)

torch.cuda.synchronize(device=device)
print(f"{local_rank=} exiting")

----------------------------------------

TITLE: Configuring Quantization Settings in PyTorch
DESCRIPTION: Sets up the quantization configuration using QConfigMapping to specify how different parts of the model should be quantized.

LANGUAGE: python
CODE:
# The old 'fbgemm' is still available but 'x86' is the recommended default.
qconfig = get_default_qconfig("x86") 
qconfig_mapping = QConfigMapping().set_global(qconfig)

----------------------------------------

TITLE: Evaluating Quantized Model Performance in PyTorch
DESCRIPTION: Compares the size and accuracy of the quantized model with the original float model.

LANGUAGE: python
CODE:
print("Size of model before quantization")
print_size_of_model(float_model)
print("Size of model after quantization")
print_size_of_model(quantized_model)
top1, top5 = evaluate(quantized_model, criterion, data_loader_test)
print("[before serilaization] Evaluation accuracy on test dataset: %2.2f, %2.2f"%(top1.avg, top5.avg))

fx_graph_mode_model_file_path = saved_model_dir + "resnet18_fx_graph_mode_quantized.pth"

torch.jit.save(torch.jit.script(quantized_model), fx_graph_mode_model_file_path)
loaded_quantized_model = torch.jit.load(fx_graph_mode_model_file_path)

top1, top5 = evaluate(loaded_quantized_model, criterion, data_loader_test)
print("[after serialization/deserialization] Evaluation accuracy on test dataset: %2.2f, %2.2f"%(top1.avg, top5.avg))

----------------------------------------

TITLE: Analyzing Flight Recorder Dumps using Python Script
DESCRIPTION: This snippet shows how to use the provided Python script to analyze Flight Recorder dump files. It includes options for selecting specific ranks and process groups.

LANGUAGE: shell
CODE:
python fr_trace.py <dump dir containing trace files> -j [--selected-ranks i j k ...] [--pg-filters tp dp]

LANGUAGE: shell
CODE:
torchfrtrace <dump dir containing trace files> -j [--selected-ranks i j k ...] [--pg-filters 0 2]

----------------------------------------

TITLE: Setup Script for C++ Extension
DESCRIPTION: setuptools configuration for building C++ extension with CPython agnostic wheel support

LANGUAGE: python
CODE:
from setuptools import setup, Extension
from torch.utils import cpp_extension

setup(name="extension_cpp",
      ext_modules=[
          cpp_extension.CppExtension(
            "extension_cpp",
            ["muladd.cpp"],
            extra_compile_args={"cxx": ["-DPy_LIMITED_API=0x03090000"]}, 
            py_limited_api=True)],
      cmdclass={'build_ext': cpp_extension.BuildExtension},
      options={"bdist_wheel": {"py_limited_api": "cp39"}}
)

----------------------------------------

TITLE: Setup Script for C++ Extension
DESCRIPTION: setuptools configuration for building C++ extension with CPython agnostic wheel support

LANGUAGE: python
CODE:
from setuptools import setup, Extension
from torch.utils import cpp_extension

setup(name="extension_cpp",
      ext_modules=[
          cpp_extension.CppExtension(
            "extension_cpp",
            ["muladd.cpp"],
            extra_compile_args={"cxx": ["-DPy_LIMITED_API=0x03090000"]}, 
            py_limited_api=True)],
      cmdclass={'build_ext': cpp_extension.BuildExtension},
      options={"bdist_wheel": {"py_limited_api": "cp39"}}
)

----------------------------------------

TITLE: Evaluating Quantized BERT Model Performance
DESCRIPTION: Evaluates the accuracy and inference time of the quantized model on the MRPC task, comparing it to the original float model.

LANGUAGE: python
CODE:
time_model_evaluation(traced_model, configs, tokenizer)
time_model_evaluation(quantized_model, configs, tokenizer)

----------------------------------------

TITLE: Example of Flight Recorder File Contents in JSON
DESCRIPTION: This snippet shows the structure of an unpickled Flight Recorder file, including version information, process group configuration, status, and recorded entries.

LANGUAGE: json
CODE:
{
  "version": "2.5",
  "pg_config": {
    "0": {
    "name": "0",
    "desc": "default_pg",
    "ranks": "[0, 1]"
    }
  },
  "pg_status": {
    "0": {
    "last_enqueued_collective": 2,
    "last_started_collective": -1,
    "last_completed_collective": 2
    }
  },
  "entries": [
  {
    "frames": [
    {
    "name": "test_short_pickle",
    "filename": "pytorch/test/distributed/test_c10d_nccl.py",
    "line": 3647
    },
    {
    "name": "spawn_main",
    "filename": ".conda/envs/pytorch-3.10/lib/python3.10/multiprocessing/spawn.py",
    "line": 116
    },
    {
    "name": "<module>",
    "filename": "<string>",
    "line": 1
    }
    ],
    "record_id": 0,
    "pg_id": 0,
    "process_group": ("0", "default_pg"),
    "collective_seq_id": 1,
    "p2p_seq_id": 0,
    "op_id": 1,
    "profiling_name": "nccl:all_reduce",
    "time_created_ns": 1724779239936775119,
    "input_sizes": [[3, 4]],
    "input_dtypes": ["Float"],
    "output_sizes": [[3, 4]],
    "output_dtypes": ["Float"],
    "state": "completed",
    "time_discovered_started_ns": null,
    "time_discovered_completed_ns": 1724779239975811724,
    "retired": true,
    "timeout_ms": 600000,
    "is_p2p": false
    },
    ...
    ]
}

----------------------------------------

TITLE: Exporting MViT Model with Dynamic Batch Size
DESCRIPTION: Solution to export MViT model with dynamic batch sizes by specifying batch dimension range from 2 to 16.

LANGUAGE: python
CODE:
import numpy as np
import torch
from torchvision.models.video import MViT_V1_B_Weights, mvit_v1_b
import traceback as tb

model = mvit_v1_b(weights=MViT_V1_B_Weights.DEFAULT)

input_frames = torch.randn(2,16, 224, 224, 3)
input_frames = np.transpose(input_frames, (0, 4, 1, 2, 3))

batch_dim = torch.export.Dim("batch", min=2, max=16)
exported_program = torch.export.export(
    model,
    (input_frames,),
    dynamic_shapes={"x": {0: batch_dim}},
)

input_frames = torch.randn(4,16, 224, 224, 3)
input_frames = np.transpose(input_frames, (0, 4, 1, 2, 3))
try:
    exported_program.module()(input_frames)
except Exception:
    tb.print_exc()

----------------------------------------

TITLE: GPU Implementation - Python Wrapper
DESCRIPTION: Default Python wrapper implementation for GPU execution showing CUDA stream handling and kernel launching.

LANGUAGE: python
CODE:
def call(args):
    arg0_1, = args
    args.clear()
    assert_size_stride(arg0_1, (1, ), (1, ))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0) # no-op to ensure context
        buf0 = empty_strided((19, ), (1, ), device='cuda', dtype=torch.float32)
        stream0 = get_cuda_stream(0)
        triton_poi_fused_add_lift_fresh_0.run(constant0, arg0_1, buf0, 19, grid=grid(19), stream=stream0)
        run_intermediate_hooks('add', buf0)
        del arg0_1
        return (buf0, )

----------------------------------------

TITLE: Setting up Conda Environment for HTA in Python
DESCRIPTION: Commands to create and activate a Conda environment for using HTA.

LANGUAGE: python
CODE:
# create the environment env_name
conda create -n env_name

# activate the environment
conda activate env_name

# When you are done, deactivate the environment by running ``conda deactivate``

----------------------------------------

TITLE: Importing PyTorch Custom Operator
DESCRIPTION: Example showing how to reference a basic PyTorch custom operator

LANGUAGE: Python
CODE:
torch.add

----------------------------------------

TITLE: HTML Meta Redirect to PyTorch Mobile Tutorial
DESCRIPTION: HTML meta refresh tag that automatically redirects users to the new tutorial location on the PyTorch documentation website.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="0; url='https://pytorch.org/tutorials/recipes/mobile_interpreter.html'" />

----------------------------------------

TITLE: Installing HTA via pip in Python
DESCRIPTION: Command to install the Holistic Trace Analysis package using pip.

LANGUAGE: python
CODE:
pip install HolisticTraceAnalysis

----------------------------------------

TITLE: Setting Thread Count in PyTorch Base Handler
DESCRIPTION: Code snippet to set the number of threads in the TorchServe base handler to optimize performance.

LANGUAGE: python
CODE:
torch.set_num_threads(num_physical_cores/num_workers)

----------------------------------------

TITLE: Model Export with torch.export
DESCRIPTION: Shows how to export a model using torch.export, including both static and dynamic shape variants for different PyTorch versions.

LANGUAGE: python
CODE:
example_inputs = (torch.rand(2, 3, 224, 224),)
# for pytorch 2.5+
exported_model = torch.export.export_for_training(model_to_quantize, example_inputs).module()

# Dynamic shapes example
dynamic_shapes = tuple(
  {0: torch.export.Dim("dim")} if i == 0 else None
  for i in range(len(example_inputs))
)
exported_model = torch.export.export_for_training(model_to_quantize, example_inputs, dynamic_shapes=dynamic_shapes).module()

----------------------------------------

TITLE: Mounting Google Drive in Colab for Data Access
DESCRIPTION: Code snippet to mount Google Drive in Colab environment and set up the data directory path for accessing tutorial datasets.

LANGUAGE: python
CODE:
from google.colab import drive
drive.mount('/content/gdrive')

LANGUAGE: python
CODE:
corpus = os.path.join("/content/gdrive/My Drive/data", corpus_name)

----------------------------------------

TITLE: Custom Joinable Class Implementation
DESCRIPTION: Example implementation of a custom Joinable class called Counter that demonstrates how to make a class compatible with the Join context manager.

LANGUAGE: python
CODE:
class CounterJoinHook(JoinHook):
    def __init__(self, counter, sync_max_count):
        self.counter = counter
        self.sync_max_count = sync_max_count

    def main_hook(self):
        t = torch.zeros(1, device=self.counter.device)
        dist.all_reduce(t)

    def post_hook(self, is_last_joiner: bool):
        if not self.sync_max_count:
            return
        rank = dist.get_rank(self.counter.process_group)
        common_rank = self.counter.find_common_rank(rank, is_last_joiner)
        if rank == common_rank:
            self.counter.max_count = self.counter.count.detach().clone()
        dist.broadcast(self.counter.max_count, src=common_rank)

class Counter(Joinable):
    def __init__(self, device, process_group):
        super(Counter, self).__init__()
        self.device = device
        self.process_group = process_group
        self.count = torch.tensor([0], device=device).float()
        self.max_count = torch.tensor([0], device=device).float()

----------------------------------------

TITLE: Annotating Add Node with QuantizationAnnotation
DESCRIPTION: Shows how to annotate inputs and outputs of an add node using QuantizationAnnotation.

LANGUAGE: python
CODE:
input_qspec_map = {}
input_act0 = add_node.args[0]
input_qspec_map[input_act0] = input_act_qspec

input_act1 = add_node.args[1]
input_qspec_map[input_act1] = input_act_qspec
     
add_node.meta["quantization_annotation"] = QuantizationAnnotation(
    input_qspec_map=input_qspec_map,
    output_qspec=output_act_qspec,
    _annotated=True,
)

----------------------------------------

TITLE: Sample Image Display Script
DESCRIPTION: Python script for displaying the generated samples from the trained DCGAN model.

LANGUAGE: python
CODE:
import argparse
import matplotlib.pyplot as plt
import torch

parser = argparse.ArgumentParser()
parser.add_argument("-i", "--sample-file", required=True)
parser.add_argument("-o", "--out-file", default="out.png")
parser.add_argument("-d", "--dimension", type=int, default=3)
options = parser.parse_args()

module = torch.jit.load(options.sample_file)
images = list(module.parameters())[0]

for index in range(options.dimension * options.dimension):
    image = images[index].detach().cpu().reshape(28, 28).mul(255).to(torch.uint8)
    array = image.numpy()
    axis = plt.subplot(options.dimension, options.dimension, 1 + index)
    plt.imshow(array, cmap="gray")
    axis.get_xaxis().set_visible(False)
    axis.get_yaxis().set_visible(False)

plt.savefig(options.out_file)
print("Saved ", options.out_file)

----------------------------------------

TITLE: Enabling Verbose Output for oneDNN in PyTorch
DESCRIPTION: This code snippet shows how to enable verbose output for oneDNN in PyTorch, which can be used to confirm if AMX is being utilized. It uses torch.backends.mkldnn.verbose to enable oneDNN verbose messages.

LANGUAGE: python
CODE:
with torch.backends.mkldnn.verbose(torch.backends.mkldnn.VERBOSE_ON):
    with torch.cpu.amp.autocast():
        model(input)

----------------------------------------

TITLE: Executing MNIST Training Example in PyTorch C++
DESCRIPTION: This command runs the compiled MNIST example, which trains a model on the MNIST dataset. The output shows the training progress, including loss and accuracy metrics for each epoch.

LANGUAGE: shell
CODE:
$ ./mnist
Train Epoch: 1 [59584/60000] Loss: 0.4232
Test set: Average loss: 0.1989 | Accuracy: 0.940
Train Epoch: 2 [59584/60000] Loss: 0.1926
Test set: Average loss: 0.1338 | Accuracy: 0.959
Train Epoch: 3 [59584/60000] Loss: 0.1390
Test set: Average loss: 0.0997 | Accuracy: 0.969
Train Epoch: 4 [59584/60000] Loss: 0.1239
Test set: Average loss: 0.0875 | Accuracy: 0.972
...

----------------------------------------

TITLE: Loading FSDP-wrapped Model with DCP
DESCRIPTION: This snippet shows how to load a previously saved FSDP-wrapped model using DCP. It recreates the model structure and uses the AppState class to manage the loading process.

LANGUAGE: python
CODE:
import os

import torch
import torch.distributed as dist
import torch.distributed.checkpoint as dcp
from torch.distributed.checkpoint.stateful import Stateful
from torch.distributed.checkpoint.state_dict import get_state_dict, set_state_dict
import torch.multiprocessing as mp
import torch.nn as nn

from torch.distributed.fsdp import FullyShardedDataParallel as FSDP

CHECKPOINT_DIR = "checkpoint"


class AppState(Stateful):
    def __init__(self, model, optimizer=None):
        self.model = model
        self.optimizer = optimizer

    def state_dict(self):
        model_state_dict, optimizer_state_dict = get_state_dict(self.model, self.optimizer)
        return {
            "model": model_state_dict,
            "optim": optimizer_state_dict
        }

    def load_state_dict(self, state_dict):
        set_state_dict(
            self.model,
            self.optimizer,
            model_state_dict=state_dict["model"],
            optim_state_dict=state_dict["optim"]
        )

class ToyModel(nn.Module):
    def __init__(self):
        super(ToyModel, self).__init__()
        self.net1 = nn.Linear(16, 16)
        self.relu = nn.ReLU()
        self.net2 = nn.Linear(16, 8)

    def forward(self, x):
        return self.net2(self.relu(self.net1(x)))


def setup(rank, world_size):
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "12355 "

    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)


def cleanup():
    dist.destroy_process_group()


def run_fsdp_checkpoint_load_example(rank, world_size):
    print(f"Running basic FSDP checkpoint loading example on rank {rank}.")
    setup(rank, world_size)

    model = ToyModel().to(rank)
    model = FSDP(model)

    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

    state_dict = { "app": AppState(model, optimizer)}
    dcp.load(
        state_dict=state_dict,
        checkpoint_id=CHECKPOINT_DIR,
    )

    cleanup()


if __name__ == "__main__":
    world_size = torch.cuda.device_count()
    print(f"Running fsdp checkpoint example on {world_size} devices.")
    mp.spawn(
        run_fsdp_checkpoint_load_example,
        args=(world_size,),
        nprocs=world_size,
        join=True,
    )

----------------------------------------

TITLE: Setting up 2D Parallel Pattern without DeviceMesh in Python
DESCRIPTION: This code snippet demonstrates how to manually set up NCCL communicators for a 2D parallel pattern without using DeviceMesh. It includes creating shard groups and replicate groups, and assigning the correct groups to each rank.

LANGUAGE: python
CODE:
import os

import torch
import torch.distributed as dist

# Understand world topology
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
print(f"Running example on {rank=} in a world with {world_size=}")

# Create process groups to manage 2-D like parallel pattern
dist.init_process_group("nccl")
torch.cuda.set_device(rank)

# Create shard groups (e.g. (0, 1, 2, 3), (4, 5, 6, 7))
# and assign the correct shard group to each rank
num_node_devices = torch.cuda.device_count()
shard_rank_lists = list(range(0, num_node_devices // 2)), list(range(num_node_devices // 2, num_node_devices))
shard_groups = (
    dist.new_group(shard_rank_lists[0]),
    dist.new_group(shard_rank_lists[1]),
)
current_shard_group = (
    shard_groups[0] if rank in shard_rank_lists[0] else shard_groups[1]
)

# Create replicate groups (for example, (0, 4), (1, 5), (2, 6), (3, 7))
# and assign the correct replicate group to each rank
current_replicate_group = None
shard_factor = len(shard_rank_lists[0])
for i in range(num_node_devices // 2):
    replicate_group_ranks = list(range(i, num_node_devices, shard_factor))
    replicate_group = dist.new_group(replicate_group_ranks)
    if rank in replicate_group_ranks:
        current_replicate_group = replicate_group

----------------------------------------

TITLE: Markdown Table - H2 2023 Docathon Leaderboard
DESCRIPTION: Tabular display of contributor rankings including GitHub usernames, points earned, and links to their merged pull requests for the H2 2023 PyTorch documentation campaign.

LANGUAGE: markdown
CODE:
| Author | Points | PR |
|--- | --- | ---|
| ahoblitz | 25 | https://github.com/pytorch/pytorch/pull/112992, [...] |
[Additional rows omitted for brevity]

----------------------------------------

TITLE: Importing Dependencies and Setting Device
DESCRIPTION: Importing necessary PyTorch modules and setting the device to CPU for AWS Graviton3.

LANGUAGE: python
CODE:
import torch
import torch.nn as nn
from torch.profiler import profile, record_function, ProfilerActivity

# AWS Graviton3 cpu
device = ("cpu")
print(f"Using {device} device")

----------------------------------------

TITLE: Implementing Meta Refresh Redirection in HTML
DESCRIPTION: This HTML snippet implements a meta refresh tag to automatically redirect the user to the ExecuTorch documentation page after 3 seconds. It's used to guide users from the deprecated PyTorch Mobile documentation to the new ExecuTorch platform.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/executorch/stable/index.html'" />

----------------------------------------

TITLE: Enabling Linux Transparent Huge Pages
DESCRIPTION: Optimizing memory allocation overhead by enabling Linux transparent huge page allocations from PyTorch C10 memory allocator.

LANGUAGE: bash
CODE:
export THP_MEM_ALLOC_ENABLE=1

----------------------------------------

TITLE: HTML Meta Refresh Redirect for PyTorch Tutorial
DESCRIPTION: An HTML meta refresh tag that automatically redirects users to the new tutorial location after 3 seconds.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html'" />

----------------------------------------

TITLE: HTML Meta Redirect to ExecuTorch Documentation
DESCRIPTION: HTML meta refresh tag that automatically redirects users to the ExecuTorch documentation page after 3 seconds.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/executorch/stable/index.html'" />

----------------------------------------

TITLE: Implementing HTML Meta Redirect for PyTorch Audio Tutorial
DESCRIPTION: This HTML snippet creates a meta refresh tag to automatically redirect the user to the new location of the Audio Data Augmentation tutorial after 3 seconds.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/audio/stable/tutorials/audio_data_augmentation_tutorial.html'" />

----------------------------------------

TITLE: Enabling ITT Profiling in PyTorch (Python)
DESCRIPTION: This snippet demonstrates how to enable ITT profiling for a specific code block in PyTorch using the emit_itt() context manager.

LANGUAGE: python
CODE:
with torch.autograd.profiler.emit_itt():
  <code-to-be-profiled...>

----------------------------------------

TITLE: Implementing HTML Meta Refresh for Redirection in PyTorch Documentation
DESCRIPTION: This HTML snippet implements a meta refresh tag to automatically redirect the user to the ExecuTorch documentation page after 3 seconds. It's used to guide users from the deprecated PyTorch Mobile documentation to the new ExecuTorch project.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/executorch/stable/index.html'" />

----------------------------------------

TITLE: HTML Meta Refresh Redirect
DESCRIPTION: An HTML meta refresh tag that automatically redirects users to the updated PyTorch model saving tutorial after 3 seconds.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/beginner/saving_loading_models.html'" />

----------------------------------------

TITLE: Implementing HTML Meta Refresh for Redirection in PyTorch Documentation
DESCRIPTION: This HTML snippet implements a meta refresh tag to automatically redirect users to the ExecuTorch documentation page after 3 seconds. It's used to guide users from the deprecated PyTorch Mobile documentation to the new ExecuTorch project.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/executorch/stable/index.html'" />

----------------------------------------

TITLE: Training ResNet50 with FP32 using IPEX Backend
DESCRIPTION: Demonstrates model training using IPEX backend with FP32 precision. Includes data loading with CIFAR10 dataset, model optimization using ipex.optimize(), and training loop implementation with torch.compile.

LANGUAGE: python
CODE:
import torch
import torchvision

LR = 0.001
DOWNLOAD = True
DATA = 'datasets/cifar10/'

transform = torchvision.transforms.Compose([
  torchvision.transforms.Resize((224, 224)),
  torchvision.transforms.ToTensor(),
  torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
train_dataset = torchvision.datasets.CIFAR10(
  root=DATA,
  train=True,
  transform=transform,
  download=DOWNLOAD,
)
train_loader = torch.utils.data.DataLoader(
  dataset=train_dataset,
  batch_size=128
)

model = torchvision.models.resnet50()
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum=0.9)
model.train()

import intel_extension_for_pytorch as ipex

# Invoke the following API optionally, to apply frontend optimizations
model, optimizer = ipex.optimize(model, optimizer=optimizer)

compile_model = torch.compile(model, backend="ipex")

for batch_idx, (data, target) in enumerate(train_loader):
    optimizer.zero_grad()
    output = compile_model(data)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()

----------------------------------------

TITLE: HTML Meta Redirect to ExecuTorch Documentation
DESCRIPTION: HTML meta refresh tag that automatically redirects users to the ExecuTorch documentation page after a 3-second delay.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/executorch/stable/index.html'" />

----------------------------------------

TITLE: Exporting MViT Model with Static Batch Size
DESCRIPTION: Initial attempt to export an MViT video classification model that fails due to static batch size constraint.

LANGUAGE: python
CODE:
import numpy as np
import torch
from torchvision.models.video import MViT_V1_B_Weights, mvit_v1_b
import traceback as tb

model = mvit_v1_b(weights=MViT_V1_B_Weights.DEFAULT)

input_frames = torch.randn(2,16, 224, 224, 3)
input_frames = np.transpose(input_frames, (0, 4, 1, 2, 3))

exported_program = torch.export.export(
    model,
    (input_frames,),
)

input_frames = torch.randn(4,16, 224, 224, 3)
input_frames = np.transpose(input_frames, (0, 4, 1, 2, 3))
try:
    exported_program.module()(input_frames)
except Exception:
    tb.print_exc()

----------------------------------------

TITLE: HTML Meta Redirect to PyTorch Tutorials
DESCRIPTION: Meta refresh tag that redirects the page to the main PyTorch tutorials website after 1 second.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="1; url='https://pytorch.org/tutorials/'" />

----------------------------------------

TITLE: Making Predictions with Deployed Model
DESCRIPTION: Example of making online predictions using the deployed endpoint through Vertex AI SDK and handling image response

LANGUAGE: shell
CODE:
instances = [{"prompt": "An examplePup dog with a baseball jersey."}]
response = endpoint.predict(instances=instances)

with open("img.jpg", "wb") as g:
    g.write(base64.b64decode(response.predictions[0]))

display.Image("img.jpg")

----------------------------------------

TITLE: Redirecting to New TorchRec Tutorial using HTML Meta Refresh
DESCRIPTION: This HTML snippet uses a meta refresh tag to automatically redirect the user to the newer version of the TorchRec tutorial on the PyTorch website.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="0; url='https://pytorch.org/tutorials/intermediate/torchrec_intro_tutorial.html'" />

----------------------------------------

TITLE: Implementing HTML Meta Redirect for PyTorch Tutorial
DESCRIPTION: This HTML snippet creates a meta refresh tag that automatically redirects the user to the new PyTorch nn Package tutorial page after a 3-second delay. It's used to ensure users are directed to the most up-to-date documentation.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/beginner/nn_tutorial.html'" />

----------------------------------------

TITLE: DCGAN Generator Implementation
DESCRIPTION: Implementation of the DCGAN generator module using PyTorch C++ frontend classes and methods.

LANGUAGE: cpp
CODE:
struct DCGANGeneratorImpl : nn::Module {
  DCGANGeneratorImpl(int kNoiseSize)
      : conv1(nn::ConvTranspose2dOptions(kNoiseSize, 256, 4)
                  .bias(false)),
        batch_norm1(256),
        conv2(nn::ConvTranspose2dOptions(256, 128, 3)
                  .stride(2)
                  .padding(1)
                  .bias(false)),
        batch_norm2(128),
        conv3(nn::ConvTranspose2dOptions(128, 64, 4)
                  .stride(2)
                  .padding(1)
                  .bias(false)),
        batch_norm3(64),
        conv4(nn::ConvTranspose2dOptions(64, 1, 4)
                  .stride(2)
                  .padding(1)
                  .bias(false))
  {
    register_module("conv1", conv1);
    register_module("conv2", conv2);
    register_module("conv3", conv3);
    register_module("conv4", conv4);
    register_module("batch_norm1", batch_norm1);
    register_module("batch_norm2", batch_norm2);
    register_module("batch_norm3", batch_norm3);
  }

  torch::Tensor forward(torch::Tensor x) {
    x = torch::relu(batch_norm1(conv1(x)));
    x = torch::relu(batch_norm2(conv2(x)));
    x = torch::relu(batch_norm3(conv3(x)));
    x = torch::tanh(conv4(x));
    return x;
  }

  nn::ConvTranspose2d conv1, conv2, conv3, conv4;
  nn::BatchNorm2d batch_norm1, batch_norm2, batch_norm3;
};
TORCH_MODULE(DCGANGenerator);

----------------------------------------

TITLE: DCGAN Training Loop
DESCRIPTION: Implementation of the GAN training loop including discriminator and generator optimization steps.

LANGUAGE: cpp
CODE:
for (int64_t epoch = 1; epoch <= kNumberOfEpochs; ++epoch) {
  int64_t batch_index = 0;
  for (torch::data::Example<>& batch : *data_loader) {
    // Train discriminator with real images.
    discriminator->zero_grad();
    torch::Tensor real_images = batch.data;
    torch::Tensor real_labels = torch::empty(batch.data.size(0)).uniform_(0.8, 1.0);
    torch::Tensor real_output = discriminator->forward(real_images).reshape(real_labels.sizes());
    torch::Tensor d_loss_real = torch::binary_cross_entropy(real_output, real_labels);
    d_loss_real.backward();

    // Train discriminator with fake images.
    torch::Tensor noise = torch::randn({batch.data.size(0), kNoiseSize, 1, 1});
    torch::Tensor fake_images = generator->forward(noise);
    torch::Tensor fake_labels = torch::zeros(batch.data.size(0));
    torch::Tensor fake_output = discriminator->forward(fake_images.detach()).reshape(fake_labels.sizes());
    torch::Tensor d_loss_fake = torch::binary_cross_entropy(fake_output, fake_labels);
    d_loss_fake.backward();

    torch::Tensor d_loss = d_loss_real + d_loss_fake;
    discriminator_optimizer.step();

    // Train generator.
    generator->zero_grad();
    fake_labels.fill_(1);
    fake_output = discriminator->forward(fake_images).reshape(fake_labels.sizes());
    torch::Tensor g_loss = torch::binary_cross_entropy(fake_output, fake_labels);
    g_loss.backward();
    generator_optimizer.step();
  }
}

----------------------------------------

TITLE: Importing Required Libraries for PyTorch Quantization
DESCRIPTION: Imports necessary Python libraries including PyTorch, torchvision, and numpy for implementing quantization techniques.

LANGUAGE: python
CODE:
import os
import sys
import time
import numpy as np

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

import torchvision
from torchvision import datasets
import torchvision.transforms as transforms

# Set up warnings
import warnings 
warnings.filterwarnings(  
    action='ignore',  
    category=DeprecationWarning,  
    module=r'.*'  
) 
warnings.filterwarnings(  
    action='default', 
    module=r'torch.ao.quantization'
) 

# Specify random seed for repeatable results  
torch.manual_seed(191009)

----------------------------------------

TITLE: Implementing Post-Training Static Quantization in PyTorch
DESCRIPTION: Demonstrates the process of post-training static quantization, including model preparation, calibration, and conversion to a quantized model.

LANGUAGE: python
CODE:
num_calibration_batches = 32

myModel = load_model(saved_model_dir + float_model_file).to('cpu')
myModel.eval()

# Fuse Conv, bn and relu
myModel.fuse_model()

# Specify quantization configuration
myModel.qconfig = torch.ao.quantization.default_qconfig
print(myModel.qconfig)
torch.ao.quantization.prepare(myModel, inplace=True)

# Calibrate
print('Post Training Quantization Prepare: Inserting Observers')
evaluate(myModel, criterion, data_loader, neval_batches=num_calibration_batches)
print('Post Training Quantization: Calibration done')

# Convert to quantized model
torch.ao.quantization.convert(myModel, inplace=True)
print('Post Training Quantization: Convert done')

print("Size of model after quantization")
print_size_of_model(myModel)

top1, top5 = evaluate(myModel, criterion, data_loader_test, neval_batches=num_eval_batches)
print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))

----------------------------------------

TITLE: HTML Meta Refresh Redirect
DESCRIPTION: HTML meta tag that implements a 3-second redirect to the PyTorch tutorials main page.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/'" />

----------------------------------------

TITLE: Implementing HTML Redirect for Deprecated PyTorch Tutorial
DESCRIPTION: This HTML snippet implements a meta refresh tag to redirect users from a deprecated tutorial page to a new Executorch export tutorial after a 3-second delay.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/executorch/stable/tutorials/export-to-executorch-tutorial.html'" />

----------------------------------------

TITLE: HTML Meta Redirect to New PyTorch Autograd Tutorial
DESCRIPTION: HTML meta refresh tag that automatically redirects users to the new PyTorch Autograd tutorial location after 3 seconds.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html'" />

----------------------------------------

TITLE: HTML Meta Redirect to PyTorch Tutorials
DESCRIPTION: Meta refresh tag that automatically redirects users to the official PyTorch tutorials page after 3 seconds.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/'" />

----------------------------------------

TITLE: HTML Meta Refresh Redirect to PyTorch Tutorials
DESCRIPTION: Implements an automatic page redirect using HTML meta refresh tag to forward users to the PyTorch YouTube tutorial series page.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="0; url='https://pytorch.org/tutorials/beginner/introyt/introyt_index.html'" />

----------------------------------------

TITLE: HTML Meta Refresh Redirect to PyTorch Tutorials
DESCRIPTION: HTML meta tag that implements a 3-second redirect to the main PyTorch tutorials page.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials'" />

----------------------------------------

TITLE: HTML Meta Redirect to Updated PyTorch Tutorial
DESCRIPTION: HTML meta refresh tag that automatically redirects users to the new tutorial location after 3 seconds.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/beginner/saving_loading_models.html'" />

----------------------------------------

TITLE: Redirecting to PyTorch Tutorials Homepage using HTML Meta Tag
DESCRIPTION: This HTML snippet uses a meta tag to automatically redirect the user to the main PyTorch tutorials page after a 2-second delay. It's used as a fallback for browsers that don't support automatic redirection.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="2; url='https://pytorch.org/tutorials'" />

----------------------------------------

TITLE: HTML Meta Redirect to ExecuTorch Documentation
DESCRIPTION: HTML meta refresh tag that automatically redirects users to the ExecuTorch stable documentation page after a 3 second delay.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/executorch/stable/index.html'" />

----------------------------------------

TITLE: Implementing HTML Meta Refresh for Redirection
DESCRIPTION: This HTML snippet sets up a meta refresh tag to automatically redirect the user to the ExecuTorch documentation page after 3 seconds.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/executorch/stable/index.html'" />

----------------------------------------

TITLE: Implementing HTML Redirect for PyTorch Tutorial
DESCRIPTION: This HTML snippet creates a meta refresh tag to automatically redirect users to the updated PyTorch tutorial on data parallel processing after 3 seconds.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html'" />

----------------------------------------

TITLE: Redirecting to Updated PyTorch Tutorial Using HTML Meta Tag
DESCRIPTION: This HTML snippet creates a meta tag that automatically redirects the user to the updated PyTorch tutorial on saving and loading models after a 3-second delay.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/beginner/saving_loading_models.html'" />

----------------------------------------

TITLE: Implementing HTML Meta Refresh for PyTorch Tutorial Redirect
DESCRIPTION: This HTML snippet creates an automatic redirect to the new tutorial page after a 3-second delay using the meta refresh tag.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/executorch/stable/kernel-library-selective-build.html'" />

----------------------------------------

TITLE: Implementing HTML Meta Refresh for PyTorch Documentation Redirect
DESCRIPTION: This HTML snippet creates a meta refresh tag to automatically redirect users to the ExecuTorch documentation page after 3 seconds. It's used to guide users from the deprecated PyTorch Mobile documentation to the new supported framework.

LANGUAGE: html
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/executorch/stable/index.html'" />

----------------------------------------

TITLE: Redirecting to PyTorch Tutorials Index using HTML meta refresh
DESCRIPTION: This HTML code snippet uses a meta refresh tag to redirect the user to the PyTorch tutorials index page after a 3-second delay.

LANGUAGE: HTML
CODE:
<meta http-equiv="Refresh" content="3; url='https://pytorch.org/tutorials/index.html'" />