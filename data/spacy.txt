TITLE: Loading spaCy Models and Pipelines with spacy.load()
DESCRIPTION: Examples of loading spaCy language models from packages, file paths, and with various configuration options like disabling or excluding specific pipeline components.

LANGUAGE: python
CODE:
nlp = spacy.load("en_core_web_sm") # package
nlp = spacy.load("/path/to/pipeline") # string path
nlp = spacy.load(Path("/path/to/pipeline")) # pathlib Path

nlp = spacy.load("en_core_web_sm", exclude=["parser", "tagger"])

----------------------------------------

TITLE: Extracting Linguistic Annotations with spaCy in Python
DESCRIPTION: This code loads a pre-trained English language model, processes a sentence, and extracts various linguistic annotations for each token. It demonstrates how to access token attributes such as text, lemma, part-of-speech, syntactic dependencies, and other properties.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Apple is looking at buying U.K. startup for $1 billion")

for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
            token.shape_, token.is_alpha, token.is_stop)

----------------------------------------

TITLE: Installing spaCy Language Models via pip
DESCRIPTION: Shows how to install a spaCy language model using pip, which is an alternative to the spaCy download command.

LANGUAGE: bash
CODE:
$ pip install -U %%SPACY_PKG_NAME%%SPACY_PKG_FLAGS
$ python -m spacy download en_core_web_sm

----------------------------------------

TITLE: Registering Custom Components with Language.factory in spaCy v3.0
DESCRIPTION: This snippet demonstrates how to use the @Language.factory decorator to register a custom component in spaCy v3.0. It shows proper configuration of factory functions with required named parameters (nlp and name) and a custom component class implementation.

LANGUAGE: python
CODE:
from spacy.language import Language

@Language.factory("my_component", default_config={"some_setting": False})
def create_component(nlp: Language, name: str, some_setting: bool):
    return MyCoolComponent(some_setting=some_setting)


class MyCoolComponent:
    def __init__(self, some_setting):
        self.some_setting = some_setting

    def __call__(self, doc):
        # Do something to the doc
        return doc

----------------------------------------

TITLE: Creating a Custom Tokenizer in spaCy with Regular Expressions
DESCRIPTION: Demonstrates how to create a custom tokenizer for spaCy using regular expressions. This example defines special cases, prefix, suffix, and infix patterns to control how text is split into tokens, including handling emoticons and punctuation.

LANGUAGE: python
CODE:
import re
import spacy
from spacy.tokenizer import Tokenizer

special_cases = {":)": [{"ORTH": ":)"}]}
prefix_re = re.compile(r'''^[\\[\\("']''')
suffix_re = re.compile(r'''[\\]\\)"']$''')
infix_re = re.compile(r'''[-~]''')
simple_url_re = re.compile(r'''^https?://''')

def custom_tokenizer(nlp):
    return Tokenizer(nlp.vocab, rules=special_cases,
                                prefix_search=prefix_re.search,
                                suffix_search=suffix_re.search,
                                infix_finditer=infix_re.finditer,
                                url_match=simple_url_re.match)

nlp = spacy.load("en_core_web_sm")
nlp.tokenizer = custom_tokenizer(nlp)
doc = nlp("hello-world. :)")
print([t.text for t in doc]) # ['hello', '-', 'world.', ':)']

----------------------------------------

TITLE: Configuring GPT-4 LLM Model in spaCy
DESCRIPTION: This snippet demonstrates how to configure a GPT-4 language model in spaCy's configuration file. It shows how to specify the model type, name, and additional parameters like temperature.

LANGUAGE: ini
CODE:
[components.llm.model]
@llm_models = "spacy.GPT-4.v1"
name = "gpt-4"
config = {"temperature": 0.0}

----------------------------------------

TITLE: Using spaCy in Python
DESCRIPTION: Demonstrates basic usage of spaCy for natural language processing. The example loads a small English model, processes a sentence, and iterates through tokens to print their text and part-of-speech tags.

LANGUAGE: python
CODE:
import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("This is a sentence.")
for token in doc:
    print(token.text, token.pos_)

----------------------------------------

TITLE: Integrating a Custom NLP Model with spaCy
DESCRIPTION: Comprehensive example of wrapping an external NLP model that provides POS tags, dependency parsing, and heads, converting the output to spaCy's document format using Doc.from_array.

LANGUAGE: python
CODE:
import your_custom_model
from spacy.language import Language
from spacy.symbols import POS, TAG, DEP, HEAD
from spacy.tokens import Doc
import numpy

@Language.component("custom_model_wrapper")
def custom_model_wrapper(doc):
    words = [token.text for token in doc]
    spaces = [token.whitespace for token in doc]
    pos, tags, deps, heads = your_custom_model(words)
    # Convert the strings to integers and add them to the string store
    pos = [doc.vocab.strings.add(label) for label in pos]
    tags = [doc.vocab.strings.add(label) for label in tags]
    deps = [doc.vocab.strings.add(label) for label in deps]
    # Create a new Doc from a numpy array
    attrs = [POS, TAG, DEP, HEAD]
    arr = numpy.array(list(zip(pos, tags, deps, heads)), dtype="uint64")
    new_doc = Doc(doc.vocab, words=words, spaces=spaces).from_array(attrs, arr)
    return new_doc

----------------------------------------

TITLE: Loading and Using spaCy Models in Python
DESCRIPTION: Python code example showing how to load a spaCy language model and process a simple text. It demonstrates two different methods for loading models: using spacy.load() and importing the model directly.

LANGUAGE: python
CODE:
import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("This is a sentence.")

----------------------------------------

TITLE: Implementing a Custom Data Reader with Variants
DESCRIPTION: Example of creating and registering a custom corpus reader function that streams data with text classification annotations and creates lexical variants. The function is registered in spaCy's readers registry and can be referenced in config files using the assigned name.

LANGUAGE: python
CODE:
from typing import Callable, Iterator, List
import spacy
from spacy.training import Example
from spacy.language import Language
import random

@spacy.registry.readers("corpus_variants.v1")
def stream_data(source: str) -> Callable[[Language], Iterator[Example]]:
    def generate_stream(nlp):
        for text, cats in read_custom_data(source):
            # Create a random variant of the example text
            i = random.randint(0, len(text) - 1)
            variant = text[:i] + text[i].upper() + text[i + 1:]
            doc = nlp.make_doc(variant)
            example = Example.from_dict(doc, {"cats": cats})
            yield example

    return generate_stream

----------------------------------------

TITLE: Implementing a Training Loop for spaCy Models (Python)
DESCRIPTION: A complete example of a training loop that initializes an optimizer, shuffles training data, creates Example objects from raw text and entity annotations, updates the model, and saves the trained pipeline to disk.

LANGUAGE: python
CODE:
optimizer = nlp.initialize()
for itn in range(100):
    random.shuffle(train_data)
    for raw_text, entity_offsets in train_data:
        doc = nlp.make_doc(raw_text)
        example = Example.from_dict(doc, {"entities": entity_offsets})
        nlp.update([example], sgd=optimizer)
nlp.to_disk("/output")

----------------------------------------

TITLE: Initializing a Doc Object in spaCy
DESCRIPTION: Two methods of constructing a Doc object: using the nlp pipeline directly or manually creating a Doc object with words and spaces. The Doc object is the core container for processed text data in spaCy.

LANGUAGE: python
CODE:
# Construction 1
doc = nlp("Some text")

# Construction 2
from spacy.tokens import Doc

words = ["hello", "world", "!"]
spaces = [True, False, False]
doc = Doc(nlp.vocab, words=words, spaces=spaces)

----------------------------------------

TITLE: Registering Custom Pipeline Components with Language.factory in spaCy
DESCRIPTION: Demonstrates how to register a custom pipeline component factory using the Language.factory decorator or function. This allows component initialization by name via Language.add_pipe and reference in config files.

LANGUAGE: python
CODE:
from spacy.language import Language

# Usage as a decorator
@Language.factory(
   "my_component",
   default_config={"some_setting": True},
)
def create_my_component(nlp, name, some_setting):
     return MyComponent(some_setting)

# Usage as function
Language.factory(
    "my_component",
    default_config={"some_setting": True},
    func=create_my_component
)

----------------------------------------

TITLE: Loading spaCy Models
DESCRIPTION: Utility function to load a spaCy pipeline from a package name or data path. It supports loading from Python packages or data directories, with options to disable, enable, or exclude specific pipeline components.

LANGUAGE: python
CODE:
nlp = util.load_model("en_core_web_sm")
nlp = util.load_model("en_core_web_sm", exclude=["ner"])
nlp = util.load_model("/path/to/data")

----------------------------------------

TITLE: Configuring Custom Transformer Span Getter in spaCy
DESCRIPTION: Configuration for a custom span getter that processes sentences with a maximum token length. This INI configuration references a custom function that will be defined in code.

LANGUAGE: ini
CODE:
[components.transformer.model.get_spans]
@span_getters = "custom_sent_spans"
max_length = 25

----------------------------------------

TITLE: Basic Text Processing with spaCy
DESCRIPTION: Demonstrates how to process a text string using spaCy's NLP pipeline. The nlp object processes the input text and returns a Doc object that contains the processed linguistic annotations.

LANGUAGE: python
CODE:
doc = nlp("This is a text")

----------------------------------------

TITLE: Lemmatization in spaCy
DESCRIPTION: Shows how to use spaCy's rule-based lemmatizer to get base forms of words. The example loads an English model, identifies the lemmatizer component, and displays the lemma for each token in a sentence.

LANGUAGE: python
CODE:
import spacy

# English pipelines include a rule-based lemmatizer
nlp = spacy.load("en_core_web_sm")
lemmatizer = nlp.get_pipe("lemmatizer")
print(lemmatizer.mode)  # 'rule'

doc = nlp("I was reading the paper.")
print([token.lemma_ for token in doc])
# ['I', 'be', 'read', 'the', 'paper', '.']

----------------------------------------

TITLE: Creating and Managing Custom Pipeline Components
DESCRIPTION: Shows how to define a custom component using decorators, add it to a pipeline, source components from other pipelines, and analyze the pipeline structure. This demonstrates the improved component APIs in spaCy v3.0.

LANGUAGE: python
CODE:
@Language.component("my_component")
def my_component(doc):
    return doc

nlp.add_pipe("my_component")
nlp.add_pipe("ner", source=other_nlp)
nlp.analyze_pipes(pretty=True)

----------------------------------------

TITLE: Initializing LLM Components in spaCy with Different Configurations
DESCRIPTION: Demonstrates various ways to construct LLM components in spaCy, including using the default GPT-3.5 model, task-specific factories, and custom models. Shows different approaches from using add_pipe to direct class instantiation.

LANGUAGE: python
CODE:
# Construction via add_pipe with the default GPT 3.5 model and an explicitly defined task
config = {"task": {"@llm_tasks": "spacy.NER.v3", "labels": ["PERSON", "ORGANISATION", "LOCATION"]}}
llm = nlp.add_pipe("llm", config=config)

# Construction via add_pipe with a task-specific factory and default GPT3.5 model
llm = nlp.add_pipe("llm_ner")

# Construction via add_pipe with a task-specific factory and custom model
llm = nlp.add_pipe("llm_ner", config={"model": {"@llm_models": "spacy.Dolly.v1", "name": "dolly-v2-12b"}})

# Construction from class
from spacy_llm.pipeline import LLMWrapper
llm = LLMWrapper(vocab=nlp.vocab, task=task, model=model, cache=cache, save_io=True)

----------------------------------------

TITLE: Implementing REST Countries Pipeline Component in spaCy
DESCRIPTION: This code implements a spaCy pipeline component that fetches country data via the REST Countries API, sets entity annotations for countries, and adds custom attributes to Doc and Span objects including capital, coordinates, and flag data.

LANGUAGE: python
CODE:
import requests
from spacy.lang.en import English
from spacy.language import Language
from spacy.matcher import PhraseMatcher
from spacy.tokens import Doc, Span, Token

@Language.factory("rest_countries")
class RESTCountriesComponent:
    def __init__(self, nlp, name, label="GPE"):
        r = requests.get("https://restcountries.com/v2/all")
        r.raise_for_status()  # make sure requests raises an error if it fails
        countries = r.json()
        # Convert API response to dict keyed by country name for easy lookup
        self.countries = {c["name"]: c for c in countries}
        self.label = label
        # Set up the PhraseMatcher with Doc patterns for each country name
        self.matcher = PhraseMatcher(nlp.vocab)
        self.matcher.add("COUNTRIES", [nlp.make_doc(c) for c in self.countries.keys()])
        # Register attributes on the Span. We'll be overwriting this based on
        # the matches, so we're only setting a default value, not a getter.
        Span.set_extension("is_country", default=None)
        Span.set_extension("country_capital", default=None)
        Span.set_extension("country_latlng", default=None)
        Span.set_extension("country_flag", default=None)
        # Register attribute on Doc via a getter that checks if the Doc
        # contains a country entity
        Doc.set_extension("has_country", getter=self.has_country)

    def __call__(self, doc):
        spans = []  # keep the spans for later so we can merge them afterwards
        for _, start, end in self.matcher(doc):
            # Generate Span representing the entity & set label
            entity = Span(doc, start, end, label=self.label)
            # Set custom attributes on entity. Can be extended with other data
            # returned by the API, like currencies, country code, calling code etc.
            entity._.set("is_country", True)
            entity._.set("country_capital", self.countries[entity.text]["capital"])
            entity._.set("country_latlng", self.countries[entity.text]["latlng"])
            entity._.set("country_flag", self.countries[entity.text]["flag"])
            spans.append(entity)
        # Overwrite doc.ents and add entity – be careful not to replace!
        doc.ents = list(doc.ents) + spans
        return doc  # don't forget to return the Doc!

    def has_country(self, doc):
        """Getter for Doc attributes. Since the getter is only called
        when we access the attribute, we can refer to the Span's 'is_country'
        attribute here, which is already set in the processing step."""
        return any([entity._.get("is_country") for entity in doc.ents])

nlp = English()
nlp.add_pipe("rest_countries", config={"label": "GPE"})
doc = nlp("Some text about Colombia and the Czech Republic")
print("Pipeline", nlp.pipe_names)  # pipeline contains component name
print("Doc has countries", doc._.has_country)  # Doc contains countries
for ent in doc.ents:
    if ent._.is_country:
        print(ent.text, ent.label_, ent._.country_capital, ent._.country_latlng, ent._.country_flag)

----------------------------------------

TITLE: Serializing and Deserializing spaCy NLP Pipeline
DESCRIPTION: Demonstrates how to save a spaCy pipeline to bytes and restore it. This includes extracting the configuration and binary data, then reconstructing the pipeline by initializing the language class from the config and loading the binary data.

LANGUAGE: python
CODE:
config = nlp.config
bytes_data = nlp.to_bytes()

LANGUAGE: python
CODE:
lang_cls = spacy.util.get_lang_class(config["nlp"]["lang"])
nlp = lang_cls.from_config(config)
nlp.from_bytes(bytes_data)

----------------------------------------

TITLE: Initializing Language Objects in spaCy
DESCRIPTION: This snippet demonstrates how to create language-specific NLP objects in spaCy. It imports and instantiates both English and German language processors, which include their respective language data such as tokenization rules, stop words, and lexical attributes.

LANGUAGE: python
CODE:
from spacy.lang.en import English
from spacy.lang.de import German

nlp_en = English()  # Includes English data
nlp_de = German()  # Includes German data

----------------------------------------

TITLE: Downloading spaCy Model Packages
DESCRIPTION: Commands demonstrating different methods to download and install spaCy language models, including using the spaCy download command and pip installation from local or remote sources.

LANGUAGE: bash
CODE:
# Download best-matching version of specific model for your spaCy installation
python -m spacy download en_core_web_sm

# pip install .tar.gz archive or .whl from path or URL
pip install /Users/you/en_core_web_sm-3.0.0.tar.gz
pip install /Users/you/en_core_web_sm-3.0.0-py3-none-any.whl
pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz

----------------------------------------

TITLE: Visualizing Named Entities with displaCy in Python
DESCRIPTION: This example demonstrates how to use spaCy's displaCy module to visualize named entities recognized in text. It loads the small English model, processes a sample text about Sebastian Thrun, and serves the visualization in a web interface.

LANGUAGE: python
CODE:
import spacy
from spacy import displacy

text = "When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously."

nlp = spacy.load("en_core_web_sm")
doc = nlp(text)
displacy.serve(doc, style="ent")

----------------------------------------

TITLE: Downloading spaCy Language Models via Command Line
DESCRIPTION: Demonstrates how to download pre-trained language models using spaCy's CLI commands, including downloading the best-matching version for your spaCy installation or an exact package version.

LANGUAGE: bash
CODE:
# Download best-matching version of a package for your spaCy installation
$ python -m spacy download en_core_web_sm

# Download exact package version
$ python -m spacy download en_core_web_sm-3.0.0 --direct

----------------------------------------

TITLE: Implementing a Custom Whitespace Tokenizer in spaCy
DESCRIPTION: A complete example of implementing a basic whitespace tokenizer in spaCy. This custom tokenizer splits text on space characters and handles edge cases like empty strings and trailing spaces to create a valid Doc object.

LANGUAGE: python
CODE:
import spacy
from spacy.tokens import Doc

class WhitespaceTokenizer:
    def __init__(self, vocab):
        self.vocab = vocab

    def __call__(self, text):
        words = text.split(" ")
        spaces = [True] * len(words)
        # Avoid zero-length tokens
        for i, word in enumerate(words):
            if word == "":
                words[i] = " "
                spaces[i] = False
        # Remove the final trailing space
        if words[-1] == " ":
            words = words[0:-1]
            spaces = spaces[0:-1]
        else:
           spaces[-1] = False

        return Doc(self.vocab, words=words, spaces=spaces)

nlp = spacy.blank("en")
nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)
doc = nlp("What's happened to me? he thought. It wasn't a dream.")
print([token.text for token in doc])

----------------------------------------

TITLE: Few-Shot Learning Examples for Translation in YAML
DESCRIPTION: YAML file containing examples for few-shot learning in translation tasks. Each example includes original text and its translation, which will be injected into the prompt to improve the LLM's performance.

LANGUAGE: yaml
CODE:
- text: 'Top of the morning to you!'
  translation: '¡Muy buenos días!'
- text: 'The weather is great today.'
  translation: 'El clima está fantástico hoy.'
- text: 'Do you know what will happen tomorrow?'
  translation: '¿Sabes qué pasará mañana?'

----------------------------------------

TITLE: Installing spaCy using pip
DESCRIPTION: Commands to install spaCy using pip package manager. It shows the recommended approach of updating pip dependencies first and then installing spaCy in a virtual environment.

LANGUAGE: bash
CODE:
pip install -U pip setuptools wheel
pip install spacy

----------------------------------------

TITLE: Accessing Tokens and Spans in a spaCy Doc
DESCRIPTION: Demonstration of accessing tokens and spans from a Doc object using indexing and slicing. Shows how to access individual tokens and contiguous spans of tokens, with support for negative indexing.

LANGUAGE: python
CODE:
doc = nlp("Give it back! He pleaded.")
assert doc[0].text == "Give"
assert doc[-1].text == "."
span = doc[1:3]
assert span.text == "it back"

----------------------------------------

TITLE: Complete Dependency Matcher Implementation for 'Founded' Relationship in spaCy
DESCRIPTION: This example shows a complete implementation of a dependency matcher that identifies 'founded' relationships in text. It defines a pattern that captures the founder (subject), the company (object), and company modifiers, then prints each matched token and its corresponding role.

LANGUAGE: python
CODE:
import spacy
from spacy.matcher import DependencyMatcher

nlp = spacy.load("en_core_web_sm")
matcher = DependencyMatcher(nlp.vocab)

pattern = [
    {
        "RIGHT_ID": "anchor_founded",
        "RIGHT_ATTRS": {"ORTH": "founded"}
    },
    {
        "LEFT_ID": "anchor_founded",
        "REL_OP": ">",
        "RIGHT_ID": "founded_subject",
        "RIGHT_ATTRS": {"DEP": "nsubj"},
    },
    {
        "LEFT_ID": "anchor_founded",
        "REL_OP": ">",
        "RIGHT_ID": "founded_object",
        "RIGHT_ATTRS": {"DEP": "dobj"},
    },
    {
        "LEFT_ID": "founded_object",
        "REL_OP": ">",
        "RIGHT_ID": "founded_object_modifier",
        "RIGHT_ATTRS": {"DEP": {"IN": ["amod", "compound"]}},
    }
]

matcher.add("FOUNDED", [pattern])
doc = nlp("Lee, an experienced CEO, has founded two AI startups.")
matches = matcher(doc)

print(matches) # [(4851363122962674176, [6, 0, 10, 9])]
# Each token_id corresponds to one pattern dict
match_id, token_ids = matches[0]
for i in range(len(token_ids)):
    print(pattern[i]["RIGHT_ID"] + ":", doc[token_ids[i]].text)

----------------------------------------

TITLE: Creating a Trainable Component in spaCy
DESCRIPTION: This example shows the basic structure for implementing a custom trainable component in spaCy. It includes a TrainablePipe subclass with the required predict and set_annotations methods, and a component factory function registered using the Language.factory decorator.

LANGUAGE: python
CODE:
from spacy.pipeline import TrainablePipe
from spacy.language import Language

class TrainableComponent(TrainablePipe):
    def predict(self, docs):
        ...

    def set_annotations(self, docs, scores):
        ...

@Language.factory("my_trainable_component")
def make_component(nlp, name, model):
    return TrainableComponent(nlp.vocab, model, name=name)

----------------------------------------

TITLE: Implementing the Instance Tensor Creation Layer in Python
DESCRIPTION: Creates a custom Thinc layer that generates tensors representing relation instances from documents. The layer combines token vectors, entity pooling, and candidate pair generation.

LANGUAGE: python
CODE:
@spacy.registry.architectures("rel_instance_tensor.v1")
def create_tensors(
    tok2vec: Model[List[Doc], List[Floats2d]],
    pooling: Model[Ragged, Floats2d],
    get_instances: Callable[[Doc], List[Tuple[Span, Span]]],
) -> Model[List[Doc], Floats2d]:

    return Model(
        "instance_tensors",
        instance_forward,
        init=instance_init,
        layers=[tok2vec, pooling],
        refs={"tok2vec": tok2vec, "pooling": pooling},
        attrs={"get_instances": get_instances},
    )


# The custom forward function
def instance_forward(
    model: Model[List[Doc], Floats2d],
    docs: List[Doc],
    is_train: bool,
) -> Tuple[Floats2d, Callable]:
    tok2vec = model.get_ref("tok2vec")
    tokvecs, bp_tokvecs = tok2vec(docs, is_train)
    get_instances = model.attrs["get_instances"]
    all_instances = [get_instances(doc) for doc in docs]
    pooling = model.get_ref("pooling")
    relations = ...

    def backprop(d_relations: Floats2d) -> List[Doc]:
        d_tokvecs = ...
        return bp_tokvecs(d_tokvecs)

    return relations, backprop


# The custom initialization method
def instance_init(
    model: Model,
    X: List[Doc] = None,
    Y: Floats2d = None,
) -> Model:
    tok2vec = model.get_ref("tok2vec")
    tok2vec.initialize(X)
    return model

----------------------------------------

TITLE: Downloading spaCy Models Programmatically in Python
DESCRIPTION: Demonstrates how to import and use spaCy's download function in Python code to download language models programmatically.

LANGUAGE: python
CODE:
import spacy
spacy.cli.download("en_core_web_sm")

----------------------------------------

TITLE: Visualizing Dependency Parsing with displaCy in Python
DESCRIPTION: This code demonstrates how to use spaCy's displaCy to visualize the dependency parse of a simple sentence. It loads the English model, processes a short text, and serves the visualization in a browser.

LANGUAGE: python
CODE:
import spacy
from spacy import displacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("This is a sentence.")
displacy.serve(doc, style="dep")

----------------------------------------

TITLE: Implementing Custom Neural Network Architecture for spaCy
DESCRIPTION: Defines a custom neural network architecture that can be used for pipeline components like taggers. The architecture is registered in spaCy's architectures registry and can be referenced in the config file.

LANGUAGE: python
CODE:
from typing import List
from thinc.types import Floats2d
from thinc.api import Model
import spacy
from spacy.tokens import Doc

@spacy.registry.architectures("custom_neural_network.v1")
def custom_neural_network(output_width: int) -> Model[List[Doc], List[Floats2d]]:
    return create_model(output_width)

----------------------------------------

TITLE: Training a spaCy Model with Custom Data
DESCRIPTION: Command to train a spaCy model using a configuration file and specifying output directory and training/development data paths.

LANGUAGE: bash
CODE:
$ python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./dev.spacy

----------------------------------------

TITLE: Configuring EntityRuler in spaCy Pipeline
DESCRIPTION: Example showing how to configure an EntityRuler with custom settings and add it to the spaCy pipeline. The config options control phrase matching attributes, validation, entity overwriting behavior, and entity ID separators.

LANGUAGE: python
CODE:
config = {
   "phrase_matcher_attr": None,
   "validate": True,
   "overwrite_ents": False,
   "ent_id_sep": "||",
}
nlp.add_pipe("entity_ruler", config=config)

----------------------------------------

TITLE: Defining Pipeline Components in spaCy Config
DESCRIPTION: Shows how to configure pipeline components in a spaCy config file, including sourcing components from existing models and creating new components from factories. Demonstrates both pre-trained and custom components.

LANGUAGE: ini
CODE:
[components]

# "parser" and "ner" are sourced from a trained pipeline
[components.parser]
source = "en_core_web_sm"

[components.ner]
source = "en_core_web_sm"

# "textcat" and "custom" are created blank from a built-in / custom factory
[components.textcat]
factory = "textcat"

[components.custom]
factory = "your_custom_factory"
your_custom_setting = true

----------------------------------------

TITLE: Evaluating Pipeline Components with Language.evaluate in spaCy Python
DESCRIPTION: Evaluates a spaCy pipeline's components using a batch of Example objects. Returns a dictionary of evaluation scores that can be customized with various parameters.

LANGUAGE: python
CODE:
scores = nlp.evaluate(examples)
print(scores)

----------------------------------------

TITLE: Creating a Doc Object Example
DESCRIPTION: Shows how to manually construct a Doc object with words and spaces information. This code demonstrates the minimal requirements for creating a Doc: the shared vocabulary and a list of words.

LANGUAGE: python
CODE:
words = ["Let", "'s", "go", "!"]
spaces = [False, True, False, False]
doc = Doc(nlp.vocab, words=words, spaces=spaces)

----------------------------------------

TITLE: Creating Training Data for spaCy NER Models in Python
DESCRIPTION: Demonstrates how to convert NER annotations into spaCy's .spacy format by creating Doc objects with entity annotations and saving them using DocBin. This approach builds training data for named entity recognition.

LANGUAGE: python
CODE:
import spacy
from spacy.tokens import DocBin

nlp = spacy.blank("en")
training_data = [
  ("Tokyo Tower is 333m tall.", [(0, 11, "BUILDING")]),
]
# the DocBin will store the example documents
db = DocBin()
for text, annotations in training_data:
    doc = nlp(text)
    ents = []
    for start, end, label in annotations:
        span = doc.char_span(start, end, label=label)
        ents.append(span)
    doc.ents = ents
    db.add(doc)
db.to_disk("./train.spacy")

----------------------------------------

TITLE: Implementing spaCy's Tokenization Algorithm in Python
DESCRIPTION: Pseudo-code implementation of spaCy's tokenization algorithm optimized for readability. The algorithm processes text by handling special cases, prefix/suffix matching, URL detection, and infix splitting to produce tokens while preserving the original text structure.

LANGUAGE: python
CODE:
def tokenizer_pseudo_code(
    text,
    special_cases,
    prefix_search,
    suffix_search,
    infix_finditer,
    token_match,
    url_match
):
    tokens = []
    for substring in text.split():
        suffixes = []
        while substring:
            if substring in special_cases:
                tokens.extend(special_cases[substring])
                substring = ""
                continue
            while prefix_search(substring) or suffix_search(substring):
                if token_match(substring):
                    tokens.append(substring)
                    substring = ""
                    break
                if substring in special_cases:
                    tokens.extend(special_cases[substring])
                    substring = ""
                    break
                if prefix_search(substring):
                    split = prefix_search(substring).end()
                    tokens.append(substring[:split])
                    substring = substring[split:]
                    if substring in special_cases:
                        continue
                if suffix_search(substring):
                    split = suffix_search(substring).start()
                    suffixes.append(substring[split:])
                    substring = substring[:split]
            if token_match(substring):
                tokens.append(substring)
                substring = ""
            elif url_match(substring):
                tokens.append(substring)
                substring = ""
            elif substring in special_cases:
                tokens.extend(special_cases[substring])
                substring = ""
            elif list(infix_finditer(substring)):
                infixes = infix_finditer(substring)
                offset = 0
                for match in infixes:
                    if offset == 0 and match.start() == 0:
                        continue
                    tokens.append(substring[offset : match.start()])
                    tokens.append(substring[match.start() : match.end()])
                    offset = match.end()
                if substring[offset:]:
                    tokens.append(substring[offset:])
                substring = ""
            elif substring:
                tokens.append(substring)
                substring = ""
        tokens.extend(reversed(suffixes))
    for match in matcher(special_cases, text):
        tokens.replace(match, special_cases[match])
    return tokens

----------------------------------------

TITLE: Installing spaCy from Source Code
DESCRIPTION: Series of commands to clone the spaCy GitHub repository and build it from source, including setting up a virtual environment and installing dependencies. This is useful for developers who want to modify the code base.

LANGUAGE: bash
CODE:
git clone https://github.com/explosion/spaCy
cd spaCy

python -m venv .env
source .env/bin/activate

# make sure you are using the latest pip
python -m pip install -U pip setuptools wheel

pip install -r requirements.txt
pip install --no-build-isolation --editable .

----------------------------------------

TITLE: Navigating the Dependency Parse Tree in spaCy with Python
DESCRIPTION: This snippet shows how to navigate the dependency parse tree by iterating through tokens in a document. For each token, it displays the text, dependency relation, head text, head part-of-speech, and children tokens.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Autonomous cars shift insurance liability toward manufacturers")
for token in doc:
    print(token.text, token.dep_, token.head.text, token.head.pos_,
            [child for child in token.children])

----------------------------------------

TITLE: Accessing Named Entity Annotations in spaCy
DESCRIPTION: This code demonstrates how to access named entity annotations at both document and token levels. It shows how to extract entity text, character offsets, entity labels, and IOB tags from a processed document.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("San Francisco considers banning sidewalk delivery robots")

# document level
ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]
print(ents)

# token level
ent_san = [doc[0].text, doc[0].ent_iob_, doc[0].ent_type_]
ent_francisco = [doc[1].text, doc[1].ent_iob_, doc[1].ent_type_]
print(ent_san)  # ['San', 'B', 'GPE']
print(ent_francisco)  # ['Francisco', 'I', 'GPE']

----------------------------------------

TITLE: Registering Stateful Class Components in spaCy v3.0
DESCRIPTION: Demonstrates how to register a class-based pipeline component with the @Language.factory decorator. The component is initialized with the nlp object and component name.

LANGUAGE: diff
CODE:
+ from spacy.language import Language

+ @Language.factory("my_component")
class MyComponent:
-   def __init__(self, nlp):
+   def __init__(self, nlp, name):
        self.nlp = nlp

    def __call__(self, doc):
        return doc

----------------------------------------

TITLE: Implementing a Debug Component with Type Hints in spaCy
DESCRIPTION: This snippet demonstrates creating a custom debug pipeline component that logs information about the nlp object and processed documents. It uses pydantic's StrictStr type for validation of the log_level parameter, ensuring only string values are accepted.

LANGUAGE: python
CODE:
import spacy
from spacy.language import Language
from spacy.tokens import Doc
from pydantic import StrictStr
import logging

@Language.factory("debug", default_config={"log_level": "DEBUG"})
class DebugComponent:
    def __init__(self, nlp: Language, name: str, log_level: StrictStr):
        self.logger = logging.getLogger(f"spacy.{name}")
        self.logger.setLevel(log_level)
        self.logger.info(f"Pipeline: {nlp.pipe_names}")

    def __call__(self, doc: Doc) -> Doc:
        is_tagged = doc.has_annotation("TAG")
        self.logger.debug(f"Doc: {len(doc)} tokens, is tagged: {is_tagged}")
        return doc

nlp = spacy.load("en_core_web_sm")
nlp.add_pipe("debug", config={"log_level": "DEBUG"})
doc = nlp("This is a text...")

----------------------------------------

TITLE: Basic Entity Ruler Usage for Named Entity Recognition in spaCy
DESCRIPTION: This example demonstrates how to use the EntityRuler component for rule-based named entity recognition. It creates patterns for organizations and geopolitical entities, adds them to the ruler, and processes text to identify matches as entities.

LANGUAGE: python
CODE:
from spacy.lang.en import English

nlp = English()
ruler = nlp.add_pipe("entity_ruler")
patterns = [{"label": "ORG", "pattern": "Apple"},
            {"label": "GPE", "pattern": [{"LOWER": "san"}, {"LOWER": "francisco"}]}]
ruler.add_patterns(patterns)

doc = nlp("Apple is opening its first big office in San Francisco.")
print([(ent.text, ent.label_) for ent in doc.ents])

----------------------------------------

TITLE: Using the spaCy train function from Python code
DESCRIPTION: Example demonstrating how to call the spaCy train function directly from Python code instead of using the command line interface, with config overrides for data paths.

LANGUAGE: python
CODE:
from spacy.cli.train import train

train("./config.cfg", overrides={"paths.train": "./train.spacy", "paths.dev": "./dev.spacy"})

----------------------------------------

TITLE: Implementing Phone Number Matching with spaCy's Matcher
DESCRIPTION: Complete example showing how to match phone numbers in text using spaCy's Matcher. The pattern identifies numbers in the format '(123) 456 789' and extracts them from the document.

LANGUAGE: python
CODE:
import spacy
from spacy.matcher import Matcher

nlp = spacy.load("en_core_web_sm")
matcher = Matcher(nlp.vocab)
pattern = [{"ORTH": "("}, {"SHAPE": "ddd"}, {"ORTH": ")"}, {"SHAPE": "ddd"},
           {"ORTH": "-", "OP": "?"}, {"SHAPE": "ddd"}]
matcher.add("PHONE_NUMBER", [pattern])

doc = nlp("Call me at (123) 456 789 or (123) 456 789!")
print([t.text for t in doc])
matches = matcher(doc)
for match_id, start, end in matches:
    span = doc[start:end]
    print(span.text)

----------------------------------------

TITLE: Wrapping a Custom Entity Recognizer in spaCy
DESCRIPTION: Example of creating a custom pipeline component that wraps an external entity recognition model, converting its output to spaCy's entity spans format.

LANGUAGE: python
CODE:
import your_custom_entity_recognizer
from spacy.training import biluo_tags_to_spans
from spacy.language import Language

@Language.component("custom_ner_wrapper")
def custom_ner_wrapper(doc):
    words = [token.text for token in doc]
    custom_entities = your_custom_entity_recognizer(words)
    doc.ents = biluo_tags_to_spans(doc, custom_entities)
    return doc

----------------------------------------

TITLE: Processing Text with Language.__call__ in spaCy
DESCRIPTION: Shows how to apply the spaCy processing pipeline to text using the Language.__call__ method. This processes text that can span multiple sentences and preserves alignment with the original string.

LANGUAGE: python
CODE:
doc = nlp("An example sentence. Another sentence.")
assert (doc[0].text, doc[0].head.tag_) == ("An", "NN")

----------------------------------------

TITLE: Creating Custom Data Augmenter in Python
DESCRIPTION: Custom augmentation callback that creates text variants in 'SpOnGeBoB cAsE'. It demonstrates how to register a custom augmenter with spaCy's registry system and shows how to transform text and yield both original and augmented examples.

LANGUAGE: python
CODE:
import spacy
import random

@spacy.registry.augmenters("spongebob_augmenter.v1")
def create_augmenter(randomize: bool = False):
    def augment(nlp, example):
        text = example.text
        if randomize:
            # Randomly uppercase/lowercase characters
            chars = [c.lower() if random.random() < 0.5 else c.upper() for c in text]
        else:
            # Uppercase followed by lowercase
            chars = [c.lower() if i % 2 else c.upper() for i, c in enumerate(text)]
        # Create augmented training example
        example_dict = example.to_dict()
        doc = nlp.make_doc("".join(chars))
        example_dict["token_annotation"]["ORTH"] = [t.text for t in doc]
        # Original example followed by augmented example
        yield example
        yield example.from_dict(doc, example_dict)

    return augment

----------------------------------------

TITLE: Configuring Score Weights in spaCy Training
DESCRIPTION: Example of configuring weighted scores in a spaCy config.cfg file to customize how different metrics contribute to the final evaluation score. This snippet shows how to weight dependency parsing, named entity recognition, and part-of-speech tagging metrics.

LANGUAGE: ini
CODE:
[training.score_weights]
dep_las = 0.4
dep_uas = null
ents_f = 0.4
tag_acc = 0.2
token_acc = 0.0
speed = 0.0

----------------------------------------

TITLE: Implementing a Stateful Acronym Detection Component
DESCRIPTION: This example demonstrates a complete stateful pipeline component that uses PhraseMatcher to detect acronyms in text, with customizable case sensitivity and document extension attributes.

LANGUAGE: python
CODE:
from spacy.language import Language
from spacy.tokens import Doc
from spacy.matcher import PhraseMatcher
import spacy

DICTIONARY = {"lol": "laughing out loud", "brb": "be right back"}
DICTIONARY.update({value: key for key, value in DICTIONARY.items()})

@Language.factory("acronyms", default_config={"case_sensitive": False})
def create_acronym_component(nlp: Language, name: str, case_sensitive: bool):
    return AcronymComponent(nlp, case_sensitive)

class AcronymComponent:
    def __init__(self, nlp: Language, case_sensitive: bool):
        # Create the matcher and match on Token.lower if case-insensitive
        matcher_attr = "TEXT" if case_sensitive else "LOWER"
        self.matcher = PhraseMatcher(nlp.vocab, attr=matcher_attr)
        self.matcher.add("ACRONYMS", [nlp.make_doc(term) for term in DICTIONARY])
        self.case_sensitive = case_sensitive
        # Register custom extension on the Doc
        if not Doc.has_extension("acronyms"):
            Doc.set_extension("acronyms", default=[])

    def __call__(self, doc: Doc) -> Doc:
        # Add the matched spans when doc is processed
        for _, start, end in self.matcher(doc):
            span = doc[start:end]
            acronym = DICTIONARY.get(span.text if self.case_sensitive else span.text.lower())
            doc._.acronyms.append((span, acronym))
        return doc

# Add the component to the pipeline and configure it
nlp = spacy.blank("en")
nlp.add_pipe("acronyms", config={"case_sensitive": False})

# Process a doc and see the results
doc = nlp("LOL, be right back")
print(doc._.acronyms)

----------------------------------------

TITLE: Loading spaCy Models from Package Name or Directory Path
DESCRIPTION: Shows different ways to load a spaCy language model in Python, either by its package name or from a directory path containing the model data.

LANGUAGE: python
CODE:
import spacy
nlp = spacy.load("en_core_web_sm")           # load package "en_core_web_sm"
nlp = spacy.load("/path/to/en_core_web_sm")  # load package from a directory

doc = nlp("This is a sentence.")

----------------------------------------

TITLE: Configuring Pipeline and Freezing Components in spaCy
DESCRIPTION: Demonstrates how to define the pipeline order and freeze specific components during training. Frozen components are not updated during training but are included in the final trained pipeline.

LANGUAGE: ini
CODE:
[nlp]
lang = "en"
pipeline = ["parser", "ner", "textcat", "custom"]

[training]
frozen_components = ["parser", "custom"]

----------------------------------------

TITLE: Registering a Configurable BERT Tokenizer for Training in spaCy
DESCRIPTION: Shows how to register a BERT tokenizer with configurable parameters (vocabulary file and lowercase setting) in spaCy's registry. This allows for flexible configuration through the training config file.

LANGUAGE: python
CODE:
@spacy.registry.tokenizers("bert_word_piece_tokenizer")
def create_bert_tokenizer(vocab_file: str, lowercase: bool):
    def create_tokenizer(nlp):
        return BertTokenizer(nlp.vocab, vocab_file, lowercase)

    return create_tokenizer

----------------------------------------

TITLE: Updating SpaCy Models with Training Examples in Python
DESCRIPTION: Example of updating SpaCy models by creating Example objects from raw text and entity annotations, then passing them to the update method with an optimizer for training.

LANGUAGE: python
CODE:
for raw_text, entity_offsets in train_data:
    doc = nlp.make_doc(raw_text)
    example = Example.from_dict(doc, {"entities": entity_offsets})
    nlp.update([example], sgd=optimizer)

----------------------------------------

TITLE: Implementing Token Matcher in spaCy
DESCRIPTION: Complete example showing how to initialize a Matcher, add a pattern, and apply it to a document to find matches. The code demonstrates capturing match IDs, start/end positions, and extracting matched spans.

LANGUAGE: python
CODE:
import spacy
from spacy.matcher import Matcher

nlp = spacy.load("en_core_web_sm")
matcher = Matcher(nlp.vocab)
# Add match ID "HelloWorld" with no callback and one pattern
pattern = [{"LOWER": "hello"}, {"IS_PUNCT": True}, {"LOWER": "world"}]
matcher.add("HelloWorld", [pattern])

doc = nlp("Hello, world! Hello world!")
matches = matcher(doc)
for match_id, start, end in matches:
    string_id = nlp.vocab.strings[match_id]  # Get string representation
    span = doc[start:end]  # The matched span
    print(match_id, string_id, start, end, span.text)

----------------------------------------

TITLE: Setting Custom Extensions and Adding Pipeline Components in spaCy
DESCRIPTION: Example showing how to add custom attributes to Doc and Token objects using extensions, and how to add a custom component to the processing pipeline.

LANGUAGE: python
CODE:
# Set custom attributes
Doc.set_extension("my_attr", default=False)
Token.set_extension("my_attr", getter=my_token_getter)
assert doc._.my_attr, token._.my_attr

# Add components to the pipeline
my_component = lambda doc: doc
nlp.add_pipe(my_component)

----------------------------------------

TITLE: Processing a Document with LLM NER Component in spaCy
DESCRIPTION: Shows how to apply an LLM named entity recognition component to a single document. This demonstrates the __call__ method of the LLMWrapper class that processes and modifies a document in place.

LANGUAGE: python
CODE:
doc = nlp("Ingrid visited Paris.")
llm_ner = nlp.add_pipe("llm_ner")
# This usually happens under the hood
processed = llm_ner(doc)

----------------------------------------

TITLE: Saving a spaCy Pipeline to Disk
DESCRIPTION: Demonstrates how to save a trained spaCy pipeline to disk using the Language.to_disk method. This creates a directory containing all the pipeline data, metadata, and configuration.

LANGUAGE: python
CODE:
nlp.to_disk("./en_example_pipeline")

----------------------------------------

TITLE: Adding a Pipeline Component in spaCy
DESCRIPTION: Demonstrates how to add a built-in pipeline component to a blank spaCy model. The example shows adding a lemmatizer component to an English language model after installing the required lookups package.

LANGUAGE: python
CODE:
# pip install -U %%SPACY_PKG_NAME[lookups]%%SPACY_PKG_FLAGS
nlp = spacy.blank("en")
nlp.add_pipe("lemmatizer")

----------------------------------------

TITLE: Setting Attribute Extension for Doc, Span, and Token Objects
DESCRIPTION: Example of implementing custom extension attributes for spaCy's Doc, Span, and Token objects. This creates a fruit detection system using custom getters.

LANGUAGE: python
CODE:
from spacy.tokens import Doc, Span, Token

fruits = ["apple", "pear", "banana", "orange", "strawberry"]
is_fruit_getter = lambda token: token.text in fruits
has_fruit_getter = lambda obj: any([t.text in fruits for t in obj])

Token.set_extension("is_fruit", getter=is_fruit_getter)
Doc.set_extension("has_fruit", getter=has_fruit_getter)
Span.set_extension("has_fruit", getter=has_fruit_getter)

----------------------------------------

TITLE: Loading spaCy Pipeline with Disabled or Excluded Components
DESCRIPTION: Examples of loading a spaCy pipeline while disabling or excluding specific components to improve loading and inference speed.

LANGUAGE: python
CODE:
# Load the pipeline without the entity recognizer
nlp = spacy.load("en_core_web_sm", exclude=["ner"])

# Load the tagger and parser but don't enable them
nlp = spacy.load("en_core_web_sm", disable=["tagger", "parser"])
# Explicitly enable the tagger later on
nlp.enable_pipe("tagger")

----------------------------------------

TITLE: Setting Entity Annotations in Cython with spaCy
DESCRIPTION: This snippet shows how to efficiently set entity annotations by writing directly to the underlying C struct using Cython. It sets entity type and IOB tags for a range of tokens in a document.

LANGUAGE: python
CODE:
# cython: infer_types=True
from spacy.typedefs cimport attr_t
from spacy.tokens.doc cimport Doc

cpdef set_entity(Doc doc, int start, int end, attr_t ent_type):
    for i in range(start, end):
        doc.c[i].ent_type = ent_type
    doc.c[start].ent_iob = 3
    for i in range(start+1, end):
        doc.c[i].ent_iob = 2

----------------------------------------

TITLE: Configuring a DependencyParser in spaCy
DESCRIPTION: Example of how to configure and add a dependency parser to a spaCy pipeline. The configuration includes settings for transition system moves, update strategy, token learning, minimum action frequency, and the underlying model architecture.

LANGUAGE: python
CODE:
from spacy.pipeline.dep_parser import DEFAULT_PARSER_MODEL
config = {
   "moves": None,
   "update_with_oracle_cut_size": 100,
   "learn_tokens": False,
   "min_action_freq": 30,
   "model": DEFAULT_PARSER_MODEL,
}
nlp.add_pipe("parser", config=config)

----------------------------------------

TITLE: Configuring NER with TransformerListener in spaCy
DESCRIPTION: Configuration for a named entity recognition component that uses a transformer model as its token-to-vector layer. The TransformerListener architecture connects to a transformer component in the pipeline.

LANGUAGE: ini
CODE:
[components.ner]
factory = "ner"

[nlp.pipeline.ner.model]
@architectures = "spacy.TransitionBasedParser.v1"
state_type = "ner"
extra_state_tokens = false
hidden_width = 128
maxout_pieces = 3
use_upper = false

[nlp.pipeline.ner.model.tok2vec]
@architectures = "spacy-transformers.TransformerListener.v1"
grad_factor = 1.0

[nlp.pipeline.ner.model.tok2vec.pooling]
@layers = "reduce_mean.v1"

----------------------------------------

TITLE: Defining Custom Attributes on spaCy Doc Objects
DESCRIPTION: Example of extending Doc functionality with custom attributes using the set_extension method. This allows adding domain-specific properties to Doc objects through getters, setters, or methods.

LANGUAGE: python
CODE:
from spacy.tokens import Doc
city_getter = lambda doc: any(city in doc.text for city in ("New York", "Paris", "Berlin"))
Doc.set_extension("has_city", getter=city_getter)
doc = nlp("I like New York")
assert doc._.has_city

----------------------------------------

TITLE: Configuring NER Pipeline Component in spaCy
DESCRIPTION: Example of configuring a named entity recognition (NER) pipeline component with custom settings. This demonstrates how to set up the model, update behavior, and custom error handling.

LANGUAGE: python
CODE:
from spacy.pipeline.ner import DEFAULT_NER_MODEL
config = {
   "moves": None,
   "update_with_oracle_cut_size": 100,
   "model": DEFAULT_NER_MODEL,
   "incorrect_spans_key": "incorrect_spans",
}
nlp.add_pipe("ner", config=config)

----------------------------------------

TITLE: Creating a Custom English Language Subclass in spaCy
DESCRIPTION: This snippet demonstrates how to extend spaCy's English language class by creating a custom subclass with modified stop words. It compares the behavior of the standard English processor with the custom version when processing the same text.

LANGUAGE: python
CODE:
from spacy.lang.en import English

class CustomEnglishDefaults(English.Defaults):
    stop_words = set(["custom", "stop"])

class CustomEnglish(English):
    lang = "custom_en"
    Defaults = CustomEnglishDefaults

nlp1 = English()
nlp2 = CustomEnglish()

print(nlp1.lang, [token.is_stop for token in nlp1("custom stop")])
print(nlp2.lang, [token.is_stop for token in nlp2("custom stop")])

----------------------------------------

TITLE: Implementing memory zones in a FastAPI web service
DESCRIPTION: Shows how to integrate spaCy's memory_zone with a FastAPI web service to maintain consistent memory usage. The nlp object is loaded once as a state variable, and each request is processed within a separate memory zone to prevent memory accumulation over time.

LANGUAGE: python
CODE:
from fastapi import FastAPI, APIRouter, Depends, Request
import spacy
from spacy.language import Language

router = APIRouter()


def make_app():
    app = FastAPI()
    app.state.NLP = spacy.load("en_core_web_sm")
    app.include_router(router)
    return app


def get_nlp(request: Request) -> Language:
    return request.app.state.NLP


@router.post("/parse")
def parse_texts(
    *, text_batch: list[str], nlp: Language = Depends(get_nlp)
) -> list[dict]:
    with nlp.memory_zone():
        # Put the spaCy call within a separate function, so we can't
        # leak the Doc objects outside the scope of the memory zone.
        output = _process_text(nlp, text_batch)
    return output


def _process_text(nlp: Language, texts: list[str]) -> list[dict]:
    # Call spaCy, and transform the output into our own data
    # structures. This function is called from inside a memory
    # zone, so must not return the spaCy objects.
    docs = list(nlp.pipe(texts))
    return [
        {
            "tokens": [{"text": t.text} for t in doc],
            "entities": [
                {"start": e.start, "end": e.end, "label": e.label_} for e in doc.ents
            ],
        }
        for doc in docs
    ]


app = make_app()

----------------------------------------

TITLE: Batch Processing Documents with Language.pipe in spaCy
DESCRIPTION: Demonstrates how to efficiently process multiple texts as a stream using the Language.pipe method. This is more efficient than processing texts individually and yields Doc objects in order.

LANGUAGE: python
CODE:
texts = ["One document.", "...", "Lots of documents"]
for doc in nlp.pipe(texts, batch_size=50):
    assert doc.has_annotation("DEP")

----------------------------------------

TITLE: Creating a Custom spaCy Pipeline Component for Employment Extraction
DESCRIPTION: This snippet creates a reusable spaCy pipeline component that extracts employment relationships between people and organizations. It uses entity merging and dependency parsing to identify these relationships.

LANGUAGE: python
CODE:
import spacy
from spacy.language import Language
from spacy import displacy

nlp = spacy.load("en_core_web_sm")

@Language.component("extract_person_orgs")
def extract_person_orgs(doc):
    person_entities = [ent for ent in doc.ents if ent.label_ == "PERSON"]
    for ent in person_entities:
        head = ent.root.head
        if head.lemma_ == "work":
            preps = [token for token in head.children if token.dep_ == "prep"]
            for prep in preps:
                orgs = [token for token in prep.children if token.ent_type_ == "ORG"]
                print({'person': ent, 'orgs': orgs, 'past': head.tag_ == "VBD"})
    return doc

# To make the entities easier to work with, we'll merge them into single tokens
nlp.add_pipe("merge_entities")
nlp.add_pipe("extract_person_orgs")

doc = nlp("Alex Smith worked at Acme Corp Inc.")
# If you're not in a Jupyter / IPython environment, use displacy.serve
displacy.render(doc, options={"fine_grained": True})

----------------------------------------

TITLE: Configuring Shared Embedding Layers in spaCy Pipeline
DESCRIPTION: Configuration for a spaCy pipeline using a shared tok2vec component. The tok2vec component creates embeddings that are shared with other components like the entity recognizer via Tok2VecListener architecture.

LANGUAGE: ini
CODE:
[components.tok2vec]
factory = "tok2vec"

[components.tok2vec.model]
@architectures = "spacy.Tok2Vec.v2"

[components.tok2vec.model.embed]
@architectures = "spacy.MultiHashEmbed.v2"

[components.tok2vec.model.encode]
@architectures = "spacy.MaxoutWindowEncoder.v2"

[components.ner]
factory = "ner"

[components.ner.model]
@architectures = "spacy.TransitionBasedParser.v1"

[components.ner.model.tok2vec]
@architectures = "spacy.Tok2VecListener.v1"

----------------------------------------

TITLE: Processing Text with Metadata Using as_tuples Parameter
DESCRIPTION: Example showing how to pass and retain metadata along with texts during processing using nlp.pipe with the as_tuples parameter. This allows associating additional context with each document.

LANGUAGE: python
CODE:
import spacy
from spacy.tokens import Doc

if not Doc.has_extension("text_id"):
    Doc.set_extension("text_id", default=None)

text_tuples = [
    ("This is the first text.", {"text_id": "text1"}),
    ("This is the second text.", {"text_id": "text2"})
]

nlp = spacy.load("en_core_web_sm")
doc_tuples = nlp.pipe(text_tuples, as_tuples=True)

docs = []
for doc, context in doc_tuples:
    doc._.text_id = context["text_id"]
    docs.append(doc)

for doc in docs:
    print(f"{doc._.text_id}: {doc.text}")

----------------------------------------

TITLE: Enhanced Employment Extraction with Complex Tense Detection
DESCRIPTION: This improved version of the extraction component handles more complex verb tenses like past progressive ('was working'). It checks for auxiliary verbs to accurately determine if employment is in the past.

LANGUAGE: python
CODE:
@Language.component("extract_person_orgs")
def extract_person_orgs(doc):
    person_entities = [ent for ent in doc.ents if ent.label_ == "PERSON"]
    for ent in person_entities:
        head = ent.root.head
        if head.lemma_ == "work":
            preps = [token for token in head.children if token.dep_ == "prep"]
            for prep in preps:
                orgs = [t for t in prep.children if t.ent_type_ == "ORG"]
                aux = [token for token in head.children if token.dep_ == "aux"]
                past_aux = any(t.tag_ == "VBD" for t in aux)
                past = head.tag_ == "VBD" or head.tag_ == "VBG" and past_aux
                print({'person': ent, 'orgs': orgs, 'past': past})
    return doc

----------------------------------------

TITLE: Configuring and Adding an EntityLinker to spaCy Pipeline
DESCRIPTION: This snippet demonstrates how to configure and add an entity linker component to a spaCy NLP pipeline. It shows the default configuration options that can be customized, including label handling, context settings, model architecture, and candidate generation.

LANGUAGE: python
CODE:
from spacy.pipeline.entity_linker import DEFAULT_NEL_MODEL
config = {
   "labels_discard": [],
   "n_sents": 0,
   "incl_prior": True,
   "incl_context": True,
   "model": DEFAULT_NEL_MODEL,
   "entity_vector_length": 64,
   "get_candidates": {'@misc': 'spacy.CandidateGenerator.v1'},
   "threshold": None,
}
nlp.add_pipe("entity_linker", config=config)

----------------------------------------

TITLE: Example of Packaging and Installing a spaCy Pipeline
DESCRIPTION: Example showing how to package a spaCy pipeline from an input directory to an output directory, and then install the generated package using pip.

LANGUAGE: bash
CODE:
$ python -m spacy package /input /output
$ cd /output/en_pipeline-0.0.0
$ pip install dist/en_pipeline-0.0.0.tar.gz

----------------------------------------

TITLE: Initializing a Transformer Component in spaCy
DESCRIPTION: Demonstrates different ways to initialize a Transformer component in spaCy, including via add_pipe with default or custom configuration, or by direct class instantiation. Shows how to configure model parameters like precision and tokenizer settings.

LANGUAGE: python
CODE:
# Construction via add_pipe with default model
trf = nlp.add_pipe("transformer")

# Construction via add_pipe with custom config
config = {
    "model": {
        "@architectures": "spacy-transformers.TransformerModel.v3",
        "name": "bert-base-uncased",
        "tokenizer_config": {"use_fast": True},
        "transformer_config": {"output_attentions": True},
        "mixed_precision": True,
        "grad_scaler_config": {"init_scale": 32768}
    }
}
trf = nlp.add_pipe("transformer", config=config)

# Construction from class
from spacy_transformers import Transformer
trf = Transformer(nlp.vocab, model)

----------------------------------------

TITLE: Customizing Infix Tokenization Rules to Preserve Hyphenated Words
DESCRIPTION: Demonstrates how to modify the infix tokenization rules to keep hyphenated words together. The example disables the default behavior of splitting on hyphens between letters, allowing terms like 'mother-in-law' to remain as a single token.

LANGUAGE: python
CODE:
import spacy
from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER
from spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS
from spacy.util import compile_infix_regex

# Default tokenizer
nlp = spacy.load("en_core_web_sm")
doc = nlp("mother-in-law")
print([t.text for t in doc]) # ['mother', '-', 'in', '-', 'law']

# Modify tokenizer infix patterns
infixes = (
    LIST_ELLIPSES
    + LIST_ICONS
    + [
        r"(?<=[0-9])[+\\-\\*^](?=[0-9-])",
        r"(?<=[{al}{q}])\\.(?=[{au}{q}])".format(
            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES
        ),
        r"(?<=[{a}]),(?=[{a}])".format(a=ALPHA),
        # ✅ Commented out regex that splits on hyphens between letters:
        # r"(?<=[{a}])(?:{h})(?=[{a}])".format(a=ALPHA, h=HYPHENS),
        r"(?<=[{a}0-9])[:<>=/](?=[{a}])".format(a=ALPHA),
    ]
)

infix_re = compile_infix_regex(infixes)
nlp.tokenizer.infix_finditer = infix_re.finditer
doc = nlp("mother-in-law")
print([t.text for t in doc]) # ['mother-in-law']

----------------------------------------

TITLE: Using transformer models at runtime in spaCy
DESCRIPTION: Python code example showing how to load a transformer-based spaCy model, configure GPU usage, and access transformer embeddings from the processed documents.

LANGUAGE: python
CODE:
import spacy
from thinc.api import set_gpu_allocator, require_gpu

# Use the GPU, with memory allocations directed via PyTorch.
# This prevents out-of-memory errors that would otherwise occur from competing
# memory pools.
set_gpu_allocator("pytorch")
require_gpu(0)

nlp = spacy.load("en_core_web_trf")
for doc in nlp.pipe(["some text", "some other text"]):
    tokvecs = doc._.trf_data.tensors[-1]

----------------------------------------

TITLE: Wrapping a PyTorch Model with Thinc
DESCRIPTION: A basic example of wrapping a PyTorch model with Thinc's PyTorchWrapper to use it with spaCy components. This allows PyTorch models to be integrated into the spaCy pipeline.

LANGUAGE: python
CODE:
from thinc.api import PyTorchWrapper

wrapped_pt_model = PyTorchWrapper(torch_model)

----------------------------------------

TITLE: Implementing Custom Sentence Boundary Detection in spaCy
DESCRIPTION: Demonstrates how to create a custom pipeline component that sets sentence boundaries based on custom rules. This example splits sentences on ellipsis tokens while still leveraging the parser for additional segmentation.

LANGUAGE: python
CODE:
from spacy.language import Language
import spacy

text = "this is a sentence...hello...and another sentence."

nlp = spacy.load("en_core_web_sm")
doc = nlp(text)
print("Before:", [sent.text for sent in doc.sents])

@Language.component("set_custom_boundaries")
def set_custom_boundaries(doc):
    for token in doc[:-1]:
        if token.text == "...":
            doc[token.i + 1].is_sent_start = True
    return doc

nlp.add_pipe("set_custom_boundaries", before="parser")
doc = nlp(text)
print("After:", [sent.text for sent in doc.sents])

----------------------------------------

TITLE: Configuring NER v3 Task with Detailed Descriptions
DESCRIPTION: Advanced configuration example for the NER v3 task with entity descriptions. This sets up the task with labels, a general description, and specific label definitions to guide the LLM.

LANGUAGE: ini
CODE:
[components.llm.task]
@llm_tasks = "spacy.NER.v3"
labels = ["DISH", "INGREDIENT", "EQUIPMENT"]
description = Entities are the names food dishes,
    ingredients, and any kind of cooking equipment.
    Adjectives, verbs, adverbs are not entities.
    Pronouns are not entities.

[components.llm.task.label_definitions]
DISH = "Known food dishes, e.g. Lobster Ravioli, garlic bread"
INGREDIENT = "Individual parts of a food dish, including herbs and spices."
EQUIPMENT = "Any kind of cooking equipment. e.g. oven, cooking pot, grill"

----------------------------------------

TITLE: Defining spaCy Pipeline Configuration in config.cfg
DESCRIPTION: Example of a configuration file excerpt that defines a spaCy pipeline with language settings and components. The config specifies an English language model with tok2vec and parser components.

LANGUAGE: ini
CODE:
[nlp]
lang = "en"
pipeline = ["tok2vec", "parser"]

[components]

[components.tok2vec]
factory = "tok2vec"
# Settings for the tok2vec component

[components.parser]
factory = "parser"
# Settings for the parser component

----------------------------------------

TITLE: Accessing Sentence Boundaries with spaCy's Doc.sents
DESCRIPTION: Shows how to iterate through the sentences in a spaCy Doc object using the Doc.sents property. The example also demonstrates how to check if sentence annotations are available using Doc.has_annotation.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("This is a sentence. This is another sentence.")
assert doc.has_annotation("SENT_START")
for sent in doc.sents:
    print(sent.text)

----------------------------------------

TITLE: Defining Gold-Standard Named Entities with BILUO Scheme in spaCy (Python)
DESCRIPTION: This snippet shows how to create an Example with gold-standard named entity annotations using the BILUO tagging scheme. The annotations identify organization, technology, and date entities in the text.

LANGUAGE: python
CODE:
doc = Doc(nlp.vocab, words=["Facebook", "released", "React", "in", "2014"])
example = Example.from_dict(doc, {"entities": ["U-ORG", "O", "U-TECHNOLOGY", "O", "U-DATE"]})

----------------------------------------

TITLE: Default Pretraining Configuration for spaCy
DESCRIPTION: The default configuration used for pretraining in spaCy. This includes settings for the objective, corpus, batcher, optimizer, and various training hyperparameters. This configuration is applied when running the pretrain command.

LANGUAGE: ini
CODE:
%%GITHUB_SPACY/spacy/default_config_pretraining.cfg

----------------------------------------

TITLE: Iterating Through Tokens in a spaCy Doc
DESCRIPTION: Example of iterating through all tokens in a Doc object. This is the primary method to access Token objects and their annotations in Python.

LANGUAGE: python
CODE:
doc = nlp("Give it back")
assert [t.text for t in doc] == ["Give", "it", "back"]

----------------------------------------

TITLE: Creating Training Data from Doc Objects in Python
DESCRIPTION: Example showing how to manually create training data by constructing Doc objects with custom entities and saving them to a binary .spacy file. This demonstrates adding tokens, spaces between tokens, and entity annotations to create a document that can be used for training.

LANGUAGE: python
CODE:
import spacy
from spacy.tokens import Doc, DocBin

nlp = spacy.blank("en")
docbin = DocBin()
words = ["Apple", "is", "looking", "at", "buying", "U.K.", "startup", "."]
spaces = [True, True, True, True, True, True, True, False]
ents = ["B-ORG", "O", "O", "O", "O", "B-GPE", "O", "O"]
doc = Doc(nlp.vocab, words=words, spaces=spaces, ents=ents)
docbin.add(doc)
docbin.to_disk("./train.spacy")

----------------------------------------

TITLE: Training a spaCy Model with GPU Acceleration
DESCRIPTION: Command to train a spaCy model using a configuration file while enabling GPU acceleration by specifying the GPU ID.

LANGUAGE: bash
CODE:
$ python -m spacy train config.cfg --gpu-id 0

----------------------------------------

TITLE: Merging Entities and Processing Financial Text in Python
DESCRIPTION: This snippet demonstrates how to merge entities and noun chunks in spaCy, and then process financial texts to extract money entities with their relationships. It uses dependency parsing to identify subjects and objects related to financial amounts.

LANGUAGE: python
CODE:
# Merge noun phrases and entities for easier analysis
nlp.add_pipe("merge_entities")
nlp.add_pipe("merge_noun_chunks")

TEXTS = [
    "Net income was $9.4 million compared to the prior year of $2.7 million.",
    "Revenue exceeded twelve billion dollars, with a loss of $1b.",
]
for doc in nlp.pipe(TEXTS):
    for token in doc:
        if token.ent_type_ == "MONEY":
            # We have an attribute and direct object, so check for subject
            if token.dep_ in ("attr", "dobj"):
                subj = [w for w in token.head.lefts if w.dep_ == "nsubj"]
                if subj:
                    print(subj[0], "-->", token)
            # We have a prepositional object with a preposition
            elif token.dep_ == "pobj" and token.head.dep_ == "prep":
                print(token.head.head, "-->", token)

----------------------------------------

TITLE: Efficient Batch Text Processing with nlp.pipe
DESCRIPTION: Shows how to process multiple texts efficiently using spaCy's nlp.pipe method, which handles batching internally. This approach is more efficient than processing texts one by one.

LANGUAGE: python
CODE:
texts = ["This is a text", "These are lots of texts", "..."]
- docs = [nlp(text) for text in texts]
+ docs = list(nlp.pipe(texts))

----------------------------------------

TITLE: Using SpanRuler for Rule-based Span Matching
DESCRIPTION: Example of creating and using a SpanRuler component to add spans to doc.spans based on pattern matches. The ruler can identify both exact string matches and token pattern matches.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.blank("en")
ruler = nlp.add_pipe("span_ruler")
patterns = [{"label": "ORG", "pattern": "Apple"},
            {"label": "GPE", "pattern": [{"LOWER": "san"}, {"LOWER": "francisco"}]}]
ruler.add_patterns(patterns)

doc = nlp("Apple is opening its first big office in San Francisco.")
print([(span.text, span.label_) for span in doc.spans["ruler"]])

----------------------------------------

TITLE: Replacing Assertions with Explicit Error Handling in spaCy
DESCRIPTION: Shows how to convert naked assertions to proper error handling with descriptive error messages. This provides users with more helpful information when errors occur.

LANGUAGE: python
CODE:
- assert score >= 0.0
+ if score < 0.0:
+     raise ValueError(Errors.789.format(score=score))

----------------------------------------

TITLE: Using Token Edges to Create and Merge Spans in spaCy
DESCRIPTION: This snippet shows how to use a token's left_edge and right_edge attributes to create a span for a syntactic phrase. It creates a span from the edges of a token's subtree and merges it into a single token, demonstrating spaCy's document modification capabilities.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Credit and mortgage account holders must submit their requests")
span = doc[doc[4].left_edge.i : doc[4].right_edge.i+1]
with doc.retokenize() as retokenizer:
    retokenizer.merge(span)
for token in doc:
    print(token.text, token.pos_, token.dep_, token.head.text)

----------------------------------------

TITLE: Importing spaCy Models as Python Modules
DESCRIPTION: Demonstrates how to import a trained spaCy pipeline directly as a Python module and load it, which is recommended for larger codebases to enable better integration with build processes and testing frameworks.

LANGUAGE: python
CODE:
import en_core_web_sm

nlp = en_core_web_sm.load()
doc = nlp("This is a sentence.")

----------------------------------------

TITLE: Multiprocessing with spaCy's nlp.pipe
DESCRIPTION: Examples of using multiprocessing to parallelize text processing with spaCy. Shows how to specify the number of processes and customize batch size for improved performance.

LANGUAGE: python
CODE:
# Multiprocessing with 4 processes
docs = nlp.pipe(texts, n_process=4)

# With as many processes as CPUs (use with caution!)
docs = nlp.pipe(texts, n_process=-1)

----------------------------------------

TITLE: Exploring Subtrees and Ancestors in spaCy's Dependency Tree
DESCRIPTION: This example demonstrates how to navigate a token's subtree and ancestors in the dependency parse. It finds the root of the sentence, its subject, and then explores all descendants of that subject, verifying their ancestor relationships.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Credit and mortgage account holders must submit their requests")

root = [token for token in doc if token.head == token][0]
subject = list(root.lefts)[0]
for descendant in subject.subtree:
    assert subject is descendant or subject.is_ancestor(descendant)
    print(descendant.text, descendant.dep_, descendant.n_lefts,
            descendant.n_rights,
            [ancestor.text for ancestor in descendant.ancestors])

----------------------------------------

TITLE: Visualizing Dependency Parsing with spaCy's displaCy
DESCRIPTION: This code demonstrates how to use spaCy's displaCy visualization module to render dependency parsing results. It loads the English model, processes a sentence about autonomous cars, and visualizes the dependency tree.

LANGUAGE: python
CODE:
import spacy
from spacy import displacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Autonomous cars shift insurance liability toward manufacturers")
# Since this is an interactive Jupyter environment, we can use displacy.render here
displacy.render(doc, style='dep')

----------------------------------------

TITLE: Installing spaCy LLM with HuggingFace Dependencies
DESCRIPTION: Command to install spaCy LLM with the necessary dependencies for using HuggingFace transformers models, including sentence piece tokenization support required by many models.

LANGUAGE: shell
CODE:
python -m pip install "spacy-llm[transformers]" "transformers[sentencepiece]"

----------------------------------------

TITLE: Integrating Entity Ruler with spaCy's Named Entity Recognizer
DESCRIPTION: This snippet shows how to integrate the EntityRuler with spaCy's statistical named entity recognizer (NER). It loads a pre-trained model, adds the EntityRuler component to the pipeline, and defines patterns that can enhance the model's predictions.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("en_core_web_sm")
ruler = nlp.add_pipe("entity_ruler")
patterns = [{"label": "ORG", "pattern": "MyCorp Inc."}]
ruler.add_patterns(patterns)

doc = nlp("MyCorp Inc. is a company in the U.S.")
print([(ent.text, ent.label_) for ent in doc.ents])

----------------------------------------

TITLE: Filtering Overlapping Spans in spaCy
DESCRIPTION: Shows how to filter a sequence of Span objects to remove duplicates and overlaps. When spans overlap, the function prefers the longest span. Useful for named entity recognition where one token can only be part of one entity.

LANGUAGE: python
CODE:
doc = nlp("This is a sentence.")
spans = [doc[0:2], doc[0:2], doc[0:4]]
filtered = filter_spans(spans)

----------------------------------------

TITLE: Training and Using a Text Categorization Model in spaCy
DESCRIPTION: Demonstrates how to create a text categorization pipeline component, add it to the NLP pipeline, train it with data, and use it to classify text documents.

LANGUAGE: python
CODE:
textcat = nlp.create_pipe("textcat")
nlp.add_pipe(textcat, last=True)
nlp.begin_training()
for itn in range(100):
   for doc, gold in train_data:
       nlp.update([doc], [gold])
doc = nlp("This is a text.")
print(doc.cats)

----------------------------------------

TITLE: Downloading spaCy Language Models with Command Line
DESCRIPTION: Commands for downloading pre-trained language models for English, German, and a multi-language entity recognition model using spaCy's command-line interface.

LANGUAGE: bash
CODE:
python -m spacy download en_core_web_sm
python -m spacy download de_core_news_sm
python -m spacy download xx_ent_wiki_sm

----------------------------------------

TITLE: Merging Tokens with Doc.retokenize in spaCy
DESCRIPTION: Shows how to merge multiple tokens into a single token using spaCy's retokenize context manager. The example merges 'New York' into a single token and demonstrates how to customize the attributes of the merged token.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("I live in New York")
print("Before:", [token.text for token in doc])

with doc.retokenize() as retokenizer:
    retokenizer.merge(doc[3:5], attrs={"LEMMA": "new york"})
print("After:", [token.text for token in doc])

----------------------------------------

TITLE: Registering Custom Pipeline Components in spaCy
DESCRIPTION: Shows how to register a custom pipeline component using the Language.component decorator or function. This allows adding simple stateless functions that process a Doc object and can be referenced by name in the pipeline.

LANGUAGE: python
CODE:
from spacy.language import Language

# Usage as a decorator
@Language.component("my_component")
def my_component(doc):
   # Do something to the doc
   return doc

# Usage as a function
Language.component("my_component2", func=my_component)

----------------------------------------

TITLE: Inspecting the Pipeline Structure
DESCRIPTION: Shows how to view the current processing pipeline components. The example prints both the full pipeline with component objects and just the human-readable component names.

LANGUAGE: python
CODE:
print(nlp.pipeline)
# [('tok2vec', <spacy.pipeline.Tok2Vec>), ('tagger', <spacy.pipeline.Tagger>), ('parser', <spacy.pipeline.DependencyParser>), ('ner', <spacy.pipeline.EntityRecognizer>), ('attribute_ruler', <spacy.pipeline.AttributeRuler>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer>)]
print(nlp.pipe_names)
# ['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']

----------------------------------------

TITLE: Implementing a Custom BERT Word Piece Tokenizer in spaCy
DESCRIPTION: Creates a custom tokenizer class that wraps the BERT WordPiece tokenizer from the 'tokenizers' library. The class maintains compatibility with spaCy by returning a Doc object with properly aligned tokens and spaces.

LANGUAGE: python
CODE:
from tokenizers import BertWordPieceTokenizer
from spacy.tokens import Doc
import spacy

class BertTokenizer:
    def __init__(self, vocab, vocab_file, lowercase=True):
        self.vocab = vocab
        self._tokenizer = BertWordPieceTokenizer(vocab_file, lowercase=lowercase)

    def __call__(self, text):
        tokens = self._tokenizer.encode(text)
        words = []
        spaces = []
        for i, (text, (start, end)) in enumerate(zip(tokens.tokens, tokens.offsets)):
            words.append(text)
            if i < len(tokens.tokens) - 1:
                # If next start != current end we assume a space in between
                next_start, next_end = tokens.offsets[i + 1]
                spaces.append(next_start > end)
            else:
                spaces.append(True)
        return Doc(self.vocab, words=words, spaces=spaces)

nlp = spacy.blank("en")
nlp.tokenizer = BertTokenizer(nlp.vocab, "bert-base-uncased-vocab.txt")
doc = nlp("Justin Drew Bieber is a Canadian singer, songwriter, and actor.")
print(doc.text, [token.text for token in doc])
# [CLS]justin drew bi##eber is a canadian singer, songwriter, and actor.[SEP]
# ['[CLS]', 'justin', 'drew', 'bi', '##eber', 'is', 'a', 'canadian', 'singer',
#  ',', 'songwriter', ',', 'and', 'actor', '.', '[SEP]']

----------------------------------------

TITLE: Using to_disk and from_disk Methods in spaCy
DESCRIPTION: These code snippets show how to save a spaCy NLP object to disk and load it back. The to_disk() method writes the object to a specified path, while from_disk() loads the object from that path.

LANGUAGE: python
CODE:
nlp.to_disk("/path")

LANGUAGE: python
CODE:
nlp.from_disk("/path")

----------------------------------------

TITLE: Optimized Named Entity Recognition with Component Disabling
DESCRIPTION: Example of processing texts as a stream while disabling unnecessary pipeline components to improve efficiency. This code demonstrates how to extract named entities from multiple texts while only running the required components.

LANGUAGE: python
CODE:
import spacy

texts = [
    "Net income was $9.4 million compared to the prior year of $2.7 million.",
    "Revenue exceeded twelve billion dollars, with a loss of $1b.",
]

nlp = spacy.load("en_core_web_sm")
for doc in nlp.pipe(texts, disable=["tok2vec", "tagger", "parser", "attribute_ruler", "lemmatizer"]):
    # Do something with the doc here
    print([(ent.text, ent.label_) for ent in doc.ents])

----------------------------------------

TITLE: Adding Special Case Rules to SpaCy Tokenizer
DESCRIPTION: Example demonstrating how to add a custom tokenization rule for handling special cases, such as contractions with specific orthographic and normalized forms.

LANGUAGE: python
CODE:
from spacy.attrs import ORTH, NORM
case = [{ORTH: "do"}, {ORTH: "n't", NORM: "not"}]
tokenizer.add_special_case("don't", case)

----------------------------------------

TITLE: Using spaCy-LLM Pipeline for Text Classification
DESCRIPTION: Demonstrates how to load a spaCy-LLM pipeline from a configuration file and process a document for text classification. This example shows the workflow for using an assembled LLM pipeline.

LANGUAGE: python
CODE:
from spacy_llm.util import assemble


nlp = assemble("config.cfg")
doc = nlp("You look gorgeous!")
print(doc.cats)

----------------------------------------

TITLE: Serializing a Doc to JSON Format in spaCy
DESCRIPTION: Demonstrates converting a Doc object to JSON format using the to_json() method, which creates a structured representation that includes the document text and can be extended with custom attributes.

LANGUAGE: python
CODE:
doc = nlp("All we have to decide is what to do with the time that is given us.")
assert doc.to_json()["text"] == doc.text

----------------------------------------

TITLE: Setting Named Entities in a spaCy Doc
DESCRIPTION: Shows how to set named entities in a Doc object using set_ents(). This method allows for manual entity annotation by providing a list of Span objects with labels.

LANGUAGE: python
CODE:
from spacy.tokens import Span
doc = nlp("Mr. Best flew to New York on Saturday morning.")
doc.set_ents([Span(doc, 0, 2, "PERSON")])
ents = list(doc.ents)
assert ents[0].label_ == "PERSON"
assert ents[0].text == "Mr. Best"

----------------------------------------

TITLE: Extracting Person-Organization Relationships with Dependency Parsing
DESCRIPTION: This code extracts relationships between people and organizations they worked for by traversing the dependency parse tree. It identifies if employment is current or past by examining verb tense.

LANGUAGE: python
CODE:
person_entities = [ent for ent in doc.ents if ent.label_ == "PERSON"]
for ent in person_entities:
    # Because the entity is a span, we need to use its root token. The head
    # is the syntactic governor of the person, e.g. the verb
    head = ent.root.head
    if head.lemma_ == "work":
        # Check if the children contain a preposition
        preps = [token for token in head.children if token.dep_ == "prep"]
        for prep in preps:
            # Check if tokens part of ORG entities are in the preposition's
            # children, e.g. at -> Acme Corp Inc.
            orgs = [token for token in prep.children if token.ent_type_ == "ORG"]
            # If the verb is in past tense, the company was a previous company
            print({"person": ent, "orgs": orgs, "past": head.tag_ == "VBD"})

----------------------------------------

TITLE: Extracting Noun Chunks from a Document in spaCy (Python)
DESCRIPTION: Shows how to iterate over base noun phrases in a document using the noun_chunks property, which returns Span objects of non-nested noun phrases.

LANGUAGE: python
CODE:
doc = nlp("A phrase with another phrase occurs.")
chunks = list(doc.noun_chunks)
assert len(chunks) == 2
assert chunks[0].text == "A phrase"
assert chunks[1].text == "another phrase"

----------------------------------------

TITLE: Using AttributeRuler to Override Token Tags in spaCy
DESCRIPTION: This example demonstrates how to use the AttributeRuler component to override the tags of specific tokens. It creates an exception rule for the phrase "The Who" to be tagged as proper noun (NNP/PROPN) instead of the default tags.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("en_core_web_sm")
text = "I saw The Who perform. Who did you see?"
doc1 = nlp(text)
print(doc1[2].tag_, doc1[2].pos_)  # DT DET
print(doc1[3].tag_, doc1[3].pos_)  # WP PRON

# Add attribute ruler with exception for "The Who" as NNP/PROPN NNP/PROPN
ruler = nlp.get_pipe("attribute_ruler")
# Pattern to match "The Who"
patterns = [[{"LOWER": "the"}, {"TEXT": "Who"}]]
# The attributes to assign to the matched token
attrs = {"TAG": "NNP", "POS": "PROPN"}
# Add rules to the attribute ruler
ruler.add(patterns=patterns, attrs=attrs, index=0)  # "The" in "The Who"
ruler.add(patterns=patterns, attrs=attrs, index=1)  # "Who" in "The Who"

doc2 = nlp(text)
print(doc2[2].tag_, doc2[2].pos_)  # NNP PROPN
print(doc2[3].tag_, doc2[3].pos_)  # NNP PROPN
# The second "Who" remains unmodified
print(doc2[5].tag_, doc2[5].pos_)  # WP PRON

----------------------------------------

TITLE: Computing Token Similarity in spaCy
DESCRIPTION: Example showing how to compute semantic similarity between tokens. The similarity method returns a scalar score indicating how similar two tokens are based on their vectors.

LANGUAGE: python
CODE:
apples, _, oranges = nlp("apples and oranges")
apples_oranges = apples.similarity(oranges)
oranges_apples = oranges.similarity(apples)
assert apples_oranges == oranges_apples

----------------------------------------

TITLE: Using spaCy Train Function in Python Script
DESCRIPTION: Example showing how to call spaCy's train function directly from a Python script instead of using the CLI command. It demonstrates passing a config file path and optional configuration overrides.

LANGUAGE: python
CODE:
from spacy.cli.train import train

train("./config.cfg", overrides={"paths.train": "./train.spacy", "paths.dev": "./dev.spacy"})

----------------------------------------

TITLE: Creating Reference Docs with Gold-Standard TAG Annotations in spaCy (Python)
DESCRIPTION: This snippet demonstrates how to create a reference Doc with gold-standard part-of-speech tags by converting tag strings to IDs and using Doc.from_array. This method enables comparison between predicted and reference documents for model training.

LANGUAGE: python
CODE:
# create the reference Doc with gold-standard TAG annotations
tags = ["NOUN", "VERB", "NOUN"]
tag_ids = [vocab.strings.add(tag) for tag in tags]
reference = Doc(vocab, words=words).from_array("TAG", numpy.array(tag_ids, dtype="uint64"))
example = Example(predicted, reference)

----------------------------------------

TITLE: Dependency Visualizer Data Structure Example in JSON
DESCRIPTION: Example JSON structure for dependency visualization with displaCy. This structure includes words with their text and part-of-speech tags, plus arcs representing dependency relations between words.

LANGUAGE: json
CODE:
{
  "words": [
    { "text": "This", "tag": "DT" },
    { "text": "is", "tag": "VBZ" },
    { "text": "a", "tag": "DT" },
    { "text": "sentence", "tag": "NN" }
  ],
  "arcs": [
    { "start": 0, "end": 1, "label": "nsubj", "dir": "left" },
    { "start": 2, "end": 3, "label": "det", "dir": "left" },
    { "start": 1, "end": 3, "label": "attr", "dir": "right" }
  ]
}

----------------------------------------

TITLE: Creating a Basic Component Factory in spaCy
DESCRIPTION: This example shows how to create a simple component factory using the @Language.factory decorator with the minimal required parameters of nlp and name.

LANGUAGE: python
CODE:
from spacy.language import Language

@Language.factory("my_component")
def my_component(nlp, name):
    return MyComponent()

----------------------------------------

TITLE: Creating Blank spaCy Pipelines with spacy.blank()
DESCRIPTION: Examples of creating blank language pipelines for different languages using the spacy.blank() function, which initializes empty pipelines without any components or trained models.

LANGUAGE: python
CODE:
nlp_en = spacy.blank("en")   # equivalent to English()
nlp_de = spacy.blank("de")   # equivalent to German()

----------------------------------------

TITLE: Adding Object Modifier to Dependency Matcher Pattern in spaCy
DESCRIPTION: This snippet demonstrates how to add a modifier to the object token in a dependency matcher pattern. It specifies that the 'founded_object' token should have a dependent with either an 'amod' or 'compound' dependency relation.

LANGUAGE: python
CODE:
pattern = [
    # ...
    {
        "LEFT_ID": "founded_object",
        "REL_OP": ">",
        "RIGHT_ID": "founded_object_modifier",
        "RIGHT_ATTRS": {"DEP": {"IN": ["amod", "compound"]}},
    }
]

----------------------------------------

TITLE: Adding Special Case Tokenization Rules in spaCy
DESCRIPTION: Example of how to add custom tokenization rules for special cases in spaCy. The code shows how to split the word "gimme" into two tokens ("gim" and "me") by adding a special case rule to the tokenizer.

LANGUAGE: python
CODE:
import spacy
from spacy.symbols import ORTH

nlp = spacy.load("en_core_web_sm")
doc = nlp("gimme that")  # phrase to tokenize
print([w.text for w in doc])  # ['gimme', 'that']

# Add special case rule
special_case = [{ORTH: "gim"}, {ORTH: "me"}]
nlp.tokenizer.add_special_case("gimme", special_case)

# Check new tokenization
print([w.text for w in nlp("gimme that")])  # ['gim', 'me', 'that']

----------------------------------------

TITLE: Adding Pipeline Components with Language.add_pipe in spaCy Python
DESCRIPTION: Adds a component to the spaCy processing pipeline using a registered component factory name. Components can be positioned using 'before', 'after', 'first', or 'last' parameters.

LANGUAGE: python
CODE:
@Language.component("component")
def component_func(doc):
    # modify Doc and return it
    return doc

nlp.add_pipe("component", before="ner")
component = nlp.add_pipe("component", name="custom_name", last=True)

# Add component from source pipeline
source_nlp = spacy.load("en_core_web_sm")
nlp.add_pipe("ner", source=source_nlp)

----------------------------------------

TITLE: Debugging the spaCy Tokenizer with explain() Method
DESCRIPTION: Example demonstrating how to debug the spaCy tokenizer using the explain() method. This method returns a list of tuples showing which tokenizer rule or pattern was matched for each token in the input text.

LANGUAGE: python
CODE:
from spacy.lang.en import English

nlp = English()
text = '\"Let\'s go!\"'
doc = nlp(text)
tok_exp = nlp.tokenizer.explain(text)
assert [t.text for t in doc if not t.is_space] == [t[1] for t in tok_exp]
for t in tok_exp:
    print(t[1], "\\t", t[0])

----------------------------------------

TITLE: Configuring Training Parameters in spaCy v3.0
DESCRIPTION: Example of a training configuration file that defines optimizer settings and learning rate schedule. This shows how to set up gradient accumulation, use the Adam optimizer, and implement a warmup linear schedule for the learning rate.

LANGUAGE: ini
CODE:
[training]
accumulate_gradient = 3

[training.optimizer]
@optimizers = "Adam.v1"

[training.optimizer.learn_rate]
@schedules = "warmup_linear.v1"
warmup_steps = 250
total_steps = 20000
initial_rate = 0.01

----------------------------------------

TITLE: Creating a Doc Object from Pre-tokenized Text in spaCy
DESCRIPTION: Demonstrates how to create a spaCy Doc object directly from pre-tokenized text by providing words and spaces information. This is useful when working with text that has already been tokenized by an external process.

LANGUAGE: python
CODE:
import spacy
from spacy.tokens import Doc

nlp = spacy.blank("en")
words = ["Hello", ",", "world", "!"]
spaces = [False, True, False, False]
doc = Doc(nlp.vocab, words=words, spaces=spaces)
print(doc.text)
print([(t.text, t.text_with_ws, t.whitespace_) for t in doc])

----------------------------------------

TITLE: Scoring Examples with a Pipeline Component
DESCRIPTION: Shows how to score a batch of examples using a pipeline component. This method calculates performance metrics for the component on the given examples.

LANGUAGE: python
CODE:
scores = pipe.score(examples)

----------------------------------------

TITLE: Component Factory with Custom Configuration
DESCRIPTION: This example demonstrates how to create a component factory with custom configuration settings, using default_config for fallback values and passing config when adding the component to the pipeline.

LANGUAGE: python
CODE:
import spacy
from spacy.language import Language

@Language.factory("my_component", default_config={"some_setting": True})
def my_component(nlp, name, some_setting: bool):
    return MyComponent(some_setting=some_setting)

nlp = spacy.blank("en")
nlp.add_pipe("my_component", config={"some_setting": False})

----------------------------------------

TITLE: Adding Lookup Lemmatizer to spaCy Pipeline
DESCRIPTION: Demonstrates how to add a lookup-based lemmatizer to a spaCy pipeline. This approach uses a lookup table to find lemmas without relying on part-of-speech information or context.

LANGUAGE: python
CODE:
# pip install -U %%SPACY_PKG_NAME[lookups]%%SPACY_PKG_FLAGS
import spacy

nlp = spacy.blank("sv")
nlp.add_pipe("lemmatizer", config={"mode": "lookup"})

----------------------------------------

TITLE: Saving spaCy Pipeline to Disk
DESCRIPTION: Shows how to save the current state of a spaCy pipeline to a directory. This method delegates to the to_disk methods of individual components to save their weights.

LANGUAGE: python
CODE:
nlp.to_disk("/path/to/pipeline")

----------------------------------------

TITLE: Managing Word Vectors in spaCy's Vocabulary
DESCRIPTION: Shows how to add word vectors to spaCy's vocabulary, import GloVe vectors, prune vectors to optimize memory usage, and save the resulting model to disk.

LANGUAGE: python
CODE:
for word, vector in vector_data:
    nlp.vocab.set_vector(word, vector)
nlp.vocab.vectors.from_glove("/path/to/vectors")
# Keep 10000 unique vectors and remap the rest
nlp.vocab.prune_vectors(10000)
nlp.to_disk("/model")

----------------------------------------

TITLE: Configuring spaCy Pipeline Components in INI Format
DESCRIPTION: This snippet shows how to specify pipeline components in spaCy's configuration file. The example defines a pipeline with four components: tok2vec (for token embeddings), tagger (for part-of-speech tagging), parser (for dependency parsing), and ner (for named entity recognition).

LANGUAGE: ini
CODE:
[nlp]
pipeline = ["tok2vec", "tagger", "parser", "ner"]

----------------------------------------

TITLE: Visualizing Dependency Parse using displaCy in spaCy
DESCRIPTION: This snippet demonstrates how to use spaCy's displaCy visualizer to render a document and its dependency parse structure. It loads a pre-trained English model, processes a sample text, and displays the dependency graph in a web browser.

LANGUAGE: python
CODE:
import spacy
from spacy import displacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Smith founded a healthcare company")
displacy.serve(doc)

----------------------------------------

TITLE: Extracting Named Entities from Text with spaCy in Python
DESCRIPTION: This snippet demonstrates how to load a spaCy model, process text to identify named entities, and extract entity details including text, character positions, and entity labels. The example identifies organizations, geopolitical entities, and monetary values in a sample sentence.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Apple is looking at buying U.K. startup for $1 billion")

for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)

----------------------------------------

TITLE: Setting Word Vectors in spaCy Vocabulary
DESCRIPTION: Sets a vector for a specific word in the vocabulary. Words can be referenced by their string form or hash value. Requires a numpy array as the vector data.

LANGUAGE: python
CODE:
nlp.vocab.set_vector("apple", array([...]))

----------------------------------------

TITLE: Initializing SpaCy Pipeline for Training in Python
DESCRIPTION: Example of initializing a SpaCy pipeline for training by passing a function that returns examples. This method prepares the pipeline components for training and returns an optimizer.

LANGUAGE: python
CODE:
get_examples = lambda: examples
optimizer = nlp.initialize(get_examples)

----------------------------------------

TITLE: Configuring Word Vectors in spaCy's MultiHashEmbed Layer
DESCRIPTION: Configuration for embedding layer that includes static word vectors. The include_static_vectors parameter enables the use of pre-trained word vectors as features in the model.

LANGUAGE: ini
CODE:
[tagger.model.tok2vec.embed]
@architectures = "spacy.MultiHashEmbed.v2"
width = 128
attrs = ["LOWER","PREFIX","SUFFIX","SHAPE"]
rows = [5000,2500,2500,2500]
include_static_vectors = true

----------------------------------------

TITLE: Creating Custom PyTorch Models in spaCy v3.0
DESCRIPTION: Example demonstrating how to wrap a PyTorch neural network model using Thinc's PyTorchWrapper. This allows using custom PyTorch models within spaCy's pipeline components while maintaining compatibility with spaCy's API.

LANGUAGE: python
CODE:
from torch import nn
from thinc.api import PyTorchWrapper

torch_model = nn.Sequential(
    nn.Linear(32, 32),
    nn.ReLU(),
    nn.Softmax(dim=1)
)
model = PyTorchWrapper(torch_model)

----------------------------------------

TITLE: Extracting Noun Chunks in spaCy with Python
DESCRIPTION: This code demonstrates how to extract noun chunks from a document using spaCy. It loads the English model, processes a sentence, and prints each chunk's text, root text, dependency relation, and head text.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Autonomous cars shift insurance liability toward manufacturers")
for chunk in doc.noun_chunks:
    print(chunk.text, chunk.root.text, chunk.root.dep_,
            chunk.root.head.text)

----------------------------------------

TITLE: Adding on_match Callback Functions to spaCy Matcher
DESCRIPTION: Demonstrates how to add a callback function that is executed when a pattern is matched. This example creates entity spans for matches of "Google I/O" and adds them to the document's entities.

LANGUAGE: python
CODE:
from spacy.lang.en import English
from spacy.matcher import Matcher
from spacy.tokens import Span

nlp = English()
matcher = Matcher(nlp.vocab)

def add_event_ent(matcher, doc, i, matches):
    # Get the current match and create tuple of entity label, start and end.
    # Append entity to the doc's entity. (Don't overwrite doc.ents!)
    match_id, start, end = matches[i]
    entity = Span(doc, start, end, label="EVENT")
    doc.ents += (entity,)
    print(entity.text)

pattern = [{"ORTH": "Google"}, {"ORTH": "I"}, {"ORTH": "/"}, {"ORTH": "O"}]
matcher.add("GoogleIO", [pattern], on_match=add_event_ent)
doc = nlp("This is a text about Google I/O")
matches = matcher(doc)

----------------------------------------

TITLE: Understanding spacy.load Under the Hood
DESCRIPTION: Abstract example showing how spacy.load() works internally. It demonstrates getting the language class, initializing it, adding pipeline components, and loading binary data.

LANGUAGE: python
CODE:
lang = "en"
pipeline = ["tok2vec", "tagger", "parser", "ner", "attribute_ruler", "lemmatizer"]
data_path = "path/to/en_core_web_sm/en_core_web_sm-3.0.0"

cls = spacy.util.get_lang_class(lang)  # 1. Get Language class, e.g. English
nlp = cls()                            # 2. Initialize it
for name in pipeline:
    nlp.add_pipe(name, config={...})   # 3. Add the component to the pipeline
nlp.from_disk(data_path)               # 4. Load in the binary data

----------------------------------------

TITLE: Adding Multiple Patterns to a spaCy Matcher
DESCRIPTION: Example demonstrating how to add multiple patterns to a matcher to match different token sequences, such as "hello world" both with and without punctuation between the words.

LANGUAGE: python
CODE:
patterns = [
    [{"LOWER": "hello"}, {"IS_PUNCT": True}, {"LOWER": "world"}],
    [{"LOWER": "hello"}, {"LOWER": "world"}]
]
matcher.add("HelloWorld", patterns)

----------------------------------------

TITLE: Implementing a Custom LLM Task in Python
DESCRIPTION: Python code to implement a custom task for the LLM component by creating a task class with generate_prompts and parse_responses methods, and registering it with the registry.

LANGUAGE: python
CODE:
from typing import Iterable, List
from spacy.tokens import Doc
from spacy_llm.registry import registry
from spacy_llm.util import split_labels


@registry.llm_tasks("my_namespace.MyTask.v1")
def make_my_task(labels: str, my_other_config_val: float) -> "MyTask":
    labels_list = split_labels(labels)
    return MyTask(labels=labels_list, my_other_config_val=my_other_config_val)


class MyTask:
    def __init__(self, labels: List[str], my_other_config_val: float):
        ...

    def generate_prompts(self, docs: Iterable[Doc]) -> Iterable[str]:
        ...

    def parse_responses(
        self, docs: Iterable[Doc], responses: Iterable[str]
    ) -> Iterable[Doc]:
        ...

----------------------------------------

TITLE: Using Statistical Sentence Segmenter in spaCy
DESCRIPTION: Shows how to use spaCy's SentenceRecognizer component for sentence segmentation. This example demonstrates loading a pipeline without the parser and enabling the senter component for faster sentence boundary detection.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("en_core_web_sm", exclude=["parser"])
nlp.enable_pipe("senter")
doc = nlp("This is a sentence. This is another sentence.")
for sent in doc.sents:
    print(sent.text)

----------------------------------------

TITLE: Adding Built-in Components to a Pipeline
DESCRIPTION: Example of adding built-in components to a blank spaCy pipeline. It shows how to create a blank English pipeline, add a sentencizer component, and add an entity ruler component.

LANGUAGE: python
CODE:
nlp = spacy.blank("en")
nlp.add_pipe("sentencizer")
# add_pipe returns the added component
ruler = nlp.add_pipe("entity_ruler")

----------------------------------------

TITLE: Working with Predicted Docs in spaCy Examples
DESCRIPTION: Shows how to access the predicted Doc objects from a list of Examples, update them with a model, and set the resulting annotations, which is a common pattern in spaCy training loops.

LANGUAGE: python
CODE:
docs = [eg.predicted for eg in examples]
predictions, _ = model.begin_update(docs)
set_annotations(docs, predictions)

----------------------------------------

TITLE: Exporting Configuration from spaCy Pipeline
DESCRIPTION: Demonstrates how to export a trainable config.cfg file from the current nlp object, which includes the current pipeline and all component configurations.

LANGUAGE: python
CODE:
nlp.config.to_disk("./config.cfg")
print(nlp.config.to_str())

----------------------------------------

TITLE: Efficiently Serializing Documents with DocBin in spaCy
DESCRIPTION: Shows how to use the DocBin class to efficiently serialize and deserialize multiple Doc objects. This example specifies which attributes to save (LEMMA, ENT_IOB, ENT_TYPE), enables storage of user data, and demonstrates how to retrieve the documents later.

LANGUAGE: python
CODE:
import spacy
from spacy.tokens import DocBin

doc_bin = DocBin(attrs=["LEMMA", "ENT_IOB", "ENT_TYPE"], store_user_data=True)
texts = ["Some text", "Lots of texts...", "..."]
nlp = spacy.load("en_core_web_sm")
for doc in nlp.pipe(texts):
    doc_bin.add(doc)
bytes_data = doc_bin.to_bytes()

# Deserialize later, e.g. in a new process
nlp = spacy.blank("en")
doc_bin = DocBin().from_bytes(bytes_data)
docs = list(doc_bin.get_docs(nlp.vocab))

----------------------------------------

TITLE: Initializing a Custom TrainablePipe Component in Python
DESCRIPTION: Example showing how to create a custom pipeline component by inheriting from TrainablePipe and registering it with a Language factory. This pattern allows for creating reusable, configurable pipeline components.

LANGUAGE: python
CODE:
from spacy.pipeline import TrainablePipe
from spacy.language import Language

class CustomPipe(TrainablePipe):
    ...

@Language.factory("your_custom_pipe", default_config={"model": MODEL})
def make_custom_pipe(nlp, name, model):
    return CustomPipe(nlp.vocab, model, name)

----------------------------------------

TITLE: Initializing Language Objects in spaCy
DESCRIPTION: Shows how to initialize a Language object either through a language-specific subclass (English) or from scratch using a Vocab object. The Language class is the core container for spaCy's NLP pipeline.

LANGUAGE: python
CODE:
# Construction from subclass
from spacy.lang.en import English
nlp = English()

# Construction from scratch
from spacy.vocab import Vocab
from spacy.language import Language
nlp = Language(Vocab())

----------------------------------------

TITLE: Configuring a Custom Model in spaCy Config File
DESCRIPTION: Example configuration section that specifies a custom PyTorch-based model architecture for a tagger component. This allows the model to be configured through spaCy's configuration system.

LANGUAGE: ini
CODE:
[components.tagger]
factory = "tagger"

[components.tagger.model]
@architectures = "CustomTorchModel.v1"
nO = 50
width = 96
hidden_width = 48
embed_size = 2000
nM = 64
nC = 8
dropout = 0.2

----------------------------------------

TITLE: Loading spaCy Pipeline from Disk
DESCRIPTION: Demonstrates loading a pipeline state from a directory using the from_disk method. Note that this only loads the serialized state and requires the correct language class to be initialized.

LANGUAGE: python
CODE:
from spacy.language import Language
nlp = Language().from_disk("/path/to/pipeline")

# Using language-specific subclass
from spacy.lang.en import English
nlp = English().from_disk("/path/to/pipeline")

----------------------------------------

TITLE: Defining Token Patterns in JSON Format for spaCy Matcher
DESCRIPTION: An example of a token pattern array for the Matcher class. This pattern matches a sequence starting with 'i' followed by lemmas 'like' or 'love', and ending with one or more nouns.

LANGUAGE: json
CODE:
[
  {"LOWER": "i"},
  {"LEMMA": {"IN": ["like", "love"]}},
  {"POS": "NOUN", "OP": "+"}
]

----------------------------------------

TITLE: Using Text Classification with OpenAI in Python
DESCRIPTION: Python code to assemble and use a spaCy pipeline with a text classifier powered by OpenAI GPT-3.5, loading from a configuration file.

LANGUAGE: python
CODE:
from spacy_llm.util import assemble

nlp = assemble("config.cfg")
doc = nlp("You look gorgeous!")
print(doc.cats)

----------------------------------------

TITLE: Adding a Transformer to spaCy Pipeline with Custom Configuration
DESCRIPTION: Example demonstrating how to add a transformer component to a spaCy pipeline with custom configuration. This shows how to customize the transformer model, tokenizer settings, and precision options.

LANGUAGE: python
CODE:
from spacy_transformers import Transformer
from spacy_transformers.pipeline_component import DEFAULT_CONFIG

nlp.add_pipe("transformer", config=DEFAULT_CONFIG["transformer"])

----------------------------------------

TITLE: Configuration for Streaming Large Corpus
DESCRIPTION: Config file example showing settings for streaming a large corpus that doesn't fit in memory. This configuration uses a custom reader for both train and dev corpora, with unlimited examples for training and a limited number for evaluation.

LANGUAGE: ini
CODE:
[corpora.dev]
@readers = "even_odd.v1"
limit = 100

[corpora.train]
@readers = "even_odd.v1"
limit = -1

[training]
max_epochs = -1
patience = 500
max_steps = 2000

----------------------------------------

TITLE: Filling Config Templates with spaCy CLI
DESCRIPTION: Command to fill in the remaining defaults in a starter configuration file. This ensures training configs are complete and reproducible without hidden defaults.

LANGUAGE: bash
CODE:
$ python -m spacy init fill-config base_config.cfg config.cfg

----------------------------------------

TITLE: Defining the Base Relation Model Architecture in Python
DESCRIPTION: Creates the basic structure for a relation extraction model using spaCy's registry system. This model takes a list of documents as input and outputs a matrix of relation predictions.

LANGUAGE: python
CODE:
@spacy.registry.architectures("rel_model.v1")
def create_relation_model(...) -> Model[List[Doc], Floats2d]:
    model = ...  # 👈 model will go here
    return model

----------------------------------------

TITLE: Examples of Cloning spaCy Project Templates
DESCRIPTION: Examples showing how to clone a spaCy project template from the default repository and from a custom repository.

LANGUAGE: bash
CODE:
$ python -m spacy project clone pipelines/ner_wikiner

LANGUAGE: bash
CODE:
$ python -m spacy project clone template --repo https://github.com/your_org/your_repo

----------------------------------------

TITLE: Running spaCy Debug Diff-Config Command
DESCRIPTION: The basic command to show differences between a config file and spaCy's default settings. This command helps users understand how their configuration differs from defaults, which is useful for debugging and sharing configurations.

LANGUAGE: bash
CODE:
$ python -m spacy debug diff-config [config_path] [--compare-to] [--optimize] [--gpu] [--pretraining] [--markdown]

----------------------------------------

TITLE: Adding Rule-based Lemmatizer to spaCy Pipeline
DESCRIPTION: Shows how to add a rule-based lemmatizer to a spaCy pipeline that has part-of-speech tagging. This approach uses rule tables to determine lemmas based on both word form and POS tags.

LANGUAGE: python
CODE:
# pip install -U %%SPACY_PKG_NAME[lookups]%%SPACY_PKG_FLAGS
import spacy

nlp = spacy.blank("de")
# Morphologizer (note: model is not yet trained!)
nlp.add_pipe("morphologizer")
# Rule-based lemmatizer
nlp.add_pipe("lemmatizer", config={"mode": "rule"})

----------------------------------------

TITLE: Getting System Information with spacy.info()
DESCRIPTION: Examples of using spacy.info() to display information about the spaCy installation, installed language models, and system setup, with options for formatting as markdown.

LANGUAGE: python
CODE:
spacy.info()
spacy.info("en_core_web_sm")
markdown = spacy.info(markdown=True, silent=True)

----------------------------------------

TITLE: Using memory zones in spaCy for efficient word counting
DESCRIPTION: Demonstrates how to use the nlp.memory_zone() context manager to count words across multiple texts while freeing internal caches after processing. This prevents memory growth over time by resetting caches when the memory zone is exited.

LANGUAGE: python
CODE:
from collections import Counter

def count_words(nlp, texts):
    counts = Counter()
    with nlp.memory_zone():
        for doc in nlp.pipe(texts):
            for token in doc:
                counts[token.text] += 1
    return counts

----------------------------------------

TITLE: Loading SpanRuler Patterns from JSONL File
DESCRIPTION: Example of loading patterns from a JSONL file for use with the SpanRuler component using the srsly utility to read the file and add patterns to the ruler.

LANGUAGE: python
CODE:
import srsly

patterns = srsly.read_jsonl("patterns.jsonl")
ruler = nlp.add_pipe("span_ruler")
ruler.add_patterns(patterns)

----------------------------------------

TITLE: Working with Serializable Lookup Tables
DESCRIPTION: Shows how to create and use serializable lookup tables in spaCy v2.2, which allow storing large dictionaries in the Vocab object and accessing them from components. These tables use Bloom filters for efficient lookups.

LANGUAGE: python
CODE:
data = {"foo": "bar"}
nlp.vocab.lookups.add_table("my_dict", data)

def custom_component(doc):
   table = doc.vocab.lookups.get_table("my_dict")
   print(table.get("foo"))  # look something up
   return doc

----------------------------------------

TITLE: Examples of Dictionary-Based Training Data for Different Components
DESCRIPTION: Practical examples of how to create training data for various spaCy pipeline components including part-of-speech tagger, entity recognizer (two approaches), and text categorizer.

LANGUAGE: python
CODE:
# Training data for a part-of-speech tagger
doc = Doc(vocab, words=["I", "like", "stuff"])
gold_dict = {"tags": ["NOUN", "VERB", "NOUN"]}
example = Example.from_dict(doc, gold_dict)

# Training data for an entity recognizer (option 1)
doc = nlp("Laura flew to Silicon Valley.")
gold_dict = {"entities": ["U-PERS", "O", "O", "B-LOC", "L-LOC"]}
example = Example.from_dict(doc, gold_dict)

# Training data for an entity recognizer (option 2)
doc = nlp("Laura flew to Silicon Valley.")
gold_dict = {"entities": [(0, 5, "PERSON"), (14, 28, "LOC")]}
example = Example.from_dict(doc, gold_dict)

# Training data for text categorization
doc = nlp("I'm pretty happy about that!")
gold_dict = {"cats": {"POSITIVE": 1.0, "NEGATIVE": 0.0}}
example = Example.from_dict(doc, gold_dict)

----------------------------------------

TITLE: Computing Similarity Between Documents and Spans in Python
DESCRIPTION: Shows how to calculate similarity between two documents, as well as between spans and tokens within documents. Uses the medium-sized model which includes word vectors.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("en_core_web_md")  # make sure to use larger package!
doc1 = nlp("I like salty fries and hamburgers.")
doc2 = nlp("Fast food tastes very good.")

# Similarity of two documents
print(doc1, "<->", doc2, doc1.similarity(doc2))
# Similarity of tokens and spans
french_fries = doc1[2:4]
burgers = doc1[5]
print(french_fries, "<->", burgers, french_fries.similarity(burgers))

----------------------------------------

TITLE: Implementing Custom Vector Embeddings in spaCy
DESCRIPTION: Custom embedding function that combines static word vectors with learned embeddings. This architecture adds static vectors to a smaller table of learned embeddings, providing a hybrid approach to token representation.

LANGUAGE: python
CODE:
from thinc.api import add, chain, remap_ids, Embed
from spacy.ml.staticvectors import StaticVectors
from spacy.ml.featureextractor import FeatureExtractor
from spacy.util import registry

@registry.architectures("my_example.MyEmbedding.v1")
def MyCustomVectors(
    output_width: int,
    vector_width: int,
    embed_rows: int,
    key2row: Dict[int, int]
) -> Model[List[Doc], List[Floats2d]]:
    return add(
        StaticVectors(nO=output_width),
        chain(
           FeatureExtractor(["ORTH"]),
           remap_ids(key2row),
           Embed(nO=output_width, nV=embed_rows)
        )
    )

----------------------------------------

TITLE: Calculating Semantic Similarity Between Spans in spaCy Python
DESCRIPTION: Computes semantic similarity between spans using cosine similarity of word vectors. The method works with Doc, Span, Token, and Lexeme objects and returns a scalar value where higher values indicate greater similarity.

LANGUAGE: python
CODE:
doc = nlp("green apples and red oranges")
green_apples = doc[:2]
red_oranges = doc[3:]
apples_oranges = green_apples.similarity(red_oranges)
oranges_apples = red_oranges.similarity(green_apples)
assert apples_oranges == oranges_apples

----------------------------------------

TITLE: Creating and Using Binary Training Data in spaCy
DESCRIPTION: Example showing how to convert Doc objects to spaCy's binary training format (.spacy) and how to load this data using the Corpus class for training.

LANGUAGE: python
CODE:
from spacy.tokens import DocBin
from spacy.training import Corpus

doc_bin = DocBin(docs=docs)
doc_bin.to_disk("./data.spacy")
reader = Corpus("./data.spacy")

----------------------------------------

TITLE: Using Doc.retokenize() Context Manager in spaCy
DESCRIPTION: Demonstrates using the retokenize() context manager to safely modify the tokenization of a Doc. This example shows merging two tokens into one.

LANGUAGE: python
CODE:
doc = nlp("Hello world!")
with doc.retokenize() as retokenizer:
    retokenizer.merge(doc[0:2])

----------------------------------------

TITLE: Processing Multiple Texts with Tokenizer.pipe Method
DESCRIPTION: Example showing how to efficiently tokenize a stream of texts using the pipe method, which yields Doc objects and supports batch processing.

LANGUAGE: python
CODE:
texts = ["One document.", "...", "Lots of documents"]
for doc in tokenizer.pipe(texts, batch_size=50):
    pass

----------------------------------------

TITLE: Using Fuzzy Matching with the Matcher in spaCy v3.5
DESCRIPTION: Demonstrates how to use the new FUZZY operators for pattern matching with the Matcher. Supports Levenshtein edit distance with default and custom limits for flexible text matching.

LANGUAGE: python
CODE:
# Match lowercase with fuzzy matching (allows up to 3 edits)
pattern = [{"LOWER": {"FUZZY": "definitely"}}]

# Match custom attribute values with fuzzy matching (allows up to 3 edits)
pattern = [{"_": {"country": {"FUZZY": "Kyrgyzstan"}}}]

# Match with exact Levenshtein edit distance limits (allows up to 4 edits)
pattern = [{"_": {"country": {"FUZZY4": "Kyrgyzstan"}}}]

----------------------------------------

TITLE: Combining PyTorch Model with spaCy Components
DESCRIPTION: Demonstrates how to chain a wrapped PyTorch model with a spaCy character embedding layer using Thinc's chain combinator. The with_array helper ensures the PyTorch model receives valid input formats.

LANGUAGE: python
CODE:
from thinc.api import chain, with_array, PyTorchWrapper
from spacy.ml import CharacterEmbed

wrapped_pt_model = PyTorchWrapper(torch_model)
char_embed = CharacterEmbed(width, embed_size, nM, nC)
model = chain(char_embed, with_array(wrapped_pt_model))

----------------------------------------

TITLE: Using Memory Zone in spaCy for Controlled Resource Allocation
DESCRIPTION: Demonstrates the memory_zone context manager which ensures all resources allocated within its block are freed at the end. This is useful for processing large volumes of text with a defined memory budget.

LANGUAGE: python
CODE:
counts = Counter()
with nlp.memory_zone():
    for doc in nlp.pipe(texts):
        for token in doc:
            counts[token.text] += 1

----------------------------------------

TITLE: Initializing EntityRecognizer in spaCy
DESCRIPTION: Examples of different ways to initialize the EntityRecognizer component in spaCy, including via add_pipe with default or custom models, or by direct instantiation from the class.

LANGUAGE: python
CODE:
# Construction via add_pipe with default model
ner = nlp.add_pipe("ner")

# Construction via add_pipe with custom model
config = {"model": {"@architectures": "my_ner"}}
parser = nlp.add_pipe("ner", config=config)

# Construction from class
from spacy.pipeline import EntityRecognizer
ner = EntityRecognizer(nlp.vocab, model)

----------------------------------------

TITLE: Configuring Raw Task with Few-Shot Examples
DESCRIPTION: Configuration for the Raw task component that includes few-shot examples loaded from an external YAML file using the FewShotReader utility. The task stores the LLM reply in a custom field.

LANGUAGE: ini
CODE:
[components.llm.task]
@llm_tasks = "spacy.Raw.v1"
field = "llm_reply"
[components.llm.task.examples]
@misc = "spacy.FewShotReader.v1"
path = "raw_examples.yml"

----------------------------------------

TITLE: Packaging spaCy Model with Custom Code
DESCRIPTION: Command line example for packaging a trained spaCy model with custom code. This ensures that custom components, architectures, or functions are included in the distributed package and will be registered when the pipeline is loaded.

LANGUAGE: bash
CODE:
### Packaging
$ python -m spacy package ./model-best ./packages --code functions.py

----------------------------------------

TITLE: Registering Stateless Function Components in spaCy v3.0
DESCRIPTION: Shows how to register a simple function-based pipeline component using the @Language.component decorator. The component takes a Doc and returns it without modification.

LANGUAGE: diff
CODE:
+ from spacy.language import Language

+ @Language.component("my_component")
def my_component(doc):
    return doc

----------------------------------------

TITLE: Using Rule-based Sentencizer Component in spaCy
DESCRIPTION: Demonstrates how to use spaCy's Sentencizer pipeline component for rule-based sentence segmentation. This component splits sentences on punctuation marks like periods, exclamation points, and question marks.

LANGUAGE: python
CODE:
import spacy
from spacy.lang.en import English

nlp = English()  # just the language with no pipeline
nlp.add_pipe("sentencizer")
doc = nlp("This is a sentence. This is another sentence.")
for sent in doc.sents:
    print(sent.text)

----------------------------------------

TITLE: Updating EntityRecognizer Model with Training Examples
DESCRIPTION: Shows how to train the EntityRecognizer model using a batch of Example objects. This method handles both prediction and loss calculation for model updating.

LANGUAGE: python
CODE:
ner = nlp.add_pipe("ner")
optimizer = nlp.initialize()
losses = ner.update(examples, sgd=optimizer)

----------------------------------------

TITLE: Configuring Independent Embedding Layers in spaCy Pipeline
DESCRIPTION: Configuration for a spaCy pipeline where the entity recognizer has its own tok2vec component. This setup makes components independent and doesn't require an upstream Tok2Vec component in the pipeline.

LANGUAGE: ini
CODE:
[components.ner]
factory = "ner"

[components.ner.model]
@architectures = "spacy.TransitionBasedParser.v1"

[components.ner.model.tok2vec]
@architectures = "spacy.Tok2Vec.v2"

[components.ner.model.tok2vec.embed]
@architectures = "spacy.MultiHashEmbed.v2"

[components.ner.model.tok2vec.encode]
@architectures = "spacy.MaxoutWindowEncoder.v2"

----------------------------------------

TITLE: Pretraining Tok2vec Layer Using spaCy CLI
DESCRIPTION: Example command for pretraining a Tok2vec layer with spaCy's CLI. This initializes pretraining using a config file and raw text data in JSONL format, saving the output to a specified directory.

LANGUAGE: bash
CODE:
$ python -m spacy pretrain config.cfg ./output_pretrain --paths.raw_text ./data.jsonl

----------------------------------------

TITLE: Accessing Token Children in spaCy
DESCRIPTION: Example showing how to access a token's immediate syntactic children using the children property. It returns a sequence of tokens whose head is the current token.

LANGUAGE: python
CODE:
doc = nlp("Give it back! He pleaded.")
give_children = doc[0].children
assert [t.text for t in give_children] == ["it", "back", "!"]

----------------------------------------

TITLE: Running spaCy train command for pipeline training
DESCRIPTION: Command for training a spaCy pipeline using a config file. Supports additional parameters for output directory, custom code, verbosity level, GPU selection, and config overrides.

LANGUAGE: bash
CODE:
$ python -m spacy train [config_path] [--output] [--code] [--verbose] [--gpu-id] [overrides]

----------------------------------------

TITLE: Visualizing Spans with spaCy's displaCy in Python
DESCRIPTION: This code demonstrates how to highlight overlapping spans in text using the span visualizer in spaCy. It creates a document with two spans - an organization ('Bank of China') and a geopolitical entity ('China') - and visualizes them using displaCy's span style.

LANGUAGE: python
CODE:
import spacy
from spacy import displacy
from spacy.tokens import Span

text = "Welcome to the Bank of China."

nlp = spacy.blank("en")
doc = nlp(text)

doc.spans["sc"] = [
    Span(doc, 3, 6, "ORG"),
    Span(doc, 5, 6, "GPE"),
]

displacy.serve(doc, style="span")

----------------------------------------

TITLE: Using PlainTextCorpus in spaCy to generate training examples (Python)
DESCRIPTION: Demonstrates how to use a PlainTextCorpus instance to generate training examples by passing it a spaCy Language object. The corpus yields Example objects for training.

LANGUAGE: python
CODE:
from spacy.training import PlainTextCorpus
import spacy

corpus = PlainTextCorpus("./docs.txt")
nlp = spacy.blank("en")
data = corpus(nlp)

----------------------------------------

TITLE: Installing spaCy Models via pip with URLs or Local Files
DESCRIPTION: Demonstrates various ways to install spaCy models using pip, including from external URLs, using spacy info to get the URL, or installing from local files.

LANGUAGE: bash
CODE:
# With external URL
$ pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl
$ pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz

# Using spacy info to get the external URL
$ pip install $(spacy info en_core_web_sm --url)

# With local file
$ pip install /Users/you/en_core_web_sm-3.0.0-py3-none-any.whl
$ pip install /Users/you/en_core_web_sm-3.0.0.tar.gz

----------------------------------------

TITLE: Using a Custom Component and Serializing the Pipeline in spaCy
DESCRIPTION: This code demonstrates how to add a custom component to a spaCy pipeline, add data to it, and serialize the entire pipeline to disk, which will trigger the component's to_disk method.

LANGUAGE: python
CODE:
nlp = spacy.load("en_core_web_sm")
my_component = nlp.add_pipe("my_component")
my_component.add({"hello": "world"})
nlp.to_disk("/path/to/pipeline")

----------------------------------------

TITLE: Making Predictions with Tok2Vec Component
DESCRIPTION: Example demonstrating how to apply the Tok2Vec model to a batch of documents to get predictions without modifying the documents.

LANGUAGE: python
CODE:
tok2vec = nlp.add_pipe("tok2vec")
scores = tok2vec.predict([doc1, doc2])

----------------------------------------

TITLE: Analyzing Pipeline Components and Dependencies in spaCy with Python
DESCRIPTION: Example showing how to use the analyze_pipes method to generate a detailed analysis of the current pipeline, showing attributes assigned, required, and scores for each component.

LANGUAGE: python
CODE:
nlp = spacy.blank("en")
nlp.add_pipe("tagger")
nlp.add_pipe("entity_linker")
analysis = nlp.analyze_pipes()

----------------------------------------

TITLE: Loading spaCy Pipeline Packages Directly Using Python Import
DESCRIPTION: Example of importing a spaCy pipeline package as a native Python package and loading it using its load() method. This approach provides better error handling and is more suitable for larger codebases.

LANGUAGE: python
CODE:
import en_core_web_sm
nlp = en_core_web_sm.load()

----------------------------------------

TITLE: Using Dependency Parser for Sentence Segmentation in spaCy
DESCRIPTION: Demonstrates the default approach for sentence boundary detection in spaCy, which uses the dependency parser. This approach provides accurate sentence boundaries based on full dependency parsing.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("This is a sentence. This is another sentence.")
for sent in doc.sents:
    print(sent.text)

----------------------------------------

TITLE: Converting Data Files to spaCy Format using CLI Command
DESCRIPTION: Example of converting existing corpora in .conll format to spaCy's binary format using the spacy convert command-line tool. This command automatically detects the input format based on file extension and outputs to the specified directory.

LANGUAGE: bash
CODE:
$ python -m spacy convert ./train.gold.conll ./corpus

----------------------------------------

TITLE: Using spaCy Debug Data Command
DESCRIPTION: The 'debug data' command analyzes, debugs, and validates training and development data. It takes a config path and optional arguments like --code, --ignore-warnings, --verbose, --no-format, and overrides. This command helps identify problems in data annotations.

LANGUAGE: bash
CODE:
$ python -m spacy debug data [config_path] [--code] [--ignore-warnings] [--verbose] [--no-format] [overrides]

----------------------------------------

TITLE: Calculating Semantic Similarity Between Docs in spaCy
DESCRIPTION: Demonstrates how to compute semantic similarity between Doc objects using similarity(). The default implementation uses cosine similarity of averaged word vectors.

LANGUAGE: python
CODE:
apples = nlp("I like apples")
oranges = nlp("I like oranges")
apples_oranges = apples.similarity(oranges)
oranges_apples = oranges.similarity(apples)
assert apples_oranges == oranges_apples

----------------------------------------

TITLE: Creating a Language Object from Configuration in spaCy
DESCRIPTION: Demonstrates how to create a Language object from a configuration file. This is how spaCy loads models using their config.cfg files, setting up the tokenizer, language data, and pipeline components.

LANGUAGE: python
CODE:
from thinc.api import Config
from spacy.language import Language

config = Config().from_disk("./config.cfg")
nlp = Language.from_config(config)

----------------------------------------

TITLE: Configuring Tok2Vec Architecture in spaCy
DESCRIPTION: Example configuration for the Tok2Vec.v2 architecture which constructs a token-to-vector model from embedding and encoding subnetworks.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.Tok2Vec.v2"

[model.embed]
@architectures = "spacy.CharacterEmbed.v2"
# ...

[model.encode]
@architectures = "spacy.MaxoutWindowEncoder.v2"
# ...

----------------------------------------

TITLE: Using Entity IDs with Entity Ruler Patterns in spaCy
DESCRIPTION: This example demonstrates how to add ID attributes to EntityRuler patterns to associate multiple patterns with the same entity. It shows how different text variations ('San Francisco' and 'San Fran') can be linked to the same entity ID.

LANGUAGE: python
CODE:
from spacy.lang.en import English

nlp = English()
ruler = nlp.add_pipe("entity_ruler")
patterns = [{"label": "ORG", "pattern": "Apple", "id": "apple"},
            {"label": "GPE", "pattern": [{"LOWER": "san"}, {"LOWER": "francisco"}], "id": "san-francisco"},
            {"label": "GPE", "pattern": [{"LOWER": "san"}, {"LOWER": "fran"}], "id": "san-francisco"}]
ruler.add_patterns(patterns)

doc1 = nlp("Apple is opening its first big office in San Francisco.")
print([(ent.text, ent.label_, ent.ent_id_) for ent in doc1.ents])

doc2 = nlp("Apple is opening its first big office in San Fran.")
print([(ent.text, ent.label_, ent.ent_id_) for ent in doc2.ents])

----------------------------------------

TITLE: Applying a TrainablePipe Component to a Document in Python
DESCRIPTION: Example demonstrating how to apply a pipeline component to a document using the __call__ method. This is typically called automatically when processing text with an nlp object.

LANGUAGE: python
CODE:
doc = nlp("This is a sentence.")
pipe = nlp.add_pipe("your_custom_pipe")
# This usually happens under the hood
processed = pipe(doc)

----------------------------------------

TITLE: Loading Token Attributes from NumPy Array with Doc.from_array
DESCRIPTION: Demonstrates loading token attributes from a NumPy array into a Doc object. This allows creating a new Doc with the same attributes as an existing one, which is useful for transferring linguistic annotations between documents.

LANGUAGE: python
CODE:
from spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA
from spacy.tokens import Doc
doc = nlp("Hello world!")
np_array = doc.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA])
doc2 = Doc(doc.vocab, words=[t.text for t in doc])
doc2.from_array([LOWER, POS, ENT_TYPE, IS_ALPHA], np_array)
assert doc[0].pos_ == doc2[0].pos_

----------------------------------------

TITLE: Initializing CuratedTransformer in spaCy Pipeline
DESCRIPTION: Examples showing different ways to initialize the CuratedTransformer component, including adding it to a pipeline with default settings, with custom configuration, or constructing it directly from the class with a model.

LANGUAGE: python
CODE:
# Construction via add_pipe with default model
trf = nlp.add_pipe("curated_transformer")

# Construction via add_pipe with custom config
config = {
    "model": {
        "@architectures": "spacy-curated-transformers.XlmrTransformer.v1",
        "vocab_size": 250002,
        "num_hidden_layers": 12,
        "hidden_width": 768,
        "piece_encoder": {
            "@architectures": "spacy-curated-transformers.XlmrSentencepieceEncoder.v1"
        }
    }
}
trf = nlp.add_pipe("curated_transformer", config=config)

# Construction from class
from spacy_curated_transformers import CuratedTransformer
trf = CuratedTransformer(nlp.vocab, model)

----------------------------------------

TITLE: Checking Token Ancestry in spaCy
DESCRIPTION: Example showing how to check if a token is an ancestor of another token in the dependency tree. The is_ancestor method returns True if the token is a parent, grandparent, etc. of another token.

LANGUAGE: python
CODE:
doc = nlp("Give it back! He pleaded.")
give = doc[0]
it = doc[1]
assert give.is_ancestor(it)

----------------------------------------

TITLE: spaCy Pretrain Command Syntax
DESCRIPTION: Full syntax for the spaCy pretrain command with all available arguments. The command requires a config path and output directory, with optional parameters for GPU usage, code importing, and resuming from previous training.

LANGUAGE: bash
CODE:
$ python -m spacy pretrain [config_path] [output_dir] [--code] [--resume-path] [--epoch-resume] [--gpu-id] [overrides]

----------------------------------------

TITLE: Downloading spaCy Pipeline Models via Command Line
DESCRIPTION: Command for downloading trained pipeline models for spaCy. It finds the best compatible version and uses pip to install the package. Supports direct downloads and source distributions.

LANGUAGE: bash
CODE:
$ python -m spacy download [model] [--direct] [--sdist] [pip_args]

----------------------------------------

TITLE: Configuring SingleLabel SpanCategorizer in spaCy
DESCRIPTION: Example showing how to configure and add the 'spancat_singlelabel' pipeline component to a spaCy NLP object with custom settings for single-label classification of spans.

LANGUAGE: python
CODE:
from spacy.pipeline.spancat import DEFAULT_SPANCAT_SINGLELABEL_MODEL
config = {
    "spans_key": "labeled_spans",
    "model": DEFAULT_SPANCAT_SINGLELABEL_MODEL,
    "suggester": {"@misc": "spacy.ngram_suggester.v1", "sizes": [1, 2, 3]},
    # Additional spancat_singlelabel parameters
    "negative_weight": 0.8,
    "allow_overlap": True,
}
nlp.add_pipe("spancat_singlelabel", config=config)

----------------------------------------

TITLE: Configuring TransitionBasedParser.v2 in spaCy
DESCRIPTION: Configuration example for the TransitionBasedParser.v2 architecture, which can be used for NER or dependency parsing. The configuration includes settings for state type, token vector representation, hidden layer width, and network architecture parameters.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.TransitionBasedParser.v2"
state_type = "ner"
extra_state_tokens = false
hidden_width = 64
maxout_pieces = 2
use_upper = true

[model.tok2vec]
@architectures = "spacy.HashEmbedCNN.v2"
pretrained_vectors = null
width = 96
depth = 4
embed_size = 2000
window_size = 1
maxout_pieces = 3
subword_features = true

----------------------------------------

TITLE: Converting BILUO Tags to Character Offsets in spaCy
DESCRIPTION: Shows how to convert BILUO token tags back to character offsets, which is useful for processing entity annotations in different formats.

LANGUAGE: python
CODE:
from spacy.training import biluo_tags_to_offsets

doc = nlp("I like London.")
tags = ["O", "O", "U-LOC", "O"]
entities = biluo_tags_to_offsets(doc, tags)
assert entities == [(7, 13, "LOC")]

----------------------------------------

TITLE: Importing Non-Trained Pipeline Languages in spaCy
DESCRIPTION: How to use languages that don't have a trained pipeline by importing them directly or using spacy.blank() to create a blank pipeline instance that contains just a tokenizer.

LANGUAGE: python
CODE:
from spacy.lang.yo import Yoruba
nlp = Yoruba()  # use directly
nlp = spacy.blank("yo")  # blank instance

----------------------------------------

TITLE: Configuring the Coref Model Architecture in spaCy
DESCRIPTION: Example configuration for the spacy-experimental.Coref.v1 architecture, which handles coreference resolution. The configuration includes parameters for embedding size, dropout, network dimensions, and antecedent handling. It uses a transformer-based token vectorization strategy with mean pooling.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy-experimental.Coref.v1"
distance_embedding_size = 20
dropout = 0.3
hidden_size = 1024
depth = 2
antecedent_limit = 50
antecedent_batch_size = 512

[model.tok2vec]
@architectures = "spacy-transformers.TransformerListener.v1"
grad_factor = 1.0
upstream = "transformer"
pooling = {"@layers":"reduce_mean.v1"}

----------------------------------------

TITLE: Loading a spaCy Tokenizer from Disk
DESCRIPTION: Shows how to load a previously serialized spaCy Tokenizer from disk using the from_disk method. This modifies the tokenizer instance in place.

LANGUAGE: python
CODE:
tokenizer = Tokenizer(nlp.vocab)
tokenizer.from_disk("/path/to/tokenizer")

----------------------------------------

TITLE: Checking for Component Presence in spaCy Pipeline
DESCRIPTION: Demonstrates how to check if a component exists in the pipeline using the has_pipe method, which is equivalent to checking if the name is in nlp.pipe_names.

LANGUAGE: python
CODE:
@Language.component("component")
def component(doc):
    return doc

nlp.add_pipe("component", name="my_component")
assert "my_component" in nlp.pipe_names
assert nlp.has_pipe("my_component")

----------------------------------------

TITLE: Initializing a spaCy Config File with CLI
DESCRIPTION: Creates a new config.cfg file with recommended settings for training spaCy models. Takes parameters for language, pipeline components, and optimization preferences.

LANGUAGE: bash
CODE:
$ python -m spacy init config [output_file] [--lang] [--pipeline] [--optimize] [--gpu] [--pretraining] [--force]

----------------------------------------

TITLE: Registering a Custom Annotation Setter in spaCy Transformers
DESCRIPTION: This snippet demonstrates how to register a custom annotation setter using the registry.annotation_setters decorator. The example creates a null annotation setter that doesn't modify the documents but follows the required function signature, taking a list of Doc objects and a FullTransformerBatch.

LANGUAGE: python
CODE:
@registry.annotation_setters("spacy-transformers.null_annotation_setter.v1")
def configure_null_annotation_setter() -> Callable:
    def setter(docs: List[Doc], trf_data: FullTransformerBatch) -> None:
        pass

    return setter

----------------------------------------

TITLE: Removing Pipeline Components in spaCy
DESCRIPTION: Demonstrates how to remove a component from the pipeline. The method returns both the name and the component function of the removed component.

LANGUAGE: python
CODE:
name, component = nlp.remove_pipe("parser")
assert name == "parser"

----------------------------------------

TITLE: Debugging Training Data with spaCy CLI
DESCRIPTION: Command to analyze and validate training and development data, providing useful statistics and identifying potential problems in the data.

LANGUAGE: bash
CODE:
$ python -m spacy debug data config.cfg

----------------------------------------

TITLE: Accessing spaCy Vocabulary Attributes
DESCRIPTION: Demonstrates how to access the strings table in the vocabulary, which manages string-to-int mapping. The example shows converting words to their integer IDs, which are used internally by spaCy.

LANGUAGE: python
CODE:
apple_id = nlp.vocab.strings["apple"]
assert type(apple_id) == int
PERSON = nlp.vocab.strings["PERSON"]
assert type(PERSON) == int

----------------------------------------

TITLE: Implementing the Initialize Method for Relation Extraction
DESCRIPTION: This method initializes the relation extraction component by setting up labels and preparing the model. It either uses explicitly provided labels or extracts them from training examples, then initializes the underlying ML model with sample data.

LANGUAGE: python
CODE:
from itertools import islice

def initialize(
    self,
    get_examples: Callable[[], Iterable[Example]],
    *,
    nlp: Language = None,
    labels: Optional[List[str]] = None,
):
    if labels is not None:
        for label in labels:
            self.add_label(label)
    else:
        for example in get_examples():
            relations = example.reference._.rel
            for indices, label_dict in relations.items():
                for label in label_dict.keys():
                    self.add_label(label)
    subbatch = list(islice(get_examples(), 10))
    doc_sample = [eg.reference for eg in subbatch]
    label_sample = self._examples_to_truth(subbatch)
    self.model.initialize(X=doc_sample, Y=label_sample)

----------------------------------------

TITLE: Full spaCy Default Configuration File
DESCRIPTION: The default configuration file used by spaCy that defines the training process and pipeline settings. This file serves as a template that can be customized for specific use cases.

LANGUAGE: ini
CODE:
%%GITHUB_SPACY/spacy/default_config.cfg

----------------------------------------

TITLE: JSON Format for Few-Shot Examples in NER v3
DESCRIPTION: Example of the JSON format required for few-shot learning with NER v3. This shows how to structure examples with 'is_entity' and 'reason' fields to help the model distinguish entities from non-entities.

LANGUAGE: json
CODE:
[
  {
    "text": "You can't get a great chocolate flavor with carob.",
    "spans": [
      {
        "text": "chocolate",
        "is_entity": false,
        "label": "==NONE==",
        "reason": "is a flavor in this context, not an ingredient"
      },
      {
        "text": "carob",
        "is_entity": true,
        "label": "INGREDIENT",
        "reason": "is an ingredient to add chocolate flavor"
      }
    ]
  },
  ...
]

----------------------------------------

TITLE: Using Operators in spaCy Matcher Token Patterns
DESCRIPTION: Example demonstrating the use of operators in token patterns. This pattern matches sequences with zero or more adjectives, followed by one or more nouns, followed by exactly two proper nouns.

LANGUAGE: json
CODE:
[
  {"POS": "ADJ", "OP": "*"},
  {"POS": "NOUN", "OP": "+"}
  {"POS": "PROPN", "OP": "{2}"}
]

----------------------------------------

TITLE: TextCategorizer Model Configuration in spaCy
DESCRIPTION: This snippet shows the configuration for a TextCategorizer component using the TextCatEnsemble architecture. It defines the complete structure of the neural network including the tok2vec sublayer with embedding and encoding components.

LANGUAGE: ini
CODE:
[components.textcat]
factory = "textcat"
labels = []

[components.textcat.model]
@architectures = "spacy.TextCatEnsemble.v2"
nO = null

[components.textcat.model.tok2vec]
@architectures = "spacy.Tok2Vec.v2"

[components.textcat.model.tok2vec.embed]
@architectures = "spacy.MultiHashEmbed.v2"
width = 64
rows = [2000, 2000, 1000, 1000, 1000, 1000]
attrs = ["ORTH", "LOWER", "PREFIX", "SUFFIX", "SHAPE", "ID"]
include_static_vectors = false

[components.textcat.model.tok2vec.encode]
@architectures = "spacy.MaxoutWindowEncoder.v2"
width = ${components.textcat.model.tok2vec.embed.width}
window_size = 1
maxout_pieces = 3
depth = 2

[components.textcat.model.linear_model]
@architectures = "spacy.TextCatBOW.v3"
exclusive_classes = true
length = 262144
ngram_size = 1
no_output_layer = false

----------------------------------------

TITLE: Creating a Span from Character Indices in spaCy Python
DESCRIPTION: Creates a Span object from character indices in a document text. The method maps text character positions to token spans and can assign entity labels. Returns None if the character indices don't map to valid tokens.

LANGUAGE: python
CODE:
doc = nlp("I like New York")
span = doc[1:4].char_span(5, 13, label="GPE")
assert span.text == "New York"

----------------------------------------

TITLE: Configuring a Tok2Vec Component in spaCy
DESCRIPTION: Configuration for defining a standalone Tok2Vec component in a spaCy pipeline that can be shared across multiple components.

LANGUAGE: toml
CODE:
[components.tok2vec]
factory = "tok2vec"

[components.tok2vec.model]
@architectures = "spacy.Tok2Vec.v2"

----------------------------------------

TITLE: Getting the length of a SpanGroup
DESCRIPTION: Shows how to get the number of spans in a SpanGroup using the len() function.

LANGUAGE: python
CODE:
doc = nlp("Their goi ng home")
doc.spans["errors"] = [doc[0:1], doc[1:3]]
assert len(doc.spans["errors"]) == 2

----------------------------------------

TITLE: Installing spaCy using conda
DESCRIPTION: Command to install spaCy using conda package manager via the conda-forge channel, which is an alternative installation method.

LANGUAGE: bash
CODE:
conda install -c conda-forge spacy

----------------------------------------

TITLE: Configuring the SpanResolver Model Architecture in spaCy
DESCRIPTION: Example configuration for the spacy-experimental.SpanResolver.v1 architecture used for resolving spans in coreference resolution. The configuration specifies neural network parameters, including hidden size, distance embedding dimensions, CNN settings, and maximum span distance. It also uses transformer-based token embeddings with mean pooling.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy-experimental.SpanResolver.v1"
hidden_size = 1024
distance_embedding_size = 64
conv_channels = 4
window_size = 1
max_distance = 128
prefix = "coref_head_clusters"

[model.tok2vec]
@architectures = "spacy-transformers.TransformerListener.v1"
grad_factor = 1.0
upstream = "transformer"
pooling = {"@layers":"reduce_mean.v1"}

----------------------------------------

TITLE: Counting Leftward Children in spaCy
DESCRIPTION: Example showing how to get the number of leftward immediate children of a token using the n_lefts property. This returns a count of the tokens to the left whose head is the current token.

LANGUAGE: python
CODE:
doc = nlp("I like New York in Autumn.")
assert doc[3].n_lefts == 1

----------------------------------------

TITLE: Disabling Pipeline Components in spaCy
DESCRIPTION: Shows how to temporarily disable a pipeline component. Disabled components are not run during processing but remain in the components list for later re-enabling.

LANGUAGE: python
CODE:
nlp.add_pipe("ner")
nlp.add_pipe("textcat")
assert nlp.pipe_names == ["ner", "textcat"]
nlp.disable_pipe("ner")
assert nlp.pipe_names == ["textcat"]
assert nlp.component_names == ["ner", "textcat"]
assert nlp.disabled == ["ner"]

----------------------------------------

TITLE: Registering a Custom PyTorch Architecture in spaCy
DESCRIPTION: Creates and registers a custom architecture that combines a PyTorch model with spaCy's CharacterEmbed. The architecture is registered in spaCy's registry, making it available through configuration files.

LANGUAGE: python
CODE:
from typing import List
from thinc.types import Floats2d
from thinc.api import Model, PyTorchWrapper, chain, with_array
import spacy
from spacy.tokens.doc import Doc
from spacy.ml import CharacterEmbed
from torch import nn

@spacy.registry.architectures("CustomTorchModel.v1")
def create_torch_model(
    nO: int,
    width: int,
    hidden_width: int,
    embed_size: int,
    nM: int,
    nC: int,
    dropout: float,
) -> Model[List[Doc], List[Floats2d]]:
    char_embed = CharacterEmbed(width, embed_size, nM, nC)
    torch_model = nn.Sequential(
        nn.Linear(width, hidden_width),
        nn.ReLU(),
        nn.Dropout2d(dropout),
        nn.Linear(hidden_width, nO),
        nn.ReLU(),
        nn.Dropout2d(dropout),
        nn.Softmax(dim=1)
    )
    wrapped_pt_model = PyTorchWrapper(torch_model)
    model = chain(char_embed, with_array(wrapped_pt_model))
    return model

----------------------------------------

TITLE: Implementing Custom Serialization Methods in spaCy Component
DESCRIPTION: This code shows how to implement to_disk and from_disk methods in a custom component to handle serialization and deserialization of component data, allowing the component to save and load its state.

LANGUAGE: python
CODE:
import srsly
from spacy.util import ensure_path

class AcronymComponent:
    # other methods here...

    def to_disk(self, path, exclude=tuple()):
        path = ensure_path(path)
        if not path.exists():
            path.mkdir()
        srsly.write_json(path / "data.json", self.data)

    def from_disk(self, path, exclude=tuple()):
        self.data = srsly.read_json(path / "data.json")
        return self

----------------------------------------

TITLE: Exporting Token Attributes to NumPy Array with Doc.to_array
DESCRIPTION: Shows how to export token attributes to a NumPy array. The method can export multiple attributes as a 2D array or a single attribute as a 1D array. Attributes can be specified by integer ID or string name.

LANGUAGE: python
CODE:
from spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA
doc = nlp(text)
# All strings mapped to integers, for easy export to numpy
np_array = doc.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA])
np_array = doc.to_array("POS")

----------------------------------------

TITLE: Downloading and loading spaCy language model
DESCRIPTION: Command to download a pre-trained English language model using the spaCy CLI tool, followed by Python code to load the model and create an NLP object.

LANGUAGE: bash
CODE:
$ python -m spacy download en_core_web_sm

>>> import spacy
>>> nlp = spacy.load("en_core_web_sm")

----------------------------------------

TITLE: Applying Trained Pipeline to Data with spaCy CLI
DESCRIPTION: Command for applying a trained spaCy pipeline to input data and storing the annotated documents in a DocBin file. Supports multiple input formats including .spacy, .jsonl, and plain text files. Can process single files or recursively traverse directories.

LANGUAGE: bash
CODE:
$ python -m spacy apply [model] [data-path] [output-file] [--code] [--text-key] [--force-overwrite] [--gpu-id] [--batch-size] [--n-process]

----------------------------------------

TITLE: Configuring SpanCategorizer Model Architecture in spaCy
DESCRIPTION: Configuration example for the SpanCategorizer model architecture that powers the SpanCategorizer component. It includes a token-to-vector model, a reducer model for mapping sequence vectors to a single vector, and a scorer model.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.SpanCategorizer.v1"
scorer = {"@layers": "spacy.LinearLogistic.v1"}

[model.reducer]
@layers = spacy.mean_max_reducer.v1"
hidden_size = 128

[model.tok2vec]
@architectures = "spacy.Tok2Vec.v1"

[model.tok2vec.embed]
@architectures = "spacy.MultiHashEmbed.v1"
# ...

[model.tok2vec.encode]
@architectures = "spacy.MaxoutWindowEncoder.v1"
# ...

----------------------------------------

TITLE: Compiling Suffix Regex for Tokenization in spaCy
DESCRIPTION: Demonstrates how to compile suffix rules into a regex object and assign it to a tokenizer. This function takes a sequence of suffix patterns and returns a compiled regex that can be used for tokenizer.suffix_search.

LANGUAGE: python
CODE:
suffixes = ("'s", "'S", r"(?<=[0-9])\+")
suffix_regex = util.compile_suffix_regex(suffixes)
nlp.tokenizer.suffix_search = suffix_regex.search

----------------------------------------

TITLE: Initializing StringStore in Python
DESCRIPTION: Creates a new StringStore instance with optional initial strings. The StringStore maps strings to hash values for consistent lookups.

LANGUAGE: python
CODE:
from spacy.strings import StringStore
stringstore = StringStore(["apple", "orange"])

----------------------------------------

TITLE: Upgrading spaCy and validating pipeline compatibility
DESCRIPTION: Commands to upgrade spaCy to the latest version and validate that all installed pipeline packages are compatible with the installed spaCy version.

LANGUAGE: bash
CODE:
$ pip install -U %%SPACY_PKG_NAME%%SPACY_PKG_FLAGS
$ python -m spacy validate

----------------------------------------

TITLE: Initializing Training Config for English Pipeline
DESCRIPTION: Creates a starter configuration file for training an English language model with tagger and parser components using the init config command.

LANGUAGE: bash
CODE:
$ python -m spacy init config ./config.cfg --lang en --pipeline tagger,parser

----------------------------------------

TITLE: Configuring TextCat with Few-Shot Examples
DESCRIPTION: Configuration for TextCat that uses a FewShotReader to load examples from a JSON file for few-shot learning, allowing the model to learn from provided examples.

LANGUAGE: ini
CODE:
[components.llm.task.examples]
@misc = "spacy.FewShotReader.v1"
path = "textcat_examples.json"

----------------------------------------

TITLE: Implementing EntityRuler for Rule-based Entity Recognition
DESCRIPTION: Demonstrates the new EntityRuler component which allows adding named entities based on pattern dictionaries. It combines rule-based and statistical named entity recognition for more powerful models.

LANGUAGE: python
CODE:
from spacy.pipeline import EntityRuler
ruler = EntityRuler(nlp)
ruler.add_patterns([{"label": "ORG", "pattern": "Apple"}])
nlp.add_pipe(ruler, before="ner")

----------------------------------------

TITLE: Component Configuration in spaCy v3.0 config.cfg
DESCRIPTION: Shows how a registered component appears in the spaCy v3.0 configuration file (config.cfg). The component is identified by its factory name and includes custom settings.

LANGUAGE: ini
CODE:
[components.my_component]
factory = "my_component"
some_setting = true

----------------------------------------

TITLE: Using select_pipes Method in spaCy for Pipeline Component Management in Python
DESCRIPTION: Examples of using the select_pipes method as a context manager to temporarily disable pipeline components for initialization or other operations. This can be used with either disable or enable parameters to specify which components to include or exclude.

LANGUAGE: python
CODE:
with nlp.select_pipes(disable=["tagger", "parser"]):
   nlp.initialize()

with nlp.select_pipes(enable="ner"):
    nlp.initialize()

disabled = nlp.select_pipes(disable=["tagger", "parser"])
nlp.initialize()
disabled.restore()

----------------------------------------

TITLE: Entity Recognition Input Format for displaCy
DESCRIPTION: This code shows the expected JSON format for named entity recognition visualization with displaCy. It includes text content, entity positions with start/end character offsets, and an optional title field.

LANGUAGE: python
CODE:
{
    "text": "But Google is starting from behind.",
    "ents": [{"start": 4, "end": 10, "label": "ORG"}],
    "title": None
}

----------------------------------------

TITLE: Defining Project Workflows in YAML Configuration
DESCRIPTION: YAML configuration that defines a workflow named 'all' which chains multiple commands to be executed in sequence, from preprocessing to training and packaging.

LANGUAGE: yaml
CODE:
workflows:
  all:
    - preprocess
    - train
    - package

----------------------------------------

TITLE: Accessing lexemes in spaCy Vocab
DESCRIPTION: Retrieves a lexeme from the vocabulary using either a string or its hash ID. This demonstrates the __getitem__ method for lexeme lookup.

LANGUAGE: python
CODE:
apple = nlp.vocab.strings["apple"]
assert nlp.vocab[apple] == nlp.vocab["apple"]

----------------------------------------

TITLE: Loading Probability Tables into Existing spaCy Models
DESCRIPTION: This code shows how to load a probability table from the spacy-lookups-data package into an existing spaCy model. This allows adding lexical probability data to a model after it's been loaded.

LANGUAGE: python
CODE:
# Requirements: pip install spacy-lookups-data
import spacy
from spacy.lookups import load_lookups
nlp = spacy.load("en_core_web_sm")
lookups = load_lookups("en", ["lexeme_prob"])
nlp.vocab.lookups.add_table("lexeme_prob", lookups.get_table("lexeme_prob"))

----------------------------------------

TITLE: Migrating Simple Training Style to Example API
DESCRIPTION: Shows how to convert the simple dictionary-based training approach to the new Example-based API in spaCy v3.0.

LANGUAGE: diff
CODE:
text = "Mark Zuckerberg is the CEO of Facebook"
annotations = {"entities": [(0, 15, "PERSON"), (30, 38, "ORG")]}
+ doc = nlp.make_doc(text)
+ example = Example.from_dict(doc, annotations)

----------------------------------------

TITLE: Adding Custom Labels to an LLM Component in spaCy
DESCRIPTION: Shows how to add a new label to an LLM component's task after initialization. This is an alternative to providing labels during task definition or through the config initialization block.

LANGUAGE: python
CODE:
llm_ner = nlp.add_pipe("llm_ner")
llm_ner.add_label("MY_LABEL")

----------------------------------------

TITLE: Finding Token Sequences with Matcher.__call__
DESCRIPTION: Demonstrates how to use the Matcher class to find all token sequences matching the supplied patterns on a Doc or Span. The example shows creating a matcher, adding a pattern to match "hello world", and applying it to a document.

LANGUAGE: python
CODE:
from spacy.matcher import Matcher

matcher = Matcher(nlp.vocab)
pattern = [{"LOWER": "hello"}, {"LOWER": "world"}]
matcher.add("HelloWorld", [pattern])
doc = nlp("hello world!")
matches = matcher(doc)

----------------------------------------

TITLE: Implementing Custom Logger Function in Python for spaCy Training
DESCRIPTION: Implements a custom logger function that writes training results to a tabular file. The function is registered to spaCy's logger registry with the name 'my_custom_logger.v1' and can be referenced in config files.

LANGUAGE: python
CODE:
import sys
from typing import IO, Tuple, Callable, Dict, Any, Optional
import spacy
from spacy import Language
from pathlib import Path

@spacy.registry.loggers("my_custom_logger.v1")
def custom_logger(log_path):
    def setup_logger(
        nlp: Language,
        stdout: IO=sys.stdout,
        stderr: IO=sys.stderr
    ) -> Tuple[Callable, Callable]:
        stdout.write(f"Logging to {log_path}\n")
        log_file = Path(log_path).open("w", encoding="utf8")
        log_file.write("step\t")
        log_file.write("score\t")
        for pipe in nlp.pipe_names:
            log_file.write(f"loss_{pipe}\t")
        log_file.write("\n")

        def log_step(info: Optional[Dict[str, Any]]):
            if info:
                log_file.write(f"{info['step']}\t")
                log_file.write(f"{info['score']}\t")
                for pipe in nlp.pipe_names:
                    log_file.write(f"{info['losses'][pipe]}\t")
                log_file.write("\n")

        def finalize():
            log_file.close()

        return log_step, finalize

    return setup_logger

----------------------------------------

TITLE: Implementing a Training Loop with the Example API
DESCRIPTION: Demonstrates a complete training loop using the new Example API in spaCy v3.0, including initialization and batch training for an entity recognition model.

LANGUAGE: python
CODE:
TRAIN_DATA = [
    ("Who is Shaka Khan?", {"entities": [(7, 17, "PERSON")]}),
    ("I like London.", {"entities": [(7, 13, "LOC")]}),
]
examples = []
for text, annots in TRAIN_DATA:
    examples.append(Example.from_dict(nlp.make_doc(text), annots))
nlp.initialize(lambda: examples)
for i in range(20):
    random.shuffle(examples)
    for batch in minibatch(examples, size=8):
        nlp.update(batch)

----------------------------------------

TITLE: Splitting Tokens with Retokenizer in spaCy
DESCRIPTION: Demonstrates splitting a token into multiple tokens using retokenize(). Requires specifying heads for dependency structure and can include linguistic attributes for the new tokens.

LANGUAGE: python
CODE:
doc = nlp("I live in NewYork")
with doc.retokenize() as retokenizer:
    heads = [(doc[3], 1), doc[2]]
    attrs = {"POS": ["PROPN", "PROPN"],
             "DEP": ["pobj", "compound"]}
    retokenizer.split(doc[3], ["New", "York"], heads=heads, attrs=attrs)

----------------------------------------

TITLE: Initializing tok2vec with Pretrained Weights in spaCy Configuration
DESCRIPTION: This snippet shows how to configure a spaCy pipeline to initialize the tok2vec component with weights from a pretrained model. It sets the path to the pretrained model file in the paths section and references it in the initialize section.

LANGUAGE: ini
CODE:
[paths]
init_tok2vec = "pretrain/model-last.bin"

[initialize]
init_tok2vec = ${paths.init_tok2vec}

----------------------------------------

TITLE: Default Punctuation Characters for Sentencizer
DESCRIPTION: The default list of punctuation characters used by the Sentencizer to detect sentence boundaries if no custom list is provided. These characters mark potential sentence endings across multiple languages and writing systems.

LANGUAGE: Python
CODE:
['!', '.', '?', '։', '؟', '۔', '܀', '܁', '܂', '߹', '।', '॥', '၊', '။', '።',
 '፧', '፨', '᙮', '᜵', '᜶', '᠃', '᠉', '᥄', '᥅', '᪨', '᪩', '᪪', '᪫',
 '᭚', '᭛', '᭞', '᭟', '᰻', '᰼', '᱾', '᱿', '‼', '‽', '⁇', '⁈', '⁉',
 '⸮', '⸼', '꓿', '꘎', '꘏', '꛳', '꛷', '꡶', '꡷', '꣎', '꣏', '꤯', '꧈',
 '꧉', '꩝', '꩞', '꩟', '꫰', '꫱', '꯫', '﹒', '﹖', '﹗', '！', '．', '？',
 '𐩖', '𐩗', '𑁇', '𑁈', '𑂾', '𑂿', '𑃀', '𑃁', '𑅁', '𑅂', '𑅃', '𑇅',
 '𑇆', '𑇍', '𑇞', '𑇟', '𑈸', '𑈹', '𑈻', '𑈼', '𑊩', '𑑋', '𑑌', '𑗂',
 '𑗃', '𑗉', '𑗊', '𑗋', '𑗌', '𑗍', '𑗎', '𑗏', '𑗐', '𑗑', '𑗒', '𑗓',
 '𑗔', '𑗕', '𑗖', '𑗗', '𑙁', '𑙂', '𑜼', '𑜽', '𑜾', '𑩂', '𑩃', '𑪛',
 '𑪜', '𑱁', '𑱂', '𖩮', '𖩯', '𖫵', '𖬷', '𖬸', '𖭄', '𛲟', '𝪈', '｡', '。']

----------------------------------------

TITLE: Configuring spaCy CharacterEmbed Architecture
DESCRIPTION: Configuration for the CharacterEmbed architecture that creates representations based on character embeddings. It uses a fixed number of UTF-8 characters from the beginning and end of each word, passing them through a feed-forward network.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.CharacterEmbed.v2"
width = 128
rows = 7000
nM = 64
nC = 8

----------------------------------------

TITLE: Initializing EntityRuler in spaCy
DESCRIPTION: Examples demonstrating two methods for initializing an EntityRuler in spaCy: via the add_pipe method and by direct instantiation from the EntityRuler class. The second method shows setting the overwrite_ents parameter to True.

LANGUAGE: python
CODE:
# Construction via add_pipe
ruler = nlp.add_pipe("entity_ruler")

# Construction from class
from spacy.pipeline import EntityRuler
ruler = EntityRuler(nlp, overwrite_ents=True)

----------------------------------------

TITLE: Configuring WithStridedSpans for CuratedTransformer in spaCy
DESCRIPTION: Configuration example for setting up strided spans processing in a CuratedTransformer component. This controls how documents are split into sequences before being processed by the transformer model.

LANGUAGE: ini
CODE:
[transformer.model.with_spans]
@architectures = "spacy-curated-transformers.WithStridedSpans.v1"
stride = 96
window = 128

----------------------------------------

TITLE: Creating Emoji Sentiment Analyzer with Custom Attributes in spaCy
DESCRIPTION: Implements a sentiment analyzer that adjusts document sentiment based on emoji patterns. It uses custom span extensions to store emoji descriptions and a callback function to adjust sentiment scores when matches are found.

LANGUAGE: python
CODE:
import emoji  # Installation: pip install emoji
from spacy.tokens import Span  # Get the global Span object

Span.set_extension("emoji_desc", default=None)  # Register the custom attribute

def label_sentiment(matcher, doc, i, matches):
    match_id, start, end = matches[i]
    if doc.vocab.strings[match_id] == "HAPPY":  # Don't forget to get string!
        doc.sentiment += 0.1  # Add 0.1 for positive sentiment
    elif doc.vocab.strings[match_id] == "SAD":
        doc.sentiment -= 0.1  # Subtract 0.1 for negative sentiment
    span = doc[start:end]
    # Verify if it is an emoji and set the extension attribute correctly.
    if emoji.is_emoji(span[0].text):
        span._.emoji_desc = emoji.demojize(span[0].text, delimiters=("", ""), language=doc.lang_).replace("_", " ")

----------------------------------------

TITLE: Merging Tokens with Retokenizer in spaCy
DESCRIPTION: Shows how to merge tokens in a Doc using the retokenize context manager, including setting linguistic attributes on the resulting merged token.

LANGUAGE: python
CODE:
doc = nlp("I like David Bowie")
with doc.retokenize() as retokenizer:
    attrs = {"LEMMA": "David Bowie"}
    retokenizer.merge(doc[2:4], attrs=attrs)

----------------------------------------

TITLE: Adding Hugging Face Hub Push Command to spaCy Project
DESCRIPTION: YAML configuration for adding a 'push_to_hub' command to a spaCy project configuration file. This allows automatic or manual uploading of trained models to Hugging Face Hub.

LANGUAGE: yaml
CODE:
- name: "push_to_hub"
  help: "Upload the trained model to the Hugging Face Hub"
  script:
    - "python -m spacy huggingface-hub push packages/en_${vars.name}-${vars.version}/dist/en_${vars.name}-${vars.version}-py3-none-any.whl"
  deps:
    - "packages/en_${vars.name}-${vars.version}/dist/en_${vars.name}-${vars.version}-py3-none-any.whl"

----------------------------------------

TITLE: Defining Project Commands in YAML Configuration
DESCRIPTION: YAML configuration that defines a preprocessing command with scripts to execute, dependencies required, and expected outputs. This enables dependency tracking between commands.

LANGUAGE: yaml
CODE:
commands:
  - name: preprocess
    help: "Convert the input data to spaCy's format"
    script:
      - 'python -m spacy convert assets/train.conllu corpus/'
      - 'python -m spacy convert assets/eval.conllu corpus/'
    deps:
      - 'assets/train.conllu'
      - 'assets/eval.conllu'
    outputs:
      - 'corpus/train.spacy'
      - 'corpus/eval.spacy'

----------------------------------------

TITLE: Simplified TextCategorizer Configuration with BOW Model
DESCRIPTION: This snippet demonstrates how to swap the default ensemble architecture with a simpler bag-of-words model for text classification. It shows how to modify the config to use the TextCatBOW architecture instead of the default ensemble.

LANGUAGE: ini
CODE:
[components.textcat]
factory = "textcat"
labels = []

[components.textcat.model]
@architectures = "spacy.TextCatBOW.v3"
exclusive_classes = true
length = 262144
ngram_size = 1
no_output_layer = false
nO = null

----------------------------------------

TITLE: Loading Model from __init__.py in Pipeline Package
DESCRIPTION: A helper function to load a spaCy model from a pipeline package's __init__.py file, allowing for customization through overrides.

LANGUAGE: python
CODE:
from spacy.util import load_model_from_init_py

def load(**overrides):
    return load_model_from_init_py(__file__, **overrides)

----------------------------------------

TITLE: Excluding Fields During Doc Serialization in spaCy (Python)
DESCRIPTION: Demonstrates how to exclude specific data fields when serializing a Doc object to bytes or disk.

LANGUAGE: python
CODE:
data = doc.to_bytes(exclude=["text", "tensor"])
doc.from_disk("./doc.bin", exclude=["user_data"])

----------------------------------------

TITLE: Using the Transformer Component on a Document
DESCRIPTION: Example showing how to manually apply the transformer component to a spaCy document. This process typically happens automatically when processing text with the nlp pipeline.

LANGUAGE: python
CODE:
doc = nlp("This is a sentence.")
trf = nlp.add_pipe("transformer")
# This usually happens under the hood
processed = transformer(doc)

----------------------------------------

TITLE: Getting Lowest Common Ancestor Matrix with Doc.get_lca_matrix
DESCRIPTION: Shows how to calculate the lowest common ancestor matrix for tokens in a document. The matrix contains the integer indices of ancestors, or -1 if no common ancestor is found. This is useful for analyzing the syntactic structure of a sentence.

LANGUAGE: python
CODE:
doc = nlp("This is a test")
matrix = doc.get_lca_matrix()
# array([[0, 1, 1, 1], [1, 1, 1, 1], [1, 1, 2, 3], [1, 1, 3, 3]], dtype=int32)

----------------------------------------

TITLE: Configuring Tok2Vec Component with Embed and Encode Sublayers
DESCRIPTION: This snippet shows how to configure a tok2vec component with separate embed and encode sublayers. It demonstrates spaCy's approach to modular architecture design where sublayers can be independently configured.

LANGUAGE: ini
CODE:
[components.tok2vec]
factory = "tok2vec"

[components.tok2vec.model]
@architectures = "spacy.Tok2Vec.v2"

[components.tok2vec.model.embed]
@architectures = "spacy.MultiHashEmbed.v2"
# ...

[components.tok2vec.model.encode]
@architectures = "spacy.MaxoutWindowEncoder.v2"
# ...

----------------------------------------

TITLE: Example spaCy Pipeline Meta JSON Structure
DESCRIPTION: A complete example of a meta.json file for a spaCy pipeline, showing all the standard fields including language, version information, requirements, pipeline components, labels, and performance metrics.

LANGUAGE: json
CODE:
{
  "name": "example_pipeline",
  "lang": "en",
  "version": "1.0.0",
  "spacy_version": ">=3.0.0,<3.1.0",
  "parent_package": "spacy",
  "requirements": ["spacy-transformers>=1.0.0,<1.1.0"],
  "description": "Example pipeline for spaCy",
  "author": "You",
  "email": "you@example.com",
  "url": "https://example.com",
  "license": "CC BY-SA 3.0",
  "sources": [{ "name": "My Corpus", "license": "MIT" }],
  "vectors": { "width": 0, "vectors": 0, "keys": 0, "name": null },
  "pipeline": ["tok2vec", "ner", "textcat"],
  "labels": {
    "ner": ["PERSON", "ORG", "PRODUCT"],
    "textcat": ["POSITIVE", "NEGATIVE"]
  },
  "performance": {
    "ents_f": 82.7300930714,
    "ents_p": 82.135523614,
    "ents_r": 83.3333333333,
    "textcat_score": 88.364323811
  },
  "speed": { "cpu": 7667.8, "gpu": null, "nwords": 10329 },
  "spacy_git_version": "61dfdd9fb"
}

----------------------------------------

TITLE: Adding Components from Source Models in spaCy v3.0
DESCRIPTION: Shows how to add a component from an existing trained pipeline using the source parameter, which handles compatibility and configuration.

LANGUAGE: diff
CODE:
source_nlp = spacy.load("en_core_web_sm")
nlp = spacy.blank("en")
- ner = source_nlp.get_pipe("ner")
- nlp.add_pipe(ner)
+ nlp.add_pipe("ner", source=source_nlp)

----------------------------------------

TITLE: Initializing Multi-Language Support in spaCy
DESCRIPTION: Methods for creating a multi-language pipeline that can be trained on more than one language, useful for tasks like named entity recognition across languages.

LANGUAGE: python
CODE:
# Standard import
from spacy.lang.xx import MultiLanguage
nlp = MultiLanguage()

# With lazy-loading
nlp = spacy.blank("xx")

----------------------------------------

TITLE: Example Usage of spaCy Debug Data Command
DESCRIPTION: A simple example showing how to run the debug data command with a configuration file. This command will analyze the training data specified in the config file and provide detailed statistics and potential issues.

LANGUAGE: bash
CODE:
$ python -m spacy debug data ./config.cfg

----------------------------------------

TITLE: Converting Files to spaCy Training Format Command
DESCRIPTION: Command for converting various file formats into spaCy's binary training data format. Supports multiple converters and options for customizing the conversion process.

LANGUAGE: bash
CODE:
$ python -m spacy convert [input_file] [output_dir] [--converter] [--file-type] [--n-sents] [--seg-sents] [--base] [--morphology] [--merge-subtokens] [--ner-map] [--lang]

----------------------------------------

TITLE: Excluding Fields in spaCy Serialization (Python)
DESCRIPTION: Example showing how to exclude specific data fields during spaCy serialization. This demonstrates using the exclude parameter when saving or loading models to control which components are included.

LANGUAGE: python
CODE:
data = nlp.to_bytes(exclude=["tokenizer", "vocab"])
nlp.from_disk("/pipeline", exclude=["ner"])

----------------------------------------

TITLE: Initializing InMemoryLookupKB in Python
DESCRIPTION: Creates a new in-memory knowledge base with the specified vocabulary and entity vector length. The vocabulary object is shared with the rest of the spaCy pipeline.

LANGUAGE: python
CODE:
from spacy.kb import InMemoryLookupKB
vocab = nlp.vocab
kb = InMemoryLookupKB(vocab=vocab, entity_vector_length=64)

----------------------------------------

TITLE: Serializing Morphologizer to Disk in spaCy (Python)
DESCRIPTION: Saves the Morphologizer pipeline component to a directory on disk. The directory will be created if it doesn't exist.

LANGUAGE: python
CODE:
morphologizer = nlp.add_pipe("morphologizer")
morphologizer.to_disk("/path/to/morphologizer")

----------------------------------------

TITLE: Creating an optimizer for SpanResolver in spaCy
DESCRIPTION: Demonstrates how to create an optimizer specifically for the SpanResolver component using the create_optimizer method.

LANGUAGE: python
CODE:
span_resolver = nlp.add_pipe("experimental_span_resolver")
optimizer = span_resolver.create_optimizer()

----------------------------------------

TITLE: Configuring Sentence Spans in SpaCy Transformer Config
DESCRIPTION: Sample configuration for using sent_spans.v1 span getter, which processes documents by sentence boundaries. This requires sentence boundaries to be set beforehand, such as by the Sentencizer component.

LANGUAGE: ini
CODE:
[transformer.model.get_spans]
@span_getters = "spacy-transformers.sent_spans.v1"

----------------------------------------

TITLE: Constructing EntityLinker Component in spaCy
DESCRIPTION: Shows three different ways to instantiate an EntityLinker component: via add_pipe with default model, via add_pipe with custom model configuration, and by directly instantiating the EntityLinker class.

LANGUAGE: python
CODE:
# Construction via add_pipe with default model
entity_linker = nlp.add_pipe("entity_linker")

# Construction via add_pipe with custom model
config = {"model": {"@architectures": "my_el.v1"}}
entity_linker = nlp.add_pipe("entity_linker", config=config)

# Construction from class
from spacy.pipeline import EntityLinker
entity_linker = EntityLinker(nlp.vocab, model)

----------------------------------------

TITLE: Defining Entity Linking Candidate Attributes in spaCy
DESCRIPTION: A reference table that documents the attributes available on entity linking candidates in spaCy. It includes both string and numeric identifiers, alias information, probability metrics, and vector representations.

LANGUAGE: markdown
CODE:
| Name            | Description                                                              |
| --------------- | ------------------------------------------------------------------------ |
| `entity`        | The entity's unique KB identifier. ~~int~~                               |
| `entity_`       | The entity's unique KB identifier. ~~str~~                               |
| `alias`         | The alias or textual mention. ~~int~~                                    |
| `alias_`        | The alias or textual mention. ~~str~~                                    |
| `prior_prob`    | The prior probability of the `alias` referring to the `entity`. ~~long~~ |
| `entity_freq`   | The frequency of the entity in a typical corpus. ~~long~~                |
| `entity_vector` | The pretrained vector of the entity. ~~numpy.ndarray~~                   |

----------------------------------------

TITLE: Adding Individual Word Vectors to spaCy Vocabulary
DESCRIPTION: This example demonstrates how to add individual word vectors to a spaCy vocabulary. It creates random vectors for a few words and adds them to a new vocabulary using the set_vector method.

LANGUAGE: python
CODE:
from spacy.vocab import Vocab

vector_data = {
    "dog": numpy.random.uniform(-1, 1, (300,)),
    "cat": numpy.random.uniform(-1, 1, (300,)),
    "orange": numpy.random.uniform(-1, 1, (300,))
}
vocab = Vocab()
for word, vector in vector_data.items():
    vocab.set_vector(word, vector)

----------------------------------------

TITLE: Configuring Weights & Biases Logger in spaCy
DESCRIPTION: Configuration snippet for integrating Weights & Biases experiment tracking with spaCy training through the WandbLogger. It defines project name, config values to remove, and logging intervals.

LANGUAGE: ini
CODE:
[training.logger]
@loggers = "spacy.WandbLogger.v3"
project_name = "monitor_spacy_training"
remove_config_values = ["paths.train", "paths.dev", "corpora.train.path", "corpora.dev.path"]
log_dataset_dir = "corpus"
model_log_interval = 1000

----------------------------------------

TITLE: Retrieving Pipeline Components in spaCy
DESCRIPTION: Shows how to retrieve a pipeline component by name using the get_pipe method. This allows access to components for configuration or inspection.

LANGUAGE: python
CODE:
parser = nlp.get_pipe("parser")
custom_component = nlp.get_pipe("custom_component")

----------------------------------------

TITLE: Serializing StringStore to Bytes in Python
DESCRIPTION: Shows how to serialize the current state of a StringStore to a binary string.

LANGUAGE: python
CODE:
store_bytes = stringstore.to_bytes()

----------------------------------------

TITLE: Saving Doc Object to Disk with Doc.to_disk
DESCRIPTION: Demonstrates how to save a Doc object to disk. This allows persisting a Doc with all its annotations for later use, which is useful for caching processing results or sharing annotated documents.

LANGUAGE: python
CODE:
doc.to_disk("/path/to/doc")

----------------------------------------

TITLE: Configuring and Adding a Tagger Component in spaCy
DESCRIPTION: Example showing how to configure a tagger with default model settings and add it to the NLP pipeline.

LANGUAGE: python
CODE:
from spacy.pipeline.tagger import DEFAULT_TAGGER_MODEL
config = {"model": DEFAULT_TAGGER_MODEL}
nlp.add_pipe("tagger", config=config)

----------------------------------------

TITLE: Adding Labels to a Tagger in spaCy
DESCRIPTION: Demonstrates how to add custom labels to a spaCy tagger component. This is necessary when defining custom tags that aren't inferred from training data.

LANGUAGE: python
CODE:
tagger = nlp.add_pipe("tagger")
tagger.add_label("MY_LABEL")

----------------------------------------

TITLE: Configuring Tok2VecListener Architecture in spaCy
DESCRIPTION: Example configuration for the Tok2VecListener.v1 architecture which acts as a proxy between an upstream Tok2Vec component and downstream components, allowing for shared token representations across multiple components.

LANGUAGE: ini
CODE:
[components.tok2vec]
factory = "tok2vec"

[components.tok2vec.model]
@architectures = "spacy.HashEmbedCNN.v2"
width = 342

[components.tagger]
factory = "tagger"

[components.tagger.model]
@architectures = "spacy.Tagger.v2"

[components.tagger.model.tok2vec]
@architectures = "spacy.Tok2VecListener.v1"
width = ${components.tok2vec.model.width}

----------------------------------------

TITLE: Replacing Token-to-Vector Listeners in spaCy Pipeline Components
DESCRIPTION: Example showing how to replace listener layers of a pipeline component with a standalone copy of the token-to-vector layer. This prevents performance degradation when some components are frozen during training.

LANGUAGE: python
CODE:
nlp = spacy.load("en_core_web_sm")
nlp.replace_listeners("tok2vec", "tagger", ["model.tok2vec"])

LANGUAGE: ini
CODE:
[training]
frozen_components = ["tagger"]

[components]

[components.tagger]
source = "en_core_web_sm"
replace_listeners = ["model.tok2vec"]

----------------------------------------

TITLE: Splitting Tokens with Doc.retokenize in spaCy
DESCRIPTION: Demonstrates how to split a single token into multiple tokens in spaCy. The example splits 'NewYork' into 'New' and 'York', showing how to specify heads for maintaining the correct syntactic structure and setting attributes for the new tokens.

LANGUAGE: python
CODE:
import spacy
from spacy import displacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("I live in NewYork")
print("Before:", [token.text for token in doc])
displacy.render(doc)  # displacy.serve if you're not in a Jupyter environment

with doc.retokenize() as retokenizer:
    heads = [(doc[3], 1), doc[2]]
    attrs = {"POS": ["PROPN", "PROPN"], "DEP": ["pobj", "compound"]}
    retokenizer.split(doc[3], ["New", "York"], heads=heads, attrs=attrs)
print("After:", [token.text for token in doc])
displacy.render(doc)  # displacy.serve if you're not in a Jupyter environment

----------------------------------------

TITLE: Migrating Tokenizer Exceptions in spaCy v3.0
DESCRIPTION: Shows how to handle tokenizer exceptions that previously set attributes beyond ORTH and NORM by using the new AttributeRuler component.

LANGUAGE: diff
CODE:
nlp = spacy.blank("en")
- nlp.tokenizer.add_special_case("don't", [{"ORTH": "do"}, {"ORTH": "n't", "LEMMA": "not"}])
+ nlp.tokenizer.add_special_case("don't", [{"ORTH": "do"}, {"ORTH": "n't"}])
+ ruler = nlp.add_pipe("attribute_ruler")
+ ruler.add(patterns=[[{"ORTH": "n't"}]], attrs={"LEMMA": "not"})

----------------------------------------

TITLE: Merging Entities using Doc.retokenize
DESCRIPTION: This snippet shows how entities are merged into single tokens behind the scenes using the Doc.retokenize context manager. This makes working with multi-token entities easier in downstream processing.

LANGUAGE: python
CODE:
with doc.retokenize() as retokenizer:
  for ent in doc.ents:
      retokenizer.merge(ent)

----------------------------------------

TITLE: Running spaCy Debug Profile Command
DESCRIPTION: Command to profile which functions take the most time in a spaCy pipeline. The input should be formatted as one JSON object per line with a 'text' key. If no input is specified, the IMDB dataset is loaded.

LANGUAGE: bash
CODE:
$ python -m spacy debug profile [model] [inputs] [--n-texts]

----------------------------------------

TITLE: Initializing DependencyParser Component in spaCy
DESCRIPTION: Shows how to initialize a dependency parser component with example data and a reference to the nlp object. Also includes a configuration example that shows how to define parser initialization in a config file.

LANGUAGE: python
CODE:
parser = nlp.add_pipe("parser")
parser.initialize(lambda: examples, nlp=nlp)

LANGUAGE: ini
CODE:
### config.cfg
[initialize.components.parser]

[initialize.components.parser.labels]
@readers = "spacy.read_labels.v1"
path = "corpus/labels/parser.json

----------------------------------------

TITLE: Using DependencyParser's __call__ Method in SpaCy (Python)
DESCRIPTION: Example of applying the dependency parser to a single document. The parser modifies the document in place, adding dependency relationship annotations between tokens in the sentence.

LANGUAGE: python
CODE:
doc = nlp("This is a sentence.")
parser = nlp.add_pipe("parser")
# This usually happens under the hood
processed = parser(doc)

----------------------------------------

TITLE: Adding Rules with Callback to DependencyMatcher in spaCy (Python)
DESCRIPTION: Demonstrates how to add a rule to a DependencyMatcher with a callback function. The callback function will be called when the pattern matches, receiving the matcher object, document, ID, and matches as arguments.

LANGUAGE: python
CODE:
def on_match(matcher, doc, id, matches):
    print('Matched!', matches)

matcher = DependencyMatcher(nlp.vocab)
matcher.add("FOUNDED", patterns, on_match=on_match)

----------------------------------------

TITLE: Creating a Custom Evaluation Script with Typer for spaCy Projects
DESCRIPTION: A Python script that uses Typer for command-line argument handling in a spaCy project. The script defines parameters for batch size, model path, and data path that can be passed from the project.yml configuration.

LANGUAGE: python
CODE:
import typer

def custom_evaluation(batch_size: int = 128, model_path: str, data_path: str):
    # The arguments are now available as positional CLI arguments
    print(batch_size, model_path, data_path)

if __name__ == "__main__":
    typer.run(custom_evaluation)

----------------------------------------

TITLE: Initializing DocBin Objects in Python
DESCRIPTION: Creates a DocBin object to hold serialized annotations for spaCy Doc objects. Users can specify which attributes to serialize and whether to include user data.

LANGUAGE: python
CODE:
from spacy.tokens import DocBin
doc_bin = DocBin(attrs=["ENT_IOB", "ENT_TYPE"])

----------------------------------------

TITLE: Checking for overlapping spans in a SpanGroup
DESCRIPTION: Demonstrates how to check if spans within a SpanGroup overlap with each other using the has_overlap property.

LANGUAGE: python
CODE:
doc = nlp("Their goi ng home")
doc.spans["errors"] = [doc[0:1], doc[1:3]]
assert not doc.spans["errors"].has_overlap
doc.spans["errors"].append(doc[2:4])
assert doc.spans["errors"].has_overlap

----------------------------------------

TITLE: Accessing Reference Docs and Gold Labels in spaCy
DESCRIPTION: Demonstrates accessing the reference Doc objects from Examples to extract gold-standard category labels for training a text classifier, showing how to compile gold standard data.

LANGUAGE: python
CODE:
for i, eg in enumerate(examples):
    for j, label in enumerate(all_labels):
        gold_labels[i][j] = eg.reference.cats.get(label, 0.0)

----------------------------------------

TITLE: Rehearsing a Pipeline Component to Prevent Catastrophic Forgetting
DESCRIPTION: Shows how to use the experimental rehearse method to update a model with a batch of data. This helps prevent catastrophic forgetting by teaching the current model to make predictions similar to an initial model.

LANGUAGE: python
CODE:
pipe = nlp.add_pipe("your_custom_pipe")
optimizer = nlp.resume_training()
losses = pipe.rehearse(examples, sgd=optimizer)

----------------------------------------

TITLE: Excluding Fields During DependencyParser Serialization in spaCy
DESCRIPTION: Demonstrates how to exclude specific fields when serializing a parser to disk. This can be useful to reduce size or avoid overwriting certain data.

LANGUAGE: python
CODE:
data = parser.to_disk("/path", exclude=["vocab"])

----------------------------------------

TITLE: Using the Dependency Matcher for Semantic Pattern Matching
DESCRIPTION: Demonstrates how to use the DependencyMatcher to find patterns within dependency parse trees. The example shows creating a pattern to match 'founded' relationships where the subject is connected via a subject dependency relation.

LANGUAGE: python
CODE:
from spacy.matcher import DependencyMatcher

matcher = DependencyMatcher(nlp.vocab)
pattern = [
    {"RIGHT_ID": "anchor_founded", "RIGHT_ATTRS": {"ORTH": "founded"}},
    {"LEFT_ID": "anchor_founded", "REL_OP": ">", "RIGHT_ID": "subject", "RIGHT_ATTRS": {"DEP": "nsubj"}}
]
matcher.add("FOUNDED", [pattern])

----------------------------------------

TITLE: Serving displaCy Visualizations in Browser
DESCRIPTION: Creates a web server to display dependency parse trees or named entity visualizations in a browser. Takes document objects and visualization options as parameters.

LANGUAGE: python
CODE:
import spacy
from spacy import displacy
nlp = spacy.load("en_core_web_sm")
doc1 = nlp("This is a sentence.")
doc2 = nlp("This is another sentence.")
displacy.serve([doc1, doc2], style="dep")

----------------------------------------

TITLE: Checking vocabulary size in spaCy
DESCRIPTION: Gets the current number of lexemes in the vocabulary using the len() function. This demonstrates the __len__ method of the Vocab class.

LANGUAGE: python
CODE:
doc = nlp("This is a sentence.")
assert len(nlp.vocab) > 0

----------------------------------------

TITLE: Configuring Base Model Copying in spaCy Initialization
DESCRIPTION: Configuration for copying tokenizer and vocabulary from existing models before initialization. This is useful for fine-tuning existing pipelines while preserving their tokenization behavior and vocabulary.

LANGUAGE: ini
CODE:
[initialize.before_init]
@callbacks = "spacy.copy_from_base_model.v1"
tokenizer = "en_core_sci_md"
vocab = "en_core_sci_md"

----------------------------------------

TITLE: Visualizing Matched Sentences with displaCy in spaCy
DESCRIPTION: Complete example that finds sentences containing 'Facebook is/was' + adjective patterns and visualizes them using displaCy. The code collects matched sentences and highlights the matched spans within the sentence context.

LANGUAGE: python
CODE:
import spacy
from spacy import displacy
from spacy.matcher import Matcher

nlp = spacy.load("en_core_web_sm")
matcher = Matcher(nlp.vocab)
matched_sents = []  # Collect data of matched sentences to be visualized

def collect_sents(matcher, doc, i, matches):
    match_id, start, end = matches[i]
    span = doc[start:end]  # Matched span
    sent = span.sent  # Sentence containing matched span
    # Append mock entity for match in displaCy style to matched_sents
    # get the match span by ofsetting the start and end of the span with the
    # start and end of the sentence in the doc
    match_ents = [{
        "start": span.start_char - sent.start_char,
        "end": span.end_char - sent.start_char,
        "label": "MATCH",
    }]
    matched_sents.append({"text": sent.text, "ents": match_ents})

pattern = [{"LOWER": "facebook"}, {"LEMMA": "be"}, {"POS": "ADV", "OP": "*"},
           {"POS": "ADJ"}]
matcher.add("FacebookIs", [pattern], on_match=collect_sents)  # add pattern
doc = nlp("I'd say that Facebook is evil. – Facebook is pretty cool, right?")
matches = matcher(doc)

# Serve visualization of sentences containing match with displaCy
# set manual=True to make displaCy render straight from a dictionary
# (if you're not running the code within a Jupyter environment, you can
# use displacy.serve instead)
displacy.render(matched_sents, style="ent", manual=True)

----------------------------------------

TITLE: Implementing a Custom Knowledge Base by Extending the KnowledgeBase Class in Python
DESCRIPTION: Example showing how to create a custom implementation of the KnowledgeBase abstract class by defining a subclass and initializing it with a vocabulary and entity vector length.

LANGUAGE: python
CODE:
from spacy.kb import KnowledgeBase
from spacy.vocab import Vocab

class FullyImplementedKB(KnowledgeBase):
  def __init__(self, vocab: Vocab, entity_vector_length: int):
      super().__init__(vocab, entity_vector_length)
      ...
vocab = nlp.vocab
kb = FullyImplementedKB(vocab=vocab, entity_vector_length=64)

----------------------------------------

TITLE: Merging DocBin Objects in Python
DESCRIPTION: Shows how to extend a DocBin with annotations from another DocBin. Both DocBin objects must have the same pre-defined attributes.

LANGUAGE: python
CODE:
doc_bin1 = DocBin(attrs=["LEMMA", "POS"])
doc_bin1.add(nlp("Hello world"))
doc_bin2 = DocBin(attrs=["LEMMA", "POS"])
doc_bin2.add(nlp("This is a sentence"))
doc_bin1.merge(doc_bin2)
assert len(doc_bin1) == 2

----------------------------------------

TITLE: Excluding Fields During Tagger Serialization in spaCy
DESCRIPTION: Shows how to exclude specific data fields when serializing a tagger component, which can be useful to reduce file size or avoid duplicating shared resources.

LANGUAGE: python
CODE:
data = tagger.to_disk("/path", exclude=["vocab"])

----------------------------------------

TITLE: Initializing a Tagger Component in spaCy
DESCRIPTION: Examples demonstrating three different ways to initialize a tagger component: via add_pipe with default model, via add_pipe with custom model configuration, and direct instantiation from the Tagger class.

LANGUAGE: python
CODE:
# Construction via add_pipe with default model
tagger = nlp.add_pipe("tagger")

# Construction via create_pipe with custom model
config = {"model": {"@architectures": "my_tagger"}}
tagger = nlp.add_pipe("tagger", config=config)

# Construction from class
from spacy.pipeline import Tagger
tagger = Tagger(nlp.vocab, model)

----------------------------------------

TITLE: Saving Vectors to Disk in spaCy (Python)
DESCRIPTION: Demonstrates how to save the current state of vectors to a directory using the to_disk method.

LANGUAGE: python
CODE:
vectors.to_disk("/path/to/vectors")


----------------------------------------

TITLE: Initializing EntityRecognizer Component in spaCy
DESCRIPTION: Demonstrates how to initialize the EntityRecognizer component with example data. The component is added to the pipeline and then initialized with a function that returns training examples.

LANGUAGE: python
CODE:
ner = nlp.add_pipe("ner")
ner.initialize(lambda: examples, nlp=nlp)

LANGUAGE: ini
CODE:
### config.cfg
[initialize.components.ner]

[initialize.components.ner.labels]
@readers = "spacy.read_labels.v1"
path = "corpus/labels/ner.json

----------------------------------------

TITLE: Applying Tok2Vec to a Document in spaCy
DESCRIPTION: Example showing how to apply the Tok2Vec component to a single document, which adds context-sensitive embeddings to the Doc.tensor attribute for use by downstream components.

LANGUAGE: python
CODE:
doc = nlp("This is a sentence.")
tok2vec = nlp.add_pipe("tok2vec")
# This usually happens under the hood
processed = tok2vec(doc)

----------------------------------------

TITLE: Accessing Rightward Children in spaCy
DESCRIPTION: Example showing how to access a token's rightward immediate children in the dependency parse using the rights property. It returns tokens to the right of the current token whose head is the current token.

LANGUAGE: python
CODE:
doc = nlp("I like New York in Autumn.")
rights = [t.text for t in doc[3].rights]
assert rights == ["in"]

----------------------------------------

TITLE: Using Matcher and PhraseMatcher for Rule-based Matching
DESCRIPTION: Demonstrates how to use the revised matcher API and the PhraseMatcher class for efficient pattern matching. Shows pattern definition and adding patterns to matchers with optional callbacks, enabling powerful pattern-specific processing.

LANGUAGE: python
CODE:
from spacy.matcher import Matcher, PhraseMatcher

matcher = Matcher(nlp.vocab)
matcher.add('HEARTS', None, [{"ORTH": "❤️", "OP": '+'}])

phrasematcher = PhraseMatcher(nlp.vocab)
phrasematcher.add("OBAMA", None, nlp("Barack Obama"))

----------------------------------------

TITLE: Configuring spacy.NER.v1 Task in INI format
DESCRIPTION: Basic configuration for the spacy.NER.v1 task component. This original version supports both zero-shot and few-shot prompting for named entity recognition.

LANGUAGE: ini
CODE:
[components.llm.task]
@llm_tasks = "spacy.NER.v1"
labels = PERSON,ORGANISATION,LOCATION
examples = null

----------------------------------------

TITLE: Creating a Custom Span Getter in Python for SpaCy Transformers
DESCRIPTION: Example code showing how to create and register a custom span getter function that extracts sentence spans from documents. This implementation returns a list of sentence spans for each document in the batch.

LANGUAGE: python
CODE:
@spacy.registry.span_getters("custom_sent_spans")
def configure_get_sent_spans() -> Callable:
    def get_sent_spans(docs: Iterable[Doc]) -> List[List[Span]]:
        return [list(doc.sents) for doc in docs]

    return get_sent_spans

----------------------------------------

TITLE: Adding labels to SpanCategorizer in spaCy (Python)
DESCRIPTION: Example showing how to manually add a new label to the SpanCategorizer component, which should be done before initialization.

LANGUAGE: python
CODE:
spancat = nlp.add_pipe("spancat")
spancat.add_label("MY_LABEL")

----------------------------------------

TITLE: Benchmarking spaCy Pipeline Accuracy
DESCRIPTION: Syntax for the spaCy benchmark accuracy command. This command evaluates the accuracy of a trained pipeline against evaluation data, with options for preprocessing, visualization, and metrics export.

LANGUAGE: bash
CODE:
$ python -m spacy benchmark accuracy [model] [data_path] [--output] [--code] [--gold-preproc] [--gpu-id] [--displacy-path] [--displacy-limit] [--per-component] [--spans-key]

----------------------------------------

TITLE: Checking Token Flags in spaCy
DESCRIPTION: Example showing how to check boolean flags on a token. The example checks if the token 'Give' is titlecased using the IS_TITLE flag.

LANGUAGE: python
CODE:
from spacy.attrs import IS_TITLE
doc = nlp("Give it back! He pleaded.")
token = doc[0]
assert token.check_flag(IS_TITLE) == True

----------------------------------------

TITLE: Retrieving Documents from DocBin in Python
DESCRIPTION: Demonstrates how to recover Doc objects from a DocBin's annotations using a shared vocabulary.

LANGUAGE: python
CODE:
docs = list(doc_bin.get_docs(nlp.vocab))

----------------------------------------

TITLE: Processing Document Stream with TextCategorizer
DESCRIPTION: Example demonstrating how to apply the TextCategorizer to a stream of documents with batching, which typically happens under the hood during normal pipeline processing.

LANGUAGE: python
CODE:
textcat = nlp.add_pipe("textcat")
for doc in textcat.pipe(docs, batch_size=50):
    pass

----------------------------------------

TITLE: Using Transformer.pipe Method in spaCy
DESCRIPTION: Applies the transformer pipeline component to a stream of documents with batching capabilities. This example shows how to process multiple documents through the transformer component.

LANGUAGE: python
CODE:
trf = nlp.add_pipe("transformer")
for doc in trf.pipe(docs, batch_size=50):
    pass

----------------------------------------

TITLE: Resetting word vectors in spaCy Vocab
DESCRIPTION: Drops the current vector table to change the vector dimensions. Required when you need to modify the vector width or shape.

LANGUAGE: python
CODE:
nlp.vocab.reset_vectors(width=300)

----------------------------------------

TITLE: Concatenating Multiple Docs with Doc.from_docs
DESCRIPTION: Shows how to concatenate multiple Doc objects into a single new Doc. This static method preserves sentence boundaries, named entities, and other linguistic annotations from the source documents.

LANGUAGE: python
CODE:
from spacy.tokens import Doc
texts = ["London is the capital of the United Kingdom.",
         "The River Thames flows through London.",
         "The famous Tower Bridge crosses the River Thames."]
docs = list(nlp.pipe(texts))
c_doc = Doc.from_docs(docs)
assert str(c_doc) == " ".join(texts)
assert len(list(c_doc.sents)) == len(docs)
assert [str(ent) for ent in c_doc.ents] == \
       [str(ent) for doc in docs for ent in doc.ents]

----------------------------------------

TITLE: Loading and Saving EntityRuler Patterns
DESCRIPTION: Code showing how to save patterns to disk and load them into a new EntityRuler component. The patterns are stored in a JSONL file format.

LANGUAGE: python
CODE:
ruler.to_disk("./patterns.jsonl")
new_ruler = nlp.add_pipe("entity_ruler").from_disk("./patterns.jsonl")

----------------------------------------

TITLE: Example of Initializing a spaCy Config File
DESCRIPTION: Practical example of creating a config file for English language with named entity recognition and text categorization components, optimized for accuracy.

LANGUAGE: bash
CODE:
$ python -m spacy init config config.cfg --lang en --pipeline ner,textcat --optimize accuracy

----------------------------------------

TITLE: Updating SpanCategorizer model in spaCy (Python)
DESCRIPTION: Example showing how to update the SpanCategorizer model by learning from examples and returning loss values.

LANGUAGE: python
CODE:
spancat = nlp.add_pipe("spancat")
optimizer = nlp.initialize()
losses = spancat.update(examples, sgd=optimizer)

----------------------------------------

TITLE: Initializing SpanRuler Component in Python
DESCRIPTION: Demonstrates how to initialize a SpanRuler component with patterns. The initialize method is called by Language.initialize and can load patterns from configuration.

LANGUAGE: python
CODE:
span_ruler = nlp.add_pipe("span_ruler")
span_ruler.initialize(lambda: [], nlp=nlp, patterns=patterns)

----------------------------------------

TITLE: Translation Task Configuration in spacy-llm
DESCRIPTION: Configuration example for the Translation task that translates text from a source to target language. Shows basic INI configuration for the Translation.v1 task component with Spanish as the target language.

LANGUAGE: ini
CODE:
[components.llm.task]
@llm_tasks = "spacy.Translation.v1"
examples = null
target_lang = "Spanish"

----------------------------------------

TITLE: Accessing transformer outputs for a token in spaCy
DESCRIPTION: Example demonstrating how to access the last hidden layer state for a specific token from a document processed with CuratedTransformer. Shows how to access the transformer data stored in the Doc extension attribute.

LANGUAGE: python
CODE:
# Get the last hidden layer output for "is" (token index 1)
doc = nlp("This is a text.")
tensors = doc._.trf_data.last_hidden_layer_state[1]

----------------------------------------

TITLE: Changing the Output Dimension of EntityRecognizer Model
DESCRIPTION: Demonstrates how to change the output dimension of the EntityRecognizer's model. Care should be taken when resizing an already trained model to avoid catastrophic forgetting.

LANGUAGE: python
CODE:
ner = nlp.add_pipe("ner")
ner.set_output(512)

----------------------------------------

TITLE: Initializing SpanCategorizer in spaCy (Python)
DESCRIPTION: Example of how to add and initialize a span categorizer component to the spaCy pipeline. The initialize method sets up the model for training using example data.

LANGUAGE: python
CODE:
spancat = nlp.add_pipe("spancat")
spancat.initialize(lambda: examples, nlp=nlp)

----------------------------------------

TITLE: Processing Document Stream with SpanFinder
DESCRIPTION: Example demonstrating how to process a stream of documents with the SpanFinder component using the pipe method, which allows for batch processing.

LANGUAGE: python
CODE:
span_finder = nlp.add_pipe("span_finder")
for doc in span_finder.pipe(docs, batch_size=50):
    pass

----------------------------------------

TITLE: Aligning Tokenization Between Different Systems in Python with spaCy
DESCRIPTION: Demonstrates how to use spaCy's Alignment object to map tokens between different tokenization schemes. This code shows how to align tokens that are split differently (like contractions) and visualize the mapping between tokens in both directions.

LANGUAGE: python
CODE:
from spacy.training import Alignment

other_tokens = ["i", "listened", "to", "obama", "'", "s", "podcasts", "."]
spacy_tokens = ["i", "listened", "to", "obama", "'s", "podcasts", "."]
align = Alignment.from_strings(other_tokens, spacy_tokens)
print(f"a -> b, lengths: {align.x2y.lengths}")  # array([1, 1, 1, 1, 1, 1, 1, 1])
print(f"a -> b, mapping: {align.x2y.data}")  # array([0, 1, 2, 3, 4, 4, 5, 6]) : two tokens both refer to "'s"
print(f"b -> a, lengths: {align.y2x.lengths}")  # array([1, 1, 1, 1, 2, 1, 1])   : the token "'s" refers to two tokens
print(f"b -> a, mappings: {align.y2x.data}")  # array([0, 1, 2, 3, 4, 5, 6, 7])

----------------------------------------

TITLE: Validating spaCy Pipeline Compatibility
DESCRIPTION: Command for checking whether installed pipeline packages are compatible with the current version of spaCy. Useful after upgrading spaCy to ensure all packages can still be used.

LANGUAGE: bash
CODE:
$ python -m spacy validate

----------------------------------------

TITLE: Computing Vector Norms for Lexemes in Python
DESCRIPTION: Shows how to access the L2 norm of a lexeme's vector representation and compare norms between different lexemes.

LANGUAGE: python
CODE:
apple = nlp.vocab["apple"]
pasta = nlp.vocab["pasta"]
apple.vector_norm  # 7.1346845626831055
pasta.vector_norm  # 7.759851932525635
assert apple.vector_norm != pasta.vector_norm

----------------------------------------

TITLE: Example Vocabulary Data in JSONL Format
DESCRIPTION: An example of vocabulary data in JSONL format, showing lexical entries for the 20 most frequent words in English training data.

LANGUAGE: json
CODE:
{"lang": "en", "settings": {"oov_prob": -20.502029418945312}}
{"orth": "the", "id": 1, "lower": "the", "norm": "the", "shape": "xxx", "prefix": "t", "suffix": "he", "length": 3, "cluster": "3", "prob": -3.1063389778137207, "like_url": false, "like_num": false, "like_email": false, "is_alpha": true, "is_ascii": true, "is_digit": false, "is_lower": true, "is_title": false, "is_punct": false, "is_space": false, "is_stop": true, "is_oov": false, "is_quote": false, "is_left_punct": false, "is_right_punct": false, "is_bracket": false, "is_upper": false}
{"orth": "I", "id": 12, "lower": "i", "norm": "i", "shape": "X", "prefix": "I", "suffix": "I", "length": 1, "cluster": "8", "prob": -4.311729431152344, "like_url": false, "like_num": false, "like_email": false, "is_alpha": true, "is_ascii": true, "is_digit": false, "is_lower": false, "is_title": false, "is_punct": false, "is_space": false, "is_stop": true, "is_oov": false, "is_quote": false, "is_left_punct": false, "is_right_punct": false, "is_bracket": false, "is_upper": true}
{"orth": "to", "id": 2, "lower": "to", "norm": "to", "shape": "xx", "prefix": "t", "suffix": "to", "length": 2, "cluster": "8", "prob": -4.454267501831055, "like_url": false, "like_num": false, "like_email": false, "is_alpha": true, "is_ascii": true, "is_digit": false, "is_lower": true, "is_title": false, "is_punct": false, "is_space": false, "is_stop": true, "is_oov": false, "is_quote": false, "is_left_punct": false, "is_right_punct": false, "is_bracket": false, "is_upper": false}
{"orth": "of", "id": 5, "lower": "of", "norm": "of", "shape": "xx", "prefix": "o", "suffix": "of", "length": 2, "cluster": "9", "prob": -4.782127857208252, "like_url": false, "like_num": false, "like_email": false, "is_alpha": true, "is_ascii": true, "is_digit": false, "is_lower": true, "is_title": false, "is_punct": false, "is_space": false, "is_stop": true, "is_oov": false, "is_quote": false, "is_left_punct": false, "is_right_punct": false, "is_bracket": false, "is_upper": false}
{"orth": "and", "id": 8, "lower": "and", "norm": "and", "shape": "xxx", "prefix": "a", "suffix": "nd", "length": 3, "cluster": "5", "prob": -4.787770748138428, "like_url": false, "like_num": false, "like_email": false, "is_alpha": true, "is_ascii": true, "is_digit": false, "is_lower": true, "is_title": false, "is_punct": false, "is_space": false, "is_stop": true, "is_oov": false, "is_quote": false, "is_left_punct": false, "is_right_punct": false, "is_bracket": false, "is_upper": false}
{"orth": "a", "id": 3, "lower": "a", "norm": "a", "shape": "x", "prefix": "a", "suffix": "a", "length": 1, "cluster": "4", "prob": -4.865312099456787, "like_url": false, "like_num": false, "like_email": false, "is_alpha": true, "is_ascii": true, "is_digit": false, "is_lower": true, "is_title": false, "is_punct": false, "is_space": false, "is_stop": true, "is_oov": false, "is_quote": false, "is_left_punct": false, "is_right_punct": false, "is_bracket": false, "is_upper": false}
{"orth": "in", "id": 6, "lower": "in", "norm": "in", "shape": "xx", "prefix": "i", "suffix": "in", "length": 2, "cluster": "3", "prob": -5.11673641204834, "like_url": false, "like_num": false, "like_email": false, "is_alpha": true, "is_ascii": true, "is_digit": false, "is_lower": true, "is_title": false, "is_punct": false, "is_space": false, "is_stop": true, "is_oov": false, "is_quote": false, "is_left_punct": false, "is_right_punct": false, "is_bracket": false, "is_upper": false}
{"orth": "'", "id": 17, "lower": "'", "norm": "'", "shape": "'", "prefix": "'", "suffix": "'", "length": 1, "cluster": "15", "prob": -5.398352146148682, "like_url": false, "like_num": false, "like_email": false, "is_alpha": false, "is_ascii": true, "is_digit": false, "is_lower": false, "is_title": false, "is_punct": true, "is_space": false, "is_stop": false, "is_oov": false, "is_quote": true, "is_left_punct": false, "is_right_punct": true, "is_bracket": false, "is_upper": false}
{"orth": "that", "id": 7, "lower": "that", "norm": "that", "shape": "xxxx", "prefix": "t", "suffix": "hat", "length": 4, "cluster": "9", "prob": -5.49262285232544, "like_url": false, "like_num": false, "like_email": false, "is_alpha": true, "is_ascii": true, "is_digit": false, "is_lower": true, "is_title": false, "is_punct": false, "is_space": false, "is_stop": true, "is_oov": false, "is_quote": false, "is_left_punct": false, "is_right_punct": false, "is_bracket": false, "is_upper": false}
{"orth": ",", "id": 4, "lower": ",", "norm": ",", "shape": ",", "prefix": ",", "suffix": ",", "length": 1, "cluster": "5", "prob": -5.627888202667236, "like_url": false, "like_num": false, "like_email": false, "is_alpha": false, "is_ascii": true, "is_digit": false, "is_lower": false, "is_title": false, "is_punct": true, "is_space": false, "is_stop": false, "is_oov": false, "is_quote": false, "is_left_punct": false, "is_right_punct": false, "is_bracket": false, "is_upper": false}
{"orth": "is", "id": 11, "lower": "is", "norm": "is", "shape": "xx", "prefix": "i", "suffix": "is", "length": 2, "cluster": "9", "prob": -5.670463562011719, "like_url": false, "like_num": false, "like_email": false, "is_alpha": true, "is_ascii": true, "is_digit": false, "is_lower": true, "is_title": false, "is_punct": false, "is_space": false, "is_stop": true, "is_oov": false, "is_quote": false, "is_left_punct": false, "is_right_punct": false, "is_bracket": false, "is_upper": false}
{"orth": "was", "id": 14, "lower": "was", "norm": "was", "shape": "xxx", "prefix": "w", "suffix": "was", "length": 3, "cluster": "7", "prob": -5.753303527832031, "like_url": false, "like_num": false, "like_email": false, "is_alpha": true, "is_ascii": true, "is_digit": false, "is_lower": true, "is_title": false, "is_punct": false, "is_space": false, "is_stop": true, "is_oov": false, "is_quote": false, "is_left_punct": false, "is_right_punct": false, "is_bracket": false, "is_upper": false}
{"orth": "for", "id": 10, "lower": "for", "norm": "for", "shape": "xxx", "prefix": "f", "suffix": "for", "length": 3, "cluster": "4", "prob": -5.829161643981934, "like_url": false, "like_num": false, "like_email": false, "is_alpha": true, "is_ascii": true, "is_digit": false, "is_lower": true, "is_title": false, "is_punct": false, "is_space": false, "is_stop": true, "is_oov": false, "is_quote": false, "is_left_punct": false, "is_right_punct": false, "is_bracket": false, "is_upper": false}
{"orth": "it", "id": 13, "lower": "it", "norm": "it", "shape": "xx", "prefix": "i", "suffix": "it", "length": 2, "cluster": "10", "prob": -5.834608554840088, "like_url": false, "like_num": false, "like_email": false, "is_alpha": true, "is_ascii": true, "is_digit": false, "is_lower": true, "is_title": false, "is_punct": false, "is_space": false, "is_stop": true, "is_oov": false, "is_quote": false, "is_left_punct": false, "is_right_punct": false, "is_bracket": false, "is_upper": false}
{"orth": ".", "id": 9, "lower": ".", "norm": ".", "shape": ".", "prefix": ".", "suffix": ".", "length": 1, "cluster": "19", "prob": -5.89316987991333, "like_url": false, "like_num": false, "like_email": false, "is_alpha": false, "is_ascii": true, "is_digit": false, "is_lower": false, "is_title": false, "is_punct": true, "is_space": false, "is_stop": false, "is_oov": false, "is_quote": false, "is_left_punct": false, "is_right_punct": false, "is_bracket": false, "is_upper": false}
{"orth": "be", "id": 21, "lower": "be", "norm": "be", "shape": "xx", "prefix": "b", "suffix": "be", "length": 2, "cluster": "3", "prob": -6.087647438049316, "like_url": false, "like_num": false, "like_email": false, "is_alpha": true, "is_ascii": true, "is_digit": false, "is_lower": true, "is_title": false, "is_punct": false, "is_space": false, "is_stop": true, "is_oov": false, "is_quote": false, "is_left_punct": false, "is_right_punct": false, "is_bracket": false, "is_upper": false}
{"orth": "you", "id": 19, "lower": "you", "norm": "you", "shape": "xxx", "prefix": "y", "suffix": "you", "length": 3, "cluster": "7", "prob": -6.103323936462402, "like_url": false, "like_num": false, "like_email": false, "is_alpha": true, "is_ascii": true, "is_digit": false, "is_lower": true, "is_title": false, "is_punct": false, "is_space": false, "is_stop": true, "is_oov": false, "is_quote": false, "is_left_punct": false, "is_right_punct": false, "is_bracket": false, "is_upper": false}
{"orth": "s", "id": 18, "lower": "s", "norm": "s", "shape": "x", "prefix": "s", "suffix": "s", "length": 1, "cluster": "12", "prob": -6.201305866241455, "like_url": false, "like_num": false, "like_email": false, "is_alpha": true, "is_ascii": true, "is_digit": false, "is_lower": true, "is_title": false, "is_punct": false, "is_space": false, "is_stop": false, "is_oov": false, "is_quote": false, "is_left_punct": false, "is_right_punct": false, "is_bracket": false, "is_upper": false}
{"orth": "he", "id": 15, "lower": "he", "norm": "he", "shape": "xx", "prefix": "h", "suffix": "he", "length": 2, "cluster": "4", "prob": -6.207818508148193, "like_url": false, "like_num": false, "like_email": false, "is_alpha": true, "is_ascii": true, "is_digit": false, "is_lower": true, "is_title": false, "is_punct": false, "is_space": false, "is_stop": true, "is_oov": false, "is_quote": false, "is_left_punct": false, "is_right_punct": false, "is_bracket": false, "is_upper": false}
{"orth": "on", "id": 16, "lower": "on", "norm": "on", "shape": "xx", "prefix": "o", "suffix": "on", "length": 2, "cluster": "4", "prob": -6.219166278839111, "like_url": false, "like_num": false, "like_email": false, "is_alpha": true, "is_ascii": true, "is_digit": false, "is_lower": true, "is_title": false, "is_punct": false, "is_space": false, "is_stop": true, "is_oov": false, "is_quote": false, "is_left_punct": false, "is_right_punct": false, "is_bracket": false, "is_upper": false}

----------------------------------------

TITLE: Configuring spaCy MultiHashEmbed Architecture
DESCRIPTION: Configuration for the MultiHashEmbed architecture which embeds multiple lexical attributes using hash embedding and concatenates the results. It supports including static word vectors and can represent subword information.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.MultiHashEmbed.v2"
width = 64
attrs = ["NORM", "PREFIX", "SUFFIX", "SHAPE"]
rows = [2000, 1000, 1000, 1000]
include_static_vectors = true

----------------------------------------

TITLE: Managing Model Weights with Language.use_params in spaCy Python
DESCRIPTION: Context manager for temporarily replacing weights of models in the spaCy pipeline with provided parameters. After the context block, models revert to their original weights.

LANGUAGE: python
CODE:
with nlp.use_params(optimizer.averages):
    nlp.to_disk("/tmp/checkpoint")

----------------------------------------

TITLE: Saving spaCy Vocabulary to Disk
DESCRIPTION: Saves the current state of the vocabulary to a directory on disk. The specified path will be created if it doesn't exist. Can exclude specific serialization fields if needed.

LANGUAGE: python
CODE:
nlp.vocab.to_disk("/path/to/vocab")

----------------------------------------

TITLE: Finishing Model Update with an Optimizer
DESCRIPTION: Demonstrates how to update model parameters using the current parameter gradients with an optimizer. This typically calls the underlying model's finish_update method.

LANGUAGE: python
CODE:
pipe = nlp.add_pipe("your_custom_pipe")
optimizer = nlp.initialize()
losses = pipe.update(examples, sgd=None)
pipe.finish_update(sgd)

----------------------------------------

TITLE: Using Pickle with spaCy Doc Objects
DESCRIPTION: Demonstrates how to pickle spaCy Doc objects efficiently. Shows the importance of pickling multiple Doc objects together rather than separately to avoid duplicating the shared Vocab reference, significantly reducing the serialized data size.

LANGUAGE: python
CODE:
doc1 = nlp("Hello world")
doc2 = nlp("This is a test")

doc1_data = pickle.dumps(doc1)
doc2_data = pickle.dumps(doc2)
print(len(doc1_data) + len(doc2_data))  # 6636116 😞

doc_data = pickle.dumps([doc1, doc2])
print(len(doc_data))  # 3319761 😃

----------------------------------------

TITLE: Applying EntityLinker to a Single Document in spaCy
DESCRIPTION: Demonstrates how to apply the EntityLinker component to a single document, which happens automatically when processing text through the nlp pipeline but can also be done explicitly.

LANGUAGE: python
CODE:
doc = nlp("This is a sentence.")
entity_linker = nlp.add_pipe("entity_linker")
# This usually happens under the hood
processed = entity_linker(doc)

----------------------------------------

TITLE: Using spaCy CLI Training Command with ConsoleLogger
DESCRIPTION: Command line example of training a spaCy model using the config file, followed by the console output showing training progress with metrics like loss and accuracy across batches.

LANGUAGE: bash
CODE:
$ python -m spacy train config.cfg

LANGUAGE: bash
CODE:
ℹ Using CPU
ℹ Loading config and nlp from: config.cfg
ℹ Pipeline: ['tok2vec', 'tagger']
ℹ Start training
ℹ Training. Initial learn rate: 0.0
ℹ Saving results to training_log.jsonl

E     #        LOSS TOK2VEC   LOSS TAGGER   TAG_ACC   SCORE
---   ------   ------------   -----------   -------   ------
  0        0           0.00         86.20      0.22     0.00
  0      200           3.08      18968.78     34.00     0.34
  0      400          31.81      22539.06     33.64     0.34
  0      600          92.13      22794.91     43.80     0.44
  0      800         183.62      21541.39     56.05     0.56
  0     1000         352.49      25461.82     65.15     0.65
  0     1200         422.87      23708.82     71.84     0.72
  0     1400         601.92      24994.79     76.57     0.77
  0     1600         662.57      22268.02     80.20     0.80
  0     1800        1101.50      28413.77     82.56     0.83
  0     2000        1253.43      28736.36     85.00     0.85
  0     2200        1411.02      28237.53     87.42     0.87
  0     2400        1605.35      28439.95     88.70     0.89

----------------------------------------

TITLE: Adding a Label to a Pipeline Component
DESCRIPTION: Shows how to add a new label to a pipeline component to be predicted by the model. This method needs to be overwritten with a custom implementation for each component type.

LANGUAGE: python
CODE:
pipe = nlp.add_pipe("your_custom_pipe")
pipe.add_label("MY_LABEL")

----------------------------------------

TITLE: Getting Aligned Named Entity Recognition Tags in spaCy
DESCRIPTION: Demonstrates accessing BILUO-formatted named entity tags aligned to the predicted tokenization, showing how entity spans are mapped between different tokenizations.

LANGUAGE: python
CODE:
words = ["Mrs", "Smith", "flew", "to", "New York"]
doc = Doc(en_vocab, words=words)
entities = [(0, 9, "PERSON"), (18, 26, "LOC")]
gold_words = ["Mrs Smith", "flew", "to", "New", "York"]
example = Example.from_dict(doc, {"words": gold_words, "entities": entities})
ner_tags = example.get_aligned_ner()
assert ner_tags == ["B-PERSON", "L-PERSON", "O", "O", "U-LOC"]

----------------------------------------

TITLE: Loading Tok2Vec Component from Disk in Python
DESCRIPTION: Loads a previously serialized Tok2Vec component from disk. This method modifies the object in place and returns it. The path can be either a string or a Path-like object, and specific serialization fields can be excluded during loading.

LANGUAGE: python
CODE:
tok2vec = nlp.add_pipe("tok2vec")
tok2vec.from_disk("/path/to/tok2vec")

----------------------------------------

TITLE: Serializing TextCategorizer to Disk in Python
DESCRIPTION: Demonstrates how to save a TextCategorizer component to disk. The component is first added to the NLP pipeline, then serialized to the specified path.

LANGUAGE: python
CODE:
textcat = nlp.add_pipe("textcat")
textcat.to_disk("/path/to/textcat")

----------------------------------------

TITLE: Configuring Raw Task Component in spaCy
DESCRIPTION: Configuration for the spacy.Raw.v1 task component that allows sending arbitrary prompts to LLMs without specific parsing requirements. The response is stored in a custom Doc attribute.

LANGUAGE: ini
CODE:
[components.llm.task]
@llm_tasks = "spacy.Raw.v1"
examples = null

----------------------------------------

TITLE: Attaching Custom Data to Documents in spaCy
DESCRIPTION: Shows how to add custom attributes to Doc objects in spaCy v2.0 using the new extension system. This approach eliminates the need to convert Doc objects to arrays or custom containers.

LANGUAGE: diff
CODE:
- doc = nlp("This is a regular doc")
- doc_array = doc.to_array(["ORTH", "POS"])
- doc_with_meta = {"doc_array": doc_array, "meta": get_doc_meta(doc_array)}

+ Doc.set_extension("meta", getter=get_doc_meta)
+ doc_with_meta = nlp(u'This is a doc with meta data')
+ meta = doc._.meta

----------------------------------------

TITLE: Accessing Sentences in a Document with spaCy (Python)
DESCRIPTION: Demonstrates how to iterate over sentences in a document using the sents property, which returns Span objects representing each sentence.

LANGUAGE: python
CODE:
doc = nlp("This is a sentence. Here's another...")
sents = list(doc.sents)
assert len(sents) == 2
assert [s.root.text for s in sents] == ["is", "'s"]

----------------------------------------

TITLE: Accessing EntityRecognizer Labels in spaCy
DESCRIPTION: Demonstrates how to access the labels that have been added to an EntityRecognizer component using the labels property. This is useful for checking available entity types.

LANGUAGE: python
CODE:
ner.add_label("MY_LABEL")
assert "MY_LABEL" in ner.labels

----------------------------------------

TITLE: Using Label Data for Tagger Initialization in spaCy
DESCRIPTION: Demonstrates how to use pre-defined label data for initializing a tagger component, avoiding the need to add labels manually or infer them from training data.

LANGUAGE: python
CODE:
labels = tagger.label_data
tagger.initialize(lambda: [], nlp=nlp, labels=labels)

----------------------------------------

TITLE: Setting Multiple Entities in the Knowledge Base in Python
DESCRIPTION: Defines a complete list of entities in the knowledge base, along with their frequencies and vectors in a batch operation. This is more efficient than adding entities individually.

LANGUAGE: python
CODE:
kb.set_entities(entity_list=["Q42", "Q463035"], freq_list=[32, 111], vector_list=[vector1, vector2])

----------------------------------------

TITLE: Setting custom attributes on Span objects with spaCy extensions
DESCRIPTION: Demonstrates how to define a custom attribute on the Span class using set_extension with a getter function. The example creates a custom attribute that detects if city names are present in the span.

LANGUAGE: python
CODE:
from spacy.tokens import Span
city_getter = lambda span: any(city in span.text for city in ("New York", "Paris", "Berlin"))
Span.set_extension("has_city", getter=city_getter)
doc = nlp("I like New York in Autumn")
assert doc[1:4]._.has_city

----------------------------------------

TITLE: Accessing Transformer Hidden State Outputs for a Token in Python
DESCRIPTION: Example demonstrating how to access the last hidden layer output from a transformer model for a specific token in a document using the TransformerData object stored in Doc._.trf_data.

LANGUAGE: python
CODE:
# Get the last hidden layer output for "is" (token index 1)
doc = nlp("This is a text.")
indices = doc._.trf_data.align[1].data.flatten()
last_hidden_state = doc._.trf_data.model_output.last_hidden_state
dim = last_hidden_state.shape[-1]
tensors = last_hidden_state.reshape(-1, dim)[indices]

----------------------------------------

TITLE: Example of DVC Integration with spaCy Project
DESCRIPTION: Example showing how to initialize Git and DVC, then generate a DVC configuration for a spaCy project using the 'all' workflow. This creates a reproducible data pipeline configuration.

LANGUAGE: bash
CODE:
$ git init
$ dvc init
$ python -m spacy project dvc all

----------------------------------------

TITLE: Serializing a spaCy Language Model to Bytes
DESCRIPTION: Demonstrates how to serialize a spaCy language model (nlp) to binary data using the to_bytes method. This allows saving the current state of the model for later use.

LANGUAGE: python
CODE:
nlp_bytes = nlp.to_bytes()

----------------------------------------

TITLE: Adding multiple patterns with callbacks to PhraseMatcher (Python)
DESCRIPTION: Shows how to add multiple patterns with a callback function that executes when matches are found. This example adds patterns for 'Obama' and healthcare-related terms.

LANGUAGE: python
CODE:
def on_match(matcher, doc, id, matches):
    print('Matched!', matches)

matcher = PhraseMatcher(nlp.vocab)
matcher.add("OBAMA", [nlp("Barack Obama")], on_match=on_match)
matcher.add("HEALTH", [nlp("health care reform"), nlp("healthcare reform")], on_match=on_match)
doc = nlp("Barack Obama urges Congress to find courage to defend his healthcare reforms")
matches = matcher(doc)

----------------------------------------

TITLE: Configuring TextCatEnsemble.v2 Architecture in spaCy
DESCRIPTION: Configuration example for TextCatEnsemble.v2, a stacked ensemble combining a linear bag-of-words model with a neural network model. It includes settings for the linear model, token embedding, and encoding components.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.TextCatEnsemble.v2"
nO = null

[model.linear_model]
@architectures = "spacy.TextCatBOW.v3"
exclusive_classes = true
length = 262144
ngram_size = 1
no_output_layer = false

[model.tok2vec]
@architectures = "spacy.Tok2Vec.v2"

[model.tok2vec.embed]
@architectures = "spacy.MultiHashEmbed.v2"
width = 64
rows = [2000, 2000, 1000, 1000, 1000, 1000]
attrs = ["ORTH", "LOWER", "PREFIX", "SUFFIX", "SHAPE", "ID"]
include_static_vectors = false

[model.tok2vec.encode]
@architectures = "spacy.MaxoutWindowEncoder.v2"
width = ${model.tok2vec.embed.width}
window_size = 1
maxout_pieces = 3
depth = 2

----------------------------------------

TITLE: Changing Vector Operations in spaCy (Python)
DESCRIPTION: Changes the embedding matrix to use different Thinc ops by calling the to_ops method with the desired ops implementation.

LANGUAGE: python
CODE:
from thinc.api import NumpyOps

vectors.to_ops(NumpyOps())


----------------------------------------

TITLE: Configuring TextCategorizer with Default Model
DESCRIPTION: Example code showing how to configure and add the 'textcat' component to a spaCy pipeline using the default single textcat model.

LANGUAGE: python
CODE:
from spacy.pipeline.textcat import DEFAULT_SINGLE_TEXTCAT_MODEL
config = {
   "model": DEFAULT_SINGLE_TEXTCAT_MODEL,
}
nlp.add_pipe("textcat", config=config)

----------------------------------------

TITLE: Parsing Dependencies for Manual Visualization
DESCRIPTION: Converts a spaCy Doc object into a dependency parse dictionary format that can be used with displaCy's manual rendering mode.

LANGUAGE: python
CODE:
import spacy
from spacy import displacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("This is a sentence.")
deps_parse = displacy.parse_deps(doc)
html = displacy.render(deps_parse, style="dep", manual=True)

----------------------------------------

TITLE: Configuring StaticVectors in spaCy
DESCRIPTION: Example configuration for the StaticVectors.v2 architecture that embeds Doc objects with their vocab's vectors table. The configuration includes output dimension, vector dimension, dropout rate, and key attribute parameters, along with weight initialization.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.StaticVectors.v2"
nO = null
nM = null
dropout = 0.2
key_attr = "ORTH"

[model.init_W]
@initializers = "glorot_uniform_init.v1"

----------------------------------------

TITLE: Creating a Basic Dependency Matcher with an Anchor Token in spaCy
DESCRIPTION: This snippet shows how to create a simple dependency matcher pattern that identifies a single token 'founded' in a sentence. It loads a spaCy model, creates a matcher, defines a pattern with a unique identifier, and applies it to a document.

LANGUAGE: python
CODE:
import spacy
from spacy.matcher import DependencyMatcher

nlp = spacy.load("en_core_web_sm")
matcher = DependencyMatcher(nlp.vocab)
pattern = [
  {
    "RIGHT_ID": "anchor_founded",       # unique name
    "RIGHT_ATTRS": {"ORTH": "founded"}  # token pattern for "founded"
  }
]
matcher.add("FOUNDED", [pattern])
doc = nlp("Smith founded two companies.")
matches = matcher(doc)
print(matches) # [(4851363122962674176, [1])]

----------------------------------------

TITLE: Checking Vector Availability with Span.has_vector in spaCy
DESCRIPTION: Shows how to check if a span has an associated word vector. The example demonstrates that the span 'like apples' has a vector representation.

LANGUAGE: python
CODE:
doc = nlp("I like apples")
assert doc[1:].has_vector

----------------------------------------

TITLE: Setting Annotations with Transformer in spaCy
DESCRIPTION: Assigns extracted features to Doc objects after prediction. Writes TransformerData to the Doc._.trf_data attribute and calls any custom annotation callback if provided.

LANGUAGE: python
CODE:
trf = nlp.add_pipe("transformer")
scores = trf.predict(docs)
trf.set_annotations(docs, scores)

----------------------------------------

TITLE: Using Sentencizer to Detect Sentence Boundaries
DESCRIPTION: Example showing how to add the Sentencizer to a spaCy pipeline and use it to automatically segment a text into sentences. The resulting sentences can be accessed via the doc.sents iterator.

LANGUAGE: Python
CODE:
from spacy.lang.en import English

nlp = English()
nlp.add_pipe("sentencizer")
doc = nlp("This is a sentence. This is another sentence.")
assert len(list(doc.sents)) == 2

----------------------------------------

TITLE: Customizing Entity Visualization Options in spaCy
DESCRIPTION: Shows how to customize the entity visualizer by filtering entity types to display (only 'ORG' entities) and overriding the default colors with custom gradient styles.

LANGUAGE: python
CODE:
colors = {"ORG": "linear-gradient(90deg, #aa9cfc, #fc9ce7)"}
options = {"ents": ["ORG"], "colors": colors}
displacy.serve(doc, style="ent", options=options)

----------------------------------------

TITLE: Serializing a custom pipe to disk in spaCy
DESCRIPTION: Demonstrates how to save a custom pipeline component to disk using the to_disk method. This allows persistence of trained models or configured components for later use.

LANGUAGE: python
CODE:
pipe = nlp.add_pipe("your_custom_pipe")
pipe.to_disk("/path/to/pipe")

----------------------------------------

TITLE: Getting spaCy extension information in Python
DESCRIPTION: Shows how to retrieve information about a previously registered extension using get_extension. The method returns a tuple containing the default value and any associated methods or functions.

LANGUAGE: python
CODE:
from spacy.tokens import Span
Span.set_extension("is_city", default=False)
extension = Span.get_extension("is_city")
assert extension == (False, None, None, None)

----------------------------------------

TITLE: Running spaCy Debug Model Command
DESCRIPTION: Command to debug a Thinc Model by running it on sample text and checking how it updates its internal weights and parameters. This helps understand the internal structure and behavior of neural networks in spaCy components.

LANGUAGE: bash
CODE:
$ python -m spacy debug model [config_path] [component] [--layers] [--dimensions] [--parameters] [--gradients] [--attributes] [--print-step0] [--print-step1] [--print-step2] [--print-step3] [--gpu-id]

----------------------------------------

TITLE: Removing patterns from PhraseMatcher (Python)
DESCRIPTION: Demonstrates how to remove a pattern from the PhraseMatcher by specifying its match ID. A KeyError is raised if the pattern doesn't exist.

LANGUAGE: python
CODE:
matcher = PhraseMatcher(nlp.vocab)
matcher.add("OBAMA", [nlp("Barack Obama")])
assert "OBAMA" in matcher
matcher.remove("OBAMA")
assert "OBAMA" not in matcher

----------------------------------------

TITLE: Updating pipeline package version compatibility in meta.json
DESCRIPTION: Diff showing how to update the spaCy version requirements in meta.json for custom pipelines that have been confirmed to work with v3.6.

LANGUAGE: diff
CODE:
- "spacy_version": ">=3.5.0,<3.6.0",
+ "spacy_version": ">=3.5.0,<3.7.0",

----------------------------------------

TITLE: Replacing Pipeline Components in spaCy
DESCRIPTION: Demonstrates how to replace an existing pipeline component with a new one. In v3, this method requires the name of a registered component factory rather than a callable.

LANGUAGE: python
CODE:
new_parser = nlp.replace_pipe("parser", "my_custom_parser")

----------------------------------------

TITLE: Processing a Document with SpanFinder
DESCRIPTION: Example showing how to process a single document with the SpanFinder component, which modifies the document in place and returns it.

LANGUAGE: python
CODE:
doc = nlp("This is a sentence.")
span_finder = nlp.add_pipe("span_finder")
# This usually happens under the hood
processed = span_finder(doc)

----------------------------------------

TITLE: Accessing Named Entities in a spaCy Doc
DESCRIPTION: Shows how to access named entities (ents) from a Doc object after named entity recognition has been applied, including getting entity text and labels.

LANGUAGE: python
CODE:
doc = nlp("Mr. Best flew to New York on Saturday morning.")
ents = list(doc.ents)
assert ents[0].label_ == "PERSON"
assert ents[0].text == "Mr. Best"

----------------------------------------

TITLE: Serializing spaCy Vectors to Binary
DESCRIPTION: Demonstrates how to serialize a Vectors object to a binary string using the to_bytes() method.

LANGUAGE: python
CODE:
vectors_bytes = vectors.to_bytes()

----------------------------------------

TITLE: Setting Entity Annotations in spaCy Docs
DESCRIPTION: Demonstrates how to modify a batch of Doc objects using pre-computed scores from the EntityRecognizer. This applies the entity predictions to the documents.

LANGUAGE: python
CODE:
ner = nlp.add_pipe("ner")
scores = ner.predict([doc1, doc2])
ner.set_annotations([doc1, doc2], scores)

----------------------------------------

TITLE: Named Entity Recognition Visualizer Data Structure in JSON
DESCRIPTION: Example JSON structure for entity visualization with displaCy. This format includes the full text and a list of entities with their character offsets and labels.

LANGUAGE: json
CODE:
{
  "text": "But Google is starting from behind.",
  "ents": [{ "start": 4, "end": 10, "label": "ORG" }]
}

----------------------------------------

TITLE: Parsing Document Spans with displaCy in Python
DESCRIPTION: Demonstrates how to use the parse_spans method to generate visualizable spans from a spaCy Doc object. This example loads a model, creates a Doc with a custom span group, parses it for visualization, and renders it with the span style.

LANGUAGE: python
CODE:
import spacy
from spacy import displacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("But Google is starting from behind.")
doc.spans['orgs'] = [doc[1:2]]
ents_parse = displacy.parse_spans(doc, options={"spans_key" : "orgs"})
html = displacy.render(ents_parse, style="span", manual=True)

----------------------------------------

TITLE: Processing Document Stream with Tok2Vec Pipe Method
DESCRIPTION: Example demonstrating how to process a stream of documents with the Tok2Vec component using its pipe method with batching for efficiency.

LANGUAGE: python
CODE:
tok2vec = nlp.add_pipe("tok2vec")
for doc in tok2vec.pipe(docs, batch_size=50):
    pass

----------------------------------------

TITLE: Getting the number of rules in PhraseMatcher (Python)
DESCRIPTION: Shows how to check the number of rules (match IDs) added to the PhraseMatcher. This is useful for tracking how many distinct patterns have been added.

LANGUAGE: python
CODE:
matcher = PhraseMatcher(nlp.vocab)
assert len(matcher) == 0
matcher.add("OBAMA", [nlp("Barack Obama")])
assert len(matcher) == 1

----------------------------------------

TITLE: Implementing an Infinite Generator Corpus
DESCRIPTION: Python implementation of a custom corpus class that generates an infinite stream of examples with even or odd numbers for text classification. This example shows how to create a corpus that can be used for streaming training data indefinitely.

LANGUAGE: python
CODE:
from typing import Callable, Iterable, Iterator
from spacy import util
import random
from spacy.training import Example
from spacy import Language


@util.registry.readers("even_odd.v1")
def create_even_odd_corpus(limit: int = -1) -> Callable[[Language], Iterable[Example]]:
    return EvenOddCorpus(limit)


class EvenOddCorpus:
    def __init__(self, limit):
        self.limit = limit

    def __call__(self, nlp: Language) -> Iterator[Example]:
        i = 0
        while i < self.limit or self.limit < 0:
            r = random.randint(0, 1000)
            cat = r % 2 == 0
            text = "This is sentence " + str(r)
            yield Example.from_dict(
                nlp.make_doc(text), {"cats": {"EVEN": cat, "ODD": not cat}}
            )
            i += 1

----------------------------------------

TITLE: Accessing Span.root Property in spaCy
DESCRIPTION: Demonstrates how to access the root token of a span, which is the token with the shortest path to the root of the sentence. This example shows how 'York' is the root token of the 'New York' span.

LANGUAGE: python
CODE:
doc = nlp("I like New York in Autumn.")
i, like, new, york, in_, autumn, dot = range(len(doc))
assert doc[new].head.text == "York"
assert doc[york].head.text == "like"
new_york = doc[new:york+1]
assert new_york.root.text == "York"

----------------------------------------

TITLE: Azure OpenAI Configuration in spaCy
DESCRIPTION: Special configuration notes for using Azure OpenAI deployments with spaCy. Unlike other providers, Azure requires specifying deployment names rather than model names, along with a base URL, API version, and model type.

LANGUAGE: markdown
CODE:
**⚠️ A note on `spacy.Azure.v1`.** Working with Azure OpenAI is slightly
different than working with models from other providers:

- In Azure LLMs have to be made available by creating a _deployment_ of a given
  model (e. g. GPT-3.5). This deployment can have an arbitrary name. The `name`
  argument, which everywhere else denotes the model name (e. g. `claude-1.0`,
  `gpt-3.5`), here refers to the _deployment name_.
- Deployed Azure OpenAI models are reachable via a resource-specific base URL,
  usually of the form `https://{resource}.openai.azure.com`. Hence the URL has
  to be specified via the `base_url` argument.
- Azure further expects the _API version_ to be specified. The default value for
  this, via the `api_version` argument, is currently `2023-05-15` but may be
  updated in the future.
- Finally, since we can't infer information about the model from the deployment
  name, `spacy-llm` requires the `model_type` to be set to either
  `"completions"` or `"chat"`, depending on whether the deployed model is a
  completion or chat model.

----------------------------------------

TITLE: Configuring Pipeline Components with Settings in Config
DESCRIPTION: Shows how to configure components using the config file format in spaCy v3.0.

LANGUAGE: ini
CODE:
[components.sentencizer]
factory = "sentencizer"
punct_chars = ["!", ".", "?"]

----------------------------------------

TITLE: Setting a Boolean Flag on a Lexeme in Python
DESCRIPTION: Demonstrates how to add a custom flag to the vocabulary and set it on a lexeme. This example creates a flag and sets it to True for the 'spaCy' lexeme.

LANGUAGE: python
CODE:
COOL_FLAG = nlp.vocab.add_flag(lambda text: False)
nlp.vocab["spaCy"].set_flag(COOL_FLAG, True)

----------------------------------------

TITLE: Implementing a Basic Dependency Matcher in spaCy Python
DESCRIPTION: This example demonstrates how to create a DependencyMatcher that finds patterns where a subject is related to the word 'founded' and 'initially' precedes 'founded'. The pattern consists of an anchor token and its relationships to other tokens in the dependency tree.

LANGUAGE: python
CODE:
from spacy.matcher import DependencyMatcher

# "[subject] ... initially founded"
pattern = [
  # anchor token: founded
  {
    "RIGHT_ID": "founded",
    "RIGHT_ATTRS": {"ORTH": "founded"}
  },
  # founded -> subject
  {
    "LEFT_ID": "founded",
    "REL_OP": ">",
    "RIGHT_ID": "subject",
    "RIGHT_ATTRS": {"DEP": "nsubj"}
  },
  # "founded" follows "initially"
  {
    "LEFT_ID": "founded",
    "REL_OP": ";",
    "RIGHT_ID": "initially",
    "RIGHT_ATTRS": {"ORTH": "initially"}
  }
]

matcher = DependencyMatcher(nlp.vocab)
matcher.add("FOUNDED", [pattern])
matches = matcher(doc)

----------------------------------------

TITLE: Implementing Person Title Extension Attributes
DESCRIPTION: This example shows how to implement the extension attribute approach for person titles, registering the getter function on Span objects and accessing the custom 'person_title' attribute from entity spans.

LANGUAGE: python
CODE:
import spacy
from spacy.tokens import Span

nlp = spacy.load("en_core_web_sm")

def get_person_title(span):
    if span.label_ == "PERSON" and span.start != 0:
        prev_token = span.doc[span.start - 1]
        if prev_token.text in ("Dr", "Dr.", "Mr", "Mr.", "Ms", "Ms."):
            return prev_token.text

# Register the Span extension as 'person_title'
Span.set_extension("person_title", getter=get_person_title)

doc = nlp("Dr Alex Smith chaired first board meeting of Acme Corp Inc.")
print([(ent.text, ent.label_, ent._.person_title) for ent in doc.ents])

----------------------------------------

TITLE: Predicting Text Categories with spaCy
DESCRIPTION: Uses the TextCategorizer component to generate prediction scores for a batch of documents without modifying them.

LANGUAGE: python
CODE:
textcat = nlp.add_pipe("textcat")
scores = textcat.predict([doc1, doc2])

----------------------------------------

TITLE: Getting Token Extensions in spaCy
DESCRIPTION: Example showing how to look up a previously registered extension by name. The get_extension method returns a 4-tuple of (default, method, getter, setter) for the extension.

LANGUAGE: python
CODE:
from spacy.tokens import Token
Token.set_extension("is_fruit", default=False)
extension = Token.get_extension("is_fruit")
assert extension == (False, None, None, None)

----------------------------------------

TITLE: Accessing Pipeline Metadata in spaCy
DESCRIPTION: Shows how to access the meta property which contains metadata about the Language class or loaded pipeline, including name, version, and other information.

LANGUAGE: python
CODE:
print(nlp.meta)

----------------------------------------

TITLE: Accessing a Lexeme's Vector Representation in Python
DESCRIPTION: Demonstrates how to access the vector representation of a lexeme and verify its data type and shape.

LANGUAGE: python
CODE:
apple = nlp.vocab["apple"]
assert apple.vector.dtype == "float32"
assert apple.vector.shape == (300,)

----------------------------------------

TITLE: Configuring TorchBiLSTMEncoder in spaCy
DESCRIPTION: Example configuration for the TorchBiLSTMEncoder.v1 architecture that encodes context using bidirectional LSTM layers. The configuration specifies width, depth, and dropout parameters. This component requires PyTorch.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.TorchBiLSTMEncoder.v1"
width = 64
depth = 2
dropout = 0.0

----------------------------------------

TITLE: Checking EntityRuler Length in spaCy Python
DESCRIPTION: Demonstrates how to check the number of patterns in an EntityRuler using the len() method. This shows adding a pattern and verifying the count increases.

LANGUAGE: python
CODE:
ruler = nlp.add_pipe("entity_ruler")
assert len(ruler) == 0
ruler.add_patterns([{"label": "ORG", "pattern": "Apple"}])
assert len(ruler) == 1

----------------------------------------

TITLE: Initializing SpanRuler in spaCy
DESCRIPTION: Examples showing two ways to initialize the SpanRuler: via the add_pipe method or by direct instantiation from the class. The examples demonstrate basic usage patterns for setting up the SpanRuler component.

LANGUAGE: python
CODE:
# Construction via add_pipe
ruler = nlp.add_pipe("span_ruler")

# Construction from class
from spacy.pipeline import SpanRuler
ruler = SpanRuler(nlp, overwrite=True)

----------------------------------------

TITLE: Installing spaCy with CUDA and transformer support
DESCRIPTION: Command for installing spaCy with CUDA 11.3 support and transformer integration. This example sets the CUDA_PATH environment variable to a custom installation location.

LANGUAGE: bash
CODE:
$ export CUDA_PATH="/opt/nvidia/cuda"
$ pip install -U %%SPACY_PKG_NAME[cuda113,transformers]%%SPACY_PKG_FLAGS

----------------------------------------

TITLE: Deserializing Binary Data to spaCy Vectors
DESCRIPTION: Shows how to load a serialized Vectors object from binary data using from_bytes(). The example creates a new Vectors instance with a StringStore and populates it with the serialized data.

LANGUAGE: python
CODE:
from spacy.vectors import Vectors
vectors_bytes = vectors.to_bytes()
new_vectors = Vectors(StringStore())
new_vectors.from_bytes(vectors_bytes)

----------------------------------------

TITLE: Sentiment Analysis with Emoji Matching in spaCy
DESCRIPTION: Demonstrates how to perform basic sentiment analysis using emoji matching in social media posts. The code defines patterns for positive and negative emoji and adjusts a sentiment score based on matches.

LANGUAGE: python
CODE:
from spacy.lang.en import English
from spacy.matcher import Matcher

nlp = English()  # We only want the tokenizer, so no need to load a pipeline
matcher = Matcher(nlp.vocab)

pos_emoji = ["😀", "😃", "😂", "🤣", "😊", "😍"]  # Positive emoji
neg_emoji = ["😞", "😠", "😩", "😢", "😭", "😒"]  # Negative emoji

# Add patterns to match one or more emoji tokens
pos_patterns = [[{"ORTH": emoji}] for emoji in pos_emoji]
neg_patterns = [[{"ORTH": emoji}] for emoji in neg_emoji]

# Function to label the sentiment
def label_sentiment(matcher, doc, i, matches):
    match_id, start, end = matches[i]
    if doc.vocab.strings[match_id] == "HAPPY":  # Don't forget to get string!
        doc.sentiment += 0.1  # Add 0.1 for positive sentiment
    elif doc.vocab.strings[match_id] == "SAD":
        doc.sentiment -= 0.1  # Subtract 0.1 for negative sentiment

matcher.add("HAPPY", pos_patterns, on_match=label_sentiment)  # Add positive pattern
matcher.add("SAD", neg_patterns, on_match=label_sentiment)  # Add negative pattern

----------------------------------------

TITLE: Configuring Span Visualizer Options in spaCy
DESCRIPTION: Shows how to configure the span visualizer to use a specific spans key ('sc') when rendering document spans.

LANGUAGE: python
CODE:
options = {"spans_key": "sc"}
displacy.serve(doc, style="span", options=options)

----------------------------------------

TITLE: Using the 'find-threshold' CLI Command in spaCy v3.5
DESCRIPTION: The 'find-threshold' CLI identifies the optimal threshold value by running trials across threshold values from 0.0 to 1.0 for components like spancat and textcat_multilabel, using a specified score metric.

LANGUAGE: bash
CODE:
$ spacy find-threshold my_pipeline data.spacy spancat threshold spans_sc_f --n_trials 20

----------------------------------------

TITLE: Configuring Summarization Task Component in spaCy
DESCRIPTION: Configuration for the spacy.Summarization.v1 task component which creates document summaries. It supports both zero-shot and few-shot prompting with customizable parameters.

LANGUAGE: ini
CODE:
[components.llm.task]
@llm_tasks = "spacy.Summarization.v1"
examples = null
max_n_words = null

----------------------------------------

TITLE: Initializing CuratedTransformer Component in spaCy
DESCRIPTION: Initializes the CuratedTransformer component for training and returns an Optimizer. The get_examples function should return examples for initializing the model.

LANGUAGE: python
CODE:
trf = nlp.add_pipe("curated_transformer")
trf.initialize(lambda: examples, nlp=nlp)

----------------------------------------

TITLE: Initializing a Tagger Component in spaCy
DESCRIPTION: Demonstrates how to add a tagger component to an NLP pipeline and initialize it with training examples. The second snippet shows how to configure the tagger initialization in a config file.

LANGUAGE: python
CODE:
tagger = nlp.add_pipe("tagger")
tagger.initialize(lambda: examples, nlp=nlp)

LANGUAGE: ini
CODE:
### config.cfg
[initialize.components.tagger]

[initialize.components.tagger.labels]
@readers = "spacy.read_labels.v1"
path = "corpus/labels/tagger.json

----------------------------------------

TITLE: Initializing EntityRuler Component in spaCy Python
DESCRIPTION: Demonstrates how to initialize an EntityRuler component with patterns. The component is added to the pipeline and initialized with patterns.

LANGUAGE: python
CODE:
entity_ruler = nlp.add_pipe("entity_ruler")
entity_ruler.initialize(lambda: [], nlp=nlp, patterns=patterns)

----------------------------------------

TITLE: Creating a Custom Extension Attribute for Person Titles
DESCRIPTION: This function creates a getter for a custom extension attribute that identifies titles for PERSON entities without modifying the original entity span, useful for maintaining compatibility with knowledge base lookups.

LANGUAGE: python
CODE:
def get_person_title(span):
    if span.label_ == "PERSON" and span.start != 0:
        prev_token = span.doc[span.start - 1]
        if prev_token.text in ("Dr", "Dr.", "Mr", "Mr.", "Ms", "Ms."):
            return prev_token.text

----------------------------------------

TITLE: Retrieving Patterns with Matcher.get
DESCRIPTION: Shows how to retrieve the pattern stored for a key. Returns the rule as an (on_match, patterns) tuple containing the callback and available patterns.

LANGUAGE: python
CODE:
matcher.add("Rule", [[{"ORTH": "test"}]])
on_match, patterns = matcher.get("Rule")

----------------------------------------

TITLE: Adding Patterns to SpanRuler in Python
DESCRIPTION: Shows how to add both phrase patterns (strings) and token patterns (lists of dicts) to a SpanRuler using the add_patterns method.

LANGUAGE: python
CODE:
patterns = [
    {"label": "ORG", "pattern": "Apple"},
    {"label": "GPE", "pattern": [{"lower": "san"}, {"lower": "francisco"}]}
]
ruler = nlp.add_pipe("span_ruler")
ruler.add_patterns(patterns)

----------------------------------------

TITLE: Configuring TransformerModel in spaCy Pipeline
DESCRIPTION: Configuration for integrating a transformer model (RoBERTa base) into spaCy with strided span processing for handling longer documents. Includes settings for mixed precision training and tokenizer configuration.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy-transformers.TransformerModel.v3"
name = "roberta-base"
tokenizer_config = {"use_fast": true}
transformer_config = {}
mixed_precision = true
grad_scaler_config = {"init_scale": 32768}

[model.get_spans]
@span_getters = "spacy-transformers.strided_spans.v1"
window = 128
stride = 96

----------------------------------------

TITLE: Setting Custom Error Handlers in spaCy Pipelines
DESCRIPTION: Shows how to define a custom error handler callback for the spaCy pipeline using Language.set_error_handler. This handler is invoked when errors occur during document processing.

LANGUAGE: python
CODE:
def warn_error(proc_name, proc, docs, e):
    print(f"An error occurred when applying component {proc_name}.")

nlp.set_error_handler(warn_error)

----------------------------------------

TITLE: Configuring Tagger.v2 in spaCy
DESCRIPTION: Configuration example for the Tagger.v2 architecture, which adds a linear layer with softmax activation over token vectors to predict part-of-speech tags. The configuration specifies output size and normalization settings.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.Tagger.v2"
nO = null
normalize = false

[model.tok2vec]
# ...

----------------------------------------

TITLE: Initializing a spaCy Matcher
DESCRIPTION: Code to initialize a Matcher object from the spacy.matcher module. The matcher requires the vocabulary from the nlp pipeline to ensure compatibility with the documents it will process.

LANGUAGE: python
CODE:
from spacy.matcher import Matcher
matcher = Matcher(nlp.vocab)

----------------------------------------

TITLE: Converting MorphAnalysis to Dictionary in Python
DESCRIPTION: Demonstrates how to convert a MorphAnalysis object to a dictionary representation using the to_dict method.

LANGUAGE: python
CODE:
feats = "Feat1=Val1,Val2|Feat2=Val2"
morph = MorphAnalysis(nlp.vocab, feats)
assert morph.to_dict() == {"Feat1": "Val1,Val2", "Feat2": "Val2"}

----------------------------------------

TITLE: Examples of Initializing Chinese PKUSeg Tokenizer
DESCRIPTION: Examples demonstrating how to initialize the Chinese PKUSeg tokenizer with different models (spaCy's OntoNotes model, pkuseg's news model, or local models) and custom user dictionaries.

LANGUAGE: python
CODE:
# Initialize the pkuseg tokenizer
cfg = {"segmenter": "pkuseg"}
nlp = Chinese.from_config({"nlp": {"tokenizer": cfg}})

# Load spaCy's OntoNotes model
nlp.tokenizer.initialize(pkuseg_model="spacy_ontonotes")

# Load pkuseg's "news" model
nlp.tokenizer.initialize(pkuseg_model="news")

# Load local model
nlp.tokenizer.initialize(pkuseg_model="/path/to/pkuseg_model")

# Override the user directory
nlp.tokenizer.initialize(pkuseg_model="spacy_ontonotes", pkuseg_user_dict="/path/to/user_dict")

----------------------------------------

TITLE: Saving DocBin to Disk in Python
DESCRIPTION: Demonstrates how to save a serialized DocBin to a file, typically with a .spacy extension. The resulting file can be used as input for spaCy training.

LANGUAGE: python
CODE:
docs = [nlp("Hello world!")]
doc_bin = DocBin(docs=docs)
doc_bin.to_disk("./data.spacy")

----------------------------------------

TITLE: Serializing SpanCategorizer to bytes
DESCRIPTION: Demonstrates how to serialize a SpanCategorizer component to a bytestring. The method accepts an optional exclude parameter to specify serialization fields to exclude.

LANGUAGE: python
CODE:
spancat = nlp.add_pipe("spancat")
spancat_bytes = spancat.to_bytes()

----------------------------------------

TITLE: Checking if a Package is Installed via pip
DESCRIPTION: Validates if a string corresponds to a package installed via pip, commonly used to check if specific pipeline packages are available.

LANGUAGE: python
CODE:
util.is_package("en_core_web_sm") # True
util.is_package("xyz") # False

----------------------------------------

TITLE: Running spaCy Tests
DESCRIPTION: Commands to run spaCy's test suite, either after building from source or by running pytest on the installed package, which helps ensure that the installation is working correctly.

LANGUAGE: bash
CODE:
pip install -r requirements.txt
python -m pytest --pyargs spacy

----------------------------------------

TITLE: Loading an LLM Component from Bytes in spaCy
DESCRIPTION: Shows how to load a previously serialized LLM component from a bytestring using the from_bytes method. This modifies the component in place and returns it.

LANGUAGE: python
CODE:
ner_bytes = llm_ner.to_bytes()
llm_ner = nlp.add_pipe("llm_ner")
llm_ner.from_bytes(ner_bytes)

----------------------------------------

TITLE: Finding tokens by end character position in spaCy Doc objects
DESCRIPTION: Demonstrates how to use the token_by_end function to find a token in a TokenC* array by the offset of its final character. The function returns the index of the token or -1 if not found.

LANGUAGE: python
CODE:
from spacy.tokens.doc cimport Doc, token_by_end
from spacy.vocab cimport Vocab

doc = Doc(Vocab(), words=["hello", "world"])
assert token_by_end(doc.c, doc.length, 5) == 0
assert token_by_end(doc.c, doc.length, 1) == -1

----------------------------------------

TITLE: Calculating Loss for EntityRecognizer Training
DESCRIPTION: Demonstrates how to calculate the loss and gradient of loss for a batch of documents and their predicted scores. These values are used for model updating during training.

LANGUAGE: python
CODE:
ner = nlp.add_pipe("ner")
scores = ner.predict([eg.predicted for eg in examples])
loss, d_loss = ner.get_loss(examples, scores)

----------------------------------------

TITLE: Problematic Component Factory with Data Loading in spaCy
DESCRIPTION: This snippet illustrates a problematic pattern where data is loaded every time the component is created, which is inefficient as it would happen both during training and runtime.

LANGUAGE: python
CODE:
@Language.factory("acronyms", default_config={"data": {}, "case_sensitive": False})
def create_acronym_component(nlp: Language, name: str, data: Dict[str, str], case_sensitive: bool):
    # 🚨 Problem: data will be loaded every time component is created
    return AcronymComponent(nlp, data, case_sensitive)

----------------------------------------

TITLE: Iterating Over Noun Chunks in a Span in spaCy Python
DESCRIPTION: Yields base noun phrases (chunks) within a span. Only works if the document has been syntactically parsed and if the noun_chunk syntax iterator is implemented for the language. Returns Span objects for each noun chunk.

LANGUAGE: python
CODE:
doc = nlp("A phrase with another phrase occurs.")
span = doc[3:5]
chunks = list(span.noun_chunks)
assert len(chunks) == 1
assert chunks[0].text == "another phrase"

----------------------------------------

TITLE: Managing spaCy Projects with CLI Commands
DESCRIPTION: Demonstrates how to clone a project template, download data assets, and run workflows using spaCy's project management commands. This enables managing end-to-end spaCy workflows for different use cases and domains.

LANGUAGE: bash
CODE:
# Clone a project template
$ python -m spacy project clone pipelines/tagger_parser_ud
$ cd tagger_parser_ud
# Download data assets
$ python -m spacy project assets
# Run a workflow
$ python -m spacy project run all

----------------------------------------

TITLE: Evaluating Coreference Clusters with LEA Score in Python
DESCRIPTION: Computes the LEA (Moosavi and Strube, 2016) precision, recall, and F-score for coreference clusters. This experimental function takes examples containing both predictions and gold-standard annotations and returns a dictionary with scores.

LANGUAGE: python
CODE:
scores = score_coref_clusters(
    examples,
    span_cluster_prefix="coref_clusters",
)
print(scores["coref_f"])

----------------------------------------

TITLE: Setting Method Extension on Doc Object
DESCRIPTION: Example of creating a method extension that makes a function available as an object method on a Doc.

LANGUAGE: python
CODE:
Doc.set_extension("hello", method=lambda doc, name: f"Hi {name}!")
assert doc._.hello("Bob") == "Hi Bob!"

----------------------------------------

TITLE: Updating a Custom Pipeline Component in spaCy
DESCRIPTION: Demonstrates how to update a custom pipeline component using a batch of Example objects and an optimizer. This method is used during training to update the component's model based on examples containing predictions and gold-standard annotations.

LANGUAGE: python
CODE:
pipe = nlp.add_pipe("your_custom_pipe")
optimizer = nlp.initialize()
losses = pipe.update(examples, sgd=optimizer)

----------------------------------------

TITLE: Setting Annotations with CuratedTransformer in spaCy
DESCRIPTION: Assigns the extracted features to Doc objects, writing DocTransformerOutput to the Doc._.trf_data attribute and calling any custom set_extra_annotations callback.

LANGUAGE: python
CODE:
trf = nlp.add_pipe("curated_transformer")
scores = trf.predict(docs)
trf.set_annotations(docs, scores)

----------------------------------------

TITLE: Finding tokens by start character position in spaCy Doc objects
DESCRIPTION: Demonstrates how to use the token_by_start function to find a token in a TokenC* array by the offset of its first character. The function returns the index of the token or -1 if not found.

LANGUAGE: python
CODE:
from spacy.tokens.doc cimport Doc, token_by_start
from spacy.vocab cimport Vocab

doc = Doc(Vocab(), words=["hello", "world"])
assert token_by_start(doc.c, doc.length, 6) == 1
assert token_by_start(doc.c, doc.length, 4) == -1

----------------------------------------

TITLE: Removing Rules from DependencyMatcher in spaCy (Python)
DESCRIPTION: Demonstrates how to remove a rule from a DependencyMatcher. This example adds a rule, verifies it exists, removes it, and then confirms it no longer exists in the matcher.

LANGUAGE: python
CODE:
matcher.add("FOUNDED", patterns)
assert "FOUNDED" in matcher
matcher.remove("FOUNDED")
assert "FOUNDED" not in matcher

----------------------------------------

TITLE: Scoring Labeled Spans with spaCy Scorer
DESCRIPTION: Demonstrates how to calculate Precision, Recall, and F-score for entity spans in spaCy using the Scorer.score_spans method. This is commonly used to evaluate named entity recognition performance.

LANGUAGE: python
CODE:
scores = Scorer.score_spans(examples, "ents")
print(scores["ents_f"])

----------------------------------------

TITLE: Initializing the Lemmatizer with custom lookups
DESCRIPTION: Example showing how to initialize the Lemmatizer with custom lookup tables, and a config file example for initialization.

LANGUAGE: python
CODE:
lemmatizer = nlp.add_pipe("lemmatizer")
lemmatizer.initialize(lookups=lookups)

LANGUAGE: ini
CODE:
### config.cfg
[initialize.components.lemmatizer]

[initialize.components.lemmatizer.lookups]
@misc = "load_my_lookups.v1"

----------------------------------------

TITLE: Loading Doc Object from Disk with Doc.from_disk
DESCRIPTION: Shows how to load a Doc object from disk. This method modifies the object in place and returns it, allowing efficient restoration of previously saved documents with all their annotations.

LANGUAGE: python
CODE:
from spacy.tokens import Doc
from spacy.vocab import Vocab
doc = Doc(Vocab()).from_disk("/path/to/doc")

----------------------------------------

TITLE: Getting Feature Count in MorphAnalysis in Python
DESCRIPTION: Returns the number of features in a MorphAnalysis object using the __len__ method.

LANGUAGE: python
CODE:
feats = "Feat1=Val1,Val2|Feat2=Val2"
morph = MorphAnalysis(nlp.vocab, feats)
assert len(morph) == 3

----------------------------------------

TITLE: Explaining Linguistic Annotations with spacy.explain()
DESCRIPTION: Examples of using spacy.explain() to get human-readable descriptions of POS tags, dependency labels, and entity types used in spaCy's linguistic annotations.

LANGUAGE: python
CODE:
spacy.explain("NORP")
# Nationalities or religious or political groups

doc = nlp("Hello world")
for word in doc:
   print(word.text, word.tag_, spacy.explain(word.tag_))
# Hello UH interjection
# world NN noun, singular or mass

----------------------------------------

TITLE: Loading DocBin from Disk in Python
DESCRIPTION: Shows how to load a previously saved DocBin from a file, typically with a .spacy extension.

LANGUAGE: python
CODE:
doc_bin = DocBin().from_disk("./data.spacy")

----------------------------------------

TITLE: Loading a CoreferenceResolver from Bytes in spaCy (Python)
DESCRIPTION: Shows how to load a coreference resolution model from a previously serialized bytestring, modifying the component in place.

LANGUAGE: python
CODE:
coref_bytes = coref.to_bytes()
coref = nlp.add_pipe("experimental_coref")
coref.from_bytes(coref_bytes)

----------------------------------------

TITLE: Rendering Entity Data using Parse Function in Python
DESCRIPTION: This snippet demonstrates how to parse entity data from a spaCy Doc object and render it using displaCy's manual mode. It converts the Doc object to displaCy's entity format before rendering.

LANGUAGE: python
CODE:
doc = nlp("But Google is starting from behind.")
ex = displacy.parse_ents(doc)
html = displacy.render(ex, style="ent", manual=True)

----------------------------------------

TITLE: EntityRuler Attributes Table in Markdown
DESCRIPTION: A markdown table documenting the attributes of the spaCy EntityRuler component, including the matcher, phrase_matcher, token_patterns, and phrase_patterns, along with their descriptions and types.

LANGUAGE: markdown
CODE:
| Name              | Description                                                                                                           |
| ----------------- | --------------------------------------------------------------------------------------------------------------------- |
| `matcher`         | The underlying matcher used to process token patterns. ~~Matcher~~                                                    |
| `phrase_matcher`  | The underlying phrase matcher used to process phrase patterns. ~~PhraseMatcher~~                                      |
| `token_patterns`  | The token patterns present in the entity ruler, keyed by label. ~~Dict[str, List[Dict[str, Union[str, List[dict]]]]~~ |
| `phrase_patterns` | The phrase patterns present in the entity ruler, keyed by label. ~~Dict[str, List[Doc]]~~                             |

----------------------------------------

TITLE: Removing Patterns by ID from SpanRuler in Python
DESCRIPTION: Shows how to remove patterns with a specific ID from a SpanRuler using the remove_by_id method.

LANGUAGE: python
CODE:
patterns = [{"label": "ORG", "pattern": "Apple", "id": "apple"}]
ruler = nlp.add_pipe("span_ruler")
ruler.add_patterns(patterns)
ruler.remove_by_id("apple")

----------------------------------------

TITLE: Applying CuratedTransformer to a Document
DESCRIPTION: Example demonstrating how to apply the CuratedTransformer component to a single spaCy document. This typically happens automatically when processing text with the nlp pipeline.

LANGUAGE: python
CODE:
doc = nlp("This is a sentence.")
trf = nlp.add_pipe("curated_transformer")
# This usually happens under the hood
processed = trf(doc)

----------------------------------------

TITLE: Accessing Document Vector Representation in spaCy (Python)
DESCRIPTION: Demonstrates how to access a document's vector representation, which defaults to an average of the token vectors.

LANGUAGE: python
CODE:
doc = nlp("I like apples")
assert doc.vector.dtype == "float32"
assert doc.vector.shape == (300,)

----------------------------------------

TITLE: Manual Jupyter Integration with displaCy and IPython
DESCRIPTION: This snippet shows how to manually integrate displaCy with Jupyter notebooks by using IPython's display and HTML functions. This approach gives more control over how the visualization is rendered in the notebook.

LANGUAGE: python
CODE:
from IPython.core.display import display, HTML

html = displacy.render(doc, style="dep")
display(HTML(html))

----------------------------------------

TITLE: Loading Vectors from Disk in Python
DESCRIPTION: Method for deserializing vector data from disk when loading a saved pipeline.

LANGUAGE: python
CODE:
def from_disk(self, path):

----------------------------------------

TITLE: Accessing TokenC Pointers in spaCy
DESCRIPTION: Example showing how to obtain pointers to TokenC structs from a Doc object. This allows direct access to the C-level token representation for performance-critical operations.

LANGUAGE: python
CODE:
token = &doc.c[3]
token_ptr = &doc.c[3]

----------------------------------------

TITLE: Configuring spacy.CandidateSelector.v1 Component
DESCRIPTION: Example configuration for the spacy.CandidateSelector.v1 component which selects entity candidates from a knowledge base. This component is required by the EntityLinker.v1 task and uses a KB loader to access entity information.

LANGUAGE: ini
CODE:
[initialize]
[initialize.components]
[initialize.components.llm]
[initialize.components.llm.candidate_selector]
@llm_misc = "spacy.CandidateSelector.v1"

# Load a KB from a KB file. For loading KBs from spaCy pipelines see spacy.KBObjectLoader.v1.
[initialize.components.llm.candidate_selector.kb_loader]
@llm_misc = "spacy.KBFileLoader.v1"
# Path to knowledge base .yaml file.
path = ${paths.el_kb}

----------------------------------------

TITLE: Using SentenceRecognizer.__call__ Method in spaCy
DESCRIPTION: Example demonstrating how to directly apply the SentenceRecognizer to a document. This shows the component being added to the pipeline and then manually called on a document.

LANGUAGE: python
CODE:
doc = nlp("This is a sentence.")
senter = nlp.add_pipe("senter")
# This usually happens under the hood
processed = senter(doc)

----------------------------------------

TITLE: Configuring Corpus for Training in INI format
DESCRIPTION: Example configuration for setting up a Corpus for training data in INI format. Specifies paths, reader function, and various parameters like gold_preproc, max_length, limit, and augmenter.

LANGUAGE: ini
CODE:
[paths]
train = "corpus/train.spacy"

[corpora.train]
@readers = "spacy.Corpus.v1"
path = ${paths.train}
gold_preproc = false
max_length = 0
limit = 0
augmenter = null

----------------------------------------

TITLE: Deserializing a Doc from Binary Format in spaCy
DESCRIPTION: Shows how to reconstruct a Doc object from a binary string using the from_bytes() method. This creates a new Doc with the same content and annotations as the original.

LANGUAGE: python
CODE:
from spacy.tokens import Doc
doc = nlp("Give it back! He pleaded.")
doc_bytes = doc.to_bytes()
doc2 = Doc(doc.vocab).from_bytes(doc_bytes)
assert doc.text == doc2.text

----------------------------------------

TITLE: Adding Morphological Analysis to Morphology Table in Python
DESCRIPTION: Inserts a morphological analysis in the morphology table and returns its hash. The features can be provided in Universal Dependencies FEATS format as a string.

LANGUAGE: python
CODE:
feats = "Feat1=Val1|Feat2=Val2"
hash = nlp.vocab.morphology.add(feats)
assert hash == nlp.vocab.strings[feats]

----------------------------------------

TITLE: Creating Installable Python Package from Pipeline Data with spaCy
DESCRIPTION: Command for generating an installable Python package from an existing pipeline data directory. The package command builds a .tar.gz archive that can be distributed and installed with pip.

LANGUAGE: bash
CODE:
$ python -m spacy package [input_dir] [output_dir] [--code] [--meta-path] [--create-meta] [--build] [--name] [--version] [--force]

----------------------------------------

TITLE: Creating an Optimizer for a Tagger Component in spaCy
DESCRIPTION: Demonstrates how to create an optimizer specifically for a tagger component in the pipeline.

LANGUAGE: python
CODE:
tagger = nlp.add_pipe("tagger")
optimizer = tagger.create_optimizer()

----------------------------------------

TITLE: Using EditTreeLemmatizer to Process a Document
DESCRIPTION: Example showing how to apply the EditTreeLemmatizer to a document, which modifies the document in place by assigning lemmas to tokens.

LANGUAGE: python
CODE:
doc = nlp("This is a sentence.")
lemmatizer = nlp.add_pipe("trainable_lemmatizer", name="lemmatizer")
# This usually happens under the hood
processed = lemmatizer(doc)

----------------------------------------

TITLE: Spans Visualization Input Format for displaCy
DESCRIPTION: This code shows the expected JSON format for visualizing text spans with displaCy. It includes the full text, a tokens array with individual tokens, and a spans array with token-based offsets and labels for each span.

LANGUAGE: python
CODE:
{
    "text": "Welcome to the Bank of China.",
    "spans": [
        {"start_token": 3, "end_token": 6, "label": "ORG"},
        {"start_token": 5, "end_token": 6, "label": "GPE"},
    ],
    "tokens": ["Welcome", "to", "the", "Bank", "of", "China", "."]
}

----------------------------------------

TITLE: Checking if Table Exists with has_table Method
DESCRIPTION: Checks if a table with a given name exists in the Lookups object using the has_table method.

LANGUAGE: python
CODE:
lookups = Lookups()
lookups.add_table("some_table")
assert lookups.has_table("some_table")

----------------------------------------

TITLE: Example of Debug Model Command with Parameter Analysis
DESCRIPTION: An example showing how to analyze dimensions, parameters, and changes in weights during initialization and training for specific layers in a spaCy pipeline component model.

LANGUAGE: bash
CODE:
$ python -m spacy debug model ./config.cfg tagger -l "5,15" -DIM -PAR -P0 -P1 -P2

----------------------------------------

TITLE: Loading Lookups from Bytes
DESCRIPTION: Loads a Lookups object from a previously serialized bytestring.

LANGUAGE: python
CODE:
lookup_bytes = lookups.to_bytes()
lookups = Lookups()
lookups.from_bytes(lookup_bytes)

----------------------------------------

TITLE: Configuring SpanFinder with Custom Settings
DESCRIPTION: Example showing how to configure a SpanFinder component with custom settings including threshold, spans_key, and length constraints.

LANGUAGE: python
CODE:
from spacy.pipeline.span_finder import DEFAULT_SPAN_FINDER_MODEL
config = {
    "threshold": 0.5,
    "spans_key": "my_spans",
    "max_length": None,
    "min_length": None,
    "model": DEFAULT_SPAN_FINDER_MODEL,
}
nlp.add_pipe("span_finder", config=config)

----------------------------------------

TITLE: Using Custom Embedding in spaCy Configuration
DESCRIPTION: Configuration for a tagger model that uses a custom embedding architecture. This demonstrates how to reference a custom registered architecture in the config file.

LANGUAGE: ini
CODE:
[tagger.model.tok2vec.embed]
@architectures = "my_example.MyEmbedding.v1"
output_width = 128

----------------------------------------

TITLE: Setting Annotations with a Tagger Component in spaCy
DESCRIPTION: Demonstrates how to apply pre-computed prediction scores to modify documents with the appropriate tags.

LANGUAGE: python
CODE:
tagger = nlp.add_pipe("tagger")
scores = tagger.predict([doc1, doc2])
tagger.set_annotations([doc1, doc2], scores)

----------------------------------------

TITLE: Configuring PretrainVectors in spaCy
DESCRIPTION: Configuration example for the PretrainVectors pretraining objective in spaCy. This objective predicts a word's vector from a static embeddings table to pretrain a Tok2Vec layer. The configuration includes component setup, vector initialization, and objective parameters.

LANGUAGE: ini
CODE:
[pretraining]
component = "tok2vec"

[initialize]
vectors = "en_core_web_lg"
...

[pretraining.objective]
@architectures = "spacy.PretrainVectors.v1"
maxout_pieces = 3
hidden_size = 300
loss = "cosine"

----------------------------------------

TITLE: Serializing SentenceRecognizer to Bytes in spaCy
DESCRIPTION: Shows how to serialize a SentenceRecognizer component to a bytestring using the to_bytes method.

LANGUAGE: python
CODE:
senter = nlp.add_pipe("senter")
senter_bytes = senter.to_bytes()

----------------------------------------

TITLE: Creating DVC Configuration for spaCy Project
DESCRIPTION: Command-line syntax for generating a DVC configuration file from a spaCy project workflow, enabling tracking and reproducibility of data pipelines.

LANGUAGE: bash
CODE:
$ python -m spacy project dvc [project_dir] [workflow_name]

----------------------------------------

TITLE: Loading and Saving spaCy Models
DESCRIPTION: Demonstrates various ways to load spaCy models using shortcuts, package names, and file paths, as well as saving and loading models to/from disk. Showcases the serialization API that supports the Pickle protocol.

LANGUAGE: python
CODE:
nlp = spacy.load("en") # shortcut link
nlp = spacy.load("en_core_web_sm") # package
nlp = spacy.load("/path/to/en") # unicode path
nlp = spacy.load(Path("/path/to/en")) # pathlib Path

nlp.to_disk("/path/to/nlp")
nlp = English().from_disk("/path/to/nlp")

----------------------------------------

TITLE: Pushing Project Outputs to Remote Storage using Bash
DESCRIPTION: Command for uploading project outputs to a remote storage location defined in the project.yml file. Files are compressed and addressed using path and hash information to prevent overwriting.

LANGUAGE: bash
CODE:
$ python -m spacy project push [remote] [project_dir]

LANGUAGE: bash
CODE:
$ python -m spacy project push my_bucket

LANGUAGE: yaml
CODE:
### project.yml
remotes:
  my_bucket: 's3://my-spacy-bucket'

----------------------------------------

TITLE: Retrieving lexeme attribute values in spaCy
DESCRIPTION: Shows how to get the value of an attribute from a LexemeC struct using the get_struct_attr static method. This example checks if a lexeme has the IS_ALPHA attribute set.

LANGUAGE: python
CODE:
from spacy.attrs cimport IS_ALPHA
from spacy.lexeme cimport Lexeme

lexeme = doc.c[3].lex
is_alpha = Lexeme.get_struct_attr(lexeme, IS_ALPHA)

----------------------------------------

TITLE: Processing Multiple Documents with SpanResolver.pipe in Python
DESCRIPTION: Example of applying the SpanResolver component to a stream of documents using the pipe method. This allows batch processing with a configurable batch size.

LANGUAGE: python
CODE:
span_resolver = nlp.add_pipe("experimental_span_resolver")
for doc in span_resolver.pipe(docs, batch_size=50):
    pass

----------------------------------------

TITLE: Loading SentenceRecognizer from Bytes in spaCy
DESCRIPTION: Demonstrates how to load a SentenceRecognizer component from a bytestring using the from_bytes method. This modifies the object in place and returns it.

LANGUAGE: python
CODE:
senter_bytes = senter.to_bytes()
senter = nlp.add_pipe("senter")
senter.from_bytes(senter_bytes)

----------------------------------------

TITLE: Building spaCy in Parallel with Multiple Jobs
DESCRIPTION: Commands to build spaCy with multiple build jobs in parallel using the SPACY_NUM_BUILD_JOBS environment variable, which improves compilation speed.

LANGUAGE: bash
CODE:
$ pip install -r requirements.txt
$ SPACY_NUM_BUILD_JOBS=4 pip install --no-build-isolation --editable .

----------------------------------------

TITLE: Creating Pipeline Components in spaCy v2/v3
DESCRIPTION: Demonstrates how to create a pipeline component using Language.create_pipe method. This was the primary method in v2, but in v3 it's recommended to use Language.add_pipe instead.

LANGUAGE: python
CODE:
parser = nlp.create_pipe("parser")

----------------------------------------

TITLE: Loading Morphologizer from Bytes in spaCy (Python)
DESCRIPTION: Loads a Morphologizer pipeline component from a bytestring. Modifies the object in place and returns it.

LANGUAGE: python
CODE:
morphologizer_bytes = morphologizer.to_bytes()
morphologizer = nlp.add_pipe("morphologizer")
morphologizer.from_bytes(morphologizer_bytes)

----------------------------------------

TITLE: Serializing a Doc to Binary Format in spaCy
DESCRIPTION: Demonstrates how to convert a Doc object to a binary string representation using the to_bytes() method, which allows for lossless serialization of the document including all annotations.

LANGUAGE: python
CODE:
doc = nlp("Give it back! He pleaded.")
doc_bytes = doc.to_bytes()

----------------------------------------

TITLE: Configuring TextCategorizer with INI Configuration
DESCRIPTION: Configuration file example showing how to set up a TextCategorizer component with a positive label and specify a path to load category labels from a JSON file.

LANGUAGE: ini
CODE:
### config.cfg
[initialize.components.textcat]
positive_label = "POS"

[initialize.components.textcat.labels]
@readers = "spacy.read_labels.v1"
path = "corpus/labels/textcat.json

----------------------------------------

TITLE: Serializing an LLM Component to Bytes in spaCy
DESCRIPTION: Demonstrates how to serialize an LLM component to a bytestring using the to_bytes method. This is useful for storing the component in memory or databases.

LANGUAGE: python
CODE:
llm_ner = nlp.add_pipe("llm_ner")
ner_bytes = llm_ner.to_bytes()

----------------------------------------

TITLE: Adding Patterns to the Matcher in spaCy
DESCRIPTION: Demonstrates the updated Matcher API in spaCy v2.0. The new version combines entity ID, callback function, and patterns into a single add() call and supports string keys.

LANGUAGE: diff
CODE:
- matcher.add_entity("GoogleNow", on_match=merge_phrases)
- matcher.add_pattern("GoogleNow", [{ORTH: "Google"}, {ORTH: "Now"}])

+ matcher.add("GoogleNow", merge_phrases, [{"ORTH": "Google"}, {"ORTH": "Now"}])

----------------------------------------

TITLE: Running spaCy Train with Tokenizer Customization Code
DESCRIPTION: This command shows how to run spaCy training with a custom code file that contains tokenizer customization callbacks. The code is provided via the --code argument.

LANGUAGE: bash
CODE:
$ python -m spacy train config.cfg --code ./functions.py

----------------------------------------

TITLE: Migrating Warning Configuration from spaCy v2.2 to v2.3
DESCRIPTION: Code diff showing how to migrate from spaCy's custom warnings system to native Python warnings in v2.3. Instead of setting SPACY_WARNING_IGNORE, you should now use Python's warnings filters.

LANGUAGE: python
CODE:
import spacy
+ import warnings

- spacy.errors.SPACY_WARNING_IGNORE.append('W007')
+ warnings.filterwarnings("ignore", message=r"\[W007\]", category=UserWarning)

----------------------------------------

TITLE: Loading spaCy Lemmatizer from Bytes
DESCRIPTION: Shows how to deserialize a lemmatizer component from a bytestring. This method modifies the existing lemmatizer object by loading the state from the bytestring.

LANGUAGE: python
CODE:
lemmatizer_bytes = lemmatizer.to_bytes()
lemmatizer = nlp.add_pipe("lemmatizer")
lemmatizer.from_bytes(lemmatizer_bytes)

----------------------------------------

TITLE: Retrieving Rules from DependencyMatcher in spaCy (Python)
DESCRIPTION: Shows how to retrieve a pattern stored for a key in a DependencyMatcher. The get method returns a tuple containing the callback function and the patterns associated with the specified rule ID.

LANGUAGE: python
CODE:
matcher.add("FOUNDED", patterns, on_match=on_match)
on_match, patterns = matcher.get("FOUNDED")

----------------------------------------

TITLE: Creating a Span from a Doc slice in Python with spaCy
DESCRIPTION: Example showing how to create a Span object by slicing a Doc object. This creates a span containing the tokens from index 1 to 3 ("it", "back", "!").

LANGUAGE: python
CODE:
doc = nlp("Give it back! He pleaded.")
span = doc[1:4]
assert [t.text for t in span] ==  ["it", "back", "!"]

----------------------------------------

TITLE: Configuring JsonlCorpus in INI format
DESCRIPTION: Example configuration for setting up a JsonlCorpus for pretraining data in INI format. Specifies the reader function, path to the JSONL file, and parameters like min_length, max_length, and limit.

LANGUAGE: ini
CODE:
### Example config
[corpora.pretrain]
@readers = "spacy.JsonlCorpus.v1"
path = "corpus/raw_text.jsonl"
min_length = 0
max_length = 0
limit = 0

----------------------------------------

TITLE: Example of Pushing spaCy Pipeline to Hugging Face Hub
DESCRIPTION: Example showing how to push a specific spaCy pipeline wheel file to the Hugging Face Hub. This makes the model available for others to download and use.

LANGUAGE: bash
CODE:
$ python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl

----------------------------------------

TITLE: Serializing a CoreferenceResolver to Bytes in spaCy (Python)
DESCRIPTION: Demonstrates how to serialize a coreference resolution model to a bytestring, which can be useful for storage or transmission without the filesystem.

LANGUAGE: python
CODE:
coref = nlp.add_pipe("experimental_coref")
coref_bytes = coref.to_bytes()

----------------------------------------

TITLE: Retrieving word vectors in spaCy Vocab
DESCRIPTION: Gets a vector for a specific word in the vocabulary. If the word is not in the current vectors, returns a zero vector with the same dimensions.

LANGUAGE: python
CODE:
nlp.vocab.get_vector("apple")

----------------------------------------

TITLE: Updating Tok2Vec Model During Training
DESCRIPTION: Example demonstrating how to update the Tok2Vec model during training using a batch of examples and an optimizer, returning the training losses.

LANGUAGE: python
CODE:
tok2vec = nlp.add_pipe("tok2vec")
optimizer = nlp.initialize()
losses = tok2vec.update(examples, sgd=optimizer)

----------------------------------------

TITLE: Serializing DocBin to Bytes in Python
DESCRIPTION: Demonstrates how to serialize a DocBin's annotations to a bytestring for storage or transmission.

LANGUAGE: python
CODE:
docs = [nlp("Hello world!")]
doc_bin = DocBin(docs=docs)
doc_bin_bytes = doc_bin.to_bytes()

----------------------------------------

TITLE: Creating Example Objects from Dictionary in spaCy
DESCRIPTION: Shows how to create an Example object using the from_dict classmethod, which takes a predicted Doc and a dictionary containing reference annotations. This approach is useful when working with the standard spaCy training format.

LANGUAGE: python
CODE:
from spacy.tokens import Doc
from spacy.training import Example

predicted = Doc(vocab, words=["Apply", "some", "sunscreen"])
token_ref = ["Apply", "some", "sun", "screen"]
tags_ref = ["VERB", "DET", "NOUN", "NOUN"]
example = Example.from_dict(predicted, {"words": token_ref, "tags": tags_ref})

----------------------------------------

TITLE: Loading Morphologizer from Disk in spaCy (Python)
DESCRIPTION: Loads a previously saved Morphologizer pipeline component from disk. This modifies the object in place and returns it.

LANGUAGE: python
CODE:
morphologizer = nlp.add_pipe("morphologizer")
morphologizer.from_disk("/path/to/morphologizer")

----------------------------------------

TITLE: Inspecting Pipeline Components Example
DESCRIPTION: Shows how to inspect available and active pipeline components using various Language object attributes.

LANGUAGE: python
CODE:
nlp = spacy.blank("en")
nlp.add_pipe("ner")
nlp.add_pipe("textcat")
assert nlp.pipe_names == ["ner", "textcat"]
nlp.disable_pipe("ner")
assert nlp.pipe_names == ["textcat"]
assert nlp.component_names == ["ner", "textcat"]
assert nlp.disabled == ["ner"]

----------------------------------------

TITLE: Calculating Loss for TextCategorizer Predictions
DESCRIPTION: Calculates the loss and gradient of loss for a batch of documents and their predicted scores, used during model training and evaluation.

LANGUAGE: python
CODE:
textcat = nlp.add_pipe("textcat")
scores = textcat.predict([eg.predicted for eg in examples])
loss, d_loss = textcat.get_loss(examples, scores)

----------------------------------------

TITLE: Configuring spacy.NER.v2 with Label Definitions in INI format
DESCRIPTION: Advanced configuration for the spacy.NER.v2 task component with detailed label definitions. This showcases how to provide explicit descriptions for entity types to guide the LLM's extraction behavior.

LANGUAGE: ini
CODE:
[components.llm.task]
@llm_tasks = "spacy.NER.v2"
labels = PERSON,SPORTS_TEAM

[components.llm.task.label_definitions]
PERSON = "Extract any named individual in the text."
SPORTS_TEAM = "Extract the names of any professional sports team. e.g. Golden State Warriors, LA Lakers, Man City, Real Madrid"

----------------------------------------

TITLE: Using Dictionary Properties for Complex Matching in spaCy
DESCRIPTION: Example of token patterns using dictionary-based property matching. The pattern matches lemmas from a list of similar verbs, followed by proper nouns with length of at least 10 characters.

LANGUAGE: json
CODE:
[
  {"LEMMA": {"IN": ["like", "love", "enjoy"]}},
  {"POS": "PROPN", "LENGTH": {">=": 10}},
]

----------------------------------------

TITLE: Iterating Through Left and Right Children in spaCy's Parse Tree
DESCRIPTION: This snippet demonstrates how to access a token's left and right children in the dependency tree. It shows accessing the children before (lefts) and after (rights) a token, as well as counting them with n_lefts and n_rights attributes.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("bright red apples on the tree")
print([token.text for token in doc[2].lefts])  # ['bright', 'red']
print([token.text for token in doc[2].rights])  # ['on']
print(doc[2].n_lefts)  # 2
print(doc[2].n_rights)  # 1

----------------------------------------

TITLE: Processing Document Stream with CuratedTransformer
DESCRIPTION: Example showing how to process a stream of documents with the CuratedTransformer component's pipe method, which batches documents for efficiency with a configurable batch size.

LANGUAGE: python
CODE:
trf = nlp.add_pipe("curated_transformer")
for doc in trf.pipe(docs, batch_size=50):
    pass

----------------------------------------

TITLE: Adding custom token flags in spaCy Vocab
DESCRIPTION: Creates a custom boolean flag for words in the vocabulary. The flag can be checked on tokens using token.check_flag() with the returned flag ID.

LANGUAGE: python
CODE:
def is_my_product(text):
    products = ["spaCy", "Thinc", "displaCy"]
    return text in products

MY_PRODUCT = nlp.vocab.add_flag(is_my_product)
doc = nlp("I like spaCy")
assert doc[2].check_flag(MY_PRODUCT) == True

----------------------------------------

TITLE: Saving Vectors to Disk in Python
DESCRIPTION: Method for serializing vector data to disk as part of pipeline saving.

LANGUAGE: python
CODE:
def to_disk(self, path):

----------------------------------------

TITLE: Configuring Named Entity Visualizer Options in spaCy
DESCRIPTION: Demonstrates how to customize the named entity visualization by filtering entity types and setting custom colors for specific entity categories.

LANGUAGE: python
CODE:
options = {"ents": ["PERSON", "ORG", "PRODUCT"],
           "colors": {"ORG": "yellow"}}
displacy.serve(doc, style="ent", options=options)

----------------------------------------

TITLE: Configuring Japanese Tokenizer in spaCy Configuration File
DESCRIPTION: Shows how to configure the Japanese tokenizer in a spaCy configuration file, setting SudachiPy as the tokenizer with split mode A.

LANGUAGE: ini
CODE:
[nlp.tokenizer]
@tokenizers = "spacy.ja.JapaneseTokenizer"
split_mode = "A"

----------------------------------------

TITLE: Initializing Scorer in spaCy (Python)
DESCRIPTION: Creates a new Scorer object with either a default scoring pipeline or a provided one. The scorer can be used to evaluate the performance of spaCy models on various NLP tasks.

LANGUAGE: python
CODE:
from spacy.scorer import Scorer

# Default scoring pipeline
scorer = Scorer()

# Provided scoring pipeline
nlp = spacy.load("en_core_web_sm")
scorer = Scorer(nlp)

----------------------------------------

TITLE: Loading Tok2Vec Component from Bytes in Python
DESCRIPTION: Loads a Tok2Vec component from a bytestring representation. The method modifies the object in place and returns the updated component. Specific serialization fields can be excluded during loading using the exclude parameter.

LANGUAGE: python
CODE:
tok2vec_bytes = tok2vec.to_bytes()
tok2vec = nlp.add_pipe("tok2vec")
tok2vec.from_bytes(tok2vec_bytes)

----------------------------------------

TITLE: Deserializing a Doc from JSON Format in spaCy
DESCRIPTION: Shows how to reconstruct a Doc object from JSON data using the from_json() method. Creates a new Doc with the same content as represented in the JSON structure.

LANGUAGE: python
CODE:
from spacy.tokens import Doc
doc = nlp("All we have to decide is what to do with the time that is given us.")
doc_json = doc.to_json()
deserialized_doc = Doc(nlp.vocab).from_json(doc_json)
assert deserialized_doc.text == doc.text == doc_json["text"]

----------------------------------------

TITLE: Using Environment Variables in spaCy Project Commands
DESCRIPTION: YAML configuration showing how to reference environment variables in project.yml. This example makes batch size and GPU ID available to commands through environment variables, providing flexibility for command-line overrides.

LANGUAGE: yaml
CODE:
env:
  batch_size: BATCH_SIZE
  gpu_id: GPU_ID

commands:
  - name: evaluate
    script:
      - 'python scripts/custom_evaluation.py ${env.batch_size}'

----------------------------------------

TITLE: Creating Code Blocks with Line Highlighting
DESCRIPTION: Shows how to create code blocks with line highlighting in Markdown. The highlight attribute specifies ranges of lines to highlight in the rendered code block.

LANGUAGE: markdown
CODE:
```python
### This is a title {highlight="1-2"}
import spacy
nlp = spacy.load("en_core_web_sm")
```

----------------------------------------

TITLE: Configuring Sequence-Based Batch Generation in spaCy
DESCRIPTION: Configuration for a batching strategy that creates batches with a fixed number of items. This simple batcher allows specifying the target batch size and optionally defining a custom length calculation function.

LANGUAGE: ini
CODE:
[training.batcher]
@batchers = "spacy.batch_by_sequence.v1"
size = 32
get_length = null

----------------------------------------

TITLE: Configuring Prodigy Integration in spaCy Project
DESCRIPTION: YAML configuration for integrating Prodigy annotation tool with spaCy, defining workflows to merge annotations, convert them to spaCy format, and train NLP models.

LANGUAGE: yaml
CODE:
vars:
  prodigy:
    train_dataset: "fashion_brands_training"
    eval_dataset: "fashion_brands_eval"

workflows:
  all:
    - data-to-spacy
    - train_spacy

commands:
  - name: "data-to-spacy"
    help: "Merge your annotations and create data in spaCy's binary format"
    script:
      - "python -m prodigy data-to-spacy corpus/ --ner ${vars.prodigy.train_dataset},eval:${vars.prodigy.eval_dataset}"
    outputs:
      - "corpus/train.spacy"
      - "corpus/dev.spacy"
  - name: "train_spacy"
    help: "Train a named entity recognition model with spaCy"
    script:
      - "python -m spacy train configs/config.cfg --output training/ --paths.train corpus/train.spacy --paths.dev corpus/dev.spacy"
    deps:
      - "corpus/train.spacy"
      - "corpus/dev.spacy"
    outputs:
      - "training/model-best"

----------------------------------------

TITLE: Serializing EntityRecognizer to Bytes in spaCy
DESCRIPTION: Demonstrates how to serialize an EntityRecognizer to a bytestring using the to_bytes method, which is useful for storage or transmission.

LANGUAGE: python
CODE:
ner = nlp.add_pipe("ner")
ner_bytes = ner.to_bytes()

----------------------------------------

TITLE: Adding Aliases to Knowledge Base Entities in Python
DESCRIPTION: Associates an alias or mention with potential entity identifiers and their prior probabilities. The entities must have been previously added to the knowledge base.

LANGUAGE: python
CODE:
kb.add_alias(alias="Douglas", entities=["Q42", "Q463035"], probabilities=[0.6, 0.3])

----------------------------------------

TITLE: Retrieving Entity Candidates from a KnowledgeBase in Python
DESCRIPTION: Example demonstrating how to use the get_candidates method of a KnowledgeBase instance to retrieve candidate entities for a text span in a spaCy document.

LANGUAGE: python
CODE:
from spacy.lang.en import English
nlp = English()
doc = nlp("Douglas Adams wrote 'The Hitchhiker's Guide to the Galaxy'.")
candidates = kb.get_candidates(doc[0:2])

----------------------------------------

TITLE: Setting Annotations with Morphologizer in spaCy
DESCRIPTION: Shows how to apply pre-computed scores to modify a batch of document objects. This is typically used in conjunction with the predict method.

LANGUAGE: python
CODE:
morphologizer = nlp.add_pipe("morphologizer")
scores = morphologizer.predict([doc1, doc2])
morphologizer.set_annotations([doc1, doc2], scores)

----------------------------------------

TITLE: Making Predictions with CoreferenceResolver
DESCRIPTION: Example demonstrating how to apply the CoreferenceResolver model to documents without modifying them, returning the predicted coreference clusters.

LANGUAGE: python
CODE:
coref = nlp.add_pipe("experimental_coref")
clusters = coref.predict([doc1, doc2])

----------------------------------------

TITLE: Adding a Label to TextCategorizer in Python
DESCRIPTION: Shows how to manually add a new label to the TextCategorizer component. This is not necessary if providing a representative data sample to the initialize method.

LANGUAGE: python
CODE:
textcat = nlp.add_pipe("textcat")
textcat.add_label("MY_LABEL")

----------------------------------------

TITLE: Loading spaCy Vocabulary from Disk
DESCRIPTION: Loads vocabulary state from a directory. This method modifies the Vocab object in place and also returns it. Can exclude specific serialization fields from loading.

LANGUAGE: python
CODE:
from spacy.vocab import Vocab
vocab = Vocab().from_disk("/path/to/vocab")

----------------------------------------

TITLE: Getting Lowest Common Ancestor Matrix for a Span in spaCy Python
DESCRIPTION: Calculates the lowest common ancestor matrix for tokens in a span. Returns a 2D numpy array containing integer indices of ancestors, with -1 indicating no common ancestor is found.

LANGUAGE: python
CODE:
doc = nlp("I like New York in Autumn")
span = doc[1:4]
matrix = span.get_lca_matrix()
# array([[0, 0, 0], [0, 1, 2], [0, 2, 2]], dtype=int32)

----------------------------------------

TITLE: Defining Corpora Configuration in spaCy
DESCRIPTION: Configuration example showing how to define corpus resources in a spaCy config file. This specifies a training corpus using the built-in Corpus reader, with settings for the file path, preprocessing options, and maximum length and limit of examples.

LANGUAGE: ini
CODE:
[corpora]

[corpora.train]
@readers = "spacy.Corpus.v1"
path = ${paths.train}
gold_preproc = false
max_length = 0
limit = 0
augmenter = null

[training]
train_corpus = "corpora.train"

----------------------------------------

TITLE: Configuring Few-Shot Examples Reader in spaCy INI
DESCRIPTION: Example configuration for loading few-shot examples from a YAML file using the FewShotReader. This allows providing example data for few-shot learning approaches in LLM tasks.

LANGUAGE: ini
CODE:
[components.llm.task.examples]
@misc = "spacy.FewShotReader.v1"
path = "ner_examples.yml"

----------------------------------------

TITLE: Initializing Sentencizer in spaCy
DESCRIPTION: Example demonstrating two ways to initialize a Sentencizer: by adding it to the pipeline using nlp.add_pipe or by creating an instance directly from the Sentencizer class.

LANGUAGE: Python
CODE:
# Construction via add_pipe
sentencizer = nlp.add_pipe("sentencizer")

# Construction from class
from spacy.pipeline import Sentencizer
sentencizer = Sentencizer()

----------------------------------------

TITLE: Scoring Tokenization in spaCy (Python)
DESCRIPTION: Evaluates tokenization quality by calculating token accuracy and precision/recall/F-score for token character spans. Documents with unknown spaces are skipped during evaluation.

LANGUAGE: python
CODE:
scores = Scorer.score_tokenization(examples)

----------------------------------------

TITLE: Initializing a TrainablePipe Component for Training in Python
DESCRIPTION: Example showing how to initialize a pipeline component for training by calling the initialize method with a function that provides training examples.

LANGUAGE: python
CODE:
pipe = nlp.add_pipe("your_custom_pipe")
pipe.initialize(lambda: [], pipeline=nlp.pipeline)

----------------------------------------

TITLE: Using Different Parameter Values for a Pipeline Component
DESCRIPTION: Shows how to temporarily modify a pipeline component's model to use given parameter values. This is useful for saving the best model using averaged weights.

LANGUAGE: python
CODE:
pipe = nlp.add_pipe("your_custom_pipe")
with pipe.use_params(optimizer.averages):
    pipe.to_disk("/best_model")

----------------------------------------

TITLE: Creating Type Annotations in Markdown for spaCy Documentation
DESCRIPTION: How to create Python type hint annotations in Markdown using custom syntax with ~~ delimiters that are rendered with special formatting and linking.

LANGUAGE: markdown
CODE:
~~Model[List[Doc], Floats2d]~~

LANGUAGE: markdown
CODE:
| Header 1 | Header 2               |
| -------- | ---------------------- |
| Column 1 | Column 2 ~~List[Doc]~~ |

----------------------------------------

TITLE: Using DependencyMatcher to Find Matches in Text in Python
DESCRIPTION: Example demonstrating how to add a pattern to the DependencyMatcher and use it to find matches in a document. The pattern looks for the word "founded" in the text.

LANGUAGE: python
CODE:
from spacy.matcher import DependencyMatcher

matcher = DependencyMatcher(nlp.vocab)
pattern = [{"RIGHT_ID": "founded_id",
  "RIGHT_ATTRS": {"ORTH": "founded"}}]
matcher.add("FOUNDED", [pattern])
doc = nlp("Bill Gates founded Microsoft.")
matches = matcher(doc)

----------------------------------------

TITLE: Initializing a Table in spaCy
DESCRIPTION: Creates a new Table object with optional name and data. Tables are used within Lookups to store key-value pairs with Bloom filter optimization.

LANGUAGE: python
CODE:
from spacy.lookups import Table
data = {"foo": "bar", "baz": 100}
table = Table(name="some_table", data=data)
assert "foo" in table
assert table["foo"] == "bar"

----------------------------------------

TITLE: Updating TextCategorizer Model with Examples
DESCRIPTION: Updates the TextCategorizer model using a batch of Example objects containing predictions and gold-standard annotations. Returns updated losses.

LANGUAGE: python
CODE:
textcat = nlp.add_pipe("textcat")
optimizer = nlp.initialize()
losses = textcat.update(examples, sgd=optimizer)

----------------------------------------

TITLE: Configuring Components with Settings in Code in spaCy v3.0
DESCRIPTION: Demonstrates how to configure a pipeline component using the config parameter on nlp.add_pipe instead of instantiating the component directly.

LANGUAGE: diff
CODE:
punct_chars = ["!", ".", "?"]
- sentencizer = Sentencizer(punct_chars=punct_chars)
+ sentencizer = nlp.add_pipe("sentencizer", config={"punct_chars": punct_chars})

----------------------------------------

TITLE: Auto-filling Default Values in a spaCy Config File
DESCRIPTION: Completes a partial configuration file with all required default values, ensuring the config is ready for training without missing values.

LANGUAGE: bash
CODE:
$ python -m spacy init fill-config [base_path] [output_file] [--diff]

----------------------------------------

TITLE: Making Predictions with Morphologizer in spaCy
DESCRIPTION: Demonstrates how to apply the morphologizer model to a batch of documents to get predictions without modifying the documents. This returns the model's prediction scores.

LANGUAGE: python
CODE:
morphologizer = nlp.add_pipe("morphologizer")
scores = morphologizer.predict([doc1, doc2])

----------------------------------------

TITLE: Renaming Pipeline Components in spaCy
DESCRIPTION: Shows how to rename a component in the pipeline, which is useful for creating custom names for pre-defined components.

LANGUAGE: python
CODE:
nlp.rename_pipe("parser", "spacy_parser")

----------------------------------------

TITLE: Calculating Loss with Morphologizer in spaCy
DESCRIPTION: Shows how to calculate the loss and gradient for a batch of examples and their predicted scores. Used during training to evaluate model performance.

LANGUAGE: python
CODE:
morphologizer = nlp.add_pipe("morphologizer")
scores = morphologizer.predict([eg.predicted for eg in examples])
loss, d_loss = morphologizer.get_loss(examples, scores)

----------------------------------------

TITLE: Updating a Tagger Model in spaCy
DESCRIPTION: Shows how to update a tagger model by learning from example objects containing predictions and gold-standard annotations.

LANGUAGE: python
CODE:
tagger = nlp.add_pipe("tagger")
optimizer = nlp.initialize()
losses = tagger.update(examples, sgd=optimizer)

----------------------------------------

TITLE: Converting a Span to a Doc Object in spaCy Python
DESCRIPTION: Creates a new Doc object from a Span with a copy of its data. This allows working with a subset of a document as if it were a complete document. Optional parameters allow for optimization when processing many spans.

LANGUAGE: python
CODE:
doc = nlp("I like New York in Autumn.")
span = doc[2:4]
doc2 = span.as_doc()
assert doc2.text == "New York"

----------------------------------------

TITLE: Checking if a Pipeline Component is Resizable
DESCRIPTION: Demonstrates how to check if a pipeline component's output dimension can be resized. Also shows how to implement custom resizing for a component.

LANGUAGE: python
CODE:
can_resize = pipe.is_resizable

----------------------------------------

TITLE: Creating Tables with JSX Components in spaCy Documentation
DESCRIPTION: How to create tables using JSX components for the spaCy documentation with headers and data rows.

LANGUAGE: markup
CODE:
<Table>
    <Tr><Th>Header 1</Th><Th>Header 2</Th></Tr></thead>
    <Tr><Td>Column 1</Td><Td>Column 2</Td></Tr>
</Table>

----------------------------------------

TITLE: Saving Lookups to Disk
DESCRIPTION: Saves the Lookups object to a directory as lookups.bin, creating the directory if it doesn't exist.

LANGUAGE: python
CODE:
lookups.to_disk("/path/to/lookups")

----------------------------------------

TITLE: Initializing Morphology Object in Python
DESCRIPTION: Creates a new Morphology object with a reference to the string store. The Morphology class stores possible morphological analyses for a language and indexes them by hash.

LANGUAGE: python
CODE:
from spacy.morphology import Morphology

morphology = Morphology(strings)

----------------------------------------

TITLE: Self-Referential Type Hints in Python Class Methods
DESCRIPTION: Demonstrates how to use string-based type annotations when the return type refers to the class itself, avoiding potential circular references.

LANGUAGE: python
CODE:
class SomeClass:
    def from_bytes(self, data: bytes) -> "SomeClass":
        ...

----------------------------------------

TITLE: Adding AttributeRuler with Configuration in spaCy
DESCRIPTION: Example showing how to add the AttributeRuler component to a spaCy pipeline with custom configuration. This sets the validate parameter to True, which enables pattern validation.

LANGUAGE: python
CODE:
config = {"validate": True}
nlp.add_pipe("attribute_ruler", config=config)

----------------------------------------

TITLE: Scoring Token Attributes Per Feature in spaCy (Python)
DESCRIPTION: Evaluates a token attribute per feature in the Universal Dependencies FEATS format. Returns micro precision/recall/F-score metrics and per-feature scores.

LANGUAGE: python
CODE:
scores = Scorer.score_token_attr_per_feat(examples, "morph")
print(scores["morph_per_feat"])

----------------------------------------

TITLE: Accessing TextCategorizer Labels in Python
DESCRIPTION: Demonstrates how to access the labels that have been added to a TextCategorizer component using the labels property.

LANGUAGE: python
CODE:
textcat.add_label("MY_LABEL")
assert "MY_LABEL" in textcat.labels

----------------------------------------

TITLE: Creating Example Object from Dictionary for Training
DESCRIPTION: Basic usage of Example.from_dict() to create a training instance from a reference Doc object and a dictionary of gold-standard annotations.

LANGUAGE: python
CODE:
example = Example.from_dict(doc, gold_dict)

----------------------------------------

TITLE: Setting PaLM API Key in Shell Environment
DESCRIPTION: Sets the environment variable for authenticating with Google's PaLM language model services. This credential is required when using PaLM models in spaCy.

LANGUAGE: shell
CODE:
export PALM_API_KEY="..."

----------------------------------------

TITLE: Creating Tags with JSX Components in spaCy Documentation
DESCRIPTION: How to use the Tag JSX component to create tags for methods, version indicators, and model requirements in the spaCy documentation.

LANGUAGE: jsx
CODE:
<Tag>method</Tag>
<Tag variant="version">4</Tag>
<Tag variant="model">tagger, parser</Tag>

----------------------------------------

TITLE: Updating Transformer Component in spaCy
DESCRIPTION: Prepares for a model update by running the transformer and communicating outputs and backpropagation callbacks to downstream components. The component doesn't perform its own weight update during this method.

LANGUAGE: python
CODE:
trf = nlp.add_pipe("transformer")
optimizer = nlp.initialize()
losses = trf.update(examples, sgd=optimizer)

----------------------------------------

TITLE: Loading DependencyParser from Disk in spaCy
DESCRIPTION: Loads a previously saved parser component from disk. Modifies the existing parser object in place and returns it after loading.

LANGUAGE: python
CODE:
parser = nlp.add_pipe("parser")
parser.from_disk("/path/to/parser")

----------------------------------------

TITLE: Batch Vector Retrieval in Python
DESCRIPTION: Method for efficiently retrieving vectors for multiple keys in a batch. Required for training with StaticVectors.

LANGUAGE: python
CODE:
def get_batch(self, keys):

----------------------------------------

TITLE: Parallel and Distributed Training with Ray
DESCRIPTION: Shows how to set up and use Ray for distributed training in spaCy. This includes installing the Ray integration package, verifying the CLI installation, and running parallel training with specified workers.

LANGUAGE: bash
CODE:
$ pip install -U %%SPACY_PKG_NAME[ray]%%SPACY_PKG_FLAGS
# Check that the CLI is registered
$ python -m spacy ray --help
# Train a pipeline
$ python -m spacy ray train config.cfg --n-workers 2

----------------------------------------

TITLE: Parallel and Distributed Training with Ray
DESCRIPTION: Shows how to set up and use Ray for distributed training in spaCy. This includes installing the Ray integration package, verifying the CLI installation, and running parallel training with specified workers.

LANGUAGE: bash
CODE:
$ pip install -U %%SPACY_PKG_NAME[ray]%%SPACY_PKG_FLAGS
# Check that the CLI is registered
$ python -m spacy ray --help
# Train a pipeline
$ python -m spacy ray train config.cfg --n-workers 2

----------------------------------------

TITLE: Customizing Language.Defaults in spaCy (Python)
DESCRIPTION: Example of customizing the default language data through spaCy's Language.Defaults class. This shows how to set custom tokenizer exceptions, stop words, affixes, and other language-specific settings.

LANGUAGE: python
CODE:
from spacy.language import language
from spacy.lang.tokenizer_exceptions import URL_MATCH
from thinc.api import Config

DEFAULT_CONFIFG = """
[nlp.tokenizer]
@tokenizers = "MyCustomTokenizer.v1"
"""

class Defaults(Language.Defaults):
   stop_words = set()
   tokenizer_exceptions = {}
   prefixes = tuple()
   suffixes = tuple()
   infixes = tuple()
   token_match = None
   url_match = URL_MATCH
   lex_attr_getters = {}
   syntax_iterators = {}
   writing_system = {"direction": "ltr", "has_case": True, "has_letters": True}
   config = Config().from_str(DEFAULT_CONFIG)

----------------------------------------

TITLE: Initializing Morphologizer Component for Training in spaCy
DESCRIPTION: Shows how to initialize the morphologizer component for training using a function that returns example objects. This method validates the network, infers missing shapes, and sets up the label scheme.

LANGUAGE: python
CODE:
morphologizer = nlp.add_pipe("morphologizer")
morphologizer.initialize(lambda: examples, nlp=nlp)

LANGUAGE: ini
CODE:
### config.cfg
[initialize.components.morphologizer]

[initialize.components.morphologizer.labels]
@readers = "spacy.read_labels.v1"
path = "corpus/labels/morphologizer.json

----------------------------------------

TITLE: Setting Custom Token Extensions in spaCy
DESCRIPTION: Example showing how to define a custom attribute on the Token class using set_extension. The example creates an 'is_fruit' property using a getter function that checks if the token's text is in a list of fruits.

LANGUAGE: python
CODE:
from spacy.tokens import Token
fruit_getter = lambda token: token.text in ("apple", "pear", "banana")
Token.set_extension("is_fruit", getter=fruit_getter)
doc = nlp("I have an apple")
assert doc[3]._.is_fruit

----------------------------------------

TITLE: Excluding Fields During Morphologizer Serialization in spaCy (Python)
DESCRIPTION: Demonstrates how to exclude specific data fields when serializing a Morphologizer component to disk.

LANGUAGE: python
CODE:
data = morphologizer.to_disk("/path", exclude=["vocab"])

----------------------------------------

TITLE: Transformer Component Source File Path
DESCRIPTION: The GitHub repository path to the source code for the Transformer pipeline component in the spacy-transformers extension package.

LANGUAGE: python
CODE:
https://github.com/explosion/spacy-transformers/blob/master/spacy_transformers/pipeline_component.py

----------------------------------------

TITLE: Retrieving the Span.subtree in spaCy
DESCRIPTION: Demonstrates how to access all tokens within a span and tokens that descend from them. The example shows the subtree of 'Give it back' includes the exclamation mark.

LANGUAGE: python
CODE:
doc = nlp("Give it back! He pleaded.")
subtree = [t.text for t in doc[:3].subtree]
assert subtree == ["Give", "it", "back", "!"]

----------------------------------------

TITLE: Vocabulary Initialization Config Example
DESCRIPTION: Shows how to reference a vocabulary data file in the initialize block of a spaCy training configuration file.

LANGUAGE: ini
CODE:
[initialize]
vocab_data = "/path/to/vocab-data.jsonl"

----------------------------------------

TITLE: Configuring Single Function Corpus Reader in spaCy Configuration (INI)
DESCRIPTION: Example showing how to define a single function that returns multiple corpora. This approach loads one corpus and divides it into train and dev partitions, with additional configuration parameters.

LANGUAGE: ini
CODE:
[corpora]
@readers = "my_custom_reader.v1"
train_path = ${paths:train}
dev_path = ${paths:dev}
shuffle = true

----------------------------------------

TITLE: Accessing Morphologizer Labels in spaCy (Python)
DESCRIPTION: Demonstrates how to access and check the labels that have been added to the Morphologizer component. These labels follow the Universal Dependencies FEATS format.

LANGUAGE: python
CODE:
morphologizer.add_label("Mood=Ind|POS=VERB|Tense=Past|VerbForm=Fin")
assert "Mood=Ind|POS=VERB|Tense=Past|VerbForm=Fin" in morphologizer.labels

----------------------------------------

TITLE: Scoring Document Classification Categories in spaCy
DESCRIPTION: Demonstrates how to evaluate document classification performance using Scorer.score_cats. This example calculates metrics for multiple labels including macro-averaged Area Under the Curve (AUC).

LANGUAGE: python
CODE:
labels = ["LABEL_A", "LABEL_B", "LABEL_C"]
scores = Scorer.score_cats(
    examples,
    "cats",
    labels=labels
)
print(scores["cats_macro_auc"])

----------------------------------------

TITLE: Updating Pipeline Disabling Syntax from spaCy v2 to v3 in Python
DESCRIPTION: Example showing the syntax change from spaCy v2 to v3 for disabling pipeline components, replacing the disable_pipes method with the new select_pipes method.

LANGUAGE: python
CODE:
- nlp.disable_pipes(["tagger", "parser"])
+ nlp.select_pipes(disable=["tagger", "parser"])

----------------------------------------

TITLE: Counting Right Dependents with Span.n_rights in spaCy
DESCRIPTION: Shows how to count tokens that are to the right of a span whose heads are within the span. The example demonstrates the span 'New York' has 1 right dependent.

LANGUAGE: python
CODE:
doc = nlp("I like New York in Autumn.")
assert doc[2:4].n_rights == 1

----------------------------------------

TITLE: Creating Links in Markdown for spaCy Documentation
DESCRIPTION: How to create links in Markdown format for the spaCy documentation, which will be automatically styled based on the link destination.

LANGUAGE: markdown
CODE:
[I am a link](https://spacy.io)

----------------------------------------

TITLE: Retrieving Span.vector_norm in spaCy
DESCRIPTION: Shows how to get the L2 norm of a span's vector representation. The example demonstrates that different spans have different vector norms.

LANGUAGE: python
CODE:
doc = nlp("I like apples")
doc[1:].vector_norm # 4.800883928527915
doc[2:].vector_norm # 6.895897646384268
assert doc[1:].vector_norm != doc[2:].vector_norm

----------------------------------------

TITLE: Updating spaCy version requirements in meta.json
DESCRIPTION: Example showing how to update the spaCy version requirements in a pipeline's meta.json file to make it compatible with v3.4 after confirming it works correctly.

LANGUAGE: diff
CODE:
- "spacy_version": ">=3.3.0,<3.4.0",
+ "spacy_version": ">=3.3.0,<3.5.0",

----------------------------------------

TITLE: Using Specific Parameters with CoreferenceResolver in spaCy (Python)
DESCRIPTION: Shows how to temporarily use specific parameter values (such as optimizer averages) with a context manager, which is useful for model evaluation or serialization.

LANGUAGE: python
CODE:
coref = nlp.add_pipe("experimental_coref")
with coref.use_params(optimizer.averages):
    coref.to_disk("/best_model")

----------------------------------------

TITLE: Removing Manual Factory Registration in spaCy v3.0
DESCRIPTION: Shows how manual registration of components in Language.factories is no longer needed as decorators handle this automatically.

LANGUAGE: diff
CODE:
- Language.factories["my_component"] = lambda nlp, **cfg: MyComponent(nlp)

----------------------------------------

TITLE: Configuring Few-Shot Learning for NER v3
DESCRIPTION: Configuration for setting up few-shot learning examples in NER v3. This shows how to configure the component to use examples from an external file path.

LANGUAGE: ini
CODE:
[components.llm.task.examples]
@misc = "spacy.FewShotReader.v1"
path = "${paths.examples}"

----------------------------------------

TITLE: Converting FEATS String to Dictionary in Python
DESCRIPTION: Static method that converts a string in Universal Dependencies FEATS format to a dictionary of features and values.

LANGUAGE: python
CODE:
from spacy.morphology import Morphology
d = Morphology.feats_to_dict("Feat1=Val1|Feat2=Val2")
assert d == {"Feat1": "Val1", "Feat2": "Val2"}

----------------------------------------

TITLE: Accessing Named Entities in a Span in spaCy Python
DESCRIPTION: Retrieves named entities that fall completely within a span. Returns a tuple of Span objects, each representing an entity with its label and text accessible through properties.

LANGUAGE: python
CODE:
doc = nlp("Mr. Best flew to New York on Saturday morning.")
span = doc[0:6]
ents = list(span.ents)
assert ents[0].label == 346
assert ents[0].label_ == "PERSON"
assert ents[0].text == "Mr. Best"

----------------------------------------

TITLE: Using Transformer.predict Method in spaCy
DESCRIPTION: Applies the transformer model to process a batch of Doc objects without modifying them. Returns the model's predictions which can later be applied to the documents.

LANGUAGE: python
CODE:
trf = nlp.add_pipe("transformer")
scores = trf.predict([doc1, doc2])

----------------------------------------

TITLE: Updating SentenceRecognizer Component in spaCy
DESCRIPTION: Demonstrates how to update a SentenceRecognizer component with training examples. This method learns from a batch of Example objects containing predictions and gold-standard annotations.

LANGUAGE: python
CODE:
senter = nlp.add_pipe("senter")
optimizer = nlp.initialize()
losses = senter.update(examples, sgd=optimizer)

----------------------------------------

TITLE: Processing a Document with SpanResolver in Python
DESCRIPTION: Example of applying the SpanResolver component to a document. The component modifies the document in-place by adding resolved spans to Doc.spans with the configured output prefix.

LANGUAGE: python
CODE:
doc = nlp("This is a sentence.")
span_resolver = nlp.add_pipe("experimental_span_resolver")
# This usually happens under the hood
processed = span_resolver(doc)

----------------------------------------

TITLE: Saving a KnowledgeBase to Disk in Python
DESCRIPTION: Example showing how to save the current state of a KnowledgeBase instance to a directory on disk.

LANGUAGE: python
CODE:
kb.to_disk(path)

----------------------------------------

TITLE: Updating code for floret vectors in spaCy v3.4
DESCRIPTION: Example of code changes required when working with floret vectors in Croatian, Finnish, Korean or Swedish pipelines. Shows how to replace vocabulary vector iteration with external word list iteration.

LANGUAGE: diff
CODE:
- lexemes = [nlp.vocab[orth] for orth in nlp.vocab.vectors]
+ lexemes = [nlp.vocab[word] for word in external_word_list]

----------------------------------------

TITLE: Retrieving a Lexeme by Orthographic Form in Cython
DESCRIPTION: Example demonstrating how to get a lexeme from the vocabulary using its orthographic form ID (hash). This method is useful when you already have the hash value of a word.

LANGUAGE: python
CODE:
lexeme = vocab.get_by_orth(doc[0].lex.norm)

----------------------------------------

TITLE: Retrieving Feature Values in MorphAnalysis in Python
DESCRIPTION: Shows how to retrieve values for a specific feature using the get method of a MorphAnalysis object.

LANGUAGE: python
CODE:
feats = "Feat1=Val1,Val2"
morph = MorphAnalysis(nlp.vocab, feats)
assert morph.get("Feat1") == ["Val1", "Val2"]

----------------------------------------

TITLE: Rendering displaCy Visualizations as HTML
DESCRIPTION: Generates HTML markup for dependency parse trees or named entity visualizations. Returns HTML that can be embedded in applications or notebooks.

LANGUAGE: python
CODE:
import spacy
from spacy import displacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("This is a sentence.")
html = displacy.render(doc, style="dep")

----------------------------------------

TITLE: Serializing SpanResolver to bytes in spaCy
DESCRIPTION: Demonstrates how to serialize the SpanResolver component to a bytestring using the to_bytes method, which is useful for in-memory serialization.

LANGUAGE: python
CODE:
span_resolver = nlp.add_pipe("experimental_span_resolver")
span_resolver_bytes = span_resolver.to_bytes()

----------------------------------------

TITLE: Scoring Named Entity Linking in spaCy
DESCRIPTION: Shows how to evaluate Named Entity Linking (NEL) performance using Scorer.score_links. This method calculates precision, recall and F-score for entity links while excluding entities with negative labels.

LANGUAGE: python
CODE:
scores = Scorer.score_links(
    examples,
    negative_labels=["NIL", ""]
)
print(scores["nel_micro_f"])

----------------------------------------

TITLE: Counting Left Dependents with Span.n_lefts in spaCy
DESCRIPTION: Demonstrates how to count tokens that are to the left of a span whose heads are within the span. The example shows the span 'York in Autumn' has 1 left dependent.

LANGUAGE: python
CODE:
doc = nlp("I like New York in Autumn.")
assert doc[3:7].n_lefts == 1

----------------------------------------

TITLE: Managing Custom Span Groups in a spaCy Doc
DESCRIPTION: Demonstrates creating and assigning custom span groups to a Doc using the spans property, which allows for storing additional span annotations beyond named entities.

LANGUAGE: python
CODE:
doc = nlp("Their goi ng home")
doc.spans["errors"] = [doc[0:1], doc[1:3]]

----------------------------------------

TITLE: Configuring Probability Tables in spaCy's config.cfg
DESCRIPTION: This code snippet shows how to specify probability tables in the config.cfg file when training a spaCy model from scratch.

LANGUAGE: ini
CODE:
[initialize.lookups]
@misc = "spacy.LookupsDataLoader.v1"
lang = ${nlp.lang}
tables = ["lexeme_prob"]

----------------------------------------

TITLE: Updating spaCy Configurations with Fill-Config Command
DESCRIPTION: Command to update a spaCy v3.2 configuration file to include all the new settings in v3.3. This uses the init fill-config CLI command to populate the new configuration with default values.

LANGUAGE: bash
CODE:
$ python -m spacy init fill-config config-v3.2.cfg config-v3.3.cfg

----------------------------------------

TITLE: Processing Multiple Documents with LLM Component's pipe Method
DESCRIPTION: Demonstrates how to use the pipe method of an LLM component to process a stream of documents with a specified batch size. This is useful for efficient processing of multiple documents.

LANGUAGE: python
CODE:
llm_ner = nlp.add_pipe("llm_ner")
for doc in llm_ner.pipe(docs, batch_size=50):
    pass

----------------------------------------

TITLE: Replacing Complex Comprehensions with Explicit Loops
DESCRIPTION: Shows how to improve readability by replacing complex nested comprehensions with explicit for loops, making the code easier to follow and maintain.

LANGUAGE: diff
CODE:
- result = [{"key": key, "scores": {f"{i}": score for i, score in enumerate(scores)}} for key, scores in values]

+ result = []
+ for key, scores in values:
+     scores_dict = {f"{i}": score for i, score in enumerate(scores)}
+     result.append({"key": key, "scores": scores_dict})

----------------------------------------

TITLE: Accessing the Sentence with Span.sent in spaCy
DESCRIPTION: Demonstrates how to access the sentence span that contains a given span. This property requires that sentence boundaries have been set on the document.

LANGUAGE: python
CODE:
doc = nlp("Give it back! He pleaded.")
span = doc[1:3]
assert span.sent.text == "Give it back!"

----------------------------------------

TITLE: Component Model Configuration in spaCy
DESCRIPTION: This snippet demonstrates how to configure a tagger component's model in spaCy using the INI configuration format. It specifies the factory and model architecture to use along with parameters like width and number of classes.

LANGUAGE: ini
CODE:
[components.tagger]
factory = "tagger"

[components.tagger.model]
@architectures = "model.v1"
width = 512
classes = 16

----------------------------------------

TITLE: Accessing Token Attributes in spaCy using Internal IDs
DESCRIPTION: This example demonstrates how to work with token attributes in spaCy using their internal IDs. It shows that attribute names can be used as both strings and constants, and illustrates how to verify the presence of annotations and lookup attribute IDs from the IDS dictionary.

LANGUAGE: python
CODE:
import spacy
from spacy.attrs import DEP

nlp = spacy.blank("en")
doc = nlp("There are many attributes.")

# DEP always has the same internal value
assert DEP == 76

# "DEP" is automatically converted to DEP
assert DEP == nlp.vocab.strings["DEP"]
assert doc.has_annotation(DEP) == doc.has_annotation("DEP")

# look up IDs in spacy.attrs.IDS
from spacy.attrs import IDS
assert IDS["DEP"] == DEP

----------------------------------------

TITLE: Configuring Overwrite Settings for Pipeline Components
DESCRIPTION: Configuration snippet demonstrating the new overwrite setting that determines whether existing annotation in the Doc is preserved or overwritten by pipeline components.

LANGUAGE: ini
CODE:
[components.tagger]
factory = "tagger"
overwrite = false

----------------------------------------

TITLE: Example Usage of find-function Command in spaCy
DESCRIPTION: Example showing how to use the find-function command to locate the definition of the spaCy.TextCatBOW.v1 registered function.

LANGUAGE: bash
CODE:
$ python -m spacy find-function spacy.TextCatBOW.v1

----------------------------------------

TITLE: Serializing a Transformer Component to Bytes in Python
DESCRIPTION: Example showing how to serialize a spaCy Transformer component to a bytestring using the to_bytes method, which can be used for saving the component state.

LANGUAGE: python
CODE:
trf = nlp.add_pipe("transformer")
trf_bytes = trf.to_bytes()

----------------------------------------

TITLE: Setting a Custom Knowledge Base for EntityLinker in spaCy
DESCRIPTION: Demonstrates how to set a custom knowledge base for the EntityLinker component using a kb_loader function that creates a KnowledgeBase instance synchronized with the current vocabulary.

LANGUAGE: python
CODE:
def create_kb(vocab):
    kb = InMemoryLookupKB(vocab, entity_vector_length=128)
    kb.add_entity(...)
    kb.add_alias(...)
    return kb
entity_linker = nlp.add_pipe("entity_linker")
entity_linker.set_kb(create_kb)

----------------------------------------

TITLE: Setting annotations with SpanResolver in spaCy
DESCRIPTION: Demonstrates how to apply the predicted spans to a batch of documents using the set_annotations method, which saves the predictions to Doc.spans using the output prefix.

LANGUAGE: python
CODE:
span_resolver = nlp.add_pipe("experimental_span_resolver")
spans = span_resolver.predict([doc1, doc2])
span_resolver.set_annotations([doc1, doc2], spans)

----------------------------------------

TITLE: Accessing Table Names in Lookups
DESCRIPTION: Gets the names of all tables stored in the Lookups object using the tables property.

LANGUAGE: python
CODE:
lookups = Lookups()
lookups.add_table("some_table")
assert lookups.tables == ["some_table"]

----------------------------------------

TITLE: Getting Vector Table Shape in Python
DESCRIPTION: Property that returns the dimensions of the vector table as a tuple of (rows, dimensions).

LANGUAGE: python
CODE:
@property
def shape(self):

----------------------------------------

TITLE: Initializing EntityLinker Component in spaCy (Python)
DESCRIPTION: Initializes the EntityLinker component with training examples and an optional custom knowledge base loader. This prepares the component for training by setting up the model and label scheme.

LANGUAGE: python
CODE:
entity_linker = nlp.add_pipe("entity_linker")
entity_linker.initialize(lambda: examples, nlp=nlp, kb_loader=my_kb)

----------------------------------------

TITLE: Adding Entities to the Knowledge Base in Python
DESCRIPTION: Adds individual entities to the knowledge base, specifying their frequency and entity vectors. Entity vectors must match the configured entity_vector_length.

LANGUAGE: python
CODE:
kb.add_entity(entity="Q42", freq=32, entity_vector=vector1)
kb.add_entity(entity="Q463035", freq=111, entity_vector=vector2)

----------------------------------------

TITLE: Configuring Pipeline Initialization in spaCy (INI format)
DESCRIPTION: Example configuration for initializing a spaCy pipeline with vectors, pretrained tok2vec weights, and component-specific initialization data.

LANGUAGE: ini
CODE:
[initialize]
vectors = "/path/to/vectors_nlp"
init_tok2vec = "/path/to/pretrain.bin"

[initialize_components]

[initialize.components.my_component]
data_path = "/path/to/component_data"

----------------------------------------

TITLE: Remote Storage Directory Structure
DESCRIPTION: Example of the nested directory structure used by spaCy's remote storage to organize files based on paths, command hashes, and content hashes.

LANGUAGE: yaml
CODE:
└── urlencoded_file_path            # Path of original file
    ├── some_command_hash           # Hash of command you ran
    │   ├── some_content_hash       # Hash of file content
    │   └── another_content_hash
    └── another_command_hash
        └── third_content_hash

----------------------------------------

TITLE: Installing the Floret Python Wrapper
DESCRIPTION: Command to install the Python wrapper for floret, an extended version of fastText that combines subwords with Bloom embeddings for compact, full-coverage vectors with no OOV words.

LANGUAGE: bash
CODE:
$ pip install floret

----------------------------------------

TITLE: Configuring REL with Few-Shot Examples
DESCRIPTION: Configuration for the REL task with few-shot learning. It specifies the labels to extract and loads examples from a JSONL file using the FewShotReader.

LANGUAGE: ini
CODE:
[components.llm.task]
@llm_tasks = "spacy.REL.v1"
labels = ["LivesIn", "Visits"]

[components.llm.task.examples]
@misc = "spacy.FewShotReader.v1"
path = "rel_examples.jsonl"

----------------------------------------

TITLE: Using merge_noun_chunks in spaCy Python
DESCRIPTION: Demonstrates how to add the merge_noun_chunks component to a spaCy pipeline to combine noun chunks into single tokens. This component should be added after the tagger and parser components.

LANGUAGE: python
CODE:
texts = [t.text for t in nlp("I have a blue car")]
assert texts == ["I", "have", "a", "blue", "car"]

nlp.add_pipe("merge_noun_chunks")
texts = [t.text for t in nlp("I have a blue car")]
assert texts == ["I", "have", "a blue car"]

----------------------------------------

TITLE: Setting spans in a SpanGroup by index
DESCRIPTION: Shows how to update an existing span in a SpanGroup by assigning a new span at a specific index.

LANGUAGE: python
CODE:
doc = nlp("Their goi ng home")
doc.spans["errors"] = [doc[0:1], doc[1:3]]
span = doc[0:2]
doc.spans["errors"][0] = span
assert doc.spans["errors"][0].text == "Their goi"

----------------------------------------

TITLE: Configuring ConsoleLogger.v2 in spaCy Config
DESCRIPTION: Example configuration for the v2 ConsoleLogger in spaCy that displays training progress in tabular format and saves logs to a file. It enables a progress bar and console output, saving results to a JSONL file.

LANGUAGE: ini
CODE:
[training.logger]
@loggers = "spacy.ConsoleLogger.v2"
progress_bar = true
console_output = true
output_file = "training_log.jsonl"

----------------------------------------

TITLE: Initializing TextCategorizer Component in spaCy
DESCRIPTION: Several examples showing different ways to initialize the TextCategorizer component, including via add_pipe with default/custom models and direct instantiation from the class.

LANGUAGE: python
CODE:
# Construction via add_pipe with default model
# Use 'textcat_multilabel' for multi-label classification
textcat = nlp.add_pipe("textcat")

# Construction via add_pipe with custom model
config = {"model": {"@architectures": "my_textcat"}}
parser = nlp.add_pipe("textcat", config=config)

# Construction from class
# Use 'MultiLabel_TextCategorizer' for multi-label classification
from spacy.pipeline import TextCategorizer
textcat = TextCategorizer(nlp.vocab, model, threshold=0.5)

----------------------------------------

TITLE: Configuring Registered Scoring Functions in spaCy
DESCRIPTION: Configuration snippet showing how to customize scoring by specifying a scoring function for a tagger component using the scorers registry. This allows for component-specific evaluation metrics.

LANGUAGE: ini
CODE:
[components.tagger]
factory = "tagger"
scorer = {"@scorers":"spacy.tagger_scorer.v1"}

----------------------------------------

TITLE: Retrieving Entity Candidates in Batch from a KnowledgeBase in Python
DESCRIPTION: Example showing how to use the get_candidates_batch method to retrieve candidate entities for multiple text spans simultaneously from a spaCy document.

LANGUAGE: python
CODE:
from spacy.lang.en import English
nlp = English()
doc = nlp("Douglas Adams wrote 'The Hitchhiker's Guide to the Galaxy'.")
candidates = kb.get_candidates((doc[0:2], doc[3:]))

----------------------------------------

TITLE: Extending a SpanGroup with the += operator
DESCRIPTION: Demonstrates how to extend a SpanGroup with spans or another SpanGroup using the += operator.

LANGUAGE: python
CODE:
doc = nlp("Their goi ng home")
doc.spans["errors"] = [doc[0:1], doc[1:3]]
doc.spans["errors"] += [doc[3:4], doc[2:3]]
assert len(doc.spans["errors"]) == 4

----------------------------------------

TITLE: Checking for Token Extensions in spaCy
DESCRIPTION: Example showing how to check whether an extension has been registered on the Token class. The has_extension method returns a boolean indicating if the extension exists.

LANGUAGE: python
CODE:
from spacy.tokens import Token
Token.set_extension("is_fruit", default=False)
assert Token.has_extension("is_fruit")

----------------------------------------

TITLE: Initializing SpanFinder Component in spaCy
DESCRIPTION: Initialize the SpanFinder component for training by providing example data. This sets up the model and validates the network structure, inferring missing shapes.

LANGUAGE: python
CODE:
span_finder = nlp.add_pipe("span_finder")
span_finder.initialize(lambda: examples, nlp=nlp)

----------------------------------------

TITLE: Using Regular Expressions in spaCy Pattern Matching
DESCRIPTION: Examples showing how to use regular expressions in token patterns to match different spellings or variations of words without creating separate patterns for each variant.

LANGUAGE: python
CODE:
pattern = [{"TEXT": {"REGEX": "^[Uu](\\.?|nited)$"}},
           {"TEXT": {"REGEX": "^[Ss](\\.?|tates)$"}},
           {"LOWER": "president"}]

----------------------------------------

TITLE: Updating spaCy Version Requirements in meta.json
DESCRIPTION: Shows how to update the spaCy version requirements in the meta.json file to ensure compatibility with newer spaCy versions, specifically changing from v3.0-specific to a broader v3.0-3.2 range.

LANGUAGE: diff
CODE:
- "spacy_version": ">=3.0.0,<3.1.0",
+ "spacy_version": ">=3.0.0,<3.2.0",

----------------------------------------

TITLE: Serializing EntityLinker Component to Bytes in spaCy (Python)
DESCRIPTION: Converts the EntityLinker component to a bytestring for serialization, including the KnowledgeBase. This allows the component to be saved in binary form.

LANGUAGE: python
CODE:
entity_linker = nlp.add_pipe("entity_linker")
entity_linker_bytes = entity_linker.to_bytes()

----------------------------------------

TITLE: Iterating over lexemes in spaCy Vocab
DESCRIPTION: Demonstrates how to iterate through all lexemes in the vocabulary. This example shows filtering to get only stop words.

LANGUAGE: python
CODE:
stop_words = (lex for lex in nlp.vocab if lex.is_stop)

----------------------------------------

TITLE: Predicting with SpanFinder in spaCy
DESCRIPTION: Apply the SpanFinder model to documents without modifying them, returning prediction scores.

LANGUAGE: python
CODE:
span_finder = nlp.add_pipe("span_finder")
scores = span_finder.predict([doc1, doc2])

----------------------------------------

TITLE: Configuring Llama2 Model in spaCy INI
DESCRIPTION: Example configuration for using a Llama2 model from HuggingFace in a spaCy pipeline component. Specifies the model registry and particular model variant to use.

LANGUAGE: ini
CODE:
[components.llm.model]
@llm_models = "spacy.Llama2.v1"
name = "Llama-2-7b-hf"

----------------------------------------

TITLE: Retrieving All Entity IDs from the Knowledge Base in Python
DESCRIPTION: Gets a list of all entity identifiers that exist in the knowledge base.

LANGUAGE: python
CODE:
all_entities = kb.get_entity_strings()

----------------------------------------

TITLE: Predicting spans with SpanCategorizer in spaCy (Python)
DESCRIPTION: Example showing how to apply the SpanCategorizer's model to documents without modifying them, returning prediction scores.

LANGUAGE: python
CODE:
spancat = nlp.add_pipe("spancat")
scores = spancat.predict([doc1, doc2])

----------------------------------------

TITLE: Serializing spaCy Vocabulary to Bytes
DESCRIPTION: Serializes the current state of the vocabulary to a binary string. This is useful for situations where you need to store or transmit the vocabulary without using the file system.

LANGUAGE: python
CODE:
vocab_bytes = nlp.vocab.to_bytes()

----------------------------------------

TITLE: Working with Private Assets in spaCy Project YAML
DESCRIPTION: Configuration for handling private assets that aren't publicly available. Specifies only the destination paths and checksums to allow team members to manually place files and verify their integrity.

LANGUAGE: yaml
CODE:
assets:
  - dest: 'assets/private_training_data.json'
    checksum: '63373dd656daa1fd3043ce166a59474c'
  - dest: 'assets/private_vectors.bin'
    checksum: '5113dc04e03f079525edd8df3f4f39e3'

----------------------------------------

TITLE: Initializing AttributeRuler with Tag Map in spaCy v3.0
DESCRIPTION: Shows how to configure the initialization of an AttributeRuler with a tag map from a JSON file in the config.

LANGUAGE: ini
CODE:
[initialize.components.attribute_ruler]

[initialize.components.attribute_ruler.tag_map]
@readers = "srsly.read_json.v1"
path = "./corpus/tag_map.json"

----------------------------------------

TITLE: Creating Entity Linking Training Example in spaCy
DESCRIPTION: Demonstrates how to create a training example for spaCy's Entity Linking component using the Example.from_dict method. The gold dictionary contains entity spans, candidate entity links with scores, and sentence boundary information.

LANGUAGE: python
CODE:
doc = nlp("Russ Cochran his reprints include EC Comics.")
gold_dict = {"entities": [(0, 12, "PERSON")],
             "links": {(0, 12): {"Q7381115": 1.0, "Q2146908": 0.0}},
             "sent_starts": [1, -1, -1, -1, -1, -1, -1, -1]}
example = Example.from_dict(doc, gold_dict)

----------------------------------------

TITLE: Getting the Total Number of Aliases in the Knowledge Base in Python
DESCRIPTION: Returns the count of aliases (textual mentions) stored in the knowledge base.

LANGUAGE: python
CODE:
total_aliases = kb.get_size_aliases()

----------------------------------------

TITLE: Pruning word vectors in spaCy Vocab
DESCRIPTION: Reduces the vector table to a specified number of unique entries. Words mapped to discarded vectors will be remapped to the closest remaining vector based on cosine similarity.

LANGUAGE: python
CODE:
nlp.vocab.prune_vectors(10000)
assert len(nlp.vocab.vectors) <= 10000

----------------------------------------

TITLE: Defining Linguistic Pattern for Matching 'Facebook is/was' + Adjective in Python
DESCRIPTION: Creates a pattern to match 'Facebook' followed by a form of 'be', an optional adverb, and an adjective. This pattern utilizes linguistic annotations to find what people are saying about Facebook in text.

LANGUAGE: python
CODE:
[{"LOWER": "facebook"}, {"LEMMA": "be"}, {"POS": "ADV", "OP": "*"}, {"POS": "ADJ"}]

----------------------------------------

TITLE: Initializing a Token Object from a Document in spaCy
DESCRIPTION: Example showing how to access a token from a pre-processed document. The token at index 0 in the document contains the text 'Give'.

LANGUAGE: python
CODE:
doc = nlp("Give it back! He pleaded.")
token = doc[0]
assert token.text == "Give"

----------------------------------------

TITLE: Using parameter values with SpanCategorizer in spaCy (Python)
DESCRIPTION: Example showing how to modify the SpanCategorizer's model to use specific parameter values, typically for saving the best model.

LANGUAGE: python
CODE:
spancat = nlp.add_pipe("spancat")
with spancat.use_params(optimizer.averages):
    spancat.to_disk("/best_model")

----------------------------------------

TITLE: Using find-threshold Command for textcat_multilabel in spaCy CLI
DESCRIPTION: Example of using the find-threshold command to determine optimal threshold values for textcat_multilabel component. It optimizes the threshold by maximizing the cats_macro_f metric.

LANGUAGE: bash
CODE:
# For textcat_multilabel:
$ python -m spacy find-threshold my_nlp data.spacy textcat_multilabel threshold cats_macro_f

----------------------------------------

TITLE: Adding multiple spans to a SpanGroup with extend
DESCRIPTION: Shows how to add multiple spans to a SpanGroup at once using the extend method. Works with both iterables of spans and other SpanGroups.

LANGUAGE: python
CODE:
doc = nlp("Their goi ng home")
doc.spans["errors"] = []
doc.spans["errors"].extend([doc[1:3], doc[0:1]])
assert len(doc.spans["errors"]) == 2
span_group = SpanGroup(doc, spans=[doc[1:4], doc[0:3]])
doc.spans["errors"].extend(span_group)

----------------------------------------

TITLE: Initializing PlainTextCorpus in spaCy (Python)
DESCRIPTION: Creates a new PlainTextCorpus object by specifying a file path for reading text data. The corpus expects UTF8-encoded text with documents separated by newlines.

LANGUAGE: python
CODE:
from spacy.training import PlainTextCorpus

corpus = PlainTextCorpus("./data/docs.txt")

----------------------------------------

TITLE: Configuring SpanCategorizer (spancat) in spaCy
DESCRIPTION: Example showing how to configure and add the 'spancat' pipeline component to a spaCy NLP object with custom settings including threshold, spans key, and an n-gram suggester.

LANGUAGE: python
CODE:
from spacy.pipeline.spancat import DEFAULT_SPANCAT_MODEL
config = {
    "threshold": 0.5,
    "spans_key": "labeled_spans",
    "max_positive": None,
    "model": DEFAULT_SPANCAT_MODEL,
    "suggester": {"@misc": "spacy.ngram_suggester.v1", "sizes": [1, 2, 3]},
}
nlp.add_pipe("spancat", config=config)

----------------------------------------

TITLE: Setting Entity Annotations with EntityLinker in spaCy (Python)
DESCRIPTION: Modifies a batch of documents by applying pre-computed knowledge base identifiers to the entities. This uses the results from the predict method to update the documents with entity links.

LANGUAGE: python
CODE:
entity_linker = nlp.add_pipe("entity_linker")
kb_ids = entity_linker.predict([doc1, doc2])
entity_linker.set_annotations([doc1, doc2], kb_ids)

----------------------------------------

TITLE: Creating an Optimizer for Tok2Vec Pipeline Component in Python
DESCRIPTION: Creates an optimizer for the Tok2Vec pipeline component. The method returns an Optimizer object that can be used for training the Tok2Vec component.

LANGUAGE: python
CODE:
tok2vec = nlp.add_pipe("tok2vec")
optimizer = tok2vec.create_optimizer()

----------------------------------------

TITLE: Initializing Labels Command in spaCy CLI
DESCRIPTION: Command for generating JSON files for labels in the data to speed up the training process. The init labels command accepts a config path, output path, and various options.

LANGUAGE: bash
CODE:
$ python -m spacy init labels [config_path] [output_path] [--code] [--verbose] [--gpu-id] [overrides]

----------------------------------------

TITLE: Getting Entity Candidates for an Alias String in Python
DESCRIPTION: Retrieves candidate entities for a given alias string, returning Candidate objects with entity IDs and probabilities.

LANGUAGE: python
CODE:
candidates = kb.get_alias_candidates("Douglas")

----------------------------------------

TITLE: Configuring spacy.NER.v2 Task in INI format
DESCRIPTION: Basic configuration for the spacy.NER.v2 task component with label definitions. This version supports explicitly defining entity labels with custom descriptions and supports zero-shot and few-shot prompting.

LANGUAGE: ini
CODE:
[components.llm.task]
@llm_tasks = "spacy.NER.v2"
labels = ["PERSON", "ORGANISATION", "LOCATION"]
examples = null

----------------------------------------

TITLE: Checking for registered extensions on Span objects in spaCy
DESCRIPTION: Demonstrates how to check if a specific extension has been registered on the Span class using has_extension. This is useful for conditional logic based on available extensions.

LANGUAGE: python
CODE:
from spacy.tokens import Span
Span.set_extension("is_city", default=False)
assert Span.has_extension("is_city")

----------------------------------------

TITLE: Installing Transformer-based Pipeline in spaCy v3.0
DESCRIPTION: Command to download and install the English transformer-based pipeline model using the spaCy CLI. This model uses RoBERTa and provides state-of-the-art accuracy for NLP tasks.

LANGUAGE: bash
CODE:
$ python -m spacy download en_core_web_trf

----------------------------------------

TITLE: Serializing TextCategorizer to Bytes in Python
DESCRIPTION: Demonstrates how to serialize a TextCategorizer component to a bytestring, which can be useful for storing or transmitting the model in binary form.

LANGUAGE: python
CODE:
textcat = nlp.add_pipe("textcat")
textcat_bytes = textcat.to_bytes()

----------------------------------------

TITLE: Installing spacy-llm Package
DESCRIPTION: Command to install the spacy-llm package using pip in the same virtual environment where spaCy is already installed.

LANGUAGE: bash
CODE:
python -m pip install spacy-llm

----------------------------------------

TITLE: Checking for Match ID with Matcher.__contains__
DESCRIPTION: Demonstrates how to check whether the matcher contains rules for a specific match ID using the 'in' operator.

LANGUAGE: python
CODE:
matcher = Matcher(nlp.vocab)
assert "Rule" not in matcher
matcher.add("Rule", [[{'ORTH': 'test'}]])
assert "Rule" in matcher

----------------------------------------

TITLE: Loading StringStore from Disk in Python
DESCRIPTION: Demonstrates how to load a StringStore's state from a directory and return the modified object.

LANGUAGE: python
CODE:
from spacy.strings import StringStore
stringstore = StringStore().from_disk("/path/to/strings")

----------------------------------------

TITLE: Setting Morphological Features on a Token in spaCy
DESCRIPTION: Example showing how to set morphological analysis on a token using a UD FEATS string. The set_morph method allows setting features like verb mood and form.

LANGUAGE: python
CODE:
doc = nlp("Give it back! He pleaded.")
doc[0].set_morph("Mood=Imp|VerbForm=Fin")
assert "Mood=Imp" in doc[0].morph
assert doc[0].morph.get("Mood") == ["Imp"]

----------------------------------------

TITLE: Creating optimizer for SpanCategorizer in spaCy (Python)
DESCRIPTION: Example demonstrating how to create an optimizer for the SpanCategorizer component.

LANGUAGE: python
CODE:
spancat = nlp.add_pipe("spancat")
optimizer = spancat.create_optimizer()

----------------------------------------

TITLE: Retrieving a Table from Lookups
DESCRIPTION: Gets a table from the Lookups object by name. Raises an error if the table doesn't exist.

LANGUAGE: python
CODE:
lookups = Lookups()
lookups.add_table("some_table", {"foo": "bar"})
table = lookups.get_table("some_table")
assert table["foo"] == "bar"

----------------------------------------

TITLE: Initializing SpanResolver Component in Python
DESCRIPTION: Different ways to initialize the SpanResolver component: via add_pipe with default model, with custom model configuration, or by direct instantiation from the class. Each method creates a pipeline component instance.

LANGUAGE: python
CODE:
# Construction via add_pipe with default model
span_resolver = nlp.add_pipe("experimental_span_resolver")

# Construction via add_pipe with custom model
config = {"model": {"@architectures": "my_span_resolver.v1"}}
span_resolver = nlp.add_pipe("experimental_span_resolver", config=config)

# Construction from class
from spacy_experimental.coref.span_resolver_component import SpanResolver
span_resolver = SpanResolver(nlp.vocab, model)

----------------------------------------

TITLE: Checking Vector Table Length in spaCy Python
DESCRIPTION: Example of determining the number of vectors in a vector table using the len() function.

LANGUAGE: python
CODE:
vectors = Vectors(shape=(3, 300))
assert len(vectors) == 3

----------------------------------------

TITLE: Setting annotations with SpanCategorizer in spaCy (Python)
DESCRIPTION: Example demonstrating how to modify Doc objects by applying pre-computed scores from the SpanCategorizer's prediction.

LANGUAGE: python
CODE:
spancat = nlp.add_pipe("spancat")
scores = spancat.predict(docs)
spancat.set_annotations(docs, scores)

----------------------------------------

TITLE: Adding Rules to Matcher with Matcher.add
DESCRIPTION: Shows how to add a rule to the matcher, consisting of an ID key, patterns, and an optional callback function. The callback function receives the arguments matcher, doc, i, and matches when a pattern is matched.

LANGUAGE: python
CODE:
def on_match(matcher, doc, id, matches):
    print('Matched!', matches)

matcher = Matcher(nlp.vocab)
patterns = [
   [{"LOWER": "hello"}, {"LOWER": "world"}],
   [{"ORTH": "Google"}, {"ORTH": "Maps"}]
]
matcher.add("TEST_PATTERNS", patterns, on_match=on_match)
doc = nlp("HELLO WORLD on Google Maps.")
matches = matcher(doc)

----------------------------------------

TITLE: Using parameter values in SpanResolver with use_params in spaCy
DESCRIPTION: Shows how to temporarily modify the model's parameters using the use_params context manager, which is useful for saving the best model based on averaged weights.

LANGUAGE: python
CODE:
span_resolver = nlp.add_pipe("experimental_span_resolver")
with span_resolver.use_params(optimizer.averages):
    span_resolver.to_disk("/best_model")

----------------------------------------

TITLE: Serializing Tok2Vec Component to Disk in Python
DESCRIPTION: Serializes the Tok2Vec pipeline component to disk at the specified path. The path can be either a string or a Path-like object. The method supports excluding specific serialization fields via the exclude parameter.

LANGUAGE: python
CODE:
tok2vec = nlp.add_pipe("tok2vec")
tok2vec.to_disk("/path/to/tok2vec")

----------------------------------------

TITLE: Loading Lexemes with Vectors in spaCy v2.3
DESCRIPTION: Code to explicitly load all lexemes for words with vectors at once, which might be needed when working directly with word vectors rather than processing texts in spaCy v2.3.

LANGUAGE: python
CODE:
for orth in nlp.vocab.vectors:
    _ = nlp.vocab[orth]

----------------------------------------

TITLE: Processing Document Stream with CoreferenceResolver
DESCRIPTION: Example demonstrating how to process a stream of documents with the CoreferenceResolver component, using the pipe method with batch processing.

LANGUAGE: python
CODE:
coref = nlp.add_pipe("experimental_coref")
for doc in coref.pipe(docs, batch_size=50):
    pass

----------------------------------------

TITLE: Processing Document Stream with CoreferenceResolver
DESCRIPTION: Example demonstrating how to process a stream of documents with the CoreferenceResolver component, using the pipe method with batch processing.

LANGUAGE: python
CODE:
coref = nlp.add_pipe("experimental_coref")
for doc in coref.pipe(docs, batch_size=50):
    pass

----------------------------------------

TITLE: Applying Annotations with a TrainablePipe in Python
DESCRIPTION: Example showing how to modify documents using pre-computed scores with the set_annotations method. This method must be overridden in custom components.

LANGUAGE: python
CODE:
pipe = nlp.add_pipe("your_custom_pipe")
scores = pipe.predict(docs)
pipe.set_annotations(docs, scores)

----------------------------------------

TITLE: Retrieving an Entity Vector from the Knowledge Base in Python
DESCRIPTION: Gets the pre-trained vector representation for a specific entity in the knowledge base.

LANGUAGE: python
CODE:
vector = kb.get_vector("Q42")

----------------------------------------

TITLE: Creating a Table from Dictionary
DESCRIPTION: Creates a new Table object from an existing dictionary using the from_dict class method.

LANGUAGE: python
CODE:
from spacy.lookups import Table
data = {"foo": "bar", "baz": 100}
table = Table.from_dict(data, name="some_table")

----------------------------------------

TITLE: Adding Documents to DocBin in Python
DESCRIPTION: Shows how to add a spaCy Doc object's annotations to a DocBin instance for later serialization.

LANGUAGE: python
CODE:
doc_bin = DocBin(attrs=["LEMMA"])
doc = nlp("This is a document to serialize.")
doc_bin.add(doc)

----------------------------------------

TITLE: Evaluating Span Predictions Accuracy in Python
DESCRIPTION: Calculates accuracy for reconstructions of spans from single tokens. This experimental function evaluates exact matches with no partial credit, used by the SpanResolver component.

LANGUAGE: python
CODE:
scores = score_span_predictions(
    examples,
    output_prefix="coref_clusters",
)
print(scores["span_coref_clusters_accuracy"])

----------------------------------------

TITLE: Retrieving Entity Candidates for Multiple Spans in Python
DESCRIPTION: Gets candidate entities for multiple text spans in a batch operation, which may be more efficient than processing individual spans.

LANGUAGE: python
CODE:
from spacy.lang.en import English
nlp = English()
doc = nlp("Douglas Adams wrote 'The Hitchhiker's Guide to the Galaxy'.")
candidates = kb.get_candidates((doc[0:2], doc[3:]))

----------------------------------------

TITLE: Setting Output Dimension for a Resizable Pipeline Component
DESCRIPTION: Demonstrates how to change the output dimension of a resizable pipeline component. This only works if the component's is_resizable property returns True.

LANGUAGE: python
CODE:
if pipe.is_resizable:
    pipe.set_output(512)

----------------------------------------

TITLE: Saving Knowledge Base to Disk in spaCy InMemoryLookupKB
DESCRIPTION: Shows how to save the current state of a knowledge base to a directory on disk. The method accepts a path parameter which can be a string or Path-like object.

LANGUAGE: python
CODE:
kb.to_disk(path)

----------------------------------------

TITLE: Initializing Vector Store in spaCy Python
DESCRIPTION: Examples of initializing vector stores in spaCy, including creating an empty vector store with specific dimensions and creating a vector store with predefined data and keys.

LANGUAGE: python
CODE:
from spacy.vectors import Vectors

empty_vectors = Vectors(shape=(10000, 300))

data = numpy.zeros((3, 300), dtype='f')
keys = ["cat", "dog", "rat"]
vectors = Vectors(data=data, keys=keys)

----------------------------------------

TITLE: Processing a Stream of Documents with TrainablePipe in Python
DESCRIPTION: Example showing how to apply a pipeline component to a stream of documents using the pipe method with batch processing. This is useful for processing large volumes of text efficiently.

LANGUAGE: python
CODE:
pipe = nlp.add_pipe("your_custom_pipe")
for doc in pipe.pipe(docs, batch_size=50):
    pass

----------------------------------------

TITLE: Removing a Doc Extension in spaCy
DESCRIPTION: Shows how to remove a previously registered extension from the Doc class using remove_extension(). Returns a tuple containing the removed extension's properties.

LANGUAGE: python
CODE:
from spacy.tokens import Doc
Doc.set_extension("has_city", default=False)
removed = Doc.remove_extension("has_city")
assert not Doc.has_extension("has_city")

----------------------------------------

TITLE: Counting Rightward Children in spaCy
DESCRIPTION: Example showing how to get the number of rightward immediate children of a token using the n_rights property. This returns a count of the tokens to the right whose head is the current token.

LANGUAGE: python
CODE:
doc = nlp("I like New York in Autumn.")
assert doc[3].n_rights == 1

----------------------------------------

TITLE: Excluding serialization fields in SpanResolver for spaCy
DESCRIPTION: Shows how to exclude specific serialization fields when saving the SpanResolver component, which can be useful for customizing what gets saved.

LANGUAGE: python
CODE:
data = span_resolver.to_disk("/path", exclude=["vocab"])

----------------------------------------

TITLE: Loading Vectors from Disk in spaCy (Python)
DESCRIPTION: Shows how to load vectors from a directory using the from_disk method, which modifies the object in place and returns it.

LANGUAGE: python
CODE:
vectors = Vectors(StringStore())
vectors.from_disk("/path/to/vectors")

----------------------------------------

TITLE: Saving SpanRuler Patterns to Disk in Python
DESCRIPTION: Shows how to save SpanRuler patterns to disk as JSONL using the to_disk method.

LANGUAGE: python
CODE:
ruler = nlp.add_pipe("span_ruler")
ruler.to_disk("/path/to/span_ruler")

----------------------------------------

TITLE: Configuring PretrainCharacters in spaCy
DESCRIPTION: Configuration example for the PretrainCharacters pretraining objective in spaCy. This objective predicts leading and trailing UTF-8 bytes as a pretraining task for a Tok2Vec layer. The configuration includes component setup and parameters for character prediction.

LANGUAGE: ini
CODE:
[pretraining]
component = "tok2vec"
...

[pretraining.objective]
@architectures = "spacy.PretrainCharacters.v1"
maxout_pieces = 3
hidden_size = 300
n_characters = 4

----------------------------------------

TITLE: Detecting and Processing Hashtags with Custom Token Extensions in spaCy
DESCRIPTION: Demonstrates how to detect hashtags in text, merge them into single tokens, and mark them with a custom attribute. This executable example uses the Matcher and token retokenization to process hashtags.

LANGUAGE: python
CODE:
import spacy
from spacy.matcher import Matcher
from spacy.tokens import Token

nlp = spacy.load("en_core_web_sm")
matcher = Matcher(nlp.vocab)

# Add pattern for valid hashtag, i.e. '#' plus any ASCII token
matcher.add("HASHTAG", [[{"ORTH": "#"}, {"IS_ASCII": True}]])

# Register token extension
Token.set_extension("is_hashtag", default=False)

doc = nlp("Hello world 😀 #MondayMotivation")
matches = matcher(doc)
hashtags = []
for match_id, start, end in matches:
    if doc.vocab.strings[match_id] == "HASHTAG":
        hashtags.append(doc[start:end])
with doc.retokenize() as retokenizer:
    for span in hashtags:
        retokenizer.merge(span)
        for token in span:
            token._.is_hashtag = True

for token in doc:
    print(token.text, token._.is_hashtag)

----------------------------------------

TITLE: Creating Code Blocks in JSX
DESCRIPTION: Shows how to create code blocks using JSX syntax. The CodeBlock component takes title and lang properties to specify the title and programming language.

LANGUAGE: jsx
CODE:
<CodeBlock title="This is a title" lang="python">
  import spacy
</CodeBlock>

----------------------------------------

TITLE: Overriding Project Variables via Command Line
DESCRIPTION: Command line example showing how to override project variables when executing a spaCy project command.

LANGUAGE: bash
CODE:
python -m spacy project run test . --vars.foo bar

----------------------------------------

TITLE: Accessing Labels from an LLM Component in spaCy
DESCRIPTION: Demonstrates how to access the labels property of an LLM component to retrieve the currently added labels. Shows how to verify that a label has been successfully added to the component.

LANGUAGE: python
CODE:
llm_ner.add_label("MY_LABEL")
assert "MY_LABEL" in llm_ner.labels

----------------------------------------

TITLE: Iterating Through Span.rights in spaCy
DESCRIPTION: Shows how to access tokens to the right of a span whose heads are within the span. In this example, 'in' is a right dependent of a token within the span 'New York'.

LANGUAGE: python
CODE:
doc = nlp("I like New York in Autumn.")
rights = [t.text for t in doc[2:4].rights]
assert rights == ["in"]

----------------------------------------

TITLE: Calculating Loss with SpanFinder in spaCy
DESCRIPTION: Calculate the loss and gradient of loss for a batch of documents and their predicted scores.

LANGUAGE: python
CODE:
span_finder = nlp.add_pipe("span_finder")
scores = span_finder.predict([eg.predicted for eg in examples])
loss, d_loss = span_finder.get_loss(examples, scores)

----------------------------------------

TITLE: Predicting Dependency Parsing Results in spaCy
DESCRIPTION: Demonstrates how to use the predict method of a dependency parser to generate parse scores for documents without modifying them.

LANGUAGE: python
CODE:
parser = nlp.add_pipe("parser")
scores = parser.predict([doc1, doc2])

----------------------------------------

TITLE: Updating serialization methods in spaCy v2.1.x
DESCRIPTION: Demonstrates changes to serialization method arguments, replacing the 'disable' parameter with 'exclude' for consistency across all serialization methods. The exclude parameter accepts a list of string names to exclude from serialization.

LANGUAGE: diff
CODE:
- nlp.to_disk("/path", disable=["parser", "ner"])
+ nlp.to_disk("/path", exclude=["parser", "ner"])
- data = nlp.tokenizer.to_bytes(vocab=False)
+ data = nlp.tokenizer.to_bytes(exclude=["vocab"])

----------------------------------------

TITLE: Retrieving All Aliases from the Knowledge Base in Python
DESCRIPTION: Gets a list of all alias strings that exist in the knowledge base.

LANGUAGE: python
CODE:
all_aliases = kb.get_alias_strings()

----------------------------------------

TITLE: Retrieving Entity Vectors from a KnowledgeBase in Python
DESCRIPTION: Example showing how to retrieve a pretrained entity vector for a specific entity ID from a KnowledgeBase instance.

LANGUAGE: python
CODE:
vector = kb.get_vector("Q42")

----------------------------------------

TITLE: Configuring Few-Shot Reader for NER Tasks in INI format
DESCRIPTION: Configuration for loading few-shot examples from an external file. Uses the spacy.FewShotReader.v1 component to read examples from a YAML file for NER tasks.

LANGUAGE: ini
CODE:
[components.llm.task.examples]
@misc = "spacy.FewShotReader.v1"
path = "ner_examples.yml"

----------------------------------------

TITLE: Loading SpanResolver from disk in spaCy
DESCRIPTION: Shows how to load a previously saved SpanResolver component from disk using the from_disk method, which modifies the object in place and returns it.

LANGUAGE: python
CODE:
span_resolver = nlp.add_pipe("experimental_span_resolver")
span_resolver.from_disk("/path/to/span_resolver")

----------------------------------------

TITLE: Creating Buttons with JSX Components in spaCy Documentation
DESCRIPTION: How to create styled link buttons using the Button JSX component in primary and secondary variants for the spaCy documentation.

LANGUAGE: jsx
CODE:
<Button to="#" variant="primary">Primary small</Button>
<Button to="#" variant="secondary">Secondary small</Button>

----------------------------------------

TITLE: Updating SpanFinder Model in spaCy
DESCRIPTION: Learn from examples containing predictions and gold-standard annotations to update the model. Uses the predict and get_loss methods internally.

LANGUAGE: python
CODE:
span_finder = nlp.add_pipe("span_finder")
optimizer = nlp.initialize()
losses = span_finder.update(examples, sgd=optimizer)

----------------------------------------

TITLE: Initializing JsonlCorpus for Raw Text Processing
DESCRIPTION: Example showing how to initialize a JsonlCorpus object from a JSONL file containing raw text data for pretraining or other processing tasks.

LANGUAGE: python
CODE:
from spacy.training import JsonlCorpus

corpus = JsonlCorpus("./data/texts.jsonl")

----------------------------------------

TITLE: Processing Document Stream with Tagger in spaCy
DESCRIPTION: Example demonstrating how to apply the tagger to a stream of documents with batching for efficiency. This is useful for processing large collections of texts.

LANGUAGE: python
CODE:
tagger = nlp.add_pipe("tagger")
for doc in tagger.pipe(docs, batch_size=50):
    pass

----------------------------------------

TITLE: Getting Document Length in spaCy
DESCRIPTION: Demonstration of getting the number of tokens in a Doc object using the len() function, which counts the total number of tokens in the processed text.

LANGUAGE: python
CODE:
doc = nlp("Give it back! He pleaded.")
assert len(doc) == 7

----------------------------------------

TITLE: Using Custom Parameters with SentenceRecognizer in spaCy
DESCRIPTION: Demonstrates how to temporarily modify the component's model to use custom parameter values within a context manager. After the context ends, original parameters are restored.

LANGUAGE: python
CODE:
senter = nlp.add_pipe("senter")
with senter.use_params(optimizer.averages):
    senter.to_disk("/best_model")

----------------------------------------

TITLE: Setting a Custom Error Handler for TrainablePipe in Python
DESCRIPTION: Example showing how to define and set a custom error handler for a pipeline component. The error handler is called when exceptions occur during document processing.

LANGUAGE: python
CODE:
def warn_error(proc_name, proc, docs, e):
    print(f"An error occurred when applying component {proc_name}.")

pipe = nlp.add_pipe("ner")
pipe.set_error_handler(warn_error)

----------------------------------------

TITLE: Creating Accordion Components
DESCRIPTION: Shows how to create collapsible accordion sections using JSX syntax. Accordions are useful for hiding lengthy content that can be expanded when needed.

LANGUAGE: jsx
CODE:
<Accordion title="This is an accordion">
  Accordion content goes here.
</Accordion>

----------------------------------------

TITLE: Implementing Markdown Headings with Custom Attributes in spaCy Documentation
DESCRIPTION: Examples of how to create headings in Markdown with optional attributes like custom IDs and tags that are used throughout the spaCy documentation.

LANGUAGE: markdown
CODE:
## Headline 2

## Headline 2 {id="some_id"}

## Headline 2 {id="some_id" tag="method"}

----------------------------------------

TITLE: Installing spaCy using conda
DESCRIPTION: Command to install spaCy via conda-forge channel using the conda package manager.

LANGUAGE: bash
CODE:
$ conda install -c conda-forge spacy

----------------------------------------

TITLE: Defining Command Dependencies and Outputs in spaCy Project YAML
DESCRIPTION: Configuration for a training command with dependencies and outputs. Specifies the required input files and the output location for the trained model, enabling intelligent skipping of unchanged commands.

LANGUAGE: yaml
CODE:
commands:
  - name: train
    help: 'Train a spaCy pipeline using the specified corpus and config'
    script:
      - 'python -m spacy train ./configs/config.cfg -o training/ --paths.train ./corpus/training.spacy --paths.dev ./corpus/evaluation.spacy'
    deps:
      - 'configs/config.cfg'
      - 'corpus/training.spacy'
      - 'corpus/evaluation.spacy'
    outputs:
      - 'training/model-best'

----------------------------------------

TITLE: Using get_aligned_spans_x2y Method in spaCy Example Class
DESCRIPTION: Demonstrates how to align predicted spans to reference tokenization, useful for assessing entity recognition accuracy against gold-standard annotations.

LANGUAGE: python
CODE:
nlp.add_pipe("my_ner")
doc = nlp("Mr and Mrs Smith flew to New York")
tokens_ref = ["Mr and Mrs", "Smith", "flew", "to", "New York"]
example = Example.from_dict(doc, {"words": tokens_ref})
ents_pred = example.predicted.ents
# Assume the NER model has found "Mr and Mrs Smith" as a named entity
assert [(ent.start, ent.end) for ent in ents_pred] == [(0, 4)]
ents_x2y = example.get_aligned_spans_x2y(ents_pred)
assert [(ent.start, ent.end) for ent in ents_x2y] == [(0, 2)]

----------------------------------------

TITLE: Loading SpanResolver from bytes in spaCy
DESCRIPTION: Shows how to load a SpanResolver component from a bytestring using the from_bytes method, which modifies the object in place and returns it.

LANGUAGE: python
CODE:
span_resolver_bytes = span_resolver.to_bytes()
span_resolver = nlp.add_pipe("experimental_span_resolver")
span_resolver.from_bytes(span_resolver_bytes)

----------------------------------------

TITLE: Iterating Through Span.lefts in spaCy
DESCRIPTION: Demonstrates how to access tokens to the left of a span whose heads are within the span. In this example, 'New' is a left dependent of a token within the span 'York in Autumn'.

LANGUAGE: python
CODE:
doc = nlp("I like New York in Autumn.")
lefts = [t.text for t in doc[3:7].lefts]
assert lefts == ["New"]

----------------------------------------

TITLE: Configuring a SentenceRecognizer with Default Model in spaCy
DESCRIPTION: Example showing how to configure a SentenceRecognizer component with the default model settings. This demonstrates importing the default model configuration and adding the component to the NLP pipeline.

LANGUAGE: python
CODE:
from spacy.pipeline.senter import DEFAULT_SENTER_MODEL
config = {"model": DEFAULT_SENTER_MODEL,}
nlp.add_pipe("senter", config=config)

----------------------------------------

TITLE: Checking if Vectors Table is Full in spaCy (Python)
DESCRIPTION: Demonstrates how to check if the vectors table is full using the is_full property. A full table can be resized using Vectors.resize, except in 'floret' mode where the table is always full.

LANGUAGE: python
CODE:
vectors = Vectors(shape=(1, 300))
vectors.add("cat", numpy.random.uniform(-1, 1, (300,)))
assert vectors.is_full

----------------------------------------

TITLE: Example LLM Lemmatization Output Format
DESCRIPTION: Shows the expected output format when using the Lemma LLM task. The output provides token-lemma pairs for each word in the input text.

LANGUAGE: text
CODE:
I: I
'm: be
buying: buy
ice: ice
cream: cream
for: for
my: my
friends: friend
.: .

----------------------------------------

TITLE: Getting Aligned Spans from Reference to Predicted Tokenization in spaCy
DESCRIPTION: Shows how to map Span objects from reference tokenization to predicted tokenization using get_aligned_spans_y2x, useful for converting entity spans between different tokenization schemes.

LANGUAGE: python
CODE:
words = ["Mr and Mrs Smith", "flew", "to", "New York"]
doc = Doc(en_vocab, words=words)
entities = [(0, 16, "PERSON")]
tokens_ref = ["Mr", "and", "Mrs", "Smith", "flew", "to", "New", "York"]
example = Example.from_dict(doc, {"words": tokens_ref, "entities": entities})
ents_ref = example.reference.ents
assert [(ent.start, ent.end) for ent in ents_ref] == [(0, 4)]
ents_y2x = example.get_aligned_spans_y2x(ents_ref)
assert [(ent.start, ent.end) for ent in ents_y2x] == [(0, 1)]

----------------------------------------

TITLE: Requesting General Bot Information
DESCRIPTION: Command to get general information about the Explosion-bot and its supported commands.

LANGUAGE: markdown
CODE:
@explosion-bot please info

----------------------------------------

TITLE: Initializing Lookups in spaCy
DESCRIPTION: Creates a new Lookups object that can be used to store large lookup tables and dictionaries for use in spaCy pipelines.

LANGUAGE: python
CODE:
from spacy.lookups import Lookups
lookups = Lookups()

----------------------------------------

TITLE: Running spaCy Tests with Different Test Suites
DESCRIPTION: Commands to update pytest and run different test suites for spaCy. Shows how to run basic tests or include slower, more comprehensive tests using the --slow flag.

LANGUAGE: bash
CODE:
$ python -m pip install -U pytest               # update pytest
$ python -m pytest --pyargs %%SPACY_PKG_NAME               # basic tests
$ python -m pytest --pyargs %%SPACY_PKG_NAME --slow        # basic and slow tests

----------------------------------------

TITLE: Getting Information for Specific spaCy Model
DESCRIPTION: Command that prints detailed information about a specific trained pipeline model. Supports options for Markdown output, silent operation, and excluding specific information.

LANGUAGE: bash
CODE:
$ python -m spacy info [model] [--markdown] [--silent] [--exclude]

----------------------------------------

TITLE: Loading SpanFinder from Bytes in spaCy
DESCRIPTION: Reconstruct a SpanFinder component from its bytestring representation, modifying the object in place.

LANGUAGE: python
CODE:
span_finder_bytes = span_finder.to_bytes()
span_finder = nlp.add_pipe("span_finder")
span_finder.from_bytes(span_finder_bytes)

----------------------------------------

TITLE: Loading spaCy Lemmatizer from Disk
DESCRIPTION: Shows how to load a previously saved lemmatizer component from disk. This method loads the serialized model and modifies the current lemmatizer object in place.

LANGUAGE: python
CODE:
lemmatizer = nlp.add_pipe("lemmatizer")
lemmatizer.from_disk("/path/to/lemmatizer")

----------------------------------------

TITLE: Initializing Tok2Vec Component in spaCy Pipeline
DESCRIPTION: Different methods to construct and initialize a Tok2Vec component in spaCy, including via add_pipe with default or custom models, and direct instantiation from the class.

LANGUAGE: python
CODE:
# Construction via add_pipe with default model
tok2vec = nlp.add_pipe("tok2vec")

# Construction via add_pipe with custom model
config = {"model": {"@architectures": "my_tok2vec"}}
parser = nlp.add_pipe("tok2vec", config=config)

# Construction from class
from spacy.pipeline import Tok2Vec
tok2vec = Tok2Vec(nlp.vocab, model)

----------------------------------------

TITLE: Serializing a custom pipe to bytes in spaCy
DESCRIPTION: Demonstrates converting a pipeline component to a bytestring representation, which is useful for database storage or transmission over networks.

LANGUAGE: python
CODE:
pipe = nlp.add_pipe("your_custom_pipe")
pipe_bytes = pipe.to_bytes()

----------------------------------------

TITLE: Basic Entity Recognition with spaCy for Employment Information
DESCRIPTION: This snippet loads a spaCy model and identifies named entities in a text about employment. It's the initial step before analyzing syntactic relationships to determine employment status.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Alex Smith worked at Acme Corp Inc.")
print([(ent.text, ent.label_) for ent in doc.ents])

----------------------------------------

TITLE: Saving a CoreferenceResolver Model to Disk in spaCy (Python)
DESCRIPTION: Demonstrates how to serialize a coreference resolution model to disk for later use or distribution.

LANGUAGE: python
CODE:
coref = nlp.add_pipe("experimental_coref")
coref.to_disk("/path/to/coref")

----------------------------------------

TITLE: Example S3 Storage Structure
DESCRIPTION: Example showing the hierarchical structure of files in an S3 bucket after pushing a trained model, with URL encoding and nested hash-based directories.

LANGUAGE: bash
CODE:
└── s3://my-spacy-bucket/training%2Fmodel-best
    └── 1d8cb33a06cc345ad3761c6050934a1b
        └── d8e20c3537a084c5c10d95899fe0b1ff

----------------------------------------

TITLE: Getting Pipe Metadata for Named Pipeline Components in spaCy with Python
DESCRIPTION: Example demonstrating how to retrieve metadata for a specific pipeline component by its instance name, which may be different from its factory name, using the get_pipe_meta method.

LANGUAGE: python
CODE:
nlp.add_pipe("ner", name="entity_recognizer")
factory_meta = nlp.get_pipe_meta("entity_recognizer")
assert factory_meta.factory == "ner"
print(factory_meta.default_config)

----------------------------------------

TITLE: Iterating through spans in a SpanGroup
DESCRIPTION: Shows how to iterate through all spans in a SpanGroup using a for loop.

LANGUAGE: python
CODE:
doc = nlp("Their goi ng home")
doc.spans["errors"] = [doc[0:1], doc[1:3]]
for error_span in doc.spans["errors"]:
    print(error_span)

----------------------------------------

TITLE: Adding Labels to Morphologizer in spaCy (Python)
DESCRIPTION: Adds a new morphological label to the Morphologizer pipeline component. The label should include the UPOS as the POS feature if the component should set annotations for both part-of-speech and morphology.

LANGUAGE: python
CODE:
morphologizer = nlp.add_pipe("morphologizer")
morphologizer.add_label("Mood=Ind|POS=VERB|Tense=Past|VerbForm=Fin")

----------------------------------------

TITLE: Removing Patterns by Label from SpanRuler in Python
DESCRIPTION: Demonstrates how to remove patterns with a specific label from a SpanRuler using the remove method.

LANGUAGE: python
CODE:
patterns = [{"label": "ORG", "pattern": "Apple", "id": "apple"}]
ruler = nlp.add_pipe("span_ruler")
ruler.add_patterns(patterns)
ruler.remove("ORG")

----------------------------------------

TITLE: Using find-threshold Command for spancat in spaCy CLI
DESCRIPTION: Example of using the find-threshold command to determine optimal threshold values for the spancat component. It optimizes the threshold by maximizing the spans_sc_f metric.

LANGUAGE: bash
CODE:
# For spancat:
$ python -m spacy find-threshold my_nlp data.spacy spancat threshold spans_sc_f

----------------------------------------

TITLE: Initializing EditTreeLemmatizer in spaCy
DESCRIPTION: Initialize the lemmatizer component for training with examples and optional label data. This can be done programmatically or through a configuration file, which allows specifying label paths.

LANGUAGE: python
CODE:
lemmatizer = nlp.add_pipe("trainable_lemmatizer", name="lemmatizer")
lemmatizer.initialize(lambda: examples, nlp=nlp)

LANGUAGE: ini
CODE:
### config.cfg
[initialize.components.lemmatizer]

[initialize.components.lemmatizer.labels]
@readers = "spacy.read_labels.v1"
path = "corpus/labels/lemmatizer.json

----------------------------------------

TITLE: Getting Entity Candidates for a Span in Python
DESCRIPTION: Retrieves potential entity candidates for a given text span, returning Candidate objects with entity IDs and probabilities.

LANGUAGE: python
CODE:
from spacy.lang.en import English
nlp = English()
doc = nlp("Douglas Adams wrote 'The Hitchhiker's Guide to the Galaxy'.")
candidates = kb.get_candidates(doc[0:2])

----------------------------------------

TITLE: Instantiating SpaCy Tokenizer
DESCRIPTION: Examples showing two ways to create a Tokenizer: constructing it directly with a vocabulary, or accessing the tokenizer from a language pipeline that already has proper settings for the language.

LANGUAGE: python
CODE:
# Construction 1
from spacy.tokenizer import Tokenizer
from spacy.lang.en import English
nlp = English()
# Create a blank Tokenizer with just the English vocab
tokenizer = Tokenizer(nlp.vocab)

# Construction 2
from spacy.lang.en import English
nlp = English()
# Create a Tokenizer with the default settings for English
# including punctuation rules and exceptions
tokenizer = nlp.tokenizer

----------------------------------------

TITLE: Error: No Compatible Model Found
DESCRIPTION: Shows the error message displayed when a user attempts to download a trained pipeline that doesn't exist or isn't compatible with their version of spaCy.

LANGUAGE: bash
CODE:
No compatible package found for [lang] (spaCy vX.X.X).

----------------------------------------

TITLE: Compiling Prefix Regex for Tokenizer
DESCRIPTION: Compiles a sequence of prefix rules into a regex object that can be used for the tokenizer's prefix search functionality.

LANGUAGE: python
CODE:
prefixes = ("§", "%", "=", r"\+")
prefix_regex = util.compile_prefix_regex(prefixes)
nlp.tokenizer.prefix_search = prefix_regex.search

----------------------------------------

TITLE: Updating a CoreferenceResolver Model in spaCy (Python)
DESCRIPTION: Shows how to train a coreference resolution model using a batch of examples. The method returns loss values that can be used to track training progress.

LANGUAGE: python
CODE:
coref = nlp.add_pipe("experimental_coref")
optimizer = nlp.initialize()
losses = coref.update(examples, sgd=optimizer)

----------------------------------------

TITLE: Excluding Fields During Tok2Vec Serialization in Python
DESCRIPTION: Example of how to exclude specific serialization fields when serializing a Tok2Vec component. This can be useful when you want to preserve certain aspects of the component while updating others during serialization/deserialization.

LANGUAGE: python
CODE:
data = tok2vec.to_disk("/path", exclude=["vocab"])

----------------------------------------

TITLE: Loading a spaCy Tokenizer from Bytes
DESCRIPTION: Shows how to load a spaCy Tokenizer from a bytestring using the from_bytes method. This is useful when the tokenizer has been stored in memory or as binary data.

LANGUAGE: python
CODE:
tokenizer_bytes = tokenizer.to_bytes()
tokenizer = Tokenizer(nlp.vocab)
tokenizer.from_bytes(tokenizer_bytes)

----------------------------------------

TITLE: Clearing All Patterns from SpanRuler in Python
DESCRIPTION: Demonstrates how to remove all patterns from a SpanRuler using the clear method.

LANGUAGE: python
CODE:
patterns = [{"label": "ORG", "pattern": "Apple", "id": "apple"}]
ruler = nlp.add_pipe("span_ruler")
ruler.add_patterns(patterns)
ruler.clear()

----------------------------------------

TITLE: Configuring TextCatCNN.v1 in spaCy
DESCRIPTION: Example configuration for the spaCy.TextCatCNN.v1 architecture which uses a CNN-based token vector calculation and mean pooling. This version does not support resizing (adding labels after training).

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.TextCatCNN.v1"
exclusive_classes = false
nO = null

[model.tok2vec]
@architectures = "spacy.HashEmbedCNN.v1"
pretrained_vectors = null
width = 96
depth = 4
embed_size = 2000
window_size = 1
maxout_pieces = 3
subword_features = true

----------------------------------------

TITLE: Using merge_entities in spaCy Python
DESCRIPTION: Shows how to add the merge_entities component to a spaCy pipeline to combine named entities into single tokens. This component should be added after the ner component.

LANGUAGE: python
CODE:
texts = [t.text for t in nlp("I like David Bowie")]
assert texts == ["I", "like", "David", "Bowie"]

nlp.add_pipe("merge_entities")

texts = [t.text for t in nlp("I like David Bowie")]
assert texts == ["I", "like", "David Bowie"]

----------------------------------------

TITLE: Setting OpenAI API Keys in Shell Environment
DESCRIPTION: Sets the required environment variables for authenticating with OpenAI services. These credentials are needed when using OpenAI models through spaCy's LLM integration.

LANGUAGE: shell
CODE:
export OPENAI_API_KEY="sk-..."
export OPENAI_API_ORG="org-..."

----------------------------------------

TITLE: Initializing DependencyParser in SpaCy (Python)
DESCRIPTION: Examples of different ways to initialize a dependency parser in spaCy, including via add_pipe with default or custom models, and direct construction from the class. The parser analyzes the syntactic structure of sentences.

LANGUAGE: python
CODE:
# Construction via add_pipe with default model
parser = nlp.add_pipe("parser")

# Construction via add_pipe with custom model
config = {"model": {"@architectures": "my_parser"}}
parser = nlp.add_pipe("parser", config=config)

# Construction from class
from spacy.pipeline import DependencyParser
parser = DependencyParser(nlp.vocab, model)

----------------------------------------

TITLE: Building and Running spaCy Website with Docker
DESCRIPTION: Commands to build the Docker image and run the website in a Docker container with proper volume mounting and port forwarding.

LANGUAGE: bash
CODE:
docker build -t spacy-io .

LANGUAGE: bash
CODE:
docker run -it \
  --rm \
  -v $(pwd):/home/node/website \
  -p 3000:3000 \
  spacy-io \
  npm run dev -- -H 0.0.0.0

----------------------------------------

TITLE: Configuring Prodigy Train Curve Analysis
DESCRIPTION: YAML project command definition for running Prodigy's train-curve recipe, which evaluates model performance using different portions of training data to determine if more annotations would be beneficial.

LANGUAGE: yaml
CODE:
- name: "train_curve"
    help: "Train the model with Prodigy by using different portions of training examples to evaluate if more annotations can potentially improve the performance"
    script:
      - "python -m prodigy train-curve --ner ${vars.prodigy.train_dataset},eval:${vars.prodigy.eval_dataset} --config configs/${vars.config} --show-plot"

----------------------------------------

TITLE: Serializing DependencyParser to Disk in spaCy
DESCRIPTION: Saves the parser component to a specified directory path. The path can be provided as a string or Path-like object, and will be created if it doesn't exist.

LANGUAGE: python
CODE:
parser = nlp.add_pipe("parser")
parser.to_disk("/path/to/parser")

----------------------------------------

TITLE: Example of Auto-filling a spaCy Config File with Visual Diff
DESCRIPTION: Shows how to fill a partial base configuration and generate a complete config file, with the --diff flag to visualize changes between the base and completed config.

LANGUAGE: bash
CODE:
$ python -m spacy init fill-config base.cfg config.cfg --diff

----------------------------------------

TITLE: Creating SpanGroup objects in SpaCy
DESCRIPTION: Demonstrates two methods for creating a SpanGroup: direct construction using the SpanGroup class and indirect construction by assigning spans to Doc.spans.

LANGUAGE: python
CODE:
doc = nlp("Their goi ng home")
spans = [doc[0:1], doc[1:3]]

# Construction 1
from spacy.tokens import SpanGroup

group = SpanGroup(doc, name="errors", spans=spans, attrs={"annotator": "matt"})
doc.spans["errors"] = group

# Construction 2
doc.spans["errors"] = spans
assert isinstance(doc.spans["errors"], SpanGroup)

----------------------------------------

TITLE: Configuring LLM Cache in spaCy INI
DESCRIPTION: Example configuration for setting up a document batch cache for LLM processing. Specifies cache location, batch size, and memory limits to optimize performance during development.

LANGUAGE: ini
CODE:
[components.llm.cache]
@llm_misc = "spacy.BatchCache.v1"
path = "path/to/cache"
batch_size = 64
max_batches_in_mem = 4

----------------------------------------

TITLE: Calculating Loss for SentenceRecognizer in spaCy
DESCRIPTION: Demonstrates how to find the loss and gradient of loss for a batch of documents and their predicted scores using the get_loss method.

LANGUAGE: python
CODE:
senter = nlp.add_pipe("senter")
scores = senter.predict([eg.predicted for eg in examples])
loss, d_loss = senter.get_loss(examples, scores)

----------------------------------------

TITLE: Getting the Total Number of Entities in the Knowledge Base in Python
DESCRIPTION: Returns the count of entities stored in the knowledge base using the Python len() function.

LANGUAGE: python
CODE:
total_entities = len(kb)

----------------------------------------

TITLE: Checking lexeme flag values in spaCy
DESCRIPTION: Shows how to check the value of a binary flag attribute on a lexeme using the c_check_flag static method. This example checks if a lexeme is a stop word.

LANGUAGE: python
CODE:
from spacy.attrs cimport IS_STOP
from spacy.lexeme cimport Lexeme

lexeme = doc.c[3].lex
is_stop = Lexeme.c_check_flag(lexeme, IS_STOP)

----------------------------------------

TITLE: Loading EntityRuler Patterns from Disk in spaCy Python
DESCRIPTION: Demonstrates how to load EntityRuler patterns from disk, either from a JSONL file or from a directory containing patterns and configuration.

LANGUAGE: python
CODE:
ruler = nlp.add_pipe("entity_ruler")
ruler.from_disk("/path/to/patterns.jsonl")  # loads patterns only
ruler.from_disk("/path/to/entity_ruler")    # loads patterns and config

----------------------------------------

TITLE: Detecting if Code is Running in Jupyter Notebook
DESCRIPTION: Checks if spaCy is running within a Jupyter notebook by detecting the IPython kernel, useful for display-related features like the displacy visualizer.

LANGUAGE: python
CODE:
html = "<h1>Hello world!</h1>"
if util.is_in_jupyter():
    from IPython.core.display import display, HTML
    display(HTML(html))

----------------------------------------

TITLE: Using spaCy PEX Executable
DESCRIPTION: Examples showing how to use a spaCy PEX executable to run Python scripts or spaCy CLI commands. The PEX file replaces the need for python command and includes all dependencies.

LANGUAGE: bash
CODE:
$ ./spacy.pex my_script.py
$ ./spacy.pex -m spacy info

----------------------------------------

TITLE: Processing Document Stream with SpanCategorizer.pipe in spaCy (Python)
DESCRIPTION: Demonstrates how to process a stream of documents with the SpanCategorizer.pipe method, which allows for batch processing of documents.

LANGUAGE: python
CODE:
spancat = nlp.add_pipe("spancat")
for doc in spancat.pipe(docs, batch_size=50):
    pass

----------------------------------------

TITLE: Loading a CoreferenceResolver Model from Disk in spaCy (Python)
DESCRIPTION: Shows how to load a previously saved coreference resolution model from disk, modifying the component in place.

LANGUAGE: python
CODE:
coref = nlp.add_pipe("experimental_coref")
coref.from_disk("/path/to/coref")

----------------------------------------

TITLE: Initializing BaseVectors in Python
DESCRIPTION: Constructor method for creating a new vector store, optionally accepting a StringStore instance.

LANGUAGE: python
CODE:
def __init__(self, *, strings=None):

----------------------------------------

TITLE: Using Parameter Context Manager with Transformer in spaCy
DESCRIPTION: Temporarily modifies the transformer model to use given parameter values within a context. Original parameters are restored when exiting the context, useful for saving optimized models.

LANGUAGE: python
CODE:
trf = nlp.add_pipe("transformer")
with trf.use_params(optimizer.averages):
    trf.to_disk("/best_model")

----------------------------------------

TITLE: Loading EditTreeLemmatizer from Disk in spaCy
DESCRIPTION: Example demonstrating how to load a trainable lemmatizer component from disk using the from_disk method. The component is loaded from a specified directory path.

LANGUAGE: python
CODE:
lemmatizer = nlp.add_pipe("trainable_lemmatizer", name="lemmatizer")
lemmatizer.from_disk("/path/to/lemmatizer")

----------------------------------------

TITLE: Adding Strings to StringStore in Python
DESCRIPTION: Demonstrates how to add a new string to the StringStore and retrieve its hash value.

LANGUAGE: python
CODE:
stringstore = StringStore(["apple", "orange"])
banana_hash = stringstore.add("banana")
assert len(stringstore) == 3
assert banana_hash == 2525716904149915114
assert stringstore[banana_hash] == "banana"
assert stringstore["banana"] == banana_hash

----------------------------------------

TITLE: Installing spaCy in a virtual environment
DESCRIPTION: Commands to create and activate a Python virtual environment, update build tools, and install spaCy within the isolated environment.

LANGUAGE: bash
CODE:
$ python -m venv .env
$ source .env/bin/activate
$ pip install -U pip setuptools wheel
$ pip install -U %%SPACY_PKG_NAME%%SPACY_PKG_FLAGS

----------------------------------------

TITLE: Running Pretraining with spaCy CLI Command
DESCRIPTION: Demonstrates how to use the new 'spacy pretrain' command for transfer learning from raw text using a language model objective similar to BERT. This helps improve model accuracy when working with limited training data.

LANGUAGE: bash
CODE:
$ python -m spacy pretrain ./raw_text.jsonl
en_core_web_lg ./pretrained-model

----------------------------------------

TITLE: Generating Project Documentation with spaCy CLI
DESCRIPTION: Command to auto-generate a Markdown README file from a project.yml file. This creates documentation that lists all commands, workflows, and assets defined in the project with instructions on how to use them.

LANGUAGE: bash
CODE:
$ python -m spacy project document --output README.md

----------------------------------------

TITLE: Creating a sub-span from a Span in Python with spaCy
DESCRIPTION: Shows how to create a sub-span by slicing an existing Span object. This creates a new span containing the tokens from positions 1 to 2 within the original span.

LANGUAGE: python
CODE:
doc = nlp("Give it back! He pleaded.")
span = doc[1:4]
assert span[1:3].text == "back!"

----------------------------------------

TITLE: Configuring TextCat v1 Task in spaCy
DESCRIPTION: Example configuration for the TextCat v1 task in spaCy, which classifies text into predefined categories like COMPLIMENT and INSULT, with no examples provided (zero-shot approach).

LANGUAGE: ini
CODE:
[components.llm.task]
@llm_tasks = "spacy.TextCat.v1"
labels = COMPLIMENT,INSULT
examples = null

----------------------------------------

TITLE: Updating EditTreeLemmatizer model in spaCy
DESCRIPTION: Train the lemmatizer model using examples and an optimizer. This method learns from examples containing predictions and gold-standard annotations, and updates the model accordingly.

LANGUAGE: python
CODE:
lemmatizer = nlp.add_pipe("trainable_lemmatizer", name="lemmatizer")
optimizer = nlp.initialize()
losses = lemmatizer.update(examples, sgd=optimizer)

----------------------------------------

TITLE: Migrating Serialization Methods in spaCy
DESCRIPTION: Example showing how to update model and vocabulary saving methods to use the new consistent API for saving and loading. This replaces methods like save_to_directory and dump with to_disk and to_bytes.

LANGUAGE: python
CODE:
- nlp.save_to_directory("/model")
- nlp.vocab.dump("/vocab")

+ nlp.to_disk("/model")
+ nlp.vocab.to_disk("/vocab")

----------------------------------------

TITLE: Using Alternative Parameters with CuratedTransformer in spaCy
DESCRIPTION: Temporarily modifies the component's model to use given parameter values within a context manager, restoring original parameters afterward.

LANGUAGE: python
CODE:
trf = nlp.add_pipe("curated_transformer")
with trf.use_params(optimizer.averages):
    trf.to_disk("/best_model")

----------------------------------------

TITLE: Instance Tensor Layer Configuration in INI Format
DESCRIPTION: Configuration for the instance tensor creation component, including token vectorization, pooling for entity representations, and instance generation settings.

LANGUAGE: ini
CODE:
[model.create_instance_tensor]
@architectures = "rel_instance_tensor.v1"

[model.create_instance_tensor.tok2vec]
@architectures = "spacy.HashEmbedCNN.v2"
# ...

[model.create_instance_tensor.pooling]
@layers = "reduce_mean.v1"

[model.create_instance_tensor.get_instances]
# ...

----------------------------------------

TITLE: Configuring spaCy MaxoutWindowEncoder Architecture
DESCRIPTION: Configuration for the MaxoutWindowEncoder architecture which encodes context using convolutions with maxout activation, layer normalization, and residual connections. It processes a window of tokens around each word.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.MaxoutWindowEncoder.v2"
width = 128
window_size = 1
maxout_pieces = 3
depth = 4

----------------------------------------

TITLE: Adding Patterns to AttributeRuler in spaCy
DESCRIPTION: Example demonstrating how to add patterns to the AttributeRuler. This adds a pattern that matches tokens with the VB tag and assigns them the VERB POS tag.

LANGUAGE: python
CODE:
ruler = nlp.add_pipe("attribute_ruler")
patterns = [[{"TAG": "VB"}]]
attrs = {"POS": "VERB"}
ruler.add(patterns=patterns, attrs=attrs)

----------------------------------------

TITLE: Defining Few-Shot Examples for Raw Task in YAML
DESCRIPTION: YAML structure for defining few-shot examples for the Raw task. Each example contains a text prompt and the expected reply that will be used to guide the LLM.

LANGUAGE: yaml
CODE:
# Each example can follow an arbitrary pattern. It might help the prompt performance though if the examples resemble
# the actual docs' content.
- text: "3 + 5 = x. What's x?"
  reply: '8'

- text: 'Write me a limerick.'
  reply:
    "There was an Old Man with a beard, Who said, 'It is just as I feared! Two
    Owls and a Hen, Four Larks and a Wren, Have all built their nests in my
    beard!"

- text: "Analyse the sentiment of the text 'This is great'."
  reply: "'This is great' expresses a very positive sentiment."

----------------------------------------

TITLE: Setting Output Dimension of DependencyParser in spaCy
DESCRIPTION: Changes the output dimension of the parser's model by calling the model's resize_output function. This allows modification of the model's output size after creation.

LANGUAGE: python
CODE:
parser = nlp.add_pipe("parser")
parser.set_output(512)

----------------------------------------

TITLE: Converting Vector Operations in Python
DESCRIPTION: Method to change the embedding matrix to use different Thinc operations.

LANGUAGE: python
CODE:
def to_ops(self, ops):

----------------------------------------

TITLE: Resizing Vectors in spaCy (Python)
DESCRIPTION: Resizes the underlying vectors array to a specified shape. Returns a list of removed items as (key, row) tuples when the number of vectors is reduced. Not supported for 'floret' mode.

LANGUAGE: python
CODE:
removed = nlp.vocab.vectors.resize((10000, 300))

----------------------------------------

TITLE: Creating a Token Object from a TokenC Pointer in Cython
DESCRIPTION: Example demonstrating how to initialize a Token object using a pointer to a TokenC struct at a specific offset in a document. This low-level operation provides direct access to token data.

LANGUAGE: python
CODE:
token = Token.cinit(&doc.c[3], doc, 3)

----------------------------------------

TITLE: Creating a Candidate Object in Python
DESCRIPTION: Example showing how to manually create a Candidate object, which represents a possible entity for a textual mention, with various properties like entity hash, frequency, and prior probability.

LANGUAGE: python
CODE:
from spacy.kb import Candidate
candidate = Candidate(kb, entity_hash, entity_freq, entity_vector, alias_hash, prior_prob)

----------------------------------------

TITLE: Loading SpanCategorizer from bytes
DESCRIPTION: Demonstrates how to load a SpanCategorizer component from a bytestring. The method modifies the object in place and returns it, accepting a bytes_data parameter and optional exclude parameter.

LANGUAGE: python
CODE:
spancat_bytes = spancat.to_bytes()
spancat = nlp.add_pipe("spancat")
spancat.from_bytes(spancat_bytes)

----------------------------------------

TITLE: Fixing SpanGroup initialization in Example creation
DESCRIPTION: Diff showing the required change for creating a SpanGroup with spans from the correct doc. In v3.6, spans must refer to the same doc as the SpanGroup, requiring the use of example.reference instead of doc.

LANGUAGE: diff
CODE:
     doc = Doc(nlp.vocab, words=tokens)  # predicted doc
     example = Example.from_dict(doc, {"ner": iob_tags})
     # use the reference doc when creating reference spans
-    span = Span(doc, 0, 5, "ORG")
+    span = Span(example.reference, 0, 5, "ORG")
     example.reference.spans[spans_key] = [span]

----------------------------------------

TITLE: Using TextCategorizer Label Data for Initialization in Python
DESCRIPTION: Shows how to access label metadata and use it to initialize a new TextCategorizer component with the same label set.

LANGUAGE: python
CODE:
labels = textcat.label_data
textcat.initialize(lambda: [], nlp=nlp, labels=labels)

----------------------------------------

TITLE: Setting span candidates with SpanCategorizer in spaCy (Python)
DESCRIPTION: Example demonstrating how to use the suggester to add span candidates to Doc objects for debugging purposes.

LANGUAGE: python
CODE:
spancat = nlp.add_pipe("spancat")
spancat.set_candidates(docs, "candidates")

----------------------------------------

TITLE: Using Extended Match Pattern API in spaCy
DESCRIPTION: Shows examples of the extended match pattern API where token patterns can map to dictionaries of properties. Demonstrates matching lemmas from a list, tokens by length, and using regex on custom attributes.

LANGUAGE: python
CODE:
# Matches "love cats" or "likes flowers"
pattern1 = [{"LEMMA": {"IN": ["like", "love"]}}, {"POS": "NOUN"}]
# Matches tokens of length >= 10
pattern2 = [{"LENGTH": {">=": 10}}]
# Matches custom attribute with regex
pattern3 = [{"_": {"country": {"REGEX": "^([Uu](\.?|nited) ?[Ss](\.?|tates)"}}}]

----------------------------------------

TITLE: Example JSONL Format for JsonlCorpus
DESCRIPTION: Example of JSONL format used by JsonlCorpus class. Each line contains a JSON object with a 'text' field containing the document text.

LANGUAGE: json
CODE:
{"text": "Can I ask where you work now and what you do, and if you enjoy it?"}
{"text": "They may just pull out of the Seattle market completely, at least until they have autonomous vehicles."}
{"text": "My cynical view on this is that it will never be free to the public. Reason: what would be the draw of joining the military? Right now their selling point is free Healthcare and Education. Ironically both are run horribly and most, that I've talked to, come out wishing they never went in."}

----------------------------------------

TITLE: Cloning a Custom spaCy Project from a Git Repository
DESCRIPTION: Command to clone a spaCy project from a custom Git repository. This allows users to clone projects from their own repositories, including private ones, as long as they have access.

LANGUAGE: bash
CODE:
python -m spacy project clone your_project --repo https://github.com/you/repo

----------------------------------------

TITLE: Pattern Matching for German International Phone Numbers in spaCy
DESCRIPTION: Creates a pattern specifically for matching international German phone numbers. Uses a combination of ORTH, SHAPE, and LENGTH attributes to identify specific number formats.

LANGUAGE: python
CODE:
[{"ORTH": "+"}, {"ORTH": "49"}, {"ORTH": "(", "OP": "?"}, {"SHAPE": "dddd"},
 {"ORTH": ")", "OP": "?"}, {"SHAPE": "dddd", "LENGTH": 6}]

----------------------------------------

TITLE: Splitting Example into Sentences in spaCy
DESCRIPTION: Demonstrates how to split a single Example object into multiple Examples, one for each sentence, based on sentence boundary annotations.

LANGUAGE: python
CODE:
doc = nlp("I went yesterday had lots of fun")
tokens_ref = ["I", "went", "yesterday", "had", "lots", "of", "fun"]
sents_ref = [True, False, False, True, False, False, False]
example = Example.from_dict(doc, {"words": tokens_ref, "sent_starts": sents_ref})
split_examples = example.split_sents()
assert split_examples[0].text == "I went yesterday "
assert split_examples[1].text == "had lots of fun"

----------------------------------------

TITLE: Loading Transformer Component from Disk in spaCy
DESCRIPTION: Loads a previously saved transformer component from disk, modifying the existing object in place. Returns the modified transformer object.

LANGUAGE: python
CODE:
trf = nlp.add_pipe("transformer")
trf.from_disk("/path/to/transformer")

----------------------------------------

TITLE: Configuring Tokenizer Customization in INI Format
DESCRIPTION: Shows how to define the initialization configuration that specifies a callback function for customizing the tokenizer before training begins.

LANGUAGE: ini
CODE:
[initialize]

[initialize.before_init]
@callbacks = "customize_tokenizer"

----------------------------------------

TITLE: Using JsonlCorpus to Generate Examples
DESCRIPTION: Example demonstrating how to use a JsonlCorpus object to generate examples from JSONL data. Creates a blank language model and feeds it to the corpus to yield examples.

LANGUAGE: python
CODE:
from spacy.training import JsonlCorpus
import spacy

corpus = JsonlCorpus("./texts.jsonl")
nlp = spacy.blank("en")
data = corpus(nlp)

----------------------------------------

TITLE: Configuring TextCatBOW.v3 Architecture in spaCy
DESCRIPTION: Configuration example for TextCatBOW.v3, an n-gram bag-of-words model for text categorization. This model is faster but potentially less accurate than other architectures, especially for short texts.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.TextCatBOW.v3"
exclusive_classes = false
length = 262144
ngram_size = 1
no_output_layer = false
nO = null

----------------------------------------

TITLE: Configuring TransformerListener for Connecting to Upstream Transformer
DESCRIPTION: Configuration for a TransformerListener that connects to a Transformer component earlier in the pipeline. Uses mean pooling to calculate token vectors from wordpiece representations with a gradient factor of 1.0.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy-transformers.TransformerListener.v1"
grad_factor = 1.0

[model.pooling]
@layers = "reduce_mean.v1"

----------------------------------------

TITLE: Serializing AttributeRuler to Disk in spaCy
DESCRIPTION: Example demonstrating how to save an AttributeRuler pipe to disk. The method requires a path parameter that can be a string or Path-like object.

LANGUAGE: python
CODE:
ruler = nlp.add_pipe("attribute_ruler")
ruler.to_disk("/path/to/attribute_ruler")

----------------------------------------

TITLE: Serializing a spaCy Tokenizer to Disk
DESCRIPTION: Demonstrates how to serialize a spaCy Tokenizer instance to disk using the to_disk method. This creates a directory containing the serialized tokenizer data.

LANGUAGE: python
CODE:
tokenizer = Tokenizer(nlp.vocab)
tokenizer.to_disk("/path/to/tokenizer")

----------------------------------------

TITLE: Initializing a DependencyMatcher in Python
DESCRIPTION: Example showing how to create a DependencyMatcher instance by providing a vocabulary object, which must be shared with the documents the matcher will operate on.

LANGUAGE: python
CODE:
from spacy.matcher import DependencyMatcher
matcher = DependencyMatcher(nlp.vocab)

----------------------------------------

TITLE: Serializing Lookups to Bytes
DESCRIPTION: Serializes the Lookups object to a bytestring for storage or transmission.

LANGUAGE: python
CODE:
lookup_bytes = lookups.to_bytes()

----------------------------------------

TITLE: Configuring MishWindowEncoder in spaCy
DESCRIPTION: Example configuration for the MishWindowEncoder.v2 architecture that encodes context using convolutions with Mish activation, layer normalization and residual connections. The configuration specifies width, window size, and depth parameters.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.MishWindowEncoder.v2"
width = 64
window_size = 1
depth = 4

----------------------------------------

TITLE: Deduplicating word vectors in spaCy Vocab
DESCRIPTION: Removes duplicate rows from the vector table while maintaining the mappings for all words in the vectors.

LANGUAGE: python
CODE:
nlp.vocab.deduplicate_vectors()

----------------------------------------

TITLE: Implementing Custom Sentence Span Getter for Transformers in spaCy
DESCRIPTION: Function that returns spans following sentence boundaries with a maximum token length constraint. It splits sentences into chunks of specified maximum length to improve transformer processing.

LANGUAGE: python
CODE:
import spacy_transformers

@spacy_transformers.registry.span_getters("custom_sent_spans")
def configure_custom_sent_spans(max_length: int):
    def get_custom_sent_spans(docs):
        spans = []
        for doc in docs:
            spans.append([])
            for sent in doc.sents:
                start = 0
                end = max_length
                while end <= len(sent):
                    spans[-1].append(sent[start:end])
                    start += max_length
                    end += max_length
                if start < len(sent):
                    spans[-1].append(sent[start:len(sent)])
        return spans

    return get_custom_sent_spans

----------------------------------------

TITLE: Error: Unhashable Type List
DESCRIPTION: Shows a TypeError that can occur when loading models in Windows environments due to Git's line ending conversion affecting binary model files.

LANGUAGE: python
CODE:
TypeError: unhashable type: 'list'

----------------------------------------

TITLE: Registering Components and Languages via Entry Points
DESCRIPTION: Shows how to use entry points in setup.py to define custom spaCy factories and languages. This allows model packages and extension packages to add their own components and languages to the built-in ones automatically.

LANGUAGE: python
CODE:
from setuptools import setup
setup(
    name="custom_extension_package",
    entry_points={
        "spacy_factories": ["your_component = component:ComponentFactory"]
        "spacy_languages": ["xyz = language:XYZLanguage"]
   }
)

----------------------------------------

TITLE: Configuring spacy.KBFileLoader.v1 Component
DESCRIPTION: Example configuration for the spacy.KBFileLoader.v1 component which loads a knowledge base from a YAML file. This component can be used with the CandidateSelector.v1 for entity linking.

LANGUAGE: ini
CODE:
[initialize.components.llm.candidate_selector.kb_loader]
@llm_misc = "spacy.KBFileLoader.v1"
# Path to knowledge base file.
path = ${paths.el_kb}

----------------------------------------

TITLE: Loading SpanFinder from Disk in spaCy
DESCRIPTION: Load a previously saved SpanFinder component from disk, modifying the object in place.

LANGUAGE: python
CODE:
span_finder = nlp.add_pipe("span_finder")
span_finder.from_disk("/path/to/span_finder")

----------------------------------------

TITLE: Loading EntityLinker from bytes in spaCy (Python)
DESCRIPTION: Example showing how to serialize an entity linker to bytes and then load it back. The method modifies the object in place and returns it.

LANGUAGE: python
CODE:
entity_linker_bytes = entity_linker.to_bytes()
entity_linker = nlp.add_pipe("entity_linker")
entity_linker.from_bytes(entity_linker_bytes)

----------------------------------------

TITLE: Predicting Tags with a Tagger Component in spaCy
DESCRIPTION: Shows how to apply a tagger model to documents without modifying them, returning the prediction scores for each document.

LANGUAGE: python
CODE:
tagger = nlp.add_pipe("tagger")
scores = tagger.predict([doc1, doc2])

----------------------------------------

TITLE: Configuring Korean Language with Rule-based Tokenizer in Config File
DESCRIPTION: Shows how to configure a Korean language pipeline to use the rule-based tokenizer instead of MeCab in a spaCy configuration file.

LANGUAGE: ini
CODE:
[nlp]
lang = "ko"
tokenizer = {"@tokenizers" = "spacy.Tokenizer.v1"}

----------------------------------------

TITLE: AttributeRuler Configuration in config.cfg
DESCRIPTION: Example showing how to configure the AttributeRuler component in a spaCy config.cfg file, specifying pattern loading from an external JSON file.

LANGUAGE: ini
CODE:
### config.cfg
[initialize.components.attribute_ruler]

[initialize.components.attribute_ruler.patterns]
@readers = "srsly.read_json.v1"
path = "corpus/attribute_ruler_patterns.json

----------------------------------------

TITLE: Setting Values in a Table
DESCRIPTION: Sets a key-value pair in a Table using the set method. String keys will be hashed.

LANGUAGE: python
CODE:
from spacy.lookups import Table
table = Table()
table.set("foo", "bar")
assert table["foo"] == "bar"

----------------------------------------

TITLE: Configuring TextCatCNN.v2 in spaCy
DESCRIPTION: Example configuration for the spaCy.TextCatCNN.v2 architecture which uses a CNN-based token vector calculation with mean pooling. This improved version supports resizing, allowing labels to be added after training.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.TextCatCNN.v2"
exclusive_classes = false
nO = null

[model.tok2vec]
@architectures = "spacy.HashEmbedCNN.v2"
pretrained_vectors = null
width = 96
depth = 4
embed_size = 2000
window_size = 1
maxout_pieces = 3
subword_features = true

----------------------------------------

TITLE: Getting Batch Vectors in spaCy (Python)
DESCRIPTION: Efficiently retrieves vectors for multiple keys as a batch. This method was introduced in spaCy v3.2.

LANGUAGE: python
CODE:
words = ["cat", "dog"]
vectors = nlp.vocab.vectors.get_batch(words)

----------------------------------------

TITLE: Saving StringStore to Disk in Python
DESCRIPTION: Shows how to serialize and save the current state of a StringStore to a directory.

LANGUAGE: python
CODE:
stringstore.to_disk("/path/to/strings")

----------------------------------------

TITLE: Excluding fields when serializing EntityLinker in spaCy (Python)
DESCRIPTION: Example demonstrating how to exclude specific fields during serialization of an entity linker. This example shows excluding the 'vocab' field when saving to disk.

LANGUAGE: python
CODE:
data = entity_linker.to_disk("/path", exclude=["vocab"])

----------------------------------------

TITLE: Removing custom extensions from Span objects in spaCy
DESCRIPTION: Shows how to remove a previously registered extension from the Span class. The method returns the configuration of the removed extension and updates the class to no longer have that extension.

LANGUAGE: python
CODE:
from spacy.tokens import Span
Span.set_extension("is_city", default=False)
removed = Span.remove_extension("is_city")
assert not Span.has_extension("is_city")

----------------------------------------

TITLE: Loading AttributeRuler from Disk in spaCy
DESCRIPTION: Example showing how to load an AttributeRuler pipe from a directory on disk. The method modifies the existing object and returns it.

LANGUAGE: python
CODE:
ruler = nlp.add_pipe("attribute_ruler")
ruler.from_disk("/path/to/attribute_ruler")

----------------------------------------

TITLE: Loading spaCy Vocabulary from Bytes
DESCRIPTION: Loads vocabulary state from a binary string. This method is the counterpart to to_bytes() and allows for restoring a vocabulary that was previously serialized to bytes.

LANGUAGE: python
CODE:
from spacy.vocab import Vocab
vocab_bytes = nlp.vocab.to_bytes()
vocab = Vocab()
vocab.from_bytes(vocab_bytes)

----------------------------------------

TITLE: Loading EntityLinker Component from Disk in spaCy (Python)
DESCRIPTION: Loads a previously saved EntityLinker component from disk, modifying the current object in place with the loaded data and model.

LANGUAGE: python
CODE:
entity_linker = nlp.add_pipe("entity_linker")
entity_linker.from_disk("/path/to/entity_linker")

----------------------------------------

TITLE: Using SentenceRecognizer.set_annotations Method in spaCy
DESCRIPTION: Example showing how to modify documents using pre-computed scores from the SentenceRecognizer. This demonstrates a two-step process of first predicting scores and then applying annotations.

LANGUAGE: python
CODE:
senter = nlp.add_pipe("senter")
scores = senter.predict([doc1, doc2])
senter.set_annotations([doc1, doc2], scores)

----------------------------------------

TITLE: Configuring Lexeme Tables in spaCy v3.0
DESCRIPTION: Shows how to configure lexeme lookup tables like lexeme_norm in the initialization section of the config file.

LANGUAGE: ini
CODE:
[initialize.lookups]
@misc = "spacy.LookupsDataLoader.v1"
lang = ${nlp.lang}
tables = ["lexeme_norm"]

----------------------------------------

TITLE: Cloning a spaCy Project Template
DESCRIPTION: Command for cloning a project template from a Git repository. By default, it uses spaCy's project templates repo, but can use any specified repository with the --repo option.

LANGUAGE: bash
CODE:
$ python -m spacy project clone [name] [dest] [--repo] [--branch] [--sparse]

----------------------------------------

TITLE: Using Configurable Attribute Names in spaCy Extensions
DESCRIPTION: Best practice for letting users configure attribute names in custom components to avoid namespace collisions and accommodate different naming preferences.

LANGUAGE: diff
CODE:
+ Doc.set_extension(self.doc_attr, default="some value")
- Doc.set_extension("my_doc_attr", default="some value")

----------------------------------------

TITLE: Using Configurable Attribute Names in spaCy Extensions
DESCRIPTION: Best practice for letting users configure attribute names in custom components to avoid namespace collisions and accommodate different naming preferences.

LANGUAGE: diff
CODE:
+ Doc.set_extension(self.doc_attr, default="some value")
- Doc.set_extension("my_doc_attr", default="some value")

----------------------------------------

TITLE: Updating spaCy train CLI command parameters in v2.1.x
DESCRIPTION: Shows how to adapt to changes in the 'spacy train' command line interface, which now uses a comma-separated list of pipeline component names via the --pipeline flag instead of separate flags like --no-parser to disable components.

LANGUAGE: diff
CODE:
- $ spacy train en /output train_data.json dev_data.json --no-parser
+ $ spacy train en /output train_data.json dev_data.json --pipeline tagger,ner

----------------------------------------

TITLE: Defining Project Directory Structure in spaCy Project YAML
DESCRIPTION: Configuration for defining the directory structure of a spaCy project. Lists directories that should be created automatically, ensuring proper organization of project files and outputs.

LANGUAGE: yaml
CODE:
directories: ['assets', 'configs', 'corpus', 'metas', 'metrics', 'notebooks', 'packages', 'scripts', 'training']

----------------------------------------

TITLE: Prediction with CuratedTransformer in spaCy
DESCRIPTION: Applies the transformer model to a batch of Doc objects without modifying them, returning the model's predictions.

LANGUAGE: python
CODE:
trf = nlp.add_pipe("curated_transformer")
scores = trf.predict([doc1, doc2])

----------------------------------------

TITLE: Deleting spans from a SpanGroup
DESCRIPTION: Demonstrates how to delete a span from a SpanGroup using the del keyword.

LANGUAGE: python
CODE:
doc = nlp("Their goi ng home")
doc.spans["errors"] = [doc[0:1], doc[1:3]]
del doc.spans[0]
assert len(doc.spans["errors"]) == 1

----------------------------------------

TITLE: Adding Custom Labels to DependencyParser in spaCy
DESCRIPTION: Demonstrates how to manually add a new dependency relation label to a parser model.

LANGUAGE: python
CODE:
parser = nlp.add_pipe("parser")
parser.add_label("MY_LABEL")

----------------------------------------

TITLE: Accessing Default Configuration for CuratedTransformer in Python
DESCRIPTION: Example showing how to import the default configuration for the CuratedTransformer component and add it to a spaCy pipeline with that configuration.

LANGUAGE: python
CODE:
from spacy_curated_transformers.pipeline.transformer import DEFAULT_CONFIG

nlp.add_pipe("curated_transformer", config=DEFAULT_CONFIG)

----------------------------------------

TITLE: Loading CuratedTransformer from bytes in spaCy
DESCRIPTION: Example demonstrating how to load a CuratedTransformer component from a bytestring. This shows serializing transformer data and then loading it into a newly added pipe.

LANGUAGE: python
CODE:
trf_bytes = trf.to_bytes()
trf = nlp.add_pipe("curated_transformer")
trf.from_bytes(trf_bytes)

----------------------------------------

TITLE: Modifying Tokenizer Suffix Rules in spaCy
DESCRIPTION: Example of how to add a new character to the existing suffix rules in spaCy's tokenizer. This code adds a new pattern to handle trailing hyphens followed by plus signs.

LANGUAGE: python
CODE:
suffixes = nlp.Defaults.suffixes + [r'''-+$''',]
suffix_regex = spacy.util.compile_suffix_regex(suffixes)
nlp.tokenizer.suffix_search = suffix_regex.search

----------------------------------------

TITLE: Enhancing Entity Recognition with SpanRuler
DESCRIPTION: Example showing how to configure SpanRuler to annotate doc.ents directly instead of doc.spans, integrating with spaCy's named entity recognition system without overwriting existing entities.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("en_core_web_sm")
# only annotate doc.ents, not doc.spans
config = {"spans_key": None, "annotate_ents": True, "overwrite": False}
ruler = nlp.add_pipe("span_ruler", config=config)
patterns = [{"label": "ORG", "pattern": "MyCorp Inc."}]
ruler.add_patterns(patterns)

doc = nlp("MyCorp Inc. is a company in the U.S.")
print([(ent.text, ent.label_) for ent in doc.ents])

----------------------------------------

TITLE: Configuring TextCatBOW v1 Architecture in spaCy
DESCRIPTION: Configuration for TextCatBOW v1, an n-gram bag-of-words model for text categorization. This version predates the resizable architecture introduced in v2, which allows adding labels to a previously trained textcat.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.TextCatBOW.v1"
exclusive_classes = false
ngram_size = 1
no_output_layer = false
nO = null

----------------------------------------

TITLE: Checking Vector Size in spaCy (Python)
DESCRIPTION: Shows how to access the total vector size (rows * dimensions) using the size property of the Vectors class.

LANGUAGE: python
CODE:
vectors = Vectors(shape=(500, 300))
assert vectors.size == 150000

----------------------------------------

TITLE: Configuring NVTX Range Markers with INI Configuration
DESCRIPTION: Example configuration for setting up NVTX range markers in spaCy pipelines, which helps with performance profiling when using NVIDIA tools.

LANGUAGE: ini
CODE:
[nlp]
after_pipeline_creation = {"@callbacks":"spacy.models_and_pipes_with_nvtx_range.v1"}

----------------------------------------

TITLE: Adding Patterns to EntityRuler in spaCy Python
DESCRIPTION: Shows how to add both string patterns and token patterns to an EntityRuler. Token patterns allow for more complex matching with multiple tokens and linguistic features.

LANGUAGE: python
CODE:
patterns = [
    {"label": "ORG", "pattern": "Apple"},
    {"label": "GPE", "pattern": [{"lower": "san"}, {"lower": "francisco"}]}
]
ruler = nlp.add_pipe("entity_ruler")
ruler.add_patterns(patterns)

----------------------------------------

TITLE: Using token_splitter in spaCy Python
DESCRIPTION: Shows how to use the token_splitter component to break up long tokens into shorter ones. This is useful for transformer pipelines where input text length is limited.

LANGUAGE: python
CODE:
config = {"min_length": 20, "split_length": 5}
nlp.add_pipe("token_splitter", config=config, first=True)
doc = nlp("aaaaabbbbbcccccdddddee")
print([token.text for token in doc])
# ['aaaaa', 'bbbbb', 'ccccc', 'ddddd', 'ee']

----------------------------------------

TITLE: Serializing AttributeRuler to Bytes in spaCy
DESCRIPTION: Example demonstrating how to serialize an AttributeRuler pipe to a bytestring for storage or transmission.

LANGUAGE: python
CODE:
ruler = nlp.add_pipe("attribute_ruler")
ruler = ruler.to_bytes()

----------------------------------------

TITLE: Retrieving Struct Attributes with Token.get_struct_attr in spaCy
DESCRIPTION: Example demonstrating how to get the value of a specific attribute from a TokenC struct using the static get_struct_attr method. This example retrieves the IS_ALPHA attribute for a token.

LANGUAGE: python
CODE:
from spacy.attrs cimport IS_ALPHA
from spacy.tokens cimport Token

is_alpha = Token.get_struct_attr(&doc.c[3], IS_ALPHA)

----------------------------------------

TITLE: Converting JSON Training Data to Binary .spacy Format
DESCRIPTION: Uses the spacy convert command to transform JSON-formatted training data into the new efficient binary .spacy format, which represents collections of Doc objects.

LANGUAGE: bash
CODE:
$ python -m spacy convert ./training.json ./output

----------------------------------------

TITLE: Checking Length of SpanRuler Patterns in Python
DESCRIPTION: Shows how to check the number of patterns added to a SpanRuler using the __len__ method.

LANGUAGE: python
CODE:
ruler = nlp.add_pipe("span_ruler")
assert len(ruler) == 0
ruler.add_patterns([{"label": "ORG", "pattern": "Apple"}])
assert len(ruler) == 1

----------------------------------------

TITLE: Installing New Language Models in spaCy v2.2
DESCRIPTION: Shows how to download and install the new language models available in spaCy v2.2, including Dutch, Norwegian, and Lithuanian models using the spaCy command line interface.

LANGUAGE: bash
CODE:
python -m spacy download nl_core_news_sm
python -m spacy download nb_core_news_sm
python -m spacy download lt_core_news_sm

----------------------------------------

TITLE: Running Complete spaCy Project Workflow
DESCRIPTION: Command to execute all steps defined in a spaCy project workflow, useful for running end-to-end NLP pipelines with integrations like Prodigy.

LANGUAGE: bash
CODE:
$ python -m spacy project run all

----------------------------------------

TITLE: Adding a single span to a SpanGroup with append
DESCRIPTION: Demonstrates how to add a single span to a SpanGroup using the append method.

LANGUAGE: python
CODE:
doc = nlp("Their goi ng home")
doc.spans["errors"] = [doc[0:1]]
doc.spans["errors"].append(doc[1:3])
assert len(doc.spans["errors"]) == 2

----------------------------------------

TITLE: Calculating Loss for DependencyParser in spaCy
DESCRIPTION: Shows how to calculate the loss and gradient of loss for parser predictions compared to gold-standard examples.

LANGUAGE: python
CODE:
parser = nlp.add_pipe("parser")
scores = parser.predict([eg.predicted for eg in examples])
loss, d_loss = parser.get_loss(examples, scores)

----------------------------------------

TITLE: Configuring SpanResolver with Custom Settings in Python
DESCRIPTION: Example of configuring a SpanResolver component with custom model and prefix settings. The component uses prefixes to identify input and output span groups in the Doc.spans container.

LANGUAGE: python
CODE:
from spacy_experimental.coref.span_resolver_component import DEFAULT_SPAN_RESOLVER_MODEL
from spacy_experimental.coref.coref_util import DEFAULT_CLUSTER_PREFIX, DEFAULT_CLUSTER_HEAD_PREFIX
config={
    "model": DEFAULT_SPAN_RESOLVER_MODEL,
    "input_prefix": DEFAULT_CLUSTER_HEAD_PREFIX,
    "output_prefix": DEFAULT_CLUSTER_PREFIX,
},
nlp.add_pipe("experimental_span_resolver", config=config)

----------------------------------------

TITLE: Updating Component Addition Syntax for spaCy v3.0
DESCRIPTION: Diff showing the migration from adding components directly to using string names with nlp.add_pipe() in spaCy v3.0. This highlights the shift from passing component instances to using registered factory names with configuration.

LANGUAGE: diff
CODE:
import spacy
from your_plugin import MyCoolComponent

nlp = spacy.load("en_core_web_sm")
- component = MyCoolComponent(some_setting=True)
- nlp.add_pipe(component)
+ nlp.add_pipe("my_component", config={"some_setting": True})

----------------------------------------

TITLE: Named Entity Recognition with Hugging Face Model Configuration
DESCRIPTION: Configuration file for setting up a named entity recognition pipeline using the Dolly model from Hugging Face through the llm component in spaCy.

LANGUAGE: ini
CODE:
[nlp]
lang = "en"
pipeline = ["llm"]

[components]

[components.llm]
factory = "llm"

[components.llm.task]
@llm_tasks = "spacy.NER.v3"
labels = ["PERSON", "ORGANISATION", "LOCATION"]

[components.llm.model]
@llm_models = "spacy.Dolly.v1"
# For better performance, use dolly-v2-12b instead
name = "dolly-v2-3b"

----------------------------------------

TITLE: Checking Number of Keys in Vectors Table (Python)
DESCRIPTION: Shows how to get the number of keys in the vectors table using the n_keys property. This counts all keys, not just unique vectors. In 'floret' mode, the keys table is not used.

LANGUAGE: python
CODE:
vectors = Vectors(shape=(10, 300))
assert len(vectors) == 10
assert vectors.n_keys == 0

----------------------------------------

TITLE: Retrieving Multiple Entity Vectors from the Knowledge Base in Python
DESCRIPTION: Gets the pre-trained vector representations for multiple entities in the knowledge base.

LANGUAGE: python
CODE:
vectors = kb.get_vectors(("Q42", "Q3107329"))

----------------------------------------

TITLE: Iterating Over Vector Key-Value Pairs in spaCy (Python)
DESCRIPTION: Demonstrates how to iterate over (key, vector) pairs in order, printing the key, its string representation, and the vector. In 'floret' mode, the keys table is empty.

LANGUAGE: python
CODE:
for key, vector in nlp.vocab.vectors.items():
   print(key, nlp.vocab.strings[key], vector)

----------------------------------------

TITLE: Serializing Morphologizer to Bytes in spaCy (Python)
DESCRIPTION: Serializes the Morphologizer pipeline component to a bytestring for storage or transmission.

LANGUAGE: python
CODE:
morphologizer = nlp.add_pipe("morphologizer")
morphologizer_bytes = morphologizer.to_bytes()

----------------------------------------

TITLE: Deserializing EntityRuler from Bytes in spaCy Python
DESCRIPTION: Demonstrates how to load EntityRuler patterns from a bytestring. This is useful for reconstructing an EntityRuler that was previously serialized.

LANGUAGE: python
CODE:
ruler_bytes = ruler.to_bytes()
ruler = nlp.add_pipe("entity_ruler")
ruler.from_bytes(ruler_bytes)

----------------------------------------

TITLE: Using doc_cleaner in spaCy Python
DESCRIPTION: Demonstrates how to use the doc_cleaner component to clean up Doc attributes, particularly useful for freeing memory by removing tensors and other values after the pipeline has run.

LANGUAGE: python
CODE:
config = {"attrs": {"tensor": None}}
nlp.add_pipe("doc_cleaner", config=config)
doc = nlp("text")
assert doc.tensor is None

----------------------------------------

TITLE: Excluding Fields During AttributeRuler Serialization in spaCy
DESCRIPTION: Example of how to exclude specific serialization fields when saving an AttributeRuler to disk. This can be useful when you want to preserve certain aspects of the model.

LANGUAGE: python
CODE:
data = ruler.to_disk("/path", exclude=["vocab"])

----------------------------------------

TITLE: Pushing spaCy Pipeline to Hugging Face Hub
DESCRIPTION: Command for pushing a packaged spaCy pipeline (.whl file) to the Hugging Face Hub. The command accepts parameters for organization, commit message, and verbosity level.

LANGUAGE: bash
CODE:
$ python -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--verbose]

----------------------------------------

TITLE: Defining a DependencyMatcher Pattern for "subject initially founded" in Python
DESCRIPTION: Example pattern for the DependencyMatcher that matches the phrase "[subject] ... initially founded". The pattern identifies the anchor token "founded", connects it to a subject dependency, and specifies that "initially" should precede "founded".

LANGUAGE: python
CODE:
### Example
# pattern: "[subject] ... initially founded"
[
  # anchor token: founded
  {
    "RIGHT_ID": "founded",
    "RIGHT_ATTRS": {"ORTH": "founded"}
  },
  # founded -> subject
  {
    "LEFT_ID": "founded",
    "REL_OP": ">",
    "RIGHT_ID": "subject",
    "RIGHT_ATTRS": {"DEP": "nsubj"}
  },
  # "founded" follows "initially"
  {
    "LEFT_ID": "founded",
    "REL_OP": ";",
    "RIGHT_ID": "initially",
    "RIGHT_ATTRS": {"ORTH": "initially"}
  }
]

----------------------------------------

TITLE: Configuring Data Augmentation in spaCy Config File
DESCRIPTION: Configuration excerpt showing how to set up the orth_variants augmenter in a spaCy training config file. It defines augmentation parameters including level (percentage of texts to augment) and lower (percentage to lowercase).

LANGUAGE: ini
CODE:
[corpora.train]
@readers = "spacy.Corpus.v1"
path = ${paths.train}
gold_preproc = false
max_length = 0
limit = 0

[corpora.train.augmenter]
@augmenters = "spacy.orth_variants.v1"
# Percentage of texts that will be augmented / lowercased
level = 0.1
lower = 0.5

[corpora.train.augmenter.orth_variants]
@readers = "srsly.read_json.v1"
path = "corpus/orth_variants.json"

----------------------------------------

TITLE: Using Factory Functions for Class Components in spaCy v3.0
DESCRIPTION: Shows how to create a factory function that returns an instance of a component class, as an alternative to decorating the class directly.

LANGUAGE: diff
CODE:
+ from spacy.language import Language

+ @Language.factory("my_component")
+ def create_my_component(nlp, name):
+     return MyComponent(nlp)

class MyComponent:
    def __init__(self, nlp):
        self.nlp = nlp

    def __call__(self, doc):
        return doc

----------------------------------------

TITLE: Serializing EditTreeLemmatizer to Disk in spaCy
DESCRIPTION: Example showing how to serialize a trainable lemmatizer component to disk using the to_disk method. The method saves the component to a specified directory path.

LANGUAGE: python
CODE:
lemmatizer = nlp.add_pipe("trainable_lemmatizer", name="lemmatizer")
lemmatizer.to_disk("/path/to/lemmatizer")

----------------------------------------

TITLE: Checking a Boolean Flag on a Lexeme in Python
DESCRIPTION: Shows how to check the value of a boolean flag on a lexeme. The example creates a flag for library names and verifies that 'spaCy' has this flag set to True.

LANGUAGE: python
CODE:
is_my_library = lambda text: text in ["spaCy", "Thinc"]
MY_LIBRARY = nlp.vocab.add_flag(is_my_library)
assert nlp.vocab["spaCy"].check_flag(MY_LIBRARY) == True

----------------------------------------

TITLE: Training Text Classification Models from CLI
DESCRIPTION: Demonstrates how to train a text classification model using spaCy's command line interface, specifying the pipeline, architecture type, and enabling multilabel classification.

LANGUAGE: bash
CODE:
$ python -m spacy train en /output /train /dev \
--pipeline textcat --textcat-arch simple_cnn \
--textcat-multilabel

----------------------------------------

TITLE: Creating a Relation Extraction Pipeline Component Skeleton in Python
DESCRIPTION: The skeleton structure of a custom relation extraction component that extends TrainablePipe. This defines the basic methods needed for a trainable spaCy pipeline component including constructor, update, predict, set_annotations, and initialize methods.

LANGUAGE: python
CODE:
from spacy.pipeline import TrainablePipe

class RelationExtractor(TrainablePipe):
     def __init__(self, vocab, model, name="rel"):
        """Create a component instance."""
        self.model = model
        self.vocab = vocab
        self.name = name

    def update(self, examples, drop=0.0, sgd=None, losses=None):
        """Learn from a batch of Example objects."""
        ...

    def predict(self, docs):
        """Apply the model to a batch of Doc objects."""
        ...

    def set_annotations(self, docs, predictions):
        """Modify a batch of Doc objects using the predictions."""
         ...

    def initialize(self, get_examples, nlp=None, labels=None):
        """Initialize the model before training."""
        ...

    def add_label(self, label):
        """Add a label to the component."""
        ...

----------------------------------------

TITLE: Understanding DocBin msgpack Structure in Python
DESCRIPTION: This snippet shows the internal structure of the msgpack object used by DocBin for serialization. It includes version information, attributes list, token data, spaces data, document lengths, and a list of unique strings.

LANGUAGE: python
CODE:
{
    "version": str,           # DocBin version number
    "attrs": List[uint64],    # e.g. [TAG, HEAD, ENT_IOB, ENT_TYPE]
    "tokens": bytes,          # Serialized numpy uint64 array with the token data
    "spaces": bytes,          # Serialized numpy boolean array with spaces data
    "lengths": bytes,         # Serialized numpy int32 array with the doc lengths
    "strings": List[str]      # List of unique strings in the token data
}

----------------------------------------

TITLE: Creating Optimizer for SpanFinder in spaCy
DESCRIPTION: Create an optimizer specifically for the SpanFinder pipeline component.

LANGUAGE: python
CODE:
span_finder = nlp.add_pipe("span_finder")
optimizer = span_finder.create_optimizer()

----------------------------------------

TITLE: Serializing CuratedTransformer to Disk in spaCy
DESCRIPTION: Saves the CuratedTransformer component to a specified directory path on disk.

LANGUAGE: python
CODE:
trf = nlp.add_pipe("curated_transformer")
trf.to_disk("/path/to/transformer")

----------------------------------------

TITLE: Configuration for Label Initialization
DESCRIPTION: Configuration example showing how to specify component labels for initialization when using a streamed corpus. This uses a built-in label reader to load labels from a JSON file for a text categorization component.

LANGUAGE: ini
CODE:
[initialize.components.textcat.labels]
@readers = "spacy.read_labels.v1"
path = "labels/textcat.json"
require = true

----------------------------------------

TITLE: Configuring Remote Storage Locations in project.yml
DESCRIPTION: YAML configuration for defining remote storage locations in a spaCy project file, with an S3 bucket as default and a local filesystem path as an alternative.

LANGUAGE: yaml
CODE:
remotes:
  default: 's3://my-spacy-bucket'
  local: '/mnt/scratch/cache'

----------------------------------------

TITLE: Relation Instance Generator Configuration in INI Format
DESCRIPTION: Configuration for the entity pair candidate generator, specifying the maximum token distance between entities to consider as potential relations.

LANGUAGE: ini
CODE:
[model.create_instance_tensor.get_instances]
@misc = "rel_instance_generator.v1"
max_length = 100

----------------------------------------

TITLE: Configuring and Adding Morphologizer Component in spaCy
DESCRIPTION: Example showing how to configure a Morphologizer component with a default model and add it to the spaCy pipeline.

LANGUAGE: python
CODE:
from spacy.pipeline.morphologizer import DEFAULT_MORPH_MODEL
config = {"model": DEFAULT_MORPH_MODEL}
nlp.add_pipe("morphologizer", config=config)

----------------------------------------

TITLE: Serializing CuratedTransformer to Bytes in spaCy
DESCRIPTION: Converts the CuratedTransformer component to a bytestring for serialization.

LANGUAGE: python
CODE:
trf = nlp.add_pipe("curated_transformer")
trf_bytes = trf.to_bytes()

----------------------------------------

TITLE: Accessing individual tokens from a Span in Python with spaCy
DESCRIPTION: Demonstrates how to access a specific token within a Span using indexing. This retrieves the token at position 1 within the span.

LANGUAGE: python
CODE:
doc = nlp("Give it back! He pleaded.")
span = doc[1:4]
assert span[1].text == "back"

----------------------------------------

TITLE: Predicting spans with SpanResolver in spaCy
DESCRIPTION: Shows how to use the predict method of the SpanResolver component to get span predictions for a batch of documents without modifying them. Returns MentionClusters for each input document.

LANGUAGE: python
CODE:
span_resolver = nlp.add_pipe("experimental_span_resolver")
spans = span_resolver.predict([doc1, doc2])

----------------------------------------

TITLE: Serializing spaCy Lemmatizer to Bytes
DESCRIPTION: Demonstrates how to serialize a lemmatizer component to a bytestring. This allows for saving the lemmatizer state without writing to disk.

LANGUAGE: python
CODE:
lemmatizer = nlp.add_pipe("lemmatizer")
lemmatizer_bytes = lemmatizer.to_bytes()

----------------------------------------

TITLE: Activating GPU for spaCy
DESCRIPTION: Configures spaCy to use GPU if available. This function should be called immediately after importing spaCy and before loading any pipelines. Returns a boolean indicating whether GPU was activated.

LANGUAGE: python
CODE:
import spacy
activated = spacy.prefer_gpu()
nlp = spacy.load("en_core_web_sm")

----------------------------------------

TITLE: Getting Vector Count in Python
DESCRIPTION: Method to return the total number of vectors in the table.

LANGUAGE: python
CODE:
def __len__(self):

----------------------------------------

TITLE: Retrieving a Doc Extension in spaCy
DESCRIPTION: Shows how to retrieve a previously registered extension on a Doc object using get_extension(). Returns a tuple containing the extension's default value, method, getter, and setter.

LANGUAGE: python
CODE:
from spacy.tokens import Doc
Doc.set_extension("has_city", default=False)
extension = Doc.get_extension("has_city")
assert extension == (False, None, None, None)

----------------------------------------

TITLE: Initializing Corpus from Files
DESCRIPTION: Example showing how to initialize a Corpus object from either a single .spacy file or a directory of files. The second example demonstrates using the limit parameter to restrict the number of examples loaded.

LANGUAGE: python
CODE:
from spacy.training import Corpus

# With a single file
corpus = Corpus("./data/train.spacy")

# With a directory
corpus = Corpus("./data", limit=10)

----------------------------------------

TITLE: Loading AttributeRuler from Bytes in spaCy
DESCRIPTION: Example showing how to load an AttributeRuler pipe from a bytestring. The method modifies the existing object in place and returns it.

LANGUAGE: python
CODE:
ruler_bytes = ruler.to_bytes()
ruler = nlp.add_pipe("attribute_ruler")
ruler.from_bytes(ruler_bytes)

----------------------------------------

TITLE: Installing spaCy with Lookup Data
DESCRIPTION: Command to install spaCy with additional lookup data which includes both lemmatization tables and normalization tables required for training new models in spaCy v2.3.

LANGUAGE: bash
CODE:
$ pip install spacy[lookups]

----------------------------------------

TITLE: Using Custom Pipeline Components for Extensions in spaCy
DESCRIPTION: Demonstrates how to integrate custom data processing into the spaCy pipeline using custom components in v2.0. This allows automatic assignment of custom attributes during document processing.

LANGUAGE: diff
CODE:
- doc = nlp("Doc with a standard pipeline")
- meta = get_meta(doc)

+ nlp.add_pipe(meta_component)
+ doc = nlp("Doc with a custom pipeline that assigns meta")
+ meta = doc._.meta

----------------------------------------

TITLE: Serializing SpanCategorizer to disk
DESCRIPTION: Demonstrates how to save a SpanCategorizer component to disk. The method accepts a path parameter and optional exclude parameter to specify serialization fields to exclude.

LANGUAGE: python
CODE:
spancat = nlp.add_pipe("spancat")
spancat.to_disk("/path/to/spancat")

----------------------------------------

TITLE: Updating CuratedTransformer Model in spaCy
DESCRIPTION: Prepares for a model update by running the transformer and communicating output and backpropagation callbacks to downstream components with transformer listener sublayers.

LANGUAGE: python
CODE:
trf = nlp.add_pipe("curated_transformer")
optimizer = nlp.initialize()
losses = trf.update(examples, sgd=optimizer)

----------------------------------------

TITLE: Reading spaCy Configuration from Standard Input
DESCRIPTION: Example showing how to pipe a generated config directly into the spaCy train command using standard input, allowing for quick experiments without saving to disk.

LANGUAGE: bash
CODE:
$ python -m spacy init config - --lang en --pipeline ner,textcat --optimize accuracy | python -m spacy train - --paths.train ./corpus/train.spacy --paths.dev ./corpus/dev.spacy

----------------------------------------

TITLE: Installing project requirements and downloading assets
DESCRIPTION: Commands to navigate to the cloned project directory, install required dependencies, and download necessary data assets for the NER demo project.

LANGUAGE: bash
CODE:
cd ner_demo
python -m pip install -r requirements.txt
python -m spacy project assets

----------------------------------------

TITLE: Full Shape Inference with Model Initialization in Thinc
DESCRIPTION: Shows how to use Thinc's full shape inference capability by initializing a model with sample inputs and outputs. This allows Thinc to deduce both input and output dimensions automatically.

LANGUAGE: python
CODE:
with Model.define_operators({">>" : chain}):
    layers = (
        Relu(hidden_width)
        >> Dropout(dropout)
        >> Relu(hidden_width)
        >> Dropout(dropout)
        >> Softmax()
    )
    model = char_embed >> with_array(layers)
    model.initialize(X=input_sample, Y=output_sample)

----------------------------------------

TITLE: Computing Semantic Similarity Between Lexemes in Python
DESCRIPTION: Demonstrates how to compute semantic similarity between two lexemes using vector representations. This example shows that similarity is symmetric between 'apple' and 'orange'.

LANGUAGE: python
CODE:
apple = nlp.vocab["apple"]
orange = nlp.vocab["orange"]
apple_orange = apple.similarity(orange)
orange_apple = orange.similarity(apple)
assert apple_orange == orange_apple

----------------------------------------

TITLE: Excluding fields during CuratedTransformer serialization
DESCRIPTION: Example showing how to exclude specific serialization fields when saving a CuratedTransformer component to disk. This demonstrates excluding the vocabulary during serialization.

LANGUAGE: python
CODE:
data = trf.to_disk("/path", exclude=["vocab"])

----------------------------------------

TITLE: Accessing Span.vector in spaCy
DESCRIPTION: Demonstrates how to access the vector representation of a span, which defaults to an average of the token vectors. The example shows the vector's data type and shape.

LANGUAGE: python
CODE:
doc = nlp("I like apples")
assert doc[1:].vector.dtype == "float32"
assert doc[1:].vector.shape == (300,)

----------------------------------------

TITLE: Initializing Morphologizer Component in spaCy
DESCRIPTION: Examples showing different ways to construct a Morphologizer component, including via add_pipe with default model, add_pipe with custom model, and direct instantiation from the class.

LANGUAGE: python
CODE:
# Construction via add_pipe with default model
morphologizer = nlp.add_pipe("morphologizer")

# Construction via create_pipe with custom model
config = {"model": {"@architectures": "my_morphologizer"}}
morphologizer = nlp.add_pipe("morphologizer", config=config)

# Construction from class
from spacy.pipeline import Morphologizer
morphologizer = Morphologizer(nlp.vocab, model)

----------------------------------------

TITLE: Serializing a spaCy Tokenizer to Bytes
DESCRIPTION: Demonstrates serializing a spaCy Tokenizer to a bytestring using the to_bytes method, which is useful for storing the tokenizer in memory or databases.

LANGUAGE: python
CODE:
tokenizer = tokenizer(nlp.vocab)
tokenizer_bytes = tokenizer.to_bytes()

----------------------------------------

TITLE: Using PhraseMatcher to find matches in text (Python)
DESCRIPTION: Demonstrates how to add patterns to the PhraseMatcher and use it to find matches in a document. This example adds a pattern for 'Barack Obama' and finds instances of it in the text.

LANGUAGE: python
CODE:
from spacy.matcher import PhraseMatcher

matcher = PhraseMatcher(nlp.vocab)
matcher.add("OBAMA", [nlp("Barack Obama")])
doc = nlp("Barack Obama lifts America one last time in emotional farewell")
matches = matcher(doc)

----------------------------------------

TITLE: Serializing Vectors to Bytes in Python
DESCRIPTION: Method for serializing vector data to a binary string.

LANGUAGE: python
CODE:
def to_bytes(self):

----------------------------------------

TITLE: Accessing and using SpanCategorizer label data
DESCRIPTION: Demonstrates how to access the label data for initialization. This data is used to initialize the model with a pre-defined label set, which can be helpful when restoring a model.

LANGUAGE: python
CODE:
labels = spancat.label_data
spancat.initialize(lambda: [], nlp=nlp, labels=labels)

----------------------------------------

TITLE: Referencing the CuratedTransformer Source Code on GitHub
DESCRIPTION: Link to the source code of the CuratedTransformer component on GitHub, showing where the implementation can be found in the spacy-curated-transformers repository.

LANGUAGE: python
CODE:
https://github.com/explosion/spacy-curated-transformers/blob/main/spacy_curated_transformers/pipeline/transformer.py

----------------------------------------

TITLE: Configuring Chinese Tokenizer in spaCy
DESCRIPTION: Configuration file settings for the Chinese tokenizer, specifying which segmentation method to use (char, jieba, or pkuseg).

LANGUAGE: ini
CODE:
[nlp.tokenizer]
@tokenizers = "spacy.zh.ChineseTokenizer"
segmenter = "char"

----------------------------------------

TITLE: Fixing NumPy Compatibility Issues with Build Constraints in Shell
DESCRIPTION: Command to install spaCy with build constraints to fix NumPy compatibility issues. Uses PIP_CONSTRAINT environment variable to specify an older version of NumPy during compilation while allowing newer versions at runtime.

LANGUAGE: shell
CODE:
PIP_CONSTRAINT=https://raw.githubusercontent.com/explosion/spacy/master/build-constraints.txt \
pip install spacy --no-cache-dir

----------------------------------------

TITLE: Installing PyTorch with CUDA support for transformer models
DESCRIPTION: Command for installing PyTorch 1.11.0 with CUDA 11.3 support using pip. This is a prerequisite step before installing spaCy with transformer support.

LANGUAGE: bash
CODE:
# See: https://pytorch.org/get-started/locally/
$ pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html

----------------------------------------

TITLE: Constructing a Doc object with POS tags in spaCy tests
DESCRIPTION: Shows how to manually create a Doc object with predefined words and part-of-speech tags for testing purposes without relying on other library functionality.

LANGUAGE: python
CODE:
def test_doc_creation_with_pos():
    doc = Doc(Vocab(), words=["hello", "world"], pos=["NOUN", "VERB"])
    assert doc[0].pos_ == "NOUN"
    assert doc[1].pos_ == "VERB"

----------------------------------------

TITLE: Migrating Lexeme Iteration from spaCy v2.2 to v2.3
DESCRIPTION: Code diff showing how to migrate from iterating over all lexemes in nlp.vocab to specifically iterating over words with vectors in spaCy v2.3, since lexemes are no longer preloaded for models with vectors.

LANGUAGE: python
CODE:
- lexemes = [w for w in nlp.vocab]
+ lexemes = [nlp.vocab[orth] for orth in nlp.vocab.vectors]

----------------------------------------

TITLE: Installing spaCy with GPU support
DESCRIPTION: Command to install spaCy with CUDA 11.3 GPU support using pip, which includes the CuPy dependency for GPU acceleration.

LANGUAGE: bash
CODE:
$ pip install -U %%SPACY_PKG_NAME[cuda113]%%SPACY_PKG_FLAGS

----------------------------------------

TITLE: Retrieving a Lexeme from the Vocabulary in Cython
DESCRIPTION: Example showing how to retrieve a LexemeC pointer from the vocabulary using a string. This demonstrates accessing word information directly from the vocabulary.

LANGUAGE: python
CODE:
lexeme = vocab.get(vocab.mem, "hello")

----------------------------------------

TITLE: Processing Documents with SpanRuler in Python
DESCRIPTION: Shows how to process a document with SpanRuler to find matches and add them to doc.spans with a specific key. The __call__ method is typically called automatically in the pipeline.

LANGUAGE: python
CODE:
ruler = nlp.add_pipe("span_ruler")
ruler.add_patterns([{"label": "ORG", "pattern": "Apple"}])

doc = nlp("A text about Apple.")
spans = [(span.text, span.label_) for span in doc.spans["ruler"]]
assert spans == [("Apple", "ORG")]

----------------------------------------

TITLE: Creating an Optimizer for CoreferenceResolver in spaCy (Python)
DESCRIPTION: Demonstrates how to create an optimizer specifically for the coreference resolution component, which can be used for model training.

LANGUAGE: python
CODE:
coref = nlp.add_pipe("experimental_coref")
optimizer = coref.create_optimizer()

----------------------------------------

TITLE: Using Built-in Dropout and Normalization in Thinc Layers
DESCRIPTION: Demonstrates Thinc's layer configuration which allows specifying dropout and normalization directly in layer constructors. This simplifies model definitions by avoiding explicit Dropout and LayerNorm components.

LANGUAGE: python
CODE:
with Model.define_operators({">>" : chain}):
    layers = (
        Relu(hidden_width, dropout=dropout, normalize=False)
        >> Relu(hidden_width, dropout=dropout, normalize=False)
        >> Softmax()
    )
    model = char_embed >> with_array(layers)
    model.initialize(X=input_sample, Y=output_sample)

----------------------------------------

TITLE: Running Train and Push Commands in Bash
DESCRIPTION: Example commands for running a training task and then pushing the results to remote storage using the spaCy project CLI.

LANGUAGE: bash
CODE:
$ python -m spacy project run train
$ python -m spacy project push

----------------------------------------

TITLE: Configuring Vectors Objective for spaCy Pretraining
DESCRIPTION: This snippet shows how to configure the Vectors pretraining objective which asks the model to predict word vectors from a static embeddings table. It specifies parameters like maxout_pieces, hidden_size, and the loss function (cosine).

LANGUAGE: ini
CODE:
### Vectors objective
[pretraining.objective]
@architectures = "spacy.PretrainVectors.v1"
maxout_pieces = 3
hidden_size = 300
loss = "cosine"

----------------------------------------

TITLE: Using EntityRecognizer's pipe Method in spaCy
DESCRIPTION: Example of applying the EntityRecognizer to a stream of documents using the pipe method with batching. This method allows efficient processing of multiple documents.

LANGUAGE: python
CODE:
ner = nlp.add_pipe("ner")
for doc in ner.pipe(docs, batch_size=50):
    pass

----------------------------------------

TITLE: Using Morphologizer.pipe Method in spaCy
DESCRIPTION: Demonstrates how to apply the morphologizer component to a stream of documents using the pipe method with a specified batch size. This is usually called by the nlp pipeline.

LANGUAGE: python
CODE:
morphologizer = nlp.add_pipe("morphologizer")
for doc in morphologizer.pipe(docs, batch_size=50):
    pass

----------------------------------------

TITLE: Installing and Setting Up spaCy Website Locally
DESCRIPTION: Commands to clone the repository, switch to the correct Node version using NVM, install dependencies, and start the development server.

LANGUAGE: bash
CODE:
# Clone the repository
git clone https://github.com/explosion/spaCy
cd spaCy/website

# Switch to the correct Node version
#
# If you don't have NVM and don't want to use it, you can manually switch to the Node version
# stated in /.nvmrc and skip this step
nvm use

# Install the dependencies
npm install

# Start the development server
npm run dev

----------------------------------------

TITLE: Configuring MaxoutWindowEncoder.v1 Architecture in spaCy
DESCRIPTION: Example configuration for the legacy MaxoutWindowEncoder.v1 architecture, which produces a model of type Model[Floats2D, Floats2D]. It encodes context using convolutions with maxout activation, layer normalization, and residual connections.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.MaxoutWindowEncoder.v1"
width = 128
window_size = 1
maxout_pieces = 3
depth = 4

----------------------------------------

TITLE: Loading a Table from Bytes
DESCRIPTION: Loads a Table object from a previously serialized bytestring.

LANGUAGE: python
CODE:
table_bytes = table.to_bytes()
table = Table()
table.from_bytes(table_bytes)

----------------------------------------

TITLE: Configuring SpaCy Tokenizer in INI Format
DESCRIPTION: Default configuration for the SpaCy tokenizer in INI format, which specifies the tokenizer to use for the NLP pipeline.

LANGUAGE: ini
CODE:
[nlp.tokenizer]
@tokenizers = "spacy.Tokenizer.v1"

----------------------------------------

TITLE: Setting Coreference Annotations in spaCy Documents (Python)
DESCRIPTION: Demonstrates how to set coreference annotations on a batch of documents after prediction. This populates the Doc.spans attribute with coreference clusters.

LANGUAGE: python
CODE:
coref = nlp.add_pipe("experimental_coref")
clusters = coref.predict([doc1, doc2])
coref.set_annotations([doc1, doc2], clusters)

----------------------------------------

TITLE: Creating Optimizer for Transformer in spaCy
DESCRIPTION: Creates an optimizer specifically for the transformer pipeline component. Returns the optimizer object that can be used for model updates.

LANGUAGE: python
CODE:
trf = nlp.add_pipe("transformer")
optimizer = trf.create_optimizer()

----------------------------------------

TITLE: Using Getter Functions for Token Extensions in spaCy
DESCRIPTION: Best practice for extending Token objects using getter functions rather than setting values explicitly, which helps manage extension data properly for the C extension classes.

LANGUAGE: diff
CODE:
+ is_fruit = lambda token: token.text in ("apple", "orange")
+ Token.set_extension("is_fruit", getter=is_fruit)

- token._.set_extension("is_fruit", default=False)
- if token.text in ("\"apple\"", "\"orange\"):
-     token._.set("is_fruit", True)

----------------------------------------

TITLE: Accessing SpanCategorizer labels
DESCRIPTION: Demonstrates how to access labels that have been added to the SpanCategorizer component. The property returns a tuple of strings representing the labels.

LANGUAGE: python
CODE:
spancat.add_label("MY_LABEL")
assert "MY_LABEL" in spancat.labels

----------------------------------------

TITLE: Migrating Document Processing from spaCy v1.x to v2.0
DESCRIPTION: Example of updating code from individually processing documents to using the more efficient pipe method, which allows for document batching and offers significant performance advantages in spaCy v2.0.

LANGUAGE: python
CODE:
- docs = (nlp(text) for text in texts)

+ docs = nlp.pipe(texts)

----------------------------------------

TITLE: Defining Remote Storage in YAML Configuration
DESCRIPTION: YAML configuration that defines remote storage locations for pushing and pulling project assets and outputs, supporting protocols like S3 and local paths.

LANGUAGE: yaml
CODE:
remotes:
  default: 's3://my-spacy-bucket'
  local: '/mnt/scratch/cache'

----------------------------------------

TITLE: Checking if Label Exists in SpanRuler in Python
DESCRIPTION: Demonstrates how to check if a specific label exists in the SpanRuler's patterns using the __contains__ method.

LANGUAGE: python
CODE:
ruler = nlp.add_pipe("span_ruler")
ruler.add_patterns([{"label": "ORG", "pattern": "Apple"}])
assert "ORG" in ruler
assert not "PERSON" in ruler

----------------------------------------

TITLE: Configuring Custom Vectors in spaCy Config
DESCRIPTION: An INI configuration snippet showing how to specify a custom registered vector function in a spaCy config file. This enables the use of BPEmb vectors in an English language pipeline.

LANGUAGE: ini
CODE:
[nlp.vectors]
@vectors = "BPEmbVectors.v1"
lang = "en"

----------------------------------------

TITLE: Simple Example of Debugging a spaCy Config in Bash
DESCRIPTION: A simple example of using the debug config command to validate a configuration file named config.cfg.

LANGUAGE: bash
CODE:
$ python -m spacy debug config config.cfg

----------------------------------------

TITLE: Configuring Characters Objective for spaCy Pretraining
DESCRIPTION: This snippet demonstrates how to configure the Characters pretraining objective which predicts leading and trailing UTF-8 bytes for words. It specifies parameters like maxout_pieces, hidden_size, and the number of characters to predict.

LANGUAGE: ini
CODE:
### Characters objective
[pretraining.objective]
@architectures = "spacy.PretrainCharacters.v1"
maxout_pieces = 3
hidden_size = 300
n_characters = 4

----------------------------------------

TITLE: Updating Morphologizer Model in spaCy
DESCRIPTION: Demonstrates how to update the morphologizer component's model by learning from examples. This method takes a batch of examples and an optimizer to update the model weights.

LANGUAGE: python
CODE:
morphologizer = nlp.add_pipe("morphologizer")
optimizer = nlp.initialize()
losses = morphologizer.update(examples, sgd=optimizer)

----------------------------------------

TITLE: Downloading spaCy Models in Jupyter Notebooks
DESCRIPTION: Shows how to download a spaCy language model in a Jupyter notebook environment using the '!' prefix to execute shell commands.

LANGUAGE: bash
CODE:
!python -m spacy download en_core_web_sm

----------------------------------------

TITLE: Serializing Transformer Component to Disk in spaCy
DESCRIPTION: Saves the transformer pipeline component to disk for later reuse. Creates the directory if it doesn't exist.

LANGUAGE: python
CODE:
trf = nlp.add_pipe("transformer")
trf.to_disk("/path/to/transformer")

----------------------------------------

TITLE: Checking String Existence in StringStore with Python
DESCRIPTION: Demonstrates how to check if a string exists in the StringStore using the 'in' operator.

LANGUAGE: python
CODE:
stringstore = StringStore(["apple", "orange"])
assert "apple" in stringstore
assert not "cherry" in stringstore

----------------------------------------

TITLE: Accessing lexeme attributes in spaCy
DESCRIPTION: Demonstrates accessing a lexeme struct from a token in a Doc object. The LexemeC struct contains information about lexical types and is typically accessed through a pointer on the TokenC struct.

LANGUAGE: python
CODE:
lex = doc.c[3].lex

----------------------------------------

TITLE: Excluding Fields During CoreferenceResolver Serialization in spaCy (Python)
DESCRIPTION: Demonstrates how to exclude specific serialization fields when saving a coreference resolution model, which can be useful for reducing file size or avoiding overwriting existing resources.

LANGUAGE: python
CODE:
data = coref.to_disk("/path", exclude=["vocab"])

----------------------------------------

TITLE: Accessing Tagger Labels in spaCy
DESCRIPTION: Shows how to access the labels that have been added to a tagger component. This returns a tuple of all label strings.

LANGUAGE: python
CODE:
tagger.add_label("MY_LABEL")
assert "MY_LABEL" in tagger.labels

----------------------------------------

TITLE: Applying Replacement Functions for Listeners in Python
DESCRIPTION: Code showing how nlp.replace_listeners() uses functions defined in model.attrs to make appropriate changes to the configuration and model when replacing a listener with a standalone component.

LANGUAGE: python
CODE:
replace_func = tok2vec_model.attrs["replace_listener_cfg"]
new_config = replace_func(tok2vec_cfg["model"], pipe_cfg["model"]["tok2vec"])
...
new_model = tok2vec_model.attrs["replace_listener"](new_model)

----------------------------------------

TITLE: Migrating Imports from gold Module to training Module
DESCRIPTION: Shows the API change for importing utility functions that have been moved from spacy.gold to spacy.training module and renamed for consistency.

LANGUAGE: diff
CODE:
- from spacy.gold import biluo_tags_from_offsets, offsets_from_biluo_tags, spans_from_biluo_tags
+ from spacy.training import offsets_to_biluo_tags, biluo_tags_to_offsets, biluo_tags_to_spans

----------------------------------------

TITLE: Running the complete NER project workflow
DESCRIPTION: Command to execute all steps in the NER project workflow, including data conversion, configuration creation, model training, and evaluation.

LANGUAGE: bash
CODE:
python -m spacy project run all

----------------------------------------

TITLE: Installing spaCy Hugging Face Hub Integration
DESCRIPTION: Commands for installing the spacy-huggingface-hub package and logging into Hugging Face. This integration enables uploading spaCy pipelines to the Hugging Face Hub.

LANGUAGE: bash
CODE:
$ pip install spacy-huggingface-hub
$ huggingface-cli login

----------------------------------------

TITLE: Saving EntityRecognizer to Disk in spaCy
DESCRIPTION: Demonstrates how to save a named entity recognizer component to disk using the to_disk method. This allows you to persist the trained model for later use.

LANGUAGE: python
CODE:
ner = nlp.add_pipe("ner")
ner.to_disk("/path/to/ner")

----------------------------------------

TITLE: Updating Model Loading and Path Handling in spaCy
DESCRIPTION: Example showing how to update model loading code to match spaCy v2.0's API. This demonstrates the transition from using the path keyword argument to using more explicit loading methods like from_disk.

LANGUAGE: python
CODE:
- nlp = spacy.load("en", path="/model")

+ nlp = spacy.load("/model")
+ nlp = spacy.blank("en").from_disk("/model/data")

----------------------------------------

TITLE: Installing SentencePiece support for transformers
DESCRIPTION: Command for installing additional SentencePiece dependencies required by certain transformer models like ALBERT, CamemBERT, XLNet, Marian, and T5.

LANGUAGE: bash
CODE:
$ pip install transformers[sentencepiece]

----------------------------------------

TITLE: First Line of Vocabulary Data File
DESCRIPTION: Defines the initial line of a vocabulary data file that specifies the language and settings. This is used to initialize the nlp object's vocabulary in spaCy.

LANGUAGE: python
CODE:
{"lang": "en", "settings": {"oov_prob": -20.502029418945312}}

----------------------------------------

TITLE: Training SpanResolver with update method in spaCy
DESCRIPTION: Shows how to train the SpanResolver model using the update method with a batch of Example objects. Returns the updated losses dictionary.

LANGUAGE: python
CODE:
span_resolver = nlp.add_pipe("experimental_span_resolver")
optimizer = nlp.initialize()
losses = span_resolver.update(examples, sgd=optimizer)

----------------------------------------

TITLE: Configuring Width for Tok2VecListener
DESCRIPTION: Configuration showing how to define the output dimension of a Tok2VecListener by referencing the width parameter from the upstream Tok2Vec component.

LANGUAGE: toml
CODE:
[components.ner.model.tok2vec]
@architectures = "spacy.Tok2VecListener.v1"
width = ${components.tok2vec.model.encode.width}

----------------------------------------

TITLE: Updating PhraseMatcher Pattern Adding in spaCy v3.0
DESCRIPTION: Shows the updated method for adding patterns to a PhraseMatcher, similar to the changes in the regular Matcher.

LANGUAGE: diff
CODE:
matcher = PhraseMatcher(nlp.vocab)
patterns = [nlp("health care reform"), nlp("healthcare reform")]
- matcher.add("HEALTH", on_match, *patterns)
+ matcher.add("HEALTH", patterns, on_match=on_match)

----------------------------------------

TITLE: Rule-based Morphological Analysis in spaCy
DESCRIPTION: Demonstrates how spaCy's rule-based approach assigns morphological features for languages with simpler morphological systems like English. This approach uses token text and fine-grained POS tags to produce features.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Where are you?")
print(doc[2].morph)  # 'Case=Nom|Person=2|PronType=Prs'
print(doc[2].pos_)  # 'PRON'

----------------------------------------

TITLE: Using to_bytes and from_bytes Methods in spaCy
DESCRIPTION: These code snippets demonstrate how to serialize a spaCy NLP object to bytes and load it back. The to_bytes() method converts the object to a byte string, while from_bytes() reconstructs the object from the serialized data.

LANGUAGE: python
CODE:
data = nlp.to_bytes()

LANGUAGE: python
CODE:
nlp.from_bytes(data)

----------------------------------------

TITLE: Retrieving the Error Handler from a TrainablePipe in Python
DESCRIPTION: Example demonstrating how to retrieve the current error handler function from a pipeline component using the get_error_handler method.

LANGUAGE: python
CODE:
pipe = nlp.add_pipe("ner")
error_handler = pipe.get_error_handler()

----------------------------------------

TITLE: Initializing Chinese Tokenizer with PKUSeg Models
DESCRIPTION: Configuration file settings for initializing the PKUSeg tokenizer with specific model paths before training, allowing the model to be serialized with the pipeline.

LANGUAGE: ini
CODE:
[initialize]

[initialize.tokenizer]
pkuseg_model = "/path/to/model"
pkuseg_user_dict = "default"

----------------------------------------

TITLE: Using Morphologizer Component on a Document in spaCy
DESCRIPTION: Example demonstrating how to apply a Morphologizer component to a document, which modifies the document in place by adding morphological features and POS tags.

LANGUAGE: python
CODE:
doc = nlp("This is a sentence.")
morphologizer = nlp.add_pipe("morphologizer")
# This usually happens under the hood
processed = morphologizer(doc)

----------------------------------------

TITLE: Configuring HashEmbedCNN Architecture in spaCy
DESCRIPTION: Example configuration for the HashEmbedCNN.v2 architecture which builds spaCy's standard token-to-vector layer using a MultiHashEmbed embedding layer and a MaxoutWindowEncoder encoding layer.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.HashEmbedCNN.v2"
pretrained_vectors = null
width = 96
depth = 4
embed_size = 2000
window_size = 1
maxout_pieces = 3
subword_features = true

----------------------------------------

TITLE: Appending a Token to a Doc in Cython
DESCRIPTION: Example showing how to create a Doc object and append a token to it using the push_back method. This demonstrates creating a Doc from a Vocab and adding a lexeme with trailing whitespace.

LANGUAGE: python
CODE:
from spacy.tokens cimport Doc
from spacy.vocab cimport Vocab

doc = Doc(Vocab())
lexeme = doc.vocab.get("hello")
doc.push_back(lexeme, True)
assert doc.text == "hello "

----------------------------------------

TITLE: Using Corpus Object to Generate Training Examples
DESCRIPTION: Example demonstrating how to use a Corpus object to generate training examples for a spaCy model. Creates a blank language model and feeds it to the corpus to yield Example objects.

LANGUAGE: python
CODE:
from spacy.training import Corpus
import spacy

corpus = Corpus("./train.spacy")
nlp = spacy.blank("en")
train_data = corpus(nlp)

----------------------------------------

TITLE: Performing Rehearsal Update with SentenceRecognizer in spaCy
DESCRIPTION: Shows how to perform a 'rehearsal' update to address catastrophic forgetting. This experimental feature teaches the current model to make predictions similar to an initial model.

LANGUAGE: python
CODE:
senter = nlp.add_pipe("senter")
optimizer = nlp.resume_training()
losses = senter.rehearse(examples, sgd=optimizer)

----------------------------------------

TITLE: Creating a Character Span from a Doc in spaCy
DESCRIPTION: Demonstrates how to create a Span object from character indices in a Doc's text using char_span(). This allows for creating annotated spans based on character positions.

LANGUAGE: python
CODE:
doc = nlp("I like New York")
span = doc.char_span(7, 15, label="GPE")
assert span.text == "New York"

----------------------------------------

TITLE: Retrieving Vectors by Key in spaCy Python
DESCRIPTION: Example of accessing a vector by its key from the vocabulary's vector store. The key can be either a string ID or the string itself, and raises KeyError if not found.

LANGUAGE: python
CODE:
cat_id = nlp.vocab.strings["cat"]
cat_vector = nlp.vocab.vectors[cat_id]
assert cat_vector == nlp.vocab["cat"].vector

----------------------------------------

TITLE: Adding Incorrect Span Annotations for NER Training in spaCy
DESCRIPTION: This Python code demonstrates how to add incorrect span annotations to a document for training the entity recognizer. It defines spans that are explicitly marked as incorrect categories.

LANGUAGE: python
CODE:
train_doc = nlp.make_doc("Barack Obama was born in Hawaii.")
# The doc.spans key can be defined in the config
train_doc.spans["incorrect_spans"] = [
  Span(doc, 0, 2, label="ORG"),
  Span(doc, 5, 6, label="PRODUCT")
]

----------------------------------------

TITLE: Using Specific Parameter Values in DependencyParser
DESCRIPTION: Shows how to temporarily use specific parameter values in a dependency parser model, such as using averaged weights during model saving.

LANGUAGE: python
CODE:
parser = DependencyParser(nlp.vocab)
with parser.use_params(optimizer.averages):
    parser.to_disk("/best_model")

----------------------------------------

TITLE: Setting Environment Variables for spaCy Project Commands
DESCRIPTION: Bash commands showing how to set environment variables before running a spaCy project command. This example sets GPU_ID as an environment variable and passes BATCH_SIZE directly when running the evaluate command.

LANGUAGE: bash
CODE:
export GPU_ID=1
BATCH_SIZE=128 python -m spacy project run evaluate

----------------------------------------

TITLE: Requesting Command-Specific Help
DESCRIPTION: Command to get help text for a specific Explosion-bot command, providing details on usage and available options.

LANGUAGE: markdown
CODE:
@explosion-bot please <command> --help

----------------------------------------

TITLE: Creating and Adding Pipeline Components in spaCy v3.0
DESCRIPTION: Demonstrates how nlp.add_pipe now returns the component itself, eliminating the need for nlp.create_pipe in most cases.

LANGUAGE: diff
CODE:
- parser = nlp.create_pipe("parser")
- nlp.add_pipe(parser)
+ parser = nlp.add_pipe("parser")

----------------------------------------

TITLE: Getting the length of a Span in Python with spaCy
DESCRIPTION: Shows how to determine the number of tokens in a Span using the len() function. The example creates a span of 3 tokens and verifies its length.

LANGUAGE: python
CODE:
doc = nlp("Give it back! He pleaded.")
span = doc[1:4]
assert len(span) == 3

----------------------------------------

TITLE: Creating Links with JSX Components in spaCy Documentation
DESCRIPTION: How to create links using the Link JSX component for the spaCy documentation, which provides special styling based on the link destination.

LANGUAGE: jsx
CODE:
<Link to="https://spacy.io">I am a link</Link>

----------------------------------------

TITLE: Serializing SpanRuler to Bytes in Python
DESCRIPTION: Shows how to serialize a SpanRuler to a bytestring using the to_bytes method.

LANGUAGE: python
CODE:
ruler = nlp.add_pipe("span_ruler")
ruler_bytes = ruler.to_bytes()

----------------------------------------

TITLE: Iterating Over Vector Values in spaCy (Python)
DESCRIPTION: Shows how to iterate through vectors that have been assigned to at least one key. Note that the number of vectors returned may be less than the length of the vectors table as some may be unassigned.

LANGUAGE: python
CODE:
for vector in nlp.vocab.vectors.values():
    print(vector)

----------------------------------------

TITLE: Configuring TextCatReduce Model Architecture in spaCy
DESCRIPTION: Configuration example for the TextCatReduce model architecture, which is a text classifier that pools token hidden representations of each Doc using different reduction methods (first, max, or mean) and applies a classification layer.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.TextCatReduce.v1"
exclusive_classes = false
use_reduce_first = false
use_reduce_last = false
use_reduce_max = false
use_reduce_mean = true
nO = null

[model.tok2vec]
@architectures = "spacy.HashEmbedCNN.v2"
pretrained_vectors = null
width = 96
depth = 4
embed_size = 2000
window_size = 1
maxout_pieces = 3
subword_features = true

----------------------------------------

TITLE: Creating Type Annotations with JSX Components in spaCy Documentation
DESCRIPTION: How to create Python type hint annotations using the TypeAnnotation JSX component with special formatting and automatic linking.

LANGUAGE: markup
CODE:
<TypeAnnotation>Model[List[Doc], Floats2d]</Typeannotation>

----------------------------------------

TITLE: Setting Struct Attributes with Token.set_struct_attr in spaCy
DESCRIPTION: Example showing how to set the value of a specific attribute in a TokenC struct using the static set_struct_attr method. This example sets the TAG attribute of a token to 0.

LANGUAGE: python
CODE:
from spacy.attrs cimport TAG
from spacy.tokens cimport Token

token = &doc.c[3]
Token.set_struct_attr(token, TAG, 0)

----------------------------------------

TITLE: Using EntityRecognizer's __call__ Method in spaCy
DESCRIPTION: Example of applying the EntityRecognizer to a document using the __call__ method, which processes the document in place and returns it. This typically happens automatically when the nlp pipeline processes text.

LANGUAGE: python
CODE:
doc = nlp("This is a sentence.")
ner = nlp.add_pipe("ner")
# This usually happens under the hood
processed = ner(doc)

----------------------------------------

TITLE: Using Logging in spaCy
DESCRIPTION: Shows how to add logging statements for providing information during training or debugging. Uses spaCy's logger which is built on Python's logging module.

LANGUAGE: python
CODE:
+ logger.info("Set up nlp object from config")
config = nlp.config.interpolate()

----------------------------------------

TITLE: Using spaCy CLI to update configuration from v3.5 to v3.6
DESCRIPTION: Command to update a spaCy v3.5 configuration file with new v3.6 settings using the init fill-config command.

LANGUAGE: cli
CODE:
$ python -m spacy init fill-config config-v3.5.cfg config-v3.6.cfg

----------------------------------------

TITLE: Fixing overlapping entities for BILUO tag conversion
DESCRIPTION: Diff showing how to clean up entity offsets to avoid overlapping entities before converting to BILUO tags. The stricter validation now raises errors for invalid entity annotations.

LANGUAGE: diff
CODE:
doc = nlp("I live in Berlin Kreuzberg")
- entities = [(10, 26, "LOC"), (10, 16, "GPE"), (17, 26, "LOC")]
+ entities = [(10, 16, "GPE"), (17, 26, "LOC")]
tags = get_biluo_tags_from_offsets(doc, entities)

----------------------------------------

TITLE: Running a Project Workflow using Command Line
DESCRIPTION: Command to execute a workflow defined in the project.yml file, which runs multiple commands in sequence with dependency tracking.

LANGUAGE: bash
CODE:
$ python -m spacy project run all

----------------------------------------

TITLE: Iterating through StringStore in Python
DESCRIPTION: Shows how to iterate through all strings in a StringStore. Note that a new store always includes an empty string at position 0.

LANGUAGE: python
CODE:
stringstore = StringStore(["apple", "orange"])
all_strings = [s for s in stringstore]
assert all_strings == ["apple", "orange"]

----------------------------------------

TITLE: Using Doc Input for spaCy Pipelines
DESCRIPTION: Code example showing how to create a Doc with custom tokenization and set custom extensions before processing, taking advantage of the new feature allowing Doc objects as input to nlp and nlp.pipe.

LANGUAGE: python
CODE:
doc = nlp.make_doc("This is text 500.")
doc._.text_id = 500
doc = nlp(doc)

----------------------------------------

TITLE: Configuring SpanRuler Component Initialization in INI
DESCRIPTION: Example configuration file for initializing a SpanRuler component with patterns from a JSONL file using srsly reader.

LANGUAGE: ini
CODE:
### config.cfg
[initialize.components.span_ruler]

[initialize.components.span_ruler.patterns]
@readers = "srsly.read_jsonl.v1"
path = "corpus/span_ruler_patterns.jsonl"

----------------------------------------

TITLE: Configuring EntityRuler Component in spaCy Config File
DESCRIPTION: Shows how to set up the EntityRuler component in a spaCy configuration file. The config defines where to load patterns from using a JSONL reader.

LANGUAGE: ini
CODE:
### config.cfg
[initialize.components.entity_ruler]

[initialize.components.entity_ruler.patterns]
@readers = "srsly.read_jsonl.v1"
path = "corpus/entity_ruler_patterns.jsonl

----------------------------------------

TITLE: Loading Tag Maps with AttributeRuler in spaCy v3.0
DESCRIPTION: Shows how to load a tag map using the AttributeRuler component instead of the previous vocabulary morphology method.

LANGUAGE: diff
CODE:
nlp = spacy.blank("en")
- nlp.vocab.morphology.load_tag_map(YOUR_TAG_MAP)
+ ruler = nlp.add_pipe("attribute_ruler")
+ ruler.load_from_tag_map(YOUR_TAG_MAP)

----------------------------------------

TITLE: Handling Immutable Collections in Python Functions
DESCRIPTION: Demonstrates safe modification of frozen collections by creating new collections instead of mutating existing ones, preventing errors with immutable default arguments.

LANGUAGE: diff
CODE:
def do_something(values: List[str] = SimpleFrozenList()):
    if some_condition:
-         values.append("foo")  # raises an error
+         values = [*values, "foo"]
    return values

----------------------------------------

TITLE: Updating spaCy init-model CLI command parameters in v2.1.x
DESCRIPTION: Demonstrates changes to the 'spacy init-model' command which now requires a single --jsonl-loc argument for a newline-delimited JSON file containing lexical entries instead of separate frequency and cluster files.

LANGUAGE: diff
CODE:
- $ spacy init-model en ./model --freqs-loc ./freqs.txt --clusters-loc ./clusters.txt
+ $ spacy init-model en ./model --jsonl-loc ./vocab.jsonl

----------------------------------------

TITLE: Initializing a SentenceRecognizer in spaCy
DESCRIPTION: Examples showing different ways to initialize a SentenceRecognizer component, including via add_pipe with default model, via create_pipe with custom model, and direct class instantiation.

LANGUAGE: python
CODE:
# Construction via add_pipe with default model
senter = nlp.add_pipe("senter")

# Construction via create_pipe with custom model
config = {"model": {"@architectures": "my_senter"}}
senter = nlp.add_pipe("senter", config=config)

# Construction from class
from spacy.pipeline import SentenceRecognizer
senter = SentenceRecognizer(nlp.vocab, model)

----------------------------------------

TITLE: Processing Document Stream with Sentencizer's pipe Method
DESCRIPTION: Example demonstrating how to use the Sentencizer's pipe method to efficiently process a stream of documents with a specified batch size for better performance.

LANGUAGE: Python
CODE:
sentencizer = nlp.add_pipe("sentencizer")
for doc in sentencizer.pipe(docs, batch_size=50):
    pass

----------------------------------------

TITLE: Setting Anthropic API Key in Shell Environment
DESCRIPTION: Sets the environment variable for authenticating with Anthropic's language model services. This credential is required when using Anthropic models like Claude in spaCy.

LANGUAGE: shell
CODE:
export ANTHROPIC_API_KEY="..."

----------------------------------------

TITLE: Accessing Token Ancestors in spaCy
DESCRIPTION: Example showing how to access a token's syntactic ancestors (parents, grandparents, etc.) using the ancestors property. It returns a sequence of tokens.

LANGUAGE: python
CODE:
doc = nlp("Give it back! He pleaded.")
it_ancestors = doc[1].ancestors
assert [t.text for t in it_ancestors] == ["Give"]
he_ancestors = doc[4].ancestors
assert [t.text for t in he_ancestors] == ["pleaded"]

----------------------------------------

TITLE: Accessing Example Alignment Object in spaCy
DESCRIPTION: Shows how to access the Alignment object that maps between predicted and reference tokenization. The alignment.y2x.data shows how tokens in reference map to tokens in predicted.

LANGUAGE: python
CODE:
tokens_x = ["Apply", "some", "sunscreen"]
x = Doc(vocab, words=tokens_x)
tokens_y = ["Apply", "some", "sun", "screen"]
example = Example.from_dict(x, {"words": tokens_y})
alignment = example.alignment
assert list(alignment.y2x.data) == [[0], [1], [2], [2]]

----------------------------------------

TITLE: Creating Aside Components in JSX
DESCRIPTION: Shows how to create aside components using JSX syntax. The Aside component takes a title property and content as children.

LANGUAGE: jsx
CODE:
<Aside title="Aside title">This is aside text.</Aside>

----------------------------------------

TITLE: Converting JSON data to .spacy format using CLI
DESCRIPTION: Command to convert JSON training data to the new .spacy binary format using the spacy convert CLI tool. This replaces the deprecated JSON input format.

LANGUAGE: bash
CODE:
$ python -m spacy convert ./data.json .

----------------------------------------

TITLE: Concatenating SpanGroups with the + operator
DESCRIPTION: Shows how to create a new SpanGroup by concatenating two existing SpanGroups using the + operator.

LANGUAGE: python
CODE:
doc = nlp("Their goi ng home")
doc.spans["errors"] = [doc[0:1], doc[1:3]]
doc.spans["other"] = [doc[0:2], doc[2:4]]
span_group = doc.spans["errors"] + doc.spans["other"]
assert len(span_group) == 4

----------------------------------------

TITLE: Visualizing Overlapping Spans with displaCy in Python
DESCRIPTION: Example demonstrating how to visualize overlapping spans using displaCy's new span style feature. This code creates custom spans for organization and geo-political entity types and renders them in an HTML visualization.

LANGUAGE: python
CODE:
import spacy
from spacy import displacy
from spacy.tokens import Span

nlp = spacy.blank("en")
text = "Welcome to the Bank of China."
doc = nlp(text)
doc.spans["custom"] = [Span(doc, 3, 6, "ORG"), Span(doc, 5, 6, "GPE")]
displacy.serve(doc, style="span", options={"spans_key": "custom"})

----------------------------------------

TITLE: Fetching Project Assets in spaCy using Bash
DESCRIPTION: Command for downloading or copying project assets defined in the project.yml file. Assets with checksums are only downloaded if no local file with the same checksum exists.

LANGUAGE: bash
CODE:
$ python -m spacy project assets [project_dir]

LANGUAGE: bash
CODE:
$ python -m spacy project assets [--sparse]

----------------------------------------

TITLE: Updating Knowledge Base Implementation in spaCy v3.5
DESCRIPTION: Code diff showing how to update from the previous KnowledgeBase constructor to the new InMemoryLookupKB implementation, which is the new default knowledge base in spaCy v3.5.

LANGUAGE: python
CODE:
- kb = KnowledgeBase()
+ kb = InMemoryLookupKB()

----------------------------------------

TITLE: Using Descriptive Component Names in spaCy Extensions
DESCRIPTION: Guidelines for choosing specific and descriptive names for custom pipeline components to avoid confusion and naming conflicts with built-in components.

LANGUAGE: diff
CODE:
+ name = "myapp_lemmatizer"
- name = "lemmatizer"

----------------------------------------

TITLE: Migrating Doc Flags to has_annotation in spaCy v3.0
DESCRIPTION: Shows how to replace the deprecated Doc flags (is_tagged, is_parsed, etc.) with the new Doc.has_annotation method that uses token attribute symbols.

LANGUAGE: diff
CODE:
doc = nlp(text)
- doc.is_parsed
+ doc.has_annotation("DEP")
- doc.is_tagged
+ doc.has_annotation("TAG")
- doc.is_sentenced
+ doc.has_annotation("SENT_START")
- doc.is_nered
+ doc.has_annotation("ENT_IOB")

----------------------------------------

TITLE: Adding Explanatory Comments for Complex Logic
DESCRIPTION: Example of a multi-line comment that explains complex logic related to component configuration and interpolation, providing context for why the code is structured this way.

LANGUAGE: diff
CODE:
+ # To create the components we need to use the final interpolated config
+ # so all values are available (if component configs use variables).
+ # Later we replace the component config with the raw config again.
interpolated = filled.interpolate() if not filled.is_interpolated else filled

----------------------------------------

TITLE: Checking Vector Properties of Tokens in Python
DESCRIPTION: Demonstrates how to check if tokens have vector representations, displaying the vector norm and whether tokens are out-of-vocabulary. Shows the difference between common words and rare words.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("en_core_web_md")
tokens = nlp("dog cat banana afskfsd")

for token in tokens:
    print(token.text, token.has_vector, token.vector_norm, token.is_oov)

----------------------------------------

TITLE: Using merge_subtokens in spaCy Python
DESCRIPTION: Demonstrates the merge_subtokens component which combines tokens with the dependency label 'subtok' into single tokens. This is particularly useful for languages like Chinese, Japanese, or Korean.

LANGUAGE: python
CODE:
doc = nlp("拜托")
print([(token.text, token.dep_) for token in doc])
# [('拜', 'subtok'), ('托', 'subtok')]

nlp.add_pipe("merge_subtokens")
doc = nlp("拜托")
print([token.text for token in doc])
# ['拜托']

----------------------------------------

TITLE: Removing a Table from Lookups
DESCRIPTION: Removes a table from the Lookups object by name and returns the removed table. Raises an error if the table doesn't exist.

LANGUAGE: python
CODE:
lookups = Lookups()
lookups.add_table("some_table")
removed_table = lookups.remove_table("some_table")
assert "some_table" not in lookups

----------------------------------------

TITLE: Sample Logger Output for spaCy-LLM Text Classification
DESCRIPTION: Shows the expected logging output when classifying text with spaCy-LLM. This includes the generated prompt sent to the LLM and the raw response received.

LANGUAGE: text
CODE:
Generated prompt for doc: You look gorgeous!

You are an expert Text Classification system. Your task is to accept Text as input
and provide a category for the text based on the predefined labels.

Classify the text below to any of the following labels: COMPLIMENT, INSULT
The task is non-exclusive, so you can provide more than one label as long as
they're comma-delimited. For example: Label1, Label2, Label3.
Do not put any other text in your answer, only one or more of the provided labels with nothing before or after.
If the text cannot be classified into any of the provided labels, answer `==NONE==`.

Here is the text that needs classification


Text:
'''
You look gorgeous!
'''

Model response for doc: You look gorgeous!
COMPLIMENT

----------------------------------------

TITLE: Applying TextCategorizer to a Single Document
DESCRIPTION: Example showing how to apply the TextCategorizer component directly to a document, which happens automatically during normal pipeline processing.

LANGUAGE: python
CODE:
doc = nlp("This is a sentence.")
textcat = nlp.add_pipe("textcat")
# This usually happens under the hood
processed = textcat(doc)

----------------------------------------

TITLE: Batch Exporting Dependency Parses as SVG Files with spaCy
DESCRIPTION: This example shows how to process multiple sentences, generate dependency visualizations for each, and save them as separate SVG files. The filenames are created from the tokens in each sentence for easy identification.

LANGUAGE: python
CODE:
import spacy
from spacy import displacy
from pathlib import Path

nlp = spacy.load("en_core_web_sm")
sentences = ["This is an example.", "This is another one."]
for sent in sentences:
    doc = nlp(sent)
    svg = displacy.render(doc, style="dep", jupyter=False)
    file_name = '-'.join([w.text for w in doc if not w.is_punct]) + ".svg"
    output_path = Path("/images/" + file_name)
    output_path.open("w", encoding="utf-8").write(svg)

----------------------------------------

TITLE: Embedding GitHub Code in Documentation
DESCRIPTION: Shows how to embed code directly from GitHub repositories in documentation. When a code block contains only a GitHub URL, the raw file contents are automatically embedded with syntax highlighting.

LANGUAGE: markdown
CODE:
```python
https://github.com/...
```

----------------------------------------

TITLE: Updating a DependencyParser Model in spaCy
DESCRIPTION: Demonstrates how to update a dependency parser model with new examples during training, including handling of the optimizer and losses.

LANGUAGE: python
CODE:
parser = nlp.add_pipe("parser")
optimizer = nlp.initialize()
losses = parser.update(examples, sgd=optimizer)

----------------------------------------

TITLE: Initializing a Lemmatizer with different configuration options
DESCRIPTION: Examples of different ways to initialize a Lemmatizer component, including with default model and with custom settings.

LANGUAGE: python
CODE:
# Construction via add_pipe with default model
lemmatizer = nlp.add_pipe("lemmatizer")

# Construction via add_pipe with custom settings
config = {"mode": "rule", "overwrite": True}
lemmatizer = nlp.add_pipe("lemmatizer", config=config)

----------------------------------------

TITLE: Validating spaCy Installation and Models
DESCRIPTION: Commands to update spaCy and validate installed models to check for compatibility after updating, which helps identify models that need to be updated.

LANGUAGE: bash
CODE:
pip install -U spacy
python -m spacy validate

----------------------------------------

TITLE: Adding Subject Relation to Dependency Matcher Pattern in spaCy
DESCRIPTION: This code snippet extends a dependency matcher pattern by adding a subject relation. It demonstrates how to link a new token (the subject) to the anchor token 'founded' using the nsubj dependency relation and the '>' relation operator.

LANGUAGE: python
CODE:
pattern = [
    {
        "RIGHT_ID": "anchor_founded",
        "RIGHT_ATTRS": {"ORTH": "founded"}
    },
    {
        "LEFT_ID": "anchor_founded",
        "REL_OP": ">",
        "RIGHT_ID": "founded_subject",
        "RIGHT_ATTRS": {"DEP": "nsubj"},
    }
    # ...
]

----------------------------------------

TITLE: Installing spaCy with Word Vectors
DESCRIPTION: Shows how to download a larger spaCy pipeline package that includes word vectors, rather than the small package which only has context-sensitive tensors.

LANGUAGE: diff
CODE:
- python -m spacy download en_core_web_sm
+ python -m spacy download en_core_web_lg

----------------------------------------

TITLE: Configuring spacy.EntityLinker.v1 Component with LLM Task
DESCRIPTION: Example configuration for the spacy.EntityLinker.v1 component which uses a language model to link entity mentions to knowledge base entries. The configuration includes path settings and initializes a candidate selector component.

LANGUAGE: ini
CODE:
[paths]
el_nlp = null

...

[components.llm.task]
@llm_tasks = "spacy.EntityLinker.v1"

[initialize]
[initialize.components]
[initialize.components.llm]
[initialize.components.llm.candidate_selector]
@llm_misc = "spacy.CandidateSelector.v1"

# Load a KB from a KB file. For loading KBs from spaCy pipelines see spacy.KBObjectLoader.v1.
[initialize.components.llm.candidate_selector.kb_loader]
@llm_misc = "spacy.KBFileLoader.v1"
# Path to knowledge base .yaml file.
path = ${paths.el_kb}

----------------------------------------

TITLE: Compiling Infix Regex for Tokenization in spaCy
DESCRIPTION: Shows how to compile infix rules into a regex object and assign it to a tokenizer. This function takes a sequence of infix patterns and returns a compiled regex that can be used for tokenizer.infix_finditer.

LANGUAGE: python
CODE:
infixes = ("…", "-", "—", r"(?<=[0-9])[+\-\*^](?=[0-9-])")
infix_regex = util.compile_infix_regex(infixes)
nlp.tokenizer.infix_finditer = infix_regex.finditer

----------------------------------------

TITLE: Configuring span_finder and spancat pipeline components in INI format
DESCRIPTION: Example configuration for training a pipeline with span_finder and spancat components. The span_finder component is added to annotating_components to allow the spancat component to train from its predictions.

LANGUAGE: ini
CODE:
[nlp]
pipeline = ["tok2vec","span_finder","spancat"]

[training]
annotating_components = ["tok2vec","span_finder"]

----------------------------------------

TITLE: Saving spaCy Lemmatizer to Disk
DESCRIPTION: Demonstrates how to initialize a lemmatizer component and save it to disk. This method serializes the lemmatizer model to a specified directory path.

LANGUAGE: python
CODE:
lemmatizer = nlp.add_pipe("lemmatizer")
lemmatizer.to_disk("/path/to/lemmatizer")

----------------------------------------

TITLE: Loading Probability Table into a Provided spaCy Model
DESCRIPTION: Shows how to properly load the probability table into a pre-trained model by first removing the empty placeholder table and then accessing a lexeme's probability to trigger loading the full table from spacy-lookups-data.

LANGUAGE: diff
CODE:
+ # prerequisite: pip install spacy-lookups-data
import spacy

nlp = spacy.load("en_core_web_md")

# remove the empty placeholder prob table
+ if nlp.vocab.lookups_extra.has_table("lexeme_prob"):
+     nlp.vocab.lookups_extra.remove_table("lexeme_prob")

# access any `.prob` to load the full table into the model
assert nlp.vocab["a"].prob == -3.9297883511

# if desired, save this model with the probability table included
nlp.to_disk("/path/to/model")

----------------------------------------

TITLE: Loading and Saving Extra Probability Lookup Tables in spaCy
DESCRIPTION: Demonstrates how to load the lexeme probability table in an English model and save it to disk. The probability table is lazily loaded when accessed and included when the model is saved.

LANGUAGE: python
CODE:
from spacy.lang.en import English
nlp = English()
doc = nlp("the")
print(doc[0].prob) # lazily loads extra prob table
nlp.to_disk("/path/to/model") # includes prob table

----------------------------------------

TITLE: Setting up Virtual Environment for spaCy Installation
DESCRIPTION: Commands to create and activate a Python virtual environment before installing spaCy, which is the recommended approach to avoid modifying system state.

LANGUAGE: bash
CODE:
python -m venv .env
source .env/bin/activate
pip install -U pip setuptools wheel
pip install spacy

----------------------------------------

TITLE: Creating Spans from Matcher Results in spaCy
DESCRIPTION: Demonstrates two methods for creating Span objects from matcher results: manually converting match tuples to spans, and using the as_spans=True parameter to directly get spans with the match_id as the label.

LANGUAGE: python
CODE:
import spacy
from spacy.matcher import Matcher
from spacy.tokens import Span

nlp = spacy.blank("en")
matcher = Matcher(nlp.vocab)
matcher.add("PERSON", [[{"lower": "barack"}, {"lower": "obama"}]])
doc = nlp("Barack Obama was the 44th president of the United States")

# 1. Return (match_id, start, end) tuples
matches = matcher(doc)
for match_id, start, end in matches:
    # Create the matched span and assign the match_id as a label
    span = Span(doc, start, end, label=match_id)
    print(span.text, span.label_)

# 2. Return Span objects directly
matches = matcher(doc, as_spans=True)
for span in matches:
    print(span.text, span.label_)

----------------------------------------

TITLE: Loading Knowledge Base from Disk in spaCy InMemoryLookupKB
DESCRIPTION: Demonstrates how to restore a knowledge base from a directory on disk. The example includes initializing the vocabulary and knowledge base before loading from disk. Returns the modified KnowledgeBase object.

LANGUAGE: python
CODE:
from spacy.vocab import Vocab
vocab = Vocab().from_disk("/path/to/vocab")
kb = InMemoryLookupKB(vocab=vocab, entity_vector_length=64)
kb.from_disk("/path/to/kb")

----------------------------------------

TITLE: Configuring ConsoleLogger v1 in spaCy
DESCRIPTION: Configuration for spaCy's ConsoleLogger v1, which outputs training progress information to the console in a tabular format. The progress_bar parameter controls whether a progress bar is displayed.

LANGUAGE: ini
CODE:
[training.logger]
@loggers = "spacy.ConsoleLogger.v1"
progress_bar = true

----------------------------------------

TITLE: Serializing a Table to Bytes
DESCRIPTION: Serializes a Table object to a bytestring for storage or transmission.

LANGUAGE: python
CODE:
table_bytes = table.to_bytes()

----------------------------------------

TITLE: Accessing Token Conjuncts in spaCy
DESCRIPTION: Example showing how to access coordinated tokens using the conjuncts property. It returns a tuple of tokens that are coordinated with the current token.

LANGUAGE: python
CODE:
doc = nlp("I like apples and oranges")
apples_conjuncts = doc[2].conjuncts
assert [t.text for t in apples_conjuncts] == ["oranges"]

----------------------------------------

TITLE: Getting File Path of an Installed Package
DESCRIPTION: Resolves the file system location of an installed package by importing it, primarily used for finding pipeline package directories.

LANGUAGE: python
CODE:
util.get_package_path("en_core_web_sm")
# /usr/lib/python3.6/site-packages/en_core_web_sm

----------------------------------------

TITLE: Optimizing Batch Size with Multiprocessing in spaCy
DESCRIPTION: Example of customizing batch size when using multiprocessing to optimize performance. This is particularly useful for shorter tasks and systems using the 'spawn' multiprocessing start method.

LANGUAGE: python
CODE:
# Default batch size is `nlp.batch_size` (typically 1000)
docs = nlp.pipe(texts, n_process=2, batch_size=2000)

----------------------------------------

TITLE: Complete MDX Document Structure
DESCRIPTION: Demonstrates the complete structure of an MDX document for spaCy documentation. Includes frontmatter, section headlines with IDs, links, asides, tables, code blocks with titles and highlighting, and infoboxes.

LANGUAGE: markdown
CODE:
---
title: Page title
---

## Headline starting a section {id="some_id"}

This is a regular paragraph with a [link](https://spacy.io) and **bold text**.

> #### This is an aside title
>
> This is aside text.

### Subheadline

| Header 1 | Header 2 |
| -------- | -------- |
| Column 1 | Column 2 |

```python {title="Code block title",highlight="2-3"}
import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("Hello world")
```

<Infobox title="Important note" variant="warning">

This is content in the infobox.

</Infobox>

----------------------------------------

TITLE: Checking for Document Vector in spaCy (Python)
DESCRIPTION: Shows how to check if a document has an associated word vector using the has_vector property.

LANGUAGE: python
CODE:
doc = nlp("I like apples")
assert doc.has_vector

----------------------------------------

TITLE: Debugging Tokenization with explain Method
DESCRIPTION: Example using the explain method to debug tokenization by showing which rules or patterns matched for each token in the input string.

LANGUAGE: python
CODE:
tok_exp = nlp.tokenizer.explain("(don't)")
assert [t[0] for t in tok_exp] == ["PREFIX", "SPECIAL-1", "SPECIAL-2", "SUFFIX"]
assert [t[1] for t in tok_exp] == ["(", "do", "n't", ")"]

----------------------------------------

TITLE: Creating a copy of a SpanGroup
DESCRIPTION: Demonstrates how to create a copy of an existing SpanGroup using the copy method.

LANGUAGE: python
CODE:
from spacy.tokens import SpanGroup

doc = nlp("Their goi ng home")
doc.spans["errors"] = [doc[1:3], doc[0:3]]
new_group = doc.spans["errors"].copy()

----------------------------------------

TITLE: Checking for pattern existence in PhraseMatcher (Python)
DESCRIPTION: Demonstrates how to check if a specific match ID exists in the PhraseMatcher. This allows confirming whether patterns for a particular concept have been added.

LANGUAGE: python
CODE:
matcher = PhraseMatcher(nlp.vocab)
assert "OBAMA" not in matcher
matcher.add("OBAMA", [nlp("Barack Obama")])
assert "OBAMA" in matcher

----------------------------------------

TITLE: Configuring ConsoleLogger.v3 in spaCy Config
DESCRIPTION: Example configuration for the v3 ConsoleLogger in spaCy that provides improved progress tracking options. It displays an evaluation progress bar, console output, and saves logs to a JSONL file.

LANGUAGE: ini
CODE:
[training.logger]
@loggers = "spacy.ConsoleLogger.v3"
progress_bar = "eval"
console_output = true
output_file = "training_log.jsonl"

----------------------------------------

TITLE: Migrating from Command Line Arguments to Config File for Training
DESCRIPTION: Shows the difference between the old v2.x command line approach with many arguments and the new v3.0 config-based approach for training a spaCy pipeline.

LANGUAGE: diff
CODE:
- python -m spacy train en ./output ./train.json ./dev.json
--pipeline tagger,parser --cnn-window 1 --bilstm-depth 0
+ python -m spacy train ./config.cfg --output ./output

----------------------------------------

TITLE: Running Commands and Workflows in spaCy Projects using Bash
DESCRIPTION: Command for executing named commands or workflows defined in the project.yml file. Commands with dependencies will only be re-run if their inputs have changed.

LANGUAGE: bash
CODE:
$ python -m spacy project run [subcommand] [project_dir] [--force] [--dry]

LANGUAGE: bash
CODE:
$ python -m spacy project run train

----------------------------------------

TITLE: Excluding Fields During Transformer Serialization in Python
DESCRIPTION: Example showing how to exclude specific serialization fields when saving a spaCy Transformer component to disk, which can be useful to reduce file size or for partial updates.

LANGUAGE: python
CODE:
data = trf.to_disk("/path", exclude=["vocab"])

----------------------------------------

TITLE: Implementing spaCy Streamlit Visualization Script
DESCRIPTION: Python script for creating a Streamlit-based visualization interface for spaCy pipelines, using the spacy-streamlit package to display interactive visualizations of model outputs.

LANGUAGE: python
CODE:
https://github.com/explosion/projects/blob/v3/integrations/streamlit/scripts/visualize.py

----------------------------------------

TITLE: Implementing spaCy Streamlit Visualization Script
DESCRIPTION: Python script for creating a Streamlit-based visualization interface for spaCy pipelines, using the spacy-streamlit package to display interactive visualizations of model outputs.

LANGUAGE: python
CODE:
https://github.com/explosion/projects/blob/v3/integrations/streamlit/scripts/visualize.py

----------------------------------------

TITLE: Loading Lookups from Disk
DESCRIPTION: Loads a Lookups object from a directory containing a lookups.bin file. Skips loading if the file doesn't exist.

LANGUAGE: python
CODE:
from spacy.lookups import Lookups
lookups = Lookups()
lookups.from_disk("/path/to/lookups")

----------------------------------------

TITLE: Using Temporary Parameters with EntityLinker in spaCy (Python)
DESCRIPTION: Temporarily modifies the EntityLinker model to use given parameter values within a context manager. After exiting the context, the original parameters are restored.

LANGUAGE: python
CODE:
entity_linker = nlp.add_pipe("entity_linker")
with entity_linker.use_params(optimizer.averages):
    entity_linker.to_disk("/best_model")

----------------------------------------

TITLE: Setting lexeme attribute values in spaCy
DESCRIPTION: Demonstrates how to set a lexeme attribute value using the set_struct_attr static method. This example sets the NORM attribute to be the same as the lowercase form.

LANGUAGE: python
CODE:
from spacy.attrs cimport NORM
from spacy.lexeme cimport Lexeme

lexeme = doc.c[3].lex
Lexeme.set_struct_attr(lexeme, NORM, lexeme.lower)

----------------------------------------

TITLE: Loading Pipeline Config from File
DESCRIPTION: Loads a pipeline's config.cfg from a file path, which contains component details and training settings. Supports config overrides and variable interpolation.

LANGUAGE: python
CODE:
config = util.load_config("/path/to/config.cfg")
print(config.to_str())

----------------------------------------

TITLE: Excluding Fields during spaCy Lemmatizer Serialization
DESCRIPTION: Demonstrates how to exclude specific fields when serializing a lemmatizer. This example shows excluding the vocabulary during serialization to disk.

LANGUAGE: python
CODE:
data = lemmatizer.to_disk("/path", exclude=["vocab"])

----------------------------------------

TITLE: Temporarily Disabling Pipeline Components with select_pipes
DESCRIPTION: Shows how to use the select_pipes context manager to temporarily disable specific components for a processing block.

LANGUAGE: python
CODE:
# 1. Use as a context manager
with nlp.select_pipes(disable=["tagger", "parser", "lemmatizer"]):
    doc = nlp("I won't be tagged and parsed")
doc = nlp("I will be tagged and parsed")

# 2. Restore manually
disabled = nlp.select_pipes(disable="ner")
doc = nlp("I won't have named entities")
disabled.restore()

----------------------------------------

TITLE: Creating optimizer for EditTreeLemmatizer in spaCy
DESCRIPTION: Initialize an optimizer for the lemmatizer component. This method creates and returns an optimizer that can be used for training the model.

LANGUAGE: python
CODE:
lemmatizer = nlp.add_pipe("trainable_lemmatizer", name="lemmatizer")
optimizer = lemmatizer.create_optimizer()

----------------------------------------

TITLE: Serializing EntityRuler to Bytes in spaCy Python
DESCRIPTION: Shows how to serialize the EntityRuler component to a bytestring, which is useful for storing or transmitting the patterns programmatically.

LANGUAGE: python
CODE:
ruler = nlp.add_pipe("entity_ruler")
ruler_bytes = ruler.to_bytes()

----------------------------------------

TITLE: Accessing Token Vector Norm in spaCy
DESCRIPTION: Example showing how to get the L2 norm of a token's vector representation using the vector_norm property. This returns the magnitude of the token's vector.

LANGUAGE: python
CODE:
doc = nlp("I like apples and pasta")
apples = doc[2]
pasta = doc[4]
apples.vector_norm  # 6.89589786529541
pasta.vector_norm  # 7.759851932525635
assert apples.vector_norm != pasta.vector_norm

----------------------------------------

TITLE: Referencing FastAPI Integration Script
DESCRIPTION: URL reference to a FastAPI integration script that implements a REST API with POST endpoints for batch text processing with spaCy.

LANGUAGE: python
CODE:
https://github.com/explosion/projects/blob/v3/integrations/fastapi/scripts/main.py

----------------------------------------

TITLE: Referencing FastAPI Integration Script
DESCRIPTION: URL reference to a FastAPI integration script that implements a REST API with POST endpoints for batch text processing with spaCy.

LANGUAGE: python
CODE:
https://github.com/explosion/projects/blob/v3/integrations/fastapi/scripts/main.py

----------------------------------------

TITLE: Setting Up Chinese Language Segmentation Options in spaCy
DESCRIPTION: Manual configuration options for the Chinese language class with three word segmentation options: character segmentation (default), Jieba, and PKUSeg with custom models.

LANGUAGE: python
CODE:
from spacy.lang.zh import Chinese

# Character segmentation (default)
nlp = Chinese()
# Jieba
cfg = {"segmenter": "jieba"}
nlp = Chinese.from_config({"nlp": {"tokenizer": cfg}})
# PKUSeg with "mixed" model provided by pkuseg
cfg = {"segmenter": "pkuseg"}
nlp = Chinese.from_config({"nlp": {"tokenizer": cfg}})
nlp.tokenizer.initialize(pkuseg_model="mixed")

----------------------------------------

TITLE: Checking for Word Vectors in spaCy Vocabulary
DESCRIPTION: Checks if a word has a vector representation in the vocabulary. Returns False if no vectors are loaded. This is commonly used before attempting to retrieve a vector to avoid errors.

LANGUAGE: python
CODE:
if nlp.vocab.has_vector("apple"):
    vector = nlp.vocab.get_vector("apple")

----------------------------------------

TITLE: Calculating Vector Table Size in Python
DESCRIPTION: Property that returns the total size of the vector table (rows * dimensions).

LANGUAGE: python
CODE:
@property
def size(self):

----------------------------------------

TITLE: Configuring File Reader for Training Augmentation in spaCy
DESCRIPTION: Example configuration for using the srsly JSON reader to load orthographic variants for training data augmentation. The reader loads JSON data from a specified path.

LANGUAGE: ini
CODE:
[corpora.train.augmenter.orth_variants]
@readers = "srsly.read_json.v1"
path = "corpus/en_orth_variants.json"

----------------------------------------

TITLE: Rendering Entity Data from Raw Dictionary in Python
DESCRIPTION: This snippet shows how to directly provide raw entity data in displaCy's format without using a spaCy Doc object. It creates a list containing a dictionary with text, entities, and title fields.

LANGUAGE: python
CODE:
ex = [{"text": "But Google is starting from behind.",
       "ents": [{"start": 4, "end": 10, "label": "ORG"}],
       "title": None}]
html = displacy.render(ex, style="ent", manual=True)

----------------------------------------

TITLE: Initializing SpanResolver for Training in Python
DESCRIPTION: Example of initializing the SpanResolver component for training using examples. This validates the network, infers missing shapes, and sets up the label scheme based on the provided data.

LANGUAGE: python
CODE:
span_resolver = nlp.add_pipe("experimental_span_resolver")
span_resolver.initialize(lambda: examples, nlp=nlp)

----------------------------------------

TITLE: Using the 'benchmark speed' CLI Command in spaCy v3.5
DESCRIPTION: The 'benchmark speed' CLI measures processing speed in words per second on batches of documents, performing warmup rounds before measurement for more accurate results.

LANGUAGE: bash
CODE:
$ spacy benchmark speed my_pipeline data.spacy

----------------------------------------

TITLE: Accessing Token Subtree in spaCy
DESCRIPTION: Example showing how to access a token's syntactic descendants using the subtree property. It returns a sequence containing the token itself and all tokens that are descendants of the token.

LANGUAGE: python
CODE:
doc = nlp("Give it back! He pleaded.")
give_subtree = doc[0].subtree
assert [t.text for t in give_subtree] == ["Give", "it", "back", "!"]

----------------------------------------

TITLE: BibTeX Citation for Croatian Lemmas Source
DESCRIPTION: Academic citation in BibTeX format for the paper describing the Croatian lemma lexicons used in the project. The citation references a 2016 conference paper by Ljubešić et al. about inflectional lexicons for Croatian and Serbian.

LANGUAGE: bibtex
CODE:
@InProceedings{ljubesic16-new,
  author = {Nikola Ljubešić and Filip Klubička and Željko Agić and Ivo-Pavao Jazbec},
  title = {New Inflectional Lexicons and Training Corpora for Improved Morphosyntactic Annotation of Croatian and Serbian},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)},
  year = {2016},
  date = {23-28},
  location = {Portorož, Slovenia},
  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Sara Goggi and Marko Grobelnik and Bente Maegaard and Joseph Mariani and Helene Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis},
  publisher = {European Language Resources Association (ELRA)},
  address = {Paris, France},
  isbn = {978-2-9517408-9-1}
 }

----------------------------------------

TITLE: Creating an Optimizer for Morphologizer in spaCy
DESCRIPTION: Demonstrates how to create an optimizer for the morphologizer component. The optimizer is used during model training to update weights.

LANGUAGE: python
CODE:
morphologizer = nlp.add_pipe("morphologizer")
optimizer = morphologizer.create_optimizer()

----------------------------------------

TITLE: Visualizing Long Texts with displaCy Sentence by Sentence
DESCRIPTION: This code shows how to visualize longer texts by breaking them into sentences. It processes a multi-sentence text about ancient Rome and passes the individual sentence spans to displaCy for clearer visualization.

LANGUAGE: python
CODE:
import spacy
from spacy import displacy

nlp = spacy.load("en_core_web_sm")
text = """In ancient Rome, some neighbors live in three adjacent houses. In the center is the house of Senex, who lives there with wife Domina, son Hero, and several slaves, including head slave Hysterium and the musical's main character Pseudolus. A slave belonging to Hero, Pseudolus wishes to buy, win, or steal his freedom. One of the neighboring houses is owned by Marcus Lycus, who is a buyer and seller of beautiful women; the other belongs to the ancient Erronius, who is abroad searching for his long-lost children (stolen in infancy by pirates). One day, Senex and Domina go on a trip and leave Pseudolus in charge of Hero. Hero confides in Pseudolus that he is in love with the lovely Philia, one of the courtesans in the House of Lycus (albeit still a virgin)."""
doc = nlp(text)
sentence_spans = list(doc.sents)
displacy.serve(sentence_spans, style="dep")

----------------------------------------

TITLE: Accessing Pointers from Different Data Structures in Cython
DESCRIPTION: Demonstrates how to get pointers from numpy arrays, C++ vectors, and memory views in Cython. This is useful for optimizing performance as pointers have explicit semantics and allow stricter compiler checks.

LANGUAGE: python
CODE:
cdef void get_pointers(np.ndarray[int, mode='c'] numpy_array, vector[int] cpp_vector, int[::1] memory_view) nogil:
pointer1 = <int*>numpy_array.data
pointer2 = cpp_vector.data()
pointer3 = &memory_view[0]

----------------------------------------

TITLE: Configuring Case Lowering Augmentation in spaCy
DESCRIPTION: Configuration for the lower_case data augmenter which converts text to lowercase. This augmentation helps make the model less sensitive to capitalization by exposing it to lowercase variants during training.

LANGUAGE: ini
CODE:
[corpora.train.augmenter]
@augmenters = "spacy.lower_case.v1"
level = 0.3

----------------------------------------

TITLE: Initializing AttributeRuler with Custom Patterns in spaCy
DESCRIPTION: Example demonstrating how to initialize the AttributeRuler component with custom patterns, both programmatically and through a configuration file.

LANGUAGE: python
CODE:
ruler = nlp.add_pipe("attribute_ruler")
ruler.initialize(lambda: [], nlp=nlp, patterns=patterns)

----------------------------------------

TITLE: Implementing Initialize Method for Component in spaCy
DESCRIPTION: This code demonstrates implementing an initialize method for a component, which is called by nlp.initialize before training. This allows loading data only during initialization rather than each time the component is created.

LANGUAGE: python
CODE:
class AcronymComponent:
    def __init__(self):
        self.data = {}

    def initialize(self, get_examples=None, nlp=None, data={}):
        self.data = data

----------------------------------------

TITLE: Using Explicit Type Hints in Python Function Definitions
DESCRIPTION: Example showing how to improve type annotations by using more specific types from the typing module (e.g., Dict[str, Any]) instead of generic types (e.g., dict).

LANGUAGE: diff
CODE:
- def func(some_arg: dict) -> None:
+ def func(some_arg: Dict[str, Any]) -> None:
    ...

----------------------------------------

TITLE: Configuring TextCat v3 Task in spaCy with Label Definitions
DESCRIPTION: Configuration for TextCat v3 task that labels documents with categories. This version adds support for label definitions that are included in the prompt to the LLM, helping to guide classification.

LANGUAGE: ini
CODE:
[components.llm.task]
@llm_tasks = "spacy.TextCat.v3"
labels = ["COMPLIMENT", "INSULT"]

[components.llm.task.label_definitions]
"COMPLIMENT" = "a polite expression of praise or admiration.",
"INSULT" = "a disrespectful or scornfully abusive remark or act."
examples = null

----------------------------------------

TITLE: Using SentenceRecognizer.pipe Method in spaCy
DESCRIPTION: Example showing how to process a stream of documents with the SentenceRecognizer using the pipe method. This demonstrates batch processing with a specified batch size.

LANGUAGE: python
CODE:
senter = nlp.add_pipe("senter")
for doc in senter.pipe(docs, batch_size=50):
    pass

----------------------------------------

TITLE: Using the Predict Method of a TrainablePipe in Python
DESCRIPTION: Example demonstrating how to use the predict method to apply a component's model to a batch of documents without modifying them. This method must be overridden in custom components.

LANGUAGE: python
CODE:
pipe = nlp.add_pipe("your_custom_pipe")
scores = pipe.predict([doc1, doc2])

----------------------------------------

TITLE: Configuring Chinese Tokenization with pkuseg
DESCRIPTION: Example showing how to load the Chinese language model with the 'pkuseg' tokenizer and customize the user dictionary for improved word segmentation in Chinese text processing.

LANGUAGE: python
CODE:
from spacy.lang.zh import Chinese

# Load with "default" model provided by pkuseg
cfg = {"pkuseg_model": "default", "require_pkuseg": True}
nlp = Chinese(meta={"tokenizer": {"config": cfg}})

# Append words to user dict
nlp.tokenizer.pkuseg_update_user_dict(["中国", "ABC"])

----------------------------------------

TITLE: Configuring Word-Based Batch Generation in spaCy
DESCRIPTION: Configuration for a batching strategy that creates minibatches with roughly a given number of words. Includes options for batch size tolerance and handling oversized examples during training.

LANGUAGE: ini
CODE:
[training.batcher]
@batchers = "spacy.batch_by_words.v1"
size = 100
tolerance = 0.2
discard_oversize = false
get_length = null

----------------------------------------

TITLE: Serializing SpanResolver to disk in spaCy
DESCRIPTION: Demonstrates how to save the SpanResolver component to disk using the to_disk method, which serializes the model data to a specified directory path.

LANGUAGE: python
CODE:
span_resolver = nlp.add_pipe("experimental_span_resolver")
span_resolver.to_disk("/path/to/span_resolver")

----------------------------------------

TITLE: Deserializing StringStore from Bytes in Python
DESCRIPTION: Demonstrates how to load a StringStore's state from a binary string.

LANGUAGE: python
CODE:
from spacy.strings import StringStore
store_bytes = stringstore.to_bytes()
new_store = StringStore().from_bytes(store_bytes)

----------------------------------------

TITLE: Loading TextCategorizer from Bytes in Python
DESCRIPTION: Shows how to load a TextCategorizer component from a bytestring. The component is serialized, added to the pipeline, and then loaded from the bytes data.

LANGUAGE: python
CODE:
textcat_bytes = textcat.to_bytes()
textcat = nlp.add_pipe("textcat")
textcat.from_bytes(textcat_bytes)

----------------------------------------

TITLE: Manually Constructing a Doc Object for Testing
DESCRIPTION: Example demonstrating how to create a spaCy Doc object with predefined annotations without loading models, useful for testing token API functionality.

LANGUAGE: python
CODE:
def test_doc_token_api_strings(en_vocab):
    words = ["Give", "it", "back", "!", "He", "pleaded", "."]
    pos = ['VERB', 'PRON', 'PART', 'PUNCT', 'PRON', 'VERB', 'PUNCT']
    heads = [0, 0, 0, 0, 5, 5, 5]
    deps = ['ROOT', 'dobj', 'prt', 'punct', 'nsubj', 'ROOT', 'punct']

    doc = Doc(en_vocab, words=words, pos=pos, heads=heads, deps=deps)
    assert doc[0].text == 'Give'
    assert doc[0].lower_ == 'give'
    assert doc[0].pos_ == 'VERB'
    assert doc[0].dep_ == 'ROOT'

----------------------------------------

TITLE: Model Configuration in INI Format
DESCRIPTION: Configuration excerpt for the relation model showing the architecture reference and placeholders for instance tensor creation and classification layer components.

LANGUAGE: ini
CODE:
[model]
@architectures = "rel_model.v1"

[model.create_instance_tensor]
# ...

[model.classification_layer]
# ...

----------------------------------------

TITLE: Updating spaCy Configuration from v3.1 to v3.2
DESCRIPTION: Command line instruction to update a spaCy v3.1 configuration file to include the new v3.2 settings using the init fill-config command. This ensures compatibility with new features.

LANGUAGE: bash
CODE:
$ python -m spacy init fill-config config-v3.1.cfg config-v3.2.cfg

----------------------------------------

TITLE: Rehearsing a Tagger Model in spaCy (Experimental)
DESCRIPTION: Demonstrates how to perform a "rehearsal" update to help address catastrophic forgetting when updating a model with new data.

LANGUAGE: python
CODE:
tagger = nlp.add_pipe("tagger")
optimizer = nlp.resume_training()
losses = tagger.rehearse(examples, sgd=optimizer)

----------------------------------------

TITLE: Requiring CPU for spaCy
DESCRIPTION: Forces spaCy to use CPU for data allocation and operations. This function should be called immediately after importing spaCy and before loading any pipelines.

LANGUAGE: python
CODE:
import spacy
spacy.require_cpu()
nlp = spacy.load("en_core_web_sm")

----------------------------------------

TITLE: Accessing Labels from DependencyParser in spaCy
DESCRIPTION: Demonstrates how to add a custom label to the parser and check if it exists in the parser's label set.

LANGUAGE: python
CODE:
parser.add_label("MY_LABEL")
assert "MY_LABEL" in parser.labels

----------------------------------------

TITLE: Basic Model Configuration in spaCy Pipeline
DESCRIPTION: A simple configuration for a textcat model component that references a model defined in a components.textcat.model block.

LANGUAGE: python
CODE:
# This references the [components.textcat.model] block above
model = ${components.textcat.model}
labels = []

----------------------------------------

TITLE: Using Thinc Custom Types for Model Architecture Definitions
DESCRIPTION: Example of using Thinc's custom types to annotate neural network model inputs and outputs, making the code more readable and specific about expected array types.

LANGUAGE: python
CODE:
def build_tagger_model(
    tok2vec: Model[List[Doc], List[Floats2d]], nO: Optional[int] = None
) -> Model[List[Doc], List[Floats2d]]:
    ...

----------------------------------------

TITLE: JSON Examples for TextCat Few-Shot Learning
DESCRIPTION: JSON structure for providing few-shot examples to the TextCat classifier. Each example contains a text field and an answer field indicating the correct classification.

LANGUAGE: json
CODE:
[
  {
    "text": "You look great!",
    "answer": "Compliment"
  },
  {
    "text": "You are not very clever at all.",
    "answer": "Insult"
  }
]

----------------------------------------

TITLE: Serializing DependencyParser to Bytes in spaCy
DESCRIPTION: Converts the parser component to a bytestring representation. This allows for binary serialization without writing to disk.

LANGUAGE: python
CODE:
parser = nlp.add_pipe("parser")
parser_bytes = parser.to_bytes()

----------------------------------------

TITLE: Serializing a Tagger to Bytes in spaCy
DESCRIPTION: Shows how to serialize a tagger component to a bytestring, which can be stored in memory or transmitted without file system access.

LANGUAGE: python
CODE:
tagger = nlp.add_pipe("tagger")
tagger_bytes = tagger.to_bytes()

----------------------------------------

TITLE: Combining Multiple Parameter Sets in spaCy Tests
DESCRIPTION: Example of using multiple parametrize decorators to test combinations of different parameters like text and punctuation.

LANGUAGE: python
CODE:
@pytest.mark.parametrize('text', ["A test sentence", "Another sentence"])
@pytest.mark.parametrize('punct', ['.', '!', '?'])

----------------------------------------

TITLE: Training spaCy Model with Custom Code
DESCRIPTION: Command to train a spaCy model using a configuration file and a custom code file. The --code argument allows spaCy to access custom functions defined in an external Python file.

LANGUAGE: bash
CODE:
python -m spacy train ./config.cfg --code ./code.py

----------------------------------------

TITLE: Expanding Regex Matches to Valid Token Sequences in spaCy
DESCRIPTION: Example demonstrating how to expand regex matches to the closest valid token boundaries when the matched substring doesn't align perfectly with token boundaries.

LANGUAGE: python
CODE:
chars_to_tokens = {}
for token in doc:
    for i in range(token.idx, token.idx + len(token.text)):
        chars_to_tokens[i] = token.i

LANGUAGE: python
CODE:
span = doc.char_span(start, end)
if span is not None:
    print("Found match:", span.text)
else:
    start_token = chars_to_tokens.get(start)
    end_token = chars_to_tokens.get(end)
    if start_token is not None and end_token is not None:
        span = doc[start_token:end_token + 1]
        print("Found closest match:", span.text)

----------------------------------------

TITLE: Installing spaCy with Lookups Data
DESCRIPTION: Command to install spaCy with the lookups option or install spacy-lookups-data separately to enable lemmatization rules for supported languages.

LANGUAGE: bash
CODE:
$ pip install -U %%SPACY_PKG_NAME[lookups]%%SPACY_PKG_FLAGS

----------------------------------------

TITLE: Installing spaCy from source
DESCRIPTION: Commands to clone the spaCy GitHub repository, set up a virtual environment, install dependencies, and compile spaCy from source for development purposes.

LANGUAGE: bash
CODE:
$ python -m pip install -U pip setuptools wheel # install/update build tools
$ git clone https://github.com/explosion/spaCy  # clone spaCy
$ cd spaCy                                      # navigate into dir
$ python -m venv .env                           # create environment in .env
$ source .env/bin/activate                      # activate virtual env
$ pip install -r requirements.txt               # install requirements
$ pip install --no-build-isolation --editable . # compile and install spaCy

----------------------------------------

TITLE: Managing Strings and Hash Values in spaCy
DESCRIPTION: Shows the changes to string handling in spaCy v2.0. The new version uses consistent hash values across vocabularies and requires explicit string addition via StringStore.add().

LANGUAGE: diff
CODE:
- nlp.vocab.strings["coffee"]       # 3672
- other_nlp.vocab.strings["coffee"] # 40259

+ nlp.vocab.strings.add("coffee")
+ nlp.vocab.strings["coffee"]       # 3197928453018144401
+ other_nlp.vocab.strings["coffee"] # 3197928453018144401

----------------------------------------

TITLE: Few-Shot Learning YAML Examples for Lemmatization
DESCRIPTION: YAML file format for providing few-shot examples to the Lemma LLM task. Each example contains a text and corresponding lemmas represented as key-value pairs.

LANGUAGE: yaml
CODE:
- text: I'm buying ice cream.
  lemmas:
    - 'I': 'I'
    - "'m": 'be'
    - 'buying': 'buy'
    - 'ice': 'ice'
    - 'cream': 'cream'
    - '.': '.'

- text: I've watered the plants.
  lemmas:
    - 'I': 'I'
    - "'ve": 'have'
    - 'watered': 'water'
    - 'the': 'the'
    - 'plants': 'plant'
    - '.': '.'

----------------------------------------

TITLE: Converting MorphAnalysis to String in Python
DESCRIPTION: Demonstrates the __str__ method that returns the morphological analysis in the Universal Dependencies FEATS string format.

LANGUAGE: python
CODE:
feats = "Feat1=Val1,Val2|Feat2=Val2"
morph = MorphAnalysis(nlp.vocab, feats)
assert str(morph) == feats

----------------------------------------

TITLE: Calculating Loss for a Tagger Model in spaCy
DESCRIPTION: Shows how to compute the loss and gradient of loss for a batch of documents with their predicted scores.

LANGUAGE: python
CODE:
tagger = nlp.add_pipe("tagger")
scores = tagger.predict([eg.predicted for eg in examples])
loss, d_loss = tagger.get_loss(examples, scores)

----------------------------------------

TITLE: Visualizing the trained model with Streamlit
DESCRIPTION: Command to launch a Streamlit visualization interface that demonstrates the outputs and capabilities of the trained NER model.

LANGUAGE: bash
CODE:
python -m spacy project run visualize-model

----------------------------------------

TITLE: Initializing SpanCategorizer Component in spaCy (Python)
DESCRIPTION: Demonstrates different ways to initialize a SpanCategorizer component in spaCy, including via add_pipe with default or custom models, and direct construction from the class.

LANGUAGE: python
CODE:
# Construction via add_pipe with default model
# Replace 'spancat' with 'spancat_singlelabel' for exclusive classes
spancat = nlp.add_pipe("spancat")

# Construction via add_pipe with custom model
config = {"model": {"@architectures": "my_spancat"}}
spancat = nlp.add_pipe("spancat", config=config)

# Construction from class
from spacy.pipeline import SpanCategorizer
spancat = SpanCategorizer(nlp.vocab, model, suggester)

----------------------------------------

TITLE: Exporting Tag Map from spaCy Model to JSON
DESCRIPTION: Shows how to export a tag map from a provided spaCy model to a JSON file by converting any internal integer IDs back to strings, making it suitable for use with the train CLI.

LANGUAGE: python
CODE:
import spacy
import srsly

nlp = spacy.load("en_core_web_sm")
tag_map = {}

# convert any integer IDs to strings for JSON
for tag, morph in nlp.vocab.morphology.tag_map.items():
    tag_map[tag] = {}
    for feat, val in morph.items():
        feat = nlp.vocab.strings.as_string(feat)
        if not isinstance(val, bool):
            val = nlp.vocab.strings.as_string(val)
        tag_map[tag][feat] = val

srsly.write_json("tag_map.json", tag_map)

----------------------------------------

TITLE: Calculating loss with EditTreeLemmatizer in spaCy
DESCRIPTION: Compute the loss and gradient for a batch of examples. This method is used during training to measure model performance and calculate gradients for optimization.

LANGUAGE: python
CODE:
lemmatizer = nlp.add_pipe("trainable_lemmatizer", name="lemmatizer")
scores = lemmatizer.model.begin_update([eg.predicted for eg in examples])
loss, d_loss = lemmatizer.get_loss(examples, scores)

----------------------------------------

TITLE: Using Parameter Values in TextCategorizer Model in Python
DESCRIPTION: Demonstrates using the use_params contextmanager to temporarily modify the TextCategorizer model's parameters, typically used for saving the best model during training.

LANGUAGE: python
CODE:
textcat = nlp.add_pipe("textcat")
with textcat.use_params(optimizer.averages):
    textcat.to_disk("/best_model")

----------------------------------------

TITLE: Initializing AttributeRuler in spaCy
DESCRIPTION: Example showing how to initialize the AttributeRuler component by adding it to the spaCy processing pipeline using the add_pipe method.

LANGUAGE: python
CODE:
# Construction via add_pipe
ruler = nlp.add_pipe("attribute_ruler")

----------------------------------------

TITLE: Customizing Transformer component annotations in spaCy
DESCRIPTION: Example showing how to customize the way the Transformer component sets annotations on Doc objects using a custom annotation setter function.

LANGUAGE: python
CODE:
def custom_annotation_setter(docs, trf_data):
    doc_data = list(trf_data.doc_data)
    for doc, data in zip(docs, doc_data):
        doc._.custom_attr = data

nlp = spacy.load("en_core_web_trf")
nlp.get_pipe("transformer").set_extra_annotations = custom_annotation_setter
doc = nlp("This is a text")
assert isinstance(doc._.custom_attr, TransformerData)
print(doc._.custom_attr.tensors)

----------------------------------------

TITLE: Defining Error Classes in spaCy
DESCRIPTION: Shows how to define error messages in spaCy's centralized error system. The example demonstrates creating error codes with static messages and messages that can be formatted with variables.

LANGUAGE: python
CODE:
class Errors:
    E123 = "Something went wrong"
    E456 = "Unexpected value: {value}"

----------------------------------------

TITLE: Configuring FileReader.v1 in spaCy
DESCRIPTION: Configuration example for the FileReader.v1 component in spaCy. This registers the file reader in the misc registry and specifies the path to a Jinja template file that will be used for prompts.

LANGUAGE: ini
CODE:
[components.llm.task.template]
@misc = "spacy.FileReader.v1"
path = "ner_template.jinja2"

----------------------------------------

TITLE: Configuring SpanCat v3 Task in spaCy LLM
DESCRIPTION: Example configuration for the SpanCat v3 task component that extracts overlapping entities. This configuration specifies entity labels including PERSON, ORGANISATION, and LOCATION.

LANGUAGE: ini
CODE:
[components.llm.task]
@llm_tasks = "spacy.SpanCat.v3"
labels = ["PERSON", "ORGANISATION", "LOCATION"]
examples = null

----------------------------------------

TITLE: Reconstructing Original Text from Tokenized Words in spaCy
DESCRIPTION: Demonstrates how to reconstruct original tokens from a list of words and the original text. Returns words and a list of boolean values indicating whether each word is followed by a space, which can be used to create a Doc object.

LANGUAGE: python
CODE:
orig_words = ["Hey", ",", "what", "'s", "up", "?"]
orig_text = "Hey, what's up?"
words, spaces = get_words_and_spaces(orig_words, orig_text)
# ['Hey', ',', 'what', "'s", 'up', '?']
# [False, True, False, True, False, False]

----------------------------------------

TITLE: Optimizing EntityRuler with Large Pattern Sets
DESCRIPTION: Technique for improving performance when adding a large number of phrase patterns to an EntityRuler by temporarily disabling other pipeline components during pattern addition.

LANGUAGE: python
CODE:
ruler = nlp.add_pipe("entity_ruler")
patterns = [{"label": "TEST", "pattern": str(i)} for i in range(100000)]
with nlp.select_pipes(enable="tagger"):
    ruler.add_patterns(patterns)

----------------------------------------

TITLE: Installing spaCy from source with extras
DESCRIPTION: Command to install spaCy from source with additional components like lookups and CUDA 10.2 support.

LANGUAGE: bash
CODE:
$ pip install --no-build-isolation --editable .[lookups,cuda102]

----------------------------------------

TITLE: Installing New Language Models in spaCy v2.3
DESCRIPTION: Commands to download and install the new language models for Danish, Japanese, Polish, Romanian, and Chinese in spaCy v2.3.

LANGUAGE: bash
CODE:
python -m spacy download da_core_news_sm
python -m spacy download ja_core_news_sm
python -m spacy download pl_core_news_sm
python -m spacy download ro_core_news_sm
python -m spacy download zh_core_web_sm

----------------------------------------

TITLE: Disabling Pipeline Components in spaCy
DESCRIPTION: Demonstrates how to disable specific pipeline components in spaCy v2.0. The new version uses the 'disable' parameter and context managers rather than boolean flags.

LANGUAGE: diff
CODE:
- nlp = spacy.load("en_core_web_sm", tagger=False, entity=False)
- doc = nlp("I don't want parsed", parse=False)

+ nlp = spacy.load("en_core_web_sm", disable=["tagger", "ner"])
+ with nlp.disable_pipes("parser"):
+    doc = nlp("I don't want parsed")

----------------------------------------

TITLE: Checking DocBin Length in Python
DESCRIPTION: Demonstrates how to check the number of Doc objects that have been added to a DocBin using the __len__ method.

LANGUAGE: python
CODE:
doc_bin = DocBin(attrs=["LEMMA"])
doc = nlp("This is a document to serialize.")
doc_bin.add(doc)
assert len(doc_bin) == 1

----------------------------------------

TITLE: Interactive spaCy Code Example
DESCRIPTION: An executable code block that demonstrates basic spaCy functionality. Users can run this code directly in the browser to see how spaCy processes text and outputs token information.

LANGUAGE: python
CODE:
import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("This is a sentence.")
for token in doc:
    print(token.text, token.pos_)

----------------------------------------

TITLE: Using Fuzzy Matching in spaCy Token Patterns
DESCRIPTION: Examples of fuzzy matching to match tokens with alternate spellings, typos, and variations without specifying every possible variant. Demonstrates basic and advanced fuzzy matching configurations.

LANGUAGE: python
CODE:
# Matches "favourite", "favorites", "gavorite", "theatre", "theatr", ...
pattern = [{"TEXT": {"FUZZY": "favorite"}},
           {"TEXT": {"FUZZY": "theater"}}]

----------------------------------------

TITLE: Updating sentence boundary detector component name in spaCy v2.1.x
DESCRIPTION: Shows how to update code to use the new name 'sentencizer' for the rule-based sentence boundary detector, as the previous name 'sbd' has been deprecated in v2.1.x.

LANGUAGE: diff
CODE:
- sentence_splitter = nlp.create_pipe("sbd")
+ sentence_splitter = nlp.create_pipe("sentencizer")

----------------------------------------

TITLE: Creating an Optimizer for EntityLinker in spaCy (Python)
DESCRIPTION: Creates an optimizer specifically for the EntityLinker component, which is used during the training process to update model parameters.

LANGUAGE: python
CODE:
entity_linker = nlp.add_pipe("entity_linker")
optimizer = entity_linker.create_optimizer()

----------------------------------------

TITLE: Configuring TextCategorizer Multilabel with Default Model
DESCRIPTION: Example code showing how to configure and add the 'textcat_multilabel' component to a spaCy pipeline with a threshold parameter and default multi textcat model.

LANGUAGE: python
CODE:
from spacy.pipeline.textcat_multilabel import DEFAULT_MULTI_TEXTCAT_MODEL
config = {
   "threshold": 0.5,
   "model": DEFAULT_MULTI_TEXTCAT_MODEL,
}
nlp.add_pipe("textcat_multilabel", config=config)

----------------------------------------

TITLE: Creating MorphAnalysis Objects from Hash IDs in spaCy
DESCRIPTION: This example demonstrates how to create a MorphAnalysis object from a features string hash ID. It first defines a features string, gets its hash from the vocabulary, then creates a MorphAnalysis object from that hash, and finally verifies the string representation matches the original features.

LANGUAGE: python
CODE:
feats = "Feat1=Val1|Feat2=Val2"
hash = nlp.vocab.strings[feats]
morph = MorphAnalysis.from_id(nlp.vocab, hash)
assert str(morph) == feats

----------------------------------------

TITLE: Registering a Custom Embedding Layer in spaCy
DESCRIPTION: Custom embedding function that uses the StaticVectors layer. This example demonstrates how to register a custom architecture that can be referenced in configuration files.

LANGUAGE: python
CODE:
from spacy.ml.staticvectors import StaticVectors
from spacy.util import registry

print("I was imported!")

@registry.architectures("my_example.MyEmbedding.v1")
def MyEmbedding(output_width: int) -> Model[List[Doc], List[Floats2d]]:
    print("I was called!")
    return StaticVectors(nO=output_width)

----------------------------------------

TITLE: Marking a test as expected to fail in spaCy tests
DESCRIPTION: Demonstrates how to mark a test that should pass but currently fails using the pytest.mark.xfail decorator with an explanation of why it fails.

LANGUAGE: python
CODE:
@pytest.mark.xfail(reason="Issue #225 - not yet implemented")
def test_en_tokenizer_splits_em_dash_infix(en_tokenizer):
    doc = en_tokenizer("Will this road take me to Puddleton?\u2014No.")
    assert doc[8].text == "\u2014"

----------------------------------------

TITLE: Configuring TextCatBOW v2 Architecture in spaCy
DESCRIPTION: Configuration for TextCatBOW v2, an n-gram bag-of-words model that introduced support for resizable text categorization, allowing labels to be added to a previously trained model.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.TextCatBOW.v2"
exclusive_classes = false
ngram_size = 1
no_output_layer = false
nO = null

----------------------------------------

TITLE: Installing Accelerate for CPU-Based Inference
DESCRIPTION: Command to install the accelerate library for CPU-based inference when a GPU is not available. This allows automatic device mapping but may result in slower performance.

LANGUAGE: shell
CODE:
python -m pip install "accelerate>=0.16.0,<1.0"

----------------------------------------

TITLE: Example usage of spaCy train command with path overrides
DESCRIPTION: Example showing how to run the train command with a config file and overriding the paths for training and development data.

LANGUAGE: bash
CODE:
$ python -m spacy train config.cfg --output ./output --paths.train ./train --paths.dev ./dev

----------------------------------------

TITLE: Example usage of spaCy train command with path overrides
DESCRIPTION: Example showing how to run the train command with a config file and overriding the paths for training and development data.

LANGUAGE: bash
CODE:
$ python -m spacy train config.cfg --output ./output --paths.train ./train --paths.dev ./dev

----------------------------------------

TITLE: Creating Ordered Lists in JSX
DESCRIPTION: Shows how to create ordered lists using JSX components. The example demonstrates a simple ordered list with two items using Ol and Li components.

LANGUAGE: markup
CODE:
<Ol>
    <Li>One</Li>
    <Li>Two</Li>
</Ol>

----------------------------------------

TITLE: Converting JSON to spaCy DocBin Format for spaCy v3
DESCRIPTION: Command to convert an existing spaCy v2 JSON training file to .spacy (DocBin) format for use with spaCy v3.

LANGUAGE: bash
CODE:
python -m spacy convert file.json .

----------------------------------------

TITLE: Adding Keys to Vector Table in Python
DESCRIPTION: Method to add a key to the vector table if possible, returning -1 if the operation is not supported.

LANGUAGE: python
CODE:
def add(self, key):

----------------------------------------

TITLE: Binary Classification Examples for TextCat
DESCRIPTION: JSON structure for binary classification examples, where POS means the example should be assigned the class label and NEG means it shouldn't, useful for tasks like spam detection.

LANGUAGE: json
CODE:
[
  {
    "text": "You won the lottery! Wire a fee of 200$ to be able to withdraw your winnings.",
    "answer": "POS"
  },
  {
    "text": "Your order #123456789 has arrived",
    "answer": "NEG"
  }
]

----------------------------------------

TITLE: Building a spaCy Executable with make
DESCRIPTION: Command to clone the spaCy repository and build an executable PEX file using make. This creates a standalone Python executable that includes spaCy and all its dependencies.

LANGUAGE: bash
CODE:
$ git clone https://github.com/explosion/spaCy
$ cd spaCy
$ make

----------------------------------------

TITLE: Using PhraseMatcher with Different Token Attributes
DESCRIPTION: Shows how the PhraseMatcher can now match on different token attributes by setting the 'attr' parameter. This allows matching on LOWER for case-insensitive matches or POS for matching sequences with the same part-of-speech tags.

LANGUAGE: python
CODE:
matcher = PhraseMatcher(nlp.vocab, attr="POS")
matcher.add("PATTERN", None, nlp("I love cats"))
doc = nlp("You like dogs")
matches = matcher(doc)

----------------------------------------

TITLE: Defining NumPy Version Constraints for spaCy Wheel Building
DESCRIPTION: Specifies the allowed version range for NumPy dependency when building spaCy wheels with wheelwright. Constrains NumPy to versions 2.0.0 or higher, but less than 3.0.0, ensuring compatibility with spaCy while allowing for updates within the major version.

LANGUAGE: plaintext
CODE:
numpy>=2.0.0,<3.0.0

----------------------------------------

TITLE: Updating Language Import Statements in spaCy
DESCRIPTION: Shows how to modify import statements for language classes in spaCy v2.0. Language classes are now imported from spacy.lang instead of directly from spacy.

LANGUAGE: diff
CODE:
- from spacy.en import English

+ from spacy.lang.en import English

----------------------------------------

TITLE: Serializing SpanFinder to Disk in spaCy
DESCRIPTION: Save the SpanFinder component to disk at a specified path.

LANGUAGE: python
CODE:
span_finder = nlp.add_pipe("span_finder")
span_finder.to_disk("/path/to/span_finder")

----------------------------------------

TITLE: Serializing SpanFinder to Disk in spaCy
DESCRIPTION: Save the SpanFinder component to disk at a specified path.

LANGUAGE: python
CODE:
span_finder = nlp.add_pipe("span_finder")
span_finder.to_disk("/path/to/span_finder")

----------------------------------------

TITLE: Parametrizing tests with multiple arguments in spaCy
DESCRIPTION: Shows how to use pytest's parametrize decorator with multiple arguments, passing both input text and expected token count to test a tokenizer's behavior.

LANGUAGE: python
CODE:
@pytest.mark.parametrize("text,expected_len", [("hello world", 2), ("I can't!", 4)])
def test_token_length(en_tokenizer, text, expected_len):  # en_tokenizer is a fixture
    doc = en_tokenizer(text)
    assert len(doc) == expected_len

----------------------------------------

TITLE: Iterating Over a C Array in Cython
DESCRIPTION: Shows the preferred method for iterating over a C array in Cython using slice notation to specify length. This pattern ensures compiler type inference while maintaining Python-like syntax with C-level performance.

LANGUAGE: cython
CODE:
cdef int c_total(const int* int_array, int length) nogil:
    total = 0
    for item in int_array[:length]:
        total += item
    return total

----------------------------------------

TITLE: Using Example.from_dict for Creating Reference Documents in spaCy (Python)
DESCRIPTION: A simplified approach to create Example objects with gold-standard part-of-speech tags using Example.from_dict. This method takes a predicted Doc object and a dictionary of annotations to generate the training example.

LANGUAGE: python
CODE:
words = ["I", "like", "stuff"]
tags = ["NOUN", "VERB", "NOUN"]
predicted = Doc(nlp.vocab, words=words)
example = Example.from_dict(predicted, {"tags": tags})

----------------------------------------

TITLE: Excluding Fields during Tokenizer Serialization
DESCRIPTION: Demonstrates how to exclude specific fields when serializing a spaCy Tokenizer. This is useful for customizing what gets saved or loaded.

LANGUAGE: python
CODE:
data = tokenizer.to_bytes(exclude=["vocab", "exceptions"])
tokenizer.from_disk("./data", exclude=["token_match"])

----------------------------------------

TITLE: Hashing Strings with hash_string Utility in Python
DESCRIPTION: Shows how to use the hash_string utility function to get a 64-bit hash for a given string.

LANGUAGE: python
CODE:
from spacy.strings import hash_string
assert hash_string("apple") == 8566208034543834098

----------------------------------------

TITLE: Checking if Table Exists in Lookups
DESCRIPTION: Checks if a table with a given name exists in the Lookups object using the __contains__ method.

LANGUAGE: python
CODE:
lookups = Lookups()
lookups.add_table("some_table")
assert "some_table" in lookups

----------------------------------------

TITLE: Setting GPU Memory Allocator for Deep Learning Frameworks
DESCRIPTION: Configuration setting to specify the GPU memory allocator for frameworks like PyTorch or TensorFlow. This helps prevent out-of-memory errors when using multiple deep learning frameworks together.

LANGUAGE: ini
CODE:
[training]
gpu_allocator = "pytorch"

----------------------------------------

TITLE: Cloning a spaCy NER demo project
DESCRIPTION: Command to clone the pipelines/ner_demo project from the spaCy projects repository to your local environment.

LANGUAGE: bash
CODE:
python -m spacy project clone pipelines/ner_demo

----------------------------------------

TITLE: Processing a stream of documents with the Lemmatizer
DESCRIPTION: Example demonstrating how to apply the Lemmatizer to a stream of documents with batching for efficiency.

LANGUAGE: python
CODE:
lemmatizer = nlp.add_pipe("lemmatizer")
for doc in lemmatizer.pipe(docs, batch_size=50):
    pass

----------------------------------------

TITLE: Improving Variable Naming in Python with spaCy
DESCRIPTION: Shows how to refactor variable names to avoid reassigning variables with different types. The example demonstrates converting from reusing the 'ents' variable to using a more specific 'ent_mappings' variable for the dictionary comprehension.

LANGUAGE: python
CODE:
ents = get_a_list_of_entities()
ents = [ent for ent in doc.ents if ent.label_ == "PERSON"]
- ents = {(ent.start, ent.end): ent.label_ for ent in ents}
+ ent_mappings = {(ent.start, ent.end): ent.label_ for ent in ents}

----------------------------------------

TITLE: Converting JSONL to spaCy JSON Training Data
DESCRIPTION: Command to convert text classification data from JSONL format to spaCy's JSON training format. The script takes a model language code and input JSONL file as parameters.

LANGUAGE: shell
CODE:
python textcatjsonl_to_trainjson.py -m en file.jsonl .

----------------------------------------

TITLE: Sample Output for Text Classification with spaCy-LLM
DESCRIPTION: Shows the resulting document categories after processing with the LLM pipeline. The output is a dictionary with scores for each possible category.

LANGUAGE: text
CODE:
{'COMPLIMENT': 1.0, 'INSULT': 0.0}

----------------------------------------

TITLE: Resuming Training for a SpaCy Pipeline in Python
DESCRIPTION: Example of continuing training on a pre-trained SpaCy pipeline by creating an optimizer and initializing rehearsal capabilities to prevent catastrophic forgetting.

LANGUAGE: python
CODE:
optimizer = nlp.resume_training()
nlp.rehearse(examples, sgd=optimizer)

----------------------------------------

TITLE: Working with Hash Values for String IDs in spaCy
DESCRIPTION: Example showing how spaCy v2.0 uses hash values instead of integer IDs for strings, making the string-to-int mapping consistent across models and processes.

LANGUAGE: python
CODE:
doc = nlp("I love coffee")
assert doc.vocab.strings["coffee"] == 3197928453018144401
assert doc.vocab.strings[3197928453018144401] == "coffee"

beer_hash = doc.vocab.strings.add("beer")
assert doc.vocab.strings["beer"] == beer_hash
assert doc.vocab.strings[beer_hash] == "beer"

----------------------------------------

TITLE: Updating Doc.merge usage to Doc.retokenize in spaCy v2.1.x
DESCRIPTION: Shows how to migrate from the deprecated Doc.merge and Span.merge methods to the more efficient Doc.retokenize context manager. This approach allows performing multiple merges in bulk within a single context manager block for better performance.

LANGUAGE: diff
CODE:
- doc[1:5].merge()
- doc[6:8].merge()
+ with doc.retokenize() as retokenizer:
+     retokenizer.merge(doc[1:5])
+     retokenizer.merge(doc[6:8])

----------------------------------------

TITLE: Customizing Span Visualization Options in spaCy
DESCRIPTION: This example shows how to customize the span visualizer by specifying a custom spans key. It creates a custom span with the label 'BANK' and configures displaCy to use this custom span instead of the default.

LANGUAGE: python
CODE:
doc.spans["custom"] = [Span(doc, 3, 6, "BANK")]
options = {"spans_key": "custom"}
displacy.serve(doc, style="span", options=options)

----------------------------------------

TITLE: Parametrizing tests with a single argument in spaCy
DESCRIPTION: Demonstrates how to use pytest's parametrize decorator to run the same test with different input values, passing a single list argument to the test function.

LANGUAGE: python
CODE:
@pytest.mark.parametrize("words", [["hello", "world"], ["this", "is", "a", "test"]])
def test_doc_length(words):
    doc = Doc(Vocab(), words=words)
    assert len(doc) == len(words)

----------------------------------------

TITLE: Streaming Documents Through DependencyParser's pipe Method in SpaCy (Python)
DESCRIPTION: Example of processing multiple documents through the dependency parser using the pipe method with batch processing. This allows efficient processing of document streams with specified batch sizes.

LANGUAGE: python
CODE:
parser = nlp.add_pipe("parser")
for doc in parser.pipe(docs, batch_size=50):
    pass

----------------------------------------

TITLE: Creating an Optimizer for TextCategorizer in Python
DESCRIPTION: Shows how to create an optimizer for the TextCategorizer pipeline component. The component is first added to the NLP pipeline, then an optimizer is created for it.

LANGUAGE: python
CODE:
textcat = nlp.add_pipe("textcat")
optimizer = textcat.create_optimizer()

----------------------------------------

TITLE: Loading SpanRuler from Bytes in Python
DESCRIPTION: Demonstrates how to load a SpanRuler from a bytestring using the from_bytes method.

LANGUAGE: python
CODE:
ruler_bytes = ruler.to_bytes()
ruler = nlp.add_pipe("span_ruler")
ruler.from_bytes(ruler_bytes)

----------------------------------------

TITLE: Example Plain Text Format for PlainTextCorpus
DESCRIPTION: Example of plain text format used by PlainTextCorpus class. Each line contains a document, and blank lines are ignored. The file is expected to be UTF-8 encoded.

LANGUAGE: text
CODE:
Can I ask where you work now and what you do, and if you enjoy it?
They may just pull out of the Seattle market completely, at least until they have autonomous vehicles.
My cynical view on this is that it will never be free to the public. Reason: what would be the draw of joining the military? Right now their selling point is free Healthcare and Education. Ironically both are run horribly and most, that I've talked to, come out wishing they never went in.

----------------------------------------

TITLE: Implementing the Modular Relation Model Architecture in Python
DESCRIPTION: Implements a modular relation extraction model by chaining an instance tensor creation layer with a classification layer. The model processes documents to extract and classify entity relationships.

LANGUAGE: python
CODE:
@spacy.registry.architectures("rel_model.v1")
def create_relation_model(
    create_instance_tensor: Model[List[Doc], Floats2d],
    classification_layer: Model[Floats2d, Floats2d],
) -> Model[List[Doc], Floats2d]:
    model = chain(create_instance_tensor, classification_layer)
    return model

----------------------------------------

TITLE: Checking Feature Presence in MorphAnalysis in Python
DESCRIPTION: Demonstrates how to check if a specific feature/value pair is present in a MorphAnalysis object using the __contains__ method.

LANGUAGE: python
CODE:
feats = "Feat1=Val1,Val2|Feat2=Val2"
morph = MorphAnalysis(nlp.vocab, feats)
assert "Feat1=Val1" in morph

----------------------------------------

TITLE: Handling Circular Imports in Type Annotations with TYPE_CHECKING
DESCRIPTION: Shows how to avoid circular imports by using conditional imports with TYPE_CHECKING and string-based type annotations, ideal for complex module dependencies.

LANGUAGE: python
CODE:
from typing TYPE_CHECKING

if TYPE_CHECKING:
    from .language import Language

def load_model(name: str) -> "Language":
    ...

----------------------------------------

TITLE: Adding Document Titles to Entity Visualizations in spaCy
DESCRIPTION: Demonstrates how to add a title to an entity visualization by setting the 'title' property in the document's user_data dictionary, which is useful for organizing multiple visualizations.

LANGUAGE: python
CODE:
doc = nlp("This is a sentence about Google.")
doc.user_data["title"] = "This is a title"
displacy.serve(doc, style="ent")

----------------------------------------

TITLE: Using List Comprehensions Instead of Filter/Map
DESCRIPTION: Demonstrates replacing built-in filter and map functions with more Pythonic list/generator comprehensions, improving readability and avoiding lambda functions.

LANGUAGE: diff
CODE:
- filtered = filter(lambda x: x in ["foo", "bar"], values)
+ filtered = (x for x in values if x in ["foo", "bar"])
- filtered = list(filter(lambda x: x in ["foo", "bar"], values))
+ filtered = [x for x in values if x in ["foo", "bar"]]

- result = map(lambda x: { x: x in ["foo", "bar"]}, values)
+ result = ({x: x in ["foo", "bar"]} for x in values)
- result = list(map(lambda x: { x: x in ["foo", "bar"]}, values))
+ result = [{x: x in ["foo", "bar"]} for x in values]

----------------------------------------

TITLE: Iterating Over C Arrays with Slice Notation in Cython
DESCRIPTION: Shows the preferred way to iterate over C arrays in Cython using slice notation with explicit length. This approach allows the compiler to optimize the code while maintaining readable syntax similar to Python.

LANGUAGE: python
CODE:
cdef int c_total(const int* int_array, int length) nogil:
    total = 0
    for item in int_array[:length]:
        total += item
    return total

----------------------------------------

TITLE: Initializing CoreferenceResolver Component
DESCRIPTION: Different methods for constructing and initializing the CoreferenceResolver component, including using add_pipe with default or custom models, and direct instantiation from the class.

LANGUAGE: python
CODE:
# Construction via add_pipe with default model
coref = nlp.add_pipe("experimental_coref")

# Construction via add_pipe with custom model
config = {"model": {"@architectures": "my_coref.v1"}}
coref = nlp.add_pipe("experimental_coref", config=config)

# Construction from class
from spacy_experimental.coref.coref_component import CoreferenceResolver
coref = CoreferenceResolver(nlp.vocab, model)

----------------------------------------

TITLE: Using span_cleaner in spaCy Python (Experimental)
DESCRIPTION: Shows how to use the experimental span_cleaner component from spacy-experimental to remove SpanGroups from doc.spans based on a key prefix. Useful after coreference resolution.

LANGUAGE: python
CODE:
config = {"prefix": "coref_head_clusters"}
nlp.add_pipe("span_cleaner", config=config)
doc = nlp("text")
assert "coref_head_clusters_1" not in doc.spans

----------------------------------------

TITLE: Performing Rehearsal Updates in SpaCy to Prevent Catastrophic Forgetting
DESCRIPTION: Example of using the experimental rehearsal feature in SpaCy to maintain model performance on previously learned tasks while learning new ones, addressing the catastrophic forgetting problem.

LANGUAGE: python
CODE:
optimizer = nlp.resume_training()
losses = nlp.rehearse(examples, sgd=optimizer)

----------------------------------------

TITLE: Checking spaCy's Tokenization
DESCRIPTION: Code to verify how spaCy tokenizes complex text examples, which is important when creating matching patterns since each dictionary in a pattern represents one token.

LANGUAGE: python
CODE:
doc = nlp("A complex-example,!")
print([token.text for token in doc])

----------------------------------------

TITLE: Loading Only Pipeline Data without Configuration
DESCRIPTION: Shows how to load only the binary data of a pipeline without using the configuration, by creating a blank Language object and calling from_disk on it.

LANGUAGE: python
CODE:
nlp = spacy.blank("en").from_disk("/path/to/data")

----------------------------------------

TITLE: Minimizing Try/Except Block Scope in Python
DESCRIPTION: Demonstrates how to limit the scope of try/except blocks to only the necessary code and specify exact exceptions to catch, preventing unintended error masking.

LANGUAGE: diff
CODE:
- try:
-     value1 = get_some_value()
-     value2 = get_some_other_value()
-     score = external_library.compute_some_score(value1, value2)
- except:
-     score = 0.0

+ value1 = get_some_value()
+ value2 = get_some_other_value()
+ try:
+     score = external_library.compute_some_score(value1, value2)
+ except ValueError:
+     score = 0.0

----------------------------------------

TITLE: Creating an Optimizer for DependencyParser in spaCy
DESCRIPTION: Demonstrates how to create a dedicated optimizer for a dependency parser component.

LANGUAGE: python
CODE:
parser = nlp.add_pipe("parser")
optimizer = parser.create_optimizer()

----------------------------------------

TITLE: Configuring Lemma LLM Task in spaCy
DESCRIPTION: Example configuration for the spacy.Lemma.v1 LLM task component that performs lemmatization using large language models. This basic configuration uses zero-shot learning.

LANGUAGE: ini
CODE:
[components.llm.task]
@llm_tasks = "spacy.Lemma.v1"
examples = null

----------------------------------------

TITLE: Deserializing a SpanGroup from Bytes in Python using spaCy
DESCRIPTION: Demonstrates how to load a span group from a bytestring using from_bytes() method. The example creates a serialized span group, then loads it into a new SpanGroup object, modifying the target object in place.

LANGUAGE: python
CODE:
from spacy.tokens import SpanGroup

doc = nlp("Their goi ng home")
doc.spans["errors"] = [doc[0:1], doc[1:3]]
group_bytes = doc.spans["errors"].to_bytes()
new_group = SpanGroup()
new_group.from_bytes(group_bytes)

----------------------------------------

TITLE: Saving Sentencizer Settings to Disk
DESCRIPTION: Example showing how to save the Sentencizer's configuration (including custom punctuation characters) to a JSON file on disk. This also happens automatically when saving an nlp object.

LANGUAGE: Python
CODE:
config = {"punct_chars": [".", "?", "!", "。"]}
sentencizer = nlp.add_pipe("sentencizer", config=config)
sentencizer.to_disk("/path/to/sentencizer.json")

----------------------------------------

TITLE: Case-Insensitive Matching with PhraseMatcher in spaCy
DESCRIPTION: Demonstrates how to perform case-insensitive matching by setting the 'attr' parameter to 'LOWER' when initializing the PhraseMatcher. This allows matching patterns regardless of letter casing.

LANGUAGE: python
CODE:
from spacy.lang.en import English
from spacy.matcher import PhraseMatcher

nlp = English()
matcher = PhraseMatcher(nlp.vocab, attr="LOWER")
patterns = [nlp.make_doc(name) for name in ["Angela Merkel", "Barack Obama"]]
matcher.add("Names", patterns)

doc = nlp("angela merkel and us president barack Obama")
for match_id, start, end in matcher(doc):
    print("Matched based on lowercase token text:", doc[start:end])

----------------------------------------

TITLE: Iterating Over MorphAnalysis Features in Python
DESCRIPTION: Shows how to iterate over all feature/value pairs in a MorphAnalysis object using the __iter__ method.

LANGUAGE: python
CODE:
feats = "Feat1=Val1,Val3|Feat2=Val2"
morph = MorphAnalysis(nlp.vocab, feats)
assert list(morph) == ["Feat1=Va1", "Feat1=Val3", "Feat2=Val2"]

----------------------------------------

TITLE: Using Keyword-Only Arguments in Python Functions
DESCRIPTION: Demonstrates how to enforce keyword-only arguments in function definitions, making function calls more explicit and readable, particularly for optional parameters.

LANGUAGE: diff
CODE:
- def do_something(name: str, validate: bool = False):
+ def do_something(name: str, *, validate: bool = False):
    ...

- do_something("some_name", True)
+ do_something("some_name", validate=True)

----------------------------------------

TITLE: Converting Dictionary to FEATS String in Python
DESCRIPTION: Static method that converts a dictionary of morphological features and values to a string in Universal Dependencies FEATS format.

LANGUAGE: python
CODE:
from spacy.morphology import Morphology
f = Morphology.dict_to_feats({"Feat1": "Val1", "Feat2": "Val2"})
assert f == "Feat1=Val1|Feat2=Val2"

----------------------------------------

TITLE: Loading a KnowledgeBase from Disk in Python
DESCRIPTION: Example demonstrating how to restore the state of a KnowledgeBase instance from a directory on disk, ensuring the vocabulary is the same as the one used to create the KB.

LANGUAGE: python
CODE:
from spacy.vocab import Vocab
vocab = Vocab().from_disk("/path/to/vocab")
kb = FullyImplementedKB(vocab=vocab, entity_vector_length=64)
kb.from_disk("/path/to/kb")

----------------------------------------

TITLE: Enabling Pipeline Components in spaCy
DESCRIPTION: Demonstrates how to re-enable a previously disabled pipeline component so it's run as part of the pipeline again. The example shows the state before and after enabling.

LANGUAGE: python
CODE:
nlp.disable_pipe("ner")
assert "ner" in nlp.disabled
assert not "ner" in nlp.pipe_names
nlp.enable_pipe("ner")
assert not "ner" in nlp.disabled
assert "ner" in nlp.pipe_names

----------------------------------------

TITLE: Using Temporary Parameter Values with EntityRecognizer
DESCRIPTION: Demonstrates how to temporarily modify the EntityRecognizer's model parameters. This is useful for operations like saving the best model with averaged parameters.

LANGUAGE: python
CODE:
ner = EntityRecognizer(nlp.vocab)
with ner.use_params(optimizer.averages):
    ner.to_disk("/best_model")

----------------------------------------

TITLE: Updating Lemmatizer initialization code
DESCRIPTION: Diff showing how to update custom Lemmatizer initialization to use the new Lookups class instead of passing dictionaries directly. This change affects anyone who created custom Lemmatizers.

LANGUAGE: diff
CODE:
from spacy.lemmatizer import Lemmatizer
+ from spacy.lookups import Lookups

lemma_index = {"verb": ("cope", "cop")}
lemma_exc = {"verb": {"coping": ("cope",)}}
lemma_rules = {"verb": [["ing", ""]]}
- lemmatizer = Lemmatizer(lemma_index, lemma_exc, lemma_rules)
+ lookups = Lookups()
+ lookups.add_table("lemma_index", lemma_index)
+ lookups.add_table("lemma_exc", lemma_exc)
+ lookups.add_table("lemma_rules", lemma_rules)
+ lemmatizer = Lemmatizer(lookups)

----------------------------------------

TITLE: Validating and Debugging Patterns in spaCy Matcher
DESCRIPTION: Shows how to validate patterns against a JSON schema using the validate=True option. This helps catch unsupported attributes during development and debugging.

LANGUAGE: python
CODE:
import spacy
from spacy.matcher import Matcher

nlp = spacy.load("en_core_web_sm")
matcher = Matcher(nlp.vocab, validate=True)
# Add match ID "HelloWorld" with unsupported attribute CASEINSENSITIVE
pattern = [{"LOWER": "hello"}, {"IS_PUNCT": True}, {"CASEINSENSITIVE": "world"}]
matcher.add("HelloWorld", [pattern])
# 🚨 Raises an error:
# MatchPatternError: Invalid token patterns for matcher rule 'HelloWorld'
# Pattern 0:
# - [pattern -> 2 -> CASEINSENSITIVE] extra fields not permitted

----------------------------------------

TITLE: Avoiding Mutable Default Arguments with SimpleFrozenList
DESCRIPTION: Shows how to use spaCy's SimpleFrozenList instead of mutable default arguments like empty lists, preventing the common Python pitfall of shared mutable state.

LANGUAGE: diff
CODE:
- def to_bytes(self, *, exclude: List[str] = []):
+ def to_bytes(self, *, exclude: List[str] = SimpleFrozenList()):
    ...

----------------------------------------

TITLE: Disabling Black Auto-formatting for Specific Code Blocks
DESCRIPTION: Demonstrates how to disable black auto-formatting for specific sections of code using fmt comments. This is useful for code that should maintain a specific formatting style, such as in tests.

LANGUAGE: diff
CODE:
+ # fmt: off
text = "I look forward to using Thingamajig.  I've been told it will make my life easier..."
deps = ["nsubj", "ROOT", "advmod", "prep", "pcomp", "dobj", "punct", "",
        "nsubjpass", "aux", "auxpass", "ROOT", "nsubj", "aux", "ccomp",
        "poss", "nsubj", "ccomp", "punct"]
+ # fmt: on

----------------------------------------

TITLE: Listing Installed SpaCy Pipeline Models
DESCRIPTION: Lists all spaCy pipeline packages installed in the current environment by checking Python entry points, without loading the nlp objects.

LANGUAGE: python
CODE:
names = util.get_installed_models()

----------------------------------------

TITLE: Using Parameter Values with SpanFinder in spaCy
DESCRIPTION: Temporarily modify the SpanFinder model to use specific parameter values within a context manager.

LANGUAGE: python
CODE:
span_finder = nlp.add_pipe("span_finder")
with span_finder.use_params(optimizer.averages):
    span_finder.to_disk("/best_model")

----------------------------------------

TITLE: Entry Structure for spaCy Vocabulary Initialization
DESCRIPTION: Defines the JSON structure for individual lexeme entries in a vocabulary initialization file. Each entry contains orthographic form, lexical attributes, and boolean flags that are used to initialize spaCy's Lexeme objects.

LANGUAGE: python
CODE:
{
    "orth": string,     # the word text
    "id": int,          # can correspond to row in vectors table
    "lower": string,
    "norm": string,
    "shape": string
    "prefix": string,
    "suffix": string,
    "length": int,
    "cluster": string,
    "prob": float,
    "is_alpha": bool,
    "is_ascii": bool,
    "is_digit": bool,
    "is_lower": bool,
    "is_punct": bool,
    "is_space": bool,
    "is_title": bool,
    "is_upper": bool,
    "like_url": bool,
    "like_num": bool,
    "like_email": bool,
    "is_stop": bool,
    "is_oov": bool,
    "is_quote": bool,
    "is_left_punct": bool,
    "is_right_punct": bool
}

----------------------------------------

TITLE: Saving an NLP Pipeline with EntityRuler
DESCRIPTION: Example of adding an EntityRuler to a pipeline, adding patterns, and saving the complete pipeline to disk. The patterns are automatically exported to the pipeline directory.

LANGUAGE: python
CODE:
nlp = spacy.load("en_core_web_sm")
ruler = nlp.add_pipe("entity_ruler")
ruler.add_patterns([{"label": "ORG", "pattern": "Apple"}])
nlp.to_disk("/path/to/pipeline")

----------------------------------------

TITLE: Saving SentenceRecognizer to Disk in spaCy
DESCRIPTION: Shows how to serialize a SentenceRecognizer component to disk using the to_disk method. This saves the component to a specified path.

LANGUAGE: python
CODE:
senter = nlp.add_pipe("senter")
senter.to_disk("/path/to/senter")

----------------------------------------

TITLE: Replacing Try/Except with Explicit Condition Checking
DESCRIPTION: Shows how to avoid using exception handling for control flow by checking conditions explicitly, making code more readable and preventing masked bugs.

LANGUAGE: diff
CODE:
- try:
-     token = doc[i]
- except IndexError:
-     token = doc[-1]

+ if i < len(doc):
+     token = doc[i]
+ else:
+     token = doc[-1]

----------------------------------------

TITLE: Using Wildcard Token Patterns in spaCy Matcher
DESCRIPTION: Demonstrates using an empty dictionary as a wildcard to match any token in a pattern. This is useful when you know the context but not the specific token content, such as extracting usernames with unknown characters.

LANGUAGE: python
CODE:
[{"ORTH": "User"}, {"ORTH": "name"}, {"ORTH": ":"}, {}]

----------------------------------------

TITLE: Including TODO Comments in Python Code
DESCRIPTION: Example of including a TODO comment in code to indicate potential future improvements, with a note about the current implementation's performance characteristics.

LANGUAGE: diff
CODE:
+ # TODO: this is currently pretty slow
dir_checksum = hashlib.md5()
for sub_file in sorted(fp for fp in path.rglob("*") if fp.is_file()):
    dir_checksum.update(sub_file.read_bytes())

----------------------------------------

TITLE: Statistical Morphology Analysis in spaCy
DESCRIPTION: Shows how spaCy's statistical Morphologizer component assigns morphological features and part-of-speech tags. The example demonstrates this for German text using a pre-trained model.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("de_core_news_sm")
doc = nlp("Wo bist du?") # English: 'Where are you?'
print(doc[2].morph)  # 'Case=Nom|Number=Sing|Person=2|PronType=Prs'
print(doc[2].pos_) # 'PRON'

----------------------------------------

TITLE: Visualizing Named Entities with spaCy's displaCy in Python
DESCRIPTION: Demonstrates how to load a spaCy model, process text to identify named entities, and visualize the results using the displaCy visualization tool with the 'ent' style option.

LANGUAGE: python
CODE:
import spacy
from spacy import displacy

text = "When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously."

nlp = spacy.load("en_core_web_sm")
doc = nlp(text)
displacy.serve(doc, style="ent")

----------------------------------------

TITLE: Loading Japanese Tokenizer in spaCy with Different Split Modes
DESCRIPTION: Demonstrates how to initialize the Japanese language class with SudachiPy, which is used for word segmentation and part-of-speech tagging. Shows both default split mode A and how to configure split mode B.

LANGUAGE: python
CODE:
from spacy.lang.ja import Japanese

# Load SudachiPy with split mode A (default)
nlp = Japanese()
# Load SudachiPy with split mode B
cfg = {"split_mode": "B"}
nlp = Japanese.from_config({"nlp": {"tokenizer": cfg}})

----------------------------------------

TITLE: Simple Config Diff Example
DESCRIPTION: A simple example of comparing a local configuration file with spaCy's default settings. This shows how to use the command in its most basic form.

LANGUAGE: bash
CODE:
$ python -m spacy debug diff-config ./config.cfg

----------------------------------------

TITLE: Updating spaCy Version Requirements in meta.json
DESCRIPTION: Example of how to update the spaCy version requirements in the meta.json file of a custom pipeline to make it compatible with spaCy v3.7.

LANGUAGE: diff
CODE:
- "spacy_version": ">=3.6.0,<3.7.0",
+ "spacy_version": ">=3.6.0,<3.8.0",

----------------------------------------

TITLE: Updating Serbian language code import
DESCRIPTION: Diff showing how to correct the import for the Serbian language class by using the correct language code 'sr' instead of the incorrect 'rs'.

LANGUAGE: diff
CODE:
- from spacy.lang.rs import Serbian
+ from spacy.lang.sr import Serbian

----------------------------------------

TITLE: Processing Matcher Results in spaCy
DESCRIPTION: Code showing how to process the results returned by the spaCy Matcher, including converting match IDs to string representations and extracting the matched spans from the document.

LANGUAGE: python
CODE:
for match_id, start, end in matches:
    string_id = nlp.vocab.strings[match_id]  # 'HelloWorld'
    span = doc[start:end]                    # The matched span

----------------------------------------

TITLE: Running flake8 Linter on spaCy Code
DESCRIPTION: Command to run the flake8 linter on the spaCy codebase to enforce code style standards and identify potential issues.

LANGUAGE: bash
CODE:
flake8 spacy

----------------------------------------

TITLE: Setting Annotations with Pre-computed Tok2Vec Scores
DESCRIPTION: Example showing how to modify documents using pre-computed scores from the Tok2Vec model, which can be useful for separating prediction and annotation steps.

LANGUAGE: python
CODE:
tok2vec = nlp.add_pipe("tok2vec")
scores = tok2vec.predict(docs)
tok2vec.set_annotations(docs, scores)

----------------------------------------

TITLE: Adding a Table to Lookups
DESCRIPTION: Adds a new table with optional data to the Lookups object. Raises an error if the table already exists.

LANGUAGE: python
CODE:
lookups = Lookups()
lookups.add_table("some_table", {"foo": "bar"})

----------------------------------------

TITLE: Initializing TextCategorizer Component in spaCy
DESCRIPTION: Creates a text categorizer component and initializes it with examples and the nlp object. This demonstrates the basic setup required before training.

LANGUAGE: python
CODE:
textcat = nlp.add_pipe("textcat")
textcat.initialize(lambda: examples, nlp=nlp)

----------------------------------------

TITLE: Defining Project Assets in YAML Configuration
DESCRIPTION: YAML configuration that defines project data assets to be downloaded. Assets can be specified by URL or Git location with checksums for verification.

LANGUAGE: yaml
CODE:
assets:
  - dest: 'assets/training.spacy'
    url: 'https://example.com/data.spacy'
    checksum: '63373dd656daa1fd3043ce166a59474c'
  - dest: 'assets/development.spacy'
    git:
      repo: 'https://github.com/example/repo'
      branch: 'master'
      path: 'path/development.spacy'
    checksum: '5113dc04e03f079525edd8df3f4f39e3'

----------------------------------------

TITLE: Serializing Sentencizer to Bytes
DESCRIPTION: Example demonstrating how to serialize the Sentencizer's settings to a bytestring, which can be useful for storing or transmitting the configuration.

LANGUAGE: Python
CODE:
config = {"punct_chars": [".", "?", "!", "。"]}
sentencizer = nlp.add_pipe("sentencizer", config=config)
sentencizer_bytes = sentencizer.to_bytes()

----------------------------------------

TITLE: Referencing GitHub Issues in Code Comments
DESCRIPTION: Example of commenting code with reference to a specific GitHub issue number to provide context for a fix or implementation detail.

LANGUAGE: diff
CODE:
+ # Ensure object is a Span, not a Doc (#1234)
if isinstance(obj, Doc):
    obj = obj[obj.start:obj.end]

----------------------------------------

TITLE: Initializing PhraseMatcher in spaCy (Python)
DESCRIPTION: Creates a new PhraseMatcher instance with the shared vocabulary. This example shows the basic initialization of a PhraseMatcher, which requires access to the vocabulary from the nlp pipeline.

LANGUAGE: python
CODE:
from spacy.matcher import PhraseMatcher
matcher = PhraseMatcher(nlp.vocab)

----------------------------------------

TITLE: Getting spaCy Installation Information
DESCRIPTION: Command for printing information about the spaCy installation, trained pipelines, and local setup. Can generate Markdown-formatted output for GitHub issues.

LANGUAGE: bash
CODE:
$ python -m spacy info [--markdown] [--silent] [--exclude]

----------------------------------------

TITLE: Running spaCy debug pieces command for word/sentence piece analysis
DESCRIPTION: Command for analyzing word- or sentencepiece statistics with the spaCy debug pieces tool. Requires a config path and can take optional code file, name parameter, and config overrides.

LANGUAGE: bash
CODE:
$ python -m spacy debug pieces [config_path] [--code] [--name] [overrides]

----------------------------------------

TITLE: Using EditTreeLemmatizer.predict method in spaCy
DESCRIPTION: Apply the lemmatizer's model to documents without modifying them. This method returns tree identifiers that can be used later to set annotations.

LANGUAGE: python
CODE:
lemmatizer = nlp.add_pipe("trainable_lemmatizer", name="lemmatizer")
tree_ids = lemmatizer.predict([doc1, doc2])

----------------------------------------

TITLE: Enabling Specific Pipeline Components with select_pipes
DESCRIPTION: Demonstrates using the enable parameter with select_pipes to temporarily enable only specific components while disabling all others.

LANGUAGE: python
CODE:
# Enable only the parser
with nlp.select_pipes(enable="parser"):
    doc = nlp("I will only be parsed")

----------------------------------------

TITLE: Getting Pointers from Different Data Structures in Cython
DESCRIPTION: Demonstrates how to obtain C pointers from numpy arrays, C++ vectors, and memory views in Cython. The function is declared 'nogil' to optimize performance and allow for multi-threading.

LANGUAGE: cython
CODE:
cdef void get_pointers(np.ndarray[int, mode='c'] numpy_array, vector[int] cpp_vector, int[::1] memory_view) nogil:
    pointer1 = <int*>numpy_array.data
    pointer2 = cpp_vector.data()
    pointer3 = &memory_view[0]

----------------------------------------

TITLE: Retrieving Vectors by Key in Python
DESCRIPTION: Method to get a vector by its key (string or integer). Raises KeyError if the key is not found.

LANGUAGE: python
CODE:
def __getitem__(self, key):

----------------------------------------

TITLE: Retrieving Factory Metadata for Pipeline Components in spaCy with Python
DESCRIPTION: Example of using the get_factory_meta class method to retrieve metadata about a pipeline component factory, including its default configuration settings.

LANGUAGE: python
CODE:
factory_meta = Language.get_factory_meta("ner")
assert factory_meta.factory == "ner"
print(factory_meta.default_config)

----------------------------------------

TITLE: Installing spacy-experimental Package in Bash
DESCRIPTION: Command to install the spacy-experimental package which contains the SpanResolver component. The component is available starting from version 0.6.0.

LANGUAGE: bash
CODE:
$ pip install -U spacy-experimental

----------------------------------------

TITLE: Specifying spaCy Pipeline Dependencies in Requirements File
DESCRIPTION: Example of specifying spaCy and its model dependencies in a requirements.txt file using direct URLs for version pinning. This approach allows for explicit dependency management in automated build processes.

LANGUAGE: text
CODE:
spacy>=3.0.0,<4.0.0
en_core_web_sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl

----------------------------------------

TITLE: Transformer component configuration in spaCy config.cfg
DESCRIPTION: Configuration snippet for the Transformer component in spaCy's config.cfg file, defining the model architecture, tokenizer settings, span getters and annotation setters.

LANGUAGE: ini
CODE:
[components.transformer]
factory = "transformer"
max_batch_items = 4096

[components.transformer.model]
@architectures = "spacy-transformers.TransformerModel.v3"
name = "bert-base-cased"
tokenizer_config = {"use_fast": true}

[components.transformer.model.get_spans]
@span_getters = "spacy-transformers.doc_spans.v1"

[components.transformer.set_extra_annotations]
@annotation_setters = "spacy-transformers.null_annotation_setter.v1"

----------------------------------------

TITLE: Setting Up a Tok2VecListener in spaCy
DESCRIPTION: Configuration for setting up a Tok2VecListener that references an upstream Tok2Vec component to receive embeddings from.

LANGUAGE: toml
CODE:
[components.ner.model.tok2vec]
@architectures = "spacy.Tok2VecListener.v1"
upstream = "tok2vec"

----------------------------------------

TITLE: Python equivalent of transformer component configuration
DESCRIPTION: Python code showing how to manually create a Transformer component that's equivalent to the configuration specified in the config.cfg file.

LANGUAGE: python
CODE:
from spacy_transformers import Transformer, TransformerModel
from spacy_transformers.annotation_setters import null_annotation_setter
from spacy_transformers.span_getters import get_doc_spans

trf = Transformer(
    nlp.vocab,
    TransformerModel(
        "bert-base-cased",
        get_spans=get_doc_spans,
        tokenizer_config={"use_fast": True},
    ),
    set_extra_annotations=null_annotation_setter,
    max_batch_items=4096,
)

----------------------------------------

TITLE: Using Custom Vectors in a Blank spaCy Pipeline
DESCRIPTION: Python code snippet demonstrating how to create a blank spaCy pipeline with custom BPEmb vectors configured through a Python dictionary. This provides an alternative to configuration file-based setup.

LANGUAGE: python
CODE:
nlp = spacy.blank("en", config={"nlp.vectors": {"@vectors": "BPEmbVectors.v1", "lang": "en"}})

----------------------------------------

TITLE: Adding Custom Entity Labels to EntityRecognizer
DESCRIPTION: Shows how to manually add new entity labels to the EntityRecognizer component. This is not necessary if you provide representative data to the initialize method.

LANGUAGE: python
CODE:
ner = nlp.add_pipe("ner")
ner.add_label("MY_LABEL")

----------------------------------------

TITLE: Example output from spaCy debug pieces command
DESCRIPTION: Sample output from the debug pieces command showing corpus statistics including median token length, mean token length, and token length ranges for both training and development corpora.

LANGUAGE: bash
CODE:
========================= Training corpus statistics =========================
Median token length: 1.0
Mean token length: 1.54
Token length range: [1, 13]

======================= Development corpus statistics =======================
Median token length: 1.0
Mean token length: 1.44
Token length range: [1, 8]

----------------------------------------

TITLE: Migrating Package Building Process
DESCRIPTION: Shows the simplified process for packaging trained models in spaCy v3.0, which no longer requires manual building of Python packages.

LANGUAGE: diff
CODE:
python -m spacy package ./output ./packages
- cd /output/en_pipeline-0.0.0
- python setup.py sdist

----------------------------------------

TITLE: Checking if Vector Table is Full in Python
DESCRIPTION: Property that indicates whether the vectors table is full and cannot accept new keys.

LANGUAGE: python
CODE:
@property
def is_full(self):

----------------------------------------

TITLE: Checking for Factory Registration in spaCy
DESCRIPTION: Shows how to check if a factory name is registered on a Language class or subclass. The example demonstrates how language-specific factories are registered only on their respective language subclasses.

LANGUAGE: python
CODE:
from spacy.language import Language
from spacy.lang.en import English

@English.component("component")
def component(doc):
    return doc

assert English.has_factory("component")
assert not Language.has_factory("component")

----------------------------------------

TITLE: Using Alternative Parameters with a Tagger in spaCy
DESCRIPTION: Shows how to temporarily modify a tagger model's parameters using a context manager, which restores the original parameters afterward.

LANGUAGE: python
CODE:
tagger = nlp.add_pipe("tagger")
with tagger.use_params(optimizer.averages):
    tagger.to_disk("/best_model")

----------------------------------------

TITLE: Using the init vectors Command in spaCy CLI
DESCRIPTION: Command for converting word vectors for use with spaCy. Creates an 'nlp' object that can be used in the initialization block of a config. Supports options for truncating, pruning, selecting vector mode, token attribute, and naming the vectors.

LANGUAGE: bash
CODE:
$ python -m spacy init vectors [lang] [vectors_loc] [output_dir] [--prune] [--truncate] [--name] [--verbose]

----------------------------------------

TITLE: Configuring Few-Shot Examples via Pipeline Initialization in INI
DESCRIPTION: Configuration snippet for initializing a spaCy pipeline with few-shot examples for the LLM component. This approach uses the get_examples callback during pipeline initialization rather than a separate examples file.

LANGUAGE: ini
CODE:
[initialize.components.llm]
n_prompt_examples = 3

----------------------------------------

TITLE: Configuring Custom Evaluation Command with Variables in project.yml
DESCRIPTION: YAML configuration that defines a custom 'evaluate' command with variable substitution. It specifies the batch size as a reusable variable and passes it to a custom evaluation script along with model and data paths.

LANGUAGE: yaml
CODE:
vars:
  batch_size: 128

commands:
  - name: evaluate
    script:
      - 'python scripts/custom_evaluation.py ${vars.batch_size} ./training/model-best ./corpus/eval.json'
    deps:
      - 'training/model-best'
      - 'corpus/eval.json'

----------------------------------------

TITLE: Loading EntityRecognizer from Disk in spaCy
DESCRIPTION: Shows how to load a previously saved named entity recognizer component from disk using the from_disk method. This modifies the object in place and returns it.

LANGUAGE: python
CODE:
ner = nlp.add_pipe("ner")
ner.from_disk("/path/to/ner")

----------------------------------------

TITLE: Implementing Custom Resizing for a Pipeline Component
DESCRIPTION: Shows how to implement a custom resize_output function and add it as an attribute to a component's model. This allows the component to be resized after initialization.

LANGUAGE: python
CODE:
def custom_resize(model, new_nO):
    # adjust model
    return model

custom_model.attrs["resize_output"] = custom_resize

----------------------------------------

TITLE: Implementing Custom Resizing for a Pipeline Component
DESCRIPTION: Shows how to implement a custom resize_output function and add it as an attribute to a component's model. This allows the component to be resized after initialization.

LANGUAGE: python
CODE:
def custom_resize(model, new_nO):
    # adjust model
    return model

custom_model.attrs["resize_output"] = custom_resize

----------------------------------------

TITLE: General Syntax for spaCy assemble Command
DESCRIPTION: The full syntax for the spaCy assemble command, showing all possible arguments including config_path, output_dir, code import option, verbosity flag, and config overrides.

LANGUAGE: bash
CODE:
$ python -m spacy assemble [config_path] [output_dir] [--code] [--verbose] [overrides]

----------------------------------------

TITLE: Saving EntityLinker Component to Disk in spaCy (Python)
DESCRIPTION: Serializes the EntityLinker component to disk, saving the model and any associated data to the specified directory path.

LANGUAGE: python
CODE:
entity_linker = nlp.add_pipe("entity_linker")
entity_linker.to_disk("/path/to/entity_linker")

----------------------------------------

TITLE: Setting Vectors by Key in spaCy Python
DESCRIPTION: Example of setting a vector for a specific key in the vocabulary's vector store. This operation is not supported in 'floret' mode.

LANGUAGE: python
CODE:
cat_id = nlp.vocab.strings["cat"]
vector = numpy.random.uniform(-1, 1, (300,))
nlp.vocab.vectors[cat_id] = vector

----------------------------------------

TITLE: Accessing Text Property of Example in spaCy
DESCRIPTION: Demonstrates accessing the text property of an Example object, which returns the text from the predicted document.

LANGUAGE: python
CODE:
raw_text = example.text

----------------------------------------

TITLE: Creating Code Blocks in Markdown
DESCRIPTION: Shows how to create code blocks with titles in Markdown. Code blocks use Prism syntax highlighter and can have an optional title defined with ### or /// prefix.

LANGUAGE: markdown
CODE:
```python
### This is a title
import spacy
```

----------------------------------------

TITLE: Example usage of spaCy debug pieces command
DESCRIPTION: Simple example showing how to run the debug pieces command with a configuration file to analyze token statistics from training and development corpora.

LANGUAGE: bash
CODE:
$ python -m spacy debug pieces ./config.cfg

----------------------------------------

TITLE: Defining a Simple PyTorch Neural Network
DESCRIPTION: Creates a sequential PyTorch neural network with linear layers, ReLU activation, dropout, and softmax output. This model can then be wrapped with Thinc for use in spaCy.

LANGUAGE: python
CODE:
from torch import nn

torch_model = nn.Sequential(
    nn.Linear(width, hidden_width),
    nn.ReLU(),
    nn.Dropout2d(dropout),
    nn.Linear(hidden_width, nO),
    nn.ReLU(),
    nn.Dropout2d(dropout),
    nn.Softmax(dim=1)
)

----------------------------------------

TITLE: Configuring FastAPI Server in spaCy Project
DESCRIPTION: YAML command definition for serving a spaCy pipeline through a FastAPI REST API, allowing the model to be accessed via HTTP requests.

LANGUAGE: yaml
CODE:
  - name: "serve"
    help: "Serve the models via a FastAPI REST API using the given host and port"
    script:
      - "uvicorn scripts.main:app --reload --host 127.0.0.1 --port 5000"
    deps:
      - "scripts/main.py"
    no_skip: true

----------------------------------------

TITLE: Calculating Loss and Gradient for NER Component
DESCRIPTION: Demonstrates how to calculate the loss and gradient of loss for a batch of documents using a named entity recognition component. This method needs to be overwritten with a custom implementation for each component type.

LANGUAGE: python
CODE:
ner = nlp.add_pipe("ner")
scores = ner.predict([eg.predicted for eg in examples])
loss, d_loss = ner.get_loss(examples, scores)

----------------------------------------

TITLE: Applying SpanCategorizer to a Document in spaCy (Python)
DESCRIPTION: Shows how to apply a SpanCategorizer to a single document. The document is processed in-place and returned with span annotations added.

LANGUAGE: python
CODE:
doc = nlp("This is a sentence.")
spancat = nlp.add_pipe("spancat")
# This usually happens under the hood
processed = spancat(doc)

----------------------------------------

TITLE: Configuring FeatureExtractor in spaCy
DESCRIPTION: Example configuration for the FeatureExtractor.v1 architecture that extracts arrays of input features from Doc objects. The configuration specifies a list of token attributes to extract.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.FeatureExtractor.v1"
columns = ["NORM", "PREFIX", "SUFFIX", "SHAPE", "ORTH"]

----------------------------------------

TITLE: Adding Pipeline Components in spaCy
DESCRIPTION: Shows how to add built-in pipeline components in spaCy v2.0. The new API provides create_pipe() and add_pipe() methods instead of direct instantiation and insertion.

LANGUAGE: diff
CODE:
- from spacy.pipeline import Tagger
- tagger = Tagger(nlp.vocab)
- nlp.pipeline.insert(0, tagger)

+ tagger = nlp.create_pipe("tagger")
+ nlp.add_pipe(tagger, first=True)

----------------------------------------

TITLE: Using the init fill-curated-transformer Command in spaCy CLI
DESCRIPTION: Command for auto-filling Hugging Face model hyperparameters and loader parameters of a Curated Transformer pipeline component in a spaCy configuration file. Accepts optional parameters for model name, revision, pipe name, and additional code.

LANGUAGE: bash
CODE:
$ python -m spacy init fill-curated-transformer [base_path] [output_file] [--model-name] [--model-revision] [--pipe-name] [--code]

----------------------------------------

TITLE: Saving EntityRuler Patterns to Disk in spaCy Python
DESCRIPTION: Shows how to save EntityRuler patterns to disk either as a standalone JSONL file or as a directory with patterns and configuration.

LANGUAGE: python
CODE:
ruler = nlp.add_pipe("entity_ruler")
ruler.to_disk("/path/to/patterns.jsonl")  # saves patterns only
ruler.to_disk("/path/to/entity_ruler")    # saves patterns and config

----------------------------------------

TITLE: Creating Headings with JSX Components in spaCy Documentation
DESCRIPTION: Examples of how to create headings using JSX components with optional attributes like custom IDs and tags for the spaCy website.

LANGUAGE: jsx
CODE:
<H2>Headline 2</H2>
<H2 id="some_id">Headline 2</H2>
<H2 id="some_id" tag="method">Headline 2</H2>

----------------------------------------

TITLE: Generating DVC Configuration with spaCy
DESCRIPTION: Auto-generates Data Version Control (DVC) config file for a spaCy project. The command calls 'dvc run' with --no-exec to generate the dvc.yaml file based on the workflows defined in project.yml.

LANGUAGE: bash
CODE:
$ python -m spacy project dvc [project_dir] [workflow] [--force] [--verbose] [--quiet]

----------------------------------------

TITLE: Implementing the Constructor for the Relation Extraction Component
DESCRIPTION: Extending the constructor to maintain a list of labels for the relation extraction component. This implementation stores labels in the component's configuration and provides properties to access and add new labels.

LANGUAGE: python
CODE:
    def __init__(self, vocab, model, name="rel"):
        """Create a component instance."""
        # ...
        self.cfg = {"labels": []}

    @property
    def labels(self) -> Tuple[str, ...]:
        """Returns the labels currently added to the component."""
        return tuple(self.cfg["labels"])

    def add_label(self, label: str):
        """Add a new label to the pipe."""
        self.cfg["labels"] = list(self.labels) + [label]

----------------------------------------

TITLE: Fetching Project Assets using Command Line
DESCRIPTION: Command to download all required data files defined in the project.yml configuration for the current project.

LANGUAGE: bash
CODE:
$ cd some_example_project
$ python -m spacy project assets

----------------------------------------

TITLE: Accessing Label Data for EditTreeLemmatizer Initialization in spaCy
DESCRIPTION: Example showing how to retrieve label data from an existing lemmatizer and use it to initialize a new instance with the same label set. This is useful for model transfer or initialization.

LANGUAGE: python
CODE:
labels = lemmatizer.label_data
lemmatizer.initialize(lambda: [], nlp=nlp, labels=labels)

----------------------------------------

TITLE: Installing spaCy Hugging Face Hub Integration
DESCRIPTION: Shell commands for installing the spacy-huggingface-hub package and verifying the CLI command registration. This package adds Hugging Face Hub commands to the spaCy CLI.

LANGUAGE: bash
CODE:
$ pip install spacy-huggingface-hub
# Check that the CLI is registered
$ python -m spacy huggingface-hub --help

----------------------------------------

TITLE: Configuring EntityLinker Model Architecture in spaCy
DESCRIPTION: Configuration example for the EntityLinker model architecture that disambiguates textual mentions to unique identifiers. This model uses a tok2vec layer followed by a Linear output layer.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.EntityLinker.v2"
nO = null

[model.tok2vec]
@architectures = "spacy.HashEmbedCNN.v2"
pretrained_vectors = null
width = 96
depth = 2
embed_size = 2000
window_size = 1
maxout_pieces = 3
subword_features = true

----------------------------------------

TITLE: Setting Property Extension with Getter and Setter
DESCRIPTION: Example of creating a property extension with getter and setter functions on a Doc object to control access to the extension value.

LANGUAGE: python
CODE:
Doc.set_extension("hello", getter=get_hello_value, setter=set_hello_value)
assert doc._.hello
doc._.hello = "Hi!"

----------------------------------------

TITLE: Applying the Lemmatizer component to a document
DESCRIPTION: Example showing how to apply the Lemmatizer component directly to a document, which modifies the document in place and returns it.

LANGUAGE: python
CODE:
doc = nlp("This is a sentence.")
lemmatizer = nlp.add_pipe("lemmatizer")
# This usually happens under the hood
processed = lemmatizer(doc)

----------------------------------------

TITLE: Using Label Data for EntityRecognizer Initialization in spaCy
DESCRIPTION: Shows how to access and use label data for initializing an EntityRecognizer with a pre-defined label set. This is the data generated by 'init labels' and used during model initialization.

LANGUAGE: python
CODE:
labels = ner.label_data
ner.initialize(lambda: [], nlp=nlp, labels=labels)

----------------------------------------

TITLE: Initializing Tok2Vec Component for Training
DESCRIPTION: Example showing how to initialize the Tok2Vec component for training by providing example data that will be used to set up the model architecture and parameters.

LANGUAGE: python
CODE:
tok2vec = nlp.add_pipe("tok2vec")
tok2vec.initialize(lambda: examples, nlp=nlp)

----------------------------------------

TITLE: Configuring Multiple Corpora in spaCy Configuration Files (INI)
DESCRIPTION: Example showing how to define multiple corpus readers in a spaCy configuration file. This configuration defines train, dev, and pretrain corpora, as well as a custom corpus, each with its own reader and path.

LANGUAGE: ini
CODE:
[corpora]

[corpora.train]
@readers = "spacy.Corpus.v1"
path = ${paths:train}

[corpora.dev]
@readers = "spacy.Corpus.v1"
path = ${paths:dev}

[corpora.pretrain]
@readers = "spacy.JsonlCorpus.v1"
path = ${paths.raw}

[corpora.my_custom_data]
@readers = "my_custom_reader.v1"

----------------------------------------

TITLE: Accessing Children in German Language Model with spaCy
DESCRIPTION: This code shows how to use left and right children functionality in a German language model. It processes a German text and extracts the left and right children of a specific token, demonstrating spaCy's multilingual capabilities.

LANGUAGE: python
CODE:
import spacy

nlp = spacy.load("de_core_news_sm")
doc = nlp("schöne rote Äpfel auf dem Baum")
print([token.text for token in doc[2].lefts])  # ['schöne', 'rote']
print([token.text for token in doc[2].rights])  # ['auf']

----------------------------------------

TITLE: Using the Project Pull Command in Bash
DESCRIPTION: Example of using the spaCy CLI to pull project data from a remote storage named 'local'.

LANGUAGE: bash
CODE:
$ python -m spacy project pull local

----------------------------------------

TITLE: Configuring MishWindowEncoder.v1 Architecture in spaCy
DESCRIPTION: Example configuration for the legacy MishWindowEncoder.v1 architecture, which produced a model of type Model[Floats2D, Floats2D]. It encodes context using convolutions with Mish activation, layer normalization, and residual connections.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.MishWindowEncoder.v1"
width = 64
window_size = 1
depth = 4

----------------------------------------

TITLE: Removing Rules from Matcher with Matcher.remove
DESCRIPTION: Demonstrates how to remove a rule from the matcher by its match ID. A KeyError is raised if the match ID does not exist.

LANGUAGE: python
CODE:
matcher.add("Rule", [[{"ORTH": "test"}]])
assert "Rule" in matcher
matcher.remove("Rule")
assert "Rule" not in matcher

----------------------------------------

TITLE: Adding Hashtag Pattern using spaCy Matcher
DESCRIPTION: Creates a pattern to match valid hashtags in text by identifying the '#' character followed by any ASCII token. This snippet shows the basic usage of spaCy's Matcher for pattern matching.

LANGUAGE: python
CODE:
matcher.add("HASHTAG", [[{"ORTH": "#"}, {"IS_ASCII": True}]])

doc = nlp("Hello world 😀 #MondayMotivation")
matches = matcher(doc)
for match_id, start, end in matches:
    string_id = doc.vocab.strings[match_id]  # Look up string ID
    span = doc[start:end]
    print(string_id, span.text)

----------------------------------------

TITLE: Sourcing a Named Entity Recognition Component in spaCy (INI)
DESCRIPTION: Configuration snippet showing how to source a named entity recognition component from an existing model (en_core_web_sm) using spaCy's configuration system. This allows copying a component from one pipeline to another.

LANGUAGE: ini
CODE:
[components.ner]
source = "en_core_web_sm"

----------------------------------------

TITLE: Predicting Entity Links with EntityLinker in spaCy (Python)
DESCRIPTION: Applies the EntityLinker model to predict knowledge base identifiers for entities in a batch of documents without modifying them. Returns KB IDs for each entity, including NIL if no match is found.

LANGUAGE: python
CODE:
entity_linker = nlp.add_pipe("entity_linker")
kb_ids = entity_linker.predict([doc1, doc2])

----------------------------------------

TITLE: Retrieving Morphological Analysis by Hash in Python
DESCRIPTION: Gets the FEATS string representation for a given hash of morphological analysis from the morphology table.

LANGUAGE: python
CODE:
feats = "Feat1=Val1|Feat2=Val2"
hash = nlp.vocab.morphology.add(feats)
assert nlp.vocab.morphology.get(hash) == feats

----------------------------------------

TITLE: Abstract Example of Pipeline Loading Process
DESCRIPTION: Demonstrates the internal steps that spacy.load() executes, including getting the language class, initializing it, adding components to the pipeline, and loading binary data.

LANGUAGE: python
CODE:
cls = spacy.util.get_lang_class(lang)  # 1. Get Language class, e.g. English
nlp = cls()                            # 2. Initialize it
for name in pipeline:
    nlp.add_pipe(name, config={...})   # 3. Add the component to the pipeline
nlp.from_disk(data_path)               # 4. Load in the binary data

----------------------------------------

TITLE: Removing a Character from Tokenizer Suffix Rules in spaCy
DESCRIPTION: Shows how to remove a specific character from the default suffix rules in spaCy's tokenizer. This example removes the backslash-escaped open square bracket from the suffix patterns.

LANGUAGE: python
CODE:
suffixes = list(nlp.Defaults.suffixes)
suffixes.remove("\\\\[")
suffix_regex = spacy.util.compile_suffix_regex(suffixes)
nlp.tokenizer.suffix_search = suffix_regex.search

----------------------------------------

TITLE: Configuration for Custom Logger in spaCy
DESCRIPTION: Config file excerpt showing how to reference a custom logger in the training.logger section with a path parameter for the log file.

LANGUAGE: ini
CODE:
### config.cfg (excerpt)
[training.logger]
@loggers = "my_custom_logger.v1"
log_path = "my_file.tab"

----------------------------------------

TITLE: Initializing SentenceRecognizer for Training in spaCy
DESCRIPTION: Example showing how to initialize the SentenceRecognizer component for training. This demonstrates providing examples for model initialization and validation.

LANGUAGE: python
CODE:
senter = nlp.add_pipe("senter")
senter.initialize(lambda: examples, nlp=nlp)

----------------------------------------

TITLE: Excluding Fields During spaCy Object Serialization in Python
DESCRIPTION: Example of how to exclude specific fields from serialization when saving a spaCy object to disk. In this case, the 'vocab' field is excluded when serializing a span_finder object.

LANGUAGE: python
CODE:
data = span_finder.to_disk("/path", exclude=["vocab"])

----------------------------------------

TITLE: Predicting Entities using EntityRecognizer in spaCy
DESCRIPTION: Shows how to apply the EntityRecognizer model to a batch of Doc objects without modifying them. This returns scores that can be used for further processing.

LANGUAGE: python
CODE:
ner = nlp.add_pipe("ner")
scores = ner.predict([doc1, doc2])

----------------------------------------

TITLE: Adding Informative Inline Comments in Python Code
DESCRIPTION: Examples of helpful inline comments that provide context and explain non-obvious logic in the codebase, making it easier for other developers to understand the code.

LANGUAGE: diff
CODE:
token_index = indices[value]
+ # Index describes Token.i of last token but Span indices are inclusive
span = doc[prev_token_index:token_index + 1]

----------------------------------------

TITLE: Excluding Fields During EditTreeLemmatizer Serialization in spaCy
DESCRIPTION: Example demonstrating how to exclude specific fields when serializing a lemmatizer component to disk. This allows for customized serialization by omitting selected data fields.

LANGUAGE: python
CODE:
data = lemmatizer.to_disk("/path", exclude=["vocab"])

----------------------------------------

TITLE: Using noqa Comments to Disable Specific Flake8 Linting Rules in Python
DESCRIPTION: Examples demonstrating how to disable specific flake8 linting rules using noqa comments. This allows developers to intentionally bypass linting rules when justified in specific situations.

LANGUAGE: python
CODE:
# The imported class isn't used in this file, but imported here, so it can be
# imported *from* here by another module.
from .submodule import SomeClass  # noqa: F401

try:
    do_something()
except:  # noqa: E722
    # This bare except is justified, for some specific reason
    do_something_else()

----------------------------------------

TITLE: Creating a Custom Annotation Setter for spacy-transformers
DESCRIPTION: Example showing how to register a custom annotation setter for the spacy-transformers package. The annotation setter function takes docs and transformer data, and applies annotations to the documents.

LANGUAGE: python
CODE:
import spacy_transformers

@spacy_transformers.registry.annotation_setters("my_annotation_setter.v1")
def configure_custom_annotation_setter():
    def annotation_setter(docs, trf_data) -> None:
       # Set annotations on the docs

    return annotation_setter

----------------------------------------

TITLE: Merging Tokens with Custom Extension Attributes in spaCy
DESCRIPTION: Demonstrates how to register a custom token extension attribute and overwrite it during retokenization. The example merges the tokens 'David' and 'Bowie' and sets the custom is_musician attribute to True for the merged token.

LANGUAGE: python
CODE:
import spacy
from spacy.tokens import Token

# Register a custom token attribute, token._.is_musician
Token.set_extension("is_musician", default=False)

nlp = spacy.load("en_core_web_sm")
doc = nlp("I like David Bowie")
print("Before:", [(token.text, token._.is_musician) for token in doc])

with doc.retokenize() as retokenizer:
    retokenizer.merge(doc[2:4], attrs={"_": {"is_musician": True}})
print("After:", [(token.text, token._.is_musician) for token in doc])

----------------------------------------

TITLE: Configuring Custom Batch Schedule in spaCy Config File
DESCRIPTION: Config file excerpt showing how to reference a custom batch size schedule in the training.batcher.size block, with parameters for starting batch size and growth factor.

LANGUAGE: ini
CODE:
[training.batcher.size]
@schedules = "my_custom_schedule.v1"
start = 2
factor = 1.005

----------------------------------------

TITLE: Scoring Token Attributes in spaCy (Python)
DESCRIPTION: Evaluates the accuracy of a single token attribute (like part-of-speech tags). Tokens with missing values in the reference document are skipped during scoring.

LANGUAGE: python
CODE:
scores = Scorer.score_token_attr(examples, "pos")
print(scores["pos_acc"])

----------------------------------------

TITLE: Accessing spans in a SpanGroup by index
DESCRIPTION: Demonstrates how to retrieve a span from a SpanGroup using indexing. Note that modifications to the returned span don't affect the original in the group.

LANGUAGE: python
CODE:
doc = nlp("Their goi ng home")
doc.spans["errors"] = [doc[0:1], doc[1:3]]
span = doc.spans["errors"][1]
assert span.text == "goi ng"
span.label_ = 'LABEL'
assert doc.spans["errors"][1].label_ != 'LABEL' # The span within the group was not updated

----------------------------------------

TITLE: Example Pipeline Metadata JSON Structure
DESCRIPTION: Shows the structure of a meta.json file that contains metadata about a spaCy pipeline, including name, language, version compatibility, and author information.

LANGUAGE: json
CODE:
{
  "name": "example_pipeline",
  "lang": "en",
  "version": "1.0.0",
  "spacy_version": ">=2.0.0,<3.0.0",
  "description": "Example pipeline for spaCy",
  "author": "You",
  "email": "you@example.com",
  "license": "CC BY-SA 3.0"
}

----------------------------------------

TITLE: Re-raising Exceptions with Context in spaCy
DESCRIPTION: Shows how to properly re-raise exceptions with additional context using the 'from' keyword. This approach maintains the original exception chain for better debugging.

LANGUAGE: python
CODE:
try:
    run_third_party_code_that_might_fail()
except ValueError as e:
+    raise ValueError(Errors.E123) from e

----------------------------------------

TITLE: Serializing EditTreeLemmatizer to Bytes in spaCy
DESCRIPTION: Example showing how to serialize a trainable lemmatizer component to a bytestring using the to_bytes method. This creates a portable binary representation of the component.

LANGUAGE: python
CODE:
lemmatizer = nlp.add_pipe("trainable_lemmatizer", name="lemmatizer")
lemmatizer_bytes = lemmatizer.to_bytes()

----------------------------------------

TITLE: Project Structure of spaCy Website
DESCRIPTION: YAML representation of the spaCy website project structure showing the organization of documentation, metadata, source code, and configuration files.

LANGUAGE: yaml
CODE:
├── docs                 # the actual markdown content
├── meta                 # JSON-formatted site metadata
|   ├── dynamicMeta.js   # At build time generated meta data
|   ├── languages.json   # supported languages and statistical models
|   ├── sidebars.json    # sidebar navigations for different sections
|   ├── site.json        # general site metadata
|   ├── type-annotations.json # Type annotations
|   └── universe.json    # data for the spaCy universe section
├── pages                # Next router pages
├── public               # static images and other assets
├── setup                # Jinja setup
├── src                  # source
|   ├── components       # React components
|   ├── fonts            # webfonts
|   ├── images           # images used in the layout
|   ├── plugins          # custom plugins to transform Markdown
|   ├── styles           # CSS modules and global styles
|   ├── templates        # page layouts
|   |   ├── docs.js      # layout template for documentation pages
|   |   ├── index.js     # global layout template
|   |   ├── models.js    # layout template for model pages
|   |   └── universe.js  # layout templates for universe
|   └── widgets          # non-reusable components with content, e.g. changelog
├── .eslintrc.json       # ESLint config file
├── .nvmrc               # NVM config file
|                        # (to support "nvm use" to switch to correct Node version)
|
├── .prettierrc          # Prettier config file
├── next.config.mjs      # Next config file
├── package.json         # package settings and dependencies
└── tsconfig.json        # TypeScript config file

----------------------------------------

TITLE: Configuring Corpus Reader for Training Data in spaCy
DESCRIPTION: Configuration for the Corpus reader which manages annotated corpora in DocBin (.spacy) format. This can be used for training and development datasets, with options for gold preprocessing, document length limits, and data augmentation.

LANGUAGE: ini
CODE:
[paths]
train = "corpus/train.spacy"

[corpora.train]
@readers = "spacy.Corpus.v1"
path = ${paths.train}
gold_preproc = false
max_length = 0
limit = 0

----------------------------------------

TITLE: Training and Loading Custom PKUSeg Models for Chinese in spaCy
DESCRIPTION: Example of training a custom PKUSeg model for Chinese word segmentation and loading it into spaCy's Chinese tokenizer for specialized domain processing.

LANGUAGE: python
CODE:
import spacy_pkuseg as pkuseg
from spacy.lang.zh import Chinese

# Train pkuseg model
pkuseg.train("train.utf8", "test.utf8", "/path/to/pkuseg_model")

# Load pkuseg model in spaCy Chinese tokenizer
cfg = {"segmenter": "pkuseg"}
nlp = Chinese.from_config({"nlp": {"tokenizer": cfg}})
nlp.tokenizer.initialize(pkuseg_model="/path/to/pkuseg_model")

----------------------------------------

TITLE: Using Variable Interpolation within Strings in spaCy Config
DESCRIPTION: Shows how to use variables inside strings in spaCy config files, similar to Python f-strings. Variables are automatically converted to strings if they are not string values.

LANGUAGE: ini
CODE:
[paths]
version = 5
root = "/Users/you/data"
train = "${paths.root}/train_${paths.version}.spacy"
# Result: /Users/you/data/train_5.spacy

----------------------------------------

TITLE: Installing spaCy with Apple M1 Optimization
DESCRIPTION: Command to install spaCy with Apple's native Accelerate library support for improved performance on M1 Macs (up to 8x faster). This installs the thinc-apple-ops package.

LANGUAGE: bash
CODE:
$ pip install spacy[apple]

----------------------------------------

TITLE: Deserializing Vectors from Bytes in Python
DESCRIPTION: Method for loading vector data from a binary string.

LANGUAGE: python
CODE:
def from_bytes(self, data):

----------------------------------------

TITLE: Pattern Matching for Phone Numbers Using SHAPE in spaCy
DESCRIPTION: Creates a pattern to match phone numbers in the format '(123) 4567 8901' or '(123) 4567-8901'. Uses the SHAPE attribute to identify digit sequences and specific punctuation.

LANGUAGE: python
CODE:
[{"ORTH": "("}, {"SHAPE": "ddd"}, {"ORTH": ")"}, {"SHAPE": "dddd"},
 {"ORTH": "-", "OP": "?"}, {"SHAPE": "dddd"}]

----------------------------------------

TITLE: Creating a Detailed Internal Error Message in spaCy
DESCRIPTION: Example of a comprehensive error message for internal inconsistencies that includes guidance for users to report bugs. This message is defined in the centralized error system.

LANGUAGE: python
CODE:
E161 = ("Found an internal inconsistency when predicting entity links. "
        "This is likely a bug in spaCy, so feel free to open an issue: "
        "https://github.com/explosion/spaCy/issues")

----------------------------------------

TITLE: Structure of Deprecated JSON Training Format
DESCRIPTION: Example structure of the deprecated JSON format for training data. This format includes document ID, paragraphs, sentences, tokens with linguistic annotations, and text categories.

LANGUAGE: python
CODE:
[{
    "id": int,                      # ID of the document within the corpus
    "paragraphs": [{                # list of paragraphs in the corpus
        "raw": string,              # raw text of the paragraph
        "sentences": [{             # list of sentences in the paragraph
            "tokens": [{            # list of tokens in the sentence
                "id": int,          # index of the token in the document
                "dep": string,      # dependency label
                "head": int,        # offset of token head relative to token index
                "tag": string,      # part-of-speech tag
                "orth": string,     # verbatim text of the token
                "ner": string       # BILUO label, e.g. "O" or "B-ORG"
            }],
            "brackets": [{          # phrase structure (NOT USED by current models)
                "first": int,       # index of first token
                "last": int,        # index of last token
                "label": string     # phrase label
            }]
        }],
        "cats": [{                  # new in v2.2: categories for text classifier
            "label": string,        # text category label
            "value": float / bool   # label applies (1.0/true) or not (0.0/false)
        }]
    }]
}]

----------------------------------------

TITLE: Running Basic pytest Commands for spaCy Testing
DESCRIPTION: Examples of different pytest commands to run tests at various levels of granularity, from all tests to specific test functions.

LANGUAGE: bash
CODE:
py.test spacy                        # run basic tests
py.test spacy --slow                 # run basic and slow tests

----------------------------------------

TITLE: Configuring NVTX Range Markers for GPU Profiling in spaCy
DESCRIPTION: Configuration for adding NVIDIA Tools Extension (NVTX) range markers to model operations. This helps with GPU profiling by clearly attributing operations to specific model components during forward and backward passes.

LANGUAGE: ini
CODE:
[nlp]
after_pipeline_creation = {"@callbacks":"spacy.models_with_nvtx_range.v1"}

----------------------------------------

TITLE: Uploading spaCy Models to Hugging Face Hub
DESCRIPTION: Shell commands for packaging a spaCy model and uploading it to the Hugging Face Hub. The process includes logging in, packaging the model as a wheel file, and pushing it to the hub.

LANGUAGE: bash
CODE:
$ huggingface-cli login
$ python -m spacy package ./en_ner_fashion ./output --build wheel
$ cd ./output/en_ner_fashion-0.0.0/dist
$ python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl

----------------------------------------

TITLE: Adding Components to Pipeline in spaCy v3.0
DESCRIPTION: Shows the updated method for adding components to the pipeline, which now takes the string name of the component factory instead of a callable.

LANGUAGE: diff
CODE:
+ @Language.component("my_component")
def my_component(doc):
    return doc

- nlp.add_pipe(my_component)
+ nlp.add_pipe("my_component")

----------------------------------------

TITLE: Finding Verbs with Subjects in spaCy's Dependency Parse
DESCRIPTION: This code identifies verbs with subjects by matching from below in the dependency tree. It finds tokens with subject dependency relations (nsubj) whose heads are verbs, demonstrating efficient tree traversal in spaCy.

LANGUAGE: python
CODE:
import spacy
from spacy.symbols import nsubj, VERB

nlp = spacy.load("en_core_web_sm")
doc = nlp("Autonomous cars shift insurance liability toward manufacturers")

# Finding a verb with a subject from below — good
verbs = set()
for possible_subject in doc:
    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:
        verbs.add(possible_subject.head)
print(verbs)

----------------------------------------

TITLE: Accessing Vector Shape in spaCy (Python)
DESCRIPTION: Demonstrates how to create vectors with a specific shape and access the shape property, which returns a tuple of (rows, dimensions).

LANGUAGE: python
CODE:
vectors = Vectors(shape(1, 300))
vectors.add("cat", numpy.random.uniform(-1, 1, (300,)))
rows, dims = vectors.shape
assert rows == 1
assert dims == 300

----------------------------------------

TITLE: Using DocBin for Efficient Doc Collection Serialization
DESCRIPTION: Illustrates how to use the new DocBin class to efficiently serialize and deserialize collections of Doc objects, including specifying which attributes to store and handling user data.

LANGUAGE: python
CODE:
from spacy.tokens import DocBin
doc_bin = DocBin(attrs=["LEMMA", "ENT_IOB", "ENT_TYPE"], store_user_data=True)
for doc in nlp.pipe(texts):
    doc_bin.add(doc)
bytes_data = doc_bin.to_bytes()
# Deserialize later, e.g. in a new process
nlp = spacy.blank("en")
doc_bin = DocBin().from_bytes(bytes_data)
docs = list(doc_bin.get_docs(nlp.vocab))

----------------------------------------

TITLE: Optimizing Warning Frequency in spaCy
DESCRIPTION: Demonstrates how to avoid raising multiple warnings in a loop by collecting information and raising a single formatted warning. This keeps output clean and manageable.

LANGUAGE: python
CODE:
+ n_empty = 0
for spans in lots_of_annotations:
    if len(spans) == 0:
-       warnings.warn(Warnings.456)
+       n_empty += 1
+ warnings.warn(Warnings.456.format(count=n_empty))

----------------------------------------

TITLE: NLP Object Configuration Example in spaCy
DESCRIPTION: Example configuration for the NLP object in spaCy, which defines the language, pipeline components, and tokenizer settings. This section is crucial for initializing the language processing pipeline.

LANGUAGE: ini
CODE:
[nlp]
lang = "en"
pipeline = ["tagger", "parser", "ner"]
before_creation = null
after_creation = null
after_pipeline_creation = null
batch_size = 1000

[nlp.tokenizer]
@tokenizers = "spacy.Tokenizer.v1"

----------------------------------------

TITLE: Overriding spaCy Config Settings via Command Line
DESCRIPTION: Example demonstrating how to override configuration settings when running spaCy train, allowing for flexibility with file paths and other system-dependent settings without modifying the config file.

LANGUAGE: bash
CODE:
$ python -m spacy train config.cfg --paths.train ./corpus/train.spacy --paths.dev ./corpus/dev.spacy --training.max_epochs 3

----------------------------------------

TITLE: Converting BILUO Tags to IOB Format in spaCy
DESCRIPTION: Shows how to convert BILUO entity tags to IOB format, which is useful when working with models or systems that only support the simpler IOB tagging scheme.

LANGUAGE: python
CODE:
from spacy.training import biluo_to_iob

tags = ["O", "O", "B-LOC", "I-LOC", "L-LOC", "O"]
iob_tags = biluo_to_iob(tags)
assert iob_tags == ["O", "O", "B-LOC", "I-LOC", "I-LOC", "O"]

----------------------------------------

TITLE: Generating a spaCy Pipeline Package using CLI
DESCRIPTION: Shows how to use spaCy's CLI command to package a trained pipeline into an installable Python package. This creates all necessary files including setup.py and MANIFEST.in.

LANGUAGE: bash
CODE:
$ python -m spacy package ./en_example_pipeline ./packages

----------------------------------------

TITLE: Using Type Hints with Component Factory in spaCy
DESCRIPTION: Demonstrates how to create a component factory with type hints in spaCy v3.0, using the Language.factory decorator and pydantic's StrictBool for type validation. This pattern enables type-checking and validation of component parameters.

LANGUAGE: python
CODE:
from spacy.language import Language
from pydantic import StrictBool

@Language.factory("my_component")
def create_my_component(
    nlp: Language,
    name: str,
    custom: StrictBool
):
   ...

----------------------------------------

TITLE: Configuring Few-Shot Example File for spaCy LLM Tasks in INI
DESCRIPTION: Configuration example showing how to set up a few-shot prompting file for the NER task in a spaCy pipeline configuration file. The example defines labels and connects to an external examples file.

LANGUAGE: ini
CODE:
[components.llm.task]
@llm_tasks = "spacy.NER.v2"
labels = PERSON,ORGANISATION,LOCATION
[components.llm.task.examples]
@misc = "spacy.FewShotReader.v1"
path = "ner_examples.yml"

----------------------------------------

TITLE: Loading a custom pipe from bytes in spaCy
DESCRIPTION: Shows how to restore a pipeline component from a bytestring, which may have been stored in a database or received over a network connection.

LANGUAGE: python
CODE:
pipe_bytes = pipe.to_bytes()
pipe = nlp.add_pipe("your_custom_pipe")
pipe.from_bytes(pipe_bytes)

----------------------------------------

TITLE: Knowledge Base YAML File Format for Entity Linking
DESCRIPTION: Example structure for a knowledge base YAML file used by the KBFileLoader.v1 component. The file defines entities with unique IDs, names, and descriptions, as well as aliases with optional prior probabilities.

LANGUAGE: yaml
CODE:
entities:
  # The key should be whatever ID identifies this entity uniquely in your knowledge base.
  ID1:
      name: "..."
      desc: "..."
  ID2:
      ...
# Data on aliases in your knowledge base - e. g. "Apple" for the entity "Apple Inc.".
aliases:
  - alias: "..."
    # List of all entities that this alias refers to.
    entities: ["ID1", "ID2", ...]
    # Optional: prior probabilities that this alias refers to the n-th entity in the "entities" attribute.
    probabilities: [0.5, 0.2, ...]
  - alias: "..."
    entities: [...]
    probabilities: [...]
  ...

----------------------------------------

TITLE: Testing for warnings in spaCy
DESCRIPTION: Demonstrates how to test that a function raises or doesn't raise specific warnings using pytest.warns contextmanager with the PhraseMatcher.

LANGUAGE: python
CODE:
def test_phrase_matcher_validation(en_vocab):
    doc1 = Doc(en_vocab, words=["Test"], deps=["ROOT"])
    doc2 = Doc(en_vocab, words=["Test"])
    matcher = PhraseMatcher(en_vocab, validate=True)
    with pytest.warns(UserWarning):
        # Warn about unnecessarily parsed document
        matcher.add("TEST1", [doc1])
    with pytest.warns(None) as record:
        matcher.add("TEST2", [docs])
        assert not record.list

----------------------------------------

TITLE: Pulling Project Outputs from Remote Storage using Bash
DESCRIPTION: Command for downloading project outputs from a remote storage location defined in the project.yml file. Only downloads files that don't exist locally, considering command strings and dependency hashes.

LANGUAGE: bash
CODE:
$ python -m spacy project pull [remote] [project_dir]

LANGUAGE: bash
CODE:
$ python -m spacy project pull my_bucket

LANGUAGE: yaml
CODE:
### project.yml
remotes:
  my_bucket: 's3://my-spacy-bucket'

----------------------------------------

TITLE: Registering a Custom Callback Function for Adding Stop Words in spaCy
DESCRIPTION: This snippet shows how to create and register a callback function that adds a custom stop word to the language defaults. The function is registered using the @spacy.registry.callbacks decorator and can be referenced in a config file.

LANGUAGE: ini
CODE:
[nlp.before_creation]
@callbacks = "customize_language_data"

LANGUAGE: python
CODE:
import spacy

@spacy.registry.callbacks("customize_language_data")
def create_callback():
    def customize_language_data(lang_cls):
        lang_cls.Defaults.stop_words.add("good")
        return lang_cls

    return customize_language_data

----------------------------------------

TITLE: Converting IOB Tags to BILUO Tags in spaCy
DESCRIPTION: Function to convert a sequence of IOB (Inside-Outside-Beginning) tags to BILUO (Beginning-Inside-Last-Unit-Outside) tags for named entity recognition. This is useful when you need to use IOB tags with a model that only supports BILUO format.

LANGUAGE: python
CODE:
from spacy.training import iob_to_biluo

tags = ["O", "O", "B-LOC", "I-LOC", "O"]
biluo_tags = iob_to_biluo(tags)
assert biluo_tags == ["O", "O", "B-LOC", "L-LOC", "O"]

----------------------------------------

TITLE: Including Custom Functions when Packaging a Pipeline
DESCRIPTION: Shows how to include custom components or functions when packaging a spaCy pipeline, using the --code flag to include Python files with registered functions.

LANGUAGE: bash
CODE:
$ python -m spacy package ./en_example_pipeline ./packages --code functions.py

----------------------------------------

TITLE: Accessing lemma data in Turkish without explicit lookup installation
DESCRIPTION: Example showing code that now requires the lookups data to be explicitly installed. This demonstrates how to get lemmas for tokens in a Turkish document.

LANGUAGE: python
CODE:
nlp = Turkish()
doc = nlp("Bu bir cümledir.")
# 🚨 This now requires the lookups data to be installed explicitly
print([token.lemma_ for token in doc])

----------------------------------------

TITLE: Basic Usage of spaCy Debug Config Command in Bash
DESCRIPTION: The basic command for debugging a spaCy configuration file. This command validates the config.cfg file and shows any validation errors.

LANGUAGE: bash
CODE:
$ python -m spacy debug config [config_path] [--code] [--show-functions] [--show-variables] [overrides]

----------------------------------------

TITLE: Installing spaCy with Transformer Support
DESCRIPTION: Command to install spaCy with transformer support using pip. This is required to use the transformer component in your spaCy pipeline.

LANGUAGE: bash
CODE:
$ pip install -U %%SPACY_PKG_NAME[transformers] %%SPACY_PKG_FLAGS

----------------------------------------

TITLE: Loading a custom pipe from disk in spaCy
DESCRIPTION: Shows how to load a previously saved pipeline component from disk using the from_disk method. This restores the component's state including any trained models or configurations.

LANGUAGE: python
CODE:
pipe = nlp.add_pipe("your_custom_pipe")
pipe.from_disk("/path/to/pipe")

----------------------------------------

TITLE: Running GPU Tests with Custom Thinc Branch
DESCRIPTION: Command for executing spaCy slow GPU tests while specifying a custom Thinc branch. The branch can be either a named branch or an unmerged PR reference.

LANGUAGE: markdown
CODE:
@explosion-bot please test_slow_gpu --thinc-branch <branch_name>

----------------------------------------

TITLE: Migrating from Doc and GoldParse to Example API
DESCRIPTION: Shows how to convert code that used the old GoldParse API to the new Example-based API in spaCy v3.0 for entity recognition annotations.

LANGUAGE: diff
CODE:
doc = nlp.make_doc("Mark Zuckerberg is the CEO of Facebook")
entities = [(0, 15, "PERSON"), (30, 38, "ORG")]
- gold = GoldParse(doc, entities=entities)
+ example = Example.from_dict(doc, {"entities": entities})

----------------------------------------

TITLE: Initialization Configuration in spaCy Config File
DESCRIPTION: Config file excerpt showing the initialization section where data resources like word vectors and pretrained tok2vec weights are specified.

LANGUAGE: ini
CODE:
[initialize]
vectors = ${paths.vectors}
init_tok2vec = ${paths.init_tok2vec}

[initialize.components]
# Settings for components

----------------------------------------

TITLE: Creating Executable Code Blocks in Markdown
DESCRIPTION: Shows how to create executable code blocks using the executable attribute. These blocks are powered by Binder and Juniper, allowing users to run the code directly in the documentation.

LANGUAGE: markdown
CODE:
```python
### {executable="true"}
import spacy
nlp = spacy.load("en_core_web_sm")
```

----------------------------------------

TITLE: Replacing Listeners in Frozen Components for spaCy
DESCRIPTION: Shows how to configure a frozen component to replace shared token-to-vector listener layers with independent copies, preventing performance degradation when other components sharing the same layer are updated.

LANGUAGE: ini
CODE:
[training]
frozen_components = ["tagger"]

[components.tagger]
source = "en_core_web_sm"
replace_listeners = ["model.tok2vec"]

----------------------------------------

TITLE: Pruning Word Vectors in spaCy to Optimize Memory Usage
DESCRIPTION: This example demonstrates how to prune word vectors to reduce memory usage while maintaining coverage. It loads a model with pre-trained vectors, prunes them to a specified number, and verifies the pruning operation.

LANGUAGE: python
CODE:
nlp = spacy.load("en_core_web_lg")
n_vectors = 105000  # number of vectors to keep
removed_words = nlp.vocab.prune_vectors(n_vectors)

assert len(nlp.vocab.vectors) <= n_vectors  # unique vectors have been pruned
assert nlp.vocab.vectors.n_keys > n_vectors  # but not the total entries

----------------------------------------

TITLE: Setting up syntax tree navigation in spaCy Doc objects
DESCRIPTION: Shows how to use set_children_from_heads to establish parent-child relationships in the syntax tree after modifying token head attributes. This function must be called to ensure consistent parse tree navigation.

LANGUAGE: python
CODE:
from spacy.tokens.doc cimport Doc, set_children_from_heads
from spacy.vocab cimport Vocab

doc = Doc(Vocab(), words=["Baileys", "from", "a", "shoe"])
doc.c[0].head = 0
doc.c[1].head = 0
doc.c[2].head = 3
doc.c[3].head = 1
set_children_from_heads(doc.c, doc.length)
assert doc.c[3].l_kids == 1

----------------------------------------

TITLE: Exporting Dependency Visualizations as SVG with spaCy
DESCRIPTION: This code snippet demonstrates how to save a dependency visualization as an SVG file. SVG format allows for easy manipulation with CSS or JavaScript and can be embedded in web pages or converted to other formats.

LANGUAGE: python
CODE:
svg = displacy.render(doc, style="dep")
output_path = Path("/images/sentence.svg")
output_path.open("w", encoding="utf-8").write(svg)

----------------------------------------

TITLE: Cloning a spaCy Project Template using Command Line
DESCRIPTION: Command that clones an existing project template from the spaCy projects repository. This copies tagger and parser project files to a local directory for customization.

LANGUAGE: bash
CODE:
python -m spacy project clone pipelines/tagger_parser_ud

----------------------------------------

TITLE: Migrating Model Loading in spaCy v3.0
DESCRIPTION: Demonstrates how to update code that loads spaCy language models. Instead of using shortcuts like 'en', users should specify the full model name like 'en_core_web_sm'.

LANGUAGE: diff
CODE:
- nlp = spacy.load("en")
+ nlp = spacy.load("en_core_web_sm")

----------------------------------------

TITLE: Custom Batcher Configuration
DESCRIPTION: Configuration snippet showing how to specify a custom batching strategy in a training config file. This example configures a custom batcher with a specific batch size for training examples.

LANGUAGE: ini
CODE:
[training.batcher]
@batchers = "filtering_batch.v1"
size = 150

----------------------------------------

TITLE: Embedding GitHub Code using JSX
DESCRIPTION: Shows how to embed GitHub code using the GitHubCode component. The component takes a URL to a GitHub file and language for syntax highlighting.

LANGUAGE: jsx
CODE:
<GitHubCode url="https://github.com/..." lang="python" />

----------------------------------------

TITLE: Configuring Annotating Components in spaCy Pipeline
DESCRIPTION: Configuration excerpt showing how to specify annotating components that set their annotations during training. This example shows a parser providing DEP features to a tagger.

LANGUAGE: ini
CODE:
[nlp]
pipeline = ["parser", "tagger"]

[components.tagger.model.tok2vec.embed]
@architectures = "spacy.MultiHashEmbed.v1"
width = ${components.tagger.model.tok2vec.encode.width}
attrs = ["NORM","DEP"]
rows = [5000,2500]
include_static_vectors = false

[training]
annotating_components = ["parser"]

----------------------------------------

TITLE: Configuring spaCy-LLM Logger with StreamHandler in Python
DESCRIPTION: Sets up the spacy-llm logger to output debug information to the console. This shows how to add a StreamHandler to the logger and set the appropriate log level to view prompts and responses.

LANGUAGE: python
CODE:
import logging
import spacy_llm


spacy_llm.logger.addHandler(logging.StreamHandler())
spacy_llm.logger.setLevel(logging.DEBUG)

----------------------------------------

TITLE: Setting lexeme flag values in spaCy
DESCRIPTION: Demonstrates how to set a binary flag attribute on a lexeme using the c_set_flag static method. This example sets the IS_STOP flag to false (0).

LANGUAGE: python
CODE:
from spacy.attrs cimport IS_STOP
from spacy.lexeme cimport Lexeme

lexeme = doc.c[3].lex
Lexeme.c_set_flag(lexeme, IS_STOP, 0)

----------------------------------------

TITLE: Setting Annotations with SpanFinder in spaCy
DESCRIPTION: Modify documents using pre-computed scores from the SpanFinder model's predictions.

LANGUAGE: python
CODE:
span_finder = nlp.add_pipe("span_finder")
scores = span_finder.predict(docs)
span_finder.set_annotations(docs, scores)

----------------------------------------

TITLE: Running a Project Command using Command Line
DESCRIPTION: Command to execute a specific command defined in the project.yml file, in this case the 'preprocess' command.

LANGUAGE: bash
CODE:
$ python -m spacy project run preprocess

----------------------------------------

TITLE: Configuration for Named Entity Recognition Labels in spaCy
DESCRIPTION: Config file excerpt showing how to specify a JSON file containing label data for the named entity recognizer component to initialize faster.

LANGUAGE: ini
CODE:
[initialize.components.ner]

[initialize.components.ner.labels]
@readers = "spacy.read_labels.v1"
path = "corpus/labels/ner.json

----------------------------------------

TITLE: Example of Creating Training Data with Doc Objects
DESCRIPTION: Code snippet demonstrating the beginning of creating training data with Doc objects. This is part of the internal training API section showing how to initialize training data.

LANGUAGE: python
CODE:
words = ["I", "like", "stuff"]
predicted = Doc(vocab, words=words)

----------------------------------------

TITLE: Ignoring Linter Rules in Python with noqa Comments
DESCRIPTION: Examples of using noqa comments to selectively ignore flake8 linter warnings for special cases where exceptions to the rules are justified.

LANGUAGE: python
CODE:
# The imported class isn't used in this file, but imported here, so it can be
# imported *from* here by another module.
from .submodule import SomeClass  # noqa: F401

try:
    do_something()
except:  # noqa: E722
    # This bare except is justified, for some specific reason
    do_something_else()

----------------------------------------

TITLE: Configuring Static Batch Size in spaCy Training
DESCRIPTION: Configuration example showing how to set a static batch size value for training using the batch_by_words batcher.

LANGUAGE: ini
CODE:
[training.batcher]
@batchers = "spacy.batch_by_words.v1"
size = 3000

----------------------------------------

TITLE: Implementing the Score Method for Evaluation
DESCRIPTION: The score method calculates performance metrics for the relation extraction component on a set of examples. It returns precision, recall, and F-score metrics that can be used during training to monitor model performance.

LANGUAGE: python
CODE:
def score(self, examples: Iterable[Example]) -> Dict[str, Any]:
    prf = PRFScore()
    for example in examples:
        ...

    return {
        "rel_micro_p": prf.precision,
        "rel_micro_r": prf.recall,
        "rel_micro_f": prf.fscore,
    }

----------------------------------------

TITLE: Excluding Fields During SentenceRecognizer Serialization in spaCy
DESCRIPTION: Shows how to exclude specific data fields during serialization of a SentenceRecognizer component.

LANGUAGE: python
CODE:
data = senter.to_disk("/path", exclude=["vocab"])

----------------------------------------

TITLE: Creating an Optimizer for EntityRecognizer
DESCRIPTION: Shows how to create an optimizer specifically for the EntityRecognizer pipeline component. This is used for updating the model during training.

LANGUAGE: python
CODE:
ner = nlp.add_pipe("ner")
optimizer = ner.create_optimizer()

----------------------------------------

TITLE: Downloading Assets from URL in spaCy Project YAML
DESCRIPTION: Configuration for downloading data assets from a public HTTPS URL or Google Cloud Storage bucket in a spaCy project. Specifies destination paths, URLs, checksums, and optional download flags.

LANGUAGE: yaml
CODE:
assets:
  # Download from public HTTPS URL
  - dest: 'assets/training.spacy'
    url: 'https://example.com/data.spacy'
    checksum: '63373dd656daa1fd3043ce166a59474c'
  # Optional download from Google Cloud Storage bucket
  - dest: 'assets/development.spacy'
    extra: True
    url: 'gs://your-bucket/corpora'
    checksum: '5113dc04e03f079525edd8df3f4f39e3'

----------------------------------------

TITLE: Configuring Dynamic Batch Size with Compounding in spaCy
DESCRIPTION: Configuration example showing how to use a registered function (compounding.v1) to create a dynamic batch size that increases during training according to a compounding schedule.

LANGUAGE: ini
CODE:
[training.batcher.size]
@schedules = "compounding.v1"
start = 100
stop = 1000
compound = 1.001

----------------------------------------

TITLE: Loading a spaCy Pipeline from Disk
DESCRIPTION: Demonstrates how to load a saved spaCy pipeline using spacy.load() with a path to the pipeline directory. This loads the configuration and model data.

LANGUAGE: python
CODE:
nlp = spacy.load("/path/to/pipeline")

----------------------------------------

TITLE: Replacing Lambda Functions with Named Functions
DESCRIPTION: Shows how to avoid lambda functions by using properly defined named functions, improving code readability and enabling proper type annotations.

LANGUAGE: diff
CODE:
- split_string: Callable[[str], List[str]] = lambda value: [v.strip() for v in value.split(",")]

+ def split_string(value: str) -> List[str]:
+     return [v.strip() for v in value.split(",")]

----------------------------------------

TITLE: Running spaCy Train with Custom Code
DESCRIPTION: This bash command demonstrates how to run spaCy training with a custom code file that contains registered callbacks. The --code argument points to the Python file that defines the custom functions.

LANGUAGE: bash
CODE:
$ python -m spacy train config.cfg --output ./output --code ./functions.py

----------------------------------------

TITLE: Loading Language Class in spaCy
DESCRIPTION: Utility function to import and load a Language class using the two-letter language code. This allows for lazy-loading of language data and importing languages by their code rather than direct import.

LANGUAGE: python
CODE:
for lang_id in ["en", "de"]:
    lang_class = util.get_lang_class(lang_id)
    lang = lang_class()

----------------------------------------

TITLE: Deserializing Sentencizer from Bytes
DESCRIPTION: Example showing how to load a Sentencizer's configuration from a serialized bytestring. This modifies the existing Sentencizer object in place and returns it.

LANGUAGE: Python
CODE:
sentencizer_bytes = sentencizer.to_bytes()
sentencizer = nlp.add_pipe("sentencizer")
sentencizer.from_bytes(sentencizer_bytes)

----------------------------------------

TITLE: Using the Retokenizer Context Manager for Token Merging
DESCRIPTION: Demonstrates the new Doc.retokenize context manager that allows merging spans of multiple tokens into a single token. Modifications to tokenization are stored and applied when the context manager exits.

LANGUAGE: python
CODE:
doc = nlp("I like David Bowie")
with doc.retokenize() as retokenizer:
    attrs = {"LEMMA": "David Bowie"}
    retokenizer.merge(doc[2:4], attrs=attrs)

----------------------------------------

TITLE: Building spaCy in Editable Mode with setup.py
DESCRIPTION: Legacy method using setup.py to build spaCy in editable mode with parallel compilation. This approach is no longer recommended but can be useful in certain scenarios.

LANGUAGE: bash
CODE:
$ pip install -r requirements.txt
$ python setup.py build_ext --inplace -j 4
$ python setup.py develop

----------------------------------------

TITLE: Using Frozen Components with Annotations in spaCy Pipeline
DESCRIPTION: Configuration showing how to use frozen components like a pre-trained NER and a sentencizer to provide required annotations (doc.sents and doc.ents) to an entity linker during training.

LANGUAGE: ini
CODE:
[nlp]
pipeline = ["sentencizer", "ner", "entity_linker"]

[components.ner]
source = "en_core_web_sm"

[training]
frozen_components = ["ner"]
annotating_components = ["sentencizer", "ner"]

----------------------------------------

TITLE: Using Quantifiers in spaCy Pattern Matching
DESCRIPTION: Example showing how to use quantifiers in token patterns to specify optional tokens or sequences, allowing for more flexible matching.

LANGUAGE: python
CODE:
pattern = [{"LOWER": "hello"},
           {"IS_PUNCT": True, "OP": "?"}]

----------------------------------------

TITLE: Chaining TransformerListener Architecture in Python
DESCRIPTION: Code showing how TransformerListener is chained with trfs2arrays in the listener architecture. This demonstrates the structure that needs to be replaced when converting to a standalone component.

LANGUAGE: python
CODE:
model = chain(
    TransformerListener(upstream_name=upstream)
    trfs2arrays(pooling, grad_factor),
)

----------------------------------------

TITLE: Generating Label Data for spaCy Components using CLI
DESCRIPTION: Command line example showing how to use the 'init labels' command to auto-generate JSON files containing label data for spaCy components from training data.

LANGUAGE: bash
CODE:
$ python -m spacy init labels config.cfg ./corpus --paths.train ./corpus/train.spacy

----------------------------------------

TITLE: Parsing Entities for Manual Visualization
DESCRIPTION: Converts a spaCy Doc object into a named entity dictionary format that can be used with displaCy's manual rendering mode.

LANGUAGE: python
CODE:
import spacy
from spacy import displacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("But Google is starting from behind.")
ents_parse = displacy.parse_ents(doc)
html = displacy.render(ents_parse, style="ent", manual=True)

----------------------------------------

TITLE: Using Label Data for Morphologizer Initialization in spaCy (Python)
DESCRIPTION: Shows how to retrieve label metadata and use it to initialize a new Morphologizer component with a pre-defined label set.

LANGUAGE: python
CODE:
labels = morphologizer.label_data
morphologizer.initialize(lambda: [], nlp=nlp, labels=labels)

----------------------------------------

TITLE: Accessing Environment Variables in spaCy Project Commands
DESCRIPTION: Example of how to define and use environment variables in spaCy project commands by mapping them in the env dictionary and referencing them in scripts.

LANGUAGE: yaml
CODE:
env:
  ENV_PATH: PATH

LANGUAGE: yaml
CODE:
- name: 'echo-path'
  script:
    - 'echo ${env.ENV_PATH}'

----------------------------------------

TITLE: Locating and Testing spaCy Installation
DESCRIPTION: Commands to find the spaCy installation directory, install test requirements, and run tests with pytest. This approach allows running tests on an installed version of spaCy.

LANGUAGE: bash
CODE:
$ python -c "import os; import spacy; print(os.path.dirname(spacy.__file__))"
$ pip install -r path/to/requirements.txt
$ python -m pytest --pyargs %%SPACY_PKG_NAME

----------------------------------------

TITLE: Additional Regular Expression Pattern Examples in spaCy
DESCRIPTION: Examples demonstrating various ways to use regular expressions with different token attributes including text, POS tags, and custom attributes.

LANGUAGE: python
CODE:
# Match different spellings of token texts
pattern = [{"TEXT": {"REGEX": "deff?in[ia]tely"}}]

# Match tokens with fine-grained POS tags starting with 'V'
pattern = [{"TAG": {"REGEX": "^V"}}]

# Match custom attribute values with regular expressions
pattern = [{"_": {"country": {"REGEX": "^[Uu](nited|\\.?) ?[Ss](tates|\\.?)$"}}}]

----------------------------------------

TITLE: Using PhraseMatcher for Efficient Terminology Matching in spaCy
DESCRIPTION: Shows how to use spaCy's PhraseMatcher to efficiently match terminology lists against text. This approach is more efficient than token patterns for large lists of multi-token terms.

LANGUAGE: python
CODE:
import spacy
from spacy.matcher import PhraseMatcher

nlp = spacy.load("en_core_web_sm")
matcher = PhraseMatcher(nlp.vocab)
terms = ["Barack Obama", "Angela Merkel", "Washington, D.C."]
# Only run nlp.make_doc to speed things up
patterns = [nlp.make_doc(text) for text in terms]
matcher.add("TerminologyList", patterns)

doc = nlp("German Chancellor Angela Merkel and US President Barack Obama "
          "converse in the Oval Office inside the White House in Washington, D.C.")
matches = matcher(doc)
for match_id, start, end in matches:
    span = doc[start:end]
    print(span.text)

----------------------------------------

TITLE: Defining Callback Functions for Different spaCy Versions in Python
DESCRIPTION: Code demonstrating how to define replacement callback functions with different signatures for compatibility with spaCy 3.7 and earlier versions, which handle different numbers of arguments.

LANGUAGE: python
CODE:
def replace_listener_pre_37(copied_tok2vec_model):
  ...

def replace_listener_post_37(copied_tok2vec_model, replaced_listener, tok2vec_pipe):
  ...

----------------------------------------

TITLE: Configuring Entity Recognizer with Partial Incorrect Annotations in spaCy
DESCRIPTION: This configuration excerpt shows how to set up the NER component to use partial incorrect annotations. It defines the key where incorrect spans will be stored and other NER-specific settings.

LANGUAGE: ini
CODE:
[components.ner]
factory = "ner"
incorrect_spans_key = "incorrect_spans"
moves = null
update_with_oracle_cut_size = 100

----------------------------------------

TITLE: Sample output of NER project workflow execution
DESCRIPTION: Example terminal output showing the execution of the NER project workflow, including data conversion, configuration creation, model initialization, and training with performance metrics at different steps.

LANGUAGE: none
CODE:
ℹ Running workflow 'all'

================================== convert ==================================
Running command: /home/user/venv/bin/python scripts/convert.py en assets/train.json corpus/train.spacy
Running command: /home/user/venv/bin/python scripts/convert.py en assets/dev.json corpus/dev.spacy

=============================== create-config ===============================
Running command: /home/user/venv/bin/python -m spacy init config --lang en --pipeline ner configs/config.cfg --force
ℹ Generated config template specific for your use case
- Language: en
- Pipeline: ner
- Optimize for: efficiency
- Hardware: CPU
- Transformer: None
✔ Auto-filled config with all values
✔ Saved config
configs/config.cfg
You can now add your data and train your pipeline:
python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy

=================================== train ===================================
Running command: /home/user/venv/bin/python -m spacy train configs/config.cfg --output training/ --paths.train corpus/train.spacy --paths.dev corpus/dev.spacy --training.eval_frequency 10 --training.max_steps 100 --gpu-id -1
ℹ Using CPU

=========================== Initializing pipeline ===========================
[2021-03-11 19:34:59,101] [INFO] Set up nlp object from config
[2021-03-11 19:34:59,109] [INFO] Pipeline: ['tok2vec', 'ner']
[2021-03-11 19:34:59,113] [INFO] Created vocabulary
[2021-03-11 19:34:59,113] [INFO] Finished initializing nlp object
[2021-03-11 19:34:59,265] [INFO] Initialized pipeline components: ['tok2vec', 'ner']
✔ Initialized pipeline

============================= Training pipeline =============================
ℹ Pipeline: ['tok2vec', 'ner']
ℹ Initial learn rate: 0.001
E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  ------------  --------  ------  ------  ------  ------
  0       0          0.00      7.90    0.00    0.00    0.00    0.00
 10      10          0.11     71.07    0.00    0.00    0.00    0.00
 20      20          0.65     22.44   50.00   50.00   50.00    0.50
 30      30          0.22      6.38   80.00   66.67  100.00    0.80
 40      40          0.00      0.00   80.00   66.67  100.00    0.80
 50      50          0.00      0.00   80.00   66.67  100.00    0.80
 60      60          0.00      0.00  100.00  100.00  100.00    1.00
 70      70          0.00      0.00  100.00  100.00  100.00    1.00
 80      80          0.00      0.00  100.00  100.00  100.00    1.00
 90      90          0.00      0.00  100.00  100.00  100.00    1.00
100     100          0.00      0.00  100.00  100.00  100.00    1.00
✔ Saved pipeline to output directory
training/model-last

----------------------------------------

TITLE: Checking if a Token Has a Vector in spaCy
DESCRIPTION: Example showing how to check if a token has a word vector associated with it using the has_vector property. Returns a boolean indicating whether vector data is attached to the token.

LANGUAGE: python
CODE:
doc = nlp("I like apples")
apples = doc[2]
assert apples.has_vector

----------------------------------------

TITLE: Converting Span to Numpy Array in spaCy Python
DESCRIPTION: Exports token attributes to a numpy array. Given a list of attribute IDs, creates an array of shape (N, M) where N is the length of the span and M is the number of attributes. Values are converted to 32-bit integers.

LANGUAGE: python
CODE:
from spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA
doc = nlp("I like New York in Autumn.")
span = doc[2:3]
# All strings mapped to integers, for easy export to numpy
np_array = span.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA])

----------------------------------------

TITLE: Error: Missing Model Module
DESCRIPTION: Shows the import error displayed when a user tries to import a spaCy model that isn't installed in the current environment.

LANGUAGE: python
CODE:
ImportError: No module named 'en_core_web_sm'

----------------------------------------

TITLE: Using Extended Pattern Syntax with Dictionary Properties in spaCy
DESCRIPTION: Examples demonstrating how to create token patterns that map to dictionaries of properties, allowing for more complex matching criteria such as lemma lists, token length, and morphological attributes.

LANGUAGE: python
CODE:
# Matches "love cats" or "likes flowers"
pattern1 = [{"LEMMA": {"IN": ["like", "love"]}},
            {"POS": "NOUN"}]

# Matches tokens of length >= 10
pattern2 = [{"LENGTH": {">=": 10}}]

# Match based on morph attributes
pattern3 = [{"MORPH": {"IS_SUBSET": ["Number=Sing", "Gender=Neut"]}}]
# "", "Number=Sing" and "Number=Sing|Gender=Neut" will match as subsets
# "Number=Plur|Gender=Neut" will not match
# "Number=Sing|Gender=Neut|Polite=Infm" will not match because it's a superset

----------------------------------------

TITLE: Token Shape Matching with PhraseMatcher in spaCy
DESCRIPTION: Shows how to match patterns based on token shapes using the PhraseMatcher with attr='SHAPE'. This technique is useful for identifying tokens with similar patterns like IP addresses without specifying exact values.

LANGUAGE: python
CODE:
from spacy.lang.en import English
from spacy.matcher import PhraseMatcher

nlp = English()
matcher = PhraseMatcher(nlp.vocab, attr="SHAPE")
matcher.add("IP", [nlp("127.0.0.1"), nlp("127.127.0.0")])

doc = nlp("Often the router will have an IP address such as 192.168.1.1 or 192.168.2.1.")
for match_id, start, end in matcher(doc):
    print("Matched based on token shape:", doc[start:end])

----------------------------------------

TITLE: Setting spaCy Config Overrides via Environment Variables
DESCRIPTION: Example showing how to use the SPACY_CONFIG_OVERRIDES environment variable to override config settings, which is useful for automated training processes. Environment variables take precedence over CLI overrides.

LANGUAGE: bash
CODE:
$ SPACY_CONFIG_OVERRIDES="--system.gpu_allocator pytorch --training.max_epochs 3" ./your_script.sh

----------------------------------------

TITLE: Using the spaCy Assemble CLI Command
DESCRIPTION: This bash command demonstrates how to use the new spaCy assemble CLI command to create a pipeline from a config file without training. This is useful for assembling pipelines with custom tokenizers or rule-based components.

LANGUAGE: bash
CODE:
$ python -m spacy assemble config.cfg ./output

----------------------------------------

TITLE: Finding Function Definition Command in spaCy CLI
DESCRIPTION: Command to find the module, path and line number for a given registered function, which helps understand where functions used in config files are defined.

LANGUAGE: bash
CODE:
$ python -m spacy find-function [func_name] [--registry]

----------------------------------------

TITLE: Few-Shot Learning YAML Examples for Sentiment Analysis
DESCRIPTION: YAML file format for providing few-shot examples to the Sentiment LLM task. Each example contains a text and corresponding sentiment score between 0 and 1.

LANGUAGE: yaml
CODE:
- text: 'This is horrifying.'
  score: 0
- text: 'This is underwhelming.'
  score: 0.25
- text: 'This is ok.'
  score: 0.5
- text: "I'm looking forward to this!"
  score: 1.0

----------------------------------------

TITLE: Implementing Custom Component with Serialization Methods in spaCy
DESCRIPTION: This code defines a custom pipeline component for spaCy that implements to_disk and from_disk methods for serialization. The component stores arbitrary JSON-serializable data and can save/load this data when the pipeline is serialized.

LANGUAGE: python
CODE:
import json
from spacy import Language
from spacy.util import ensure_path

@Language.factory("my_component")
class CustomComponent:
    def __init__(self, nlp: Language, name: str = "my_component"):
        self.name = name
        self.data = []

    def __call__(self, doc):
        # Do something to the doc here
        return doc

    def add(self, data):
        # Add something to the component's data
        self.data.append(data)

    def to_disk(self, path, exclude=tuple()):
        # This will receive the directory path + /my_component
        path = ensure_path(path)
        if not path.exists():
            path.mkdir()
        data_path = path / "data.json"
        with data_path.open("w", encoding="utf8") as f:
            f.write(json.dumps(self.data))

    def from_disk(self, path, exclude=tuple()):
        # This will receive the directory path + /my_component
        data_path = path / "data.json"
        with data_path.open("r", encoding="utf8") as f:
            self.data = json.load(f)
        return self

----------------------------------------

TITLE: Using Registered Functions for Complex Config Values in spaCy v3.0
DESCRIPTION: Shows how to use registered functions for passing complex objects as component configuration values to ensure JSON serializability.

LANGUAGE: diff
CODE:
- config = {"model": MyTaggerModel()}
+ config= {"model": {"@architectures": "MyTaggerModel"}}
tagger = nlp.add_pipe("tagger", config=config)

----------------------------------------

TITLE: Defining a Training Command in project.yml
DESCRIPTION: YAML configuration for a training command in a spaCy project, specifying the script to execute, dependencies, and output locations.

LANGUAGE: yaml
CODE:
- name: train
  help: 'Train a spaCy pipeline using the specified corpus and config'
  script:
    - 'spacy train ./config.cfg --output training/'
  deps:
    - 'corpus/train'
    - 'corpus/dev'
    - 'config.cfg'
  outputs:
    - 'training/model-best'

----------------------------------------

TITLE: Pipeline Processing Flow in spaCy
DESCRIPTION: Demonstrates how the pipeline processes text under the hood. It shows document creation and sequential application of each component in the pipeline.

LANGUAGE: python
CODE:
doc = nlp.make_doc("This is a sentence")  # Create a Doc from raw text
for name, proc in nlp.pipeline:           # Iterate over components in order
    doc = proc(doc)                       # Apply each component

----------------------------------------

TITLE: Configurable Callback with Arguments for Stop Words Customization in spaCy
DESCRIPTION: This code demonstrates how to create a callback function that accepts configurable arguments. It allows specifying additional stop words and a debug flag through the config file, making customization more flexible without code changes.

LANGUAGE: ini
CODE:
[nlp.before_creation]
@callbacks = "customize_language_data"
extra_stop_words = ["ooh", "aah"]
debug = true

LANGUAGE: python
CODE:
from typing import List
import spacy

@spacy.registry.callbacks("customize_language_data")
def create_callback(extra_stop_words: List[str] = [], debug: bool = False):
    def customize_language_data(lang_cls):
        lang_cls.Defaults.stop_words.update(extra_stop_words)
        if debug:
            print("Updated stop words")
        return lang_cls

    return customize_language_data

----------------------------------------

TITLE: Using the 'apply' CLI Command in spaCy v3.5
DESCRIPTION: The 'apply' CLI command applies a spaCy pipeline to text files and saves the annotated documents to a single .spacy file. It supports .txt, .jsonl, and .spacy input files.

LANGUAGE: bash
CODE:
$ spacy apply en_core_web_sm my_texts/ output.spacy

----------------------------------------

TITLE: Configuring NER Labels Initialization in spaCy Config
DESCRIPTION: Example configuration block showing how to initialize NER component labels by reading from a JSON file. This is used in the [initialize] section of a spaCy config file.

LANGUAGE: ini
CODE:
[initialize.components.ner]

[initialize.components.ner.labels]
@readers = "spacy.read_labels.v1"
path = "corpus/labels/ner.json

----------------------------------------

TITLE: Configuring OpenAI Model with LangChain in spaCy INI
DESCRIPTION: Example configuration for using an OpenAI model via LangChain in a spaCy pipeline. Specifies the model name, query mechanism, and temperature setting.

LANGUAGE: ini
CODE:
[components.llm.model]
@llm_models = "langchain.OpenAI.v1"
name = "gpt-3.5-turbo"
query = {"@llm_queries": "spacy.CallLangChain.v1"}
config = {"temperature": 0.0}

----------------------------------------

TITLE: Implementing a Tokenizer Customization Callback in Python
DESCRIPTION: Defines a registered callback function that customizes tokenizer behavior by removing a suffix and adding a special case token, which can be used during pipeline initialization.

LANGUAGE: python
CODE:
from spacy.util import registry, compile_suffix_regex

@registry.callbacks("customize_tokenizer")
def make_customize_tokenizer():
    def customize_tokenizer(nlp):
        # remove a suffix
        suffixes = list(nlp.Defaults.suffixes)
        suffixes.remove("\\[")
        suffix_regex = compile_suffix_regex(suffixes)
        nlp.tokenizer.suffix_search = suffix_regex.search

        # add a special case
        nlp.tokenizer.add_special_case("_SPECIAL_", [{"ORTH": "_SPECIAL_"}])
    return customize_tokenizer

----------------------------------------

TITLE: Uploading spaCy Pipelines to Hugging Face Hub using CLI
DESCRIPTION: This snippet demonstrates the command sequence for installing the spacy-huggingface-hub package, logging in to Hugging Face, packaging a spaCy model, and uploading it to the Hugging Face Hub.

LANGUAGE: bash
CODE:
$ pip install spacy-huggingface-hub
$ huggingface-cli login
$ python -m spacy package ./en_ner_fashion ./output --build wheel
$ cd ./output/en_ner_fashion-0.0.0/dist
$ python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl

----------------------------------------

TITLE: Example Project Directory Structure in spaCy
DESCRIPTION: Example directory structure for a complete spaCy project, showing the organization of configuration files, assets, scripts, outputs, and other project components.

LANGUAGE: yaml
CODE:
├── project.yml          # the project settings
├── project.lock         # lockfile that tracks inputs/outputs
├── assets/              # downloaded data assets
├── configs/             # pipeline config.cfg files used for training
├── corpus/              # output directory for training corpus
├── metas/               # pipeline meta.json templates used for packaging
├── metrics/             # output directory for evaluation metrics
├── notebooks/           # directory for Jupyter notebooks
├── packages/            # output directory for pipeline Python packages
├── scripts/             # directory for scripts, e.g. referenced in commands
├── training/            # output directory for trained pipelines
└── ...                  # any other files, like a requirements.txt etc.

----------------------------------------

TITLE: Disabling the Parser in spaCy for Performance
DESCRIPTION: This snippet shows how to disable the parser component when loading a spaCy model for improved performance. This is useful when syntactic information is not needed in the pipeline.

LANGUAGE: python
CODE:
nlp = spacy.load("en_core_web_sm", disable=["parser"])

----------------------------------------

TITLE: Configuration for Custom Neural Network Architecture in spaCy
DESCRIPTION: Config file excerpt showing how to use a custom neural network architecture for a tagger component, specifying the architecture name and output width parameter.

LANGUAGE: ini
CODE:
### config.cfg
[components.tagger]
factory = "tagger"

[components.tagger.model]
@architectures = "custom_neural_network.v1"
output_width = 512

----------------------------------------

TITLE: Creating Abbreviations with JSX Components in spaCy Documentation
DESCRIPTION: How to create abbreviations with explanatory tooltips using the Abbr JSX component in the spaCy documentation.

LANGUAGE: jsx
CODE:
<Abbr title="Explanation">Abbreviation</Abbr>

----------------------------------------

TITLE: Checking Rule Existence in DependencyMatcher in spaCy (Python)
DESCRIPTION: Shows how to check if a specific rule ID exists in a DependencyMatcher. The example creates a matcher, verifies a rule ID doesn't exist, adds the rule, and then confirms the rule ID is now present in the matcher.

LANGUAGE: python
CODE:
matcher = DependencyMatcher(nlp.vocab)
assert "FOUNDED" not in matcher
matcher.add("FOUNDED", [pattern])
assert "FOUNDED" in matcher

----------------------------------------

TITLE: Demonstrating Prompt Sharding Example in spacy-llm
DESCRIPTION: This example shows how prompts are split (sharded) when the document text is too long for the model's context window. It demonstrates the sharding process with a simple sentiment analysis prompt and a 25-token context window.

LANGUAGE: python
CODE:
Estimate the sentiment of this text:
"{text}"
Estimated sentiment:

----------------------------------------

TITLE: Defining a Token Pattern in spaCy
DESCRIPTION: Example of a pattern definition for matching a sequence of tokens in spaCy. This pattern matches the word "hello", followed by punctuation, followed by the word "world".

LANGUAGE: python
CODE:
[{"LOWER": "hello"}, {"IS_PUNCT": True}, {"LOWER": "world"}]

----------------------------------------

TITLE: Loading Custom Vectors Configuration Example
DESCRIPTION: Example of how to implement custom vectors by extending the BaseVectors abstract class, particularly for BPEmb subword embeddings.

LANGUAGE: bash
CODE:
$ python -m spacy init fill-config config-v3.6.cfg config-v3.7.cfg

----------------------------------------

TITLE: Implementing the Predict Method for Relation Extraction
DESCRIPTION: The predict method applies the relation extraction model to a batch of documents and returns the predictions. It delegates to the model's predict function and ensures the output is a proper array.

LANGUAGE: python
CODE:
def predict(self, docs: Iterable[Doc]) -> Floats2d:
    predictions = self.model.predict(docs)
    return self.model.ops.asarray(predictions)

----------------------------------------

TITLE: Accessing Entity Identifiers in spaCy's Entity Linking
DESCRIPTION: This example shows how to access knowledge base identifiers for entities after processing text with a custom entity linking pipeline. It demonstrates accessing entity IDs at both document and token levels, retrieving the entity text, label, and KB identifier.

LANGUAGE: python
CODE:
import spacy

# "my_custom_el_pipeline" is assumed to be a custom NLP pipeline that was trained and serialized to disk
nlp = spacy.load("my_custom_el_pipeline")
doc = nlp("Ada Lovelace was born in London")

# Document level
ents = [(e.text, e.label_, e.kb_id_) for e in doc.ents]
print(ents)  # [('Ada Lovelace', 'PERSON', 'Q7259'), ('London', 'GPE', 'Q84')]

# Token level
ent_ada_0 = [doc[0].text, doc[0].ent_type_, doc[0].ent_kb_id_]
ent_ada_1 = [doc[1].text, doc[1].ent_type_, doc[1].ent_kb_id_]
ent_london_5 = [doc[5].text, doc[5].ent_type_, doc[5].ent_kb_id_]
print(ent_ada_0)  # ['Ada', 'PERSON', 'Q7259']
print(ent_ada_1)  # ['Lovelace', 'PERSON', 'Q7259']
print(ent_london_5)  # ['London', 'GPE', 'Q84']

----------------------------------------

TITLE: Defining Orthographic Variants in JSON
DESCRIPTION: JSON structure defining replacement rules for text variations. It includes 'single' token replacements (like ellipses) and 'paired' token replacements (like quotation marks) that the augmenter will use during training.

LANGUAGE: json
CODE:
{
  "single": [{ "tags": ["NFP"], "variants": ["…", "..."] }],
  "paired": [
    {
      "tags": ["``, "''"],
      "variants": [
        ["'", "'"],
        ["'", "'"] 
      ]
    }
  ]
}

----------------------------------------

TITLE: Using the spaCy Matcher with Line Highlighting
DESCRIPTION: Demonstrates how to use spaCy's pattern matcher with highlighted important lines. This example shows defining patterns to match specific text sequences in natural language text.

LANGUAGE: python
CODE:
import spacy
from spacy.matcher import Matcher

nlp = spacy.load('en_core_web_sm')
matcher = Matcher(nlp.vocab)
pattern = [{"LOWER": "hello"}, {"IS_PUNCT": True}, {"LOWER": "world"}]
matcher.add("HelloWorld", None, pattern)
doc = nlp("Hello, world! Hello world!")
matches = matcher(doc)

----------------------------------------

TITLE: Disabling Black Formatting for Specific Code Blocks in Python
DESCRIPTION: Example showing how to disable black formatting for specific code blocks using fmt: off/on comments. This is useful for maintaining readability in certain contexts like language data files or test constructs.

LANGUAGE: python
CODE:
# fmt: off
text = "I look forward to using Thingamajig.  I've been told it will make my life easier..."
heads = [1, 1, 1, 1, 3, 4, 1, 6, 11, 11, 11, 11, 14, 14, 11, 16, 17, 14, 11]
deps = ["nsubj", "ROOT", "advmod", "prep", "pcomp", "dobj", "punct", "",
        "nsubjpass", "aux", "auxpass", "ROOT", "nsubj", "aux", "ccomp",
        "poss", "nsubj", "ccomp", "punct"]
# fmt: on

----------------------------------------

TITLE: Processing Document Stream with EntityLinker.pipe in spaCy
DESCRIPTION: Shows how to process a stream of documents using the EntityLinker.pipe method, which is more efficient for batch processing with a configurable batch size.

LANGUAGE: python
CODE:
entity_linker = nlp.add_pipe("entity_linker")
for doc in entity_linker.pipe(docs, batch_size=50):
    pass

----------------------------------------

TITLE: Setting Cohere API Key in Shell Environment
DESCRIPTION: Sets the environment variable for authenticating with Cohere's language model services. This credential is required when using Cohere models in spaCy.

LANGUAGE: shell
CODE:
export CO_API_KEY="..."

----------------------------------------

TITLE: Using Shape Inference in Thinc Models
DESCRIPTION: Demonstrates Thinc's shape inference capabilities where dimensions of intermediate layers can be inferred automatically. Only the hidden width is specified while input and output dimensions are inferred.

LANGUAGE: python
CODE:
with Model.define_operators({">>" : chain}):
    layers = (
        Relu(hidden_width, width)
        >> Dropout(dropout)
        >> Relu(hidden_width)
        >> Dropout(dropout)
        >> Softmax(nO)
    )

----------------------------------------

TITLE: Implementing the Update Method for Training the Relation Extractor
DESCRIPTION: The update method handles model training by processing a batch of examples, making predictions, calculating loss and gradients, and updating the model. This is called during training to improve the model's performance based on examples.

LANGUAGE: python
CODE:
def update(
    self,
    examples: Iterable[Example],
    *,
    drop: float = 0.0,
    sgd: Optional[Optimizer] = None,
    losses: Optional[Dict[str, float]] = None,
) -> Dict[str, float]:
    # ...
    docs = [eg.predicted for eg in examples]
    predictions, backprop = self.model.begin_update(docs)
    loss, gradient = self.get_loss(examples, predictions)
    backprop(gradient)
    losses[self.name] += loss
    # ...
    return losses

----------------------------------------

TITLE: Registering a Custom Whitespace Tokenizer for Training in spaCy
DESCRIPTION: Demonstrates how to register a whitespace tokenizer in spaCy's registry system to make it available during training. This allows the tokenizer to be specified in the training configuration file.

LANGUAGE: python
CODE:
@spacy.registry.tokenizers("whitespace_tokenizer")
def create_whitespace_tokenizer():
    def create_tokenizer(nlp):
        return WhitespaceTokenizer(nlp.vocab)

    return create_tokenizer

----------------------------------------

TITLE: Defining Few-Shot Examples for Summarization Task in YAML
DESCRIPTION: YAML structure for defining few-shot examples for the Summarization task. Each example contains a text to be summarized and the expected summary output to guide the LLM.

LANGUAGE: yaml
CODE:
- text: >
    The United Nations, referred to informally as the UN, is an
    intergovernmental organization whose stated purposes are  to maintain
    international peace and security, develop friendly relations among nations,
    achieve international cooperation, and serve as a centre for harmonizing the
    actions of nations. It is the world's largest international organization.
    The UN is headquartered on international territory in New York City, and the
    organization has other offices in Geneva, Nairobi, Vienna, and The Hague,
    where the International Court of Justice is headquartered.\n\n The UN was
    established after World War II with the aim of preventing future world wars,
    and succeeded the League of  Nations, which was characterized as
    ineffective.
  summary:
    'The UN is an international organization that promotes global peace,
    cooperation, and harmony. Established after WWII, its purpose is to prevent
    future world wars.'

----------------------------------------

TITLE: Disabling POS Tagging and Lemmatization in spaCy
DESCRIPTION: Code example showing how to disable part-of-speech tagging and lemmatization components when loading a spaCy pipeline. This is useful for faster processing when these features aren't needed.

LANGUAGE: python
CODE:
# Note: English doesn't include a morphologizer
nlp = spacy.load("en_core_web_sm", disable=["tagger", "attribute_ruler", "lemmatizer"])
nlp = spacy.load("en_core_web_trf", disable=["tagger", "attribute_ruler", "lemmatizer"])

----------------------------------------

TITLE: Creating Ordered Lists in Markdown
DESCRIPTION: Shows how to create ordered lists in Markdown format. The example demonstrates a simple numbered list with two items.

LANGUAGE: markdown
CODE:
1. One
2. Two

----------------------------------------

TITLE: Training EntityLinker Component in spaCy (Python)
DESCRIPTION: Updates the EntityLinker model based on a batch of examples, learning both entity linking capabilities and context encoding. Returns a dictionary of training losses.

LANGUAGE: python
CODE:
entity_linker = nlp.add_pipe("entity_linker")
optimizer = nlp.initialize()
losses = entity_linker.update(examples, sgd=optimizer)

----------------------------------------

TITLE: Setting Parser Annotations in spaCy Documents
DESCRIPTION: Shows how to apply pre-computed dependency parsing scores to modify Doc objects with the dependency structure.

LANGUAGE: python
CODE:
parser = nlp.add_pipe("parser")
scores = parser.predict([doc1, doc2])
parser.set_annotations([doc1, doc2], scores)

----------------------------------------

TITLE: Implementing the Relation Candidate Generator in Python
DESCRIPTION: Creates a function that generates candidate entity pairs for relation classification based on a maximum token distance. This helps filter out unlikely entity pairs when documents contain many entities.

LANGUAGE: python
CODE:
@spacy.registry.misc("rel_instance_generator.v1")
def create_instances(max_length: int) -> Callable[[Doc], List[Tuple[Span, Span]]]:
    def get_candidates(doc: "Doc") -> List[Tuple[Span, Span]]:
        candidates = []
        for ent1 in doc.ents:
            for ent2 in doc.ents:
                if ent1 != ent2:
                    if max_length and abs(ent2.start - ent1.start) <= max_length:
                        candidates.append((ent1, ent2))
        return candidates
    return get_candidates

----------------------------------------

TITLE: Registering the Custom Doc Extension Attribute in Python
DESCRIPTION: Registers a custom extension attribute on the Doc class to store relation data. The attribute stores a dictionary of entity pairs mapped to their relation probabilities.

LANGUAGE: python
CODE:
from spacy.tokens import Doc
Doc.set_extension("rel", default={})

----------------------------------------

TITLE: Updating Matcher Pattern Adding in spaCy v3.0
DESCRIPTION: Shows the updated method for adding patterns to a Matcher, which now requires a list of patterns as the second argument and an optional on_match callback.

LANGUAGE: diff
CODE:
matcher = Matcher(nlp.vocab)
patterns = [[{"TEXT": "Google"}, {"TEXT": "Now"}], [{"TEXT": "GoogleNow"}]]
- matcher.add("GoogleNow", on_match, *patterns)
+ matcher.add("GoogleNow", patterns, on_match=on_match)

----------------------------------------

TITLE: Few-Shot Learning Examples in YAML format for NER Tasks
DESCRIPTION: Example YAML file structure for providing few-shot learning examples to the NER tasks. Each example contains text and corresponding entity annotations grouped by label type.

LANGUAGE: yaml
CODE:
- text: Jack and Jill went up the hill.
  entities:
    PERSON:
      - Jack
      - Jill
    LOCATION:
      - hill
- text: Jack fell down and broke his crown.
  entities:
    PERSON:
      - Jack

----------------------------------------

TITLE: Switching from Rule-based to Lookup Lemmatization in spaCy
DESCRIPTION: Code example demonstrating how to replace a trainable or rule-based lemmatizer with a lookup lemmatizer for specific language models. This requires the spacy-lookups-data package.

LANGUAGE: python
CODE:
# Requirements: pip install spacy-lookups-data
nlp = spacy.load("en_core_web_sm")
nlp.remove_pipe("lemmatizer")
nlp.add_pipe("lemmatizer", config={"mode": "lookup"}).initialize()

----------------------------------------

TITLE: Creating Aside Components in Markdown
DESCRIPTION: Shows how to create aside components using Markdown format. Asides are displayed in the right-hand column and are created using blockquotes with level 4 headlines as titles.

LANGUAGE: markdown
CODE:
> #### Aside title
>
> This is aside text.

----------------------------------------

TITLE: Configuring CoreferenceResolver Component with Custom Settings
DESCRIPTION: Example showing how to configure the CoreferenceResolver component with custom settings when adding it to the spaCy pipeline.

LANGUAGE: python
CODE:
from spacy_experimental.coref.coref_component import DEFAULT_COREF_MODEL
from spacy_experimental.coref.coref_util import DEFAULT_CLUSTER_PREFIX
config={
    "model": DEFAULT_COREF_MODEL,
    "span_cluster_prefix": DEFAULT_CLUSTER_PREFIX,
}
nlp.add_pipe("experimental_coref", config=config)

----------------------------------------

TITLE: Configuring and Adding EditTreeLemmatizer to spaCy Pipeline
DESCRIPTION: Example showing how to configure and add the trainable lemmatizer to a spaCy pipeline using the default edit tree lemmatizer model.

LANGUAGE: python
CODE:
from spacy.pipeline.edit_tree_lemmatizer import DEFAULT_EDIT_TREE_LEMMATIZER_MODEL
config = {"model": DEFAULT_EDIT_TREE_LEMMATIZER_MODEL}
nlp.add_pipe("trainable_lemmatizer", config=config, name="lemmatizer")

----------------------------------------

TITLE: Creating Spans from Character Offsets in spaCy
DESCRIPTION: This snippet demonstrates how to create a named entity span from character offsets rather than token indices, using the Doc.char_span method in spaCy.

LANGUAGE: python
CODE:
fb_ent = doc.char_span(0, 2, label="ORG")

----------------------------------------

TITLE: Example of Relation Output Format in Python
DESCRIPTION: Demonstrates how to access and interpret the relation predictions stored on the Doc object. The output shows entity spans and the probability scores for different relation types between entity pairs.

LANGUAGE: python
CODE:
doc = nlp("Amsterdam is the capital of the Netherlands.")
print("spans", [(e.start, e.text, e.label_) for e in doc.ents])
for value, rel_dict in doc._.rel.items():
    print(f"{value}: {rel_dict}")

# spans [(0, 'Amsterdam', 'LOC'), (6, 'Netherlands', 'LOC')]
# (0, 6): {'CAPITAL_OF': 0.89, 'LOCATED_IN': 0.75, 'UNRELATED': 0.002}
# (6, 0): {'CAPITAL_OF': 0.01, 'LOCATED_IN': 0.13, 'UNRELATED': 0.017}

----------------------------------------

TITLE: Installing spaCy in Editable Mode for Development
DESCRIPTION: Commands to install spaCy in editable mode for development purposes. This setup allows changes to Python files to be reflected immediately while requiring recompilation for Cython files.

LANGUAGE: bash
CODE:
$ pip install -r requirements.txt
$ pip install --no-build-isolation --editable .

----------------------------------------

TITLE: Configuring REL v1 Task in spaCy
DESCRIPTION: Configuration for the REL task which extracts relationships between named entities. This example defines two relationship types: LivesIn and Visits.

LANGUAGE: ini
CODE:
[components.llm.task]
@llm_tasks = "spacy.REL.v1"
labels = ["LivesIn", "Visits"]

----------------------------------------

TITLE: Disabling Components Except NER in Transformer-based spaCy Models
DESCRIPTION: This snippet demonstrates how to load a transformer-based spaCy model while keeping only the transformer and NER components. Unlike standard models, the transformer component is retained as NER depends on it.

LANGUAGE: python
CODE:
nlp = spacy.load("en_core_web_trf", disable=["tagger", "parser", "attribute_ruler", "lemmatizer"])

----------------------------------------

TITLE: Installing spaCy with pip and updating dependencies
DESCRIPTION: Commands to update pip, setuptools, and wheel, then install the latest version of spaCy using pip.

LANGUAGE: bash
CODE:
$ pip install -U pip setuptools wheel
$ pip install -U %%SPACY_PKG_NAME%%SPACY_PKG_FLAGS

----------------------------------------

TITLE: Creating Optimizer for CuratedTransformer in spaCy
DESCRIPTION: Creates an optimizer for the CuratedTransformer pipeline component to be used during training.

LANGUAGE: python
CODE:
trf = nlp.add_pipe("curated_transformer")
optimizer = trf.create_optimizer()

----------------------------------------

TITLE: Implementing the Set Annotations Method for Relation Extraction
DESCRIPTION: This method stores the model's predictions in the document's custom attribute 'rel'. It maps each prediction to the appropriate entity pair by using the model's get_instances function to identify relevant candidates.

LANGUAGE: python
CODE:
def set_annotations(self, docs: Iterable[Doc], predictions: Floats2d):
    c = 0
    get_instances = self.model.attrs["get_instances"]
    for doc in docs:
        for (e1, e2) in get_instances(doc):
            offset = (e1.start, e2.start)
            if offset not in doc._.rel:
                doc._.rel[offset] = {}
            for j, label in enumerate(self.labels):
                doc._.rel[offset][label] = predictions[c, j]
            c += 1

----------------------------------------

TITLE: Using Custom Word Vectors for Similarity in spaCy
DESCRIPTION: This code snippet demonstrates how to load a spaCy pipeline with custom word vectors and use them to calculate similarity between documents. It loads Latin word vectors and compares two Latin sentences.

LANGUAGE: python
CODE:
nlp_latin = spacy.load("/tmp/la_vectors_wiki_lg")
doc1 = nlp_latin("Caecilius est in horto")
doc2 = nlp_latin("servus est in atrio")
doc1.similarity(doc2)

----------------------------------------

TITLE: Creating a Chain Model with Typed Layers in Thinc
DESCRIPTION: This snippet shows how to create a model by chaining multiple layers with type annotations. It demonstrates Thinc's type system for ensuring compatibility between layers, specifying input and output types for each component in the chain.

LANGUAGE: python
CODE:
from typing import List
from thinc.api import Model, chain
from thinc.types import Floats2d
def chain_model(
    tok2vec: Model[List[Doc], List[Floats2d]],
    layer1: Model[List[Floats2d], Floats2d],
    layer2: Model[Floats2d, Floats2d]
) -> Model[List[Doc], Floats2d]:
    model = chain(tok2vec, layer1, layer2)
    return model

----------------------------------------

TITLE: Initializing a Vocab instance in spaCy
DESCRIPTION: Creates a new Vocab object with optional predefined strings. The Vocab object stores lexemes and provides lookup capabilities for accessing them.

LANGUAGE: python
CODE:
from spacy.vocab import Vocab
vocab = Vocab(strings=["hello", "world"])

----------------------------------------

TITLE: Accessing Vectors with External Word List for Floret Vectors
DESCRIPTION: Code diff showing how to modify a workflow when using floret vectors instead of default vectors. Since floret vectors don't include a fixed word list, an external word list must be used instead of iterating over vocabulary orthography.

LANGUAGE: diff
CODE:
- lexemes = [nlp.vocab[orth] for orth in nlp.vocab.vectors]
+ lexemes = [nlp.vocab[word] for word in external_word_list]

----------------------------------------

TITLE: Explicit Variable Type Annotation in Python
DESCRIPTION: Shows the preferred format for annotating variable types in Python, using the explicit annotation syntax rather than the comment-based format.

LANGUAGE: diff
CODE:
- var = value    # type: Type
+ var: Type = value

----------------------------------------

TITLE: Installing spaCy with extra dependencies
DESCRIPTION: Example command showing how to install spaCy with additional dependencies (lookups and transformers) specified in brackets.

LANGUAGE: bash
CODE:
$ pip install %%SPACY_PKG_NAME[lookups,transformers]%%SPACY_PKG_FLAGS

----------------------------------------

TITLE: Configuring TextCatEnsemble.v1 in spaCy
DESCRIPTION: Example configuration for the spaCy.TextCatEnsemble.v1 architecture which stacks a bag-of-words model and a neural network model with an internal CNN Tok2Vec layer. This version builds internal tok2vec and linear_model components.

LANGUAGE: ini
CODE:
[model]
@architectures = "spacy.TextCatEnsemble.v1"
exclusive_classes = false
pretrained_vectors = null
width = 64
embed_size = 2000
conv_depth = 2
window_size = 1
ngram_size = 1
dropout = null
nO = null

----------------------------------------

TITLE: Implementing BPEmb Subword Embeddings for spaCy Vectors
DESCRIPTION: A complete implementation of a custom vectors class for spaCy that supports BPEmb subword embeddings. The class extends BaseVectors and implements all required methods including vector retrieval and batch processing. Includes registration of the vectors implementation for use in spaCy pipelines.

LANGUAGE: python
CODE:
# requires: pip install bpemb
import warnings
from pathlib import Path
from typing import Callable, Optional, cast

from bpemb import BPEmb
from thinc.api import Ops, get_current_ops
from thinc.backends import get_array_ops
from thinc.types import Floats2d

from spacy.strings import StringStore
from spacy.util import registry
from spacy.vectors import BaseVectors
from spacy.vocab import Vocab


class BPEmbVectors(BaseVectors):
    def __init__(
        self,
        *,
        strings: Optional[StringStore] = None,
        lang: Optional[str] = None,
        vs: Optional[int] = None,
        dim: Optional[int] = None,
        cache_dir: Optional[Path] = None,
        encode_extra_options: Optional[str] = None,
        model_file: Optional[Path] = None,
        emb_file: Optional[Path] = None,
    ):
        kwargs = {}
        if lang is not None:
            kwargs["lang"] = lang
        if vs is not None:
            kwargs["vs"] = vs
        if dim is not None:
            kwargs["dim"] = dim
        if cache_dir is not None:
            kwargs["cache_dir"] = cache_dir
        if encode_extra_options is not None:
            kwargs["encode_extra_options"] = encode_extra_options
        if model_file is not None:
            kwargs["model_file"] = model_file
        if emb_file is not None:
            kwargs["emb_file"] = emb_file
        self.bpemb = BPEmb(**kwargs)
        self.strings = strings
        self.name = repr(self.bpemb)
        self.n_keys = -1
        self.mode = "BPEmb"
        self.to_ops(get_current_ops())

    def __contains__(self, key):
        return True

    def is_full(self):
        return True

    def add(self, key, *, vector=None, row=None):
        warnings.warn(
            (
                "Skipping BPEmbVectors.add: the bpemb vector table cannot be "
                "modified. Vectors are calculated from bytepieces."
            )
        )
        return -1

    def __getitem__(self, key):
        return self.get_batch([key])[0]

    def get_batch(self, keys):
        keys = [self.strings.as_string(key) for key in keys]
        bp_ids = self.bpemb.encode_ids(keys)
        ops = get_array_ops(self.bpemb.emb.vectors)
        indices = ops.asarray(ops.xp.hstack(bp_ids), dtype="int32")
        lengths = ops.asarray([len(x) for x in bp_ids], dtype="int32")
        vecs = ops.reduce_mean(cast(Floats2d, self.bpemb.emb.vectors[indices]), lengths)
        return vecs

    @property
    def shape(self):
        return self.bpemb.vectors.shape

    def __len__(self):
        return self.shape[0]

    @property
    def vectors_length(self):
        return self.shape[1]

    @property
    def size(self):
        return self.bpemb.vectors.size

    def to_ops(self, ops: Ops):
        self.bpemb.emb.vectors = ops.asarray(self.bpemb.emb.vectors)


@registry.vectors("BPEmbVectors.v1")
def create_bpemb_vectors(
    lang: Optional[str] = "multi",
    vs: Optional[int] = None,
    dim: Optional[int] = None,
    cache_dir: Optional[Path] = None,
    encode_extra_options: Optional[str] = None,
    model_file: Optional[Path] = None,
    emb_file: Optional[Path] = None,
) -> Callable[[Vocab], BPEmbVectors]:
    def bpemb_vectors_factory(vocab: Vocab) -> BPEmbVectors:
        return BPEmbVectors(
            strings=vocab.strings,
            lang=lang,
            vs=vs,
            dim=dim,
            cache_dir=cache_dir,
            encode_extra_options=encode_extra_options,
            model_file=model_file,
            emb_file=emb_file,
        )

    return bpemb_vectors_factory

----------------------------------------

TITLE: Pruning Vectors with spaCy CLI Command
DESCRIPTION: This bash command shows how to use the spaCy CLI to initialize word vectors with pruning. It creates a blank spaCy pipeline with vectors for only the specified number of most frequent words.

LANGUAGE: bash
CODE:
$ python -m spacy init vectors en la.300d.vec.tgz /tmp/la_vectors_web_md --prune 10000

----------------------------------------

TITLE: Error: spaCy Command Not Found
DESCRIPTION: Shows the error that occurs when the shell cannot find the spaCy command, which happens because spaCy doesn't add itself to the PATH variable.

LANGUAGE: bash
CODE:
command not found: spacy

----------------------------------------

TITLE: Configuring Strided Spans in SpaCy Transformer Config
DESCRIPTION: Sample configuration for using strided_spans.v1 span getter, which processes documents using fixed-size windows with configurable overlap. Parameters control window size and stride length for overlapping spans.

LANGUAGE: ini
CODE:
[transformer.model.get_spans]
@span_getters = "spacy-transformers.strided_spans.v1"
window = 128
stride = 96

----------------------------------------

TITLE: Creating Issue-Specific Test Functions in Python with Pytest
DESCRIPTION: This code snippet demonstrates how to properly create test functions for bug fixes in spaCy using pytest. It shows how to mark a test with a specific issue number using pytest.mark.issue decorator to reference the GitHub issue being fixed.

LANGUAGE: python
CODE:
# Assume you're fixing Issue #1234
@pytest.mark.issue(1234)
def test_issue1234():
    ...

----------------------------------------

TITLE: Using Centralized Error Messages in spaCy
DESCRIPTION: Demonstrates how to replace inline error messages with references to centralized error codes from the Errors class. Shows both static error messages and those requiring formatting with variables.

LANGUAGE: python
CODE:
if something_went_wrong:
-    raise ValueError("Something went wrong!")
+    raise ValueError(Errors.E123)

if not isinstance(value, int):
-    raise ValueError(f"Unexpected value: {value}")
+    raise ValueError(Errors.E456.format(value=value))

----------------------------------------

TITLE: Installing spaCy from Source with Extra Features
DESCRIPTION: Command to install spaCy from source with additional features such as lookup tables and CUDA support for GPU acceleration, which extends the functionality of the base installation.

LANGUAGE: bash
CODE:
pip install --no-build-isolation --editable .[lookups,cuda102]

----------------------------------------

TITLE: Matching Regular Expressions on Full Text in spaCy
DESCRIPTION: Executable example demonstrating how to match regular expressions on the full document text rather than individual tokens, using re.finditer and Doc.char_span to create valid token spans.

LANGUAGE: python
CODE:
import spacy
import re

nlp = spacy.load("en_core_web_sm")
doc = nlp("The United States of America (USA) are commonly known as the United States (U.S. or US) or America.")

expression = r"[Uu](nited|\.) ?[Ss](tates|\.)" 
for match in re.finditer(expression, doc.text):
    start, end = match.span()
    span = doc.char_span(start, end)
    # This is a Span object or None if match doesn't map to valid token sequence
    if span is not None:
        print("Found match:", span.text)

----------------------------------------

TITLE: Running spaCy debug-data CLI command with training and development data
DESCRIPTION: This command uses spaCy's debug-data utility to analyze and validate English language training and development data. It helps identify potential issues in your dataset before training a model, such as cyclic dependencies, invalid annotations, or low data labels.

LANGUAGE: bash
CODE:
$ python -m spacy debug-data en train.json dev.json

----------------------------------------

TITLE: Combining Regex and Fuzzy Matching with Lists in spaCy
DESCRIPTION: Examples demonstrating how to combine REGEX and FUZZY operators with IN and NOT_IN list attributes for more flexible matching patterns.

LANGUAGE: python
CODE:
pattern = [{"TEXT": {"FUZZY": {"IN": ["awesome", "cool", "wonderful"]}}}]

pattern = [{"TEXT": {"REGEX": {"NOT_IN": ["^awe(some)?$", "^wonder(ful)?"]}}}]

----------------------------------------

TITLE: Configuring Tok2Vec with Default Model in Python
DESCRIPTION: Example showing how to configure a Tok2Vec component with the default model using the config parameter when adding the component to the spaCy pipeline.

LANGUAGE: python
CODE:
from spacy.pipeline.tok2vec import DEFAULT_TOK2VEC_MODEL
config = {"model": DEFAULT_TOK2VEC_MODEL}
nlp.add_pipe("tok2vec", config=config)

----------------------------------------

TITLE: Project Entry JSON Template for spaCy Universe
DESCRIPTION: The JSON template structure required for submitting a project to the spaCy Universe collection. This template includes all possible fields with examples of how to format each one properly.

LANGUAGE: json
CODE:
{
    "id": "unique-project-id",
    "title": "Project title",
    "slogan": "A short summary",
    "description": "A longer description – *Markdown allowed!*",
    "github": "user/repo",
    "pip": "package-name",
    "code_example": [
        "import spacy",
        "import package_name",
        "",
        "nlp = spacy.load('en')",
        "nlp.add_pipe(package_name)"
    ],
    "code_language": "python",
    "url": "https://example.com",
    "thumb": "https://example.com/thumb.jpg",
    "image": "https://example.com/image.jpg",
    "author": "Your Name",
    "author_links": {
        "twitter": "username",
        "github": "username",
        "website": "https://example.com"
    },
    "category": ["pipeline", "standalone"],
    "tags": ["some-tag", "etc"]
}

----------------------------------------

TITLE: Configuring Sentencizer Pipeline Component in Python
DESCRIPTION: Example showing how to configure a Sentencizer with custom settings and add it to the spaCy pipeline. The config parameter allows specifying custom punctuation characters that mark sentence ends.

LANGUAGE: Python
CODE:
config = {"punct_chars": None}
nlp.add_pipe("sentencizer", config=config)

----------------------------------------

TITLE: Filtering warnings in spaCy tests
DESCRIPTION: Shows how to use the pytest.mark.filterwarnings decorator to ignore specific warnings that are expected but not relevant to the test being performed.

LANGUAGE: python
CODE:
@pytest.mark.filterwarnings("ignore:\\[W036")
def test_matcher_empty(en_vocab):
    matcher = Matcher(en_vocab)
    matcher(Doc(en_vocab, words=["test"]))

----------------------------------------

TITLE: Command-Line Configuration Override Example for spaCy Training
DESCRIPTION: Example command that demonstrates how to override configuration values from the command line when running 'spacy train'. This is particularly useful for specifying data paths without hardcoding them in the config file.

LANGUAGE: bash
CODE:
$ python -m spacy train config.cfg --paths.train ./corpus/train.spacy

----------------------------------------

TITLE: Creating Custom Pipeline Components for HTML Cleanup in spaCy
DESCRIPTION: Implements a custom pipeline component that identifies and merges HTML line break tags (<br>, <BR/>). It uses a matcher to find patterns, merges the tokens, and flags them with a custom token extension attribute.

LANGUAGE: python
CODE:
import spacy
from spacy.language import Language
from spacy.matcher import Matcher
from spacy.tokens import Token

# We're using a component factory because the component needs to be
# initialized with the shared vocab via the nlp object
@Language.factory("html_merger")
def create_bad_html_merger(nlp, name):
    return BadHTMLMerger(nlp.vocab)

class BadHTMLMerger:
    def __init__(self, vocab):
        patterns = [
            [{"ORTH": "<"}, {"LOWER": "br"}, {"ORTH": ">"}],
            [{"ORTH": "<"}, {"LOWER": "br/"}, {"ORTH": ">"}],
        ]
        # Register a new token extension to flag bad HTML
        Token.set_extension("bad_html", default=False)
        self.matcher = Matcher(vocab)
        self.matcher.add("BAD_HTML", patterns)

    def __call__(self, doc):
        # This method is invoked when the component is called on a Doc
        matches = self.matcher(doc)
        spans = []  # Collect the matched spans here
        for match_id, start, end in matches:
            spans.append(doc[start:end])
        with doc.retokenize() as retokenizer:
            for span in spans:
                retokenizer.merge(span)
                for token in span:
                    token._.bad_html = True  # Mark token as bad HTML
        return doc

nlp = spacy.load("en_core_web_sm")
nlp.add_pipe("html_merger", last=True)  # Add component to the pipeline
doc = nlp("Hello<br>world! <br/> This is a test.")
for token in doc:
    print(token.text, token._.bad_html)

----------------------------------------

TITLE: Counting Token Attribute Frequencies with Doc.count_by
DESCRIPTION: Demonstrates how to count frequencies of token attributes using the count_by method, which returns a dictionary of {attribute_value: count} pairs. The example shows counting word forms (ORTH attribute) in a document and displays the numeric IDs of the tokens.

LANGUAGE: python
CODE:
from spacy.attrs import ORTH
doc = nlp("apple apple orange banana")
assert doc.count_by(ORTH) == {7024: 1, 119552: 1, 2087: 2}
doc.to_array([ORTH])
# array([[11880], [11880], [7561], [12800]])

----------------------------------------

TITLE: Running the Matcher on a Document
DESCRIPTION: Shows the basic syntax for applying a configured matcher to a document. The matcher will find all matches in the order they appear in the text.

LANGUAGE: python
CODE:
doc = nlp(YOUR_TEXT_HERE)
matcher(doc)

----------------------------------------

TITLE: Example of serializing a pipe with specific fields in spaCy
DESCRIPTION: Demonstrates serializing a component to disk, which is useful for saving the state of a trained component. This can exclude specific serialization fields if needed.

LANGUAGE: python
CODE:
data = pipe.to_disk("/path")

----------------------------------------

TITLE: Pipeline Component Configuration in INI Format
DESCRIPTION: Example of how pipeline components are defined in a spaCy configuration file. Components refer to their factory name rather than registry references, and can include various arguments and variable interpolation.

LANGUAGE: ini
CODE:
[components.my_component]
factory = "foo"
some_arg = "bar"
other_arg = ${paths.some_path}

----------------------------------------

TITLE: Basic Command Syntax for Explosion-bot
DESCRIPTION: The fundamental syntax for invoking the Explosion-bot in GitHub comments. The bot must be mentioned at the beginning of the comment followed by the specific test command.

LANGUAGE: markdown
CODE:
@explosion-bot please test_gpu

----------------------------------------

TITLE: Implementing a Neural Network with Native Thinc Layers
DESCRIPTION: Creates a neural network entirely using Thinc's API instead of PyTorch. This example uses Thinc's operator overloading with chain to create a sequential model similar to the PyTorch example.

LANGUAGE: python
CODE:
from thinc.api import chain, with_array, Model, Relu, Dropout, Softmax
from spacy.ml import CharacterEmbed

char_embed = CharacterEmbed(width, embed_size, nM, nC)
with Model.define_operators({">>" : chain}):
    layers = (
        Relu(hidden_width, width)
        >> Dropout(dropout)
        >> Relu(hidden_width, hidden_width)
        >> Dropout(dropout)
        >> Softmax(nO, hidden_width)
    )
    model = char_embed >> with_array(layers)

----------------------------------------

TITLE: Serializing a SpanGroup to Bytes in Python using spaCy
DESCRIPTION: Demonstrates how to serialize a span group to a bytestring using the to_bytes() method. This example first creates a document with spans representing errors, then serializes that span group.

LANGUAGE: python
CODE:
doc = nlp("Their goi ng home")
doc.spans["errors"] = [doc[0:1], doc[1:3]]
group_bytes = doc.spans["errors"].to_bytes()

----------------------------------------

TITLE: Creating a Custom Pipeline Component to Expand Person Entities
DESCRIPTION: This function expands PERSON entities by including titles like 'Dr.', 'Mr.', and 'Ms.' by checking the token before each PERSON entity and extending the span if a title is found.

LANGUAGE: python
CODE:
from spacy.language import Language
from spacy.tokens import Span

@Language.component("expand_person_entities")
def expand_person_entities(doc):
    new_ents = []
    for ent in doc.ents:
        # Only check for title if it's a person and not the first token
        if ent.label_ == "PERSON" and ent.start != 0:
            prev_token = doc[ent.start - 1]
            if prev_token.text in ("Dr", "Dr.", "Mr", "Mr.", "Ms", "Ms."):
                new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)
                new_ents.append(new_ent)
            else:
                new_ents.append(ent)
        else:
            new_ents.append(ent)
    doc.ents = new_ents
    return doc

----------------------------------------

TITLE: Using StringStore for String-Hash Conversion in Python
DESCRIPTION: This example demonstrates the basic usage of the StringStore class, showing how to convert between strings and hashes, the requirement to add strings before reverse lookup, and iterating through keys. It highlights the need to call add() before fetching a string by its hash value.

LANGUAGE: python
CODE:
from spacy.strings import StringStore

ss = StringStore()
hashval = ss["spacy"] # 10639093010105930009
try:
    # this won't work
    ss[hashval]
except KeyError:
    print(f"key {hashval} unknown in the StringStore.")

ss.add("spacy")
assert ss[hashval] == "spacy" # it works now

# There is no `.keys` property, but you can iterate over keys
# The empty string will never be in the list of keys
for key in ss:
    print(key)

----------------------------------------

TITLE: Writing Proper Python Docstrings in spaCy
DESCRIPTION: Example of a well-formatted docstring for a method in the spaCy project, including argument documentation, return value, and a link to the API documentation.

LANGUAGE: python
CODE:
def has_pipe(self, name: str) -> bool:
    """Check if a component name is present in the pipeline. Equivalent to
    `name in nlp.pipe_names`.

    name (str): Name of the component.
    RETURNS (bool): Whether a component of the name exists in the pipeline.

    DOCS: https://spacy.io/api/language#has_pipe
    """
    ...

----------------------------------------

TITLE: Converting IOB to spaCy DocBin Format for spaCy v3
DESCRIPTION: Command to convert an IOB file to .spacy (DocBin) format for use with spaCy v3. Uses the en_core_web_sm model as a base, splits the data, and creates 10 files.

LANGUAGE: bash
CODE:
python -m spacy convert -c iob -s -n 10 -b en_core_web_sm file.iob .

----------------------------------------

TITLE: Setting Entity Annotations in spaCy Documents
DESCRIPTION: This example shows how to set entity annotations at the document level using either doc.set_ents or by directly assigning to doc.ents. It creates a new entity span for 'fb' and adds it to the document's entities.

LANGUAGE: python
CODE:
import spacy
from spacy.tokens import Span

nlp = spacy.load("en_core_web_sm")
doc = nlp("fb is hiring a new vice president of global policy")
ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]
print('Before', ents)
# The model didn't recognize "fb" as an entity :(

# Create a span for the new entity
fb_ent = Span(doc, 0, 1, label="ORG")
orig_ents = list(doc.ents)

# Option 1: Modify the provided entity spans, leaving the rest unmodified
doc.set_ents([fb_ent], default="unmodified")

# Option 2: Assign a complete list of ents to doc.ents
doc.ents = orig_ents + [fb_ent]

ents = [(e.text, e.start, e.end, e.label_) for e in doc.ents]
print('After', ents)
# [('fb', 0, 1, 'ORG')] 🎉

----------------------------------------

TITLE: spaCy Configuration Example for Trainable Components
DESCRIPTION: This INI-formatted configuration snippet demonstrates how to configure a text categorization component in spaCy. It shows how the model is defined separately and then passed to the component, allowing for modular architecture design and easy swapping of models.

LANGUAGE: ini
CODE:
[components]

[components.textcat]
factory = "textcat"
labels = []

# This function is created and then passed to the "textcat" component as
# the argument "model"
[components.textcat.model]
@architectures = "spacy.TextCatBOW.v3"
exclusive_classes = true
length = 262144
ngram_size = 1
no_output_layer = false

[components.other_textcat]
factory = "textcat"

----------------------------------------

TITLE: Configuring VS Code for Black Formatting in Python
DESCRIPTION: JSON configuration for VS Code settings to use black as the Python code formatter and enable auto-formatting on save. This setup helps maintain consistent code style across the spaCy project.

LANGUAGE: json
CODE:
{
  "python.formatting.provider": "black",
  "[python]": {
    "editor.formatOnSave": true
  }
}

----------------------------------------

TITLE: Testing for expected errors in spaCy
DESCRIPTION: Shows how to test that a function raises the expected error using pytest.raises contextmanager when providing invalid inputs to a Doc constructor.

LANGUAGE: python
CODE:
words = ["a", "b", "c", "d", "e"]
ents = ["Q-PERSON", "I-PERSON", "O", "I-PERSON", "I-GPE"]
with pytest.raises(ValueError):
    Doc(Vocab(), words=words, ents=ents)

----------------------------------------

TITLE: Converting IOB to JSON Format for spaCy v2
DESCRIPTION: Command to convert an IOB file to JSON format for spaCy v2. Splits the data into 10 files and uses the 'en' language model as a base.

LANGUAGE: bash
CODE:
python -m spacy convert -c iob -s -n 10 -b en file.iob

----------------------------------------

TITLE: Pipeline Package Directory Structure
DESCRIPTION: Illustrates the directory structure created by the spacy package command, showing the organization of files including metadata, setup files, and the actual pipeline data.

LANGUAGE: yaml
CODE:
└── /
    ├── MANIFEST.in                           # to include meta.json
    ├── meta.json                             # pipeline meta data
    ├── setup.py                              # setup file for pip installation
    ├── en_example_pipeline                   # pipeline directory
    │    ├── __init__.py                      # init for pip installation
    │    └── en_example_pipeline-1.0.0        # pipeline data
    │        ├── config.cfg                   # pipeline config
    │        ├── meta.json                    # pipeline meta
    │        └── ...                          # directories with component data
    └── dist
        └── en_example_pipeline-1.0.0.tar.gz  # installable package

----------------------------------------

TITLE: Loading spaCy Models via Direct Import
DESCRIPTION: Alternative method to load spaCy models by importing them directly as Python modules and then calling their load() method, which is useful in some deployment scenarios.

LANGUAGE: python
CODE:
import spacy
import en_core_web_sm

nlp = en_core_web_sm.load()
doc = nlp("This is a sentence.")

----------------------------------------

TITLE: Suppressing Original Exceptions in spaCy
DESCRIPTION: Demonstrates how to re-raise exceptions without showing the original exception by using 'from None'. This is useful when the original exception isn't helpful for the user.

LANGUAGE: python
CODE:
try:
    run_our_own_code_that_might_fail_confusingly()
except ValueError:
+    raise ValueError(Errors.E123) from None

----------------------------------------

TITLE: Getting spaCy Model URLs via Command Line
DESCRIPTION: Shows how to get the download URL for a spaCy model compatible with your version of spaCy using the spacy info command with the --url flag.

LANGUAGE: bash
CODE:
spacy info en_core_web_sm --url

----------------------------------------

TITLE: Creating Callback Functions with Type Annotations in Python
DESCRIPTION: Example demonstrating how to properly annotate a function that returns a callback, using Callable to specify argument and return types of the callback function.

LANGUAGE: python
CODE:
def create_callback(some_arg: bool) -> Callable[[str, int], List[str]]:
    def callback(arg1: str, arg2: int) -> List[str]:
        ...

    return callback

----------------------------------------

TITLE: Creating Tables in Markdown for spaCy Documentation
DESCRIPTION: How to create tables in Markdown format for the spaCy documentation, including special formatting for return values and divider rows.

LANGUAGE: markdown
CODE:
| Header 1 | Header 2 |
| -------- | -------- |
| Column 1 | Column 2 |

LANGUAGE: markdown
CODE:
| Header 1 | Header 2 | Header 3 |
| -------- | -------- | -------- |
| Column 1 | Column 2 | Column 3 |
| _Hello_  |          |          |
| Column 1 | Column 2 | Column 3 |

----------------------------------------

TITLE: Packaging the trained spaCy model
DESCRIPTION: Command to package the trained NER model for distribution or deployment, creating a standalone spaCy model package.

LANGUAGE: bash
CODE:
python -m spacy project run package

----------------------------------------

TITLE: Creating Infoboxes with Different Variants
DESCRIPTION: Shows how to create infoboxes using JSX syntax. Infoboxes can have different variants (regular, warning, danger) to indicate different types of information or alerts.

LANGUAGE: jsx
CODE:
<Infobox title="Information">Regular infobox</Infobox>
<Infobox title="Important note" variant="warning">This is a warning.</Infobox>
<Infobox title="Be careful!" variant="danger">This is dangerous.</Infobox>

----------------------------------------

TITLE: Defining Dependencies for spaCy Training Quickstart Configuration
DESCRIPTION: Specifies the required packages for compiling spaCy's training quickstart configuration. It requires Jinja2 version 3.1.0 or higher for template processing and srsly library for data serialization.

LANGUAGE: text
CODE:
jinja2>=3.1.0
srsly