TITLE: Defining Cumulative Metrics in YAML
DESCRIPTION: Example of defining a cumulative metric in a YAML file. It demonstrates how to configure a metric that aggregates a measure over a given time window.

LANGUAGE: yaml
CODE:
metrics:
  - name: wau_rolling_7
    type: cumulative
    label: Weekly active users
    type_params:
      measure:
        name: active_users
        fill_nulls_with: 0
        join_to_timespine: true
      cumulative_type_params:
        window: 7 days

----------------------------------------

TITLE: Implementing Basic SQL Structure with CTEs in dbt
DESCRIPTION: Demonstrates the basic structure of a SQL query in dbt, including the use of CTEs for imports and transformations, and a final select statement.

LANGUAGE: sql
CODE:
with

events as (

    ...

),

{# CTE comments go here #}
filtered_events as (

    ...

)

select * from filtered_events

----------------------------------------

TITLE: Configuring dbt_project.yml in YAML
DESCRIPTION: Example of key configuration options in the dbt_project.yml file, including project name, version, required dbt version, profile, and paths for various resource types.

LANGUAGE: yaml
CODE:
name: your_project_name
version: 1.0.0
require-dbt-version: ">=1.0.0"
profile: your_profile_name
model-paths: ["models"]
seed-paths: ["seeds"]
test-paths: ["tests"]
analysis-paths: ["analyses"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]
docs-paths: ["docs"]
vars:
  # Project variables here

----------------------------------------

TITLE: Defining Model and Column Descriptions in YAML
DESCRIPTION: This snippet demonstrates how to add descriptions to models and columns in a YAML file. It includes examples of adding tests and handling quoted column names.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: events
    description: This table contains clickstream events from the marketing website

    columns:
      - name: event_id
        description: This is a unique identifier for the event
        tests:
          - unique
          - not_null

      - name: user-id
        quote: true
        description: The user who performed the event
        tests:
          - not_null

----------------------------------------

TITLE: Testing Modified Models and Downstream Dependencies in dbt
DESCRIPTION: This command shows how to run tests only on modified models and their downstream dependencies in dbt, using the state comparison feature. It's useful for efficient CI/CD pipelines.

LANGUAGE: bash
CODE:
dbt test -s state:modified+ --defer --state path/to/prod/artifacts

----------------------------------------

TITLE: Selecting from Sources in SQL
DESCRIPTION: Demonstrates how to reference source tables in SQL models using the source() function, which helps establish data lineage.

LANGUAGE: sql
CODE:
select
  ...

from {{ source('jaffle_shop', 'orders') }}

left join {{ source('jaffle_shop', 'customers') }} using (customer_id)

----------------------------------------

TITLE: Standard Staging Model for Stripe Payments
DESCRIPTION: Example of a staging model showing standard transformation patterns including renaming, type casting, basic computations, and categorization using CTEs.

LANGUAGE: sql
CODE:
with

source as (

    select * from {{ source('stripe','payment') }}

),

renamed as (

    select
        -- ids
        id as payment_id,
        orderid as order_id,

        -- strings
        paymentmethod as payment_method,
        case
            when payment_method in ('stripe', 'paypal', 'credit_card', 'gift_card') then 'credit'
            else 'cash'
        end as payment_type,
        status,

        -- numerics
        amount as amount_cents,
        amount / 100.0 as amount,

        -- booleans
        case
            when status = 'successful' then true
            else false
        end as is_completed_payment,

        -- dates
        date_trunc('day', created) as created_date,

        -- timestamps
        created::timestamp_ltz as created_at

    from source

)

select * from renamed

----------------------------------------

TITLE: Basic Incremental Filter Logic in SQL
DESCRIPTION: SQL query demonstrating how to filter for new or updated records in an incremental model using a timestamp-based comparison.

LANGUAGE: sql
CODE:
select * from orders

where
  updated_at > (select max(updated_at) from {{ this }})

----------------------------------------

TITLE: Defining Order Items Semantic Model in YAML
DESCRIPTION: Configures the semantic model for order items including entity relationships, time dimensions, and revenue measures. Includes various revenue calculations based on product types and aggregation methods.

LANGUAGE: yaml
CODE:
semantic_models:
   - name: order_items
      defaults:
         agg_time_dimension: ordered_at
      description: |
         Items contatined in each order. The grain of the table is one row per order item.
      model: ref('order_items')
      entities:
         - name: order_item
           type: primary
           expr: order_item_id
         - name: order_id
           type: foreign
           expr: order_id
         - name: product
           type: foreign
           expr: product_id
      dimensions:
         - name: ordered_at
           expr: date_trunc('day', ordered_at)
           type: time
           type_params:
             time_granularity: day
         - name: is_food_item
           type: categorical
         - name: is_drink_item
           type: categorical
      measures:
         - name: revenue
           description: The revenue generated for each order item. Revenue is calculated as a sum of revenue associated with each product in an order.
           agg: sum
           expr: product_price
         - name: food_revenue
           description: The revenue generated for each order item. Revenue is calculated as a sum of revenue associated with each product in an order.
           agg: sum
           expr: case when is_food_item = 1 then product_price else 0 end
         - name: drink_revenue
           description: The revenue generated for each order item. Revenue is calculated as a sum of revenue associated with each product in an order.
           agg: sum
           expr: case when is_drink_item = 1 then product_price else 0 end
         - name: median_revenue
           description: The median revenue generated for each order item.
           agg: median
           expr: product_price

----------------------------------------

TITLE: Creating Customer Analysis Model in SQL
DESCRIPTION: A SQL model that combines customer and order data to analyze customer ordering patterns. It joins customer information with aggregated order statistics.

LANGUAGE: sql
CODE:
with customer_orders as (
    select
        customer_id,
        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from jaffle_shop.orders

    group by 1
)

select
    customers.customer_id,
    customers.first_name,
    customers.last_name,
    customer_orders.first_order_date,
    customer_orders.most_recent_order_date,
    coalesce(customer_orders.number_of_orders, 0) as number_of_orders

from jaffle_shop.customers

left join customer_orders using (customer_id)

----------------------------------------

TITLE: Configuring Generic Tests in YAML
DESCRIPTION: Configure multiple generic tests for an orders model including uniqueness, null checks, value validation, and referential integrity.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: orders
    columns:
      - name: order_id
        tests:
          - unique
          - not_null
      - name: status
        tests:
          - accepted_values:
              values: ['placed', 'shipped', 'completed', 'returned']
      - name: customer_id
        tests:
          - relationships:
              to: ref('customers')
              field: id

----------------------------------------

TITLE: Displaying dbt Project Directory Structure
DESCRIPTION: Complete file tree structure of a sample dbt project for Jaffle Shop, showing the organization of models, tests, seeds, analyses, and configuration files across different layers and functional areas.

LANGUAGE: shell
CODE:
jaffle_shop
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ analyses
‚îú‚îÄ‚îÄ seeds
‚îÇ   ‚îî‚îÄ‚îÄ employees.csv
‚îú‚îÄ‚îÄ dbt_project.yml
‚îú‚îÄ‚îÄ macros
‚îÇ   ‚îî‚îÄ‚îÄ cents_to_dollars.sql
‚îú‚îÄ‚îÄ models
‚îÇ   ‚îú‚îÄ‚îÄ intermediate
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ finance
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ _int_finance__models.yml
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ int_payments_pivoted_to_orders.sql
‚îÇ   ‚îú‚îÄ‚îÄ marts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ finance
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _finance__models.yml
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ orders.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ payments.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ marketing
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ _marketing__models.yml
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ customers.sql
‚îÇ   ‚îú‚îÄ‚îÄ staging
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ jaffle_shop
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _jaffle_shop__docs.md
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _jaffle_shop__models.yml
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _jaffle_shop__sources.yml
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_jaffle_shop__customers.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ base_jaffle_shop__deleted_customers.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stg_jaffle_shop__customers.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stg_jaffle_shop__orders.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stripe
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ _stripe__models.yml
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ _stripe__sources.yml
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ stg_stripe__payments.sql
‚îÇ   ‚îî‚îÄ‚îÄ utilities
‚îÇ       ‚îî‚îÄ‚îÄ all_dates.sql
‚îú‚îÄ‚îÄ packages.yml
‚îú‚îÄ‚îÄ snapshots
‚îî‚îÄ‚îÄ tests
    ‚îî‚îÄ‚îÄ assert_positive_value_for_total_amount.sql

----------------------------------------

TITLE: Complete Semantic Model Example - YAML
DESCRIPTION: Detailed example showing two semantic models with full configuration including transactions and customers models with entities, dimensions, and measures.

LANGUAGE: yaml
CODE:
semantic_models:
  - name: transaction
    model: ref('fact_transactions')
    description: "Transaction fact table at the transaction level. This table contains one row per transaction and includes the transaction timestamp."
    defaults:
      agg_time_dimension: transaction_date
    entities:
      - name: transaction
        type: primary
        expr: transaction_id
      - name: customer
        type: foreign
        expr: customer_id
    dimensions:
      - name: transaction_date
        type: time
        type_params:
          time_granularity: day
      - name: transaction_location
        type: categorical
        expr: order_country
    measures:
      - name: transaction_total
        description: "The total value of the transaction."
        agg: sum
      - name: sales
        description: "The total sale of the transaction."
        agg: sum
        expr: transaction_total
      - name: median_sales
        description: "The median sale of the transaction."
        agg: median
        expr: transaction_total
  - name: customers
    model: ref('dim_customers')
    description: "A customer dimension table."
    entities:
      - name: customer
        type: primary
        expr: customer_id
    dimensions:
      - name: first_name
        type: categorical

----------------------------------------

TITLE: Creating dbt Test Configuration in YAML
DESCRIPTION: YAML configuration that defines tests for multiple database models including customers, stg_customers, and stg_orders. Tests include unique constraints, null checks, accepted value validations, and relationship checks between tables.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: customers
    columns:
      - name: customer_id
        tests:
          - unique
          - not_null

  - name: stg_customers
    columns:
      - name: customer_id
        tests:
          - unique
          - not_null

  - name: stg_orders
    columns:
      - name: order_id
        tests:
          - unique
          - not_null
      - name: status
        tests:
          - accepted_values:
              values: ['placed', 'shipped', 'completed', 'return_pending', 'returned']
      - name: customer_id
        tests:
          - not_null
          - relationships:
              to: ref('stg_customers')
              field: customer_id

----------------------------------------

TITLE: Creating a Singular Data Test in SQL
DESCRIPTION: Define a singular data test that checks if total payment amounts are positive. The test fails if it finds any orders with negative total amounts.

LANGUAGE: sql
CODE:
select
    order_id,
    sum(amount) as total_amount
from {{ ref('fct_payments') }}
group by 1
having total_amount < 0

----------------------------------------

TITLE: Configuring Seed-Specific Options in dbt Project YAML
DESCRIPTION: This snippet demonstrates how to configure seed-specific options like quote_columns, column_types, and delimiter in the dbt_project.yml file.

LANGUAGE: yaml
CODE:
seeds:
  [<resource-path>]:
    [+][quote_columns]: true | false
    [+][column_types]: {column_name: datatype}
    [+][delimiter]: <string>

----------------------------------------

TITLE: Defining Data Tests for Models in dbt YAML
DESCRIPTION: This snippet demonstrates how to define data tests for models in a dbt YAML file. It includes examples of custom tests, test configurations, and column-level tests.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: <model_name>
    tests:
      - [<test_name>]:
          <argument_name>: <argument_value>
          [config]:
            [<test_config>]: <config-value>

    [columns]:
      - name: <column_name>
        tests:
          - [<test_name>]
          - [<test_name>]:
              <argument_name>: <argument_value>
              [config]:
                [<test_config>]: <config-value>

----------------------------------------

TITLE: Structuring dbt SQL Model with Organized Column Types
DESCRIPTION: Example SQL model demonstrating proper field naming conventions and organization by data types. The model shows source data transformation with clear grouping of IDs, strings, numerics, booleans, dates, and timestamps, following best practices for field naming and data type handling.

LANGUAGE: sql
CODE:
with

source as (

    select * from {{ source('ecom', 'raw_orders') }}

),

renamed as (

    select

        ----------  ids
        id as order_id,
        store_id as location_id,
        customer as customer_id,

        ---------- strings
        status as order_status,

        ---------- numerics
        (order_total / 100.0)::float as order_total,
        (tax_paid / 100.0)::float as tax_paid,

        ---------- booleans
        is_fulfilled,

        ---------- dates
        date(order_date) as ordered_date,

        ---------- timestamps
        ordered_at

    from source

)

select * from renamed

----------------------------------------

TITLE: Configuring profiles.yml for dbt Core
DESCRIPTION: This YAML snippet demonstrates the structure of a profiles.yml file for dbt Core. It includes global configurations (deprecated in v1.8+) and profile settings with connection details for data platforms.

LANGUAGE: yaml
CODE:
config:
  send_anonymous_usage_stats: <true | false>
  use_colors: <true | false>
  partial_parse: <true | false>
  printer_width: <integer>
  write_json: <true | false>
  warn_error: <true | false>
  warn_error_options: <include: all | include: [<error-name>] | include: all, exclude: [<error-name>]>
  log_format: <text | json | default>
  debug: <true | false>
  version_check: <true | false>
  fail_fast: <true | false>
  indirect_selection: <eager | cautious | buildable | empty>
  use_experimental_parser: <true | false>
  static_parser: <true | false>
  cache_selected_only: <true | false>
  populate_cache: <true | false>

<profile-name>:
  target: <target-name> # this is the default target
  outputs:
    <target-name>:
      type: <bigquery | postgres | redshift | snowflake | other>
      schema: <schema_identifier>
      threads: <natural_number>

      ### database-specific connection details
      ...

    <target-name>: # additional targets
      ...

<profile-name>: # additional profiles
  ...

----------------------------------------

TITLE: Sample Webhook JSON Payloads
DESCRIPTION: Example JSON payloads for different webhook events including job.run.started, job.run.completed, and job.run.errored

LANGUAGE: json
CODE:
{
  "accountId": 1,
  "webhooksID": "wsu_12345abcde",
  "eventId": "wev_2L6Z3l8uPedXKPq9D2nWbPIip7Z",
  "timestamp": "2023-01-31T19:28:15.742843678Z",
  "eventType": "job.run.started",
  "webhookName": "test",
  "data": {
    "jobId": "123",
    "jobName": "Daily Job (dbt build)",
    "runId": "12345",
    "environmentId": "1234",
    "environmentName": "Production",
    "dbtVersion": "1.0.0",
    "projectName": "Snowflake Github Demo",
    "projectId": "167194",
    "runStatus": "Running",
    "runStatusCode": 3,
    "runStatusMessage": "None",
    "runReason": "Kicked off from UI by test@test.com",
    "runStartedAt": "2023-01-31T19:28:07Z"
  }
}

----------------------------------------

TITLE: Basic Semantic Model Structure - YAML
DESCRIPTION: Basic template showing the required and optional components for configuring a semantic model in YAML format.

LANGUAGE: yaml
CODE:
semantic_models:
  - name: the_name_of_the_semantic_model
    description: same as always
    model: ref('some_model')
    defaults:
      agg_time_dimension: dimension_name
    entities:
      - see more information in entities
    measures:
      - see more information in the measures section
    dimensions:
      - see more information in the dimensions section
    primary_entity: if the semantic model has no primary entity, then this property is required.

----------------------------------------

TITLE: Configuring dbt Project Settings in YAML
DESCRIPTION: YAML configuration for a dbt project that specifies materialization settings and schema organizations for different model directories.

LANGUAGE: yaml
CODE:
name: jaffle_shop
config-version: 2
...

models:
  jaffle_shop:
    +materialized: view
    marts:
      +materialized: table
      marketing:
        +schema: marketing

----------------------------------------

TITLE: Creating Snowflake OAuth Security Integration
DESCRIPTION: SQL command to create a security integration in Snowflake for external OAuth authentication. This snippet includes placeholders for key configuration values such as the integration name, OAuth issuer, JWS keys URL, and audience list.

LANGUAGE: sql
CODE:
create security integration your_integration_name
type = external_oauth
enabled = true
external_oauth_type = okta
external_oauth_issuer = ''
external_oauth_jws_keys_url = ''
external_oauth_audience_list = ('')
external_oauth_token_user_mapping_claim = 'sub'
external_oauth_snowflake_user_mapping_attribute = 'email_address'
external_oauth_any_role_mode = 'ENABLE'

----------------------------------------

TITLE: Defining Simple Revenue Metric in dbt
DESCRIPTION: YAML configuration for defining a simple revenue metric in dbt Semantic Layer. The metric is configured with basic properties including name, description, label, and type parameters specifying the measure to use.

LANGUAGE: yaml
CODE:
metrics:
  - name: revenue
    description: Sum of the order total.
    label: Revenue
    type: simple
    type_params:
      measure: order_total

----------------------------------------

TITLE: Defining Data Types for Spark in dbt Unit Tests
DESCRIPTION: This snippet demonstrates how to specify various data types including integer, float, string, boolean, date, timestamp, array, map, and named struct in a dbt unit test for Spark.

LANGUAGE: yaml
CODE:
unit_tests:
  - name: test_my_data_types
    model: fct_data_types
    given:
      - input: ref('stg_data_types')
        rows:
         - int_field: 1
           float_field: 2.0
           str_field: my_string
           str_escaped_field: "my,cool'string"
           bool_field: true
           date_field: 2020-01-02
           timestamp_field: 2013-11-03 00:00:00-0
           timestamptz_field: 2013-11-03 00:00:00-0
           int_array_field: 'array(1, 2, 3)'
           map_field: 'map("10", "t", "15", "f", "20", NULL)'
           named_struct_field: 'named_struct("a", 1, "b", 2, "c", 3)'

----------------------------------------

TITLE: Incremental Model with Filtering Logic
DESCRIPTION: Shows how to implement an incremental model with filtering logic using the is_incremental() macro to process only new records since the last run.

LANGUAGE: sql
CODE:
{{
    config(
        materialized='incremental'
    )
}}

select
    *,
    my_slow_function(my_column)

from {{ ref('app_data_events') }}

{% if is_incremental() %}

  -- this filter will only be applied on an incremental run
  -- (uses >= to include records whose timestamp occurred since the last run of this model)
  -- (If event_time is NULL or the table is truncated, the condition will always be true and load all records)
where event_time >= (select coalesce(max(event_time),'1900-01-01') from {{ this }} )

{% endif %}

----------------------------------------

TITLE: Comprehensive SQL Query Example in dbt
DESCRIPTION: Illustrates a complex SQL query following dbt best practices, including multiple CTEs, joins, case statements, and formatting conventions.

LANGUAGE: sql
CODE:
with

my_data as (

    select
        field_1,
        field_2,
        field_3,
        cancellation_date,
        expiration_date,
        start_date

    from {{ ref('my_data') }}

),

some_cte as (

    select
        id,
        field_4,
        field_5

    from {{ ref('some_cte') }}

),

some_cte_agg as (

    select
        id,
        sum(field_4) as total_field_4,
        max(field_5) as max_field_5

    from some_cte

    group by 1

),

joined as (

    select
        my_data.field_1,
        my_data.field_2,
        my_data.field_3,

        -- use line breaks to visually separate calculations into blocks
        case
            when my_data.cancellation_date is null
                and my_data.expiration_date is not null
                then expiration_date
            when my_data.cancellation_date is null
                then my_data.start_date + 7
            else my_data.cancellation_date
        end as cancellation_date,

        some_cte_agg.total_field_4,
        some_cte_agg.max_field_5

    from my_data

    left join some_cte_agg
        on my_data.id = some_cte_agg.id

    where my_data.field_1 = 'abc' and
        (
            my_data.field_2 = 'def' or
            my_data.field_2 = 'ghi'
        )

    having count(*) > 1

)

select * from joined

----------------------------------------

TITLE: Modern YAML Snapshot Configuration (v1.9+)
DESCRIPTION: Example of configuring a snapshot using YAML in dbt versions 1.9 and later

LANGUAGE: yaml
CODE:
snapshots:
  - name: orders_snapshot
    relation: source('jaffle_shop', 'orders')
    config:
      schema: snapshots
      database: analytics
      unique_key: id
      strategy: timestamp
      updated_at: updated_at
      dbt_valid_to_current: "to_date('9999-12-31')"

----------------------------------------

TITLE: Basic dbt CLI Commands
DESCRIPTION: Essential dbt command line commands for running, building and testing dbt projects. These commands work in both dbt Cloud CLI and dbt Core environments.

LANGUAGE: bash
CODE:
dbt run

LANGUAGE: bash
CODE:
dbt build

LANGUAGE: bash
CODE:
dbt test

LANGUAGE: bash
CODE:
dbt --help

LANGUAGE: bash
CODE:
dbt COMMAND_NAME --help

----------------------------------------

TITLE: Setting up profiles.yml for Database Connections
DESCRIPTION: Example profiles.yml configuration showing how to set up multiple targets (dev and prod) for a Postgres database connection, including connection parameters and thread settings.

LANGUAGE: yaml
CODE:
# example profiles.yml file
jaffle_shop:
  target: dev
  outputs:
    dev:
      type: postgres
      host: localhost
      user: alice
      password: <password>
      port: 5432
      dbname: jaffle_shop
      schema: dbt_alice
      threads: 4

    prod:  # additional prod target
      type: postgres
      host: prod.db.example.com
      user: alice
      password: <prod_password>
      port: 5432
      dbname: jaffle_shop
      schema: analytics
      threads: 8

----------------------------------------

TITLE: Configuring DBT Model Properties in YAML
DESCRIPTION: Complete schema for defining model properties in DBT, including model configuration, column definitions, testing rules, versioning, and metadata. The configuration supports features like model descriptions, documentation settings, access controls, constraints, and column-level properties.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: <model name>
    description: <markdown_string>
    docs:
      show: true | false
      node_color: <color_id>
    latest_version: <version_identifier>
    deprecation_date: <YAML_DateTime>
    access: private | protected | public
    config:
      <model_config>: <config_value>
    constraints:
      - <constraint>
    tests:
      - <test>
      - ...
    columns:
      - name: <column_name>
        description: <markdown_string>
        meta: {<dictionary>}
        quote: true | false
        constraints:
          - <constraint>
        tests:
          - <test>
          - ...
        tags: [<string>]
        granularity: <any supported time granularity>
      - name: ...
    time_spine:
      standard_granularity_column: <column_name>
    versions:
      - v: <version_identifier>
        defined_in: <definition_file_name>
        description: <markdown_string>
        docs:
          show: true | false
        access: private | protected | public
        constraints:
          - <constraint>
        config:
          <model_config>: <config_value>
        tests:
          - <test>
          - ...
        columns:
          - include: <include_value>
            exclude: <exclude_list>
          - name: <column_name>
            quote: true | false
            constraints:
              - <constraint>
            tests:
              - <test>
              - ...
            tags: [<string>]
        - v: ...

----------------------------------------

TITLE: Defining Metrics in YAML for dbt v1.8+
DESCRIPTION: Example of the metrics specification configuration for dbt version 1.8 and higher. It shows the structure and required fields for defining metrics in a YAML file.

LANGUAGE: yaml
CODE:
metrics:
  - name: metric name                     ## Required
    description: description               ## Optional
    type: the type of the metric          ## Required
    type_params:                          ## Required
      - specific properties for the metric type
    config:                               ## Optional
      meta:
        my_meta_config:  'config'         ## Optional
    label: The display name for your metric. This value will be shown in downstream tools. ## Required
    filter: |
      {{  Dimension('entity__name') }} > 0 and {{ Dimension(' entity__another_name') }} is not
      null and {{ Metric('metric_name', group_by=['entity_name']) }} > 5

----------------------------------------

TITLE: Using Out-of-the-Box Data Tests in dbt YAML
DESCRIPTION: This snippet demonstrates how to use the four built-in generic data tests in dbt: not_null, unique, accepted_values, and relationships. It includes examples for each test type.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: orders
    columns:
      - name: order_id
        tests:
          - not_null
          - unique:
              config:
                where: "order_id > 21"
      - name: status
        tests:
          - accepted_values:
              values: ['placed', 'shipped', 'completed', 'returned']
      - name: customer_id
        tests:
          - relationships:
              to: ref('customers')
              field: id

----------------------------------------

TITLE: Model-Specific Configuration in Project YAML
DESCRIPTION: YAML configuration for model-specific settings in dbt_project.yml, including materialization, SQL headers, and other model-specific properties.

LANGUAGE: yaml
CODE:
models:
  [<resource-path>]:
    [+]materialized: <materialization_name>
    [+]sql_header: <string>
    [+]on_configuration_change: apply | continue | fail
    [+]unique_key: <column_name_or_expression>

----------------------------------------

TITLE: dbt Materialization Comparison Table
DESCRIPTION: A detailed comparison table showing the differences between view, table, and incremental materializations across various metrics including build time, costs, data freshness, and complexity.

LANGUAGE: markdown
CODE:
|                      | view                                 | table                                  | incremental                            |
| -------------------- | ------------------------------------ | -------------------------------------- | -------------------------------------- |
| üõ†Ô∏è‚åõ **build time**  | üíö  fastest ‚Äî only stores logic      | ‚ù§Ô∏è  slowest ‚Äî linear to size of data   | üíõ  medium ‚Äî builds flexible portion   |
| üõ†Ô∏èüí∏ **build costs** | üíö  lowest ‚Äî no data processed       | ‚ù§Ô∏è  highest ‚Äî all data processed       | üíõ  medium ‚Äî some data processed       |
| üìäüí∏ **query costs** | ‚ù§Ô∏è  higher ‚Äî reprocess every query   | üíö  lower ‚Äî data in warehouse          | üíö  lower ‚Äî data in warehouse          |
| üçÖüå± **freshness**   | üíö  best ‚Äî up-to-the-minute of query | üíõ  moderate ‚Äî up to most recent build | üíõ  moderate ‚Äî up to most recent build |
| üß†ü§î **complexity**  | üíö simple - maps to warehouse object | üíö simple - map to warehouse concept   | üíõ moderate - adds logical complexity  |

----------------------------------------

TITLE: Configuring ClickHouse Profile in dbt
DESCRIPTION: YAML configuration for setting up a ClickHouse connection profile in dbt's profiles.yml file. Includes all available connection parameters and their default values for both HTTP and native drivers.

LANGUAGE: yaml
CODE:
<profile-name>:
  target: <target-name>
  outputs:
    <target-name>:
      type: clickhouse
      schema: [ default ]
      driver: [ http ]
      host: [ localhost ]
      port: [ 8123 ]
      user: [ default ]
      password: [ <empty string> ]
      cluster: [ <empty string> ]
      verify: [ True ]
      secure: [ False ]
      retries: [ 1 ]
      compression: [ <empty string> ]
      connect_timeout: [ 10 ]
      send_receive_timeout: [ 300 ]
      cluster_mode: [ False ]
      use_lw_deletes: [ False ]
      check_exchange: [ True ]
      local_suffix: [ _local ]
      local_db_prefix: [ <empty string> ]
      allow_automatic_deduplication: [ False ]
      tcp_keepalive: [ False ]
      custom_settings: [ { } ]
      sync_request_timeout: [ 5 ]
      compress_block_size: [ 1048576 ]

----------------------------------------

TITLE: Referencing Docs Blocks in YAML Schema
DESCRIPTION: This snippet demonstrates how to reference a docs block in a YAML schema file using the doc() function. It shows how to include the content of a docs block in a model's description.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: events
    description: '{{ doc("table_events") }}'

    columns:
      - name: event_id
        description: This is a unique identifier for the event
        tests:
            - unique
            - not_null

----------------------------------------

TITLE: Testing and Documenting Sources in YAML
DESCRIPTION: Configures tests and documentation for source tables including descriptions, column definitions and data quality tests.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: jaffle_shop
    description: This is a replica of the Postgres database used by our app
    tables:
      - name: orders
        description: >
          One record per order. Includes cancelled and deleted orders.
        columns:
          - name: id
            description: Primary key of the orders table
            tests:
              - unique
              - not_null
          - name: status
            description: Note that the status can change over time

----------------------------------------

TITLE: Comprehensive Rebuild and Retest of Failed Models and Tests in dbt
DESCRIPTION: This command demonstrates a comprehensive approach to rebuilding and retesting failed models and tests, including unrelated failures. It uses dbt's build command with state comparison and multiple result status selectors.

LANGUAGE: bash
CODE:
dbt build --select state:modified+ result:error+ result:fail+ --defer --state path/to/prod/artifacts

----------------------------------------

TITLE: Declaring an Exposure in dbt YAML Configuration
DESCRIPTION: This YAML snippet demonstrates how to declare an exposure in a dbt project. It includes required fields like name, type, and owner, as well as optional fields such as label, url, and depends_on. The exposure represents a dashboard that depends on specific models, sources, and metrics.

LANGUAGE: yaml
CODE:
version: 2

exposures:

  - name: weekly_jaffle_metrics
    label: Jaffles by the Week
    type: dashboard
    maturity: high
    url: https://bi.tool/dashboards/1
    description: >
      Did someone say "exponential growth"?

    depends_on:
      - ref('fct_orders')
      - ref('dim_customers')
      - source('gsheets', 'goals')
      - metric('count_orders')

    owner:
      name: Callum McData
      email: data@jaffleshop.com

----------------------------------------

TITLE: Defining Derived Metrics in YAML
DESCRIPTION: Example of defining a derived metric in a YAML file. It shows how to create a metric based on an expression of other metrics.

LANGUAGE: yaml
CODE:
metrics:
  - name: order_gross_profit
    description: Gross profit from each order.
    type: derived
    label: Order gross profit
    type_params:
      expr: revenue - cost
      metrics:
        - name: order_total
          alias: revenue
        - name: order_cost
          alias: cost

----------------------------------------

TITLE: Example Source Properties Configuration for Jaffle Shop
DESCRIPTION: Practical example showing source configuration for a Jaffle Shop database with orders and customers tables. Demonstrates usage of database settings, schema, loader information, meta fields, tags, and column-level testing.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: jaffle_shop
    database: raw
    schema: public
    loader: emr # informational only (free text)
    loaded_at_field: _loaded_at # configure for all sources

    meta:
      contains_pii: true
      owner: "@alice"

    tags:
      - ecom
      - pii

    quoting:
      database: false
      schema: false
      identifier: false

    tables:
      - name: orders
        identifier: Orders_
        loaded_at_field: updated_at # override source defaults
        columns:
          - name: id
            tests:
              - unique

          - name: price_in_usd
            tests:
              - not_null

      - name: customers
        quoting:
          identifier: true # override source defaults
        columns:
            tests:
              - unique

----------------------------------------

TITLE: Configuring Table Materialization in SQL Model
DESCRIPTION: This SQL snippet demonstrates how to configure a table materialization directly in a model file. It includes additional Redshift-specific performance optimization configs for sorting and distribution.

LANGUAGE: sql
CODE:
{{ config(materialized='table', sort='timestamp', dist='user_id') }}

select *
from ...

----------------------------------------

TITLE: Data Masking with Dynamic Views in SQL
DESCRIPTION: Demonstrates how to implement data masking in a dbt model using Dynamic Views and SQL case statements. This example masks an email address for non-auditor users.

LANGUAGE: sql
CODE:
CASE
WHEN is_account_group_member('auditors') THEN email
ELSE regexp_extract(email, '^.*@(.*)$', 1)
END

----------------------------------------

TITLE: Basic Measures Configuration in DBT Semantic Layer
DESCRIPTION: Basic YAML configuration for defining measures in a semantic model, including optional parameters like description, aggregation type, and time dimension settings.

LANGUAGE: yaml
CODE:
semantic_models:
  - name: semantic_model_name
   ..rest of the semantic model config
    measures:
      - name: The name of the measure
        description: 'same as always' ## Optional
        agg: the aggregation type.
        expr: the field
        agg_params: 'specific aggregation properties such as a percentile'  ## Optional
        agg_time_dimension: The time field. Defaults to the default agg time dimension for the semantic model. ##  Optional
        non_additive_dimension: 'Use these configs when you need non-additive dimensions.' ## Optional
        config:
          meta:  {<dictionary>} Set metadata for a resource and organize resources.

----------------------------------------

TITLE: Incremental Python Model with PySpark
DESCRIPTION: This Python code defines an incremental model using PySpark. It shows how to filter for new data in an incremental run, either based on the maximum value in the current table or a fixed time window.

LANGUAGE: python
CODE:
import pyspark.sql.functions as F

def model(dbt, session):
    dbt.config(materialized = "incremental")
    df = dbt.ref("upstream_table")

    if dbt.is_incremental:

        # only new rows compared to max in current table
        max_from_this = f"select max(updated_at) from {dbt.this}"
        df = df.filter(df.updated_at >= session.sql(max_from_this).collect()[0][0])

        # or only rows from the past 3 days
        df = df.filter(df.updated_at >= F.date_add(F.current_timestamp(), F.lit(-3)))

    ...

    return df

----------------------------------------

TITLE: Markdown in Description
DESCRIPTION: Example of using markdown formatting in descriptions with proper YAML quoting

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: dim_customers
    description: "**\[Read more](https://www.google.com/)**"

    columns:
      - name: customer_id
        description: Primary key.

----------------------------------------

TITLE: Comprehensive Source Configuration in dbt_project.yml
DESCRIPTION: A comprehensive example of source configuration in the dbt_project.yml file, demonstrating various configuration options for multiple sources and tables.

LANGUAGE: yaml
CODE:
name: jaffle_shop
config-version: 2
...
sources:
  # project names
  jaffle_shop:
    +enabled: true

  events:
    # source names
    clickstream:
      # table names
      pageviews:
        +enabled: false
      link_clicks:
        +enabled: true

----------------------------------------

TITLE: File Structure for dbt Staging Models
DESCRIPTION: Example directory structure showing organization of staging models, including source-specific folders and naming conventions for SQL, YAML, and documentation files.

LANGUAGE: shell
CODE:
models/staging
‚îú‚îÄ‚îÄ jaffle_shop
‚îÇ   ‚îú‚îÄ‚îÄ _jaffle_shop__docs.md
‚îÇ   ‚îú‚îÄ‚îÄ _jaffle_shop__models.yml
‚îÇ   ‚îú‚îÄ‚îÄ _jaffle_shop__sources.yml
‚îÇ   ‚îú‚îÄ‚îÄ base
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_jaffle_shop__customers.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ base_jaffle_shop__deleted_customers.sql
‚îÇ   ‚îú‚îÄ‚îÄ stg_jaffle_shop__customers.sql
‚îÇ   ‚îî‚îÄ‚îÄ stg_jaffle_shop__orders.sql
‚îî‚îÄ‚îÄ stripe
    ‚îú‚îÄ‚îÄ _stripe__models.yml
    ‚îú‚îÄ‚îÄ _stripe__sources.yml
    ‚îî‚îÄ‚îÄ stg_stripe__payments.sql

----------------------------------------

TITLE: Configuring Materialization in Project YAML
DESCRIPTION: Sets up model materialization configuration in the dbt_project.yml file. Defines the materialization strategy at the project level using the config-version and models configuration.

LANGUAGE: yaml
CODE:
[config-version]: 2

models:
  [<resource-path>]:
    +materialized: [<materialization_name>]

----------------------------------------

TITLE: Unit Test with Dictionary Format
DESCRIPTION: Example of a unit test using dictionary format for both input and output data, testing email address validation logic.

LANGUAGE: yml
CODE:
unit_tests:
  - name: test_is_valid_email_address # this is the unique name of the test
    model: dim_customers # name of the model I'm unit testing
    given: # the mock data for your inputs
      - input: ref('stg_customers')
        rows:
         - {email: cool@example.com,     email_top_level_domain: example.com}
         - {email: cool@unknown.com,     email_top_level_domain: unknown.com}
         - {email: badgmail.com,         email_top_level_domain: gmail.com}
         - {email: missingdot@gmailcom,  email_top_level_domain: gmail.com}
      - input: ref('top_level_email_domains')
        rows:
         - {tld: example.com}
         - {tld: gmail.com}
    expect: # the expected output given the inputs above
      rows:
        - {email: cool@example.com,    is_valid_email_address: true}
        - {email: cool@unknown.com,    is_valid_email_address: false}
        - {email: badgmail.com,        is_valid_email_address: false}
        - {email: missingdot@gmailcom, is_valid_email_address: false}

----------------------------------------

TITLE: Setting dbt Resource Properties
DESCRIPTION: Properties are declared in properties.yml files to describe resources such as models, snapshots, seeds, and tests. They can include descriptions, tests, and metadata about the resources.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: my_model
    description: "A description of my model"
    columns:
      - name: id
        tests:
          - unique
          - not_null

----------------------------------------

TITLE: Analyzing tables after creation in Apache Spark
DESCRIPTION: Example of using post-hooks to analyze tables after creation in Apache Spark. This shows how to define multiple hooks and use macros in hooks.

LANGUAGE: yaml
CODE:
models:
  jaffle_shop: # this is the project name
    marts:
      finance:
        +post-hook:
          # this can be a list
          - "analyze table {{ this }} compute statistics for all columns"
          # or call a macro instead
          - "{{ analyze_table() }}"

----------------------------------------

TITLE: Defining Ratio Metric Structure in YAML for dbt
DESCRIPTION: This snippet outlines the complete specification for defining ratio metrics in dbt. It includes all possible parameters such as name, description, type, label, and type_params with numerator and denominator details.

LANGUAGE: yaml
CODE:
metrics:
  - name: The metric name # Required
    description: the metric description # Optional
    type: ratio # Required
    label: String that defines the display value in downstream tools. (such as orders_total or "orders_total") #Required
    type_params: # Required
      numerator: The name of the metric used for the numerator, or structure of properties # Required
        name: Name of metric used for the numerator # Required
        filter: Filter for the numerator # Optional
        alias: Alias for the numerator # Optional
      denominator: The name of the metric used for the denominator, or structure of properties # Required
        name: Name of metric used for the denominator # Required
        filter: Filter for the denominator # Optional
        alias: Alias for the denominator # Optional

----------------------------------------

TITLE: Accessing Config in Incremental Materialization
DESCRIPTION: Example showing how to access the unique_key configuration in an incremental materialization using the config.get function.

LANGUAGE: jinja
CODE:
{% materialization incremental, default -%}
  {%- set unique_key = config.get('unique_key') -%}
  ...

----------------------------------------

TITLE: YAML Configuration Error Example
DESCRIPTION: Example of invalid YAML configuration in dbt_project.yml showing incorrect indentation that causes compilation errors.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: customers
      columns: # this is indented too far!
      - name: customer_id
        tests:
          - unique
          - not_null

----------------------------------------

TITLE: Configuring dbt Model with YAML
DESCRIPTION: Example YAML configuration for a dbt model named 'events', demonstrating proper formatting, column definitions, and test specifications. Shows how to define column metadata, apply tests, and establish relationships between models.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: events
    columns:
      - name: event_id
        description: This is a unique identifier for the event
        tests:
          - unique
          - not_null

      - name: event_time
        description: "When the event occurred in UTC (eg. 2018-01-01 12:00:00)"
        tests:
          - not_null

      - name: user_id
        description: The ID of the user who recorded the event
        tests:
          - not_null
          - relationships:
              to: ref('users')
              field: id

----------------------------------------

TITLE: Unit Test with SQL Format
DESCRIPTION: Example of a unit test using SQL format for reference data and fixture-based output, demonstrating SQL-based data input.

LANGUAGE: yml
CODE:
unit_tests:
  - name: test_is_valid_email_address # this is the unique name of the test
    model: dim_customers # name of the model I'm unit testing
    given: # the mock data for your inputs
      - input: ref('stg_customers')
        rows:
         - {email: cool@example.com,     email_top_level_domain: example.com}
         - {email: cool@unknown.com,     email_top_level_domain: unknown.com}
         - {email: badgmail.com,         email_top_level_domain: gmail.com}
         - {email: missingdot@gmailcom,  email_top_level_domain: gmail.com}
      - input: ref('top_level_email_domains')
        format: sql
        rows: |
          select 'example.com' as tld union all
          select 'gmail.com' as tld
    expect: # the expected output given the inputs above
      format: sql
      fixture: valid_email_address_fixture_output

----------------------------------------

TITLE: Implementing a Simple Microbatch Incremental Model
DESCRIPTION: This SQL model shows a simplified implementation of a microbatch incremental model, demonstrating how it eliminates the need for explicit incremental logic.

LANGUAGE: sql
CODE:
{{
    config(
        materialized='incremental',
        incremental_strategy='microbatch',
        event_time='event_occured_at',
        batch_size='day',
        lookback=3,
        begin='2020-01-01',
        full_refresh=false
    )
}}

select * from {{ ref('stg_events') }} -- this ref will be auto-filtered

----------------------------------------

TITLE: Configuring dbt Project Default Settings
DESCRIPTION: Demonstrates how to set default materializations and schemas for different parts of the dbt project using the dbt_project.yml file.

LANGUAGE: yaml
CODE:
-- dbt_project.yml

models:
  jaffle_shop:
    staging:
      +materialized: view
    intermediate:
      +materialized: ephemeral
    marts:
      +materialized: table
      finance:
        +schema: finance
      marketing:
        +schema: marketing

----------------------------------------

TITLE: Basic Incremental Model Configuration in SQL
DESCRIPTION: Demonstrates the basic configuration of an incremental model using the config block with materialized parameter.

LANGUAGE: sql
CODE:
{{
    config(
        materialized='incremental'
    )
}}

select ...


----------------------------------------

TITLE: Setting a Custom Overview in Markdown
DESCRIPTION: This snippet shows how to create a custom overview for dbt Docs using a special __overview__ docs block. It includes an example of providing project-specific information and external links.

LANGUAGE: markdown
CODE:
{% docs __overview__ %}
# Monthly Recurring Revenue (MRR) playbook.
This dbt project is a worked example to demonstrate how to model subscription
revenue. **Check out the full write-up \[here](https://blog.getdbt.com/modeling-subscription-revenue/),
as well as the repo for this project \[here](https://github.com/dbt-labs/mrr-playbook/).**
...

{% enddocs %}

----------------------------------------

TITLE: Configuration Example in dbt_project.yml
DESCRIPTION: Example showing how to configure models in a dbt project using the dbt_project.yml file with nested paths and the required + prefix for config keys.

LANGUAGE: yaml
CODE:
models:
  jaffle_shop:  # project name
    +materialized: view
    marketing:
      +materialized: table
      +schema: marketing

    sales:
      +materialized: incremental
      +schema: sales

----------------------------------------

TITLE: Configuring Table Materialization in Properties YAML
DESCRIPTION: This YAML configuration in a properties.yml file sets the materialization type for a model named 'events' to 'table'.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: events
    config:
      materialized: table

----------------------------------------

TITLE: Building and Testing Erroneous Models and Related Changes in dbt
DESCRIPTION: This command shows how to rebuild and retest erroneous models and related changes using dbt's build command, state comparison, and result status features. It's useful for comprehensive debugging and fixing of failed runs.

LANGUAGE: bash
CODE:
dbt build --select state:modified+ result:error+ --defer --state path/to/prod/artifacts

----------------------------------------

TITLE: Granting Postgres Permissions for dbt User
DESCRIPTION: This SQL snippet demonstrates how to grant the necessary permissions for a dbt user in a Postgres database. It includes connecting to the database, granting read permissions on the source schema, creating and owning the destination schema, and granting write permissions on the destination schema.

LANGUAGE: sql
CODE:
grant connect on database database_name to user_name;

-- Grant read permissions on the source schema
grant usage on schema source_schema to user_name;
grant select on all tables in schema source_schema to user_name;
alter default privileges in schema source_schema grant select on tables to user_name;

-- Create destination schema and make user_name the owner
create schema if not exists destination_schema;
alter schema destination_schema owner to user_name;

-- Grant write permissions on the destination schema
grant usage on schema destination_schema to user_name;
grant create on schema destination_schema to user_name;
grant insert, update, delete, truncate on all tables in schema destination_schema to user_name;
alter default privileges in schema destination_schema grant insert, update, delete, truncate on tables to user_name;

----------------------------------------

TITLE: Configuring Materializations in dbt Project YAML
DESCRIPTION: This YAML configuration in dbt_project.yml sets the materialization strategy for different model directories. It materializes all models in the 'events' directory as tables, while leaving models in the 'csvs' directory as the default view materialization.

LANGUAGE: yaml
CODE:
name: my_project
version: 1.0.0
config-version: 2

models:
  my_project:
    events:
      # materialize all models in models/events as tables
      +materialized: table
    csvs:
      # this is redundant, and does not need to be set
      +materialized: view

----------------------------------------

TITLE: Configuring Snapshots in dbt_project.yml
DESCRIPTION: Project-level snapshot configuration in dbt, including target schema, database, strategy and other key settings. Supports both pre-1.9 and post-1.9 versions.

LANGUAGE: yaml
CODE:
snapshots:
  [<resource-path>]:
    [+]target_schema: <string>
    [+]target_database: <string>
    [+]unique_key: <column_name_or_expression>
    [+]strategy: timestamp | check
    [+]updated_at: <column_name>
    [+]check_cols: [<column_name>] | all
    [+]invalidate_hard_deletes: true | false

----------------------------------------

TITLE: Real-World Unit Test Example
DESCRIPTION: Complex YAML configuration demonstrating a real-world unit test for order items summary, including multiple inputs and expected outputs.

LANGUAGE: yaml
CODE:
unit_tests:

  - name: test_order_items_count_drink_items_with_zero_drinks
    description: >
      Scenario: Order without any drinks
        When the `order_items_summary` table is built
        Given an order with nothing but 1 food item
        Then the count of drink items is 0

    # Model
    model: order_items_summary

    # Inputs
    given:
      - input: ref('order_items')
        rows:
          - {
              order_id: 76,
              order_item_id: 3,
              is_drink_item: false,
            }
      - input: ref('stg_orders')
        rows:
          - { order_id: 76 }

    # Output
    expect:
      rows:
        - {
            order_id: 76,
            count_drink_items: 0,
          }

----------------------------------------

TITLE: Deserializing YAML to Python Object with fromyaml in dbt
DESCRIPTION: This snippet demonstrates how to use the fromyaml context method to convert a YAML string into a Python dictionary. It then shows how to access and modify the resulting data structure.

LANGUAGE: jinja
CODE:
{% set my_yml_str -%}

dogs:
 - good
 - bad

{%- endset %}

{% set my_dict = fromyaml(my_yml_str) %}

{% do log(my_dict['dogs'], info=true) %}
-- ["good", "bad"]

{% do my_dict['dogs'].pop() %}
{% do log(my_dict['dogs'], info=true) %}
-- ["good"]

----------------------------------------

TITLE: Creating a dbt Project Style Guide Template in Markdown
DESCRIPTION: This snippet provides a comprehensive template for a dbt project style guide. It covers SQL style, model organization, naming conventions, configurations, testing practices, and CTE usage. The guide is designed to be customized and used as a starting point for teams implementing dbt projects.

LANGUAGE: markdown
CODE:
# dbt Example Style Guide

## SQL Style

- Use lowercase keywords.
- Use trailing commas.

## Model Organization

Our models (typically) fit into two main categories:\

- Staging &mdash; Contains models that clean and standardize data.        
- Marts &mdash; Contains models which combine or heavily transform data. 

Things to note:

- There are different types of models that typically exist in each of the above categories. See [Model Layers](#model-layers) for more information.
- Read [How we structure our dbt projects](/best-practices/how-we-structure/1-guide-overview) for an example and more details around organization.

## Model Layers

- Only models in `staging` should select from [sources](https://docs.getdbt.com/docs/building-a-dbt-project/using-sources).
- Models not in the `staging` folder should select from [refs](https://docs.getdbt.com/reference/dbt-jinja-functions/ref).

## Model File Naming and Coding

- All objects should be plural.  
  Example: `stg_stripe__invoices.sql` vs. `stg_stripe__invoice.sql`

- All models should use the naming convention `<type/dag_stage>_<source/topic>__<additional_context>`. See [this article](https://docs.getdbt.com/blog/stakeholder-friendly-model-names) for more information.

  - Models in the **staging** folder should use the source's name as the `<source/topic>` and the entity name as the `additional_context>`.

    Examples:

    - seed_snowflake_spend.csv
    - base_stripe\_\_invoices.sql
    - stg_stripe\_\_customers.sql
    - stg_salesforce\_\_customers.sql
    - int_customers\_\_unioned.sql
    - fct_orders.sql

- Schema, table, and column names should be in `snake_case`.

- Limit the use of abbreviations that are related to domain knowledge. An onboarding employee will understand `current_order_status` better than `current_os`.

- Use names based on the _business_ rather than the source terminology.

- Each model should have a primary key to identify the unique row and should be named `<object>_id`. For example, `account_id`. This makes it easier to know what `id` is referenced in downstream joined models.

- For `base` or `staging` models, columns should be ordered in categories, where identifiers are first and date/time fields are at the end.
- Date/time columns should be named according to these conventions:

  - Timestamps: `<event>_at`  
    Format: UTC  
    Example: `created_at`

  - Dates: `<event>_date`
    Format: Date  
    Example: `created_date`

- Booleans should be prefixed with `is_` or `has_`.
  Example: `is_active_customer` and `has_admin_access`

- Price/revenue fields should be in decimal currency (for example, `19.99` for $19.99; many app databases store prices as integers in cents). If a non-decimal currency is used, indicate this with suffixes. For example, `price_in_cents`.

- Avoid using reserved words (such as [these](https://docs.snowflake.com/en/sql-reference/reserved-keywords.html) for Snowflake) as column names.

- Consistency is key! Use the same field names across models where possible. For example, a key to the `customers` table should be named `customer_id` rather than `user_id`.

## Model Configurations

- Model configurations at the [folder level](https://docs.getdbt.com/reference/model-configs#configuring-directories-of-models-in-dbt_projectyml) should be considered (and if applicable, applied) first.
- More specific configurations should be applied at the model level [using one of these methods](https://docs.getdbt.com/reference/model-configs#apply-configurations-to-one-model-only).
- Models within the `marts` folder should be materialized as `table` or `incremental`.
  - By default, `marts` should be materialized as `table` within `dbt_project.yml`.
  - If switching to `incremental`, this should be specified in the model's configuration.

## Testing

- At a minimum, `unique` and `not_null` tests should be applied to the expected primary key of each model.

## CTEs

For more information about why we use so many CTEs, read [this glossary entry](https://docs.getdbt.com/terms/cte).

- Where performance permits, CTEs should perform a single, logical unit of work.
- CTE names should be as verbose as needed to convey what they do.
- CTEs with confusing or noteable logic should be commented with SQL comments as you would with any complex functions and should be located above the CTE.
- CTEs duplicated across models should be pulled out and created as their own models.

----------------------------------------

TITLE: Defining Derived Metrics Schema in YAML
DESCRIPTION: This snippet shows the complete specification for defining derived metrics in YAML format, including all possible parameters and their descriptions.

LANGUAGE: yaml
CODE:
metrics:
  - name: the metric name # Required
    description: the metric description # Optional
    type: derived # Required
    label: The value that will be displayed in downstream tools #Required
    type_params: # Required
      expr: the derived expression # Required
      metrics: # The list of metrics used in the derived metrics # Required
        - name: the name of the metrics. must reference a metric you have already defined # Required
          alias: optional alias for the metric that you can use in the expr # Optional
          filter: optional filter to apply to the metric # Optional
          offset_window: set the period for the offset window, such as 1 month. This will return the value of the metric one month from the metric time. # Optional

----------------------------------------

TITLE: Building and Testing Fresh Sources and Downstream Models in dbt
DESCRIPTION: This set of commands shows how to build and test models based on fresh data sources and their downstream dependencies. It uses dbt's source freshness check and the build command with state comparison.

LANGUAGE: bash
CODE:
dbt source freshness
dbt build --select source_status:fresher+ --state path/to/prod/artifacts

----------------------------------------

TITLE: Structuring SQL with CTEs in dbt
DESCRIPTION: Example of organizing SQL code using Common Table Expressions (CTEs) with import, logical, and final CTEs for better code organization and readability.

LANGUAGE: sql
CODE:
with

import_orders as (

    -- query only non-test orders
    select * from {{ source('jaffle_shop', 'orders') }}
    where amount > 0
),

import_customers as (
    select * from {{ source('jaffle_shop', 'customers') }}
),

logical_cte_1 as (

    -- perform some math on import_orders

),

logical_cte_2 as (

    -- perform some math on import_customers
),

final_cte as (

    -- join together logical_cte_1 and logical_cte_2
)

select * from final_cte

----------------------------------------

TITLE: SQL Full Outer Join Implementation Example
DESCRIPTION: Example SQL demonstrating how MetricFlow implements a full outer join between sales and returns tables.

LANGUAGE: sql
CODE:
select
  sales.user_id,
  sales.total_sales,
  returns.total_returns
from sales
full outer join returns
  on sales.user_id = returns.user_id
where sales.user_id is not null or returns.user_id is not null;

----------------------------------------

TITLE: Implementing Orders Mart Model in dbt
DESCRIPTION: SQL implementation of an orders mart model that combines order data with payment information. The model joins staged order data with intermediate payment calculations.

LANGUAGE: sql
CODE:
-- orders.sql

with

orders as  (

    select * from {{ ref('stg_jaffle_shop__orders' )}}

),

order_payments as (

    select * from {{ ref('int_payments_pivoted_to_orders') }}

),

orders_and_order_payments_joined as (

    select
        orders.order_id,
        orders.customer_id,
        orders.order_date,
        coalesce(order_payments.total_amount, 0) as amount,
        coalesce(order_payments.gift_card_amount, 0) as gift_card_amount

    from orders

    left join order_payments on orders.order_id = order_payments.order_id

)

select * from orders_and_payments_joined

----------------------------------------

TITLE: Implementing Cross-Database Date Addition in dbt
DESCRIPTION: This snippet demonstrates how to create a cross-database compatible dateadd macro using dbt's dispatch functionality. It includes the main macro and adapter-specific implementations for BigQuery, Postgres, and Redshift.

LANGUAGE: sql
CODE:
{% macro dateadd(datepart, interval, from_date_or_timestamp) %}

  {{ return(adapter.dispatch('dateadd', 'dbt_utils')(datepart, interval, from_date_or_timestamp)) }}

{% endmacro %}

{% macro default__dateadd(datepart, interval, from_date_or_timestamp) %}

    dateadd(

        {{ datepart }},

        {{ interval }},

        {{ from_date_or_timestamp }}

        )

{% endmacro %}

{% macro bigquery__dateadd(datepart, interval, from_date_or_timestamp) %}

        datetime_add(

            cast( {{ from_date_or_timestamp }} as datetime),

        interval {{ interval }} {{ datepart }}

        )

{% endmacro %}

{% macro postgres__dateadd(datepart, interval, from_date_or_timestamp) %}

    {{ from_date_or_timestamp }} + ((interval '1 {{ datepart }}') * ({{ interval }}))

{% endmacro %}

{# redshift should use default instead of postgres #}

{% macro redshift__dateadd(datepart, interval, from_date_or_timestamp) %}

    {{ return(dbt_utils.default__dateadd(datepart, interval, from_date_or_timestamp)) }}

{% endmacro %}

----------------------------------------

TITLE: Ratio Metric with Filtered Input in dbt YAML
DESCRIPTION: This example shows how to define a ratio metric with a filter applied to the input metric. It demonstrates the use of the 'filter' parameter to constrain the numerator, and the 'alias' parameter to avoid naming conflicts in SQL queries.

LANGUAGE: yaml
CODE:
metrics:
  - name: frequent_purchaser_ratio
    description: Fraction of active users who qualify as frequent purchasers
    type: ratio
    type_params:
      numerator:
        name: distinct_purchasers
        filter: |
          {{Dimension('customer__is_frequent_purchaser')}}
        alias: frequent_purchasers
      denominator:
        name: distinct_purchasers

----------------------------------------

TITLE: DATEADD Implementation using dbt macro
DESCRIPTION: Demonstrates the standardized dbt macro syntax for date addition that works across different database platforms.

LANGUAGE: sql
CODE:
{{ dateadd(datepart, interval, from_date_or_timestamp) }}

LANGUAGE: sql
CODE:
{{ dateadd(datepart="month", interval=1, from_date_or_timestamp="'2021-08-12'") }}

----------------------------------------

TITLE: Displaying dbt Project Models Directory Structure
DESCRIPTION: Shows the recommended folder structure for organizing dbt models across staging, intermediate, and marts layers with corresponding YAML configuration files.

LANGUAGE: shell
CODE:
models
‚îú‚îÄ‚îÄ intermediate
‚îÇ   ‚îî‚îÄ‚îÄ finance
‚îÇ       ‚îú‚îÄ‚îÄ _int_finance__models.yml
‚îÇ       ‚îî‚îÄ‚îÄ int_payments_pivoted_to_orders.sql
‚îú‚îÄ‚îÄ marts
‚îÇ   ‚îú‚îÄ‚îÄ finance
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _finance__models.yml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ orders.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ payments.sql
‚îÇ   ‚îî‚îÄ‚îÄ marketing
‚îÇ       ‚îú‚îÄ‚îÄ _marketing__models.yml
‚îÇ       ‚îî‚îÄ‚îÄ customers.sql
‚îú‚îÄ‚îÄ staging
‚îÇ   ‚îú‚îÄ‚îÄ jaffle_shop
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _jaffle_shop__docs.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _jaffle_shop__models.yml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _jaffle_shop__sources.yml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_jaffle_shop__customers.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ base_jaffle_shop__deleted_customers.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stg_jaffle_shop__customers.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stg_jaffle_shop__orders.sql
‚îÇ   ‚îî‚îÄ‚îÄ stripe
‚îÇ       ‚îú‚îÄ‚îÄ _stripe__models.yml
‚îÇ       ‚îú‚îÄ‚îÄ _stripe__sources.yml
‚îÇ       ‚îî‚îÄ‚îÄ stg_stripe__payments.sql
‚îî‚îÄ‚îÄ utilities
    ‚îî‚îÄ‚îÄ all_dates.sql

----------------------------------------

TITLE: Executing dbt Model
DESCRIPTION: Demonstrates how to run a dbt model using the command line interface. This command triggers the transformation of data within your warehouse.

LANGUAGE: shell
CODE:
dbt run

----------------------------------------

TITLE: Creating a Generic Not Null Test
DESCRIPTION: Define a generic test that can be reused to check for null values across different models and columns.

LANGUAGE: sql
CODE:
{% test not_null(model, column_name) %}

    select *
    from {{ model }}
    where {{ column_name }} is null

{% endtest %}

----------------------------------------

TITLE: Custom Schema Name Generation Implementation
DESCRIPTION: Example of implementing the built-in alternative pattern for schema name generation using generate_schema_name_for_env.

LANGUAGE: sql
CODE:
{% macro generate_schema_name(custom_schema_name, node) -%}
    {{ generate_schema_name_for_env(custom_schema_name, node) }}
{%- endmacro %}

----------------------------------------

TITLE: Simple Metrics Implementation Example
DESCRIPTION: Practical example showing how to implement multiple simple metrics including customer count and large orders tracking, demonstrating filters, null handling, and dimension references.

LANGUAGE: yaml
CODE:
metrics: 
    - name: customers
      description: Count of customers
      type: simple # Pointers to a measure you created in a semantic model
      label: Count of customers
      type_params:
        measure: 
          name: customers # The measure you are creating a proxy of.
          fill_nulls_with: 0 
          join_to_timespine: true
          alias: customer_count
          filter: {{ Dimension('customer__customer_total') }} >= 20
    - name: large_orders
      description: "Order with order values over 20."
      type: simple
      label: Large orders
      type_params:
        measure: 
          name: orders
      filter: | # For any metric you can optionally include a filter on dimension values
        {{Dimension('customer__order_total_dim')}} >= 20

----------------------------------------

TITLE: Retrieving Columns from Relation in SQL using dbt adapter
DESCRIPTION: This snippet demonstrates how to use the `adapter.get_columns_in_relation` method to retrieve a list of columns in a table.

LANGUAGE: sql
CODE:
{%- set columns = adapter.get_columns_in_relation(this) -%}

{% for column in columns %}
  {{ log("Column: " ~ column, info=true) }}
{% endfor %}

----------------------------------------

TITLE: Configuring Pre-Hook with Macro in SQL Model
DESCRIPTION: Demonstrates how to call a macro within a pre-hook configuration block in a dbt model file.

LANGUAGE: sql
CODE:
{{ config(
    pre_hook=[
      "{{ some_macro() }}"
    ]
) }}

----------------------------------------

TITLE: Defining a Macro in dbt
DESCRIPTION: This snippet shows how to define a macro in dbt that converts cents to dollars with a configurable scale parameter.

LANGUAGE: sql
CODE:
{% macro cents_to_dollars(column_name, scale=2) %}
    ({{ column_name }} / 100)::numeric(16, {{ scale }})
{% endmacro %}

----------------------------------------

TITLE: Full Refresh Command in Bash
DESCRIPTION: Demonstrates how to trigger a full refresh of an incremental model using the command line interface.

LANGUAGE: bash
CODE:
$ dbt run --full-refresh --select my_incremental_model+

----------------------------------------

TITLE: Defining Model Documentation in YAML for dbt
DESCRIPTION: This YAML snippet defines documentation for dbt models including 'customers', 'stg_customers', and 'stg_orders'. It specifies descriptions, column details, and tests for each model.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: customers
    description: One record per customer
    columns:
      - name: customer_id
        description: Primary key
        tests:
          - unique
          - not_null
      - name: first_order_date
        description: NULL when a customer has not yet placed an order.

  - name: stg_customers
    description: This model cleans up customer data
    columns:
      - name: customer_id
        description: Primary key
        tests:
          - unique
          - not_null

  - name: stg_orders
    description: This model cleans up order data
    columns:
      - name: order_id
        description: Primary key
        tests:
          - unique
          - not_null
      - name: status
        tests:
          - accepted_values:
              values: ['placed', 'shipped', 'completed', 'return_pending', 'returned']
      - name: customer_id
        tests:
          - not_null
          - relationships:
              to: ref('stg_customers')
              field: customer_id

----------------------------------------

TITLE: Detecting Missing Columns in SQL using dbt adapter
DESCRIPTION: This snippet demonstrates how to use the `adapter.get_missing_columns` method to detect new columns in a source table and add them to the target table.

LANGUAGE: sql
CODE:
{%- set target_relation = api.Relation.create(
      database='database_name',
      schema='schema_name',
      identifier='table_name') -%}


{% for col in adapter.get_missing_columns(target_relation, this) %}
  alter table {{this}} add column "{{col.name}}" {{col.data_type}};
{% endfor %}

----------------------------------------

TITLE: Defining Complex Metrics with MetricFlow
DESCRIPTION: YAML configuration for defining a complex metric using MetricFlow, calculating the percentage of food orders from returning customers.

LANGUAGE: yaml
CODE:
metrics:
  - name: food_order_pct_of_order_total_returning
    description: Revenue from food orders from returning customers
    label: "Food % of Order Total"
    type: ratio
    type_params:
      numerator: food_order
      denominator: order_total
    filter: |
      {{ Dimension('customer__is_new_customer') }} = false

----------------------------------------

TITLE: Configuring Variables in dbt Project YAML
DESCRIPTION: Example of defining project variables in dbt_project.yml file, showing global and package-scoped variable declarations for start dates, platforms, and app IDs.

LANGUAGE: yaml
CODE:
name: my_dbt_project
version: 1.0.0

config-version: 2

vars:
  # The `start_date` variable will be accessible in all resources
  start_date: '2016-06-01'

  # The `platforms` variable is only accessible to resources in the my_dbt_project project
  my_dbt_project:
    platforms: ['web', 'mobile']

  # The `app_ids` variable is only accessible to resources in the snowplow package
  snowplow:
    app_ids: ['marketing', 'app', 'landing-page']

models:
    ...

----------------------------------------

TITLE: Basic Python Model Structure
DESCRIPTION: Defines the basic structure of a Python model in dbt, including the required 'model' function and its parameters.

LANGUAGE: python
CODE:
def model(dbt, session):

    ...

    return final_df

----------------------------------------

TITLE: Defining Basic Conversion Metric Structure in YAML
DESCRIPTION: Basic YAML structure for defining a conversion metric with all possible parameters and configurations.

LANGUAGE: yaml
CODE:
metrics:
  - name: The metric name # Required
    description: The metric description # Optional
    type: conversion # Required
    label: YOUR_LABEL # Required
    type_params: # Required
      conversion_type_params: # Required
        entity: ENTITY # Required
        calculation: CALCULATION_TYPE # Optional
        base_measure: 
          name: The name of the measure # Required
          fill_nulls_with: Set the value in your metric definition instead of null # Optional
          join_to_timespine: true/false # Optional
          filter: The filter used to apply to the base measure. # Optional
        conversion_measure:
          name: The name of the measure # Required
          fill_nulls_with: Set the value in your metric definition instead of null # Optional
          join_to_timespine: true/false # Optional
        window: TIME_WINDOW # Optional
        constant_properties: # Optional
          - base_property: DIMENSION or ENTITY # Required
            conversion_property: DIMENSION or ENTITY

----------------------------------------

TITLE: Configuring BigQuery Connection Profile
DESCRIPTION: Create a profiles.yml file to store BigQuery connection details.

LANGUAGE: yaml
CODE:
jaffle_shop:
    target: dev
    outputs:
        dev:
            type: bigquery
            method: service-account
            keyfile: /Users/BBaggins/.dbt/dbt-tutorial-project-331118.json
            project: grand-highway-265418
            dataset: dbt_bbagins
            threads: 1
            timeout_seconds: 300
            location: US
            priority: interactive

----------------------------------------

TITLE: Configuring dbt Exposures in YAML
DESCRIPTION: Shows how to configure dbt exposures using the 'config' property in a YAML file. This includes settings for enabling/disabling the exposure and metadata.

LANGUAGE: yaml
CODE:
version: 2

exposures:
  - name: <exposure_name>
    config:
      [enabled](/reference/resource-configs/enabled): true | false
      [meta](/reference/resource-configs/meta): {dictionary}

----------------------------------------

TITLE: Configuring IAM User Authentication via Extended Attributes in YAML
DESCRIPTION: Example YAML configuration for setting up IAM user authentication with Redshift Serverless using environment variables for sensitive credentials.

LANGUAGE: yaml
CODE:
host: my-production-instance.myregion.redshift-serverless.amazonaws.com
method: iam
region: us-east-2
access_key_id: '{{ env_var(''DBT_ENV_ACCESS_KEY_ID'') }}'
secret_access_key: '{{ env_var(''DBT_ENV_SECRET_ACCESS_KEY'') }}'

----------------------------------------

TITLE: Configuring Database Override in SQL Model File
DESCRIPTION: This SQL configuration changes a specific model to be built into a database called jaffle_shop using the config block at the top of the file.

LANGUAGE: sql
CODE:
{{ config(database="jaffle_shop") }}

select * from ...

----------------------------------------

TITLE: Configuring pre-hooks and post-hooks in model properties files
DESCRIPTION: Define pre-hooks and post-hooks for models in YAML properties files. Hooks can be single SQL statements or lists of statements.

LANGUAGE: yaml
CODE:
models:
  - name: [<model_name>]
    config:
      pre_hook: <sql-statement> | [<sql-statement>]
      post_hook: <sql-statement> | [<sql-statement>]

----------------------------------------

TITLE: Configuring dbt Models in YAML
DESCRIPTION: Example of how to configure a dbt model using the 'config' property in a YAML file. This allows setting model-specific configurations.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: <model_name>
    config:
      [<model_config>](/reference/model-configs): <config_value>
      ...

----------------------------------------

TITLE: Configuring Python Models
DESCRIPTION: Demonstrates how to configure Python models using the dbt.config() method within the model file.

LANGUAGE: python
CODE:
def model(dbt, session):

    # setting configuration
    dbt.config(materialized="table")

----------------------------------------

TITLE: Setting dbt Variables via Command Line
DESCRIPTION: Examples of setting or overriding dbt variables using the --vars command line option, demonstrating different YAML dictionary formats.

LANGUAGE: bash
CODE:
$ dbt run --vars '{"key": "value", "date": 20180101}'

LANGUAGE: bash
CODE:
$ dbt run --vars '{key: value, date: 20180101}'

LANGUAGE: bash
CODE:
$ dbt run --vars 'key: value'

----------------------------------------

TITLE: Referencing Other Models in Python
DESCRIPTION: Shows how to reference other models or sources within a Python model using dbt.ref() and dbt.source() methods.

LANGUAGE: python
CODE:
def model(dbt, session):

    # DataFrame representing an upstream model
    upstream_model = dbt.ref("upstream_model_name")

    # DataFrame representing an upstream source
    upstream_source = dbt.source("upstream_source_name", "table_name")

    ...

----------------------------------------

TITLE: Simple Metrics Basic Configuration Schema
DESCRIPTION: Template showing the complete specification structure for configuring simple metrics in dbt, including all available parameters and their organization.

LANGUAGE: yaml
CODE:
metrics:
  - name: The metric name # Required
    description: the metric description # Optional
    type: simple # Required
    label: The value that will be displayed in downstream tools # Required
    type_params: # Required
      measure: 
        name: The name of your measure # Required
        alias: The alias applied to the measure. # Optional
        filter: The filter applied to the measure. # Optional
        fill_nulls_with: Set value instead of null  (such as zero) # Optional
        join_to_timespine: true/false # Boolean that indicates if the aggregated measure should be joined to the time spine table to fill in missing dates. # Optional

----------------------------------------

TITLE: Configuring Analysis Properties in dbt YAML
DESCRIPTION: YAML configuration schema for defining analysis properties in dbt projects. Specifies required and optional properties including name, description, documentation settings, configuration tags, and column definitions. Files can be placed in analyses/ or models/ directories with flexible naming conventions.

LANGUAGE: yaml
CODE:
version: 2

analyses:
  - name: <analysis_name> # required
    description: <markdown_string>
    docs:
      show: true | false
      node_color: <color_id> # Use name (such as node_color: purple) or hex code with quotes (such as node_color: "#cd7f32")
    config:
      tags: <string> | [<string>]
    columns:
      - name: <column_name>
        description: <markdown_string>
      - name: ... # declare properties of additional columns

  - name: ... # declare properties of additional analyses

----------------------------------------

TITLE: Configuring Redshift Database Authentication in YAML
DESCRIPTION: Example profiles.yml configuration for Redshift using database (password-based) authentication method. Includes both required and optional configuration parameters.

LANGUAGE: yaml
CODE:
company-name:
  target: dev
  outputs:
    dev:
      type: redshift
      host: hostname.region.redshift.amazonaws.com
      user: username
      password: password1
      dbname: analytics
      schema: analytics
      port: 5439

      # Optional Redshift configs:
      sslmode: prefer
      role: None
      ra3_node: true 
      autocommit: true 
      threads: 4
      connect_timeout: None

----------------------------------------

TITLE: Configuring Sort and Distribution Keys in Redshift Models
DESCRIPTION: Examples of configuring sort keys and distribution keys for Redshift tables using dbt config blocks. Shows single sort key, multiple sort keys, and interleaved sort key configurations.

LANGUAGE: sql
CODE:
-- Example with one sort key
{{ config(materialized='table', sort='reporting_day', dist='unique_id') }}

select ...


-- Example with multiple sort keys
{{ config(materialized='table', sort=['category', 'region', 'reporting_day'], dist='received_at') }}

select ...


-- Example with interleaved sort keys
{{ config(materialized='table',
          sort_type='interleaved'
          sort=['category', 'region', 'reporting_day'],
          dist='unique_id')
}}

select ...

----------------------------------------

TITLE: SQL CROSS JOIN with Date Spine and Users Example
DESCRIPTION: Shows a practical example of cross joining a date spine table with a users table to create a cartesian product of all dates for each user. Uses dbt ref syntax for table references.

LANGUAGE: sql
CODE:
select
   users.user_id as user_id,
   date.date as date
from {{ ref('users') }} as users
cross join {{ ref('date_spine') }} as date
order by 1

----------------------------------------

TITLE: Configuring Postgres Profile in YAML for dbt
DESCRIPTION: This YAML configuration sets up a Postgres target in the dbt profiles.yml file. It includes essential connection parameters and optional settings for SSL, roles, and connection behavior.

LANGUAGE: yaml
CODE:
company-name:
  target: dev
  outputs:
    dev:
      type: postgres
      host: [hostname]
      user: [username]
      password: [password]
      port: [port]
      dbname: [database name] # or database instead of dbname
      schema: [dbt schema]
      threads: [optional, 1 or more]
      keepalives_idle: 0 # default 0, indicating the system default. See below
      connect_timeout: 10 # default 10 seconds
      retries: 1  # default 1 retry on error/timeout when opening connections
      search_path: [optional, override the default postgres search_path]
      role: [optional, set the role dbt assumes when executing queries]
      sslmode: [optional, set the sslmode used to connect to the database]
      sslcert: [optional, set the sslcert to control the certifcate file location]
      sslkey: [optional, set the sslkey to control the location of the private key]
      sslrootcert: [optional, set the sslrootcert config value to a new file path in order to customize the file location that contain root certificates]

----------------------------------------

TITLE: Configuring Saved Query in semantic_model.yml (dbt v1.9+)
DESCRIPTION: Example of configuring a saved query with caching, tags, and exports in the semantic_model.yml file for dbt version 1.9 and higher.

LANGUAGE: yaml
CODE:
saved_queries:
  - name: test_saved_query
    description: "{{ doc('saved_query_description') }}"
    label: Test saved query
    config:
      cache:
        enabled: true | false
        tags: 'my_tag'
    query_params:
      metrics:
        - simple_metric
      group_by:
        - "Dimension('user__ds')"
      where:
        - "{{ Dimension('user__ds', 'DAY') }} <= now()"
        - "{{ Dimension('user__ds', 'DAY') }} >= '2023-01-01'"
    exports:
      - name: my_export
        config:
          export_as: table 
          alias: my_export_alias
          schema: my_export_schema_name

----------------------------------------

TITLE: Basic Package Installation Configuration
DESCRIPTION: Example of basic package configuration in packages.yml showing different ways to specify package sources including hub packages, git repositories, and local packages.

LANGUAGE: yaml
CODE:
packages:
  - package: dbt-labs/snowplow
    version: 0.7.0

  - git: "https://github.com/dbt-labs/dbt-utils.git"
    revision: 0.9.2

  - local: /opt/dbt/redshift

----------------------------------------

TITLE: Defining Conversion Metrics in YAML
DESCRIPTION: Example of defining a conversion metric in a YAML file. It shows the structure and required fields for creating a conversion metric.

LANGUAGE: yaml
CODE:
metrics:
  - name: The metric name 
    description: The metric description 
    type: conversion 
    label: YOUR_LABEL 
    type_params: #
      conversion_type_params: 
        entity: ENTITY
        calculation: CALCULATION_TYPE 
        base_measure: 
          name: The name of the measure 
          fill_nulls_with: Set the value in your metric definition instead of null (such as zero) 
          join_to_timespine: true/false
        conversion_measure:
          name: The name of the measure 
          fill_nulls_with: Set the value in your metric definition instead of null (such as zero) 
          join_to_timespine: true/false
        window: TIME_WINDOW
        constant_properties:
          - base_property: DIMENSION or ENTITY 
            conversion_property: DIMENSION or ENTITY 

----------------------------------------

TITLE: Configuring Sources YAML
DESCRIPTION: YAML configuration for declaring source tables from the jaffle_shop database

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: jaffle_shop
    description: This is a replica of the Postgres database used by our app
    database: raw
    schema: jaffle_shop
    tables:
      - name: customers
        description: One record per customer.
      - name: orders
        description: One record per order. Includes cancelled and deleted orders.

----------------------------------------

TITLE: Executing dbt Build Command Example
DESCRIPTION: Example showing the execution of dbt build command that processes seeds, models, tests and snapshots in the correct dependency order. The output demonstrates successful execution of 1 seed, 1 view model, 4 tests, and 1 snapshot.

LANGUAGE: bash
CODE:
$ dbt build
Running with dbt=0.21.0-b2
Found 1 model, 4 tests, 1 snapshot, 1 analysis, 341 macros, 0 operations, 1 seed file, 2 sources, 2 exposures

18:49:43 | Concurrency: 1 threads (target='dev')
18:49:43 |
18:49:43 | 1 of 7 START seed file dbt_jcohen.my_seed............................ [RUN]
18:49:43 | 1 of 7 OK loaded seed file dbt_jcohen.my_seed........................ [INSERT 2 in 0.09s]
18:49:43 | 2 of 7 START view model dbt_jcohen.my_model.......................... [RUN]
18:49:43 | 2 of 7 OK created view model dbt_jcohen.my_model..................... [CREATE VIEW in 0.12s]
18:49:43 | 3 of 7 START test not_null_my_seed_id................................ [RUN]
18:49:43 | 3 of 7 PASS not_null_my_seed_id...................................... [PASS in 0.05s]
18:49:43 | 4 of 7 START test unique_my_seed_id.................................. [RUN]
18:49:43 | 4 of 7 PASS unique_my_seed_id........................................ [PASS in 0.03s]
18:49:43 | 5 of 7 START snapshot snapshots.my_snapshot.......................... [RUN]
18:49:43 | 5 of 7 OK snapshotted snapshots.my_snapshot.......................... [INSERT 0 5 in 0.27s]
18:49:43 | 6 of 7 START test not_null_my_model_id............................... [RUN]
18:49:43 | 6 of 7 PASS not_null_my_model_id..................................... [PASS in 0.03s]
18:49:43 | 7 of 7 START test unique_my_model_id................................. [RUN]
18:49:43 | 7 of 7 PASS unique_my_model_id....................................... [PASS in 0.02s]
18:49:43 |
18:49:43 | Finished running 1 seed, 1 view model, 4 tests, 1 snapshot in 1.01s.

Completed successfully

Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7

----------------------------------------

TITLE: Configuring Global Incremental Strategy in YAML
DESCRIPTION: Sets the default incremental strategy for all models in the dbt project configuration file.

LANGUAGE: yaml
CODE:
models:
  +incremental_strategy: "insert_overwrite"

----------------------------------------

TITLE: Configuring persist_docs for Models in dbt_project.yml
DESCRIPTION: This snippet shows how to enable persist_docs for models in the dbt_project.yml file. It allows persisting documentation for both relations and columns.

LANGUAGE: yml
CODE:
models:
  [<resource-path>]:
    +persist_docs:
      relation: true
      columns: true

----------------------------------------

TITLE: Configuring Model Grants in YAML
DESCRIPTION: Example of configuring select permissions for a specific model using YAML configuration.

LANGUAGE: yaml
CODE:
models:
  - name: specific_model
    config:
      grants:
        select: ['reporter', 'bi']

----------------------------------------

TITLE: Configuring Column Properties for DBT Models
DESCRIPTION: YAML configuration for defining column properties within DBT models, including data type, description, quote settings, tests, tags, and metadata.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: <model_name>
    columns:
      - name: <column_name>
        data_type: <string>
        description: <markdown_string>
        quote: true | false
        tests: ...
        tags: ...
        meta: ...
      - name: <another_column>
        ...

----------------------------------------

TITLE: Multi-Hop Join Semantic Model Configuration
DESCRIPTION: YAML configuration for implementing multi-hop joins across three related semantic models: sales, user_signup, and country.

LANGUAGE: yaml
CODE:
semantic_models:
  - name: sales
    defaults:
      agg_time_dimension: first_ordered_at
    entities:
      - name: id
        type: primary
      - name: user_id
        type: foreign
    measures:
      - name: average_purchase_price
        agg: avg
        expr: purchase_price
    dimensions:
      - name: metric_time
        type: time
        type_params:
  - name: user_signup
    entities:
      - name: user_id
        type: primary
      - name: country_id
        type: unique
    dimensions:
      - name: signup_date
        type: time
      - name: country_dim

  - name: country
    entities:
      - name: country_id
        type: primary
    dimensions:
      - name: country_name
        type: categorical

----------------------------------------

TITLE: Configuring dbt Sources in YAML
DESCRIPTION: Shows how to configure dbt sources using the 'config' property in a YAML file. This includes configurations for both the source and individual tables within the source.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: <source_name>
    config:
      [<source_config>](/reference/source-configs): <config_value>
    tables:
      - name: <table_name>
        config:
          [<source_config>](/reference/source-configs): <config_value>

----------------------------------------

TITLE: Configuring Custom Schema in SQL Model
DESCRIPTION: Example of setting a custom schema for a specific model using a config block within the SQL file.

LANGUAGE: sql
CODE:
{{ config(schema='marketing') }}

select ...

----------------------------------------

TITLE: Version-Aware Staging Logic
DESCRIPTION: Shows how to implement version-aware transformation logic that handles different business rules based on effective dates.

LANGUAGE: sql
CODE:
select
	cost_id,
	...,
	cost + tax as final_cost,
        1 || '-' || dbt_valid_from as version
from costs_snapshot
where dbt_valid_from <= to_timestamp('02/10/22 08:00:00')

union all

select
	cost_id,
	...,
	cost as final_cost,
        2 || '-' || dbt_valid_from as version
from costs_snapshot
where to_timestamp('02/10/22 08:00:00') between dbt_valid_to and coalesce(dbt_valid_from, to_timestamp('01/01/99 00:00:00'))

----------------------------------------

TITLE: Querying Order Counts and Distinct Customers with SQL COUNT
DESCRIPTION: This SQL query demonstrates the use of COUNT and COUNT DISTINCT functions to calculate the number of orders and unique customers per month. It uses date_part for month extraction and includes a GROUP BY clause for aggregation.

LANGUAGE: sql
CODE:
select
	date_part('month', order_date) as order_month,
	count(order_id) as count_all_orders,
	count(distinct(customer_id)) as count_distinct_customers
from {{ ref('orders') }}
group by 1

----------------------------------------

TITLE: Defining a SQL Model in dbt
DESCRIPTION: Example of a SQL model definition in dbt, showing the structure of a typical model file.

LANGUAGE: sql
CODE:
-- lots of SQL

final as (

    select
        customer_id,
        customer_name,
        -- ... many more ...
    from ...

)

select * from final

----------------------------------------

TITLE: Dynamic Column Generation using run_query
DESCRIPTION: Advanced example showing how to use run_query to dynamically generate SQL columns based on distinct payment methods from a database table.

LANGUAGE: sql
CODE:
{% set payment_methods_query %}
select distinct payment_method from app_data.payments
order by 1
{% endset %}

{% set results = run_query(payment_methods_query) %}

{% if execute %}
{# Return the first column #}
{% set results_list = results.columns[0].values() %}
{% else %}
{% set results_list = [] %}
{% endif %}

select
order_id,
{% for payment_method in results_list %}
sum(case when payment_method = '{{ payment_method }}' then amount end) as {{ payment_method }}_amount,
{% endfor %}
sum(amount) as total_amount
from {{ ref('raw_payments') }}
group by 1

----------------------------------------

TITLE: Config.require Usage
DESCRIPTION: Shows how to use config.require to enforce required configuration parameters in materializations.

LANGUAGE: sql
CODE:
{% materialization incremental, default -%}
  {%- set unique_key = config.require('unique_key') -%}
  ...

----------------------------------------

TITLE: Incremental Model Unit Tests
DESCRIPTION: YAML configuration showing unit tests for an incremental model in both full refresh and incremental modes.

LANGUAGE: yaml
CODE:
unit_tests:
  - name: my_incremental_model_full_refresh_mode
    model: my_incremental_model
    overrides:
      macros:
        is_incremental: false 
    given:
      - input: ref('events')
        rows:
          - {event_id: 1, event_time: 2020-01-01}
    expect:
      rows:
        - {event_id: 1, event_time: 2020-01-01}

  - name: my_incremental_model_incremental_mode
    model: my_incremental_model
    overrides:
      macros:
        is_incremental: true 
    given:
      - input: ref('events')
        rows:
          - {event_id: 1, event_time: 2020-01-01}
          - {event_id: 2, event_time: 2020-01-02}
          - {event_id: 3, event_time: 2020-01-03}
      - input: this 
        rows:
          - {event_id: 1, event_time: 2020-01-01}
    expect:
      rows:
        - {event_id: 2, event_time: 2020-01-02}
        - {event_id: 3, event_time: 2020-01-03}

----------------------------------------

TITLE: Configuring Tags in dbt Project YAML
DESCRIPTION: This snippet shows how to configure tags for models, snapshots, seeds, and saved queries in the dbt_project.yml file. It supports single strings or lists of strings for tag values.

LANGUAGE: yaml
CODE:
models:
  <resource-path>:
    +tags: <string> | [<string>] # Supports single strings or list of strings

snapshots:
  <resource-path>:
    +tags: <string> | [<string>]

seeds:
  <resource-path>:
    +tags: <string> | [<string>]

saved-queries:
  <resource-path>:
    +tags: <string> | [<string>]

----------------------------------------

TITLE: Configuring Dynamic Source Database in YAML
DESCRIPTION: YAML configuration showing how to set up a dynamic source database that changes based on environment variables. This enables flexible database targeting across different deployment environments.

LANGUAGE: yaml
CODE:
sources:
  - name: sensitive_source
    database: "{{ env_var('SENSITIVE_SOURCE_DATABASE') }}"
    tables:
      - name: table_with_pii

----------------------------------------

TITLE: Defining sources in dbt using YAML configuration
DESCRIPTION: This YAML configuration defines sources for a dbt project, specifying the source name, database, and associated tables.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: jaffle_shop # this is the source_name
    database: raw

    tables:
      - name: customers # this is the table_name
      - name: orders

----------------------------------------

TITLE: Executing dbt Clone Command in Bash
DESCRIPTION: Examples of using the dbt clone command with various options. These commands demonstrate cloning models from a specified state to target schemas, with options for selecting specific models, full refresh, and parallel execution.

LANGUAGE: bash
CODE:
# clone all of my models from specified state to my target schema(s)
dbt clone --state path/to/artifacts

# clone one_specific_model of my models from specified state to my target schema(s)
dbt clone --select "one_specific_model" --state path/to/artifacts

# clone all of my models from specified state to my target schema(s) and recreate all pre-existing relations in the current target
dbt clone --state path/to/artifacts --full-refresh

# clone all of my models from specified state to my target schema(s), running up to 50 clone statements in parallel
dbt clone --state path/to/artifacts --threads 50

----------------------------------------

TITLE: Limiting Data Processing in Development Environment using SQL and Jinja
DESCRIPTION: This snippet demonstrates how to use Jinja templating to limit the amount of data processed in a development environment. It checks the target name and applies a date filter only when running in the 'dev' target.

LANGUAGE: sql
CODE:
select
*
from event_tracking.events
{% if target.name == 'dev' %}
where created_at >= dateadd('day', -3, current_date)
{% endif %}

----------------------------------------

TITLE: Validating All Semantic Nodes
DESCRIPTION: Command to validate all semantic nodes in a project, deferring to the production schema.

LANGUAGE: bash
CODE:
dbt sl validate

----------------------------------------

TITLE: Defining a Model Contract in YAML
DESCRIPTION: Example of how to define a model contract in a YAML file, including column definitions and constraints.

LANGUAGE: yaml
CODE:
models:
  - name: dim_customers
    config:
      contract:
        enforced: true
    columns:
      - name: customer_id
        data_type: int
        constraints:
          - type: not_null
      - name: customer_name
        data_type: string
      ...

----------------------------------------

TITLE: AWS IAM Policy Configuration for Glue
DESCRIPTION: Sample IAM policy configuration for enabling Glue Interactive Sessions with required permissions.

LANGUAGE: yaml
CODE:
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "Read_and_write_databases",
            "Action": [
                "glue:SearchTables",
                "glue:BatchCreatePartition",
                "glue:CreatePartitionIndex"
                //... additional permissions truncated
            ],
            "Resource": [
                "arn:aws:glue:<region>:<AWS Account>:catalog",
                "arn:aws:glue:<region>:<AWS Account>:table/<dbt output database>/*",
                "arn:aws:glue:<region>:<AWS Account>:database/<dbt output database>"
            ],
            "Effect": "Allow"
        }
    ]
}

----------------------------------------

TITLE: Basic SQL GROUP BY Usage
DESCRIPTION: Demonstrates a simple SQL query using GROUP BY to count records by a specific field. It includes WHERE, ORDER BY, and other common clauses to show typical usage.

LANGUAGE: sql
CODE:
select 
	my_first_field,
	count(id) as cnt --or any other aggregate function (sum, avg, etc.) 
from my_table
where my_first_field is not null
group by 1 --grouped by my_first_field
order by 1 desc

----------------------------------------

TITLE: Defining Test Paths in dbt_project.yml
DESCRIPTION: Specifies the directories where dbt should look for singular and generic tests. By default, dbt searches in the 'tests' directory if not specified.

LANGUAGE: yml
CODE:
test-paths: [directorypath]

----------------------------------------

TITLE: Configuring Sources in properties.yml (dbt 1.9+)
DESCRIPTION: Example of configuring sources in the properties.yml file for dbt version 1.9 and later. It shows how to set enabled, event_time, and meta configurations for both source and table levels.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: [<source-name>]
    [config]:
      [enabled]: true | false
      [event_time]: my_time_field
      [meta]: {<dictionary>}

    tables:
      - name: [<source-table-name>]
        [config]:
          [enabled]: true | false
          [event_time]: my_time_field
          [meta]: {<dictionary>}

----------------------------------------

TITLE: Configuring Staging Models as Views in dbt_project.yml
DESCRIPTION: This YAML snippet shows how to explicitly set the materialization strategy for staging models to views in the dbt_project.yml file.

LANGUAGE: yaml
CODE:
models:
  jaffle_shop:
    staging:
      +materialized: view

----------------------------------------

TITLE: Configuring Pre-Hook in Project Config
DESCRIPTION: Example of setting project-wide pre-hooks in dbt_project.yml configuration file.

LANGUAGE: yaml
CODE:
models:
  <project_name>:
    +pre-hook:
      - "{{ some_macro() }}"

----------------------------------------

TITLE: Configuring BigQuery labels in dbt
DESCRIPTION: Example of how to configure labels for BigQuery tables and views in dbt using the labels config. This can be done in model files or the dbt_project.yml file.

LANGUAGE: yaml
CODE:
models:
  my_project:
    snowplow:
      +labels:
        domain: clickstream
    finance:
      +labels:
        domain: finance

----------------------------------------

TITLE: Defining YAML Frontmatter for dbt Documentation
DESCRIPTION: This YAML snippet defines the frontmatter for a dbt documentation page about source freshness. It includes the title, ID, and a brief description of the feature.

LANGUAGE: yaml
CODE:
---
title: "Source freshness"
id: "source-freshness"
description: "Validate that data freshness meets expectations and alert if stale."
---

----------------------------------------

TITLE: Snapshot Reference in Models
DESCRIPTION: Example of referencing a snapshot in downstream models using the ref function

LANGUAGE: sql
CODE:
select * from {{ ref('orders_snapshot') }}

----------------------------------------

TITLE: YAML Entity Configuration v1.9+
DESCRIPTION: Complete specification for configuring entities in dbt semantic models including all available parameters and metadata options.

LANGUAGE: yaml
CODE:
semantic_models:
  - name: semantic_model_name
   ..rest of the semantic model config
    entities:
      - name: entity_name  ## Required
        type: Primary, natural, foreign, or unique ## Required
        description: A description of the field or role the entity takes in this table  ## Optional
        expr: The field that denotes that entity (transaction_id).  ## Optional
              Defaults to name if unspecified.  
        [config]: Specify configurations for entity.  ## Optional
          [meta]: {<dictionary>} Set metadata for a resource and organize resources. Accepts plain text, spaces, and quotes.  ## Optional

----------------------------------------

TITLE: SQL Inner Join Example with Car Data
DESCRIPTION: Shows a practical example of joining two tables (car_type and car_color) using dbt ref syntax, combining data based on matching user_ids to create a comprehensive view of car details.

LANGUAGE: sql
CODE:
select
   car_type.user_id as user_id,
   car_type.car_type as type,
   car_color.car_color as color
from {{ ref('car_type') }} as car_type
inner join {{ ref('car_color') }} as car_color
on car_type.user_id = car_color.user_id

----------------------------------------

TITLE: Defining Dimensions in a dbt Semantic Model
DESCRIPTION: Example of defining dimensions in a dbt semantic model YAML file, including categorical and time dimensions with various parameters.

LANGUAGE: yaml
CODE:
semantic_models:
  - name: transactions
    description: A record for every transaction that takes place. Carts are considered multiple transactions for each SKU. 
    model: {{ ref('fact_transactions') }}
    defaults:
      agg_time_dimension: order_date
# --- entities --- 
  entities: 
    - name: transaction
      type: primary
      ...
# --- measures --- 
  measures: 
      ... 
# --- dimensions ---
  dimensions:
    - name: order_date
      type: time
      type_params:
        time_granularity: day
      label: "Date of transaction"
      config: 
        meta:
          data_owner: "Finance team"
      expr: ts
    - name: is_bulk
      type: categorical
      expr: case when quantity > 10 then true else false end
    - name: type
      type: categorical

----------------------------------------

TITLE: Model-Specific Configuration in Properties YAML
DESCRIPTION: YAML configuration for model-specific settings in properties.yml files, including materialization and other model properties.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: [<model-name>]
    config:
      materialized: <materialization_name>
      sql_header: <string>
      on_configuration_change: apply | continue | fail
      unique_key: <column_name_or_expression>

----------------------------------------

TITLE: Testing Expressions and Using Custom Generic Tests in dbt YAML
DESCRIPTION: This snippet shows how to test expressions at the model level and use custom generic tests in dbt YAML files. It includes examples of expression tests and a custom primary_key test.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: orders
    description: Order overview data mart, offering key details for each order including if it's a customer's first order and a food vs. drink item breakdown. One row per order.
    tests:
      - dbt_utils.expression_is_true:
          expression: "order_items_subtotal = subtotal"
      - dbt_utils.expression_is_true:
          expression: "order_total = subtotal + tax_paid"
    columns:
      - name: order_id
        tests:
          - primary_key  # name of my custom generic test

----------------------------------------

TITLE: Calculating Monthly Average Order Amount in SQL
DESCRIPTION: Query demonstrating how to calculate rounded monthly average order amounts from the jaffle_shop orders table, excluding returned and pending return orders. Uses date truncation for monthly grouping and the AVG function with rounding.

LANGUAGE: sql
CODE:
select
	date_trunc('month', order_date) as order_month,
	round(avg(amount)) as avg_order_amount
from {{ ref('orders') }}
where status not in ('returned', 'return_pending')
group by 1

----------------------------------------

TITLE: Example of Multiple Exposure Definitions in YAML for dbt
DESCRIPTION: This example shows how to define multiple exposures in a single YAML file for dbt. It includes three different exposures: a dashboard, a machine learning model, and an application. Each exposure demonstrates various properties such as maturity, type, url, description, depends_on, and owner information.

LANGUAGE: yaml
CODE:
version: 2

exposures:

  - name: weekly_jaffle_metrics
    label: Jaffles by the Week              # optional
    type: dashboard                         # required
    maturity: high                          # optional
    url: https://bi.tool/dashboards/1       # optional
    description: >
      Did someone say "exponential growth"?

    depends_on:                             # expected
      - ref('fct_orders')
      - ref('dim_customers')
      - source('gsheets', 'goals')
      - metric('count_orders')

    owner:
      name: Callum McData
      email: data@jaffleshop.com


      
  - name: jaffle_recommender
    maturity: medium
    type: ml
    url: https://jupyter.org/mycoolalg
    description: >
      Deep learning to power personalized "Discover Sandwiches Weekly"
    
    depends_on:
      - ref('fct_orders')
      
    owner:
      name: Data Science Drew
      email: data@jaffleshop.com

      
  - name: jaffle_wrapped
    type: application
    description: Tell users about their favorite jaffles of the year
    depends_on: [ ref('fct_orders') ]
    owner: { email: summer-intern@jaffleshop.com }

----------------------------------------

TITLE: Creating an Analysis SQL File in dbt
DESCRIPTION: This SQL snippet demonstrates how to create an analysis file in dbt. It calculates a running total by account using data from journal entries and accounts. The analysis uses dbt's ref function to reference other models in an environment-agnostic way.

LANGUAGE: sql
CODE:
-- analyses/running_total_by_account.sql

with journal_entries as (

  select *
  from {{ ref('quickbooks_adjusted_journal_entries') }}

), accounts as (

  select *
  from {{ ref('quickbooks_accounts_transformed') }}

)

select
  txn_date,
  account_id,
  adjusted_amount,
  description,
  account_name,
  sum(adjusted_amount) over (partition by account_id order by id rows unbounded preceding)
from journal_entries
order by account_id, id

----------------------------------------

TITLE: Overriding Global Macros in dbt YAML
DESCRIPTION: Example of configuring dispatch to override global dbt macros with custom implementations from a helper package. This allows for organization-wide customization of dbt's core functionality.

LANGUAGE: yaml
CODE:
dispatch:
  - macro_namespace: dbt
    search_order: ['my_project', 'my_org_dbt_helpers', 'dbt']

----------------------------------------

TITLE: Complete dbt Project Configuration YAML
DESCRIPTION: Example showing all available configurations in the dbt_project.yml file, including model paths, configurations, and project settings.

LANGUAGE: yaml
CODE:
name: string

config-version: 2
version: version

profile: profilename

model-paths: [directorypath]
seed-paths: [directorypath]
test-paths: [directorypath]
analysis-paths: [directorypath]
macro-paths: [directorypath]
snapshot-paths: [directorypath]
docs-paths: [directorypath]
asset-paths: [directorypath]

packages-install-path: directorypath

clean-targets: [directorypath]

query-comment: string

require-dbt-version: version-range | [version-range]

flags:
  <global-configs>

dbt-cloud:
  project-id: project_id
  defer-env-id: environment_id

quoting:
  database: true | false
  schema: true | false
  identifier: true | false

metrics:
  <metric-configs>

models:
  <model-configs>

seeds:
  <seed-configs>

semantic-models:
  <semantic-model-configs>

saved-queries:
  <saved-queries-configs>

snapshots:
  <snapshot-configs>

sources:
  <source-configs>
  
tests:
  <test-configs>

vars:
  <variables>

on-run-start: sql-statement | [sql-statement]
on-run-end: sql-statement | [sql-statement]

dispatch:
  - macro_namespace: packagename
    search_order: [packagename]

restrict-access: true | false

----------------------------------------

TITLE: Configuring Basic Semantic Models with Joins in YAML
DESCRIPTION: Example configuration showing two semantic models (transactions and user_signup) with entity definitions for joining, including measure and dimension specifications.

LANGUAGE: yaml
CODE:
semantic_models:
  - name: transactions
    entities:
      - name: id
        type: primary
      - name: user
        type: foreign
        expr: user_id
    measures:
      - name: average_purchase_price
        agg: avg
        expr: purchase_price
  - name: user_signup
    entities:
      - name: user
        type: primary
        expr: user_id
    dimensions:
      - name: type
        type: categorical

----------------------------------------

TITLE: Validating Modified Semantic Nodes
DESCRIPTION: Command to validate only modified semantic nodes using state selection in a CI job.

LANGUAGE: bash
CODE:
dbt sl validate --select state:modified+

----------------------------------------

TITLE: Defining Cumulative Metrics in YAML
DESCRIPTION: Example YAML configuration for defining cumulative metrics in dbt, including all-time, trailing 1-month, and month-to-date metrics.

LANGUAGE: yaml
CODE:
metrics:
  - name: cumulative_order_total
    label: Cumulative order total (All-Time)    
    description: The cumulative value of all orders
    type: cumulative
    type_params:
      measure: 
        name: order_total
  
  - name: cumulative_order_total_l1m
    label: Cumulative order total (L1M)   
    description: Trailing 1-month cumulative order total
    type: cumulative
    type_params:
      measure: 
        name: order_total
      cumulative_type_params:
        window: 1 month
  
  - name: cumulative_order_total_mtd
    label: Cumulative order total (MTD)
    description: The month-to-date value of all orders
    type: cumulative
    type_params:
      measure: 
        name: order_total
      cumulative_type_params:
        grain_to_date: month

----------------------------------------

TITLE: Configuring Snapshot Properties in YAML for dbt v1.9+
DESCRIPTION: This YAML structure defines snapshot properties for dbt version 1.9 and later. It includes configurations for snapshot name, description, meta data, documentation settings, config options, tests, and column-level properties.

LANGUAGE: yaml
CODE:
version: 2

snapshots:
  - name: <snapshot name>
    [description](/reference/resource-properties/description): <markdown_string>
    [meta](/reference/resource-configs/meta): {<dictionary>}
    [docs](/reference/resource-configs/docs):
      show: true | false
      node_color: <color_id> # Use name (such as node_color: purple) or hex code with quotes (such as node_color: "#cd7f32")
    [config](/reference/resource-properties/config):
      [<snapshot_config>](/reference/snapshot-configs): <config_value>
    [tests](/reference/resource-properties/data-tests):
      - <test>
      - ...
    columns:
      - name: <column name>
        [description](/reference/resource-properties/description): <markdown_string>
        [meta](/reference/resource-configs/meta): {<dictionary>}
        [quote](/reference/resource-properties/quote): true | false
        [tags](/reference/resource-configs/tags): [<string>]
        [tests](/reference/resource-properties/data-tests):
          - <test>
          - ... # declare additional tests
      - ... # declare properties of additional columns

    - name: ... # declare properties of additional snapshots

----------------------------------------

TITLE: Incremental Python Model Example
DESCRIPTION: Shows an example of an incremental Python model with filtering for new data.

LANGUAGE: python
CODE:
import snowflake.snowpark.functions as F

def model(dbt, session):
    dbt.config(materialized = "incremental")
    df = dbt.ref("upstream_table")

    if dbt.is_incremental:

        # only new rows compared to max in current table
        max_from_this = f"select max(updated_at) from {dbt.this}"
        df = df.filter(df.updated_at >= session.sql(max_from_this).collect()[0][0])

        # or only rows from the past 3 days
        df = df.filter(df.updated_at >= F.dateadd("day", F.lit(-3), F.current_timestamp()))

    ...

    return df

----------------------------------------

TITLE: Implementing a Custom View Materialization in dbt
DESCRIPTION: Demonstrates a complete implementation of a custom view materialization, including setup, execution, and cleanup phases.

LANGUAGE: sql
CODE:
{%- materialization my_view, default -%}

  {%- set target_relation = api.Relation.create(
        identifier=this.identifier, schema=this.schema, database=this.database,
        type='view') -%}

  -- ... setup database ...
  -- ... run pre-hooks...

  -- build model
  {% call statement('main') -%}
    {{ create_view_as(target_relation, sql) }}
  {%- endcall %}
  
  -- ... run post-hooks ...
  -- ... clean up the database...

  -- Return the relations created in this materialization
  {{ return({'relations': [target_relation]}) }}

{%- endmaterialization -%}

----------------------------------------

TITLE: Configuring Incremental Model with Merge Strategy in Spark SQL
DESCRIPTION: This snippet demonstrates how to configure an incremental model using the 'merge' strategy with Delta file format in Spark SQL. It includes both the source code and the resulting run code.

LANGUAGE: sql
CODE:
{{ config(
    materialized='incremental',
    file_format='delta', # or 'iceberg' or 'hudi'
    unique_key='user_id',
    incremental_strategy='merge'
) }}

with new_events as (

    select * from {{ ref('events') }}

    {% if is_incremental() %}
    where date_day >= date_add(current_date, -1)
    {% endif %}

)

select
    user_id,
    max(date_day) as last_seen

from events
group by 1

LANGUAGE: sql
CODE:
create temporary view merge_incremental__dbt_tmp as

    with new_events as (

        select * from analytics.events


        where date_day >= date_add(current_date, -1)


    )

    select
        user_id,
        max(date_day) as last_seen

    from events
    group by 1

;

merge into analytics.merge_incremental as DBT_INTERNAL_DEST
    using merge_incremental__dbt_tmp as DBT_INTERNAL_SOURCE
    on DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id
    when matched then update set *
    when not matched then insert *

----------------------------------------

TITLE: Explicit Root Project Macro Override Configuration
DESCRIPTION: Demonstrates how to explicitly configure the default search behavior where root project macros take precedence over package macros, using my_root_project as an example.

LANGUAGE: yml
CODE:
dispatch:
  - macro_namespace: dbt_utils
    search_order: ['my_root_project', 'dbt_utils']

----------------------------------------

TITLE: Implementing Warning-Level Generic Test
DESCRIPTION: Creates a generic test with default warning-level severity that can be overridden in specific test instances.

LANGUAGE: sql
CODE:
{% test warn_if_odd(model, column_name) %}

    {{ config(severity = 'warn') }}

    select *
    from {{ model }}
    where ({{ column_name }} % 2) = 1

{% endtest %}

----------------------------------------

TITLE: Creating dimension table documentation and tests
DESCRIPTION: YAML configuration for documenting and testing the dim_product dimension table.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: dim_product
    columns:
      - name: product_key 
        description: The surrogate key of the product
        tests:
          - not_null
          - unique
      - name: productid 
        description: The natural key of the product
        tests:
          - not_null
          - unique
      - name: product_name 
        description: The product name
        tests:
          - not_null

----------------------------------------

TITLE: Running dbt Tests with Configuration and Test Name Filters
DESCRIPTION: This example demonstrates how to execute specific dbt tests based on model configuration (incremental strategy) and test name.

LANGUAGE: bash
CODE:
dbt test --select "config.incremental_strategy:insert_overwrite,test_name:unique"   # execute all `unique` tests that select from models using the `insert_overwrite` incremental strategy

----------------------------------------

TITLE: Configuring DuckDB Profile with Extensions and S3 Settings in YAML
DESCRIPTION: This YAML snippet demonstrates how to configure a DuckDB profile in dbt, including specifying the database path, loading extensions for S3 and Parquet support, and setting AWS credentials for S3 access.

LANGUAGE: yaml
CODE:
your_profile_name:
  target: dev
  outputs:
    dev:
      type: duckdb
      path: 'file_path/database_name.duckdb'
      extensions:
        - httpfs
        - parquet
      settings:
        s3_region: my-aws-region
        s3_access_key_id: "{{ env_var('S3_ACCESS_KEY_ID') }}"
        s3_secret_access_key: "{{ env_var('S3_SECRET_ACCESS_KEY') }}"

----------------------------------------

TITLE: Defining Exposure Properties in YAML for dbt
DESCRIPTION: This snippet demonstrates the structure and properties of exposures in dbt. It includes required fields like name and type, as well as optional fields such as description, url, maturity, tags, meta, owner, and depends_on. The code also shows how to reference models, seeds, sources, and metrics in the depends_on section.

LANGUAGE: yaml
CODE:
version: 2

exposures:
  - name: <string_with_underscores>
    description: <markdown_string>
    type: {dashboard, notebook, analysis, ml, application}
    url: <string>
    maturity: {high, medium, low}  # Indicates level of confidence or stability in the exposure
    tags: [<string>]
    meta: {<dictionary>}
    owner:
      name: <string>
      email: <string>
    
    depends_on:
      - ref('model')
      - ref('seed')
      - source('name', 'table')
      - metric('metric_name')
      
    label: "Human-Friendly Name for this Exposure!"
    config:
      enabled: true | false

  - name: ... # declare properties of additional exposures

----------------------------------------

TITLE: Adding a Model to a Group in Schema YAML
DESCRIPTION: Assigns a specific model to the 'finance' group using the schema.yml file. This method allows for individual model group assignment.

LANGUAGE: yaml
CODE:
models:
  - name: model_name
    config:
      group: finance

----------------------------------------

TITLE: dbt Page Front Matter Configuration
DESCRIPTION: YAML front matter configuration for the dbt tips and tricks documentation page, defining title, description, sidebar label, and pagination settings.

LANGUAGE: yaml
CODE:
---
title: "dbt tips and tricks"
description: "Check out any dbt-related tips and tricks to help you work faster and be more productive."
sidebar_label: "dbt tips and tricks"
pagination_next: null
---

----------------------------------------

TITLE: Documenting Standard dbt Macro
DESCRIPTION: Example of documenting a macro called 'cents_to_dollars' using a schema.yml file. The documentation includes the macro name, description, and argument specifications with their types and descriptions.

LANGUAGE: yaml
CODE:
version: 2

macros:
  - name: cents_to_dollars
    description: A macro to convert cents to dollars
    arguments:
      - name: column_name
        type: string
        description: The name of the column you want to convert
      - name: precision
        type: integer
        description: Number of decimal places. Defaults to 2.

----------------------------------------

TITLE: Configuring Source Properties Schema in YAML
DESCRIPTION: Comprehensive schema showing all available source property configurations in dbt. Includes settings for source name, database, schema, loader, freshness checks, quoting, and table/column level configurations.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: <string> # required
    description: <markdown_string>
    database: <database_name>
    schema: <schema_name>
    loader: <string>
    loaded_at_field: <column_name>
    meta: {<dictionary>}
    tags: [<string>]
    
    config:
      <source_config>: <config_value>

    overrides: <string>

    freshness:
      warn_after:
        count: <positive_integer>
        period: minute | hour | day
      error_after:
        count: <positive_integer>
        period: minute | hour | day
      filter: <where-condition>

    quoting:
      database: true | false
      schema: true | false
      identifier: true | false

    tables:
      - name: <string> #required
        description: <markdown_string>
        meta: {<dictionary>}
        identifier: <table_name>
        loaded_at_field: <column_name>
        tests:
          - <test>
        tags: [<string>]
        freshness:
          warn_after:
            count: <positive_integer>
            period: minute | hour | day
          error_after:
            count: <positive_integer>
            period: minute | hour | day
          filter: <where-condition>

        quoting:
          database: true | false
          schema: true | false
          identifier: true | false
        external: {<dictionary>}
        columns:
          - name: <column_name>
            description: <markdown_string>
            meta: {<dictionary>}
            quote: true | false
            tests:
              - <test>
            tags: [<string>]

----------------------------------------

TITLE: SQLFluff Configuration Example
DESCRIPTION: Configuration example for setting up SQLFluff linting in dbt Cloud CI jobs using the dbt templater.

LANGUAGE: ini
CODE:
templater = dbt

----------------------------------------

TITLE: Overriding Package Macros with Custom Implementation in dbt SQL
DESCRIPTION: Example of overriding an existing 'concat' macro from dbt-utils package with a custom implementation for Redshift, while falling back to dbt-utils for other adapters. This shows how to extend package functionality selectively.

LANGUAGE: sql
CODE:
{% macro concat(fields) -%}
    {{ return(adapter.dispatch('concat')(fields)) }}
{%- endmacro %}

{% macro default__concat(fields) -%}
    {{ return(dbt_utils.concat(fields)) }}
{%- endmacro %}

{% macro redshift__concat(fields) %}
    {% for field in fields %}
        nullif({{ field }},'') {{ ' || ' if not loop.last }}
    {% endfor %}
{% endmacro %}

----------------------------------------

TITLE: Grant Select Operation Macro
DESCRIPTION: A macro that creates an operation to grant select privileges on schema objects to a specified role. Uses run_query to execute the SQL statements.

LANGUAGE: sql
CODE:
{% macro grant_select(role) %}
{% set sql %}
    grant usage on schema {{ target.schema }} to role {{ role }};
    grant select on all tables in schema {{ target.schema }} to role {{ role }};
    grant select on all views in schema {{ target.schema }} to role {{ role }};
{% endset %}

{% do run_query(sql) %}
{% do log("Privileges granted", info=True) %}
{% endmacro %}

----------------------------------------

TITLE: Conditional SQL Query Based on Target Name in dbt
DESCRIPTION: This SQL snippet demonstrates how to use the target name in a dbt project to conditionally limit data queried in non-production environments. It selects all columns from a table, but applies a date filter only when the target is not 'prod'.

LANGUAGE: sql
CODE:
select *
from a_big_table

-- limit the amount of data queried in dev
{% if target.name != 'prod' %}
where created_at > date_trunc('month', current_date)
{% endif %}

----------------------------------------

TITLE: Configuring Model Access in Groups
DESCRIPTION: Demonstrates setting access modifiers for grouped models in the schema.yml file. Shows how to set a model as private within a group.

LANGUAGE: yaml
CODE:
models:
  - name: finance_private_model
    access: private
    config:
      group: finance

  # in a different group!
  - name: marketing_model
    config:
      group: marketing

----------------------------------------

TITLE: Configuring Full Refresh for Models in dbt_project.yml
DESCRIPTION: YAML configuration to set the full_refresh option for models in the dbt_project.yml file. The full_refresh parameter can be set to true or false to control whether models always or never perform full refreshes.

LANGUAGE: yml
CODE:
models:
  [<resource-path>]:
    +full_refresh: false | true

----------------------------------------

TITLE: Generating YAML for Single dbt Model using Codegen
DESCRIPTION: Example command using dbt Codegen package to generate YAML documentation for a single model with its output structure.

LANGUAGE: bash
CODE:
dbt run-operation generate_model_yaml --args '{"model_names": [ "activity_based_interest_activated"] }'

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: activity_based_interest_activated
    description: ""
    columns:
      - name: id
        description: ""
      - name: user_id
        description: ""

----------------------------------------

TITLE: Configuring Snapshot Unique Key in YAML
DESCRIPTION: Example of setting a unique key for a snapshot in a YAML configuration file.

LANGUAGE: yaml
CODE:
snapshots:
  - name: orders_snapshot
    relation: source('my_source', 'my_table')
    config:
      unique_key: order_id

----------------------------------------

TITLE: Configuring Source Freshness in YAML
DESCRIPTION: Defines freshness monitoring configuration for source tables including warning thresholds and loaded timestamp fields.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: jaffle_shop
    database: raw
    freshness:
      warn_after: {count: 12, period: hour}
      error_after: {count: 24, period: hour}
    loaded_at_field: _etl_loaded_at

    tables:
      - name: orders
        freshness:
          warn_after: {count: 6, period: hour}
          error_after: {count: 12, period: hour}

      - name: customers

      - name: product_skus
        freshness: null

----------------------------------------

TITLE: Configuring Package Access Restriction in YAML
DESCRIPTION: Demonstrates how to restrict access to models defined in a package using the 'restrict-access' config in dbt_project.yml.

LANGUAGE: yaml
CODE:
restrict-access: True  # default is False

----------------------------------------

TITLE: Incremental Model Example
DESCRIPTION: SQL model demonstrating incremental processing logic that filters for new events based on timestamp.

LANGUAGE: sql
CODE:
{{
    config(
        materialized='incremental'
    )
}}

select * from {{ ref('events') }}
{% if is_incremental() %}
where event_time > (select max(event_time) from {{ this }})
{% endif %}

----------------------------------------

TITLE: Materialized View Configurations in dbt Project File
DESCRIPTION: YAML configuration options for Redshift materialized views including distribution strategy, sort keys, auto-refresh, and backup settings.

LANGUAGE: yaml
CODE:
models:
  [<resource-path>]:
    [+]materialized: materialized_view
    [+]on_configuration_change: apply | continue | fail
    [+]dist: all | auto | even | <field-name>
    [+]sort: <field-name> | [<field-name>]
    [+]sort_type: auto | compound | interleaved
    [+]auto_refresh: true | false
    [+]backup: true | false

----------------------------------------

TITLE: Using the At Operator in dbt Commands
DESCRIPTION: Illustrates the usage of the '@' operator in dbt run commands. This operator selects a model, its descendants, and all ancestors of those descendants, which is useful in continuous integration environments.

LANGUAGE: bash
CODE:
dbt run --select "@my_model"         # select my_model, its descendants, and the ancestors of its descendants

----------------------------------------

TITLE: Basic ref Function Usage in SQL
DESCRIPTION: Basic example of using the ref function to reference a node in dbt.

LANGUAGE: sql
CODE:
select * from {{ ref("node_name") }}

----------------------------------------

TITLE: Example docs-paths configuration in dbt_project.yml
DESCRIPTION: Provides a complete example of how to configure docs-paths in the dbt_project.yml file, specifying a subdirectory named 'docs' for docs blocks.

LANGUAGE: yaml
CODE:
docs-paths: ["docs"]

----------------------------------------

TITLE: Using re Module for Regular Expressions
DESCRIPTION: Demonstrates using the re module for pattern matching in DBT. Example shows validating an S3 path using regular expressions with error handling.

LANGUAGE: jinja
CODE:
{% set my_string = 's3://example/path' %}
{% set s3_path_pattern = 's3://[a-z0-9-_/]+' %}

{% set re = modules.re %}
{% set is_match = re.match(s3_path_pattern, my_string, re.IGNORECASE) %}
{% if not is_match %}
    {%- do exceptions.raise_compiler_error(
        my_string ~ ' is not a valid s3 path'
    ) -%}
{% endif %}

----------------------------------------

TITLE: YAML Entity Implementation Example v1.9+
DESCRIPTION: Practical example showing how to define different types of entities in a semantic model with metadata configuration.

LANGUAGE: yaml
CODE:
entities:
  - name: transaction
    type: primary
    expr: id_transaction
  - name: order
    type: foreign
    expr: id_order
  - name: user
    type: foreign
    expr: substring(id_order from 2)
    entities:
  - name: transaction
    type: 
    description: A description of the field or role the entity takes in this table ## Optional
    expr: The field that denotes that entity (transaction_id).  
          Defaults to name if unspecified.
    [config]:
      [meta]:
        data_owner: "Finance team"

----------------------------------------

TITLE: Common Metadata Structure in dbt Artifacts (JSON)
DESCRIPTION: This JSON structure represents the common metadata included in all dbt artifacts. It contains information about the dbt version, schema version, generation timestamp, adapter type, environment variables, and invocation ID.

LANGUAGE: json
CODE:
{
  "metadata": {
    "dbt_version": "string",
    "dbt_schema_version": "string",
    "generated_at": "string",
    "adapter_type": "string",
    "env": {},
    "invocation_id": "string"
  }
}

----------------------------------------

TITLE: Defining a Microbatch Incremental Model for Sessions
DESCRIPTION: This SQL model defines a sessions table using the microbatch incremental strategy. It configures event_time, begin date, and batch_size, which are required for microbatch processing.

LANGUAGE: sql
CODE:
{{ config(
    materialized='incremental',
    incremental_strategy='microbatch',
    event_time='session_start',
    begin='2020-01-01',
    batch_size='day'
) }}

with page_views as (

    -- this ref will be auto-filtered
    select * from {{ ref('page_views') }}

),

customers as (

    -- this ref won't
    select * from {{ ref('customers') }}

),

select
  page_views.id as session_id,
  page_views.page_view_start as session_start,
  customers.*
  from page_views
  left join customers
    on page_views.customer_id = customer.id

----------------------------------------

TITLE: Implementing Visit to Buy Conversion Rate Example
DESCRIPTION: Example of implementing a conversion metric to track user visits leading to purchases within a 7-day window.

LANGUAGE: yaml
CODE:
- name: visit_to_buy_conversion_rate_7d
  description: "Conversion rate from visiting to transaction in 7 days"
  type: conversion
  label: Visit to buy conversion rate (7-day window)
  type_params:
    conversion_type_params:
      base_measure:
        name: visits
        fill_nulls_with: 0
        filter: {{ Dimension('visits__referrer_id') }} = 'facebook'
      conversion_measure:
        name: sellers
      entity: user
      window: 7 days

----------------------------------------

TITLE: Unit Test Configuration for Email Validation
DESCRIPTION: YAML configuration defining unit tests for email validation logic with test cases covering various edge cases including invalid formats and domains.

LANGUAGE: yaml
CODE:
unit_tests:
  - name: test_is_valid_email_address
    description: "Check my is_valid_email_address logic captures all known edge cases - emails without ., emails without @, and emails from invalid domains."
    model: dim_customers
    given:
      - input: ref('stg_customers')
        rows:
          - {email: cool@example.com,    email_top_level_domain: example.com}
          - {email: cool@unknown.com,    email_top_level_domain: unknown.com}
          - {email: badgmail.com,        email_top_level_domain: gmail.com}
          - {email: missingdot@gmailcom, email_top_level_domain: gmail.com}
      - input: ref('top_level_email_domains')
        rows:
          - {tld: example.com}
          - {tld: gmail.com}
    expect:
      rows:
        - {email: cool@example.com,    is_valid_email_address: true}
        - {email: cool@unknown.com,    is_valid_email_address: false}
        - {email: badgmail.com,        is_valid_email_address: false}
        - {email: missingdot@gmailcom, is_valid_email_address: false}

----------------------------------------

TITLE: Creating a Metric Query with GraphQL
DESCRIPTION: Illustrates how to create a metric query using the GraphQL API, including metrics, group by, where filters, and ordering.

LANGUAGE: graphql
CODE:
createQuery(
  environmentId: BigInt!
  metrics: [MetricInput!]!
  groupBy: [GroupByInput!] = null
  limit: Int = null
  where: [WhereInput!] = null
  order: [OrderByInput!] = null
): CreateQueryResult

----------------------------------------

TITLE: Documenting Custom dbt Materializations
DESCRIPTION: Example of documenting custom materializations in dbt using the materialization macro naming convention. Shows documentation for both default and custom adapter implementations.

LANGUAGE: yaml
CODE:
version: 2

macros:
  - name: materialization_my_materialization_name_default
    description: A custom materialization to insert records into an append-only table and track when they were added.
  - name: materialization_my_materialization_name_xyz
    description: A custom materialization to insert records into an append-only table and track when they were added.

----------------------------------------

TITLE: Semantic Model Configuration - YAML
DESCRIPTION: Examples of configuring semantic models using either models/semantic.yml or dbt_project.yml files.

LANGUAGE: yaml
CODE:
semantic_models:
  - name: orders
    config:
      enabled: true | false
      group: some_group
      meta:
        some_key: some_value

LANGUAGE: yaml
CODE:
semantic-models:
  my_project_name:
    +enabled: true | false
    +group: some_group
    +meta:
      some_key: some_value

----------------------------------------

TITLE: Configuring Fact Table Model in SQL File for Firebolt
DESCRIPTION: This snippet demonstrates how to configure a fact table model directly in the SQL file using a config block, including setting the table type, primary index, and aggregating indexes.

LANGUAGE: jinja
CODE:
{{ config(
    materialized = "table"
    table_type = "fact"
    primary_index = [ "<column-name>", ... ],
    indexes = [
      {
        "index_type": "aggregating"
        "key_columns": [ "<column-name>", ... ],
        "aggregation": [ "<agg-sql>", ... ],
      },
      ...
    ]
) }}

----------------------------------------

TITLE: Fetching Available Dimensions for Metrics with GraphQL
DESCRIPTION: Shows how to query available dimensions for given metrics using the GraphQL API.

LANGUAGE: graphql
CODE:
dimensions(
  environmentId: BigInt!
  metrics: [MetricInput!]!
): [Dimension!]!

----------------------------------------

TITLE: Setting flags in dbt_project.yml
DESCRIPTION: Example of setting the 'fail_fast' flag in the dbt_project.yml file. This sets a default for running the project anywhere, anytime, by anyone.

LANGUAGE: yaml
CODE:
# dbt_project.yml
flags:
  # set default for running this project -- anywhere, anytime, by anyone
  fail_fast: true

----------------------------------------

TITLE: Configuring Databricks OAuth Authentication in dbt
DESCRIPTION: YAML configuration for connecting to Databricks using OAuth client-based authentication. Requires host, http_path, schema, client_id, and client_secret.

LANGUAGE: yaml
CODE:
your_profile_name:
  target: dev
  outputs:
    dev:
      type: databricks
      catalog: CATALOG_NAME #optional catalog name if you are using Unity Catalog
      schema: SCHEMA_NAME # Required
      host: YOUR_ORG.databrickshost.com # Required
      http_path: /SQL/YOUR/HTTP/PATH # Required
      auth_type: oauth # Required if using OAuth-based authentication
      client_id: OAUTH_CLIENT_ID # The ID of your OAuth application. Required if using OAuth-based authentication
      client_secret: XXXXXXXXXXXXXXXXXXXXXXXXXXX # OAuth client secret. # Required if using OAuth-based authentication
      threads: 1_OR_MORE  # Optional, default 1

----------------------------------------

TITLE: Semantic Model Configuration - YAML
DESCRIPTION: Examples of configuring semantic models using either models/semantic.yml or dbt_project.yml files.

LANGUAGE: yaml
CODE:
semantic_models:
  - name: orders
    config:
      enabled: true | false
      group: some_group
      meta:
        some_key: some_value

LANGUAGE: yaml
CODE:
semantic-models:
  my_project_name:
    +enabled: true | false
    +group: some_group
    +meta:
      some_key: some_value

----------------------------------------

TITLE: Conditional Data Filtering Using target.name in SQL
DESCRIPTION: A SQL example demonstrating how to use target.name to limit data in development environments by filtering recent records only when in dev mode.

LANGUAGE: sql
CODE:
select
  *
from source('web_events', 'page_views')
{% if target.name == 'dev' %}
where created_at >= dateadd('day', -3, current_date)
{% endif %}

----------------------------------------

TITLE: Configuring General Options for Seeds in dbt Project YAML
DESCRIPTION: This snippet illustrates how to configure general options for seeds in the dbt_project.yml file, including enabled, tags, hooks, database, schema, and more.

LANGUAGE: yaml
CODE:
seeds:
  [<resource-path>]:
    [+][enabled]: true | false
    [+][tags]: <string> | [<string>]
    [+][pre-hook]: <sql-statement> | [<sql-statement>]
    [+][post-hook]: <sql-statement> | [<sql-statement>]
    [+][database]: <string>
    [+][schema]: <string>
    [+][alias]: <string>
    [+][persist_docs]: <dict>
    [+][full_refresh]: <boolean>
    [+][meta]: {<dictionary>}
    [+][grants]: {<dictionary>}
    [+][event_time]: my_time_field

----------------------------------------

TITLE: Referencing a Seed in a SQL Model
DESCRIPTION: Example of how to reference a seed file in a downstream SQL model using the 'ref' function in dbt.

LANGUAGE: sql
CODE:
-- This refers to the table created from seeds/country_codes.csv
select * from {{ ref('country_codes') }}

----------------------------------------

TITLE: Creating fact table documentation and tests
DESCRIPTION: YAML configuration for documenting and testing the fct_sales fact table.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: fct_sales
    columns:

      - name: sales_key
        description: The surrogate key of the fct sales
        tests:
          - not_null
          - unique

      - name: product_key
        description: The foreign key of the product
        tests:
          - not_null

      - name: customer_key
        description: The foreign key of the customer
        tests:
          - not_null 
      
      ... 

      - name: orderqty
        description: The quantity of the product 
        tests:
          - not_null

      - name: revenue
        description: The revenue obtained by multiplying unitprice and orderqty

----------------------------------------

TITLE: Creating Staging Models in dbt
DESCRIPTION: SQL staging models for customers and orders tables that handle initial data transformation and column mapping.

LANGUAGE: sql
CODE:
select
    ID as customer_id,
    FIRST_NAME as first_name,
    LAST_NAME as last_name

from dbo.customers

LANGUAGE: sql
CODE:
select
    ID as order_id,
    USER_ID as customer_id,
    ORDER_DATE as order_date,
    STATUS as status

from dbo.orders

----------------------------------------

TITLE: Python Webhook Processing for dbt Cloud Error Notifications
DESCRIPTION: Python script that validates webhook authenticity, fetches run logs from dbt Cloud Admin API, and formats error messages for Slack. Handles webhook validation, API interaction, and message formatting for both summary and detailed error information.

LANGUAGE: python
CODE:
import hashlib
import hmac
import json
import re

auth_header = input_data['auth_header']
raw_body = input_data['raw_body']

# Access secret credentials
secret_store = StoreClient('YOUR_SECRET_HERE')
hook_secret = secret_store.get('DBT_WEBHOOK_KEY')
api_token = secret_store.get('DBT_CLOUD_SERVICE_TOKEN')

# Validate the webhook came from dbt Cloud
signature = hmac.new(hook_secret.encode('utf-8'), raw_body.encode('utf-8'), hashlib.sha256).hexdigest()

if signature != auth_header:
  raise Exception("Calculated signature doesn't match contents of the Authorization header. This webhook may not have been sent from dbt Cloud.")

full_body = json.loads(raw_body)
hook_data = full_body['data'] 

# Steps derived from these commands won't have their error details shown inline, as they're messy
commands_to_skip_logs = ['dbt source', 'dbt docs']

run_id = hook_data['runId']
account_id = full_body['accountId']

url = f'https://YOUR_ACCESS_URL/api/v2/accounts/{account_id}/runs/{run_id}/?include_related=["run_steps"]'
headers = {'Authorization': f'Token {api_token}'}
run_data_response = requests.get(url, headers=headers)
run_data_response.raise_for_status()
run_data_results = run_data_response.json()['data']

step_summary_post = f"""
*<{run_data_results['href']}|{hook_data['runStatus']} for Run #{run_id} on Job \"{hook_data['jobName']}\">*

*Environment:* {hook_data['environmentName']} | *Trigger:* {hook_data['runReason']} | *Duration:* {run_data_results['duration_humanized']}

"""

threaded_errors_post = ""

for step in run_data_results['run_steps']:
  if step['status_humanized'] == 'Success':
    step_summary_post += f"""
‚úÖ {step['name']} ({step['status_humanized']} in {step['duration_humanized']})
"""
  else:
    step_summary_post += f"""
‚ùå {step['name']} ({step['status_humanized']} in {step['duration_humanized']})
"""

    show_logs = not any(cmd in step['name'] for cmd in commands_to_skip_logs)
    if show_logs:
      full_log = step['logs']
      full_log = re.sub('\x1b?\[[0-9]+m[0-9:]*', '', full_log)
    
      summary_start = re.search('(?:Completed with \d+ error.* and \d+ warnings?:|Database Error|Compilation Error|Runtime Error)', full_log)
    
      line_items = re.findall('(^.*(?:Failure|Error) in .*\n.*\n.*)', full_log, re.MULTILINE)

      if not summary_start:
        continue
      
      threaded_errors_post += f"""
*{step['name']}*
"""    
      if len(line_items) == 0:
        relevant_log = f'```{full_log[summary_start.start():]}```'
      else:
        relevant_log = summary_start[0]
        for item in line_items:
          relevant_log += f'\n```\n{item.strip()}\n```\n'
      threaded_errors_post += f"""
{relevant_log}
"""

send_error_thread = len(threaded_errors_post) > 0

output = {'step_summary_post': step_summary_post, 'send_error_thread': send_error_thread, 'threaded_errors_post': threaded_errors_post}

----------------------------------------

TITLE: Converting MERGE to Incremental dbt Model
DESCRIPTION: Shows how to convert a MERGE statement into an incremental dbt model using CTEs and incremental materialization config

LANGUAGE: sql
CODE:
MERGE INTO ride_details USING (
    SELECT
        ride_id,
        subtotal,
        tip

    FROM rides_to_load AS rtl

    ON ride_details.ride_id = rtl.ride_id

    WHEN MATCHED THEN UPDATE

    SET ride_details.tip = rtl.tip

    WHEN NOT MATCHED THEN INSERT (ride_id, subtotal, tip)
    VALUES (rtl.ride_id, rtl.subtotal, NVL(rtl.tip, 0, rtl.tip)
);

LANGUAGE: sql
CODE:
{{
    config(
        materialized='incremental',
        unique_key='ride_id',
        incremental_strategy='merge'
    )
}}

WITH

using_clause AS (

    SELECT
        ride_id,
        subtotal,
        tip,
        max(load_timestamp) as load_timestamp

    FROM {{ ref('rides_to_load') }}


    {% if is_incremental() %}

        WHERE load_timestamp > (SELECT max(load_timestamp) FROM {{ this }})

    {% endif %}

),

updates AS (

    SELECT
        ride_id,
        subtotal,
        tip,
        load_timestamp

    FROM using_clause

    {% if is_incremental() %}

        WHERE ride_id IN (SELECT ride_id FROM {{ this }})

    {% endif %}

),

inserts AS (

    SELECT
        ride_id,
        subtotal,
        NVL(tip, 0, tip),
        load_timestamp

    FROM using_clause

    WHERE ride_id NOT IN (SELECT ride_id FROM updates)

)

SELECT * FROM updates UNION inserts

----------------------------------------

TITLE: Configuring BigQuery Extended Attributes in YAML
DESCRIPTION: YAML configuration for setting up BigQuery extended attributes in dbt Cloud, including service account details and optional fields.

LANGUAGE: yaml
CODE:
priority: interactive
keyfile_json:
  type: xxx
  project_id: xxx
  private_key_id: xxx
  private_key: '{{ env_var(''DBT_ENV_SECRET_PROJECTXXX_PRIVATE_KEY'') }}'
  client_email: xxx
  client_id: xxx
  auth_uri: xxx
  token_uri: xxx
  auth_provider_x509_cert_url: xxx
  client_x509_cert_url: xxx
execution_project: buck-stops-here-456

----------------------------------------

TITLE: Configuring Database Access Permissions
DESCRIPTION: Comprehensive set of grant statements for configuring access to databases, schemas, tables, and views for different roles.

LANGUAGE: sql
CODE:
grant all on warehouse warehouse_name to role role_name;
grant usage on database database_name to role role_name;
grant create schema on database database_name to role role_name; 
grant usage on schema database.an_existing_schema to role role_name;
grant create table on schema database.an_existing_schema to role role_name;
grant create view on schema database.an_existing_schema to role role_name;
grant usage on future schemas in database database_name to role role_name;
grant monitor on future schemas in database database_name to role role_name;
grant select on future tables in database database_name to role role_name;
grant select on future views in database database_name to role role_name;
grant usage on all schemas in database database_name to role role_name;
grant monitor on all schemas in database database_name to role role_name;
grant select on all tables in database database_name to role role_name;
grant select on all views in database database_name to role role_name;

----------------------------------------

TITLE: Creating Snowflake OAuth Security Integration
DESCRIPTION: SQL query to create a security integration in Snowflake for OAuth authentication with dbt Cloud. Requires ACCOUNTADMIN role or CREATE INTEGRATION privilege.

LANGUAGE: sql
CODE:
CREATE OR REPLACE SECURITY INTEGRATION DBT_CLOUD
  TYPE = OAUTH
  ENABLED = TRUE
  OAUTH_CLIENT = CUSTOM
  OAUTH_CLIENT_TYPE = 'CONFIDENTIAL'
  OAUTH_REDIRECT_URI = '<REDIRECT_URI>'
  OAUTH_ISSUE_REFRESH_TOKENS = TRUE
  OAUTH_REFRESH_TOKEN_VALIDITY = 7776000;
  OAUTH_USE_SECONDARY_ROLES = 'IMPLICIT';

----------------------------------------

TITLE: Setting Snowflake Session Parameters Globally
DESCRIPTION: Example of using sql_header in dbt_project.yml to set Snowflake session parameters for all models. This applies the session alteration to every model in the project.

LANGUAGE: yml
CODE:
config-version: 2

models:
  +sql_header: "alter session set timezone = 'Australia/Sydney';"

----------------------------------------

TITLE: Configuring an append incremental strategy in SQL
DESCRIPTION: Example of configuring an append incremental strategy in a dbt SQL model file

LANGUAGE: sql
CODE:
{{ config(
    materialized='incremental',
    incremental_strategy='append',
) }}

select * from {{ ref('events') }}
{% if is_incremental() %}
  where event_ts > (select max(event_ts) from {{ this }})
{% endif %}

----------------------------------------

TITLE: Configuring meta for exposures in exposures.yml
DESCRIPTION: Shows how to configure meta properties for exposures in an exposures.yml file. Meta can be set at the exposure level.

LANGUAGE: yaml
CODE:
version: 2

exposures:
  - name: exposure_name
    meta: {<dictionary>}

----------------------------------------

TITLE: Importing Prerequisites Component in Markdown
DESCRIPTION: Import statement for the visual editor prerequisites component from a markdown snippet file.

LANGUAGE: markdown
CODE:
import Prerequisites from '/snippets/_visual-editor-prerequisites.md';

----------------------------------------

TITLE: Tableau Workbook Refresh Integration Code
DESCRIPTION: Main Python implementation for validating webhooks from dbt Cloud and triggering Tableau workbook refreshes. Handles authentication with Tableau API, workbook identification, and refresh command execution.

LANGUAGE: python
CODE:
import requests
import hashlib
import json
import hmac

# Access secret credentials
secret_store = StoreClient('YOUR_STORAGE_SECRET_HERE')
hook_secret = secret_store.get('DBT_WEBHOOK_KEY')
server_url = secret_store.get('TABLEAU_SITE_URL')
server_name = secret_store.get('TABLEAU_SITE_NAME')
pat_name = secret_store.get('TABLEAU_API_TOKEN_NAME')
pat_secret = secret_store.get('TABLEAU_API_TOKEN_SECRET')

#Enter the name of the workbook to refresh
workbook_name = "YOUR_WORKBOOK_NAME"
api_version = "ENTER_COMPATIBLE_VERSION"

#Validate authenticity of webhook coming from dbt Cloud
auth_header = input_data['auth_header']
raw_body = input_data['raw_body']

signature = hmac.new(hook_secret.encode('utf-8'), raw_body.encode('utf-8'), hashlib.sha256).hexdigest()

if signature != auth_header:
raise Exception("Calculated signature doesn't match contents of the Authorization header. This webhook may not have been sent from dbt Cloud.")

full_body = json.loads(raw_body)
hook_data = full_body['data'] 

if hook_data['runStatus'] == "Success":

#Authenticate with Tableau Server to get an authentication token
auth_url = f"{server_url}/api/{api_version}/auth/signin"
auth_data = {
    "credentials": {
        "personalAccessTokenName": pat_name,
        "personalAccessTokenSecret": pat_secret,
        "site": {
            "contentUrl": server_name
        }
    }
}
auth_headers = {
    "Accept": "application/json",
    "Content-Type": "application/json"
}
auth_response = requests.post(auth_url, data=json.dumps(auth_data), headers=auth_headers)

#Extract token to use for subsequent calls
auth_token = auth_response.json()["credentials"]["token"]
site_id = auth_response.json()["credentials"]["site"]["id"]

#Extract the workbook ID
workbooks_url = f"{server_url}/api/{api_version}/sites/{site_id}/workbooks"
workbooks_headers = {
    "Accept": "application/json",
    "Content-Type": "application/json",
    "X-Tableau-Auth": auth_token
}
workbooks_params = {
    "filter": f"name:eq:{workbook_name}"
}
workbooks_response = requests.get(workbooks_url, headers=workbooks_headers, params=workbooks_params)

#Assign workbook ID
workbooks_data = workbooks_response.json()
workbook_id = workbooks_data["workbooks"]["workbook"][0]["id"]

# Refresh the workbook
refresh_url = f"{server_url}/api/{api_version}/sites/{site_id}/workbooks/{workbook_id}/refresh"
refresh_data = {}
refresh_headers = {
    "Accept": "application/json",
    "Content-Type": "application/json",
    "X-Tableau-Auth": auth_token
}

refresh_trigger = requests.post(refresh_url, data=json.dumps(refresh_data), headers=refresh_headers)
return {"message": "Workbook refresh has been queued"}

----------------------------------------

TITLE: Executing dbt run-operation Command in Bash
DESCRIPTION: This snippet demonstrates the syntax for using the dbt run-operation command. It shows how to specify a macro to invoke and how to pass arguments to the macro using YAML-formatted strings.

LANGUAGE: bash
CODE:
$ dbt run-operation {macro} --args '{args}'
  {macro}        Specify the macro to invoke. dbt will call this macro
                        with the supplied arguments and then exit
  --args ARGS           Supply arguments to the macro. This dictionary will be
                        mapped to the keyword arguments defined in the
                        selected macro. This argument should be a YAML string,
                        eg. '{my_variable: my_value}'

----------------------------------------

TITLE: Using a Macro from a Package in dbt
DESCRIPTION: This snippet shows how to use a macro from an external package (dbt-utils) in a dbt SQL model.

LANGUAGE: sql
CODE:
select
  field_1,
  field_2,
  field_3,
  field_4,
  field_5,
  count(*)
from my_table
{{ dbt_utils.dimensions(5) }}

----------------------------------------

TITLE: Implementing generate_database_name Macro in Jinja2
DESCRIPTION: This Jinja2 macro, generate_database_name, controls how dbt generates model database names. It uses the supplied database config if present, otherwise defaulting to the target database.

LANGUAGE: jinja2
CODE:
{% macro generate_database_name(custom_database_name=none, node=none) -%}

    {%- set default_database = target.database -%}
    {%- if custom_database_name is none -%}

        {{ default_database }}

    {%- else -%}

        {{ custom_database_name | trim }}

    {%- endif -%}

{%- endmacro %}

----------------------------------------

TITLE: General Model Configuration Example
DESCRIPTION: Example showing how to configure models in dbt_project.yml with directory-specific settings.

LANGUAGE: yaml
CODE:
name: dbt_labs

models:
  dbt_labs:
    events:
      +enabled: true
      +materialized: view
      base:
        +materialized: ephemeral

----------------------------------------

TITLE: Creating a Staging Model in dbt
DESCRIPTION: This dbt SQL code creates a staging model that selects and transforms data from a raw source table. It includes config for materialization and demonstrates the use of Jinja templating for dynamic SQL generation.

LANGUAGE: sql
CODE:
{{{
    config(
        materialized='view'
    )
}}}

with raw_data as (
    select * 
    from {{ source('raw', 'some_raw_table')}}
    where is_test_record = false
),

cleaned as (
    select messageid,
           orderid::int as orderid,
           sk_id,
           case when client_name in ['a', 'b', 'c'] then clientid else -1 end
    from   raw_data
)

select * from cleaned

----------------------------------------

TITLE: Configuring transaction control for hooks in dbt_project.yml
DESCRIPTION: Example of using dictionaries in dbt_project.yml to control transaction behavior for hooks, allowing them to run outside of transactions.

LANGUAGE: yaml
CODE:
models:
  +pre-hook:
    sql: "SQL-statement"
    transaction: false
  +post-hook:
    sql: "SQL-statement"
    transaction: false

----------------------------------------

TITLE: Setting Up Snowflake Key Pair Authentication in dbt
DESCRIPTION: This configuration demonstrates how to use key pair authentication for Snowflake in dbt. It includes options for specifying the private key path or the key itself, along with an optional passphrase for encrypted keys.

LANGUAGE: yaml
CODE:
my-snowflake-db:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: [account id]
      user: [username]
      role: [user role]

      # Keypair config
      private_key_path: [path/to/private.key]
      # or private_key instead of private_key_path
      private_key_passphrase: [passphrase for the private key, if key is encrypted]

      database: [database name]
      warehouse: [warehouse name]
      schema: [dbt schema]
      threads: [1 or more]
      client_session_keep_alive: False
      query_tag: [anything]

      # optional
      connect_retries: 0 # default 0
      connect_timeout: 10 # default: 10
      retry_on_database_errors: False # default: false
      retry_all: False  # default: false
      reuse_connections: True # default: True if client_session_keep_alive is False, otherwise None

----------------------------------------

TITLE: Loading Sample Data into Microsoft Fabric
DESCRIPTION: SQL script to create and populate three tables (customers, orders, payments) in Microsoft Fabric warehouse using sample Jaffle Shop data from external Parquet files.

LANGUAGE: sql
CODE:
DROP TABLE dbo.customers;

CREATE TABLE dbo.customers
(
    [ID] [int],
    [FIRST_NAME] [varchar](8000),
    [LAST_NAME] [varchar](8000)
);

COPY INTO [dbo].[customers]
FROM 'https://dbtlabsynapsedatalake.blob.core.windows.net/dbt-quickstart-public/jaffle_shop_customers.parquet'
WITH (
    FILE_TYPE = 'PARQUET'
);

----------------------------------------

TITLE: Configuring dbt Profile for IBM watsonx.data Spark Connection
DESCRIPTION: Example YAML configuration for setting up a dbt profile to connect to IBM watsonx.data Spark. It includes settings for connection method, schema, host, catalog, SSL usage, and authentication details.

LANGUAGE: yaml
CODE:
project_name:
  target: "dev"
  outputs:
    dev:
      type: watsonx_spark
      method: http
      schema: [schema name]
      host: [hostname]
      uri: [uri]
      catalog: [catalog name]
      use_ssl: false
      auth:
        instance: [Watsonx.data Instance ID]
        user: [username]
        apikey: [apikey]

----------------------------------------

TITLE: Configuring Sources in dbt_project.yml (dbt 1.9+)
DESCRIPTION: Example of configuring sources in the dbt_project.yml file for dbt version 1.9 and later. It demonstrates how to set enabled, event_time, and meta configurations for sources.

LANGUAGE: yaml
CODE:
sources:
  [<resource-path>]:
    [+][enabled]: true | false
    [+][event_time]: my_time_field
    [+][meta]:
      key: value

----------------------------------------

TITLE: Configuring Dispatch Search Order in dbt YAML
DESCRIPTION: Example of configuring the dispatch search order in dbt_project.yml to override package macros. This configuration tells dbt to search for macro implementations in the specified order across different packages.

LANGUAGE: yaml
CODE:
dispatch:
  - macro_namespace: dbt_utils
    search_order: ['my_project', 'dbt_utils']

----------------------------------------

TITLE: Creating dbt Cloud API Python script
DESCRIPTION: Python script to call the dbt Cloud API and run a job. Requires environment variables for API key, account ID, project ID, and job ID.

LANGUAGE: python
CODE:
#------------------------------------------------------------------------------
# get environment variables
#------------------------------------------------------------------------------
api_base        = os.getenv('DBT_URL', 'https://cloud.getdbt.com/') # default to multitenant url
job_cause       = os.getenv('DBT_JOB_CAUSE', 'API-triggered job') # default to generic message
git_branch      = os.getenv('DBT_JOB_BRANCH', None) # default to None
schema_override = os.getenv('DBT_JOB_SCHEMA_OVERRIDE', None) # default to None
api_key         = os.environ['DBT_API_KEY']  # no default here, just throw an error here if key not provided
account_id      = os.environ['DBT_ACCOUNT_ID'] # no default here, just throw an error here if id not provided
project_id      = os.environ['DBT_PROJECT_ID'] # no default here, just throw an error here if id not provided
job_id          = os.environ['DBT_PR_JOB_ID'] # no default here, just throw an error here if id not provided

----------------------------------------

TITLE: Configuring pre-hooks and post-hooks for models in dbt_project.yml
DESCRIPTION: Define pre-hooks and post-hooks for models in the dbt_project.yml file. Hooks can be single SQL statements or lists of statements.

LANGUAGE: yaml
CODE:
models:
  [<resource-path>]:
    +pre-hook: SQL-statement | [SQL-statement]
    +post-hook: SQL-statement | [SQL-statement]

----------------------------------------

TITLE: Multiple Columns Check Example (DBT 1.9+)
DESCRIPTION: Complete example showing how to check multiple specific columns in a snapshot using YAML configuration. Includes source relation, schema, and multiple check columns.

LANGUAGE: yaml
CODE:
snapshots:
  - name: orders_snapshot_check
    relation: source('jaffle_shop', 'orders')
    config:
      schema: snapshots
      unique_key: id
      strategy: check
      check_cols:
        - status
        - is_cancelled

----------------------------------------

TITLE: Configuring Local Package Installation
DESCRIPTION: Configuration for installing a local package during development by referencing the parent directory in packages.yml.

LANGUAGE: yaml
CODE:
packages:
    - local: ../ # this means "one directory above the current directory"

----------------------------------------

TITLE: Creating an Hourly Time Spine SQL Model
DESCRIPTION: SQL code to create an hourly time spine table using dbt's date_spine function, covering dates from 2000 to 2025 and filtered to a specific range.

LANGUAGE: sql
CODE:
{{{
    config(
        materialized = 'table',
    )
}}}

with hours as (

    {{
        dbt.date_spine(
            'hour',
            "to_date('01/01/2000','mm/dd/yyyy')",
            "to_date('01/01/2025','mm/dd/yyyy')"
        )
    }}

),

final as (
    select cast(date_hour as timestamp) as date_hour
    from hours
)

select * from final
where date_day > dateadd(year, -4, current_timestamp()) 
and date_hour < dateadd(day, 30, current_timestamp())

----------------------------------------

TITLE: Setting Project-Level Grants
DESCRIPTION: Example of setting default grants for all models in dbt_project.yml

LANGUAGE: yaml
CODE:
models:
  +grants:
    select: ['user_a', 'user_b']

----------------------------------------

TITLE: Insert Overwrite Strategy with Partitioning in AWS Glue
DESCRIPTION: Demonstrates the insert_overwrite strategy with partition_by configuration. This approach atomically replaces partitions and is useful for maintaining partitioned tables.

LANGUAGE: sql
CODE:
{{ config(
    materialized='incremental',
    partition_by=['date_day'],
    file_format='parquet'
) }}

/*
  Every partition returned by this query will be overwritten
  when this model runs
*/

with new_events as (

    select * from {{ ref('events') }}

    {% if is_incremental() %}
    where date_day >= date_add(current_date, -1)
    {% endif %}

)

select
    date_day,
    count(*) as users

from events
group by 1

----------------------------------------

TITLE: Package Installation Reference Table
DESCRIPTION: A structured table listing essential dbt packages with their descriptions, including codegen, dbt_utils, project_evaluator, expectations, audit_helper, artifacts, and meta_testing packages.

LANGUAGE: markdown
CODE:
| Package | Description |
|---------|-------------|
| [`dbt_codegen`](https://hub.getdbt.com/dbt-labs/codegen/latest/) |Use the package to help you generate YML files for your models and sources and SQL files for your staging models. |
| [`dbt_utils`](https://hub.getdbt.com/dbt-labs/dbt_utils/latest/) | The package contains macros useful for daily development. For example, `date_spine` generates a table with all dates between the ones provided as parameters. |
| [`dbt_project_evaluator`](https://hub.getdbt.com/dbt-labs/dbt_project_evaluator/latest) | The package compares your dbt project against a list of our best practices and provides suggestions and guidelines on how to update your models. |
| [`dbt_expectations`](https://hub.getdbt.com/calogica/dbt_expectations/latest) | The package contains many tests beyond those built into dbt. |
| [`dbt_audit_helper`](https://hub.getdbt.com/#:~:text=adwords-,audit_helper,-codegen) | The package lets you compare the output of 2 queries. Use it when refactoring existing logic to ensure that the new results are identical. |
| [`dbt_artifacts`](https://hub.getdbt.com/brooklyn-data/dbt_artifacts/latest) | The package saves information about your dbt runs directly to your data platform so that you can track the performance of models over time. |
| [`dbt_meta_testing`](https://hub.getdbt.com/tnightengale/dbt_meta_testing/latest) | This package checks that your dbt project is sufficiently tested and documented. |

----------------------------------------

TITLE: Running dbt clean command
DESCRIPTION: Executes the dbt clean command to delete paths specified in the clean-targets list of the dbt_project.yml file.

LANGUAGE: shell
CODE:
dbt clean

----------------------------------------

TITLE: Keyboard Shortcut Examples for dbt Cloud IDE
DESCRIPTION: Demonstrates how to use specific character combinations to trigger IDE features, such as revealing dbt functions using double underscores.

LANGUAGE: text
CODE:
__

----------------------------------------

TITLE: generate_schema_name_for_env Macro Implementation
DESCRIPTION: This snippet shows the implementation of the generate_schema_name_for_env macro, which uses different logic based on the target environment.

LANGUAGE: sql
CODE:
{% macro generate_schema_name_for_env(custom_schema_name, node) -%}

    {%- set default_schema = target.schema -%}
    {%- if target.name == 'prod' and custom_schema_name is not none -%}

        {{ custom_schema_name | trim }}

    {%- else -%}

        {{ default_schema }}

    {%- endif -%}

{%- endmacro %}

----------------------------------------

TITLE: Implementing Log Function in Python for dbt
DESCRIPTION: This Python function logs a message to either a log file or stdout. It takes two parameters: 'msg' for the message to log, and 'info' to determine the logging destination. The function uses fire_event to trigger appropriate logging events.

LANGUAGE: python
CODE:
def log(msg: str, info: bool = False) -> str: 
    """Logs a line to either the log file or stdout.

    :param msg: The message to log
    :param info: If `False`, write to the log file. If `True`, write to
        both the log file and stdout.

    > macros/my_log_macro.sql

        {% macro some_macro(arg1, arg2) %}
          {{ log("Running some_macro: " ~ arg1 ~ ", " ~ arg2) }}
        {% endmacro %}"""
    if info:
        fire_event(JinjaLogInfo(msg=msg, node_info=get_node_info()))
    else:
        fire_event(JinjaLogDebug(msg=msg, node_info=get_node_info()))
    return ""

----------------------------------------

TITLE: SQL Unique Key Example
DESCRIPTION: Example demonstrating a table structure with both primary and unique keys, where email addresses must be unique but can contain null values.

LANGUAGE: sql
CODE:
student_id (primary key)
email (unique key)
first_name
last_name

----------------------------------------

TITLE: Creating Tables in Starburst Galaxy
DESCRIPTION: SQL statements to create schema and tables for Jaffle Shop data in Starburst Galaxy

LANGUAGE: sql
CODE:
CREATE SCHEMA jaffle_shop WITH (location='s3://YOUR_S3_BUCKET_NAME/dbt-quickstart/');

CREATE TABLE jaffle_shop.jaffle_shop_customers (
    id VARCHAR,
    first_name VARCHAR,
    last_name VARCHAR
)
WITH (
    external_location = 's3://YOUR_S3_BUCKET_NAME/dbt-quickstart/jaffle-shop-customers/',
    format = 'csv',
    type = 'hive',
    skip_header_line_count=1
);

----------------------------------------

TITLE: Configuring meta for sources in schema.yml
DESCRIPTION: Shows how to configure meta properties for sources, their tables, and columns in a schema.yml file. Meta can be set at the source, table, and column levels.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: model_name
    config:
      meta: {<dictionary>}

    tables:
      - name: table_name
        config:
          meta: {<dictionary>}

        columns:
          - name: column_name
            meta: {<dictionary>}

----------------------------------------

TITLE: Defining dbt Source with Different Database in YAML
DESCRIPTION: YAML configuration that demonstrates how to specify a source table located in a different database using the database property. This example shows configuration for jaffle_shop sources in the 'raw' database.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: jaffle_shop
    database: raw
    tables:
      - name: orders
      - name: customers


----------------------------------------

TITLE: SQL Row Limit Example
DESCRIPTION: Example showing how to specify row limits in SQL queries within the dbt Cloud IDE

LANGUAGE: sql
CODE:
SELECT * FROM table limit 100

----------------------------------------

TITLE: Configuring Model Access in SQL File
DESCRIPTION: This snippet demonstrates how to set the access level directly in a SQL model file using the config block. It sets the access to 'public' for the model defined in 'my_public_model.sql'.

LANGUAGE: sql
CODE:
-- models/my_public_model.sql

{{ config(access = "public") }}

select ...

----------------------------------------

TITLE: Dropping Schema in SQL using dbt adapter
DESCRIPTION: This snippet demonstrates how to use the `adapter.drop_schema` method to drop a schema and all its objects in the target database.

LANGUAGE: sql
CODE:
{% do adapter.drop_schema(api.Relation.create(database=target.database, schema="my_schema")) %}

----------------------------------------

TITLE: Creating Snowflake OAuth Security Integration for Entra ID
DESCRIPTION: SQL command to create or replace a security integration in Snowflake for external OAuth authentication with Entra ID. This snippet includes placeholders for the integration name, Azure AD issuer, JWS key endpoint, and Snowflake application ID URI.

LANGUAGE: sql
CODE:
create or replace security integration <whatever you want to name it>
   type = external_oauth
   enabled = true
   external_oauth_type = azure
   external_oauth_issuer = '<AZURE_AD_ISSUER>'
   external_oauth_jws_keys_url = '<AZURE_AD_JWS_KEY_ENDPOINT>'
   external_oauth_audience_list = ('<SNOWFLAKE_APPLICATION_ID_URI>')
   external_oauth_token_user_mapping_claim = 'upn'
   external_oauth_any_role_mode = 'ENABLE'
   external_oauth_snowflake_user_mapping_attribute = 'login_name';

----------------------------------------

TITLE: Configuring Model-Specific Grants in SQL
DESCRIPTION: Example of configuring grants directly in a model's SQL file using config blocks

LANGUAGE: sql
CODE:
{{ config(materialized = 'table', grants = {
    'select': 'bi_user'
}) }}

----------------------------------------

TITLE: Invalid dbt Config Block with Dashes
DESCRIPTION: Example of an invalid configuration syntax that will cause errors due to the dash in 'post-hook'.

LANGUAGE: sql
CODE:
{{ config(
    post-hook="grant select on {{ this }} to role reporter",
    materialized='table'
) }}

select ...

----------------------------------------

TITLE: Hybrid Columnar Compression for Archive in Oracle dbt Model
DESCRIPTION: Examples of configuring Hybrid Columnar Compression for archival purposes with both LOW and HIGH compression options.

LANGUAGE: sql
CODE:
{{config(materialized='table', table_compression_clause='COLUMN STORE COMPRESS FOR ARCHIVE LOW')}}
SELECT c.cust_id, c.cust_first_name, c.cust_last_name
from {{ source('sh_database', 'customers') }} c

----------------------------------------

TITLE: Configuring meta for snapshots in schema.yml
DESCRIPTION: Shows how to configure meta properties for snapshots and their columns in a schema.yml file. Meta can be set at both the snapshot and column level.

LANGUAGE: yaml
CODE:
version: 2

snapshots:
  - name: snapshot_name
    config:
      meta: {<dictionary>}

    columns:
      - name: column_name
        meta: {<dictionary>}

----------------------------------------

TITLE: Configuring Project-Level Test Failure Storage in YAML
DESCRIPTION: This snippet from dbt_project.yml demonstrates how to set 'store_failures_as' at the project level and for specific subfolders. It shows different storage methods for the project and its subfolders.

LANGUAGE: yaml
CODE:
name: "my_project"
version: "1.0.0"
config-version: 2
profile: "sandcastle"

tests:
  my_project:
    +store_failures_as: table
    my_subfolder_1:
      +store_failures_as: view
    my_subfolder_2:
      +store_failures_as: ephemeral

----------------------------------------

TITLE: Sample dbt Build Output
DESCRIPTION: Example of successful dbt build command output showing compilation and testing results.

LANGUAGE: jinja
CODE:
(venv) ‚ûú  jaffle_shop_duckdb git:(duckdb) dbt build
15:10:12  Running with dbt=1.8.1
15:10:13  Registered adapter: duckdb=1.8.1
15:10:13  Found 5 models, 3 seeds, 20 data tests, 416 macros
15:10:13  
15:10:14  Concurrency: 24 threads (target='dev')
15:10:14  
15:10:14  1 of 28 START seed file main.raw_customers ..................................... [RUN]
15:10:14  2 of 28 START seed file main.raw_orders ........................................ [RUN]
15:10:14  3 of 28 START seed file main.raw_payments ...................................... [RUN]
....

15:10:15  27 of 28 PASS relationships_orders_customer_id__customer_id__ref_customers_ .... [PASS in 0.32s]
15:10:15  
15:10:15  Finished running 3 seeds, 3 view models, 20 data tests, 2 table models in 0 hours 0 minutes and 1.52 seconds (1.52s).
15:10:15  
15:10:15  Completed successfully
15:10:15  
15:10:15  Done. PASS=28 WARN=0 ERROR=0 SKIP=0 TOTAL=28

----------------------------------------

TITLE: Referencing a Docs Block in YAML Schema
DESCRIPTION: This snippet demonstrates how to use the `doc` function to reference the previously defined 'orders' docs block in a schema.yml file. The function is used within the description field of a model.

LANGUAGE: yaml
CODE:
version: 2
models:
  - name: orders
    description: "{{ doc('orders') }}"

----------------------------------------

TITLE: Configuring dbt Profile for Azure Synapse Analytics
DESCRIPTION: YAML configuration for setting up a dbt profile to connect to Azure Synapse Analytics. Includes server, database, schema, and authentication details.

LANGUAGE: yaml
CODE:
your_profile_name:
  target: dev
  outputs:
    dev:
      type: synapse
      driver: 'ODBC Driver 17 for SQL Server' # (The ODBC Driver installed on your system)
      server: workspacename.sql.azuresynapse.net # (Dedicated SQL endpoint of your workspace here)
      port: 1433
      database: exampledb
      schema: schema_name
      user: username
      password: password

----------------------------------------

TITLE: Configuring Incremental Model Grants
DESCRIPTION: Example of setting up grants for an incremental model with multiple recipients

LANGUAGE: sql
CODE:
{{ config(materialized = 'incremental', grants = {
    'select': ['bi_user', 'reporter']
}) }}

----------------------------------------

TITLE: Setting Compression Options in Greenplum dbt Model
DESCRIPTION: Shows how to configure table compression settings including compression type, level, and block size for optimizing storage and performance.

LANGUAGE: sql
CODE:
{{
    config(
        ...
        appendonly='true',
        compresstype='ZLIB',
        compresslevel=3,
        blocksize=32768
        ...
    )
}}


select ...

----------------------------------------

TITLE: Basic SQL TRIM Function Syntax
DESCRIPTION: Demonstrates the basic syntax of the SQL TRIM function, which removes leading and trailing characters from a string. By default, it removes blank spaces if no specific characters are specified.

LANGUAGE: sql
CODE:
trim(<field_name> [, <characters_to_remove>])

----------------------------------------

TITLE: Advanced dbt Node Selection
DESCRIPTION: Examples demonstrating complex node selection using operators like plus (+), at (@), asterisk (*), and comma for defining subsets of nodes.

LANGUAGE: bash
CODE:
# multiple arguments can be provided to --select
dbt run --select "my_first_model my_second_model"

# select my_model and all of its children
dbt run --select "my_model+"     

# select my_model, its children, and the parents of its children
dbt run --models @my_model          

# these arguments can be projects, models, directory paths, tags, or sources
dbt run --select "tag:nightly my_model finance.base.*"

# use methods and intersections for more complex selectors
dbt run --select "path:marts/finance,tag:nightly,config.materialized:table"

----------------------------------------

TITLE: Generating dbt Project Documentation
DESCRIPTION: This command generates the documentation for a dbt project, including copying the index.html file, compiling resources, and creating the catalog.json file.

LANGUAGE: shell
CODE:
dbt docs generate

----------------------------------------

TITLE: Configuring dbt Profiles for watsonx.data Presto
DESCRIPTION: Example YAML configuration for connecting dbt to IBM watsonx.data Presto instances in both software and SaaS environments. Includes all required connection parameters like authentication credentials, host details, and SSL configuration.

LANGUAGE: yaml
CODE:
my_project:
  outputs:
    software:
      type: presto
      method: BasicAuth
      user: [user]
      password: [password]
      host: [hostname]
      database: [catalog name]
      schema: [your dbt schema]
      port: [port number]
      threads: [1 or more]
      ssl_verify: path/to/certificate

    saas:
      type: presto
      method: BasicAuth
      user: [user]
      password: [api_key]
      host: [hostname]
      database: [catalog name]
      schema: [your dbt schema]
      port: [port number]
      threads: [1 or more]

  target: software

----------------------------------------

TITLE: Azure Private Link Request Template
DESCRIPTION: Template for submitting a new Azure Private Link request to dbt Support, including required fields like Databricks instance name, Azure resource ID, environment, and region.

LANGUAGE: plaintext
CODE:
Subject: New Azure Multi-Tenant Private Link Request
- Type: Databricks
- Databricks instance name:
- Databricks Azure resource ID:
- dbt Cloud multi-tenant environment: EMEA
- Azure region: Region that hosts your Databricks workspace (like, WestEurope, NorthEurope)

----------------------------------------

TITLE: Configuring Materialized Views in SQL for dbt-Postgres
DESCRIPTION: This snippet shows how to configure materialized views directly in a SQL model file, including settings for materialization, configuration changes, and indexes.

LANGUAGE: sql
CODE:
{{ config(
    [materialized]="materialized_view",
    [on_configuration_change]="apply" | "continue" | "fail",
    [indexes]=[
        {
            "columns": ["<column-name>"],
            "unique": true | false,
            "type": "hash" | "btree",
        }
    ]
) }}

----------------------------------------

TITLE: Configuring meta for metrics in metrics.yml
DESCRIPTION: Shows how to configure meta properties for metrics in a metrics.yml file. Meta can be set directly on the metric or under the config block, depending on the dbt version.

LANGUAGE: yaml
CODE:
metrics:
  - name: number_of_people
    label: "Number of people"
    description: Total count of people
    type: simple
    type_params:
      measure: people
    config:
      meta:
        my_meta_config: 'config_value'

----------------------------------------

TITLE: Running Single dbt Model Using Select Flag
DESCRIPTION: Demonstrates how to execute a single dbt model using the --select command line flag. This command runs only the specified 'customers' model instead of the entire project.

LANGUAGE: shell
CODE:
$ dbt run --select customers

----------------------------------------

TITLE: Defining Tagged Columns for Testing in dbt
DESCRIPTION: This YAML configuration defines a model with a tagged column and a test, which can be selected using the tag.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: orders
    columns:
      - name: order_id
        tags: [my_column_tag]
        tests:
          - unique

----------------------------------------

TITLE: Running dbt Tests on Single Model
DESCRIPTION: Command to execute tests on a specific dbt model using the --select flag. This allows testing individual models instead of running tests on the entire project.

LANGUAGE: shell
CODE:
dbt test --select customers

----------------------------------------

TITLE: Parquet File Format Configuration
DESCRIPTION: Configuration for materializing a table as partitioned Parquet files using file-based connectors.

LANGUAGE: sql
CODE:
{{
  config(
    materialized='table',
    properties= {
      "format": "'PARQUET'",
      "partitioning": "ARRAY['bucket(id, 2)']"}
  )
}}

----------------------------------------

TITLE: Executing dbt Models Based on Source Selection
DESCRIPTION: This snippet illustrates how to run dbt models by selecting all models that use a specific source (in this case, Snowplow) and their parent models.

LANGUAGE: bash
CODE:
dbt run --select "@source:snowplow"   # build all models that select from snowplow sources, plus their parents

----------------------------------------

TITLE: Configuring Vertica Connection in profiles.yml for dbt
DESCRIPTION: This YAML snippet demonstrates how to configure the profiles.yml file for connecting dbt to a Vertica database. It includes various connection parameters such as host, port, authentication details, and other Vertica-specific options.

LANGUAGE: yaml
CODE:
your-profile:
  outputs:
    dev:
      type: vertica # Don't change this!
      host: [hostname]
      port: [port] # or your custom port (optional)
      username: [your username]
      password: [your password]
      database: [database name]
      oauth_access_token: [access token]
      schema: [dbt schema]
      connection_load_balance: True
      backup_server_node: [list of backup hostnames or IPs]
      retries: [1 or more]
      autocommit: False
      
      threads: [1 or more]
  target: dev

----------------------------------------

TITLE: Creating a Daily Time Spine SQL Model for BigQuery
DESCRIPTION: SQL code to create a daily time spine table specifically for BigQuery, using DATE() instead of TO_DATE() and covering dates from 2000 to 2025.

LANGUAGE: sql
CODE:
{{config(materialized='table')}}
with days as (
    {{dbt.date_spine(
        'day',
        "DATE(2000,01,01)",
        "DATE(2025,01,01)"
    )
    }}
),

final as (
    select cast(date_day as date) as date_day
    from days
)

select *
from final
where date_day > date_add(DATE(current_timestamp()), INTERVAL -4 YEAR)
and date_day < date_add(DATE(current_timestamp()), INTERVAL 30 DAY)

----------------------------------------

TITLE: Configuring Starrocks Model in dbt Project YAML
DESCRIPTION: This snippet shows how to configure a Starrocks model in the dbt_project.yml file. It includes various configuration options such as materialization type, keys, table type, distribution, partitioning, and properties.

LANGUAGE: yaml
CODE:
models:
  <resource-path>:
    materialized: table       // table or view or materialized_view
    keys: ['id', 'name', 'some_date']
    table_type: 'PRIMARY'     // PRIMARY or DUPLICATE or UNIQUE
    distributed_by: ['id']
    buckets: 3                // default 10
    partition_by: ['some_date']
    partition_by_init: ["PARTITION p1 VALUES [('1971-01-01 00:00:00'), ('1991-01-01 00:00:00')),PARTITION p1972 VALUES [('1991-01-01 00:00:00'), ('1999-01-01 00:00:00'))"]
    properties: [{"replication_num":"1", "in_memory": "true"}]
    refresh_method: 'async' // only for materialized view default manual

----------------------------------------

TITLE: Defining Model Columns with Constraints in YAML
DESCRIPTION: YAML configuration for a dbt model specifying column names, data types, and constraints.

LANGUAGE: yaml
CODE:
models:
  - name: dim_customers
    config:
      materialized: table
      contract:
        enforced: true
    columns:
      - name: customer_id
        data_type: int
        constraints:
          - type: not_null
      - name: customer_name
        data_type: string
      - name: non_integer
        data_type: numeric(38,3)

----------------------------------------

TITLE: Snapshot Config Block with Target Database
DESCRIPTION: Configuration block for setting target database within a snapshot SQL file using Jinja templating.

LANGUAGE: jinja2
CODE:
{{ config(
  target_database="string"
) }}

----------------------------------------

TITLE: Renaming Relation in SQL using dbt adapter
DESCRIPTION: This snippet demonstrates how to use the `adapter.rename_relation` method to rename a relation in the database.

LANGUAGE: sql
CODE:
{%- set old_relation = adapter.get_relation(
      database=this.database,
      schema=this.schema,
      identifier=this.identifier) -%}

{%- set backup_relation = adapter.get_relation(
      database=this.database,
      schema=this.schema,
      identifier=this.identifier ~ "__dbt_backup") -%}

{% do adapter.rename_relation(old_relation, backup_relation) %}

----------------------------------------

TITLE: Defining Test Description in YAML
DESCRIPTION: Add descriptions to singular tests using a YAML configuration file in the tests directory.

LANGUAGE: yaml
CODE:
version: 2
data_tests:
  - name: assert_total_payment_amount_is_positive
    description: >
      Refunds have a negative amount, so the total amount should always be >= 0.
      Therefore return records where total amount < 0 to make the test fail.

----------------------------------------

TITLE: Configuring Materialized Views for Development (BigQuery)
DESCRIPTION: SQL configuration for materialized views in development environment for BigQuery, including refresh interval and max staleness settings.

LANGUAGE: sql
CODE:
{{\nconfig(\n    materialized = 'materialized_view',\n    on_configuration_change = 'apply',\n    enable_refresh = True,\n    refresh_interval_minutes = 30\n    max_staleness = 'INTERVAL 60 MINUTE'\n)\n}}

----------------------------------------

TITLE: Using RANK Function in SQL Query
DESCRIPTION: This SQL query demonstrates how to use the RANK function to rank orders by their order date. It shows the basic syntax and how RANK handles ties in the ranking.

LANGUAGE: sql
CODE:
select
	order_id,
	order_date,
	rank() over (order by order_date) as order_rank
from {{ ref('orders') }}

----------------------------------------

TITLE: Configuring dbt Packages in YAML
DESCRIPTION: Example packages.yml configuration showing different package types including Hub packages, git repositories, and subdirectory specifications with version pins.

LANGUAGE: yaml
CODE:
packages:
  - package: dbt-labs/dbt_utils
    version: 0.7.1
  - package: brooklyn-data/dbt_artifacts
    version: 1.2.0
    install-prerelease: true
  - package: dbt-labs/codegen
    version: 0.4.0
  - package: calogica/dbt_expectations
    version: 0.4.1
  - git: https://github.com/dbt-labs/dbt-audit-helper.git
    revision: 0.4.0
  - git: "https://github.com/dbt-labs/dbt-labs-experimental-features"
    subdirectory: "materialized-views"
    revision: 0.0.1
  - package: dbt-labs/snowplow
    version: 0.13.0

----------------------------------------

TITLE: Configuring Table Materialization in Doris/SelectDB for dbt
DESCRIPTION: This snippet shows how to configure a dbt model as a Doris table using either the project file or a config block in the model file. It includes various configuration options such as duplicate_key, partition_by, distributed_by, and properties.

LANGUAGE: yaml
CODE:
models:
  <resource-path>:
    +materialized: table
    +duplicate_key: [ <column-name>, ... ],
    +partition_by: [ <column-name>, ... ],
    +partition_type: <engine-type>,
    +partition_by_init: [<pertition-init>, ... ]
    +distributed_by: [ <column-name>, ... ],
    +buckets: int,
    +properties: {<key>:<value>,...}

LANGUAGE: jinja
CODE:
{{ config(
    materialized = "table",
    duplicate_key = [ "<column-name>", ... ],
    partition_by = [ "<column-name>", ... ],
    partition_type = "<engine-type>",
    partition_by_init = ["<pertition-init>", ... ]
    distributed_by = [ "<column-name>", ... ],
    buckets = "int",
    properties = {"<key>":"<value>",...}
      ...
    ]
) }}

----------------------------------------

TITLE: Configuring Pre-Hook in YAML Properties
DESCRIPTION: Shows how to configure pre-hooks in a model's YAML properties file using macro calls.

LANGUAGE: yaml
CODE:
models:
  - name: <model_name>
    config:
      pre_hook:
        - "{{ some_macro() }}"

----------------------------------------

TITLE: Basic dbt ls Command Structure
DESCRIPTION: Shows the complete command syntax including all available arguments for the dbt ls command, including resource type filtering, selection arguments, and output formatting options.

LANGUAGE: bash
CODE:
dbt ls
     [--resource-type {model,semantic_model,source,seed,snapshot,metric,test,exposure,analysis,default,all}]
     [--select SELECTION_ARG [SELECTION_ARG ...]]
     [--models SELECTOR [SELECTOR ...]]
     [--exclude SELECTOR [SELECTOR ...]]
     [--selector YML_SELECTOR_NAME]
     [--output {json,name,path,selector}]
     [--output-keys KEY_NAME [KEY_NAME]]

----------------------------------------

TITLE: Runtime Error Example in dbt Packages
DESCRIPTION: Example of the runtime error that occurs when using an incompatible dbt_utils package version with current dbt Cloud installation.

LANGUAGE: shell
CODE:
Running with dbt=xxx
Runtime Error
  Failed to read package: Runtime Error
    Invalid config version: 1, expected 2  
  Error encountered in dbt_utils/dbt_project.yml

----------------------------------------

TITLE: Invalid dbt Project Configuration
DESCRIPTION: Example showing invalid configuration in dbt_project.yml with an unrecognized key.

LANGUAGE: yaml
CODE:
name: jaffle_shop
hello: world # this is not allowed


----------------------------------------

TITLE: Configuring Lake Formation Tags in Schema YAML
DESCRIPTION: Example showing how to configure Lake Formation tags and column-level tags for an incremental Iceberg table model.

LANGUAGE: sql
CODE:
{{
  config(
    materialized='incremental',
    incremental_strategy='append',
    on_schema_change='append_new_columns',
    table_type='iceberg',
    schema='test_schema',
    lf_tags_config={
          'enabled': true,
          'tags': {
            'tag1': 'value1',
            'tag2': 'value2'
          },
          'tags_columns': {
            'tag1': {
              'value1': ['column1', 'column2'],
              'value2': ['column3', 'column4']
            }
          },
          'inherited_tags': ['tag1', 'tag2']
    }
  )
}}

----------------------------------------

TITLE: Downstream Incremental Model Configuration
DESCRIPTION: Shows configuration for an incremental model to capture historical versions without using snapshots, including settings for schema changes and refresh behavior.

LANGUAGE: sql
CODE:
{{
    config(
      materialized='incremental',
	  full_refresh=false,
	  schema='history',
	  on_schema_change='sync_all_columns'
    )
}}

----------------------------------------

TITLE: Configuring Unique Key in SQL Model
DESCRIPTION: Example of setting a unique key in an incremental model's SQL file using the config block.

LANGUAGE: sql
CODE:
{{
    config(
        materialized='incremental',
        unique_key='id'
    )
}}

----------------------------------------

TITLE: Configuring Starrocks Model in Properties YAML
DESCRIPTION: This snippet demonstrates how to configure a Starrocks model in the models/properties.yml file. It includes the same configuration options as the project YAML, but in a different format.

LANGUAGE: yaml
CODE:
models:
  - name: <model-name>
    config:
      materialized: table       // table or view or materialized_view
      keys: ['id', 'name', 'some_date']
      table_type: 'PRIMARY'     // PRIMARY or DUPLICATE or UNIQUE
      distributed_by: ['id']
      buckets: 3                // default 10
      partition_by: ['some_date']
      partition_by_init: ["PARTITION p1 VALUES [('1971-01-01 00:00:00'), ('1991-01-01 00:00:00')),PARTITION p1972 VALUES [('1991-01-01 00:00:00'), ('1999-01-01 00:00:00'))"]
      properties: [{"replication_num":"1", "in_memory": "true"}]
      refresh_method: 'async' // only for materialized view default manual

----------------------------------------

TITLE: Configuring Data Tests in YAML Property File
DESCRIPTION: This snippet shows how to configure data tests in a YAML property file. It includes configurations for both resource-level and column-level tests, with options like fail_calc, limit, severity, and store_failures.

LANGUAGE: yaml
CODE:
version: 2

<resource_type>:
  - name: <resource_name>
    tests:
      - <test_name>:
          name:
          [description]: "markdown formatting"
          <argument_name>: <argument_value>
          [config]:
            [fail_calc]: <string>
            [limit]: <integer>
            [severity]: error | warn
            [error_if]: <string>
            [warn_if]: <string>
            [store_failures]: true | false
            [where]: <string>

----------------------------------------

TITLE: Configuring Custom Database for Models in dbt
DESCRIPTION: Demonstrates how to specify a custom database for a model in the dbt_project.yml file. This configuration sets the 'sales_metrics' model to be created in the 'reporting' database instead of the default target database.

LANGUAGE: yaml
CODE:
models:
  your_project:
    sales_metrics:
      +database: reporting

----------------------------------------

TITLE: Legacy SQL Snapshot Configuration (v1.8 and earlier)
DESCRIPTION: Example of configuring a snapshot using SQL and Jinja templating in dbt versions 1.8 and earlier

LANGUAGE: sql
CODE:
{% snapshot orders_snapshot %}

{{
    config(
      target_database='analytics',
      target_schema='snapshots',
      unique_key='id',
      strategy='timestamp',
      updated_at='updated_at',
    )
}}

select * from {{ source('jaffle_shop', 'orders') }}

{% endsnapshot %}

----------------------------------------

TITLE: Setting Custom Database for Test Results in dbt
DESCRIPTION: Shows how to configure a custom database for storing test results in the dbt_project.yml file. This setup enables storing test failures and specifies the 'test_results' database for storage.

LANGUAGE: yaml
CODE:
tests:
  +store_failures: true
  +database: test_results

----------------------------------------

TITLE: Defining Sources and Models in dbt YAML Configuration
DESCRIPTION: This YAML configuration defines sources and models for a dbt project. It includes source tables with column definitions and tests, as well as model configurations with tags, materialization settings, and column tests.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: raw_jaffle_shop
    description: A replica of the postgres database used to power the jaffle_shop app.
    tables:
      - name: customers
        columns:
          - name: id
            description: Primary key of the table
            tests:
              - unique
              - not_null

      - name: orders
        columns:
          - name: id
            description: Primary key of the table
            tests:
              - unique
              - not_null

          - name: user_id
            description: Foreign key to customers

          - name: status
            tests:
              - accepted_values:
                  values: ['placed', 'shipped', 'completed', 'return_pending', 'returned']


models:
  - name: stg_jaffle_shop__customers
    config:
      tags: ['pii']
    columns:
      - name: customer_id
        tests:
          - unique
          - not_null

  - name: stg_jaffle_shop__orders
    config:
      materialized: view
    columns:
      - name: order_id
        tests:
          - unique
          - not_null
      - name: status
        tests:
          - accepted_values:
              values: ['placed', 'shipped', 'completed', 'return_pending', 'returned']
              config:
                severity: warn

----------------------------------------

TITLE: Basic Query Execution with run_query
DESCRIPTION: Simple example demonstrating how to execute a basic SELECT query and print results using run_query macro.

LANGUAGE: jinja2
CODE:
{% set results = run_query('select 1 as id') %}
{% do results.print_table() %}

-- do something with `results` here...

----------------------------------------

TITLE: Configuring Unit Test Structure in dbt_project.yml
DESCRIPTION: Defines the basic structure and properties for configuring unit tests in dbt, including test naming, model selection, input data configuration, and expected outputs.

LANGUAGE: yml
CODE:
unit_tests:
  - name: <test-name> # this is the unique name of the test
    model: <model-name> 
      versions: #optional
        include: <list-of-versions-to-include> #optional
        exclude: <list-of-versions-to-exclude> #optional
    config: 
      meta: {dictionary}
      tags: <string> | [<string>]
      enabled: {boolean} # optional. v1.9 or higher. If not configured, defaults to `true`
    given:
      - input: <ref_or_source_call> # optional for seeds
        format: dict | csv | sql
        # either define rows inline or name of fixture
        rows: {dictionary} | <string>
        fixture: <fixture-name> # sql or csv 
      - input: ... # declare additional inputs
    expect:
      format: dict | csv | sql
      # either define rows inline of rows or name of fixture
      rows: {dictionary} | <string>
      fixture: <fixture-name> # sql or csv 
    overrides: # optional: configuration for the dbt execution environment
      macros:
        is_incremental: true | false
        dbt_utils.current_timestamp: <string>
        # ... any other jinja function from https://docs.getdbt.com/reference/dbt-jinja-functions
        # ... any other context property
      vars: {dictionary}
      env_vars: {dictionary}
  - name: <test-name> ... # declare additional unit tests

----------------------------------------

TITLE: Staging Customer Model
DESCRIPTION: A staging model that performs initial transformation of customer data.

LANGUAGE: sql
CODE:
select
    id as customer_id,
    first_name,
    last_name

from jaffle_shop.customers

----------------------------------------

TITLE: Configuring Incremental Model with Append Strategy in Spark SQL
DESCRIPTION: This snippet demonstrates how to configure an incremental model using the 'append' strategy in Spark SQL. It includes both the source code and the resulting run code.

LANGUAGE: sql
CODE:
{{ config(
    materialized='incremental',
    incremental_strategy='append',
) }}

--  All rows returned by this query will be appended to the existing table

select * from {{ ref('events') }}
{% if is_incremental() %}
  where event_ts > (select max(event_ts) from {{ this }})
{% endif %}

LANGUAGE: sql
CODE:
create temporary view spark_incremental__dbt_tmp as

    select * from analytics.events

    where event_ts >= (select max(event_ts) from {{ this }})

;

insert into table analytics.spark_incremental
    select `date_day`, `users` from spark_incremental__dbt_tmp

----------------------------------------

TITLE: Configuring Model Access in properties.yml (New Method)
DESCRIPTION: This snippet demonstrates the new method (for dbt v1.7 or higher) of setting model access in the properties.yml file. It uses the 'config' key to set the access level to public for 'my_public_model'.

LANGUAGE: yml
CODE:
version: 2

models:
  - name: my_public_model
    config:
      access: public # newly supported in v1.7

----------------------------------------

TITLE: Examples of Non-Boolean Config Flags in dbt CLI
DESCRIPTION: Provides specific examples of using non-boolean configuration flags in dbt CLI commands, including setting printer width and indirect selection mode.

LANGUAGE: bash
CODE:
dbt run --printer-width=80 
dbt test --indirect-selection=eager

----------------------------------------

TITLE: Implementing Conditional Macro Execution with project_name in dbt
DESCRIPTION: This macro demonstrates how to use the project_name context variable to conditionally execute a root-level project macro for vacuuming tables in a Redshift database. It checks if a specific macro exists in the root project and uses it if available, otherwise falling back to a default implementation.

LANGUAGE: sql
CODE:
/*
  This macro vacuums tables in a Redshift database. If a macro exists in the
  root-level project called `get_tables_to_vacuum`, this macro will call _that_
  macro to find the tables to vacuum. If the macro is not defined in the root
  project, this macro will use a default implementation instead.
*/

{% macro vacuum_tables() %}

  {% set root_project = context[project_name] %}
  {% if root_project.get_tables_to_vacuum %}
    {% set tables = root_project.get_tables_to_vacuum() %}
  {% else %}
    {% set tables = redshift.get_tables_to_vacuum() %}
  {% endif %}

  {% for table in tables %}
    {% do redshift.vacuum_table(table) %}
  {% endfor %}

{% endmacro %}

----------------------------------------

TITLE: Configuring OAuth Token-Based Authentication
DESCRIPTION: Configuration for BigQuery using OAuth token-based authentication with refresh token and client credentials.

LANGUAGE: yaml
CODE:
my-bigquery-db:
  target: dev
  outputs:
    dev:
      type: bigquery
      method: oauth-secrets
      project: GCP_PROJECT_ID
      dataset: DBT_DATASET_NAME
      threads: 4
      refresh_token: TOKEN
      client_id: CLIENT_ID
      client_secret: CLIENT_SECRET
      token_uri: REDIRECT_URI

----------------------------------------

TITLE: Checking LibYAML Installation in Python
DESCRIPTION: This command checks if LibYAML is installed in the current Python environment. LibYAML can significantly speed up YAML parsing in dbt projects.

LANGUAGE: python
CODE:
python -c "from yaml import CLoader"

----------------------------------------

TITLE: Setting Default Access in dbt_project.yml
DESCRIPTION: This code shows how to set default access levels for models in a specific subfolder using the dbt_project.yml file. It sets the group and access level for all models in the 'subfolder_name'.

LANGUAGE: yml
CODE:
models:
  my_project_name:
    subfolder_name:
      +group: my_group
      +access: private  # sets default for all models in this subfolder

----------------------------------------

TITLE: Configuring persist_docs for Seeds in dbt_project.yml
DESCRIPTION: This snippet shows how to enable persist_docs for seeds in the dbt_project.yml file, allowing documentation persistence for both relations and columns.

LANGUAGE: yml
CODE:
seeds:
  [<resource-path>]:
    +persist_docs:
      relation: true
      columns: true

----------------------------------------

TITLE: Configuring Event Time for Sources in dbt_project.yml
DESCRIPTION: Example of configuring the event_time field for a source table in the dbt_project.yml file. This is used to represent the actual timestamp of events.

LANGUAGE: yaml
CODE:
sources:
  events:
    clickstream:
      +event_time: event_timestamp

----------------------------------------

TITLE: Setting Log Format for dbt Log File
DESCRIPTION: Shows how to set the log format specifically for the log file when running dbt.

LANGUAGE: text
CODE:
dbt --log-format-file json run

----------------------------------------

TITLE: Running Tests on Sources in dbt
DESCRIPTION: These commands demonstrate how to run tests on various source configurations in dbt.

LANGUAGE: bash
CODE:
# tests on all sources
dbt test --select "source:*"

# tests on one source
dbt test --select "source:jaffle_shop"

# tests on two or more specific sources
dbt test --select "source:jaffle_shop source:raffle_bakery"

# tests on one source table
dbt test --select "source:jaffle_shop.customers"

# tests on everything _except_ sources
dbt test --exclude "source:*"

----------------------------------------

TITLE: Expanding Column Types in SQL using dbt adapter
DESCRIPTION: This snippet shows how to use the `adapter.expand_target_column_types` method to expand column types in a target relation to match the schema of a source relation.

LANGUAGE: sql
CODE:
{% set tmp_relation = adapter.get_relation(...) %}
{% set target_relation = adapter.get_relation(...) %}

{% do adapter.expand_target_column_types(tmp_relation, target_relation) %}

----------------------------------------

TITLE: Configuring batch_size in dbt_project.yml
DESCRIPTION: This example demonstrates how to set the 'batch_size' configuration to 'day' for the 'user_sessions' model in the dbt_project.yml file.

LANGUAGE: yaml
CODE:
models:
  my_project:
    user_sessions:
      +batch_size: day

----------------------------------------

TITLE: Using Snowflake Cortex for Thread Summarization
DESCRIPTION: SQL code demonstrating how to use Snowflake's cortex.complete() function to generate summaries of Slack thread content using the llama2-70b-chat model.

LANGUAGE: sql
CODE:
select trim(
    snowflake.cortex.complete(
        'llama2-70b-chat',
        concat(
            'Write a short, two sentence summary of this Slack thread. Focus on issues raised. Be brief. <thread>',
            text_to_summarize,
            '</thread>. The users involved are: <users>',
            participant_metadata.participant_users::text,
            '</users>'
        )
    )
) as thread_summary,

----------------------------------------

TITLE: Configuring Project-Level Test Severity in YAML
DESCRIPTION: This snippet shows how to set the default severity for all tests in a package or project using the dbt_project.yml file. It sets the severity to 'warn' for all tests and configures a warning threshold for tests in a specific package.

LANGUAGE: yaml
CODE:
tests:
  +severity: warn  # all tests

  <package_name>:
    +warn_if: >10 # tests in <package_name>

----------------------------------------

TITLE: Configuring Custom Database for Snapshots in YAML File
DESCRIPTION: Demonstrates an alternative method to specify a custom database for snapshots using a separate YAML file. This configuration sets the database to 'snapshots' for the named snapshot.

LANGUAGE: yaml
CODE:
version: 2

snapshots:
  - name: snapshot_name
    config:
      database: snapshots

----------------------------------------

TITLE: Configuring Singular Test Severity in SQL
DESCRIPTION: This snippet shows how to configure a singular test in dbt. It sets the error threshold using the config block at the beginning of the SQL file.

LANGUAGE: sql
CODE:
{{ config(error_if = '>50') }}

select ...

----------------------------------------

TITLE: Configuring Model-Specific Incremental Strategy in SQL
DESCRIPTION: Demonstrates how to configure incremental strategy for a specific model with additional parameters like unique_key.

LANGUAGE: sql
CODE:
{{
  config(
    materialized='incremental',
    unique_key='date_day',
    incremental_strategy='delete+insert',
    ...
  )
}}

select ...

----------------------------------------

TITLE: Configuring StarRocks Authentication in dbt Profiles
DESCRIPTION: YAML configuration for StarRocks database connection in dbt profiles.yml file using username/password authentication. Includes required fields like host, port, schema, and credentials.

LANGUAGE: yaml
CODE:
my-starrocks-db:
  target: dev
  outputs:
    dev:
      type: starrocks
      host: localhost
      port: 9030
      schema: analytics
      
      # User/password auth
      username: your_starrocks_username
      password: your_starrocks_password

----------------------------------------

TITLE: Project-Level Index Configuration in YAML
DESCRIPTION: Example of configuring index settings at the project level in dbt_project.yml. Demonstrates how to set default materialization and index types for specific model groups.

LANGUAGE: yaml
CODE:
models:
  your_project_name:
    materialized: view
    staging:
      materialized: table
      index: HEAP

----------------------------------------

TITLE: AAD Principal Auto-Provisioning Configuration
DESCRIPTION: Project configuration for automatically provisioning Microsoft Entra ID principals in the database.

LANGUAGE: yaml
CODE:
models:
  your_project_name:
    auto_provision_aad_principals: true

----------------------------------------

TITLE: Configuring Custom Delimiter for a Specific Seed
DESCRIPTION: This YAML snippet shows how to set a custom delimiter (semicolon) for a specific seed file named 'country_codes' in the seeds/properties.yml configuration file.

LANGUAGE: yaml
CODE:
version: 2

seeds:
  - name: country_codes
    config:
      delimiter: ";"

----------------------------------------

TITLE: Initial Customer Model Query
DESCRIPTION: SQL query to create the first dbt model combining customer and order data

LANGUAGE: sql
CODE:
with customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from jaffle_shop.customers

),

orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from jaffle_shop.orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final

----------------------------------------

TITLE: Enabling persist_docs Globally in dbt_project.yml
DESCRIPTION: This snippet demonstrates how to enable persist_docs globally for all models in the dbt_project.yml file, allowing documentation persistence for both relations and columns.

LANGUAGE: yml
CODE:
models:
  +persist_docs:
    relation: true
    columns: true

----------------------------------------

TITLE: Failed dbt run with syntax error
DESCRIPTION: Example showing a dbt run command that encounters a syntax error in the customers model while successfully completing other models.

LANGUAGE: shell
CODE:
Running with dbt=1.6.1
Registered adapter: duckdb=1.6.0
Found 5 models, 3 seeds, 20 tests, 0 sources, 0 exposures, 0 metrics, 348 macros, 0 groups, 0 semantic models

Concurrency: 24 threads (target='dev')
 
1 of 5 START sql view model main.stg_customers ................................. [RUN]
2 of 5 START sql view model main.stg_orders .................................... [RUN]
3 of 5 START sql view model main.stg_payments .................................. [RUN]
1 of 5 OK created sql view model main.stg_customers ............................ [OK in 0.06s]
2 of 5 OK created sql view model main.stg_orders ............................... [OK in 0.06s]
3 of 5 OK created sql view model main.stg_payments ............................. [OK in 0.07s]
4 of 5 START sql table model main.customers .................................... [RUN]
5 of 5 START sql table model main.orders ....................................... [RUN]
4 of 5 ERROR creating sql table model main.customers ........................... [ERROR in 0.03s]
5 of 5 OK created sql table model main.orders .................................. [OK in 0.04s]
 
Finished running 3 view models, 2 table models in 0 hours 0 minutes and 0.15 seconds (0.15s).
  
Completed with 1 error and 0 warnings:
  
Runtime Error in model customers (models/customers.sql)
 Parser Error: syntax error at or near "selct"

Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5

----------------------------------------

TITLE: Including Saved Queries in DBT Build
DESCRIPTION: Command to include all saved queries in the DBT build process using the --resource-type flag.

LANGUAGE: text
CODE:
dbt build --resource-type saved_query

----------------------------------------

TITLE: Running Generic Tests in dbt
DESCRIPTION: This command selects and runs only generic tests in dbt.

LANGUAGE: bash
CODE:
dbt test --select "test_type:generic"

----------------------------------------

TITLE: Basic Check Columns Configuration (DBT 1.9+)
DESCRIPTION: YAML configuration for defining check columns in a DBT snapshot using the newer 1.9+ syntax. Specifies snapshot name, source relation, and configuration including schema, unique key, and columns to check.

LANGUAGE: yaml
CODE:
snapshots:
- name: snapshot_name
  relation: source('my_source', 'my_table')
  config:
    schema: string
    unique_key: column_name_or_expression
    strategy: check
    check_cols:
      - column_name

----------------------------------------

TITLE: Checking Relation Existence in SQL using dbt adapter
DESCRIPTION: This snippet shows how to use the `load_relation` method to check if a relation exists in the database.

LANGUAGE: sql
CODE:
{% set relation_exists = load_relation(ref('my_model')) is not none %}
{% if relation_exists %}
      {{ log("my_model has already been built", info=true) }}
{% else %}
      {{ log("my_model doesn't exist in the warehouse. Maybe it was dropped?", info=true) }}
{% endif %}

----------------------------------------

TITLE: Configuring fail_calc in One-off SQL Tests
DESCRIPTION: Setting fail_calc configuration for a custom SQL test using config block.

LANGUAGE: sql
CODE:
{{ config(fail_calc = "sum(total_revenue) - sum(revenue_accounted_for)") }}

select ...

----------------------------------------

TITLE: Configuring fail_calc in One-off SQL Tests
DESCRIPTION: Setting fail_calc configuration for a custom SQL test using config block.

LANGUAGE: sql
CODE:
{{ config(fail_calc = "sum(total_revenue) - sum(revenue_accounted_for)") }}

select ...

----------------------------------------

TITLE: Product Segment Classification with LLM
DESCRIPTION: SQL and Jinja template code for classifying text content into predefined product segments using Snowflake Cortex, with error handling for non-deterministic responses.

LANGUAGE: sql
CODE:
{% set segments = ['Warehouse configuration', 'dbt Cloud IDE', 'dbt Core', 'SQL', 'dbt Orchestration', 'dbt Explorer', 'Unknown'] %}

select trim(
    snowflake.cortex.complete(
        'llama2-70b-chat',
        concat(
            'Identify the dbt product segment that this message relates to, out of [{{ segments | join ("|"}} ]. Your response should be only the segment with no explanation. <message>',
            text,
            '</message>'
        )
    )
) as product_segment_raw,

coalesce(regexp_substr(product_segment_raw, '{{ segments | join ("|"}} '), 'Unknown') as product_segment

----------------------------------------

TITLE: Defining Data Types for Postgres in dbt Unit Tests
DESCRIPTION: This snippet shows how to specify various data types including integer, float, numeric, string, boolean, date, timestamp, and JSON in a dbt unit test for Postgres. Arrays are not currently supported.

LANGUAGE: yaml
CODE:
unit_tests:
  - name: test_my_data_types
    model: fct_data_types
    given:
      - input: ref('stg_data_types')
        rows:
         - int_field: 1
           float_field: 2.0
           numeric_field: 1
           str_field: my_string
           str_escaped_field: "my,cool'string"
           bool_field: true
           date_field: 2020-01-02
           timestamp_field: 2013-11-03 00:00:00-0
           timestamptz_field: 2013-11-03 00:00:00-0
           json_field: '{"bar": "baz", "balance": 7.77, "active": false}'

----------------------------------------

TITLE: Configuring Column Types for Nested Seed Files
DESCRIPTION: Illustrates how to configure column types for seed files located in nested directories. This example shows the configuration for a seed file at 'seeds/marketing/utm_mappings.csv'.

LANGUAGE: yaml
CODE:
seeds:
  jaffle_shop:
    marketing:
      utm_mappings:
        +column_types:
          ...

----------------------------------------

TITLE: Specifying Custom Database for Snapshots in dbt
DESCRIPTION: Illustrates how to set a custom database for snapshots in the dbt_project.yml file. This configuration places the 'your_snapshot' in the 'snapshots' database rather than the default target database.

LANGUAGE: yaml
CODE:
snapshots:
  your_project:
    your_snapshot:
      +database: snapshots

----------------------------------------

TITLE: Overriding ref Function with Custom Database in dbt (Jinja)
DESCRIPTION: This macro overrides the ref method in dbt to return a Relation with the database name set to 'dev'. It handles both single and double argument calls to ref, as well as the version parameter.

LANGUAGE: jinja
CODE:
{% macro ref() %}

-- extract user-provided positional and keyword arguments
{% set version = kwargs.get('version') or kwargs.get('v') %}
{% set packagename = none %}
{%- if (varargs | length) == 1 -%}
    {% set modelname = varargs[0] %}
{%- else -%}
    {% set packagename = varargs[0] %}
    {% set modelname = varargs[1] %}
{% endif %}

-- call builtins.ref based on provided positional arguments
{% set rel = None %}
{% if packagename is not none %}
    {% set rel = builtins.ref(packagename, modelname, version=version) %}
{% else %}
    {% set rel = builtins.ref(modelname, version=version) %}
{% endif %}

-- finally, override the database name with "dev"
{% set newrel = rel.replace_path(database="dev") %}
{% do return(newrel) %}

{% endmacro %}

----------------------------------------

TITLE: Testing All dbt Sources Using Shell Command
DESCRIPTION: This command runs tests on all sources in a dbt project. It uses the '--select' option with a wildcard to target all sources.

LANGUAGE: shell
CODE:
dbt test --select "source:*"

----------------------------------------

TITLE: Defining External Sources in dbt YAML
DESCRIPTION: YAML configuration schema for defining external table sources in dbt. Includes properties for specifying external table location, format settings, partition details and additional custom properties. Used to configure source tables that exist outside the data warehouse.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: <source_name>
    tables:
      - name: <table_name>
        external:
          location: <string>
          file_format: <string>
          row_format: <string>
          tbl_properties: <string>      
          partitions:
            - name: <column_name>
              data_type: <string>
              description: <string>
              meta: {dictionary}
            - ...
          <additional_property>: <additional_value>

----------------------------------------

TITLE: Configuring Extrica Profiles in DBT
DESCRIPTION: Example configuration for dbt profiles.yml file showing both development and production environments. Includes all required connection parameters for Extrica Trino integration including JWT authentication, host configuration, and threading settings.

LANGUAGE: yaml
CODE:
<profile-name>:
  outputs:
    dev:
      type: extrica
      method: jwt 
      username: [username for jwt auth]
      password: [password for jwt auth]  
      host: [extrica hostname]
      port: [port number]
      schema: [dev_schema]
      catalog: [catalog_name]
      threads: [1 or more]

    prod:
      type: extrica
      method: jwt 
      username: [username for jwt auth]
      password: [password for jwt auth]  
      host: [extrica hostname]
      port: [port number]
      schema: [dev_schema]
      catalog: [catalog_name]
      threads: [1 or more]
  target: dev

----------------------------------------

TITLE: Configuring Index and Distribution in Model SQL
DESCRIPTION: Example of setting index and distribution types directly in a dbt model file. Shows how to configure a HEAP index with ROUND_ROBIN distribution at the model level.

LANGUAGE: sql
CODE:
{{
    config(
        index='HEAP',
        dist='ROUND_ROBIN'
        )
}}

select *
from ...

----------------------------------------

TITLE: SQL IN Operator with Subquery
DESCRIPTION: Example showing the syntax for using the IN operator with a subquery for dynamic filtering.

LANGUAGE: sql
CODE:
where status in (select ‚Ä¶)

----------------------------------------

TITLE: Filtering Customer Names with SQL LIKE
DESCRIPTION: Example query demonstrating how to use the LIKE operator to filter customer records where first names begin with the uppercase letter 'J'. Uses the Jaffle Shop's customers table as a reference.

LANGUAGE: sql
CODE:
select
    user_id,
    first_name
from {{ ref('customers') }}
where first_name like 'J%'
order by 1

----------------------------------------

TITLE: Sample Output of dbt Invocation Help Command
DESCRIPTION: Shows the expected output when running the 'dbt invocation help' command, including available commands and flags.

LANGUAGE: bash
CODE:
dbt invocation help
Manage invocations

Usage:
  dbt invocation [command]

Available Commands:
  list        List active invocations

Flags:
  -h, --help   help for invocation

Global Flags:
      --log-format LogFormat   The log format, either json or plain. (default plain)
      --log-level LogLevel     The log level, one of debug, info, warning, error or fatal. (default info)
      --no-color               Disables colorization of the output.
  -q, --quiet                  Suppress all non-error logging to stdout.

Use "dbt invocation [command] --help" for more information about a command.

----------------------------------------

TITLE: Configuring Model Groups in YAML
DESCRIPTION: Shows how to configure model groups using dbt_project.yml and schema.yml files. Groups can be assigned to control access and dependencies between different model sets.

LANGUAGE: yaml
CODE:
models:
  [<resource-path>]:
    +group: GROUP_NAME

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: MODEL_NAME
    group: GROUP

----------------------------------------

TITLE: Basic SQL Server Authentication Profile
DESCRIPTION: Basic dbt profile configuration for SQL Server using standard username/password authentication.

LANGUAGE: yaml
CODE:
your_profile_name:
  target: dev
  outputs:
    dev:
      type: sqlserver
      driver: 'ODBC Driver 18 for SQL Server'
      server: hostname or IP of your server
      port: 1433
      database: database
      schema: schema_name
      user: username
      password: password

----------------------------------------

TITLE: SQL Generation for Ratio Metrics in dbt
DESCRIPTION: This SQL snippet demonstrates how dbt generates SQL for ratio metrics using different semantic models. It shows the computation of numerator and denominator in sub-queries and their joining based on common dimensions to calculate the final ratio.

LANGUAGE: sql
CODE:
select
  subq_15577.metric_time as metric_time,
  cast(subq_15577.mql_queries_created_test as double) / cast(nullif(subq_15582.distinct_query_users, 0) as double) as mql_queries_per_active_user
from (
  select
    metric_time,
    sum(mql_queries_created_test) as mql_queries_created_test
  from (
    select
      cast(query_created_at as date) as metric_time,
      case when query_status in ('PENDING','MODE') then 1 else 0 end as mql_queries_created_test
    from prod_dbt.mql_query_base mql_queries_test_src_2552 
  ) subq_15576
  group by
    metric_time
) subq_15577
inner join (
  select
    metric_time,
    count(distinct distinct_query_users) as distinct_query_users
  from (
    select
      cast(query_created_at as date) as metric_time,
      case when query_status in ('MODE','PENDING') then email else null end as distinct_query_users
    from prod_dbt.mql_query_base mql_queries_src_2585 
  ) subq_15581
  group by
    metric_time
) subq_15582
on
  (
    (
      subq_15577.metric_time = subq_15582.metric_time
    ) or (
      (
        subq_15577.metric_time is null
      ) and (
        subq_15582.metric_time is null
      )
    )
  )

----------------------------------------

TITLE: Creating Staging Model for Coalesced Updated_at (dbt 1.9+)
DESCRIPTION: Example of creating a staging model to coalesce updated_at and created_at columns for use in snapshot configuration for dbt version 1.9 and above.

LANGUAGE: sql
CODE:
select  * coalesce (updated_at, created_at) as updated_at_for_snapshot
from {{ source('jaffle_shop', 'orders') }}

----------------------------------------

TITLE: Computing Monthly Maximum Order Amounts with SQL MAX
DESCRIPTION: Example query demonstrating how to use MAX function with GROUP BY to calculate maximum order amounts per month from a Jaffle Shop orders table. The query extracts the month from order_date and groups results accordingly.

LANGUAGE: sql
CODE:
select
	date_part('month', order_date) as order_month,
	max(amount) as max_amaount
from {{ ref('orders') }}
group by 1

----------------------------------------

TITLE: Prepending Dynamic Comment in dbt YAML
DESCRIPTION: Illustrates how to use Jinja templating to create a dynamic comment based on the configured user in the active dbt target.

LANGUAGE: yaml
CODE:
query-comment: "run by {{ target.user }} in dbt"

----------------------------------------

TITLE: Synchronous Client Usage Example
DESCRIPTION: Example showing how to initialize and use the synchronous SemanticLayerClient to query metrics.

LANGUAGE: python
CODE:
from dbtsl import SemanticLayerClient

client = SemanticLayerClient(
    environment_id=123,
    auth_token="<your-semantic-layer-api-token>",
    host="semantic-layer.cloud.getdbt.com",
)

# query the first metric by `metric_time`
def main():
    with client.session():
        metrics = client.metrics()
        table = client.query(
            metrics=[metrics[0].name],
            group_by=["metric_time"],
        )
        print(table)

main()

----------------------------------------

TITLE: Help Command Output Example
DESCRIPTION: Sample output of the help command showing available commands and usage information for the dbt environment command.

LANGUAGE: bash
CODE:
‚ùØ dbt help environment
Interact with dbt environments

Usage:
  dbt environment [command]

Aliases:
  environment, env

Available Commands:
  show        Show the working environment

Flags:
  -h, --help   help for environment

Use "dbt environment [command] --help" for more information about a command.

----------------------------------------

TITLE: Configuring Project-Wide and Seed-Specific Delimiters in dbt_project.yml
DESCRIPTION: This YAML snippet demonstrates setting a default delimiter for all seeds in a project and overriding it for a specific seed file within the dbt_project.yml file.

LANGUAGE: yaml
CODE:
seeds:
  jaffle_shop: 
    +delimiter: "|" # default delimiter for seeds in jaffle_shop project will be "|"
    seed_a:
      +delimiter: "," # delimiter for seed_a will be ","

----------------------------------------

TITLE: Analyzing Customer Orders in Databricks
DESCRIPTION: This SQL query joins customer and order data in Databricks to analyze customer order history. It calculates the first and most recent order dates, as well as the total number of orders for each customer.

LANGUAGE: sql
CODE:
with customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from jaffle_shop_customers

),

orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from jaffle_shop_orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final

----------------------------------------

TITLE: Quoting Identifiers in SQL using dbt adapter
DESCRIPTION: This snippet shows how to use the `adapter.quote` method to properly quote identifiers for the current database adapter.

LANGUAGE: sql
CODE:
select 
      'abc' as {{ adapter.quote('table_name') }},
      'def' as {{ adapter.quote('group by') }} 

----------------------------------------

TITLE: Schema Permission Grant for Test Failure Storage
DESCRIPTION: SQL command to resolve permission issues by creating and granting access to the test failure storage schema.

LANGUAGE: sql
CODE:
create schema if not exists dev_username_dbt_test__audit authorization username;

----------------------------------------

TITLE: Querying Test Information with GraphQL
DESCRIPTION: Example GraphQL query to retrieve test information including run ID, account details, and test state. The query demonstrates how to fetch test results which can be 'error', 'fail', 'warn', or 'pass'.

LANGUAGE: graphql
CODE:
{
  job(id: 123) {
    test(uniqueId: "test.internal_analytics.not_null_metrics_id") {
      runId
      accountId
      projectId
      uniqueId
      name
      columnName
      state
    }
  }
}

----------------------------------------

TITLE: Configuring Full Refresh for Seeds in dbt_project.yml
DESCRIPTION: YAML configuration to set the full_refresh option for seed files in the dbt_project.yml file. Controls whether seeds always or never perform full refreshes.

LANGUAGE: yml
CODE:
seeds:
  [<resource-path>]:
    +full_refresh: false | true

----------------------------------------

TITLE: Configuring batch_size in SQL model config block
DESCRIPTION: This example illustrates how to set the 'batch_size' configuration to 'day' for a model directly in the SQL file using a config block.

LANGUAGE: sql
CODE:
{{ config(
    materialized='incremental',
    batch_size='day'
) }}

----------------------------------------

TITLE: Triggering dbt Cloud Job with Airflow DbtCloudRunJobOperator
DESCRIPTION: Example of using the DbtCloudRunJobOperator in an Airflow DAG to trigger a dbt Cloud job. This snippet demonstrates how to integrate dbt Cloud with Apache Airflow for orchestration.

LANGUAGE: python
CODE:
# No actual code provided, but the image shows usage of DbtCloudRunJobOperator

----------------------------------------

TITLE: Basic Source Override Structure in dbt YAML
DESCRIPTION: Basic structure for overriding a source configuration in dbt. Shows the fundamental syntax for specifying source overrides with database and schema properties.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: <source_name>
    overrides: <package name>

    database: ...
    schema: ...

----------------------------------------

TITLE: Verify dbt Installation
DESCRIPTION: Command to check if dbt is already installed on the system

LANGUAGE: bash
CODE:
which dbt

----------------------------------------

TITLE: Configuring Table Materialization in dbt Project YAML
DESCRIPTION: Illustrates the configuration of a ClickHouse table materialization in the dbt_project.yml file, including engine, order_by, and partition_by options.

LANGUAGE: yaml
CODE:
models:
  <resource-path>:
    +materialized: table
    +order_by: [ <column-name>, ... ]
    +engine: <engine-type>
    +partition_by: [ <column-name>, ... ]

----------------------------------------

TITLE: Staging Customer Model
DESCRIPTION: SQL query for the staging customers model that extracts and transforms customer data.

LANGUAGE: sql
CODE:
select
    id as customer_id,
    first_name,
    last_name

from jaffle_shop_customers

----------------------------------------

TITLE: Setting Custom Schema for Seeds in dbt
DESCRIPTION: Configuration for placing seeds in a custom 'mappings' schema using dbt_project.yml

LANGUAGE: yaml
CODE:
seeds:
  your_project:
    product_mappings:
      +schema: mappings

----------------------------------------

TITLE: Semantic Models and Metrics YAML Configuration
DESCRIPTION: YAML configuration showing how to define semantic models, measures, and metrics with the Metric() object for filtering activated accounts.

LANGUAGE: yaml
CODE:
semantic_models:
    - name: model_runs
      ... # Placeholder for other configurations
      entities:
        - name: model_run
          type: primary
        - name: account
          type: foreign
      measures:
        - name: data_model_runs
          agg: sum
          expr: 1
          create_metric: true

    - name: accounts
      ... # Placeholder for other configurations
      entities:
        - name: account
          type: primary
      measures:
        - name: accounts
          agg: sum
          expr: 1
          create_metric: true
  metrics:
    - name: activated_accounts
      label: Activated Accounts
      type: simple
      type_params:
        measure: accounts
      filter: |
        {{ Metric('data_model_runs', group_by=['account']) }} > 5

----------------------------------------

TITLE: Disabling Query Comments in dbt YAML
DESCRIPTION: Shows two ways to disable query comments in the dbt_project.yml file.

LANGUAGE: yaml
CODE:
query-comment:

LANGUAGE: yaml
CODE:
query-comment: null

----------------------------------------

TITLE: Source Freshness Output in JSON for dbt
DESCRIPTION: This JSON structure represents the output of a dbt source freshness check. It includes metadata about the check execution and detailed freshness information for each source table, including timestamps and pass/fail status.

LANGUAGE: json
CODE:
{
    "meta": {
        "generated_at": "2019-02-15T00:53:03.971126Z",
        "elapsed_time": 0.21452808380126953
    },
    "sources": {
        "source.project_name.source_name.table_name": {
            "max_loaded_at": "2019-02-15T00:45:13.572836+00:00Z",
            "snapshotted_at": "2019-02-15T00:53:03.880509+00:00Z",
            "max_loaded_at_time_ago_in_s": 481.307673,
            "state": "pass",
            "criteria": {
                "warn_after": {
                    "count": 12,
                    "period": "hour"
                },
                "error_after": {
                    "count": 1,
                    "period": "day"
                }
            }
        }
    }
}

----------------------------------------

TITLE: Creating BigQuery Extended Attributes via API
DESCRIPTION: API request to create extended attributes for BigQuery configuration in dbt Cloud.

LANGUAGE: shell
CODE:
curl --request POST \
--url https://cloud.getdbt.com/api/v3/accounts/XXXXX/projects/YYYYY/extended-attributes/ \
--header 'Accept: application/json' \
--header 'Authorization: Bearer ZZZZZ' \
--header 'Content-Type: application/json' \
--data '{
"id": null,
"extended_attributes": {"type":"service_account","project_id":"xxx","private_key_id":"xxx","private_key":"{{ env_var('DBT_ENV_SECRET_PROJECTXXX_PRIVATE_KEY')    }}","client_email":"xxx","client_id":xxx,"auth_uri":"https://accounts.google.com/o/oauth2/auth","token_uri":"https://oauth2.googleapis.com/token","auth_provider_x509_cert_url":"https://www.googleapis.com/oauth2/v1/certs","client_x509_cert_url":"xxx"},
"state": 1
}'

----------------------------------------

TITLE: Basic Date Formatting with run_started_at in SQL
DESCRIPTION: Demonstrates how to format the run_started_at timestamp to extract just the date portion in YYYY-MM-DD format using strftime.

LANGUAGE: sql
CODE:
select
	'{{ run_started_at.strftime("%Y-%m-%d") }}' as date_day

from ...

----------------------------------------

TITLE: Configuring External Table in dbt_project.yml for Firebolt
DESCRIPTION: This snippet shows how to configure an external table in the dbt_project.yml file for Firebolt, including specifying the S3 location, object pattern, and column definitions.

LANGUAGE: yaml
CODE:
sources:
  - name: firebolt_external
    schema: "{{ target.schema }}"
    loader: S3

    tables:
      - name: <table-name>
        external:
          url: 's3://<bucket_name>/'
          object_pattern: '<regex>'
          type: '<type>'
          credentials:
            aws_key_id: <key-id>
            aws_secret_key: <key-secret>
          object_pattern: '<regex>'
          compression: '<compression-type>'
          partitions:
            - name: <partition-name>
              data_type: <partition-type>
              regex: '<partition-definition-regex>'
          columns:
            - name: <column-name>
              data_type: <type>

----------------------------------------

TITLE: Querying Model Timing Information in GraphQL for dbt
DESCRIPTION: This GraphQL query retrieves execution timing information for a specific model within a dbt job. It includes details such as runId, projectId, name, uniqueId, resourceType, executeStartedAt, executeCompletedAt, and executionTime.

LANGUAGE: graphql
CODE:
{
  job(id: 123) {
    model(uniqueId: "model.jaffle_shop.dim_user") {
      runId
      projectId
      name
      uniqueId
      resourceType
      executeStartedAt
      executeCompletedAt
      executionTime
    }
  }
}

----------------------------------------

TITLE: Implementing is_even Generic Test in SQL
DESCRIPTION: Defines a custom generic test that checks if values in a specified column are even numbers. The test accepts standard model and column_name arguments.

LANGUAGE: sql
CODE:
{% test is_even(model, column_name) %}

with validation as (

    select
        {{ column_name }} as even_field

    from {{ model }}

),

validation_errors as (

    select
        even_field

    from validation
    -- if this is true, then even_field is actually odd!
    where (even_field % 2) = 1

)

select *
from validation_errors

{% endtest %}

----------------------------------------

TITLE: Configuring meta for saved queries in dbt_project.yml
DESCRIPTION: Demonstrates how to set meta properties for saved queries in the dbt_project.yml file. Meta is defined as a dictionary under the saved-queries configuration block.

LANGUAGE: yaml
CODE:
saved-queries:
  [<resource-path>]:
    +meta: {<dictionary>}

----------------------------------------

TITLE: Setting Primary and Shard Keys in SingleStore
DESCRIPTION: Shows configuration of primary and shard keys for a SingleStore table using dbt config blocks.

LANGUAGE: sql
CODE:
{{
    config(
        primary_key=['id', 'user_id'],
        shard_key=['id']
    )
}}

select ...

----------------------------------------

TITLE: Preserving Leading Zeros in Zipcode Using varchar
DESCRIPTION: Demonstrates how to use the column_types configuration to preserve leading zeros in a zipcode column by specifying it as varchar(5). This example is specifically for dbt version 0.16.0 and onwards.

LANGUAGE: yaml
CODE:
seeds:
  jaffle_shop: # you must include the project name
    warehouse_locations:
      +column_types:
        zipcode: varchar(5)

----------------------------------------

TITLE: Incorrect Absolute Path Configuration
DESCRIPTION: Example of discouraged absolute path usage for snapshot directory configuration. This approach should be avoided as it reduces project portability.

LANGUAGE: yml
CODE:
snapshot-paths: ["/Users/username/project/snapshots"]

----------------------------------------

TITLE: Failed dbt retry without fixing errors
DESCRIPTION: Example showing a dbt retry execution without fixing the previous syntax error, resulting in the same failure.

LANGUAGE: shell
CODE:
Running with dbt=1.6.1
Registered adapter: duckdb=1.6.0
Found 5 models, 3 seeds, 20 tests, 0 sources, 0 exposures, 0 metrics, 348 macros, 0 groups, 0 semantic models

Concurrency: 24 threads (target='dev')

1 of 1 START sql table model main.customers .................................... [RUN]
1 of 1 ERROR creating sql table model main.customers ........................... [ERROR in 0.03s]

Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5

----------------------------------------

TITLE: Querying Definition and Applied States using GraphQL
DESCRIPTION: A GraphQL query that demonstrates how to compare the definition and applied states of a model using the Discovery API. The query retrieves model names, raw code, and execution info for both states.

LANGUAGE: graphql
CODE:
query Compare($environmentId: Int!, $first: Int!) {
	environment(id: $environmentId) {
		definition {
			models(first: $first) {
				edges {
					node {
						name
						rawCode
					}
				}
			}
		}
		applied {
			models(first: $first) {
				edges {
					node {
						name
						rawCode 
						executionInfo {
							executeCompletedAt
						}
					}
				}
			}
		}
	}
}

----------------------------------------

TITLE: Configuring Model Groups in SQL
DESCRIPTION: Demonstrates how to set group configuration directly in SQL model files using config blocks.

LANGUAGE: sql
CODE:
{{ config(
  group='GROUP_NAME'
) }}

select ...

----------------------------------------

TITLE: Configuring Table Materialization in DBT Model
DESCRIPTION: Example of configuring table materialization in a dbt model file using the config block.

LANGUAGE: sql
CODE:
{{
    config(
        materialized='table'
        )
}}

select *
from ...

----------------------------------------

TITLE: Status method response example
DESCRIPTION: Example JSON response for the status method, showing server status, logs, timestamp, and process ID.

LANGUAGE: json
CODE:
{
    "result": {
        "status": "ready",
        "error": null,
        "logs": [..],
        "timestamp": "2019-10-07T16:30:09.875534Z",
        "pid": 76715
    },
    "id": "2db9a2fe-9a39-41ef-828c-25e04dd6b07d",
    "jsonrpc": "2.0"
}

----------------------------------------

TITLE: Configuring profiles.yml for dbt Core (v1.8+)
DESCRIPTION: This YAML snippet shows the structure of a profiles.yml file for dbt Core version 1.8 and later. Global configurations are removed, and the focus is on profile settings and connection details for data platforms.

LANGUAGE: yaml
CODE:
<profile-name>:
  target: <target-name> # this is the default target
  outputs:
    <target-name>:
      type: <bigquery | postgres | redshift | snowflake | other>
      schema: <schema_identifier>
      threads: <natural_number>

      ### database-specific connection details
      ...

    <target-name>: # additional targets
      ...

<profile-name>: # additional profiles
  ...

----------------------------------------

TITLE: Collecting Statistics on Teradata Tables using dbt Post-Hooks
DESCRIPTION: Configures a post-hook to collect statistics on specified columns after table creation or significant modification.

LANGUAGE: yaml
CODE:
{{ config(
  post_hook=[
    "COLLECT STATISTICS ON  {{ this }} COLUMN (column_1,  column_2  ...);"
    ]
)}}

----------------------------------------

TITLE: Configuring Multiple Overrides in dbt Unit Test
DESCRIPTION: Example showing how to override macros, variables, and environment variables in a dbt unit test. Demonstrates setting up test inputs with mock data and defining expected outputs.

LANGUAGE: yaml
CODE:
 - name: test_my_model_overrides
    model: my_model
    given:
      - input: ref('my_model_a')
        rows:
          - {id: 1, a: 1}
      - input: ref('my_model_b')
        rows:
          - {id: 1, b: 2}
          - {id: 2, b: 2}
    overrides:
      macros:
        type_numeric: override
        invocation_id: 123
      vars:
        my_test: var_override
      env_vars:
        MY_TEST: env_var_override
    expect:
      rows:
        - {macro_call: override, var_call: var_override, env_var_call: env_var_override, invocation_id: 123}

----------------------------------------

TITLE: Configuring Custom Generic Test Severity in SQL
DESCRIPTION: This snippet demonstrates how to set the default severity for all instances of a custom generic test in dbt. It uses the config block inside the test definition to set the severity to 'warn'.

LANGUAGE: sql
CODE:
{% test <testname>(model, column_name) %}

{{ config(severity = 'warn') }}

select ...

{% endtest %}

----------------------------------------

TITLE: Schema.yml V1 to V2 Migration - Custom Test Example
DESCRIPTION: Shows how to migrate custom schema tests from v1 to v2 syntax, specifically using a dbt_utils recency test as an example.

LANGUAGE: yaml
CODE:
events:
  constraints:
    dbt_utils.recency:
            - {field: created_at, datepart: day, interval: 1}

LANGUAGE: yaml
CODE:
models:
  - name: events
    tests:
      - dbt_utils.recency:
          field: created_at
          datepart: day
          interval: 1

----------------------------------------

TITLE: Selecting Models by Package in dbt
DESCRIPTION: Examples of using the package method to select models from specific packages or the current project.

LANGUAGE: bash
CODE:
# These three selectors are equivalent
dbt run --select "package:snowplow"
dbt run --select "snowplow"
dbt run --select "snowplow.*"

----------------------------------------

TITLE: Basic Source Quoting Configuration in YAML
DESCRIPTION: Demonstrates the basic structure for configuring quoting behavior in dbt source definitions. Shows how to set quoting options at both source and table levels using boolean flags.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: jaffle_shop
    quoting:
      database: true | false
      schema: true | false
      identifier: true | false
    tables:
      - name: orders
        quoting:
          database: true | false
          schema: true | false
          identifier: true | false

----------------------------------------

TITLE: Configuring Unlogged Tables in SQL for dbt-Postgres
DESCRIPTION: This snippet demonstrates how to configure an unlogged table in dbt using SQL. Unlogged tables can be faster but less safe than ordinary tables.

LANGUAGE: sql
CODE:
{{ config(materialized='table', unlogged=True) }}

select ...

----------------------------------------

TITLE: Configuring Doris Connection Profile in dbt
DESCRIPTION: YAML configuration for setting up a Doris database connection in dbt's profiles.yml file. Includes essential connection parameters like host, port, schema, username, and password.

LANGUAGE: yaml
CODE:
dbt-doris:
  target: dev
  outputs:
    dev:
      type: doris
      host: 127.0.0.1
      port: 9030
      schema: database_name
      username: username
      password: password

----------------------------------------

TITLE: Configuring Expectations/Constraints in dbt
DESCRIPTION: Shows how to implement data quality conditions using dbt constraints for Upsolver expectations.

LANGUAGE: yaml
CODE:
models:
  - name: <model name>
    # required
    config:
      contract:
        enforced: true
    # model-level constraints
    constraints:
      - type: check
        columns: ['<column1>', '<column2>']
        expression: "column1 <= column2"
        name: <constraint_name>
      - type: not_null
        columns: ['column1', 'column2']
        name: <constraint_name>

    columns:
      - name: <column3>
        data_type: string

        # column-level constraints
        constraints:
          - type: not_null
          - type: check
            expression: "REGEXP_LIKE(<column3>, '^[0-9]{4}[a-z]{5}$')"
            name: <constraint_name>

----------------------------------------

TITLE: Creating Staging Models
DESCRIPTION: SQL queries that define staging models for customers and orders, serving as intermediate tables for the final customer model.

LANGUAGE: sql
CODE:
select
    id as customer_id,
    first_name,
    last_name
from `dbt-tutorial`.jaffle_shop.customers

LANGUAGE: sql
CODE:
select
    id as order_id,
    user_id as customer_id,
    order_date,
    status
from `dbt-tutorial`.jaffle_shop.orders

----------------------------------------

TITLE: Slim CI Pipeline Configuration
DESCRIPTION: YAML configuration for the CI pipeline that performs state-based testing of changes.

LANGUAGE: yaml
CODE:
pipelines:
  pull-requests:
    '**':
      - step:
          name: Set up and build
          caches:
            - pip
          script:
            - python -m pip install -r requirements.txt
            - mkdir ~/.dbt
            - cp .ci/profiles.yml ~/.dbt/profiles.yml
            - dbt deps
            - >-
              export API_ROOT="https://api.bitbucket.org/2.0/repositories/$BITBUCKET_REPO_FULL_NAME/downloads";
              mkdir target-deferred/;
              for file in manifest.json run_results.json; do
                curl -s -L --request GET \
                  -u "$BITBUCKET_USERNAME:$BITBUCKET_APP_PASSWORD" \
                  --url "$API_ROOT/$file" \
                  --fail --output target-deferred/$file;
              done || true
            - >-
              if [ -f target-deferred/manifest.json ]; then
                export DBT_FLAGS="--defer --state target-deferred/ --select +state:modified";
              else
                export DBT_FLAGS="";
              fi
            - dbt seed
            - dbt run $DBT_FLAGS
            - dbt test $DBT_FLAGS

----------------------------------------

TITLE: Querying Environment with BigInt ID in GraphQL for dbt
DESCRIPTION: This query demonstrates the use of BigInt for the environment ID, which replaces the deprecated Int type. It retrieves the uniqueId and last run ID for models in the specified environment.

LANGUAGE: graphql
CODE:
query ($environmentId: BigInt!, $first: Int!) {
  environment(id: $environmentId) {
    applied {
      models(first: $first) {
        edges {
          node {
            uniqueId
            executionInfo {
              lastRunId
            }
          }
        }
      }
    }
  }
}


----------------------------------------

TITLE: Configuring meta for snapshots in dbt_project.yml
DESCRIPTION: Demonstrates setting meta properties for snapshots in the dbt_project.yml file. Meta is defined as a dictionary under the snapshots configuration block.

LANGUAGE: yaml
CODE:
snapshots:
  [<resource-path>]:
    +meta: {<dictionary>}

----------------------------------------

TITLE: Configuring Fact Table Model in properties.yml for Firebolt
DESCRIPTION: This snippet shows how to configure a fact table model in the properties.yml file, including setting the table type, primary index, and aggregating indexes.

LANGUAGE: yaml
CODE:
models:
  - name: <model-name>
    config:
      materialized: table
      table_type: fact
      primary_index: [ <column-name>, ... ]
      indexes:
        - index_type: aggregating
          key_columns: [ <column-name>, ... ]
          aggregation: [ <agg-sql>, ... ]
        ...

----------------------------------------

TITLE: Using env_var with Default Value in dbt_project.yml
DESCRIPTION: Shows how to use the env_var function with a default value in a dbt_project.yml file to set model materialization.

LANGUAGE: yaml
CODE:
...
models:
  jaffle_shop:
    +materialized: "{{ env_var('DBT_MATERIALIZATION', 'view') }}"

----------------------------------------

TITLE: Viewing model contents in dbt Cloud IDE
DESCRIPTION: This snippet demonstrates how to print the full contents of the 'model' object when using the dbt Cloud IDE, using the tojson filter for formatting.

LANGUAGE: jinja
CODE:
{{ model | tojson(indent = 4) }}

----------------------------------------

TITLE: Setting Default Store Failures for Generic Test Blocks
DESCRIPTION: Demonstrates how to set the default store_failures configuration for all instances of a generic (schema) test within its test block definition.

LANGUAGE: sql
CODE:
{% test <testname>(model, column_name) %}

{{ config(store_failures = false) }}

select ...

{% endtest %}

----------------------------------------

TITLE: Table Optimization with ORDER BY
DESCRIPTION: Example of using the order_by configuration parameter to optimize table storage with Vertica's ORDER BY clause.

LANGUAGE: sql
CODE:
{{ config(  materialized='table',  order_by='product_key') }} 

select * from public.product_dimension

----------------------------------------

TITLE: Configuring dbt Profiles for Dremio Cloud
DESCRIPTION: YAML configuration for the profiles.yml file to set up the connection between dbt and Dremio Cloud.

LANGUAGE: yaml
CODE:
dremioSamples:
  outputs:
    cloud_dev:
      dremio_space: dev
      dremio_space_folder: no_schema
      object_storage_path: dev
      object_storage_source: $scratch
      pat: <this_is_the_personal_access_token>
      cloud_host: api.dremio.cloud
      cloud_project_id: <id_of_project_you_belong_to>
      threads: 1
      type: dremio
      use_ssl: true
      user: <your_username>
  target: dev

----------------------------------------

TITLE: Basic Schema Source Definition in dbt YAML
DESCRIPTION: Basic structure for defining a source with schema property in dbt. Shows the standard format for specifying database, schema, and table configurations in the source definition.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: <source_name>
    database: <database_name>
    schema: <schema_name>
    tables:
      - name: <table_name>
      - ...

----------------------------------------

TITLE: Counting All vs Distinct Customer Orders
DESCRIPTION: Example query comparing total order count against distinct customer count using COUNT and COUNT DISTINCT functions in dbt's Jaffle Shop orders table.

LANGUAGE: sql
CODE:
select
	count(customer_id) as cnt_all_orders,
	count(distinct customer_id) as cnt_distinct_customers
from {{ ref('orders') }}

----------------------------------------

TITLE: Compiled SQL with Applied Quoting
DESCRIPTION: Shows the final compiled SQL output after dbt applies the configured quoting rules to source references.

LANGUAGE: sql
CODE:
select
  ...

-- this should be quoted
from "raw"."jaffle_shop"."orders"

-- here, the identifier should be unquoted
left join "raw"."jaffle_shop".customers using (order_id)

----------------------------------------

TITLE: Query Tag Configuration
DESCRIPTION: Example of setting Snowflake query tags via dbt configuration

LANGUAGE: yaml
CODE:
models:
  <resource-path>:
    +query_tag: dbt_special

----------------------------------------

TITLE: Running Core dbt Commands in Shell
DESCRIPTION: This snippet showcases the primary dbt commands used for different stages of data transformation. Each command serves a specific purpose in the dbt workflow, from building and compiling models to generating documentation and running tests.

LANGUAGE: shell
CODE:
dbt build

LANGUAGE: shell
CODE:
dbt compile

LANGUAGE: shell
CODE:
dbt docs generate

LANGUAGE: shell
CODE:
dbt run

LANGUAGE: shell
CODE:
dbt seed

LANGUAGE: shell
CODE:
dbt snapshot

LANGUAGE: shell
CODE:
dbt test

----------------------------------------

TITLE: Configuring Dremio Software Profile with PAT in YAML
DESCRIPTION: YAML configuration template for setting up a Dremio Software profile using Personal Access Token (PAT) authentication. Includes connection settings and storage configuration.

LANGUAGE: yaml
CODE:
[project name]:
  outputs:
    dev:
      pat: [personal access token]
      port: [port]
      software_host: [hostname or IP address]
      object_storage_source: [name
      object_storage_path: [path]
      dremio_space: [name]
      dremio_space_folder: [path]
      threads: [integer >= 1]
      type: dremio
      use_ssl: [true|false]
      user: [username]
  target: dev

----------------------------------------

TITLE: Creating BigQuery Environment Variable via API
DESCRIPTION: API request to create a secret environment variable for storing the BigQuery private key in dbt Cloud.

LANGUAGE: shell
CODE:
curl --request POST \
--url https://cloud.getdbt.com/api/v3/accounts/XXXXX/projects/YYYYY/environment-variables/bulk/ \
--header 'Accept: application/json' \
--header 'Authorization: Bearer ZZZZZ' \
--header 'Content-Type: application/json' \
--data '{
"env_var": [
{
    "new_name": "DBT_ENV_SECRET_PROJECTXXX_PRIVATE_KEY",
    "project": "Value by default for the entire project",
    "ENVIRONMENT_NAME_1": "Optional, if wanted, value for environment name 1",
    "ENVIRONMENT_NAME_2": "Optional, if wanted, value for environment name 2"
}
]
}'

----------------------------------------

TITLE: Configuring RisingWave Profile in dbt
DESCRIPTION: YAML configuration for setting up a RisingWave connection profile in dbt's profiles.yml file. Includes connection parameters like host, user, password, database name, port and schema.

LANGUAGE: yaml
CODE:
default:
  outputs:
    dev:
      type: risingwave
      host: [host name] 
      user: [user name]
      pass: [password]
      dbname: [database name]
      port: [port]
      schema: [dbt schema]
  target: dev

----------------------------------------

TITLE: Configuring Z-Order in dbt model
DESCRIPTION: This snippet shows how to configure Z-Order for a dbt model using the 'config' block. Z-Order is used to improve query performance by co-locating related data.

LANGUAGE: sql
CODE:
config(

materialized='incremental',

zorder="column_A" | ["column_A", "column_B"]

)

----------------------------------------

TITLE: Setting Model Configuration
DESCRIPTION: Example of setting model configuration parameters including materialization type and unique key.

LANGUAGE: jinja
CODE:
{{
  config(
    materialized='incremental',
    unique_key='id'
  )
}}

----------------------------------------

TITLE: Configuring Snapshot Meta Column Names in Project YAML
DESCRIPTION: This snippet shows how to set custom names for snapshot metadata columns in the dbt_project.yml file. It allows for project-wide configuration of column names for snapshots.

LANGUAGE: yml
CODE:
snapshots:
  [<resource-path>]:
    +snapshot_meta_column_names:
      dbt_valid_from: <string>
      dbt_valid_to: <string>
      dbt_scd_id: <string>
      dbt_updated_at: <string>
      dbt_is_deleted: <string>

----------------------------------------

TITLE: Implementing ConnectionManager open Method for dbt Adapter
DESCRIPTION: This code snippet demonstrates how to implement the open method for a ConnectionManager class in a dbt adapter.

LANGUAGE: Python
CODE:
@classmethod
def open(cls, connection):
    if connection.state == 'open':
        logger.debug('Connection is already open, skipping open.')
        return connection

    credentials = connection.credentials

    try:
        handle = myadapter_library.connect(
            host=credentials.host,
            port=credentials.port,
            username=credentials.username,
            password=credentials.password,
            catalog=credentials.database
        )
        connection.state = 'open'
        connection.handle = handle
    return connection

----------------------------------------

TITLE: Configuring Store Failures for Singular SQL Tests
DESCRIPTION: Shows how to configure store_failures for a singular (data) test using SQL configuration. Sets the store_failures parameter to true for the specific test.

LANGUAGE: sql
CODE:
{{ config(store_failures = true) }}

select ...

----------------------------------------

TITLE: Default Quoting Configuration for Most Adapters
DESCRIPTION: This YAML snippet shows the default quoting configuration for most adapters in dbt, where all settings are set to true. This allows the use of reserved words and special characters in identifiers.

LANGUAGE: yaml
CODE:
quoting:
  database: true
  schema: true
  identifier: true

----------------------------------------

TITLE: Configuring docs-paths in dbt_project.yml
DESCRIPTION: Specifies the docs-paths configuration in the dbt_project.yml file. This setting allows you to define custom directories where docs blocks are located.

LANGUAGE: yaml
CODE:
docs-paths: [directorypath]

----------------------------------------

TITLE: Accessing Sources in DBT Graph
DESCRIPTION: Example showing how to union all Snowplow sources that begin with 'event_' using the graph.sources attribute.

LANGUAGE: sql
CODE:
{% set sources = [] -%}
{% for node in graph.sources.values() -%}
  {%- if node.name.startswith('event_') and node.source_name == 'snowplow' -%}
    {%- do sources.append(source(node.source_name, node.name)) -%}
  {%- endif -%}
{%- endfor %}

select * from (
  {%- for source in sources %}
    select * from {{ source }} {% if not loop.last %} union all {% endif %}
  {% endfor %}
)

----------------------------------------

TITLE: Creating Snowflake Warehouse
DESCRIPTION: SQL code to create a new warehouse in Snowflake with XSmall size

LANGUAGE: sql
CODE:
create or replace warehouse COMPUTE_WH with warehouse_size=XSMALL

----------------------------------------

TITLE: Configuring Service Account JSON Authentication
DESCRIPTION: Configuration for authenticating with BigQuery using service account JSON credentials directly in the profiles file.

LANGUAGE: yaml
CODE:
my-bigquery-db:
  target: dev
  outputs:
    dev:
      type: bigquery
      method: service-account-json
      project: GCP_PROJECT_ID
      dataset: DBT_DATASET_NAME
      threads: 4
      keyfile_json:
        type: xxx
        project_id: xxx
        private_key_id: xxx
        private_key: xxx
        client_email: xxx
        client_id: xxx
        auth_uri: xxx
        token_uri: xxx
        auth_provider_x509_cert_url: xxx
        client_x509_cert_url: xxx

----------------------------------------

TITLE: Configuring Grants for Teradata Models in dbt
DESCRIPTION: Defines access grants for dbt models, specifying which users have select or insert permissions on the generated datasets.

LANGUAGE: yaml
CODE:
models:
  - name: model_name
    config:
      materialized: table
      grants:
        select: ["user_b"]
        insert: ["user_c"]

----------------------------------------

TITLE: Basic SQL Model Reference Example
DESCRIPTION: Demonstrates how a dbt ref() function compiles differently in development vs production environments

LANGUAGE: sql
CODE:
-- in models/my_model.sql
select * from {{ ref('model_a') }}

LANGUAGE: sql
CODE:
-- in target/compiled/models/my_model.sql
select * from analytics.dbt_dconnors.model_a

LANGUAGE: sql
CODE:
-- in target/compiled/models/my_model.sql
select * from analytics.analytics.model_a

----------------------------------------

TITLE: Poll method response example
DESCRIPTION: Example JSON response for the poll method, showing task results, elapsed time, logs, and status.

LANGUAGE: json
CODE:
{
    "result": {
        "results": [],
        "generated_at": "2019-10-11T18:25:22.477203Z",
        "elapsed_time": 0.8381369113922119,
        "logs": [],
        "tags": {
            "command": "run --select my_model",
            "branch": "abc123"
        },
        "status": "success"
    },
    "id": "2db9a2fe-9a39-41ef-828c-25e04dd6b07d",
    "jsonrpc": "2.0"
}

----------------------------------------

TITLE: Configuring BigQuery table partitioning in dbt
DESCRIPTION: Example of how to configure partitioning for a BigQuery table using the partition_by config in dbt. This snippet shows partitioning by a timestamp column with daily granularity.

LANGUAGE: sql
CODE:
{{ config(
    materialized='table',
    partition_by={
      "field": "created_at",
      "data_type": "timestamp",
      "granularity": "day"
    }
)}}

select
  user_id,
  event_name,
  created_at

from {{ ref('events') }}

----------------------------------------

TITLE: Conditionally Enabling Sources in sources.yml
DESCRIPTION: Example of conditionally enabling sources and specific source tables using the inline config property in the sources.yml file.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: my_source
    config:
      enabled: true
    tables:
      - name: my_source_table  # enabled
      - name: ignore_this_one  # not enabled
        config:
          enabled: false

----------------------------------------

TITLE: Implementing Custom Alias Generation for Concurrent Development in SQL
DESCRIPTION: This SQL macro generates custom alias names for tables to support concurrent development in Firebolt, which lacks native schema support. It prefixes table names with the schema value from profiles.yml.

LANGUAGE: sql
CODE:
{% macro generate_alias_name(custom_alias_name=none, node=none) -%}
    {%- if custom_alias_name is none -%}
        {{ node.schema }}__{{ node.name }}
    {%- else -%}
        {{ node.schema }}__{{ custom_alias_name | trim }}
    {%- endif -%}
{%- endmacro %}

----------------------------------------

TITLE: Configuring Lookback in Properties File
DESCRIPTION: Sets the lookback value to 2 for the user_sessions model using a properties YAML file.

LANGUAGE: yaml
CODE:
models:
  - name: user_sessions
    config:
      lookback: 2

----------------------------------------

TITLE: Using compare_queries Macro for Row Auditing in dbt
DESCRIPTION: Example of using the compare_queries macro from audit_helper to compare rows between an old table and a new dbt model.

LANGUAGE: sql
CODE:
{% set old_fct_orders_query %}
select
    id as order_id,
    amount,
    customer_id
from old_etl_schema.fct_orders
{% endset %}

{% set new_fct_orders_query %}
select
    order_id,
    amount,
    customer_id
from {{ ref('fct_orders') }}
{% endset %}

{{ audit_helper.compare_queries(
    a_query=old_fct_orders_query,
    b_query=new_fct_orders_query,
    primary_key="order_id"
) }}

----------------------------------------

TITLE: Disabling Sources from a Package in dbt_project.yml
DESCRIPTION: Example of disabling all sources imported from a package named 'events' using the dbt_project.yml file.

LANGUAGE: yaml
CODE:
sources:
  events:
    +enabled: false

----------------------------------------

TITLE: Pinning dbt Version Range in YAML
DESCRIPTION: Demonstrates how to specify a range of compatible dbt versions using both array and comma-separated syntax in the dbt_project.yml file.

LANGUAGE: yaml
CODE:
require-dbt-version: [">=1.0.0", "<2.0.0"]

LANGUAGE: yaml
CODE:
require-dbt-version: ">=1.0.0,<2.0.0"

----------------------------------------

TITLE: Configuring Iceberg Tables in dbt Project
DESCRIPTION: Example showing how to enable and configure Iceberg table format in dbt_project.yml

LANGUAGE: yaml
CODE:
flags:
  enable_iceberg_materializations: True

----------------------------------------

TITLE: Configuring Parallel Execution in Oracle dbt Model
DESCRIPTION: Demonstrates how to configure parallel execution for table materialization using the parallel parameter. This example uses 4 parallel executions.

LANGUAGE: sql
CODE:
{{config(materialized='table', parallel=4}}
SELECT c.cust_id, c.cust_first_name, c.cust_last_name
from {{ source('sh_database', 'customers') }} c

----------------------------------------

TITLE: Configuring Schema Change Behavior with ignore Parameter
DESCRIPTION: Example of using on_schema_change='ignore' configuration for incremental models. This is the default behavior that maintains existing schema.

LANGUAGE: sql
CODE:
{{config(materialized = 'incremental',on_schema_change='ignore')}} 

select * from {{ ref('seed_added') }}

----------------------------------------

TITLE: Configuring SingleStore Profile in YAML
DESCRIPTION: YAML configuration for setting up SingleStore connection details in profiles.yml. Includes required and optional parameters for connecting to a SingleStore database.

LANGUAGE: yaml
CODE:
singlestore:
  target: dev
  outputs:
    dev:
      type: singlestore
      host: [hostname]  # optional, default localhost
      port: [port number]  # optional, default 3306
      user: [user]  # optional, default root
      password: [password]  # optional, default empty
      database: [database name]  # required
      schema: [prefix for tables that dbt will generate]  # required
      threads: [1 or more]  # optional, default 1

----------------------------------------

TITLE: Creating Customer Model SQL
DESCRIPTION: SQL query for the customers model, combining data from customers and orders tables.

LANGUAGE: sql
CODE:
with customers as (

    select * from {{ ref('stg_customers') }}

),

orders as (

    select * from {{ ref('stg_orders') }}

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final

----------------------------------------

TITLE: Configuring Incremental Materialization in dbt Project YAML
DESCRIPTION: Shows the configuration of an incremental ClickHouse table materialization in the dbt_project.yml file, including unique_key and inserts_only options.

LANGUAGE: yaml
CODE:
models:
  <resource-path>:
    +materialized: incremental
    +order_by: [ <column-name>, ... ]
    +engine: <engine-type>
    +partition_by: [ <column-name>, ... ]
    +unique_key: [ <column-name>, ... ]
    +inserts_only: [ True|False ]

----------------------------------------

TITLE: Environment-Aware Model Configuration
DESCRIPTION: YAML configuration demonstrating environment-aware materialization settings in dbt_project.yml.

LANGUAGE: yaml
CODE:
models:
  +materialized: "{{ 'table' if target.name == 'prod' else 'view' }}"

----------------------------------------

TITLE: Defining Model Access in YAML Configuration
DESCRIPTION: This snippet shows how to define access modifiers for dbt models in a YAML configuration file. It demonstrates setting the access level to private, protected, or public for a specific model.

LANGUAGE: yml
CODE:
version: 2

models:
  - name: model_name
    access: private | protected | public

----------------------------------------

TITLE: Configuring Index for View in SQL
DESCRIPTION: This SQL snippet shows how to configure an index for a view using dbt's config block, specifying columns and cluster.

LANGUAGE: sql
CODE:
{{ config(materialized='view',
          indexes=[{'columns': ['col_a'], 'cluster': 'cluster_a'}]) }}
          indexes=[{'columns': ['symbol']}]) }}

select ...

----------------------------------------

TITLE: Accessing Exposures in DBT Graph
DESCRIPTION: Example demonstrating how to access and display exposures that depend on a specific model.

LANGUAGE: sql
CODE:
{% set exposures = [] -%}
{% for exposure in graph.exposures.values() -%}
  {%- if model['unique_id'] in exposure.depends_on.nodes -%}
    {%- do exposures.append(exposure) -%}
  {%- endif -%}
{%- endfor %}

-- HELLO database administrator! Before dropping this view,
-- please be aware that doing so will affect:

{% for exposure in exposures %}
--   * {{ exposure.name }} ({{ exposure.type }})
{% endfor %}

----------------------------------------

TITLE: Running dbt Parse Command in Bash
DESCRIPTION: This snippet demonstrates how to run the dbt parse command in a terminal. It shows the command execution and its output, including the location of the performance information file.

LANGUAGE: bash
CODE:
$ dbt parse
13:02:52  Running with dbt=1.5.0
13:02:53  Performance info: target/perf_info.json

----------------------------------------

TITLE: Casting Multiple Columns in SQL SELECT Statement
DESCRIPTION: Shows how to use the CAST function to convert multiple columns (order_id and customer_id) from numeric types to strings in a SELECT statement. This example uses dbt's ref function to reference the 'orders' model.

LANGUAGE: sql
CODE:
select 
	cast(order_id as string) as order_id,
	cast(customer_id as string) as customer_id,
	order_date,
	status
from {{ ref('orders') }}

----------------------------------------

TITLE: Configuring 'begin' with relative dates in SQL
DESCRIPTION: Sets the 'begin' configuration using a relative date (yesterday) in a SQL config block. It demonstrates the use of Jinja templating with datetime module to dynamically set the begin timestamp.

LANGUAGE: sql
CODE:
{{
    config(
        materialized = 'incremental',
        incremental_strategy='microbatch',
        unique_key = 'run_id',
        begin=(modules.datetime.datetime.now() - modules.datetime.timedelta(1)).isoformat(),
        event_time='created_at',
        batch_size='day',
    )
}}

----------------------------------------

TITLE: Incremental Model Configuration for Firebolt
DESCRIPTION: This snippet demonstrates how to configure an incremental model for Firebolt, using the 'append' strategy and filtering for new records.

LANGUAGE: sql
CODE:
{{ config(
   materialized = 'incremental',
   incremental_strategy='append'
) }}

/* All rows returned by this query will be appended to the existing model */


select * from {{ ref('raw_orders') }}
{% if is_incremental() %}
   where order_date > (select max(order_date) from {{ this }})
{% endif %}

----------------------------------------

TITLE: Zip Strict with Invalid Input
DESCRIPTION: Shows how zip_strict handles invalid input by raising a TypeError when non-iterable values are provided, demonstrating its stricter error handling compared to regular zip.

LANGUAGE: jinja
CODE:
{% set my_list_a = 12 %}
{% set my_list_b = ['alice', 'bob'] %}
{% set my_zip = zip_strict(my_list_a, my_list_b) %}

Compilation Error in ... (...)
  'int' object is not iterable

----------------------------------------

TITLE: Defining Data Tests for Seeds in dbt YAML
DESCRIPTION: This snippet illustrates how to define data tests for seeds in a dbt YAML file. It includes examples of seed-level and column-level tests with configurations.

LANGUAGE: yaml
CODE:
version: 2

seeds:
  - name: <seed_name>
    tests:
      - [<test_name>]
      - [<test_name>]:
          <argument_name>: <argument_value>
          [config]:
            [<test_config>]: <config-value>

    columns:
      - name: <column_name>
        tests:
          - [<test_name>]
          - [<test_name>]:
              <argument_name>: <argument_value>
              [config]:
                [<test_config>]: <config-value>

----------------------------------------

TITLE: Specifying Minimum dbt Version in YAML
DESCRIPTION: Shows how to set a minimum required dbt version using the >= operator in the dbt_project.yml file.

LANGUAGE: yaml
CODE:
require-dbt-version: ">=1.0.0"

----------------------------------------

TITLE: Configuring View Materialization in Doris/SelectDB for dbt
DESCRIPTION: This snippet demonstrates how to configure a dbt model as a Doris view using either the project file or a config block in the model file. It shows the syntax for setting the materialization type to 'view'.

LANGUAGE: yaml
CODE:
models:
  <resource-path>:
    +materialized: view

LANGUAGE: jinja
CODE:
{{ config(materialized = "view") }}

----------------------------------------

TITLE: Schema Grants Macro Implementation
DESCRIPTION: A reusable macro for granting schema usage permissions to specified users.

LANGUAGE: jinja2
CODE:
{% macro grant_usage_to_schemas(schemas, user) %}
  {% for schema in schemas %}
    grant usage on schema {{ schema }} to {{ user }};
  {% endfor %}
{% endmacro %}

----------------------------------------

TITLE: Creating Teradata Database
DESCRIPTION: SQL command to create a new database named jaffle_shop with specified permissions.

LANGUAGE: sql
CODE:
CREATE DATABASE jaffle_shop AS PERM = 1e9;

----------------------------------------

TITLE: Setting Model Access in properties.yml (Older Method)
DESCRIPTION: This code snippet illustrates the older method of configuring model access in the properties.yml file. It sets the access level to public for a specific model named 'my_public_model'.

LANGUAGE: yml
CODE:
version: 2

models:
  - name: my_public_model
    access: public # Older method, still supported

----------------------------------------

TITLE: SQL Model Definition with Mismatched Types
DESCRIPTION: SQL model definition that will fail contract enforcement due to mismatched data types.

LANGUAGE: sql
CODE:
select
  'abc123' as customer_id,
  'My Best Customer' as customer_name

----------------------------------------

TITLE: Basic DBT Runner Implementation in Python
DESCRIPTION: Demonstrates initialization and basic usage of dbtRunner to execute dbt commands programmatically. Shows how to create and invoke commands with arguments and process results.

LANGUAGE: python
CODE:
from dbt.cli.main import dbtRunner, dbtRunnerResult

# initialize
dbt = dbtRunner()

# create CLI args as a list of strings
cli_args = ["run", "--select", "tag:my_tag"]

# run the command
res: dbtRunnerResult = dbt.invoke(cli_args)

# inspect the results
for r in res.result:
    print(f"{r.node.name}: {r.status}")

----------------------------------------

TITLE: Configuring Snapshot with Legacy SQL-based Syntax
DESCRIPTION: Example of configuring a snapshot using the legacy SQL-based syntax with Jinja blocks. This snippet demonstrates how to set various configuration options such as target schema, strategy, and unique key.

LANGUAGE: sql
CODE:
{% snapshot orders_snapshot %}

{{ config(
    target_schema="<string>",
    target_database="<string>",
    unique_key="<column_name_or_expression>",
    strategy="timestamp" | "check",
    updated_at="<column_name>",
    check_cols=["<column_name>"] | "all"
    invalidate_hard_deletes : true | false
) 
}}

select * from {{ source('jaffle_shop', 'orders') }}

{% endsnapshot %}

----------------------------------------

TITLE: Alternative Format for Defining Tests in dbt YAML
DESCRIPTION: This snippet shows an alternative format for defining data tests in dbt YAML files. It demonstrates how to use a single dictionary with top-level keys for test properties, including the 'test_name' key.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: orders
    columns:
      - name: status
        tests:
          - name: unexpected_order_status_today
            test_name: accepted_values  # name of the generic test to apply
            values:
              - placed
              - shipped
              - completed
              - returned
            config:
              where: "order_date = current_date"

----------------------------------------

TITLE: Configuring meta for seeds in schema.yml
DESCRIPTION: Shows how to configure meta properties for seeds and their columns in a schema.yml file. Meta can be set at both the seed and column level.

LANGUAGE: yaml
CODE:
version: 2

seeds:
  - name: seed_name
    config:
      meta: {<dictionary>}

    columns:
      - name: column_name
        meta: {<dictionary>}

----------------------------------------

TITLE: Practical Asset Path Implementation Example
DESCRIPTION: Complete example showing how to configure asset-paths to include files from an assets subdirectory for documentation generation.

LANGUAGE: yaml
CODE:
asset-paths: ["assets"]

----------------------------------------

TITLE: Configuring table options in dbt YAML
DESCRIPTION: Example of configuring table options like file_format and partition_by in a dbt YAML file

LANGUAGE: yaml
CODE:
models:
  - name: my_model
    config:
      file_format: delta
      partition_by: date_day
      location_root: /mnt/root
      tblproperties:
        this.is.my.key: 12

----------------------------------------

TITLE: Configuring Distribution Key in Greenplum dbt Model
DESCRIPTION: Demonstrates how to specify a distribution key for a Greenplum table to optimize data distribution across segments. By default, data is distributed randomly, but this can be customized for better join performance.

LANGUAGE: sql
CODE:
{{
    config(
        ...
        distributed_by='<field_name>'
        ...
    )
}}


select ...

----------------------------------------

TITLE: Legacy Metrics Calculation Query
DESCRIPTION: SQL example showing how to calculate metrics using the legacy metrics.calculate() macro with weekly grain and dimensions.

LANGUAGE: sql
CODE:
select * 
from {{ metrics.calculate(  
[metric('orders)',
metric('revenue)'],
    grain='week',
    dimensions=['metric_time', 'customer_type'],
) }}

----------------------------------------

TITLE: Setting Seed-Specific Delimiter in seeds/properties.yml
DESCRIPTION: This YAML snippet shows how to configure a custom delimiter for a specific seed file using the seeds/properties.yml configuration file.

LANGUAGE: yaml
CODE:
version: 2

seeds:
  - name: <seed_name>
    config: 
      delimiter: "|"

----------------------------------------

TITLE: Marking Real Differences in Data
DESCRIPTION: SQL code that identifies genuine differences between rows by comparing current and previous grain IDs within a partition.

LANGUAGE: sql
CODE:
mark_real_diffs as (

  select
      *,
      coalesce(
          lag(grain_id) over (partition by entity_id order by updated_at_date),
          'first_record'
      ) as previous_grain_id,
      case
          when grain_id != previous_grain_id then true 
          else false
      end as is_real_diff

  from base_product

),

----------------------------------------

TITLE: Merge Strategy with Apache Hudi in AWS Glue
DESCRIPTION: Example of using the merge strategy with Apache Hudi integration. This configuration requires specific Hudi libraries and AWS Glue runtime 2.

LANGUAGE: sql
CODE:
{{ config(
    materialized='incremental',
    incremental_strategy='merge',
    unique_key='user_id',
    file_format='hudi'
) }}

with new_events as (

    select * from {{ ref('events') }}

    {% if is_incremental() %}
    where date_day >= date_add(current_date, -1)
    {% endif %}

)

select
    user_id,
    max(date_day) as last_seen

from events
group by 1

LANGUAGE: yaml
CODE:
extra_jars: "s3://dbt-glue-hudi/Dependencies/hudi-spark.jar,s3://dbt-glue-hudi/Dependencies/spark-avro_2.11-2.4.4.jar"

----------------------------------------

TITLE: Configuring Incremental Materialization in Doris/SelectDB for dbt
DESCRIPTION: This snippet demonstrates how to configure an incremental Doris table in dbt using either the project file or a config block in the model file. It includes configuration options specific to incremental models, such as unique_key.

LANGUAGE: yaml
CODE:
models:
  <resource-path>:
    +materialized: incremental
    +unique_key: [ <column-name>, ... ],
    +partition_by: [ <column-name>, ... ],
    +partition_type: <engine-type>,
    +partition_by_init: [<pertition-init>, ... ]
    +distributed_by: [ <column-name>, ... ],
    +buckets: int,
    +properties: {<key>:<value>,...}

LANGUAGE: jinja
CODE:
{{ config(
    materialized = "incremental",
    unique_key = [ "<column-name>", ... ],
    partition_by = [ "<column-name>", ... ],
    partition_type = "<engine-type>",
    partition_by_init = ["<pertition-init>", ... ]
    distributed_by = [ "<column-name>", ... ],
    buckets = "int",
    properties = {"<key>":"<value>",...}
      ...
    ]
) }}

----------------------------------------

TITLE: Equivalent Query Using CTE Instead of HAVING
DESCRIPTION: This example achieves the same result as the previous HAVING example, but uses a Common Table Expression (CTE) instead. It demonstrates an alternative approach that may be more readable in some cases.

LANGUAGE: sql
CODE:
with counts as (
	select
		customer_id,
		count(order_id) as num_orders
	from {{ ref('orders') }}
	group by 1
)
select
	customer_id,
	num_orders
from counts
where num_orders > 1

----------------------------------------

TITLE: Configuring dbt Models
DESCRIPTION: Configurations can be set using the config() macro in SQL files or in dbt_project.yml to control how resources are built in the warehouse. They define materialization, schema locations, and other build properties.

LANGUAGE: sql
CODE:
{{ config(
    materialized='table',
    schema='analytics'
) }}

SELECT * FROM my_source

----------------------------------------

TITLE: Declaring a Group in YAML
DESCRIPTION: Defines a 'finance' group with owner details in a YAML file. This snippet demonstrates how to declare a group with various owner properties.

LANGUAGE: yaml
CODE:
groups:
  - name: finance
    owner:
      # 'name' or 'email' is required; additional properties allowed
      email: finance@jaffleshop.com
      slack: finance-data
      github: finance-data-team

----------------------------------------

TITLE: Configuring Custom Schema for Marketing Models in dbt
DESCRIPTION: Example of setting a custom schema 'marketing' for a group of marketing-related models in dbt_project.yml

LANGUAGE: yaml
CODE:
models:
  your_project:
    marketing: #  Grouping or folder for set of models
      +schema: marketing

----------------------------------------

TITLE: Configuring Certificate Authentication for Starburst/Trino in dbt
DESCRIPTION: Example YAML configuration for setting up a certificate-authenticated connection to a Starburst/Trino cluster in dbt's profiles.yml file. Includes essential connection parameters and certificate-specific settings.

LANGUAGE: yaml
CODE:
trino:
  target: dev
  outputs:
    dev:
      type: trino
      method: certificate 
      cert: [path/to/cert_file]
      client_certificate: [path/to/client/cert]
      client_private_key: [path to client key]
      database: [database name]
      schema: [your dbt schema]
      port: [port number]
      threads: [1 or more]

----------------------------------------

TITLE: Padding Leading Zeros with SQL in DBT
DESCRIPTION: SQL function to add leading zeros to numeric fields like zipcodes using the LPAD function. This method is recommended for DBT versions prior to v0.16.0.

LANGUAGE: sql
CODE:
lpad(zipcode, 5, '0')

----------------------------------------

TITLE: Setting hard_deletes in dbt_project.yml
DESCRIPTION: Project-level configuration for hard_deletes in dbt_project.yml. Allows setting the default behavior for handling deleted records across all snapshots.

LANGUAGE: yaml
CODE:
snapshots:
  [<resource-path>]:
    +hard_deletes: "ignore" | "invalidate" | "new_record"

----------------------------------------

TITLE: Defining Sources JSON Structure in dbt
DESCRIPTION: This snippet outlines the structure of the Sources JSON file produced by dbt's source freshness command. It includes top-level keys and the structure of each entry in the results array.

LANGUAGE: json
CODE:
{
  "metadata": {},
  "elapsed_time": 0,
  "results": [
    {
      "unique_id": "",
      "max_loaded_at": "",
      "snapshotted_at": "",
      "max_loaded_at_time_ago_in_s": 0,
      "criteria": {},
      "status": "",
      "execution_time": 0,
      "timing": []
    }
  ]
}

----------------------------------------

TITLE: Configuring Fact Table Model in dbt_project.yml for Firebolt
DESCRIPTION: This snippet demonstrates how to configure a fact table model in the dbt_project.yml file, including setting the table type, primary index, and aggregating indexes.

LANGUAGE: yaml
CODE:
models:
  <resource-path>:
    +materialized: table
    +table_type: fact
    +primary_index: [ <column-name>, ... ]
    +indexes:
      - index_type: aggregating
        key_columns: [ <column-name>, ... ]
        aggregation: [ <agg-sql>, ... ]
      ...

----------------------------------------

TITLE: Configuring dbt Profile for Rockset Connection
DESCRIPTION: This YAML snippet demonstrates how to set up the dbt profile for connecting to Rockset. It includes the necessary fields such as workspace, API key, and API server.

LANGUAGE: yaml
CODE:
rockset:
  target: dev
  outputs:
    dev:
      type: rockset
      workspace: [schema]
      api_key: [api_key]
      api_server: [api_server] # (Default is api.rs2.usw2.rockset.com)

----------------------------------------

TITLE: Clone Jaffle Shop Repository
DESCRIPTION: Command to clone the Jaffle Shop git repository for DuckDB setup.

LANGUAGE: bash
CODE:
git clone https://github.com/dbt-labs/jaffle_shop_duckdb.git

----------------------------------------

TITLE: Configuring hard_deletes in schema.yml
DESCRIPTION: YAML configuration for hard_deletes in a snapshot schema file. Defines how deleted records should be handled using one of three options: ignore, invalidate, or new_record.

LANGUAGE: yaml
CODE:
snapshots:
  - name: <snapshot_name>
    config:
      hard_deletes: 'ignore' | 'invalidate' | 'new_record'

----------------------------------------

TITLE: Configuring Snapshot Meta Column Names in YAML
DESCRIPTION: This snippet shows how to configure custom names for snapshot metadata columns in a YAML schema file. It demonstrates setting custom names for all available metadata columns.

LANGUAGE: yaml
CODE:
snapshots:
  - name: <snapshot_name>
    config:
      snapshot_meta_column_names:
        dbt_valid_from: <string>
        dbt_valid_to: <string>
        dbt_scd_id: <string>
        dbt_updated_at: <string>
        dbt_is_deleted: <string>

----------------------------------------

TITLE: Configuring meta for semantic models in dbt_project.yml
DESCRIPTION: Shows how to configure meta properties for semantic models in the dbt_project.yml file. Meta is defined as a dictionary under the semantic-models configuration block.

LANGUAGE: yaml
CODE:
semantic-models:
  [<resource-path>]:
    +meta: {<dictionary>}

----------------------------------------

TITLE: Configuring Seed-Specific Options in Property YAML
DESCRIPTION: This snippet shows how to configure seed-specific options in a seed's properties.yml file, including quote_columns, column_types, and delimiter.

LANGUAGE: yaml
CODE:
version: 2

seeds:
  - name: [<seed-name>]
    config:
      [quote_columns]: true | false
      [column_types]: {column_name: datatype}
      [delimiter]: <string>

----------------------------------------

TITLE: Configuring Thrift Connection in dbt Profiles
DESCRIPTION: YAML configuration for setting up a Thrift connection to a Spark cluster in the dbt profiles file.

LANGUAGE: yaml
CODE:
your_profile_name:
  target: dev
  outputs:
    dev:
      type: spark
      method: thrift
      schema: [database/schema name]
      host: [hostname]
      
      # optional
      port: [port]              # default 10001
      user: [user]
      auth: [e.g. KERBEROS]
      kerberos_service_name: [e.g. hive]
      use_ssl: [true|false]   # value of hive.server2.use.SSL, default false
      server_side_parameters:
        "spark.driver.memory": "4g" 

----------------------------------------

TITLE: Cloning the airflow-dbt-cloud Repository
DESCRIPTION: Git commands to clone the airflow-dbt-cloud repository and navigate into the project directory. This repository contains example Airflow DAGs for orchestrating dbt Cloud jobs.

LANGUAGE: bash
CODE:
git clone https://github.com/dbt-labs/airflow-dbt-cloud.git
cd airflow-dbt-cloud

----------------------------------------

TITLE: Model Documentation in Schema File
DESCRIPTION: Configuration for individual model documentation settings in schema.yml, including visibility and node color options.

LANGUAGE: yml
CODE:
version: 2

models:
  - name: model_name
    docs:
      show: true | false
      node_color: color_id # Use name (such as node_color: purple) or hex code with quotes (such as node_color: "#cd7f32")

----------------------------------------

TITLE: Specifying File Format in dbt Model for IBM watsonx.data Spark
DESCRIPTION: Shows how to specify the file format (iceberg, hive, delta, or hudi) for a dbt model when working with IBM watsonx.data Spark. This configuration is used alongside the table materialization setting.

LANGUAGE: sql
CODE:
{{
  config(
    materialized='table',
    file_format='iceberg' or 'hive' or 'delta' or 'hudi'
  )
}}

----------------------------------------

TITLE: Basic SELECT Query in dbt
DESCRIPTION: Demonstrates a basic SELECT statement in dbt that retrieves specific columns from an orders table with a limit clause. Uses dbt's ref function to reference the source table and includes column-level comments for clarity.

LANGUAGE: sql
CODE:
select
	order_id, --your first column you want selected
	customer_id, --your second column you want selected
	order_date --your last column you want selected (and so on)
from {{ ref('orders') }} --the table/view/model you want to select from
limit 3

----------------------------------------

TITLE: SQL Query for Join Optimization Example
DESCRIPTION: A SQL query joining three tables used to illustrate how query optimization can improve performance by reordering joins based on table sizes.

LANGUAGE: sql
CODE:
select 
  *
from a
join b on a.id = b.a_id
join c on b.id = c.b_id

----------------------------------------

TITLE: Raising Compiler Error in dbt SQL
DESCRIPTION: This snippet demonstrates how to use `exceptions.raise_compiler_error` to raise a compiler error with a custom message when an invalid number is provided. It checks if the number is outside the range of 0 to 100.

LANGUAGE: sql
CODE:
{% if number < 0 or number > 100 %}
  {{ exceptions.raise_compiler_error("Invalid `number`. Got: " ~ number) }}
{% endif %}

----------------------------------------

TITLE: Configuring Generic Test Failure Storage in YAML
DESCRIPTION: This YAML snippet shows how to set 'store_failures_as' for generic tests in a _models.yml file. It configures different storage methods for 'not_null' and 'unique' tests on a column.

LANGUAGE: yaml
CODE:
models:
  - name: my_model
    columns:
      - name: id
        tests:
          - not_null:
              config:
                store_failures_as: view
          - unique:
              config:
                store_failures_as: ephemeral

----------------------------------------

TITLE: Git Merge Conflict Markers Example
DESCRIPTION: Example showing the standard Git conflict markers that appear in files with merge conflicts. These markers indicate the conflicting sections of code that need to be resolved.

LANGUAGE: git
CODE:
<<<<<< HEAD
    your current code
======
    conflicting code
>>>>>> (some branch identifier)

----------------------------------------

TITLE: Implementing Table Partitioning in Greenplum dbt Model
DESCRIPTION: Demonstrates how to set up table partitioning using fields_string and raw_partition configurations, including column definitions and partition specifications.

LANGUAGE: sql
CODE:
{% set fields_string %}
    some_filed int4 null,
    date_field timestamp NULL
{% endset %}


{% set raw_partition %}
   PARTITION BY RANGE (date_field)
   (
       START ('2021-01-01'::timestamp) INCLUSIVE
       END ('2023-01-01'::timestamp) EXCLUSIVE
       EVERY (INTERVAL '1 day'),
       DEFAULT PARTITION default_part
   );
{% endset %}

{{
   config(
       ...
       fields_string=fields_string,
       raw_partition=raw_partition,
       ...
   )
}}

select *

----------------------------------------

TITLE: Using env_var in profiles.yml for Database Credentials
DESCRIPTION: Demonstrates how to use the env_var function in a profiles.yml file to securely include database credentials from environment variables.

LANGUAGE: yaml
CODE:
profile:
  target: prod
  outputs:
    prod:
      type: postgres
      host: 127.0.0.1
      # IMPORTANT: Make sure to quote the entire Jinja string here
      user: "{{ env_var('DBT_USER') }}"
      password: "{{ env_var('DBT_PASSWORD') }}"
      ....

----------------------------------------

TITLE: Configuring fail_calc for Generic Test Error Handling
DESCRIPTION: Example of handling sum() aggregation in fail_calc to prevent None/null errors by using a case statement to return 0 when no rows exist.

LANGUAGE: yaml
CODE:
fail_calc: "case when count(*) > 0 then sum(n_records) else 0 end"

----------------------------------------

TITLE: Default generate_schema_name Macro Implementation in dbt
DESCRIPTION: This snippet shows the default implementation of the generate_schema_name macro in dbt. It concatenates the default schema with a custom schema name if provided.

LANGUAGE: sql
CODE:
{% macro generate_schema_name(custom_schema_name, node) -%}

    {%- set default_schema = target.schema -%}
    {%- if custom_schema_name is none -%}

        {{ default_schema }}

    {%- else -%}

        {{ default_schema }}_{{ custom_schema_name | trim }}

    {%- endif -%}

{%- endmacro %}

----------------------------------------

TITLE: Setting table_kind for Teradata Tables in dbt
DESCRIPTION: Defines the table kind for Teradata tables, with options for MULTISET or SET.

LANGUAGE: yaml
CODE:
{{
  config(
      materialized="table",
      table_kind="SET"
  )
}}

LANGUAGE: yaml
CODE:
seeds:
  <project-name>:
    table_kind: "SET"

----------------------------------------

TITLE: Configuring Unlogged Tables in YAML for dbt-Postgres
DESCRIPTION: This snippet shows how to set all models to be unlogged tables using the dbt_project.yml configuration file.

LANGUAGE: yaml
CODE:
models:
  +unlogged: true

----------------------------------------

TITLE: Defining Test Cases for dbt Adapter
DESCRIPTION: This code snippet shows how to define test cases for a dbt adapter using pytest and the dbt testing framework.

LANGUAGE: Python
CODE:
import pytest

from dbt.tests.adapter.basic.test_base import BaseSimpleMaterializations
from dbt.tests.adapter.basic.test_singular_tests import BaseSingularTests
from dbt.tests.adapter.basic.test_singular_tests_ephemeral import BaseSingularTestsEphemeral
from dbt.tests.adapter.basic.test_empty import BaseEmpty
from dbt.tests.adapter.basic.test_ephemeral import BaseEphemeral
from dbt.tests.adapter.basic.test_incremental import BaseIncremental
from dbt.tests.adapter.basic.test_generic_tests import BaseGenericTests
from dbt.tests.adapter.basic.test_snapshot_check_cols import BaseSnapshotCheckCols
from dbt.tests.adapter.basic.test_snapshot_timestamp import BaseSnapshotTimestamp
from dbt.tests.adapter.basic.test_adapter_methods import BaseAdapterMethod

class TestSimpleMaterializationsMyAdapter(BaseSimpleMaterializations):
    pass

class TestSingularTestsMyAdapter(BaseSingularTests):
    pass

# ... more test classes ...

----------------------------------------

TITLE: Custom Macro Directory Configuration
DESCRIPTION: Example of using a custom directory name for storing macros instead of the default 'macros' directory.

LANGUAGE: yml
CODE:
macro-paths: ["custom_macros"]

----------------------------------------

TITLE: Configuring Source Freshness in YAML for dbt
DESCRIPTION: This YAML configuration defines source freshness settings for a dbt project. It specifies freshness criteria for different tables, including warning and error thresholds, as well as custom filters.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: jaffle_shop
    database: raw

    freshness:
      warn_after: {count: 12, period: hour}
      error_after: {count: 24, period: hour}

    loaded_at_field: _etl_loaded_at

    tables:
      - name: customers

      - name: orders
        freshness:
          warn_after: {count: 6, period: hour}
          error_after: {count: 12, period: hour}
          filter: datediff('day', _etl_loaded_at, current_timestamp) < 2

      - name: product_skus
        freshness: null

----------------------------------------

TITLE: Configuring meta for saved queries in semantic_models.yml
DESCRIPTION: Shows how to configure meta properties for saved queries in a semantic_models.yml file. Meta is set under the config block for each saved query.

LANGUAGE: yaml
CODE:
saved_queries:
  - name: saved_query_name
    config:
      meta: {<dictionary>}

----------------------------------------

TITLE: Parameter Override in DBT Runner
DESCRIPTION: Shows how to override command parameters using keyword arguments instead of CLI-style string arguments.

LANGUAGE: python
CODE:
from dbt.cli.main import dbtRunner
dbt = dbtRunner()

# these are equivalent
dbt.invoke(["--fail-fast", "run", "--select", "tag:my_tag"])
dbt.invoke(["run"], select=["tag:my_tag"], fail_fast=True)

----------------------------------------

TITLE: Using Generate Database Name Macro
DESCRIPTION: Example of using generate_database_name macro to maintain consistent database naming with models.

LANGUAGE: sql
CODE:
{{
    config(
      target_database=generate_database_name('snapshots')
    )
}}

----------------------------------------

TITLE: Example of Debug Log Format in dbt
DESCRIPTION: Illustrates the debug format used in log files, which includes detailed timestamp, invocation_id, thread_id, and log level.

LANGUAGE: text
CODE:
============================== 16:12:08.555032 | 9089bafa-4010-4f38-9b42-564ec9106e07 ==============================
16:12:08.555032 [info ] [MainThread]: Running with dbt=1.8.0
16:12:08.751069 [info ] [MainThread]: Registered adapter: postgres=1.8.0

----------------------------------------

TITLE: Setting Character Set and Collation in SingleStore
DESCRIPTION: Shows how to specify character set and collation settings for a SingleStore table.

LANGUAGE: sql
CODE:
{{
    config(
        charset='utf8mb4',
        collation='utf8mb4_general_ci'
    )
}}

select ...

----------------------------------------

TITLE: Configuring Source Enablement in DBT
DESCRIPTION: Configuration options for enabling/disabling source definitions in dbt projects

LANGUAGE: yaml
CODE:
sources:
  [<resource-path>]:
    +enabled: true | false

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: [<source-name>]
    config:
      enabled: true | false
    tables:
      - name: [<source-table-name>]
        config:
          enabled: true | false

----------------------------------------

TITLE: Complete dbt Show Output Example
DESCRIPTION: Full example showing the command execution and formatted table output for a staging orders model.

LANGUAGE: bash
CODE:
dbt show --select "stg_orders"
21:17:38 Running with dbt=1.5.0-b5
21:17:38 Found 5 models, 20 tests, 0 snapshots, 0 analyses, 425 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
21:17:38
21:17:38 Concurrency: 24 threads (target='dev')
21:17:38
21:17:38 Previewing node 'stg_orders' :
| order_id | customer_id | order_date | status    |
|----------+-------------+------------+--------   |
| 1        |           1 | 2023-01-01 | returned  |
| 2        |           3 | 2023-01-02 | completed |
| 3        |          94 | 2023-01-03 | completed |
| 4        |          50 | 2023-01-04 | completed |
| 5        |          64 | 2023-01-05 | completed |

----------------------------------------

TITLE: Configuring Check Strategy in YAML (dbt 1.9+)
DESCRIPTION: Configuration for check-based snapshot strategy using YAML format. Allows monitoring specific columns or all columns for changes.

LANGUAGE: yaml
CODE:
snapshots:
- name: snapshot_name
  relation: source('my_source', 'my_table')
  config:
    strategy: check
    check_cols: [column_name] | "all"

----------------------------------------

TITLE: Setting Replicated Distribution in Greenplum dbt Model
DESCRIPTION: Shows how to configure a table for replicated distribution across all segments using the distributed_replicated parameter.

LANGUAGE: sql
CODE:
{{
    config(
        ...
        distributed_replicated=true
        ...
    )
}}


select ...

----------------------------------------

TITLE: Reusing Manifest Objects in DBT Runner
DESCRIPTION: Shows how to optimize performance by reusing manifest objects between commands. Includes manifest parsing and validation example.

LANGUAGE: python
CODE:
from dbt.cli.main import dbtRunner, dbtRunnerResult
from dbt.contracts.graph.manifest import Manifest

# use 'parse' command to load a Manifest
res: dbtRunnerResult = dbtRunner().invoke(["parse"])
manifest: Manifest = res.result

# introspect manifest
# e.g. assert every public model has a description
for node in manifest.nodes.values():
    if node.resource_type == "model" and node.access == "public":
        assert node.description != "", f"{node.name} is missing a description"

# reuse this manifest in subsequent commands to skip parsing
dbt = dbtRunner(manifest=manifest)
cli_args = ["run", "--select", "tag:my_tag"]
res = dbt.invoke(cli_args)

----------------------------------------

TITLE: Aggregating Test Failures with Metadata in dbt
DESCRIPTION: This SQL snippet demonstrates how to create a base model in dbt that aggregates test failures and joins them with metadata. It uses BigQuery-specific features like wildcard selectors and partitioning.

LANGUAGE: sql
CODE:
{{ config(
       materialized = 'incremental',
       partition_by = {'field': 'load_date', 'data_type': 'date'},
       incremental_strategy = 'merge',
       unique_key='row_key',
       full_refresh=false,
       tags=['dq_test_warning_failures','customer_mart', 'data_health']
   )
}}

WITH failures as (
   SELECT
       count(*) as test_failures,
       _TABLE_SUFFIX as table_suffix,
   FROM {{ var('customer_mart_schema') }}_dbt_test__audit.`*`
   GROUP BY _TABLE_SUFFIX
),

metadata as (
   SELECT
       test_owner,
       test_alias,
       test_description,
       split(test_alias, '__')[SAFE_ORDINAL(2)] as test_name,
       test_severity
   FROM {{ref('test_warning_metadata')}}
),

SELECT
   m.*,
   f.*
FROM metadata m
LEFT JOIN failures f on m.test_alias = f.table_suffix
WHERE m.is_active is TRUE

----------------------------------------

TITLE: Configuring pre-hooks and post-hooks in snapshot SQL files
DESCRIPTION: Define pre-hooks and post-hooks directly in snapshot SQL files using the config block. Hooks can be single SQL statements or lists of statements.

LANGUAGE: sql
CODE:
{% snapshot snapshot_name %}
{{ config(
    pre_hook="SQL-statement" | ["SQL-statement"],
    post_hook="SQL-statement" | ["SQL-statement"],
) }}

select ...

{% end_snapshot %}

----------------------------------------

TITLE: Defining Customer Orders Model with Dependencies in dbt SQL
DESCRIPTION: A dbt SQL model that aggregates customer order data by referencing a staging model 'stg_orders' using the ref function. The model calculates first order date, most recent order date, and total number of orders per customer.

LANGUAGE: sql
CODE:
select
    customer_id,
    min(order_date) as first_order_date,
    max(order_date) as most_recent_order_date,
    count(order_id) as number_of_orders
from {{ ref('stg_orders') }}
group by 1


----------------------------------------

TITLE: Using the N-Plus Operator in dbt Commands
DESCRIPTION: Shows how to use the 'n-plus' operator in dbt run commands to specify the number of edges to traverse when selecting models and their dependencies. This allows for more fine-grained control over the selection of ancestors and descendants.

LANGUAGE: bash
CODE:
dbt run --select "my_model+1"        # select my_model and its first-degree descendants
dbt run --select "2+my_model"        # select my_model, its first-degree ancestors ("parents"), and its second-degree ancestors ("grandparents")
dbt run --select "3+my_model+4"      # select my_model, its ancestors up to the 3rd degree, and its descendants down to the 4th degree

----------------------------------------

TITLE: Initializing dbt Project
DESCRIPTION: Create a new dbt project named 'jaffle_shop' using the init command.

LANGUAGE: shell
CODE:
dbt init jaffle_shop

----------------------------------------

TITLE: Configuring table_option for Teradata Tables in dbt
DESCRIPTION: Defines various table options for Teradata tables, including fallback protection, journaling, and compression settings.

LANGUAGE: yaml
CODE:
{{
  config(
      materialized="table",
      table_option="NO FALLBACK, NO JOURNAL, CHECKSUM = ON,
        NO MERGEBLOCKRATIO,
        WITH CONCURRENT ISOLATED LOADING FOR ALL"
  )
}}

LANGUAGE: yaml
CODE:
seeds:
  <project-name>:
    table_option: "NO FALLBACK, NO JOURNAL, CHECKSUM = ON,
      NO MERGEBLOCKRATIO,
      WITH CONCURRENT ISOLATED LOADING FOR ALL"

----------------------------------------

TITLE: Configuring Yellowbrick Profile in YAML for dbt
DESCRIPTION: This YAML configuration sets up a Yellowbrick target in the dbt profiles.yml file. It includes essential connection parameters such as host, user, password, port, and database name, as well as optional settings for role assumption and SSL configuration.

LANGUAGE: yaml
CODE:
company-name:
  target: dev
  outputs:
    dev:
      type: yellowbrick
      host: [hostname]
      user: [username]
      password: [password]
      port: [port]
      dbname: [database name]
      schema: [dbt schema]
      [role]: [optional, set the role dbt assumes when executing queries]
      [sslmode]: [optional, set the sslmode used to connect to the database]
      [sslrootcert]: [optional, set the sslrootcert config value to a new file path to customize the file location that contains root certificates]

----------------------------------------

TITLE: Configuring Updated_at in dbt_project.yml
DESCRIPTION: Example of configuring the updated_at parameter globally for snapshots in the dbt_project.yml file. It sets the strategy to timestamp and specifies the updated_at column for all snapshots in the specified resource path.

LANGUAGE: yaml
CODE:
snapshots:
  [<resource-path>]:
    +strategy: timestamp
    +updated_at: column_name

----------------------------------------

TITLE: Creating Initial Customer Model
DESCRIPTION: SQL query that combines customer and order data to create a comprehensive customer analytics model with order history and metrics.

LANGUAGE: sql
CODE:
with customers as (
    select
        id as customer_id,
        first_name,
        last_name
    from `dbt-tutorial`.jaffle_shop.customers
),

orders as (
    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status
    from `dbt-tutorial`.jaffle_shop.orders
),

customer_orders as (
    select
        customer_id,
        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders
    from orders
    group by 1
),

final as (
    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders
    from customers
    left join customer_orders using (customer_id)
)

select * from final

----------------------------------------

TITLE: Setting Custom Database for Seeds in dbt
DESCRIPTION: Shows how to configure a custom database for seeds in the dbt_project.yml file. This example sets the 'product_categories' seed to be loaded into the 'staging' database instead of the target database.

LANGUAGE: yaml
CODE:
seeds:
  your_project:
    product_categories:
      +database: staging

----------------------------------------

TITLE: Session Information Configuration in Oracle dbt Profile
DESCRIPTION: Shows how to configure custom session information in profile.yml for tracking dbt sessions in Oracle V$SESSION view.

LANGUAGE: yaml
CODE:
dbt_test:
   target: dev
   outputs:
      dev:
         type: oracle
         user: "{{ env_var('DBT_ORACLE_USER') }}"
         pass: "{{ env_var('DBT_ORACLE_PASSWORD') }}"
         database: "{{ env_var('DBT_ORACLE_DATABASE') }}"
         tns_name: "{{ env_var('DBT_ORACLE_TNS_NAME') }}"
         schema: "{{ env_var('DBT_ORACLE_SCHEMA') }}"
         threads: 4
         session_info:
            action: "dbt run"
            client_identifier: "dbt-unique-client-uuid"
            client_info: "dbt Python3.9 thin driver"
            module: "dbt-oracle-1.8.x"

----------------------------------------

TITLE: Incorrect Model Path Configuration
DESCRIPTION: Demonstrates an anti-pattern of using absolute paths in model-paths configuration, which should be avoided.

LANGUAGE: yml
CODE:
model-paths: ["/Users/username/project/models"]

----------------------------------------

TITLE: Creating a Predictor Model in dbt for MindsDB
DESCRIPTION: This snippet shows how to create a predictor model in dbt for MindsDB. It uses the 'predictor' materialization and includes configuration for the integration, prediction field, and optional encoder settings.

LANGUAGE: sql
CODE:
-- my_first_model.sql    
    {{
        config(
            materialized='predictor',
            integration='photorep',
            predict='name',
            predict_alias='name',
            using={
                'encoders.location.module': 'CategoricalAutoEncoder',
                'encoders.rental_price.module': 'NumericEncoder'
            }
        )
    }}
      select * from stores

----------------------------------------

TITLE: Defining Data Tests for Snapshots in dbt YAML
DESCRIPTION: This snippet shows how to define data tests for snapshots in a dbt YAML file. It includes examples of snapshot-level and column-level tests with configurations.

LANGUAGE: yaml
CODE:
version: 2

snapshots:
  - name: <snapshot_name>
    tests:
      - [<test_name>]
      - [<test_name>]:
          <argument_name>: <argument_value>
          [config]:
            [<test_config>]: <config-value>

    columns:
      - name: <column_name>
        tests:
          - [<test_name>]
          - [<test_name>]:
              <argument_name>: <argument_value>
              [config]:
                [<test_config>]: <config-value>

----------------------------------------

TITLE: Using no-clean-project-files-only flag with dbt clean
DESCRIPTION: Executes the dbt clean command to delete all paths specified in the clean-targets list, including those outside the current dbt project.

LANGUAGE: shell
CODE:
dbt clean --no-clean-project-files-only

----------------------------------------

TITLE: Google Workspace SSO Configuration Parameters
DESCRIPTION: Required configuration values for setting up the OAuth client ID in Google Cloud Platform, including application type, authorized domains, and redirect URIs.

LANGUAGE: markdown
CODE:
| Configuration          | Value        | notes |
| ---------------------- | ------------ | ------ |
| **Application type**   | internal     | required |
| **Application name**   | dbt Cloud    | required |
| **Application logo**   | Download the logo <a href="https://www.getdbt.com/ui/img/dbt-icon.png" target="_blank" rel="noopener noreferrer">here</a> | optional |
| **Authorized domains** | `getdbt.com` (US multi-tenant) `getdbt.com` and `dbt.com`(US Cell 1) `dbt.com` (EMEA or AU) | If deploying into a VPC, use the domain for your deployment |
| **Scopes** | `email, profile, openid` | The default scopes are sufficient |

----------------------------------------

TITLE: Configuring Snapshot with invalidate_hard_deletes in YAML
DESCRIPTION: YAML configuration example showing how to enable hard delete invalidation in a dbt snapshot configuration file.

LANGUAGE: yaml
CODE:
snapshots:
  - name: snapshot
    relation: source('my_source', 'my_table')
    config:
      strategy: timestamp
      invalidate_hard_deletes: true | false

----------------------------------------

TITLE: Dynamic Table Configuration Examples
DESCRIPTION: Shows configuration options for Snowflake dynamic tables including target lag and warehouse settings

LANGUAGE: yaml
CODE:
models:
  <resource-path>:
    +materialized: dynamic_table
    +target_lag: "30 minutes"
    +snowflake_warehouse: "warehouse_name"

----------------------------------------

TITLE: Listing Models by Package
DESCRIPTION: Example showing how to list all models from the snowplow package using the select argument.

LANGUAGE: bash
CODE:
$ dbt ls --select snowplow.*
snowplow.snowplow_base_events
snowplow.snowplow_base_web_page_context
snowplow.snowplow_id_map
snowplow.snowplow_page_views
snowplow.snowplow_sessions
...

----------------------------------------

TITLE: Running Unit Tests via Shell Command
DESCRIPTION: Shell command to execute the unit test macro using dbt run-operation

LANGUAGE: shell
CODE:
dbt run-operation test_to_literal

----------------------------------------

TITLE: Configuring Declarative Caching in YAML for dbt Semantic Layer
DESCRIPTION: This YAML snippet demonstrates how to configure declarative caching for a saved query in the dbt Semantic Layer. It enables caching and defines an export for the cached data.

LANGUAGE: yaml
CODE:
saved_queries:
  - name: my_saved_query
    ... # Rest of the saved queries configuration.
    config:
      cache:
        enabled: true  # Set to true to enable, defaults to false.
    exports:
      - name: order_data_key_metrics
        config:
          export_as: table

----------------------------------------

TITLE: Combining Directory and Tag Filters in dbt CLI
DESCRIPTION: Shows how to combine directory path and tag filters to select models that match both criteria using comma separation.

LANGUAGE: bash
CODE:
dbt run --select "marts.finance,tag:nightly"

----------------------------------------

TITLE: Listing Active dbt Invocations
DESCRIPTION: Demonstrates how to use the 'list' command to display active invocations in the dbt Cloud CLI, including their ID, status, type, arguments, and start time.

LANGUAGE: shell
CODE:
dbt invocation list

----------------------------------------

TITLE: Configuring SQL Insert Job in dbt
DESCRIPTION: Illustrates the configuration for a SQL insert job as a dbt model using 'incremental' materialization with 'insert' strategy.

LANGUAGE: sql
CODE:
{{ config(  materialized='incremental',
            sync=True|False,
            map_columns_by_name=True|False,
            incremental_strategy='insert',
            options={
              'option_name': 'option_value'
            },
            primary_key=[{}]
          )
}}
SELECT ...
FROM {{ ref(<model>) }}
WHERE ...
GROUP BY ...
HAVING COUNT(DISTINCT orderid::string) ...

----------------------------------------

TITLE: Configuring pre-hooks and post-hooks for seeds in dbt_project.yml
DESCRIPTION: Define pre-hooks and post-hooks for seeds in the dbt_project.yml file. Hooks can be single SQL statements or lists of statements.

LANGUAGE: yaml
CODE:
seeds:
  [<resource-path>]:
    +pre-hook: SQL-statement | [SQL-statement]
    +post-hook: SQL-statement | [SQL-statement]

----------------------------------------

TITLE: Validating Python Version in CLI
DESCRIPTION: Command to check the installed Python version, which must be Python 3.

LANGUAGE: shell
CODE:
$ python3 --version
Python 3.11.4 # Must be Python 3

----------------------------------------

TITLE: Configuring Singular Test Failure Storage in SQL
DESCRIPTION: This snippet demonstrates how to set the 'store_failures_as' config for a singular test in a .sql file. It configures the test to store failures as a table.

LANGUAGE: sql
CODE:
{{ config(store_failures_as="table") }}

-- custom singular test
select 1 as id
where 1=0

----------------------------------------

TITLE: Hybrid Columnar Compression for Query in Oracle dbt Model
DESCRIPTION: Examples of configuring Hybrid Columnar Compression for query optimization with both LOW and HIGH compression options.

LANGUAGE: sql
CODE:
{{config(materialized='table', table_compression_clause='COLUMN STORE COMPRESS FOR QUERY LOW')}}
SELECT c.cust_id, c.cust_first_name, c.cust_last_name
from {{ source('sh_database', 'customers') }} c

----------------------------------------

TITLE: Configuring DBT Seed Properties in YAML
DESCRIPTION: YAML configuration template for defining seed properties in DBT projects. Shows the structure for configuring seeds including descriptions, documentation settings, tests, and column-level properties. The configuration supports multiple seeds and columns with various properties like meta data, quotes, and tags.

LANGUAGE: yaml
CODE:
version: 2

seeds:
  - name: <string>
    description: <markdown_string>
    docs:
      show: true | false
      node_color: <color_id> # Use name (such as node_color: purple) or hex code with quotes (such as node_color: "#cd7f32")
    config:
      <seed_config>: <config_value>
    tests:
      - <test>
      - ... # declare additional tests
    columns:
      - name: <column name>
        description: <markdown_string>
        meta: {<dictionary>}
        quote: true | false
        tags: [<string>]
        tests:
          - <test>
          - ... # declare additional tests

      - name: ... # declare properties of additional columns

  - name: ... # declare properties of additional seeds

----------------------------------------

TITLE: Statement Block with Inline Reference
DESCRIPTION: Shows how to use statement blocks with inline model references, eliminating the need for explicit dependency declaration.

LANGUAGE: sql
CODE:
{% call statement('states', fetch_result=True) -%}

    select distinct state from {{ ref('users') }}

    /*
    The unique states are: {{ load_result('states')['data'] }}
    */

{%- endcall %}

select id * 2 from {{ ref('users') }}

----------------------------------------

TITLE: Configuring Dependencies for Downstream dbt Cloud Project
DESCRIPTION: This YAML snippet shows how to set up the dependencies.yml file in a downstream dbt Cloud project to enable cross-project references with an upstream project.

LANGUAGE: yaml
CODE:
# dependencies.yml file in dbt Cloud downstream project
projects:
- name: upstream_project_name

----------------------------------------

TITLE: Configuring Indexes for Tables in SQL for dbt-Postgres
DESCRIPTION: This snippet demonstrates how to configure indexes for a table model in dbt using SQL. It includes examples of hash and unique indexes.

LANGUAGE: sql
CODE:
{{ config(
    materialized = 'table',
    indexes=[
      {'columns': ['column_a'], 'type': 'hash'},
      {'columns': ['column_a', 'column_b'], 'unique': True},
    ]
)}}

select ...

----------------------------------------

TITLE: Key-value YAML Selector Definition
DESCRIPTION: Demonstrates key-value pair syntax for simple selector definitions.

LANGUAGE: yaml
CODE:
definition:
  tag: nightly

----------------------------------------

TITLE: Configuring meta for sources in dbt_project.yml
DESCRIPTION: Demonstrates setting meta properties for sources in the dbt_project.yml file. Meta is defined as a dictionary under the sources configuration block.

LANGUAGE: yaml
CODE:
sources:
  [<resource-path>]:
    +meta: {<dictionary>}

----------------------------------------

TITLE: Config Directory Location Check
DESCRIPTION: Shows the configured location for the profiles.yml file and exits

LANGUAGE: text
CODE:
dbt debug --config-dir
To view your profiles.yml file, run:

open /Users/alice/.dbt

----------------------------------------

TITLE: Configuring Saved Query with Exports in semantic_model.yml
DESCRIPTION: Example of a saved query configuration with exports, including metrics, group by clauses, and where conditions.

LANGUAGE: yaml
CODE:
saved_queries:
  - name: order_metrics
    description: Relevant order metrics
    config:
      tags:
        - order_metrics
    query_params:
      metrics:
        - orders
        - large_order
        - food_orders
        - order_total
      group_by:
        - Entity('order_id')
        - TimeDimension('metric_time', 'day')
        - Dimension('customer__customer_name')
        - ... # Additional group_by
      where:
        - "{{TimeDimension('metric_time')}} > current_timestamp - interval '1 week'"
         - ... # Additional where clauses
    exports:
      - name: order_metrics
        config:
          export_as: table # Options available: table, view
          alias: my_export_alias # Optional - defaults to Export name
          schema: my_export_schema_name # Optional - defaults to deployment schema

----------------------------------------

TITLE: Configuring meta for metrics in dbt_project.yml
DESCRIPTION: Demonstrates setting meta properties for metrics in the dbt_project.yml file. Meta is defined as a dictionary under the metrics configuration block.

LANGUAGE: yaml
CODE:
metrics:
  [<resource-path>]:
    +meta: {<dictionary>}

----------------------------------------

TITLE: Complex Model with Documentation Configuration
DESCRIPTION: Example of a SQL model with documentation configuration including node color and materialization settings.

LANGUAGE: sql
CODE:
{{
    config(
        materialized = 'view',
        tags=['finance'],
        docs={'node_color': 'red'}
    )
}}

with orders as (
    
    select * from {{ ref('stg_tpch_orders') }} 

),
order_item as (
    
    select * from {{ ref('order_items') }}

),
order_item_summary as (

    select 
        order_key,
        sum(gross_item_sales_amount) as gross_item_sales_amount,
        sum(item_discount_amount) as item_discount_amount,
        sum(item_tax_amount) as item_tax_amount,
        sum(net_item_sales_amount) as net_item_sales_amount
    from order_item
    group by
        1
),
final as (

    select 

        orders.order_key, 
        orders.order_date,
        orders.customer_key,
        orders.status_code,
        orders.priority_code,
        orders.clerk_name,
        orders.ship_priority,
                
        1 as order_count,                
        order_item_summary.gross_item_sales_amount,
        order_item_summary.item_discount_amount,
        order_item_summary.item_tax_amount,
        order_item_summary.net_item_sales_amount
    from
        orders
        inner join order_item_summary
            on orders.order_key = order_item_summary.order_key
)
select 
    *
from
    final

order by
    order_date

----------------------------------------

TITLE: Configuring Timestamp Strategy in SQL (dbt ‚â§1.8)
DESCRIPTION: Configuration for timestamp-based snapshot strategy using SQL/Jinja template format. Defines snapshot configuration within SQL file.

LANGUAGE: jinja2
CODE:
{% snapshot [snapshot_name](snapshot_name) %}

{{ config(
  strategy="timestamp",
  updated_at="column_name"
) }}

select ...

{% endsnapshot %}

----------------------------------------

TITLE: Creating a BigQuery Temporary UDF with SQL Header
DESCRIPTION: Demonstrates using the set_sql_header macro to create a BigQuery Temporary User-Defined Function (UDF) before model creation. This example shows how to define and use a custom function within a model.

LANGUAGE: sql
CODE:
-- Supply a SQL header:
{% call set_sql_header(config) %}
  CREATE TEMPORARY FUNCTION yes_no_to_boolean(answer STRING)
  RETURNS BOOLEAN AS (
    CASE
    WHEN LOWER(answer) = 'yes' THEN True
    WHEN LOWER(answer) = 'no' THEN False
    ELSE NULL
    END
  );
{%- endcall %}

-- Supply your model code:


select yes_no_to_boolean(yes_no) from {{ ref('other_model') }}

----------------------------------------

TITLE: Configuring Snowflake Warehouses
DESCRIPTION: Creates three warehouses (loading, transforming, reporting) with specific configurations for size, auto-suspend, and auto-resume settings.

LANGUAGE: sql
CODE:
create warehouse loading
    warehouse_size = xsmall
    auto_suspend = 3600
    auto_resume = false
    initially_suspended = true;

create warehouse transforming
    warehouse_size = xsmall
    auto_suspend = 60
    auto_resume = true
    initially_suspended = true;

create warehouse reporting
    warehouse_size = xsmall
    auto_suspend = 60
    auto_resume = true
    initially_suspended = true;

----------------------------------------

TITLE: Defining Constraints in SQL Model
DESCRIPTION: Example SQL model with a simple structure that will have constraints applied to it.

LANGUAGE: sql
CODE:
{{{
  config(
    materialized = "table"
  )
}}}

select 
  1 as id, 
  'My Favorite Customer' as customer_name, 
  cast('2019-01-01' as date) as first_transaction_date

----------------------------------------

TITLE: Project-wide Late Binding View Configuration
DESCRIPTION: YAML configuration to set all views as late-binding at the project level in dbt_project.yml

LANGUAGE: yaml
CODE:
models:
  +bind: false # Materialize all views as late-binding
  project_name:
    ....

----------------------------------------

TITLE: Configuring Late Binding Views in Redshift
DESCRIPTION: Example of creating late binding views in Redshift using dbt, which prevents views from being dropped when upstream dependencies change.

LANGUAGE: sql
CODE:
{{ config(materialized='view', bind=False) }}

select *
from source.data

----------------------------------------

TITLE: Configuring Quoting Settings in dbt_project.yml
DESCRIPTION: This YAML snippet shows how to configure quoting settings for database, schema, and identifier in the dbt_project.yml file. Each setting can be set to true or false.

LANGUAGE: yaml
CODE:
quoting:
  database: true | false
  schema: true | false
  identifier: true | false

----------------------------------------

TITLE: Setting up External Table Options for IBM Netezza
DESCRIPTION: Configuration for external table options in et_options.yml file, required for dbt seed operations. Specifies data loading parameters like skip rows, delimiter, date delimiter, and maximum errors allowed.

LANGUAGE: yaml
CODE:
- !ETOptions
    SkipRows: "1"
    Delimiter: "','"
    DateDelim: "'-'"
    MaxErrors: " 0 "

----------------------------------------

TITLE: Setting User Profile Configurations in profiles.yml (Deprecated)
DESCRIPTION: Shows how to set default values for global configurations in the profiles.yml file. This method is deprecated as of dbt v1.8 and is only applicable to older versions.

LANGUAGE: yaml
CODE:
config:
  <THIS-CONFIG>: true

----------------------------------------

TITLE: Testing Materialize Connection in dbt
DESCRIPTION: Command to verify the connection between dbt and Materialize instance.

LANGUAGE: bash
CODE:
dbt debug

----------------------------------------

TITLE: Defining Generic Test Block Limits in SQL
DESCRIPTION: Sets a default failure limit for all instances of a generic test within its test block definition. Configures the limit to 500 failures.

LANGUAGE: sql
CODE:
{% test <testname>(model, column_name) %}

{{ config(limit = 500) }}

select ...

{% endtest %}

----------------------------------------

TITLE: Snapshot Configuration in YAML Properties File
DESCRIPTION: Configuration of individual snapshots using YAML properties file, including database, schema, and strategy settings. Available in dbt v1.9+.

LANGUAGE: yaml
CODE:
snapshots:
  - name: <string>
    config:
      database: <string>
      schema: <string>
      unique_key: <column_name_or_expression>
      strategy: timestamp | check
      updated_at: <column_name>
      check_cols: [<column_name>] | all

----------------------------------------

TITLE: SQLFluff Linting Command
DESCRIPTION: Command syntax for running SQLFluff linting through the dbt Cloud CLI.

LANGUAGE: bash
CODE:
dbt sqlfluff lint [PATHS]... [flags]

----------------------------------------

TITLE: Target-Aware Database Configuration
DESCRIPTION: Example of using target variable to dynamically set database based on environment.

LANGUAGE: yml
CODE:
snapshots:
  +target_database: "{% if target.name == 'dev' %}dev{% else %}{{ target.database }}{% endif %}"

----------------------------------------

TITLE: Configuring meta for seeds in dbt_project.yml
DESCRIPTION: Demonstrates how to set meta properties for seeds in the dbt_project.yml file. Meta is defined as a dictionary under the seeds configuration block.

LANGUAGE: yaml
CODE:
seeds:
  [<resource-path>]:
    +meta: {<dictionary>}

----------------------------------------

TITLE: Accessing Custom Metadata Environment Variables in SQL
DESCRIPTION: Demonstrates how to access custom metadata environment variables in a SQL file using the dbt_metadata_envs context variable.

LANGUAGE: sql
CODE:
-- {{ dbt_metadata_envs }}

select 1 as id

----------------------------------------

TITLE: Table Clustering Configuration
DESCRIPTION: Shows how to configure table clustering in Snowflake through dbt

LANGUAGE: sql
CODE:
{{
  config(
    materialized='table',
    cluster_by=['session_start']
  )
}}

select
  session_id,
  min(event_time) as session_start,
  max(event_time) as session_end,
  count(*) as count_pageviews
from {{ source('snowplow', 'event') }}
group by 1

----------------------------------------

TITLE: CLI args method request example
DESCRIPTION: Example JSON request for running a dbt command using CLI syntax via the RPC server.

LANGUAGE: json
CODE:
{
    "jsonrpc": "2.0",
    "method": "cli_args",
    "id": "<request id>",
    "params": {
        "cli": "run --select abc+ --exclude +def",
        "task_tags": {
            "branch": "feature/my-branch",
            "commit": "c0ff33b01"
        }
    }
}

----------------------------------------

TITLE: Serializing Python Dictionary using dbt tojson Method
DESCRIPTION: Demonstrates how to use the tojson context method to convert a Python dictionary into a JSON string. The example creates a dictionary, serializes it to JSON, and logs the result.

LANGUAGE: jinja
CODE:
{% set my_dict = {"abc": 123} %}
{% set my_json_string = tojson(my_dict) %}

{% do log(my_json_string) %}

----------------------------------------

TITLE: Configuring Simple and Derived Metrics with Null Handling in YAML
DESCRIPTION: Demonstrates configuration of website visits and leads metrics, including null value handling with fill_nulls_with parameter. Shows how to create a derived metric for calculating leads to website visit ratio.

LANGUAGE: yaml
CODE:
metrics:
  - name: website_visits
    type: simple
    type_params:
      measure:
        name: bookings
  - name: leads
    type: simple
    type_params:
      measure:
        name: bookings
        fill_nulls_with: 0 # This fills null values with zero
  - name: leads_to_website_visit
    type: derived
    type_params:
      expr: leads/website_visits
      metrics:
        - name: leads
        - name: website_visits

----------------------------------------

TITLE: Configuring SQL Header for dbt Snapshots
DESCRIPTION: Shows how to set the sql_header configuration for a dbt snapshot using both inline config and dbt_project.yml. This allows injecting SQL statements before the snapshot creation.

LANGUAGE: sql
CODE:
{% snapshot [snapshot_name](snapshot_name) %}

{{ config(
  sql_header="<sql-statement>"
) }}

select ...

{% endsnapshot %}

LANGUAGE: yml
CODE:
snapshots:
  [<resource-path>](/reference/resource-configs/resource-path):
    +sql_header: <sql-statement>

----------------------------------------

TITLE: Listing Schema Tests of Incremental Models
DESCRIPTION: Example showing how to list schema tests for incremental models using materialization configuration filtering.

LANGUAGE: bash
CODE:
$ dbt ls --select config.materialized:incremental,test_type:schema
model.my_project.logs_parsed
model.my_project.events_categorized

----------------------------------------

TITLE: Disabling All Data Tests from a Package
DESCRIPTION: This snippet demonstrates how to disable all data tests from a specific package using the dbt_project.yml file.

LANGUAGE: yaml
CODE:
tests:
  package_name:
    +enabled: false

----------------------------------------

TITLE: Performance Information JSON Output from dbt Parse
DESCRIPTION: This snippet shows the structure and content of the performance information JSON file generated by the dbt parse command. It includes details about parsing time, project structure, and individual parser performance for different components of the dbt project.

LANGUAGE: json
CODE:
{
    "path_count": 7,
    "is_partial_parse_enabled": false,
    "parse_project_elapsed": 0.20151838900000008,
    "patch_sources_elapsed": 0.00039490800000008264,
    "process_manifest_elapsed": 0.029363873999999957,
    "load_all_elapsed": 0.240095269,
    "projects": [
        {
            "project_name": "my_project",
            "elapsed": 0.07518750299999999,
            "parsers": [
                {
                    "parser": "model",
                    "elapsed": 0.04545303199999995,
                    "path_count": 1
                },
                {
                    "parser": "operation",
                    "elapsed": 0.0006415469999998535,
                    "path_count": 1
                },
                {
                    "parser": "seed",
                    "elapsed": 0.026538173000000054,
                    "path_count": 2
                }
            ],
            "path_count": 4
        },
        {
            "project_name": "dbt_postgres",
            "elapsed": 0.0016448299999998195,
            "parsers": [
                {
                    "parser": "operation",
                    "elapsed": 0.00021672399999994596,
                    "path_count": 1
                }
            ],
            "path_count": 1
        },
        {
            "project_name": "dbt",
            "elapsed": 0.006580432000000025,
            "parsers": [
                {
                    "parser": "operation",
                    "elapsed": 0.0002488560000000195,
                    "path_count": 1
                },
                {
                    "parser": "docs",
                    "elapsed": 0.002500640000000054,
                    "path_count": 1
                }
            ],
            "path_count": 2
        }
    ]
}

----------------------------------------

TITLE: Debug Log File Example - dbt Initialization
DESCRIPTION: Detailed debug log output showing dbt initialization, configuration parameters, and project scanning results

LANGUAGE: text
CODE:
============================== 21:21:15.272780 | 48cef052-3819-4550-a83a-4a648aef5a31 ==============================
21:21:15.272780 [info ] [MainThread]: Running with dbt=1.5.0-b5
21:21:15.273802 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/jerco/dev/scratch/testy/logs', 'profiles_dir': '/Users/jerco/.dbt', 'version_check': 'False', 'use_colors': 'False', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
21:21:16.190990 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
21:21:16.191404 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
21:21:16.207330 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 1 analysis, 535 macros, 0 operations, 1 seed file, 0 sources, 0 exposures, 0 metrics, 0 groups

----------------------------------------

TITLE: Including Data Tests in DBT Test Command
DESCRIPTION: Command to include all data tests when running DBT tests using the --resource-type flag (available in DBT v1.9+).

LANGUAGE: text
CODE:
dbt test --resource-type test

----------------------------------------

TITLE: Running DBT Tests with Select Options (Pre-1.8)
DESCRIPTION: Examples of running dbt tests with various selection criteria for versions before 1.8. Shows how to select specific models, packages, and test types (singular or generic).

LANGUAGE: bash
CODE:
# run tests for one_specific_model
dbt test --select "one_specific_model"

# run tests for all models in package
dbt test --select "some_package.*"

# run only tests defined singularly
dbt test --select "test_type:singular"

# run only tests defined generically
dbt test --select "test_type:generic"

# run singular tests limited to one_specific_model
dbt test --select "one_specific_model,test_type:singular"

# run generic tests limited to one_specific_model
dbt test --select "one_specific_model,test_type:generic"

----------------------------------------

TITLE: Setting on_configuration_change in properties.yml
DESCRIPTION: Shows how to set the on_configuration_change option in a model's properties.yml file. This configuration applies to a specific named model.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: [<model-name>]
    config:
      [materialized]: <materialization_name>
      on_configuration_change: apply | continue | fail

----------------------------------------

TITLE: Creating Snowflake Network Rule for dbt Cloud Access
DESCRIPTION: SQL command to create a network rule in Snowflake allowing access from dbt Cloud. This is part of configuring network policies to restrict access to your Snowflake account.

LANGUAGE: sql
CODE:
CREATE NETWORK RULE allow_dbt_cloud_access
  MODE = INGRESS
  TYPE = AWSVPCEID
  VALUE_LIST = ('<VPCE_ID>'); -- Replace '<VPCE_ID>' with the actual ID provided

----------------------------------------

TITLE: Configuring Model Alias in dbt_project.yml
DESCRIPTION: Sets a custom alias for a sales_total model at the project level using dbt_project.yml configuration.

LANGUAGE: yaml
CODE:
models:
  your_project:
    sales_total:
      +alias: sales_dashboard

----------------------------------------

TITLE: Installing dbt Core and Adapter
DESCRIPTION: Commands for installing dbt Core and database adapter plugins using pip.

LANGUAGE: shell
CODE:
python -m pip install dbt-core dbt-ADAPTER_NAME

LANGUAGE: shell
CODE:
python -m pip install dbt-core dbt-postgres

----------------------------------------

TITLE: Advanced + Prefix Usage with Dictionaries and Resource Paths
DESCRIPTION: Illustrates complex scenarios where + prefix is required for dictionary configs and when config keys overlap with resource paths. Shows configuration for persist_docs and tags with proper disambiguation.

LANGUAGE: yaml
CODE:
name: jaffle_shop
config-version: 2

...

models:
  +persist_docs:
    relation: true
    columns: true

  jaffle_shop:
    schema: my_schema
    +tags:
      - "hello"
    tags:
      materialized: view

----------------------------------------

TITLE: DBT Cents to Dollars Macro Documentation in YAML
DESCRIPTION: YAML configuration documenting the cents_to_dollars macro arguments including their types and descriptions.

LANGUAGE: yaml
CODE:
version: 2

macros:
  - name: cents_to_dollars
    arguments:
      - name: column_name
        type: column name or expression
        description: "The name of a column, or an expression ‚Äî anything that can be `select`-ed as a column"

      - name: scale
        type: integer
        description: "The number of decimal places to round to. Default is 2."


----------------------------------------

TITLE: Querying Column-Level Information in GraphQL for dbt
DESCRIPTION: This GraphQL query fetches detailed column-level information for a specific model within a dbt job. It includes column properties such as name, index, type, comment, description, tags, and meta. Note that this query only works if the job has generated documentation using the 'dbt docs generate' command.

LANGUAGE: graphql
CODE:
{
  job(id: 123) {
    model(uniqueId: "model.jaffle_shop.dim_user") {
      columns {
        name
        index
        type
        comment
        description
        tags
        meta
      }
    }
  }
}

----------------------------------------

TITLE: Creating Version Logging Macro in dbt
DESCRIPTION: A dbt macro that logs the currently installed version of dbt using the dbt_version variable. This macro can be used for debugging and version verification purposes.

LANGUAGE: sql
CODE:
{% macro get_version() %}

  {% do log("The installed version of dbt is: " ~ dbt_version, info=true) %}

{% endmacro %}

----------------------------------------

TITLE: Configuring Model Contract in YAML
DESCRIPTION: Example YAML configuration for a dbt model with enforced contract and disabled type aliasing.

LANGUAGE: yaml
CODE:
models:
  - name: my_model
    config:
      contract:
        enforced: true
        alias_types: false  # true by default

----------------------------------------

TITLE: Creating External Catalog for Hive in Starrocks
DESCRIPTION: This SQL snippet demonstrates how to create an external catalog in Starrocks for Hive. It specifies the Hive metastore URI and sets the catalog type to 'hive'.

LANGUAGE: sql
CODE:
CREATE EXTERNAL CATALOG `hive_catalog`
PROPERTIES (
    "hive.metastore.uris"  =  "thrift://127.0.0.1:8087",
    "type"="hive"
);

----------------------------------------

TITLE: Granting Schema Privileges using DBT Schemas Variable in YAML
DESCRIPTION: Example configuration showing how to use the 'schemas' variable in dbt_project.yml to grant usage and select privileges on schemas to a reporter group. This implementation uses Redshift-specific syntax to grant schema access, table selection rights, and default privileges for future tables.

LANGUAGE: yaml
CODE:
...

on-run-end:
  - "{% for schema in schemas%}grant usage on schema {{ schema }} to group reporter;{% endfor%}"
  - "{% for schema in schemas %}grant select on all tables in schema {{ schema }} to group reporter;{% endfor%}"
  - "{% for schema in schemas %}alter default privileges in schema {{ schema }}  grant select on tables to group reporter;{% endfor %}"

----------------------------------------

TITLE: Creating Custom Indices in SQL Server
DESCRIPTION: Example of creating clustered and non-clustered indices using post-hook configurations with custom macros.

LANGUAGE: sql
CODE:
{{
    config({
        "as_columnstore": false,
        "materialized": 'table',
        "post-hook": [
            "{{ create_clustered_index(columns = ['row_id', 'row_id_complement'], unique=True) }}",
            "{{ create_nonclustered_index(columns = ['modified_date']) }}",
            "{{ create_nonclustered_index(columns = ['row_id'], includes = ['modified_date']) }}",
        ]
    })

}}

select *
from ...

----------------------------------------

TITLE: Calculating Sum of Order Amounts by Customer in SQL
DESCRIPTION: This SQL query demonstrates how to use the SUM function to calculate the total order amount for each customer from the 'orders' table. It groups the results by customer_id and limits the output to 3 rows.

LANGUAGE: sql
CODE:
select
	customer_id,
	sum(order_amount) as all_orders_amount
from {{ ref('orders') }}
group by 1
limit 3

----------------------------------------

TITLE: Granting Schema Privileges with On-Run-End Hook
DESCRIPTION: Example of using on-run-end hook to grant usage privileges on all schemas to a reporter group. This utilizes the schemas variable that's only available in on-run-end context.

LANGUAGE: yml
CODE:
on-run-end:
  - "{% for schema in schemas %}grant usage on schema {{ schema }} to group reporter; {% endfor %}"

----------------------------------------

TITLE: Configuring Unique Key in Project YAML
DESCRIPTION: Example of setting a unique key in the dbt_project.yml file for all models in a specific path.

LANGUAGE: yaml
CODE:
name: jaffle_shop

models:
  jaffle_shop:
    staging:
      +unique_key: id

----------------------------------------

TITLE: Creating dbt Docs Blocks Structure
DESCRIPTION: Example of docs blocks markdown structure for documenting reusable column descriptions.

LANGUAGE: markdown
CODE:
{% docs activity_based_interest__id %}  

Primary key of the table. See sql for key definition.

{% enddocs %}

{% docs activity_based_interest__user_id %}  

The internal company id for a given user.

{% enddocs %}

----------------------------------------

TITLE: Setting dbt_valid_to_current in SQL snapshot
DESCRIPTION: SQL configuration block for setting dbt_valid_to_current along with other snapshot parameters

LANGUAGE: sql
CODE:
{{
    config(
        unique_key='id',
        strategy='timestamp',
        updated_at='updated_at',
        dbt_valid_to_current='string'
    )
}}

----------------------------------------

TITLE: SQL LOWER Function with Customer Data Example
DESCRIPTION: Practical example showing how to use LOWER function to convert customer first and last names to lowercase in a dbt model.

LANGUAGE: sql
CODE:
select 
	customer_id,
	lower(first_name) as first_name,
	lower(last_name) as last_name
from {{ ref('customers') }}

----------------------------------------

TITLE: Adding Descriptions to Generic and Singular Tests
DESCRIPTION: This snippet demonstrates how to add descriptions to both generic and singular tests in YAML files, which is available from dbt v1.9.

LANGUAGE: yaml
CODE:
models:
  - name: my_model
    columns:
      - name: delivery_status
        tests:
          - accepted_values:
              values: ['delivered', 'pending', 'failed']
              description: "This test checks whether there are unexpected delivery statuses. If it fails, check with logistics team"

LANGUAGE: yaml
CODE:
data_tests: 
  - name: my_custom_test
    description: "This test checks whether the rolling average of returns is inside of expected bounds. If it isn't, flag to customer success team"

----------------------------------------

TITLE: Setting Project-Level Materialization in DBT
DESCRIPTION: Project-level configuration for materializations in dbt_project.yml, showing how to set default materializations for different model groups.

LANGUAGE: yaml
CODE:
models:
  your_project_name:
    materialized: view
    staging:
      materialized: table

----------------------------------------

TITLE: Hook Configuration with Nested Curlies in dbt
DESCRIPTION: Valid example of using nested curly braces in dbt hooks, which is an exception to the general rule

LANGUAGE: jinja
CODE:
{{ config(post_hook="grant select on {{ this }} to role bi_role") }}

----------------------------------------

TITLE: Schema Authorization in Redshift
DESCRIPTION: SQL command for creating and authorizing a schema for test results in Redshift

LANGUAGE: sql
CODE:
create schema if not exists dev_username_dbt_test__audit authorization username;

----------------------------------------

TITLE: Setting Default Package Installation Path in dbt
DESCRIPTION: Demonstrates the default configuration for package installation directory in dbt_project.yml. By default, packages are installed in the dbt_packages directory.

LANGUAGE: yaml
CODE:
packages-install-path: dbt_packages

----------------------------------------

TITLE: Versioned Model Configuration
DESCRIPTION: YAML configuration for versioned models in dbt with version specifications.

LANGUAGE: yaml
CODE:
models:
  - name: model_name
    latest_version: 2
    versions:
      - v: 2
      - v: 1

----------------------------------------

TITLE: Basic SQL Filtering with Not Equal
DESCRIPTION: Simple example showing how to filter out employee orders from an orders table using direct comparison.

LANGUAGE: sql
CODE:
select * from {{ source('backend_db', 'orders') }}
where status != 'employee_order'

----------------------------------------

TITLE: Configuring Schema for Individual Models
DESCRIPTION: Example of setting a custom schema directly in a model file using a config block

LANGUAGE: sql
CODE:
{{ config(
    schema='marketing'
) }}

----------------------------------------

TITLE: Recommended Relative Path Configuration
DESCRIPTION: Recommended approach using relative paths for snapshot directory configuration. Shows the proper way to specify the default snapshots directory.

LANGUAGE: yml
CODE:
snapshot-paths: ["snapshots"]

----------------------------------------

TITLE: Configuring defer-env-id in dbt_cloud.yml
DESCRIPTION: Shows how to set the defer-env-id in the dbt_cloud.yml file to manually specify the source environment for deferral artifacts when using the dbt Cloud CLI.

LANGUAGE: yaml
CODE:
context:
  active-host: ...
  active-project: ...
  defer-env-id: '123456'

----------------------------------------

TITLE: Configuring persist_docs for Snapshots in dbt_project.yml
DESCRIPTION: This snippet demonstrates how to enable persist_docs for snapshots in the dbt_project.yml file, allowing documentation persistence for both relations and columns.

LANGUAGE: yml
CODE:
snapshots:
  [<resource-path>]:
    +persist_docs:
      relation: true
      columns: true

----------------------------------------

TITLE: DBT Cents to Dollars Conversion Macro in SQL
DESCRIPTION: SQL macro that converts a numeric value from cents to dollars with configurable decimal precision.

LANGUAGE: sql
CODE:
{% macro cents_to_dollars(column_name, scale=2) %}
    ({{ column_name }} / 100)::numeric(16, {{ scale }})
{% endmacro %}


----------------------------------------

TITLE: Deserializing JSON to Python Object with fromjson in dbt
DESCRIPTION: This snippet demonstrates how to use the fromjson context method to convert a JSON string into a Python dictionary. It then logs a value from the resulting dictionary.

LANGUAGE: jinja
CODE:
{% set my_json_str = '{"abc": 123}' %}
{% set my_dict = fromjson(my_json_str) %}

{% do log(my_dict['abc']) %}

----------------------------------------

TITLE: Configuring SQL Copy Job in dbt
DESCRIPTION: Shows the configuration for a SQL copy job as a dbt model using the 'incremental' materialization type.

LANGUAGE: sql
CODE:
{{ config(  materialized='incremental',
            sync=True|False,
            source = 'S3'| 'KAFKA' | ... ,
            options={
              'option_name': 'option_value'
            },
            partition_by=[{}]
          )
}}
SELECT * FROM {{ ref(<model>) }}

----------------------------------------

TITLE: Setting Session Properties with Pre-Hook in SQL
DESCRIPTION: Example of using a dbt pre-hook to set session properties for a specific model, demonstrating how to set query max run time.

LANGUAGE: sql
CODE:
{{
  config(
    pre_hook="set session query_max_run_time='10m'"
  )
}}

----------------------------------------

TITLE: Basic SQL LEFT JOIN Syntax
DESCRIPTION: Basic syntax template for performing a LEFT JOIN operation between two tables. Shows the fundamental structure with placeholder fields and table names.

LANGUAGE: sql
CODE:
select
    <fields>
from <table_1> as t1
left join <table_2> as t2
on t1.id = t2.id

----------------------------------------

TITLE: Sample dbt Environment Show Output
DESCRIPTION: Example output of the dbt environment show command displaying local configuration, cloud configuration, and Snowflake connection details.

LANGUAGE: bash
CODE:
‚ùØ dbt env show
Local Configuration:
  Active account ID              185854
  Active project ID             271692
  Active host name              cloud.getdbt.com
  dbt_cloud.yml file path       /Users/cesar/.dbt/dbt_cloud.yml
  dbt_project.yml file path     /Users/cesar/git/cloud-cli-test-project/dbt_project.yml
  dbt Cloud CLI version         0.35.7
  OS info                       darwin arm64

Cloud Configuration:
  Account ID                    185854
  Project ID                    271692
  Project name                  Snowflake
  Environment ID                243762
  Environment name              Development
  Defer environment ID          [N/A]
  dbt version                   1.6.0-latest
  Target name                   default
  Connection type               snowflake

Snowflake Connection Details:
  Account                       ska67070
  Warehouse                     DBT_TESTING_ALT
  Database                      DBT_TEST
  Schema                        CLOUD_CLI_TESTING
  Role                          SYSADMIN
  User                          dbt_cloud_user
  Client session keep alive     false

----------------------------------------

TITLE: Configuring Unique Key in Properties YAML
DESCRIPTION: Example of setting a unique key in the models/properties.yml file for an incremental model.

LANGUAGE: yaml
CODE:
models:
  - name: my_incremental_model
    description: "An incremental model example with a unique key."
    config:
      materialized: incremental
      unique_key: id

----------------------------------------

TITLE: Configuring Specific Generic Test Limits in YAML
DESCRIPTION: Sets a failure limit for a specific instance of a generic test applied to a column. Demonstrates how to configure the limit parameter for accepted_values test on a specific column.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: large_table
    columns:
      - name: very_unreliable_column
        tests:
          - accepted_values:
              values: ["a", "b", "c"]
              config:
                limit: 1000  # will only include the first 1000 failures

----------------------------------------

TITLE: Dropping Relation in SQL using dbt adapter
DESCRIPTION: This snippet shows how to use the `adapter.drop_relation` method to drop a relation in the database.

LANGUAGE: sql
CODE:
{% do adapter.drop_relation(this) %}

----------------------------------------

TITLE: Cross-Project Model Reference in SQL
DESCRIPTION: Example of how to reference a model from another project using the two-argument ref syntax in a SQL model file.

LANGUAGE: sql
CODE:
with monthly_revenue as (
  
    select * from {{ ref('jaffle_finance', 'monthly_revenue') }}

),

...

----------------------------------------

TITLE: Batch Size Configuration Macro
DESCRIPTION: Custom macro to adjust the batch size for prepared statements in dbt-trino adapter.

LANGUAGE: sql
CODE:
{% macro trino__get_batch_size() %}
  {{ return(10000) }} -- Adjust this number as you see fit
{% endmacro %}

----------------------------------------

TITLE: Implementing Model Versions in dbt
DESCRIPTION: Shows how to define and manage different versions of a model using YAML configuration in dbt.

LANGUAGE: yaml
CODE:
models:
  - name: dim_customers
    latest_version: 2
    config:
      contract:
        enforced: true
    columns:
      - name: id
        data_type: integer
        description: hello
        constraints:
          - type: not_null
          - type: primary_key
          - type: check
            expression: "id > 0"
        tests:
          - unique
      - name: customer_name
        data_type: text
      - name: first_transaction_date
        data_type: date
    versions:
      - v: 2
        columns: 
          - include: '*'
            exclude: ['first_transaction_date']
      - v: 1
        columns:
          - include: '*'
        defined_in: dim_customers

----------------------------------------

TITLE: Basic SQL MIN Function Syntax
DESCRIPTION: The fundamental syntax for using the MIN aggregate function in SQL queries to find the minimum value of a field.

LANGUAGE: sql
CODE:
min(<field_name>)

----------------------------------------

TITLE: Return Expression in dbt Macro
DESCRIPTION: Example of using return as an expression in a dbt macro to output a list directly as a string. The macro returns a simple list [1,2,3] that can be used in SQL templates.

LANGUAGE: sql
CODE:
{% macro get_data() %}

  {{ return([1,2,3]) }}
  
{% endmacro %}

----------------------------------------

TITLE: Loading Statement Results in dbt
DESCRIPTION: Example of loading and accessing statement results using the load_result function.

LANGUAGE: sql
CODE:
{%- set states = load_result('states') -%}
{%- set states_data = states['data'] -%}
{%- set states_status = states['response'] -%}

----------------------------------------

TITLE: Configuring Target Schema in dbt_project.yml
DESCRIPTION: Sets the target schema for snapshots in the dbt_project.yml file. This configuration applies to all snapshots unless overridden.

LANGUAGE: yaml
CODE:
snapshots:
  [<resource-path>]:
    +target_schema: string

----------------------------------------

TITLE: Configuring Dict Format Mock Data in DBT Unit Tests
DESCRIPTION: Example of using the default dictionary format for providing inline mock data in dbt unit tests. Shows how to structure test cases with dictionary rows containing key-value pairs.

LANGUAGE: yaml
CODE:
unit_tests:
  - name: test_my_model
    model: my_model
    given:
      - input: ref('my_model_a')
        format: dict
        rows:
          - {id: 1, name: gerda}
          - {id: 2, b: michelle}

----------------------------------------

TITLE: Installing Astro CLI with Homebrew
DESCRIPTION: Command to install the Astro CLI using Homebrew package manager. This CLI is used for working with Airflow locally.

LANGUAGE: bash
CODE:
brew install astro

----------------------------------------

TITLE: Configuring Model Enablement with as_bool Filter in dbt_project.yml
DESCRIPTION: Demonstrates how to use the as_bool filter to conditionally enable/disable models based on the target environment. The filter coerces the result of a target name comparison into a boolean value.

LANGUAGE: yml
CODE:
models:
  my_project:
    for_export:
      enabled: "{{ (target.name == 'prod') | as_bool }}"

----------------------------------------

TITLE: Configuring Information Schema for Columns in Databricks
DESCRIPTION: Flag that determines whether to use information_schema instead of describe extended for retrieving column metadata in Unity Catalog tables. Default is False. Using True improves handling of complex struct types but requires additional REPAIR TABLE query.

LANGUAGE: markdown
CODE:
use_info_schema_for_columns: False

----------------------------------------

TITLE: Setting Session Properties in dbt for IBM watsonx.data Presto
DESCRIPTION: Demonstrates how to set session properties for a specific dbt model using a pre-hook. This example sets the query_max_run_time to 10 minutes.

LANGUAGE: sql
CODE:
{{
  config(
    pre_hook="set session query_max_run_time='10m'"
  )
}}

----------------------------------------

TITLE: Configuring use_fastload for Teradata Seeds in dbt
DESCRIPTION: Enables the use of fastload for handling dbt seed commands, which can speed up loading for large seed files.

LANGUAGE: yaml
CODE:
seeds:
  <project-name>:
    +use_fastload: true

----------------------------------------

TITLE: Configuring Snapshots with Custom Hash UDF in Teradata
DESCRIPTION: Sets up a snapshot configuration using a custom hash UDF for generating unique hash values in the dbt_scd_id column.

LANGUAGE: sql
CODE:
{% snapshot snapshot_example %}
{{
  config(
    target_schema='snapshots',
    unique_key='id',
    strategy='check',
    check_cols=["c2"],
    snapshot_hash_udf='GLOBAL_FUNCTIONS.hash_md5'
  )
}}
select * from {{ ref('order_payments') }}
{% endsnapshot %}

----------------------------------------

TITLE: Querying Orders with SQL ANY Operator
DESCRIPTION: This SQL query demonstrates the use of the ANY operator with LIKE to filter orders based on their status. It returns orders whose status starts with 'return' or 'ship'.

LANGUAGE: sql
CODE:
select
    order_id,
    status
from {{ ref('orders') }}
where status like any ('return%', 'ship%')

----------------------------------------

TITLE: Basic SQL UPPER Function Syntax
DESCRIPTION: Basic syntax for using the UPPER function to convert a string column to uppercase.

LANGUAGE: sql
CODE:
upper(<string_column>)

----------------------------------------

TITLE: Advanced Macro for JSON Query Comments in dbt
DESCRIPTION: Presents an advanced Jinja macro that generates a JSON query comment with detailed information about the dbt execution context.

LANGUAGE: jinja2
CODE:
{% macro query_comment(node) %}
    {%- set comment_dict = {} -%}
    {%- do comment_dict.update(
        app='dbt',
        dbt_version=dbt_version,
        profile_name=target.get('profile_name'),
        target_name=target.get('target_name'),
    ) -%}
    {%- if node is not none -%}
      {%- do comment_dict.update(
        file=node.original_file_path,
        node_id=node.unique_id,
        node_name=node.name,
        resource_type=node.resource_type,
        package_name=node.package_name,
        relation={
            "database": node.database,
            "schema": node.schema,
            "identifier": node.identifier
        }
      ) -%}
    {% else %}
      {%- do comment_dict.update(node_id='internal') -%}
    {%- endif -%}
    {% do return(tojson(comment_dict)) %}
{% endmacro %}

----------------------------------------

TITLE: Defining Table Indexes in SingleStore
DESCRIPTION: Demonstrates how to configure multiple indexes with different types on a SingleStore table.

LANGUAGE: sql
CODE:
{{
    config(
        materialized='table',
        shard_key=['id'],
        indexes=[{'columns': ['order_date', 'id']}, {'columns': ['status'], 'type': 'hash'}]
    )
}}

select ...

----------------------------------------

TITLE: Complex Selector with Exclude Logic
DESCRIPTION: Example of a complex selector definition using union and intersection operators with exclude functionality.

LANGUAGE: yaml
CODE:
selectors:
  - name: nightly_diet_snowplow
    description: "Non-incremental Snowplow models that power nightly exports"
    definition:
      union:
        - intersection:
            - method: source
              value: snowplow
              childrens_parents: true
            - method: tag
              value: nightly
        - method: path
          value: models/export
        - exclude:
            - intersection:
                - method: package
                  value: snowplow
                - method: config.materialized
                  value: incremental
            - method: fqn
              value: export_performance_timing

----------------------------------------

TITLE: Creating Customer Model in dbt
DESCRIPTION: dbt SQL model that transforms and combines customer and order data to create a consolidated customer view with order history metrics.

LANGUAGE: sql
CODE:
with customers as (

select
    ID as customer_id,
    FIRST_NAME as first_name,
    LAST_NAME as last_name

from dbo.customers
),

orders as (

    select
        ID as order_id,
        USER_ID as customer_id,
        ORDER_DATE as order_date,
        STATUS as status

    from dbo.orders
),

customer_orders as (

    select
        customer_id,
        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders
    from orders
    group by customer_id
),

final as (
    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders
    from customers
    left join customer_orders on customers.customer_id = customer_orders.customer_id
)

select * from final

----------------------------------------

TITLE: Configuring Model Materialization in dbt Properties.yml
DESCRIPTION: Example showing how to configure conditional model materialization based on target environment. Uses Jinja templating to set materialization as view in development and table in production/CI contexts.

LANGUAGE: yaml
CODE:
# Configure this model to be materialized as a view
# in development and a table in production/CI contexts

models:
  - name: dim_customers
    config:
      materialized: "{{ 'view' if target.name == 'dev' else 'table' }}"

----------------------------------------

TITLE: Setting Default File Format Configurations in dbt Project YAML
DESCRIPTION: This snippet shows how to set default file format configurations for models, seeds, and snapshots in the dbt project YAML file.

LANGUAGE: yaml
CODE:
models:
  +file_format: delta # or iceberg or hudi
  
seeds:
  +file_format: delta # or iceberg or hudi
  
snapshots:
  +file_format: delta # or iceberg or hudi

----------------------------------------

TITLE: Setting Secrets in fly.io App
DESCRIPTION: Set required secrets for the application using the flyctl secrets set command, including dbt Cloud tokens and Datadog API information.

LANGUAGE: shell
CODE:
flyctl secrets set DBT_CLOUD_SERVICE_TOKEN=abc123 DBT_CLOUD_AUTH_TOKEN=def456 DD_API_KEY=ghi789 DD_SITE=datadoghq.com

----------------------------------------

TITLE: Return Statement with Do Tag in dbt Macro
DESCRIPTION: Example of using return as a statement with a do tag in a dbt macro. This approach executes the return function without generating an output string, useful for operations that don't need direct template insertion.

LANGUAGE: sql
CODE:
{% macro get_data() %}

  {% do return([1,2,3]) %}
  
{% endmacro %}

----------------------------------------

TITLE: Recommended Test Path Configuration in dbt_project.yml
DESCRIPTION: Demonstrates the recommended way to specify test paths using relative paths. This approach is preferred over using absolute paths for better portability.

LANGUAGE: yml
CODE:
test-paths: ["test"]

----------------------------------------

TITLE: Source Reference in SQL Model
DESCRIPTION: Example of how to reference a source in a dbt SQL model using the source() function.

LANGUAGE: sql
CODE:
select * from {{ source('jaffle_shop', 'orders') }}

----------------------------------------

TITLE: Configuring Non-Columnstore Table in SQL Server
DESCRIPTION: Example of disabling columnstore materialization for a specific model by setting as_columnstore to false.

LANGUAGE: sql
CODE:
{{
    config(
        as_columnstore=false
        )
}}

select *
from ...

----------------------------------------

TITLE: Configuring OAuth Console Authentication for Starburst/Trino in dbt
DESCRIPTION: Example YAML configuration for setting up an OAuth Console-authenticated connection to a Starburst/Trino cluster in dbt's profiles.yml file. Includes essential connection parameters and OAuth Console-specific settings.

LANGUAGE: yaml
CODE:
sandbox-galaxy:
  target: oauth_console
  outputs:
    oauth:
      type: trino
      method: oauth_console
      host: bunbundersders.trino.galaxy-dev.io
      catalog: dbt_target
      schema: dataders
      port: 443

----------------------------------------

TITLE: Git Commit Commands
DESCRIPTION: Commands to commit and push changes to GitHub repository.

LANGUAGE: shell
CODE:
git add 
git commit -m "Your commit message"
git push

----------------------------------------

TITLE: Configuring Updated_at in SQL for dbt 1.8 and Earlier
DESCRIPTION: Example of configuring the updated_at parameter for a snapshot using SQL in dbt versions 1.8 and earlier. It sets the strategy to timestamp and specifies the updated_at column within a Jinja config block.

LANGUAGE: jinja2
CODE:
{{ config(
  strategy="timestamp",
  updated_at="column_name"
) }}

----------------------------------------

TITLE: Defining a Docs Block in Jinja2
DESCRIPTION: This snippet shows how to define a docs block named 'orders' in a Markdown file using Jinja2 syntax. The block contains arbitrary documentation content.

LANGUAGE: jinja2
CODE:
{% docs orders %}

# docs
- go
- here
 
{% enddocs %}

----------------------------------------

TITLE: Applying a Predictor in dbt for MindsDB
DESCRIPTION: This snippet demonstrates how to apply a predictor in dbt for MindsDB. It uses the 'table' materialization to create or replace a table in the selected integration with the results of the predictor.

LANGUAGE: sql
CODE:
    {{ config(materialized='table', predictor_name='TEST_PREDICTOR_NAME', integration='photorep') }}
        select a, bc from ddd where name > latest

----------------------------------------

TITLE: Using Inline CSV Format in DBT Unit Tests
DESCRIPTION: Demonstrates how to provide mock data using inline CSV format. The rows parameter contains a CSV string with headers and data rows.

LANGUAGE: yaml
CODE:
unit_tests:
  - name: test_my_model
    model: my_model
    given:
      - input: ref('my_model_a')
        format: csv
        rows: |
          id,name
          1,gerda
          2,michelle

----------------------------------------

TITLE: Cloning the dbt dimensional modeling repository
DESCRIPTION: Command to clone the GitHub repository containing the dbt project for dimensional modeling.

LANGUAGE: shell
CODE:
git clone https://github.com/Data-Engineer-Camp/dbt-dimensional-modelling.git
cd dbt-dimensional-modelling/adventureworks

----------------------------------------

TITLE: Configuring Column Properties for DBT Analyses
DESCRIPTION: YAML configuration for defining column properties within DBT analyses, with simplified column attributes focused on description and data type.

LANGUAGE: yaml
CODE:
version: 2

analyses:
  - name: <analysis_name>
    columns:
      - name: <column_name>
        description: <markdown_string>
        data_type: <string>
      - name: <another_column>

----------------------------------------

TITLE: Example: Setting Target Schema for All Snapshots
DESCRIPTION: Demonstrates how to set a common target schema named 'snapshots' for all snapshot tables in the project.

LANGUAGE: yaml
CODE:
snapshots:
  +target_schema: snapshots

----------------------------------------

TITLE: Concatenating Customer Names Using SQL CONCAT
DESCRIPTION: Example showing how to combine first and last names into a full name using the CONCAT function. This query references a customers table from the Jaffle Shop example and adds a space between the name components.

LANGUAGE: sql
CODE:
select
	user_id,
	first_name,
	last_name,
	concat(first_name, ' ', last_name) as full_name
from {{ ref('customers') }}
limit 3

----------------------------------------

TITLE: Listing dbt Core and Adapter Versions for December 2024 Release
DESCRIPTION: This code block lists the specific versions of dbt Core, shared interfaces, and adapters included in the December 2024 Compatible release of dbt Cloud.

LANGUAGE: yaml
CODE:
dbt-core==1.9.0

# shared interfaces
dbt-adapters==1.10.4
dbt-common==1.14.0
dbt-semantic-interfaces==0.7.4

# adapters
dbt-athena==1.9.0
dbt-bigquery==1.9.0
dbt-databricks==1.9.0
dbt-fabric==1.8.8
dbt-postgres==1.9.0
dbt-redshift==1.9.0
dbt-snowflake==1.9.0
dbt-spark==1.9.0
dbt-synapse==1.8.2
dbt-teradata==1.8.2
dbt-trino==1.8.5

----------------------------------------

TITLE: Configuring Basic Query Comment in dbt YAML
DESCRIPTION: Demonstrates how to set a simple string query comment in the dbt_project.yml file.

LANGUAGE: yaml
CODE:
query-comment: string

----------------------------------------

TITLE: Execute HTTP POST Request to Discovery API
DESCRIPTION: Example of making a curl request to the Discovery API with proper authorization headers and query body.

LANGUAGE: shell
CODE:
curl 'YOUR_API_URL' \
  -H 'authorization: Bearer YOUR_TOKEN' \
  -H 'content-type: application/json'
  -X POST
  --data QUERY_BODY

----------------------------------------

TITLE: Setting flags via command line options
DESCRIPTION: Examples of setting the 'fail_fast' flag using command line options. This sets the flag for a specific invocation of dbt.

LANGUAGE: bash
CODE:
dbt run --fail-fast # set to True for this specific invocation
dbt run --no-fail-fast # set to False

----------------------------------------

TITLE: Configuring Out-of-the-Box Generic Test Severity in YAML
DESCRIPTION: This snippet demonstrates how to configure a specific instance of an out-of-the-box generic test in dbt. It sets the severity, error threshold, and warning threshold for a unique test on a column.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: large_table
    columns:
      - name: slightly_unreliable_column
        tests:
          - unique:
              config:
                severity: error
                error_if: ">1000"
                warn_if: ">10"

----------------------------------------

TITLE: Configuring an insert_overwrite incremental strategy in SQL
DESCRIPTION: Example of configuring an insert_overwrite incremental strategy with partitioning in a dbt SQL model file

LANGUAGE: sql
CODE:
{{ config(
    materialized='incremental',
    partition_by=['date_day'],
    file_format='parquet'
) }}

with new_events as (

    select * from {{ ref('events') }}

    {% if is_incremental() %}
    where date_day >= date_add(current_date, -1)
    {% endif %}

)

select
    date_day,
    count(*) as users

from new_events
group by 1

----------------------------------------

TITLE: Configuring dbt_valid_to_current in schema.yml
DESCRIPTION: YAML configuration for setting dbt_valid_to_current in a snapshot's schema file

LANGUAGE: yaml
CODE:
snapshots:
  - name: my_snapshot
    config:
      dbt_valid_to_current: "string"


----------------------------------------

TITLE: Selecting Resources by Result Status in dbt
DESCRIPTION: This command demonstrates how to use the result selector to run dbt on resources with a specific status from a previous execution. It also shows how to use the --defer and --state flags for state comparison.

LANGUAGE: bash
CODE:
dbt run --select "result:<status>" --defer --state path/to/prod/artifacts

----------------------------------------

TITLE: Creating Redshift Schemas
DESCRIPTION: SQL commands to create the required schemas for the jaffle_shop and stripe data

LANGUAGE: sql
CODE:
create schema if not exists jaffle_shop;
create schema if not exists stripe;

----------------------------------------

TITLE: Installing dbt-core in Editable Mode
DESCRIPTION: Commands to install dbt-core in editable mode, allowing local changes to be reflected immediately without reinstallation.

LANGUAGE: shell
CODE:
python -m pip install -e editable-requirements.txt

----------------------------------------

TITLE: IP Address Configuration for dbt Cloud Migration
DESCRIPTION: List of new and old IP addresses that need to be configured in firewalls and database grants during the migration process. New IPs must be added before migration, and old IPs should be removed after migration is complete.

LANGUAGE: plaintext
CODE:
New IPs to add:
52.3.77.232
3.214.191.130
34.233.79.135

Old IPs to remove post-migration:
52.45.144.63
54.81.134.249
52.22.161.231

----------------------------------------

TITLE: Configuring File Exclusions with .dbtignore
DESCRIPTION: Example patterns for excluding files and directories from dbt processing using .dbtignore file. Shows common patterns for ignoring Python files, specific directories, and pattern-based exclusions.

LANGUAGE: markdown
CODE:
# .dbtignore

# ignore individual .py files
not-a-dbt-model.py
another-non-dbt-model.py

# ignore all .py files
**.py

# ignore all .py files with "codegen" in the filename
*codegen*.py

# ignore all folders in a directory
path/to/folders/**

# ignore some folders in a directory
path/to/folders/subfolder/**

----------------------------------------

TITLE: Legacy Check Columns Configuration (DBT ‚â§1.8)
DESCRIPTION: Jinja2/SQL configuration for check columns in DBT snapshots using the pre-1.9 syntax. Shows how to set check columns within a config block.

LANGUAGE: jinja2
CODE:
{{ config(
  strategy="check",
  check_cols=["column_name"]
) }}

----------------------------------------

TITLE: Defining Data Types for Redshift in dbt Unit Tests
DESCRIPTION: This snippet shows how to specify various data types including integer, float, string, date, timestamp, and JSON in a dbt unit test for Redshift. Arrays are not currently supported.

LANGUAGE: yaml
CODE:
unit_tests:
  - name: test_my_data_types
    model: fct_data_types
    given:
      - input: ref('stg_data_types')
        rows:
         - int_field: 1
           float_field: 2.0
           str_field: my_string
           str_escaped_field: "my,cool'string"
           date_field: 2020-01-02
           timestamp_field: 2013-11-03 00:00:00-0
           timestamptz_field: 2013-11-03 00:00:00-0
           json_field: '{"bar": "baz", "balance": 7.77, "active": false}'

----------------------------------------

TITLE: Converting UPDATE to dbt Model
DESCRIPTION: Demonstrates converting an UPDATE statement into a dbt model using CASE statements and column selection

LANGUAGE: sql
CODE:
UPDATE orders

SET type = 'return'

WHERE total < 0

LANGUAGE: sql
CODE:
SELECT
    CASE
        WHEN total < 0 THEN 'return'
        ELSE type
    END AS type,

    order_id,
    order_date

FROM {{ ref('stg_orders') }}

----------------------------------------

TITLE: YAML Configuration for dbt Model Testing
DESCRIPTION: YAML configuration for testing the Python model output using dbt's built-in not_null test to ensure the parsed datetime values are valid.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: transactions
    columns:
      - name: parsed_transaction_time
        tests:
          - not_null

----------------------------------------

TITLE: Basic Jinja Expression Template in dbt
DESCRIPTION: Example template showing where Jinja expressions should be placed in dbt_utils.date_spine function.

LANGUAGE: jinja
CODE:
  {{ dbt_utils.date_spine(
      datepart="day",
      start_date=[ USE JINJA HERE ]
      )
  }}

----------------------------------------

TITLE: Setting a Specific Latest Version in dbt Model Configuration
DESCRIPTION: This snippet demonstrates how to set a specific 'latest_version' that differs from the highest version number. It shows setting version 2 as the latest, even though version 3 exists, which would make version 3 a 'prerelease' version.

LANGUAGE: yaml
CODE:
models:
  - name: model_name
    latest_version: 2
    versions:
      - v: 3
      - v: 2
      - v: 1

----------------------------------------

TITLE: Setting table properties in SQL
DESCRIPTION: Example of setting table properties in a dbt SQL model file

LANGUAGE: sql
CODE:
{{ config(
    tblproperties={
      'delta.autoOptimize.optimizeWrite' : 'true',
      'delta.autoOptimize.autoCompact' : 'true'
    }
 ) }}

----------------------------------------

TITLE: Basic Statement Block Usage in dbt SQL
DESCRIPTION: Example of a basic statement block that queries distinct states from a users table. Includes dependency declaration using depends_on.

LANGUAGE: sql
CODE:
-- depends_on: {{ ref('users') }}

{%- call statement('states', fetch_result=True) -%}

    select distinct state from {{ ref('users') }}

{%- endcall -%}

----------------------------------------

TITLE: Basic dbt Semantic Layer CLI Commands
DESCRIPTION: Essential command line operations for working with the dbt Semantic Layer, including parsing projects, querying metrics, and listing dimensions. These commands help in developing and validating semantic models and metrics.

LANGUAGE: bash
CODE:
dbt parse

LANGUAGE: bash
CODE:
dbt sl query --metrics revenue --group-by metric_time__month

LANGUAGE: bash
CODE:
dbt sl list dimensions --metrics [metric name]

LANGUAGE: bash
CODE:
dbt sl list --help

LANGUAGE: bash
CODE:
dbt sl --help

LANGUAGE: bash
CODE:
dbt sl [subcommand] --help

----------------------------------------

TITLE: Defining Measures in Semantic Model YAML
DESCRIPTION: Example of how to define measures in a semantic model, including sum aggregations and custom expressions.

LANGUAGE: yaml
CODE:
measures:
  - name: order_total
    description: The total amount for each order including taxes.
    agg: sum
  - name: tax_paid
    description: The total tax paid on each order.
    agg: sum
- name: order_count
  description: The count of individual orders.
  expr: 1
  agg: sum

----------------------------------------

TITLE: Recommended Model Path Configuration
DESCRIPTION: Shows the recommended way to specify model paths using relative paths instead of absolute paths.

LANGUAGE: yml
CODE:
model-paths: ["models"]

----------------------------------------

TITLE: dbt Package Installation Output
DESCRIPTION: Example output from running dbt deps command showing package installation status, version information, and available updates.

LANGUAGE: txt
CODE:
Installing dbt-labs/dbt_utils@0.7.1
  Installed from version 0.7.1
  Up to date!
Installing brooklyn-data/dbt_artifacts@1.2.0
  Installed from version 1.2.0
Installing dbt-labs/codegen@0.4.0
  Installed from version 0.4.0
  Up to date!
Installing calogica/dbt_expectations@0.4.1
  Installed from version 0.4.1
  Up to date!
Installing https://github.com/dbt-labs/dbt-audit-helper.git@0.4.0
  Installed from revision 0.4.0
Installing https://github.com/dbt-labs/dbt-labs-experimental-features@0.0.1
  Installed from revision 0.0.1
   and subdirectory materialized-views
Installing dbt-labs/snowplow@0.13.0
  Installed from version 0.13.0
  Updated version available: 0.13.1
Installing calogica/dbt_date@0.4.0
  Installed from version 0.4.0
  Up to date!

Updates available for packages: ['tailsdotcom/dbt_artifacts', 'dbt-labs/snowplow']
Update your versions in packages.yml, then run dbt deps

----------------------------------------

TITLE: Incorrect Test Path Configuration in dbt_project.yml
DESCRIPTION: Shows an example of how not to configure test paths. Using absolute paths is discouraged as it reduces portability and can cause issues across different environments.

LANGUAGE: yml
CODE:
test-paths: ["/Users/username/project/test"]

----------------------------------------

TITLE: Configuring dbt Project YAML
DESCRIPTION: Update the dbt_project.yml file with project-specific settings.

LANGUAGE: yaml
CODE:
name: jaffle_shop

...

profile: jaffle_shop

...

models:
    jaffle_shop:
    ...

----------------------------------------

TITLE: Setting on_configuration_change in dbt_project.yml
DESCRIPTION: Demonstrates how to set the on_configuration_change option in the dbt_project.yml file. This configuration applies to models in the specified resource path.

LANGUAGE: yaml
CODE:
models:
  [<resource-path>]:
    [+][materialized]: <materialization_name>
    [+]on_configuration_change: apply | continue | fail

----------------------------------------

TITLE: Setting Snowflake Session Parameters for a Model
DESCRIPTION: Example of using sql_header to set Snowflake session parameters for a specific model. This demonstrates how to alter session settings before model creation.

LANGUAGE: sql
CODE:
{{ config(
  sql_header="alter session set timezone = 'Australia/Sydney';"
) }}

select * from {{ ref('other_model') }}

----------------------------------------

TITLE: Selecting Versioned Models in dbt
DESCRIPTION: Examples of using the version method to select versioned models based on their version identifier and latest version status.

LANGUAGE: bash
CODE:
dbt list --select "version:latest"      # only 'latest' versions
dbt list --select "version:prerelease"  # versions newer than the 'latest' version
dbt list --select "version:old"         # versions older than the 'latest' version

dbt list --select "version:none"        # models that are *not* versioned

----------------------------------------

TITLE: Basic Target Database Configuration in dbt Project
DESCRIPTION: Basic configuration for setting target database in dbt_project.yml for snapshots.

LANGUAGE: yml
CODE:
snapshots:
  [<resource-path>]:
    +target_database: string

----------------------------------------

TITLE: Configuring Updated_at in YAML for dbt 1.9+
DESCRIPTION: Example of configuring the updated_at parameter for a snapshot using YAML in dbt version 1.9 and above. It sets the strategy to timestamp and specifies the updated_at column.

LANGUAGE: yaml
CODE:
snapshots:
  - name: snapshot
    relation: source('my_source', 'my_table')
    config:
      strategy: timestamp
      updated_at: column_name

----------------------------------------

TITLE: Testing Primary Keys in dbt Models
DESCRIPTION: Core test configuration for ensuring primary key integrity by validating that a column is both unique and not null.

LANGUAGE: yaml
CODE:
columns:
  - name: primary_key_column
    tests:
      - unique
      - not_null

----------------------------------------

TITLE: Selecting Tests by Type in dbt
DESCRIPTION: Examples of using the test_type method to select tests based on their type.

LANGUAGE: bash
CODE:
dbt test --select "test_type:unit"           # run all unit tests
dbt test --select "test_type:data"           # run all data tests
dbt test --select "test_type:generic"        # run all generic data tests
dbt test --select "test_type:singular"       # run all singular data tests

----------------------------------------

TITLE: AWS PrivateLink Request Template
DESCRIPTION: Template for submitting a new AWS PrivateLink request to dbt Support, including required fields like Databricks instance name, cluster AWS Region, and dbt Cloud environment.

LANGUAGE: plaintext
CODE:
Subject: New AWS Multi-Tenant PrivateLink Request
- Type: Databricks
- Databricks instance name:
- Databricks cluster AWS Region (e.g., us-east-1, eu-west-2):
- dbt Cloud multi-tenant environment (US, EMEA, AU):

----------------------------------------

TITLE: Config.get Usage Examples
DESCRIPTION: Demonstrates different ways to use config.get including no default, alternate value, and default value scenarios.

LANGUAGE: sql
CODE:
{% materialization incremental, default -%}
  -- Example w/ no default. unique_key will be None if the user does not provide this configuration
  {%- set unique_key = config.get('unique_key') -%}

  -- Example w/ alternate value. Use alternative of 'id' if 'unique_key' config is provided, but it is None
  {%- set unique_key = config.get('unique_key') or 'id' -%}

  -- Example w/ default value. Default to 'id' if the 'unique_key' config does not exist
  {%- set unique_key = config.get('unique_key', default='id') -%}
  ...

----------------------------------------

TITLE: Setting Session Properties in dbt Pre-Hook for IBM watsonx.data Spark
DESCRIPTION: Demonstrates how to set a session property using a dbt pre-hook to modify the current configuration for a user session. This example sets the query_max_run_time to 10 minutes.

LANGUAGE: sql
CODE:
{{
  config(
    pre_hook="set session query_max_run_time='10m'"
  )
}}

----------------------------------------

TITLE: Compiled SQL output for stg_orders model
DESCRIPTION: This snippet shows the compiled SQL output for the stg_orders model, which includes a CTE to select data from the raw_orders table and rename columns.

LANGUAGE: sql
CODE:
with source as (
    select * from "jaffle_shop"."main"."raw_orders"

),

renamed as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from source

)

select * from renamed

----------------------------------------

TITLE: Multi-Adapter String Literal Implementation
DESCRIPTION: Enhanced version of to_literal macro supporting multiple database adapters using adapter.dispatch

LANGUAGE: sql
CODE:
{% macro to_literal(text) %}

    {{ return(adapter.dispatch('to_literal', 'dbt_sample_package')(text)) }}

{% endmacro %}

{% macro default__to_literal(text) %}

    '{{- text -}}'

{% endmacro %}

{% macro postgres__to_literal(text) %}

    E'{{- text -}}'

{% endmacro %}

----------------------------------------

TITLE: Defining Data Sources in dbt YAML Configuration
DESCRIPTION: This snippet shows the basic structure for defining data sources in a dbt YAML configuration file. It includes placeholders for source name, database name, and loader, with a placeholder for table definitions.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: <source_name>
    database: <database_name>
    loader: <string>
    tables:
      - ...

----------------------------------------

TITLE: Defining Model Versions in dbt YAML Configuration
DESCRIPTION: This YAML snippet demonstrates how to configure model versions in a dbt project. It includes version identifiers, file references, column specifications, and optional latest version designation.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: model_name
    versions:
      - v: <version_identifier> # required
        defined_in: <file_name> # optional -- default is <model_name>_v<v>
        columns:
          # specify all columns, or include/exclude columns from the top-level model YAML definition
          - include: <include_value>
            exclude: <exclude_list>
          # specify additional columns
          - name: <column_name> # required
      - v: ...
    
    # optional
    latest_version: <version_identifier>

----------------------------------------

TITLE: dbt Project Configuration for Staging Models
DESCRIPTION: YAML configuration showing materialization settings for staging models within the dbt project.

LANGUAGE: yaml
CODE:
models:
  jaffle_shop:
    staging:
      +materialized: view

----------------------------------------

TITLE: Rendering Integration Tool Cards in JSX
DESCRIPTION: This code snippet demonstrates how to create a grid layout of cards representing various tools that integrate with the dbt Semantic Layer. It uses custom Card components and includes external links for some tools.

LANGUAGE: jsx
CODE:
<div className="grid--3-col">

 <Card
    title="Tableau"
    link="/docs/cloud-integrations/semantic-layer/tableau"
    body="Learn how to connect to Tableau for querying metrics and collaborating with your team."
    icon="tableau-software"/>
  
  <Card
    title="Google Sheets"
    link="/docs/cloud-integrations/semantic-layer/gsheets"
    body="Discover how to connect to Google Sheets for querying metrics and collaborating with your team."
    icon="google-sheets-logo-icon"/>

  <Card
    title="Microsoft Excel"
    link="/docs/cloud-integrations/semantic-layer/excel"
    body="Connect to Microsoft Excel to query metrics and collaborate with your team. Available for Excel Desktop or Excel Online."
    icon="excel"/>

  <div className="card-container">
    <Card
      title="Dot"
      link="https://docs.getdot.ai/dot/integrations/dbt-semantic-layer"
      body="Enable everyone to analyze data with AI in Slack or Teams."
      icon="dot-ai"/>
      <a href="https://docs.getdot.ai/dot/integrations/dbt-semantic-layer"
      className="external-link"
      target="_blank"
      rel="noopener noreferrer">
      <Icon name='fa-external-link' />
    </a>
  </div>

  <div className="card-container">
    <Card
      title="Hex"
      link="https://learn.hex.tech/docs/connect-to-data/data-connections/dbt-integration#dbt-semantic-layer-integration"
      body="Check out how to connect, analyze metrics, collaborate, and discover more data possibilities."
      icon="hex"/>
      <a href="https://learn.hex.tech/docs/connect-to-data/data-connections/dbt-integration#dbt-semantic-layer-integration"
      className="external-link"
      target="_blank"
      rel="noopener noreferrer">
      <Icon name='fa-external-link' />
    </a>
  </div>

<div className="card-container">
  <Card
    title="Klipfolio PowerMetrics"
    body="Learn how to connect to a streamlined metrics catalog and deliver metric-centric analytics to business users."
    icon="klipfolio"
    link="https://support.klipfolio.com/hc/en-us/articles/18164546900759-PowerMetrics-Adding-dbt-Semantic-Layer-metrics"/>
    <a href="https://support.klipfolio.com/hc/en-us/articles/18164546900759-PowerMetrics-Adding-dbt-Semantic-Layer-metrics"
    className="external-link"
      target="_blank"
      rel="noopener noreferrer">
      <Icon name='fa-external-link' />
    </a>
</div>

<div className="card-container">
  <Card
    title="Lightdash"
    body="Check out how to connect, query, and consume reliable dbt metrics in real time "
    link="https://docs.lightdash.com/references/dbt-semantic-layer"
    icon="lightdash"/>
    <a href="https://docs.lightdash.com/references/dbt-semantic-layer"
    className="external-link"
      target="_blank"
      rel="noopener noreferrer">
      <Icon name='fa-external-link' />
    </a>
</div>

<div className="card-container">
  <Card
    title="Mode"
    body="Discover how to connect, access, and get trustworthy metrics and insights."
    link="https://mode.com/help/articles/supported-databases#dbt-semantic-layer"
    icon="mode"/>
    <a href="https://mode.com/help/articles/supported-databases#dbt-semantic-layer"
    className="external-link"
      target="_blank"
      rel="noopener noreferrer">
      <Icon name='fa-external-link' />
    </a>
</div>

<div className="card-container">
  <Card
    title="Push.ai"
    body="Explore how to connect and use metrics to power reports and insights that drive change."
    link="https://docs.push.ai/data-sources/semantic-layers/dbt"
    icon="push"/>
    <a href="https://docs.push.ai/data-sources/semantic-layers/dbt"
    className="external-link"
      target="_blank"
      rel="noopener noreferrer">
      <Icon name='fa-external-link' />
    </a>
</div>

<div className="card-container">
  <Card
    title="Sigma (Preview)"
    body="Connect Sigma to the dbt Semantic Layer to allow you to leverage your predefined dbt metrics in Sigma workbooks."
    link="https://help.sigmacomputing.com/docs/configure-a-dbt-semantic-layer-integration"
    icon="sigma"/>
    <a href="https://help.sigmacomputing.com/docs/configure-a-dbt-semantic-layer-integration"
    className="external-link"
      target="_blank"
      rel="noopener noreferrer">
      <Icon name='fa-external-link' />
    </a>
</div>


<div className="card-container">
  <Card
    title="Steep"
    body="Connect Steep to the dbt Semantic Layer for centralized, scalable analytics."
    link="https://help.steep.app/integrations/dbt-cloud"
    icon="steep"/>
    <a href="https://help.steep.app/integrations/dbt-cloud"
    className="external-link"
      target="_blank"
      rel="noopener noreferrer">
      <Icon name='fa-external-link' />
    </a>
</div>

</div>

----------------------------------------

TITLE: Basic Snapshot Path Configuration
DESCRIPTION: Default configuration for specifying snapshot directory paths in dbt_project.yml. This setting allows you to define where dbt will look for snapshot definitions.

LANGUAGE: yml
CODE:
snapshot-paths: [directorypath]

----------------------------------------

TITLE: Generating Selective dbt Documentation
DESCRIPTION: This command generates documentation for a specific subset of nodes in the project, limiting the catalog.json content to the selected nodes.

LANGUAGE: shell
CODE:
dbt docs generate --select +orders

----------------------------------------

TITLE: Configuring Seed Properties with YAML Properties File
DESCRIPTION: Shows how to configure quote_columns using a properties.yml file in the seeds directory (available in DBT v0.21+).

LANGUAGE: yaml
CODE:
version: 2

seeds:
  - name: mappings
    config:
      quote_columns: true

----------------------------------------

TITLE: Building Single dbt Seed Using Select Flag - Shell
DESCRIPTION: Command to build a specific seed file in dbt using the --select flag. This functionality was introduced in dbt v0.16.0 and allows for selective seed building. The command can be used with both --select and --exclude options.

LANGUAGE: shell
CODE:
$ dbt seed --select country_codes

----------------------------------------

TITLE: Logging into Google Cloud and Enabling Google Drive Access
DESCRIPTION: This gcloud command logs into Google Cloud, enables access to Google Drive, and updates the Application Default Credentials (ADC) file used by many Google Cloud libraries for API authentication.

LANGUAGE: bash
CODE:
gcloud auth login --enable-gdrive-access --update-adc

----------------------------------------

TITLE: Configuring Test Alias in properties.yml
DESCRIPTION: Specifies a custom alias for a unique data test in the properties.yml file.

LANGUAGE: yaml
CODE:
models:
  - name: orders
    columns:
      - name: order_id
        tests:
          - unique:
              alias: unique_order_id_test

----------------------------------------

TITLE: Compile SQL method request example
DESCRIPTION: Example JSON request for compiling a SQL query using the compile_sql method.

LANGUAGE: json
CODE:
{
    "jsonrpc": "2.0",
    "method": "compile_sql",
    "id": "2db9a2fe-9a39-41ef-828c-25e04dd6b07d",
    "params": {
        "timeout": 60,
        "sql": "c2VsZWN0IHt7IDEgKyAxIH19IGFzIGlk",
        "name": "my_first_query"
    }
}

----------------------------------------

TITLE: Redshift Seed Error Example
DESCRIPTION: Example of database error when running dbt seed command with changed columns in Redshift, showing column does not exist error.

LANGUAGE: shell
CODE:
$ dbt seed
Running with dbt=0.16.0-rc2
Found 0 models, 0 tests, 0 snapshots, 0 analyses, 149 macros, 0 operations, 1 seed file, 0 sources

12:14:46 | Concurrency: 1 threads (target='dev_redshift')
12:14:46 |
12:14:46 | 1 of 1 START seed file dbt_claire.country_codes...................... [RUN]
12:14:46 | 1 of 1 ERROR loading seed file dbt_claire.country_codes.............. [ERROR in 0.23s]
12:14:46 |
12:14:46 | Finished running 1 seed in 1.75s.

Completed with 1 error and 0 warnings:

Database Error in seed country_codes (seeds/country_codes.csv)
  column "country_name" of relation "country_codes" does not exist

Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1

----------------------------------------

TITLE: Selecting Resources Using Exposure Method in dbt
DESCRIPTION: Examples of using the exposure method to select parent resources of specified exposures.

LANGUAGE: bash
CODE:
dbt run --select "+exposure:weekly_kpis"                # run all models that feed into the weekly_kpis exposure
dbt test --select "+exposure:*"                         # test all resources upstream of all exposures
dbt ls --select "+exposure:*" --resource-type source    # list all source tables upstream of all exposures

----------------------------------------

TITLE: Converting List to Set using set_strict() in Jinja
DESCRIPTION: Shows how to use the 'set_strict' context method to convert a list with duplicate elements to a unique set, similar to the 'set' method.

LANGUAGE: jinja
CODE:
{% set my_list = [1, 2, 2, 3] %}
{% set my_set = set(my_list) %}
{% do log(my_set) %}  {# {1, 2, 3} #}

----------------------------------------

TITLE: Including Query Comment as Job Labels in BigQuery
DESCRIPTION: Demonstrates how to configure dbt to include query comment items as job labels for BigQuery executions.

LANGUAGE: yaml
CODE:
query-comment:
  job-label: True

----------------------------------------

TITLE: Generating dbt Docs with Empty Catalog
DESCRIPTION: This command generates documentation without running database queries to populate catalog.json, useful for faster generation in development environments.

LANGUAGE: shell
CODE:
dbt docs generate --empty-catalog

----------------------------------------

TITLE: Configuring Tags in SQL Model Files
DESCRIPTION: This snippet shows how to apply tags to a model directly in the SQL file using the config block. It supports both single string and list of strings for tag values.

LANGUAGE: sql
CODE:
{{ config(
    tags="<string>" | ["<string>"]
) }}

----------------------------------------

TITLE: Creating Unit Test Macro for String Literal Function
DESCRIPTION: A unit test macro that validates the to_literal function behavior by comparing expected and actual results

LANGUAGE: sql
CODE:
{% macro test_to_literal() %}

    {% = dbt_sample_package.to_literal('test string') %}

    {% if result != "'test string'" %}

        {{ exceptions.raise_compiler_error('The test is failed') }}

    {% endif %}

{% endmacro %}

----------------------------------------

TITLE: Source Freshness Override Configuration
DESCRIPTION: Example demonstrating how to override freshness configurations at both source and table levels, including warning and error thresholds for data freshness checks.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: github
    overrides: github_source

    freshness:
      warn_after:
        count: 1
        period: day
      error_after:
        count: 2
        period: day

    tables:
      - name: issue_assignee
        freshness:
          warn_after:
            count: 2
            period: day
          error_after:
            count: 4
            period: day

----------------------------------------

TITLE: Configuring Snapshot Groups
DESCRIPTION: Shows how to configure groups for snapshot resources using both YAML and SQL configurations.

LANGUAGE: yaml
CODE:
snapshots:
  [<resource-path>]:
    +group: GROUP_NAME

LANGUAGE: sql
CODE:
{% snapshot [snapshot_name] %}

{{ config(
  group='GROUP_NAME'
) }}

select ...

{% endsnapshot %}

----------------------------------------

TITLE: Zip with Default Value
DESCRIPTION: Shows how to use zip with a default value when handling non-iterable inputs. Returns empty list when invalid input is provided instead of raising an error.

LANGUAGE: jinja
CODE:
{% set my_list_a = 12 %}
{% set my_list_b = ['alice', 'bob'] %}
{% set my_zip = zip(my_list_a, my_list_b, default = []) | list %}
{% do log(my_zip) %}  {# [] #}

----------------------------------------

TITLE: Defining Source with Custom Schema and Identifier in dbt YAML
DESCRIPTION: This YAML configuration defines a source with custom schema and identifier properties to handle poorly named database objects. It maps the actual database names to more meaningful names for use in dbt models.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: jaffle_shop
    schema: postgres_backend_public_schema
    database: raw
    tables:
      - name: orders
        identifier: api_orders

----------------------------------------

TITLE: Configuring Snapshot Path in dbt_project.yml
DESCRIPTION: This code snippet demonstrates how to change the default snapshot directory in a dbt project by modifying the 'snapshot-paths' configuration in the dbt_project.yml file. It allows specifying a custom directory for storing snapshot files.

LANGUAGE: yaml
CODE:
snapshot-paths: ["snapshots"]

----------------------------------------

TITLE: Valid dbt Project Name Example
DESCRIPTION: A corrected example showing the proper snake_case format for a dbt project name.

LANGUAGE: yml
CODE:
name: jaffle_shop

----------------------------------------

TITLE: Alternative dbt Docker Container Run Command
DESCRIPTION: This is an alternative command to run a dbt Docker container, mounting the local project directory and profiles.yml file in a slightly different way. It also uses host networking and executes the 'ls' command.

LANGUAGE: bash
CODE:
docker run \
--network=host \
--mount type=bind,source=path/to/project,target=/usr/app \
--mount type=bind,source=path/to/profiles.yml.dbt,target=/root/.dbt/ \
<dbt_image_name> \
ls

----------------------------------------

TITLE: Configuring Version-Specific Unit Tests in dbt YAML
DESCRIPTION: Examples showing different ways to configure unit tests for versioned models in dbt, including running tests on all versions, specific versions using include, or excluding certain versions. The configuration is done through YAML syntax in the dbt project files.

LANGUAGE: yaml
CODE:
# my test_is_valid_email_address unit test will run on all versions of my_model
unit_tests:
  - name: test_is_valid_email_address
    model: my_model
    ...
            
# my test_is_valid_email_address unit test will run on ONLY version 2 of my_model
unit_tests:
  - name: test_is_valid_email_address 
    model: my_model 
    versions:
      include: 
        - 2
    ...
            
# my test_is_valid_email_address unit test will run on all versions EXCEPT 1 of my_model
unit_tests:
  - name: test_is_valid_email_address
    model: my_model 
    versions:
      exclude: 
        - 1
    ...

----------------------------------------

TITLE: dbt Table Creation Using CREATE TABLE AS
DESCRIPTION: Illustrates how dbt creates tables using a CREATE TABLE AS statement, which infers column types from the SELECT query.

LANGUAGE: sql
CODE:
create table dbt_alice.my_table as (
  select id, created from some_other_table
)

----------------------------------------

TITLE: Single Model Configuration
DESCRIPTION: Demonstrates how to apply configuration to a specific model using its full path.

LANGUAGE: yaml
CODE:
name: jaffle_shop

models:
  jaffle_shop:
    staging:
      stripe:
        payments:
          +enabled: false

----------------------------------------

TITLE: Configuring valid_history Incremental Strategy for Teradata
DESCRIPTION: Sets up the valid_history incremental materialization strategy for managing historical data efficiently in Teradata.

LANGUAGE: yaml
CODE:
{{
    config(
        materialized='incremental',
        unique_key='id',
        on_schema_change='fail',
        incremental_strategy='valid_history',
        valid_period='valid_period_col',
        use_valid_to_time='no',
)
}}

----------------------------------------

TITLE: Configuring Data Tests in dbt_project.yml
DESCRIPTION: This snippet shows how to configure data tests in the dbt_project.yml file. It includes various configuration options such as fail_calc, limit, severity, and store_failures.

LANGUAGE: yaml
CODE:
tests:
  [<resource-path>]:
    [+][fail_calc]: <string>
    [+][limit]: <integer>
    [+][severity]: error | warn
    [+][error_if]: <string>
    [+][warn_if]: <string>
    [+][store_failures]: true | false
    [+][where]: <string>

----------------------------------------

TITLE: Raising Warning in dbt SQL
DESCRIPTION: This snippet shows how to use `exceptions.warn` to raise a compiler warning with a custom message when an invalid number is provided. It checks if the number is outside the range of 0 to 100. The warning can be elevated to an error using the `--warn-error` flag.

LANGUAGE: sql
CODE:
{% if number < 0 or number > 100 %}
  {% do exceptions.warn("Invalid `number`. Got: " ~ number) %}
{% endif %}

----------------------------------------

TITLE: Configuring Dremio Cloud Profile in YAML
DESCRIPTION: YAML configuration template for setting up a Dremio Cloud profile in dbt. Includes essential parameters like cloud host, project ID, authentication, and storage settings.

LANGUAGE: yaml
CODE:
[project name]:
  outputs:
    dev:
      cloud_host: api.dremio.cloud
      cloud_project_id: [project ID]
      object_storage_source: [name]
      object_storage_path: [path]
      dremio_space: [name]
      dremio_space_folder: [path]
      pat: [personal access token]
      threads: [integer >= 1]
      type: dremio
      use_ssl: true
      user: [email address]
  target: dev

----------------------------------------

TITLE: Configuring LDAP Authentication for Hive in YAML
DESCRIPTION: YAML configuration for setting up LDAP authentication with Hive in dbt's profiles.yml file. This is the recommended method for Cloudera Data Platform (CDP).

LANGUAGE: yaml
CODE:
your_profile_name:
  target: dev
  outputs:
    dev:
     type: hive
     host: HOST_NAME
     http_path: YOUR/HTTP/PATH # optional, http path to Hive default value: None
     port: PORT # default value: 10000
     auth_type: ldap
     use_http_transport: BOOLEAN # default value: true
     use_ssl: BOOLEAN # TLS should always be used with LDAP to ensure secure transmission of credentials, default value: true
     username: USERNAME
     password: PASSWORD
     schema: SCHEMA_NAME

----------------------------------------

TITLE: Configuring View Materialization in dbt Project YAML
DESCRIPTION: Demonstrates how to configure a ClickHouse view materialization in the dbt_project.yml file.

LANGUAGE: yaml
CODE:
models:
  <resource-path>:
    +materialized: view

----------------------------------------

TITLE: Configuring All Distribution Style in dbt
DESCRIPTION: Configuration for implementing 'all' distribution style in a dbt model, which copies data to all nodes. Best for smaller, infrequently updated tables.

LANGUAGE: python
CODE:
{{ config(materialized='table', dist='all') }}

----------------------------------------

TITLE: Writing Simple Multi-line Descriptions in dbt YAML
DESCRIPTION: This snippet demonstrates how to use the '>' operator in YAML to create a simple, single-paragraph description for a dbt model. Interior line breaks are removed, and Markdown formatting can be used.

LANGUAGE: yaml
CODE:
  version: 2

  models:
  - name: customers
    description: >
      Lorem ipsum **dolor** sit amet, consectetur adipisicing elit, sed do eiusmod
      tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam,
      quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo
      consequat.

----------------------------------------

TITLE: Setting event_time in SQL model config block
DESCRIPTION: Example of configuring event_time directly in a SQL model file using config blocks.

LANGUAGE: sql
CODE:
{{ config(
    event_time='my_time_field'
) }}

----------------------------------------

TITLE: Handling Invalid Iterable with set() in Jinja
DESCRIPTION: Shows how the 'set' context method handles an invalid iterable by returning None instead of raising an error.

LANGUAGE: jinja
CODE:
{% set my_invalid_iterable = 1234 %}
{% set my_set = set(my_invalid_iterable) %}
{% do log(my_set) %}  {# None #}

----------------------------------------

TITLE: YAML Quoting for dbt Version Requirements
DESCRIPTION: Illustrates correct and incorrect ways to quote the require-dbt-version value in YAML. Proper quoting is essential for correct parsing.

LANGUAGE: yaml
CODE:
# ‚úÖ These will work
require-dbt-version: ">=1.0.0" # Double quotes are OK
require-dbt-version: '>=1.0.0' # So are single quotes

# ‚ùå These will not work
require-dbt-version: >=1.0.0 # No quotes? No good
require-dbt-version: ">= 1.0.0" # Don't put whitespace after the equality signs

----------------------------------------

TITLE: dbt Show Inline Query Preview
DESCRIPTION: Example of using dbt show with an inline query that references another model.

LANGUAGE: bash
CODE:
dbt show --inline "select * from {{ ref('model_name') }}"

----------------------------------------

TITLE: Building Grain ID Macro in dbt
DESCRIPTION: A dbt macro that creates a unique key from specified columns using dbt_utils.surrogate_key, excluding unwanted columns from the key generation.

LANGUAGE: sql
CODE:
{%- macro build_key_from_columns(dbt_relation, exclude=[]) -%}

{% set cols = dbt_utils.get_filtered_columns_in_relation(dbt_relation, exclude)  %}

{{ return(dbt_utils.surrogate_key(cols)) }}

{%- endmacro -%}

----------------------------------------

TITLE: Practical Source Quoting Example
DESCRIPTION: Shows a complete example of source quoting configuration with mixed quoting settings between source and table levels. Demonstrates how table-level settings override source-level defaults.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: jaffle_shop
    database: raw
    quoting:
      database: true
      schema: true
      identifier: true

    tables:
      - name: orders
      - name: customers
        # This overrides the `jaffle_shop` quoting config
        quoting:
          identifier: false

----------------------------------------

TITLE: Advanced Row Compression in Oracle dbt Model
DESCRIPTION: Shows how to enable Advanced Row Compression for OLTP systems using ROW STORE COMPRESS ADVANCED clause.

LANGUAGE: sql
CODE:
{{config(materialized='table', table_compression_clause='ROW STORE COMPRESS ADVANCED')}}
SELECT c.cust_id, c.cust_first_name, c.cust_last_name
from {{ source('sh_database', 'customers') }} c

----------------------------------------

TITLE: Creating a CSV Seed File in dbt
DESCRIPTION: This snippet shows the structure of a CSV seed file containing country codes and names. Seed files in dbt are used to load static data into your data warehouse.

LANGUAGE: text
CODE:
country_code,country_name
US,United States
CA,Canada
GB,United Kingdom
...

----------------------------------------

TITLE: Statement Block with Dependencies Example
DESCRIPTION: Demonstrates statement block usage with explicit dependency declaration and result loading into a comment.

LANGUAGE: sql
CODE:
-- depends_on: {{ ref('users') }}

{% call statement('states', fetch_result=True) -%}

    select distinct state from {{ ref('users') }}

    /*
    The unique states are: {{ load_result('states')['data'] }}
    */
{%- endcall %}

----------------------------------------

TITLE: Recommended Relative Path Configuration
DESCRIPTION: Demonstrates the recommended way to configure macro paths using relative paths in dbt_project.yml.

LANGUAGE: yml
CODE:
macro-paths: ["macros"]

----------------------------------------

TITLE: Fixing Stuck Snapshot Records - SQL Update Query
DESCRIPTION: SQL update statement to fix 'stuck' records in snapshot tables by setting dbt_valid_to to null for affected records. Includes a subquery to identify the records that need to be updated.

LANGUAGE: sql
CODE:
update <your snapshot table>
set dbt_valid_to = null
where dbt_scd_id in (

  with base as (

      select *,

          -- Replace `<your unique key>` with your specified unique_key 
          <your unique key> as dbt_unique_key

      -- Replace <your snapshot table> with a Snapshot table name
      from <your snapshot table>

  ),

  ranked as (

      select *,
          row_number() over (
              partition by dbt_unique_key
              order by dbt_valid_from desc
          ) as change_idx

      from base

  ),

  to_migrate as (

      select *
      from ranked
      where change_idx = 1
      and dbt_valid_to is not null

  )
 
  select dbt_scd_id from to_migrate

);

----------------------------------------

TITLE: Source Configuration in Subfolder
DESCRIPTION: Shows how to configure a source table that is nested in a YAML file within a subfolder.

LANGUAGE: yaml
CODE:
sources:
  your_project_name:
    subdirectory_name:
      source_name:
        source_table_name:
          +enabled: false

----------------------------------------

TITLE: Setting Redshift Database Permissions
DESCRIPTION: SQL statements for granting various permissions including schema creation, usage, table creation, view creation, and select privileges. These commands demonstrate how to set up basic access controls for users and roles in a Redshift database.

LANGUAGE: sql
CODE:
grant create schema on database database_name to user_name;
grant usage on schema database.schema_name to user_name;
grant create table on schema database.schema_name to user_name;
grant create view on schema database.schema_name to user_name;
grant usage for schemas in database database_name to role role_name;
grant select on all tables in database database_name to user_name;
grant select on all views in database database_name to user_name;

----------------------------------------

TITLE: Including Modified and Downstream Models in dbt Compare Command
DESCRIPTION: This SQL command demonstrates how to include models that were directly modified and those one step downstream using the modified+1 selector in the dbt compare feature.

LANGUAGE: sql
CODE:
--select state:modified+1

----------------------------------------

TITLE: Running dbt with Cache Limited to Selected Models
DESCRIPTION: This command shows how to run dbt with cache population limited to only the selected models, which can improve speed and performance when focusing on a specific subset of models (in this case, Salesforce models).

LANGUAGE: text
CODE:
dbt --cache-selected-only run --select salesforce

----------------------------------------

TITLE: Configuring Source Freshness in YAML
DESCRIPTION: Core YAML configuration structure for defining source freshness rules in DBT. Shows how to set warning and error thresholds at both source and table levels.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: <source_name>
    freshness:
      warn_after:
        count: <positive_integer>
        period: minute | hour | day
      error_after:
        count: <positive_integer>
        period: minute | hour | day
      filter: <boolean_sql_expression>
    loaded_at_field: <column_name_or_expression>

    tables:
      - name: <table_name>
        freshness:
          warn_after:
            count: <positive_integer>
            period: minute | hour | day
          error_after:
            count: <positive_integer>
            period: minute | hour | day
          filter: <boolean_sql_expression>
        loaded_at_field: <column_name_or_expression>

----------------------------------------

TITLE: Error Message for BigQuery Drive Credential Permission Denied
DESCRIPTION: This snippet shows the error message users encounter when BigQuery lacks permission to access Google Drive credentials.

LANGUAGE: plaintext
CODE:
Access denied: BigQuery BigQuery: Permission denied while getting Drive credentials

----------------------------------------

TITLE: Casting Column Types in dbt SQL Model
DESCRIPTION: Demonstrates how to cast a column to a specific type (timestamp) within a dbt model SQL query.

LANGUAGE: sql
CODE:
select
    id,
    created::timestamp as created
from some_other_table

----------------------------------------

TITLE: DBT DATE_TRUNC Macro Example
DESCRIPTION: Example using dbt's cross-database macro for DATE_TRUNC function, showing how to truncate dates to week, month, and year levels using the Jaffle Shop dataset.

LANGUAGE: sql
CODE:
select
   order_id,
   order_date,
   {{ date_trunc("week", "order_date") }} as order_week,
   {{ date_trunc("month", "order_date") }} as order_month,
   {{ date_trunc("year", "order_date") }} as order_year
from {{ ref('orders') }}

----------------------------------------

TITLE: Serializing Python Dictionary to YAML String using toyaml in dbt
DESCRIPTION: This snippet demonstrates how to use the toyaml context method to convert a Python dictionary to a YAML string. It creates a dictionary, serializes it using toyaml, and then logs the resulting YAML string.

LANGUAGE: jinja
CODE:
{% set my_dict = {"abc": 123} %}
{% set my_yaml_string = toyaml(my_dict) %}

{% do log(my_yaml_string) %}

----------------------------------------

TITLE: Basic Macro Path Configuration in dbt
DESCRIPTION: Basic configuration syntax for specifying macro paths in dbt_project.yml file. Defines the directory where dbt will search for macros.

LANGUAGE: yml
CODE:
macro-paths: [directorypath]

----------------------------------------

TITLE: Successful dbt retry after fixing errors
DESCRIPTION: Example showing a successful dbt retry execution after fixing the syntax error in the customers model.

LANGUAGE: shell
CODE:
Running with dbt=1.6.1
Registered adapter: duckdb=1.6.0
Found 5 models, 3 seeds, 20 tests, 0 sources, 0 exposures, 0 metrics, 348 macros, 0 groups, 0 semantic models
 
Concurrency: 24 threads (target='dev')

1 of 1 START sql table model main.customers .................................... [RUN]
1 of 1 OK created sql table model main.customers ............................... [OK in 0.05s]

Finished running 1 table model in 0 hours 0 minutes and 0.09 seconds (0.09s).
 
Completed successfully
  
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1

----------------------------------------

TITLE: Configuring Multiple Unique Keys
DESCRIPTION: Example of configuring multiple columns as unique keys for both models and snapshots.

LANGUAGE: sql
CODE:
{{ config(
    materialized='incremental',
    unique_key=['order_id', 'location_id']
) }}

with...

----------------------------------------

TITLE: Defining dbt model tests in YAML
DESCRIPTION: Example of specifying data tests for dbt models in a YAML file.

LANGUAGE: YAML
CODE:
models:
  - name: model_name
    columns:
      - name: column_name
        tests:
          - unique
          - not_null

----------------------------------------

TITLE: Support Request Template for Redshift-managed PrivateLink
DESCRIPTION: Template for submitting a PrivateLink configuration request to dbt Support for Redshift-managed endpoints. Includes required fields for cluster information and environment details.

LANGUAGE: text
CODE:
Subject: New Multi-Tenant PrivateLink Request
- Type: Redshift-managed
- Redshift cluster name:
- Redshift cluster AWS account ID:
- Redshift cluster AWS Region (e.g., us-east-1, eu-west-2):
- dbt Cloud multi-tenant environment (US, EMEA, AU):

----------------------------------------

TITLE: Granting Schema Permissions in Databricks SQL
DESCRIPTION: SQL statements demonstrating how to grant different levels of permissions on a schema to a principal in Databricks. Shows examples of granting all privileges, table creation rights, and view creation permissions.

LANGUAGE: sql
CODE:
grant all privileges on schema schema_name to principal;
grant create table on schema schema_name to principal;
grant create view on schema schema_name to principal;

----------------------------------------

TITLE: Alternative CONCAT Syntax Using Operator
DESCRIPTION: Alternative syntax for concatenation using the || operator, which provides the same functionality as CONCAT in supported data platforms.

LANGUAGE: sql
CODE:
select first_name || last_name AS full_name from {{ ref('customers') }}

----------------------------------------

TITLE: Defining Customer Retention Metric with Offset in YAML
DESCRIPTION: This example shows how to define a customer retention metric using a 1-month offset window to compare current active customers with those from the previous month.

LANGUAGE: yaml
CODE:
- name: customer_retention
  description: Percentage of customers that are active now and those active 1 month ago
  label: customer_retention
  type_params:
    expr: (active_customers/ active_customers_prev_month)
    metrics:
      - name: active_customers
        alias: current_active_customers
      - name: active_customers
        offset_window: 1 month
        alias: active_customers_prev_month

----------------------------------------

TITLE: Configuring Multiple Schemas for dbt Seeds in dbt_project.yml
DESCRIPTION: YAML configuration showing how to set custom schemas for seeds in a dbt project. Demonstrates setting a default schema for all seeds and a specific schema for seeds in a subdirectory.

LANGUAGE: yaml
CODE:
name: jaffle_shop
...

seeds:
  jaffle_shop:
    schema: mappings # all seeds in this project will use the schema "mappings" by default
    marketing:
      schema: marketing # seeds in the "seeds/marketing/" subdirectory will use the schema "marketing"

----------------------------------------

TITLE: Passing Git access token for private packages in YAML
DESCRIPTION: Example of how to pass a Git access token as an environment variable when specifying a private package repository URL in packages.yml

LANGUAGE: yaml
CODE:
packages:
- git: "https://{{env_var('DBT_ENV_SECRET_GIT_CREDENTIAL')}}@github.com/dbt-labs/awesome_repo.git"

----------------------------------------

TITLE: Recommended Asset Path Configuration with Relative Path
DESCRIPTION: Demonstrates the recommended way to configure asset-paths using relative paths to specify asset directories.

LANGUAGE: yaml
CODE:
asset-paths: ["assets"]

----------------------------------------

TITLE: Selected Resources Example Response Format
DESCRIPTION: Shows the format of the selected_resources variable output, which contains the fully-qualified names of selected models and snapshots.

LANGUAGE: json
CODE:
["model.my_project.model1", "model.my_project.model2", "snapshot.my_project.my_snapshot"]

----------------------------------------

TITLE: Generating Static dbt Documentation
DESCRIPTION: This command generates a static documentation page suitable for hosting on cloud storage providers, combining catalog.json and manifest.json into a single index.html file.

LANGUAGE: shell
CODE:
dbt docs generate --static

----------------------------------------

TITLE: Configuring Network Rules and External Access in YAML
DESCRIPTION: Creates network rules and external access integration using pre-hooks in dbt model configuration

LANGUAGE: yaml
CODE:
models:
  - name: external_access_sample
    config:
      pre_hook: 
        - "create or replace network rule test_network_rule type = host_port mode = egress value_list= ('api.carbonintensity.org.uk:443');"
        - "create or replace external access integration test_external_access_integration allowed_network_rules = (test_network_rule) enabled = true;"

----------------------------------------

TITLE: Executing Intersection Operations for Descendants in dbt CLI
DESCRIPTION: Demonstrates finding common descendants between staging models using the + suffix operator with comma separation.

LANGUAGE: bash
CODE:
dbt run --select "stg_invoices+,stg_accounts+"

----------------------------------------

TITLE: Configuring Lookback in SQL Model
DESCRIPTION: Sets the lookback value to 2 for the user_sessions model directly in the SQL model file using a config block.

LANGUAGE: sql
CODE:
{{ config(
    lookback=2
) }}

----------------------------------------

TITLE: Unloading a model to S3 using post-hook in Redshift
DESCRIPTION: Example of using a post-hook to unload a model to S3 in Redshift. This demonstrates how to use the 'this' variable in hooks.

LANGUAGE: sql
CODE:
{{ config(
  post_hook = "unload ('select from {{ this }}') to 's3:/bucket_name/{{ this }}'"
) }}

select ...

----------------------------------------

TITLE: Subdirectory Model Configuration
DESCRIPTION: Shows how to apply configuration to models within a specific subdirectory of a project.

LANGUAGE: yaml
CODE:
name: jaffle_shop

models:
  jaffle_shop:
    staging:
      +enabled: false

----------------------------------------

TITLE: Testing a Specific dbt Source Using Shell Command
DESCRIPTION: This command runs tests on a single source and all its tables in a dbt project. It specifies the source name after the 'source:' selector.

LANGUAGE: shell
CODE:
dbt test --select source:jaffle_shop

----------------------------------------

TITLE: Adding Comments to Database Objects in SQL
DESCRIPTION: This snippet shows the syntax for adding comments to database objects such as tables and views. It demonstrates how to use the 'COMMENT ON' statement to add metadata to database objects.

LANGUAGE: sql
CODE:
comment on [database object type] <database object name> is 'comment text here';

----------------------------------------

TITLE: dbt Incremental Check
DESCRIPTION: Example of checking if a model is running incrementally

LANGUAGE: sql
CODE:
{{ is_incremental() }}

----------------------------------------

TITLE: Configuring Index and Distribution in Azure Synapse
DESCRIPTION: Example of how to configure custom index and distribution settings for a Synapse table model. Shows how to override default CLUSTERED COLUMNSTORE INDEX and ROUND_ROBIN distribution settings.

LANGUAGE: sql
CODE:
{{
    config(
        index='HEAP',
        dist='ROUND_ROBIN'
        )
}}
SELECT * FROM {{ ref('some_model') }}

----------------------------------------

TITLE: Configuring persist_docs for Models in SQL files
DESCRIPTION: This snippet demonstrates how to enable persist_docs for individual models within SQL files using the config block.

LANGUAGE: sql
CODE:
{{ config(
  persist_docs={"relation": true, "columns": true}
) }}

select ...

----------------------------------------

TITLE: Installing dbt Adapter Plugin from Source
DESCRIPTION: Example commands for installing a dbt adapter plugin (Redshift) from source code, including cloning the repository and installing via pip.

LANGUAGE: shell
CODE:
git clone https://github.com/dbt-labs/dbt-redshift.git
cd dbt-redshift
python -m pip install .

----------------------------------------

TITLE: Logging batch details in dbt microbatch models
DESCRIPTION: This snippet demonstrates how to log batch details including ID and event time range during the execution of a microbatch model in dbt.

LANGUAGE: jinja
CODE:
{% if model.batch %}
  {{ log("Processing batch with ID: " ~ model.batch.id, info=True) }}
  {{ log("Batch event time range: " ~ model.batch.event_time_start ~ " to " ~ model.batch.event_time_end, info=True) }}
{% endif %}

----------------------------------------

TITLE: Configuring Multiple Seed Directories in dbt_project.yml
DESCRIPTION: Illustrates how to split seeds across two directories, 'seeds' and 'custom_seeds'. Note that it's recommended to use subdirectories within 'seeds/' instead for better organization.

LANGUAGE: yml
CODE:
seed-paths: ["seeds", "custom_seeds"]

----------------------------------------

TITLE: CLI Output Example - dbt Model Creation
DESCRIPTION: Example of dbt's CLI output showing the creation of a view model with timing information

LANGUAGE: bash
CODE:
21:35:48  6 of 7 OK created view model dbt_testing.name_list......................... [CREATE VIEW in 0.17s]

----------------------------------------

TITLE: Creating dbt Slack Daily Summary Model
DESCRIPTION: SQL transformation to aggregate Slack messages into daily summaries per user and community

LANGUAGE: sql
CODE:
select
    DATE_TRUNC(DATE(message_time), DAY) AS day,
    domain,
    username,
    count(*) AS messages
from {{ source('metrics', 'slack_messages') }}

group by day, domain, username

----------------------------------------

TITLE: Adding a Tag to a Specific Data Test
DESCRIPTION: This snippet demonstrates how to add a tag to a specific instance of a generic data test in a YAML file and to a singular data test in an SQL file.

LANGUAGE: yaml
CODE:
models:
  - name: my_model
    columns:
      - name: id
        tests:
          - unique:
              tags: ['my_tag']

LANGUAGE: sql
CODE:
{{ config(tags = ['my_tag']) }}

select ...

----------------------------------------

TITLE: Running dbt Commands with Exposure References
DESCRIPTION: These dbt CLI commands demonstrate how to run and test models that an exposure depends on. The commands use the '+exposure:' selector to specify the exposure by name.

LANGUAGE: shell
CODE:
dbt run -s +exposure:weekly_jaffle_report
dbt test -s +exposure:weekly_jaffle_report

----------------------------------------

TITLE: Displaying dbt Project Macro Count
DESCRIPTION: Shows the output of a dbt run command that demonstrates the default number of macros included in a dbt project, even with minimal user-defined content.

LANGUAGE: shell
CODE:
$ dbt run
Running with dbt=0.17.0
Found 1 model, 0 tests, 0 snapshots, 0 analyses, 138 macros, 0 operations, 0 seed files, 0 sources

----------------------------------------

TITLE: Configuring Service Account File Authentication
DESCRIPTION: Configuration for connecting to BigQuery using a service account keyfile for authentication.

LANGUAGE: yaml
CODE:
my-bigquery-db:
  target: dev
  outputs:
    dev:
      type: bigquery
      method: service-account
      project: GCP_PROJECT_ID
      dataset: DBT_DATASET_NAME
      threads: 4
      keyfile: /PATH/TO/BIGQUERY/keyfile.json

----------------------------------------

TITLE: Converting Warnings to Errors in dbt CLI
DESCRIPTION: This snippet demonstrates how to use the --warn-error flag in the dbt CLI to convert all warnings to errors during a dbt run command.

LANGUAGE: text
CODE:
dbt --warn-error run
...

----------------------------------------

TITLE: Displaying Intermediate Layer File Structure in Shell
DESCRIPTION: Shows the directory structure for intermediate models in a dbt project, demonstrating the organization of files and folders based on business groupings.

LANGUAGE: shell
CODE:
models/intermediate
‚îî‚îÄ‚îÄ finance
    ‚îú‚îÄ‚îÄ _int_finance__models.yml
    ‚îî‚îÄ‚îÄ int_payments_pivoted_to_orders.sql

----------------------------------------

TITLE: Incorrect Absolute Path Configuration
DESCRIPTION: Shows an example of incorrect macro path configuration using absolute paths, which should be avoided.

LANGUAGE: yml
CODE:
macro-paths: ["/Users/username/project/macros"]

----------------------------------------

TITLE: Configuring Analysis Paths with Relative Path
DESCRIPTION: This example demonstrates the recommended way to specify analysis paths using a relative path. It sets the analysis directory to 'analyses' within the project structure.

LANGUAGE: yaml
CODE:
analysis-paths: ["analyses"]

----------------------------------------

TITLE: Configuring Printer Width in profiles.yml
DESCRIPTION: YAML configuration to modify the default printer width from 80 characters to a custom width.

LANGUAGE: yaml
CODE:
config:
  printer_width: 120

----------------------------------------

TITLE: SQL AND Operator Usage Examples
DESCRIPTION: Demonstrates the use of AND operator in WHERE clauses, case statements, and joins. It shows the basic syntax for each use case.

LANGUAGE: sql
CODE:
-- and in a where clause
where <condition_1> and <condition_2> and‚Ä¶ 

-- and in a case statement
case when <condition_1> and <condition_2> then <result_1> ‚Ä¶ 

-- and in a join
from <table_a>
join <table_b> on
<a_id_1> = <b_id_1> and <a_id_2> = <b_id_2>

----------------------------------------

TITLE: Running Exports for a Single Saved Query in dbt Cloud CLI
DESCRIPTION: Use the dbt sl export command to run exports for a single saved query in the development environment. This command allows specifying parameters like the saved query name, export selection, and output configurations.

LANGUAGE: bash
CODE:
dbt sl export --saved-query sq_name

----------------------------------------

TITLE: Original SQL Model Code
DESCRIPTION: Initial SQL implementation of the order_items_summary model before bug fix.

LANGUAGE: sql
CODE:
with

order_items as (

    select * from {{ ref('order_items') }}

)

select
    order_id,

    sum(supply_cost) as order_cost,
    sum(product_price) as order_items_subtotal,
    count(order_item_id) as count_order_items,
    count(
        case
            when is_food_item then 1
            else 0
        end
    ) as count_food_items,
    count(
        case
            when is_drink_item then 1
            else 0
        end
    ) as count_drink_items

from order_items

group by 1

----------------------------------------

TITLE: Configuring dbt Snapshots in YAML
DESCRIPTION: Shows how to configure a dbt snapshot using the 'config' property in a YAML file. This allows setting snapshot-specific configurations.

LANGUAGE: yaml
CODE:
version: 2

snapshots:
  - name: <snapshot_name>
    config:
      [<snapshot_config>](/reference/snapshot-configs): <config_value>
      ...

----------------------------------------

TITLE: Accessing model config in dbt Jinja
DESCRIPTION: This snippet demonstrates how to access the 'model' object's config settings and use conditional logic based on materialization type.

LANGUAGE: jinja
CODE:
{% if model.config.materialized == 'view' %}
  {{ log(model.name ~ " is a view.", info=True) }}
{% endif %}

----------------------------------------

TITLE: Executing dbt Environment Show Command
DESCRIPTION: Basic command to display local and dbt Cloud configuration details using the dbt environment show command or its shorthand version.

LANGUAGE: shell
CODE:
dbt environment show

LANGUAGE: shell
CODE:
dbt env show

----------------------------------------

TITLE: Example of Text Log Format in dbt
DESCRIPTION: Shows the default text format for console logs in dbt, which includes a simple timestamp prefix.

LANGUAGE: text
CODE:
23:30:16  Running with dbt=1.8.0
23:30:17  Registered adapter: postgres=1.8.0

----------------------------------------

TITLE: Adding Separators in Surrogate Key Generation
DESCRIPTION: This SQL example shows how to add separators between concatenated fields when generating surrogate keys to avoid potential collisions between different combinations of values.

LANGUAGE: sql
CODE:
select
  *,
  concat(
    coalesce(cast(user_id as string), ''),
    '|',
    coalesce(cast(product_id as string), '')
    ) as _surrogate_key
from example_ids

----------------------------------------

TITLE: Setting File Log Colors via CLI Flags
DESCRIPTION: Commands to enable or disable colored output in file logs using CLI flags.

LANGUAGE: text
CODE:
dbt --use-colors-file run
dbt --no-use-colors-file run

----------------------------------------

TITLE: Special Properties in dbt YAML Files
DESCRIPTION: List of special properties that can only be defined in YAML files and cannot be configured using config() blocks or dbt_project.yml. These properties have unique characteristics such as special Jinja rendering contexts or resource creation capabilities.

LANGUAGE: markdown
CODE:
- description
- tests
- docs
- columns
- quote
- source properties (loaded_at_field, freshness)
- exposure properties (type, maturity)
- macro properties (arguments)

----------------------------------------

TITLE: Basic SQL Query with LIMIT Clause
DESCRIPTION: Demonstrates the basic structure of a SQL query using the LIMIT clause to restrict the number of returned rows.

LANGUAGE: sql
CODE:
select
	some_rows
from my_data_source
limit <integer>

----------------------------------------

TITLE: Querying Metrics with Time Dimensions
DESCRIPTION: Example of querying metrics using time dimensions in dbt Semantic Layer CLI commands.

LANGUAGE: bash
CODE:
# dbt Cloud users
dbt sl query --metrics users_created,users_deleted --group-by metric_time__year --order-by metric_time__year

# dbt Core users
mf query --metrics users_created,users_deleted --group-by metric_time__year --order-by metric_time__year

----------------------------------------

TITLE: Configuring hard_deletes in SQL snapshot
DESCRIPTION: SQL-level configuration for hard_deletes within a snapshot file. Shows how to set the hard_deletes option along with other snapshot configurations.

LANGUAGE: sql
CODE:
{{
    config(
        unique_key='id',
        strategy='timestamp',
        updated_at='updated_at',
        hard_deletes='ignore' | 'invalidate' | 'new_record'
    )
}}

----------------------------------------

TITLE: Running dbt in Quiet Mode
DESCRIPTION: Demonstrates how to run dbt in quiet mode from the command line, suppressing non-error logs.

LANGUAGE: text
CODE:
dbt --quiet run
...

----------------------------------------

TITLE: Setting dbt Environment Variable in Shell
DESCRIPTION: This snippet demonstrates how to set a dbt environment variable using the export command in a shell environment. It shows the general format for setting any dbt configuration option as an environment variable.

LANGUAGE: text
CODE:
$ export DBT_<THIS-CONFIG>=True
dbt run

----------------------------------------

TITLE: Running Modified Data Models with dbt Build
DESCRIPTION: Command to build only changed data models and their downstream dependencies when a PR merges. This is the default merge job behavior that helps reduce computing costs while ensuring latest changes are in production.

LANGUAGE: sql
CODE:
dbt build --select state:modified+

----------------------------------------

TITLE: Using dbt-utils Macros for Table Union
DESCRIPTION: Example of using dbt-utils macros to combine multiple tables created by Fivetran's Google Drive connector into a single model.

LANGUAGE: sql
CODE:
{{ config(materialized='table') }}

{% set relations = dbt_utils.get_relations_by_pattern('fivetran_schema', 'weekly_export_%') %}

{{ dbt_utils.union_relations(relations) }}

----------------------------------------

TITLE: Bundling Multiple Unit Tests
DESCRIPTION: A macro that combines multiple unit tests to run them together

LANGUAGE: sql
CODE:
{% macro run_unit_tests() %}

    {% do test_to_literal() %}

    {% do another_test() %}

{% endmacro %}

----------------------------------------

TITLE: Basic Model Path Configuration in dbt
DESCRIPTION: Demonstrates the basic syntax for defining model paths in dbt_project.yml. This configuration tells dbt where to look for model files.

LANGUAGE: yml
CODE:
model-paths: [directorypath]

----------------------------------------

TITLE: Configuring event_time in dbt_project.yml for Models
DESCRIPTION: Example of setting event_time configuration for models in the project configuration file.

LANGUAGE: yml
CODE:
models:
  [resource-path:]:
    +event_time: my_time_field

----------------------------------------

TITLE: Defining Project-Specific Overviews in Markdown
DESCRIPTION: This snippet demonstrates how to create custom overview pages for specific dbt projects or packages within a documentation site. It includes examples for 'dbt_utils' and 'snowplow' packages.

LANGUAGE: markdown
CODE:
{% docs __dbt_utils__ %}
# Utility macros
Our dbt project heavily uses this suite of utility macros, especially:
- `surrogate_key`
- `test_equality`
- `pivot`
{% enddocs %}

{% docs __snowplow__ %}
# Snowplow sessionization
Our organization uses this package of transformations to roll Snowplow events
up to page views and sessions.
{% enddocs %}

----------------------------------------

TITLE: Using Relations in SQL Templates
DESCRIPTION: Shows various ways to use Relation objects in SQL templates, including accessing properties and checking relation types.

LANGUAGE: jinja2
CODE:
{% set relation = api.Relation.create(schema='snowplow', identifier='events') %}

-- Return the `database` for this relation
{{ relation.database }}

-- Return the `schema` (or dataset) for this relation
{{ relation.schema }}

-- Return the `identifier` for this relation
{{ relation.identifier }}

-- Return relation name without the database
{{ relation.include(database=false) }}

-- Return true if the relation is a table
{{ relation.is_table }}

-- Return true if the relation is a view
{{ relation.is_view }}

-- Return true if the relation is a cte
{{ relation.is_cte }}

----------------------------------------

TITLE: Configuring ODBC Connection in dbt Profiles
DESCRIPTION: YAML configuration for setting up an ODBC connection to Databricks SQL endpoint or interactive cluster in the dbt profiles file.

LANGUAGE: yaml
CODE:
your_profile_name:
  target: dev
  outputs:
    dev:
      type: spark
      method: odbc
      driver: [path/to/driver]
      schema: [database/schema name]
      host: [yourorg.sparkhost.com]
      organization: [org id]    # Azure Databricks only
      token: [abc123]
      
      # one of:
      endpoint: [endpoint id]
      cluster: [cluster id]
      
      # optional
      port: [port]              # default 443
      user: [user]
      server_side_parameters:
        "spark.driver.memory": "4g" 

----------------------------------------

TITLE: Calculating Business Hours Using Subquery in SQL
DESCRIPTION: SQL query that calculates business hours between two dates by summing partial hours from the start time, full business hours in between, and partial hours at the end time. Uses a subquery to count business hours from an hours table.

LANGUAGE: sql
CODE:
select
    (60 - extract(minute from start_time) +
    ( select count_if(is_business_hour) * 60 from all_business_hours where date_hour > start_date  and date_hour < end_date ) +
   ( extract(minute  from end_time)
from table

----------------------------------------

TITLE: Setting Email ID in Jinja
DESCRIPTION: Demonstrates setting a string variable containing an email address.

LANGUAGE: jinja
CODE:
{% set email_id = "'admin@example.com'" %}

----------------------------------------

TITLE: YAML Surrogate Key Example
DESCRIPTION: Example showing how to create a surrogate key by combining multiple columns using a pipe separator.

LANGUAGE: yaml
CODE:
entities:
  - name: brand_target_key # Entity name or identified.
    type: foreign # This can be any entity type key. 
    expr: date_key || '|' || brand_code # Defines the expression for linking fields to form the surrogate key.

----------------------------------------

TITLE: Optimized Incremental Model Implementation
DESCRIPTION: Refactored implementation as an incremental model with conditional logic for handling historical data loads versus incremental updates.

LANGUAGE: sql
CODE:
{{config(materialized = 'incremental', unique_key = 'invocation_id')}}

with model_execution as (

    select *
    from {{ ref('stg_dbt_run_model_events') }}
    where
        1=1
    {% if target.name == 'dev' %}

        and collector_tstamp >= dateadd(d, -{{var('testing_days_of_data')}}, current_date)

    {% elif is_incremental() %}

        --incremental runs re-process a full day everytime to get an accurate mode below
        and collector_tstamp > (select max(max_collector_tstamp)::date from {{ this }})

    {% endif %}

),

{% if is_incremental() %}
new_models as (

    select
        project_id,
        model_id,
        invocation_id,
        dvce_created_tstamp,
        true as is_new
    from {{ ref('stg_dbt_run_model_events') }} as base_table
    where
        exists (
                select 1
                from model_execution
                where
                    base_table.project_id = model_execution.project_id
                    and base_table.model_id = model_execution.model_id
            )
    qualify
        row_number() over(partition by project_id, model_id order by dvce_created_tstamp) = 1


),
{% endif %}

diffed as (

    select model_execution.*,

        {% if is_incremental() %}

            new_models.is_new,

        {% else %}

            row_number() over (
                partition by project_id, model_id
                order by dvce_created_tstamp
            ) = 1 as is_new,

        {% endif %}

        model_execution.contents != mode(model_execution.contents) over (
            partition by model_execution.project_id, model_execution.model_id, model_execution.dvce_created_tstamp::date
        ) as is_changed

    from model_execution
        {% if is_incremental() %}
            left join new_models on
                model_execution.project_id = new_models.project_id
                and model_execution.model_id = new_models.model_id
                and model_execution.invocation_id = new_models.invocation_id
                and model_execution.dvce_created_tstamp = new_models.dvce_created_tstamp
        {% endif %}

),

final as (

    select
        invocation_id,
        max(collector_tstamp) as max_collector_tstamp,
        max(model_complexity) as model_complexity,
        max(model_total) as count_models,
        sum(case when is_new or is_changed then 1 else 0 end) as count_changed,
        sum(case when skipped = true then 1 else 0 end) as count_skip,
        sum(case when error is null or error = 'false' then 0 else 1 end) as count_error,
        sum(case when (error is null or error = 'false') and skipped = false then 1 else 0 end) as count_succeed

    from diffed
    group by 1

)

select * from final

----------------------------------------

TITLE: Simple Table Name Mapping Example in dbt
DESCRIPTION: Example showing how to map a simplified table name 'orders' to the actual database table 'api_orders', with corresponding SQL compilation example.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: jaffle_shop
    tables:
      - name: orders
        identifier: api_orders

LANGUAGE: sql
CODE:
select * from {{ source('jaffle_shop', 'orders') }}

LANGUAGE: sql
CODE:
select * from jaffle_shop.api_orders

----------------------------------------

TITLE: Using Default Values with var() Function
DESCRIPTION: Demonstrates how to use the var() function with a default value. If the variable is not defined in the project configuration, the specified default value will be used instead.

LANGUAGE: sql
CODE:
-- Use 'activation' as the event_type if the variable is not defined.
select * from events where event_type = '{{ var("event_type", "activation") }}'

----------------------------------------

TITLE: Creating Snowflake Databases
DESCRIPTION: SQL commands to create raw and analytics databases using sysadmin role.

LANGUAGE: sql
CODE:
use role sysadmin;
create database raw;
create database analytics;

----------------------------------------

TITLE: Setting Log Format for dbt CLI
DESCRIPTION: Demonstrates how to set the log format to JSON when running dbt from the command line.

LANGUAGE: text
CODE:
dbt --log-format json run

----------------------------------------

TITLE: Implementing DATEADD in Snowflake
DESCRIPTION: Standard Snowflake DATEADD function implementation that supports hour, minute, and second intervals.

LANGUAGE: sql
CODE:
dateadd( {{ datepart }}, {{ interval }}, {{ from_date }} )

----------------------------------------

TITLE: Configuring Incremental Model Using this Reference in dbt
DESCRIPTION: Demonstrates how to use the 'this' reference in an incremental model to filter data based on the maximum event_time from the current model. The example shows a typical pattern for incremental loading with a where clause that only executes during incremental runs.

LANGUAGE: sql
CODE:
{{ config(materialized='incremental') }}

select
    *,
    my_slow_function(my_column)

from raw_app_data.events

{% if is_incremental() %}
  where event_time > (select max(event_time) from {{ this }})
{% endif %}

----------------------------------------

TITLE: Configuring Concurrent Batches in dbt_project.yml
DESCRIPTION: This YAML snippet demonstrates how to set the 'concurrent_batches' config in the dbt_project.yml file to enable parallel execution of microbatches for all models.

LANGUAGE: yaml
CODE:
models:
  +concurrent_batches: true # value set to true to run batches in parallel

----------------------------------------

TITLE: Configuring Timestamp Strategy in YAML (dbt 1.9+)
DESCRIPTION: Configuration for timestamp-based snapshot strategy using YAML format. Requires specifying the snapshot name, relation source, and updated_at column.

LANGUAGE: yaml
CODE:
snapshots:
- name: snapshot_name
  relation: source('my_source', 'my_table')
  config:
    strategy: timestamp
    updated_at: column_name

----------------------------------------

TITLE: Implementing generate_alias_name Macro in dbt
DESCRIPTION: This Jinja macro defines the generate_alias_name function, which controls how dbt generates aliases for models. It accepts a custom alias name and a node, using the custom alias if provided, or falling back to the node name or versioned name.

LANGUAGE: jinja2
CODE:
{% macro generate_alias_name(custom_alias_name=none, node=none) -%}

    {%- if custom_alias_name -%}

        {{ custom_alias_name | trim }}

    {%- elif node.version -%}

        {{ return(node.name ~ "_v" ~ (node.version | replace(".", "_"))) }}

    {%- else -%}

        {{ node.name }}

    {%- endif -%}

{%- endmacro %}

----------------------------------------

TITLE: Configuring Custom Test Directory in dbt_project.yml
DESCRIPTION: This YAML snippet demonstrates how to update the test-paths configuration in the dbt_project.yml file to specify a custom directory for storing test files. In this example, tests will be stored in a directory named 'my_cool_tests' instead of the default 'tests' directory.

LANGUAGE: yaml
CODE:
test-paths: ["my_cool_tests"]

----------------------------------------

TITLE: Creating an Intermediate Model for Pivoting Payments in SQL
DESCRIPTION: Demonstrates an intermediate model that pivots payment data to the order grain. It uses Jinja templating to create a dynamic pivot query for different payment methods.

LANGUAGE: sql
CODE:
{%- set payment_methods = ['bank_transfer','credit_card','coupon','gift_card'] -%}

with

payments as (

   select * from {{ ref('stg_stripe__payments') }}

),

pivot_and_aggregate_payments_to_order_grain as (

   select
      order_id,
      {% for payment_method in payment_methods -%}

         sum(
            case
               when payment_method = '{{ payment_method }}' and
                    status = 'success'
               then amount
               else 0
            end
         ) as {{ payment_method }}_amount,

      {%- endfor %}
      sum(case when status = 'success' then amount end) as total_amount

   from payments

   group by 1

)

select * from pivot_and_aggregate_payments_to_order_grain

----------------------------------------

TITLE: Basic dbt Project Name Configuration
DESCRIPTION: The basic required configuration for setting a project name in dbt_project.yml. The name must consist of letters, digits, and underscores only, and cannot start with a digit.

LANGUAGE: yml
CODE:
name: string

----------------------------------------

TITLE: Example Seed File with Pipe Delimiter
DESCRIPTION: This text snippet shows an example of a seed file using a pipe (|) as the delimiter for separating values.

LANGUAGE: text
CODE:
col_a|col_b|col_c
1|2|3
4|5|6
...

----------------------------------------

TITLE: DATEADD Implementation in Databricks
DESCRIPTION: Shows the Databricks date_add function syntax which specifically handles day intervals.

LANGUAGE: sql
CODE:
date_add( {{ startDate }}, {{ numDays }} )

----------------------------------------

TITLE: Basic DBT Macro Argument Type Definition in YAML
DESCRIPTION: Template showing the basic structure for documenting macro arguments with types in a YAML file.

LANGUAGE: yaml
CODE:
version: 2

macros:
  - name: <macro name>
    arguments:
      - name: <arg name>
        type: <string>


----------------------------------------

TITLE: Append Strategy Incremental Model in AWS Glue
DESCRIPTION: Example of implementing an append strategy for incremental models in AWS Glue. This approach uses a simple insert into statement to add new records based on an event timestamp comparison.

LANGUAGE: sql
CODE:
{{ config(
    materialized='incremental',
    incremental_strategy='append',
) }}

--  All rows returned by this query will be appended to the existing table

select * from {{ ref('events') }}
{% if is_incremental() %}
  where event_ts > (select max(event_ts) from {{ this }})
{% endif %}

----------------------------------------

TITLE: Original Ephemeral Model Implementation
DESCRIPTION: Initial implementation of dbt_model_summary as an ephemeral model using window functions to track model execution statistics.

LANGUAGE: sql
CODE:
{{config(materialized = 'ephemeral')}}

with model_execution as (

    select * from {{ ref('stg_dbt_run_model_events') }}

),

diffed as (

    select *,

        row_number() over (
            partition by project_id, model_id
            order by dvce_created_tstamp
        ) = 1 as is_new,

        contents != mode(contents) over (
            partition by project_id, model_id, dvce_created_tstamp::date
        ) as is_changed

    from model_execution

),

final as (

    select
        invocation_id,
        max(model_complexity) as model_complexity,
        max(model_total) as count_models,
        sum(case when is_new or is_changed then 1 else 0 end) as count_changed,
        sum(case when skipped = true then 1 else 0 end) as count_skip,
        sum(case when error is null or error = 'false' then 0 else 1 end) as count_error,
        sum(case when (error is null or error = 'false') and skipped = false then 1 else 0 end) as count_succeed

    from diffed
    group by 1

)

select * from final

----------------------------------------

TITLE: Mac Virtual Environment Setup
DESCRIPTION: Commands to set up and activate a virtual environment on Mac, including installing dependencies.

LANGUAGE: shell
CODE:
python3 -m venv venv
source venv/bin/activate
python3 -m pip install --upgrade pip
python3 -m pip install -r requirements.txt
source venv/bin/activate

----------------------------------------

TITLE: Variable Definition in dbt Project Configuration
DESCRIPTION: Shows how to define variables in the dbt_project.yml configuration file. This example sets up a project configuration with an event_type variable set to 'activation'.

LANGUAGE: yaml
CODE:
name: my_dbt_project
version: 1.0.0

config-version: 2

# Define variables here
vars:
  event_type: activation

----------------------------------------

TITLE: Collecting Speedscope Profiles of dbt Commands with py-spy
DESCRIPTION: This snippet shows how to use py-spy to collect speedscope profiles of dbt commands. It includes the command to install py-spy and the command to record a profile for the dbt parse command.

LANGUAGE: shell
CODE:
python -m pip install py-spy
sudo py-spy record -s -f speedscope -- dbt parse

----------------------------------------

TITLE: Correct CLI Flag Placement in dbt Commands
DESCRIPTION: Demonstrates the proper placement of CLI flags after the 'dbt' prefix and subcommands, using the '--no-populate-cache' flag as an example.

LANGUAGE: bash
CODE:
dbt run --no-populate-cache

----------------------------------------

TITLE: Configuring Databricks Token Authentication in dbt
DESCRIPTION: YAML configuration for connecting to Databricks using token-based authentication. Requires host, http_path, schema, and personal access token (PAT).

LANGUAGE: yaml
CODE:
your_profile_name:
  target: dev
  outputs:
    dev:
      type: databricks
      catalog: CATALOG_NAME #optional catalog name if you are using Unity Catalog]
      schema: SCHEMA_NAME # Required
      host: YOURORG.databrickshost.com # Required
      http_path: /SQL/YOUR/HTTP/PATH # Required
      token: dapiXXXXXXXXXXXXXXXXXXXXXXX # Required Personal Access Token (PAT) if using token-based authentication
      threads: 1_OR_MORE  # Optional, default 1

----------------------------------------

TITLE: Creating Tabbed Resource Views in dbt
DESCRIPTION: Shows how to create tabbed views for different resource types like models and sources. Includes configuration for model files and project settings.

LANGUAGE: sql
CODE:
        <Tabs
        defaultValue="models"
        values={[
            { label: 'Models', value: 'models', },
            { label: 'Sources', value:'sources', },
        ]
        }>

        <TabItem value="models">

        <File name='models/<modelname>.sql'>

        ```

        \{\{ config(

        ) \}\}

        select ...


        ```

        </File>

        <File name='dbt_project.yml'>

        ```
        models:
        [resource-path](/reference/resource-configs/resource-path):


        ```

        </File>

        </TabItem>

        <TabItem value="sources">

        <File name='dbt_project.yml'>

        ```
        sources:
        [resource-path](/reference/resource-configs/resource-path):


        ```

        </File>

        </TabItem>

        </Tabs>

----------------------------------------

TITLE: Project-Level Materialization Configuration
DESCRIPTION: Project-level configuration for materializing models as views and tables, with columnstore settings in dbt_project.yml.

LANGUAGE: yaml
CODE:
models:
  your_project_name:
    materialized: view
    staging:
      materialized: table
      as_columnstore: False

----------------------------------------

TITLE: Zip Strict with Valid Lists
DESCRIPTION: Demonstrates the zip_strict context method with valid iterables. Functions similarly to regular zip but enforces stricter type checking.

LANGUAGE: jinja
CODE:
{% set my_list_a = [1, 2] %}
{% set my_list_b = ['alice', 'bob'] %}
{% set my_zip = zip_strict(my_list_a, my_list_b) | list %}
{% do log(my_zip) %}  {# [(1, 'alice'), (2, 'bob')] #}

----------------------------------------

TITLE: Defining Data Types for BigQuery in dbt Unit Tests
DESCRIPTION: This snippet demonstrates how to specify various data types including integer, float, string, date, timestamp, geography, JSON, arrays, and structs in a dbt unit test for BigQuery.

LANGUAGE: yaml
CODE:
unit_tests:
  - name: test_my_data_types
    model: fct_data_types
    given:
      - input: ref('stg_data_types')
        rows:
         - int_field: 1
           float_field: 2.0
           str_field: my_string
           str_escaped_field: "my,cool'string"
           date_field: 2020-01-02
           timestamp_field: 2013-11-03 00:00:00-0
           timestamptz_field: 2013-11-03 00:00:00-0
           bigint_field: 1
           geography_field: 'st_geogpoint(75, 45)'
           json_field: {"name": "Cooper", "forname": "Alice"}
           str_array_field: ['a','b','c']
           int_array_field: [1, 2, 3]
           date_array_field: ['2020-01-01']
           struct_field: 'struct("Isha" as name, 22 as age)'
           struct_of_struct_field: 'struct(struct(1 as id, "blue" as color) as my_struct)'
           struct_array_field: ['struct(st_geogpoint(75, 45) as my_point)', 'struct(st_geogpoint(75, 35) as my_point)']

----------------------------------------

TITLE: Exporting MetricFlow Query Results to CSV
DESCRIPTION: Command to export the results of a MetricFlow query to a CSV file in dbt Core.

LANGUAGE: bash
CODE:
mf query --metrics order_total --group-by metric_time,is_food_order --limit 10 --order-by -metric_time --where "is_food_order = True" --start-time '2017-08-22' --end-time '2017-08-27' --csv query_example.csv

----------------------------------------

TITLE: Example of Fact Table with Aggregating Index in Firebolt
DESCRIPTION: This snippet provides an example of configuring a fact table with an aggregating index in Firebolt, including the primary index and aggregation settings.

LANGUAGE: jinja
CODE:
{{ config(
    materialized = "table",
    table_type = "fact",
    primary_index = "id",
    indexes = [
      {
        "index_type": "aggregating",
        "key_columns": "order_id",
        "aggregation": ["COUNT(DISTINCT status)", "AVG(customer_id)"]
      }
    ]
) }}

----------------------------------------

TITLE: Schema Usage Grants with On-Run-End Hooks
DESCRIPTION: Example of granting schema usage permissions using on-run-end hooks, which is still necessary as schema-level grants are not part of the new grants config.

LANGUAGE: yaml
CODE:
on-run-end:
	# better as a macro
	- "{% for schema in schemas %}grant usage on schema {{ schema }} to reporter;{% endfor %}"

----------------------------------------

TITLE: Displaying dbt SQL Error Example in Shell
DESCRIPTION: Example shell output showing how dbt displays database errors when encountering SQL syntax issues. The example demonstrates a syntax error in a view model creation where an unexpected identifier was found.

LANGUAGE: shell
CODE:
$ dbt run --select customers
Running with dbt=0.15.0
Found 3 models, 9 tests, 0 snapshots, 0 analyses, 133 macros, 0 operations, 0 seed files, 0 sources

14:04:12 | Concurrency: 1 threads (target='dev')
14:04:12 |
14:04:12 | 1 of 1 START view model dbt_alice.customers.......................... [RUN]
14:04:13 | 1 of 1 ERROR creating view model dbt_alice.customers................. [ERROR in 0.81s]
14:04:13 |
14:04:13 | Finished running 1 view model in 1.68s.

Completed with 1 error and 0 warnings:

Database Error in model customers (models/customers.sql)
  Syntax error: Expected ")" but got identifier `your-info-12345` at [13:15]
  compiled SQL at target/run/jaffle_shop/customers.sql

Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1

----------------------------------------

TITLE: Configuring Warning Options in dbt Project YAML
DESCRIPTION: This YAML configuration in dbt_project.yml shows how to set up warning error options, including specifying which warnings to treat as errors, which to warn about, and which to silence.

LANGUAGE: yaml
CODE:
name: "my_dbt_project"
tests:
  +enabled: True
flags:
  warn_error_options:
    error: # Previously called "include"
    warn: # Previously called "exclude"
    silence: # To silence or ignore warnings
      - NoNodesForSelectionCriteria

----------------------------------------

TITLE: Configuring Quiet Mode in dbt profiles.yml
DESCRIPTION: Shows how to configure quiet mode in the dbt profiles.yml file to suppress non-error logs.

LANGUAGE: yaml
CODE:
config:
  quiet: true

----------------------------------------

TITLE: Configuring Model Grants Using New Grants Config
DESCRIPTION: Demonstration of the new grants config feature in dbt v1.2, showing how to grant and revoke select permissions on an incremental model.

LANGUAGE: sql
CODE:
{{ config(
	materialized = 'incremental',
	grants = {'select': ['another_user']}
) }}

select ...

----------------------------------------

TITLE: Python Spark Model with Configuration
DESCRIPTION: Example of a Python model using Spark with custom engine configuration and encryption settings

LANGUAGE: python
CODE:
def model(dbt, spark_session):
    dbt.config(
        materialized="table",
        engine_config={
            "CoordinatorDpuSize": 1,
            "MaxConcurrentDpus": 3,
            "DefaultExecutorDpuSize": 1
        },
        spark_encryption=True,
        spark_cross_account_catalog=True,
        spark_requester_pays=True
        polling_interval=15,
        timeout=120,
    )

    data = [(1,), (2,), (3,), (4,)]

    df = spark_session.createDataFrame(data, ["A"])

    return df

----------------------------------------

TITLE: Configuring All Models in dbt
DESCRIPTION: Shows how to apply configuration to all models in a dbt project without using a resource path.

LANGUAGE: yaml
CODE:
models:
  +enabled: false

----------------------------------------

TITLE: Overriding Model Materialization in SQL File
DESCRIPTION: This snippet shows how to override the project-level materialization configuration for a specific model. It sets the 'customers' model to be materialized as a view using a config block at the top of the SQL file.

LANGUAGE: sql
CODE:
{{
  config(
    materialized='view'
  )
}}

with customers as (

    select
        id as customer_id
        ...

)

----------------------------------------

TITLE: Accessing Models in DBT Graph
DESCRIPTION: Example demonstrating how to access and print information about models in the Snowplow package using the graph variable.

LANGUAGE: sql
CODE:
{% if execute %}
  {% for node in graph.nodes.values()
     | selectattr("resource_type", "equalto", "model")
     | selectattr("package_name", "equalto", "snowplow") %}
    {% do log(node.unique_id ~ ", materialized: " ~ node.config.materialized, info=true) %}
  {% endfor %}
{% endif %}

----------------------------------------

TITLE: Configuring incremental predicates in dbt model
DESCRIPTION: This snippet demonstrates how to configure incremental predicates for a dbt model. It filters rows in both the target and source tables based on specified conditions.

LANGUAGE: sql
CODE:
{{

config(

materialized='incremental',

incremental_strategy = 'merge',

unique_key = 'id',

incremental_predicates = [

"dbt_internal_target.create_at >= '2023-01-01'",	"dbt_internal_source.create_at >= '2023-01-01'"],

)

}}

----------------------------------------

TITLE: Defining Versions Without Explicit Latest Version in dbt YAML
DESCRIPTION: This example shows a dbt model configuration where multiple versions are defined without explicitly setting the 'latest_version'. In this case, the latest version will default to the highest version number (3).

LANGUAGE: yaml
CODE:
models:
  - name: model_name
    versions:
      - v: 3
      - v: 2
      - v: 1

----------------------------------------

TITLE: Running Tagged Tests in dbt
DESCRIPTION: This command selects and runs tests tagged with 'my_test_tag' in dbt.

LANGUAGE: bash
CODE:
dbt test --select "tag:my_test_tag"

----------------------------------------

TITLE: Configuring Model Grants with Post-Hooks (Legacy Approach)
DESCRIPTION: Example of granting select permissions using post-hooks in an incremental model, which was the previous approach before dbt v1.2.

LANGUAGE: sql
CODE:
{{ config(
	materialized = 'incremental',
	post_hook = ["grant select on {{ this }} to reporter"]
) }}

select ...

----------------------------------------

TITLE: Retrieving Features for Predictions from Snowflake Feature Store
DESCRIPTION: This Python code demonstrates how to retrieve feature values for inference using the Snowflake Feature Store, which can be used for making predictions with deployed models.

LANGUAGE: python
CODE:
infernce_spine = session.create_dataframe(
    [
        ('1', '3937', "2019-07-01 00:00"), 
        ('2', '2', "2019-07-01 00:00"),
        ('3', '927', "2019-07-01 00:00"),
    ], 
    schema=["INSTANCE_ID", "CUSTOMER_ID", "EVENT_TIMESTAMP"])

inference_dataset = fs.retrieve_feature_values(
    spine_df=infernce_spine,
    features=[customer_transactions_fv],
    spine_timestamp_col="EVENT_TIMESTAMP",
)

inference_dataset.to_pandas()

----------------------------------------

TITLE: Creating Foreign Tables in Teradata
DESCRIPTION: SQL commands to create foreign tables for customers, orders and payments data, referencing CSV files in cloud storage.

LANGUAGE: sql
CODE:
CREATE FOREIGN TABLE jaffle_shop.customers (
    id integer,
    first_name varchar (100),
    last_name varchar (100),
    email varchar (100)
)
USING (
    LOCATION ('/gs/storage.googleapis.com/clearscape_analytics_demo_data/dbt/raw_customers.csv')
)
NO PRIMARY INDEX;

CREATE FOREIGN TABLE jaffle_shop.orders (
    id integer,
    user_id integer,
    order_date date,
    status varchar(100)
)
USING (
    LOCATION ('/gs/storage.googleapis.com/clearscape_analytics_demo_data/dbt/raw_orders.csv')
)
NO PRIMARY INDEX;

CREATE FOREIGN TABLE jaffle_shop.payments (
    id integer,
    orderid integer,
    paymentmethod varchar (100),
    amount integer
)
USING (
    LOCATION ('/gs/storage.googleapis.com/clearscape_analytics_demo_data/dbt/raw_payments.csv')
)
NO PRIMARY INDEX;

----------------------------------------

TITLE: Coercing Jinja Output to Native Python Types with as_native Filter
DESCRIPTION: The as_native filter uses ast.literal_eval to convert Jinja-compiled output into its native Python representation. It can return any Python native type such as set, list, tuple, or dict. Care should be taken to ensure input matches expectations.

LANGUAGE: jinja
CODE:
{{ some_jinja_expression | as_native }}

----------------------------------------

TITLE: Setting Color Output via CLI Flags
DESCRIPTION: Commands to enable or disable colored output using CLI flags.

LANGUAGE: text
CODE:
dbt --use-colors run
dbt --no-use-colors run

----------------------------------------

TITLE: Disabling Color Output in profiles.yml
DESCRIPTION: YAML configuration to disable colored output in the terminal.

LANGUAGE: yaml
CODE:
config:
  use_colors: False

----------------------------------------

TITLE: Implementing Warehouse-Specific Code Blocks in dbt
DESCRIPTION: Demonstrates how to structure warehouse-specific SQL queries using the WHCode component. Shows examples for multiple warehouses with different SQL syntax.

LANGUAGE: sql
CODE:
        <WHCode>

        <div warehouse="warehouse#1">

        ```
        select * from `dbt-tutorial.jaffle_shop.customers`
        ```

        </div>

        <div warehouse="warehouse#2">

        ```
        select * from default.jaffle_shop_customers
        ```

        </div>

        </WHCode>

----------------------------------------

TITLE: Project Hook Conditional Example
DESCRIPTION: Example of adding a conditional check to project hooks for source freshness command

LANGUAGE: yaml
CODE:
on-run-start:
  - '{{ ... if flags.WHICH != \'freshness\' }}'

----------------------------------------

TITLE: Casting Column Types in SQL for dbt Models
DESCRIPTION: This snippet demonstrates how to use the standard SQL CAST function to explicitly define column types in a dbt model. It shows casting an order_id to an integer and an order_price to a double with specified precision.

LANGUAGE: sql
CODE:
select
    cast(order_id as integer),
    cast(order_price as double(6,2)) -- a more generic way of doing type conversion
from {{ ref('stg_orders') }}

----------------------------------------

TITLE: Configuring SQL Merge Job in dbt
DESCRIPTION: Shows the configuration for a SQL merge job as a dbt model using 'incremental' materialization with 'merge' strategy.

LANGUAGE: sql
CODE:
{{ config(  materialized='incremental',
            sync=True|False,
            map_columns_by_name=True|False,
            incremental_strategy='merge',
            options={
              'option_name': 'option_value'
            },
            primary_key=[{}]
          )
}}
SELECT ...
FROM {{ ref(<model>) }}
WHERE ...
GROUP BY ...
HAVING COUNT ...

----------------------------------------

TITLE: Selecting Resources by Type in dbt
DESCRIPTION: Examples of using the resource_type method to select nodes of specific types.

LANGUAGE: bash
CODE:
dbt build --select "resource_type:exposure"    # build all resources upstream of exposures
dbt list --select "resource_type:test"         # list all tests in your project
dbt list --select "resource_type:source"       # list all sources in your project

----------------------------------------

TITLE: Documentation-only Job Command
DESCRIPTION: Command used for creating documentation-only jobs that run after production jobs. Uses dbt compile to generate documentation.

LANGUAGE: sql
CODE:
dbt compile

----------------------------------------

TITLE: Defining Array in SQL
DESCRIPTION: Example of an array in SQL containing multiple string elements. Arrays in SQL are similar to arrays in other programming languages and contain multiple elements accessible by position.

LANGUAGE: sql
CODE:
["cheesecake", "cupcake", "brownie"]

----------------------------------------

TITLE: Configuring Model Documentation in dbt Project
DESCRIPTION: Configuration for model documentation settings in dbt_project.yml, allowing control of visibility and node colors for documentation.

LANGUAGE: yml
CODE:
models:
  [<resource-path>]:
    +docs:
      show: true | false
      node_color: color_id # Use name (such as node_color: purple) or hex code with quotes (such as node_color: "#cd7f32")

----------------------------------------

TITLE: Adding Network Rule to Snowflake Network Policy
DESCRIPTION: SQL command to add the created network rule to a Snowflake network policy, enabling access for dbt Cloud.

LANGUAGE: sql
CODE:
ALTER NETWORK POLICY <network_policy_name>
  ADD ALLOWED_NETWORK_RULE_LIST =('allow_dbt_cloud_access');

----------------------------------------

TITLE: Storing Authentication Secrets in Zapier
DESCRIPTION: Python code for storing authentication credentials securely in Zapier's Storage. This snippet uses StoreClient to save API tokens and webhook keys for both dbt Cloud and Tableau.

LANGUAGE: python
CODE:
store = StoreClient('abc123') #replace with your UUID secret
store.set('DBT_WEBHOOK_KEY', 'abc123') #replace with your dbt Cloud Webhook key
store.set('TABLEAU_SITE_URL', 'abc123') #replace with your Tableau Site URL, inclusive of https:// and .com
store.set('TABLEAU_SITE_NAME', 'abc123') #replace with your Tableau Site/Server Name
store.set('TABLEAU_API_TOKEN_NAME', 'abc123') #replace with your Tableau API Token Name
store.set('TABLEAU_API_TOKEN_SECRET', 'abc123') #replace with your Tableau API Secret

----------------------------------------

TITLE: Configuring MFA Token Retry Settings
DESCRIPTION: Optional YAML configuration to prevent automatic retries for MFA authentication attempts.

LANGUAGE: yaml
CODE:
connect_retries: 0

----------------------------------------

TITLE: Using ARRAY_AGG for Order Status Aggregation in SQL
DESCRIPTION: This query demonstrates how to use ARRAY_AGG to aggregate distinct order statuses by month. It uses date truncation to group orders and creates an array of unique status values for each month.

LANGUAGE: sql
CODE:
select
    date_trunc('month', order_date) as order_month,
    array_agg(distinct status) as status_array
from  {{ ref('orders') }}
group by 1
order by 1

----------------------------------------

TITLE: Source Data Snapshot Implementation
DESCRIPTION: Demonstrates proper implementation of snapshots on source data using timestamp-based strategy to track changes.

LANGUAGE: sql
CODE:
{% snapshot costs_snapshot %}

{{
    config(
      target_database='analytics',
      target_schema='snapshots',
      unique_key='cost_id',
      strategy='timestamp',
      updated_at='updated_at'
    )
}}

select * from {{ source('source', 'costs') }}

{% endsnapshot %}

----------------------------------------

TITLE: Querying Payment Methods with Jinja in dbt SQL (Error Example)
DESCRIPTION: This snippet demonstrates an incorrect way to query payment methods, which will cause an error during the parse phase because it assumes a table has been returned when the query hasn't been run yet.

LANGUAGE: sql
CODE:
{% set payment_method_query %}
select distinct
payment_method
from {{ ref('raw_payments') }}
order by 1
{% endset %}

{% set results = run_query(payment_method_query) %}

{# Return the first column #}
{% set payment_methods = results.columns[0].values() %}

----------------------------------------

TITLE: Model Stacking Example in dbt
DESCRIPTION: Demonstrates how to stack models using ref() function with a raw data model and a dependent model.

LANGUAGE: sql
CODE:
select *
from public.raw_data

LANGUAGE: sql
CODE:
select *
from {{ref('model_a')}}

----------------------------------------

TITLE: Configuring Primary Key Tests in dbt YAML
DESCRIPTION: This snippet demonstrates how to configure 'not_null' and 'unique' tests for a primary key column in a dbt model using YAML configuration.

LANGUAGE: yaml
CODE:
models:
  - name: orders
    columns:
      - name: order_id
        tests:
          - unique
          - not_null

----------------------------------------

TITLE: Incorrect Analysis Path Configuration with Absolute Path
DESCRIPTION: This snippet illustrates an incorrect way to specify analysis paths using an absolute path. Absolute paths should be avoided in the configuration.

LANGUAGE: yaml
CODE:
analysis-paths: ["/Users/username/project/analyses"]

----------------------------------------

TITLE: Configuring 'begin' in properties YAML file
DESCRIPTION: Sets the 'begin' configuration for the 'user_sessions' model in a properties YAML file. The begin timestamp is set to '2024-01-01 00:00:00'.

LANGUAGE: yaml
CODE:
models:
  - name: user_sessions
    config:
      begin: "2024-01-01 00:00:00"

----------------------------------------

TITLE: Configuring Version Objects in JSX
DESCRIPTION: Example of adding version objects to the versions array in dbt-versions.js, including version number and end-of-life date configuration.

LANGUAGE: jsx
CODE:
exports.versions = [
	{
		version: "1.2",
		EOLDate: "2023-01-01"
	}
]

----------------------------------------

TITLE: Practical SQL Query Example with LIMIT
DESCRIPTION: Shows a practical example of using LIMIT in a query that selects order data, ranks it, and limits the output to 5 rows.

LANGUAGE: sql
CODE:
select
	order_id,
	order_date,
	rank () over (order by order_date) as order_rnk
from {{ ref('orders') }}
order by 2
limit 5

----------------------------------------

TITLE: Disabling dbt Version Check via Command Line
DESCRIPTION: Demonstrates how to run dbt with version checking disabled using the --no-version-check flag. This command suppresses version compatibility error messages that would normally occur when running dbt with an incompatible version.

LANGUAGE: bash
CODE:
dbt --no-version-check run
Running with dbt=1.0.0
Found 13 models, 2 tests, 1 archives, 0 analyses, 204 macros, 2 operations....

----------------------------------------

TITLE: Configuring defer-env-id in dbt_project.yml
DESCRIPTION: Demonstrates how to set the defer-env-id in the dbt_project.yml file to manually specify the source environment for deferral artifacts when using the dbt Cloud CLI.

LANGUAGE: yaml
CODE:
dbt-cloud:
  defer-env-id: '123456'

----------------------------------------

TITLE: Testing a Specific dbt Source Table Using Shell Command
DESCRIPTION: This command runs tests on a single table within a specific source in a dbt project. It uses the 'source:' selector followed by the source name and table name.

LANGUAGE: shell
CODE:
dbt test --select source:jaffle_shop.orders

----------------------------------------

TITLE: Configuring Table Materialization in Model Config Block
DESCRIPTION: Demonstrates how to configure a ClickHouse table materialization using a config block in a model file, including engine, order_by, and partition_by options.

LANGUAGE: jinja
CODE:
{{ config(
    materialized = "table",
    engine = "<engine-type>",
    order_by = [ "<column-name>", ... ],
    partition_by = [ "<column-name>", ... ],
      ...
    ]
) }}

----------------------------------------

TITLE: Navigating to dbt Project Directory
DESCRIPTION: Command to change directory to a dbt project folder before using the CLI.

LANGUAGE: bash
CODE:
cd ~/dbt-projects/jaffle_shop

----------------------------------------

TITLE: Project Configuration in dbt_project.yml
DESCRIPTION: Configuration snippet showing how to specify the dbt Cloud project ID in the dbt_project.yml file.

LANGUAGE: yaml
CODE:
# dbt_project.yml
name:
version:
# Your project configs...

dbt-cloud: 
    project-id: PROJECT_ID

----------------------------------------

TITLE: Running dbt Source Freshness Command
DESCRIPTION: The command to execute source freshness checks in dbt. Results are stored in sources.json by default, but the output location can be customized using the -o flag.

LANGUAGE: bash
CODE:
dbt source freshness

LANGUAGE: bash
CODE:
dbt source freshness -o custom/path/sources.json

----------------------------------------

TITLE: Configuring MindsDB Profile in dbt
DESCRIPTION: Basic profile configuration for connecting dbt to MindsDB. Includes essential connection parameters like host, port, database, schema, username and password settings.

LANGUAGE: yaml
CODE:
mindsdb:
  outputs:
    dev:
      database: 'mindsdb'
      host: '127.0.0.1'
      password: ''
      port: 47335
      schema: 'mindsdb'
      type: mindsdb
      username: 'mindsdb'
  target: dev

----------------------------------------

TITLE: Basic dbt Snapshot Implementation
DESCRIPTION: Demonstrates a basic (but not recommended) approach to snapshotting a fact table to track changes over time using dbt's snapshot functionality.

LANGUAGE: sql
CODE:
{% snapshot snapshot_fct_income %}

{{
    config(
      target_database='analytics',
      target_schema='snapshots',
      unique_key='id',
      strategy='check',
      check_cols=['income']
    )
}}

select
    month_year || ' - ' || product_category as id,    
    *
from {{ ref('fct_income') }}

{% endsnapshot %}

----------------------------------------

TITLE: Package Model Override Example
DESCRIPTION: Example showing how to disable a package model to implement a custom version, specifically for the segment package's web page views model

LANGUAGE: yaml
CODE:
models:
  segment:
    base:
      segment_web_page_views:
        +enabled: false

----------------------------------------

TITLE: Defining Indexes for Teradata Tables in dbt
DESCRIPTION: Configures various types of indexes for Teradata tables, including primary indexes, partitioning, and secondary indexes.

LANGUAGE: yaml
CODE:
{{
  config(
      materialized="table",
      index="PRIMARY INDEX(id)
      PARTITION BY RANGE_N(create_date
                        BETWEEN DATE '2020-01-01'
                        AND     DATE '2021-01-01'
                        EACH INTERVAL '1' MONTH)
      INDEX index_attrA (attrA) WITH LOAD IDENTITY"
  )
}}

LANGUAGE: yaml
CODE:
seeds:
  <project-name>:
    index: "PRIMARY INDEX(id)
      PARTITION BY RANGE_N(create_date
                        BETWEEN DATE '2020-01-01'
                        AND     DATE '2021-01-01'
                        EACH INTERVAL '1' MONTH)
      INDEX index_attrA (attrA) WITH LOAD IDENTITY"

----------------------------------------

TITLE: dbt Run Output Showing Dependency Resolution
DESCRIPTION: Terminal output from executing 'dbt run' command showing how dbt automatically resolves and executes models in the correct order based on their dependencies.

LANGUAGE: txt
CODE:
$ dbt run
Running with dbt=0.16.0
Found 2 models, 28 tests, 0 snapshots, 0 analyses, 130 macros, 0 operations, 0 seed files, 3 sources

11:42:52 | Concurrency: 8 threads (target='dev_snowflake')
11:42:52 |
11:42:52 | 1 of 2 START view model dbt_claire.stg_jaffle_shop__orders........... [RUN]
11:42:55 | 1 of 2 OK creating view model dbt_claire.stg_jaffle_shop__orders..... [CREATE VIEW in 2.50s]
11:42:55 | 2 of 2 START relation dbt_claire.customer_orders..................... [RUN]
11:42:56 | 2 of 2 OK creating view model dbt_claire.customer_orders............. [CREATE VIEW in 0.60s]
11:42:56 | Finished running 2 view models in 15.13s.


Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2

----------------------------------------

TITLE: Using the Plus Operator in dbt Commands
DESCRIPTION: Demonstrates how to use the '+' operator in dbt run commands to select models and their dependencies. The operator can be placed before, after, or on both sides of a model name to include ancestors, descendants, or both.

LANGUAGE: bash
CODE:
dbt run --select "my_model+"         # select my_model and all descendants
dbt run --select "+my_model"         # select my_model and all ancestors
dbt run --select "+my_model+"        # select my_model, and all of its ancestors and descendants

----------------------------------------

TITLE: Schema Creation with Authorization
DESCRIPTION: SQL syntax showing how schema authorization is implemented when creating new schemas.

LANGUAGE: sql
CODE:
CREATE SCHEMA [schema_name] AUTHORIZATION [schema_authorization]

----------------------------------------

TITLE: Staging Customer Data Model
DESCRIPTION: SQL model for staging customer data from the source table

LANGUAGE: sql
CODE:
select
    id as customer_id,
    first_name,
    last_name

from {{ source('jaffle_shop', 'customers') }}

----------------------------------------

TITLE: Configuring GitLab PR Template URL in dbt Cloud
DESCRIPTION: Example of a GitLab PR template URL configuration for dbt Cloud. It includes parameters for source and target branches in the merge request.

LANGUAGE: plaintext
CODE:
https://gitlab.com/<org>/<repo>/-/merge_requests/new?merge_request[source_branch]={{source}}&merge_request[target_branch]={{destination}}

----------------------------------------

TITLE: Database Connection Profiles Configuration
DESCRIPTION: YAML configuration for database connections using environment variables for CI and production environments.

LANGUAGE: yaml
CODE:
your_project:
  target: ci
  outputs:
    ci:
      type: postgres
      host: "{{ env_var('DB_CI_HOST') }}"
      port: "{{ env_var('DB_CI_PORT') | int }}"
      user: "{{ env_var('DB_CI_USER') }}"
      password: "{{ env_var('DB_CI_PWD') }}"
      dbname: "{{ env_var('DB_CI_DBNAME') }}"
      schema: "{{ env_var('DB_CI_SCHEMA') }}"
      threads: 16
      keepalives_idle: 0
    prod:
      type: postgres
      host: "{{ env_var('DB_PROD_HOST') }}"
      port: "{{ env_var('DB_PROD_PORT') | int }}"
      user: "{{ env_var('DB_PROD_USER') }}"
      password: "{{ env_var('DB_PROD_PWD') }}"
      dbname: "{{ env_var('DB_PROD_DBNAME') }}"
      schema: "{{ env_var('DB_PROD_SCHEMA') }}"
      threads: 16
      keepalives_idle: 0

----------------------------------------

TITLE: Applying COALESCE to Replace Null Order Statuses
DESCRIPTION: This SQL query uses COALESCE to replace null values in the order_status column with 'not_returned'. It demonstrates a practical application of COALESCE in data cleaning.

LANGUAGE: sql
CODE:
select
	order_id,
	order_date,
	coalesce(order_status, 'not_returned') as order_status
from {{ ref('orders') }}

----------------------------------------

TITLE: Running Modified Models and Downstream Dependencies in dbt
DESCRIPTION: This command demonstrates how to run only modified models and their downstream dependencies in dbt, using the state comparison feature. It's useful for efficient CI/CD pipelines.

LANGUAGE: bash
CODE:
dbt run -s state:modified+ --defer --state path/to/prod/artifacts

----------------------------------------

TITLE: Running clean_stale_models Macro with Multiple Arguments
DESCRIPTION: This example demonstrates invoking the clean_stale_models macro using dbt run-operation, passing multiple arguments including 'days' and 'dry_run' parameters.

LANGUAGE: bash
CODE:
$ dbt run-operation clean_stale_models --args '{days: 7, dry_run: True}'

----------------------------------------

TITLE: Generating Surrogate Keys in Postgres
DESCRIPTION: This SQL snippet shows how to create a surrogate key in Postgres by concatenating columns and applying an MD5 hash function, without needing to handle null values explicitly.

LANGUAGE: sql
CODE:
md5 ( concat (column1, column2) )

----------------------------------------

TITLE: Setting Custom Seed Directory in dbt_project.yml
DESCRIPTION: Demonstrates how to use a custom directory named 'custom_seeds' instead of the default 'seeds' directory for storing seed files.

LANGUAGE: yml
CODE:
seed-paths: ["custom_seeds"]

----------------------------------------

TITLE: Basic SQL CROSS JOIN Syntax
DESCRIPTION: Demonstrates the basic syntax structure for creating a cross join between two tables without using join keys.

LANGUAGE: sql
CODE:
select
    <fields>
from <table_1> as t1
cross join <table_2> as t2

----------------------------------------

TITLE: Excluding Models from dbt Run
DESCRIPTION: This snippet demonstrates how to use the --exclude flag with dbt run to select all models in a package and their children, except for a specific model and its children.

LANGUAGE: bash
CODE:
dbt run --select "my_package".*+ --exclude "my_package.a_big_model+"

----------------------------------------

TITLE: Defining a Jinja Macro in dbt
DESCRIPTION: This snippet demonstrates how to define a Jinja macro named 'make_cool' that takes an 'uncool_id' parameter and calls a 'do_cool_thing' function. It showcases proper spacing and indentation within Jinja blocks.

LANGUAGE: jinja
CODE:
{% macro make_cool(uncool_id) %}

    do_cool_thing({{ uncool_id }})

{% endmacro %}

----------------------------------------

TITLE: Appending Default Comment in dbt YAML
DESCRIPTION: Shows how to use the dictionary syntax to append the default comment instead of prepending it.

LANGUAGE: yaml
CODE:
query-comment:
  append: True

----------------------------------------

TITLE: Creating Python Virtual Environment on Unix/macOS
DESCRIPTION: Commands to create and activate a Python virtual environment using venv on Unix/macOS systems.

LANGUAGE: shell
CODE:
python3 -m venv env

LANGUAGE: shell
CODE:
source env/bin/activate

LANGUAGE: shell
CODE:
which python

LANGUAGE: shell
CODE:
env/bin/python

----------------------------------------

TITLE: Configuring OAuth Authentication for Starburst/Trino in dbt
DESCRIPTION: Example YAML configuration for setting up an OAuth-authenticated connection to a Starburst/Trino cluster in dbt's profiles.yml file. Includes essential connection parameters and OAuth-specific settings.

LANGUAGE: yaml
CODE:
sandbox-galaxy:
  target: oauth
  outputs:
    oauth:
      type: trino
      method: oauth
      host: bunbundersders.trino.galaxy-dev.io
      catalog: dbt_target
      schema: dataders
      port: 443

----------------------------------------

TITLE: Configuring event_time in model properties.yml
DESCRIPTION: Example of setting event_time configuration in a model's property file.

LANGUAGE: yml
CODE:
models:
  - name: model_name
    [config]:
      event_time: my_time_field

----------------------------------------

TITLE: Using External Source in dbt Model
DESCRIPTION: This Jinja snippet demonstrates how to reference an external source table in a dbt model using the source macro.

LANGUAGE: jinja
CODE:
{{ source('external_example', 'hive_table_name') }}

----------------------------------------

TITLE: Lake Formation Project Configuration in YAML
DESCRIPTION: Project-level configuration for Lake Formation tags in dbt_project.yml file

LANGUAGE: yaml
CODE:
  +lf_tags_config:
    enabled: true
    tags:
      tag1: value1
      tag2: value2
    tags_columns:
      tag1:
        value1: [ column1, column2 ]
    inherited_tags: [ tag1, tag2 ]

----------------------------------------

TITLE: Setting GitHub Enterprise PR Template URL in dbt Cloud
DESCRIPTION: Example of a GitHub Enterprise PR template URL configuration for dbt Cloud. It includes a custom domain and uses variables for branch comparison.

LANGUAGE: plaintext
CODE:
https://git.<mycompany>.com/<org>/<repo>/compare/{{destination}}..{{source}}

----------------------------------------

TITLE: Defining Macro Properties in YAML Configuration
DESCRIPTION: YAML configuration template for declaring macro properties in dbt. Shows the structure for defining macro names, descriptions, documentation visibility, metadata, and argument specifications including types and descriptions.

LANGUAGE: yaml
CODE:
version: 2

macros:
  - name: <macro name>
    description: <markdown_string>
    docs:
      show: true | false
    meta: {<dictionary>}
    arguments:
      - name: <arg name>
        type: <string>
        description: <markdown_string>
      - ... # declare properties of additional arguments

  - name: ... # declare properties of additional macros


----------------------------------------

TITLE: Querying Model Execution Times with GraphQL
DESCRIPTION: GraphQL queries to get model execution times and historical run data from the Discovery API

LANGUAGE: graphql
CODE:
query AppliedModels($environmentId: BigInt!, $first: Int!) {
  environment(id: $environmentId) {
    applied {
      models(first: $first) {
        edges {
          node {
            name
            uniqueId
            materializedType
            executionInfo {
              lastSuccessRunId
              executionTime
              executeStartedAt
            }
          }
        }
      }
    }
  }
}

LANGUAGE: graphql
CODE:
query ModelHistoricalRuns(
  $environmentId: BigInt!
  $uniqueId: String
  $lastRunCount: Int
) {
  environment(id: $environmentId) {
    applied {
      modelHistoricalRuns(
        uniqueId: $uniqueId
        lastRunCount: $lastRunCount
      ) {
        name
        runId
        runElapsedTime
        runGeneratedAt
        executionTime
        executeStartedAt
        executeCompletedAt
        status
      }
    }
  }
}

----------------------------------------

TITLE: Checking Source Freshness in dbt
DESCRIPTION: This command demonstrates how to check the freshness of data sources in dbt. It's typically run as part of a job to generate state information for subsequent commands.

LANGUAGE: bash
CODE:
dbt source freshness

----------------------------------------

TITLE: Schema Creation SQL Statement
DESCRIPTION: SQL statement demonstrating schema creation with authorization specification

LANGUAGE: sql
CODE:
CREATE SCHEMA [schema_name] AUTHORIZATION [schema_authorization]

----------------------------------------

TITLE: Running the dbt-rpc server
DESCRIPTION: Command to start the dbt-rpc server, showing the default host and port configuration.

LANGUAGE: bash
CODE:
$ dbt-rpc serve
Running with dbt=0.15.0

16:34:31 | Concurrency: 8 threads (target='dev')
16:34:31 |
16:34:31 | Done.
Serving RPC server at 0.0.0.0:8580
Send requests to http://localhost:8580/jsonrpc

----------------------------------------

TITLE: Demonstrating Null Value Handling in Surrogate Key Generation
DESCRIPTION: This SQL example illustrates the problem of null values when concatenating columns for surrogate key generation, showing how nulls can result in unintended null surrogate keys.

LANGUAGE: sql
CODE:
with 

example_ids as (
  select
  123 as user_id,
  123 as product_id

  union all

  select
  123 as user_id,
  null as product_id

  union all

  select
  null as user_id,
  123 as product_id

)

select
  *,
  concat(user_id, product_id) as _surrogate_key
from example_ids

----------------------------------------

TITLE: Co-locating Models and Seeds in dbt_project.yml
DESCRIPTION: Shows how to configure dbt to look for both models and seeds in the 'models' directory. This works because dbt distinguishes between .csv files (seeds) and .sql files (models).

LANGUAGE: yml
CODE:
seed-paths: ["models"]
model-paths: ["models"]

----------------------------------------

TITLE: Configuring YAML Frontmatter for dbt Cloud Deploy Jobs Documentation
DESCRIPTION: YAML frontmatter defining the title, description, and tags for the dbt Cloud deploy jobs documentation page.

LANGUAGE: yaml
CODE:
---
title: "Deploy jobs"
description: "Learn how to create and schedule deploy jobs in dbt Cloud for the scheduler to run. When you run with dbt Cloud, you get built-in observability, logging, and alerting." 
tags: [scheduler]
---

----------------------------------------

TITLE: Basic SQL Inner Join Syntax
DESCRIPTION: Demonstrates the basic syntax structure for creating an inner join between two tables using a single matching key field.

LANGUAGE: sql
CODE:
select
    <fields>
from <table_1> as t1
inner join <table_2> as t2
on t1.id = t2.id

----------------------------------------

TITLE: Complete Slack Thread Summarization Model
DESCRIPTION: Full dbt model implementation for aggregating and summarizing Slack threads using Snowflake Cortex, including incremental processing and partition handling.

LANGUAGE: sql
CODE:
{{{
    config(
        materialized='incremental',
        unique_key='unique_key',
        full_refresh=false
    )
-}}}

{%- set partition_by = [
    {'column': 'summary_period', 'sql': 'date_trunc(day, sent_at)'},
    {'column': 'product_segment', 'sql': 'lower(product_segment)'},
    {'column': 'is_further_attention_needed', 'sql': 'is_further_attention_needed'},
] -%}

{% set partition_by_sqls = [] -%}
{% set partition_by_columns = [] -%}

{% for p in partition_by -%}
    {% do partition_by_sqls.append(p.sql) -%}
    {% do partition_by_columns.append(p.column) -%}
{% endfor -%}


with

summaries as (

    select * from {{ ref('fct_slack_thread_llm_summaries') }}
    where not has_townie_participant

),

aggregated as (
    select distinct
        {% for p in partition_by -%}
            {{ p.sql }} as {{ p.column }},
        {% endfor -%}

        array_agg(
            object_construct(
                'permalink', thread_permalink,
                'thread', thread_summary
            )
        ) over (partition by {{ partition_by_sqls | join(', ') }}) as agg_threads,
        count(*) over (partition by {{ partition_by_sqls | join(', ') }}) as num_records,
        
        {{ dbt_utils.generate_surrogate_key(partition_by_columns) }} as unique_key
    from summaries
    {% if is_incremental() %}
        where unique_key not in (select this.unique_key from {{ this }} as this) 
    {% endif %}

),

summarised as (

    select
        *,
        trim(snowflake.cortex.complete(
            'llama2-70b-chat',
            concat(
                'In a few bullets, describe the key takeaways from these threads. For each object in the array, summarise the `thread` field, then provide the Slack permalink URL from the `permalink` field for that element in markdown format at the end of each summary. Do not repeat my request back to me in your response.',
                agg_threads::text
            )
        )) as overall_summary
    from aggregated

),

final as (
    select 
        * exclude overall_summary,
        regexp_replace(
            overall_summary, '(^Sure.+:\n*)', ''
        ) as overall_summary

    from summarised
)

select * from final

----------------------------------------

TITLE: Selecting Resources by Result Status in dbt
DESCRIPTION: Examples of using the result method to select resources based on their result status from a prior run.

LANGUAGE: bash
CODE:
dbt run --select "result:error" --state path/to/artifacts # run all models that generated errors on the prior invocation of dbt run
dbt test --select "result:fail" --state path/to/artifacts # run all tests that failed on the prior invocation of dbt test
dbt build --select "1+result:fail" --state path/to/artifacts # run all the models associated with failed tests from the prior invocation of dbt build
dbt seed --select "result:error" --state path/to/artifacts # run all seeds that generated errors on the prior invocation of dbt seed.

----------------------------------------

TITLE: PowerBI HTML iFrame Measure
DESCRIPTION: PowerBI measure code to embed a dbt data health tile using an HTML iFrame.

LANGUAGE: html
CODE:
<iframe src='https://1234.metadata.ACCESS_URL/exposure-tile?uniqueId=exposure.EXPOSURE_NAME&environmentType=staging&environmentId=123456789&token=YOUR_METADATA_TOKEN' title='Exposure status tile' height='400'></iframe>

----------------------------------------

TITLE: Defining Entities in Semantic Model YAML
DESCRIPTION: Example of how to define entities in a semantic model, including primary and foreign entity types.

LANGUAGE: yaml
CODE:
semantic_models:
  - name: orders
    ...
    entities:
      # we use the column for the name here because order is a reserved word in SQL
      - name: order_id
        type: primary
      - name: location
        type: foreign
        expr: location_id
      - name: customer
        type: foreign
        expr: customer_id

    dimensions:
      ...
    measures:
      ...

----------------------------------------

TITLE: Calling Macro in On-Run-End Hook
DESCRIPTION: Demonstration of calling a custom macro (grant_select) within an on-run-end hook, passing the schemas variable as an argument.

LANGUAGE: yml
CODE:
on-run-end: "{{ grant_select(schemas) }}"

----------------------------------------

TITLE: Configuring IBM DB2 Connection in profiles.yml for dbt
DESCRIPTION: This YAML configuration sets up the connection details for an IBM DB2 target in dbt. It specifies the adapter type, schema, database, host, port, protocol, and authentication credentials.

LANGUAGE: yaml
CODE:
your_profile_name:
  target: dev
  outputs:
    dev:
      type: ibmdb2
      schema: analytics
      database: test
      host: localhost
      port: 50000
      protocol: TCPIP
      username: my_username
      password: my_password

----------------------------------------

TITLE: YAML Model Configuration with Tests
DESCRIPTION: YAML configuration defining data quality tests for a model with different failure storage strategies.

LANGUAGE: yaml
CODE:
models:
  - name: my_model
    columns:
      - name: created_at
        tests:
          - not_null:
              config:
                store_failures_as: view
          - unique:
              config:
                store_failures_as: ephemeral

----------------------------------------

TITLE: DBT Oracle Profile Configuration
DESCRIPTION: YAML configuration for Oracle connection profile in dbt

LANGUAGE: yaml
CODE:
dbt_test:
   target: dev
   outputs:
      dev:
         type: oracle
         user: "{{ env_var('DBT_ORACLE_USER') }}"
         pass: "{{ env_var('DBT_ORACLE_PASSWORD') }}"
         database: "{{ env_var('DBT_ORACLE_DATABASE') }}"
         tns_name: "{{ env_var('DBT_ORACLE_TNS_NAME') }}"
         schema: "{{ env_var('DBT_ORACLE_SCHEMA') }}"
         threads: 4

----------------------------------------

TITLE: Querying Model Historical Runs in GraphQL for dbt
DESCRIPTION: This query replaces the deprecated modelByEnvironment with the environment query. It retrieves historical run information for a specific model in the given environment, including uniqueId, execution time, and completion timestamp.

LANGUAGE: graphql
CODE:
query ($environmentId: BigInt!, $uniqueId: String) {
  environment(id: $environmentId) {
    applied {
      modelHistoricalRuns(uniqueId: $uniqueId) {
        uniqueId
        executionTime
        executeCompletedAt
      }
    }
  }
}


----------------------------------------

TITLE: Group Access Control Example
DESCRIPTION: Demonstrates how to implement group-based access control between marketing and finance models using private access modifiers.

LANGUAGE: yaml
CODE:
models:
  - name: finance_model
    access: private
    group: finance
  - name: marketing_model
    group: marketing

LANGUAGE: sql
CODE:
select * from {{ ref('finance_model') }}

----------------------------------------

TITLE: Configuring fail_calc in Specific Model Tests
DESCRIPTION: Configuration of fail_calc for a specific instance of a generic test within a model's YAML definition.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: my_model
    columns:
      - name: my_columns
        tests:
          - unique:
              config:
                fail_calc: "case when count(*) > 0 then sum(n_records) else 0 end"

----------------------------------------

TITLE: Default Quoting Configuration for Snowflake
DESCRIPTION: This YAML snippet shows the default quoting configuration for Snowflake in dbt, where all settings are set to false. This avoids making identifiers case-sensitive and easier to query.

LANGUAGE: yaml
CODE:
quoting:
  database: false
  schema: false
  identifier: false

----------------------------------------

TITLE: Configuring dbt Project Profile in dbt_project.yml
DESCRIPTION: Example of specifying the profile name in a dbt project configuration file. This defines which profile dbt should use for database connections.

LANGUAGE: yaml
CODE:
# Example dbt_project.yml file
name: 'jaffle_shop'
profile: 'jaffle_shop'
...

----------------------------------------

TITLE: Implementing get_response Method for dbt Adapter
DESCRIPTION: This code snippet shows how to implement the get_response method for a dbt adapter, which returns information about the last executed command.

LANGUAGE: Python
CODE:
@classmethod
def get_response(cls, cursor) -> AdapterResponse:
    code = cursor.sqlstate or "OK"
    rows = cursor.rowcount
    status_message = f"{code} {rows}"
    return AdapterResponse(
        _message=status_message,
        code=code,
        rows_affected=rows
    )

----------------------------------------

TITLE: Overriding Export Configurations in Development
DESCRIPTION: Use additional flags with the dbt sl export command to override export configurations in the development environment. This allows testing different materialization types, schemas, or aliases without modifying the original configuration.

LANGUAGE: bash
CODE:
dbt sl export --saved-query sq_number1 --export-as table --alias new_export

----------------------------------------

TITLE: Setting BitBucket PR Template URL in dbt Cloud
DESCRIPTION: Example of a BitBucket PR template URL configuration for dbt Cloud. It uses query parameters to set the source and destination branches.

LANGUAGE: plaintext
CODE:
https://bitbucket.org/<org>/<repo>/pull-requests/new?source={{source}}&dest={{destination}}

----------------------------------------

TITLE: Using as_number filter for environment variables
DESCRIPTION: Example of using the as_number filter to coerce a string environment variable to an integer in a YAML configuration.

LANGUAGE: yaml
CODE:
debug:
  target: dev
  outputs:
    dev:
      type: postgres
      user: "{{ env_var('DBT_USER') }}"
      password: "{{ env_var('DBT_PASS') }}"
      host: "{{ env_var('DBT_HOST') }}"
      port: "{{ env_var('DBT_PORT') | as_number }}"
      dbname: analytics
      schema: analytics

----------------------------------------

TITLE: Testing RisingWave Connection
DESCRIPTION: Command to verify the connection between dbt and RisingWave using the dbt debug command.

LANGUAGE: bash
CODE:
dbt debug

----------------------------------------

TITLE: Running Specific Seeds with dbt Seed Command in Bash
DESCRIPTION: This command demonstrates how to use the dbt seed command to run a specific seed named 'country_codes'. It shows the output of the command, including the number of models, tests, and seed files found, as well as the execution details and timing.

LANGUAGE: bash
CODE:
$ dbt seed --select "country_codes"
Found 2 models, 3 tests, 0 archives, 0 analyses, 53 macros, 0 operations, 2 seed files

14:46:15 | Concurrency: 1 threads (target='dev')
14:46:15 |
14:46:15 | 1 of 1 START seed file analytics.country_codes........................... [RUN]
14:46:15 | 1 of 1 OK loaded seed file analytics.country_codes....................... [INSERT 3 in 0.01s]
14:46:16 |
14:46:16 | Finished running 1 seed in 0.14s.

----------------------------------------

TITLE: Configuring Required dbt Version in YAML
DESCRIPTION: Demonstrates how to set the required dbt version in the dbt_project.yml file to prevent users from running the project with an unsupported version of dbt Core.

LANGUAGE: yaml
CODE:
#/dbt_project.yml

name: 'your_company_project'

version: '0.1.0'

require-dbt-version: ">=0.17.2"

...

----------------------------------------

TITLE: Creating Virtual Environment Alias
DESCRIPTION: Shell command to create an alias for activating the virtual environment.

LANGUAGE: shell
CODE:
alias env_dbt='source <PATH_TO_VIRTUAL_ENV_CONFIG>/bin/activate'

----------------------------------------

TITLE: SQL Self Join Example with Products Table
DESCRIPTION: Illustrates a practical example of a self join using a products table to join parent product names onto child SKUs. This query uses a left join to include all products, even those without parents.

LANGUAGE: sql
CODE:
select
   products.sku_id,
   products.sku_name,
   products.parent_id,
   parents.sku_name as parent_name
from {{ ref('products') }} as products
left join {{ ref('products') }} as parents
on products.parent_id = parents.sku_id

----------------------------------------

TITLE: Indirect Selection Examples in dbt
DESCRIPTION: These examples demonstrate different modes of indirect selection in dbt, including eager, buildable, cautious, and empty modes.

LANGUAGE: bash
CODE:
dbt test --select "orders"
dbt build --select "orders"

dbt test --select "orders" --indirect-selection=buildable
dbt build --select "orders" --indirect-selection=buildable

dbt test --select "orders" --indirect-selection=cautious
dbt build --select "orders" --indirect-selection=cautious

dbt test --select "orders" --indirect-selection=empty
dbt build --select "orders" --indirect-selection=empty

----------------------------------------

TITLE: SQL CASE WHEN Example with Order Value Bucketing
DESCRIPTION: Illustrates a practical example of using a CASE WHEN statement to categorize order values into 'low', 'medium', and 'high' buckets based on the order amount. This query uses the Jaffle Shop's orders table as a reference.

LANGUAGE: sql
CODE:
select
    order_id,
    round(amount) as amount,
    case when amount between 0 and 10 then 'low'
         when amount between 11 and 20 then 'medium'
         else 'high'
    end as order_value_bucket
from {{ ref('orders') }}

----------------------------------------

TITLE: Configuring clean-targets in dbt_project.yml
DESCRIPTION: Specifies directories to be removed by the 'dbt clean' command. This configuration should only include directories containing artifacts such as compiled files, logs, or installed packages.

LANGUAGE: yaml
CODE:
clean-targets: [directorypath]

----------------------------------------

TITLE: Configuring Kerberos Authentication for Hive in YAML
DESCRIPTION: YAML configuration for setting up Kerberos authentication with Hive in dbt's profiles.yml file. This method uses GSSAPI to share Kerberos credentials.

LANGUAGE: yaml
CODE:
your_profile_name:
  target: dev
  outputs:
    dev:
      type: hive
      host: HOSTNAME
      port: PORT # default value: 10000
      auth_type: GSSAPI
      kerberos_service_name: KERBEROS_SERVICE_NAME # default value: None
      use_http_transport: BOOLEAN # default value: true
      use_ssl: BOOLEAN # TLS should always be used to ensure secure transmission of credentials, default value: true
      schema: SCHEMA_NAME

----------------------------------------

TITLE: Continuous Deployment Pipeline Configuration
DESCRIPTION: YAML configuration for the CD pipeline that deploys to production and stores artifacts.

LANGUAGE: yaml
CODE:
image: python:3.8

pipelines:
  branches:
    main:
      - step:
          name: Deploy to production
          caches:
            - pip
          artifacts:
            - target/*.json
          script:
            - python -m pip install -r requirements.txt
            - mkdir ~/.dbt
            - cp .ci/profiles.yml ~/.dbt/profiles.yml
            - dbt deps
            - dbt seed --target prod
            - dbt run --target prod
            - dbt snapshot --target prod
      - step:
          name: Upload artifacts for slim CI runs
          script:
            - pipe: atlassian/bitbucket-upload-file:0.3.2
              variables:
                BITBUCKET_USERNAME: $BITBUCKET_USERNAME
                BITBUCKET_APP_PASSWORD: $BITBUCKET_APP_PASSWORD
                FILENAME: 'target/*.json'

----------------------------------------

TITLE: Project-level Snapshot Configuration
DESCRIPTION: Example of setting invalidate_hard_deletes at the project level in dbt_project.yml file.

LANGUAGE: yaml
CODE:
snapshots:
  [<resource-path>]:
    +strategy: timestamp
    +invalidate_hard_deletes: true

----------------------------------------

TITLE: Configuring Column Properties for DBT Seeds
DESCRIPTION: YAML configuration for defining column properties within DBT seeds, including data type specifications and column attributes.

LANGUAGE: yaml
CODE:
version: 2

seeds:
  - name: <seed_name>
    columns:
      - name: <column_name>
        description: <markdown_string>
        data_type: <string>
        quote: true | false
        tests: ...
        tags: ...
        meta: ...
      - name: <another_column>
            ...

----------------------------------------

TITLE: SQL RIGHT JOIN Example with Car Data
DESCRIPTION: Practical example of a RIGHT JOIN between car_type and car_color tables, demonstrating how to join tables and handle null values when records don't match.

LANGUAGE: sql
CODE:
select
   car_type.user_id as user_id,
   car_type.car_type as type,
   car_color.car_color as color
from {{ ref('car_type') }} as car_type
right join {{ ref('car_color') }} as car_color
on car_type.user_id = car_color.user_id

----------------------------------------

TITLE: Node Selection Using Pattern Matching in CLI
DESCRIPTION: Example of using Unix-style wildcards for node selection in dbt CLI commands.

LANGUAGE: bash
CODE:
dbt ls --select "tag:team_*"

----------------------------------------

TITLE: Enabling Static Parser in dbt YAML Configuration
DESCRIPTION: This snippet illustrates how to enable the static parser in a dbt project's profiles.yml file. The static parser can improve parsing performance for certain types of files.

LANGUAGE: yaml
CODE:
config:
  static_parser: true

----------------------------------------

TITLE: Setting Azure DevOps PR Template URL in dbt Cloud
DESCRIPTION: Example of an Azure DevOps PR template URL configuration for dbt Cloud. It includes parameters for the source and target references.

LANGUAGE: plaintext
CODE:
https://dev.azure.com/<org>/<project>/_git/<repo>/pullrequestcreate?sourceRef={{source}}&targetRef={{destination}}

----------------------------------------

TITLE: Generate Schema Name - Python
DESCRIPTION: Updated two-argument variant of generate_schema_name macro replacing deprecated single-argument version.

LANGUAGE: python
CODE:
def generate_schema_name(custom_schema_name, node):
    # Implementation using both arguments
    return custom_schema_name

----------------------------------------

TITLE: Basic Connection Testing with dbt Debug
DESCRIPTION: Tests only the database connection without performing other system checks

LANGUAGE: shell
CODE:
dbt debug --connection

----------------------------------------

TITLE: Creating BigQuery Slack Messages Table Schema
DESCRIPTION: SQL schema definition for storing Slack message data including timestamp, user details, and message content

LANGUAGE: sql
CODE:
CREATE TABLE `openlineage.metrics.slack_messages`
(
  message_time TIMESTAMP NOT NULL,
  domain STRING NOT NULL,
  username STRING,
  realname STRING,
  email STRING,
  channel STRING,
  permalink STRING,
  text STRING
)

----------------------------------------

TITLE: Deactivating Virtual Environment
DESCRIPTION: Command to deactivate the current Python virtual environment.

LANGUAGE: shell
CODE:
deactivate

----------------------------------------

TITLE: Creating Databricks Catalogs for Dev and Prod Environments
DESCRIPTION: SQL commands to create separate catalogs for development and production environments in Databricks Unity Catalog. These catalogs serve as top-level containers for schemas.

LANGUAGE: sql
CODE:
create catalog if not exists dev;
create catalog if not exists prod;

----------------------------------------

TITLE: Querying Monthly Order Averages with String Handling in SQL
DESCRIPTION: Example query demonstrating string operations including date to string casting and string comparison filtering. The query calculates average order amounts by month while excluding specific order statuses.

LANGUAGE: sql
CODE:
select
	date_trunc('month', order_date)::string as order_month,
	round(avg(amount)) as avg_order_amount
from {{ ref('orders') }}
where status not in ('returned', 'return_pending')
group by 1

----------------------------------------

TITLE: Configuring Analysis Paths with Custom Directory
DESCRIPTION: This example shows how to specify a custom directory named 'custom_analyses' for storing analysis files in the dbt project.

LANGUAGE: yaml
CODE:
analysis-paths: ["custom_analyses"]

----------------------------------------

TITLE: Collecting Freshness Results in SQL
DESCRIPTION: Shows how to properly return freshness check results in a custom collect_freshness macro implementation.

LANGUAGE: sql
CODE:
{{ return(load_result('collect_freshness')) }}

----------------------------------------

TITLE: SQL Query Using AND Operator in WHERE Clause
DESCRIPTION: Example query using the AND operator in a WHERE clause to filter orders that are shipped and have an amount greater than $20. It uses the sample Jaffle Shop's orders table.

LANGUAGE: sql
CODE:
select
	order_id,
	status,
	round(amount) as amount
from {{ ref('orders') }}
where status = 'shipped' and amount > 20
limit 3

----------------------------------------

TITLE: Basic Asset Paths Configuration in dbt
DESCRIPTION: Basic syntax for configuring asset-paths in dbt_project.yml file. Accepts a list of directory paths.

LANGUAGE: yaml
CODE:
asset-paths: [directorypath]

----------------------------------------

TITLE: Configuring iomete Target in dbt profiles.yml
DESCRIPTION: This YAML snippet demonstrates how to set up an iomete target in the profiles.yml file for dbt. It includes configuration for connection details such as cluster, host, port, schema, account number, user, and password.

LANGUAGE: yaml
CODE:
iomete:
  target: dev
  outputs:
    dev:
      type: iomete
      cluster: cluster_name
      host: <region_name>.iomete.com
      port: 443
      schema: database_name
      account_number: iomete_account_number
      user: iomete_user_name
      password: iomete_user_password

----------------------------------------

TITLE: Basic Schema Grants in dbt On-Run-End Hook
DESCRIPTION: Demonstrates how to grant usage permissions on schemas to a database user using the schemas context variable in an on-run-end hook.

LANGUAGE: sql
CODE:
{% for schema in schemas %}grant usage on schema {{ schema }} to db_reader;{% endfor %}

----------------------------------------

TITLE: Configuring Multiple Environment Connections in dbt Core using profiles.yml
DESCRIPTION: Example showing how to set up different database connections for development and production environments using targets in profiles.yml configuration file. This allows connecting to different data warehouses within the same profile.

LANGUAGE: yaml
CODE:
my_project:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: dev_account
      # other dev connection details
    prod:
      type: snowflake
      account: prod_account
      # other prod connection details

----------------------------------------

TITLE: Custom Materialization Override Example in dbt
DESCRIPTION: Example showing how to properly override a built-in materialization from the root project using a wrapping materialization pattern.

LANGUAGE: sql
CODE:
{% materialization view, default %}
{{ return(my_cool_package.materialization_view_default()) }}
{% endmaterialization %}

----------------------------------------

TITLE: Setting Project-Level Test Limits in YAML
DESCRIPTION: Configures default failure limits at the project level for all tests or specific packages. Sets global limit to 1000 and package-specific limit to 50.

LANGUAGE: yaml
CODE:
tests:
  +limit: 1000  # all tests
  
  <package_name>:
    +limit: 50 # tests in <package_name>

----------------------------------------

TITLE: Configuring meta for models in dbt_project.yml
DESCRIPTION: Demonstrates how to configure meta properties for models in the dbt_project.yml file. Meta properties are defined as a dictionary under the models configuration block.

LANGUAGE: yaml
CODE:
models:
  [<resource-path>]:
    +meta: {<dictionary>}

----------------------------------------

TITLE: Creating a Full Outer Join in SQL
DESCRIPTION: This snippet shows the basic structure of a full outer join in SQL. It joins two tables (t1 and t2) on their 'id' fields, selecting specified fields from both tables.

LANGUAGE: sql
CODE:
select
    <fields>
from <table_1> as t1
full outer join <table_1> as t2
on t1.id = t2.id

----------------------------------------

TITLE: Configuring a Microbatch Model with Unique Key for Postgres
DESCRIPTION: This SQL configuration demonstrates how to set up a microbatch incremental model for Postgres, which requires the unique_key config in addition to standard microbatch configs.

LANGUAGE: sql
CODE:
{{ config(
    materialized='incremental',
    incremental_strategy='microbatch',
    unique_key='sales_id',
    event_time='transaction_date',
    begin='2023-01-01',
    batch_size='day'
) }}

select
    sales_id,
    transaction_date,
    customer_id,
    product_id,
    total_amount
from {{ source('sales', 'transactions') }}

----------------------------------------

TITLE: Configuring 'begin' in dbt_project.yml
DESCRIPTION: Sets the 'begin' configuration for the 'user_sessions' model in the dbt_project.yml file. The begin timestamp is set to '2024-01-01 00:00:00'.

LANGUAGE: yaml
CODE:
models:
  my_project:
    user_sessions:
      +begin: "2024-01-01 00:00:00"

----------------------------------------

TITLE: Complete Incremental Model Implementation
DESCRIPTION: Full implementation of an incremental model combining configuration, base query, and conditional logic using the is_incremental() macro.

LANGUAGE: sql
CODE:
{{
    config(
        materialized='incremental',
        unique_key='order_id'
    )
}}

select * from orders

{% if is_incremental() %}

where
  updated_at > (select max(updated_at) from {{ this }})

{% endif %}

----------------------------------------

TITLE: Querying Job and Model Data with GraphQL
DESCRIPTION: Example GraphQL query demonstrating how to fetch job models and source information. The query accepts a job ID and optional run ID to retrieve model execution times and source freshness criteria.

LANGUAGE: graphql
CODE:
query JobQueryExample {
  # Provide runId for looking at specific run, otherwise it defaults to latest run
  job(id: 940) {
    # Get all models from this job's latest run
    models(schema: "analytics") {
      uniqueId
      executionTime
    }

    # Or query a single node
    source(uniqueId: "source.jaffle_shop.snowplow.event") {
      uniqueId
      sourceName
      name
      state
      maxLoadedAt
      criteria {
        warnAfter {
          period
          count
        }
        errorAfter {
          period
          count
        }
      }
      maxLoadedAtTimeAgoInS
    }
  }
}

----------------------------------------

TITLE: Running Singular Tests in dbt
DESCRIPTION: This command selects and runs only singular tests in dbt.

LANGUAGE: bash
CODE:
dbt test --select "test_type:singular"

----------------------------------------

TITLE: Configuring Empty Test Block in YAML
DESCRIPTION: Demonstrates the correct way to handle empty test configurations in YAML files after v1.5 upgrade. Shows both incorrect and correct implementations.

LANGUAGE: yaml
CODE:
models:
  - name: my_model
    tests: [] # todo! add tests later
    config: ...

----------------------------------------

TITLE: Custom Incremental Strategy Implementation
DESCRIPTION: Demonstrates how to create a custom incremental strategy using macros and SQL template logic.

LANGUAGE: sql
CODE:
{% macro get_incremental_insert_only_sql(arg_dict) %}

  {% do return(some_custom_macro_with_sql(arg_dict["target_relation"], arg_dict["temp_relation"], arg_dict["unique_key"], arg_dict["dest_columns"], arg_dict["incremental_predicates"])) %}

{% endmacro %}


{% macro some_custom_macro_with_sql(target_relation, temp_relation, unique_key, dest_columns, incremental_predicates) %}

    {%- set dest_cols_csv = get_quoted_csv(dest_columns | map(attribute="name")) -%}

    insert into {{ target_relation }} ({{ dest_cols_csv }})
    (
        select {{ dest_cols_csv }}
        from {{ temp_relation }}
    )

{% endmacro %}

----------------------------------------

TITLE: Setting Custom Schema in dbt Project Configuration
DESCRIPTION: Configuration in dbt_project.yml to specify a custom schema for marketing-related models.

LANGUAGE: yml
CODE:
name: jaffle_shop
...

models:
  jaffle_shop:
    marketing:
      schema: marketing # seeds in the `models/mapping/ subdirectory will use the marketing schema

----------------------------------------

TITLE: Comparing Traditional Dashboards and Analytical Applications in Markdown
DESCRIPTION: This markdown table compares the characteristics of traditional dashboards with analytical applications, highlighting the differences in approach, functionality, and user experience.

LANGUAGE: markdown
CODE:
<table>
  <tr>
   <td><strong>Traditional Dashboards</strong></td>
   <td><strong>Analytical Applications</strong></td>
  </tr>
  <tr>
   <td>Built for generic use cases</td>
   <td>Purpose-built for specific use cases</td>
  </tr>
  <tr>
   <td>Standard dashboard interactions</td>
   <td>Interactive based on the desired workflow</td>
  </tr>
  <tr>
   <td>Fixed, static layout</td>
   <td>Dynamic layout determined by logic</td>
  </tr>
  <tr>
   <td>Each element is a tile</td>
   <td>Elements can be grouped and purposefully-arranged</td>
  </tr>
  <tr>
   <td>Filters are global</td>
   <td>Users have preferences and their own defaults</td>
  </tr>
  <tr>
   <td>Minimal software development lifecycle</td>
   <td>Strong SDLC to promote user trust</td>
  </tr>
  <tr>
   <td>Look and feel are ignored</td>
   <td>Custom look and feel to match company products</td>
  </tr>
  <tr>
   <td>Low bar for performance</td>
   <td>High bar for performance</td>
  </tr>
</table>

----------------------------------------

TITLE: Checking Docker Container Status with Astro CLI
DESCRIPTION: Command to verify the status of the Docker containers after stopping the Airflow deployment.

LANGUAGE: bash
CODE:
astrocloud dev ps

----------------------------------------

TITLE: Defining JSON Object in SQL
DESCRIPTION: Example of a JSON object in SQL containing customer and order information. JSON objects are composed of key-value pairs enclosed in curly brackets.

LANGUAGE: sql
CODE:
{"customer_id":2947, "order_id":4923, "order_items":"cheesecake"}

----------------------------------------

TITLE: Incorrect Asset Path Configuration with Absolute Path
DESCRIPTION: Shows an anti-pattern of using absolute paths in asset-paths configuration, which should be avoided.

LANGUAGE: yaml
CODE:
asset-paths: ["/Users/username/project/assets"]

----------------------------------------

TITLE: SQL UPPER Function with Customer Data Example
DESCRIPTION: Example showing how to use UPPER function to convert customer first names to uppercase in a SELECT statement, using dbt's ref function for table reference.

LANGUAGE: sql
CODE:
select 
	customer_id,
	upper(first_name) as first_name,
	last_name
from {{ ref('customers') }}

----------------------------------------

TITLE: Configuring dbt Tests in YAML
DESCRIPTION: Illustrates how to configure dbt tests using the 'config' property in a YAML file. This includes configurations for both resource-level and column-level tests.

LANGUAGE: yaml
CODE:
version: 2

<resource_type>:
  - name: <resource_name>
    tests:
      - [<test_name>](#test_name):
          <argument_name>: <argument_value>
          config:
            <test_config>: <config-value>
            ...

    [columns](/reference/resource-properties/columns):
      - name: <column_name>
        tests:
          - [<test_name>](#test_name)
          - [<test_name>](#test_name):
              <argument_name>: <argument_value>
              config:
                [<test_config>](/reference/data-test-configs): <config-value>
                ...

----------------------------------------

TITLE: Creating a Snowflake Feature Store in Python
DESCRIPTION: This Python code snippet demonstrates how to create or connect to a Snowflake Feature Store using the snowflake-ml-python package.

LANGUAGE: python
CODE:
from snowflake.ml.feature_store import (
    FeatureStore,
    FeatureView,
    Entity,
    CreationMode
)

fs = FeatureStore(
    session=session, 
    database=fs_db, 
    name=fs_schema, 
    default_warehouse='WH_DBT',
    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,
)

----------------------------------------

TITLE: Bitbucket Pipelines configuration for running dbt Cloud job on pull request
DESCRIPTION: Bitbucket Pipelines configuration to run a dbt Cloud job when a pull request is created. Uses custom schema override based on PR details.

LANGUAGE: yaml
CODE:
image: python:3.11.1


pipelines:
  pull-requests:
    '**':
      - step:
          name: 'Run dbt Cloud PR Job'
          script:
            - if [ "${BITBUCKET_PR_DESTINATION_BRANCH}" != "main" ]; then printf 'PR Destination is not master, exiting.'; exit; fi
            - export DBT_URL="https://cloud.getdbt.com"
            - export DBT_JOB_CAUSE="Bitbucket Pipeline CI Job"
            - export DBT_JOB_BRANCH=$BITBUCKET_BRANCH
            - export DBT_JOB_SCHEMA_OVERRIDE="DBT_CLOUD_PR_"$BITBUCKET_PROJECT_KEY"_"$BITBUCKET_PR_ID
            - export DBT_ACCOUNT_ID=00000 # enter your account id here
            - export DBT_PROJECT_ID=00000 # enter your project id here
            - export DBT_PR_JOB_ID=00000 # enter your job id here
            - python python/run_and_monitor_dbt_job.py

----------------------------------------

TITLE: Creating Default Index for View in SQL
DESCRIPTION: This SQL code demonstrates how to create a default index for a view that uses all columns, using dbt's config block.

LANGUAGE: sql
CODE:
{{ config(materialized='view',
    indexes=[{'default': True}]) }}

select ...

----------------------------------------

TITLE: Merge Strategy with Unique Key
DESCRIPTION: Shows merge strategy configuration which updates existing records and inserts new ones based on a unique key.

LANGUAGE: sql
CODE:
{{ config( materialized = 'incremental', incremental_strategy = 'merge',  unique_key='promotion_key'   )  }}

select * FROM  public.promotion_dimension

----------------------------------------

TITLE: GraphQL Fragment Query Example
DESCRIPTION: Example of using GraphQL fragments to query across lineage and retrieve results from specific node types.

LANGUAGE: graphql
CODE:
query ($environmentId: BigInt!, $first: Int!) {
  environment(id: $environmentId) {
    applied {
      models(first: $first, filter: { uniqueIds: "MODEL.PROJECT.MODEL_NAME" }) {
        edges {
          node {
            name
            ancestors(types: [Model, Source, Seed, Snapshot]) {
              ... on ModelAppliedStateNestedNode {
                name
                resourceType
                materializedType
                executionInfo {
                  executeCompletedAt
                }
              }
              ... on SourceAppliedStateNestedNode {
                sourceName
                name
                resourceType
                freshness {
                  maxLoadedAt
                }
              }
              ... on SnapshotAppliedStateNestedNode {
                name
                resourceType
                executionInfo {
                  executeCompletedAt
                }
              }
              ... on SeedAppliedStateNestedNode {
                name
                resourceType
                executionInfo {
                  executeCompletedAt
                }
              }
            }
          }
        }
      }
    }
  }
}

----------------------------------------

TITLE: Removing packages and compiled files with dbt clean
DESCRIPTION: Configures 'clean-targets' to remove both the target directory and installed packages. This is the preferred configuration and matches the dbt starter project setup.

LANGUAGE: yaml
CODE:
clean-targets:
    - target
    - dbt_packages

----------------------------------------

TITLE: Configuring dbt Cloud CLI YAML Template
DESCRIPTION: Template for the dbt_cloud.yml configuration file that stores project settings and authentication details for the dbt Cloud CLI.

LANGUAGE: yaml
CODE:
version: "1"
context:
  active-project: "<project id from the list below>"
  active-host: "<active host from the list>"
  defer-env-id: "<optional defer environment id>"
projects:
  - project-name: "<project-name>"
    project-id: "<project-id>"
    account-name: "<account-name>"
    account-id: "<account-id>"
    account-host: "<account-host>" # for example, "cloud.getdbt.com"
    token-name: "<pat-or-service-token-name>"
    token-value: "<pat-or-service-token-value>"

  - project-name: "<project-name>"
    project-id: "<project-id>"
    account-name: "<account-name>"
    account-id: "<account-id>"
    account-host: "<account-host>" # for example, "cloud.getdbt.com"
    token-name: "<pat-or-service-token-name>"
    token-value: "<pat-or-service-token-value>"

----------------------------------------

TITLE: Configuring State Comparison Behavior in dbt_project.yml
DESCRIPTION: Sets the 'state_modified_compare_more_unrendered_values' flag to true in the dbt_project.yml file. This configuration is necessary for accurate state comparison when using 'state:modified' in dbt v1.9 or higher.

LANGUAGE: yaml
CODE:
state_modified_compare_more_unrendered_values: true

----------------------------------------

TITLE: Configuring Model Alias in properties.yml
DESCRIPTION: Specifies a custom alias for a model in the models/properties.yml file for centralized configuration.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: sales_total
    config:
      alias: sales_dashboard

----------------------------------------

TITLE: Gaggle Total Facts Aggregation
DESCRIPTION: SQL query that aggregates workspace (Gaggle) level metrics including user counts, event counts, and order information across all users.

LANGUAGE: sql
CODE:
gaggle_total_facts as (

    select
        gaggles.gaggle_id,
        gaggles.gaggle_name,
        gaggles.created_at,

        min(users.first_event) as first_event,
        max(users.most_recent_event) as most_recent_event,
        sum(number_of_events) as number_of_events,
        count(users.user_id) as number_of_users,

        min(users.first_order) as first_order,
        max(users.most_recent_order) as most_recent_order,
        sum(users.number_of_orders) as number_of_orders

    from users
    left join gaggles on users.gaggle_id = gaggles.gaggle_id

    group by 1,2,3

),

----------------------------------------

TITLE: Running dbt Transformations
DESCRIPTION: Command to execute the dbt transformation pipeline using the cloud_dev target.

LANGUAGE: shell
CODE:
$ dbt run -t cloud_dev

----------------------------------------

TITLE: MetricFlow CLI Query Commands
DESCRIPTION: Commands for querying joined data using MetricFlow in both dbt Cloud and Core environments.

LANGUAGE: yaml
CODE:
dbt sl query --metrics average_purchase_price --group-by metric_time,user_id__type

LANGUAGE: yaml
CODE:
mf query --metrics average_purchase_price --group-by metric_time,user_id__type

----------------------------------------

TITLE: DATEDIFF Function Using dbt Macro
DESCRIPTION: An example of using the dbt DATEDIFF macro to calculate the difference between two dates in a standardized way across different databases. This query selects all fields from the 'orders' table and calculates the difference in days between order dates and a specific date.

LANGUAGE: sql
CODE:
select
   *,
   {{ datediff("order_date", "'2022-06-09'", "day") }}
from {{ ref('orders') }}

----------------------------------------

TITLE: Snapshot Configuration Using Config Block
DESCRIPTION: Legacy method for configuring individual snapshots using Jinja templating in SQL files. Includes essential snapshot parameters.

LANGUAGE: jinja
CODE:
{{ config(
    target_schema="<string>",
    target_database="<string>",
    unique_key="<column_name_or_expression>",
    strategy="timestamp" | "check",
    updated_at="<column_name>",
    check_cols=["<column_name>"] | "all"
) }}

----------------------------------------

TITLE: Specifying Project Version in dbt_project.yml (YAML)
DESCRIPTION: Demonstrates how to specify the project version in dbt_project.yml. The version must be in semantic version format (e.g., 1.0.0) and is optional from dbt version 1.5 onwards.

LANGUAGE: yaml
CODE:
version: version

----------------------------------------

TITLE: Python Model Implementation for Oracle ADB
DESCRIPTION: Example Python model implementations for Oracle Autonomous Database using OML4PY

LANGUAGE: python
CODE:
def model(dbt, session):
    dbt.config(materialized="table")
    s_df = dbt.ref("sales_cost")
    return s_df

----------------------------------------

TITLE: Basic SQL RIGHT JOIN Syntax
DESCRIPTION: Basic syntax structure for creating a RIGHT JOIN between two tables using an ID field as the join condition.

LANGUAGE: sql
CODE:
select
    <fields>
from <table_1> as t1
right join <table_2> as t2
on t1.id = t2.id

----------------------------------------

TITLE: Defining Latest Version for a dbt Model in YAML
DESCRIPTION: This snippet shows how to specify the 'latest_version' property for a dbt model in a YAML configuration file. It demonstrates setting the latest version to 2 and defining multiple versions of the model.

LANGUAGE: yaml
CODE:
models:
  - name: model_name
    latest_version: 2
    versions:
      - v: 2
      - v: 1

----------------------------------------

TITLE: Importing Latest Release Stage Component
DESCRIPTION: Import statement for a Markdown component that defines release stage information

LANGUAGE: markdown
CODE:
import Latest from '/snippets/_release-stages-from-versionless.md'

<Latest/>

----------------------------------------

TITLE: Creating Snowflake Infrastructure
DESCRIPTION: SQL commands to create a new warehouse, databases, and schemas in Snowflake for the project setup.

LANGUAGE: sql
CODE:
create warehouse transforming;
create database raw;
create database analytics;
create schema raw.jaffle_shop;
create schema raw.stripe;

----------------------------------------

TITLE: Basic Variable Usage in SQL Model
DESCRIPTION: Demonstrates how to use the var() function to inject a variable value into a SQL query. This example filters events based on an event_type variable that must be defined in the project configuration.

LANGUAGE: sql
CODE:
select * from events where event_type = '{{ var("event_type") }}'

----------------------------------------

TITLE: Querying Metrics in dbt Semantic Layer
DESCRIPTION: Examples of how different queries count towards the Queried Metrics usage in the dbt Semantic Layer. These snippets demonstrate various query types and their corresponding metric counts.

LANGUAGE: shell
CODE:
dbt sl query --metrics revenue --group-by metric_time

LANGUAGE: shell
CODE:
dbt sl query --metrics revenue --group-by metric_time,user__country

LANGUAGE: shell
CODE:
dbt sl query --metrics revenue,gross_sales --group-by metric_time,user__country

LANGUAGE: shell
CODE:
dbt sl query --metrics revenue --group-by metric_time --explain

LANGUAGE: shell
CODE:
dbt sl query --metrics revenue,gross_sales --group-by metric_time --explain

----------------------------------------

TITLE: Using before_begin and after_commit in hooks
DESCRIPTION: Example of using before_begin and after_commit helper macros to run hooks outside of transactions in Postgres or Redshift.

LANGUAGE: sql
CODE:
{{
  config(
    pre_hook=before_begin("SQL-statement"),
    post_hook=after_commit("SQL-statement")
  )
}}

select ...

----------------------------------------

TITLE: Graph Structure Example in JSON
DESCRIPTION: Example showing the structure of the graph context variable containing nodes, sources, exposures, metrics, and groups.

LANGUAGE: json
CODE:
{
  "nodes": {
    "model.my_project.model_name": {
      "unique_id": "model.my_project.model_name",
      "config": {"materialized": "table", "sort": "id"},
      "tags": ["abc", "123"],
      "path": "models/path/to/model_name.sql"
    }
  },
  "sources": {
    "source.my_project.snowplow.event": {
      "unique_id": "source.my_project.snowplow.event",
      "database": "analytics",
      "schema": "analytics",
      "tags": ["abc", "123"],
      "path": "models/path/to/schema.yml"
    }
  }}

----------------------------------------

TITLE: Signing Up for fly.io in Shell
DESCRIPTION: Use the flyctl command to sign up for a fly.io account through the command line interface.

LANGUAGE: shell
CODE:
flyctl auth signup

----------------------------------------

TITLE: Invalid SQL Syntax Example
DESCRIPTION: Demonstrates a SQL query with an invalid syntax error (misspelled 'select' keyword)

LANGUAGE: sql
CODE:
selecte dateadd('day', 1, getdate()) as tomorrow

----------------------------------------

TITLE: Configuring a merge incremental strategy in SQL
DESCRIPTION: Example of configuring a merge incremental strategy with a unique key in a dbt SQL model file

LANGUAGE: sql
CODE:
{{ config(
    materialized='incremental',
    file_format='delta', # or 'hudi'
    unique_key='user_id',
    incremental_strategy='merge'
) }}

with new_events as (

    select * from {{ ref('events') }}

    {% if is_incremental() %}
    where date_day >= date_add(current_date, -1)
    {% endif %}

)

select
    user_id,
    max(date_day) as last_seen

from events
group by 1

----------------------------------------

TITLE: MySQL Configuration for Timestamp Handling
DESCRIPTION: MySQL 5.7 configuration settings to address timestamp initialization issues that affect dbt snapshots functionality.

LANGUAGE: ini
CODE:
[mysqld]
explicit_defaults_for_timestamp = true
sql_mode = "ALLOW_INVALID_DATES,{other_sql_modes}"

----------------------------------------

TITLE: Rendering Navigation Cards in JSX
DESCRIPTION: JSX markup for rendering a grid of navigation cards that link to different collaboration-related documentation sections. Uses a custom Card component to display titles, descriptions, icons, and links.

LANGUAGE: jsx
CODE:
<div className="grid--2-col">

<Card
    title="Discover data with dbt Explorer"
    body="Learn about dbt Explorer and how to interact with it to understand, improve, and leverage your dbt projects."
    link="/docs/collaborate/explore-projects"
    icon="dbt-bit"/>

<Card
    title="Git version control"
    body="Learn about Git and version control."
    link="/docs/collaborate/git-version-control"
    icon="dbt-bit"/>

</div>
<br />
<div className="grid--2-col">

<Card
    title="Document your dbt projects"
    body="Learn how good documentation for your dbt models helps stakeholders discover and understand your datasets."
    link="/docs/collaborate/build-and-view-your-docs"
    icon="dbt-bit"/>

<Card
    title="Model governance"
    body="Learn about the dbt Cloud features related to model governance (like model access)."
    link="/docs/collaborate/govern/about-model-governance"
    icon="dbt-bit"/>

</div>

----------------------------------------

TITLE: Defining Analysis Paths in dbt_project.yml
DESCRIPTION: This snippet shows the basic structure for specifying analysis-paths in the dbt_project.yml file. It allows you to define custom directories where analysis SQL files are located.

LANGUAGE: yaml
CODE:
analysis-paths: [directorypath]

----------------------------------------

TITLE: Example SQL Query Pattern
DESCRIPTION: Demonstrates how dimensions and measures translate to SQL group by and aggregation patterns.

LANGUAGE: sql
CODE:
select
  metric_time_day,  -- time
  country,  -- categorical dimension
  sum(revenue_usd) -- measure
from
  snowflake.fact_transactions  -- sql table
group by metric_time_day, country  -- dimensions

----------------------------------------

TITLE: Configuring batch_size in properties.yml
DESCRIPTION: This example shows how to set the 'batch_size' configuration to 'day' for the 'user_sessions' model in a properties YAML file.

LANGUAGE: yaml
CODE:
models:
  - name: user_sessions
    config:
      batch_size: day

----------------------------------------

TITLE: Database Role and User Setup SQL Commands
DESCRIPTION: SQL commands to create and configure development and production roles with appropriate permissions for CI/CD pipeline execution in a database.

LANGUAGE: sql
CODE:
create role role_dev;

grant create on database [dbname] to role_dev;

create role role_prod;

grant create on database [dbname] to role_prod;

create role dev_ci with login password [password];

grant role_dev to dev_ci;

create schema dbt_ci;

grant all on schema dbt_ci to role_dev;

alter schema dbt_ci owner to role_dev;

create role dbt_bitbucket with login password [password];

grant role_prod to dbt_bitbucket;

----------------------------------------

TITLE: Staging Orders Data Model
DESCRIPTION: SQL model for staging order data from the source table

LANGUAGE: sql
CODE:
select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from {{ source('jaffle_shop', 'orders') }}

----------------------------------------

TITLE: Filtering Orders with SQL WHERE Clause in dbt
DESCRIPTION: Demonstrates basic usage of WHERE clause in a dbt model to filter out returned orders from an orders table. Shows how to combine SELECT statement with WHERE clause for data filtering.

LANGUAGE: sql
CODE:
select
	order_id,
	customer_id,
	amount
from {{ ref('orders') }}
where status != 'returned'

----------------------------------------

TITLE: SQL ROUND Function Example with Jaffle Shop Orders
DESCRIPTION: Example query demonstrating how to round the amount field in an orders table to one decimal place, including casting order_id to string and selecting multiple columns.

LANGUAGE: sql
CODE:
select 
	cast(order_id as string) as order_id,
	order_date,
	amount,
	round(amount, 1) as rounded_amount
from {{ ref('orders') }}

----------------------------------------

TITLE: Basic SQL Model in dbt
DESCRIPTION: A simple dbt model that selects a single column with a constant value. This SQL is compatible across different database platforms.

LANGUAGE: sql
CODE:
select 1 as my_column

----------------------------------------

TITLE: Generating MD5 Hash Using local_md5 in dbt
DESCRIPTION: Demonstrates how to use the local_md5 context variable to generate an MD5 hash of a string within dbt templates. Shows both the source template and its compiled output.

LANGUAGE: sql
CODE:
-- source
{%- set value_hash = local_md5("hello world") -%}
'{{ value_hash }}'

-- compiled
'5eb63bbbe01eeed093cb22bb8f5acdc3'

----------------------------------------

TITLE: Configuring Streamlit Access in Snowflake
DESCRIPTION: SQL commands to set up required permissions and access controls for creating Streamlit applications in Snowflake.

LANGUAGE: sql
CODE:
grant usage on database <database_name> to role public;
grant usage on schema <database_name>.<schema_name> to role public;
grant create streamlit on schema <database_name>.<schema_name> to role public;
grant create stage on schema <database_name>.<schema_name> to role public;
grant usage on warehouse <warehouse_name> to role public;

----------------------------------------

TITLE: Advanced Ratio Metric with Filters in dbt YAML
DESCRIPTION: This example shows an advanced ratio metric that calculates the ratio of food orders to total orders, with filters and aliases applied to both numerator and denominator. It demonstrates the use of explicit keys for name attributes and additional parameters.

LANGUAGE: yaml
CODE:
metrics:
  - name: food_order_pct
    description: "The food order count as a ratio of the total order count, filtered by location"
    label: Food order ratio by location
    type: ratio
    type_params:
      numerator:
        name: food_orders
        filter: location = 'New York'
        alias: ny_food_orders
      denominator:
        name: orders
        filter: location = 'New York'
        alias: ny_orders

----------------------------------------

TITLE: Setting with_statistics for Teradata Tables in dbt
DESCRIPTION: Configures whether statistics should be copied from the base table when creating a new table.

LANGUAGE: yaml
CODE:
{{
  config(
      materialized="table",
      with_statistics="true"
  )
}}

----------------------------------------

TITLE: Using Column Name for Updated_at in YAML (dbt 1.9+)
DESCRIPTION: Example of using a specific column name ('updated_at') for the updated_at parameter in a snapshot configuration using YAML for dbt version 1.9 and above.

LANGUAGE: yaml
CODE:
snapshots:
  - name: orders_snapshot
    relation: source('jaffle_shop', 'orders')
    config:
      schema: snapshots
      unique_key: id
      strategy: timestamp
      updated_at: updated_at

----------------------------------------

TITLE: Disabling Anonymous Usage Stats in dbt Core YAML Configuration
DESCRIPTION: This YAML configuration snippet shows how to disable anonymous usage statistics collection in dbt Core by adding a flag to the dbt_project.yml file.

LANGUAGE: yaml
CODE:
flags:
  send_anonymous_usage_stats: false

----------------------------------------

TITLE: Installing dbt-infer Package with pip
DESCRIPTION: Command to install the dbt-infer adapter package via pip package manager

LANGUAGE: python
CODE:
pip install dbt-infer

----------------------------------------

TITLE: SQL Model Bug Fix Diff
DESCRIPTION: Diff showing the changes made to fix the counting logic in the order_items_summary model.

LANGUAGE: diff
CODE:
17c17
<     count(
---
>     sum(
23c23
<     count(
---
>     sum(

----------------------------------------

TITLE: Configuring Spark Utils Compatibility with DBT Utils
DESCRIPTION: Shows how to configure dispatch to use spark_utils compatibility package as a shim for dbt_utils, establishing a search order that checks spark_utils before falling back to dbt_utils.

LANGUAGE: yml
CODE:
dispatch:
  - macro_namespace: dbt_utils
    search_order: ['spark_utils', 'dbt_utils']

----------------------------------------

TITLE: Invalid SQL Data Value Example
DESCRIPTION: Illustrates a SQL query with an impossible date value that will fail at runtime

LANGUAGE: sql
CODE:
select cast('2025-01-32' as date) as tomorrow

----------------------------------------

TITLE: Example Join Query for Distribution Styles
DESCRIPTION: Sample query demonstrating how joins work with different distribution styles in Redshift.

LANGUAGE: python
CODE:
select <your_list_of_columns>
from visitors
left join known_visitor_profiles
on visitors.person_id = known_visitor_profiles.person_id

----------------------------------------

TITLE: Customer Data Transformation Model
DESCRIPTION: A dbt model that combines and transforms customer and order data using CTEs to create a consolidated customer view with order history.

LANGUAGE: sql
CODE:
with customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from jaffle_shop.customers

),

orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from jaffle_shop.orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final

----------------------------------------

TITLE: Setting Log Color Preferences via CLI
DESCRIPTION: Demonstrates how to enable or disable colored output in the log file using command-line flags.

LANGUAGE: text
CODE:
dbt --use-colors-file run
dbt --no-use-colors-file run

----------------------------------------

TITLE: SQL Usage of Quoted Sources
DESCRIPTION: Demonstrates how to reference sources with different quoting configurations in SQL models, showing the syntax for source references.

LANGUAGE: sql
CODE:
select
  ...

-- this should be quoted
from {{ source('jaffle_shop', 'orders') }}

-- here, the identifier should be unquoted
left join {{ source('jaffle_shop', 'customers') }} using (order_id)

----------------------------------------

TITLE: Configuring Cluster for Materialized View in SQL
DESCRIPTION: This snippet demonstrates how to configure a custom cluster for a materialized view in a SQL file using dbt's config block.

LANGUAGE: sql
CODE:
{{ config(materialized='materializedview', cluster='not_default') }}

select ...

----------------------------------------

TITLE: DATE_TRUNC Implementation in BigQuery and Redshift
DESCRIPTION: Demonstrates the syntax for using DATE_TRUNC function in BigQuery and Redshift where date/time field is the first argument.

LANGUAGE: sql
CODE:
date_trunc(<date/time field>, <date part>)

----------------------------------------

TITLE: Using Jinja Whitespace Control Operators
DESCRIPTION: Examples of Jinja syntax patterns for controlling whitespace in template output. The minus sign can be added at the start or end of Jinja blocks to strip preceding or following whitespace.

LANGUAGE: jinja
CODE:
{{- ... -}}

LANGUAGE: jinja
CODE:
{%- ... %}

LANGUAGE: jinja
CODE:
{#- ... -#}

----------------------------------------

TITLE: Configuring Basic Model Deprecation in DBT
DESCRIPTION: Example of setting a past deprecation date for a DBT model using YAML configuration. Shows basic model configuration with a deprecation date in the past.

LANGUAGE: yaml
CODE:
models:
  - name: my_model
    description: deprecated
    deprecation_date: 1999-01-01 00:00:00.00+00:00

----------------------------------------

TITLE: Statement Results Data Structure
DESCRIPTION: Shows the Python data structure format of returned statement results.

LANGUAGE: python
CODE:
>>> log(states_data)

[
  ['PA'],
  ['NY'],
  ['CA'],
	...
]

----------------------------------------

TITLE: Model F Reference Example
DESCRIPTION: Shows how model references are compiled when using defer functionality

LANGUAGE: sql
CODE:
# in models/model_f.sql
with 

model_b as (
 select * from {{ ref('model_b') }}
),

model_c as (
 select * from {{ ref('model_c') }}
),

...

LANGUAGE: sql
CODE:
# in target/compiled/models/model_f.sql
with 

model_b as (
 select * from analytics.analytics.model_b
),

model_c as (
 select * from analytics.dbt_dconnors.model_b
),

...

----------------------------------------

TITLE: Configuring Dynamic Tables for Development (Snowflake)
DESCRIPTION: SQL configuration for dynamic tables in development environment for Snowflake, including warehouse and target lag settings.

LANGUAGE: sql
CODE:
{{\nconfig(\n    materialized = 'dynamic_table',\n    snowflake_warehouse = '<warehouse>',\n    target_lag = '<desired_lag>',\n    on_configuration_change = 'apply',\n)\n}}

----------------------------------------

TITLE: Configuring dbt Seeds in YAML
DESCRIPTION: Demonstrates the configuration of a dbt seed using the 'config' property in a YAML file. This allows setting seed-specific configurations.

LANGUAGE: yaml
CODE:
version: 2

seeds:
  - name: <seed_name>
    config:
      [<seed_config>](/reference/seed-configs): <config_value>
      ...

----------------------------------------

TITLE: Project-Level Check Columns Configuration
DESCRIPTION: YAML configuration in dbt_project.yml for setting default check column behavior across snapshots. Demonstrates both specific column selection and 'all' columns option.

LANGUAGE: yaml
CODE:
snapshots:
  [<resource-path>]:
    +strategy: check
    +check_cols: [column_name] | all

----------------------------------------

TITLE: Configuring Snapshot Properties in YAML for dbt v1.8 and Earlier
DESCRIPTION: This YAML structure defines snapshot properties for dbt versions 1.8 and earlier. It is similar to the v1.9+ configuration but can be placed in either the snapshots/ or models/ directory.

LANGUAGE: yaml
CODE:
version: 2

snapshots:
  - name: <snapshot name>
    [description](/reference/resource-properties/description): <markdown_string>
    [meta](/reference/resource-configs/meta): {<dictionary>}
    [docs](/reference/resource-configs/docs):
      show: true | false
      node_color: <color_id> # Use name (such as node_color: purple) or hex code with quotes (such as node_color: "#cd7f32")
    [config](/reference/resource-properties/config):
      [<snapshot_config>](/reference/snapshot-configs): <config_value>
    [tests](/reference/resource-properties/data-tests):
      - <test>
      - ...
    columns:
      - name: <column name>
        [description](/reference/resource-properties/description): <markdown_string>
        [meta](/reference/resource-configs/meta): {<dictionary>}
        [quote](/reference/resource-properties/quote): true | false
        [tags](/reference/resource-configs/tags): [<string>]
        [tests](/reference/resource-properties/data-tests):
          - <test>
          - ... # declare additional tests
      - ... # declare properties of additional columns

    - name: ... # declare properties of additional snapshots

----------------------------------------

TITLE: Basic Zip Usage with Lists
DESCRIPTION: Demonstrates basic usage of the zip context method to combine two lists into tuples. Shows how to create an iterator of tuples containing corresponding elements from each input list.

LANGUAGE: jinja
CODE:
{% set my_list_a = [1, 2] %}
{% set my_list_b = ['alice', 'bob'] %}
{% set my_zip = zip(my_list_a, my_list_b) | list %}
{% do log(my_zip) %}  {# [(1, 'alice'), (2, 'bob')] #}

----------------------------------------

TITLE: Using compare_column_values Macro for Column Auditing in dbt
DESCRIPTION: Example of using the compare_column_values macro from audit_helper to compare specific column values between an old table and a new dbt model.

LANGUAGE: sql
CODE:
{% set old_etl_relation_query %}
select * from public.dim_product
where is_latest
{% endset %}

{% set new_etl_relation_query %}
select * from {{ ref('dim_product') }}
{% endset %}

{% set audit_query = audit_helper.compare_column_values(
    a_query=old_etl_relation_query,
    b_query=new_etl_relation_query,
    primary_key="product_id",
    column_to_compare="status"
) %}

{% set audit_results = run_query(audit_query) %}

{% if execute %}
    {% do audit_results.print_table() %}
{% endif %}

----------------------------------------

TITLE: Simple SQL Query for Parsing Example
DESCRIPTION: A basic SQL query used to demonstrate the parsing process and syntax tree generation. It selects an order_id and sum of amounts, filters by year, and groups by order_id.

LANGUAGE: sql
CODE:
select 
  order_id, 
  sum(amount) as total_order_amount
from order_items
where 
  date_trunc('year', ordered_at) = '2025-01-01'
group by 1

----------------------------------------

TITLE: Coalescing Columns for Updated_at in SQL (dbt 1.8 and Earlier)
DESCRIPTION: Example of coalescing two columns (updated_at and created_at) to create a reliable updated_at column for snapshot configuration in SQL for dbt versions 1.8 and earlier.

LANGUAGE: sql
CODE:
{% snapshot orders_snapshot %}

{{
    config(
      target_schema='snapshots',
      unique_key='id',

      strategy='timestamp',
      updated_at='updated_at_for_snapshot'
    )
}}

select
    *,
    coalesce(updated_at, created_at) as updated_at_for_snapshot

from {{ source('jaffle_shop', 'orders') }}

{% endsnapshot %}

----------------------------------------

TITLE: Power Users Identification Query
DESCRIPTION: SQL query that identifies power users for each account based on creation date, activity level, and order history using array aggregation.

LANGUAGE: sql
CODE:
corporate_power_users as (

   select

       corporate_email,

       get(array_agg(user_id) within group (order by created_at asc), 0)::int as first_user_id,

       get(array_agg(user_id) within group (order by number_of_events desc), 0)::int as most_active_user_id,

       get(array_agg(user_id) within group (order by number_of_orders desc), 0)::int as most_orders_user_id

   from {{ ref('jafflegaggle_contacts') }}

   where corporate_email is not null

   group by 1

),

----------------------------------------

TITLE: Disabling Partial Parsing in dbt CLI Command
DESCRIPTION: This command demonstrates how to disable partial parsing when running dbt from the command line. It overrides the configuration set in profiles.yml.

LANGUAGE: text
CODE:
dbt --no-partial-parse run

----------------------------------------

TITLE: Configuring Seed Alias in dbt_project.yml
DESCRIPTION: Sets a custom alias for seeds at the project level using dbt_project.yml configuration.

LANGUAGE: yaml
CODE:
seeds:
  your_project:
    product_categories:
      +alias: categories_data

----------------------------------------

TITLE: CLI-style YAML Selector Definition
DESCRIPTION: Shows how to define a simple CLI-style selector using tags.

LANGUAGE: yaml
CODE:
definition:
  'tag:nightly'

----------------------------------------

TITLE: SQL EXTRACT Function Example with Multiple Date Parts
DESCRIPTION: Shows a practical example using the EXTRACT function to get week, month, and year from order dates in a dbt jaffle shop orders table. Demonstrates extracting multiple date components in a single query.

LANGUAGE: sql
CODE:
select 
	order_id,
	order_date,
	extract(week from order_date) as order_week,
	extract(month from order_date) as order_month,
	extract(year from order_date) as order_year
from {{ ref('orders') }}

----------------------------------------

TITLE: Referencing dbt Jinja Function in SQL
DESCRIPTION: Example of using the 'target' Jinja function in SQL, which requires knowledge of the current connection profile.

LANGUAGE: sql
CODE:
{{target}}

----------------------------------------

TITLE: Column Quote Configuration Example with Tests
DESCRIPTION: Practical example of configuring column quoting for a source table with tests, specifically for Snowflake compatibility. Shows how to properly quote columns to avoid SQL compilation errors.

LANGUAGE: yml
CODE:
version: 2

sources:
  - name: stripe
    tables:
      - name: payment
        columns:
          - name: orderID
            quote: true
            tests:
              - not_null

----------------------------------------

TITLE: DATEADD Implementation in BigQuery
DESCRIPTION: Illustrates BigQuery's date_add function syntax which uses INTERVAL keyword but doesn't support sub-day intervals.

LANGUAGE: sql
CODE:
date_add( {{ from_date }}, INTERVAL {{ interval }} {{ datepart }} )

----------------------------------------

TITLE: Python Model for API Integration
DESCRIPTION: Snowflake Python model that uses external access integration to fetch data from the Carbon Intensity API

LANGUAGE: python
CODE:
import snowflake.snowpark as snowpark
def model(dbt, session: snowpark.Session):
    dbt.config(
        materialized="table",
        external_access_integrations=["test_external_access_integration"],
        packages=["httpx==0.26.0"]
    )
    import httpx
    return session.create_dataframe(
            [{"carbon_intensity": httpx.get(url="https://api.carbonintensity.org.uk/intensity").text}]
    )

----------------------------------------

TITLE: Referencing Model Versions
DESCRIPTION: Demonstrates how to reference specific versions of a model in dbt SQL code.

LANGUAGE: sql
CODE:
select * from {{ ref('dim_customers', v=2) }}

----------------------------------------

TITLE: Extracting Columns from Event Streams with dbt Macro
DESCRIPTION: This dbt macro iterates through every row in the event stream tables to identify all of the columns that will be part of the domain model table.

LANGUAGE: SQL
CODE:
{%- macro stream_model_extract_columns_macro(streams_var, streams_schema='streams') -%}

SELECT DISTINCT
    CONCAT('DATA:', KEY, ' ', 'AS', ' ', UPPER(e.KEY)) AS COLUMN_NAME
FROM
(
{% for stream in streams_var %}
    SELECT
        '{{ stream }}' as streamName,
        RECORD_CONTENT:data AS data
    FROM {{ source(streams_schema, stream ) }}
    {%- if not loop.last %} UNION ALL{% endif -%}
{% endfor %}
), LATERAL FLATTEN( INPUT => data ) AS e

{%- endmacro -%}

----------------------------------------

TITLE: Defining Constraints in YAML for Specific Model
DESCRIPTION: YAML configuration for defining constraints on a specific model named 'dim_customers'.

LANGUAGE: yaml
CODE:
models:
  - name: dim_customers
    config:
      contract:
        enforced: true
    columns:
      - name: id
        data_type: int
        constraints:
          - type: not_null
          - type: primary_key
          - type: check
            expression: "id > 0"
      - name: customer_name
        data_type: text
      - name: first_transaction_date
        data_type: date

----------------------------------------

TITLE: Using clean-project-files-only flag with dbt clean
DESCRIPTION: Runs the dbt clean command with the default behavior, deleting all paths within the project directory specified in clean-targets.

LANGUAGE: shell
CODE:
dbt clean --clean-project-files-only

----------------------------------------

TITLE: Status method request example
DESCRIPTION: Example JSON request for the status method, which returns the current status of the rpc server.

LANGUAGE: json
CODE:
{
    "jsonrpc": "2.0",
    "method": "status",
    "id": "2db9a2fe-9a39-41ef-828c-25e04dd6b07d"
}

----------------------------------------

TITLE: Configuring watsonx.data Spark Connection Profile
DESCRIPTION: Example YAML configuration for connecting dbt to IBM watsonx.data Spark. This profile should be placed in the profiles.yml file in the .dbt directory and includes all necessary connection parameters for both SaaS and Software instances.

LANGUAGE: yaml
CODE:
project_name:
  target: "dev"
  outputs:
    dev:
      type: watsonx_spark
      method: http
      schema: [schema name]
      host: [hostname]
      uri: [uri]
      catalog: [catalog name]
      use_ssl: false
      auth:
        instance: [Watsonx.data Instance ID]
        user: [username]
        apikey: [apikey]

----------------------------------------

TITLE: Building a Feature Table as a dbt Model
DESCRIPTION: This SQL query demonstrates how to use the rolling_agg macro to create various time-based aggregations for customer transactions, forming a feature table in dbt.

LANGUAGE: sql
CODE:
select
   tx_datetime,
   customer_id,
   tx_amount,
   {{ rolling_agg("TX_AMOUNT", "CUSTOMER_ID", "TX_DATETIME", "1 days", "sum") }}
   as tx_amount_1d,
   {{ rolling_agg("TX_AMOUNT", "CUSTOMER_ID", "TX_DATETIME", "7 days", "sum") }}
   as tx_amount_7d,
   {{ rolling_agg("TX_AMOUNT", "CUSTOMER_ID", "TX_DATETIME", "30 days", "sum") }}
   as tx_amount_30d,
   {{ rolling_agg("TX_AMOUNT", "CUSTOMER_ID", "TX_DATETIME", "1 days", "avg") }}
   as tx_amount_avg_1d,
   {{ rolling_agg("TX_AMOUNT", "CUSTOMER_ID", "TX_DATETIME", "7 days", "avg") }}
   as tx_amount_avg_7d,
   {{ rolling_agg("TX_AMOUNT", "CUSTOMER_ID", "TX_DATETIME", "30 days", "avg") }}
   as tx_amount_avg_30d,
   {{ rolling_agg("*", "CUSTOMER_ID", "TX_DATETIME", "1 days", "count") }}
   as tx_cnt_1d,
   {{ rolling_agg("*", "CUSTOMER_ID", "TX_DATETIME", "7 days", "count") }}
   as tx_cnt_7d,
   {{ rolling_agg("*", "CUSTOMER_ID", "TX_DATETIME", "30 days", "count") }}
   as tx_cnt_30d
from {{ ref("stg_transactions") }}

----------------------------------------

TITLE: Stopping Docker Container with Astro CLI
DESCRIPTION: Command to stop the Docker container running the Airflow deployment when finished using it.

LANGUAGE: bash
CODE:
$ astrocloud dev stop

----------------------------------------

TITLE: Using pytz Module for Timezone Operations
DESCRIPTION: Shows how to use the pytz module to handle timezone operations in DBT. Example demonstrates localizing a datetime object to US/Eastern timezone.

LANGUAGE: jinja
CODE:
{% set dt = modules.datetime.datetime(2002, 10, 27, 6, 0, 0) %}
{% set dt_local = modules.pytz.timezone('US/Eastern').localize(dt) %}
{{ dt_local }}

----------------------------------------

TITLE: SQL CASE WHEN Basic Syntax
DESCRIPTION: Demonstrates the general syntax for SQL CASE WHEN statements. This structure allows for multiple scenarios to be evaluated and corresponding results to be returned based on the conditions.

LANGUAGE: sql
CODE:
case when [scenario 1] then [result 1]
     when [scenario 2] then [result 2]
    -- ‚Ä¶as many scenarios as you want
     when [scenario n] then [result n]
     else [fallback result] -- this else is optional
end as <new_field_name>

----------------------------------------

TITLE: Configuring Snapshot with Coalesced Updated_at in YAML (dbt 1.9+)
DESCRIPTION: Example of configuring a snapshot using the coalesced updated_at column from a staging model in YAML for dbt version 1.9 and above.

LANGUAGE: yaml
CODE:
snapshots:
  - name: orders_snapshot
    relation: ref('staging_orders')
    config:
      schema: snapshots
      unique_key: id
      strategy: timestamp
      updated_at: updated_at_for_snapshot

----------------------------------------

TITLE: Implementing Hashed Surrogate Keys in dbt
DESCRIPTION: Example of using dbt_utils.generate_surrogate_key() to create hashed surrogate keys from multiple columns in a reporting model.

LANGUAGE: sql
CODE:
with 

orders as (
	select * from {{ ref('fct_orders') }} 
),

agg as (

	select 
		date_trunc(day, order_date) as report_date 	
		user_id, 
		count(*) as total_orders
  from orders
  group by 1,2

),

final as (
	
	select
		{{ dbt_utils.generate_surrogate_key([
				'report_date', 
				'user_id'
			])
		}} as unique_key, 
		*
	from agg

)

select * from final

----------------------------------------

TITLE: Configuring OAuth Security Integration in Snowflake
DESCRIPTION: SQL command to modify OAuth Security integration by setting the external OAuth scope mapping attribute to 'scp'.

LANGUAGE: sql
CODE:
ALTER INTEGRATION <my_int_name> SET EXTERNAL_OAUTH_SCOPE_MAPPING_ATTRIBUTE = 'scp';

----------------------------------------

TITLE: Configuring Log Colors in dbt profiles.yml
DESCRIPTION: Shows how to disable colored output in the log file by setting a configuration in profiles.yml.

LANGUAGE: yaml
CODE:
config:
  use_colors_file: False

----------------------------------------

TITLE: Configuring Column Quoting in DBT Sources
DESCRIPTION: YAML configuration for controlling column name quoting in source table definitions. Essential for preserving column casing and handling reserved words in source tables.

LANGUAGE: yml
CODE:
version: 2

sources:
  - name: source_name
    tables:
      - name: table_name
        columns:
          - name: column_name
            quote: true | false

----------------------------------------

TITLE: Valid dbt Profile Configuration
DESCRIPTION: Example of a valid dbt profiles.yml configuration structure.

LANGUAGE: yaml
CODE:
jaffle_shop: # this does not match the profile: key
  target: dev

  outputs:
    dev:
      type: postgres
      schema: dbt_alice
      ... # other connection details

----------------------------------------

TITLE: DATEADD Implementation in Postgres
DESCRIPTION: Shows Postgres' interval-based date addition syntax since it lacks a native DATEADD function.

LANGUAGE: sql
CODE:
{{ from_date }} + (interval '{{ interval }} {{ datepart }}')

----------------------------------------

TITLE: Configuring Iceberg Table Format in dbt Model for Snowflake
DESCRIPTION: This YAML configuration snippet demonstrates how to set up an Iceberg table format for a dbt model in Snowflake. It specifies the materialization type, table format, and additional Iceberg-specific properties.

LANGUAGE: yaml
CODE:
{{ config(
    materialized='table',
    table_format='iceberg',
    location_root='@my_iceberg_storage.my_stage/iceberg_storage',
    auto_create_objects=true
) }}

----------------------------------------

TITLE: dbt Cloud Project Configuration
DESCRIPTION: YAML configuration for setting up custom defer environment in dbt Cloud

LANGUAGE: yaml
CODE:
dbt-cloud:
  project-id: <Your project id>
  defer-env-id: <An environment id>

----------------------------------------

TITLE: Component Mileage Query
DESCRIPTION: SQL query to calculate total mileage for a specific component using the multivalued dimension model.

LANGUAGE: sql
CODE:
select
    mdim_components.component_id,
    sum(fct_daily_mileage.miles) as miles
from
    fct_daily_mileage
inner join
    mdim_components
    on
        fct_daily_mileage.component_sk = mdim_components.component_sk
group by
    1
where
    component_id = 'Wheel-1'

----------------------------------------

TITLE: Dynamic Schema Usage Grants Macro
DESCRIPTION: Advanced macro implementation for automatically granting schema usage based on model-level select grants.

LANGUAGE: sql
CODE:
{% macro grant_usage_on_schemas_where_select() %}
    /*
      Note: This is pseudo code only, for demonstration purposes
      For every role that can access at least one object in a schema,
      grant 'usage' on that schema to the role.
      That way, users with the role can run metadata queries showing objects
      in that schema (a common need for BI tools)
    */
    {% set schema_grants = {} %}
    {% if execute %}
      {% for node in graph.nodes.values() %}
        {% set grants = node.config.get('grants') %}
        {% set select_roles = grants['select'] if grants else [] %}
        {% if select_roles %}
          {% set database_schema = node.database ~ "." ~ node.schema %}
          {% if database_schema in database_schemas %}
            {% do schema_grants[database_schema].add(select_roles) %}
          {% else %}
            {% do schema_grants.update({database_schema: set(select_roles)}) %}
          {% endif %}
        {% endif %}
      {% endfor %}
    {% endif %}
    {% set grant_list %}
      {% for schema in schema_grants %}
        {% for role in schema_grants[schema] %}
          grant usage on schema {{ schema }} to {{ role }};
        {% endfor %}
      {% endfor %}
    {% endset %}
    {{ return(grant_list) }}
{% endmacro %}

----------------------------------------

TITLE: Configuring General Options for Seeds in Property YAML
DESCRIPTION: This snippet shows how to configure general options for seeds in a seed's properties.yml file, including enabled, tags, hooks, database, schema, and more.

LANGUAGE: yaml
CODE:
version: 2

seeds:
  - name: [<seed-name>]
    config:
      [enabled]: true | false
      [tags]: <string> | [<string>]
      [pre_hook]: <sql-statement> | [<sql-statement>]
      [post_hook]: <sql-statement> | [<sql-statement>]
      [database]: <string>
      [schema]: <string>
      [alias]: <string>
      [persist_docs]: <dict>
      [full_refresh]: <boolean>
      [meta]: {<dictionary>}
      [grants]: {<dictionary>}
      [event_time]: my_time_field

----------------------------------------

TITLE: Basic SQL DISTINCT Query Structure
DESCRIPTION: Demonstrates the basic syntax for using DISTINCT in a SELECT statement to remove duplicate rows from query results.

LANGUAGE: sql
CODE:
select
	distinct
	row_1,
	row_2
from my_data_source

----------------------------------------

TITLE: Dynamic Database Selection Using target.name in YAML
DESCRIPTION: A YAML configuration example showing how to dynamically set different source databases based on the target environment (dev/qa/prod) in Snowflake.

LANGUAGE: yaml
CODE:
version: 2
 
sources:
  - name: source_name 
    database: |
      {%- if  target.name == "dev" -%} raw_dev
      {%- elif target.name == "qa"  -%} raw_qa
      {%- elif target.name == "prod"  -%} raw_prod
      {%- else -%} invalid_database
      {%- endif -%}
    schema: source_schema

----------------------------------------

TITLE: Kill method request example
DESCRIPTION: Example JSON request for the kill method, used to terminate a running task.

LANGUAGE: json
CODE:
{
    "jsonrpc": "2.0",
    "method": "kill",
    "id": "2db9a2fe-9a39-41ef-828c-25e04dd6b07d",
    "params": {
        "task_id": "{ the task id to terminate }"
    }
}

----------------------------------------

TITLE: Array Operation Macros in dbt
DESCRIPTION: Demonstrates array manipulation functions like append, concat and construct

LANGUAGE: sql
CODE:
{{ dbt.array_append("array_column", "element_column") }}
{{ dbt.array_concat("array_column_1", "array_column_2") }}

----------------------------------------

TITLE: Compiling SQL for a Saved Query
DESCRIPTION: Example of compiling SQL for a saved query named 'new_customer_orders' with a limit of 5 records.

LANGUAGE: sql
CODE:
select * from {{ semantic_layer.query(saved_query="new_customer_orders", limit=5, compile=True}}

----------------------------------------

TITLE: Implementing Grouped Recency Check in dbt YAML Configuration
DESCRIPTION: This YAML snippet demonstrates how to configure a grouped recency check in dbt using dbt-utils. It checks for daily records for each turnstile in each station, grouping by station_id and turnstile_id.

LANGUAGE: yaml
CODE:
models:
  - name: turnstile_entries
    tests:
      - dbt_utils.recency:
          datepart: day
          field: recorded_at
          interval: 1
          # Check for recency for each turnstile_id at each station_id
          group_by_columns:
            - station_id
            - turnstile_id

----------------------------------------

TITLE: Table Replacement in Snowflake
DESCRIPTION: Illustrative example of how dbt might generate SQL to replace an existing table in Snowflake. Creates schema if not exists, then uses CREATE OR REPLACE TABLE statement.

LANGUAGE: sql
CODE:
create schema if not exists analytics.dbt_alice;

create or replace table analytics.dbt_alice.test_model as (
    select 1 as my_column
);

----------------------------------------

TITLE: Table Replacement in Snowflake
DESCRIPTION: Illustrative example of how dbt might generate SQL to replace an existing table in Snowflake. Creates schema if not exists, then uses CREATE OR REPLACE TABLE statement.

LANGUAGE: sql
CODE:
create schema if not exists analytics.dbt_alice;

create or replace table analytics.dbt_alice.test_model as (
    select 1 as my_column
);

----------------------------------------

TITLE: Configuring Dynamic Tables in dbt (Snowflake)
DESCRIPTION: SQL configuration block for setting up a dynamic table (Snowflake's equivalent of materialized views) in dbt for the Snowflake adapter.

LANGUAGE: sql
CODE:
{{\nconfig(\n    materialized = 'dynamic_table',\n)\n}}

----------------------------------------

TITLE: Referencing Special Properties in dbt YAML Configuration
DESCRIPTION: This snippet demonstrates the syntax for referencing special properties in dbt, which cannot be configured in the dbt_project.yml file or using config() blocks. It uses a placeholder for the property name.

LANGUAGE: yaml
CODE:
dbt_project.yml

----------------------------------------

TITLE: Converting List to Set using set() in Jinja
DESCRIPTION: Demonstrates how to use the 'set' context method to convert a list with duplicate elements to a unique set.

LANGUAGE: jinja
CODE:
{% set my_list = [1, 2, 2, 3] %}
{% set my_set = set(my_list) %}
{% do log(my_set) %}  {# {1, 2, 3} #}

----------------------------------------

TITLE: Declaring Project Dependencies in dbt YAML Configuration
DESCRIPTION: This snippet shows how to create a dependencies.yml file to declare upstream project dependencies, which is crucial for establishing connections between projects in a dbt Mesh architecture.

LANGUAGE: yaml
CODE:
projects:
  - name: jaffle_shop

----------------------------------------

TITLE: Custom Test Directory Configuration in dbt_project.yml
DESCRIPTION: Demonstrates how to use a custom directory name for tests instead of the default 'tests' directory. This example uses 'custom_tests' as the test directory.

LANGUAGE: yml
CODE:
test-paths: ["custom_tests"]

----------------------------------------

TITLE: Running Adapter-Specific Tests
DESCRIPTION: Shell commands to run unit tests for specific database adapters using different profiles

LANGUAGE: shell
CODE:
# Run unit tests on BigQuery
dbt run-operation run_unit_tests --profile bigquery
# `default__test_to_literal` is internally called.

# Run unit tests on postgres
dbt run-operation run_unit_tests --profile postgres
# `postgres__test_to_literal` is internally called.

----------------------------------------

TITLE: GitHub Actions Workflow for Pre-commit Checks
DESCRIPTION: CI workflow configuration that runs pre-commit checks on pull requests. Includes steps for installing dependencies, identifying changed files, and executing pre-commit hooks.

LANGUAGE: yaml
CODE:
name: pre-commit-check

on:
  pull_request:
    branches: 
      - main

jobs:
  pre-commit-pip:
    name: Install pre-commit via pip
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: ['ubuntu-latest']
        python-version: [3.8]

    env:
      DBT_PROFILES_DIR: .
      DB_PASSWORD: ${{ secrets.DB_PASSWORD }}

    steps:
      - name: Checkout branch
        uses: actions/checkout@v3

      - name: Install dbt and pre-commit
        shell: bash -l {0}
        run: |
          python -m pip install dbt-postgres pre-commit

      - name: Get changed files
        id: get_file_changes
        uses: trilom/file-changes-action@v1.2.4
        with:
          output: ' '

      - name: Get changed .sql files in /models to lint
        id: get_files_to_lint
        shell: bash -l {0}
        run: |
          echo "::set-output name=files::$(
          echo \
          $(echo ${{ steps.get_file_changes.outputs.files_modified }} |
          tr -s ' ' '\n' |
          grep -E '^models.*[.]sql$' |
          tr -s '\n' ' ') \
          $(echo ${{ steps.get_file_changes.outputs.files_added }} |
          tr -s ' ' '\n' |
          grep -E '^models.*[.]sql$' |
          tr -s '\n' ' ')
          )"

      - name: Run pre-commit
        shell: bash -l {0}
        run: |
          pre-commit run --files ${{ steps.get_files_to_lint.outputs.files }}

----------------------------------------

TITLE: Database and Schema Override Example for GitHub Source
DESCRIPTION: Example showing how to override the database and schema configuration for a Fivetran GitHub Source package. Demonstrates practical application of source overrides.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: github
    overrides: github_source

    database: RAW
    schema: github_data

----------------------------------------

TITLE: DATEDIFF Function Syntax for Snowflake, Databricks, and Redshift
DESCRIPTION: The standard syntax for the DATEDIFF function in Snowflake, Databricks, and Amazon Redshift. It takes three arguments: date part (unit of measurement), start date/time, and end date/time.

LANGUAGE: sql
CODE:
datediff(<date part>, <start date/time>, <end date/time>)

----------------------------------------

TITLE: Configuring Materialized Views for Development (Redshift)
DESCRIPTION: SQL configuration for materialized views in development environment for Redshift, disabling auto-refresh.

LANGUAGE: sql
CODE:
{{\nconfig(\n    materialized = 'materialized_view',\n    on_configuration_change = 'apply',\n    auto_refresh = False\n)\n}}

----------------------------------------

TITLE: Deleting Test Data from Temp Table in SQL Server Stored Procedure
DESCRIPTION: This SQL Server code snippet demonstrates a DELETE operation on the temporary table to remove test data. It's part of the stored procedure approach to data transformation.

LANGUAGE: sql
CODE:
   DELETE tmp
   FROM #temp_orders AS tmp
   INNER JOIN
     criteria_table cwo WITH (NOLOCK)
   ON tmp.orderid = cwo.orderid
   WHERE ISNULL(tmp.is_test_record,'false') = 'true'

----------------------------------------

TITLE: Specifying Relative Paths in dbt Projects
DESCRIPTION: Demonstration of correct and incorrect path specifications in dbt projects. Paths must be relative to dbt_project.yml file location to ensure proper project functionality.

LANGUAGE: yaml
CODE:
{props.path}

LANGUAGE: yaml
CODE:
{props.absolute}

----------------------------------------

TITLE: RPC request format specification
DESCRIPTION: JSON schema for submitting queries to the dbt-rpc server, including method, id, and optional timeout parameter.

LANGUAGE: json
CODE:
{
    "jsonrpc": "2.0",
    "method": "{ a valid rpc server command }",
    "id": "{ a unique identifier for this query }",
    "params": {
        "timeout": { timeout for the query in seconds, optional },
    }
}

----------------------------------------

TITLE: Running DBT Tests with Enhanced Options (1.8+)
DESCRIPTION: Examples of running dbt tests with expanded selection criteria for version 1.8 and later. Includes options for data tests, unit tests, and combined selections.

LANGUAGE: bash
CODE:
# run data and unit tests
dbt test

# run only data tests
dbt test --select test_type:data

# run only unit tests
dbt test --select test_type:unit

# run tests for one_specific_model
dbt test --select "one_specific_model"

# run tests for all models in package
dbt test --select "some_package.*"

# run only data tests defined singularly
dbt test --select "test_type:singular"

# run only data tests defined generically
dbt test --select "test_type:generic"

# run data tests limited to one_specific_model
dbt test --select "one_specific_model,test_type:data"

# run unit tests limited to one_specific_model
dbt test --select "one_specific_model,test_type:unit"

----------------------------------------

TITLE: Configuring Lookback in dbt_project.yml
DESCRIPTION: Sets the lookback value to 2 for the user_sessions model using the project configuration file.

LANGUAGE: yaml
CODE:
models:
  my_project:
    user_sessions:
      +lookback: 2

----------------------------------------

TITLE: Querying Multiple Order Tables in SQL
DESCRIPTION: Example of querying different order tables to determine the correct source of truth. This snippet demonstrates the confusion that can arise from poorly named models.

LANGUAGE: sql
CODE:
-- select * from analytics.order  limit 10
-- select * from analytics.orders  limit 10
select * from analytics.orders_new  limit 10

----------------------------------------

TITLE: Querying Multiple Order Tables in SQL
DESCRIPTION: Example of querying different order tables to determine the correct source of truth. This snippet demonstrates the confusion that can arise from poorly named models.

LANGUAGE: sql
CODE:
-- select * from analytics.order  limit 10
-- select * from analytics.orders  limit 10
select * from analytics.orders_new  limit 10

----------------------------------------

TITLE: Configuring Materialized Views in dbt (Postgres/Redshift/Databricks/BigQuery)
DESCRIPTION: SQL configuration block for setting up a materialized view in dbt for Postgres, Redshift, Databricks, or BigQuery adapters.

LANGUAGE: sql
CODE:
{{\nconfig(\n    materialized = 'materialized_view',\n)\n}}

----------------------------------------

TITLE: Running dbt from Failure with Command Line
DESCRIPTION: Command to rerun failed dbt models using the build command with selective execution and state artifacts

LANGUAGE: bash
CODE:
dbt build --select result:error+ --defer --state <previous_state_artifacts>

----------------------------------------

TITLE: Executing Intersection Operations for Ancestors in dbt CLI
DESCRIPTION: Shows how to use comma-separated arguments without spaces to find common ancestors between multiple models using the + prefix operator.

LANGUAGE: bash
CODE:
dbt run --select "+snowplow_sessions,+fct_orders"

----------------------------------------

TITLE: Interpreting dbt Core CLI Output for Model Build Status
DESCRIPTION: This snippet demonstrates how to read the output from a dbt build command. It shows the start and completion entries for a model, including timestamps, model type, and build duration.

LANGUAGE: shell
CODE:
20:24:51  5 of 10 START sql view model main.stg_products ......... [RUN]
20:24:51  5 of 10 OK created sql view model main.stg_products .... [OK in 0.13s]

----------------------------------------

TITLE: State Selection with Test Exclusion
DESCRIPTION: Shell commands showing how to run state selection while excluding specific test types like relationship tests.

LANGUAGE: shell
CODE:
dbt run -s "state:modified"
dbt test -s "state:modified" --exclude "test_name:relationships"

----------------------------------------

TITLE: Linux Installation Commands
DESCRIPTION: Commands to extract and verify dbt Cloud CLI installation on Linux

LANGUAGE: bash
CODE:
tar -xf dbt_0.29.9_linux_amd64.tar.gz
./dbt --version

----------------------------------------

TITLE: Using debug() in SQL Queries
DESCRIPTION: These SQL snippets demonstrate how to place the debug() command before and after a query execution to pinpoint where errors occur.

LANGUAGE: sql
CODE:
{% set my_results = run_query(sql_statement) %}
{{ debug() }}

LANGUAGE: sql
CODE:
{{ debug() }}
{% set my_results = run_query(sql_statement) %}

----------------------------------------

TITLE: Implementing Derived Metrics Examples in YAML
DESCRIPTION: This snippet demonstrates practical examples of derived metrics, including a basic gross profit calculation, a filtered metric for food orders, and a month-over-month growth calculation with offset.

LANGUAGE: yaml
CODE:
metrics:
  - name: order_gross_profit
    description: Gross profit from each order.
    type: derived
    label: Order gross profit
    type_params:
      expr: revenue - cost
      metrics:
        - name: order_total
          alias: revenue
        - name: order_cost
          alias: cost
  - name: food_order_gross_profit
    label: Food order gross profit
    description: "The gross profit for each food order."
    type: derived
    type_params:
      expr: revenue - cost
      metrics:
        - name: order_total
          alias: revenue
          filter: |
            {{ Dimension('order__is_food_order') }} = True
        - name: order_cost
          alias: cost
          filter: |
            {{ Dimension('order__is_food_order') }} = True
  - name: order_total_growth_mom
    description: "Percentage growth of orders total completed to 1 month ago"
    type: derived
    label: Order total growth % M/M
    type_params:
      expr: (order_total - order_total_prev_month)*100/order_total_prev_month
      metrics: 
        - name: order_total
        - name: order_total
          offset_window: 1 month
          alias: order_total_prev_month

----------------------------------------

TITLE: Defining Data Tests for Sources in dbt YAML
DESCRIPTION: This snippet shows how to define data tests for sources in a dbt YAML file. It includes examples of table-level and column-level tests with configurations.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: <source_name>
    tables:
    - name: <table_name>
      tests:
        - [<test_name>]
        - [<test_name>]:
            <argument_name>: <argument_value>
            [config]:
              [<test_config>]: <config-value>

      columns:
        - name: <column_name>
          tests:
            - [<test_name>]
            - [<test_name>]:
                <argument_name>: <argument_value>
                [config]:
                  [<test_config>]: <config-value>

----------------------------------------

TITLE: Configuring MySQL Connection Profile in profiles.yml
DESCRIPTION: Configuration settings for connecting dbt to MySQL database. Specifies required fields like server, port, schema, credentials and SSL settings.

LANGUAGE: yaml
CODE:
your_profile_name:
  target: dev
  outputs:
    dev:
      type: mysql
      server: localhost
      port: 3306
      schema: analytics
      username: your_mysql_username
      password: your_mysql_password
      ssl_disabled: True

----------------------------------------

TITLE: Extracting Month and Calculating Average Order Amount with SQL DATE_PART
DESCRIPTION: This SQL query demonstrates how to use the DATE_PART function to extract the month from an order date and calculate the average order amount for each month. It uses the Jaffle Shop's 'orders' table as a reference.

LANGUAGE: sql
CODE:
select
	date_part('month', order_date) as order_month,
	round(avg(amount)) as avg_order_amount
from {{ ref('orders') }}
group by 1

----------------------------------------

TITLE: SQL Data Structure - User Role Information
DESCRIPTION: Example table showing user role data including location, role types, and zone information used for filtering and analysis.

LANGUAGE: sql
CODE:
| User_Id | Location   | Role        | Level | Zone |
|---------|------------|-------------|--------|------|
| 123     | California | Editor      | AAA    | 1    |
| 427     | Utah       | Participant | ABA    | 1    |
| 864     | Georgia    | Admin       | CCC    | 3    |

----------------------------------------

TITLE: Configuring Materialized Views for Development (Postgres)
DESCRIPTION: SQL configuration for materialized views in development environment for Postgres, including manual refresh settings.

LANGUAGE: sql
CODE:
{{\nconfig(\n    materialized = 'materialized_view',\n    on_configuration_change = 'apply',\n)\n}}

----------------------------------------

TITLE: Programmatic Model Name Generation with dbt CLI
DESCRIPTION: Command chain to automatically generate a list of model names from a specific directory using dbt ls and xargs.

LANGUAGE: bash
CODE:
dbt ls -m models/core/activity_based_interest --output name | xargs -I{} echo -n ' "{}",'

----------------------------------------

TITLE: Filtering Duplicate Records
DESCRIPTION: SQL query that filters out duplicate records based on the is_real_diff flag.

LANGUAGE: sql
CODE:
filter_real_diffs as (

    select *
  
    from mark_real_diffs
  
    where is_real_diff = true

)

select * from filter_real_diffs

----------------------------------------

TITLE: Configuring Concurrent Batches in SQL Model File
DESCRIPTION: This SQL snippet shows how to configure an incremental model with microbatch strategy and enable parallel batch execution using the 'concurrent_batches' config within the model file.

LANGUAGE: sql
CODE:
{{
  config(
    materialized='incremental',
    incremental_strategy='microbatch',
    event_time='session_start',
    begin='2020-01-01',
    batch_size='day',
    concurrent_batches=true, # value set to true to run batches in parallel
    ...
  )
}}

select ...

----------------------------------------

TITLE: DBT-Core Python Version Compatibility Table in Markdown
DESCRIPTION: Markdown table showing compatibility between Python versions 3.9-3.12 and dbt-core versions 1.0-1.8. Indicates support with checkmarks and lack of support with X marks.

LANGUAGE: markdown
CODE:
| dbt-core version | v1.8 | v1.7 | v1.6 | v1.5 | v1.4 | v1.3 | v1.2 | v1.1 | v1.0 |
|:-----------------|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:-----:|:-----:|
| Python 3.12      | ‚úÖ    | ‚úÖ    | ‚ùå    | ‚ùå    | ‚ùå    | ‚ùå    | ‚ùå    | ‚ùå    | ‚ùå    |
| Python 3.11      | ‚úÖ    | ‚úÖ    | ‚úÖ    | ‚úÖ    | ‚úÖ    | ‚ùå    | ‚ùå    | ‚ùå    | ‚ùå    |
| Python 3.10      | ‚úÖ    | ‚úÖ    | ‚úÖ    | ‚úÖ    | ‚úÖ    | ‚úÖ    | ‚úÖ    | ‚úÖ    | ‚úÖ    |
| Python 3.9       | ‚úÖ    | ‚úÖ    | ‚úÖ    | ‚úÖ    | ‚úÖ    | ‚úÖ    | ‚úÖ    | ‚úÖ    | ‚úÖ    |

----------------------------------------

TITLE: Boolean Config Flag Usage in dbt CLI
DESCRIPTION: Illustrates the structure for enabling or disabling boolean configuration flags in dbt CLI commands, using placeholders for subcommand and config name.

LANGUAGE: bash
CODE:
dbt <SUBCOMMAND> --<THIS-CONFIG> 
dbt <SUBCOMMAND> --no-<THIS-CONFIG>

----------------------------------------

TITLE: Implementing debug macro in dbt SQL macro
DESCRIPTION: Demonstrates how to use the `{{ debug() }}` macro within a dbt SQL macro to open an iPython debugger for development purposes. The `DBT_MACRO_DEBUGGING` environment variable must be set to use this feature.

LANGUAGE: sql
CODE:
{% macro my_macro() %}

  {% set something_complex = my_complicated_macro() %}
  
  {{ debug() }}

{% endmacro %}

----------------------------------------

TITLE: Filtering Customers with Multiple Orders Using HAVING
DESCRIPTION: This example uses the HAVING clause to filter customers who have placed more than one order. It demonstrates how HAVING can be used with aggregates, which is not possible with WHERE.

LANGUAGE: sql
CODE:
select
    customer_id,
    count(order_id) as num_orders
from {{ ref('orders') }}
group by 1
having num_orders > 1 --if you replace this with `where`, this query would not successfully run

----------------------------------------

TITLE: Hive Catalog Configuration Settings
DESCRIPTION: Recommended Hive connector settings for working with dbt, configuring metastore cache and refresh intervals.

LANGUAGE: java
CODE:
hive.metastore-cache-ttl=0s
hive.metastore-refresh-interval=5s

----------------------------------------

TITLE: dbt Show Test Failure Preview
DESCRIPTION: Example showing how to use dbt show to preview failing test results, specifically for duplicate value detection.

LANGUAGE: bash
CODE:
$ dbt build -s "my_model_with_duplicates"
13:22:47 Running with dbt=1.5.0
...
13:22:48 Completed with 1 error and 0 warnings:
13:22:48
13:22:48 Failure in test unique_my_model_with_duplicates (models/schema.yml)
13:22:48   Got 1 result, configured to fail if not 0
13:22:48
13:22:48   compiled code at target/compiled/my_dbt_project/models/schema.yml/unique_my_model_with_duplicates_id.sql
13:22:48
13:22:48 Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2

$ dbt show -s "unique_my_model_with_duplicates_id"
13:22:53 Running with dbt=1.5.0
13:22:53 Found 4 models, 2 tests, 0 snapshots, 0 analyses, 309 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
13:22:53
13:22:53 Concurrency: 5 threads (target='dev')
13:22:53
13:22:53 Previewing node 'unique_my_model_with_duplicates_id':
| unique_field | n_records |
| ------------ | --------- |
|            1 |         2 |

----------------------------------------

TITLE: SQL Query Example - User Login Data Structure
DESCRIPTION: Example table structure showing login data with multiple date fields and user IDs, demonstrating the kind of data analytics engineers work with.

LANGUAGE: sql
CODE:
| Login_Date | Session_Date | User_Id |
|------------|--------------|----------|
| 2022-08-01 | 2022-08-01   | 123     |
| 2022-08-01 | 2022-08-03   | 123     |
| 2022-08-04 | 2022-08-04   | 975     |
| 2022-08-04 | 2022-08-04   | NULL    |

----------------------------------------

TITLE: Incorrect docs-paths configuration using absolute path
DESCRIPTION: Shows an example of how not to configure docs-paths using an absolute path. This approach is discouraged as it reduces portability and can cause issues across different environments.

LANGUAGE: yaml
CODE:
docs-paths: ["/Users/username/project/docs"]

----------------------------------------

TITLE: Displaying Query Builder Functions Table in HTML
DESCRIPTION: This HTML snippet creates a table showing the various functions available in the Query Builder menu, including Metrics, Group By, Time Range, Where, Order By, and Limit.

LANGUAGE: html
CODE:
<table>
  <thead>
    <tr>
      <th>Menu items</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Metrics</td>
      <td>Search and select metrics.</td>
    </tr>
    <tr>
      <td>Group By</td>
      <td>Search and select dimensions or entities to group by. Dimensions are grouped by the entity of the semantic model they come from. You may choose dimensions on their own without metrics.</td>
    </tr>
    <tr>
      <td>Time Range</td>
      <td>Quickly select time ranges to look at the data, which applies to the main time series for the metrics (metric time), or do more advanced filter using the "Custom" selection.</td>
    </tr>
    <tr>
      <td>Where</td>
      <td>Filter your data. This includes categorical and time filters.</td>
    </tr>
    <tr>
      <td>Order By</td>
      <td>Return your data order.</td>
    </tr>
    <tr>
      <td>Limit</td>
      <td>Set a limit for the rows of your output.</td>
    </tr>
  </tbody>
</table>

----------------------------------------

TITLE: Structured Event Example - dbt Model Execution
DESCRIPTION: Example of a structured JSON event showing detailed information about a model execution including node info and metadata

LANGUAGE: json
CODE:
{
  "data": {
    "description": "sql view model dbt_jcohen.my_model",
    "index": 1,
    "node_info": {
      "materialized": "view",
      "meta": {
        "first": "some_value",
        "second": "1234"
      },
      "node_finished_at": "",
      "node_name": "my_model",
      "node_path": "my_model.sql",
      "node_relation": {
        "alias": "my_model",
        "database": "my_database",
        "relation_name": "\"my_database\".\"my_schema\".\"my_model\"",
        "schema": "my_schema"
      },
      "node_started_at": "2023-04-12T19:27:27.435364",
      "node_status": "started",
      "resource_type": "model",
      "unique_id": "model.my_dbt_project.my_model"
    },
    "total": 1
  },
  "info": {
    "category": "",
    "code": "Q011",
    "extra": {
      "my_custom_env_var": "my_custom_value"
    },
    "invocation_id": "206b4e61-8447-4af7-8035-b174ab3ac991",
    "level": "info",
    "msg": "1 of 1 START sql view model my_database.my_model ................................ [RUN]",
    "name": "LogStartLine",
    "pid": 95894,
    "thread": "Thread-1",
    "ts": "2023-04-12T19:27:27.436283Z"
  }
}

----------------------------------------

TITLE: Setting up Bastion Server User Account in Shell
DESCRIPTION: Shell commands for creating and configuring a user account on the bastion server for SSH tunnel access from dbt Cloud.

LANGUAGE: shell
CODE:
sudo groupadd dbtcloud
sudo useradd -m -g dbtcloud dbtcloud
sudo su - dbtcloud
mkdir ~/.ssh
chmod 700 ~/.ssh
touch ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys

----------------------------------------

TITLE: Testing Deduplication Results
DESCRIPTION: YAML configuration for testing the uniqueness and non-null constraints of the grain_id column.

LANGUAGE: yaml
CODE:
models:
  - name: my_model
    columns:
      - name: grain_id
        tests:
          - unique
          - not_null

----------------------------------------

TITLE: Doc Block Description Example
DESCRIPTION: Shows how to use doc blocks for longer descriptions with markdown formatting

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: fct_orders
    description: This table has basic information about orders, as well as some derived facts based on payments

    columns:
      - name: status
        description: '{{ doc("orders_status") }}'

----------------------------------------

TITLE: Using Refs and Sources in SQL Queries
DESCRIPTION: This SQL snippet demonstrates the proper use of refs and sources in dbt models. It shows how to reference other models and source tables instead of querying raw tables directly, which is a best practice in dbt projects.

LANGUAGE: sql
CODE:
select *
from {{ ref('stg_customers') }}
left join {{ source('raw', 'orders') }} using (customer_id)

----------------------------------------

TITLE: Creating Snowflake Staff Member Table Schema
DESCRIPTION: SQL definition for creating a transient table in Snowflake to store staff member information with various fields including personal details, administrative flags, and reference IDs.

LANGUAGE: sql
CODE:
create or replace TRANSIENT TABLE STAGING.BASE.STG_STAFF_MEMBER (
      ID NUMBER(38,0),
      CREATEDATETIME TIMESTAMP_NTZ(9),
      UPDATEDATETIME TIMESTAMP_NTZ(9),
      VERSION NUMBER(38,0),
      FIRSTNAME VARCHAR(16777216),
      JOBTITLE VARCHAR(16777216),
      LASTNAME VARCHAR(16777216),
      MIDDLENAME VARCHAR(16777216),
      ISCAREADMIN BOOLEAN,
      ISARCHIVED BOOLEAN,
      ADDRESSID VARCHAR(16777216),
      ENTERPRISEID VARCHAR(16777216),
      ISDELETED BOOLEAN
);

----------------------------------------

TITLE: Multi-Adapter Unit Tests
DESCRIPTION: Unit tests implementation supporting multiple database adapters with specific test cases for each

LANGUAGE: sql
CODE:
{% macro test_to_literal() %}

    {{ return(adapter.dispatch('test_to_literal', 'integration_tests')(text)) }}

{% endmacro %}

{% macro default__test_to_literal() %}

    {% result = dbt_sample_package.to_literal('test string') %}

    {% if result != "'test string'" %}

        {{ exceptions.raise_compiler_error('The test is failed') }}

    {% endif %}

{% endmacro %}

{% macro postgres__test_to_literal() %}

    {% result = dbt_sample_package.to_literal('test string') %}

    {% if result != "E'test string'" %}

        {{ exceptions.raise_compiler_error('The test is failed') }}

    {% endif %}

{% endmacro%}

----------------------------------------

TITLE: dbt List Command Examples
DESCRIPTION: Examples of using the dbt ls command to preview and troubleshoot node selection syntax.

LANGUAGE: bash
CODE:
dbt ls --select "path/to/my/models" # Lists all models in a specific directory.
dbt ls --select "source_status:fresher+" # Shows sources updated since the last dbt source freshness run.
dbt ls --select state:modified+ # Displays nodes modified in comparison to a previous state.
dbt ls --select "result:<status>+" state:modified+ --state ./<dbt-artifact-path> # Lists nodes that match certain result statuses and are modified.

----------------------------------------

TITLE: SQL TRIM Function Example with Concatenation
DESCRIPTION: Illustrates the use of the TRIM function in combination with the CONCAT function. It adds asterisks to strings and then removes them using TRIM, demonstrating how TRIM handles patterns at both ends of a string.

LANGUAGE: sql
CODE:
select
    first_name,
    concat('*', first_name, '**') as test_string,
    trim(test_string, '*') as back_to_first_name
from {{ ref('customers') }}
limit 3

----------------------------------------

TITLE: Configuring Indirect Selection in Project YAML
DESCRIPTION: Example of setting indirect selection behavior in the dbt_project.yml configuration file using the flags block.

LANGUAGE: yaml
CODE:
flags:
  indirect_selection: cautious

----------------------------------------

TITLE: Adding a Model to a Group In-file
DESCRIPTION: Assigns a model to the 'finance' group directly in the SQL file using a config block. This method provides inline group configuration.

LANGUAGE: sql
CODE:
{{ config(group = 'finance') }}

select ...

----------------------------------------

TITLE: Defining a Simple Metric in YAML for dbt Semantic Layer
DESCRIPTION: This YAML snippet defines a simple metric for counting the total number of queries run in the Semantic Layer. It uses a measure called 'queries' defined elsewhere in the semantic model.

LANGUAGE: yaml
CODE:
metrics:
  - name: queries
    description: The total number of queries run
    type: simple
    label: Semantic Layer Queries
    type_params:
      measure: queries

----------------------------------------

TITLE: Defining and Populating Temp Table in SQL Server Stored Procedure
DESCRIPTION: This SQL Server code snippet demonstrates creating a temporary table, selecting data from a raw table, and inserting it into the temp table. It's part of a stored procedure approach to data transformation.

LANGUAGE: sql
CODE:
IF OBJECT_ID('tempdb..#temp_orders') IS NOT NULL DROP TABLE #temp_orders
   SELECT  messageid
           ,orderid
           ,sk_id
           ,client
   FROM    some_raw_table
   WHERE   . . .
   INTO   #temp_orders

----------------------------------------

TITLE: Defining Seed Paths in dbt_project.yml
DESCRIPTION: Specifies the directory paths where dbt should look for seed files. By default, dbt expects seeds to be in the 'seeds' directory.

LANGUAGE: yml
CODE:
seed-paths: [directorypath]

----------------------------------------

TITLE: Configuring Semantic Model with Cross-Project References in YAML
DESCRIPTION: This snippet demonstrates how to define a semantic model in YAML, using the two-argument 'ref' function to reference a model from another project. It shows the structure for specifying entities, dimensions, and measures within the semantic model.

LANGUAGE: yaml
CODE:
semantic_models:
  - name: customer_orders
    defaults:
      agg_time_dimension: first_ordered_at
    description: |
      Customer grain mart that aggregates customer orders.
    model: ref('jaffle_finance', 'fct_orders') # ref('project_name', 'model_name')
    entities:
      ...rest of configuration...
    dimensions:
      ...rest of configuration...
    measures:
      ...rest of configuration...

----------------------------------------

TITLE: SQL Query String Examples in dbt
DESCRIPTION: Different approaches to creating SQL query strings with Jinja, showing both correct and incorrect methods

LANGUAGE: sql
CODE:
{# Either of these work #}

{% set query_sql = 'select * from ' ~ ref('my_model') %}

{% set query_sql %}
select * from {{ ref('my_model') }}
{% endset %}

{# This does not #}
{% set query_sql = "select * from {{ ref('my_model')}}" %}

----------------------------------------

TITLE: Configuring fail_calc in Generic Test Definition
DESCRIPTION: Setting default fail_calc configuration within a generic test block definition.

LANGUAGE: sql
CODE:
{% test <testname>(model, column_name) %}

{{ config(fail_calc = "missing_in_a + missing_in_b") }}

select ...

{% endtest %}

----------------------------------------

TITLE: Including Data Tests in DBT Build
DESCRIPTION: Command to include all data tests in the DBT build process using the --resource-type flag.

LANGUAGE: text
CODE:
dbt build --resource-type test

----------------------------------------

TITLE: Querying Exposure Information with GraphQL in dbt
DESCRIPTION: This GraphQL query retrieves information about a specific exposure in a dbt job, including owner details, URL, and information about parent sources and models. It demonstrates the structure and available fields for querying exposure objects.

LANGUAGE: graphql
CODE:
{
  job(id: 123) {
    exposure(name: "my_awesome_exposure") {
      runId
      projectId
      name
      uniqueId
      resourceType
      ownerName
      url
      ownerEmail
      parentsSources {
        uniqueId
        sourceName
        name
        state
        maxLoadedAt
        criteria {
          warnAfter {
            period
            count
          }
          errorAfter {
            period
            count
          }
        }
        maxLoadedAtTimeAgoInS
      }
      parentsModels {
        uniqueId
      }
    }
  }
}

----------------------------------------

TITLE: Basic Mileage Aggregation Query
DESCRIPTION: SQL query to calculate total mileage accumulated for each bike by joining fact and dimension tables.

LANGUAGE: sql
CODE:
select
    dim_bikes.bike_id,
    sum(fct_daily_mileage.miles) as miles
from
    fct_daily_mileage
inner join
    dim_bikes
    on
        fct_daily_mileage.bike_sk = dim_bikes.bike_sk
group by
    1

----------------------------------------

TITLE: Running dbt Models with Tag-based Selection
DESCRIPTION: This example shows how to run dbt models by selecting models with specific tags and their related models (children or both parents and children).

LANGUAGE: bash
CODE:
dbt run --select "tag:nightly+"      # select "nightly" models and all children
dbt run --select "+tag:nightly+"      # select "nightly" models and all parents and children

----------------------------------------

TITLE: Multiline Model Description in YAML
DESCRIPTION: Demonstrates using YAML block notation for longer descriptions spanning multiple lines

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: dim_customers
    description: >
      One record per customer. Note that a customer must have made a purchase to
      be included in this <Term id="table" /> ‚Äî customer accounts that were created but never
      used have been filtered out.

    columns:
      - name: customer_id
        description: Primary key.

----------------------------------------

TITLE: Compiled SQL output for inline query
DESCRIPTION: This snippet demonstrates the compiled SQL output for an inline query that selects all columns from the raw_orders table.

LANGUAGE: sql
CODE:
select * from "jaffle_shop"."main"."raw_orders"

----------------------------------------

TITLE: Implementing GitHub Pull Request Template in Markdown
DESCRIPTION: Complete pull request template used by dbt Labs for standardizing PR submissions. Includes sections for project description, validation, changes, and a comprehensive checklist for quality control.

LANGUAGE: markdown
CODE:
<!---

Provide a short summary in the Title above. Examples of good PR titles:

* "Feature: add so-and-so models"

* "Fix: deduplicate such-and-such"

* "Update: dbt version 0.13.0"

-->

## Description & motivation

<!---
Describe your changes, and why you're making them. Is this linked to an open
issue, a Trello card, or another pull request? Link it here.
-->

## To-do before merge

<!---
(Optional -- remove this section if not needed)
Include any notes about things that need to happen before this PR is merged, e.g.:
- [ ] Change the base branch
- [ ] Update dbt Cloud jobs
- [ ] Ensure PR #56 is merged
-->

## Screenshots:

<!---
Include a screenshot of the relevant section of the updated DAG. You can access
your version of the DAG by running `dbt docs generate && dbt docs serve`.
-->

## Validation of models:

<!---
Include any output that confirms that the models do what is expected. This might
be a link to an in-development dashboard in your BI tool, or a query that
compares an existing model with a new one.
-->

## Changes to existing models:

<!---
Include this section if you are changing any existing models. Link any related
pull requests on your BI tool, or instructions for merge (e.g. whether old
models should be dropped after merge, or whether a full-refresh run is required)
-->

## Checklist:

<!---
This checklist is mostly useful as a reminder of small things that can easily be
forgotten ‚Äì it is meant as a helpful tool rather than hoops to jump through.
Put an `x` in all the items that apply, make notes next to any that haven't been
addressed, and remove any items that are not relevant to this PR.
-->

- [ ] My pull request represents one logical piece of work.
- [ ] My commits are related to the pull request and look clean.
- [ ] My SQL follows the style guide.
- [ ] I have materialized my models appropriately.
- [ ] I have added appropriate tests and documentation to any new models.
- [ ] I have updated the README file.

{%- if project.warehouse == 'redshift' %}
- [ ] I have added sort and dist keys to models materialized as tables.
- [ ] I have validated the SQL in any late-binding views.
{% endif %}

----------------------------------------

TITLE: Using COALESCE Function in SQL
DESCRIPTION: This snippet demonstrates the general syntax for using the COALESCE function in SQL. It takes multiple inputs and returns the first non-null value.

LANGUAGE: sql
CODE:
coalesce(<input_1>, <input_2>,...<input_n>)

----------------------------------------

TITLE: Basic RFM Segmentation Model in SQL
DESCRIPTION: Initial SQL query that calculates current RFM segments for users based on payment data. Includes CTEs for raw payments, RFM value calculation, percentile scoring, and final segment assignment.

LANGUAGE: sql
CODE:
WITH payments AS(
    SELECT *
    FROM ref {{'fact_payments'}}
),
rfm_values AS (
    SELECT  user_id,
            MAX(payment_date) AS max_payment_date,
            NOW() - MAX(payment_date) AS recency,
            COUNT(DISTINCT payment_id) AS frequency,
            SUM(payment_amount) AS monetary
    FROM payments
    GROUP BY user_id
)

----------------------------------------

TITLE: Current Segment Extraction Model in SQL
DESCRIPTION: Simple query to extract the most recent segment status from the historical model for operational use cases like reverse ETL.

LANGUAGE: sql
CODE:
WITH rfm_segments AS(
    SELECT *
    FROM ref {{'model_rfm_segments_hist'}}
),    
current_segments AS(
    SELECT *
    FROM rfm_segments
    WHERE date_month = (SELECT MAX(date_month) FROM rfm_segments)
)
SELECT *
FROM current_segments

----------------------------------------

TITLE: Basic State Modified Selection Commands
DESCRIPTION: Shell commands demonstrating basic state selection for running and testing modified resources in dbt.

LANGUAGE: shell
CODE:
dbt run -s "state:modified"
dbt test -s "state:modified"

----------------------------------------

TITLE: Listing Metrics with MetricFlow
DESCRIPTION: Command to list available metrics and their dimensions using MetricFlow in dbt Cloud and Core.

LANGUAGE: bash
CODE:
dbt sl list metrics <metric_name> # In dbt Cloud

mf list metrics <metric_name> # In dbt Core

----------------------------------------

TITLE: Adding More Dimensions to Semantic Models
DESCRIPTION: YAML configuration showing how to add additional dimensions like is_food_order to the orders semantic model.

LANGUAGE: yaml
CODE:
semantic_models:
  - name: orders
    description: |
      A model containing order data. The grain of the table is the order id.
    model: ref('orders')
    defaults:
      agg_time_dimension: metric_time
    entities:
      - name: order_id
        type: primary
      - name: customer
        type: foreign
        expr: customer_id
    measures:
      - name: order_total
        agg: sum
    dimensions:
      - name: metric_time
        expr: cast(ordered_at as date)
        type: time
        type_params:
          time_granularity: day
      - name: is_food_order
        type: categorical

----------------------------------------

TITLE: Basic Semantic Model Configuration in YAML
DESCRIPTION: Initial setup of a semantic model configuration for orders, including model name, defaults, and basic metadata.

LANGUAGE: yaml
CODE:
semantic_models:
  #The name of the semantic model.
  - name: orders
    defaults:
      agg_time_dimension: ordered_at
    description: |
      Order fact table. This table is at the order grain with one row per order. 
    #The name of the dbt model and schema
    model: ref('orders')

----------------------------------------

TITLE: Setting DBT Profile Names with Environment Targets
DESCRIPTION: Demonstrates the standard practice of using company name as the profile name with dev and prod target environments in dbt.

LANGUAGE: yaml
CODE:
dev
prod

----------------------------------------

TITLE: Using Log Function in SQL Macro for dbt
DESCRIPTION: This SQL macro demonstrates how to use the log function within a dbt macro. It logs a message that includes the values of two arguments passed to the macro.

LANGUAGE: sql
CODE:
{% macro some_macro(arg1, arg2) %}

	{{ log("Running some_macro: " ~ arg1 ~ ", " ~ arg2) }}

{% endmacro %}

----------------------------------------

TITLE: Running Tests on Tagged Columns in dbt
DESCRIPTION: This command selects and runs tests on columns tagged with 'my_column_tag' in dbt.

LANGUAGE: bash
CODE:
dbt test --select "tag:my_column_tag"

----------------------------------------

TITLE: Analyzing Customer Orders in BigQuery
DESCRIPTION: This SQL query joins customer and order data in BigQuery to analyze customer order history. It calculates the first and most recent order dates, as well as the total number of orders for each customer.

LANGUAGE: sql
CODE:
with customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from `dbt-tutorial`.jaffle_shop.customers

),

orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from `dbt-tutorial`.jaffle_shop.orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final

----------------------------------------

TITLE: GitHub Star Count Loading Script
DESCRIPTION: Python script snippet that fetches GitHub star counts via API and loads them into BigQuery

LANGUAGE: python
CODE:
dataset_ref = client.dataset('metrics')
table_ref = dataset_ref.table('github_stars')
table = client.get_table(table_ref)

now = int(time.time())

for project in projects:
  url = 'https://api.github.com/repos/' + project
  response = requests.get(url)
  watchers = response.json()['watchers']
  client.insert_rows(table, [(now,project,watchers)])

----------------------------------------

TITLE: dbt Requirements Configuration
DESCRIPTION: YAML configuration specifying the required dbt adapter package version for the pipeline.

LANGUAGE: yaml
CODE:
dbt-[adapter] ~= 1.0

----------------------------------------

TITLE: Configuring Snapshot with Timestamp Strategy
DESCRIPTION: Example of configuring a snapshot using the timestamp strategy. This strategy uses an updated_at field to determine if a row has changed.

LANGUAGE: sql
CODE:
{% snapshot orders_snapshot_timestamp %}

    {{
        config(
          target_schema='snapshots',
          strategy='timestamp',
          unique_key='id',
          updated_at='updated_at',
        )
    }}

    select * from {{ source('jaffle_shop', 'orders') }}

{% endsnapshot %}

----------------------------------------

TITLE: Enabling Detailed Logging for Cache Events in dbt
DESCRIPTION: This command demonstrates how to use the --log-cache-events flag with dbt to enable detailed logging for cache events during compilation. This is useful for debugging and understanding the caching behavior of dbt.

LANGUAGE: text
CODE:
dbt --log-cache-events compile

----------------------------------------

TITLE: Defining Entities in Semantic Model
DESCRIPTION: Configuration of entity definitions including primary and foreign keys used for joining semantic models.

LANGUAGE: yaml
CODE:
    entities:
      - name: order_id
        type: primary
      - name: location
        type: foreign
        expr: location_id
      - name: customer
        type: foreign
        expr: customer_id

----------------------------------------

TITLE: Setting Global Snapshot Database
DESCRIPTION: Example of setting a global target database named 'snapshots' for all snapshot resources.

LANGUAGE: yml
CODE:
snapshots:
  +target_database: snapshots

----------------------------------------

TITLE: Configuring File Log Colors in profiles.yml
DESCRIPTION: YAML configuration to disable colored output in file logs only.

LANGUAGE: yaml
CODE:
config:
  use_colors_file: False

----------------------------------------

TITLE: Querying Exposure Information in dbt using GraphQL
DESCRIPTION: This GraphQL query retrieves detailed information about all exposures in a given job, including owner details, URL, and parent sources and models. It demonstrates how to use the exposures object within a job query.

LANGUAGE: graphql
CODE:
{
  job(id: 123) {
    exposures(jobId: 123) {
      runId
      projectId
      name
      uniqueId
      resourceType
      ownerName
      url
      ownerEmail
      parentsSources {
        uniqueId
        sourceName
        name
        state
        maxLoadedAt
        criteria {
          warnAfter {
            period
            count
          }
          errorAfter {
            period
            count
          }
        }
        maxLoadedAtTimeAgoInS
      }
      parentsModels {
        uniqueId
      }
    }
  }
}

----------------------------------------

TITLE: Creating a Rolling Aggregation Macro in dbt SQL
DESCRIPTION: This SQL macro defines a reusable function for creating rolling window aggregations, which is useful for generating time-based features in dbt models.

LANGUAGE: sql
CODE:
{% macro rolling_agg(column, partition_by, order_by, interval='30 days', agg_function='sum') %}
   {{ agg_function }}({{ column }}) over (
       partition by {{ partition_by }}
       order by {{ order_by }}
       range between interval '{{ interval }}' preceding and current row
   )
{% endmacro %}

----------------------------------------

TITLE: Configuring dbt Semantic Models in YAML
DESCRIPTION: Illustrates how to configure dbt semantic models using the 'config' property in a YAML file. This includes settings for enabling/disabling the model, grouping, and metadata.

LANGUAGE: yaml
CODE:
version: 2

semantic_models:
  - name: <semantic_model_name>
    config:
      [enabled](/reference/resource-configs/enabled): true | false
      [group](/reference/resource-configs/group): <string>
      [meta](/reference/resource-configs/meta): {dictionary}

----------------------------------------

TITLE: Generating Surrogate Keys with Coalesce in BigQuery, Redshift, and Snowflake
DESCRIPTION: This SQL snippet demonstrates how to create a surrogate key by concatenating columns with coalesce to handle null values, then applying an MD5 hash function.

LANGUAGE: sql
CODE:
md5 ( concat ( coalesce(column1, '_this_used_to_be_null_'), coalesce(column2, '_this_used_to_be_null_') ) )

----------------------------------------

TITLE: Running Basic dbt Commands in Production
DESCRIPTION: The simplest version of production dbt commands, running models and then testing them. This assumes that slightly incorrect data can be tolerated as testing occurs after the run.

LANGUAGE: bash
CODE:
dbt run

dbt test

----------------------------------------

TITLE: Configuring Materialized Views for Development (Databricks)
DESCRIPTION: SQL configuration for materialized views in development environment for Databricks.

LANGUAGE: sql
CODE:
{{\nconfig(\n     materialized='materialized_view',\n)\n}}

----------------------------------------

TITLE: Selecting Models by Config in dbt
DESCRIPTION: Examples of using the config method to select models based on their configuration properties.

LANGUAGE: bash
CODE:
dbt run --select "config.materialized:incremental"    # run all models that are materialized incrementally
dbt run --select "config.schema:audit"              # run all models that are created in the `audit` schema
dbt run --select "config.cluster_by:geo_country"      # run all models clustered by `geo_country`

----------------------------------------

TITLE: Defining SCD Type II Dimensions in Semantic Model
DESCRIPTION: Example of defining SCD Type II dimensions in a dbt semantic model YAML file, including validity parameters and natural entity type.

LANGUAGE: yaml
CODE:
semantic_models:
  - name: sales_person_tiers
    description: SCD Type II table of tiers for salespeople 
    model: {{ ref('sales_person_tiers') }}
    defaults:
      agg_time_dimension: tier_start

    dimensions:
      - name: tier_start
        type: time
        label: "Start date of tier"
        expr: start_date
        type_params:
          time_granularity: day
          validity_params:
            is_start: True
      - name: tier_end 
        type: time
        label: "End date of tier"
        expr: end_date
        type_params:
          time_granularity: day
          validity_params:
            is_end: True
      - name: tier
        type: categorical

    primary_entity: sales_person

    entities:
      - name: sales_person
        type: natural 
        expr: sales_person_id

----------------------------------------

TITLE: Traditional SQL Table Creation and Data Insertion
DESCRIPTION: Shows a traditional SQL approach to creating a table with specified column types and inserting data, which is not the dbt way of doing things.

LANGUAGE: sql
CODE:
create table dbt_alice.my_table
  id integer,
  created timestamp;

insert into dbt_alice.my_table (
  select id, created from some_other_table
)

----------------------------------------

TITLE: Example dbt init Command Output
DESCRIPTION: This code snippet shows an example of the command-line output when running dbt init. It demonstrates the interactive prompts for user information and the confirmation message upon successful profile creation.

LANGUAGE: bash
CODE:
$ dbt init
Running with dbt=1.0.0
Setting up your profile.
user (yourname@jaffleshop.com): summerintern@jaffleshop.com
schema (usually dbt_<yourname>): dbt_summerintern
threads (your favorite number, 1-10) [8]: 6
Profile internal-snowflake written to /Users/intern/.dbt/profiles.yml using project's profile_template.yml and your supplied values. Run 'dbt debug' to validate the connection.

----------------------------------------

TITLE: Email Domain Extraction Macro in dbt
DESCRIPTION: dbt macro that uses regex to extract the domain portion of an email address and converts it to lowercase for consistency.

LANGUAGE: sql
CODE:
{% macro extract_email_domain(email) %}

{# This is the SQL to extract the email domain in the Snowflake Flavor of SQL #}

	regexp_substr(lower({{ email }}), '@(.*)', 1, 1, 'e',1)

{% endmacro %}

----------------------------------------

TITLE: Configuring Local Package with Relative Path in YAML for dbt
DESCRIPTION: This snippet demonstrates how to configure a local package using a relative path in the packages.yml file. However, this configuration is not supported by the dbt Cloud CLI and will only work in the dbt Cloud IDE.

LANGUAGE: yaml
CODE:
# repository_root/my_dbt_project_in_a_subdirectory/packages.yml

packages:
  - local: ../shared_macros

----------------------------------------

TITLE: Using Default Schema YAML Configuration in dbt
DESCRIPTION: Example showing the conventional schema.yml file naming pattern in dbt projects, which is the default terminology but may become difficult to manage as the project grows.

LANGUAGE: yaml
CODE:
schema.yml

----------------------------------------

TITLE: Non-Boolean Config Flag Usage in dbt CLI
DESCRIPTION: Shows the structure for using non-boolean configuration flags in dbt CLI commands, with placeholders for subcommand, config name, and setting value.

LANGUAGE: bash
CODE:
<SUBCOMMAND> --<THIS-CONFIG>=<SETTING>

----------------------------------------

TITLE: Querying Parent Models and Sources in GraphQL for dbt
DESCRIPTION: This GraphQL query demonstrates how to fetch information about a model's parent models and parent sources within a dbt job. It retrieves the runId, uniqueId, executionTime for parent models, and runId, uniqueId, state for parent sources.

LANGUAGE: graphql
CODE:
{
  job(id: 123) {
    model(uniqueId: "model.jaffle_shop.dim_user") {
      parentsModels {
        runId
        uniqueId
        executionTime
      }
      parentsSources {
        runId
        uniqueId
        state
      }
    }
  }
}

----------------------------------------

TITLE: New Data Tests Configuration Syntax
DESCRIPTION: Updated YAML syntax for configuring data tests in model definitions using the new data_tests key.

LANGUAGE: yaml
CODE:
models:
  - name: orders
    columns:
      - name: order_id
        data_tests:
          - unique
          - not_null

----------------------------------------

TITLE: Conditional Query Execution with run_query
DESCRIPTION: Pattern for handling query results conditionally and implementing fallback logic using run_query with length filter.

LANGUAGE: sql
CODE:
{% if execute %}
{% set results = run_query(payment_methods_query) %}
{% if results|length > 0 %}
  	-- do something with `results` here...
{% else %}
    -- do fallback here...
{% endif %}
{% endif %}

----------------------------------------

TITLE: Test Selection Syntax Examples in dbt
DESCRIPTION: These examples show various ways to select and run tests on specific models, directories, and based on different criteria in dbt.

LANGUAGE: bash
CODE:
# Run tests on a model (indirect selection)
dbt test --select "customers"

# Run tests on two or more specific models (indirect selection)
dbt test --select "customers orders"

# Run tests on all models in the models/staging/jaffle_shop directory (indirect selection)
dbt test --select "staging.jaffle_shop"

# Run tests downstream of a model (note this will select those tests directly!)
dbt test --select "stg_customers+"

# Run tests upstream of a model (indirect selection)
dbt test --select "+stg_customers"

# Run tests on all models with a particular tag (direct + indirect)
dbt test --select "tag:my_model_tag"

# Run tests on all models with a particular materialization (indirect selection)
dbt test --select "config.materialized:table"

----------------------------------------

TITLE: Setting Model Access Levels in dbt YAML Configuration
DESCRIPTION: This snippet demonstrates how to set access levels for models within a group, using 'protected' and 'private' access settings to control model visibility and usage across the project.

LANGUAGE: yaml
CODE:
models: 
  - name: fct_marketing_model
    group: marketing
    access: protected
  - name: stg_marketing_model
    group: marketing
    access: private

----------------------------------------

TITLE: Basic dbt Core CLI with BashOperator Example
DESCRIPTION: Example showing how to use Airflow's BashOperator to execute dbt commands

LANGUAGE: python
CODE:
dbt run --select `<manually-selected-failed-model>`

----------------------------------------

TITLE: Querying Tableau Metadata API for Workbook Dependencies
DESCRIPTION: GraphQL query to retrieve workbook metadata including names, IDs, and upstream table dependencies. Used for troubleshooting FQN matching between Tableau and dbt.

LANGUAGE: graphql
CODE:
query {
  workbooks {
    name
    uri
    id
    luid
    projectLuid
    projectName
    upstreamTables {
      id
      name
      schema
      database {
        name
        connectionType
    }
  }
}
}

----------------------------------------

TITLE: Generating Sequences Macro in dbt
DESCRIPTION: A dbt macro that generates database sequences for models marked with surrogate key configuration. Handles both full refresh and incremental scenarios.

LANGUAGE: yaml
CODE:
{% macro generate_sequences() %}

    {% if execute %}
      
    {% set models = graph.nodes.values() | selectattr('resource_type', 'eq', 'model') %}
    {% set sk_models = [] %}
    {% for model in models %}
        {% if model.config.meta.surrogate_key %}
          {% do sk_models.append(model) %}
        {% endif %}
    {% endfor %}

    {% endif %}

    {% for model in sk_models %}

        {% if flags.FULL_REFRESH or model.config.materialized == 'table' %}
        create or replace sequence {{ model.database }}.{{ model.schema }}.{{ model.name }}_seq;

        {% else %}
        create sequence if not exists {{ model.database }}.{{ model.schema }}.{{ model.name }}_seq;
        
        {% endif %}
    
    {% endfor %}
  
{% endmacro %}

----------------------------------------

TITLE: Configuring Dynamic Tables with Environment-Specific Settings
DESCRIPTION: SQL configuration for dynamic tables using environment-specific target lag settings.

LANGUAGE: sql
CODE:
{{\nconfig(\n    materialized = 'dynamic_table',\n    snowflake_warehouse = 'transforming',\n    target_lag = target_lag_environment(),\n    on_configuration_change = 'apply',\n)\n}}

----------------------------------------

TITLE: YAML Resource Configuration with Tests
DESCRIPTION: YAML configuration defining relationship tests between models for use with defer

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: model_b
    columns:
      - name: id
        tests:
          - relationships:
              to: ref('model_a')
              field: id

----------------------------------------

TITLE: Configuring dbt Saved Queries in YAML
DESCRIPTION: Demonstrates how to configure dbt saved queries using the 'config' property in a YAML file. This includes various settings such as caching, enabling/disabling, export options, grouping, metadata, and schema.

LANGUAGE: yaml
CODE:
version: 2

saved-queries:
  - name: <saved-query-name>
    config:
      [cache](/docs/build/saved-queries#parameters): 
        enabled: true | false
      [enabled](/reference/resource-configs/enabled): true | false
      [export_as](/docs/build/saved-queries#parameters): view | table 
      [group](/reference/resource-configs/group): <string>
      [meta](/reference/resource-configs/meta): {dictionary}
      [schema](/reference/resource-configs/schema): <string>

----------------------------------------

TITLE: Configuring Column Types in seeds/properties.yml
DESCRIPTION: Shows an alternative way to specify column types for a seed file using a separate properties.yml file in the seeds directory. It configures the same column types as the previous example.

LANGUAGE: yaml
CODE:
version: 2

seeds:
  - name: country_codes
    config:
      column_types:
        country_code: varchar(2)
        country_name: varchar(32)

----------------------------------------

TITLE: Table Replacement in BigQuery
DESCRIPTION: Illustrative example of how dbt might generate SQL to replace an existing table in BigQuery. Uses CREATE OR REPLACE TABLE statement.

LANGUAGE: sql
CODE:
-- Make an API call to create a dataset (no DDL interface for this)!!;

create or replace table `dbt-dev-87681`.`dbt_alice`.`test_model` as (
  select 1 as my_column
);

----------------------------------------

TITLE: Querying Test Information in dbt Jobs using GraphQL
DESCRIPTION: This GraphQL query retrieves information about all tests in a specific job. It demonstrates how to query various fields of the tests object, including runId, accountId, projectId, uniqueId, name, columnName, and state.

LANGUAGE: graphql
CODE:
{
  job(id: 123) {
    tests {
      runId
      accountId
      projectId
      uniqueId
      name
      columnName
      state
    }
  }
}

----------------------------------------

TITLE: Configuring Post-Hook in SQL Model
DESCRIPTION: Example of adding a post-hook configuration to a dbt model using a config block to execute ALTER TABLE statements after model creation.

LANGUAGE: sql
CODE:
{{ config(
    post_hook=[
      "alter table {{ this }} ..."
    ]
) }}

----------------------------------------

TITLE: Front Matter Configuration
DESCRIPTION: YAML configuration for the documentation page metadata including title, description, and sidebar information.

LANGUAGE: yaml
CODE:
---
title: How can I consolidate projects in dbt Cloud?
description: "Consolidating projects in dbt Cloud"
sidebar_label: 'How to consolidate projects'
id: consolidate-projects
---

----------------------------------------

TITLE: Displaying Unknown Error Message in dbt Cloud IDE
DESCRIPTION: This snippet shows the error message users might encounter when launching the dbt Cloud IDE due to a missing repository or other issues. The error indicates that the IDE session was terminated unexpectedly.

LANGUAGE: shell
CODE:
Your IDE session experienced an unknown error and was terminated. Please contact support.

----------------------------------------

TITLE: Configuring Incremental Model with Insert Overwrite Strategy in Spark SQL
DESCRIPTION: This snippet shows how to configure an incremental model using the 'insert_overwrite' strategy with partitioning in Spark SQL. It includes both the source code and the resulting run code.

LANGUAGE: sql
CODE:
{{ config(
    materialized='incremental',
    partition_by=['date_day'],
    file_format='parquet'
) }}

/*
  Every partition returned by this query will be overwritten
  when this model runs
*/

with new_events as (

    select * from {{ ref('events') }}

    {% if is_incremental() %}
    where date_day >= date_add(current_date, -1)
    {% endif %}

)

select
    date_day,
    count(*) as users

from events
group by 1

LANGUAGE: sql
CODE:
create temporary view spark_incremental__dbt_tmp as

    with new_events as (

        select * from analytics.events


        where date_day >= date_add(current_date, -1)


    )

    select
        date_day,
        count(*) as users

    from events
    group by 1

;

insert overwrite table analytics.spark_incremental
    partition (date_day)
    select `date_day`, `users` from spark_incremental__dbt_tmp

----------------------------------------

TITLE: dbt Build Command for Main Project
DESCRIPTION: Command to run dbt build while excluding models tagged from the smaller subset project.

LANGUAGE: shell
CODE:
dbt build --exclude tag:smaller_subset_project

----------------------------------------

TITLE: Custom Directory Configuration
DESCRIPTION: Shows how to use a custom directory name (transformations) instead of the default models directory.

LANGUAGE: yml
CODE:
model-paths: ["transformations"]

----------------------------------------

TITLE: Querying Models by Schema in GraphQL
DESCRIPTION: GraphQL query to find all models within a specific schema. Returns the uniqueId and executionTime for each model in the schema.

LANGUAGE: graphql
CODE:
{
  job(id: 123) {
    models(schema: "analytics") {
      uniqueId
      executionTime
    }
  }
}

----------------------------------------

TITLE: Defining top-level vars in dbt_project.yml
DESCRIPTION: Example of defining variables at the top-level of dbt_project.yml using the new config-version: 2. This improves variable scoping semantics.

LANGUAGE: yaml
CODE:
name: my_project
version: 1.0.0

config-version: 2

vars:
  my_var: 1
  another_var: true

models:
  ...

----------------------------------------

TITLE: Setting Indirect Selection via Environment Variable
DESCRIPTION: Example of setting indirect selection behavior using an environment variable before running dbt.

LANGUAGE: shell
CODE:
$ export DBT_INDIRECT_SELECTION=cautious
dbt run

----------------------------------------

TITLE: Selecting All Columns Except One Using dbt_utils.star() Macro
DESCRIPTION: This snippet demonstrates how to use the dbt_utils.star() macro to select all columns from a table except for a specific column. It shows how to reference the table and exclude a column using the 'except' argument.

LANGUAGE: sql
CODE:
select
	{{ dbt_utils.star(from=ref('table_a'), except=['column_56']) }}
from {{ ref('table_a') }}

----------------------------------------

TITLE: Comparing cURL vs dbt-cloud-cli Job Triggers
DESCRIPTION: Demonstrates the difference in complexity between triggering a dbt Cloud job using cURL versus dbt-cloud-cli.

LANGUAGE: bash
CODE:
curl -H "Authorization:Token $DBT_CLOUD_API_TOKEN" -H "Content-Type:application/json" -d '{"cause":"Triggered using cURL"}' https://cloud.getdbt.com/api/v2/accounts/$DBT_CLOUD_ACCOUNT_ID/jobs/43167/run/

LANGUAGE: bash
CODE:
dbt-cloud job run --job-id 43167

----------------------------------------

TITLE: Email Domain Extraction Model in dbt SQL
DESCRIPTION: SQL model that extracts email domains from user emails using a custom macro for standardization and downstream analysis.

LANGUAGE: sql
CODE:
    select
        id as user_id,
        name as user_name,
        email,

        {{ extract_email_domain('email') }} AS email_domain,

        gaggle_id,
        created_at

    from source

----------------------------------------

TITLE: Example Model SQL with References
DESCRIPTION: SQL model definition showing reference to upstream model that can be deferred

LANGUAGE: sql
CODE:
select

    id,
    count(*)

from {{ ref('model_a') }}
group by 1

----------------------------------------

TITLE: Configuring Warning Options in dbt Profiles YAML
DESCRIPTION: This YAML configuration in profiles.yml demonstrates how to set up warning error options, including specifying which warnings to treat as errors, which to warn about, and which to silence.

LANGUAGE: yaml
CODE:
config:
  warn_error_options:
    error: # Previously called "include"
    warn: # Previously called "exclude"
      - NoNodesForSelectionCriteria
    silence: # Silence or ignore warnings
      - NoNodesForSelectionCriteria

----------------------------------------

TITLE: Using Shorthand Syntax for Casting in Modern Data Warehouses
DESCRIPTION: This snippet shows an alternative syntax for casting column types using the '::' operator, which is supported in some modern data warehouses like Redshift, Snowflake, and Postgres. It casts order_id to an integer and order_price to a numeric type with specified precision.

LANGUAGE: sql
CODE:
select
    order_id::integer,
    order_price::numeric(6,2) -- you might find this in Redshift, Snowflake, and Postgres
from {{ ref('stg_orders') }}

----------------------------------------

TITLE: Configuring Test Failure Storage in YAML
DESCRIPTION: This YAML configuration enables storing test failures and specifies a custom schema suffix for the test audit views.

LANGUAGE: yaml
CODE:
tests:
  project_name:
    +store_failures: true
    +schema: test

----------------------------------------

TITLE: GitHub Action for running dbt Cloud job on merge
DESCRIPTION: GitHub Action configuration to run a dbt Cloud job when code is merged to the main branch. Requires setting environment variables for dbt Cloud API access.

LANGUAGE: yaml
CODE:
name: run dbt Cloud job on push

on:
  push:
    branches:
      - 'main'

jobs:

  run_dbt_cloud_job:
    name: Run dbt Cloud Job
    runs-on: ubuntu-latest

    env:
      DBT_ACCOUNT_ID: 00000 # enter your account id
      DBT_PROJECT_ID: 00000 # enter your project id
      DBT_PR_JOB_ID:  00000 # enter your job id
      DBT_API_KEY: ${{ secrets.DBT_API_KEY }}
      DBT_JOB_CAUSE: 'GitHub Pipeline CI Job' 
      DBT_JOB_BRANCH: ${{ github.ref_name }}

    steps:
      - uses: "actions/checkout@v4"
      - uses: "actions/setup-python@v5"
        with:
          python-version: "3.9"
      - name: Run dbt Cloud job
        run: "python python/run_and_monitor_dbt_job.py"

----------------------------------------

TITLE: Database Schema Grants Macro
DESCRIPTION: A macro utilizing the database_schemas variable to grant permissions across multiple databases and schemas.

LANGUAGE: jinja2
CODE:
{% macro grant_usage_to_schemas(database_schemas, user) %}
  {% for (database, schema) in database_schemas %}
    grant usage on {{ database }}.{{ schema }} to {{ user }};
  {% endfor %}
{% endmacro %}

----------------------------------------

TITLE: Finding Unique Columns using Grep
DESCRIPTION: Bash command to extract and sort unique column names from YAML documentation file.

LANGUAGE: bash
CODE:
grep '      \- name:' models/core/activity_based_interest/_activity_based_interest_docs.yml | cut -c 15- | sort -u

----------------------------------------

TITLE: Appending Custom Comment in dbt YAML
DESCRIPTION: Shows how to use the dictionary syntax to append a custom comment that includes dynamic content based on the target user.

LANGUAGE: yaml
CODE:
query-comment:
  comment: "run by {{ target.user }} in dbt"
  append: True

----------------------------------------

TITLE: Configuring Starrocks Model in SQL File
DESCRIPTION: This snippet shows how to configure a Starrocks model directly in the SQL file using a Jinja config block. It includes the same configuration options as the YAML examples.

LANGUAGE: jinja
CODE:
{{ config(
    materialized = 'table',
    keys=['id', 'name', 'some_date'],
    table_type='PRIMARY',
    distributed_by=['id'],
    buckets=3,
    partition_by=['some_date'],
    ....
) }}

----------------------------------------

TITLE: Querying Available Metrics with GraphQL
DESCRIPTION: Demonstrates how to fetch available metrics using the GraphQL API.

LANGUAGE: graphql
CODE:
metrics(environmentId: BigInt!): [Metric!]!

----------------------------------------

TITLE: Configuring Project Version in YAML
DESCRIPTION: Example of required config-version setting in dbt_project.yml after deprecation of version 1. Projects must use config-version: 2 to be compatible with dbt v0.19.0.

LANGUAGE: yaml
CODE:
config-version: 2

----------------------------------------

TITLE: Querying First and Last Order Dates with SQL MIN and MAX
DESCRIPTION: A SQL query demonstrating how to use MIN and MAX functions to find the first and last order dates for customers, grouped by customer_id. Uses dbt's jaffle_shop sample dataset with the orders table.

LANGUAGE: sql
CODE:
select
	customer_id,
	min(order_date) as first_order_date,
	max(order_date) as last_order_date
from {{ ref('orders') }}
group by 1
limit 3

----------------------------------------

TITLE: Basic dbt Show Model Preview
DESCRIPTION: Basic example of using dbt show to preview a model by selecting it directly.

LANGUAGE: bash
CODE:
dbt show --select "model_name.sql"

----------------------------------------

TITLE: Accessing Environment Variables in Jinja
DESCRIPTION: Example of accessing environment variables using the env_var Jinja function with optional default value.

LANGUAGE: jinja
CODE:
{{env_var('DBT_KEY','OPTIONAL_DEFAULT')}}

----------------------------------------

TITLE: Defining src_container Model in YAML
DESCRIPTION: This YAML file defines the src_container model, specifying the expected columns and their descriptions, along with some tests.

LANGUAGE: YAML
CODE:
---
version: 2

models:
  - name: src_container
    description: pass the OMG model variable to generate the data
    columns:
      - name: templateName
        description: STRING Specifies the templateName
        tests:
          - not_null
      - name: complete
        description: STRING Specifies the complete
      - name: aggregateID
        description: STRING Specifies the aggregateID
      - name: recipientID
        description: STRING Specifies the recipientID
      - name: templateID
        description: STRING Specifies the templateID
      - name: templateType
        description: STRING Specifies the templateType
      - name: state
        description: STRING Specifies the state
      - name: id
        description: STRING Specifies the id
      - name: orgID

----------------------------------------

TITLE: Disabling concurrent_batches in a dbt SQL model
DESCRIPTION: This example demonstrates how to disable concurrent_batches within a specific dbt model SQL file. It configures the model as incremental with microbatch strategy and enforces sequential batch execution.

LANGUAGE: sql
CODE:
{{
  config(
    materialized='incremental',
    incremental_strategy='microbatch'
    concurrent_batches=false
  )
}}
select ...

----------------------------------------

TITLE: Implementing Grain ID in Model
DESCRIPTION: SQL snippet showing how to call the build_key_from_columns macro to create a grain_id column in a model.

LANGUAGE: sql
CODE:
{{ build_key_from_columns(source('name', 'table_name')) }} as grain_id,

----------------------------------------

TITLE: Configuring Even Distribution Style in dbt
DESCRIPTION: Configuration for implementing 'even' distribution style in a dbt model, which distributes data equally across nodes in a round-robin fashion.

LANGUAGE: python
CODE:
{{ config(materialized='table', dist='even') }}

----------------------------------------

TITLE: Accessing Metrics in DBT Graph
DESCRIPTION: Example showing how to access and process metrics using a custom macro.

LANGUAGE: sql
CODE:
{% macro get_metric_sql_for(metric_name) %}

  {% set metrics = graph.metrics.values() %}
  
  {% set metric = (metrics | selectattr('name', 'equalto', metric_name) | list).pop() %}

  {% set metric_sql = get_metric_timeseries_sql(
      relation = metric['model'],
      type = metric['type'],
      expression = metric['sql']
  ) %}

  {{ return(metric_sql) }}

{% endmacro %}

----------------------------------------

TITLE: Using dbt ref and source functions in SQL models
DESCRIPTION: Examples of using the dbt ref() and source() functions to reference other models and source data in SQL queries.

LANGUAGE: SQL
CODE:
{{ ref('model_name') }}
{{ source('source_name', 'table_name') }}

----------------------------------------

TITLE: SQL Model for Data Processing
DESCRIPTION: Incremental SQL model that parses and structures the JSON response from the Carbon Intensity API

LANGUAGE: sql
CODE:
{{
    config(
        materialized='incremental',
        unique_key='dbt_invocation_id'
    )
}}

with raw as (
    select parse_json(carbon_intensity) as carbon_intensity_json
    from {{ ref('external_access_demo') }}
)

select
    '{{ invocation_id }}' as dbt_invocation_id,
    value:from::TIMESTAMP_NTZ as start_time,
    value:to::TIMESTAMP_NTZ as end_time,
    value:intensity.actual::NUMBER as actual_intensity,
    value:intensity.forecast::NUMBER as forecast_intensity,
    value:intensity.index::STRING as intensity_index
from raw,
    lateral flatten(input => raw.carbon_intensity_json:data)

----------------------------------------

TITLE: SQL SELECT vs DML Example
DESCRIPTION: Basic SQL keywords mentioned in the context of data transformations. The document emphasizes using SELECT statements over DDL/DML operations for better maintainability and portability.

LANGUAGE: sql
CODE:
select
cascade

----------------------------------------

TITLE: Installing dbt Semantic Layer SDK - Async Version
DESCRIPTION: Command to install the asynchronous version of the dbt Semantic Layer SDK using pip package manager.

LANGUAGE: bash
CODE:
pip install "dbt-sl-sdk[sync]"

----------------------------------------

TITLE: Connecting to JDBC API with URL String
DESCRIPTION: Example of a JDBC connection URL string for the dbt Semantic Layer API, including protocol, host, environment ID, and service token.

LANGUAGE: plaintext
CODE:
jdbc:arrow-flight-sql://semantic-layer.cloud.getdbt.com:443?&environmentId=202339&token=SERVICE_TOKEN

----------------------------------------

TITLE: SQL Column Quoting Example
DESCRIPTION: Example of SQL query using quoted column names, typically used when working with reserved words or preserving specific column casing.

LANGUAGE: sql
CODE:
select user_group as "group"

----------------------------------------

TITLE: Setting Model Access in YAML
DESCRIPTION: Shows how to set a model's access level in a YAML configuration file.

LANGUAGE: yaml
CODE:
models:
  - name: my_model
    access: public

----------------------------------------

TITLE: Requiring Specific dbt Version in YAML
DESCRIPTION: Shows how to require a specific dbt version, though this approach is not recommended for flexibility reasons.

LANGUAGE: yaml
CODE:
require-dbt-version: "1.5.0"

----------------------------------------

TITLE: Configuring Partitioned Tables in Apache Hive with dbt
DESCRIPTION: Demonstrates how to create a partitioned table in Hive using dbt's configuration options. The example shows table creation with city-based partitioning and includes sample data generation. Note that the partitioned column must be the last column in the SELECT statement.

LANGUAGE: sql
CODE:
{{
    config(
        materialized='table',
        unique_key='id',
        partition_by=['city'],
    )
}}

with source_data as (
     select 1 as id, "Name 1" as name, "City 1" as city,
     union all
     select 2 as id, "Name 2" as name, "City 2" as city,
     union all
     select 3 as id, "Name 3" as name, "City 2" as city,
     union all
     select 4 as id, "Name 4" as name, "City 1" as city,
)

select * from source_data

----------------------------------------

TITLE: Timestamp Strategy Example with Full Configuration
DESCRIPTION: Complete example of timestamp strategy implementation including schema configuration and unique key specification.

LANGUAGE: yaml
CODE:
snapshots:
  - name: orders_snapshot_timestamp
    relation: source('jaffle_shop', 'orders')
    config:
      schema: snapshots
      strategy: timestamp
      unique_key: id
      updated_at: updated_at

----------------------------------------

TITLE: Querying run steps data via API
DESCRIPTION: Example API request to retrieve run steps data for an individual run, using the Retrieve Run endpoint

LANGUAGE: http
CODE:
GET https://YOUR_ACCESS_URL/api/v2/accounts/{accountId}/runs/{runId}/?include_related=["run_steps"]

----------------------------------------

TITLE: Listing dbt Core and Adapter Versions for March 2025 Release
DESCRIPTION: This code block lists the specific versions of dbt Core, shared interfaces, and adapters included in the March 2025 Compatible release of dbt Cloud.

LANGUAGE: yaml
CODE:
dbt-core==1.9.3

# shared interfaces
dbt-adapters==1.14.1
dbt-common==1.15.0
dbt-semantic-interfaces==0.7.4

# adapters
dbt-athena==1.9.2
dbt-bigquery==1.9.1
dbt-databricks==1.9.7
dbt-fabric==1.9.2
dbt-postgres==1.9.0
dbt-redshift==1.9.1
dbt-snowflake==1.9.2
dbt-spark==1.9.2
dbt-synapse==1.8.2
dbt-teradata==1.9.1
dbt-trino==1.9.0

----------------------------------------

TITLE: Listing Semantic Models
DESCRIPTION: Example showing how to list all resources upstream of a specific semantic model.

LANGUAGE: bash
CODE:
dbt ls -s +semantic_model:orders

----------------------------------------

TITLE: Configuring Column Types in dbt_project.yml
DESCRIPTION: Example of how to override inferred data types for dbt seeds by providing explicit instructions in the dbt_project.yml file.

LANGUAGE: yaml
CODE:
seeds:
  jaffle_shop:
    customers:
      +column_types:
        id: varchar(16)
        email: varchar(64)
        created_at: timestamp

----------------------------------------

TITLE: Using LOWER Function in SQL
DESCRIPTION: Demonstrates the basic syntax for using the LOWER function in SQL to convert a string value to lowercase.

LANGUAGE: sql
CODE:
lower('<string_value>')

----------------------------------------

TITLE: Executing Unit Test Command
DESCRIPTION: Shell command to run the model and its associated unit tests.

LANGUAGE: shell
CODE:
dbt build --select hello_world

----------------------------------------

TITLE: Querying Payment Methods with Jinja in dbt SQL (Corrected)
DESCRIPTION: This snippet shows the correct way to query payment methods using the `execute` variable. It wraps the problematic Jinja in an `{% if execute %}` statement to avoid errors during the parse phase.

LANGUAGE: sql
CODE:
{% set payment_method_query %}
select distinct
payment_method
from {{ ref('raw_payments') }}
order by 1
{% endset %}

{% set results = run_query(payment_method_query) %}
{% if execute %}
{# Return the first column #}
{% set payment_methods = results.columns[0].values() %}
{% else %}
{% set payment_methods = [] %}
{% endif %}

----------------------------------------

TITLE: Configuring Event Time for Page Views Model in YAML
DESCRIPTION: This YAML configuration sets the event_time for the page_views model to page_view_start, which is essential for microbatch processing.

LANGUAGE: yaml
CODE:
models:
  - name: page_views
    config:
      event_time: page_view_start

----------------------------------------

TITLE: Creating and Testing Surrogate Key in dbt SQL Model
DESCRIPTION: Creates a surrogate key by concatenating country_code and order_id columns, which can then be tested for uniqueness.

LANGUAGE: sql
CODE:
select
  country_code || '-' || order_id as surrogate_key,
  ...

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: orders
    columns:
      - name: surrogate_key
        tests:
          - unique

----------------------------------------

TITLE: Configuring Source Quoting in dbt YAML
DESCRIPTION: This YAML configuration demonstrates how to use the 'quoting' property to force quoting of database, schema, and identifier values for a source in dbt. It includes an example for the jaffle_shop source with overrides for specific tables.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: jaffle_shop
    database: raw
    quoting:
      database: true
      schema: true
      identifier: true

    tables:
      - name: order_items
      - name: orders
        # This overrides the `jaffle_shop` quoting config
        quoting:
          identifier: false

----------------------------------------

TITLE: Handling Null Values with Coalesce in Surrogate Key Generation
DESCRIPTION: This SQL snippet demonstrates how to use coalesce and casting to handle null values when generating surrogate keys, ensuring that nulls are replaced with a default value.

LANGUAGE: sql
CODE:
select
  *,
  concat(
    coalesce(cast(user_id as string), '_this_used_to_be_null_'),
    coalesce(cast(product_id as string), '_this_used_to_be_null_')
    ) as _surrogate_key
from example_ids

----------------------------------------

TITLE: Setting Indirect Selection via CLI
DESCRIPTION: Example of using the --indirect-selection flag directly in the CLI command to run tests with cautious selection mode.

LANGUAGE: shell
CODE:
dbt test --indirect-selection cautious

----------------------------------------

TITLE: Defining Model Contracts in SingleStore
DESCRIPTION: Example of defining model contracts with column constraints in YAML configuration.

LANGUAGE: yaml
CODE:
models:
  - name: dim_customers
    config:
      materialized: table
      contract:
        enforced: true
    columns:
      - name: customer_id
        data_type: int
        constraints:
          - type: not_null
      - name: customer_name
        data_type: text

----------------------------------------

TITLE: Documenting Models in YAML files
DESCRIPTION: This snippet shows how to provide descriptions for models and their columns in YAML files, which can then be persisted using the persist_docs configuration.

LANGUAGE: yml
CODE:
version: 2

models:
  - name: dim_customers
    description: One record per customer
    columns:
      - name: customer_id
        description: Primary key

----------------------------------------

TITLE: Running dbt Archive Migration Script
DESCRIPTION: Command line instructions for migrating archive tables to snapshots using dbt's built-in migration tool.

LANGUAGE: bash
CODE:
$ dbt snapshot-migrate --from-archive

----------------------------------------

TITLE: Listing dbt Core and Adapter Versions for January 2025 Release
DESCRIPTION: This code block lists the specific versions of dbt Core, shared interfaces, and adapters included in the January 2025 Compatible release of dbt Cloud.

LANGUAGE: yaml
CODE:
dbt-core==1.9.1

# shared interfaces
dbt-adapters==1.13.1
dbt-common==1.14.0
dbt-semantic-interfaces==0.7.4

# adapters
dbt-athena==1.9.0
dbt-bigquery==1.9.1
dbt-databricks==1.9.1
dbt-fabric==1.9.0
dbt-postgres==1.9.0
dbt-redshift==1.9.0
dbt-snowflake==1.9.0
dbt-spark==1.9.0
dbt-synapse==1.8.2
dbt-teradata==1.9.0
dbt-trino==1.9.0

----------------------------------------

TITLE: Adding Metadata to Sources in dbt_project.yml
DESCRIPTION: Example of adding metadata to a source using the meta field in the dbt_project.yml file. This can be used for tracking additional context, documentation, or logging.

LANGUAGE: yaml
CODE:
sources:
  events:
    clickstream:
      +meta:
        source_system: "Google analytics"
        data_owner: "marketing_team"

----------------------------------------

TITLE: Installing dbt Packages via Command Line
DESCRIPTION: Examples of using dbt deps command to install packages from different sources including Hub packages and Git repositories.

LANGUAGE: shell
CODE:
dbt deps --upgrade

LANGUAGE: shell
CODE:
dbt deps --add-package dbt-labs/dbt_utils@1.0.0

LANGUAGE: shell
CODE:
dbt deps --add-package dbt-labs/snowplow@">=0.7.0,<0.8.0"

LANGUAGE: shell
CODE:
dbt deps --add-package https://github.com/fivetran/dbt_amplitude@v0.3.0 --source git

LANGUAGE: shell
CODE:
dbt deps --add-package /opt/dbt/redshift --source local

----------------------------------------

TITLE: Running Models Downstream of Source in dbt
DESCRIPTION: Command to run all models downstream of a specific source using the source selector syntax. The + symbol indicates all downstream dependencies should be included.

LANGUAGE: shell
CODE:
$ dbt run --select source:jaffle_shop+

----------------------------------------

TITLE: Enhanced dbt YAML with Relationships
DESCRIPTION: Advanced YAML configuration including relationship tests and categorical value validations for the staff member table.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: STAFF_MEMBER
    description: This table contains information about the staff members.
    columns:
      - name: ID
        description: The unique identifier for the staff member.
        tests:
          - unique
          - not_null
      - name: CREATEDATETIME
        description: The timestamp when the record was created.
        tests:
          - not_null
      - name: UPDATEDATETIME
        description: The timestamp when the record was last updated.
      - name: VERSION
        description: Version number of the record.
      - name: FIRSTNAME
        description: The first name of the staff member.
        tests:
          - not_null
      - name: JOBTITLE
        description: The job title of the staff member. This is a categorical field.
        tests:
          - not_null
          - accepted_values:
              values: ['Job Title 1', 'Job Title 2', 'Job Title 3']
      - name: LASTNAME
        description: The last name of the staff member.
        tests:
          - not_null
      - name: MIDDLENAME
        description: The middle name of the staff member.
      - name: ISCARADMIN
        description: Boolean value indicating if the staff member is a care administrator.
        tests:
          - accepted_values:
              values: ['true', 'false']
      - name: ISARCHIVED
        description: Boolean value indicating if the staff member record is archived.
        tests:
          - accepted_values:
              values: ['true', 'false']
      - name: COMMUNITYID
        description: Identifier for the community of the staff member.
        tests:
          - relationships:
              to: STAGING.COMMUNITY.ID
              field: ID
      - name: ENTERPRISEID
        description: Identifier for the enterprise of the staff member.
        tests:
          - relationships:
              to: STAGING.ENTERPRISE.ID
              field: ID
      - name: ISDELETED
        description: Boolean value indicating if the staff member record is deleted.
        tests:
          - accepted_values:
              values: ['true', 'false']

----------------------------------------

TITLE: Installing Homebrew on Mac and Linux
DESCRIPTION: Command to install Homebrew package manager on Mac and Linux systems. This is a prerequisite for running the docs site locally.

LANGUAGE: bash
CODE:
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

----------------------------------------

TITLE: Implementing Semantic Layer Metadata Retrieval
DESCRIPTION: Python UDF to retrieve metadata from dbt Cloud Semantic Layer including metrics and dimensions definitions.

LANGUAGE: sql
CODE:
create or replace function retrieve_sl_metadata()
    returns object
    language python
    runtime_version = 3.9
    handler = 'main'
    external_access_integrations = (dbt_cloud_semantic_layer_integration)
    packages = ('requests')
    secrets = ('cred' = dbt_cloud_service_token)
as
$$
from typing import Dict
import _snowflake
import requests

query = """
query GetMetrics($environmentId: BigInt!) {
  metrics(environmentId: $environmentId) {
    description
    name
    queryableGranularities
    type
    dimensions {
      description
      name
      type
    }
  }
}
"""

def main():
    session = requests.Session()
    token = _snowflake.get_generic_secret_string('cred')
    session.headers = {'Authorization': f'Bearer {token}'}
    payload = {"query": query, "variables": {"environmentId": 1}}
    response = session.post("https://semantic-layer.cloud.getdbt.com/api/graphql", json=payload)
    response.raise_for_status()
    return response.json()
$$;

----------------------------------------

TITLE: Creating Schema in SQL using dbt adapter
DESCRIPTION: This snippet shows how to use the `adapter.create_schema` method to create a new schema in the target database.

LANGUAGE: sql
CODE:
{% do adapter.create_schema(api.Relation.create(database=target.database, schema="my_schema")) %}

----------------------------------------

TITLE: Creating Ratio Metric for Food Revenue Percentage
DESCRIPTION: Configures a ratio metric to calculate the percentage of total revenue that comes from food items by comparing food_revenue to total revenue.

LANGUAGE: yaml
CODE:
- name: food_revenue_pct
  description: The % of order revenue from food.
  label: Food Revenue %
  type: ratio
  type_params:
    numerator: food_revenue
    denominator: revenue

----------------------------------------

TITLE: Setting Up Roles and Warehouse Permissions
DESCRIPTION: Creates and assigns permissions for loader, transformer, and reporter roles using securityadmin role.

LANGUAGE: sql
CODE:
use role securityadmin;

create role loader;
grant all on warehouse loading to role loader; 

create role transformer;
grant all on warehouse transforming to role transformer;

create role reporter;
grant all on warehouse reporting to role reporter;

----------------------------------------

TITLE: Updating dbt_utils Package Version
DESCRIPTION: Example of the packages.yml configuration showing how to specify the dbt_utils package. Replace 'xxx' with the latest version from dbt hub.

LANGUAGE: shell
CODE:
packages:
- package: dbt-labs/dbt_utils

version: xxx

----------------------------------------

TITLE: Non-additive Dimensions Configuration
DESCRIPTION: Example of configuring non-additive dimensions for measures that cannot be aggregated over certain dimensions like time periods.

LANGUAGE: yaml
CODE:
semantic_models:
  - name: subscriptions
    description: A subscription table with one row per date for each active user and their subscription plans.
    measures: 
      - name: count_users
        description: Count of users at the end of the month 
        expr: user_id
        agg: count_distinct
        non_additive_dimension: 
          name: subscription_date
          window_choice: max

----------------------------------------

TITLE: Implementing Print Function in dbt Macro
DESCRIPTION: Example of using the print() function within a dbt macro to log information about macro execution. The function outputs messages to both the log file and stdout, and remains visible even when QUIET config is enabled.

LANGUAGE: sql
CODE:
  {% macro some_macro(arg1, arg2) %}
    {{ print("Running some_macro: " ~ arg1 ~ ", " ~ arg2) }}
  {% endmacro %}

----------------------------------------

TITLE: Activated Accounts SQL Implementation
DESCRIPTION: SQL example showing how to calculate activated accounts by counting accounts with more than five data model runs.

LANGUAGE: sql
CODE:
with data_models_per_user as (
    select
        account_id as account,
        count(model_runs) as data_model_runs
    from 
        {{ ref('fct_model_runs') }}
    group by 
        account_id
),

activated_accounts as (
    select
        count(distinct account_id) as activated_accounts
    from 
        {{ ref('dim_accounts') }}
    left join 
        data_models_per_user 
    on 
        {{ ref('dim_accounts') }}.account_id = data_models_per_user.account
    where 
        data_models_per_user.data_model_runs > 5
)

select
    *
from 
    activated_accounts

----------------------------------------

TITLE: DBT Deprecation Warning Output Example
DESCRIPTION: Example of the warning message displayed when referencing a model scheduled for future deprecation using the dbt parse command.

LANGUAGE: text
CODE:
$ dbt parse
15:48:14  Running with dbt=1.6.0
15:48:14  Registered adapter: postgres=1.6.0
15:48:14  [WARNING]: While compiling 'my_model_ref': Found a reference to my_model, which is slated for deprecation on '2038-01-19T03:14:07-00:00'.

----------------------------------------

TITLE: Defining Git Terminology in Markdown Table
DESCRIPTION: A markdown table defining common Git terms used in dbt development, including repository, branch, checkout, commit, main, merge, pull request, push, and remote.

LANGUAGE: markdown
CODE:
| Name | Definition |
| --- | --- |
| Repository or repo | A repository is a directory that stores all the files, folders, and content needed for your project. You can think of this as an object database of the project, storing everything from the files themselves to the versions of those files, commits, and deletions. Repositories are not limited by user and can be shared and copied. |
| Branch | A branch is a parallel version of a repository. It is contained within the repository but does not affect the primary or main branch allowing you to work freely without disrupting the live version. When you've made the changes you want to make, you can merge your branch back into the main branch to publish your changes |
| Checkout | The `checkout` command is used to create a new branch, change your current working branch to a different branch, or switch to a different version of a file from a different branch. |
| Commit | A commit is a user's change to a file (or set of files). When you make a commit to save your work, Git creates a unique ID that allows you to keep a record of the specific changes committed along with who made them and when. Commits usually contain a commit message which is a brief description of what changes were made. |
| main | The primary, base branch of all repositories. All committed and accepted changes should be on the main branch.<br /><br /> In the Cloud IDE, the main branch is protected. This means you can't directly edit, format, or lint files and execute dbt commands in your protected primary git branch. Since the dbt Cloud IDE prevents commits to the protected branch, you can commit those changes to a new branch.|
| Merge | Merge takes the changes from one branch and adds them into another (usually main) branch. These commits are usually first requested via pull request before being merged by a maintainer. |
| Pull Request | If someone has changed code on a separate branch of a project and wants it to be reviewed to add to the main branch, they can submit a pull request. Pull requests ask the repo maintainers to review the commits made, and then, if acceptable, merge the changes upstream. A pull happens when adding the changes to the main branch. |
| Push | A `push` updates a remote branch with the commits made to the current branch. You are literally _pushing_ your changes into the remote. |
| Remote | This is the version of a repository or branch that is hosted on a server. Remote versions can be connected to local clones so that changes can be synced. |

----------------------------------------

TITLE: Defining Git Terminology in Markdown Table
DESCRIPTION: A markdown table defining common Git terms used in dbt development, including repository, branch, checkout, commit, main, merge, pull request, push, and remote.

LANGUAGE: markdown
CODE:
| Name | Definition |
| --- | --- |
| Repository or repo | A repository is a directory that stores all the files, folders, and content needed for your project. You can think of this as an object database of the project, storing everything from the files themselves to the versions of those files, commits, and deletions. Repositories are not limited by user and can be shared and copied. |
| Branch | A branch is a parallel version of a repository. It is contained within the repository but does not affect the primary or main branch allowing you to work freely without disrupting the live version. When you've made the changes you want to make, you can merge your branch back into the main branch to publish your changes |
| Checkout | The `checkout` command is used to create a new branch, change your current working branch to a different branch, or switch to a different version of a file from a different branch. |
| Commit | A commit is a user's change to a file (or set of files). When you make a commit to save your work, Git creates a unique ID that allows you to keep a record of the specific changes committed along with who made them and when. Commits usually contain a commit message which is a brief description of what changes were made. |
| main | The primary, base branch of all repositories. All committed and accepted changes should be on the main branch.<br /><br /> In the Cloud IDE, the main branch is protected. This means you can't directly edit, format, or lint files and execute dbt commands in your protected primary git branch. Since the dbt Cloud IDE prevents commits to the protected branch, you can commit those changes to a new branch.|
| Merge | Merge takes the changes from one branch and adds them into another (usually main) branch. These commits are usually first requested via pull request before being merged by a maintainer. |
| Pull Request | If someone has changed code on a separate branch of a project and wants it to be reviewed to add to the main branch, they can submit a pull request. Pull requests ask the repo maintainers to review the commits made, and then, if acceptable, merge the changes upstream. A pull happens when adding the changes to the main branch. |
| Push | A `push` updates a remote branch with the commits made to the current branch. You are literally _pushing_ your changes into the remote. |
| Remote | This is the version of a repository or branch that is hosted on a server. Remote versions can be connected to local clones so that changes can be synced. |

----------------------------------------

TITLE: Referencing Special Properties in dbt Python Configuration
DESCRIPTION: This snippet shows the Python syntax for config() blocks in dbt, which cannot be used to configure special properties. It serves as an example of where special properties cannot be set.

LANGUAGE: python
CODE:
config()

----------------------------------------

TITLE: Partitioning Configuration in Oracle dbt Model
DESCRIPTION: Demonstrates how to configure table partitioning along with other options like parallel execution and compression in an incremental model.

LANGUAGE: sql
CODE:
{
    config(
        materialized='incremental',
        unique_key='group_id',
        parallel=4,
        partition_config={"clause": "PARTITION BY HASH(PROD_NAME) PARTITIONS 4"},
        table_compression_clause='COLUMN STORE COMPRESS FOR QUERY LOW')
}}
SELECT *
FROM {{ source('sh_database', 'sales') }}

----------------------------------------

TITLE: Identifying Affected Snapshot Records - SQL Query
DESCRIPTION: SQL query to identify 'stuck' records in snapshot tables where the check strategy failed. Looks for records that are the most recent for a given unique_key but have both dbt_valid_from and dbt_valid_to values set.

LANGUAGE: sql
CODE:
with base as (

    select *,

        -- Replace `<your unique key>` with your specified unique_key 
        <your unique key> as dbt_unique_key

    -- Replace <your snapshot table> with a snapshot table name
    from <your snapshot table>

),

ranked as (

    select *,
        row_number() over (
            partition by dbt_unique_key
            order by dbt_valid_from desc
        ) as change_idx

    from base

),

to_migrate as (

    select *
    from ranked
    where change_idx = 1
    and dbt_valid_to is not null

)

select * from to_migrate
limit 100;

----------------------------------------

TITLE: Querying Metric Values with Parameters
DESCRIPTION: Example of querying metric values with various parameters such as metrics, group_by, grain, where, limit, and order_by.

LANGUAGE: sql
CODE:
select * from {{
semantic_layer.query(metrics=['food_order_amount', 'order_gross_profit'],
group_by=[Dimension('metric_time').grain('month'),'customer__customer_type'],
where="{{ Dimension('metric_time').grain('month')  }} >= '2017-03-09' AND {{ Dimension('customer__customer_type' }} in ('new') AND {{ Entity('order_id') }} = 10")
}}

----------------------------------------

TITLE: Validating dbt Cloud Webhook and Refreshing Mode Report
DESCRIPTION: This Python script validates the authenticity of a dbt Cloud webhook, checks if the job run was successful, and then triggers a refresh of a specified Mode report using the Mode API.

LANGUAGE: python
CODE:
import hashlib
import hmac
import json

#replace with the report token you want to run
account_username = 'YOUR_MODE_ACCOUNT_USERNAME_HERE'
report_token = 'YOUR_REPORT_TOKEN_HERE'

auth_header = input_data['auth_header']
raw_body = input_data['raw_body']

# Access secret credentials
secret_store = StoreClient('YOUR_SECRET_HERE')
hook_secret = secret_store.get('DBT_WEBHOOK_KEY')
username = secret_store.get('MODE_API_TOKEN')
password = secret_store.get('MODE_API_SECRET')

# Validate the webhook came from dbt Cloud
signature = hmac.new(hook_secret.encode('utf-8'), raw_body.encode('utf-8'), hashlib.sha256).hexdigest()

if signature != auth_header:
  raise Exception("Calculated signature doesn't match contents of the Authorization header. This webhook may not have been sent from dbt Cloud.")

full_body = json.loads(raw_body)
hook_data = full_body['data'] 

if hook_data['runStatus'] == "Success":

  # Create a report run with the Mode API
  url = f'https://app.mode.com/api/{account_username}/reports/{report_token}/run'

  params = {
    'parameters': {
      "user_id": 123, 
      "location": "San Francisco"
    } 
  }
  headers = {
    'Content-Type': 'application/json',
    'Accept': 'application/hal+json'
  }
  response = requests.post(
    url, 
    json=params, 
    headers=headers, 
    auth=HTTPBasicAuth(username, password)
  )
  response.raise_for_status()

return

----------------------------------------

TITLE: Configuring Production Environment Connection in YAML
DESCRIPTION: YAML configuration for production environment connection settings in dbt Cloud, using service account authentication on a separate account with production-specific settings.

LANGUAGE: yaml
CODE:
account: 456prod
project: analytics
dataset: main
method: service-account-json
threads: 16

----------------------------------------

TITLE: Describing Base Properties of dbt Resource Objects
DESCRIPTION: This snippet shows the common properties found in all resource objects within the nodes, sources, metrics, exposures, macros, and docs sections of the manifest.json file.

LANGUAGE: json
CODE:
{
  "name": "resource_name",
  "unique_id": "resource_type.package.resource_name",
  "package_name": "package_name",
  "root_path": "/absolute/path/to/package",
  "path": "relative/path/within/resource/path",
  "original_file_path": "relative/path/including/resource/path"
}

----------------------------------------

TITLE: Implementing DATEADD in BigQuery
DESCRIPTION: BigQuery date_add function implementation. Note that dateparts less than a day are not supported.

LANGUAGE: sql
CODE:
date_add( {{ from_date }}, INTERVAL {{ interval }} {{ datepart }} )

----------------------------------------

TITLE: Alternative dbt Config Block Syntax
DESCRIPTION: Correct way to specify configurations with special characters using the dictionary-style config block syntax in dbt models.

LANGUAGE: sql
CODE:
{{
  config({
    "post-hook": "grant select on {{ this }} to role reporter",
    "materialized": "table"
  })
}}


select ...

----------------------------------------

TITLE: Installing Python and Dependencies for dbt Core on CentOS
DESCRIPTION: Installs required packages including Python, gcc, and development libraries for CentOS to support dbt Core installation.

LANGUAGE: shell
CODE:
sudo yum install redhat-rpm-config gcc libffi-devel \
  python-devel openssl-devel

----------------------------------------

TITLE: Defining Booking Change Metric with 7-Day Offset in YAML
DESCRIPTION: This example demonstrates how to create a derived metric that calculates the difference in bookings between the current date and 7 days ago, which can be queried at various granularities.

LANGUAGE: yaml
CODE:
- name: d7_booking_change
  description: Difference between bookings now and 7 days ago
  type: derived
  label: d7 bookings change
  type_params:
    expr: bookings - bookings_7_days_ago
    metrics:
      - name: bookings
        alias: current_bookings
      - name: bookings
        offset_window: 7 days
        alias: bookings_7_days_ago

----------------------------------------

TITLE: BigQuery Sharded Tables Configuration in dbt
DESCRIPTION: Example demonstrating how to configure sharded tables in BigQuery using wildcards in the identifier, including filtering by table suffix.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: ga
    tables:
      - name: events
        identifier: "events_*"

LANGUAGE: sql
CODE:
select * from {{ source('ga', 'events') }}

-- filter on shards by suffix
where _table_suffix > '20200101'

LANGUAGE: sql
CODE:
select * from `my_project`.`ga`.`events_*`

-- filter on shards by suffix
where _table_suffix > '20200101'

----------------------------------------

TITLE: Configuring concurrent_batches in a dbt SQL model
DESCRIPTION: This example demonstrates how to set concurrent_batches within a specific dbt model SQL file. It configures the model as incremental with microbatch strategy and enables concurrent batch execution.

LANGUAGE: sql
CODE:
{{
  config(
    materialized='incremental',
    concurrent_batches=true,
    incremental_strategy='microbatch'
        ...
  )
}}
select ...

----------------------------------------

TITLE: Configuring Kerberos Authentication for Starburst/Trino in dbt
DESCRIPTION: Example YAML configuration for setting up a Kerberos-authenticated connection to a Starburst/Trino cluster in dbt's profiles.yml file. Includes essential connection parameters and Kerberos-specific settings.

LANGUAGE: yaml
CODE:
trino:
  target: dev
  outputs:
    dev:
      type: trino
      method: kerberos
      user: commander
      keytab: /tmp/trino.keytab
      krb5_config: /tmp/krb5.conf
      principal: trino@EXAMPLE.COM
      host: trino.example.com
      port: 443
      database: analytics
      schema: public

----------------------------------------

TITLE: Configuring Kerberos Authentication for Starburst/Trino in dbt
DESCRIPTION: Example YAML configuration for setting up a Kerberos-authenticated connection to a Starburst/Trino cluster in dbt's profiles.yml file. Includes essential connection parameters and Kerberos-specific settings.

LANGUAGE: yaml
CODE:
trino:
  target: dev
  outputs:
    dev:
      type: trino
      method: kerberos
      user: commander
      keytab: /tmp/trino.keytab
      krb5_config: /tmp/krb5.conf
      principal: trino@EXAMPLE.COM
      host: trino.example.com
      port: 443
      database: analytics
      schema: public

----------------------------------------

TITLE: Constructing dbt Cloud Enterprise Login URL
DESCRIPTION: This snippet demonstrates the format of the URL that users should navigate to for logging into dbt Cloud. It requires replacing placeholder values with the specific login slug and access URL for the user's region and plan.

LANGUAGE: plaintext
CODE:
https://YOUR_ACCESS_URL/enterprise-login/LOGIN-SLUG

----------------------------------------

TITLE: Accessing CLI Arguments in dbt SQL Model
DESCRIPTION: Example demonstrating how to access invocation_args_dict and dbt_metadata_envs variables to view CLI arguments and environment variables in a model.

LANGUAGE: sql
CODE:
-- invocation_args_dict:
-- {{ invocation_args_dict }}

-- dbt_metadata_envs:
-- {{ dbt_metadata_envs }}

select 1 as id

----------------------------------------

TITLE: Setting Default Severity for Generic Data Tests
DESCRIPTION: This snippet shows how to set the default severity for all instances of a generic data test using a config block in the test's SQL definition.

LANGUAGE: sql
CODE:
{% test my_test() %}

    {{ config(severity = 'warn') }}

    select ...

{% endtest %}

----------------------------------------

TITLE: Renaming RPC Tasks in dbt Server
DESCRIPTION: The 'compile' and 'execute' RPC tasks have been renamed to 'compile_sql' and 'execute_sql' respectively. Users should update their RPC calls accordingly.

LANGUAGE: yaml
CODE:
# Old RPC call:
# - task: compile
# - task: execute

# New RPC call:
- task: compile_sql
- task: execute_sql

----------------------------------------

TITLE: Disabling Telemetry Example
DESCRIPTION: Code showing how to disable telemetry in the SDK.

LANGUAGE: python
CODE:
from dbtsl.env import PLATFORM
PLATFORM.anonymous = True

----------------------------------------

TITLE: Configuring SQL Header for dbt Models
DESCRIPTION: Demonstrates how to set the sql_header configuration for a dbt model using both inline config and dbt_project.yml. This allows injecting SQL statements before the model creation.

LANGUAGE: sql
CODE:
{{ config(
  sql_header="<sql-statement>"
) }}

select ...

LANGUAGE: yml
CODE:
[config-version](/reference/project-configs/config-version): 2

models:
  [<resource-path>](/reference/resource-configs/resource-path):
    +sql_header: <sql-statement>

----------------------------------------

TITLE: Bitbucket Pipelines Linting Configuration
DESCRIPTION: Bitbucket Pipelines configuration for running SQLFluff linting on dbt models.

LANGUAGE: yaml
CODE:
image: python:3.11.1

pipelines:
  branches:
    '**':
      - step:
          name: Lint dbt project
          script:
            - python -m pip install sqlfluff==0.13.1
            - sqlfluff lint models --dialect snowflake --rules L019,L020,L021,L022

    'main':
      - step:
          script:
            - python --version

----------------------------------------

TITLE: Executing Preferred dbt Production Commands
DESCRIPTION: A more robust set of dbt production commands. It includes testing source data before transformations, running models, testing excluding sources, and checking source freshness. This approach protects data integrity and allows for staleness detection.

LANGUAGE: bash
CODE:
dbt test -s source:* (or dbt test -m source:* if you are on a version earlier than dbt v0.21.0)

dbt run

dbt test --exclude source:*

dbt source `freshness ` (or dbt source snapshot-freshness if you are on a version earlier to dbt v0.21.0) (this could optionally be the first step)

----------------------------------------

TITLE: Retrieving Snowflake PrivateLink Configuration
DESCRIPTION: SQL command to retrieve the PrivateLink configuration from Snowflake using the SYSTEM$GET_PRIVATELINK_CONFIG function. This information is required for setting up the PrivateLink connection.

LANGUAGE: sql
CODE:
USE ROLE ACCOUNTADMIN;
SYSTEM$GET_PRIVATELINK_CONFIG;

----------------------------------------

TITLE: Implementing DATEADD in Postgres
DESCRIPTION: Custom Postgres implementation of date addition using intervals since it doesn't provide a built-in dateadd function.

LANGUAGE: sql
CODE:
{{ from_date }} + (interval '{{ interval }} {{ datepart }}')

----------------------------------------

TITLE: Querying Transformed Data in Dremio Cloud
DESCRIPTION: SQL query to analyze the average tip amount by vendor ID from the transformed data in Dremio Cloud.

LANGUAGE: sql
CODE:
SELECT vendor_id,
       AVG(tip_amount)
FROM dev.application."nyc_treips_with_weather"
GROUP BY vendor_id

----------------------------------------

TITLE: Configuring Model Enablement in DBT
DESCRIPTION: Examples of enabling/disabling dbt models through project configuration and in-model config blocks

LANGUAGE: yaml
CODE:
models:
  [<resource-path>]:
    +enabled: true | false

LANGUAGE: sql
CODE:
{{ config(
  enabled=true | false
) }}

select ...

----------------------------------------

TITLE: Setting Global Configurations in dbt_project.yml
DESCRIPTION: Demonstrates how to set global configurations using the flags dictionary in the dbt_project.yml file. This is the only place to opt out of behavior changes while legacy behavior is still supported.

LANGUAGE: yaml
CODE:
flags:
  <global_config>: <value>

----------------------------------------

TITLE: Configuring Time Spine Join for Derived Metrics in YAML
DESCRIPTION: Shows how to configure a leads metric with both fill_nulls_with and join_to_timespine parameters to ensure complete daily coverage and proper null handling in derived metrics.

LANGUAGE: yaml
CODE:
- name: leads
  type: simple
  type_params:
    measure:
      name: bookings
      fill_nulls_with: 0
      join_to_timespine: true

----------------------------------------

TITLE: Using a Custom Macro in a dbt Model
DESCRIPTION: This snippet demonstrates how to use a custom macro (cents_to_dollars) within a dbt SQL model.

LANGUAGE: sql
CODE:
select
  id as payment_id,
  {{ cents_to_dollars('amount') }} as amount_usd,
  ...
from app_data.payments

----------------------------------------

TITLE: Schema.yml V1 to V2 Migration - Unique Constraints Example
DESCRIPTION: Demonstrates how to convert v1 schema unique constraints to v2 syntax, showing both column-level and model-level unique tests.

LANGUAGE: yaml
CODE:
events:
  constraints:
    unique:
      - id
      - "concat(first_name, last_name)"

LANGUAGE: yaml
CODE:
models:
  - name: events
    tests:
      - unique: "concat(first_name, last_name)"

    columns:
      - name: id
        tests:
          - unique

----------------------------------------

TITLE: Writing Complex Multi-line Descriptions in dbt YAML
DESCRIPTION: This snippet shows how to use the '|' operator in YAML to create more complex descriptions for dbt models. Interior line breaks are maintained, allowing for more structured formatting, and Markdown can be used for enhanced presentation.

LANGUAGE: yaml
CODE:
  version: 2

  models:
  - name: customers
    description: |
      ### Lorem ipsum

      * dolor sit amet, consectetur adipisicing elit, sed do eiusmod
      * tempor incididunt ut labore et dolore magna aliqua.

----------------------------------------

TITLE: Configuring Kerberos Authentication for Impala in dbt
DESCRIPTION: Configuration for Kerberos authentication using GSSAPI, including service name and transport settings. Required for Impala instances configured with Kerberos authentication.

LANGUAGE: yaml
CODE:
your_profile_name:
  target: dev
  outputs:
    dev:
      type: impala
      host: [hostname]
      port: [port]
      auth_type: [GSSAPI]
      kerberos_service_name: [kerberos service name]
      use_http_transport: true
      use_ssl: true
      dbname: [db name]
      schema: [schema name]
      retries: [retries]

----------------------------------------

TITLE: Analyzing Model Run Times with Python
DESCRIPTION: Python script to query the Discovery API and visualize model execution time data using matplotlib

LANGUAGE: python
CODE:
# Import libraries
import os
import matplotlib.pyplot as plt
import pandas as pd
import requests

# Set API key
auth_token = *[SERVICE_TOKEN_HERE]*

# Query the API
def query_discovery_api(auth_token, gql_query, variables):
    response = requests.post('https://metadata.cloud.getdbt.com/graphql',
        headers={"authorization": "Bearer "+auth_token, "content-type": "application/json"},
        json={"query": gql_query, "variables": variables})
    data = response.json()['data']

    return data

# Get the latest run metadata for all models
models_latest_metadata = query_discovery_api(auth_token, query_one, variables_query_one)['environment']

# Convert to dataframe
models_df = pd.DataFrame([x['node'] for x in models_latest_metadata['applied']['models']['edges']])

# Unnest the executionInfo column
models_df = pd.concat([models_df.drop(['executionInfo'], axis=1), models_df['executionInfo'].apply(pd.Series)], axis=1)

# Sort the models by execution time
models_df_sorted = models_df.sort_values('executionTime', ascending=False)

print(models_df_sorted)

# Get the uniqueId of the longest running model
longest_running_model = models_df_sorted.iloc[0]['uniqueId']

# Define second query variables
variables_query_two = {
    "environmentId": *[ENVR_ID_HERE]*
    "lastRunCount": 10,
    "uniqueId": longest_running_model
}

# Get the historical run metadata for the longest running model
model_historical_metadata = query_discovery_api(auth_token, query_two, variables_query_two)['environment']['applied']['modelHistoricalRuns']

# Convert to dataframe
model_df = pd.DataFrame(model_historical_metadata)

# Filter dataframe to only successful runs
model_df = model_df[model_df['status'] == 'success']

# Convert the runGeneratedAt, executeStartedAt, and executeCompletedAt columns to datetime
model_df['runGeneratedAt'] = pd.to_datetime(model_df['runGeneratedAt'])
model_df['executeStartedAt'] = pd.to_datetime(model_df['executeStartedAt'])
model_df['executeCompletedAt'] = pd.to_datetime(model_df['executeCompletedAt'])

# Plot the runElapsedTime over time
plt.plot(model_df['runGeneratedAt'], model_df['runElapsedTime'])
plt.title('Run Elapsed Time')
plt.show()

# Plot the executionTime over time
plt.plot(model_df['executeStartedAt'], model_df['executionTime'])
plt.title(model_df['name'].iloc[0]+" Execution Time")
plt.show()

----------------------------------------

TITLE: Co-installed Components Query
DESCRIPTION: SQL query to find components installed at the same time as a specific component using window functions.

LANGUAGE: sql
CODE:
select distinct
    component_id
from
    mdim_components
qualify
    sum(iff(component_id = 'Tube-3', 1, 0)) over (partition by valid_from_at, valid_to_at) > 0

----------------------------------------

TITLE: Configuring dbt Metrics in YAML
DESCRIPTION: Demonstrates how to configure dbt metrics using the 'config' property in a YAML file. This includes settings for enabling/disabling the metric, grouping, and metadata.

LANGUAGE: yaml
CODE:
version: 2

metrics:
  - name: <metric_name>
    config:
      [enabled](/reference/resource-configs/enabled): true | false
      [group](/reference/resource-configs/group): <string>
      [meta](/reference/resource-configs/meta): {dictionary}

----------------------------------------

TITLE: Unambiguous resource configuration in dbt_project.yml
DESCRIPTION: Example of using the '+' syntax for unambiguous resource configuration in dbt_project.yml with config-version: 2.

LANGUAGE: yaml
CODE:
models:
  my_project:
    reporting:
      +partition_by:
        field: date_day
        data_type: timestamp

----------------------------------------

TITLE: GraphQL Pagination Example
DESCRIPTION: Example of implementing pagination in GraphQL queries using the PageInfo object.

LANGUAGE: graphql
CODE:
pageInfo {
  startCursor
  endCursor
  hasNextPage
}
totalCount # Total number of records across all pages

----------------------------------------

TITLE: Loading Data from S3 to Redshift
DESCRIPTION: SQL COPY commands to load data from S3 into Redshift tables

LANGUAGE: sql
CODE:
copy jaffle_shop.customers( id, first_name, last_name)
from 's3://dbt-data-lake-xxxx/jaffle_shop_customers.csv'
iam_role 'arn:aws:iam::XXXXXXXXXX:role/RoleName'
region 'us-east-1'
delimiter ','
ignoreheader 1
acceptinvchars;
   
copy jaffle_shop.orders(id, user_id, order_date, status)
from 's3://dbt-data-lake-xxxx/jaffle_shop_orders.csv'
iam_role 'arn:aws:iam::XXXXXXXXXX:role/RoleName'
region 'us-east-1'
delimiter ','
ignoreheader 1
acceptinvchars;

copy stripe.payment(id, orderid, paymentmethod, status, amount, created)
from 's3://dbt-data-lake-xxxx/stripe_payments.csv'
iam_role 'arn:aws:iam::XXXXXXXXXX:role/RoleName'
region 'us-east-1'
delimiter ','
ignoreheader 1
Acceptinvchars;

----------------------------------------

TITLE: Creating Customer Analysis Model in dbt
DESCRIPTION: SQL model that combines customer and order data to create a comprehensive customer analysis view with order history and metrics.

LANGUAGE: sql
CODE:
with customers as (

select
    ID as customer_id,
    FIRST_NAME as first_name,
    LAST_NAME as last_name

from dbo.customers
),

orders as (

    select
        ID as order_id,
        USER_ID as customer_id,
        ORDER_DATE as order_date,
        STATUS as status

    from dbo.orders
),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by customer_id
),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders on customers.customer_id = customer_orders.customer_id
)

select * from final

----------------------------------------

TITLE: Configuring 'begin' in SQL model config block
DESCRIPTION: Sets the 'begin' configuration for a model directly in the SQL file using a config block. The begin timestamp is set to '2024-01-01 00:00:00'.

LANGUAGE: sql
CODE:
{{ config(
    begin='2024-01-01 00:00:00'
) }}

----------------------------------------

TITLE: Describing Git Button Actions in Cloud IDE with Markdown Table
DESCRIPTION: A markdown table explaining the various Git actions available through the Git button in the dbt Cloud IDE, including abort merge, change branch, commit, create new branch, initialize project, open pull request, pull changes, and refresh git state.

LANGUAGE: markdown
CODE:
| Name | Actions |
| --- | --- |
| Abort merge | This option allows you to cancel a merge that had conflicts. Be careful with this action because all changes will be reset and this operation can't be reverted, so make sure to commit or save all your changes before you start a merge. |
| Change branch | This option allows you to change between branches (checkout). |
| Commit | A commit is an individual change to a file (or set of files). When you make a commit to save your work, Git creates a unique ID (a.k.a. the "SHA" or "hash") that allows you to keep record of the specific changes committed along with who made them and when. Commits usually contain a commit message which is a brief description of what changes were made. When you make changes to your code in the future, you'll need to commit them as well. |
| Create new branch | This allows you to branch off of your base branch and edit your project. You'll notice after initializing your project that the main branch is protected.<br /><br /> This means you can directly edit, format, or lint files and execute dbt commands in your protected primary git branch. When ready, you can commit those changes to a new branch. |
| Initialize your project | This is done when first setting up your project. Initializing a project creates all required directories and files within an empty repository by using the dbt starter project. <br></br> Note: This option will not display if your repo isn't completely empty (i.e. includes a README file). <br></br> Once you click **Initialize your project**, click **Commit** to finish setting up your project. |
| Open pull request | This allows you to open a pull request in Git for peers to review changes before merging into the base branch. |
| Pull changes from main | This option is available if you are on any local branch that is behind the remote version of the base branch or the remote version of the branch that you're currently on. |
| Pull from remote | This option is available if you're on the local base branch and changes have recently been pushed to the remote version of the branch. Pulling in changes from the remote repo allows you to pull in the most recent version of the base branch. |
| Rollback to remote | Reset changes to your repository directly from the Cloud IDE. You can rollback your repository back to an earlier clone from your remote. To do this, click on the three dot ellipsis in the bottom right-hand side of the IDE and select **Rollback to remote**. |
| Refresh git state | This enables you to pull new branches from a different remote branch to your local branch with just one command. |

----------------------------------------

TITLE: Launching fly.io App in Shell
DESCRIPTION: Use the flyctl launch command to publish and prepare the app for receiving webhook events.

LANGUAGE: shell
CODE:
flyctl launch

----------------------------------------

TITLE: Enabling Partial Parsing in dbt YAML Configuration
DESCRIPTION: This snippet shows how to enable partial parsing in a dbt project's profiles.yml file. Partial parsing can improve performance by parsing only changed files.

LANGUAGE: yaml
CODE:
config:
  partial_parse: true

----------------------------------------

TITLE: Excluding Specific Resources in dbt Commands
DESCRIPTION: These examples show how to exclude specific resources by name or lineage when running dbt test, seed, and snapshot commands.

LANGUAGE: bash
CODE:
dbt test --exclude "not_null_orders_order_id"
dbt test --exclude "orders"

dbt seed --exclude "account_parent_mappings"

dbt snapshot --exclude "snap_order_statuses"

----------------------------------------

TITLE: Testing Webhook Endpoint Authorization Headers
DESCRIPTION: Shell command to test if an endpoint supports Authorization headers

LANGUAGE: shell
CODE:
curl -H 'Authorization: 123' -X POST https://<your-webhook-endpoint>

----------------------------------------

TITLE: Querying Model Historical Runs with GraphQL in DBT
DESCRIPTION: GraphQL query to fetch historical run data for a specific model including run ID, generation time, execution time, status, and test results. Uses environment ID and model's unique ID to retrieve the last 20 runs.

LANGUAGE: graphql
CODE:
query {
  environment(id: 834) {
    applied {
      modelHistoricalRuns(
        uniqueId: "model.marketing.customers" # Use this format for unique ID: RESOURCE_TYPE.PACKAGE_NAME.RESOURCE_NAME
        lastRunCount: 20
      ) {
        runId # Get historical results for a particular model
        runGeneratedAt
        executionTime # View build time across runs
        status
        tests {
          name
          status
          executeCompletedAt
        } # View test results across runs
      }
    }
  }
}

----------------------------------------

TITLE: Selecting Models by Path in dbt
DESCRIPTION: Examples of using the path method to select models or sources defined at specific paths.

LANGUAGE: bash
CODE:
# These two selectors are equivalent
dbt run --select "path:models/staging/github"
dbt run --select "models/staging/github"

# These two selectors are equivalent
dbt run --select "path:models/staging/github/stg_issues.sql"
dbt run --select "models/staging/github/stg_issues.sql"

----------------------------------------

TITLE: Creating Databricks Secret Scope for dbt Cloud API Key
DESCRIPTION: These bash commands create a Databricks secret scope and store the dbt Cloud API key securely. Replace placeholders with your actual scope name, key name, and API key value.

LANGUAGE: bash
CODE:
databricks secrets create-scope --scope <YOUR_SECRET_SCOPE>
databricks secrets put --scope  <YOUR_SECRET_SCOPE> --key  <YOUR_SECRET_KEY> --string-value "<YOUR_DBT_CLOUD_API_KEY>"

----------------------------------------

TITLE: Configuring Simple Food Revenue Metric in dbt
DESCRIPTION: Defines a simple metric to calculate revenue from food items. Uses the food_revenue measure as the basis for calculation.

LANGUAGE: yaml
CODE:
- name: food_revenue
  description: The revenue from food in each order.
  label: Food Revenue
  type: simple
  type_params:
    measure: food_revenue

----------------------------------------

TITLE: Including Considerations Component
DESCRIPTION: Imports and renders a markdown component containing considerations for Tableau auto-exposures setup

LANGUAGE: markdown
CODE:
import ConsiderationsTableau from '/snippets/_auto-exposures-considerations-tb.md';

<ConsiderationsTableau/>

----------------------------------------

TITLE: Basic Athena Query Example
DESCRIPTION: A simple SELECT query to test Athena connectivity and data access.

LANGUAGE: sql
CODE:
select * from jaffle_shop.customers

----------------------------------------

TITLE: Configuring event_time for Sources
DESCRIPTION: Examples of setting event_time configuration for source tables in project and properties files.

LANGUAGE: yml
CODE:
sources:
  [resource-path:]:
    +event_time: my_time_field

LANGUAGE: yml
CODE:
sources:
  - name: source_name
    [config]:
      event_time: my_time_field

----------------------------------------

TITLE: Defining Dependencies in dbt
DESCRIPTION: Configuration of both package and project dependencies in dependencies.yml, showing how to reference both public packages and other dbt projects.

LANGUAGE: yaml
CODE:
packages:
  - package: dbt-labs/dbt_utils
    version: 1.1.1

projects:
  - name: jaffle_finance  # case sensitive and matches the 'name' in the 'dbt_project.yml'

----------------------------------------

TITLE: Creating dimension table SQL transformation
DESCRIPTION: SQL code for creating the dim_product dimension table, including CTEs, joins, and surrogate key generation.

LANGUAGE: sql
CODE:
with stg_product as (
    select *
    from {{ ref('product') }}
),

stg_product_subcategory as (
    select *
    from {{ ref('productsubcategory') }}
),

stg_product_category as (
    select *
    from {{ ref('productcategory') }}
)

select
    {{ dbt_utils.generate_surrogate_key(['stg_product.productid']) }} as product_key, 
    stg_product.productid,
    stg_product.name as product_name,
    stg_product.productnumber,
    stg_product.color,
    stg_product.class,
    stg_product_subcategory.name as product_subcategory_name,
    stg_product_category.name as product_category_name
from stg_product
left join stg_product_subcategory on stg_product.productsubcategoryid = stg_product_subcategory.productsubcategoryid
left join stg_product_category on stg_product_subcategory.productcategoryid = stg_product_category.productcategoryid

----------------------------------------

TITLE: Command Line Usage of Version Macro
DESCRIPTION: Example of running the get_version macro from the command line using dbt run-operation, showing the output format.

LANGUAGE: bash
CODE:
$ dbt run-operation get_version
The installed version of dbt is 0.16.0

----------------------------------------

TITLE: Defining Data Types for Snowflake in dbt Unit Tests
DESCRIPTION: This snippet shows how to specify various data types including integer, float, string, date, timestamp, variant, geometry, geography, object, and arrays in a dbt unit test for Snowflake.

LANGUAGE: yaml
CODE:
unit_tests:
  - name: test_my_data_types
    model: fct_data_types
    given:
      - input: ref('stg_data_types')
        rows:
         - int_field: 1
           float_field: 2.0
           str_field: my_string
           str_escaped_field: "my,cool'string"
           date_field: 2020-01-02
           timestamp_field: 2013-11-03 00:00:00-0
           timestamptz_field: 2013-11-03 00:00:00-0
           number_field: 3
           variant_field: 3
           geometry_field: POINT(1820.12 890.56)
           geography_field: POINT(-122.35 37.55)
           object_field: {'Alberta':'Edmonton','Manitoba':'Winnipeg'}
           str_array_field: ['a','b','c']
           int_array_field: [1, 2, 3]

----------------------------------------

TITLE: Defining Model Constraints in YAML
DESCRIPTION: Example YAML configuration for defining constraints on a dbt model, including column-level and model-level constraints.

LANGUAGE: yaml
CODE:
models:
  - name: <model_name>
    
    # required
    config:
      contract: {enforced: true}
    
    # model-level constraints
    constraints:
      - type: primary_key
        columns: [first_column, second_column, ...]
      - type: foreign_key # multi_column
        columns: [first_column, second_column, ...]
        to: ref('my_model_to') | source('source', 'source_table')
        to_columns: [other_model_first_column, other_model_second_columns, ...]
      - type: check
        columns: [first_column, second_column, ...]
        expression: "first_column != second_column"
        name: human_friendly_name
      - type: ...
    
    columns:
      - name: first_column
        data_type: string
        
        # column-level constraints
        constraints:
          - type: not_null
          - type: unique
          - type: foreign_key
            to: ref('my_model_to') | source('source', 'source_table')
            to_columns: [other_model_column]
          - type: ...

----------------------------------------

TITLE: Running a Python Query with Arrow Result
DESCRIPTION: Shows how to query the GraphQL API and decode the Arrow result using Python, including status polling and conversion to a pandas DataFrame.

LANGUAGE: python
CODE:
import base64
import pyarrow as pa
import time

headers = {"Authorization":"Bearer <token>"}
query_result_request = """
{
  query(environmentId: 70, queryId: "12345678") {
    sql
    status
    error
    arrowResult
  }
}
"""

while True:
  gql_response = requests.post(
    "https://semantic-layer.cloud.getdbt.com/api/graphql",
    json={"query": query_result_request},
    headers=headers,
  )
  if gql_response.json()["data"]["status"] in ["FAILED", "SUCCESSFUL"]:
    break
  # Set an appropriate interval between polling requests
  time.sleep(1)

def to_arrow_table(byte_string: str) -> pa.Table:
  """Get a raw base64 string and convert to an Arrow Table."""
  with pa.ipc.open_stream(base64.b64decode(byte_string)) as reader:
    return pa.Table.from_batches(reader, reader.schema)

arrow_table = to_arrow_table(gql_response.json()["data"]["query"]["arrowResult"])

# Perform whatever functionality is available, like convert to a pandas table.
print(arrow_table.to_pandas())

----------------------------------------

TITLE: Basic Terminal Navigation Commands
DESCRIPTION: Common terminal commands for navigating directories when working with dbt projects from the command line.

LANGUAGE: bash
CODE:
cd

LANGUAGE: bash
CODE:
ls

LANGUAGE: bash
CODE:
pwd

----------------------------------------

TITLE: Querying Seed Information with GraphQL in dbt
DESCRIPTION: GraphQL query example demonstrating how to fetch seed information from a dbt job, including unique ID, name, execution time, and status. The query targets a specific job using its ID and retrieves core seed metadata.

LANGUAGE: graphql
CODE:
{
  job(id: 123) {
    seeds {
      uniqueId
      name
      executionTime
      status
    }
  }
}

----------------------------------------

TITLE: Querying Sample Data in BigQuery
DESCRIPTION: Basic SQL queries to verify access to sample data tables in the dbt-tutorial dataset.

LANGUAGE: sql
CODE:
select * from `dbt-tutorial.jaffle_shop.customers`;
select * from `dbt-tutorial.jaffle_shop.orders`;
select * from `dbt-tutorial.stripe.payment`;

----------------------------------------

TITLE: Listing dbt Core and Adapter Versions for February 2025 Release
DESCRIPTION: This code block lists the specific versions of dbt Core, shared interfaces, and adapters included in the February 2025 Compatible release of dbt Cloud.

LANGUAGE: yaml
CODE:
dbt-core==1.9.2

# shared interfaces
dbt-adapters==1.14.0
dbt-common==1.14.0
dbt-semantic-interfaces==0.7.4

# adapters
dbt-athena==1.9.1
dbt-bigquery==1.9.1
dbt-databricks==1.9.4
dbt-fabric==1.9.0
dbt-postgres==1.9.0
dbt-redshift==1.9.0
dbt-snowflake==1.9.1
dbt-spark==1.9.1
dbt-synapse==1.8.2
dbt-teradata==1.9.1
dbt-trino==1.9.0

----------------------------------------

TITLE: Installing dbt Core and Adapter Packages
DESCRIPTION: Example command for installing both dbt-core and a specific adapter package (in this case, Snowflake) using pip. This is now recommended since v1.8.

LANGUAGE: sql
CODE:
python3 -m pip install dbt-core dbt-snowflake

----------------------------------------

TITLE: Installing dbt Core and Adapter (version 1.8+)
DESCRIPTION: Command to install both dbt-core and the specific adapter package for dbt version 1.8 and newer. This approach is necessary because adapters and dbt Core versions have been decoupled.

LANGUAGE: bash
CODE:
python -m pip install dbt-core {props.meta.pypi_package}

----------------------------------------

TITLE: Checking dbt Profile Directory Location
DESCRIPTION: Command to verify the expected location of your profiles.yml file in your dbt installation.

LANGUAGE: bash
CODE:
$ dbt debug --config-dir

----------------------------------------

TITLE: Examples of Boolean Config Flags in dbt CLI
DESCRIPTION: Demonstrates specific examples of enabling and disabling boolean configuration flags in dbt CLI commands, using the version check flag as an example.

LANGUAGE: bash
CODE:
dbt run --version-check
dbt run --no-version-check

----------------------------------------

TITLE: Setting Secrets for fly.io App in Shell
DESCRIPTION: Command to set required secret environment variables for the fly.io application, including dbt Cloud and PagerDuty authentication tokens.

LANGUAGE: shell
CODE:
flyctl secrets set DBT_CLOUD_SERVICE_TOKEN=abc123 DBT_CLOUD_AUTH_TOKEN=def456 PD_ROUTING_KEY=ghi789

----------------------------------------

TITLE: Configuring JWT Authentication for Starburst/Trino in dbt
DESCRIPTION: Example YAML configuration for setting up a JWT-authenticated connection to a Starburst/Trino cluster in dbt's profiles.yml file. Includes essential connection parameters and JWT-specific settings.

LANGUAGE: yaml
CODE:
trino:
  target: dev
  outputs:
    dev:
      type: trino
      method: jwt 
      jwt_token: [my_long_jwt_token_string]
      host: [hostname]
      database: [database name]
      schema: [your dbt schema]
      port: [port number]
      threads: [1 or more]

----------------------------------------

TITLE: Fetching Query Results with GraphQL
DESCRIPTION: Demonstrates how to fetch query results using the GraphQL API, including status checking and result formatting options.

LANGUAGE: graphql
CODE:
query(
  environmentId: BigInt!
  queryId: String!
): QueryResult!

----------------------------------------

TITLE: Checking Python Version on MacOS for dbt Core Compatibility
DESCRIPTION: Verifies the installed Python version on MacOS to ensure compatibility with dbt Core, which requires Python 3.8 or higher.

LANGUAGE: shell
CODE:
python --version

----------------------------------------

TITLE: Running Exports for Multiple Saved Queries
DESCRIPTION: Use the dbt sl export-all command to run exports for multiple saved queries simultaneously. This command executes all configured exports for all saved queries in the project.

LANGUAGE: bash
CODE:
dbt sl export-all

----------------------------------------

TITLE: Using Wildcards in dbt Node Selection
DESCRIPTION: Examples of using Unix-style wildcards in dbt node selection commands.

LANGUAGE: bash
CODE:
dbt list --select "*.folder_name.*"
dbt list --select "package:*_source"

----------------------------------------

TITLE: Final Customer Model with References
DESCRIPTION: The final customer model that uses dbt refs to reference staging models and combines customer and order data.

LANGUAGE: sql
CODE:
with customers as (

    select * from {{ ref('stg_customers') }}

),

orders as (

    select * from {{ ref('stg_orders') }}

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final

----------------------------------------

TITLE: Excluding Unit Tests from DBT Test Command
DESCRIPTION: Command to exclude all unit tests when running DBT tests using the --exclude-resource-type flag (available in DBT v1.9+).

LANGUAGE: text
CODE:
dbt test --exclude-resource-type unit_test

----------------------------------------

TITLE: Specifying config version in dbt_project.yml
DESCRIPTION: Example of specifying the new config-version: 2 in the dbt_project.yml file. This enables new functionality in dbt v0.17.0.

LANGUAGE: yaml
CODE:
name: my_project
version: 1.0.0

config-version: 2

models:
  ...

----------------------------------------

TITLE: Configuring Incremental Strategy in Model
DESCRIPTION: SQL model configuration for setting the incremental_strategy at the individual model level.

LANGUAGE: sql
CODE:
{{
  config(
    materialized='incremental',
    unique_key='id',
    incremental_strategy='delete+insert'
  )
}}

select ...

----------------------------------------

TITLE: Running and testing dbt models
DESCRIPTION: Command to run and test dbt models.

LANGUAGE: shell
CODE:
dbt run && dbt test

----------------------------------------

TITLE: Installing ODBC Prerequisites on Debian/Ubuntu
DESCRIPTION: Command to install ODBC header files required for the Microsoft Fabric connector

LANGUAGE: bash
CODE:
sudo apt install unixodbc-dev

----------------------------------------

TITLE: Configuring Incremental Model with Partition Copy in dbt
DESCRIPTION: This snippet demonstrates how to enable partition copy for an incremental model in dbt. It adds the 'copy_partitions' flag to the model configuration, which uses the BigQuery driver integration for improved performance when writing data.

LANGUAGE: sql
CODE:
{{ config(
    materialized = 'incremental',
    incremental_strategy = 'insert_overwrite',
    partition_by = {
      "field": "day",
      "data_type": "date",
      "copy_partitions": true
    }
) }}

select
    day,
    campaign_id,
    NULLIF(COUNTIF(action = 'impression'), 0) impressions_count
from {{ source('logs', 'tracking_events') }}
group by day, campaign_id

----------------------------------------

TITLE: Adding Models to a Group at Project Level
DESCRIPTION: Configures models in the 'finance' directory to be part of the 'finance' group using the dbt_project.yml file.

LANGUAGE: yaml
CODE:
models:
  marts:
    finance:
      +group: finance

----------------------------------------

TITLE: Manual Archive Migration SQL
DESCRIPTION: SQL commands for manually renaming archive meta-columns when migrating to snapshots in PostgreSQL.

LANGUAGE: sql
CODE:
alter table archived.orders_archived rename "valid_from" to dbt_valid_from;
alter table archived.orders_archived rename "valid_to" to dbt_valid_to;
alter table archived.orders_archived rename "scd_id" to dbt_scd_id;

----------------------------------------

TITLE: Querying Sources by Schema in GraphQL
DESCRIPTION: GraphQL query example showing how to find all sources within a specific schema. Returns the uniqueId and state (pass, error, fail) for each source found.

LANGUAGE: graphql
CODE:
{
  job(id: 123) {
    sources(schema: "analytics") {
      uniqueId
      state
    }
  }
}

----------------------------------------

TITLE: Adding Required Entries to .gitignore for dbt Cloud
DESCRIPTION: This code snippet shows the correct entries to add to a .gitignore file for dbt Cloud projects. It includes directories that should be ignored by Git to ensure proper functioning of dbt Cloud.

LANGUAGE: bash
CODE:
# ‚úÖ Correct 
target/
dbt_packages/
logs/
# legacy -- renamed to dbt_packages in dbt v1
dbt_modules/

----------------------------------------

TITLE: Running Validations in dbt Cloud and Core
DESCRIPTION: Commands to run validations for the Semantic Layer in dbt Cloud and dbt Core. This snippet shows how to execute the validate-configs command in different environments.

LANGUAGE: bash
CODE:
dbt sl validate # dbt Cloud users
mf validate-configs # dbt Core users

----------------------------------------

TITLE: Configuring dbt Cloud Connection in Airflow DAG
DESCRIPTION: Python code snippet for updating the account_id and job_id in the Airflow DAG file. These IDs are used to connect to the specific dbt Cloud job.

LANGUAGE: python
CODE:
# For the dbt Cloud Job URL https://YOUR_ACCESS_URL/#/accounts/16173/projects/36467/jobs/65767/
# The account_id is 16173 and the job_id is 65767
# Update lines 34 and 35
ACCOUNT_ID = "16173"
JOB_ID = "65767"

----------------------------------------

TITLE: Environment Query Before Deprecation
DESCRIPTION: Example of an environment query using Int data type before deprecation

LANGUAGE: graphql
CODE:
query ($environmentId: Int!, $first: Int!) {
environment(id: $environmentId) {
    applied {
    models(first: $first) {
        edges {
        node {
            uniqueId
            executionInfo {
            lastRunId
            }
        }
        }
    }
    }
}
}

----------------------------------------

TITLE: Install dbt Cloud CLI via Homebrew
DESCRIPTION: Commands to install dbt Cloud CLI using Homebrew package manager on macOS

LANGUAGE: bash
CODE:
brew untap dbt-labs/dbt
brew tap dbt-labs/dbt-cli
brew install dbt

----------------------------------------

TITLE: Configuring Incremental Strategy in Project YAML
DESCRIPTION: YAML configuration for setting the incremental_strategy at the project level.

LANGUAGE: yaml
CODE:
models:
  incremental_strategy: "delete+insert"

----------------------------------------

TITLE: Complete Semantic Model Configuration
DESCRIPTION: Full example of a semantic model configuration combining all components including entities, measures, and dimensions.

LANGUAGE: yaml
CODE:
semantic_models:
  #The name of the semantic model.
  - name: orders
    defaults:
      agg_time_dimension: ordered_at
    description: |
      Order fact table. This table is at the order grain with one row per order. 
    #The name of the dbt model and schema
    model: ref('orders')
    #Entities. These usually corespond to keys in the table.
    entities:
      - name: order_id
        type: primary
      - name: location
        type: foreign
        expr: location_id
      - name: customer
        type: foreign
        expr: customer_id
    #Measures. These are the aggregations on the columns in the table.
    measures: 
      - name: order_total
        description: The total revenue for each order.
        agg: sum
      - name: order_count
        expr: 1
        agg: sum
      - name: tax_paid
        description: The total tax paid on each order. 
        agg: sum
      - name: customers_with_orders
        description: Distinct count of customers placing orders
        agg: count_distinct
        expr: customer_id
      - name: locations_with_orders
        description: Distinct count of locations with order
        expr: location_id
        agg: count_distinct
      - name: order_cost
        description: The cost for each order item. Cost is calculated as a sum of the supply cost for each order item. 
        agg: sum
    #Dimensions. Either categorical or time. These add additional context to metrics. The typical querying pattern is Metric by Dimension.  
    dimensions:
      - name: ordered_at
        type: time
        type_params:
          time_granularity: day 
      - name: order_total_dim
        type: categorical
        expr: order_total
      - name: is_food_order
        type: categorical
      - name: is_drink_order
        type: categorical

----------------------------------------

TITLE: Incremental Model with Merge Strategy
DESCRIPTION: Example SQL configuration for incremental model using merge strategy.

LANGUAGE: sql
CODE:
{{ config(
    materialized='incremental',
    incremental_strategy='merge',
    unique_key='user_id',
    file_format='hudi',
    hudi_options={
        'hoodie.datasource.write.precombine.field': 'eventtime',
    }
) }}

with new_events as (
    select * from {{ ref('events') }}
    {% if is_incremental() %}
    where date_day >= date_add(current_date, -1)
    {% endif %}
)

select
    user_id,
    max(date_day) as last_seen
from events
group by 1

----------------------------------------

TITLE: Configuring SQLite Profile in YAML for dbt
DESCRIPTION: This YAML snippet demonstrates how to configure a SQLite target in the profiles.yml file for dbt. It includes essential fields such as type, threads, database, schema, and paths for schema files.

LANGUAGE: yaml
CODE:
your_profile_name:
  target: dev
  outputs:
    dev:
      type: sqlite
      threads: 1
      database: 'database'
      schema: 'main'
      schemas_and_paths:
        main: 'file_path/database_name.db'
      schema_directory: 'file_path'
      #optional fields
      extensions:
        - "/path/to/sqlean/crypto.so"

----------------------------------------

TITLE: Python Request to Discovery API
DESCRIPTION: Python implementation for making a POST request to the Discovery API using the requests library.

LANGUAGE: python
CODE:
response = requests.post(
    'YOUR_API_URL',
    headers={"authorization": "Bearer "+YOUR_TOKEN, "content-type": "application/json"},
    json={"query": QUERY_BODY, "variables": VARIABLES}
)

metadata = response.json()['data'][ENDPOINT]

----------------------------------------

TITLE: Importing Google Cloud Source Repository in dbt Cloud using SSH URL
DESCRIPTION: Example of the SSH URL format used to import a Google Cloud Source repository into dbt Cloud. This URL includes the user email, project ID, and repository name.

LANGUAGE: plaintext
CODE:
ssh://drew@fishtownanalytics.com@source.developers.google.com:2022/p/dbt-integration-tests/r/drew-debug

----------------------------------------

TITLE: Structured HTML Layout for dbt Documentation
DESCRIPTION: HTML structure using custom Card components to display documentation navigation for dbt output organization options

LANGUAGE: html
CODE:
<div className="grid--2-col">

<Card
    title="Custom schemas"
    body="Learn how to use the <code>schema</code> configuration key to specify a custom schema."
    link="/docs/build/custom-schemas"
    icon="dbt-bit"/>

<Card
    title="Custom databases"
    body="Learn how to use the <code>database</code> configuration key to specify a custom database."
    link="/docs/build/custom-databases"
    icon="dbt-bit"/>

</div>
<br />
<div className="grid--2-col">

<Card
    title="Custom aliases"
    body="Learn how to use the <code>alias</code> model configuration to change the name of a model's identifier in the database."
    link="/docs/build/custom-aliases"
    icon="dbt-bit"/>

<Card
    title="Custom target names"
    body="Learn how to define a custom target name for a dbt Cloud job."
    link="/docs/build/custom-target-names"
    icon="dbt-bit"/>

</div>

----------------------------------------

TITLE: Listing File Paths
DESCRIPTION: Example demonstrating how to list file paths for resources using the path output format.

LANGUAGE: bash
CODE:
dbt ls --select snowplow.* --output path
models/base/snowplow_base_events.sql
models/base/snowplow_base_web_page_context.sql
models/identification/snowplow_id_map.sql
...

----------------------------------------

TITLE: Installing dbt Core with Adapter
DESCRIPTION: New installation procedure for dbt Core v1.8+ requiring explicit installation of both dbt-core and the adapter package.

LANGUAGE: shell
CODE:
pip install dbt-core dbt-ADAPTER_NAME

LANGUAGE: shell
CODE:
pip install dbt-core dbt-snowflake

----------------------------------------

TITLE: Querying Snapshot Information for a dbt Job using GraphQL
DESCRIPTION: This GraphQL query retrieves information about all snapshots in a specific dbt job. It demonstrates how to use the job and snapshots objects to fetch details such as uniqueId, name, executionTime, and other relevant fields.

LANGUAGE: graphql
CODE:
{
  job(id: 123) {
    snapshots {
      uniqueId
      name
      executionTime
      environmentId
      executeStartedAt
      executeCompletedAt
    }
  }
}

----------------------------------------

TITLE: Install dbt Cloud CLI via pip
DESCRIPTION: Command to install dbt Cloud CLI using pip package manager

LANGUAGE: bash
CODE:
pip install dbt --no-cache-dir

----------------------------------------

TITLE: Authenticating GraphQL API Requests
DESCRIPTION: Demonstrates how to authenticate API requests using a dbt Cloud service account token passed through a header.

LANGUAGE: shell
CODE:
{"Authorization": "Bearer <SERVICE TOKEN>"}

----------------------------------------

TITLE: Configuring Incremental Materialization in Model Config Block
DESCRIPTION: Illustrates how to configure an incremental ClickHouse table materialization using a config block in a model file, including unique_key and inserts_only options.

LANGUAGE: jinja
CODE:
{{ config(
    materialized = "incremental",
    engine = "<engine-type>",
    order_by = [ "<column-name>", ... ],
    partition_by = [ "<column-name>", ... ],
    unique_key = [ "<column-name>", ... ],
    inserts_only = [ True|False ],
      ...
    ]
) }}

----------------------------------------

TITLE: Example Infer BigQuery Configuration
DESCRIPTION: Complete example of an Infer configuration using BigQuery as the underlying data warehouse, showing all required fields and typical values

LANGUAGE: yaml
CODE:
infer_bigquery:
  apikey: 1234567890abcdef
  username: my_name@example.com
  url: https://app.getinfer.io
  type: infer
  data_config:
    dataset: my_dataset
    job_execution_timeout_seconds: 300
    job_retries: 1
    keyfile: bq-user-creds.json
    location: EU
    method: service-account
    priority: interactive
    project: my-bigquery-project
    threads: 1
    type: bigquery

----------------------------------------

TITLE: Calling Stream Model Macro in dbt Model
DESCRIPTION: This SQL file calls the stream_model_macro with the container variable to generate the data for the src_container model.

LANGUAGE: SQL
CODE:
{{ stream_model_macro(var('container')) }}

----------------------------------------

TITLE: Configuring Test Result Storage Schema
DESCRIPTION: Configuration for storing test failures in a custom schema with store_failures enabled

LANGUAGE: yaml
CODE:
tests:
  +store_failures: true
  +schema: _sad_test_failures

----------------------------------------

TITLE: Authorizing Stage PrivateLink Access in Snowflake Azure
DESCRIPTION: SQL command to authorize access to Snowflake internal stage through a private endpoint in Azure. This step is necessary for completing the setup of Snowflake hosted on Azure.

LANGUAGE: sql
CODE:
USE ROLE ACCOUNTADMIN;

-- Azure Private Link
SELECT SYSTEMS$AUTHORIZE_STAGE_PRIVATELINK_ACCESS ( `AZURE PRIVATE ENDPOINT RESOURCE ID` );

----------------------------------------

TITLE: Creating and Loading Jaffle Shop Tables in Azure Synapse
DESCRIPTION: SQL script to create and populate three tables (customers, orders, payments) in Azure Synapse Analytics using COPY INTO statements from Parquet files.

LANGUAGE: sql
CODE:
CREATE TABLE dbo.customers
(
    [ID] [bigint],
    [FIRST_NAME] [varchar](8000),
    [LAST_NAME] [varchar](8000)
);

COPY INTO [dbo].[customers]
FROM 'https://dbtlabsynapsedatalake.blob.core.windows.net/dbt-quickstart-public/jaffle_shop_customers.parquet'
WITH (
    FILE_TYPE = 'PARQUET'
);

CREATE TABLE dbo.orders
(
    [ID] [bigint],
    [USER_ID] [bigint],
    [ORDER_DATE] [date],
    [STATUS] [varchar](8000)
);

COPY INTO [dbo].[orders]
FROM 'https://dbtlabsynapsedatalake.blob.core.windows.net/dbt-quickstart-public/jaffle_shop_orders.parquet'
WITH (
    FILE_TYPE = 'PARQUET'
);

CREATE TABLE dbo.payments
(
    [ID] [bigint],
    [ORDERID] [bigint],
    [PAYMENTMETHOD] [varchar](8000),
    [STATUS] [varchar](8000),
    [AMOUNT] [bigint],
    [CREATED] [date]
);

COPY INTO [dbo].[payments]
FROM 'https://dbtlabsynapsedatalake.blob.core.windows.net/dbt-quickstart-public/stripe_payments.parquet'
WITH (
    FILE_TYPE = 'PARQUET'
);

----------------------------------------

TITLE: Audit Log Event Types Table Structure
DESCRIPTION: Markdown table structure showing authentication event types and their descriptions.

LANGUAGE: markdown
CODE:
| Event Name                 | Event Type                               | Description                                            |
| -------------------------- | ---------------------------------------- | ------------------------------------------------------ |
| Auth Provider Changed      | auth_provider.Changed          | Authentication provider settings changed               |
| Credential Login Succeeded | auth.CredentialsLoginSucceeded | User successfully logged in with username and password |
| SSO Login Failed           | auth.SsoLoginFailed            | User login via SSO failed                              |
| SSO Login Succeeded        | auth.SsoLoginSucceeded         | User successfully logged in via SSO                    |

----------------------------------------

TITLE: Installing dbt Semantic Layer SDK - Sync Version
DESCRIPTION: Command to install the synchronous version of the dbt Semantic Layer SDK using pip package manager.

LANGUAGE: bash
CODE:
pip install "dbt-sl-sdk[sync]"

----------------------------------------

TITLE: Configuring Insecure Hive Connection in YAML
DESCRIPTION: YAML configuration for setting up an insecure Hive connection in dbt's profiles.yml file. This method is recommended only for testing with a local Hive install.

LANGUAGE: yaml
CODE:
your_profile_name:
  target: dev
  outputs:
    dev:
      type: hive
      host: localhost
      port: PORT # default value: 10000
      schema: SCHEMA_NAME

----------------------------------------

TITLE: SQL Left Join Implementation Example
DESCRIPTION: Example SQL showing how MetricFlow implements a left join between transactions and user_signup tables.

LANGUAGE: sql
CODE:
select
  transactions.user_id,
  transactions.purchase_price,
  user_signup.type
from transactions
left outer join user_signup
  on transactions.user_id = user_signup.user_id
where transactions.purchase_price is not null
group by
  transactions.user_id,
  user_signup.type;

----------------------------------------

TITLE: Querying Sources by Database, Schema, and Identifier in GraphQL
DESCRIPTION: GraphQL query example demonstrating how to find a specific source by providing database, schema, and identifier parameters. Returns the uniqueId of the matching source.

LANGUAGE: graphql
CODE:
{
  job(id: 123) {
    sources(
      database: "analytics"
      schema: "analytics"
      identifier: "dim_customers"
    ) {
      uniqueId
    }
  }
}

----------------------------------------

TITLE: Configuring View Materialization in Model Config Block
DESCRIPTION: Shows how to configure a ClickHouse view materialization using a config block in a model file.

LANGUAGE: jinja
CODE:
{{ config(materialized = "view") }}

----------------------------------------

TITLE: Installing dbt-watsonx-spark Package
DESCRIPTION: Command to install both dbt-core and dbt-watsonx-spark adapter packages using pip. Note that from dbt v1.8, adapters no longer automatically install dbt-core.

LANGUAGE: sh
CODE:
python -m pip install dbt-core dbt-watsonx-spark

----------------------------------------

TITLE: Configuring quote_columns for Teradata in dbt_project.yml
DESCRIPTION: Sets the quote_columns option for seeds to prevent warnings and handle column headers with spaces.

LANGUAGE: yaml
CODE:
seeds:
  +quote_columns: false  #or `true` if you have csv column headers with spaces

----------------------------------------

TITLE: Setting Snowflake User Public Key
DESCRIPTION: SQL command to set the RSA public key for a Snowflake user for key pair authentication.

LANGUAGE: sql
CODE:
alter user jsmith set rsa_public_key='MIIBIjANBgkqh...';

----------------------------------------

TITLE: Checking dbt and Dremio Adapter Versions
DESCRIPTION: Command to verify the installed versions of dbt Core (must be 1.5 or newer) and the Dremio adapter (must be 1.5.0 or newer).

LANGUAGE: shell
CODE:
$ dbt --version
Core:
  - installed: 1.5.0 # Must be 1.5 or newer
  - latest:    1.6.3 - Update available!

  Your version of dbt-core is out of date!
  You can find instructions for upgrading here:
  https://docs.getdbt.com/docs/installation

Plugins:
  - dremio: 1.5.0 - Up to date! # Must be 1.5 or newer

----------------------------------------

TITLE: Full Outer Join Example with Car Data in SQL
DESCRIPTION: This example demonstrates a full outer join between 'car_type' and 'car_color' tables, joining on 'user_id'. It selects the user_id, car type, and color, ordering the results by user_id.

LANGUAGE: sql
CODE:
select
   car_type.user_id as user_id,
   car_type.car_type as type,
   car_color.car_color as color
from {{ ref('car_type') }} as car_type
full outer join {{ ref('car_color') }} as car_color
on car_type.user_id = car_color.user_id
order by 1

----------------------------------------

TITLE: Basic YAML Selector Structure
DESCRIPTION: Defines the basic structure of selectors.yml file with name, description and definition fields.

LANGUAGE: yaml
CODE:
selectors:
  - name: nodes_to_joy
    definition: ...
  - name: nodes_to_a_grecian_urn
    description: Attic shape with a fair attitude
    default: true
    definition: ...

----------------------------------------

TITLE: Fetching Data Platform Dialect with GraphQL
DESCRIPTION: Shows how to query the data platform dialect used for the dbt Semantic Layer connection.

LANGUAGE: graphql
CODE:
{
  environmentInfo(environmentId: BigInt!) {
    dialect
  }
}

----------------------------------------

TITLE: Configuring Insecure Impala Connection in dbt
DESCRIPTION: Basic configuration for connecting to Impala without authentication, recommended only for testing purposes. Defines host, port, database name and schema settings.

LANGUAGE: yaml
CODE:
your_profile_name:
  target: dev
  outputs:
    dev:
      type: impala
      host: [host]
      port: [port]
      dbname: [db name]
      schema: [schema name]

----------------------------------------

TITLE: Using Environment Variables in SQL Model
DESCRIPTION: Example of using dbt Cloud run ID in a SQL model for auditing purposes. The code demonstrates how to inject run metadata using environment variables with a default fallback value.

LANGUAGE: sql
CODE:
{{ config(materialized='incremental', unique_key='user_id') }}

with users_aggregated as (

    select
        user_id,
        min(event_time) as first_event_time,
        max(event_time) as last_event_time,
        count(*) as count_total_events

    from {{ ref('users') }}
    group by 1

)

select *,
    -- Inject the run id if present, otherwise use "manual"
    '{{ env_var("DBT_CLOUD_RUN_ID", "manual") }}' as _audit_run_id

from users_aggregated

----------------------------------------

TITLE: Installing dbt Database Adapter via pip
DESCRIPTION: Command to install a specific dbt adapter from PyPI using pip. The installation includes dbt-core and all required dependencies.

LANGUAGE: bash
CODE:
python -m pip install adapter-name

----------------------------------------

TITLE: Configuring Inputs for dbt Unit Tests in YAML
DESCRIPTION: This YAML snippet demonstrates how to set up inputs for a dbt unit test. It includes examples of referencing models, defining mock data, and structuring the test configuration. The test checks for valid email addresses using mock data for customers and top-level email domains.

LANGUAGE: yaml
CODE:
unit_tests:
  - name: test_is_valid_email_address # this is the unique name of the test
    model: dim_customers # name of the model I'm unit testing
    given: # the mock data for your inputs
      - input: ref('stg_customers')
        rows:
         - {email: cool@example.com,     email_top_level_domain: example.com}
         - {email: cool@unknown.com,     email_top_level_domain: unknown.com}
         - {email: badgmail.com,         email_top_level_domain: gmail.com}
         - {email: missingdot@gmailcom,  email_top_level_domain: gmail.com}
      - input: ref('top_level_email_domains')
        rows:
         - {tld: example.com}
         - {tld: gmail.com}

----------------------------------------

TITLE: Querying Seed Information in dbt Job using GraphQL
DESCRIPTION: This GraphQL query retrieves information about a specific seed in a dbt job. It demonstrates how to query for database, schema, uniqueId, name, status, and error fields of a seed object.

LANGUAGE: graphql
CODE:
{
  job(id: 123) {
    seed(uniqueId: "seed.jaffle_shop.raw_customers") {
      database
      schema
      uniqueId
      name
      status
      error
    }
  }
}

----------------------------------------

TITLE: Implementing Double Entry Accounting in SQL
DESCRIPTION: SQL code demonstrating how to implement double entry accounting by creating debit and credit entries for bill transactions. Shows how to handle transaction types and account IDs using UNION ALL to combine entries.

LANGUAGE: sql
CODE:
select
    transaction_id,
    transaction_date,
    customer_id,
    vendor_id,
    amount,
    payed_to_account_id as account_id,
    'debit' as transaction_type,
    'bill' as transaction_source
from bill_join
 
union all

select
    transaction_id,
    transaction_date,
    customer_id,
    vendor_id,
    amount,
    payable_account_id as account_id,
    'credit' as transaction_type,
    'bill' as transaction_source
from bill_join

----------------------------------------

TITLE: Navigating to Project Directory in Shell
DESCRIPTION: Change directory to the cloned repository location using the cd command in the shell.

LANGUAGE: shell
CODE:
#example: replace with your actual path
cd ~/Documents/GitHub/dbt-cloud-webhooks-datadog

----------------------------------------

TITLE: Saved Queries Configuration Examples
DESCRIPTION: Examples demonstrating the correct naming conventions for saved queries configuration in dbt_project.yml and semantic_models.yml files.

LANGUAGE: yaml
CODE:
saved-queries:
  my_saved_query:
    +cache:
      enabled: true

LANGUAGE: yaml
CODE:
saved_queries:
  - name: saved_query_name
    config:
      cache:
        enabled: true

----------------------------------------

TITLE: Creating Redshift Tables
DESCRIPTION: SQL commands to create the customer, order, and payment tables with their respective schema definitions

LANGUAGE: sql
CODE:
create table jaffle_shop.customers(
    id integer,
    first_name varchar(50),
    last_name varchar(50)
);

create table jaffle_shop.orders(
    id integer,
    user_id integer,
    order_date date,
    status varchar(50)
);

create table stripe.payment(
    id integer,
    orderid integer,
    paymentmethod varchar(50),
    status varchar(50),
    amount integer,
    created date
);

----------------------------------------

TITLE: Using PyPI Packages in Python Models
DESCRIPTION: Demonstrates how to use third-party packages in Python models and configure required packages.

LANGUAGE: python
CODE:
import holidays

def is_holiday(date_col):
    # Chez Jaffle
    french_holidays = holidays.France()
    is_holiday = (date_col in french_holidays)
    return is_holiday

def model(dbt, session):
    dbt.config(
        materialized = "table",
        packages = ["holidays"]
    )

    orders_df = dbt.ref("stg_orders")

    df = orders_df.to_pandas()

    # apply our function
    # (columns need to be in uppercase on Snowpark)
    df["IS_HOLIDAY"] = df["ORDER_DATE"].apply(is_holiday)
    df["ORDER_DATE"].dt.tz_localize('UTC') # convert from Number/Long to tz-aware Datetime

    # return final dataset (Pandas DataFrame)
    return df

----------------------------------------

TITLE: Querying Environment Data in GraphQL for dbt
DESCRIPTION: This query retrieves detailed information about models in a dbt project, including basic properties, execution info, test results, catalog info, lineage, and governance details. It demonstrates querying both applied and definition states of an environment.

LANGUAGE: graphql
CODE:
query Example {
	environment(id: 834){ # Get the latest state of the production environment
		applied { # The state of an executed node as it exists as an object in the database
			models(first: 100){ # Pagination to ensure manageable response for large projects
				edges { node {
					uniqueId, name, description, rawCode, compiledCode, # Basic properties
					database, schema, alias, # Table/view identifier (can also filter by)
					executionInfo {executeCompletedAt, executionTime}, # Metadata from when the model was built
					tests {name, executionInfo{lastRunStatus, lastRunError}}, # Latest test results
					catalog {columns {name, description, type}, stats {label, value}}, # Catalog info
					ancestors(types:[Source]) {name, ...on SourceAppliedStateNode {freshness{maxLoadedAt, freshnessStatus}}}, # Source freshness }
					children {name, resourceType}}} # Immediate dependencies in lineage
				totalCount } # Number of models in the project
		}
		definition { # The logical state of a given project node given its most recent manifest generated
			models(first: 100, filter:{access:public}){ # Filter on model access (or other properties)
				edges { node {
					rawCode, # Compare to see if/how the model has changed since the last build
					jobDefinitionId, runGeneratedAt,	# When the code was last compiled or run
					contractEnforced, group, version}}} # Model governance
		}
	}


----------------------------------------

TITLE: Configuring Firebolt Connection Profile in YAML
DESCRIPTION: This snippet shows the structure of a Firebolt connection profile in the profiles.yml file. It includes required and optional fields for connecting to Firebolt from dbt.

LANGUAGE: yaml
CODE:
<profile-name>:
  target: <target-name>
  outputs:
    <target-name>:
      type: firebolt
      client_id: "<id>"
      client_secret: "<secret>"
      database: "<database-name>"
      engine_name: "<engine-name>"
      account_name: "<account-name>"
      schema: <tablename-prefix>
      threads: 1
      #optional fields
      host: "<hostname>"

----------------------------------------

TITLE: Complete Transaction Model with Multiple Aggregations
DESCRIPTION: Comprehensive example showing a semantic model for transactions with various measure types including sums, averages, counts, and percentiles.

LANGUAGE: yaml
CODE:
semantic_models:
  - name: transactions
    description: A record of every transaction that takes place. Carts are considered  multiple transactions for each sku.
    model: ref('schema.transactions')
    defaults:
      agg_time_dimension: transaction_date
    entities:
      - name: transaction_id
        type: primary
      - name: customer_id
        type: foreign
    measures:
      - name: transaction_amount_usd
        description: Total usd value of transactions
        expr: transaction_amount_usd
        agg: sum

----------------------------------------

TITLE: Support Request Template for Redshift Interface-type PrivateLink
DESCRIPTION: Template for submitting a PrivateLink configuration request to dbt Support for Interface-type endpoints. Includes required fields for endpoint service and environment details.

LANGUAGE: text
CODE:
Subject: New Multi-Tenant PrivateLink Request
- Type: Redshift Interface-type
- VPC Endpoint Service Name:
- Redshift cluster AWS Region (e.g., us-east-1, eu-west-2):
- dbt Cloud multi-tenant environment (US, EMEA, AU):

----------------------------------------

TITLE: Installing audit_helper Package in dbt
DESCRIPTION: Add the audit_helper package to your dbt project's packages.yml file to install it.

LANGUAGE: yaml
CODE:
packages:
- package: dbt-labs/audit_helper
  version: 0.7.0

----------------------------------------

TITLE: Asynchronous Client Usage Example
DESCRIPTION: Example demonstrating how to initialize and use the AsyncSemanticLayerClient for asynchronous operations.

LANGUAGE: python
CODE:
import asyncio
from dbtsl.asyncio import AsyncSemanticLayerClient

client = AsyncSemanticLayerClient(
    environment_id=123,
    auth_token="<your-semantic-layer-api-token>",
    host="semantic-layer.cloud.getdbt.com",
)

async def main():
    async with client.session():
        metrics = await client.metrics()
        table = await client.query(
            metrics=[metrics[0].name],
            group_by=["metric_time"],
        )
        print(table)

asyncio.run(main())

----------------------------------------

TITLE: Executing dbt Models with Package and Relationship Filters
DESCRIPTION: This snippet demonstrates how to run dbt models by selecting all models in a specific package and their children, or a specific model with its parents and children.

LANGUAGE: bash
CODE:
dbt run --select "my_package.*+"      # select all models in my_package and their children
dbt run --select "+some_model+"       # select some_model and all parents and children

----------------------------------------

TITLE: Updating Dremio Adapter Relation Pattern
DESCRIPTION: Python code to update the relation.py file in the Dremio adapter directory to support schema names containing dots and spaces.

LANGUAGE: python
CODE:
PATTERN = re.compile(r"""((?:[^."']|"[^"]*"|'[^']*')+)""")
return ".".join(PATTERN.split(identifier)[1::2])

----------------------------------------

TITLE: Advanced MetricFlow Query with Multiple Metrics
DESCRIPTION: Complex MetricFlow query example showing how to query multiple metrics with multiple dimensions and explanation.

LANGUAGE: bash
CODE:
mf query --metrics orders,revenue --group-by metric_time__month,customer_type --explain

----------------------------------------

TITLE: SQL Join for Visit-Buy Conversion Analysis
DESCRIPTION: SQL query to join visits and buys tables for conversion analysis within a 7-day window.

LANGUAGE: sql
CODE:
select
  v.ds,
  v.user_id,
  v.referrer_id,
  b.ds,
  b.uuid,
  1 as buys
from visits v
inner join (
    select *, uuid_string() as uuid from buys
) b
on
v.user_id = b.user_id and v.ds <= b.ds and v.ds > b.ds - interval '7 days'

----------------------------------------

TITLE: Basic SQL ROUND Function Syntax
DESCRIPTION: The basic syntax for the ROUND function that takes a numeric value and optional decimal places parameter.

LANGUAGE: sql
CODE:
round(<numeric column or data>, [optional] <number of decimal places>)

----------------------------------------

TITLE: Pulling dbt Docker Image from GitHub Packages
DESCRIPTION: This command pulls a dbt Docker image from GitHub Packages. Replace <db_adapter_name> with the desired database adapter and <version_tag> with the specific version or 'latest'.

LANGUAGE: bash
CODE:
docker pull ghcr.io/dbt-labs/<db_adapter_name>:<version_tag>

----------------------------------------

TITLE: Selecting Specific Exports from a Saved Query
DESCRIPTION: Use the --select flag with the dbt sl export command to run specific exports from a saved query. This allows you to choose which exports to execute without running all exports defined for the query.

LANGUAGE: bash
CODE:
dbt sl export --saved-query sq_name --select export_1,export2

----------------------------------------

TITLE: Configuring dbt Profile for Athena Connection in YAML
DESCRIPTION: This YAML snippet demonstrates how to configure the dbt profile for connecting to Athena. It includes essential parameters such as staging directory, data directory, region, database, schema, and optional AWS profile name.

LANGUAGE: yaml
CODE:
default:
  outputs:
    dev:
      type: athena
      s3_staging_dir: [s3_staging_dir]
      s3_data_dir: [s3_data_dir]
      s3_data_naming: [table_unique] # the type of naming convention used when writing to S3
      region_name: [region_name]
      database: [database name]
      schema: [dev_schema]
      aws_profile_name: [optional profile to use from your AWS shared credentials file.]
      threads: [1 or more]
      num_retries: [0 or more] # number of retries performed by the adapter. Defaults to 5
  target: dev

----------------------------------------

TITLE: Configuring Custom Alias for a SQL Model in dbt
DESCRIPTION: This snippet demonstrates how to set a custom alias for a dbt model directly in the SQL file. It uses the config block to set the alias to 'sessions' and also specifies a custom schema.

LANGUAGE: sql
CODE:
-- This model will be created in the database with the identifier `sessions`
-- Note that in this example, `alias` is used along with a custom schema
{{ config(alias='sessions', schema='google_analytics') }}

select * from ...

----------------------------------------

TITLE: Including Snapshots in DBT Build
DESCRIPTION: Command to include all snapshots in the DBT build process using the --resource-type flag.

LANGUAGE: text
CODE:
dbt build --resource-type snapshot

----------------------------------------

TITLE: Polars Integration Example
DESCRIPTION: Code showing how to convert query results to a Polars DataFrame.

LANGUAGE: python
CODE:
import polars as pl

arrow_table = client.query(...)
polars_df = pl.from_arrow(arrow_table)

----------------------------------------

TITLE: Configuring Surrogate Key Meta in YAML
DESCRIPTION: YAML configuration to enable surrogate key generation for a dbt model using meta properties.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: dim_customers
	description: all customers
    config:
      meta:
        surrogate_key: true

----------------------------------------

TITLE: Complete snapshot configuration example
DESCRIPTION: Comprehensive example showing schema configuration with dbt_valid_to_current and column descriptions

LANGUAGE: yaml
CODE:
snapshots:
  - name: my_snapshot
    config:
      strategy: timestamp
      updated_at: updated_at
      dbt_valid_to_current: "to_date('9999-12-31')"
    columns:
      - name: dbt_valid_from
        description: The timestamp when the record became valid.
      - name: dbt_valid_to
        description: >
          The timestamp when the record ceased to be valid. For current records,
          this is either `NULL` or the value specified in `dbt_valid_to_current`
          (like `'9999-12-31'`).

----------------------------------------

TITLE: Executing dbt Run
DESCRIPTION: Build the example models in the project using the run command.

LANGUAGE: shell
CODE:
dbt run

----------------------------------------

TITLE: Describing Top-level Keys of dbt Manifest JSON
DESCRIPTION: This snippet outlines the main sections of the manifest.json file, including metadata, nodes, sources, metrics, exposures, groups, macros, docs, parent_map, child_map, group_map, selectors, and disabled resources.

LANGUAGE: json
CODE:
{
  "metadata": {},
  "nodes": {},
  "sources": {},
  "metrics": {},
  "exposures": {},
  "groups": {},
  "macros": {},
  "docs": {},
  "parent_map": {},
  "child_map": {},
  "group_map": {},
  "selectors": {},
  "disabled": []
}

----------------------------------------

TITLE: String Manipulation Macros in dbt
DESCRIPTION: Examples of string operations like concat, hash, length and position

LANGUAGE: sql
CODE:
{{ dbt.concat(["column_1", "column_2"]) }}
{{ dbt.hash("column") }}
{{ dbt.length("column") }}

----------------------------------------

TITLE: Configuring Materialize Connection Profile in dbt
DESCRIPTION: Configuration settings for connecting dbt to a Materialize instance. Includes essential parameters like host, port, authentication credentials, database name, cluster settings, and connection timeouts.

LANGUAGE: yaml
CODE:
materialize:
  target: dev
  outputs:
    dev:
      type: materialize
      host: [host]
      port: [port]
      user: [user@domain.com]
      pass: [password]
      dbname: [database]
      cluster: [cluster] # default 'default'
      schema: [dbt schema]
      sslmode: require
      keepalives_idle: 0 # default: 0, indicating the system default
      connect_timeout: 10 # default: 10 seconds
      retries: 1 # default: 1, retry on error/timeout when opening connections

----------------------------------------

TITLE: Installing ODBC Headers on Debian/Ubuntu
DESCRIPTION: Command to install required ODBC header files on Debian/Ubuntu systems before setting up SQL Server connection.

LANGUAGE: bash
CODE:
sudo apt install unixodbc-dev

----------------------------------------

TITLE: Handling DBT Adapter Response Dictionary
DESCRIPTION: The adapter_response dictionary contains metadata returned from database operations in dbt. It includes fields like 'code' for success status, 'rows_affected' for number of modified rows, and 'bytes_processed' for data processing metrics. For unsupported row counts or view creation, rows_affected returns -1.

LANGUAGE: python
CODE:
adapter_response = {
    "code": "success",
    "rows_affected": -1,  # -1 for operations where row count isn't applicable
    "bytes_processed": 1000
}

----------------------------------------

TITLE: Retrieving Snowflake OAuth Client Credentials
DESCRIPTION: SQL query to retrieve the OAuth client ID and client secret from Snowflake for configuring the dbt Cloud connection.

LANGUAGE: sql
CODE:
with

integration_secrets as (
  select parse_json(system$show_oauth_client_secrets('DBT_CLOUD')) as secrets
)

select
  secrets:"OAUTH_CLIENT_ID"::string     as client_id,
  secrets:"OAUTH_CLIENT_SECRET"::string as client_secret
from
  integration_secrets;

----------------------------------------

TITLE: dbt Compilation Context Variable
DESCRIPTION: Example of a dbt context variable used during model compilation

LANGUAGE: sql
CODE:
{{ this }}

----------------------------------------

TITLE: Logging Registered Adapter Version
DESCRIPTION: This code snippet shows an example log message indicating the version of a registered adapter (Snowflake in this case) that is being invoked by dbt.

LANGUAGE: plaintext
CODE:
[0m13:13:48.572182 [info ] [MainThread]: Registered adapter: snowflake=1.9.0

----------------------------------------

TITLE: Invalid Snapshot Configuration Pre-dbt 1.4
DESCRIPTION: Example of incorrect snapshot configuration prior to dbt version 1.4 where snapshot was incorrectly materialized as a table instead of a snapshot.

LANGUAGE: sql
CODE:
{% snapshot snappy %}
  {{ config(materialized = 'table', ...) }}
  ...
{% endsnapshot %}

----------------------------------------

TITLE: Configuring dbt-spark Authentication Profile
DESCRIPTION: Example configuration for dbt-spark authentication in profiles.yml showing the required ODBC driver setup and connection parameters.

LANGUAGE: yaml
CODE:
your_profile_name:
  target: dev
  outputs:
    dev:
      type: spark
      method: odbc
      driver: '/opt/simba/spark/lib/64/libsparkodbc_sb64.so'
      schema: my_schema
      host: dbc-l33t-nwb.cloud.databricks.com
      endpoint: 8657cad335ae63e3
      token: [my_secret_token]

----------------------------------------

TITLE: Compile Results JSON Output
DESCRIPTION: Example JSON output from dbt compile command showing execution details and compiled SQL.

LANGUAGE: json
CODE:
{
      "status": "success",
      "timing": [
        {
          "name": "compile",
          "started_at": "2023-10-12T16:35:28.510434Z",
          "completed_at": "2023-10-12T16:35:28.519086Z"
        },
        {
          "name": "execute",
          "started_at": "2023-10-12T16:35:28.521633Z",
          "completed_at": "2023-10-12T16:35:28.521641Z"
        }
      ],
      "thread_id": "Thread-2",
      "execution_time": 0.0408780574798584,
      "adapter_response": {},
      "message": null,
      "failures": null,
      "unique_id": "model.my_project.my_model",
      "compiled": true,
      "compiled_code": "select now() as created_at",
      "relation_name": "\"postgres\".\"dbt_dbeatty\".\"my_model\""
    }

----------------------------------------

TITLE: Versioned ref Function Examples
DESCRIPTION: Shows how to reference specific versions of models using the ref function.

LANGUAGE: sql
CODE:
select * from {{ ref('model_name', version=1) }}

LANGUAGE: sql
CODE:
select * from {{ ref('model_name') }}

----------------------------------------

TITLE: Configuring a materialized view in SQL
DESCRIPTION: Example of configuring a materialized view in a dbt SQL model file

LANGUAGE: sql
CODE:
{{ config(
   materialized = 'materialized_view'
 ) }}

----------------------------------------

TITLE: Making Predictions with Layer ML Model
DESCRIPTION: SQL query example showing how to use Layer's predict function to make predictions using a pre-trained ML model within dbt.

LANGUAGE: sql
CODE:
SELECT
    id,
    layer.predict("layer/clothing/models/objectdetection", ARRAY[image])
FROM
    {{ ref("products") }}

----------------------------------------

TITLE: Using target context in on-run-start and on-run-end hooks in dbt 0.13.0
DESCRIPTION: Example of using the target context in on-run-start and on-run-end hooks, as the special Jinja variable {{this}} is no longer implemented for these hooks in dbt 0.13.0.

LANGUAGE: jinja
CODE:
{{ target }}

----------------------------------------

TITLE: Configuring OAuth via gcloud in profiles.yml
DESCRIPTION: Configuration for connecting to BigQuery using OAuth authentication via gcloud. Requires local OAuth setup with gcloud CLI.

LANGUAGE: yaml
CODE:
my-bigquery-db:
  target: dev
  outputs:
    dev:
      type: bigquery
      method: oauth
      project: GCP_PROJECT_ID
      dataset: DBT_DATASET_NAME
      threads: 4

----------------------------------------

TITLE: Creating Users and Assigning Roles
DESCRIPTION: Creates users for different purposes (ETL, transformation, reporting) and assigns appropriate roles and default warehouses.

LANGUAGE: sql
CODE:
create user stitch_user
    password = '_generate_this_'
    default_warehouse = loading
    default_role = loader; 

create user claire
    password = '_generate_this_'
    default_warehouse = transforming
    default_role = transformer
    must_change_password = true;

create user dbt_cloud_user
    password = '_generate_this_'
    default_warehouse = transforming
    default_role = transformer;

create user looker_user
    password = '_generate_this_'
    default_warehouse = reporting
    default_role = reporter;

grant role loader to user stitch_user;
grant role transformer to user dbt_cloud_user;
grant role transformer to user claire;
grant role reporter to user looker_user;

----------------------------------------

TITLE: Defining Customer Dimension Model with Email Validation
DESCRIPTION: SQL model that creates a dimension table for customers with email validation logic using regex pattern matching and domain validation.

LANGUAGE: sql
CODE:
with customers as (
    select * from {{ ref('stg_customers') }}
),

accepted_email_domains as (
    select * from {{ ref('top_level_email_domains') }}
),
	
check_valid_emails as (
    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customers.email,
	      coalesce (regexp_like(
            customers.email, '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$'
        )
        = true
        and accepted_email_domains.tld is not null,
        false) as is_valid_email_address
    from customers
		left join accepted_email_domains
        on customers.email_top_level_domain = lower(accepted_email_domains.tld)
)

select * from check_valid_emails

----------------------------------------

TITLE: Defining Customer Dimension Model with Email Validation
DESCRIPTION: SQL model that creates a dimension table for customers with email validation logic using regex pattern matching and domain validation.

LANGUAGE: sql
CODE:
with customers as (
    select * from {{ ref('stg_customers') }}
),

accepted_email_domains as (
    select * from {{ ref('top_level_email_domains') }}
),
	
check_valid_emails as (
    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customers.email,
	      coalesce (regexp_like(
            customers.email, '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$'
        )
        = true
        and accepted_email_domains.tld is not null,
        false) as is_valid_email_address
    from customers
		left join accepted_email_domains
        on customers.email_top_level_domain = lower(accepted_email_domains.tld)
)

select * from check_valid_emails

----------------------------------------

TITLE: Running Saved Queries Downstream of a Model in Production
DESCRIPTION: Use the dbt build command with the --select option to run all saved queries downstream of a specific model in a production environment. This ensures that exports are refreshed after the model is built.

LANGUAGE: bash
CODE:
dbt build --select orders+

----------------------------------------

TITLE: Triggering dbt Cloud Job with Azure Data Factory
DESCRIPTION: Steps to trigger a dbt Cloud job using the dbt API in Azure Data Factory. This integration allows for seamless data transformation after ingestion jobs in ADF.

LANGUAGE: bash
CODE:
# No actual code provided, but steps are outlined for ADF integration

----------------------------------------

TITLE: Database Connection Profile Configuration
DESCRIPTION: Example dbt profiles.yml configuration for CI environment using PostgreSQL. Demonstrates proper credential management using environment variables.

LANGUAGE: yaml
CODE:
‚Äã‚Äãjaffle_shop:
  target: ci
  outputs:
    ci:
      type: postgres
      host: <your_host>
      user: <user>
      password: "{{ env_var('DB_PASSWORD') }}"
      port: 5432
      dbname: <database>
      schema: ci
      threads: 4

----------------------------------------

TITLE: Overriding dbt_utils.star Macro in dbt Unit Test
DESCRIPTION: Example demonstrating how to override the dbt_utils.star macro by explicitly setting column lists, necessary because the star macro requires a relation object.

LANGUAGE: yaml
CODE:
unit_tests:
    - name: my_other_unit_test
      model: my_model_that_uses_star
      overrides:
        macros:
          # explicity set star to relevant list of columns
          dbt_utils.star: col_a,col_b,col_c 
      ...

----------------------------------------

TITLE: Implementing Basic String Literal Macro in dbt
DESCRIPTION: A simple dbt macro that creates a string literal from input text

LANGUAGE: sql
CODE:
{% macro to_literal(text) %}

    '{{- text -}}'

{% endmacro %}

----------------------------------------

TITLE: Using Help Flag with dbt Environment Command
DESCRIPTION: Commands to access help documentation for the dbt environment command and its subcommands using --help or -h flags.

LANGUAGE: shell
CODE:
dbt environment [command] --help

LANGUAGE: shell
CODE:
dbt environment [command] -h

----------------------------------------

TITLE: Using Returned Data in dbt Model
DESCRIPTION: Example of using the returned list from get_data() macro in a SQL model. Demonstrates iterating over the returned list to create a comma-separated list of values in the SQL query.

LANGUAGE: sql
CODE:
select
  -- getdata() returns a list!
  {% for i in get_data() %}
    {{ i }}
    {%- if not loop.last %},{% endif -%}
  {% endfor %}

----------------------------------------

TITLE: Configuring Infer Profile in dbt
DESCRIPTION: YAML configuration template for setting up the Infer connection profile in dbt's profiles.yml file, including API endpoint, credentials, and data warehouse configuration

LANGUAGE: yaml
CODE:
<profile-name>:
  target: <target-name>
  outputs:
    <target-name>:
      type: infer
      url: "<infer-api-endpoint>"
      username: "<infer-api-username>"
      apikey: "<infer-apikey>"
      data_config:
        [configuration for your underlying data warehouse]

----------------------------------------

TITLE: Running dbt Docker Container with Project Mount
DESCRIPTION: This command runs a dbt Docker container, mounting the local project directory and profiles.yml file. It uses host networking and executes the 'ls' command in the container.

LANGUAGE: bash
CODE:
docker run \
--network=host \
--mount type=bind,source=path/to/project,target=/usr/app \
--mount type=bind,source=path/to/profiles.yml,target=/root/.dbt/profiles.yml \
<dbt_image_name> \
ls

----------------------------------------

TITLE: Comment Syntax Examples in dbt Cloud IDE
DESCRIPTION: Shows the different comment syntaxes used for different file types in the IDE, including Jinja for SQL files and standard syntax for Markdown files.

LANGUAGE: text
CODE:
({# #})

LANGUAGE: text
CODE:
(/* */)

LANGUAGE: text
CODE:
(<!-- -->)

----------------------------------------

TITLE: Comment Syntax Examples in dbt Cloud IDE
DESCRIPTION: Shows the different comment syntaxes used for different file types in the IDE, including Jinja for SQL files and standard syntax for Markdown files.

LANGUAGE: text
CODE:
({# #})

LANGUAGE: text
CODE:
(/* */)

LANGUAGE: text
CODE:
(<!-- -->)

----------------------------------------

TITLE: Defining Notification Groups in YAML
DESCRIPTION: Creates groups with owner information for receiving model-level notifications. Each group requires a name and owner with an email address. Additional custom properties can be included.

LANGUAGE: yaml
CODE:
version: 2

groups:
  - name: finance
    owner:
      name: "Finance team"
      email: finance@dbtlabs.com
      favorite_food: donuts

  - name: marketing
    owner:
      name: "Marketing team"
      email: marketing@dbtlabs.com
      favorite_food: jaffles

  - name: docs
    owner:
      name: "Documentation team"
      email: docs@dbtlabs.com
      favorite_food: pizza

----------------------------------------

TITLE: Installing MetricFlow for dbt Core
DESCRIPTION: Commands to install MetricFlow from PyPI for use with dbt Core. Requires creating a virtual environment and installing MetricFlow with the appropriate adapter.

LANGUAGE: bash
CODE:
python -m venv venv
pip install dbt-metricflow
python -m pip install "dbt-metricflow[your_adapter_name]"

----------------------------------------

TITLE: Configuring Custom Seed Directory in dbt_project.yml
DESCRIPTION: This code snippet demonstrates how to update the seed-paths configuration in the dbt_project.yml file to specify a custom directory for seed files. In this example, the seed files will be located in a directory named 'custom_seeds' instead of the default 'seeds' directory.

LANGUAGE: yaml
CODE:
seed-paths: ["custom_seeds"]

----------------------------------------

TITLE: Configuring Package Dependencies
DESCRIPTION: Example of how to specify package dependencies in packages.yml file with version constraints for installing from hub.getdbt.com.

LANGUAGE: yaml
CODE:
packages:
  - package: dbt-labs/dbt_utils
    version: ["‚Ä∫0.6.5", "0.7.0"]

----------------------------------------

TITLE: Configuring meta for macros in dbt_project.yml
DESCRIPTION: Demonstrates how to set meta properties for macros in the dbt_project.yml file. Meta is defined as a dictionary under the macros configuration block.

LANGUAGE: yaml
CODE:
macros:
  [<resource-path>]:
    +meta: {<dictionary>}

----------------------------------------

TITLE: DATEDIFF using dbt Macro
DESCRIPTION: Example of using dbt's cross-database DATEDIFF macro with the jaffle shop dataset to calculate the difference between order dates and a specific date. This macro automatically handles syntax differences across different data warehouses.

LANGUAGE: sql
CODE:
select
	*,
	{{ datediff("order_date", "'2022-06-09'", "day") }}
from {{ ref('orders') }}

----------------------------------------

TITLE: Rendering Identifiers Without Database in dbt (Jinja)
DESCRIPTION: This snippet demonstrates how to modify the ref macro to render only the schema and object identifier, without the database reference. This is useful for Snowflake warehouses when changing the database name post-build.

LANGUAGE: jinja
CODE:
  -- render identifiers without a database
  {% do return(rel.include(database=false)) %}

----------------------------------------

TITLE: Configuring Snapshot with Check Strategy
DESCRIPTION: Example of configuring a snapshot using the check strategy. This strategy is useful for tables which do not have a reliable updated_at column.

LANGUAGE: sql
CODE:
{% snapshot orders_snapshot_check %}

    {{
        config(
          strategy='check',
          unique_key='id',
          check_cols=['status', 'is_cancelled'],
        )
    }}

    select * from {{ source('jaffle_shop', 'orders') }}

{% endsnapshot %}

----------------------------------------

TITLE: Configuring a streaming table in SQL
DESCRIPTION: Example of configuring a streaming table in a dbt SQL model file

LANGUAGE: sql
CODE:
{{ config(
   materialized = 'streaming_table'
 ) }}

----------------------------------------

TITLE: Configuring IBM Netezza Profile in dbt
DESCRIPTION: Example configuration for connecting dbt to IBM Netezza instances using profiles.yml. Includes essential connection parameters like user, password, host, database, schema, and port settings.

LANGUAGE: yaml
CODE:
my_project:
  outputs:
    dev:
      type: netezza
      user: [user]
      password: [password]
      host: [hostname]
      database: [catalog name]
      schema: [schema name]
      port: 5480
      threads: [1 or more]

  target: dev

----------------------------------------

TITLE: AWS Principal ARN Configuration
DESCRIPTION: AWS IAM principal ARN required for granting dbt AWS account access to the VPC Endpoint Service.

LANGUAGE: plaintext
CODE:
arn:aws:iam::346425330055:role/MTPL_Admin

----------------------------------------

TITLE: Configuring GitHub PR Template URL in dbt Cloud
DESCRIPTION: Example of a GitHub PR template URL configuration for dbt Cloud. It uses variables to dynamically set the source and destination branches.

LANGUAGE: plaintext
CODE:
https://github.com/<org>/<repo>/compare/{{destination}}..{{source}}

----------------------------------------

TITLE: Configuring Project-wide Model Groups
DESCRIPTION: Shows how to configure model groups at the project level using dbt_project.yml, allowing group assignment by directory structure.

LANGUAGE: yaml
CODE:
config-version: 2
name: "jaffle_shop"

[...]

models:
  jaffle_shop:
    staging:
      +group: data_engineering
    marts:
      sales:
        +group: finance
      campaigns:
        +group: marketing

----------------------------------------

TITLE: Referencing Aliased Models in dbt SQL
DESCRIPTION: This SQL snippet shows how to reference models that have custom aliases. It uses the ref function with the model's filename, regardless of any aliasing configurations.

LANGUAGE: sql
CODE:
-- Use the model's filename in ref's, regardless of any aliasing configs

select * from {{ ref('ga_sessions') }}
union all
select * from {{ ref('snowplow_sessions') }}

----------------------------------------

TITLE: Displaying dbt Project Additional Folders Structure
DESCRIPTION: Shows the structure of supplementary folders in a dbt project including analyses, seeds, macros, snapshots, and tests.

LANGUAGE: shell
CODE:
jaffle_shop
‚îú‚îÄ‚îÄ analyses
‚îú‚îÄ‚îÄ seeds
‚îÇ   ‚îî‚îÄ‚îÄ employees.csv
‚îú‚îÄ‚îÄ macros
‚îÇ   ‚îú‚îÄ‚îÄ _macros.yml
‚îÇ   ‚îî‚îÄ‚îÄ cents_to_dollars.sql
‚îú‚îÄ‚îÄ snapshots
‚îî‚îÄ‚îÄ tests
‚îî‚îÄ‚îÄ assert_positive_value_for_total_amount.sql

----------------------------------------

TITLE: Navigating to Project Directory in Shell
DESCRIPTION: Command to change the current directory to the cloned repository location.

LANGUAGE: shell
CODE:
cd ~/Documents/GitHub/dbt-cloud-webhooks-pagerduty

----------------------------------------

TITLE: Aliasing All Fields Using dbt_utils.star() Macro
DESCRIPTION: This example shows how to use the dbt_utils.star() macro with the 'relation_alias' argument to alias all fields in a model without explicitly renaming them. It demonstrates how to apply a prefix to all selected columns.

LANGUAGE: sql
CODE:
select
	{{ dbt_utils.star(from=ref('table_a'), relation_alias='my_new_alias') }}
from {{ ref('table_a') }}

----------------------------------------

TITLE: Processing dbt Cloud Webhooks with Python in Zapier
DESCRIPTION: Python script that validates webhook authenticity, retrieves run logs from dbt Cloud Admin API, and formats a summary message for Microsoft Teams. Handles both successful and failed job runs, including error message extraction and formatting.

LANGUAGE: python
CODE:
import hashlib
import hmac
import json
import re


auth_header = input_data['auth_header']
raw_body = input_data['raw_body']

# Access secret credentials
secret_store = StoreClient('YOUR_SECRET_HERE')
hook_secret = secret_store.get('DBT_WEBHOOK_KEY')
api_token = secret_store.get('DBT_CLOUD_SERVICE_TOKEN')

# Validate the webhook came from dbt Cloud
signature = hmac.new(hook_secret.encode('utf-8'), raw_body.encode('utf-8'), hashlib.sha256).hexdigest()

if signature != auth_header:
  raise Exception("Calculated signature doesn't match contents of the Authorization header. This webhook may not have been sent from dbt Cloud.")

full_body = json.loads(raw_body)
hook_data = full_body['data'] 

# Steps derived from these commands won't have their error details shown inline, as they're messy
commands_to_skip_logs = ['dbt source', 'dbt docs']

# When testing, you will want to hardcode run_id and account_id to IDs that exist; the sample webhook won't work. 
run_id = hook_data['runId']
account_id = full_body['accountId']

# Fetch run info from the dbt Cloud Admin API
url = f'https://YOUR_ACCESS_URL/api/v2/accounts/{account_id}/runs/{run_id}/?include_related=["run_steps"]'
headers = {'Authorization': f'Token {api_token}'}
run_data_response = requests.get(url, headers=headers)
run_data_response.raise_for_status()
run_data_results = run_data_response.json()['data']

# Overall run summary
outcome_message = f"""
**\[{hook_data['runStatus']} for Run #{run_id} on Job \"{hook_data['jobName']}\"]({run_data_results['href']})**


**Environment:** {hook_data['environmentName']} | **Trigger:** {hook_data['runReason']} | **Duration:** {run_data_results['duration_humanized']}

"""

# Step-specific summaries
for step in run_data_results['run_steps']:
  if step['status_humanized'] == 'Success':
    outcome_message += f"""
‚úÖ {step['name']} ({step['status_humanized']} in {step['duration_humanized']})
"""
  else:
    outcome_message += f"""
‚ùå {step['name']} ({step['status_humanized']} in {step['duration_humanized']})
"""
    show_logs = not any(cmd in step['name'] for cmd in commands_to_skip_logs)
    if show_logs:
      full_log = step['logs']
      # Remove timestamp and any colour tags
      full_log = re.sub('\x1b?\[[0-9]+m[0-9:]*', '', full_log)
    
      summary_start = re.search('(?:Completed with \d+ error.* and \d+ warnings?:|Database Error|Compilation Error|Runtime Error)', full_log)
    
      line_items = re.findall('(^.*(?:Failure|Error) in .*\n.*\n.*)', full_log, re.MULTILINE)
    
      if len(line_items) == 0:
        relevant_log = f'```{full_log[summary_start.start() if summary_start else 0:]}```'
      else:
        relevant_log = summary_start[0]
        for item in line_items:
          relevant_log += f'\n```\n{item.strip()}\n```\n'
      outcome_message += f"""
{relevant_log}
"""

# Zapier looks for the `output` dictionary for use in subsequent steps
output = {'outcome_message': outcome_message}

----------------------------------------

TITLE: Using DATE_TRUNC in Google BigQuery and Amazon Redshift
DESCRIPTION: Shows the syntax for using the DATE_TRUNC function in Google BigQuery and Amazon Redshift, where the date/time field is the first argument.

LANGUAGE: sql
CODE:
date_trunc(<date/time field>, <date part>)

----------------------------------------

TITLE: Conditional Logic Using flags Variable in dbt SQL
DESCRIPTION: Example showing how to use the flags variable to conditionally execute SQL statements based on CLI flags like FULL_REFRESH.

LANGUAGE: sql
CODE:
{% if flags.FULL_REFRESH %}
drop table ...
{% else %}
-- no-op
{% endif %}

----------------------------------------

TITLE: Complete Snapshot Configuration Example (v1.8 and earlier)
DESCRIPTION: Comprehensive example of a snapshot configuration in SQL format for dbt versions 1.8 and earlier.

LANGUAGE: sql
CODE:
{% snapshot orders_snapshot %}

    {{
        config(
          target_schema='snapshots',
          strategy='timestamp',
          unique_key='id',
          updated_at='updated_at',
          invalidate_hard_deletes=True,
        )
    }}

    select * from {{ source('jaffle_shop', 'orders') }}

{% endsnapshot %}

----------------------------------------

TITLE: dbt Glue Profile Configuration
DESCRIPTION: Example profiles.yml configuration for dbt Glue connection.

LANGUAGE: yaml
CODE:
type: glue
query-comment: This is a glue dbt example
role_arn: arn:aws:iam::1234567890:role/GlueInteractiveSessionRole
region: us-east-1
workers: 2
worker_type: G.1X
idle_timeout: 10
schema: "dbt_demo"
session_provisioning_timeout_in_seconds: 120
location: "s3://dbt_demo_bucket/dbt_demo_data"

----------------------------------------

TITLE: Defining Ratio Metrics in YAML
DESCRIPTION: Example of defining ratio metrics in a YAML file. It demonstrates how to create metrics involving a numerator and denominator, with optional filters.

LANGUAGE: yaml
CODE:
metrics:
  - name: cancellation_rate
    type: ratio
    label: Cancellation rate
    type_params:
      numerator: cancellations
      denominator: transaction_amount
    filter: |   
      {{ Dimension('customer__country') }} = 'MX'
  - name: enterprise_cancellation_rate
    type: ratio
    type_params:
      numerator:
        name: cancellations
        filter: {{ Dimension('company__tier') }} = 'enterprise'  
      denominator: transaction_amount
    filter: | 
      {{ Dimension('customer__country') }} = 'MX' 

----------------------------------------

TITLE: Refactored Customer Model
DESCRIPTION: Updated customer model SQL that references the staging models using dbt refs.

LANGUAGE: sql
CODE:
with customers as (

    select * from {{ ref('stg_customers') }}

),

orders as (

    select * from {{ ref('stg_orders') }}

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final

----------------------------------------

TITLE: Documentation Job Setup Example
DESCRIPTION: Example showing the command for generating documentation in a dbt job. The dbt docs generate command can be used to create documentation artifacts.

LANGUAGE: sql
CODE:
dbt docs generate

----------------------------------------

TITLE: Displaying dbt Marts Directory Structure
DESCRIPTION: Shows the recommended folder structure for organizing marts by department including finance and marketing models.

LANGUAGE: shell
CODE:
models/marts
‚îú‚îÄ‚îÄ finance
‚îÇ   ‚îú‚îÄ‚îÄ _finance__models.yml
‚îÇ   ‚îú‚îÄ‚îÄ orders.sql
‚îÇ   ‚îî‚îÄ‚îÄ payments.sql
‚îî‚îÄ‚îÄ marketing
    ‚îú‚îÄ‚îÄ _marketing__models.yml
    ‚îî‚îÄ‚îÄ customers.sql

----------------------------------------

TITLE: Basic SQL ORDER BY Syntax
DESCRIPTION: Demonstrates the basic syntax for using ORDER BY clause in SQL queries. The clause comes after FROM, WHERE, and GROUP BY statements to sort results by specified fields in ascending or descending order.

LANGUAGE: sql
CODE:
select
	column_1,
	column_2
from source_table
order by <field(s)> <asc/desc>

----------------------------------------

TITLE: GitLab CI Linting Configuration
DESCRIPTION: GitLab CI pipeline configuration for running SQLFluff linting on dbt models.

LANGUAGE: yaml
CODE:
image: python:3.9

stages:
  - pre-build

lint-project:
  stage: pre-build
  rules:
    - if: $CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH != 'main'
  script:
    - python -m pip install sqlfluff
    - sqlfluff lint models --dialect snowflake

----------------------------------------

TITLE: Installing dbt-core from Source (v1.8+)
DESCRIPTION: Steps to clone and install dbt-core from GitHub source code for version 1.8 and above. Includes installing required dependencies via pip.

LANGUAGE: shell
CODE:
git clone https://github.com/dbt-labs/dbt-core.git
cd dbt-core
python -m pip install -r requirements.txt

----------------------------------------

TITLE: Basic EXTRACT Function Syntax in SQL
DESCRIPTION: Demonstrates the basic syntax of the EXTRACT function for extracting date parts from a date/time field. Shows the standard format using the 'from' keyword.

LANGUAGE: yaml
CODE:
extract(<date_part> from <date/time field>)

----------------------------------------

TITLE: Accessing Groups in DBT Graph
DESCRIPTION: Example showing how to access group information using a custom macro.

LANGUAGE: sql
CODE:
{% macro get_group_owner_for(group_name) %}

  {% set groups = graph.groups.values() %}
  
  {% set owner = (groups | selectattr('owner', 'equalto', group_name) | list).pop() %}

  {{ return(owner) }}

{% endmacro %}

----------------------------------------

TITLE: Configuring Snapshot Materialization (Pre-1.9)
DESCRIPTION: Demonstrates the configuration of a ClickHouse snapshot materialization for dbt versions before 1.9, specifying target_schema, unique_key, strategy, and updated_at options.

LANGUAGE: jinja
CODE:
{{
   config(
     target_schema = "<schema_name>",
     unique_key = "<column-name>",
     strategy = "<strategy>",
     updated_at = "<unpdated_at_column-name>",
   )
}}

----------------------------------------

TITLE: Using dbt snapshot Command with CLI Options
DESCRIPTION: Command line interface help output for the dbt snapshot command showing available options including select/exclude patterns, profile configuration, and threading options.

LANGUAGE: bash
CODE:
$ dbt snapshot --help
usage: dbt snapshot [-h] [--profiles-dir PROFILES_DIR]
                                     [--profile PROFILE] [--target TARGET]
                                     [--vars VARS] [--bypass-cache]
                                     [--threads THREADS]
                                     [--select SELECTOR [SELECTOR ...]]
                                     [--exclude EXCLUDE [EXCLUDE ...]]

optional arguments:
  --select SELECTOR [SELECTOR ...]
                        Specify the snapshots to include in the run.
  --exclude EXCLUDE [EXCLUDE ...]
                        Specify the snapshots to exclude in the run.

----------------------------------------

TITLE: Basic Microsoft Entra ID Authentication Profile
DESCRIPTION: YAML configuration for Microsoft Entra ID username and password authentication setup in profiles.yml

LANGUAGE: yaml
CODE:
your_profile_name:
  target: dev
  outputs:
    dev:
      type: fabric
      driver: 'ODBC Driver 18 for SQL Server'
      server: hostname or IP of your server
      port: 1433
      database: exampledb
      schema: schema_name
      authentication: ActiveDirectoryPassword
      user: bill.gates@microsoft.com
      password: iheartopensource

----------------------------------------

TITLE: Percentile Aggregation Example
DESCRIPTION: Example of configuring a percentile-based measure with specific aggregation parameters for calculating continuous or discrete percentiles.

LANGUAGE: yaml
CODE:
name: p99_transaction_value
description: The 99th percentile transaction value
expr: transaction_amount_usd
agg: percentile
agg_params:
  percentile: .99
  use_discrete_percentile: False

----------------------------------------

TITLE: Starting Local Airflow Deployment with Astro CLI
DESCRIPTION: Command to start a local Airflow deployment using the Astro CLI. This sets up a Docker container with all necessary components for running Airflow.

LANGUAGE: bash
CODE:
astro dev start

----------------------------------------

TITLE: Complete Semantic Model YAML Example
DESCRIPTION: A full example of a semantic model definition in YAML, including entities, dimensions, and measures.

LANGUAGE: yaml
CODE:
semantic_models:
  - name: orders
    defaults:
      agg_time_dimension: ordered_at
    description: |
      Order fact table. This table is at the order grain with one row per order.

    model: ref('stg_orders')

    entities:
      - name: order_id
        type: primary
      - name: location
        type: foreign
        expr: location_id
      - name: customer
        type: foreign
        expr: customer_id

    dimensions:
      - name: ordered_at
        expr: date_trunc('day', ordered_at)
        # use date_trunc(ordered_at, DAY) if using BigQuery
        type: time
        type_params:
          time_granularity: day
      - name: is_large_order
        type: categorical
        expr: case when order_total > 50 then true else false end

    measures:
      - name: order_total
        description: The total revenue for each order.
        agg: sum
      - name: order_count
        description: The count of individual orders.
        expr: 1
        agg: sum
      - name: tax_paid
        description: The total tax paid on each order.
        agg: sum

----------------------------------------

TITLE: Excluding Specific Models in dbt Compare Command
DESCRIPTION: This SQL command demonstrates how to exclude a large model (fct_orders) from the comparison when using the dbt compare feature in Advanced CI. This can help reduce job time and resource consumption.

LANGUAGE: sql
CODE:
--select state:modified --exclude fct_orders

----------------------------------------

TITLE: Querying Metrics with MetricFlow
DESCRIPTION: Example command showing how to query metrics using the MetricFlow CLI with revenue metric and metric_time dimension.

LANGUAGE: bash
CODE:
mf query --metrics revenue --group-by metric_time

----------------------------------------

TITLE: Applying LOWER Function to Customer Data in SQL
DESCRIPTION: Shows how to use the LOWER function in a SELECT statement to convert first and last names to lowercase in a customers table.

LANGUAGE: sql
CODE:
select 
	customer_id,
	lower(first_name) as first_name,
	lower(last_name) as last_name
from {{ ref('customers') }}

----------------------------------------

TITLE: Snowflake Schema Listing - SQL
DESCRIPTION: Modified snowflake__list_schemas macro returning Agate dataframe with name column for schema listing.

LANGUAGE: sql
CODE:
select
  schema_name as "name"
from information_schema.schemata

----------------------------------------

TITLE: Filtering Current Records in dbt SQL
DESCRIPTION: Using a Jinja if statement to optionally filter for only current records based on a global variable.

LANGUAGE: SQL
CODE:
{% if var("current_records_only") %}

where valid_to = cast('{{ var("future_proof_date") }}' as timestamp)

{% endif %}

----------------------------------------

TITLE: Safeguarding access to batch properties in dbt
DESCRIPTION: This snippet shows how to safely access batch properties of the 'model' object during microbatch execution, using conditional logic to check if the batch property is populated.

LANGUAGE: jinja
CODE:
{% if model.batch %}
  {{ log(model.batch.id) }}  # Log the batch ID #
  {{ log(model.batch.event_time_start) }}  # Log the start time of the batch #
  {{ log(model.batch.event_time_end) }}  # Log the end time of the batch #
{% endif %}

----------------------------------------

TITLE: Configuring SQL Materialized Views in dbt
DESCRIPTION: Demonstrates the configuration for SQL materialized views as a dbt model using the 'materializedview' materialization type.

LANGUAGE: sql
CODE:
{{ config(  materialized='materializedview',
            sync=True|False,
            options={'option_name': 'option_value'}
        )
}}
SELECT ...
FROM {{ ref(<model>) }}
WHERE ...
GROUP BY ...

----------------------------------------

TITLE: Installing ODBC Header Files on Debian/Ubuntu
DESCRIPTION: Command to install ODBC header files, which are required before installing the dbt-synapse adapter on Debian/Ubuntu systems.

LANGUAGE: bash
CODE:
sudo apt install unixodbc-dev

----------------------------------------

TITLE: Configuring Database Overrides in dbt_project.yml for Jaffle Shop
DESCRIPTION: This YAML configuration sets all models in the jaffle_shop project to be built into a database called jaffle_shop. It also includes a commented option for BigQuery users.

LANGUAGE: yaml
CODE:
name: jaffle_shop

models:
  jaffle_shop:
    +database: jaffle_shop

    # For BigQuery users:
    # project: jaffle_shop

----------------------------------------

TITLE: Logging into fly.io in Shell
DESCRIPTION: Command to log in to fly.io using the flyctl CLI tool if not already authenticated.

LANGUAGE: shell
CODE:
flyctl auth login

----------------------------------------

TITLE: Defining Dimensions in Semantic Model YAML
DESCRIPTION: Example of how to define dimensions in a semantic model, including time and categorical dimensions.

LANGUAGE: yaml
CODE:
dimensions:
  - name: ordered_at
    expr: date_trunc('day', ordered_at)
    type: time
    type_params:
      time_granularity: day
  - name: is_large_order
    type: categorical
    expr: case when order_total > 50 then true else false end

----------------------------------------

TITLE: Error Message: Could Not Find Package in dbt
DESCRIPTION: Example shell error output when dbt fails to find macros in a package specified in dispatch search_order configuration. This occurs when a referenced package contains no viable macro candidates for dispatching.

LANGUAGE: shell
CODE:
Compilation Error
  In dispatch: Could not find package 'my_project'

----------------------------------------

TITLE: Semantic Model Definition
DESCRIPTION: YAML configuration for defining semantic models with entities, dimensions, measures and metrics

LANGUAGE: yaml
CODE:
semantic_models:
  - name: orders
    defaults:
      agg_time_dimension: order_date
    description: |
      Order fact table. This table's grain is one row per order.
    model: ref('fct_orders')
    entities: 
      - name: order_id
        type: primary
      - name: customer
        expr: customer_id
        type: foreign
    dimensions:   
      - name: order_date
        type: time
        type_params:
          time_granularity: day
    measures:   
      - name: order_total
        description: The total amount for each order including taxes.
        agg: sum
        expr: amount

----------------------------------------

TITLE: Specifying dbt Project Configuration Version in YAML
DESCRIPTION: This snippet demonstrates how to set the config-version in a dbt_project.yml file. It specifies the project as using the v2 structure, which is the default for current dbt versions.

LANGUAGE: yml
CODE:
config-version: 2

----------------------------------------

TITLE: Configuring Key-Based Distribution in dbt
DESCRIPTION: Configuration for implementing key-based distribution in a dbt model using person_id as the distribution key. Used for co-locating related data on the same node.

LANGUAGE: python
CODE:
{{ config(materialized='table', dist='person_id') }}

----------------------------------------

TITLE: Database Maintenance with run_query
DESCRIPTION: Example of using run_query to perform database maintenance operations like VACUUM on a table.

LANGUAGE: sql
CODE:
{% macro run_vacuum(table) %}

  {% set query %}
    vacuum table {{ table }}
  {% endset %}

  {% do run_query(query) %}
{% endmacro %}

----------------------------------------

TITLE: Configuring Rowstore Storage Type in SingleStore
DESCRIPTION: Demonstrates how to configure a table materialization to use in-memory rowstore storage type in SingleStore.

LANGUAGE: sql
CODE:
{{ config(materialized='table', storage_type='rowstore') }}

select ...

----------------------------------------

TITLE: Example SQL Query with Unquoted Identifiers
DESCRIPTION: This SQL snippet demonstrates how dbt creates relations without quotes when quoting is set to false. It shows a create table statement with unquoted database, schema, and table names.

LANGUAGE: sql
CODE:
create table analytics.dbt_alice.dim_customers

----------------------------------------

TITLE: Defining Groups and Access Modifiers in YAML
DESCRIPTION: Shows how to define groups with owners and set access modifiers for specific models in a YAML configuration file.

LANGUAGE: yaml
CODE:
groups:
  - name: customer_success
    owner:
      name: Customer Success Team
      email: cx@jaffle.shop

models:
  - name: dim_customers
    group: customer_success
    access: public
    
  - name: int_customer_history_rollup
    group: customer_success
    access: private
    
  - name: stg_customer__survey_results
    group: customer_success
    access: protected

----------------------------------------

TITLE: Testing Column Expression for Uniqueness in dbt
DESCRIPTION: Tests the uniqueness of a concatenated expression of country_code and order_id directly in the model configuration.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: orders
    tests:
      - unique:
          column_name: "(country_code || '-' || order_id)"

----------------------------------------

TITLE: Signing Up for fly.io in Shell
DESCRIPTION: Command to sign up for a fly.io account using the flyctl CLI tool.

LANGUAGE: shell
CODE:
flyctl auth signup

----------------------------------------

TITLE: Invoking grant_select Macro with dbt run-operation
DESCRIPTION: This example shows how to use the dbt run-operation command to invoke the grant_select macro, passing a 'role' argument with the value 'reporter'.

LANGUAGE: bash
CODE:
$ dbt run-operation grant_select --args '{role: reporter}'

----------------------------------------

TITLE: Converting Sequential INSERTs using UNION ALL
DESCRIPTION: Shows how to combine multiple INSERT statements into a single dbt model using UNION ALL

LANGUAGE: sql
CODE:
CREATE TABLE all_customers

INSERT INTO all_customers SELECT * FROM us_customers

INSERT INTO all_customers SELECT * FROM eu_customers

LANGUAGE: sql
CODE:
SELECT * FROM {{ ref('us_customers') }}

UNION ALL

SELECT * FROM {{ ref('eu_customers') }}

----------------------------------------

TITLE: Using DATE_TRUNC in Snowflake and Databricks
DESCRIPTION: Demonstrates the syntax for using the DATE_TRUNC function in Snowflake and Databricks, where the date part is the first argument.

LANGUAGE: sql
CODE:
date_trunc(<date_part>, <date/time field>)

----------------------------------------

TITLE: Setting Global Variables in dbt YAML
DESCRIPTION: Example of setting global variables in the dbt_project.yml file for future-proofing dates and filtering current records.

LANGUAGE: YAML
CODE:
vars:
 future_proof_date: '9999-12-31'
 current_records_only: true

----------------------------------------

TITLE: Schema Naming Convention in dbt Cloud CI
DESCRIPTION: Shows the naming pattern for temporary schemas created during CI runs. The schema name follows the format dbt_cloud_pr_<job_id>_<pr_id>.

LANGUAGE: text
CODE:
dbt_cloud_pr_1862_1704

----------------------------------------

TITLE: Implementing Interactive Catalog Explorer Command
DESCRIPTION: Python code demonstrating how to create an interactive CLI application for exploring dbt catalog artifacts using Click and Pydantic.

LANGUAGE: python
CODE:
class CatalogExploreCommand(ClickBaseModel):
    """An inteactive application for exploring catalog artifacts."""
    file: Path = Field(default="catalog.json", description="Catalog file path.")
    title: str = Field(default="Data Catalog", description="ASCII art title for the app.")
    title_font: str = Field(default="rand-large", description="ASCII art title font")

    def get_catalog(self) -> Catalog:
        return Catalog.parse_file(self.file)

    def execute(self):
        import inquirer
        self.print_title()
        while True:
            node_type_options = [
                inquirer.List(
                    "node_type",
                    message="Select node type to explore",
                    choices=[node_type.value for node_type in NodeType],
                )
            ]
            node_type = NodeType(inquirer.prompt(node_type_options)["node_type"])
            self.explore(node_type=node_type)
            if not click.confirm("Explore another node type?"):
                break

----------------------------------------

TITLE: Checking Selected Node Status in dbt
DESCRIPTION: Demonstrates how to check if a specific model is included in the current selection using the selected_resources variable within a hook. The macro logs different messages based on whether model1 is selected or not.

LANGUAGE: sql
CODE:
/*
  Check if a given model is selected and trigger a different action, depending on the result
*/

{% if execute %}
  {% if 'model.my_project.model1' in selected_resources %}
  
    {% do log("model1 is included based on the current selection", info=true) %}
  
  {% else %}

    {% do log("model1 is not included based on the current selection", info=true) %}

  {% endif %}
{% endif %}

/*
  Example output when running the code in on-run-start 
  when doing `dbt build`, including all nodels
---------------------------------------------------------------
  model1 is included based on the current selection


  Example output when running the code in on-run-start 
  when doing `dbt run --select model2` 
---------------------------------------------------------------
  model1 is not included based on the current selection
*/

----------------------------------------

TITLE: Setting Query Band for Teradata in dbt Profiles
DESCRIPTION: Configures a query band at the profiles level, which can be used for session-wide settings and telemetry tracking.

LANGUAGE: yaml
CODE:
query_band: 'application=dbt;'

----------------------------------------

TITLE: Configuring SQLFluff with dbtonic Linting Rules
DESCRIPTION: This code snippet shows an example configuration for SQLFluff with dbt-specific (dbtonic) linting rules. It sets various options for SQL formatting, capitalization, aliasing, and other style preferences.

LANGUAGE: INI
CODE:
[sqlfluff]
templater = dbt
runaway_limit = 10
max_line_length = 80
indent_unit = space

[sqlfluff:indentation]
tab_space_size = 4

[sqlfluff:layout:type:comma]
spacing_before = touch
line_position = trailing

[sqlfluff:rules:capitalisation.keywords] 
capitalisation_policy = lower

[sqlfluff:rules:aliasing.table]
aliasing = explicit

[sqlfluff:rules:aliasing.column]
aliasing = explicit

[sqlfluff:rules:aliasing.expression]
allow_scalar = False

[sqlfluff:rules:capitalisation.identifiers]
extended_capitalisation_policy = lower

[sqlfluff:rules:capitalisation.functions]
capitalisation_policy = lower

[sqlfluff:rules:capitalisation.literals]
capitalisation_policy = lower

[sqlfluff:rules:ambiguous.column_references]
group_by_and_order_by_style = implicit

----------------------------------------

TITLE: dbt Seed Full Refresh Command
DESCRIPTION: Solution command for resolving seed column changes by using the --full-refresh flag, which drops and rebuilds the table structure.

LANGUAGE: shell
CODE:
dbt seed --full-refresh

----------------------------------------

TITLE: Bitbucket Pipelines configuration for running dbt Cloud job on merge
DESCRIPTION: Bitbucket Pipelines configuration to run a dbt Cloud job when code is merged to the main branch. Requires setting environment variables for dbt Cloud API access.

LANGUAGE: yaml
CODE:
image: python:3.11.1


pipelines:
  branches:
    'main': # override if your default branch doesn't run on a branch named "main"
      - step:
          name: 'Run dbt Cloud Job'
          script:
            - export DBT_URL="https://cloud.getdbt.com" # if you have a single-tenant deployment, adjust this accordingly
            - export DBT_JOB_CAUSE="Bitbucket Pipeline CI Job"
            - export DBT_ACCOUNT_ID=00000 # enter your account id here
            - export DBT_PROJECT_ID=00000 # enter your project id here
            - export DBT_PR_JOB_ID=00000 # enter your job id here
            - python python/run_and_monitor_dbt_job.py

----------------------------------------

TITLE: Configuring CrateDB Profile in dbt
DESCRIPTION: Example configuration for setting up a CrateDB target in dbt's profiles.yml file. Includes essential connection parameters like host, port, credentials, and schema settings. Note that CrateDB uses 'crate' as its only catalog and 'doc' as its default schema.

LANGUAGE: yaml
CODE:
cratedb_analytics:
  target: dev
  outputs:
    dev:
      type: cratedb
      host: [clustername].aks1.westeurope.azure.cratedb.net
      port: 5432
      user: [username]
      pass: [password]
      dbname: crate         # Do not change this value. CrateDB's only catalog is `crate`.
      schema: doc           # Define the schema name. CrateDB's default schema is `doc`.

----------------------------------------

TITLE: Implementing Incremental Logic in SQL Model
DESCRIPTION: Shows an example of how to implement incremental logic in a dbt SQL model. It checks if the model is incremental and not in full-refresh mode to determine whether to select all records or only new ones.

LANGUAGE: sql
CODE:
select * from all_events

-- if the table already exists and `--full-refresh` is
-- not set, then only add new records. otherwise, select
-- all records.
{% if is_incremental() %}
   where collector_tstamp > (
     select coalesce(max(max_tstamp), '0001-01-01') from {{ this }}
   )
{% endif %}

----------------------------------------

TITLE: Legacy iFrame Implementation
DESCRIPTION: Legacy job-based implementation for embedding data health tile using iFrame.

LANGUAGE: html
CODE:
<iframe src='https://metadata.YOUR_ACCESS_URL/exposure-tile?name=<exposure_name>&jobId=<job_id>&token=<metadata_only_token>' title='Exposure Status Tile'></iframe>

----------------------------------------

TITLE: DATEDIFF in Snowflake, Redshift, and Databricks
DESCRIPTION: Standard DATEDIFF function syntax for Snowflake, Amazon Redshift, and Databricks platforms. Takes three arguments: date part (unit), start date/time, and end date/time.

LANGUAGE: sql
CODE:
datediff(<date part>, <start date/time>, <end date/time>)

----------------------------------------

TITLE: Lambda Filter Macro in Jinja
DESCRIPTION: A Jinja macro that generates appropriate filtering logic based on materialization type for lambda views implementation.

LANGUAGE: jinja
CODE:
{% macro lambda_filter(column_name) %}
{% set materialized = config.require('materialized') %}
{% set filter_time = var(lambda_timestamp, run_started_at) %}
{% if materialized == 'view' %}
    where {{ column_name }} >= '{{ filter_time }}'
{% elif is_incremental() %}
    where {{ column_name }} >= (select max({{ column_name }}) from {{ this }})
    and {{ column_name }} < '{{ filter_time }}'
{% else %}
    where {{ column_name }} < '{{ filter_time }}'
{% endif %}
{% endmacro %}

----------------------------------------

TITLE: Role Masking Configuration for Postgres
DESCRIPTION: SQL commands to mask roles for dbt Cloud and Bitbucket users to ensure proper ownership of database objects.

LANGUAGE: sql
CODE:
alter role dbt_bitbucket set role role_prod;

alter role dbt_cloud set role role_prod;

----------------------------------------

TITLE: Running Tests on Specific Resource Types in dbt
DESCRIPTION: These commands show how to run tests on models with specific materializations, seeds, and snapshots in dbt.

LANGUAGE: bash
CODE:
# Run tests on all models with a particular materialization
dbt test --select "config.materialized:table"

# Run tests on all seeds, which use the 'seed' materialization
dbt test --select "config.materialized:seed"

# Run tests on all snapshots, which use the 'snapshot' materialization
dbt test --select "config.materialized:snapshot"

----------------------------------------

TITLE: Complete Source Freshness Configuration Example
DESCRIPTION: Comprehensive example showing source freshness configuration with multiple tables, custom thresholds, and filters.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: jaffle_shop
    database: raw

    freshness:
      warn_after: {count: 12, period: hour}
      error_after: {count: 24, period: hour}

    loaded_at_field: _etl_loaded_at

    tables:
      - name: customers

      - name: orders
        freshness:
          warn_after: {count: 6, period: hour}
          error_after: {count: 12, period: hour}
          filter: datediff('day', _etl_loaded_at, current_timestamp) < 2

      - name: product_skus
        freshness:

----------------------------------------

TITLE: Setting Extended Attributes for Snowflake MFA
DESCRIPTION: YAML configuration for enabling Snowflake MFA authentication in dbt Cloud environment settings.

LANGUAGE: yaml
CODE:
authenticator: username_password_mfa

----------------------------------------

TITLE: Running Models Downstream of Specific Source Table in dbt
DESCRIPTION: Command to run all models downstream of a specific source table using the source selector syntax. This allows for more granular control by specifying both the source and table name.

LANGUAGE: shell
CODE:
$ dbt run --select source:jaffle_shop.orders+

----------------------------------------

TITLE: Enabling Google Drive Access with gcloud for OAuth
DESCRIPTION: This command uses gcloud to authenticate and enable Google Drive access for the application default credentials, useful when OAuth is being used and Google Sheet access has been verified.

LANGUAGE: bash
CODE:
gcloud auth application-default login --disable-quota-project

----------------------------------------

TITLE: Configuring Greenplum Profile in YAML for dbt
DESCRIPTION: This YAML configuration sets up a Greenplum target in the profiles.yml file for dbt. It includes connection details such as host, user, password, port, database name, schema, and additional options like threads, keepalives_idle, connect_timeout, search_path, role, and sslmode.

LANGUAGE: yaml
CODE:
company-name:
  target: dev
  outputs:
    dev:
      type: greenplum
      host: [hostname]
      user: [username]
      password: [password]
      port: [port]
      dbname: [database name]
      schema: [dbt schema]
      threads: [1 or more]
      keepalives_idle: 0 # default 0, indicating the system default. See below
      connect_timeout: 10 # default 10 seconds
      search_path: [optional, override the default postgres search_path]
      role: [optional, set the role dbt assumes when executing queries]
      sslmode: [optional, set the sslmode used to connect to the database]

----------------------------------------

TITLE: Compiled SQL Query for Custom-Named Source in dbt
DESCRIPTION: This SQL snippet shows the compiled result of the dbt model query, demonstrating how the custom schema and identifier are used in the final SQL statement.

LANGUAGE: sql
CODE:
select * from raw.postgres_backend_public_schema.api_orders

----------------------------------------

TITLE: Configuring dbt sources in YAML
DESCRIPTION: Example of defining source data configurations in a dbt YAML file.

LANGUAGE: YAML
CODE:
sources:
  - name: source_name
    tables:
      - name: table_name

----------------------------------------

TITLE: Increment Sequence Macro in dbt
DESCRIPTION: A simple dbt macro that returns the next value from a sequence for the current model.

LANGUAGE: yaml
CODE:
{%- macro increment_sequence() -%}
  
  {{ this.name }}_seq.nextval

{%- endmacro -%}

----------------------------------------

TITLE: Displaying dbt Core Version Release Table in Markdown
DESCRIPTION: This markdown snippet creates a table showing dbt Core versions, their initial release dates, and support levels. It includes links to upgrade documentation and uses HTML for formatting.

LANGUAGE: markdown
CODE:
| dbt Core | Initial release | Support level and end date |
|:-------------------------------------------------------------:|:---------------:|:-------------------------------------:|
| [**v1.9**](/docs/dbt-versions/core-upgrade/upgrading-to-v1.9) | Dec 9, 2024 | <b> Active Support &mdash; Dec 8, 2025</b>|
| [**v1.8**](/docs/dbt-versions/core-upgrade/upgrading-to-v1.8) | May 9, 2024 | <b>Active Support &mdash; May 8, 2025</b>|
| [**v1.7**](/docs/dbt-versions/core-upgrade/upgrading-to-v1.7) | Nov 2, 2023 | <div align="left">**dbt Core and dbt Cloud Developer & Team customers:** End of Life <br /> **dbt Cloud Enterprise customers:** Critical Support until further notice <sup>1</sup></div> |
| [**v1.6**](/docs/dbt-versions/core-upgrade/upgrading-to-v1.6) | Jul 31, 2023 | End of Life ‚ö†Ô∏è |
| [**v1.5**](/docs/dbt-versions/core-upgrade/upgrading-to-v1.5) | Apr 27, 2023 | End of Life ‚ö†Ô∏è |
| [**v1.4**](/docs/dbt-versions/core-upgrade/Older%20versions/upgrading-to-v1.4) | Jan 25, 2023 | End of Life ‚ö†Ô∏è |
| [**v1.3**](/docs/dbt-versions/core-upgrade/Older%20versions/upgrading-to-v1.3) | Oct 12, 2022 | End of Life ‚ö†Ô∏è |
| [**v1.2**](/docs/dbt-versions/core-upgrade/Older%20versions/upgrading-to-v1.2) | Jul 26, 2022 | End of Life ‚ö†Ô∏è |
| [**v1.1**](/docs/dbt-versions/core-upgrade/Older%20versions/upgrading-to-v1.1) | Apr 28, 2022 | End of Life ‚ö†Ô∏è |
| [**v1.0**](/docs/dbt-versions/core-upgrade/Older%20versions/upgrading-to-v1.0) | Dec 3, 2021 | End of Life ‚ö†Ô∏è |
| **v0.X** ‚õîÔ∏è | (Various dates) | Deprecated ‚õîÔ∏è | Deprecated ‚õîÔ∏è |

----------------------------------------

TITLE: Specifying dbt Version Range in YAML Configuration
DESCRIPTION: Demonstrates how to set the require-dbt-version in the dbt_project.yml file. It shows the correct YAML syntax for specifying version ranges.

LANGUAGE: yaml
CODE:
require-dbt-version: version-range | [version-range]

----------------------------------------

TITLE: Defining Users Model in YAML
DESCRIPTION: This YAML file defines the users model, specifying the expected columns, their descriptions, and associated tests.

LANGUAGE: YAML
CODE:
---
version: 2

models:
  - name: users
    description: Lovely humans that use our app
    columns:
      - name: id
        description: INT The id of this user
        tests:
          - not_null
          - unique
      - name: email
        description: STRING User's contact email
        tests:
          - not_null
      - name: state
        description: STRING The current state of the user
        tests:
          - accepted_values:
             values:
               - "active"
               - "invited"
          - not_null

----------------------------------------

TITLE: Configuring Column Types in dbt_project.yml
DESCRIPTION: Demonstrates how to specify column types for a seed file named 'country_codes' in the dbt_project.yml file. It sets custom varchar types for 'country_code' and 'country_name' columns.

LANGUAGE: yaml
CODE:
seeds:
  jaffle_shop:
    country_codes:
      +column_types:
        country_code: varchar(2)
        country_name: varchar(32)

----------------------------------------

TITLE: Basic Source Table Identifier Configuration in dbt
DESCRIPTION: Basic structure for defining a source table with a custom identifier in dbt. Shows how to map a source table name to a different database table name.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: <source_name>
    database: <database_name>
    tables:
      - name: <table_name>
        identifier: <table_identifier>

----------------------------------------

TITLE: Referencing a Private Model in SQL
DESCRIPTION: Attempts to reference a private model from another group, which will result in an error due to access restrictions.

LANGUAGE: sql
CODE:
select * from {{ ref('finance_private_model') }}

----------------------------------------

TITLE: Executing DBT Models Downstream of Seed in Shell
DESCRIPTION: Command to run all models that depend on a seed named 'country_codes' using the plus operator in DBT's model selection syntax. This will execute all models that reference or depend on the country_codes seed file.

LANGUAGE: shell
CODE:
$ dbt run --select country_codes+

----------------------------------------

TITLE: Incorrect Username/Password Error Message
DESCRIPTION: Error message shown when authentication fails due to incorrect credentials in Snowflake.

LANGUAGE: text
CODE:
Failed to connect to DB: xxxxxxx.snowflakecomputing.com:443. Incorrect username or password was specified.

----------------------------------------

TITLE: Configuring Layer BigQuery Profile in YAML
DESCRIPTION: Sample configuration for setting up Layer BigQuery profile in profiles.yml. Includes both Layer authentication and BigQuery connection parameters.

LANGUAGE: yaml
CODE:
layer-profile:
  target: dev
  outputs:
    dev:
      # Layer authentication
      type: layer_bigquery
      layer_api_key: [the API Key to access your Layer account (opt)]
      # Bigquery authentication
      method: service-account
      project: [GCP project id]
      dataset: [the name of your dbt dataset]
      threads: [1 or more]
      keyfile: [/path/to/bigquery/keyfile.json]

----------------------------------------

TITLE: Displaying NoneType Error Message in Shell
DESCRIPTION: This snippet shows the error message that users may encounter when trying to access the dbt Cloud IDE. The error suggests a problem with enumerating fields on a NoneType object.

LANGUAGE: shell
CODE:
NoneType object has no attribute 
enumerate_fields'

----------------------------------------

TITLE: Connecting to Teradata Database in JupyterLab
DESCRIPTION: Jupyter notebook magic command to connect to a local Teradata database instance.

LANGUAGE: ipynb
CODE:
%connect local

----------------------------------------

TITLE: Creating a One Big Table (OBT)
DESCRIPTION: SQL code for creating a One Big Table by joining fact and dimension tables.

LANGUAGE: sql
CODE:
with f_sales as (
    select * from {{ ref('fct_sales') }}
),

d_customer as (
    select * from {{ ref('dim_customer') }}
),

d_credit_card as (
    select * from {{ ref('dim_credit_card') }}
),

d_address as (
    select * from {{ ref('dim_address') }}
),

d_order_status as (
    select * from {{ ref('dim_order_status') }}
),

d_product as (
    select * from {{ ref('dim_product') }}
),

d_date as (
    select * from {{ ref('dim_date') }}
)

select
    {{ dbt_utils.star(from=ref('fct_sales'), relation_alias='f_sales', except=[
        "product_key", "customer_key", "creditcard_key", "ship_address_key", "order_status_key", "order_date_key"
    ]) }},
    {{ dbt_utils.star(from=ref('dim_product'), relation_alias='d_product', except=["product_key"]) }},
    {{ dbt_utils.star(from=ref('dim_customer'), relation_alias='d_customer', except=["customer_key"]) }},
    {{ dbt_utils.star(from=ref('dim_credit_card'), relation_alias='d_credit_card', except=["creditcard_key"]) }},
    {{ dbt_utils.star(from=ref('dim_address'), relation_alias='d_address', except=["address_key"]) }},
    {{ dbt_utils.star(from=ref('dim_order_status'), relation_alias='d_order_status', except=["order_status_key"]) }},
    {{ dbt_utils.star(from=ref('dim_date'), relation_alias='d_date', except=["date_key"]) }}
from f_sales
left join d_product on f_sales.product_key = d_product.product_key
left join d_customer on f_sales.customer_key = d_customer.customer_key
left join d_credit_card on f_sales.creditcard_key = d_credit_card.creditcard_key
left join d_address on f_sales.ship_address_key = d_address.address_key
left join d_order_status on f_sales.order_status_key = d_order_status.order_status_key
left join d_date on f_sales.order_date_key = d_date.date_key

----------------------------------------

TITLE: Defining Data Integrity Tests in YAML for dbt
DESCRIPTION: This snippet demonstrates how to define Data Integrity tests (Generic Tests) in a YAML file for a dbt project. It shows tests for uniqueness and not null constraints on a customer model's ID column.

LANGUAGE: yaml
CODE:
version: 2
models:
  - name: customer
    columns:
      - name: id
        description: Unique ID associated with the record
        tests:
          - unique:
              alias: id__unique
          - not_null:
              alias: id__not_null

----------------------------------------

TITLE: Using dbt_utils union_relations Macro
DESCRIPTION: Simplified approach using dbt_utils package's union_relations macro to automatically handle column mismatches across multiple tables in a UNION ALL operation.

LANGUAGE: sql
CODE:
{{ dbt_utils.union_relations(

    relations=[ref('sessions'), ref('sales'), ref('ads')

) }}

----------------------------------------

TITLE: Configuring Column Orientation in Greenplum dbt Model
DESCRIPTION: Demonstrates how to set column orientation for a table, which can improve query performance for certain workloads.

LANGUAGE: sql
CODE:
{{
    config(
        ...
        orientation='column'
        ...
    )
}}


select ...

----------------------------------------

TITLE: Basic Dispatch Configuration Structure in YAML
DESCRIPTION: Demonstrates the basic structure of dispatch configuration in dbt_project.yml. Shows how to specify multiple macro namespaces and their search orders.

LANGUAGE: yml
CODE:
dispatch:
  - macro_namespace: packagename
    search_order: [packagename]
  - macro_namespace: packagename
    search_order: [packagename]

----------------------------------------

TITLE: Private Package Configuration
DESCRIPTION: Configuration example for accessing private packages using native integration and Git tokens.

LANGUAGE: yaml
CODE:
packages:
  - private: dbt-labs/awesome_repo
    revision: "0.9.5"
  - private: dbt-labs/awesome_repo
    provider: "github"

----------------------------------------

TITLE: Installing Python and Dependencies for dbt Core on Ubuntu/Debian
DESCRIPTION: Installs required packages including Python, git, and development libraries for Ubuntu/Debian to support dbt Core installation. Also upgrades cffi and installs a specific version of cryptography.

LANGUAGE: shell
CODE:
sudo apt-get install git libpq-dev python-dev python3-pip
sudo apt-get remove python-cffi
sudo pip install --upgrade cffi
pip install cryptography~=3.4

----------------------------------------

TITLE: Installing Snowflake Adapter Example
DESCRIPTION: Specific example showing how to install the Snowflake adapter for dbt using pip.

LANGUAGE: bash
CODE:
python -m pip install dbt-snowflake

----------------------------------------

TITLE: Configuring Session Connection in dbt Profiles
DESCRIPTION: YAML configuration for setting up a session connection to a pySpark session in the dbt profiles file.

LANGUAGE: yaml
CODE:
your_profile_name:
  target: dev
  outputs:
    dev:
      type: spark
      method: session
      schema: [database/schema name]
      host: NA                           # not used, but required by `dbt-core`
      server_side_parameters:
        "spark.driver.memory": "4g" 

----------------------------------------

TITLE: Specifying Version in Resource Property File (YAML)
DESCRIPTION: Shows how to specify the version in a .yml property file. Version 2 is accepted by dbt versions up to 1.4.latest. From version 1.5, this configuration is no longer required.

LANGUAGE: yaml
CODE:
version: 2  # Only 2 is accepted by dbt versions up to 1.4.latest.

models: 
    ...

----------------------------------------

TITLE: Configuring Decodable Profile in YAML for dbt
DESCRIPTION: This YAML snippet shows how to configure a Decodable profile in the dbt profiles.yml file. It includes all necessary fields such as account name, profile name, and optional settings like materialization of tests and timeout.

LANGUAGE: yaml
CODE:
dbt-decodable:       
  target: dev         
  outputs:           
    dev:              
      type: decodable
      database: None  
      schema: None    
      account_name: [your account]          
      profile_name: [name of the profile]   
      materialize_tests: [true | false]     
      timeout: [ms]                         
      preview_start: [earliest | latest]    
      local_namespace: [namespace prefix]   

----------------------------------------

TITLE: Seeding the database
DESCRIPTION: Commands to seed the database with AdventureWorks data using dbt seed.

LANGUAGE: shell
CODE:
# seed duckdb 
dbt seed --target duckdb

# seed postgres
dbt seed --target postgres

----------------------------------------

TITLE: Validating Snowflake Connection String Format for dbt
DESCRIPTION: This snippet illustrates the correct and incorrect formats for Snowflake connection strings when setting up dbt. It shows that the account identifier should be used without the full domain for Snowflake, and optionally includes the Azure region.

LANGUAGE: plaintext
CODE:
‚úÖ `db5261993` or `db5261993.east-us-2.azure`
‚ùå `db5261993.eu-central-1.snowflakecomputing.com`

----------------------------------------

TITLE: Combining State and Result Selectors in dbt
DESCRIPTION: This command shows how to combine state and result selectors in a single dbt invocation. It selects resources with a specific status from a previous run OR any new or modified models.

LANGUAGE: bash
CODE:
dbt run --select "result:<status>+" state:modified+ --defer --state ./<dbt-artifact-path>

----------------------------------------

TITLE: Complete dbt_utils DATEADD Macro Implementation
DESCRIPTION: Full implementation of the dbt_utils dateadd macro showing adapter-specific implementations for different warehouses.

LANGUAGE: sql
CODE:
{% macro dateadd(datepart, interval, from_date_or_timestamp) %}
  {{ adapter_macro('dbt_utils.dateadd', datepart, interval, from_date_or_timestamp) }}
{% endmacro %}

{% macro default__dateadd(datepart, interval, from_date_or_timestamp) %}

    dateadd(
        {{ datepart }},
        {{ interval }},
        {{ from_date_or_timestamp }}
        )

{% endmacro %}

{% macro bigquery__dateadd(datepart, interval, from_date_or_timestamp) %}

        datetime_add(
            cast( {{ from_date_or_timestamp }} as datetime),
        interval {{ interval }} {{ datepart }}
        )

{% endmacro %}

{% macro postgres__dateadd(datepart, interval, from_date_or_timestamp) %}

    {{ from_date_or_timestamp }} + ((interval '1 {{ datepart }}') * ({{ interval }}))

{% endmacro %}

----------------------------------------

TITLE: Configuring Infer Profile in dbt's profiles.yml
DESCRIPTION: This YAML snippet demonstrates how to structure the Infer profile in the dbt profiles.yml file. It includes placeholders for the Infer API endpoint, username, API key, and underlying data warehouse configuration.

LANGUAGE: yaml
CODE:
<profile-name>:
  target: <target-name>
  outputs:
    <target-name>:
      type: infer
      url: "<infer-api-endpoint>"
      username: "<infer-api-username>"
      apikey: "<infer-apikey>"
      data_config:
        [configuration for your underlying data warehouse]

----------------------------------------

TITLE: Defining Group Ownership in dbt YAML Configuration
DESCRIPTION: This snippet demonstrates how to create a group and define its owner in the __groups.yml file. It sets up a 'marketing' group with an assigned owner.

LANGUAGE: yaml
CODE:
groups: 
  - name: marketing
    owner:
        name: Ben Jaffleck 
        email: ben.jaffleck@jaffleshop.com

----------------------------------------

TITLE: Upgrading pip Dependencies for dbt Installation
DESCRIPTION: Command to upgrade pip, wheel, and setuptools to their latest versions before installing dbt. This ensures better dependency resolution and faster install times using precompiled wheels.

LANGUAGE: shell
CODE:
python -m pip install --upgrade pip wheel setuptools

----------------------------------------

TITLE: Configuring Python Model Notebook Location in Databricks
DESCRIPTION: Flag that controls the location where Python model notebooks are stored. Default False writes to /Shared/dbt_python_models/{{schema}}/. True writes to /Users/{{current user}}/{{catalog}}/{{schema}}/. Shared folder usage is being deprecated for governance reasons.

LANGUAGE: markdown
CODE:
use_user_folder_for_python: False

----------------------------------------

TITLE: Specifying Custom Configurations for Generic Data Tests
DESCRIPTION: This snippet shows how to specify custom configurations for generic data tests, including a custom Snowflake warehouse configuration.

LANGUAGE: yaml
CODE:
models:
  - name: my_model
    columns:
      - name: color
        tests:
          - accepted_values:
              values: ['blue', 'red']
              config:
                severity: warn
                snowflake_warehouse: my_warehouse

----------------------------------------

TITLE: Example of Model Versioning with Column Inclusion in dbt
DESCRIPTION: This example demonstrates practical usage of model versioning in dbt, showing how to include or exclude columns in different versions. It includes a top-level model definition and four versions with various column configurations.

LANGUAGE: yaml
CODE:
models:
  - name: customers
    columns:
      - name: customer_id
        description: Unique identifier for this table
        data_type: text
        constraints:
          - type: not_null
        tests:
          - unique
      - name: customer_country
        data_type: text
        description: "Country where the customer currently lives"
      - name: first_purchase_date
        data_type: date
    
    versions:
      - v: 4
      
      - v: 3
        columns:
          - include: "*"
          - name: customer_country
            data_type: text
            description: "Country where the customer first lived at time of first purchase"
      
      - v: 2
        columns:
          - include: "*"
            exclude:
              - customer_country
      
      - v: 1
        columns:
          - include: []
          - name: id
            data_type: int

----------------------------------------

TITLE: Configuring AWS CodeCommit PR Template URL in dbt Cloud
DESCRIPTION: Example of an AWS CodeCommit PR template URL configuration for dbt Cloud. It includes the full path to create a new pull request with specified branches.

LANGUAGE: plaintext
CODE:
https://console.aws.amazon.com/codesuite/codecommit/repositories/<repo>/pull-requests/new/refs/heads/{{destination}}/.../refs/heads/{{source}}

----------------------------------------

TITLE: Setting up Snowflake Cortex User Permissions
DESCRIPTION: SQL commands to create and configure necessary roles and permissions for using Snowflake Cortex functions.

LANGUAGE: sql
CODE:
use role accountadmin;

create role cortex_user_role;
grant database role snowflake.cortex_user to role cortex_user_role;

grant role cortex_user_role to user some_user;

----------------------------------------

TITLE: GitHub Actions CI Linting Configuration
DESCRIPTION: GitHub Actions workflow configuration for running SQLFluff linting on dbt models.

LANGUAGE: yaml
CODE:
name: lint dbt project on push

on:
  push:
    branches-ignore:
      - 'main'

jobs:
  lint_project:
    name: Run SQLFluff linter
    runs-on: ubuntu-latest
  
    steps:
      - uses: "actions/checkout@v3"
      - uses: "actions/setup-python@v4"
        with:
          python-version: "3.9"
      - name: Install SQLFluff
        run: "python -m pip install sqlfluff"
      - name: Lint project
        run: "sqlfluff lint models --dialect snowflake"

----------------------------------------

TITLE: Standardizing DATEADD with dbt Macro
DESCRIPTION: dbt_utils macro implementation that provides a standardized syntax for date addition across different warehouses.

LANGUAGE: sql
CODE:
{{ dbt_utils.dateadd(datepart, interval, from_date_or_timestamp) }}

LANGUAGE: sql
CODE:
{{ dbt_utils.dateadd(month, 1, '2021-08-12' }}

----------------------------------------

TITLE: Setting One-off Test Limits in SQL
DESCRIPTION: Configures a failure limit for a one-off data test using the config macro. Sets the limit to 1000 failures.

LANGUAGE: sql
CODE:
{{ config(limit = 1000) }}

select ...

----------------------------------------

TITLE: Customizing Package Installation Path in dbt
DESCRIPTION: Shows how to configure a custom directory path for installing dbt packages. In this example, packages will be installed in a directory named 'packages' instead of the default 'dbt_packages'.

LANGUAGE: yaml
CODE:
packages-install-path: packages

----------------------------------------

TITLE: Correct Jinja Variable Usage in dbt
DESCRIPTION: Proper way to use var() context method within date_spine macro

LANGUAGE: jinja
CODE:
  {{ dbt_utils.date_spine(
      datepart="day",
      start_date=var('start_date')
      )
  }}

----------------------------------------

TITLE: Checking dbt Core Version
DESCRIPTION: Example output of the --version command in dbt Core, showing the installed version of dbt-core and associated plugins like Snowflake adapter.

LANGUAGE: text
CODE:
$ dbt --version
Core:
  - installed: 1.7.6
  - latest:    1.7.6 - Up to date!
Plugins:
  - snowflake: 1.7.1 - Up to date!

----------------------------------------

TITLE: Suppressing Print Output with CLI Flag
DESCRIPTION: Command to suppress print() messages from showing in stdout using the --no-print flag.

LANGUAGE: text
CODE:
dbt --no-print run

----------------------------------------

TITLE: Configuring enable_truthy_nulls_equals_macro in dbt_project.yml for Snowflake
DESCRIPTION: The enable_truthy_nulls_equals_macro flag is set to False by default. Setting it to True in the dbt_project.yml file enables null-safe equality on the dbt equals macro, which is used in incremental and snapshot materializations. This allows NULL = NULL comparisons to evaluate to TRUE instead of UNKNOWN.

LANGUAGE: yaml
CODE:
enable_truthy_nulls_equals_macro: True

----------------------------------------

TITLE: Custom Schema Name Example in dbt
DESCRIPTION: Example showing how to use a custom schema name that differs from the source name. Demonstrates mapping a complex schema name to a simpler source reference name.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: jaffle_shop
    schema: postgres_backend_public_schema
    tables:
      - name: orders

----------------------------------------

TITLE: Configuring Snowflake Quoting in dbt_project.yml
DESCRIPTION: Configuration example for setting Snowflake quoting behavior to match previous versions, enabling quoting for both schema and identifier.

LANGUAGE: yaml
CODE:
quoting:
  schema: true
  identifier: true


----------------------------------------

TITLE: Generating a Training Dataset from Snowflake Feature Store
DESCRIPTION: This Python code shows how to generate a training dataset using the Snowflake Feature Store, which retrieves point-in-time correct features for given entities and timestamps.

LANGUAGE: python
CODE:
spine_df = session.create_dataframe(
    [
        ('1', '3937', "2019-05-01 00:00"), 
        ('2', '2', "2019-05-01 00:00"),
        ('3', '927', "2019-05-01 00:00"),
    ], 
    schema=["INSTANCE_ID", "CUSTOMER_ID", "EVENT_TIMESTAMP"])

train_dataset = fs.generate_dataset(
    name= "customers_fv",
    version= "1_0",
    spine_df=spine_df,
    features=[customer_transactions_fv],
    spine_timestamp_col= "EVENT_TIMESTAMP",
    spine_label_cols = []
)

----------------------------------------

TITLE: Storing dbt Cloud Secrets using Python in Zapier
DESCRIPTION: Python code snippet that uses Zapier's StoreClient to save dbt Cloud API token and webhook key. Requires a valid Zapier Storage UUID and corresponding dbt Cloud credentials. The stored values will persist for 3 months if accessed at least once during that period.

LANGUAGE: python
CODE:
store = StoreClient('abc123') #replace with your UUID secret
store.set('DBT_WEBHOOK_KEY', 'abc123') #replace with webhook secret
store.set('DBT_CLOUD_SERVICE_TOKEN', 'abc123') #replace with your dbt Cloud API token

----------------------------------------

TITLE: Executing dbt retry after successful run
DESCRIPTION: Example output when running dbt retry after a successful dbt run operation, showing that there are no failed operations to retry.

LANGUAGE: shell
CODE:
Running with dbt=1.6.1
Registered adapter: duckdb=1.6.0
Found 5 models, 3 seeds, 20 tests, 0 sources, 0 exposures, 0 metrics, 348 macros, 0 groups, 0 semantic models
 
Nothing to do. Try checking your model configs and model specification args

----------------------------------------

TITLE: Configuring Post-Hooks for Permissions in dbt_project.yml
DESCRIPTION: This YAML snippet demonstrates how to use post-hooks in the dbt_project.yml file to grant permissions to other transformers and BI users. This ensures that any changes made will be accessible to collaborators and utilized in the BI layer.

LANGUAGE: yaml
CODE:
on-run-end:
  - "grant select on all tables in schema {{ target.schema }} to role transformer"
  - "grant select on all tables in schema {{ target.schema }} to role reporter"

----------------------------------------

TITLE: Configuring meta for exposures in dbt_project.yml
DESCRIPTION: Demonstrates setting meta properties for exposures in the dbt_project.yml file. Meta is defined as a dictionary under the exposures configuration block.

LANGUAGE: yaml
CODE:
exposures:
  [<resource-path>]:
    +meta: {<dictionary>}

----------------------------------------

TITLE: Generic Test Configuration in YAML
DESCRIPTION: Shows how to configure and apply a generic test to a model column in the YAML configuration file.

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: users
    columns:
      - name: favorite_number
        tests:
          - is_even
            description: "This is a test"

----------------------------------------

TITLE: Running Source Freshness Commands in Bash for dbt
DESCRIPTION: These Bash commands demonstrate how to run dbt source freshness checks. They include examples of checking all sources, selecting specific sources, and specifying custom output paths for the freshness report.

LANGUAGE: bash
CODE:
# Snapshot freshness for all Snowplow tables:
$ dbt source freshness --select "source:snowplow"

# Snapshot freshness for a particular source table:
$ dbt source freshness --select "source:snowplow.event"

# Output source freshness info to a different path
$ dbt source freshness --output target/source_freshness.json

----------------------------------------

TITLE: Configuring meta for semantic models in semantic_models.yml
DESCRIPTION: Demonstrates how to set meta properties for semantic models in a semantic_models.yml file. Meta is configured under the config block for each semantic model.

LANGUAGE: yaml
CODE:
semantic_models:
  - name: semantic_model_name
    config:
      meta: {<dictionary>}

----------------------------------------

TITLE: Configuring dbt Sources in YAML
DESCRIPTION: YAML configuration for defining data sources in dbt, allowing version control of data source mappings and enabling source freshness reporting.

LANGUAGE: yaml
CODE:
sources:
  - name: jaffle_shop
    tables:
      - name: orders
      - name: customers

----------------------------------------

TITLE: Updating Relation Class Usage in Python
DESCRIPTION: The 'table_name' field has been removed from Relations. Macros using this field will now return errors. Users should refer to the updated class reference for the correct usage.

LANGUAGE: python
CODE:
# Old code (will now raise an error):
# relation.table_name

# New code (check updated class reference for correct usage)
# relation.identifier  # or other appropriate property

----------------------------------------

TITLE: dbt Configuration Prefix Example
DESCRIPTION: Demonstrates the use of + prefix in dbt_project.yml to distinguish configuration names from folder names. The prefix is specifically used for configs under resource keys in dbt_project.yml and does not apply to config() Jinja macros or .yml file config properties.

LANGUAGE: yaml
CODE:
+enabled: true

----------------------------------------

TITLE: Rendering Source Table Reference in dbt Pre-hook
DESCRIPTION: Demonstrates using the .render() method to explicitly process a source() call within a pre-hook configuration. This ensures the source reference is evaluated even when using the --empty flag for optimization.

LANGUAGE: jinja
CODE:
{{ config(
    pre_hook = [
        "alter external table {{ source('sys', 'customers').render() }} refresh"
    ]
) }}

select ...

----------------------------------------

TITLE: Controlling Whitespace in dbt Macro Definitions with Jinja
DESCRIPTION: Example of using Jinja's whitespace control syntax with minus signs to remove extra whitespace in macro definitions. This affects how the code appears in the target/compiled folder.

LANGUAGE: jinja
CODE:
{%- macro generate_schema_name(...) -%} ... {%- endmacro -%}

LANGUAGE: jinja
CODE:
{{- ... -}}

LANGUAGE: jinja
CODE:
{%- ... %}

----------------------------------------

TITLE: Seed Batch Size Configuration
DESCRIPTION: Setting the maximum batch size for seed file insertion using project variables.

LANGUAGE: yaml
CODE:
vars:
  max_batch_size: 200 # Any integer less than or equal to 2100 will do.

----------------------------------------

TITLE: Initial dbt Model Query
DESCRIPTION: First dbt model for transforming customer data

LANGUAGE: sql
CODE:
with customers as (
    select
        id as customer_id,
        first_name,
        last_name
    from dbt_quickstart.jaffle_shop.jaffle_shop_customers
),
orders as (
    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status
    from dbt_quickstart.jaffle_shop.jaffle_shop_orders
)

----------------------------------------

TITLE: Example Model Name List
DESCRIPTION: Shows a list of inconsistently named user-related models that demonstrate common naming problems in dbt projects.

LANGUAGE: text
CODE:
* users
* user_dimensions
* user_properties
* dim_users_attributed
* dim_users_revenue_attributed

----------------------------------------

TITLE: Poll method request example
DESCRIPTION: Example JSON request for the poll method, used to check the status of a running or completed task.

LANGUAGE: json
CODE:
{
    "jsonrpc": "2.0",
    "method": "poll",
    "id": "2db9a2fe-9a39-41ef-828c-25e04dd6b07d",
    "params": {
        "request_token": "f86926fa-6535-4891-8d24-2cfc65d2a347",
        "logs": true,
        "logs_start": 0
    }
}

----------------------------------------

TITLE: Custom Schema Without Target Concatenation
DESCRIPTION: Macro that generates schema names without concatenating target schema in production, while maintaining unique schemas per developer in development.

LANGUAGE: jinja
CODE:
{% macro generate_schema_name(custom_schema_name, node) -%}
    {%- set default_schema = target.schema -%}
    {%- if custom_schema_name is none -%}
        {{ default_schema }}
    {%- elif  env_var('DBT_ENV_TYPE','DEV') == 'PROD' -%}
        {{ custom_schema_name | trim }}
    {%- else -%}
        {{ default_schema }}_{{ custom_schema_name | trim }}
    {%- endif -%}
{%- endmacro %}

----------------------------------------

TITLE: Defining a Simple Metric in YAML for dbt
DESCRIPTION: This YAML snippet defines a simple metric named 'order_total' using MetricFlow. It specifies the metric's name, description, type, label, and the measure it's based on.

LANGUAGE: yaml
CODE:
metrics:
  - name: order_total
    description: Sum of total order amount. Includes tax + revenue.
    type: simple
    label: Order Total
    type_params:
      measure: order_total

----------------------------------------

TITLE: Package Version Range Specification
DESCRIPTION: Example showing how to specify version ranges for packages using semantic versioning.

LANGUAGE: yaml
CODE:
packages:
  - package: dbt-labs/snowplow
    version: [">=0.7.0", "<0.8.0"]

----------------------------------------

TITLE: Using Environment Variables in dbt Semantic Layer
DESCRIPTION: Examples showing how to reference environment variables in Semantic Layer credentials for warehouse and account configuration. These snippets demonstrate the syntax for environment variable references in dbt.

LANGUAGE: jinja
CODE:
{{env_var('DBT_WAREHOUSE')}}

LANGUAGE: jinja
CODE:
{{env_var('DBT_ACCOUNT')}}

----------------------------------------

TITLE: Analyzing Customer Orders in Snowflake
DESCRIPTION: This SQL query joins customer and order data in Snowflake to analyze customer order history. It calculates the first and most recent order dates, as well as the total number of orders for each customer.

LANGUAGE: sql
CODE:
with customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from raw.jaffle_shop.customers

),

orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from raw.jaffle_shop.orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final

----------------------------------------

TITLE: Invalid SQL Function Arguments Example
DESCRIPTION: Shows a SQL query with incorrect argument order in the dateadd function

LANGUAGE: sql
CODE:
select dateadd('day', getdate(), 1) as tomorrow

----------------------------------------

TITLE: Using Columns in SQL Templates
DESCRIPTION: Demonstrates how to use Column objects in SQL templates with examples of string and numeric columns.

LANGUAGE: jinja2
CODE:
-- String column
{%- set string_column = api.Column('name', 'varchar', char_size=255) %}

-- Return true if the column is a string
{{ string_column.is_string() }}

-- Return true if the column is a numeric
{{ string_column.is_numeric() }}

-- Return true if the column is a number
{{ string_column.is_number() }}

-- Return true if the column is an integer
{{ string_column.is_integer() }}

-- Return true if the column is a float
{{ string_column.is_float() }}

-- Numeric column
{%- set numeric_column = api.Column('distance_traveled', 'numeric', numeric_precision=12, numeric_scale=4) %}

-- Return true if the column is a string
{{ numeric_column.is_string() }}

-- Return true if the column is a numeric
{{ numeric_column.is_numeric() }}

-- Return true if the column is a number
{{ numeric_column.is_number() }}

-- Return true if the column is an integer
{{ numeric_column.is_integer() }}

-- Return true if the column is a float
{{ numeric_column.is_float() }}

-- Static methods

-- Return the string data type for this database adapter with a given size
{{ api.Column.string_type(255) }}

-- Return the numeric data type for this database adapter with a given precision and scale
{{ api.Column.numeric_type('numeric', 12, 4) }}

----------------------------------------

TITLE: Configuring Indexes for Multiple Resources in YAML for dbt-Postgres
DESCRIPTION: This snippet shows how to configure indexes for multiple resources at once using the dbt_project.yml file.

LANGUAGE: yaml
CODE:
models:
  project_name:
    subdirectory:
      +indexes:
        - columns: ['column_a']
          type: hash

----------------------------------------

TITLE: Registering Feature Views in Snowflake
DESCRIPTION: This Python code demonstrates how to create a feature view from a dbt-produced feature table and register it in the Snowflake Feature Store.

LANGUAGE: python
CODE:
# Create a dataframe from our feature table produced in dbt
customers_transactions_df = session.sql(f"""
    SELECT 
        CUSTOMER_ID,
        TX_DATETIME,
        TX_AMOUNT_1D,
        TX_AMOUNT_7D,
        TX_AMOUNT_30D,
        TX_AMOUNT_AVG_1D,
        TX_AMOUNT_AVG_7D,
        TX_AMOUNT_AVG_30D,
        TX_CNT_1D,
        TX_CNT_7D,
        TX_CNT_30D     
    FROM {fs_db}.{fs_data_schema}.ft_customer_transactions
    """)

# Create a feature view on top of these features
customer_transactions_fv = FeatureView(
    name="customer_transactions_fv", 
    entities=[customer],
    feature_df=customers_transactions_df,
    timestamp_col="TX_DATETIME",
    refresh_freq=None,
    desc="Customer transaction features with window aggregates")

# Register the feature view for use beyond the session
customer_transactions_fv = fs.register_feature_view(
    feature_view=customer_transactions_fv,
    version="1",
    #overwrite=True,
    block=True)

----------------------------------------

TITLE: Run SQL method request example
DESCRIPTION: Example JSON request for executing a SQL query using the run_sql method.

LANGUAGE: json
CODE:
{
    "jsonrpc": "2.0",
    "method": "run_sql",
    "id": "2db9a2fe-9a39-41ef-828c-25e04dd6b07d",
    "params": {
        "timeout": 60,
        "sql": "c2VsZWN0IHt7IDEgKyAxIH19IGFzIGlk",
        "name": "my_first_query"
    }
}

----------------------------------------

TITLE: Installing dbt Adapter (version 1.7 and earlier)
DESCRIPTION: Command to install the specific adapter package for dbt versions 1.7 and earlier. This installation automatically includes dbt-core and additional dependencies.

LANGUAGE: bash
CODE:
python -m pip install {props.meta.pypi_package}

----------------------------------------

TITLE: Incremental Model Configuration in dbt
DESCRIPTION: Configuration block for setting up an incremental model in dbt, specifying materialization type and unique key for record updates.

LANGUAGE: sql
CODE:
{{
    config(
        materialized='incremental',
        unique_key='order_id'
    )
}}

select ...

----------------------------------------

TITLE: Using ILIKE for Card Payment Classification in SQL
DESCRIPTION: Example query demonstrating how to use the ILIKE operator to classify payment methods as card or non-card payments based on case-insensitive pattern matching with the '%card' pattern.

LANGUAGE: sql
CODE:
select
   payment_id,
   order_id,
   payment_method,
   case when payment_method ilike '%card' then 'card_payment' else 'non_card_payment' end as was_card
from {{ ref('payments') }}

----------------------------------------

TITLE: Defining dbt Snapshot in SQL (Version 1.8 and earlier)
DESCRIPTION: Shows the basic structure for defining a dbt snapshot using SQL and Jinja in version 1.8 and earlier. This snippet provides the skeleton for creating a named snapshot.

LANGUAGE: jinja2
CODE:
{% snapshot snapshot_name %}

{% endsnapshot %}

----------------------------------------

TITLE: Updating dbt_project.yml Configuration
DESCRIPTION: Shows how to remove the example configuration from dbt_project.yml file while maintaining project-specific settings. Demonstrates the before and after states of the configuration file.

LANGUAGE: yaml
CODE:
# before
models:
  jaffle_shop:
    +materialized: table
    example:
      +materialized: view

LANGUAGE: yaml
CODE:
# after
models:
  jaffle_shop:
    +materialized: table

----------------------------------------

TITLE: Configuring Tags for Saved Queries in dbt Project
DESCRIPTION: Example showing how to configure tags for saved queries in the dbt_project.yml file to enable categorization and filtering of resources.

LANGUAGE: yml
CODE:
[saved-queries]:
  jaffle_shop:
    customer_order_metrics:
      +tags: order_metrics

----------------------------------------

TITLE: Column Class Implementation in Python
DESCRIPTION: Defines the Column class for representing database column metadata with methods for type checking and formatting.

LANGUAGE: python
CODE:
class Column(object):
  def __init__(self, column, dtype, char_size=None, numeric_size=None):
    """
      column: The name of the column represented by this object
      dtype: The data type of the column (database-specific)
      char_size: If dtype is a variable width character type, the size of the column, or else None
      numeric_size: If dtype is a fixed precision numeric type, the size of the column, or else None
   """


# Example usage:
col = Column('name', 'varchar', 255)
col.is_string() # True
col.is_numeric() # False
col.is_number() # False
col.is_integer() # False
col.is_float() # False
col.string_type() # character varying(255)
col.numeric_type('numeric', 12, 4) # numeric(12,4)

----------------------------------------

TITLE: Configuring Materialized Views in YAML for dbt-Postgres
DESCRIPTION: This snippet demonstrates how to configure materialized views in the dbt_project.yml file, including settings for materialization, configuration changes, and indexes.

LANGUAGE: yaml
CODE:
models:
  [<resource-path>]:
    [+][materialized]: materialized_view
    [+][on_configuration_change]: apply | continue | fail
    [+][indexes]:
      - columns: [<column-name>]
        unique: true | false
        type: hash | btree

----------------------------------------

TITLE: Basic Directory Structure for dbt Project
DESCRIPTION: Shows the typical file structure of a dbt project including the requirements.txt and key configuration files.

LANGUAGE: shell
CODE:
/my_dbt_project/
‚îú‚îÄ‚îÄ dbt_project.yml
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ my_model.sql
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ my_test.sql
‚îî‚îÄ‚îÄ requirements.txt

----------------------------------------

TITLE: Hierarchy Traversal Query
DESCRIPTION: Recursive SQL query to traverse component hierarchies and establish parent-child relationships with valid time ranges.

LANGUAGE: sql
CODE:
with recursive
components as (
    select
        *,
        installed_at as valid_from_at,
        removed_at as valid_to_at
    from
        erp_components
),

top_assemblies as (
    select * from components where assembly_id is null
),

traversal as (
    select
        0 as component_hierarchy_depth,
        false as is_circular,
        [component_id] as component_trace,
        component_id as top_assembly_id,
        assembly_id,
        component_id,
        installed_at,
        removed_at,
        valid_from_at,
        valid_to_at
    from
        top_assemblies

    union all

    select
        traversal.component_hierarchy_depth + 1 as component_hierarchy_depth,
        array_contains(components.component_id::variant, traversal.component_trace) as is_circular,
        array_append(traversal.component_trace, components.component_id) as component_trace,
        traversal.top_assembly_id,
        components.assembly_id,
        components.component_id,
        components.installed_at,
        components.removed_at,
        greatest(traversal.valid_from_at, components.valid_from_at) as valid_from_at,
        least(traversal.valid_to_at, components.valid_to_at) as valid_to_at
    from
        traversal
    inner join
        components
        on
            traversal.component_id = components.assembly_id
            and
            (
                traversal.valid_from_at < components.valid_to_at
                and
                traversal.valid_to_at >= components.valid_from_at
            )
    where
        not array_contains(components.component_id::variant, traversal.component_trace)
        and traversal.component_hierarchy_depth < 20
),

final as (
    select distinct *
    from
        traversal
    where
        valid_from_at < valid_to_at
)

select * from final

----------------------------------------

TITLE: dbt Debug Command Usage Reference
DESCRIPTION: Comprehensive list of all available flags and options for the dbt debug command

LANGUAGE: text
CODE:
Usage: dbt debug [OPTIONS]

 Show information on the current dbt environment and check dependencies, then
 test the database connection. Not to be confused with the --debug option
 which increases verbosity.

Options:
 --cache-selected-only / --no-cache-selected-only
                At start of run, populate relational cache
                only for schemas containing selected nodes,
                or for all schemas of interest.

 -d, --debug / --no-debug    
                Display debug logging during dbt execution.
                Useful for debugging and making bug reports.

 --defer / --no-defer      
                If set, resolve unselected nodes by
                deferring to the manifest within the --state
                directory.

 --defer-state DIRECTORY     
                Override the state directory for deferral
                only.

 --deprecated-favor-state TEXT  
                Internal flag for deprecating old env var.

 -x, --fail-fast / --no-fail-fast
                 Stop execution on first failure.

 --favor-state / --no-favor-state
                If set, defer to the argument provided to
                the state flag for resolving unselected
                nodes, even if the node(s) exist as a
                database object in the current environment.

 --indirect-selection [eager|cautious|buildable|empty]
                Choose which tests to select that are
                adjacent to selected resources. Eager is
                most inclusive, cautious is most exclusive,
                and buildable is in between. Empty includes
                no tests at all.

 --log-cache-events / --no-log-cache-events
                Enable verbose logging for relational cache
                events to help when debugging.

 --log-format [text|debug|json|default]
                Specify the format of logging to the console
                and the log file. Use --log-format-file to
                configure the format for the log file
                differently than the console.

 --log-format-file [text|debug|json|default]
                Specify the format of logging to the log
                file by overriding the default value and the
                general --log-format setting.

 --log-level [debug|info|warn|error|none]
                Specify the minimum severity of events that
                are logged to the console and the log file.
                Use --log-level-file to configure the
                severity for the log file differently than
                the console.

 --log-level-file [debug|info|warn|error|none]
                Specify the minimum severity of events that
                are logged to the log file by overriding the
                default value and the general --log-level
                setting.

 --log-path PATH         
                Configure the 'log-path'. Only applies this
                setting for the current run. Overrides the
                'DBT_LOG_PATH' if it is set.

 --partial-parse / --no-partial-parse
                Allow for partial parsing by looking for and
                writing to a pickle file in the target
                directory. This overrides the user
                configuration file.

 --populate-cache / --no-populate-cache
                At start of run, use `show` or
                `information_schema` queries to populate a
                relational cache, which can speed up
                subsequent materializations.

 --print / --no-print      
                Output all {{ print() }} macro calls.

 --printer-width INTEGER     
                Sets the width of terminal output

 --profile TEXT         
                Which existing profile to load. Overrides
                setting in dbt_project.yml.

 -q, --quiet / --no-quiet    
                Suppress all non-error logging to stdout.
                Does not affect {{ print() }} macro calls.

 -r, --record-timing-info PATH  
                When this option is passed, dbt will output
                low-level timing stats to the specified
                file. Example: `--record-timing-info
                output.profile`

 --send-anonymous-usage-stats / --no-send-anonymous-usage-stats
                Send anonymous usage stats to dbt Labs.

 --state DIRECTORY        
                Unless overridden, use this state directory
                for both state comparison and deferral.

 --static-parser / --no-static-parser
                Use the static parser.

 -t, --target TEXT        
                Which target to load for the given profile

 --use-colors / --no-use-colors 
                Specify whether log output is colorized in
                the console and the log file. Use --use-
                colors-file/--no-use-colors-file to colorize
                the log file differently than the console.

 --use-colors-file / --no-use-colors-file
                Specify whether log file output is colorized
                by overriding the default value and the
                general --use-colors/--no-use-colors
                setting.

 --use-experimental-parser / --no-use-experimental-parser
                Enable experimental parsing features.

 -V, -v, --version        
                Show version information and exit

 --version-check / --no-version-check
                If set, ensure the installed dbt version
                matches the require-dbt-version specified in
                the dbt_project.yml file (if any).
                Otherwise, allow them to differ.

 --warn-error   
                If dbt would normally warn, instead raise an
                exception. Examples include --select that
                selects nothing, deprecations,
                configurations with no associated models,
                invalid test configurations, and missing
                sources/refs in tests.

 --warn-error-options WARNERROROPTIONSTYPE
                If dbt would normally warn, instead raise an
                exception based on include/exclude
                configuration. Examples include --select
                that selects nothing, deprecations,
                configurations with no associated models,
                invalid test configurations, and missing
                sources/refs in tests. This argument should
                be a YAML string, with keys 'include' or
                'exclude'. eg. '{"include": "all",
                "exclude": ["NoNodesForSelectionCriteria"]}'

 --write-json / --no-write-json 
                Whether or not to write the manifest.json
                and run_results.json files to the target
                directory

 --connection          
                Test the connection to the target database
                independent of dependency checks.
                Available in dbt Cloud IDE and dbt Core CLI

 --config-dir          
                Print a system-specific command to access
                the directory that the current dbt project
                is searching for a profiles.yml. Then, exit.
                This flag renders other debug step flags no-
                ops.

 --profiles-dir PATH       
                Which directory to look in for the
                profiles.yml file. If not set, dbt will look
                in the current working directory first, then
                HOME/.dbt/

 --project-dir PATH       
                Which directory to look in for the
                dbt_project.yml file. Default is the current
                working directory and its parents.

 --vars YAML           
                Supply variables to the project. This
                argument overrides variables defined in your
                dbt_project.yml file. This argument should
                be a YAML string, eg. '{my_variable:
                my_value}'

 -h, --help           
                Show this message and exit.

----------------------------------------

TITLE: Retrieving Relation Object in SQL using dbt adapter
DESCRIPTION: This snippet demonstrates how to use the `adapter.get_relation` method to retrieve a cached Relation object identified by database, schema, and identifier.

LANGUAGE: sql
CODE:
{%- set source_relation = adapter.get_relation(
      database="analytics",
      schema="dbt_drew",
      identifier="orders") -%}

{{ log("Source Relation: " ~ source_relation, info=true) }}

----------------------------------------

TITLE: Selecting Models with Complex Configurations in dbt
DESCRIPTION: Examples of selecting models with boolean, dictionary, and list configurations using the config method.

LANGUAGE: bash
CODE:
dbt ls -s config.materialized:incremental
dbt ls -s config.unique_key:column_a
dbt ls -s config.grants.select:reporter
dbt ls -s config.meta.contains_pii:true
dbt ls -s config.transient:true

----------------------------------------

TITLE: Configuring Iceberg Table with ORC Format and Bucketing in dbt
DESCRIPTION: Demonstrates how to create an Iceberg table with ORC format and a bucketing partitioning strategy using dbt configuration. The table uses a bucketing strategy on the 'id' column with 2 buckets.

LANGUAGE: sql
CODE:
{{
  config(
    materialized='table',
    properties={
      "format": "'ORC'", -- Specifies the file format
      "partitioning": "ARRAY['bucket(id, 2)']", -- Defines the partitioning strategy
    }
  )
}}

----------------------------------------

TITLE: Configuring pre-hooks and post-hooks for snapshots in dbt_project.yml
DESCRIPTION: Define pre-hooks and post-hooks for snapshots in the dbt_project.yml file. Hooks can be single SQL statements or lists of statements.

LANGUAGE: yaml
CODE:
snapshots:
  [<resource-path>]:
    +pre-hook: SQL-statement | [SQL-statement]
    +post-hook: SQL-statement | [SQL-statement]

----------------------------------------

TITLE: Referencing Models in dbt SQL Transformations
DESCRIPTION: This snippet demonstrates the use of the ref function in dbt to reference other models, allowing for the separation of data cleaning and transformation logic into distinct models.

LANGUAGE: SQL
CODE:
{{ ref('model_name') }}

----------------------------------------

TITLE: Defining relationships in Looker LookML
DESCRIPTION: Example LookML code for defining relationships between fact and dimension tables in Looker.

LANGUAGE: lookml
CODE:
explore: fct_order {
  join: dim_user {
    sql_on: ${fct_order.user_key} = ${dim_user.user_key} ;;
    relationship: many_to_one
  }
}

----------------------------------------

TITLE: Creating Relation Objects in Python
DESCRIPTION: Demonstrates how to create a Relation object using the create class method with optional parameters for database, schema, identifier and type.

LANGUAGE: python
CODE:
class Relation:
  def create(database=None, schema=None, identifier=None,
             type=None):
  """
    database (optional): The name of the database for this relation
    schema (optional): The name of the schema (or dataset, if on BigQuery) for this relation
    identifier (optional): The name of the identifier for this relation
    type (optional): Metadata about this relation, eg: "table", "view", "cte"
  """

----------------------------------------

TITLE: Defining External Source in dbt YAML
DESCRIPTION: This YAML snippet shows how to define an external source in the sources.yaml file for dbt. It specifies the schema using the external catalog and database, and defines a table within that source.

LANGUAGE: yaml
CODE:
sources:
  - name: external_example
    schema: hive_catalog.hive_db
    tables:
      - name: hive_table_name

----------------------------------------

TITLE: Implementing Model Contracts in dbt
DESCRIPTION: Demonstrates how to define model contracts in dbt using YAML configuration to enforce column types and constraints for a dim_customers table.

LANGUAGE: yaml
CODE:
models:
  - name: dim_customers
    config:
      contract:
        enforced: true
    columns:
      - name: id
        data_type: integer
        description: hello
        constraints:
          - type: not_null
          - type: primary_key
          - type: check
            expression: "id > 0"
        tests:
          - unique
      - name: customer_name
        data_type: text
      - name: first_transaction_date
        data_type: date

----------------------------------------

TITLE: Serving dbt Documentation with Custom Host
DESCRIPTION: This command starts a webserver to serve the generated documentation on a specified host, allowing customization of the serving address.

LANGUAGE: shell
CODE:
dbt docs serve --host ""

----------------------------------------

TITLE: Generating dbt Docs Without Recompilation
DESCRIPTION: This command generates documentation without recompiling the project, skipping the resource compilation step.

LANGUAGE: shell
CODE:
dbt docs generate --no-compile

----------------------------------------

TITLE: Implementing Slack Notification Decorator in Python
DESCRIPTION: This code snippet defines a decorator function that sends Slack notifications on function completion or failure. It wraps the main function, sending success or error messages to a specified Slack webhook.

LANGUAGE: python
CODE:
from dlt.common.runtime.slack import send_slack_message

def notify_on_completion(hook):
    def decorator(func):
        def wrapper(*args, **kwargs):
            try:
                load_info = func(*args, **kwargs)
                message = f"Function {func.__name__} completed successfully. Load info: {load_info}"
                send_slack_message(hook, message)
                return load_info
            except Exception as e:
                message = f"Function {func.__name__} failed. Error: {str(e)}"
                send_slack_message(hook, message)
                raise
        return wrapper
    return decorator

----------------------------------------

TITLE: Naming a dbt Snapshot 'order_snapshot' in YAML
DESCRIPTION: Illustrates how to name a dbt snapshot 'order_snapshot' using YAML configuration in version 1.9 and above. It includes all necessary configuration options for the snapshot.

LANGUAGE: yaml
CODE:
snapshots:
  - name: order_snapshot
    relation: source('my_source', 'my_table')
    config:
      schema: string
      database: string
      unique_key: column_name_or_expression
      strategy: timestamp | check
      updated_at: column_name  # Required if strategy is 'timestamp'

----------------------------------------

TITLE: Configuring compute for a Python model
DESCRIPTION: Example of configuring compute resources for a Python model in dbt

LANGUAGE: python
CODE:
def model(dbt, session):
    dbt.config(
      http_path="sql/protocolv1/..."
    )

----------------------------------------

TITLE: Implementing Defensive Package Dependency Check in SQL Macro
DESCRIPTION: A SQL macro that demonstrates defensive coding practices in dbt packages by checking for required dependencies before execution. The macro verifies the existence of a date_spine macro in the global dbt namespace and raises a compiler error if not found.

LANGUAGE: sql
CODE:
{% macro a_few_days_in_september() %}

    {% if not dbt.get('date_spine') %}
      {{ exceptions.raise_compiler_error("Expected to find the dbt.date_spine macro, but it could not be found") }}
    {% endif %}

    {{ date_spine("day", "cast('2020-01-01' as date)", "cast('2030-12-31' as date)") }}

{% endmacro %}

----------------------------------------

TITLE: Check Strategy Example with Multiple Columns
DESCRIPTION: Complete example of check strategy implementation monitoring multiple columns for changes.

LANGUAGE: yaml
CODE:
snapshots:
  - name: orders_snapshot_check
    relation: source('jaffle_shop', 'orders')
    config:
      schema: snapshots
      unique_key: id
      strategy: check
      check_cols:
        - status
        - is_cancelled

----------------------------------------

TITLE: Snowflake Seed Error Example
DESCRIPTION: Example of database error when running dbt seed command with changed columns in Snowflake, showing invalid identifier error for COUNTRY_NAME column.

LANGUAGE: shell
CODE:
$ dbt seed
Running with dbt=0.16.0-rc2
Found 0 models, 0 tests, 0 snapshots, 0 analyses, 130 macros, 0 operations, 1 seed file, 0 sources

12:12:27 | Concurrency: 8 threads (target='dev_snowflake')
12:12:27 |
12:12:27 | 1 of 1 START seed file dbt_claire.country_codes...................... [RUN]
12:12:30 | 1 of 1 ERROR loading seed file dbt_claire.country_codes.............. [ERROR in 2.78s]
12:12:31 |
12:12:31 | Finished running 1 seed in 10.05s.

Completed with 1 error and 0 warnings:

Database Error in seed country_codes (seeds/country_codes.csv)
  000904 (42000): SQL compilation error: error line 1 at position 62
  invalid identifier 'COUNTRY_NAME'

Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1

----------------------------------------

TITLE: SQL Foreign Key Example Tables
DESCRIPTION: Example showing two related tables with a foreign key relationship between customers and orders.

LANGUAGE: sql
CODE:
customer_id (primary key)
customer_name

LANGUAGE: sql
CODE:
order_id (primary key)
order_date
customer_id (foreign key)

----------------------------------------

TITLE: Displaying Help for dbt Invocation Command
DESCRIPTION: Shows how to use the 'help' flag with the dbt invocation command to display usage information and available options.

LANGUAGE: shell
CODE:
dbt invocation help

LANGUAGE: shell
CODE:
dbt help invocation

----------------------------------------

TITLE: Naming a dbt Snapshot 'orders_snapshot' in SQL
DESCRIPTION: Shows how to name a dbt snapshot 'orders_snapshot' using SQL and Jinja in version 1.8 and earlier. This example demonstrates the basic structure for defining a named snapshot.

LANGUAGE: jinja2
CODE:
{% snapshot orders_snapshot %}
...
{% endsnapshot %}

----------------------------------------

TITLE: Configuring Development Environment Connection in YAML
DESCRIPTION: YAML configuration for development environment connection settings in dbt Cloud, using OAuth authentication with limited threads for better log readability.

LANGUAGE: yaml
CODE:
account: 123dev
project: dev
dataset: dbt_winnie
method: oauth
threads: 1

----------------------------------------

TITLE: Configuring Model Materialization in dbt_project.yml
DESCRIPTION: This snippet demonstrates how to configure model materialization at the project level in dbt_project.yml. It sets the 'jaffle_shop' models to be materialized as tables, while the 'example' models are set to be materialized as views.

LANGUAGE: yaml
CODE:
models:
  jaffle_shop:
    +materialized: table
  example:
    +materialized: view

----------------------------------------

TITLE: Running dbt_project_evaluator Package in dbt
DESCRIPTION: This command runs and tests all models in the dbt_project_evaluator package. It's used to identify any warnings or misalignments in your dbt project.

LANGUAGE: shell
CODE:
dbt build --select package:dbt_project_evaluator

----------------------------------------

TITLE: Viewing Active Session Status in dbt Cloud CLI
DESCRIPTION: Use the 'dbt invocation list' command in a separate terminal window to view the status of active sessions and debug long-running sessions or 'Session occupied' errors.

LANGUAGE: bash
CODE:
dbt invocation list

----------------------------------------

TITLE: Executing dbt run with Full Refresh in Bash
DESCRIPTION: Demonstrates how to run dbt with the full-refresh flag to treat incremental models as table models. This is useful for schema changes or reprocessing the entire incremental model.

LANGUAGE: shell
CODE:
dbt run --full-refresh

----------------------------------------

TITLE: Using dictionaries for transaction control in hooks
DESCRIPTION: Example of using dictionaries to control transaction behavior in hooks, allowing hooks to run outside of transactions.

LANGUAGE: sql
CODE:
{{
  config(
    pre_hook={
      "sql": "SQL-statement",
      "transaction": False
    },
    post_hook={
      "sql": "SQL-statement",
      "transaction": False
    }
  )
}}

select ...

----------------------------------------

TITLE: Running dbt Build Command in Production
DESCRIPTION: The dbt build command used in production jobs to rebuild and update materialized tables with the latest source data. This ensures models like 'customers' stay current as new records are added.

LANGUAGE: shell
CODE:
dbt build

----------------------------------------

TITLE: Keyboard Shortcuts for dbt Copilot
DESCRIPTION: Key commands for accessing dbt Copilot's prompt window across different operating systems

LANGUAGE: plaintext
CODE:
Mac: Cmd+B
Windows: Ctrl+B

----------------------------------------

TITLE: Setting up and running dbt docs site locally
DESCRIPTION: Series of commands to clone the repo, install dependencies, and run the docs site locally. This allows contributors to preview their changes before submitting.

LANGUAGE: bash
CODE:
brew install node
git clone https://github.com/dbt-labs/docs.getdbt.com.git
cd docs.getdbt.com
cd website
make install
make run
make build

----------------------------------------

TITLE: Project-level dbt_valid_to_current configuration
DESCRIPTION: Setting dbt_valid_to_current in the dbt_project.yml file for global configuration

LANGUAGE: yml
CODE:
snapshots:
  [<resource-path>]:
    +dbt_valid_to_current: "string"

----------------------------------------

TITLE: Validating Webhook Authenticity in Python
DESCRIPTION: Python code snippet showing how to validate incoming webhooks using HMAC SHA256 verification with the secret token

LANGUAGE: python
CODE:
auth_header = request.headers.get('authorization', None)
app_secret = os.environ['MY_DBT_CLOUD_AUTH_TOKEN'].encode('utf-8')
signature = hmac.new(app_secret, request_body, hashlib.sha256).hexdigest()
return signature == auth_header

----------------------------------------

TITLE: Updating Project Name in dbt_project.yml
DESCRIPTION: This snippet shows how to update the project name in the dbt_project.yml file. The project name is set to 'jaffle_shop'.

LANGUAGE: yaml
CODE:
name: 'jaffle_shop'

----------------------------------------

TITLE: Configuring compute resources in dbt_project.yml
DESCRIPTION: Example of configuring compute resources for different model directories in dbt_project.yml

LANGUAGE: yaml
CODE:
models:
  +databricks_compute: "Compute1"     # use the `Compute1` warehouse/cluster for all models in the project...
  my_project:
    clickstream:
      +databricks_compute: "Compute2" # ...except for the models in the `clickstream` folder, which will use `Compute2`.

snapshots:
  +databricks_compute: "Compute1"     # all Snapshot models are configured to use `Compute1`.

----------------------------------------

TITLE: Validating Specific Semantic Nodes
DESCRIPTION: Command to validate a specific semantic node (metric:revenue) in a CI job.

LANGUAGE: bash
CODE:
dbt sl validate --select metric:revenue

----------------------------------------

TITLE: Adding Service User permissions using Azure CLI
DESCRIPTION: This code snippet demonstrates how to use the Azure CLI to add the required permissions for the service user in Azure DevOps. It shows examples for adding ViewSubscriptions, EditSubscriptions, DeleteSubscriptions, PullRequestContribute, and GenericContribute permissions.

LANGUAGE: bash
CODE:
az devops security permission update --organization https://dev.azure.com/<org_name> --namespace-id cb594ebe-87dd-4fc9-ac2c-6a10a4c92046 --subject <service_account>@xxxxxx.onmicrosoft.com --token PublisherSecurity/<azure_devops_project_object_id> --allow-bit 1

az devops security permission update --organization https://dev.azure.com/<org_name> --namespace-id cb594ebe-87dd-4fc9-ac2c-6a10a4c92046 --subject <service_account>@xxxxxx.onmicrosoft.com --token PublisherSecurity/<azure_devops_project_object_id> --allow-bit 2

az devops security permission update --organization https://dev.azure.com/<org_name> --namespace-id cb594ebe-87dd-4fc9-ac2c-6a10a4c92046 --subject <service_account>@xxxxxx.onmicrosoft.com --token PublisherSecurity/<azure_devops_project_object_id> --allow-bit 4

az devops security permission update --organization https://dev.azure.com/<org_name> --namespace-id 2e9eb7ed-3c0a-47d4-87c1-0ffdd275fd87 --subject <service_account>@xxxxxx.onmicrosoft.com --token repoV2/<azure_devops_project_object_id>/<azure_devops_repository_object_id> --allow-bit 16384

az devops security permission update --organization https://dev.azure.com/<org_name> --namespace-id 2e9eb7ed-3c0a-47d4-87c1-0ffdd275fd87 --subject <service_account>@xxxxxx.onmicrosoft.com --token repoV2/<azure_devops_project_object_id>/<azure_devops_repository_object_id> --allow-bit 4

----------------------------------------

TITLE: Displaying Metric Time Code in Markdown
DESCRIPTION: This Markdown snippet shows how to reference the metric_time dimension in the context of filtering time dimensions in the Query Builder.

LANGUAGE: markdown
CODE:
For time dimensions, you can use the time range selector to filter on presets or custom options. The time range selector applies only to the primary time dimension (`metric_time`). For all other time dimensions that aren't `metric_time`, you can use the "Where" option to apply filters.

----------------------------------------

TITLE: Configuring LDAP Authentication for Impala in dbt
DESCRIPTION: Configuration for LDAP authentication with Impala, including HTTP transport and SSL options. Suitable for Cloudera Data Platform (CDP) environments.

LANGUAGE: yaml
CODE:
your_profile_name:
  target: dev
  outputs:
    dev:
     type: impala
     host: [host name]
     http_path: [optional, http path to Impala]
     port: [port]
     auth_type: ldap
     use_http_transport: [true / false]
     use_ssl: [true / false]
     username: [username]
     password: [password]
     dbname: [db name]
     schema: [schema name]
     retries: [retries]

----------------------------------------

TITLE: File Structure for dbt Dependencies Management
DESCRIPTION: The dbt project can use either a single dependencies.yml file for both package and project dependencies, or separate packages.yml and dependencies.yml files depending on requirements like Jinja templating needs and private package access.

LANGUAGE: yaml
CODE:
# dependencies.yml
# Can contain both package and project dependencies

# packages.yml
# Used for package dependencies, especially with Jinja templating or private repositories

----------------------------------------

TITLE: Pandas Integration Example
DESCRIPTION: Code showing how to convert query results to a pandas DataFrame.

LANGUAGE: python
CODE:
arrow_table = client.query(...)
pandas_df = arrow_table.to_pandas()

----------------------------------------

TITLE: Referencing DBT Adapters in Markdown
DESCRIPTION: Code references showing the recommended dbt-databricks adapter name and the legacy dbt-spark adapter name in markdown format.

LANGUAGE: markdown
CODE:
dbt-databricks
dbt-spark

----------------------------------------

TITLE: Configuring Snowflake Cortex User Permissions
DESCRIPTION: SQL commands to create and configure necessary roles and permissions for Snowflake Cortex access. These commands create a cortex_user_role and grant appropriate permissions to the Semantic Layer and deployment users.

LANGUAGE: sql
CODE:
create role cortex_user_role;
grant database role SNOWFLAKE.CORTEX_USER to role cortex_user_role;
grant role cortex_user_role to user SL_USER;
grant role cortex_user_role to user DEPLOYMENT_USER;

----------------------------------------

TITLE: Configuring Web Crawler Access Rules in robots.txt
DESCRIPTION: Basic robots.txt directives that specify crawler permissions. Sets a universal user-agent rule (*) and disallows access to the /learn directory.

LANGUAGE: robotstxt
CODE:
User-agent: *
Disallow: /learn

----------------------------------------

TITLE: Querying Defined Metrics in dbt Project
DESCRIPTION: SQL query to fetch all defined metrics in a dbt project using the semantic_layer.metrics() function.

LANGUAGE: sql
CODE:
select * from {{ 
	semantic_layer.metrics() 
}}

----------------------------------------

TITLE: Environment Query After Deprecation
DESCRIPTION: Updated environment query using BigInt data type after deprecation changes

LANGUAGE: graphql
CODE:
query ($environmentId: BigInt!, $first: Int!) {
environment(id: $environmentId) {
    applied {
    models(first: $first) {
        edges {
        node {
            uniqueId
            executionInfo {
            lastRunId
            }
        }
        }
    }
    }
}
}

----------------------------------------

TITLE: Configuring Time Spine Models in YAML
DESCRIPTION: Example YAML configuration for hourly and daily time spine models in dbt, specifying the standard granularity column and custom granularities.

LANGUAGE: yaml
CODE:
models: 
  - name: time_spine_hourly 
    description: my favorite time spine
    time_spine:
      standard_granularity_column: date_hour 
      custom_granularities:
        - name: fiscal_year
          column_name: fiscal_year_column
    columns:
      - name: date_hour
        granularity: hour 

  - name: time_spine_daily
    time_spine:
      standard_granularity_column: date_day 
    columns:
      - name: date_day
        granularity: day

----------------------------------------

TITLE: Basic Ratio Metric Example in dbt YAML
DESCRIPTION: This example demonstrates a basic ratio metric that calculates the ratio of food orders to total orders. It includes the essential parameters like name, description, label, type, and type_params with numerator and denominator.

LANGUAGE: yaml
CODE:
metrics:
  - name: food_order_pct
    description: "The food order count as a ratio of the total order count"
    label: Food order ratio
    type: ratio
    type_params: 
      numerator: food_orders
      denominator: orders

----------------------------------------

TITLE: Defining Semantic Models in YAML
DESCRIPTION: YAML configuration for defining semantic models for orders and customers, including entities, measures, and dimensions.

LANGUAGE: yaml
CODE:
semantic_models:
  - name: orders
    description: |
      A model containing order data. The grain of the table is the order id.
    model: ref('orders')
    defaults:
      agg_time_dimension: metric_time
    entities:
      - name: order_id
        type: primary
      - name: customer
        type: foreign
        expr: customer_id
    measures:
      - name: order_total
        agg: sum
    dimensions:
      - name: metric_time
        expr: cast(ordered_at as date)
        type: time
        type_params:
          time_granularity: day
  - name: customers
    description: >
      Customer dimension table. The grain of the table is one row per
        customer.
    model: ref('customers')
    defaults:
      agg_time_dimension: first_ordered_at
    entities:
      - name: customer 
        type: primary
        expr: customer_id
    dimensions:
      - name: is_new_customer
        type: categorical
        expr: case when first_ordered_at is not null then true else false end
      - name: first_ordered_at
        type: time
        type_params:
          time_granularity: day

----------------------------------------

TITLE: Using Jinja in SQL Queries for dbt
DESCRIPTION: This example shows how to incorporate Jinja logic and macros within a SQL query. It includes conditional statements, variable interpolation, and calling a custom macro, all formatted according to the provided style guide.

LANGUAGE: sql
CODE:
select
    entity_id,
    entity_type,
    {% if this %}

        {{ that }},

    {% else %}

        {{ the_other_thing }},

    {% endif %}
    {{ make_cool('uncool_id') }} as cool_id

----------------------------------------

TITLE: Executing dbt Semantic Layer Commands
DESCRIPTION: Essential dbt commands for working with the Semantic Layer, including parsing semantic manifests, listing dimensions, and querying metrics during development.

LANGUAGE: bash
CODE:
dbt parse

LANGUAGE: bash
CODE:
dbt sl list dimensions --metrics [metric name]

LANGUAGE: bash
CODE:
dbt sl query [query options]

----------------------------------------

TITLE: Implementing Month-over-Month Revenue Growth Metric
DESCRIPTION: Defines a derived metric that calculates the month-over-month revenue growth percentage using offset windows and mathematical expressions.

LANGUAGE: yaml
CODE:
- name: revenue_growth_mom
  description: "Percentage growth of revenue compared to 1 month ago. Excluded tax"
  type: derived
  label: Revenue Growth % M/M
  type_params:
    expr: (current_revenue - revenue_prev_month) * 100 / revenue_prev_month
    metrics:
      - name: revenue
        alias: current_revenue
      - name: revenue
        offset_window: 1 month
        alias: revenue_prev_month

----------------------------------------

TITLE: Querying Metrics with dbt Cloud CLI
DESCRIPTION: Command line examples for querying metrics using the dbt Cloud CLI. Shows how to query revenue by month and list available dimensions for the revenue metric.

LANGUAGE: bash
CODE:
dbt sl query revenue --group-by metric_time__month
dbt sl list dimensions --metrics revenue # list all dimensions available for the revenue metric

----------------------------------------

TITLE: Build Command for Modified Models
DESCRIPTION: Shell command to build all modified models and their downstream dependencies after cloning.

LANGUAGE: shell
CODE:
dbt build --select state:modified+

----------------------------------------

TITLE: Configuring dbt-databricks Authentication Profile
DESCRIPTION: Simplified configuration for dbt-databricks authentication in profiles.yml showing the streamlined connection parameters without ODBC driver requirement.

LANGUAGE: yaml
CODE:
your_profile_name:
  target: dev
  outputs:
    dev:
      type: databricks
      schema: my_schema
      host:  dbc-l33t-nwb.cloud.databricks.com
      http_path: /sql/1.0/endpoints/8657cad335ae63e3
      token: [my_secret_token]

----------------------------------------

TITLE: Python ML Model Training
DESCRIPTION: Python code to train a logistic regression model for predicting Formula 1 race positions

LANGUAGE: python
CODE:
import snowflake.snowpark.functions as F
from sklearn.model_selection import train_test_split
import pandas as pd
from sklearn.metrics import confusion_matrix, balanced_accuracy_score
import io
from sklearn.linear_model import LogisticRegression
from joblib import dump, load
import joblib
import logging
import sys
from joblib import dump, load

----------------------------------------

TITLE: Setting up Formula 1 Database and Tables
DESCRIPTION: SQL code to create and configure Formula 1 database structure, file formats, and load data from S3

LANGUAGE: sql
CODE:
-- create and define our formula1 database
create or replace database formula1;
use database formula1; 
create or replace schema raw; 
use schema raw; 

-- define our file format for reading in the csvs 
create or replace file format csvformat
type = csv
field_delimiter =','
field_optionally_enclosed_by = '"', 
skip_header=1;

----------------------------------------

TITLE: Analyzing table statistics in SQL
DESCRIPTION: This SQL command computes statistics for specific columns in a table. These statistics are used by the query optimizer to select optimal execution plans.

LANGUAGE: sql
CODE:
ANALYZE TABLE mytable COMPUTE STATISTICS FOR

COLUMNS col1, col2, col3

----------------------------------------

TITLE: Basic SQL HAVING Clause Structure
DESCRIPTION: Demonstrates the basic structure of a SQL query using the HAVING clause. It shows where HAVING fits in relation to other clauses like GROUP BY and ORDER BY.

LANGUAGE: sql
CODE:
select
	-- query
from <table>
group by <field(s)>
having condition
[optional order by]

----------------------------------------

TITLE: Example dbt YAML File Structure
DESCRIPTION: Demonstrates the file naming flexibility for dbt YAML configuration files. Files can be placed in models/, seeds/, snapshots/, or macros/ directories depending on their purpose.

LANGUAGE: yaml
CODE:
whatever_you_want.yml

----------------------------------------

TITLE: SQL LEFT JOIN Example with dbt References
DESCRIPTION: Practical example showing a LEFT JOIN between car_type and car_color tables using dbt's ref function. Demonstrates joining tables based on user_id and selecting specific columns.

LANGUAGE: sql
CODE:
select
   car_type.user_id as user_id,
   car_type.car_type as type,
   car_color.car_color as color
from {{ ref('car_type') }} as car_type
left join {{ ref('car_color') }} as car_color
on car_type.user_id = car_color.user_id

----------------------------------------

TITLE: Creating and Activating Python Virtual Environment for dbt
DESCRIPTION: Commands to create and activate a Python virtual environment specifically for dbt installation. Includes commands for both Windows and Unix-based systems (Mac/Linux).

LANGUAGE: shell
CODE:
python3 -m venv dbt-env
source dbt-env/bin/activate
dbt-env\Scripts\activate

----------------------------------------

TITLE: Configuring Materializations in dbt_project.yml
DESCRIPTION: This YAML snippet demonstrates how to configure materializations for different folders in a dbt project. It shows how to set views for marketing and paid_ads folders, while setting tables for the google subfolder.

LANGUAGE: yaml
CODE:
models:
  jaffle_shop:
    marketing:
      +materialized: view
      paid_ads:
        google:
          +materialized: table

----------------------------------------

TITLE: Displaying dbt Core and Plugin Versions
DESCRIPTION: This code snippet demonstrates how to use the 'dbt --version' command to display the installed versions of dbt Core and its plugins, as well as checking if they are up to date.

LANGUAGE: shell
CODE:
$ dbt --version
Core:
  - installed: 1.8.0
  - latest:    1.8.0 - Up to date!

Plugins:
  - snowflake: 1.9.0 - Up to date!

----------------------------------------

TITLE: Building Order Items Mart SQL Model
DESCRIPTION: Creates a denormalized order items table by joining orders, products, and supplies information. This logical mart serves as the foundation for the semantic model, bringing together all relevant dimensions and measures with proper timestamps.

LANGUAGE: sql
CODE:
{{
   config(
      materialized = 'table',
   )
}}

with

order_items as (

   select * from {{ ref('stg_order_items') }}

),

orders as (

   select * from {{ ref('stg_orders')}}

),

products as (

   select * from {{ ref('stg_products') }}

),

supplies as (

   select * from {{ ref('stg_supplies') }}

),

order_supplies_summary as (

   select
      product_id,
      sum(supply_cost) as supply_cost

   from supplies

   group by 1
),

joined as (

   select
      order_items.*,
      products.product_price,
      order_supplies_summary.supply_cost,
      products.is_food_item,
      products.is_drink_item,
      orders.ordered_at

   from order_items

   left join orders on order_items.order_id  = orders.order_id

   left join products on order_items.product_id = products.product_id

   left join order_supplies_summary on order_items.product_id = order_supplies_summary.product_id

)

select * from joined

----------------------------------------

TITLE: Adding Experimental Features Package in YAML
DESCRIPTION: This YAML snippet shows how to add the experimental-features package to your dbt project to continue using the insert_by_period materialization.

LANGUAGE: yaml
CODE:
packages:
  - git: https://github.com/dbt-labs/dbt-labs-experimental-features
    subdirectory: insert_by_period
    revision: XXXX #optional but highly recommended. Provide a full git sha hash, e.g. 1c0bfacc49551b2e67d8579cf8ed459d68546e00. If not provided, uses the current HEAD.

----------------------------------------

TITLE: Referencing a Logical Model in Semantic Model YAML
DESCRIPTION: Example of how to reference a corresponding logical model using the 'ref' function in a semantic model definition.

LANGUAGE: yaml
CODE:
semantic_models:
  - name: orders
    description: |
      Model containing order data. The grain of the table is the order id.
    model: ref('stg_orders')
    entities: ...
    dimensions: ...
    measures: ...

----------------------------------------

TITLE: Installing dbt Core and Adapter in Python
DESCRIPTION: Example command for installing both dbt-core and a specific adapter (Snowflake in this case) using pip. This installation method is recommended since dbt Core v1.8 and may become required in future versions.

LANGUAGE: sql
CODE:
python3 -m pip install dbt-core dbt-snowflake

----------------------------------------

TITLE: Clone Command for Modified Incremental Models
DESCRIPTION: Shell command to clone all pre-existing incremental models that have been modified or are downstream dependencies.

LANGUAGE: shell
CODE:
dbt clone --select state:modified+,config.materialized:incremental,state:old

----------------------------------------

TITLE: Configuring Staging Environment Connection in YAML
DESCRIPTION: YAML configuration for staging environment connection settings in dbt Cloud, using service account authentication with increased thread count for better performance.

LANGUAGE: yaml
CODE:
account: 123dev
project: staging
dataset: main
method: service-account-json
threads: 16

----------------------------------------

TITLE: Defining dbt Community Page Structure with JSX and Markdown
DESCRIPTION: JSX/Markdown hybrid component that structures the community joining page using a section container and grid layout with Card components for different engagement channels

LANGUAGE: jsx
CODE:
---
title: Join the Community
id: join
---

<section className="community-home">

<div className="grid--3-col">

<Card
    title="Join us on Slack"
    body="Follow the pulse of the dbt Community! Chat with other practitioners in your city, country or worldwide about data work, tech stacks, or simply share a killer meme."
link="https://www.getdbt.com/community/join-the-community/"
    icon="slack"
/>

<Card
    title="Community Forum"
    body="Have a question about how to do something in dbt? Hop into the Community Forum and work with others to create long lived community knowledge."
    link="/community/forum"
    icon="discussions"
/>

<Card
    title="How to contribute"
    body="Want to get involved? This is the place! Learn how to contribute to our open source repositories, write for the blog, speak at a meetup and more."
    link="community/contribute" icon="pencil-paper"
/>

<Card
    title="Code of Conduct"
    body="We are committed to creating a space where everyone can feel welcome and safe. Our Code of Conduct reflects the agreement that all Community members make to uphold these ideals."
    link="community/resources/code-of-conduct/"
    icon="folder"
/>

<Card
    title="Upcoming events"
    body="Whether it's in-person Meetups in your local area, Coalesce ‚Äì the annual Analytics Engineering Conference ‚Äì or online Office Hours there are options for everyone."
    link="community/events"
    icon="calendar" />

<Card
    title="Watch past events"
    body="Get a taste for the energy of our live events, get inspired, or prepare for an upcoming event by watching recordings from our YouTube archives."
    link="https://www.youtube.com/playlist?list=PL0QYlrC86xQl1DGKBopQZiZ6tSqrMlD2M"
    icon="star"
/>

</div>
</section>

----------------------------------------

TITLE: Creating Environment-Specific Target Lag Macro
DESCRIPTION: Jinja macro to set different target lag values for production and development environments.

LANGUAGE: sql
CODE:
{% macro target_lag_environment() %}\n{% set lag = '1 minute' if target.name == "prod" else '35 days' %}\n{{ return(lag) }}\n{% endmacro %}

----------------------------------------

TITLE: Launching fly.io App in Shell
DESCRIPTION: Command to launch and deploy the fly.io application, which will prompt for configuration options.

LANGUAGE: shell
CODE:
flyctl launch

----------------------------------------

TITLE: Configuring on-run-end Hook in dbt_project.yml
DESCRIPTION: This YAML snippet shows how to configure the on-run-end hook in dbt_project.yml to call a custom macro for project evaluation.

LANGUAGE: yaml
CODE:
on-run-end: "{{ dbt_project_evaluator.print_dbt_project_evaluator_issues() }}"

----------------------------------------

TITLE: Azure DevOps pipeline for running dbt Cloud job on merge
DESCRIPTION: Azure DevOps pipeline configuration to run a dbt Cloud job when code is merged to the main branch. Requires setting variables for dbt Cloud API access.

LANGUAGE: yaml
CODE:
name: Run dbt Cloud Job

trigger: [ main ] # runs on pushes to main

variables:
  DBT_URL:                 https://cloud.getdbt.com # no trailing backslash, adjust this accordingly for single-tenant deployments
  DBT_JOB_CAUSE:           'Azure Pipeline CI Job' # provide a descriptive job cause here for easier debugging down the road
  DBT_ACCOUNT_ID:          00000 # enter your account id
  DBT_PROJECT_ID:          00000 # enter your project id
  DBT_PR_JOB_ID:           00000 # enter your job id

steps:
  - task: UsePythonVersion@0
    inputs:
      versionSpec: '3.7'
    displayName: 'Use Python 3.7'

  - script: |
      python -m pip install requests
    displayName: 'Install python dependencies'

  - script: |
      python -u ./python/run_and_monitor_dbt_job.py
    displayName: 'Run dbt job '
    env:
      DBT_API_KEY: $(DBT_API_KEY) # Set these values as secrets in the Azure pipelines Web UI

----------------------------------------

TITLE: Date Parsing Python Model in dbt
DESCRIPTION: A dbt Python model that uses the dateutil library to parse various datetime string formats into standardized datetime objects. The model reads data from a source table into a dataframe, applies the parsing function, and returns the transformed dataframe.

LANGUAGE: python
CODE:
import dateutil

def try_dateutil_parse(x):
    try:
        return dateutil.parser.parse(x)
    except:
        return

def model(dbt, session):
    df = dbt.ref("source_data")
    df['parsed_transaction_time'] = df['transaction_time'].apply(try_dateutil_parse)
    return df

----------------------------------------

TITLE: Python Slack Message Processing for dbt Cloud Errors
DESCRIPTION: Python script that processes dbt Cloud run data and formats error messages for Slack threads. Focuses on extracting error information from run logs and formatting it for threaded Slack messages.

LANGUAGE: python
CODE:
import re

# Access secret credentials
secret_store = StoreClient('YOUR_SECRET_HERE')
api_token = secret_store.get('DBT_CLOUD_SERVICE_TOKEN')

commands_to_skip_logs = ['dbt source', 'dbt docs']
run_id = input_data['run_id']
account_id = input_data['account_id']
url = f'https://YOUR_ACCESS_URL/api/v2/accounts/{account_id}/runs/{run_id}/?include_related=["run_steps"]'
headers = {'Authorization': f'Token {api_token}'}

response = requests.get(url, headers=headers)
response.raise_for_status()
results = response.json()['data']

threaded_errors_post = ""
for step in results['run_steps']:
  show_logs = not any(cmd in step['name'] for cmd in commands_to_skip_logs)
  if not show_logs:
    continue
  if step['status_humanized'] != 'Success':
    full_log = step['logs']
    full_log = re.sub('\x1b?\[[0-9]+m[0-9:]*', '', full_log)
    
    summary_start = re.search('(?:Completed with \d+ error.* and \d+ warnings?:|Database Error|Compilation Error|Runtime Error)', full_log)
    
    line_items = re.findall('(^.*(?:Failure|Error) in .*\n.*\n.*)', full_log, re.MULTILINE)
    if not summary_start:
      continue
      
    threaded_errors_post += f"""
*{step['name']}*
"""    
    if len(line_items) == 0:
      relevant_log = f'```{full_log[summary_start.start():]}```'
    else:
      relevant_log = summary_start[0]
      for item in line_items:
        relevant_log += f'\n```\n{item.strip()}\n```\n'
    threaded_errors_post += f"""
{relevant_log}
"""

output = {'threaded_errors_post': threaded_errors_post}

----------------------------------------

TITLE: Using DATE_TRUNC with dbt Macro
DESCRIPTION: Illustrates how to use the DATE_TRUNC function with a dbt macro, which simplifies cross-database compatibility and syntax.

LANGUAGE: sql
CODE:
select
	order_id,
	order_date,
	{{ date_trunc("week", "order_date") }} as order_week,
	{{ date_trunc("month", "order_date") }} as order_month,
	{{ date_trunc("year", "order_date") }} as order_year
from {{ ref('orders') }}

----------------------------------------

TITLE: Sample Output of dbt Invocation List Command
DESCRIPTION: Illustrates the expected output when running the 'dbt invocation list' command, showing details of an active invocation including ID, status, type, arguments, and start time.

LANGUAGE: bash
CODE:
dbt invocation list

Active Invocations:
  ID                             6dcf4723-e057-48b5-946f-a4d87e1d117a
  Status                         running
  Type                           cli
  Args                           [run --select test.sql]
  Started At                     2025-01-24 11:03:19

‚ûú  jaffle-shop git:(test-cli) ‚úó 

----------------------------------------

TITLE: Configuring Snapshot in dbt SQL
DESCRIPTION: Example of configuring a dbt snapshot using the check strategy on a change_id column.

LANGUAGE: SQL
CODE:
{% snapshot snp_product %}
{{{
   config(
     target_schema=generate_schema_name('snapshots'),
     unique_key='assetid',
     strategy='check',
     check_cols=['change_id']
   )
}}}
select * from {{ ref('base_product') }}
{% endsnapshot %}

----------------------------------------

TITLE: JSON Output Examples
DESCRIPTION: Examples showing how to output results in JSON format, including custom key selection for the output.

LANGUAGE: bash
CODE:
$ dbt ls --select snowplow.* --output json
{"name": "snowplow_events", "resource_type": "model", "package_name": "snowplow",  ...}
{"name": "snowplow_page_views", "resource_type": "model", "package_name": "snowplow",  ...}
...

LANGUAGE: bash
CODE:
$ dbt ls --select snowplow.* --output json --output-keys "name resource_type description"
{"name": "snowplow_events", "description": "This is a pretty cool model",  ...}
{"name": "snowplow_page_views", "description": "This model is even cooler",  ...}
...

----------------------------------------

TITLE: Implementing Basic Lambda View Union in SQL and Jinja
DESCRIPTION: Basic implementation of a lambda view that unions current view and historical table data using run_started_at timestamp for filtering.

LANGUAGE: sql
CODE:
with current_view as (
    select * from {{ ref('current_view') }}
    where max_collector_tstamp >= '{{ run_started_at }}'
),

historical_table as (
    select * from {{ ref('historical_table') }}
    where max_collector_tstamp < '{{ run_started_at }}'
),

unioned_tables as (
    select * from current_view
    union all
    select * from historical_table
)

select * from unioned_tables

----------------------------------------

TITLE: Listing Tests by Tag
DESCRIPTION: Example demonstrating how to list tests with a specific tag using both select and resource-type arguments.

LANGUAGE: bash
CODE:
$ dbt ls --select tag:nightly --resource-type test
my_project.schema_test.not_null_orders_order_id
my_project.schema_test.unique_orders_order_id
my_project.schema_test.not_null_products_product_id
my_project.schema_test.unique_products_product_id
...

----------------------------------------

TITLE: Creating Partial Files in Markdown
DESCRIPTION: Example of creating and implementing partial files for content reuse in documentation.

LANGUAGE: markdown
CODE:
/snippets/_partial-name.md

LANGUAGE: markdown
CODE:
## Header 2

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam fermentum porttitor dui, id scelerisque enim scelerisque at.

LANGUAGE: markdown
CODE:
Docs content here.

import SetUpPages from '/snippets/_partial-name.md';
<!-- It's important to leave a blank line or a comment between import and usage, otherwise it won't work -->
<SetUpPages />

Docs content here.

----------------------------------------

TITLE: Running a Single Snapshot in dbt using Shell Command
DESCRIPTION: This command demonstrates how to run a specific snapshot named 'order_snapshot' using dbt's snapshot command with the --select flag. This allows for targeted execution of individual snapshots within a dbt project.

LANGUAGE: shell
CODE:
$ dbt snapshot --select order_snapshot

----------------------------------------

TITLE: Creating Collapsible Sections in dbt Docs
DESCRIPTION: Demonstrates how to create collapsible sections with headers in documentation.

LANGUAGE: markdown
CODE:
<Collapsible header="The header info">
<div>
<p>Shows and hides children elements</p>
</div>
</Collapsible>
</div>

----------------------------------------

TITLE: Running dbt with Debug Logging
DESCRIPTION: Demonstrates how to run dbt with debug-level logging enabled, which provides verbose output.

LANGUAGE: text
CODE:
dbt --debug run
...

----------------------------------------

TITLE: Complete Snapshot Configuration Example (v1.9+)
DESCRIPTION: Comprehensive example of a snapshot configuration in YAML format for dbt version 1.9 and above.

LANGUAGE: yaml
CODE:
snapshots:
  - name: orders_snapshot
    relation: source('jaffle_shop', 'orders')
    config:
      schema: snapshots
      database: analytics
      unique_key: id
      strategy: timestamp
      updated_at: updated_at
      invalidate_hard_deletes: true

----------------------------------------

TITLE: Example of JSON Log Format in dbt
DESCRIPTION: Shows the structured JSON format for logs, which includes detailed information about each log entry.

LANGUAGE: json
CODE:
{"data": {"log_version": 3, "version": "=1.8.0"}, "info": {"category": "", "code": "A001", "extra": {}, "invocation_id": "82131fa0-d2b4-4a77-9436-019834e22746", "level": "info", "msg": "Running with dbt=1.8.0", "name": "MainReportVersion", "pid": 7875, "thread": "MainThread", "ts": "2024-05-29T23:32:54.993336Z"}}
{"data": {"adapter_name": "postgres", "adapter_version": "=1.8.0"}, "info": {"category": "", "code": "E034", "extra": {}, "invocation_id": "82131fa0-d2b4-4a77-9436-019834e22746", "level": "info", "msg": "Registered adapter: postgres=1.8.0", "name": "AdapterRegistered", "pid": 7875, "thread": "MainThread", "ts": "2024-05-29T23:32:56.437986Z"}}

----------------------------------------

TITLE: Setting invalidate_hard_deletes in Jinja SQL
DESCRIPTION: Example of configuring hard delete invalidation within a snapshot SQL file using Jinja templating.

LANGUAGE: jinja2
CODE:
{{
  config(
    strategy="timestamp",
    invalidate_hard_deletes=True
  )
}}

----------------------------------------

TITLE: Running dbt Compile Without Cache Population
DESCRIPTION: This command demonstrates how to quickly compile a model in dbt without populating the cache, which can be useful when the model doesn't require database metadata or introspective queries.

LANGUAGE: text
CODE:
dbt --no-populate-cache compile --select my_model_name

----------------------------------------

TITLE: Complete snapshot schema example with hard_deletes
DESCRIPTION: Comprehensive example showing how to configure a snapshot with hard_deletes and additional metadata columns for tracking deleted records.

LANGUAGE: yaml
CODE:
snapshots:
  - name: my_snapshot
    config:
      hard_deletes: new_record  # options are: 'ignore', 'invalidate', or 'new_record'
      strategy: timestamp
      updated_at: updated_at
    columns:
      - name: dbt_valid_from
        description: Timestamp when the record became valid.
      - name: dbt_valid_to
        description: Timestamp when the record stopped being valid.
      - name: dbt_is_deleted
        description: Indicates whether the record was deleted.

----------------------------------------

TITLE: Test Results JSON Output
DESCRIPTION: Example JSON output from dbt test command showing test execution results and compiled test SQL.

LANGUAGE: json
CODE:
"results": [
    {
      "status": "pass",
      "timing": [
        {
          "name": "compile",
          "started_at": "2023-10-12T17:20:51.279437Z",
          "completed_at": "2023-10-12T17:20:51.317312Z"
        },
        {
          "name": "execute",
          "started_at": "2023-10-12T17:20:51.319812Z",
          "completed_at": "2023-10-12T17:20:51.441967Z"
        }
      ],
      "thread_id": "Thread-2",
      "execution_time": 0.1807551383972168,
      "adapter_response": {
        "_message": "SELECT 1",
        "code": "SELECT",
        "rows_affected": 1
      },
      "message": null,
      "failures": 0,
      "unique_id": "test.my_project.unique_my_model_created_at.a9276afbbb",
      "compiled": true,
      "compiled_code": "\n    \n    \n\nselect\n    created_at as unique_field,\n    count(*) as n_records\n\nfrom \"postgres\".\"dbt_dbeatty\".\"my_model\"\nwhere created_at is not null\ngroup by created_at\nhaving count(*) > 1\n\n\n",
      "relation_name": null
    }

----------------------------------------

TITLE: Configuring Legacy Surrogate Key Behavior in YAML
DESCRIPTION: This YAML snippet shows how to enable legacy behavior for the surrogate_key function in the dbt project configuration file.

LANGUAGE: yaml
CODE:
#dbt_project.yml
vars:
  surrogate_key_treat_nulls_as_empty_strings: true #turn on legacy behavior

----------------------------------------

TITLE: Source Freshness Query Generation
DESCRIPTION: SQL query generated by DBT for checking source freshness, showing both compiled and template versions.

LANGUAGE: sql
CODE:
select
  max(_etl_loaded_at) as max_loaded_at,
  convert_timezone('UTC', current_timestamp()) as snapshotted_at
from raw.jaffle_shop.orders

where datediff('day', _etl_loaded_at, current_timestamp) < 2

LANGUAGE: sql
CODE:
select
  max({{ loaded_at_field }}) as max_loaded_at,
  {{ current_timestamp() }} as snapshotted_at
from {{ source }}
{% if filter %}
where {{ filter }}
{% endif %}

----------------------------------------

TITLE: Error Message for Schema Version Mismatch
DESCRIPTION: Example error message shown when there's a mismatch between state artifacts schema versions during state-based operations.

LANGUAGE: text
CODE:
Expected a schema version of "https://schemas.getdbt.com/dbt/manifest/v5.json" in <state-path>/manifest.json, but found "https://schemas.getdbt.com/dbt/manifest/v4.json". Are you running with a different version of dbt?

----------------------------------------

TITLE: Timestamp Casting Examples
DESCRIPTION: Examples of casting date fields to timestamps for the loaded_at_field configuration, including timezone conversion.

LANGUAGE: yaml
CODE:
loaded_at_field: "completed_date::timestamp"

LANGUAGE: yaml
CODE:
loaded_at_field: "CAST(completed_date AS TIMESTAMP)"

LANGUAGE: yaml
CODE:
loaded_at_field: "convert_timezone('Australia/Sydney', 'UTC', created_at_local)"

----------------------------------------

TITLE: Configuring Future Model Deprecation in DBT
DESCRIPTION: Example of setting a future deprecation date for a DBT model using version 2 YAML configuration. Demonstrates the newer configuration format with a future deprecation date.

LANGUAGE: yaml
CODE:
version: 2
models:
  - name: my_model
    description: deprecating in the future
    deprecation_date: 2999-01-01 00:00:00.00+00:00

----------------------------------------

TITLE: Basic Model Description in YAML
DESCRIPTION: Shows how to add simple descriptions to a model and its columns in schema.yml

LANGUAGE: yaml
CODE:
version: 2

models:
  - name: dim_customers
    description: One record per customer

    columns:
      - name: customer_id
        description: Primary key

----------------------------------------

TITLE: Defining Source Database in dbt YAML Configuration
DESCRIPTION: This snippet demonstrates how to specify the database for a source in a dbt YAML configuration file. It includes the version, source name, database name, and table names.

LANGUAGE: yaml
CODE:
version: 2

sources:
  - name: <source_name>
    database: <database_name>
    tables:
      - name: <table_name>
      - ...

----------------------------------------

TITLE: Configuring Basic On-Run Hooks in dbt
DESCRIPTION: Basic configuration structure for on-run-start and on-run-end hooks in dbt_project.yml. These hooks can accept either a single SQL statement or an array of SQL statements to be executed.

LANGUAGE: yml
CODE:
on-run-start: sql-statement | [sql-statement]
on-run-end: sql-statement | [sql-statement]

----------------------------------------

TITLE: Basic Defer Command Usage in Shell
DESCRIPTION: Examples of using the defer command with dbt run and test operations to utilize production artifacts

LANGUAGE: shell
CODE:
dbt run --select [...] --defer --state path/to/artifacts
dbt test --select [...] --defer --state path/to/artifacts

----------------------------------------

TITLE: Using Source Status Selector in dbt
DESCRIPTION: This set of commands demonstrates how to use the source_status selector to run dbt operations on fresher sources and their children. It requires running dbt source freshness before using the selector.

LANGUAGE: bash
CODE:
dbt source freshness
dbt build --select "source_status:fresher+" --state path/to/prod/artifacts

----------------------------------------

TITLE: Configuring Project Evaluator Severity in dbt_project.yml
DESCRIPTION: YAML configuration to set the severity level for dbt project evaluator tests using environment variables.

LANGUAGE: yaml
CODE:
tests:
  dbt_project_evaluator:
    +severity: "{{ env_var('DBT_PROJECT_EVALUATOR_SEVERITY', 'warn') }}"

----------------------------------------

TITLE: Checking dbt Core Version
DESCRIPTION: Use this command to verify the installed version of dbt Core.

LANGUAGE: shell
CODE:
dbt --version

----------------------------------------

TITLE: Configuring dbt Profiles for Exasol Connection
DESCRIPTION: YAML configuration for setting up Exasol connection in dbt profiles.yml file. Includes basic authentication parameters like host, port, username, password, database name, and schema.

LANGUAGE: yaml
CODE:
dbt-exasol:
  target: dev
  outputs:
    dev:
      type: exasol
      threads: 1
      dsn: HOST:PORT
      user: USERNAME
      password: PASSWORD
      dbname: db
      schema: SCHEMA

----------------------------------------

TITLE: Compiling dbt Analyses
DESCRIPTION: This command compiles the analysis SQL files in a dbt project. After running this command, the compiled SQL can be found in the target/compiled directory and used in other tools like data visualization platforms.

LANGUAGE: shell
CODE:
dbt compile

----------------------------------------

TITLE: Compiling dbt Analyses
DESCRIPTION: This command compiles the analysis SQL files in a dbt project. After running this command, the compiled SQL can be found in the target/compiled directory and used in other tools like data visualization platforms.

LANGUAGE: shell
CODE:
dbt compile

----------------------------------------

TITLE: Adding a Snapshot to a Project
DESCRIPTION: Example of how to add a snapshot to a dbt project, including creating the file, defining the snapshot block, and adding configurations.

LANGUAGE: sql
CODE:
{% snapshot orders_snapshot %}

{{
    config(
      target_database='analytics',
      target_schema='snapshots',
      unique_key='id',

      strategy='timestamp',
      updated_at='updated_at',
    )
}}

select * from {{ source('jaffle_shop', 'orders') }}

{% endsnapshot %}