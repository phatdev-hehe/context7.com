TITLE: Initializing AsyncWebCrawler and Performing a Basic Crawl in Python
DESCRIPTION: This code demonstrates how to create an AsyncWebCrawler instance, fetch a webpage, and print the first 300 characters of its Markdown output. The script uses asyncio for asynchronous execution.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://example.com")
        print(result.markdown[:300])  # Print first 300 chars

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Complete AsyncWebCrawler Usage Example in Python
DESCRIPTION: A comprehensive example demonstrating the complete workflow of using AsyncWebCrawler with BrowserConfig and CrawlerRunConfig, including setting up an extraction strategy, performing a crawl, and processing the results. This showcases the recommended approach for a typical crawling scenario.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
import json

async def main():
    # 1. Browser config
    browser_cfg = BrowserConfig(
        browser_type="firefox",
        headless=False,
        verbose=True
    )

    # 2. Run config
    schema = {
        "name": "Articles",
        "baseSelector": "article.post",
        "fields": [
            {
                "name": "title", 
                "selector": "h2", 
                "type": "text"
            },
            {
                "name": "url", 
                "selector": "a", 
                "type": "attribute", 
                "attribute": "href"
            }
        ]
    }

    run_cfg = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        extraction_strategy=JsonCssExtractionStrategy(schema),
        word_count_threshold=15,
        remove_overlay_elements=True,
        wait_for="css:.post"  # Wait for posts to appear
    )

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(
            url="https://example.com/blog",
            config=run_cfg
        )

        if result.success:
            print("Cleaned HTML length:", len(result.cleaned_html))
            if result.extracted_content:
                articles = json.loads(result.extracted_content)
                print("Extracted articles:", articles[:2])
        else:
            print("Error:", result.error_message)

asyncio.run(main())

----------------------------------------

TITLE: Implementing an Asynchronous Web Crawler with crawl4ai in Python
DESCRIPTION: A complete example demonstrating how to use crawl4ai to implement an asynchronous web crawler. The code shows browser configuration, extraction strategy setup, LLM content filtering, and crawler execution. It handles both successful content extraction and error cases.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def main():
    # 1) Browser config: headless, bigger viewport, no proxy
    browser_conf = BrowserConfig(
        headless=True,
        viewport_width=1280,
        viewport_height=720
    )

    # 2) Example extraction strategy
    schema = {
        "name": "Articles",
        "baseSelector": "div.article",
        "fields": [
            {"name": "title", "selector": "h2", "type": "text"},
            {"name": "link", "selector": "a", "type": "attribute", "attribute": "href"}
        ]
    }
    extraction = JsonCssExtractionStrategy(schema)

    # 3) Example LLM content filtering

    gemini_config = LLMConfig(
        provider="gemini/gemini-1.5-pro" 
        api_token = "env:GEMINI_API_TOKEN"
    )

    # Initialize LLM filter with specific instruction
    filter = LLMContentFilter(
        llm_config=gemini_config,  # or your preferred provider
        instruction="""
        Focus on extracting the core educational content.
        Include:
        - Key concepts and explanations
        - Important code examples
        - Essential technical details
        Exclude:
        - Navigation elements
        - Sidebars
        - Footer content
        Format the output as clean markdown with proper code blocks and headers.
        """,
        chunk_token_threshold=500,  # Adjust based on your needs
        verbose=True
    )

    md_generator = DefaultMarkdownGenerator(
    content_filter=filter,
    options={"ignore_links": True}

    # 4) Crawler run config: skip cache, use extraction
    run_conf = CrawlerRunConfig(
        markdown_generator=md_generator,
        extraction_strategy=extraction,
        cache_mode=CacheMode.BYPASS,
    )

    async with AsyncWebCrawler(config=browser_conf) as crawler:
        # 4) Execute the crawl
        result = await crawler.arun(url="https://example.com/news", config=run_conf)

        if result.success:
            print("Extracted content:", result.extracted_content)
        else:
            print("Error:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Configuring AsyncWebCrawler with BrowserConfig and CrawlerRunConfig in Python
DESCRIPTION: This example shows how to customize the crawler's behavior using BrowserConfig for browser settings and CrawlerRunConfig for controlling the crawl process. It sets the browser to headless mode and bypasses caching for fresh content.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

async def main():
    browser_conf = BrowserConfig(headless=True)  # or False to see the browser
    run_conf = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler(config=browser_conf) as crawler:
        result = await crawler.arun(
            url="https://example.com",
            config=run_conf
        )
        print(result.markdown)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Using JsonCssExtractionStrategy for Structured Data Extraction in Crawl4AI
DESCRIPTION: Demonstrates how to extract structured data using CSS selectors. This approach defines a schema with nested fields to extract news article teasers including categories, headlines, summaries, and images.

LANGUAGE: python
CODE:
async def extract_news_teasers():
    schema = {
        "name": "News Teaser Extractor",
        "baseSelector": ".wide-tease-item__wrapper",
        "fields": [
            {
                "name": "category",
                "selector": ".unibrow span[data-testid='unibrow-text']",
                "type": "text",
            },
            {
                "name": "headline",
                "selector": ".wide-tease-item__headline",
                "type": "text",
            },
            {
                "name": "summary",
                "selector": ".wide-tease-item__description",
                "type": "text",
            },
            {
                "name": "time",
                "selector": "[data-testid='wide-tease-date']",
                "type": "text",
            },
            {
                "name": "image",
                "type": "nested",
                "selector": "picture.teasePicture img",
                "fields": [
                    {"name": "src", "type": "attribute", "attribute": "src"},
                    {"name": "alt", "type": "attribute", "attribute": "alt"},
                ],
            },
            {
                "name": "link",
                "selector": "a[href]",
                "type": "attribute",
                "attribute": "href",
            },
        ],
    }

    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)

    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
            extraction_strategy=extraction_strategy,
            bypass_cache=True,
        )

        assert result.success, "Failed to crawl the page"

        news_teasers = json.loads(result.extracted_content)
        print(f"Successfully extracted {len(news_teasers)} news teasers")
        print(json.dumps(news_teasers[0], indent=2))

await extract_news_teasers()

----------------------------------------

TITLE: Implementing Advanced Web Crawling with crawl4ai
DESCRIPTION: This snippet demonstrates a comprehensive implementation of crawl4ai's advanced features including proxy configuration, PDF and screenshot capture, SSL certificate verification, custom headers, and session reuse. It shows how to configure the browser, set up crawling parameters, execute the crawl, and handle the results including saving captured assets.

LANGUAGE: python
CODE:
import os, asyncio
from base64 import b64decode
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

async def main():
    # 1. Browser config with proxy + headless
    browser_cfg = BrowserConfig(
        proxy_config={
            "server": "http://proxy.example.com:8080",
            "username": "myuser",
            "password": "mypass",
        },
        headless=True,
    )

    # 2. Crawler config with PDF, screenshot, SSL, custom headers, and ignoring caches
    crawler_cfg = CrawlerRunConfig(
        pdf=True,
        screenshot=True,
        fetch_ssl_certificate=True,
        cache_mode=CacheMode.BYPASS,
        headers={"Accept-Language": "en-US,en;q=0.8"},
        storage_state="my_storage.json",  # Reuse session from a previous sign-in
        verbose=True,
    )

    # 3. Crawl
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(
            url = "https://secure.example.com/protected", 
            config=crawler_cfg
        )
        
        if result.success:
            print("[OK] Crawled the secure page. Links found:", len(result.links.get("internal", [])))
            
            # Save PDF & screenshot
            if result.pdf:
                with open("result.pdf", "wb") as f:
                    f.write(b64decode(result.pdf))
            if result.screenshot:
                with open("result.png", "wb") as f:
                    f.write(b64decode(result.screenshot))
            
            # Check SSL cert
            if result.ssl_certificate:
                print("SSL Issuer CN:", result.ssl_certificate.issuer.get("CN", ""))
        else:
            print("[ERROR]", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Using AsyncWebCrawler to Extract Web Content in Python
DESCRIPTION: This snippet demonstrates how to use the AsyncWebCrawler class from the crawl4ai package to asynchronously crawl a website and extract content in markdown format. It showcases the basic asynchronous workflow pattern with context manager and awaitable methods.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    # Create an instance of AsyncWebCrawler
    async with AsyncWebCrawler() as crawler:
        # Run the crawler on a URL
        result = await crawler.arun(url="https://crawl4ai.com")

        # Print the extracted content
        print(result.markdown)

# Run the async main function
asyncio.run(main())

----------------------------------------

TITLE: Comprehensive Crawl4AI Example with Multiple Parameters
DESCRIPTION: A complete example combining various configuration options including content selection, JavaScript execution, extraction strategy, screenshots, and anti-bot measures in a real-world crawling scenario.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def main():
    # Example schema
    schema = {
        "name": "Articles",
        "baseSelector": "article.post",
        "fields": [
            {"name": "title", "selector": "h2", "type": "text"},
            {"name": "link",  "selector": "a",  "type": "attribute", "attribute": "href"}
        ]
    }

    run_config = CrawlerRunConfig(
        # Core
        verbose=True,
        cache_mode=CacheMode.ENABLED,
        check_robots_txt=True,   # Respect robots.txt rules
        
        # Content
        word_count_threshold=10,
        css_selector="main.content",
        excluded_tags=["nav", "footer"],
        exclude_external_links=True,
        
        # Page & JS
        js_code="document.querySelector('.show-more')?.click();",
        wait_for="css:.loaded-block",
        page_timeout=30000,
        
        # Extraction
        extraction_strategy=JsonCssExtractionStrategy(schema),
        
        # Session
        session_id="persistent_session",
        
        # Media
        screenshot=True,
        pdf=True,
        
        # Anti-bot
        simulate_user=True,
        magic=True,
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://example.com/posts", config=run_config)
        if result.success:
            print("HTML length:", len(result.cleaned_html))
            print("Extraction JSON:", result.extracted_content)
            if result.screenshot:
                print("Screenshot length:", len(result.screenshot))
            if result.pdf:
                print("PDF bytes length:", len(result.pdf))
        else:
            print("Error:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Building a Complete Advanced Web Crawler with crawl4ai
DESCRIPTION: Demonstrates a comprehensive crawler implementation combining multiple filtering techniques, domain restrictions, and a keyword relevance scorer. This example uses BestFirstCrawlingStrategy to prioritize the most relevant pages and analyzes results with depth-based statistics.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy
from crawl4ai.deep_crawling import BestFirstCrawlingStrategy
from crawl4ai.deep_crawling.filters import (
    FilterChain,
    DomainFilter,
    URLPatternFilter,
    ContentTypeFilter
)
from crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer

async def run_advanced_crawler():
    # Create a sophisticated filter chain
    filter_chain = FilterChain([
        # Domain boundaries
        DomainFilter(
            allowed_domains=["docs.example.com"],
            blocked_domains=["old.docs.example.com"]
        ),
        
        # URL patterns to include
        URLPatternFilter(patterns=["*guide*", "*tutorial*", "*blog*"]),
        
        # Content type filtering
        ContentTypeFilter(allowed_types=["text/html"])
    ])

    # Create a relevance scorer
    keyword_scorer = KeywordRelevanceScorer(
        keywords=["crawl", "example", "async", "configuration"],
        weight=0.7
    )

    # Set up the configuration
    config = CrawlerRunConfig(
        deep_crawl_strategy=BestFirstCrawlingStrategy(
            max_depth=2,
            include_external=False,
            filter_chain=filter_chain,
            url_scorer=keyword_scorer
        ),
        scraping_strategy=LXMLWebScrapingStrategy(),
        stream=True,
        verbose=True
    )

    # Execute the crawl
    results = []
    async with AsyncWebCrawler() as crawler:
        async for result in await crawler.arun("https://docs.example.com", config=config):
            results.append(result)
            score = result.metadata.get("score", 0)
            depth = result.metadata.get("depth", 0)
            print(f"Depth: {depth} | Score: {score:.2f} | {result.url}")

    # Analyze the results
    print(f"Crawled {len(results)} high-value pages")
    print(f"Average score: {sum(r.metadata.get('score', 0) for r in results) / len(results):.2f}")

    # Group by depth
    depth_counts = {}
    for result in results:
        depth = result.metadata.get("depth", 0)
        depth_counts[depth] = depth_counts.get(depth, 0) + 1

    print("Pages crawled by depth:")
    for depth, count in sorted(depth_counts.items()):
        print(f"  Depth {depth}: {count} pages")

if __name__ == "__main__":
    asyncio.run(run_advanced_crawler())

----------------------------------------

TITLE: Definition of arun() Method in AsyncWebCrawler
DESCRIPTION: The signature of the primary arun() method used for performing single-page crawls. This method accepts a URL and an optional CrawlerRunConfig to control the crawling behavior, returning a CrawlResult object with the crawled data.

LANGUAGE: python
CODE:
async def arun(
    self,
    url: str,
    config: Optional[CrawlerRunConfig] = None,
    # Legacy parameters for backward compatibility...
) -> CrawlResult:
    ...

----------------------------------------

TITLE: Setting Up and Running a Basic Web Crawler with Crawl4AI
DESCRIPTION: Demonstrates the basic setup of AsyncWebCrawler with default configurations and how to execute a simple crawl request to a website. The example shows the initialization of browser and run configurations, executing the crawler, and retrieving markdown content.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig

async def main():
    browser_config = BrowserConfig()  # Default browser configuration
    run_config = CrawlerRunConfig()   # Default crawl run configuration

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://example.com",
            config=run_config
        )
        print(result.markdown)  # Print clean markdown content

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Implementing Multi-URL Concurrent Crawling with Crawl4AI
DESCRIPTION: This code demonstrates parallel crawling of multiple URLs using Crawl4AI's arun_many() method. It shows two approaches: streaming mode for processing results as they become available, and batch mode for handling all results at once. The crawler intelligently adapts concurrency based on system resources.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

async def quick_parallel_example():
    urls = [
        "https://example.com/page1",
        "https://example.com/page2",
        "https://example.com/page3"
    ]
    
    run_conf = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        stream=True  # Enable streaming mode
    )

    async with AsyncWebCrawler() as crawler:
        # Stream results as they complete
        async for result in await crawler.arun_many(urls, config=run_conf):
            if result.success:
                print(f"[OK] {result.url}, length: {len(result.markdown.raw_markdown)}")
            else:
                print(f"[ERROR] {result.url} => {result.error_message}")

        # Or get all results at once (default behavior)
        run_conf = run_conf.clone(stream=False)
        results = await crawler.arun_many(urls, config=run_conf)
        for res in results:
            if res.success:
                print(f"[OK] {res.url}, length: {len(res.markdown.raw_markdown)}")
            else:
                print(f"[ERROR] {res.url} => {res.error_message}")

if __name__ == "__main__":
    asyncio.run(quick_parallel_example())

----------------------------------------

TITLE: Session-Based Multi-Page Crawling
DESCRIPTION: Implements a session-based crawling approach for navigating through multiple pages while maintaining state between requests. This technique is useful for sites where content is loaded sequentially through pagination or 'Load More' buttons.

LANGUAGE: python
CODE:
async def multi_page_session_crawl():
    async with AsyncWebCrawler() as crawler:
        session_id = "page_navigation_session"
        url = "https://example.com/paged-content"

        for page_number in range(1, 4):
            result = await crawler.arun(
                url=url,
                session_id=session_id,
                js_code="document.querySelector('.next-page-button').click();" if page_number > 1 else None,
                css_selector=".content-section",
                bypass_cache=True
            )
            print(f"Page {page_number} Content:")
            print(result.markdown.raw_markdown[:500])  # Print first 500 characters

# asyncio.run(multi_page_session_crawl())

----------------------------------------

TITLE: Configuring BFSDeepCrawlStrategy with Parameters in Python
DESCRIPTION: This code shows how to create a BFSDeepCrawlStrategy instance with various configuration options including depth control, domain filtering, page limits, and scoring thresholds. BFS explores all links at one depth before moving deeper.

LANGUAGE: python
CODE:
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy

# Basic configuration
strategy = BFSDeepCrawlStrategy(
    max_depth=2,               # Crawl initial page + 2 levels deep
    include_external=False,    # Stay within the same domain
    max_pages=50,              # Maximum number of pages to crawl (optional)
    score_threshold=0.3,       # Minimum score for URLs to be crawled (optional)
)

----------------------------------------

TITLE: Core Usage of CrawlerRunConfig with AsyncWebCrawler
DESCRIPTION: Shows the basic implementation of CrawlerRunConfig with AsyncWebCrawler, including core parameters like verbose logging and cache mode settings, plus error handling for robots.txt blocking.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

async def main():
    run_config = CrawlerRunConfig(
        verbose=True,            # Detailed logging
        cache_mode=CacheMode.ENABLED,  # Use normal read/write cache
        check_robots_txt=True,   # Respect robots.txt rules
        # ... other parameters
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://example.com",
            config=run_config
        )
        
        # Check if blocked by robots.txt
        if not result.success and result.status_code == 403:
            print(f"Error: {result.error_message}")

----------------------------------------

TITLE: Executing JavaScript and Using CSS Selectors in Crawl4AI
DESCRIPTION: Shows how to execute custom JavaScript during crawling to interact with the page. In this example, it automatically clicks a 'Load More' button to retrieve additional content.

LANGUAGE: python
CODE:
async def js_and_css():
    async with AsyncWebCrawler(verbose=True) as crawler:
        js_code = ["const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More')); loadMoreButton && loadMoreButton.click();"]
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
            js_code=js_code,
            # css_selector="YOUR_CSS_SELECTOR_HERE",
            bypass_cache=True
        )
        print(len(result.markdown))

await js_and_css()

----------------------------------------

TITLE: Comprehensive Web Crawling Example with Crawl4AI
DESCRIPTION: A complete example demonstrating advanced usage of Crawl4AI with various configuration options. The code shows content filtering, processing, cache control, and handling of results including processing images and links from the crawled page.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig, CacheMode

async def main():
    browser_config = BrowserConfig(verbose=True)
    run_config = CrawlerRunConfig(
        # Content filtering
        word_count_threshold=10,
        excluded_tags=['form', 'header'],
        exclude_external_links=True,
        
        # Content processing
        process_iframes=True,
        remove_overlay_elements=True,
        
        # Cache control
        cache_mode=CacheMode.ENABLED  # Use cache if available
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://example.com",
            config=run_config
        )
        
        if result.success:
            # Print clean content
            print("Content:", result.markdown[:500])  # First 500 chars
            
            # Process images
            for image in result.media["images"]:
                print(f"Found image: {image['src']}")
            
            # Process links
            for link in result.links["internal"]:
                print(f"Internal link: {link['href']}")
                
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Building a Knowledge Graph using LLMExtractionStrategy with Pydantic Schema in Python
DESCRIPTION: This code demonstrates how to extract a knowledge graph from web content using crawl4ai's LLMExtractionStrategy. It defines Pydantic models for entities and relationships, configures the LLM extraction strategy with specific parameters, and processes a web page to extract structured data into a knowledge graph format.

LANGUAGE: python
CODE:
import os
import json
import asyncio
from typing import List
from pydantic import BaseModel, Field
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import LLMExtractionStrategy

class Entity(BaseModel):
    name: str
    description: str

class Relationship(BaseModel):
    entity1: Entity
    entity2: Entity
    description: str
    relation_type: str

class KnowledgeGraph(BaseModel):
    entities: List[Entity]
    relationships: List[Relationship]

async def main():
    # LLM extraction strategy
    llm_strat = LLMExtractionStrategy(
        provider="openai/gpt-4",
        api_token=os.getenv('OPENAI_API_KEY'),
        schema=KnowledgeGraph.schema_json(),
        extraction_type="schema",
        instruction="Extract entities and relationships from the content. Return valid JSON.",
        chunk_token_threshold=1400,
        apply_chunking=True,
        input_format="html",
        extra_args={"temperature": 0.1, "max_tokens": 1500}
    )

    crawl_config = CrawlerRunConfig(
        extraction_strategy=llm_strat,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:
        # Example page
        url = "https://www.nbcnews.com/business"
        result = await crawler.arun(url=url, config=crawl_config)

        if result.success:
            with open("kb_result.json", "w", encoding="utf-8") as f:
                f.write(result.extracted_content)
            llm_strat.show_usage()
        else:
            print("Crawl failed:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Basic Web Crawling with Crawl4AI
DESCRIPTION: Demonstrates a simple web crawl using Crawl4AI. It creates an AsyncWebCrawler instance and runs it against a news website, printing the length of the extracted markdown content.

LANGUAGE: python
CODE:
async def simple_crawl():
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(url="https://www.nbcnews.com/business")
        print(len(result.markdown))
await simple_crawl()

----------------------------------------

TITLE: Extracting Structured Data Using LLM in Python with Crawl4AI
DESCRIPTION: This snippet demonstrates how to use LLM-based extraction to parse web content into structured data following a Pydantic schema. It supports both open-source models (like Ollama) and commercial providers like OpenAI, with configurable parameters for response quality.

LANGUAGE: python
CODE:
import os
import json
import asyncio
from pydantic import BaseModel, Field
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy

class OpenAIModelFee(BaseModel):
    model_name: str = Field(..., description="Name of the OpenAI model.")
    input_fee: str = Field(..., description="Fee for input token for the OpenAI model.")
    output_fee: str = Field(
        ..., description="Fee for output token for the OpenAI model."
    )

async def extract_structured_data_using_llm(
    provider: str, api_token: str = None, extra_headers: Dict[str, str] = None
):
    print(f"\n--- Extracting Structured Data with {provider} ---")

    if api_token is None and provider != "ollama":
        print(f"API token is required for {provider}. Skipping this example.")
        return

    browser_config = BrowserConfig(headless=True)

    extra_args = {"temperature": 0, "top_p": 0.9, "max_tokens": 2000}
    if extra_headers:
        extra_args["extra_headers"] = extra_headers

    crawler_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        word_count_threshold=1,
        page_timeout=80000,
        extraction_strategy=LLMExtractionStrategy(
            llm_config = LLMConfig(provider=provider,api_token=api_token),
            schema=OpenAIModelFee.model_json_schema(),
            extraction_type="schema",
            instruction="""From the crawled content, extract all mentioned model names along with their fees for input and output tokens. 
            Do not miss any models in the entire content.""",
            extra_args=extra_args,
        ),
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://openai.com/api/pricing/", config=crawler_config
        )
        print(result.extracted_content)

if __name__ == "__main__":

    asyncio.run(
        extract_structured_data_using_llm(
            provider="openai/gpt-4o", api_token=os.getenv("OPENAI_API_KEY")
        )
    )

----------------------------------------

TITLE: Implementing Batch Processing with MemoryAdaptiveDispatcher in Crawl4AI
DESCRIPTION: Demonstrates a complete async function for batch processing URLs with MemoryAdaptiveDispatcher. Configures the browser, crawler, and dispatcher with memory management and detailed monitoring.

LANGUAGE: python
CODE:
async def crawl_batch():
    browser_config = BrowserConfig(headless=True, verbose=False)
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        stream=False  # Default: get all results at once
    )
    
    dispatcher = MemoryAdaptiveDispatcher(
        memory_threshold_percent=70.0,
        check_interval=1.0,
        max_session_permit=10,
        monitor=CrawlerMonitor(
            display_mode=DisplayMode.DETAILED
        )
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        # Get all results at once
        results = await crawler.arun_many(
            urls=urls,
            config=run_config,
            dispatcher=dispatcher
        )
        
        # Process all results after completion
        for result in results:
            if result.success:
                await process_result(result)
            else:
                print(f"Failed to crawl {result.url}: {result.error_message}")

----------------------------------------

TITLE: Extraction Strategy Configuration
DESCRIPTION: Demonstrates how to configure advanced data extraction using the extraction_strategy parameter, which enables CSS or LLM-based extraction strategies.

LANGUAGE: python
CODE:
run_config = CrawlerRunConfig(
    extraction_strategy=my_css_or_llm_strategy
)

----------------------------------------

TITLE: Complete Crawl4AI Example with LLM Extraction
DESCRIPTION: This comprehensive example demonstrates the full workflow of using LLM-based extraction in Crawl4AI. It includes defining a Pydantic model schema, configuring the LLM extraction strategy, setting up the crawler, running it against a URL, and processing the extracted JSON results.

LANGUAGE: python
CODE:
import os
import asyncio
import json
from pydantic import BaseModel, Field
from typing import List
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy

class Product(BaseModel):
    name: str
    price: str

async def main():
    # 1. Define the LLM extraction strategy
    llm_strategy = LLMExtractionStrategy(
        llm_config = LLMConfig(provider="openai/gpt-4o-mini", api_token=os.getenv('OPENAI_API_KEY')),
        schema=Product.schema_json(), # Or use model_json_schema()
        extraction_type="schema",
        instruction="Extract all product objects with 'name' and 'price' from the content.",
        chunk_token_threshold=1000,
        overlap_rate=0.0,
        apply_chunking=True,
        input_format="markdown",   # or "html", "fit_markdown"
        extra_args={"temperature": 0.0, "max_tokens": 800}
    )

    # 2. Build the crawler config
    crawl_config = CrawlerRunConfig(
        extraction_strategy=llm_strategy,
        cache_mode=CacheMode.BYPASS
    )

    # 3. Create a browser config if needed
    browser_cfg = BrowserConfig(headless=True)

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        # 4. Let's say we want to crawl a single page
        result = await crawler.arun(
            url="https://example.com/products",
            config=crawl_config
        )

        if result.success:
            # 5. The extracted content is presumably JSON
            data = json.loads(result.extracted_content)
            print("Extracted items:", data)
            
            # 6. Show usage stats
            llm_strategy.show_usage()  # prints token usage
        else:
            print("Error:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Running a web crawl with Python using AsyncWebCrawler
DESCRIPTION: Example Python code that demonstrates how to use the AsyncWebCrawler class to asynchronously crawl a website and print the extracted markdown content. The code shows the basic setup with error handling and async context management.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import *

async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
        )
        print(result.markdown)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Implementing Deep Crawling with Best-First Crawling Strategy in Python
DESCRIPTION: This code demonstrates how to set up and use Crawl4AI's deep crawling feature with a BestFirstCrawlingStrategy. It configures URL filtering based on domains, patterns, and content types, along with a keyword scorer to prioritize pages containing specific terms.

LANGUAGE: python
CODE:
import time
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeepCrawlStrategy
from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy
from crawl4ai.deep_crawling import DomainFilter, ContentTypeFilter, FilterChain, URLPatternFilter, KeywordRelevanceScorer, BestFirstCrawlingStrategy
import asyncio

# Create a filter chain to filter urls based on patterns, domains and content type
filter_chain = FilterChain(
    [
        DomainFilter(
            allowed_domains=["docs.crawl4ai.com"],
            blocked_domains=["old.docs.crawl4ai.com"],
        ),
        URLPatternFilter(patterns=["*core*", "*advanced*"],),
        ContentTypeFilter(allowed_types=["text/html"]),
    ]
)

# Create a keyword scorer that prioritises the pages with certain keywords first
keyword_scorer = KeywordRelevanceScorer(
    keywords=["crawl", "example", "async", "configuration"], weight=0.7
)

# Set up the configuration
deep_crawl_config = CrawlerRunConfig(
    deep_crawl_strategy=BestFirstCrawlingStrategy(
        max_depth=2,
        include_external=False,
        filter_chain=filter_chain,
        url_scorer=keyword_scorer,
    ),
    scraping_strategy=LXMLWebScrapingStrategy(),
    stream=True,
    verbose=True,
)

async def main():
    async with AsyncWebCrawler() as crawler:
        start_time = time.perf_counter()
        results = []
        async for result in await crawler.arun(url="https://docs.crawl4ai.com", config=deep_crawl_config):
            print(f"Crawled: {result.url} (Depth: {result.metadata['depth']}), score: {result.metadata['score']:.2f}")
            results.append(result)
        duration = time.perf_counter() - start_time
        print(f"\nâœ… Crawled {len(results)} high-value pages in {duration:.2f} seconds")

asyncio.run(main())

----------------------------------------

TITLE: Complete AsyncWebCrawler Implementation Example in crawl4ai
DESCRIPTION: Comprehensive example demonstrating how to configure and use the AsyncWebCrawler with both BrowserConfig and CrawlerRunConfig. Shows initialization, configuration setup, and execution of a web crawl with error handling and result processing.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

async def main():
    # Configure the browser
    browser_cfg = BrowserConfig(
        headless=False,
        viewport_width=1280,
        viewport_height=720,
        proxy="http://user:pass@myproxy:8080",
        text_mode=True
    )

    # Configure the run
    run_cfg = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        session_id="my_session",
        css_selector="main.article",
        excluded_tags=["script", "style"],
        exclude_external_links=True,
        wait_for="css:.article-loaded",
        screenshot=True,
        stream=True
    )

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(
            url="https://example.com/news",
            config=run_cfg
        )
        if result.success:
            print("Final cleaned_html length:", len(result.cleaned_html))
            if result.screenshot:
                print("Screenshot captured (base64, length):", len(result.screenshot))
        else:
            print("Crawl failed:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())


----------------------------------------

TITLE: Extracting Cryptocurrency Prices with CSS Selectors in Python
DESCRIPTION: This code demonstrates how to extract cryptocurrency prices from a website using Crawl4AI's JsonCssExtractionStrategy. It defines a schema with a base selector and fields, sets up a crawler configuration, and processes the extracted JSON data.

LANGUAGE: python
CODE:
import json
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def extract_crypto_prices():
    # 1. Define a simple extraction schema
    schema = {
        "name": "Crypto Prices",
        "baseSelector": "div.crypto-row",    # Repeated elements
        "fields": [
            {
                "name": "coin_name",
                "selector": "h2.coin-name",
                "type": "text"
            },
            {
                "name": "price",
                "selector": "span.coin-price",
                "type": "text"
            }
        ]
    }

    # 2. Create the extraction strategy
    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)

    # 3. Set up your crawler config (if needed)
    config = CrawlerRunConfig(
        # e.g., pass js_code or wait_for if the page is dynamic
        # wait_for="css:.crypto-row:nth-child(20)"
        cache_mode = CacheMode.BYPASS,
        extraction_strategy=extraction_strategy,
    )

    async with AsyncWebCrawler(verbose=True) as crawler:
        # 4. Run the crawl and extraction
        result = await crawler.arun(
            url="https://example.com/crypto-prices",
            
            config=config
        )

        if not result.success:
            print("Crawl failed:", result.error_message)
            return

        # 5. Parse the extracted JSON
        data = json.loads(result.extracted_content)
        print(f"Extracted {len(data)} coin entries")
        print(json.dumps(data[0], indent=2) if data else "No data found")

asyncio.run(extract_crypto_prices())

----------------------------------------

TITLE: Accessing CrawlResult Properties in Python
DESCRIPTION: This function demonstrates how to access all available properties from a CrawlResult object, including basic info, HTML content, Markdown output, media and links, extracted content, and binary data like screenshots and PDFs. It handles both successful and failed crawl results.

LANGUAGE: python
CODE:
async def handle_result(result: CrawlResult):
    if not result.success:
        print("Crawl error:", result.error_message)
        return
    
    # Basic info
    print("Crawled URL:", result.url)
    print("Status code:", result.status_code)
    
    # HTML
    print("Original HTML size:", len(result.html))
    print("Cleaned HTML size:", len(result.cleaned_html or ""))

    # Markdown output
    if result.markdown:
        print("Raw Markdown:", result.markdown.raw_markdown[:300])
        print("Citations Markdown:", result.markdown.markdown_with_citations[:300])
        if result.markdown.fit_markdown:
            print("Fit Markdown:", result.markdown.fit_markdown[:200])

    # Media & Links
    if "images" in result.media:
        print("Image count:", len(result.media["images"]))
    if "internal" in result.links:
        print("Internal link count:", len(result.links["internal"]))

    # Extraction strategy result
    if result.extracted_content:
        print("Structured data:", result.extracted_content)
    
    # Screenshot/PDF
    if result.screenshot:
        print("Screenshot length:", len(result.screenshot))
    if result.pdf:
        print("PDF bytes length:", len(result.pdf))

----------------------------------------

TITLE: Implementing Streaming Mode with AsyncWebCrawler in Python
DESCRIPTION: Demonstrates how to enable streaming mode with the AsyncWebCrawler using MemoryAdaptiveDispatcher to process results in real-time as they become available. It configures memory thresholds and monitoring options while processing results immediately through an async iterator.

LANGUAGE: python
CODE:
async def crawl_streaming():
    browser_config = BrowserConfig(headless=True, verbose=False)
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        stream=True  # Enable streaming mode
    )
    
    dispatcher = MemoryAdaptiveDispatcher(
        memory_threshold_percent=70.0,
        check_interval=1.0,
        max_session_permit=10,
        monitor=CrawlerMonitor(
            display_mode=DisplayMode.DETAILED
        )
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        # Process results as they become available
        async for result in await crawler.arun_many(
            urls=urls,
            config=run_config,
            dispatcher=dispatcher
        ):
            if result.success:
                # Process each result immediately
                await process_result(result)
            else:
                print(f"Failed to crawl {result.url}: {result.error_message}")

----------------------------------------

TITLE: Defining E-commerce Schema Structure in Python
DESCRIPTION: A comprehensive schema for extracting e-commerce data from HTML pages, including categories, products, features, reviews, and related items. The schema uses CSS selectors to navigate the HTML structure and extract different data types.

LANGUAGE: python
CODE:
schema = {
    "name": "E-commerce Product Catalog",
    "baseSelector": "div.category",
    # (1) We can define optional baseFields if we want to extract attributes 
    # from the category container
    "baseFields": [
        {"name": "data_cat_id", "type": "attribute", "attribute": "data-cat-id"}, 
    ],
    "fields": [
        {
            "name": "category_name",
            "selector": "h2.category-name",
            "type": "text"
        },
        {
            "name": "products",
            "selector": "div.product",
            "type": "nested_list",    # repeated sub-objects
            "fields": [
                {
                    "name": "name",
                    "selector": "h3.product-name",
                    "type": "text"
                },
                {
                    "name": "price",
                    "selector": "p.product-price",
                    "type": "text"
                },
                {
                    "name": "details",
                    "selector": "div.product-details",
                    "type": "nested",  # single sub-object
                    "fields": [
                        {
                            "name": "brand",
                            "selector": "span.brand",
                            "type": "text"
                        },
                        {
                            "name": "model",
                            "selector": "span.model",
                            "type": "text"
                        }
                    ]
                },
                {
                    "name": "features",
                    "selector": "ul.product-features li",
                    "type": "list",
                    "fields": [
                        {"name": "feature", "type": "text"} 
                    ]
                },
                {
                    "name": "reviews",
                    "selector": "div.review",
                    "type": "nested_list",
                    "fields": [
                        {
                            "name": "reviewer", 
                            "selector": "span.reviewer", 
                            "type": "text"
                        },
                        {
                            "name": "rating", 
                            "selector": "span.rating", 
                            "type": "text"
                        },
                        {
                            "name": "comment", 
                            "selector": "p.review-text", 
                            "type": "text"
                        }
                    ]
                },
                {
                    "name": "related_products",
                    "selector": "ul.related-products li",
                    "type": "list",
                    "fields": [
                        {
                            "name": "name", 
                            "selector": "span.related-name", 
                            "type": "text"
                        },
                        {
                            "name": "price", 
                            "selector": "span.related-price", 
                            "type": "text"
                        }
                    ]
                }
            ]
        }
    ]
}

----------------------------------------

TITLE: Defining BrowserConfig Class in Python
DESCRIPTION: Shows the structure of the BrowserConfig class which controls how the browser is launched and behaves. Parameters include browser type, headless mode, proxy configuration, viewport settings, and various other browser behavior options.

LANGUAGE: python
CODE:
class BrowserConfig:
    def __init__(
        browser_type="chromium",
        headless=True,
        proxy_config=None,
        viewport_width=1080,
        viewport_height=600,
        verbose=True,
        use_persistent_context=False,
        user_data_dir=None,
        cookies=None,
        headers=None,
        user_agent=None,
        text_mode=False,
        light_mode=False,
        extra_args=None,
        # ... other advanced parameters omitted here
    ):
        ...

----------------------------------------

TITLE: Initializing LLMExtractionStrategy in Python
DESCRIPTION: Configuration for the LLMExtractionStrategy class used for extracting structured data using Language Models. This strategy supports custom instructions, schema-based extraction, and configurable chunking parameters.

LANGUAGE: python
CODE:
LLMExtractionStrategy(
    # Required Parameters
    provider: str = DEFAULT_PROVIDER,     # LLM provider (e.g., "ollama/llama2")
    api_token: Optional[str] = None,      # API token
    
    # Extraction Configuration
    instruction: str = None,              # Custom extraction instruction
    schema: Dict = None,                  # Pydantic model schema for structured data
    extraction_type: str = "block",       # "block" or "schema"
    
    # Chunking Parameters
    chunk_token_threshold: int = 4000,    # Maximum tokens per chunk
    overlap_rate: float = 0.1,           # Overlap between chunks
    word_token_rate: float = 0.75,       # Word to token conversion rate
    apply_chunking: bool = True,         # Enable/disable chunking
    
    # API Configuration
    base_url: str = None,                # Base URL for API
    extra_args: Dict = {},               # Additional provider arguments
    verbose: bool = False                # Enable verbose logging
)

----------------------------------------

TITLE: Implementing Basic Deep Crawl with BFSDeepCrawlStrategy in Python
DESCRIPTION: This code demonstrates a minimal implementation of a 2-level deep web crawl using BFSDeepCrawlStrategy. It configures the crawler to stay within the same domain and prints basic information about the crawled pages.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy
from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy

async def main():
    # Configure a 2-level deep crawl
    config = CrawlerRunConfig(
        deep_crawl_strategy=BFSDeepCrawlStrategy(
            max_depth=2, 
            include_external=False
        ),
        scraping_strategy=LXMLWebScrapingStrategy(),
        verbose=True
    )
    
    async with AsyncWebCrawler() as crawler:
        results = await crawler.arun("https://example.com", config=config)
        
        print(f"Crawled {len(results)} pages in total")
        
        # Access individual results
        for result in results[:3]:  # Show first 3 results
            print(f"URL: {result.url}")
            print(f"Depth: {result.metadata.get('depth', 0)}")

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: LLM-Based Structured Data Extraction
DESCRIPTION: Extracts structured data from a webpage using language model-based extraction. This example defines a Pydantic model for OpenAI's pricing information and uses a language model to extract the relevant data according to this schema.

LANGUAGE: python
CODE:
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field
import os, json

class OpenAIModelFee(BaseModel):
    model_name: str = Field(..., description="Name of the OpenAI model.")
    input_fee: str = Field(..., description="Fee for input token for the OpenAI model.")
    output_fee: str = Field(
        ..., description="Fee for output token for the OpenAI model."
    )

async def extract_structured_data_using_llm(provider: str, api_token: str = None, extra_headers: dict = None):
    print(f"\n--- Extracting Structured Data with {provider} ---")
    
    # Skip if API token is missing (for providers that require it)
    if api_token is None and provider != "ollama":
        print(f"API token is required for {provider}. Skipping this example.")
        return

    extra_args = {"extra_headers": extra_headers} if extra_headers else {}

    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url="https://openai.com/api/pricing/",
            word_count_threshold=1,
            extraction_strategy=LLMExtractionStrategy(
                provider=provider,
                api_token=api_token,
                schema=OpenAIModelFee.schema(),
                extraction_type="schema",
                instruction="""Extract all model names along with fees for input and output tokens."
                "{model_name: 'GPT-4', input_fee: 'US$10.00 / 1M tokens', output_fee: 'US$30.00 / 1M tokens'}""",
                **extra_args
            ),
            bypass_cache=True,
        )
        print(json.loads(result.extracted_content)[:5])

# Usage:
await extract_structured_data_using_llm("openai/gpt-4o-mini", os.getenv("OPENAI_API_KEY"))

----------------------------------------

TITLE: LLM Extraction Example in Python
DESCRIPTION: Example of using LLMExtractionStrategy with a Pydantic model schema to extract structured article data from a website. The example demonstrates schema definition, strategy creation, and accessing the extracted data.

LANGUAGE: python
CODE:
from pydantic import BaseModel
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.types import LLMConfig

# Define schema
class Article(BaseModel):
    title: str
    content: str
    author: str

# Create strategy
strategy = LLMExtractionStrategy(
    llm_config = LLMConfig(provider="ollama/llama2"),
    schema=Article.schema(),
    instruction="Extract article details"
)

# Use with crawler
result = await crawler.arun(
    url="https://example.com/article",
    extraction_strategy=strategy
)

# Access extracted data
data = json.loads(result.extracted_content)

----------------------------------------

TITLE: Running E-commerce Data Extraction with Crawl4AI
DESCRIPTION: This code demonstrates how to use AsyncWebCrawler with JsonCssExtractionStrategy to extract data from an e-commerce HTML page. It configures the crawler, applies the extraction schema, and processes the results into structured JSON.

LANGUAGE: python
CODE:
import json
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

ecommerce_schema = {
    # ... the advanced schema from above ...
}

async def extract_ecommerce_data():
    strategy = JsonCssExtractionStrategy(ecommerce_schema, verbose=True)
    
    config = CrawlerRunConfig()
    
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url="https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html",
            extraction_strategy=strategy,
            config=config
        )

        if not result.success:
            print("Crawl failed:", result.error_message)
            return
        
        # Parse the JSON output
        data = json.loads(result.extracted_content)
        print(json.dumps(data, indent=2) if data else "No data found.")

asyncio.run(extract_ecommerce_data())

----------------------------------------

TITLE: Advanced Multi-Page Crawling with JavaScript Execution in Crawl4AI
DESCRIPTION: Implements a complex crawling scenario that handles paginated content using custom JavaScript execution. This example crawls GitHub commit history across multiple pages by simulating clicks on the pagination buttons.

LANGUAGE: python
CODE:
import re
from bs4 import BeautifulSoup

async def crawl_typescript_commits():
    first_commit = ""
    async def on_execution_started(page):
        nonlocal first_commit
        try:
            while True:
                await page.wait_for_selector('li.Box-sc-g0xbh4-0 h4')
                commit = await page.query_selector('li.Box-sc-g0xbh4-0 h4')
                commit = await commit.evaluate('(element) => element.textContent')
                commit = re.sub(r'\s+', '', commit)
                if commit and commit != first_commit:
                    first_commit = commit
                    break
                await asyncio.sleep(0.5)
        except Exception as e:
            print(f"Warning: New content didn't appear after JavaScript execution: {e}")

    async with AsyncWebCrawler(verbose=True) as crawler:
        crawler.crawler_strategy.set_hook('on_execution_started', on_execution_started)

        url = "https://github.com/microsoft/TypeScript/commits/main"
        session_id = "typescript_commits_session"
        all_commits = []

        js_next_page = """
        const button = document.querySelector('a[data-testid="pagination-next-button"]');
        if (button) button.click();
        """

        for page in range(3):  # Crawl 3 pages
            result = await crawler.arun(
                url=url,
                session_id=session_id,
                css_selector="li.Box-sc-g0xbh4-0",
                js=js_next_page if page > 0 else None,
                bypass_cache=True,
                js_only=page > 0
            )

            assert result.success, f"Failed to crawl page {page + 1}"

            soup = BeautifulSoup(result.cleaned_html, 'html.parser')
            commits = soup.select("li")
            all_commits.extend(commits)

            print(f"Page {page + 1}: Found {len(commits)} commits")

        await crawler.crawler_strategy.kill_session(session_id)
        print(f"Successfully crawled {len(all_commits)} commits across 3 pages")

await crawl_typescript_commits()

----------------------------------------

TITLE: Customizing Crawler Options with CrawlerRunConfig in Crawl4AI
DESCRIPTION: Demonstrates how to customize the behavior of the crawler using CrawlerRunConfig options. The example shows setting minimum word count thresholds, excluding external links, removing overlay elements, and processing iframe content.

LANGUAGE: python
CODE:
run_config = CrawlerRunConfig(
    word_count_threshold=10,        # Minimum words per content block
    exclude_external_links=True,    # Remove external links
    remove_overlay_elements=True,   # Remove popups/modals
    process_iframes=True           # Process iframe content
)

result = await crawler.arun(
    url="https://example.com",
    config=run_config
)

----------------------------------------

TITLE: Implementing LXML Web Scraping Strategy in Crawl4AI
DESCRIPTION: This snippet demonstrates how to use the LXMLWebScrapingStrategy for faster HTML document processing compared to the default BeautifulSoup-based strategy. It shows the configuration and usage of AsyncWebCrawler with the LXML strategy.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LXMLWebScrapingStrategy

async def main():
    config = CrawlerRunConfig(
        scraping_strategy=LXMLWebScrapingStrategy()  # Faster alternative to default BeautifulSoup
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://example.com", 
            config=config
        )

----------------------------------------

TITLE: Installing Crawl4AI with All Optional Features
DESCRIPTION: Commands to install Crawl4AI with all optional features including PyTorch and transformers, followed by setup.

LANGUAGE: bash
CODE:
pip install crawl4ai[all]
crawl4ai-setup

----------------------------------------

TITLE: Combining PruningContentFilter and BM25ContentFilter in Two Passes
DESCRIPTION: This example demonstrates a two-pass filtering approach, first removing noisy content with PruningContentFilter, then ranking the remaining content against a user query using BM25ContentFilter. This approach avoids crawling the page twice by reusing the raw HTML from the first pass.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter
from bs4 import BeautifulSoup

async def main():
    # 1. Crawl with minimal or no markdown generator, just get raw HTML
    config = CrawlerRunConfig(
        # If you only want raw HTML, you can skip passing a markdown_generator
        # or provide one but focus on .html in this example
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://example.com/tech-article", config=config)

        if not result.success or not result.html:
            print("Crawl failed or no HTML content.")
            return
        
        raw_html = result.html
        
        # 2. First pass: PruningContentFilter on raw HTML
        pruning_filter = PruningContentFilter(threshold=0.5, min_word_threshold=50)
        
        # filter_content returns a list of "text chunks" or cleaned HTML sections
        pruned_chunks = pruning_filter.filter_content(raw_html)
        # This list is basically pruned content blocks, presumably in HTML or text form
        
        # For demonstration, let's combine these chunks back into a single HTML-like string
        # or you could do further processing. It's up to your pipeline design.
        pruned_html = "\n".join(pruned_chunks)
        
        # 3. Second pass: BM25ContentFilter with a user query
        bm25_filter = BM25ContentFilter(
            user_query="machine learning",
            bm25_threshold=1.2,
            language="english"
        )
        
        # returns a list of text chunks
        bm25_chunks = bm25_filter.filter_content(pruned_html)  
        
        if not bm25_chunks:
            print("Nothing matched the BM25 query after pruning.")
            return
        
        # 4. Combine or display final results
        final_text = "\n---\n".join(bm25_chunks)
        
        print("==== PRUNED OUTPUT (first pass) ====")
        print(pruned_html[:500], "... (truncated)")  # preview

        print("\n==== BM25 OUTPUT (second pass) ====")
        print(final_text[:500], "... (truncated)")

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Implementing BestFirstCrawlingStrategy with KeywordRelevanceScorer in Python
DESCRIPTION: This code shows how to set up intelligent crawling using BestFirstCrawlingStrategy with a KeywordRelevanceScorer. It prioritizes crawling pages based on keyword relevance, allowing for more targeted exploration of web content.

LANGUAGE: python
CODE:
from crawl4ai.deep_crawling import BestFirstCrawlingStrategy
from crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer

# Create a scorer
scorer = KeywordRelevanceScorer(
    keywords=["crawl", "example", "async", "configuration"],
    weight=0.7
)

# Configure the strategy
strategy = BestFirstCrawlingStrategy(
    max_depth=2,
    include_external=False,
    url_scorer=scorer,
    max_pages=25,              # Maximum number of pages to crawl (optional)
)

----------------------------------------

TITLE: Basic PruningContentFilter Implementation Pattern
DESCRIPTION: Simplified code pattern demonstrating the essential setup for the PruningContentFilter with fixed threshold parameters. Shows the three-step process of creating a filter, a markdown generator, and a crawler configuration.

LANGUAGE: python
CODE:
prune_filter = PruningContentFilter(
    threshold=0.5,
    threshold_type="fixed",
    min_word_threshold=10
)
md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)
config = CrawlerRunConfig(markdown_generator=md_generator)

----------------------------------------

TITLE: Implementing Hooks in AsyncWebCrawler with Python
DESCRIPTION: A complete example demonstrating the proper implementation of all available hooks in AsyncWebCrawler. The code shows how to configure the browser and crawler, define hook functions for different stages of the crawling process, attach them to the crawler strategy, and execute a crawl with the hooks in place. It includes examples of route filtering, viewport adjustment, and custom header injection.

LANGUAGE: python
CODE:
import asyncio
import json
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from playwright.async_api import Page, BrowserContext

async def main():
    print("ðŸ”— Hooks Example: Demonstrating recommended usage")

    # 1) Configure the browser
    browser_config = BrowserConfig(
        headless=True,
        verbose=True
    )

    # 2) Configure the crawler run
    crawler_run_config = CrawlerRunConfig(
        js_code="window.scrollTo(0, document.body.scrollHeight);",
        wait_for="body",
        cache_mode=CacheMode.BYPASS
    )

    # 3) Create the crawler instance
    crawler = AsyncWebCrawler(config=browser_config)

    #
    # Define Hook Functions
    #

    async def on_browser_created(browser, **kwargs):
        # Called once the browser instance is created (but no pages or contexts yet)
        print("[HOOK] on_browser_created - Browser created successfully!")
        # Typically, do minimal setup here if needed
        return browser

    async def on_page_context_created(page: Page, context: BrowserContext, **kwargs):
        # Called right after a new page + context are created (ideal for auth or route config).
        print("[HOOK] on_page_context_created - Setting up page & context.")
        
        # Example 1: Route filtering (e.g., block images)
        async def route_filter(route):
            if route.request.resource_type == "image":
                print(f"[HOOK] Blocking image request: {route.request.url}")
                await route.abort()
            else:
                await route.continue_()

        await context.route("**", route_filter)

        # Example 2: (Optional) Simulate a login scenario
        # (We do NOT create or close pages here, just do quick steps if needed)
        # e.g., await page.goto("https://example.com/login")
        # e.g., await page.fill("input[name='username']", "testuser")
        # e.g., await page.fill("input[name='password']", "password123")
        # e.g., await page.click("button[type='submit']")
        # e.g., await page.wait_for_selector("#welcome")
        # e.g., await context.add_cookies([...])
        # Then continue

        # Example 3: Adjust the viewport
        await page.set_viewport_size({"width": 1080, "height": 600})
        return page

    async def before_goto(
        page: Page, context: BrowserContext, url: str, **kwargs
    ):
        # Called before navigating to each URL.
        print(f"[HOOK] before_goto - About to navigate: {url}")
        # e.g., inject custom headers
        await page.set_extra_http_headers({
            "Custom-Header": "my-value"
        })
        return page

    async def after_goto(
        page: Page, context: BrowserContext, 
        url: str, response, **kwargs
    ):
        # Called after navigation completes.
        print(f"[HOOK] after_goto - Successfully loaded: {url}")
        # e.g., wait for a certain element if we want to verify
        try:
            await page.wait_for_selector('.content', timeout=1000)
            print("[HOOK] Found .content element!")
        except:
            print("[HOOK] .content not found, continuing anyway.")
        return page

    async def on_user_agent_updated(
        page: Page, context: BrowserContext, 
        user_agent: str, **kwargs
    ):
        # Called whenever the user agent updates.
        print(f"[HOOK] on_user_agent_updated - New user agent: {user_agent}")
        return page

    async def on_execution_started(page: Page, context: BrowserContext, **kwargs):
        # Called after custom JavaScript execution begins.
        print("[HOOK] on_execution_started - JS code is running!")
        return page

    async def before_retrieve_html(page: Page, context: BrowserContext, **kwargs):
        # Called before final HTML retrieval.
        print("[HOOK] before_retrieve_html - We can do final actions")
        # Example: Scroll again
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
        return page

    async def before_return_html(
        page: Page, context: BrowserContext, html: str, **kwargs
    ):
        # Called just before returning the HTML in the result.
        print(f"[HOOK] before_return_html - HTML length: {len(html)}")
        return page

    #
    # Attach Hooks
    #

    crawler.crawler_strategy.set_hook("on_browser_created", on_browser_created)
    crawler.crawler_strategy.set_hook(
        "on_page_context_created", on_page_context_created
    )
    crawler.crawler_strategy.set_hook("before_goto", before_goto)
    crawler.crawler_strategy.set_hook("after_goto", after_goto)
    crawler.crawler_strategy.set_hook(
        "on_user_agent_updated", on_user_agent_updated
    )
    crawler.crawler_strategy.set_hook(
        "on_execution_started", on_execution_started
    )
    crawler.crawler_strategy.set_hook(
        "before_retrieve_html", before_retrieve_html
    )
    crawler.crawler_strategy.set_hook(
        "before_return_html", before_return_html
    )

    await crawler.start()

    # 4) Run the crawler on an example page
    url = "https://example.com"
    result = await crawler.arun(url, config=crawler_run_config)
    
    if result.success:
        print("\nCrawled URL:", result.url)
        print("HTML length:", len(result.html))
    else:
        print("Error:", result.error_message)

    await crawler.close()

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Initializing RegexChunking in Python
DESCRIPTION: Configuration for the RegexChunking class that splits text based on regex patterns. By default, it splits text on double newlines.

LANGUAGE: python
CODE:
RegexChunking(
    patterns: List[str] = None  # Regex patterns for splitting
                               # Default: [r'\n\n']
)

----------------------------------------

TITLE: Crawling Dynamic Content with Sessions in Python
DESCRIPTION: Shows how to crawl GitHub commits across multiple pages while preserving session state. This example uses JsonCssExtractionStrategy to extract structured data and JavaScript execution to navigate pagination.

LANGUAGE: python
CODE:
from crawl4ai.async_configs import CrawlerRunConfig
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
from crawl4ai.cache_context import CacheMode

async def crawl_dynamic_content():
    async with AsyncWebCrawler() as crawler:
        session_id = "github_commits_session"
        url = "https://github.com/microsoft/TypeScript/commits/main"
        all_commits = []

        # Define extraction schema
        schema = {
            "name": "Commit Extractor",
            "baseSelector": "li.Box-sc-g0xbh4-0",
            "fields": [{
                "name": "title", "selector": "h4.markdown-title", "type": "text"
            }],
        }
        extraction_strategy = JsonCssExtractionStrategy(schema)

        # JavaScript and wait configurations
        js_next_page = """document.querySelector('a[data-testid="pagination-next-button"]').click();"""
        wait_for = """() => document.querySelectorAll('li.Box-sc-g0xbh4-0').length > 0"""

        # Crawl multiple pages
        for page in range(3):
            config = CrawlerRunConfig(
                url=url,
                session_id=session_id,
                extraction_strategy=extraction_strategy,
                js_code=js_next_page if page > 0 else None,
                wait_for=wait_for if page > 0 else None,
                js_only=page > 0,
                cache_mode=CacheMode.BYPASS
            )

            result = await crawler.arun(config=config)
            if result.success:
                commits = json.loads(result.extracted_content)
                all_commits.extend(commits)
                print(f"Page {page + 1}: Found {len(commits)} commits")

        # Clean up session
        await crawler.crawler_strategy.kill_session(session_id)
        return all_commits

----------------------------------------

TITLE: Implementing PruningContentFilter in Crawl4AI
DESCRIPTION: This snippet demonstrates how to set up and use the PruningContentFilter, which discards less relevant nodes based on text density, link density, and tag importance. It configures threshold parameters and integrates the filter with the crawler.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.content_filter_strategy import PruningContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

async def main():
    # Step 1: Create a pruning filter
    prune_filter = PruningContentFilter(
        # Lower â†’ more content retained, higher â†’ more content pruned
        threshold=0.45,           
        # "fixed" or "dynamic"
        threshold_type="dynamic",  
        # Ignore nodes with <5 words
        min_word_threshold=5      
    )

    # Step 2: Insert it into a Markdown Generator
    md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)
    
    # Step 3: Pass it to CrawlerRunConfig
    config = CrawlerRunConfig(
        markdown_generator=md_generator
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news.ycombinator.com", 
            config=config
        )
        
        if result.success:
            # 'fit_markdown' is your pruned content, focusing on "denser" text
            print("Raw Markdown length:", len(result.markdown.raw_markdown))
            print("Fit Markdown length:", len(result.markdown.fit_markdown))
        else:
            print("Error:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: CSS Extraction Example in Python
DESCRIPTION: Example of using JsonCssExtractionStrategy to extract structured product data from a website using CSS selectors. The example demonstrates schema definition for extracting product titles, prices, and images.

LANGUAGE: python
CODE:
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

# Define schema
schema = {
    "name": "Product List",
    "baseSelector": ".product-card",
    "fields": [
        {
            "name": "title",
            "selector": "h2.title",
            "type": "text"
        },
        {
            "name": "price",
            "selector": ".price",
            "type": "text",
            "transform": "strip"
        },
        {
            "name": "image",
            "selector": "img",
            "type": "attribute",
            "attribute": "src"
        }
    ]
}

# Create and use strategy
strategy = JsonCssExtractionStrategy(schema)
result = await crawler.arun(
    url="https://example.com/products",
    extraction_strategy=strategy
)

----------------------------------------

TITLE: Using LLMContentFilter for Intelligent Content Extraction
DESCRIPTION: Complete example demonstrating how to use LLMContentFilter with an LLM (like GPT-4) to intelligently filter and extract content from webpages. This filter uses AI to understand context and extract relevant information based on custom instructions.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, LLMConfig
from crawl4ai.content_filter_strategy import LLMContentFilter

async def main():
    # Initialize LLM filter with specific instruction
    filter = LLMContentFilter(
        llm_config = LLMConfig(provider="openai/gpt-4o",api_token="your-api-token"), #or use environment variable
        instruction="""
        Focus on extracting the core educational content.
        Include:
        - Key concepts and explanations
        - Important code examples
        - Essential technical details
        Exclude:
        - Navigation elements
        - Sidebars
        - Footer content
        Format the output as clean markdown with proper code blocks and headers.
        """,
        chunk_token_threshold=4096,  # Adjust based on your needs
        verbose=True
    )

    config = CrawlerRunConfig(
        content_filter=filter
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://example.com", config=config)
        print(result.markdown.fit_markdown)  # Filtered markdown content

----------------------------------------

TITLE: Using a Custom Dispatcher with arun_many in Python
DESCRIPTION: This example shows how to use arun_many with a custom MemoryAdaptiveDispatcher that controls concurrency based on system memory usage, with specific thresholds and permit limits configured.

LANGUAGE: python
CODE:
dispatcher = MemoryAdaptiveDispatcher(
    memory_threshold_percent=70.0,
    max_session_permit=10
)
results = await crawler.arun_many(
    urls=["https://site1.com", "https://site2.com", "https://site3.com"],
    config=my_run_config,
    dispatcher=dispatcher
)

----------------------------------------

TITLE: Comprehensive Link and Media Filtering with Crawl4AI
DESCRIPTION: Complete example demonstrating how to filter links and media in a single crawl operation. This snippet shows how to exclude external links, specific domains, social media links, and external images while ensuring images are fully loaded.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def main():
    # Suppose we want to keep only internal links, remove certain domains, 
    # and discard external images from the final crawl data.
    crawler_cfg = CrawlerRunConfig(
        exclude_external_links=True,
        exclude_domains=["spammyads.com"],
        exclude_social_media_links=True,   # skip Twitter, Facebook, etc.
        exclude_external_images=True,      # keep only images from main domain
        wait_for_images=True,             # ensure images are loaded
        verbose=True
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://www.example.com", config=crawler_cfg)

        if result.success:
            print("[OK] Crawled:", result.url)
            
            # 1. Links
            in_links = result.links.get("internal", [])
            ext_links = result.links.get("external", [])
            print("Internal link count:", len(in_links))
            print("External link count:", len(ext_links))  # should be zero with exclude_external_links=True
            
            # 2. Images
            images = result.media.get("images", [])
            print("Images found:", len(images))
            
            # Let's see a snippet of these images
            for i, img in enumerate(images[:3]):
                print(f"  - {img['src']} (alt={img.get('alt','')}, score={img.get('score','N/A')})")
        else:
            print("[ERROR] Failed to crawl. Reason:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Creating a Custom Scraping Strategy in Crawl4AI
DESCRIPTION: This example shows how to create a custom scraping strategy by inheriting from ContentScrapingStrategy. It demonstrates the required structure for implementing both synchronous and asynchronous scraping methods that return a ScrapingResult object with detailed content structure.

LANGUAGE: python
CODE:
from crawl4ai import ContentScrapingStrategy, ScrapingResult, MediaItem, Media, Link, Links

class CustomScrapingStrategy(ContentScrapingStrategy):
    def scrap(self, url: str, html: str, **kwargs) -> ScrapingResult:
        # Implement your custom scraping logic here
        return ScrapingResult(
            cleaned_html="<html>...</html>",  # Cleaned HTML content
            success=True,                     # Whether scraping was successful
            media=Media(
                images=[                      # List of images found
                    MediaItem(
                        src="https://example.com/image.jpg",
                        alt="Image description",
                        desc="Surrounding text",
                        score=1,
                        type="image",
                        group_id=1,
                        format="jpg",
                        width=800
                    )
                ],
                videos=[],                    # List of videos (same structure as images)
                audios=[]                     # List of audio files (same structure as images)
            ),
            links=Links(
                internal=[                    # List of internal links
                    Link(
                        href="https://example.com/page",
                        text="Link text",
                        title="Link title",
                        base_domain="example.com"
                    )
                ],
                external=[]                   # List of external links (same structure)
            ),
            metadata={                        # Additional metadata
                "title": "Page Title",
                "description": "Page description"
            }
        )

    async def ascrap(self, url: str, html: str, **kwargs) -> ScrapingResult:
        # For simple cases, you can use the sync version
        return await asyncio.to_thread(self.scrap, url, html, **kwargs)

----------------------------------------

TITLE: Using LLMContentFilter for Intelligent Markdown Generation
DESCRIPTION: Shows how to integrate LLM-powered content filtering in Crawl4AI to generate more focused and relevant markdown output. The example configures a Gemini-powered content filter with specific extraction instructions and applies it to generate a markdown summary for a webpage.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator
from crawl4ai.content_filter_strategy import LLMContentFilter
from crawl4ai.types import LLMConfig
import asyncio

llm_config = LLMConfig(provider="gemini/gemini-1.5-pro", api_token="env:GEMINI_API_KEY")

markdown_generator = DefaultMarkdownGenerator(
    content_filter=LLMContentFilter(llm_config=llm_config, instruction="Extract key concepts and summaries")
)

config = CrawlerRunConfig(markdown_generator=markdown_generator)
async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://docs.crawl4ai.com", config=config)
        print(result.markdown.fit_markdown)

asyncio.run(main())

----------------------------------------

TITLE: Anti-Bot Detection Configuration
DESCRIPTION: Shows how to enable anti-bot detection features including magic mode, user simulation, and navigator overrides to avoid being blocked by websites.

LANGUAGE: python
CODE:
run_config = CrawlerRunConfig(
    magic=True,
    simulate_user=True,
    override_navigator=True
)

----------------------------------------

TITLE: Setting Custom Headers in Crawl4AI
DESCRIPTION: Shows two approaches for setting custom headers in Crawl4AI: configuring them at the crawler strategy level or passing them directly to the arun() method. The example demonstrates setting language preferences and custom user agent strings, which can be useful for authentication or locale testing.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    # Option 1: Set headers at the crawler strategy level
    crawler1 = AsyncWebCrawler(
        # The underlying strategy can accept headers in its constructor
        crawler_strategy=None  # We'll override below for clarity
    )
    crawler1.crawler_strategy.update_user_agent("MyCustomUA/1.0")
    crawler1.crawler_strategy.set_custom_headers({
        "Accept-Language": "fr-FR,fr;q=0.9"
    })
    result1 = await crawler1.arun("https://www.example.com")
    print("Example 1 result success:", result1.success)

    # Option 2: Pass headers directly to `arun()`
    crawler2 = AsyncWebCrawler()
    result2 = await crawler2.arun(
        url="https://www.example.com",
        headers={"Accept-Language": "es-ES,es;q=0.9"}
    )
    print("Example 2 result success:", result2.success)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Basic JavaScript Execution in Crawl4AI
DESCRIPTION: Demonstrates how to execute JavaScript code in Crawl4AI using CrawlerRunConfig, with examples of scrolling to the bottom of a page and clicking a "Load More" button.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async def main():
    # Single JS command
    config = CrawlerRunConfig(
        js_code="window.scrollTo(0, document.body.scrollHeight);"
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news.ycombinator.com",  # Example site
            config=config
        )
        print("Crawled length:", len(result.cleaned_html))

    # Multiple commands
    js_commands = [
        "window.scrollTo(0, document.body.scrollHeight);",
        # 'More' link on Hacker News
        "document.querySelector('a.morelink')?.click();",  
    ]
    config = CrawlerRunConfig(js_code=js_commands)

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news.ycombinator.com",  # Another pass
            config=config
        )
        print("After scroll+click, length:", len(result.cleaned_html))

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Implementing Knowledge-Optimal Crawler in Python
DESCRIPTION: Example of using the Knowledge-Optimal Crawler to efficiently extract data while maximizing knowledge acquisition. The system implements smart content prioritization and objective-driven crawling to minimize data extraction while maximizing knowledge gain.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler
from crawl4ai.optimization import KnowledgeOptimizer

async with AsyncWebCrawler() as crawler:
    optimizer = KnowledgeOptimizer(
        objective="Understand GPU instance pricing and limitations across cloud providers",
        required_knowledge=[
            "pricing structure",
            "GPU specifications",
            "usage limits",
            "availability zones"
        ],
        confidence_threshold=0.85
    )
    
    result = await crawler.arun(
        urls=[
            "https://aws.amazon.com/ec2/pricing/",
            "https://cloud.google.com/gpu",
            "https://azure.microsoft.com/pricing/"
        ],
        optimizer=optimizer,
        optimization_mode="minimal_extraction"
    )
    
    print(f"Knowledge Coverage: {result.knowledge_coverage}")
    print(f"Data Efficiency: {result.efficiency_ratio}")
    print(f"Extracted Content: {result.optimal_content}")

----------------------------------------

TITLE: Defining CrawlResult and MarkdownGenerationResult Models in Python
DESCRIPTION: Base models that define the structure of crawl results in Crawl4AI. The MarkdownGenerationResult captures different markdown formats, while CrawlResult stores comprehensive crawl data including HTML, extracted content, media, links, and more.

LANGUAGE: python
CODE:
class MarkdownGenerationResult(BaseModel):
    raw_markdown: str
    markdown_with_citations: str
    references_markdown: str
    fit_markdown: Optional[str] = None
    fit_html: Optional[str] = None

class CrawlResult(BaseModel):
    url: str
    html: str
    success: bool
    cleaned_html: Optional[str] = None
    media: Dict[str, List[Dict]] = {}
    links: Dict[str, List[Dict]] = {}
    downloaded_files: Optional[List[str]] = None
    screenshot: Optional[str] = None
    pdf : Optional[bytes] = None
    markdown: Optional[Union[str, MarkdownGenerationResult]] = None
    extracted_content: Optional[str] = None
    metadata: Optional[dict] = None
    error_message: Optional[str] = None
    session_id: Optional[str] = None
    response_headers: Optional[dict] = None
    status_code: Optional[int] = None
    ssl_certificate: Optional[SSLCertificate] = None
    class Config:
        arbitrary_types_allowed = True

----------------------------------------

TITLE: Configuring crawl4ai to Capture Lazy-Loaded Images in Python
DESCRIPTION: This example demonstrates how to configure AsyncWebCrawler to capture lazy-loaded images by enabling wait_for_images, scan_full_page, and scroll_delay options. The crawler will wait for images to load fully and scroll through the entire page to trigger lazy-loading mechanisms.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig
from crawl4ai.async_configs import CacheMode

async def main():
    config = CrawlerRunConfig(
        # Force the crawler to wait until images are fully loaded
        wait_for_images=True,

        # Option 1: If you want to automatically scroll the page to load images
        scan_full_page=True,  # Tells the crawler to try scrolling the entire page
        scroll_delay=0.5,     # Delay (seconds) between scroll steps

        # Option 2: If the site uses a 'Load More' or JS triggers for images,
        # you can also specify js_code or wait_for logic here.

        cache_mode=CacheMode.BYPASS,
        verbose=True
    )

    async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:
        result = await crawler.arun("https://www.example.com/gallery", config=config)
        
        if result.success:
            images = result.media.get("images", [])
            print("Images found:", len(images))
            for i, img in enumerate(images[:5]):
                print(f"[Image {i}] URL: {img['src']}, Score: {img.get('score','N/A')}")
        else:
            print("Error:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Configuring DFSDeepCrawlStrategy with Parameters in Python
DESCRIPTION: This code demonstrates how to create a DFSDeepCrawlStrategy instance for depth-first crawling. It configures depth limits, domain boundaries, page limits, and scoring thresholds for depth-first traversal of web content.

LANGUAGE: python
CODE:
from crawl4ai.deep_crawling import DFSDeepCrawlStrategy

# Basic configuration
strategy = DFSDeepCrawlStrategy(
    max_depth=2,               # Crawl initial page + 2 levels deep
    include_external=False,    # Stay within the same domain
    max_pages=30,              # Maximum number of pages to crawl (optional)
    score_threshold=0.5,       # Minimum score for URLs to be crawled (optional)
)

----------------------------------------

TITLE: Configuring Extraction Strategy with Dynamic Content in Crawl4AI
DESCRIPTION: This snippet demonstrates how to set up a crawler configuration that combines JavaScript interaction with structured data extraction. It defines a JSON CSS extraction strategy with a schema targeting commit information, then configures the crawler with JavaScript for pagination and wait conditions.

LANGUAGE: python
CODE:
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

schema = {
    "name": "Commits",
    "baseSelector": "li.Box-sc-g0xbh4-0",
    "fields": [
        {"name": "title", "selector": "h4.markdown-title", "type": "text"}
    ]
}
config = CrawlerRunConfig(
    session_id="ts_commits_session",
    js_code=js_next_page,
    wait_for=wait_for_more,
    extraction_strategy=JsonCssExtractionStrategy(schema)
)

----------------------------------------

TITLE: Initializing JsonCssExtractionStrategy in Python
DESCRIPTION: Configuration for the JsonCssExtractionStrategy class used for CSS selector-based structured data extraction. This strategy requires a schema defining base selectors and fields to extract.

LANGUAGE: python
CODE:
JsonCssExtractionStrategy(
    schema: Dict[str, Any],    # Extraction schema
    verbose: bool = False      # Enable verbose logging
)

# Schema Structure
schema = {
    "name": str,              # Schema name
    "baseSelector": str,      # Base CSS selector
    "fields": [               # List of fields to extract
        {
            "name": str,      # Field name
            "selector": str,  # CSS selector
            "type": str,     # Field type: "text", "attribute", "html", "regex"
            "attribute": str, # For type="attribute"
            "pattern": str,  # For type="regex"
            "transform": str, # Optional: "lowercase", "uppercase", "strip"
            "default": Any    # Default value if extraction fails
        }
    ]
}

----------------------------------------

TITLE: Multi-Step Interaction Example with Crawl4AI
DESCRIPTION: A comprehensive example demonstrating multi-step interaction with a website, specifically showing how to navigate through multiple pages of GitHub commits by clicking the "Next Page" button and waiting for new content.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

async def multi_page_commits():
    browser_cfg = BrowserConfig(
        headless=False,  # Visible for demonstration
        verbose=True
    )
    session_id = "github_ts_commits"
    
    base_wait = """js:() => {
        const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');
        return commits.length > 0;
    }"""

    # Step 1: Load initial commits
    config1 = CrawlerRunConfig(
        wait_for=base_wait,
        session_id=session_id,
        cache_mode=CacheMode.BYPASS,
        # Not using js_only yet since it's our first load
    )

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(
            url="https://github.com/microsoft/TypeScript/commits/main",
            config=config1
        )
        print("Initial commits loaded. Count:", result.cleaned_html.count("commit"))

        # Step 2: For subsequent pages, we run JS to click 'Next Page' if it exists
        js_next_page = """
        const selector = 'a[data-testid="pagination-next-button"]';
        const button = document.querySelector(selector);
        if (button) button.click();
        """
        
        # Wait until new commits appear
        wait_for_more = """js:() => {
            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');
            if (!window.firstCommit && commits.length>0) {
                window.firstCommit = commits[0].textContent;
                return false;
            }
            // If top commit changes, we have new commits
            const topNow = commits[0]?.textContent.trim();
            return topNow && topNow !== window.firstCommit;
        }"""

        for page in range(2):  # let's do 2 more "Next" pages
            config_next = CrawlerRunConfig(
                session_id=session_id,
                js_code=js_next_page,
                wait_for=wait_for_more,
                js_only=True,       # We're continuing from the open tab
                cache_mode=CacheMode.BYPASS
            )
            result2 = await crawler.arun(
                url="https://github.com/microsoft/TypeScript/commits/main",
                config=config_next
            )
            print(f"Page {page+2} commits count:", result2.cleaned_html.count("commit"))

        # Optionally kill session
        await crawler.crawler_strategy.kill_session(session_id)

async def main():
    await multi_page_commits()

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Configuring SemaphoreDispatcher in Crawl4AI
DESCRIPTION: Sets up a SemaphoreDispatcher for simple concurrency control with a fixed limit. Includes optional rate limiter and monitoring configuration for efficient crawling operations.

LANGUAGE: python
CODE:
from crawl4ai.async_dispatcher import SemaphoreDispatcher

dispatcher = SemaphoreDispatcher(
    max_session_permit=20,         # Maximum concurrent tasks
    rate_limiter=RateLimiter(      # Optional rate limiting
        base_delay=(0.5, 1.0),
        max_delay=10.0
    ),
    monitor=CrawlerMonitor(        # Optional monitoring
        max_visible_rows=15,
        display_mode=DisplayMode.DETAILED
    )
)

----------------------------------------

TITLE: Accessing CrawlResult Fields in Python
DESCRIPTION: Demonstrates how to access various fields from the CrawlResult object including success status, HTTP details, links, markdown content, and extracted structured data. Also handles error cases when the crawl fails.

LANGUAGE: python
CODE:
if result.success:
    print(result.status_code, result.response_headers)
    print("Links found:", len(result.links.get("internal", [])))
    if result.markdown:
        print("Markdown snippet:", result.markdown.raw_markdown[:200])
    if result.extracted_content:
        print("Structured JSON:", result.extracted_content)
else:
    print("Error:", result.error_message)

----------------------------------------

TITLE: Form Interaction with Crawl4AI
DESCRIPTION: Example of how to interact with forms on a webpage, including filling in form fields and submitting the form using JavaScript through the js_code parameter.

LANGUAGE: javascript
CODE:
js_form_interaction = """
document.querySelector('#your-search').value = 'TypeScript commits';
document.querySelector('form').submit();
"""

config = CrawlerRunConfig(
    js_code=js_form_interaction,
    wait_for="css:.commit"
)
result = await crawler.arun(url="https://github.com/search", config=config)

----------------------------------------

TITLE: Basic Usage of CosineStrategy for Web Content Extraction in Python
DESCRIPTION: Demonstrates how to initialize the CosineStrategy with basic parameters and use it with AsyncWebCrawler to extract content from a URL based on semantic similarity.

LANGUAGE: python
CODE:
from crawl4ai.extraction_strategy import CosineStrategy

strategy = CosineStrategy(
    semantic_filter="product reviews",    # Target content type
    word_count_threshold=10,             # Minimum words per cluster
    sim_threshold=0.3                    # Similarity threshold
)

async with AsyncWebCrawler() as crawler:
    result = await crawler.arun(
        url="https://example.com/reviews",
        extraction_strategy=strategy
    )
    
    content = result.extracted_content

----------------------------------------

TITLE: Using Hooks for Custom Workflow in Web Crawling
DESCRIPTION: Demonstrates how to use hooks to execute custom logic at specific stages of the crawling process. This example sets up a 'before_goto' hook that runs just before navigating to the target URL.

LANGUAGE: python
CODE:
async def custom_hook_workflow():
    async with AsyncWebCrawler() as crawler:
        # Set a 'before_goto' hook to run custom code just before navigation
        crawler.crawler_strategy.set_hook("before_goto", lambda page: print("[Hook] Preparing to navigate..."))
        
        # Perform the crawl operation
        result = await crawler.arun(
            url="https://crawl4ai.com",
            bypass_cache=True
        )
        print(result.markdown.raw_markdown[:500])  # Display the first 500 characters

asyncio.run(custom_hook_workflow())

----------------------------------------

TITLE: Configuring MemoryAdaptiveDispatcher in Crawl4AI
DESCRIPTION: Initializes a MemoryAdaptiveDispatcher that automatically manages concurrency based on system memory usage. Includes parameters for memory thresholds, concurrency limits, rate limiting, and monitoring.

LANGUAGE: python
CODE:
from crawl4ai.async_dispatcher import MemoryAdaptiveDispatcher

dispatcher = MemoryAdaptiveDispatcher(
    memory_threshold_percent=90.0,  # Pause if memory exceeds this
    check_interval=1.0,             # How often to check memory
    max_session_permit=10,          # Maximum concurrent tasks
    rate_limiter=RateLimiter(       # Optional rate limiting
        base_delay=(1.0, 2.0),
        max_delay=30.0,
        max_retries=2
    ),
    monitor=CrawlerMonitor(         # Optional monitoring
        max_visible_rows=15,
        display_mode=DisplayMode.DETAILED
    )
)

----------------------------------------

TITLE: LLMContentFilter for Focused Content Extraction
DESCRIPTION: Example configuring LLMContentFilter to focus on extracting specific types of content from a webpage. This configuration targets technical documentation, code examples, and API references, reformatting them into well-structured markdown.

LANGUAGE: python
CODE:
filter = LLMContentFilter(
    instruction="""
    Focus on extracting specific types of content:
    - Technical documentation
    - Code examples
    - API references
    Reformat the content into clear, well-structured markdown
    """,
    chunk_token_threshold=4096
)

----------------------------------------

TITLE: Definition of arun_many() Method in AsyncWebCrawler
DESCRIPTION: The signature of the arun_many() method used for batch processing multiple URLs. This method implements intelligent rate limiting, resource monitoring, and concurrent crawling, returning a list of CrawlResult objects.

LANGUAGE: python
CODE:
async def arun_many(
    self,
    urls: List[str],
    config: Optional[CrawlerRunConfig] = None,
    # Legacy parameters maintained for backwards compatibility...
) -> List[CrawlResult]:
    """
    Process multiple URLs with intelligent rate limiting and resource monitoring.
    """

----------------------------------------

TITLE: Timing Control in Crawl4AI
DESCRIPTION: Shows how to control timing aspects of the crawling process including page timeout, delay before returning HTML, and delays between multiple requests.

LANGUAGE: python
CODE:
config = CrawlerRunConfig(
    page_timeout=60000,  # 60s limit
    delay_before_return_html=2.5
)

----------------------------------------

TITLE: Defining CrawlerRunConfig Class in Python
DESCRIPTION: Shows the structure of the CrawlerRunConfig class which controls how each crawl operates. Parameters include extraction settings, caching, JavaScript execution, wait conditions, and rate limiting options.

LANGUAGE: python
CODE:
class CrawlerRunConfig:
    def __init__(
        word_count_threshold=200,
        extraction_strategy=None,
        markdown_generator=None,
        cache_mode=None,
        js_code=None,
        wait_for=None,
        screenshot=False,
        pdf=False,
        enable_rate_limiting=False,
        rate_limit_config=None,
        memory_threshold_percent=70.0,
        check_interval=1.0,
        max_session_permit=20,
        display_mode=None,
        verbose=True,
        stream=False,  # Enable streaming for arun_many()
        # ... other advanced parameters omitted
    ):
        ...

----------------------------------------

TITLE: Deploying Crawl4AI to Cloud Platforms
DESCRIPTION: Shows how to use the CloudDeployer module to deploy Crawl4AI services to various cloud platforms (AWS, GCP, Azure). The code configures auto-scaling, instance types, regions, and monitoring, then retrieves deployment endpoints after successful deployment.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler
from crawl4ai.deploy import CloudDeployer

# Initialize deployer
deployer = CloudDeployer()

# Deploy crawler service
deployment = await deployer.deploy(
    service_name="crawler-cluster",
    platform="aws",  # or "gcp", "azure"
    config={
        "instance_type": "compute-optimized",
        "auto_scaling": {
            "min_instances": 2,
            "max_instances": 10,
            "scale_based_on": "cpu_usage"
        },
        "region": "us-east-1",
        "monitoring": True
    }
)

# Get deployment status and endpoints
print(f"Service Status: {deployment.status}")
print(f"API Endpoint: {deployment.endpoint}")
print(f"Monitor URL: {deployment.monitor_url}")

----------------------------------------

TITLE: Installing Crawl4AI via pip
DESCRIPTION: This command shows how to install or upgrade to the latest version of Crawl4AI using pip. The -U flag ensures that existing installations are upgraded to the newest version.

LANGUAGE: bash
CODE:
pip install -U crawl4ai

----------------------------------------

TITLE: Link Structure Example in Crawl4AI Result Object
DESCRIPTION: Example data structure showing how links are stored in the CrawlResult object. This snippet illustrates the format of internal and external links, including the href, text, title, and base_domain properties for each link.

LANGUAGE: python
CODE:
result.links = {
  "internal": [
    {
      "href": "https://kidocode.com/",
      "text": "",
      "title": "",
      "base_domain": "kidocode.com"
    },
    {
      "href": "https://kidocode.com/degrees/technology",
      "text": "Technology Degree",
      "title": "KidoCode Tech Program",
      "base_domain": "kidocode.com"
    },
    # ...
  ],
  "external": [
    # possibly other links leading to third-party sites
  ]
}

----------------------------------------

TITLE: Implementing Rotating Proxies with Crawl4AI
DESCRIPTION: This example shows how to dynamically rotate proxies for different requests using a custom function to retrieve a new proxy for each URL. It uses the clone method on CrawlerRunConfig to create a new configuration with the updated proxy settings.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def get_next_proxy():
    # Your proxy rotation logic here
    return {"server": "http://next.proxy.com:8080"}

async def main():
    browser_config = BrowserConfig()
    run_config = CrawlerRunConfig()
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        # For each URL, create a new run config with different proxy
        for url in urls:
            proxy = await get_next_proxy()
            # Clone the config and update proxy - this creates a new browser context
            current_config = run_config.clone(proxy_config=proxy)
            result = await crawler.arun(url=url, config=current_config)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

----------------------------------------

TITLE: Processing Media Elements from Crawl Results
DESCRIPTION: Example of how to process and filter media elements (like images) from the crawl results, including accessing their URLs, descriptions, and relevance scores.

LANGUAGE: python
CODE:
images = result.media.get("images", [])
for img in images:
    if img.get("score", 0) > 5:
        print("High-value image:", img["src"])

----------------------------------------

TITLE: Configuring Browser Settings with BrowserConfig in Python
DESCRIPTION: Example showing how to create a BrowserConfig object with custom browser settings including browser type, headless mode, viewport dimensions, proxy settings, and user agent specification.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler, BrowserConfig

browser_cfg = BrowserConfig(
    browser_type="chromium",
    headless=True,
    viewport_width=1280,
    viewport_height=720,
    proxy="http://user:pass@proxy:8080",
    user_agent="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/116.0.0.0 Safari/537.36",
)

----------------------------------------

TITLE: Working with Dispatcher Results for Parallel Crawling
DESCRIPTION: Demonstrates how to access dispatch_result information when crawling URLs in parallel, including task IDs, memory usage, and execution durations.

LANGUAGE: python
CODE:
# Example usage:
for result in results:
    if result.success and result.dispatch_result:
        dr = result.dispatch_result
        print(f"URL: {result.url}, Task ID: {dr.task_id}")
        print(f"Memory: {dr.memory_usage:.1f} MB (Peak: {dr.peak_memory:.1f} MB)")
        print(f"Duration: {dr.end_time - dr.start_time}")

----------------------------------------

TITLE: Using Basic URL Pattern Filter for Crawling in Python
DESCRIPTION: This code demonstrates how to create and apply a basic URL pattern filter to limit crawling to URLs containing specific patterns. It uses wildcard patterns to match URLs containing 'blog' or 'docs'.

LANGUAGE: python
CODE:
from crawl4ai.deep_crawling.filters import FilterChain, URLPatternFilter

# Only follow URLs containing "blog" or "docs"
url_filter = URLPatternFilter(patterns=["*blog*", "*docs*"])

config = CrawlerRunConfig(
    deep_crawl_strategy=BFSDeepCrawlStrategy(
        max_depth=1,
        filter_chain=FilterChain([url_filter])
    )
)

----------------------------------------

TITLE: Load More Example with Crawl4AI
DESCRIPTION: A complete example showing how to handle "Load More" functionality by combining JavaScript execution, wait conditions, and session management in a multi-step crawling process.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async def main():
    # Step 1: Load initial Hacker News page
    config = CrawlerRunConfig(
        wait_for="css:.athing:nth-child(30)"  # Wait for 30 items
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news.ycombinator.com",
            config=config
        )
        print("Initial items loaded.")

        # Step 2: Let's scroll and click the "More" link
        load_more_js = [
            "window.scrollTo(0, document.body.scrollHeight);",
            # The "More" link at page bottom
            "document.querySelector('a.morelink')?.click();"  
        ]
        
        next_page_conf = CrawlerRunConfig(
            js_code=load_more_js,
            wait_for="""js:() => {
                return document.querySelectorAll('.athing').length > 30;
            }""",
            # Mark that we do not re-navigate, but run JS in the same session:
            js_only=True,
            session_id="hn_session"
        )

        # Re-use the same crawler session
        result2 = await crawler.arun(
            url="https://news.ycombinator.com",  # same URL but continuing session
            config=next_page_conf
        )
        total_items = result2.cleaned_html.count("athing")
        print("Items after load-more:", total_items)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Configuring Basic HTTP and SOCKS Proxies with Crawl4AI
DESCRIPTION: This example shows how to set up a crawler with both HTTP and SOCKS proxies using the BrowserConfig class in Crawl4AI. The proxy is specified directly as a URL string parameter.

LANGUAGE: python
CODE:
from crawl4ai.async_configs import BrowserConfig

# Using proxy URL
browser_config = BrowserConfig(proxy="http://proxy.example.com:8080")
async with AsyncWebCrawler(config=browser_config) as crawler:
    result = await crawler.arun(url="https://example.com")

# Using SOCKS proxy
browser_config = BrowserConfig(proxy="socks5://proxy.example.com:1080")
async with AsyncWebCrawler(config=browser_config) as crawler:
    result = await crawler.arun(url="https://example.com")

----------------------------------------

TITLE: Crawler Configuration Command-Line Options for Crawl4AI
DESCRIPTION: Shows how to configure crawler settings either by referencing a YAML configuration file or by specifying parameters directly in the command line.

LANGUAGE: bash
CODE:
# Using config file
crwl https://example.com -C crawler.yml

# Using direct parameters
crwl https://example.com -c "css_selector=#main,delay_before_return_html=2,scan_full_page=true"

----------------------------------------

TITLE: Installing Crawl4AI with Transformers Support
DESCRIPTION: Command for installing Crawl4AI with transformer dependencies, enabling text summarization and Hugging Face models integration.

LANGUAGE: bash
CODE:
pip install crawl4ai[transformer]

----------------------------------------

TITLE: Implementing CSS-based Structured Data Extraction in Crawl4AI
DESCRIPTION: Complete example that demonstrates how to use JsonCssExtractionStrategy to extract structured data from HTML. The example uses raw HTML input and shows how to access the extracted JSON data from the result.

LANGUAGE: python
CODE:
import asyncio
import json
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def main():
    schema = {
        "name": "Example Items",
        "baseSelector": "div.item",
        "fields": [
            {"name": "title", "selector": "h2", "type": "text"},
            {"name": "link", "selector": "a", "type": "attribute", "attribute": "href"}
        ]
    }
    raw_html = "<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>"

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="raw://" + raw_html,
            config=CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                extraction_strategy=JsonCssExtractionStrategy(schema)
            )
        )
        data = json.loads(result.extracted_content)
        print(data)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Content Filtering Pipeline with CosineStrategy
DESCRIPTION: Example of creating a complete content filtering pipeline function that uses CosineStrategy to extract pricing features from a URL and return structured results.

LANGUAGE: python
CODE:
strategy = CosineStrategy(
    semantic_filter="pricing plans features",
    word_count_threshold=15,
    sim_threshold=0.5,
    top_k=3
)

async def extract_pricing_features(url: str):
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url=url,
            extraction_strategy=strategy
        )
        
        if result.success:
            content = json.loads(result.extracted_content)
            return {
                'pricing_features': content,
                'clusters': len(content),
                'similarity_scores': [item['score'] for item in content]
            }

----------------------------------------

TITLE: Link Handling Configuration in Crawl4AI
DESCRIPTION: Shows how to configure link processing to exclude external links, social media links, and specific domains from the crawled content.

LANGUAGE: python
CODE:
run_config = CrawlerRunConfig(
    exclude_external_links=True,         # Remove external links from final content
    exclude_social_media_links=True,     # Remove links to known social sites
    exclude_domains=["ads.example.com"], # Exclude links to these domains
    exclude_social_media_domains=["facebook.com","twitter.com"], # Extend the default list
)

----------------------------------------

TITLE: Installing Crawl4AI Basic Package
DESCRIPTION: Command to install the core Crawl4AI library with essential dependencies, excluding advanced features like transformers or PyTorch.

LANGUAGE: bash
CODE:
pip install crawl4ai

----------------------------------------

TITLE: Initializing OverlappingWindowChunking in Python
DESCRIPTION: Configuration for the OverlappingWindowChunking class that creates chunks with specified window size and overlap, useful for processing large documents.

LANGUAGE: python
CODE:
OverlappingWindowChunking(
    window_size: int = 1000,   # Chunk size in words
    overlap: int = 100         # Overlap size in words
)

----------------------------------------

TITLE: Using Step-by-Step Approach with Session ID for Dynamic Content Loading in Crawl4AI
DESCRIPTION: This code demonstrates maintaining browser state across multiple arun() calls using a session ID. The first call loads the initial page, and the second call executes JavaScript to click a 'Next' button and waits for new content to load before extraction.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler, CacheMode

js_code = [
    # This JS finds the "Next" button and clicks it
    "const nextButton = document.querySelector('button.next'); nextButton && nextButton.click();"
]

wait_for_condition = "css:.new-content-class"

async with AsyncWebCrawler(headless=True, verbose=True) as crawler:
    # 1. Load the initial page
    result_initial = await crawler.arun(
        url="https://example.com",
        cache_mode=CacheMode.BYPASS,
        session_id="my_session"
    )

    # 2. Click the 'Next' button and wait for new content
    result_next = await crawler.arun(
        url="https://example.com",
        session_id="my_session",
        js_code=js_code,
        wait_for=wait_for_condition,
        js_only=True,
        cache_mode=CacheMode.BYPASS
    )

# `result_next` now contains the updated HTML after clicking 'Next'

----------------------------------------

TITLE: Media Filtering Configuration
DESCRIPTION: Demonstrates how to control image handling in crawled content, particularly for excluding external images.

LANGUAGE: python
CODE:
run_config = CrawlerRunConfig(
    exclude_external_images=True  # Strip images from other domains
)

----------------------------------------

TITLE: LLMContentFilter for Exact Content Preservation
DESCRIPTION: Example showing how to configure LLMContentFilter to maintain exact original wording while filtering irrelevant content. This configuration preserves the original language and structure while removing only clearly irrelevant elements.

LANGUAGE: python
CODE:
filter = LLMContentFilter(
    instruction="""
    Extract the main educational content while preserving its original wording and substance completely.
    1. Maintain the exact language and terminology
    2. Keep all technical explanations and examples intact
    3. Preserve the original flow and structure
    4. Remove only clearly irrelevant elements like navigation menus and ads
    """,
    chunk_token_threshold=4096
)

----------------------------------------

TITLE: Complete Example of Crawl4AI Functionality
DESCRIPTION: A comprehensive script that demonstrates all three crawling methods (web URL, local file, and raw HTML) in sequence. The script crawls a Wikipedia page, saves it locally, then verifies consistency between the three different crawling approaches using assertions.

LANGUAGE: python
CODE:
import os
import sys
import asyncio
from pathlib import Path
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import CrawlerRunConfig

async def main():
    wikipedia_url = "https://en.wikipedia.org/wiki/apple"
    script_dir = Path(__file__).parent
    html_file_path = script_dir / "apple.html"

    async with AsyncWebCrawler() as crawler:
        # Step 1: Crawl the Web URL
        print("\n=== Step 1: Crawling the Wikipedia URL ===")
        web_config = CrawlerRunConfig(bypass_cache=True)
        result = await crawler.arun(url=wikipedia_url, config=web_config)

        if not result.success:
            print(f"Failed to crawl {wikipedia_url}: {result.error_message}")
            return

        with open(html_file_path, 'w', encoding='utf-8') as f:
            f.write(result.html)
        web_crawl_length = len(result.markdown)
        print(f"Length of markdown from web crawl: {web_crawl_length}\n")

        # Step 2: Crawl from the Local HTML File
        print("=== Step 2: Crawling from the Local HTML File ===")
        file_url = f"file://{html_file_path.resolve()}"
        file_config = CrawlerRunConfig(bypass_cache=True)
        local_result = await crawler.arun(url=file_url, config=file_config)

        if not local_result.success:
            print(f"Failed to crawl local file {file_url}: {local_result.error_message}")
            return

        local_crawl_length = len(local_result.markdown)
        assert web_crawl_length == local_crawl_length, "Markdown length mismatch"
        print("âœ… Markdown length matches between web and local file crawl.\n")

        # Step 3: Crawl Using Raw HTML Content
        print("=== Step 3: Crawling Using Raw HTML Content ===")
        with open(html_file_path, 'r', encoding='utf-8') as f:
            raw_html_content = f.read()
        raw_html_url = f"raw:{raw_html_content}"
        raw_config = CrawlerRunConfig(bypass_cache=True)
        raw_result = await crawler.arun(url=raw_html_url, config=raw_config)

        if not raw_result.success:
            print(f"Failed to crawl raw HTML content: {raw_result.error_message}")
            return

        raw_crawl_length = len(raw_result.markdown)
        assert web_crawl_length == raw_crawl_length, "Markdown length mismatch"
        print("âœ… Markdown length matches between web and raw HTML crawl.\n")

        print("All tests passed successfully!")
    if html_file_path.exists():
        os.remove(html_file_path)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Content Selection and Filtering Configuration
DESCRIPTION: Demonstrates how to use CSS selectors and exclusion parameters to focus crawling on specific page regions while removing unwanted elements like forms and navigation.

LANGUAGE: python
CODE:
run_config = CrawlerRunConfig(
    css_selector=".main-content",  # Focus on .main-content region only
    excluded_tags=["form", "nav"], # Remove entire tag blocks
    remove_forms=True,             # Specifically strip <form> elements
    remove_overlay_elements=True,  # Attempt to remove modals/popups
)

----------------------------------------

TITLE: Browser Flow and Timing Configuration
DESCRIPTION: Shows how to configure page navigation timing, including waiting for specific elements, setting delays, and page timeouts for dynamic content.

LANGUAGE: python
CODE:
run_config = CrawlerRunConfig(
    wait_for="css:.dynamic-content", # Wait for .dynamic-content
    delay_before_return_html=2.0,    # Wait 2s before capturing final HTML
    page_timeout=60000,             # Navigation & script timeout (ms)
)

----------------------------------------

TITLE: Semantic Content Extraction Using Cosine Similarity
DESCRIPTION: Uses cosine similarity-based semantic clustering to extract relevant content from a webpage. This technique identifies and extracts sections that are contextually similar to specified keywords or topics.

LANGUAGE: python
CODE:
from crawl4ai.extraction_strategy import CosineStrategy

async def cosine_similarity_extraction():
    async with AsyncWebCrawler() as crawler:
        strategy = CosineStrategy(
            word_count_threshold=10,
            max_dist=0.2, # Maximum distance between two words
            linkage_method="ward", # Linkage method for hierarchical clustering (ward, complete, average, single)
            top_k=3, # Number of top keywords to extract
            sim_threshold=0.3, # Similarity threshold for clustering
            semantic_filter="McDonald's economic impact, American consumer trends", # Keywords to filter the content semantically using embeddings
            verbose=True
        )
        
        result = await crawler.arun(
            url="https://www.nbcnews.com/business/consumer/how-mcdonalds-e-coli-crisis-inflation-politics-reflect-american-story-rcna177156",
            extraction_strategy=strategy
        )
        print(json.loads(result.extracted_content)[:5])

asyncio.run(cosine_similarity_extraction())

----------------------------------------

TITLE: Processing Image Data from CrawlResult in Crawl4AI
DESCRIPTION: Demonstrates how to iterate through the images collected in the media dictionary of a CrawlResult, accessing their source URLs and alt text attributes.

LANGUAGE: python
CODE:
images = result.media.get("images", [])
for img in images:
    print("Image URL:", img["src"], "Alt:", img.get("alt"))

----------------------------------------

TITLE: Setting Page Limits in crawl4ai Web Crawler
DESCRIPTION: Demonstrates how to limit the total number of pages crawled using the max_pages parameter, which is useful for controlling API costs, setting predictable execution times, and testing crawl configurations.

LANGUAGE: python
CODE:
# Limit to exactly 20 pages regardless of depth
strategy = BFSDeepCrawlStrategy(
    max_depth=3,
    max_pages=20
)

----------------------------------------

TITLE: Initializing AsyncWebCrawler Class in Python
DESCRIPTION: The constructor for AsyncWebCrawler class, which accepts parameters for crawler strategy, browser configuration, caching behavior, base directory, and thread safety options. This is the core class used for initiating asynchronous web crawling in the Crawl4AI library.

LANGUAGE: python
CODE:
class AsyncWebCrawler:
    def __init__(
        self,
        crawler_strategy: Optional[AsyncCrawlerStrategy] = None,
        config: Optional[BrowserConfig] = None,
        always_bypass_cache: bool = False,           # deprecated
        always_by_pass_cache: Optional[bool] = None, # also deprecated
        base_directory: str = ...,
        thread_safe: bool = False,
        **kwargs,
    ):
        """
        Create an AsyncWebCrawler instance.

        Args:
            crawler_strategy: 
                (Advanced) Provide a custom crawler strategy if needed.
            config: 
                A BrowserConfig object specifying how the browser is set up.
            always_bypass_cache: 
                (Deprecated) Use CrawlerRunConfig.cache_mode instead.
            base_directory:     
                Folder for storing caches/logs (if relevant).
            thread_safe: 
                If True, attempts some concurrency safeguards. Usually False.
            **kwargs: 
                Additional legacy or debugging parameters.
        """
    )

----------------------------------------

TITLE: JavaScript Execution Configuration
DESCRIPTION: Demonstrates how to configure JavaScript execution within the crawler, allowing for custom script execution and interaction with dynamic page elements.

LANGUAGE: python
CODE:
run_config = CrawlerRunConfig(
    js_code=[
        "window.scrollTo(0, document.body.scrollHeight);",
        "document.querySelector('.load-more')?.click();"
    ],
    js_only=False
)

----------------------------------------

TITLE: Loading SSL certificate from a local file
DESCRIPTION: Demonstrates loading an SSL certificate from a local file containing certificate data in ASN.1 or DER format.

LANGUAGE: python
CODE:
cert = SSLCertificate.from_file("/path/to/cert.der")

----------------------------------------

TITLE: Implementing Non-Streaming Mode for Web Crawling in Python
DESCRIPTION: This code demonstrates the default non-streaming mode for web crawling. It collects all results before returning them, suitable for batch processing where the complete dataset is needed before processing.

LANGUAGE: python
CODE:
config = CrawlerRunConfig(
    deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1),
    stream=False  # Default behavior
)

async with AsyncWebCrawler() as crawler:
    # Wait for ALL results to be collected before returning
    results = await crawler.arun("https://example.com", config=config)
    
    for result in results:
        process_result(result)

----------------------------------------

TITLE: Implementing a Basic Web Crawl with Crawl4AI
DESCRIPTION: Minimal Python script demonstrating a basic crawl using AsyncWebCrawler with BrowserConfig and CrawlerRunConfig. This example crawls example.com and prints the first 300 characters of extracted markdown text.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://www.example.com",
        )
        print(result.markdown[:300])  # Show the first 300 characters of extracted text

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Installing Crawl4AI with PyTorch Features
DESCRIPTION: Commands to install Crawl4AI with PyTorch-based features for text clustering, cosine similarity, or advanced semantic chunking, followed by setup.

LANGUAGE: bash
CODE:
pip install crawl4ai[torch]
crawl4ai-setup

----------------------------------------

TITLE: Defining the CrawlResult Class Structure in Python
DESCRIPTION: Class definition for CrawlResult, which encapsulates all data returned from a crawl operation including the URL, HTML content, success status, and various optional fields for media, links, and metadata.

LANGUAGE: python
CODE:
class CrawlResult(BaseModel):
    url: str
    html: str
    success: bool
    cleaned_html: Optional[str] = None
    media: Dict[str, List[Dict]] = {}
    links: Dict[str, List[Dict]] = {}
    downloaded_files: Optional[List[str]] = None
    screenshot: Optional[str] = None
    pdf : Optional[bytes] = None
    markdown: Optional[Union[str, MarkdownGenerationResult]] = None
    extracted_content: Optional[str] = None
    metadata: Optional[dict] = None
    error_message: Optional[str] = None
    session_id: Optional[str] = None
    response_headers: Optional[dict] = None
    status_code: Optional[int] = None
    ssl_certificate: Optional[SSLCertificate] = None
    dispatch_result: Optional[DispatchResult] = None
    ...

----------------------------------------

TITLE: Implementing SEO Filter for Quality Assessment in Web Crawling
DESCRIPTION: This code snippet shows how to use the SEOFilter to identify pages with strong SEO characteristics. It can help target high-quality content based on SEO signals in the page structure.

LANGUAGE: python
CODE:
from crawl4ai.deep_crawling.filters import FilterChain, SEOFilter

----------------------------------------

TITLE: Implementing Streaming Mode for Real-time Web Crawling in Python
DESCRIPTION: This code shows how to use streaming mode to process crawl results as they become available. It returns an async iterator that yields results immediately, making it ideal for real-time applications and reducing memory pressure.

LANGUAGE: python
CODE:
config = CrawlerRunConfig(
    deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1),
    stream=True  # Enable streaming
)

async with AsyncWebCrawler() as crawler:
    # Returns an async iterator
    async for result in await crawler.arun("https://example.com", config=config):
        # Process each result as it becomes available
        process_result(result)

----------------------------------------

TITLE: Creating and Modifying Configurations with clone() Method in crawl4ai
DESCRIPTION: Demonstrates how to create a base configuration and create variations using the clone() method, which allows for creating modified copies without changing the original config.

LANGUAGE: python
CODE:
# Create a base configuration
base_config = CrawlerRunConfig(
    cache_mode=CacheMode.ENABLED,
    word_count_threshold=200
)

# Create variations using clone()
stream_config = base_config.clone(stream=True)
no_cache_config = base_config.clone(
    cache_mode=CacheMode.BYPASS,
    stream=True
)

----------------------------------------

TITLE: Structured Data Extraction Example Command
DESCRIPTION: Complete example showing structured data extraction with CSS-based configuration and schema, outputting in JSON format with verbose logging.

LANGUAGE: bash
CODE:
crwl https://example.com \
    -e extract_css.yml \
    -s css_schema.json \
    -o json \
    -v

----------------------------------------

TITLE: Using DefaultMarkdownGenerator with AsyncWebCrawler
DESCRIPTION: A minimal example demonstrating how to use the DefaultMarkdownGenerator with AsyncWebCrawler to extract markdown from a webpage. This code sets up a basic crawler configuration, performs a crawl on example.com, and prints the raw markdown output.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

async def main():
    config = CrawlerRunConfig(
        markdown_generator=DefaultMarkdownGenerator()
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://example.com", config=config)
        
        if result.success:
            print("Raw Markdown Output:\n")
            print(result.markdown)  # The unfiltered markdown from the page
        else:
            print("Crawl failed:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Implementing BM25ContentFilter with DefaultMarkdownGenerator
DESCRIPTION: Example code for using BM25ContentFilter to filter webpage content based on relevance to a search query. The filter is configured with a query, threshold, and stemming option, then passed to the DefaultMarkdownGenerator for markdown conversion.

LANGUAGE: python
CODE:
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from crawl4ai.content_filter_strategy import BM25ContentFilter
from crawl4ai import CrawlerRunConfig

bm25_filter = BM25ContentFilter(
    user_query="machine learning",
    bm25_threshold=1.2,
    use_stemming=True
)

md_generator = DefaultMarkdownGenerator(
    content_filter=bm25_filter,
    options={"ignore_links": True}
)

config = CrawlerRunConfig(markdown_generator=md_generator)

----------------------------------------

TITLE: LLM-based Extraction Strategy Configuration for Crawl4AI
DESCRIPTION: JSON configuration for using an LLM (large language model) to extract structured data from web pages with a defined JSON schema and custom instructions.

LANGUAGE: json
CODE:
{
  "crawler_config": {
    "type": "CrawlerRunConfig",
    "params": {
      "extraction_strategy": {
        "type": "LLMExtractionStrategy",
        "params": {
          "instruction": "Extract article title, author, publication date and main content",
          "provider": "openai/gpt-4",
          "api_token": "your-api-token",
          "schema": {
            "type": "dict",
            "value": {
              "title": "Article Schema",
              "type": "object",
              "properties": {
                "title": {
                  "type": "string",
                  "description": "The article's headline"
                },
                "author": {
                  "type": "string",
                  "description": "The author's name"
                },
                "published_date": {
                  "type": "string",
                  "format": "date-time",
                  "description": "Publication date and time"
                },
                "content": {
                  "type": "string",
                  "description": "The main article content"
                }
              },
              "required": ["title", "content"]
            }
          }
        }
      }
    }
  }
}

----------------------------------------

TITLE: Extracting Cryptocurrency Prices with XPath and Raw HTML in Python
DESCRIPTION: This example demonstrates using JsonXPathExtractionStrategy with the raw:// scheme to extract data from inline HTML. It shows how to define XPath selectors, configure the crawler, and process dummy HTML without making network requests.

LANGUAGE: python
CODE:
import json
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.extraction_strategy import JsonXPathExtractionStrategy

async def extract_crypto_prices_xpath():
    # 1. Minimal dummy HTML with some repeating rows
    dummy_html = """
    <html>
      <body>
        <div class='crypto-row'>
          <h2 class='coin-name'>Bitcoin</h2>
          <span class='coin-price'>$28,000</span>
        </div>
        <div class='crypto-row'>
          <h2 class='coin-name'>Ethereum</h2>
          <span class='coin-price'>$1,800</span>
        </div>
      </body>
    </html>
    """

    # 2. Define the JSON schema (XPath version)
    schema = {
        "name": "Crypto Prices via XPath",
        "baseSelector": "//div[@class='crypto-row']",
        "fields": [
            {
                "name": "coin_name",
                "selector": ".//h2[@class='coin-name']",
                "type": "text"
            },
            {
                "name": "price",
                "selector": ".//span[@class='coin-price']",
                "type": "text"
            }
        ]
    }

    # 3. Place the strategy in the CrawlerRunConfig
    config = CrawlerRunConfig(
        extraction_strategy=JsonXPathExtractionStrategy(schema, verbose=True)
    )

    # 4. Use raw:// scheme to pass dummy_html directly
    raw_url = f"raw://{dummy_html}"

    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url=raw_url,
            config=config
        )

        if not result.success:
            print("Crawl failed:", result.error_message)
            return

        data = json.loads(result.extracted_content)
        print(f"Extracted {len(data)} coin rows")
        if data:
            print("First item:", data[0])

asyncio.run(extract_crypto_prices_xpath())

----------------------------------------

TITLE: Crawler Configuration for Crawl4AI in YAML Format
DESCRIPTION: Defines crawler behavior settings in YAML format, including cache management, page loading options, content processing settings, and scroll behavior controls.

LANGUAGE: yaml
CODE:
# crawler.yml
cache_mode: "bypass"
wait_until: "networkidle"
page_timeout: 30000
delay_before_return_html: 0.5
word_count_threshold: 100
scan_full_page: true
scroll_delay: 0.3
process_iframes: false
remove_overlay_elements: true
magic: true
verbose: true

----------------------------------------

TITLE: Configuring HTML Cleaning Options in Crawl4AI
DESCRIPTION: Example showing how to configure the crawler to clean HTML by excluding specific tags and attributes. The cleaned result appears in the cleaned_html field of the CrawlResult.

LANGUAGE: python
CODE:
config = CrawlerRunConfig(
    excluded_tags=["form", "header", "footer"],
    keep_data_attributes=False
)
result = await crawler.arun("https://example.com", config=config)
print(result.cleaned_html)  # Freed of forms, header, footer, data-* attributes

----------------------------------------

TITLE: Running Crawl4AI in Docker (Experimental)
DESCRIPTION: Commands to pull and run the experimental Docker container for Crawl4AI, exposing a crawling service on port 11235.

LANGUAGE: bash
CODE:
docker pull unclecode/crawl4ai:basic
docker run -p 11235:11235 unclecode/crawl4ai:basic

----------------------------------------

TITLE: Development Configuration Settings for Crawl4AI in YAML
DESCRIPTION: YAML configuration for Crawl4AI in a development environment. Includes enabling hot reloading for faster iteration, longer timeouts for debugging, and more verbose logging at DEBUG level.

LANGUAGE: yaml
CODE:
app:
  reload: True               # Enable hot reloading
  timeout_keep_alive: 300    # Longer timeout for debugging

logging:
  level: "DEBUG"            # More verbose logging

----------------------------------------

TITLE: Configuring LLM with LLMConfig in Python
DESCRIPTION: Example of creating an LLMConfig for integrating with an LLM provider. The configuration specifies the provider model and API token retrieved from environment variables.

LANGUAGE: python
CODE:
llm_config = LLMConfig(provider="openai/gpt-4o-mini", api_token=os.getenv("OPENAI_API_KEY"))

----------------------------------------

TITLE: Basic Batch Mode Example with arun_many in Python
DESCRIPTION: A minimal example showing how to use arun_many with a default dispatcher to crawl multiple URLs in batch mode. The code iterates through the results to check success status for each crawled URL.

LANGUAGE: python
CODE:
# Minimal usage: The default dispatcher will be used
results = await crawler.arun_many(
    urls=["https://site1.com", "https://site2.com"],
    config=CrawlerRunConfig(stream=False)  # Default behavior
)

for res in results:
    if res.success:
        print(res.url, "crawled OK!")
    else:
        print("Failed:", res.url, "-", res.error_message)

----------------------------------------

TITLE: Using Custom Browser with Persistent User Profile in Python
DESCRIPTION: This example shows how to configure crawl4ai with a persistent user profile directory, which allows maintaining login sessions, cookies, and browser settings between crawling sessions. This is particularly useful for accessing content that requires authentication or has anti-bot measures.

LANGUAGE: python
CODE:
import os, sys
from pathlib import Path
import asyncio, time
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

async def test_news_crawl():
    # Create a persistent user data directory
    user_data_dir = os.path.join(Path.home(), ".crawl4ai", "browser_profile")
    os.makedirs(user_data_dir, exist_ok=True)

    browser_config = BrowserConfig(
        verbose=True,
        headless=True,
        user_data_dir=user_data_dir,
        use_persistent_context=True,
    )
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        url = "ADDRESS_OF_A_CHALLENGING_WEBSITE"
        
        result = await crawler.arun(
            url,
            config=run_config,
            magic=True,
        )
        
        print(f"Successfully crawled {url}")
        print(f"Content length: {len(result.markdown)}")

----------------------------------------

TITLE: Launching Playwright Chromium with Custom User Data Directory on Windows
DESCRIPTION: PowerShell command to launch the Playwright Chromium binary on Windows with a custom user data directory. This allows you to set up a persistent profile with your logins and preferences.

LANGUAGE: powershell
CODE:
"C:\Users\<you>\AppData\Local\ms-playwright\chromium-1234\chrome-win\chrome.exe" ^
    --user-data-dir="C:\Users\<you>\my_chrome_profile"

----------------------------------------

TITLE: Installing Crawl4AI Using pip
DESCRIPTION: Basic installation commands for Crawl4AI using pip, which installs the asynchronous version with Playwright for web crawling and sets up the browser.

LANGUAGE: bash
CODE:
pip install crawl4ai
crawl4ai-setup # Setup the browser

----------------------------------------

TITLE: Browser Configuration Command-Line Options for Crawl4AI
DESCRIPTION: Shows how to configure browser settings either by referencing a YAML configuration file or by specifying parameters directly in the command line.

LANGUAGE: bash
CODE:
# Using config file
crwl https://example.com -B browser.yml

# Using direct parameters
crwl https://example.com -b "headless=true,viewport_width=1280,user_agent_mode=random"

----------------------------------------

TITLE: Media Extraction and Handling
DESCRIPTION: Extracts images from a website and displays information about them including URL, alt text, and relevance score. This example also demonstrates how to take a screenshot of the webpage during the crawling process.

LANGUAGE: python
CODE:
async def media_handling():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business", 
            bypass_cache=True,
            exclude_external_images=False,
            screenshot=True
        )
        for img in result.media['images'][:5]:
            print(f"Image URL: {img['src']}, Alt: {img['alt']}, Score: {img['score']}")
        
asyncio.run(media_handling())

----------------------------------------

TITLE: Implementing Web Embedding Index in Python
DESCRIPTION: Example of creating and maintaining a semantic search infrastructure for crawled content using vector embeddings. The system provides automatic embedding generation, intelligent content chunking, and semantic search capabilities for efficient content retrieval.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler
from crawl4ai.indexing import WebIndex

# Initialize and build index
index = WebIndex(model="efficient-mini")

async with AsyncWebCrawler() as crawler:
    # Crawl and index content
    await index.build(
        urls=["https://docs.example.com"],
        crawler=crawler,
        options={
            "chunk_method": "semantic",
            "update_policy": "incremental",
            "embedding_batch_size": 100
        }
    )

    # Search through indexed content
    results = await index.search(
        query="How to implement OAuth authentication?",
        filters={
            "content_type": "technical",
            "recency": "6months"
        },
        top_k=5
    )

    # Get similar content
    similar = await index.find_similar(
        url="https://docs.example.com/auth/oauth",
        threshold=0.85
    )

----------------------------------------

TITLE: Article Content Extraction with CosineStrategy
DESCRIPTION: Example of configuring CosineStrategy specifically for extracting main article content from blog posts, with parameters optimized for longer text blocks.

LANGUAGE: python
CODE:
strategy = CosineStrategy(
    semantic_filter="main article content",
    word_count_threshold=100,  # Longer blocks for articles
    top_k=1                   # Usually want single main content
)

result = await crawler.arun(
    url="https://example.com/blog/post",
    extraction_strategy=strategy
)

----------------------------------------

TITLE: CSS-Based Wait Conditions in Crawl4AI
DESCRIPTION: Shows how to use CSS selectors with the wait_for parameter to make the crawler wait until a specific element appears on the page before proceeding.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async def main():
    config = CrawlerRunConfig(
        # Wait for at least 30 items on Hacker News
        wait_for="css:.athing:nth-child(30)"  
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news.ycombinator.com",
            config=config
        )
        print("We have at least 30 items loaded!")
        # Rough check
        print("Total items in HTML:", result.cleaned_html.count("athing"))  

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Handling Markdown Generation Results
DESCRIPTION: Demonstrates how to work with the markdown generation features, including raw markdown, citations, references, and filtered content produced by content filters like Pruning or BM25.

LANGUAGE: python
CODE:
if result.markdown:
    md_res = result.markdown
    print("Raw MD:", md_res.raw_markdown[:300])
    print("Citations MD:", md_res.markdown_with_citations[:300])
    print("References:", md_res.references_markdown)
    if md_res.fit_markdown:
        print("Pruned text:", md_res.fit_markdown[:300])

LANGUAGE: python
CODE:
print(result.markdown.raw_markdown[:200])
print(result.markdown.fit_markdown)
print(result.markdown.fit_html)

----------------------------------------

TITLE: Configuring DefaultMarkdownGenerator with Options
DESCRIPTION: Example showing how to customize the DefaultMarkdownGenerator by passing an options dictionary. This code configures the generator to ignore links, disable HTML escaping, and wrap text at 80 characters when converting HTML to markdown.

LANGUAGE: python
CODE:
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async def main():
    # Example: ignore all links, don't escape HTML, and wrap text at 80 characters
    md_generator = DefaultMarkdownGenerator(
        options={
            "ignore_links": True,
            "escape_html": False,
            "body_width": 80
        }
    )

    config = CrawlerRunConfig(
        markdown_generator=md_generator
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://example.com/docs", config=config)
        if result.success:
            print("Markdown:\n", result.markdown[:500])  # Just a snippet
        else:
            print("Crawl failed:", result.error_message)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

----------------------------------------

TITLE: Working with Raw and Cleaned HTML Content
DESCRIPTION: Examples of accessing the original HTML content and sanitized versions, which have scripts, styles, or excluded tags removed based on the crawl configuration.

LANGUAGE: python
CODE:
# Possibly large
print(len(result.html))

LANGUAGE: python
CODE:
print(result.cleaned_html[:500])  # Show a snippet

LANGUAGE: python
CODE:
if result.markdown.fit_html:
    print("High-value HTML content:", result.markdown.fit_html[:300])

----------------------------------------

TITLE: Using Markdown Generator with Citations in Crawl4AI
DESCRIPTION: Configures and uses the DefaultMarkdownGenerator to convert webpage content to markdown with citations. The example demonstrates how to access different markdown formats from the result.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

config = CrawlerRunConfig(
    markdown_generator=DefaultMarkdownGenerator(
        options={"citations": True, "body_width": 80}  # e.g. pass html2text style options
    )
)
result = await crawler.arun(url="https://example.com", config=config)

md_res = result.markdown  # or eventually 'result.markdown'
print(md_res.raw_markdown[:500])
print(md_res.markdown_with_citations)
print(md_res.references_markdown)

----------------------------------------

TITLE: Setting Up Crawler Run Configuration in Python
DESCRIPTION: Example demonstrating how to create a CrawlerRunConfig object to control crawl behavior with parameters for content waiting, word count thresholds, content exclusion, and enabling streaming for multiple crawls.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

run_cfg = CrawlerRunConfig(
    wait_for="css:.main-content",
    word_count_threshold=15,
    excluded_tags=["nav", "footer"],
    exclude_external_links=True,
    stream=True,  # Enable streaming for arun_many()
)

----------------------------------------

TITLE: Installing Crawl4AI Package with All Dependencies in Python
DESCRIPTION: This snippet shows the pip install command for Crawl4AI with all optional dependencies included. This is mentioned in the v0.5.0 release notes as part of the 'Get Started' instructions.

LANGUAGE: bash
CODE:
pip install "crawl4ai[all]"

----------------------------------------

TITLE: Adding Attribution Badges with HTML for Crawl4AI
DESCRIPTION: HTML code examples for adding different themed attribution badges to projects that use Crawl4AI. Includes animated disco theme, night theme with neon, classic dark and light themes, and a simple shield badge option.

LANGUAGE: html
CODE:
<!-- Disco Theme (Animated) -->
<a href="https://github.com/unclecode/crawl4ai">
  <img src="https://raw.githubusercontent.com/unclecode/crawl4ai/main/docs/assets/powered-by-disco.svg" alt="Powered by Crawl4AI" width="200"/>
</a>

<!-- Night Theme (Dark with Neon) -->
<a href="https://github.com/unclecode/crawl4ai">
  <img src="https://raw.githubusercontent.com/unclecode/crawl4ai/main/docs/assets/powered-by-night.svg" alt="Powered by Crawl4AI" width="200"/>
</a>

<!-- Dark Theme (Classic) -->
<a href="https://github.com/unclecode/crawl4ai">
  <img src="https://raw.githubusercontent.com/unclecode/crawl4ai/main/docs/assets/powered-by-dark.svg" alt="Powered by Crawl4AI" width="200"/>
</a>

<!-- Light Theme (Classic) -->
<a href="https://github.com/unclecode/crawl4ai">
  <img src="https://raw.githubusercontent.com/unclecode/crawl4ai/main/docs/assets/powered-by-light.svg" alt="Powered by Crawl4AI" width="200"/>
</a>

<!-- Simple Shield Badge -->
<a href="https://github.com/unclecode/crawl4ai">
  <img src="https://img.shields.io/badge/Powered%20by-Crawl4AI-blue?style=flat-square" alt="Powered by Crawl4AI"/>
</a>

----------------------------------------

TITLE: Initializing RateLimiter in Crawl4AI
DESCRIPTION: Defines a RateLimiter class for managing request pacing to avoid overloading servers. Implements random delays, exponential backoff for rate-limiting errors, and configurable retry logic.

LANGUAGE: python
CODE:
class RateLimiter:
    def __init__(
        # Random delay range between requests
        base_delay: Tuple[float, float] = (1.0, 3.0),  
        
        # Maximum backoff delay
        max_delay: float = 60.0,                        
        
        # Retries before giving up
        max_retries: int = 3,                          
        
        # Status codes triggering backoff
        rate_limit_codes: List[int] = [429, 503]        
    )

----------------------------------------

TITLE: Using AsyncWebCrawler with Context Manager in Python
DESCRIPTION: Shows how to use the AsyncWebCrawler as a context manager with 'async with', which automatically handles starting and closing resources. This is the recommended approach for most use cases as it ensures proper cleanup.

LANGUAGE: python
CODE:
async with AsyncWebCrawler(config=browser_cfg) as crawler:
    result = await crawler.arun("https://example.com")
    # The crawler automatically starts/closes resources

----------------------------------------

TITLE: Configuring Robots.txt Compliance for crawl4ai
DESCRIPTION: Example showing how to create a CrawlerRunConfig that respects robots.txt files and sets a custom user agent string. This enables ethical web crawling by following site owner-defined access rules.

LANGUAGE: python
CODE:
run_config = CrawlerRunConfig(
    check_robots_txt=True,  # Enable robots.txt compliance
    user_agent="MyBot/1.0"  # Identify your crawler
)

----------------------------------------

TITLE: Enabling SSL certificate fetching in Crawl4AI configuration
DESCRIPTION: Demonstrates how to enable SSL certificate fetching in a CrawlerRunConfig object by setting fetch_ssl_certificate=True.

LANGUAGE: python
CODE:
CrawlerRunConfig(fetch_ssl_certificate=True, ...)

----------------------------------------

TITLE: Dependencies List for crawl4ai Python Project
DESCRIPTION: A comprehensive list of Python package dependencies with version specifications for the crawl4ai project. Includes libraries for async database access, HTML processing, LLM integration, web scraping, file handling, text processing, and other utilities.

LANGUAGE: plaintext
CODE:
# Note: These requirements are also specified in pyproject.toml
# This file is kept for development environment setup and compatibility
aiosqlite~=0.20
lxml~=5.3
litellm>=1.53.1
numpy>=1.26.0,<3
pillow~=10.4
playwright>=1.49.0
python-dotenv~=1.0
requests~=2.26
beautifulsoup4~=4.12
tf-playwright-stealth>=1.1.0
xxhash~=3.4
rank-bm25~=0.2
aiofiles>=24.1.0
colorama~=0.4
snowballstemmer~=2.2
pydantic>=2.10
pyOpenSSL>=24.3.0
psutil>=6.1.1
nltk>=3.9.1
rich>=13.9.4
cssselect>=1.2.0
faust-cchardet>=2.1.19

----------------------------------------

TITLE: Using RateLimiter in Crawl4AI
DESCRIPTION: Shows how to initialize and use a RateLimiter with custom settings. Demonstrates parameter configuration for controlling delays, retries, and status code handling for efficient crawling.

LANGUAGE: python
CODE:
from crawl4ai import RateLimiter

# Create a RateLimiter with custom settings
rate_limiter = RateLimiter(
    base_delay=(2.0, 4.0),  # Random delay between 2-4 seconds
    max_delay=30.0,         # Cap delay at 30 seconds
    max_retries=5,          # Retry up to 5 times on rate-limiting errors
    rate_limit_codes=[429, 503]  # Handle these HTTP status codes
)

# RateLimiter will handle delays and retries internally
# No additional setup is required for its operation

----------------------------------------

TITLE: Authenticating and Using the Crawl4AI Python SDK
DESCRIPTION: Example of how to authenticate and use the Crawl4AI Python SDK client. The code demonstrates creating a client, authenticating with an email, and making a crawl request which automatically includes the authentication token.

LANGUAGE: python
CODE:
from crawl4ai.docker_client import Crawl4aiDockerClient

async with Crawl4aiDockerClient() as client:
    # Authenticate first
    await client.authenticate("user@example.com")
    
    # Now all requests will include the token automatically
    result = await client.crawl(urls=["https://example.com"])

----------------------------------------

TITLE: Using Fit Markdown with PruningContentFilter in Python
DESCRIPTION: This example demonstrates how to use the PruningContentFilter with the AsyncWebCrawler to generate both raw and filtered markdown from a web page. The crawler is configured with a threshold of 0.6 to remove noisy content segments.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from crawl4ai.content_filter_strategy import PruningContentFilter

async def main():
    config = CrawlerRunConfig(
        markdown_generator=DefaultMarkdownGenerator(
            content_filter=PruningContentFilter(threshold=0.6),
            options={"ignore_links": True}
        )
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://news.example.com/tech", config=config)
        if result.success:
            print("Raw markdown:\n", result.markdown)
            
            # If a filter is used, we also have .fit_markdown:
            md_object = result.markdown  # or your equivalent
            print("Filtered markdown:\n", md_object.fit_markdown)
        else:
            print("Crawl failed:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Configuring Proxy Settings in Crawl4AI
DESCRIPTION: Demonstrates how to route crawl traffic through a proxy server using Crawl4AI's BrowserConfig. The example configures proxy settings with server address and authentication credentials, then performs a test crawl to whatismyip.com to verify the proxy is working.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def main():
    browser_cfg = BrowserConfig(
        proxy_config={
            "server": "http://proxy.example.com:8080",
            "username": "myuser",
            "password": "mypass",
        },
        headless=True
    )
    crawler_cfg = CrawlerRunConfig(
        verbose=True
    )

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(
            url="https://www.whatismyip.com/",
            config=crawler_cfg
        )
        if result.success:
            print("[OK] Page fetched via proxy.")
            print("Page HTML snippet:", result.html[:200])
        else:
            print("[ERROR]", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Initializing CrawlerMonitor in Crawl4AI
DESCRIPTION: Creates a CrawlerMonitor instance for real-time visibility into crawling operations. Configures the display mode and number of visible rows in the monitoring dashboard.

LANGUAGE: python
CODE:
from crawl4ai import CrawlerMonitor, DisplayMode
monitor = CrawlerMonitor(
    # Maximum rows in live display
    max_visible_rows=15,          

    # DETAILED or AGGREGATED view
    display_mode=DisplayMode.DETAILED  
)

----------------------------------------

TITLE: JavaScript-Based Wait Conditions in Crawl4AI
DESCRIPTION: Demonstrates how to use JavaScript functions with the wait_for parameter to define complex waiting conditions such as waiting for a specific number of elements.

LANGUAGE: javascript
CODE:
wait_condition = """() => {
    const items = document.querySelectorAll('.athing');
    return items.length > 50;  // Wait for at least 51 items
}"""

config = CrawlerRunConfig(wait_for=f"js:{wait_condition}")

----------------------------------------

TITLE: Crawling Raw HTML Content with Crawl4AI
DESCRIPTION: Illustrates how to use AsyncWebCrawler to process raw HTML content by prefixing the HTML string with raw:. This approach enables processing HTML strings directly without requiring them to be stored in files or accessed via URLs.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import CrawlerRunConfig

async def crawl_raw_html():
    raw_html = "<html><body><h1>Hello, World!</h1></body></html>"
    raw_html_url = f"raw:{raw_html}"
    config = CrawlerRunConfig(bypass_cache=True)
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=raw_html_url, config=config)
        if result.success:
            print("Markdown Content from Raw HTML:")
            print(result.markdown)
        else:
            print(f"Failed to crawl raw HTML: {result.error_message}")

asyncio.run(crawl_raw_html())

----------------------------------------

TITLE: Crawling a Web URL with Crawl4AI
DESCRIPTION: Demonstrates how to use AsyncWebCrawler to crawl a live web page by providing a URL with http:// or https:// prefix. The example fetches content from a Wikipedia page and outputs its markdown representation.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import CrawlerRunConfig

async def crawl_web():
    config = CrawlerRunConfig(bypass_cache=True)
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://en.wikipedia.org/wiki/apple", 
            config=config
        )
        if result.success:
            print("Markdown Content:")
            print(result.markdown)
        else:
            print(f"Failed to crawl: {result.error_message}")

asyncio.run(crawl_web())

----------------------------------------

TITLE: Enabling Logging and Debugging in Crawl4AI
DESCRIPTION: Demonstrates how to enable verbose logging for debugging purposes using the BrowserConfig. This helps in troubleshooting issues that may occur during the crawling process.

LANGUAGE: python
CODE:
browser_config = BrowserConfig(verbose=True)

async with AsyncWebCrawler(config=browser_config) as crawler:
    run_config = CrawlerRunConfig()
    result = await crawler.arun(url="https://example.com", config=run_config)

----------------------------------------

TITLE: Basic Web Crawling with Crawl4AI
DESCRIPTION: Demonstrates a simple web crawling operation using AsyncWebCrawler to fetch content from a news website. The function prints the first 500 characters of the raw markdown content extracted from the page.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler

async def simple_crawl():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
            bypass_cache=True # By default this is False, meaning the cache will be used
        )
        print(result.markdown.raw_markdown[:500])  # Print the first 500 characters
        
asyncio.run(simple_crawl())

----------------------------------------

TITLE: Content Filtering Documentation Updates (Diff)
DESCRIPTION: Updates to the detailed documentation for the PruningContentFilter feature in the content filtering guide.

LANGUAGE: diff
CODE:
Added detailed section explaining the PruningContentFilter.

----------------------------------------

TITLE: Extracting Main Content with Fit Markdown Feature
DESCRIPTION: Demonstrates the Fit Markdown feature which extracts only the main content from web pages, removing sidebars, ads, and other distractions.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler

async def fit_markdown_demo():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://janineintheworld.com/places-to-visit-in-central-mexico")
        print(result.fit_markdown)  # Shows main content in Markdown format

# Run the demo
await fit_markdown_demo()

----------------------------------------

TITLE: Initializing and Running Performance Monitoring System in Crawl4AI
DESCRIPTION: Demonstrates how to initialize and start the CrawlMonitor system for tracking resource usage, active crawls, and performance metrics. The monitor can be configured to use either CLI or GUI interfaces, with customizable metrics and refresh rates.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler
from crawl4ai.monitor import CrawlMonitor

# Initialize monitoring
monitor = CrawlMonitor()

# Start monitoring with CLI interface
await monitor.start(
    mode="cli",  # or "gui"
    refresh_rate="1s",
    metrics={
        "resources": ["cpu", "memory", "network"],
        "crawls": ["active", "queued", "completed"],
        "performance": ["success_rate", "response_times"]
    }
)

# Example CLI output:
"""
Crawl4AI Monitor (Live) - Press Q to exit
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
System Usage:
 â”œâ”€ CPU: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ 70%
 â””â”€ Memory: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ 2.1GB/8GB

Active Crawls:
ID    URL                   Status    Progress
001   docs.example.com     ðŸŸ¢ Active   75%
002   api.service.com      ðŸŸ¡ Queue    -

Metrics (Last 5min):
 â”œâ”€ Success Rate: 98%
 â”œâ”€ Avg Response: 0.6s
 â””â”€ Pages/sec: 8.5
"""

----------------------------------------

TITLE: Exporting SSL certificate as JSON with optional file saving
DESCRIPTION: Demonstrates how to export an SSL certificate as a JSON string or save it directly to a file.

LANGUAGE: python
CODE:
json_data = cert.to_json()  # returns JSON string
cert.to_json("certificate.json")  # writes file, returns None

----------------------------------------

TITLE: Working with Extracted Structured Content
DESCRIPTION: Shows how to access and parse structured content that was extracted using an extraction strategy (like CSS selectors or an LLM-based extractor) during crawling.

LANGUAGE: python
CODE:
if result.extracted_content:
    data = json.loads(result.extracted_content)
    print(data)

----------------------------------------

TITLE: Configuring PruningContentFilter for Junk Removal
DESCRIPTION: Code example showing how to set up PruningContentFilter for removing boilerplate content without a specific query. The filter is configured with threshold values and minimum word requirements to identify and discard non-essential content.

LANGUAGE: python
CODE:
from crawl4ai.content_filter_strategy import PruningContentFilter

prune_filter = PruningContentFilter(
    threshold=0.5,
    threshold_type="fixed",  # or "dynamic"
    min_word_threshold=50
)

----------------------------------------

TITLE: Using Content Filters with DefaultMarkdownGenerator in Crawl4AI
DESCRIPTION: This snippet demonstrates how to apply a PruningContentFilter to the DefaultMarkdownGenerator for content filtering. It compares the length of raw Markdown versus filtered Markdown after crawling a webpage.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.content_filter_strategy import PruningContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

md_generator = DefaultMarkdownGenerator(
    content_filter=PruningContentFilter(threshold=0.4, threshold_type="fixed")
)

config = CrawlerRunConfig(
    cache_mode=CacheMode.BYPASS,
    markdown_generator=md_generator
)

async with AsyncWebCrawler() as crawler:
    result = await crawler.arun("https://news.ycombinator.com", config=config)
    print("Raw Markdown length:", len(result.markdown.raw_markdown))
    print("Fit Markdown length:", len(result.markdown.fit_markdown))

----------------------------------------

TITLE: Implementing Domain Specific Scrapers in Python
DESCRIPTION: Example of using specialized extraction strategies optimized for common website types and platforms. The code demonstrates using pre-configured extractors for academic sites and e-commerce platforms to provide consistent data extraction without additional configuration.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler
from crawl4ai.extractors import AcademicExtractor, EcommerceExtractor

async with AsyncWebCrawler() as crawler:
    # Academic paper extraction
    papers = await crawler.arun(
        url="https://arxiv.org/list/cs.AI/recent",
        extractor="academic",  # Built-in extractor type
        site_type="arxiv",     # Specific site optimization
        extract_fields=[
            "title", 
            "authors", 
            "abstract", 
            "citations"
        ]
    )
    
    # E-commerce product data
    products = await crawler.arun(
        url="https://store.example.com/products",
        extractor="ecommerce",
        extract_fields=[
            "name",
            "price",
            "availability",
            "reviews"
        ]
    )

----------------------------------------

TITLE: Dynamic Content Handling with JavaScript Execution
DESCRIPTION: Shows how to interact with dynamic content on web pages by executing JavaScript code. This example clicks a 'Load More' button to retrieve additional content from a news website before extracting the page content.

LANGUAGE: python
CODE:
async def crawl_dynamic_content():
    # You can use wait_for to wait for a condition to be met before returning the result
    # wait_for = "() => {
    #     return Array.from(document.querySelectorAll('article.tease-card')).length > 10;
    # }"

    # wait_for can be also just a css selector
    # wait_for = "article.tease-card:nth-child(10)"

    async with AsyncWebCrawler(verbose=True) as crawler:
        js_code = [
            "const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More')); loadMoreButton && loadMoreButton.click();"
        ]
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
            js_code=js_code,
            # wait_for=wait_for,
            bypass_cache=True,
        )
        print(result.markdown.raw_markdown[:500])  # Print first 500 characters

asyncio.run(crawl_dynamic_content())

----------------------------------------

TITLE: Iframe Processing Example with AsyncWebCrawler
DESCRIPTION: Demonstrates how to crawl a page with iframe content and merge it into the final output with process_iframes and remove_overlay_elements options.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async def main():
    config = CrawlerRunConfig(
        process_iframes=True,
        remove_overlay_elements=True
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://example.org/iframe-demo", 
            config=config
        )
        print("Iframe-merged length:", len(result.cleaned_html))

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Working with Screenshots and PDF Generation
DESCRIPTION: Demonstrates how to save screenshot and PDF data that was captured during crawling when those options were enabled in the crawler configuration.

LANGUAGE: python
CODE:
import base64
if result.screenshot:
    with open("page.png", "wb") as f:
        f.write(base64.b64decode(result.screenshot))

LANGUAGE: python
CODE:
if result.pdf:
    with open("page.pdf", "wb") as f:
        f.write(result.pdf)

----------------------------------------

TITLE: Using Crawl4AI Python SDK with Docker
DESCRIPTION: Python code example demonstrating how to use the Crawl4AI Python SDK to interact with the Docker container, showing both streaming and non-streaming API calls.

LANGUAGE: python
CODE:
from crawl4ai.docker_client import Crawl4aiDockerClient
from crawl4ai import BrowserConfig, CrawlerRunConfig

async def main():
    async with Crawl4aiDockerClient(base_url="http://localhost:8000", verbose=True) as client:
      # If JWT is enabled, you can authenticate like this: (more on this later)
        # await client.authenticate("test@example.com")
        
        # Non-streaming crawl
        results = await client.crawl(
            ["https://example.com", "https://python.org"],
            browser_config=BrowserConfig(headless=True),
            crawler_config=CrawlerRunConfig()
        )
        print(f"Non-streaming results: {results}")
        
        # Streaming crawl
        crawler_config = CrawlerRunConfig(stream=True)
        async for result in await client.crawl(
            ["https://example.com", "https://python.org"],
            browser_config=BrowserConfig(headless=True),
            crawler_config=crawler_config
        ):
            print(f"Streamed result: {result}")
        
        # Get schema
        schema = await client.get_schema()
        print(f"Schema: {schema}")

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Specifying Custom Download Path in Crawl4AI
DESCRIPTION: Shows how to set a custom download directory using the downloads_path attribute in the BrowserConfig object. This example creates a directory in the current working directory and configures the crawler to save downloads there.

LANGUAGE: python
CODE:
from crawl4ai.async_configs import BrowserConfig
import os

downloads_path = os.path.join(os.getcwd(), "my_downloads")  # Custom download path
os.makedirs(downloads_path, exist_ok=True)

config = BrowserConfig(accept_downloads=True, downloads_path=downloads_path)

async def main():
    async with AsyncWebCrawler(config=config) as crawler:
        result = await crawler.arun(url="https://example.com")
        # ...

----------------------------------------

TITLE: Basic Extraction Example Command
DESCRIPTION: Complete example showing basic extraction with browser and crawler configurations, outputting in JSON format.

LANGUAGE: bash
CODE:
crwl https://example.com \
    -B browser.yml \
    -C crawler.yml \
    -o json

----------------------------------------

TITLE: Handling Crawler Errors in Crawl4AI
DESCRIPTION: Shows how to properly handle errors that may occur during crawling. The example demonstrates checking the success status of the crawl result and retrieving error messages and status codes when failures occur.

LANGUAGE: python
CODE:
run_config = CrawlerRunConfig()
result = await crawler.arun(url="https://example.com", config=run_config)

if not result.success:
    print(f"Crawl failed: {result.error_message}")
    print(f"Status code: {result.status_code}")

----------------------------------------

TITLE: Enabling Downloads with BrowserConfig in Crawl4AI
DESCRIPTION: Demonstrates how to enable downloads by setting the accept_downloads parameter in the BrowserConfig object and passing it to the AsyncWebCrawler instance. This is the first step required to handle downloads in Crawl4AI.

LANGUAGE: python
CODE:
from crawl4ai.async_configs import BrowserConfig, AsyncWebCrawler

async def main():
    config = BrowserConfig(accept_downloads=True)  # Enable downloads globally
    async with AsyncWebCrawler(config=config) as crawler:
        # ... your crawling logic ...

asyncio.run(main())

----------------------------------------

TITLE: Passing storage_state as a File in Python
DESCRIPTION: Example showing how to use a JSON file path for storage_state with AsyncWebCrawler. This approach is useful for persistent storage between program runs.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    async with AsyncWebCrawler(
        headless=True,
        storage_state="mystate.json"  # Uses a JSON file instead of a dictionary
    ) as crawler:
        result = await crawler.arun(url='https://example.com/protected')
        if result.success:
            print("Crawl succeeded with pre-loaded session data!")
            print("Page HTML length:", len(result.html))

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Complex Nested Configuration JSON Example
DESCRIPTION: Advanced JSON configuration example showing deep nesting with content filtering settings for the markdown generator in Crawl4AI.

LANGUAGE: json
CODE:
{
    "crawler_config": {
        "type": "CrawlerRunConfig",
        "params": {
            "markdown_generator": {
                "type": "DefaultMarkdownGenerator",
                "params": {
                    "content_filter": {
                        "type": "PruningContentFilter",
                        "params": {
                            "threshold": 0.48,
                            "threshold_type": "fixed"
                        }
                    }
                }
            }
        }
    }
}

----------------------------------------

TITLE: Sequence Diagram for Event-Driven Crawling Process in Crawl4AI
DESCRIPTION: A mermaid sequence diagram illustrating the flow of information between Client, Server, and Crawler components in the proposed event-driven architecture. The diagram shows how a client initiates a crawl, receives event updates, provides instructions mid-crawl, and receives completion notification.

LANGUAGE: mermaid
CODE:
sequenceDiagram
    participant Client
    participant Server
    participant Crawler

    Client->>Server: Start crawl request
    Server->>Crawler: Initiate crawl with Process ID
    Crawler-->>Server: Event: Page hit
    Server-->>Client: Stream: Page hit event
    Client->>Server: Instruction for Process ID
    Server->>Crawler: Update crawl with new instructions
    Crawler-->>Server: Event: Crawl completed
    Server-->>Client: Stream: Crawl completed

----------------------------------------

TITLE: Accessing MarkdownGenerationResult Properties in Python
DESCRIPTION: This code snippet shows how to access the different markdown formats available in the MarkdownGenerationResult object, including raw markdown, markdown with citations, references, and filtered content.

LANGUAGE: python
CODE:
md_obj = result.markdown  # your library's naming may vary
print("RAW:\n", md_obj.raw_markdown)
print("CITED:\n", md_obj.markdown_with_citations)
print("REFERENCES:\n", md_obj.references_markdown)
print("FIT:\n", md_obj.fit_markdown)

----------------------------------------

TITLE: Accessing Downloaded Files in Crawl4AI
DESCRIPTION: Shows how to access and process downloaded files using the downloaded_files attribute of the CrawlResult object. This example prints the file paths and sizes of all downloaded files.

LANGUAGE: python
CODE:
if result.downloaded_files:
    print("Downloaded files:")
    for file_path in result.downloaded_files:
        print(f"- {file_path}")
        file_size = os.path.getsize(file_path)
        print(f"- File size: {file_size} bytes")
else:
    print("No files downloaded.")

----------------------------------------

TITLE: Manual Start and Close of AsyncWebCrawler in Python
DESCRIPTION: Demonstrates how to manually manage the lifecycle of the AsyncWebCrawler by explicitly calling start() and close() methods. This approach is useful for long-running applications or when more control is needed over the crawler's lifecycle.

LANGUAGE: python
CODE:
crawler = AsyncWebCrawler(config=browser_cfg)
await crawler.start()

result1 = await crawler.arun("https://example.com")
result2 = await crawler.arun("https://another.com")

await crawler.close()

----------------------------------------

TITLE: Configuring SEO Filter for Web Crawling with crawl4ai
DESCRIPTION: Sets up an SEO filter to evaluate pages based on specific keywords in metadata. The filter uses a threshold score to determine which pages to include in the crawl. The configuration is applied to a BFS crawling strategy.

LANGUAGE: python
CODE:
seo_filter = SEOFilter(
    threshold=0.5,  # Minimum score (0.0 to 1.0)
    keywords=["tutorial", "guide", "documentation"]
)

config = CrawlerRunConfig(
    deep_crawl_strategy=BFSDeepCrawlStrategy(
        max_depth=1,
        filter_chain=FilterChain([seo_filter])
    )
)

----------------------------------------

TITLE: Second Run: Reusing Saved State in Python
DESCRIPTION: Example showing how to reuse a previously saved storage state to access protected content without repeating the login flow. This approach enables faster and more efficient crawling of authenticated pages.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CacheMode
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

async def main():
    # Second run: no need to hook on_browser_created this time.
    # Just provide the previously saved storage state.
    async with AsyncWebCrawler(
        headless=True,
        verbose=True,
        use_persistent_context=True,
        user_data_dir="./my_user_data",
        storage_state="my_storage_state.json"  # Reuse previously exported state
    ) as crawler:
        
        # Now the crawler starts already logged in
        result = await crawler.arun(
            url='https://example.com/protected-page',
            cache_mode=CacheMode.BYPASS,
            markdown_generator=DefaultMarkdownGenerator(options={"ignore_links": True}),
        )
        print("Second run result success:", result.success)
        if result.success:
            print("Protected page HTML length:", len(result.html))

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Using Deprecated Boolean Cache Flags with AsyncWebCrawler in Crawl4AI
DESCRIPTION: Example of the deprecated approach using boolean bypass_cache flag with AsyncWebCrawler to disable cache for a web scraping operation. This code fetches content from NBC News business section with caching disabled.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler

async def use_proxy():
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
            bypass_cache=True  # Old way
        )
        print(len(result.markdown))

async def main():
    await use_proxy()

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: LLM-Based Extraction with OpenAI
DESCRIPTION: Demonstrates how to use an AI-powered extraction strategy with OpenAI's GPT-4 model to extract structured data according to a Pydantic schema with instructions.

LANGUAGE: python
CODE:
import asyncio
import json
from pydantic import BaseModel, Field
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy

class ArticleData(BaseModel):
    headline: str
    summary: str

async def main():
    llm_strategy = LLMExtractionStrategy(
        llm_config = LLMConfig(provider="openai/gpt-4",api_token="sk-YOUR_API_KEY")
        schema=ArticleData.schema(),
        extraction_type="schema",
        instruction="Extract 'headline' and a short 'summary' from the content."
    )

    config = CrawlerRunConfig(
        exclude_external_links=True,
        word_count_threshold=20,
        extraction_strategy=llm_strategy
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://news.ycombinator.com", config=config)
        article = json.loads(result.extracted_content)
        print(article)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Monitoring Extraction Performance in Python
DESCRIPTION: Example of configuring the CosineStrategy with verbose logging and performance-related parameters to monitor and optimize the extraction process for content similarity analysis.

LANGUAGE: python
CODE:
strategy = CosineStrategy(
    verbose=True,  # Enable logging
    word_count_threshold=20,  # Filter short content
    top_k=5  # Limit results
)

----------------------------------------

TITLE: Creating Modified Configuration with clone() Method in crawl4ai
DESCRIPTION: Example demonstrating how to create a modified copy of an existing configuration using the clone() method. This creates a new configuration with streaming enabled and cache bypassing, while inheriting other settings from the original.

LANGUAGE: python
CODE:
# Create a modified copy with the clone() method
stream_cfg = run_cfg.clone(
    stream=True,
    cache_mode=CacheMode.BYPASS
)

----------------------------------------

TITLE: Using New CacheMode Enum with AsyncWebCrawler in Crawl4AI
DESCRIPTION: Updated approach using the new CacheMode enum with CrawlerRunConfig to control caching behavior in Crawl4AI. This example shows how to bypass cache when crawling NBC News business section by setting CacheMode.BYPASS.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CacheMode
from crawl4ai.async_configs import CrawlerRunConfig

async def use_proxy():
    # Use CacheMode in CrawlerRunConfig
    config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)  
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
            config=config  # Pass the configuration object
        )
        print(len(result.markdown))

async def main():
    await use_proxy()

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Crawling a Local HTML File with Crawl4AI
DESCRIPTION: Shows how to use AsyncWebCrawler to process a local HTML file by prefixing the file path with file://. This approach allows processing of locally stored HTML documents using the same crawler interface.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import CrawlerRunConfig

async def crawl_local_file():
    local_file_path = "/path/to/apple.html"  # Replace with your file path
    file_url = f"file://{local_file_path}"
    config = CrawlerRunConfig(bypass_cache=True)
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=file_url, config=config)
        if result.success:
            print("Markdown Content from Local File:")
            print(result.markdown)
        else:
            print(f"Failed to crawl local file: {result.error_message}")

asyncio.run(crawl_local_file())

----------------------------------------

TITLE: Simulating Full-Page Scrolling for Dynamic Content
DESCRIPTION: Configuration to handle websites that load content dynamically as users scroll down the page, simulating scrolling behavior with customizable delay to capture all content.

LANGUAGE: python
CODE:
await crawler.crawl(
    url="https://example.com",
    scan_full_page=True,   # Enables scrolling
    scroll_delay=0.2       # Waits 200ms between scrolls (optional)
)

----------------------------------------

TITLE: Handling Downloaded Files from Crawl Results
DESCRIPTION: Example showing how to process file paths for content that was automatically downloaded during the crawling process when the accept_downloads option was enabled.

LANGUAGE: python
CODE:
if result.downloaded_files:
    for file_path in result.downloaded_files:
        print("Downloaded:", file_path)

----------------------------------------

TITLE: Pattern-Based Extraction with JsonCssExtractionStrategy
DESCRIPTION: Shows how to combine content filtering with CSS-based extraction strategy to extract structured data from a webpage using a defined schema.

LANGUAGE: python
CODE:
import asyncio
import json
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def main():
    # Minimal schema for repeated items
    schema = {
        "name": "News Items",
        "baseSelector": "tr.athing",
        "fields": [
            {"name": "title", "selector": "span.titleline a", "type": "text"},
            {
                "name": "link", 
                "selector": "span.titleline a", 
                "type": "attribute", 
                "attribute": "href"
            }
        ]
    }

    config = CrawlerRunConfig(
        # Content filtering
        excluded_tags=["form", "header"],
        exclude_domains=["adsite.com"],
        
        # CSS selection or entire page
        css_selector="table.itemlist",

        # No caching for demonstration
        cache_mode=CacheMode.BYPASS,

        # Extraction strategy
        extraction_strategy=JsonCssExtractionStrategy(schema)
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news.ycombinator.com/newest", 
            config=config
        )
        data = json.loads(result.extracted_content)
        print("Sample extracted item:", data[:1])  # Show first item

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Multi-Platform Docker Build with Buildx
DESCRIPTION: Commands to set up Docker Buildx and build Crawl4AI for multiple platforms (AMD64 and ARM64) and push to a registry.

LANGUAGE: bash
CODE:
# Set up buildx builder
docker buildx create --use

# Build for multiple platforms
docker buildx build \
  --platform linux/amd64,linux/arm64 \
  -t crawl4ai \
  --push \
  .

----------------------------------------

TITLE: BrowserConfig JSON Structure Example
DESCRIPTION: Example JSON output showing the type-params pattern for a simple BrowserConfig object with headless mode enabled.

LANGUAGE: json
CODE:
{
    "type": "BrowserConfig",
    "params": {
        "headless": true
    }
}

----------------------------------------

TITLE: Reusing Browser Sessions for Efficient Crawling
DESCRIPTION: Implementation of session reuse to avoid creating new browser contexts for each crawl operation, improving performance and reducing memory usage for large-scale crawling.

LANGUAGE: python
CODE:
session_id = await crawler.create_session()

# Use the same session for multiple crawls
await crawler.crawl(
    url="https://example.com/page1",
    session_id=session_id  # Reuse the session
)
await crawler.crawl(
    url="https://example.com/page2",
    session_id=session_id
)

----------------------------------------

TITLE: Cache Control Configuration with CacheMode
DESCRIPTION: Demonstrates how to configure caching behavior using the CacheMode enum, which controls how the crawler reads from and writes to the local cache.

LANGUAGE: python
CODE:
run_config = CrawlerRunConfig(
    cache_mode=CacheMode.BYPASS
)

----------------------------------------

TITLE: Domain Filtering for External and Social Media Links
DESCRIPTION: Example showing how to configure the crawler to exclude external links and social media links. This snippet demonstrates setting up CrawlerRunConfig to filter out unwanted domains during the crawling process.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def main():
    crawler_cfg = CrawlerRunConfig(
        exclude_external_links=True,          # No links outside primary domain
        exclude_social_media_links=True       # Skip recognized social media domains
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            "https://www.example.com",
            config=crawler_cfg
        )
        if result.success:
            print("[OK] Crawled:", result.url)
            print("Internal links count:", len(result.links.get("internal", [])))
            print("External links count:", len(result.links.get("external", [])))  
            # Likely zero external links in this scenario
        else:
            print("[ERROR]", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Downloading Required Models for Crawl4AI
DESCRIPTION: Command to download the required models after installing Crawl4AI with torch, transformer, or all options to boost performance and speed.

LANGUAGE: bash
CODE:
crawl4ai-download-models

----------------------------------------

TITLE: Upgrading to Crawl4AI v0.4.1
DESCRIPTION: Command to install or upgrade to the latest version of Crawl4AI using pip package manager.

LANGUAGE: bash
CODE:
pip install crawl4ai --upgrade

----------------------------------------

TITLE: Implementing Content Relevance Filter in crawl4ai
DESCRIPTION: Creates a content relevance filter that analyzes page content for semantic similarity to a specific query. This BM25-based filter evaluates head section content against the provided query using a configurable threshold.

LANGUAGE: python
CODE:
from crawl4ai.deep_crawling.filters import FilterChain, ContentRelevanceFilter

# Create a content relevance filter
relevance_filter = ContentRelevanceFilter(
    query="Web crawling and data extraction with Python",
    threshold=0.7  # Minimum similarity score (0.0 to 1.0)
)

config = CrawlerRunConfig(
    deep_crawl_strategy=BFSDeepCrawlStrategy(
        max_depth=1,
        filter_chain=FilterChain([relevance_filter])
    )
)

----------------------------------------

TITLE: Excluding External Images in Crawl4AI
DESCRIPTION: Example showing how to configure the crawler to exclude external images. This snippet demonstrates setting the exclude_external_images parameter in CrawlerRunConfig to only keep images from the primary domain.

LANGUAGE: python
CODE:
crawler_cfg = CrawlerRunConfig(
    exclude_external_images=True
)

----------------------------------------

TITLE: Cloning BrowserConfig in Python
DESCRIPTION: Example of using the clone() method to create modified copies of a BrowserConfig. The example shows creating a base configuration and then a debug configuration with modified parameters.

LANGUAGE: python
CODE:
# Create a base browser config
base_browser = BrowserConfig(
    browser_type="chromium",
    headless=True,
    text_mode=True
)

# Create a visible browser config for debugging
debug_browser = base_browser.clone(
    headless=False,
    verbose=True
)

----------------------------------------

TITLE: Enabling robots.txt Compliance in Crawl4AI
DESCRIPTION: Shows how to enable robots.txt compliance in crawl operations. This feature makes the crawler respect website crawling policies defined in robots.txt files by setting a simple configuration flag.

LANGUAGE: python
CODE:
config = CrawlerRunConfig(check_robots_txt=True)

----------------------------------------

TITLE: Handling Mixed Content Pages with CosineStrategy
DESCRIPTION: Example of configuring CosineStrategy for pages with mixed content types, using more flexible matching parameters and larger clusters to capture diverse but relevant sections.

LANGUAGE: python
CODE:
# For mixed content pages
strategy = CosineStrategy(
    semantic_filter="product features",
    sim_threshold=0.4,      # More flexible matching
    max_dist=0.3,          # Larger clusters
    top_k=3                # Multiple relevant sections
)

----------------------------------------

TITLE: Configuring Similarity Threshold in CosineStrategy
DESCRIPTION: Examples of how to configure similarity thresholds in CosineStrategy, demonstrating strict and loose matching configurations for different content extraction needs.

LANGUAGE: python
CODE:
# Strict matching
strategy = CosineStrategy(sim_threshold=0.8)

# Loose matching
strategy = CosineStrategy(sim_threshold=0.3)

----------------------------------------

TITLE: Capturing PDFs and Screenshots with Crawl4AI
DESCRIPTION: Shows how to create visual records of web pages by capturing both screenshots and PDF exports in one pass. The example saves the base64-encoded screenshot and PDF data to files after crawling a Wikipedia page. This approach is particularly useful for large or complex pages.

LANGUAGE: python
CODE:
import os, asyncio
from base64 import b64decode
from crawl4ai import AsyncWebCrawler, CacheMode

async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://en.wikipedia.org/wiki/List_of_common_misconceptions",
            cache_mode=CacheMode.BYPASS,
            pdf=True,
            screenshot=True
        )
        
        if result.success:
            # Save screenshot
            if result.screenshot:
                with open("wikipedia_screenshot.png", "wb") as f:
                    f.write(b64decode(result.screenshot))
            
            # Save PDF
            if result.pdf:
                with open("wikipedia_page.pdf", "wb") as f:
                    f.write(result.pdf)
            
            print("[OK] PDF & screenshot captured.")
        else:
            print("[ERROR]", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: PDF Processing with Crawl4AI
DESCRIPTION: Demonstrates how to extract text, images, and metadata from PDF files using the new PDF processing capabilities. The example shows how to use PDFCrawlerStrategy and PDFContentScrapingStrategy to process both local and remote PDF files.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.processors.pdf import PDFCrawlerStrategy, PDFContentScrapingStrategy
import asyncio

async def main():
    async with AsyncWebCrawler(crawler_strategy=PDFCrawlerStrategy()) as crawler:
        result = await crawler.arun(
            "https://arxiv.org/pdf/2310.06825.pdf",
            config=CrawlerRunConfig(
                scraping_strategy=PDFContentScrapingStrategy()
            )
        )
        print(result.markdown)  # Access extracted text
        print(result.metadata)  # Access PDF metadata (title, author, etc.)

asyncio.run(main())

----------------------------------------

TITLE: Accessing Page Metadata
DESCRIPTION: Example of accessing page-level metadata such as title, description, and Open Graph data that was discovered during crawling.

LANGUAGE: python
CODE:
if result.metadata:
    print("Title:", result.metadata.get("title"))
    print("Author:", result.metadata.get("author"))

----------------------------------------

TITLE: Filtering Content by Word Count in CosineStrategy
DESCRIPTION: Shows how to use the word_count_threshold parameter to filter out short content blocks and focus on substantial paragraphs in the extraction process.

LANGUAGE: python
CODE:
# Only consider substantial paragraphs
strategy = CosineStrategy(word_count_threshold=50)

----------------------------------------

TITLE: Triggering Downloads with JavaScript in Crawl4AI
DESCRIPTION: Demonstrates how to trigger downloads by simulating user interactions on a web page using JavaScript in the CrawlerRunConfig. This example clicks on a download link for .exe files and waits for the download to start.

LANGUAGE: python
CODE:
from crawl4ai.async_configs import CrawlerRunConfig

config = CrawlerRunConfig(
    js_code="""
        const downloadLink = document.querySelector('a[href$=".exe"]');
        if (downloadLink) {
            downloadLink.click();
        }
    """,
    wait_for=5  # Wait 5 seconds for the download to start
)

result = await crawler.arun(url="https://www.python.org/downloads/", config=config)

----------------------------------------

TITLE: LLM Q&A Commands with Crawl4AI CLI
DESCRIPTION: Demonstrates how to use the Q&A functionality to ask questions about crawled content, including simple questions, viewing content before asking, and combining with advanced crawling options.

LANGUAGE: bash
CODE:
# Simple question
crwl https://example.com -q "What is the main topic discussed?"

# View content then ask questions
crwl https://example.com -o markdown  # See content first
crwl https://example.com -q "Summarize the key points"
crwl https://example.com -q "What are the conclusions?"

# Combined with advanced crawling
crwl https://example.com \
    -B browser.yml \
    -c "css_selector=article,scan_full_page=true" \
    -q "What are the pros and cons mentioned?"

----------------------------------------

TITLE: Typical Initialization of AsyncWebCrawler with BrowserConfig
DESCRIPTION: Demonstrates the typical way to initialize the AsyncWebCrawler with a BrowserConfig object that specifies browser type, headless mode, and verbosity settings. This is the recommended way to configure the browser environment.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler, BrowserConfig

browser_cfg = BrowserConfig(
    browser_type="chromium",
    headless=True,
    verbose=True
)

crawler = AsyncWebCrawler(config=browser_cfg)

----------------------------------------

TITLE: Configuring Top Content Clusters in CosineStrategy
DESCRIPTION: Demonstrates how to set the number of top content clusters to return using the top_k parameter, which controls the diversity of extracted content.

LANGUAGE: python
CODE:
# Get top 5 most relevant content clusters
strategy = CosineStrategy(top_k=5)

----------------------------------------

TITLE: Advanced Session Crawling with Custom Execution Hooks in Python
DESCRIPTION: Demonstrates using custom hooks to handle complex scenarios, such as waiting for content to load dynamically. This technique ensures new content loads before the next action is performed.

LANGUAGE: python
CODE:
async def advanced_session_crawl_with_hooks():
    first_commit = ""

    async def on_execution_started(page):
        nonlocal first_commit
        try:
            while True:
                await page.wait_for_selector("li.commit-item h4")
                commit = await page.query_selector("li.commit-item h4")
                commit = await commit.evaluate("(element) => element.textContent").strip()
                if commit and commit != first_commit:
                    first_commit = commit
                    break
                await asyncio.sleep(0.5)
        except Exception as e:
            print(f"Warning: New content didn't appear: {e}")

    async with AsyncWebCrawler() as crawler:
        session_id = "commit_session"
        url = "https://github.com/example/repo/commits/main"
        crawler.crawler_strategy.set_hook("on_execution_started", on_execution_started)

        js_next_page = """document.querySelector('a.pagination-next').click();"""

        for page in range(3):
            config = CrawlerRunConfig(
                url=url,
                session_id=session_id,
                js_code=js_next_page if page > 0 else None,
                css_selector="li.commit-item",
                js_only=page > 0,
                cache_mode=CacheMode.BYPASS
            )

            result = await crawler.arun(config=config)
            print(f"Page {page + 1}: Found {len(result.extracted_content)} commits")

        await crawler.crawler_strategy.kill_session(session_id)

asyncio.run(advanced_session_crawl_with_hooks())

----------------------------------------

TITLE: CSS-based Structured Data Extraction Command
DESCRIPTION: Shows how to extract structured data using CSS selectors by specifying the extraction configuration file, schema file, and output format.

LANGUAGE: bash
CODE:
crwl https://example.com \
    -e extract_css.yml \
    -s css_schema.json \
    -o json

----------------------------------------

TITLE: Configuring Memory-Adaptive Dispatcher for Efficient Crawling in Python
DESCRIPTION: This code shows how to implement Crawl4AI's MemoryAdaptiveDispatcher to dynamically adjust concurrency based on available system memory. It demonstrates both batch and streaming modes for handling multiple URLs.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, MemoryAdaptiveDispatcher
import asyncio

# Configure the dispatcher (optional, defaults are used if not provided)
dispatcher = MemoryAdaptiveDispatcher(
    memory_threshold_percent=80.0,  # Pause if memory usage exceeds 80%
    check_interval=0.5,  # Check memory every 0.5 seconds
)

async def batch_mode():
    async with AsyncWebCrawler() as crawler:
        results = await crawler.arun_many(
            urls=["https://docs.crawl4ai.com", "https://github.com/unclecode/crawl4ai"],
            config=CrawlerRunConfig(stream=False),  # Batch mode
            dispatcher=dispatcher,
        )
        for result in results:
            print(f"Crawled: {result.url} with status code: {result.status_code}")

async def stream_mode():
    async with AsyncWebCrawler() as crawler:
        # OR, for streaming:
        async for result in await crawler.arun_many(
            urls=["https://docs.crawl4ai.com", "https://github.com/unclecode/crawl4ai"],
            config=CrawlerRunConfig(stream=True),
            dispatcher=dispatcher,
        ):
            print(f"Crawled: {result.url} with status code: {result.status_code}")

print("Dispatcher in batch mode:")
asyncio.run(batch_mode())
print("-" * 50)
print("Dispatcher in stream mode:")
asyncio.run(stream_mode())

----------------------------------------

TITLE: Implementing Topic-Based Segmentation with TextTiling
DESCRIPTION: A Python class that uses NLTK's TextTilingTokenizer to create topic-coherent chunks, identifying natural boundaries between different subjects within a text.

LANGUAGE: python
CODE:
from nltk.tokenize import TextTilingTokenizer

class TopicSegmentationChunking:
    def __init__(self):
        self.tokenizer = TextTilingTokenizer()

    def chunk(self, text):
        return self.tokenizer.tokenize(text)

# Example Usage
text = """This is an introduction.
This is a detailed discussion on the topic."""
chunker = TopicSegmentationChunking()
print(chunker.chunk(text))

----------------------------------------

TITLE: Configuring BrowserConfig with User Data Directory
DESCRIPTION: Python code snippet showing how to configure BrowserConfig with a user data directory. This configuration tells Crawl4AI to use a persistent browser profile for authentic identity-based crawling.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

browser_config = BrowserConfig(
    headless=True,
    use_managed_browser=True,
    user_data_dir="/home/<you>/my_chrome_profile",
    browser_type="chromium"
)

----------------------------------------

TITLE: LLM-based Structured Data Extraction Command
DESCRIPTION: Shows how to extract structured data using LLM-based extraction by specifying the extraction configuration file, schema file, and output format.

LANGUAGE: bash
CODE:
crwl https://example.com \
    -e extract_llm.yml \
    -s llm_schema.json \
    -o json

----------------------------------------

TITLE: Integrated JavaScript Execution and Waiting for Dynamic Content in Python
DESCRIPTION: Combines JavaScript execution and waiting logic for concise handling of dynamic content. This approach embeds async logic directly in the JavaScript to detect when new content has loaded after pagination.

LANGUAGE: python
CODE:
async def integrated_js_and_wait_crawl():
    async with AsyncWebCrawler() as crawler:
        session_id = "integrated_session"
        url = "https://github.com/example/repo/commits/main"

        js_next_page_and_wait = """
        (async () => {
            const getCurrentCommit = () => document.querySelector('li.commit-item h4').textContent.trim();
            const initialCommit = getCurrentCommit();
            document.querySelector('a.pagination-next').click();
            while (getCurrentCommit() === initialCommit) {
                await new Promise(resolve => setTimeout(resolve, 100));
            }
        })();
        """

        for page in range(3):
            config = CrawlerRunConfig(
                url=url,
                session_id=session_id,
                js_code=js_next_page_and_wait if page > 0 else None,
                css_selector="li.commit-item",
                js_only=page > 0,
                cache_mode=CacheMode.BYPASS
            )

            result = await crawler.arun(config=config)
            print(f"Page {page + 1}: Found {len(result.extracted_content)} commits")

        await crawler.crawler_strategy.kill_session(session_id)

asyncio.run(integrated_js_and_wait_crawl())

----------------------------------------

TITLE: Implementing Fixed-Length Word Chunking in Python
DESCRIPTION: A Python class that segments text into chunks with a fixed word count, allowing for consistent chunk sizes regardless of content structure.

LANGUAGE: python
CODE:
class FixedLengthWordChunking:
    def __init__(self, chunk_size=100):
        self.chunk_size = chunk_size

    def chunk(self, text):
        words = text.split()
        return [' '.join(words[i:i + self.chunk_size]) for i in range(0, len(words), self.chunk_size)]

# Example Usage
text = "This is a long text with many words to be chunked into fixed sizes."
chunker = FixedLengthWordChunking(chunk_size=5)
print(chunker.chunk(text))

----------------------------------------

TITLE: Optimizing Chunking for Long Documents in Python
DESCRIPTION: Example of optimizing LLMExtractionStrategy for processing long documents by configuring smaller chunk sizes and appropriate overlap settings to improve extraction quality.

LANGUAGE: python
CODE:
# For long documents
strategy = LLMExtractionStrategy(
    chunk_token_threshold=2000,  # Smaller chunks
    overlap_rate=0.1           # 10% overlap
)

----------------------------------------

TITLE: Interactive Q&A Example Commands
DESCRIPTION: Complete example showing an interactive Q&A workflow where content is first crawled and viewed, then questions are asked about the content.

LANGUAGE: bash
CODE:
# First crawl and view
crwl https://example.com -o markdown

# Then ask questions
crwl https://example.com -q "What are the main points?"
crwl https://example.com -q "Summarize the conclusions"

----------------------------------------

TITLE: Implementing BM25ContentFilter in Crawl4AI
DESCRIPTION: This snippet shows how to use the BM25ContentFilter, which ranks content based on relevance to a specific query. It sets up a filter with a user query and threshold parameters, then integrates it with the crawler configuration.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.content_filter_strategy import BM25ContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

async def main():
    # 1) A BM25 filter with a user query
    bm25_filter = BM25ContentFilter(
        user_query="startup fundraising tips",
        # Adjust for stricter or looser results
        bm25_threshold=1.2  
    )

    # 2) Insert into a Markdown Generator
    md_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)
    
    # 3) Pass to crawler config
    config = CrawlerRunConfig(
        markdown_generator=md_generator
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news.ycombinator.com", 
            config=config
        )
        if result.success:
            print("Fit Markdown (BM25 query-based):")
            print(result.markdown.fit_markdown)
        else:
            print("Error:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Combining Chunking with Cosine Similarity for Content Relevance
DESCRIPTION: A Python implementation that combines text chunking with cosine similarity analysis to extract relevant text segments based on a user query. It uses scikit-learn's TF-IDF vectorizer to compute similarity scores.

LANGUAGE: python
CODE:
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class CosineSimilarityExtractor:
    def __init__(self, query):
        self.query = query
        self.vectorizer = TfidfVectorizer()

    def find_relevant_chunks(self, chunks):
        vectors = self.vectorizer.fit_transform([self.query] + chunks)
        similarities = cosine_similarity(vectors[0:1], vectors[1:]).flatten()
        return [(chunks[i], similarities[i]) for i in range(len(chunks))]

# Example Workflow
text = """This is a sample document. It has multiple sentences. 
We are testing chunking and similarity."""

chunker = SlidingWindowChunking(window_size=5, step=3)
chunks = chunker.chunk(text)
query = "testing chunking"
extractor = CosineSimilarityExtractor(query)
relevant_chunks = extractor.find_relevant_chunks(chunks)

print(relevant_chunks)

----------------------------------------

TITLE: Performance Optimization for CosineStrategy
DESCRIPTION: Shows how to optimize CosineStrategy performance by adjusting thresholds, limiting results, and enabling verbose mode for monitoring extraction performance.

LANGUAGE: python
CODE:
strategy = CosineStrategy(
    word_count_threshold=10,  # Filter early
    top_k=5,                 # Limit results
    verbose=True             # Monitor performance
)

----------------------------------------

TITLE: Downloading Multiple Files with Crawl4AI
DESCRIPTION: A complete example that demonstrates downloading multiple files from a website by clicking on all download links. It includes configuring the browser, running JavaScript to trigger downloads, and processing the results.

LANGUAGE: python
CODE:
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig
import os
from pathlib import Path

async def download_multiple_files(url: str, download_path: str):
    config = BrowserConfig(accept_downloads=True, downloads_path=download_path)
    async with AsyncWebCrawler(config=config) as crawler:
        run_config = CrawlerRunConfig(
            js_code="""
                const downloadLinks = document.querySelectorAll('a[download]');
                for (const link of downloadLinks) {
                    link.click();
                    // Delay between clicks
                    await new Promise(r => setTimeout(r, 2000));  
                }
            """,
            wait_for=10  # Wait for all downloads to start
        )
        result = await crawler.arun(url=url, config=run_config)

        if result.downloaded_files:
            print("Downloaded files:")
            for file in result.downloaded_files:
                print(f"- {file}")
        else:
            print("No files downloaded.")

# Usage
download_path = os.path.join(Path.home(), ".crawl4ai", "downloads")
os.makedirs(download_path, exist_ok=True)

asyncio.run(download_multiple_files("https://www.python.org/downloads/windows/", download_path))

----------------------------------------

TITLE: Custom Clustering Configuration in CosineStrategy
DESCRIPTION: Demonstrates advanced configuration of clustering parameters in CosineStrategy, including alternative linkage methods and multilingual model support.

LANGUAGE: python
CODE:
strategy = CosineStrategy(
    linkage_method='complete',  # Alternative clustering method
    max_dist=0.4,              # Larger clusters
    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support
)

----------------------------------------

TITLE: Using the Crawl4AI command-line interface
DESCRIPTION: Examples of using the command-line interface (CLI) for Crawl4AI, showing different options including basic crawling with markdown output, deep crawling with BFS strategy and page limits, and LLM extraction with specific queries.

LANGUAGE: bash
CODE:
# Basic crawl with markdown output
crwl https://www.nbcnews.com/business -o markdown

# Deep crawl with BFS strategy, max 10 pages
crwl https://docs.crawl4ai.com --deep-crawl bfs --max-pages 10

# Use LLM extraction with a specific question
crwl https://www.example.com/products -q "Extract all product prices"

----------------------------------------

TITLE: Combining Content Filters with Additional Crawler Configuration Options
DESCRIPTION: Example demonstrating how to combine content filtering with other crawling options like word count threshold and tag exclusions. This shows multi-level filtering capabilities in Crawl4AI.

LANGUAGE: python
CODE:
config = CrawlerRunConfig(
    word_count_threshold=10,
    excluded_tags=["nav", "footer", "header"],
    exclude_external_links=True,
    markdown_generator=DefaultMarkdownGenerator(
        content_filter=PruningContentFilter(threshold=0.5)
    )
)

----------------------------------------

TITLE: Accessing Dispatch Results in AsyncWebCrawler
DESCRIPTION: Shows how to access and use the dispatch information provided in crawler results. The DispatchResult dataclass contains metadata about each crawl task including memory usage, timing information, and error details.

LANGUAGE: python
CODE:
@dataclass
class DispatchResult:
    task_id: str
    memory_usage: float
    peak_memory: float
    start_time: datetime
    end_time: datetime
    error_message: str = ""

LANGUAGE: python
CODE:
for result in results:
    if result.success:
        dr = result.dispatch_result
        print(f"URL: {result.url}")
        print(f"Memory: {dr.memory_usage:.1f}MB")
        print(f"Duration: {dr.end_time - dr.start_time}")

----------------------------------------

TITLE: Accessing Fit Markdown Output in Crawl4AI
DESCRIPTION: Simple code example showing how to access the filtered markdown and HTML output after a successful crawl. The filtered content is available in the result object's markdown attribute.

LANGUAGE: python
CODE:
fit_md = result.markdown.fit_markdown
fit_html = result.markdown.fit_html

----------------------------------------

TITLE: Streaming Results from Crawl4AI API in Python
DESCRIPTION: Asynchronous Python function that demonstrates how to stream crawl results from the Crawl4AI API, processing them as they arrive in NDJSON format.

LANGUAGE: python
CODE:
async def test_stream_crawl(session, token: str):
    """Test the /crawl/stream endpoint with multiple URLs."""
    url = "http://localhost:8000/crawl/stream"
    payload = {
        "urls": [
            "https://example.com",
            "https://example.com/page1",  
            "https://example.com/page2",  
            "https://example.com/page3",  
        ],
        "browser_config": {"headless": True, "viewport": {"width": 1200}},
        "crawler_config": {"stream": True, "cache_mode": "aggressive"}
    }

    # headers = {"Authorization": f"Bearer {token}"} # If JWT is enabled, more on this later
    
    try:
        async with session.post(url, json=payload, headers=headers) as response:
            status = response.status
            print(f"Status: {status} (Expected: 200)")
            assert status == 200, f"Expected 200, got {status}"
            
            # Read streaming response line-by-line (NDJSON)
            async for line in response.content:
                if line:
                    data = json.loads(line.decode('utf-8').strip())
                    print(f"Streamed Result: {json.dumps(data, indent=2)}")
    except Exception as e:
        print(f"Error in streaming crawl test: {str(e)}")

----------------------------------------

TITLE: Installing Crawl4AI with Transformer Features
DESCRIPTION: Commands to install Crawl4AI with Hugging Face transformer-based summarization or generation strategies, followed by setup.

LANGUAGE: bash
CODE:
pip install crawl4ai[transformer]
crawl4ai-setup

----------------------------------------

TITLE: Setting Up Authenticated Proxy in Crawl4AI
DESCRIPTION: This example demonstrates how to configure a crawler with an authenticated proxy by providing a proxy_config dictionary containing server, username, and password credentials.

LANGUAGE: python
CODE:
from crawl4ai.async_configs import BrowserConfig

proxy_config = {
    "server": "http://proxy.example.com:8080",
    "username": "user",
    "password": "pass"
}

browser_config = BrowserConfig(proxy_config=proxy_config)
async with AsyncWebCrawler(config=browser_config) as crawler:
    result = await crawler.arun(url="https://example.com")

----------------------------------------

TITLE: Enabling Text-Only Mode for Faster Crawling
DESCRIPTION: Configuration for a lightweight crawling mode that disables images, JavaScript, and reduces viewport size, making the crawling process 3-4 times faster for text-focused operations.

LANGUAGE: python
CODE:
crawler = AsyncPlaywrightCrawlerStrategy(
    text_mode=True  # Set this to True to enable text-only crawling
)

----------------------------------------

TITLE: Content Cleaning and Markdown Formatting
DESCRIPTION: Demonstrates how to clean and format web content by excluding specific HTML tags, removing overlay elements, and setting a word count threshold. The function compares the length of the full markdown versus the fit markdown output.

LANGUAGE: python
CODE:
async def clean_content():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://janineintheworld.com/places-to-visit-in-central-mexico",
            excluded_tags=['nav', 'footer', 'aside'],
            remove_overlay_elements=True,
            word_count_threshold=10,
            bypass_cache=True
        )
        full_markdown_length = len(result.markdown.raw_markdown)
        fit_markdown_length = len(result.markdown.fit_markdown)
        print(f"Full Markdown Length: {full_markdown_length}")
        print(f"Fit Markdown Length: {fit_markdown_length}")
        print(result.markdown.fit_markdown[:1000])
        

asyncio.run(clean_content())

----------------------------------------

TITLE: Creating a Custom Content Filter for Crawl4AI
DESCRIPTION: Sample code showing how to create a custom content filter by inheriting from the RelevantContentFilter base class. This allows implementing site-specific or specialized filtering logic beyond the built-in options.

LANGUAGE: python
CODE:
from crawl4ai.content_filter_strategy import RelevantContentFilter

class MyCustomFilter(RelevantContentFilter):
    def filter_content(self, html, min_word_threshold=None):
        # parse HTML, implement custom logic
        return [block for block in ... if ... some condition...]

----------------------------------------

TITLE: Basic Session Management with CrawlerRunConfig in Python
DESCRIPTION: Demonstrates how to use BrowserConfig and CrawlerRunConfig to maintain state across multiple requests using a session_id. This allows for sequential crawling of different pages while preserving browser state.

LANGUAGE: python
CODE:
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig

async with AsyncWebCrawler() as crawler:
    session_id = "my_session"

    # Define configurations
    config1 = CrawlerRunConfig(
        url="https://example.com/page1", session_id=session_id
    )
    config2 = CrawlerRunConfig(
        url="https://example.com/page2", session_id=session_id
    )

    # First request
    result1 = await crawler.arun(config=config1)

    # Subsequent request using the same session
    result2 = await crawler.arun(config=config2)

    # Clean up when done
    await crawler.crawler_strategy.kill_session(session_id)

----------------------------------------

TITLE: Accessing SSL Certificate Information
DESCRIPTION: Example of how to access SSL certificate data from a CrawlResult when the fetch_ssl_certificate option was enabled during crawling.

LANGUAGE: python
CODE:
if result.ssl_certificate:
    print("Issuer:", result.ssl_certificate.issuer)

----------------------------------------

TITLE: Link Analysis and Filtering
DESCRIPTION: Extracts and filters links from a webpage, distinguishing between internal and external links. The function demonstrates how to exclude external links and social media links while displaying information about the first few internal links found.

LANGUAGE: python
CODE:
async def link_analysis():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
            bypass_cache=True,
            exclude_external_links=True,
            exclude_social_media_links=True,
            # exclude_domains=["facebook.com", "twitter.com"]
        )
        print(f"Found {len(result.links['internal'])} internal links")
        print(f"Found {len(result.links['external'])} external links")

        for link in result.links['internal'][:5]:
            print(f"Href: {link['href']}\nText: {link['text']}\n")
                

asyncio.run(link_analysis())

----------------------------------------

TITLE: Link Analysis and Filtering
DESCRIPTION: Extracts and filters links from a webpage, distinguishing between internal and external links. The function demonstrates how to exclude external links and social media links while displaying information about the first few internal links found.

LANGUAGE: python
CODE:
async def link_analysis():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
            bypass_cache=True,
            exclude_external_links=True,
            exclude_social_media_links=True,
            # exclude_domains=["facebook.com", "twitter.com"]
        )
        print(f"Found {len(result.links['internal'])} internal links")
        print(f"Found {len(result.links['external'])} external links")

        for link in result.links['internal'][:5]:
            print(f"Href: {link['href']}\nText: {link['text']}\n")
                

asyncio.run(link_analysis())

----------------------------------------

TITLE: Passing storage_state as a Dictionary in Python
DESCRIPTION: Python example demonstrating how to provide storage_state as a dictionary directly to AsyncWebCrawler. This approach allows dynamic session data management without requiring a file.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    storage_dict = {
        "cookies": [
            {
                "name": "session",
                "value": "abcd1234",
                "domain": "example.com",
                "path": "/",
                "expires": 1675363572.037711,
                "httpOnly": False,
                "secure": False,
                "sameSite": "None"
            }
        ],
        "origins": [
            {
                "origin": "https://example.com",
                "localStorage": [
                    {"name": "token", "value": "my_auth_token"},
                    {"name": "refreshToken", "value": "my_refresh_token"}
                ]
            }
        ]
    }

    async with AsyncWebCrawler(
        headless=True,
        storage_state=storage_dict
    ) as crawler:
        result = await crawler.arun(url='https://example.com/protected')
        if result.success:
            print("Crawl succeeded with pre-loaded session data!")
            print("Page HTML length:", len(result.html))

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Basic Session-Based Crawling Example in Python
DESCRIPTION: A simple complete example of session-based crawling that demonstrates reusing a session ID across multiple requests, executing JavaScript to load more content dynamically, and properly closing the session.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig
from crawl4ai.cache_context import CacheMode

async def basic_session_crawl():
    async with AsyncWebCrawler() as crawler:
        session_id = "dynamic_content_session"
        url = "https://example.com/dynamic-content"

        for page in range(3):
            config = CrawlerRunConfig(
                url=url,
                session_id=session_id,
                js_code="document.querySelector('.load-more-button').click();" if page > 0 else None,
                css_selector=".content-item",
                cache_mode=CacheMode.BYPASS
            )
            
            result = await crawler.arun(config=config)
            print(f"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items")

        await crawler.crawler_strategy.kill_session(session_id)

asyncio.run(basic_session_crawl())

----------------------------------------

TITLE: Product Review Analysis with CosineStrategy
DESCRIPTION: Shows how to configure CosineStrategy for extracting multiple customer reviews from product pages, with parameters adjusted for shorter text segments.

LANGUAGE: python
CODE:
strategy = CosineStrategy(
    semantic_filter="customer reviews and ratings",
    word_count_threshold=20,   # Reviews can be shorter
    top_k=10,                 # Get multiple reviews
    sim_threshold=0.4         # Allow variety in review content
)

----------------------------------------

TITLE: Using HTTP-only Crawler Strategy for Lightweight Web Scraping in Python
DESCRIPTION: This code demonstrates how to configure and use Crawl4AI's lightweight AsyncHTTPCrawlerStrategy. This strategy is ideal for simple scraping tasks where JavaScript rendering isn't required, providing faster and more memory-efficient crawling.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, HTTPCrawlerConfig
from crawl4ai.async_crawler_strategy import AsyncHTTPCrawlerStrategy
import asyncio

# Use the HTTP crawler strategy
http_crawler_config = HTTPCrawlerConfig(
        method="GET",
        headers={"User-Agent": "MyCustomBot/1.0"},
        follow_redirects=True,
        verify_ssl=True
)

async def main():
    async with AsyncWebCrawler(crawler_strategy=AsyncHTTPCrawlerStrategy(browser_config =http_crawler_config)) as crawler:
        result = await crawler.arun("https://example.com")
        print(f"Status code: {result.status_code}")
        print(f"Content length: {len(result.html)}")

asyncio.run(main())

----------------------------------------

TITLE: Basic BM25ContentFilter Implementation Pattern
DESCRIPTION: Simplified code pattern for implementing BM25ContentFilter with a specific user query. Shows the core setup required for query-based content filtering in Crawl4AI.

LANGUAGE: python
CODE:
bm25_filter = BM25ContentFilter(
    user_query="health benefits fruit",
    bm25_threshold=1.2
)
md_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)
config = CrawlerRunConfig(markdown_generator=md_generator)

----------------------------------------

TITLE: Using Storage State for Session Persistence in Crawl4AI
DESCRIPTION: Demonstrates how to preserve cookies and localStorage between crawling sessions using the storage_state parameter. The example shows how to provide a pre-configured storage state dictionary with cookies and localStorage values to maintain authenticated sessions without re-logging in.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    storage_dict = {
        "cookies": [
            {
                "name": "session",
                "value": "abcd1234",
                "domain": "example.com",
                "path": "/",
                "expires": 1699999999.0,
                "httpOnly": False,
                "secure": False,
                "sameSite": "None"
            }
        ],
        "origins": [
            {
                "origin": "https://example.com",
                "localStorage": [
                    {"name": "token", "value": "my_auth_token"}
                ]
            }
        ]
    }

    # Provide the storage state as a dictionary to start "already logged in"
    async with AsyncWebCrawler(
        headless=True,
        storage_state=storage_dict
    ) as crawler:
        result = await crawler.arun("https://example.com/protected")
        if result.success:
            print("Protected page content length:", len(result.html))
        else:
            print("Failed to crawl protected page")

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: High-Traffic Configuration Settings for Crawl4AI in YAML
DESCRIPTION: YAML configuration for Crawl4AI in high-traffic scenarios. Includes more conservative memory threshold limits and more aggressive rate limiting to prevent server overload and maintain stability.

LANGUAGE: yaml
CODE:
crawler:
  memory_threshold_percent: 85.0  # More conservative memory limit
  rate_limiter:
    base_delay: [2.0, 4.0]       # More aggressive rate limiting

----------------------------------------

TITLE: Docker Deployment Commands for Crawl4AI
DESCRIPTION: Bash commands for building and running Crawl4AI as a Docker container. The container provides a FastAPI server with endpoints for streaming and non-streaming crawls, health checks, schema information, and content extraction.

LANGUAGE: bash
CODE:
# Build the image (from the project root)
docker build -t crawl4ai .

# Run the container
docker run -d -p 8000:8000 --name crawl4ai crawl4ai

----------------------------------------

TITLE: Installing Firecrawl for Speed Comparison
DESCRIPTION: Installs the Firecrawl library to perform a speed comparison with Crawl4AI.

LANGUAGE: python
CODE:
!pip install firecrawl

----------------------------------------

TITLE: Basic arun() Usage with CrawlerRunConfig in Crawl4AI
DESCRIPTION: Demonstrates the fundamental approach to using the arun() method with the new CrawlerRunConfig parameter structure instead of direct arguments.

LANGUAGE: python
CODE:
await crawler.arun(
    url="https://example.com",  
    config=my_run_config
)

----------------------------------------

TITLE: Installing Playwright Dependencies on Ubuntu
DESCRIPTION: Command to install additional system dependencies required by Playwright on Ubuntu to resolve installation issues.

LANGUAGE: bash
CODE:
sudo apt-get install -y \
    libwoff1 \
    libopus0 \
    libwebp7 \
    libwebpdemux2 \
    libenchant-2-2 \
    libgudev-1.0-0 \
    libsecret-1-0 \
    libhyphen0 \
    libgdk-pixbuf2.0-0 \
    libegl1 \
    libnotify4 \
    libxslt1.1 \
    libevent-2.1-7 \
    libgles2 \
    libxcomposite1 \
    libatk1.0-0 \
    libatk-bridge2.0-0 \
    libepoxy0 \
    libgtk-3-0 \
    libharfbuzz-icu0 \
    libgstreamer-gl1.0-0 \
    libgstreamer-plugins-bad1.0-0 \
    gstreamer1.0-plugins-good \
    gstreamer1.0-plugins-bad \
    libxt6 \
    libxaw7 \
    xvfb \
    fonts-noto-color-emoji \
    libfontconfig \
    libfreetype6 \
    xfonts-cyrillic \
    xfonts-scalable \
    fonts-liberation \
    fonts-ipafont-gothic \
    fonts-wqy-zenhei \
    fonts-tlwg-loma-otf \
    fonts-freefont-ttf

----------------------------------------

TITLE: Implementing Proxy Rotation with RoundRobinProxyStrategy in Crawl4AI
DESCRIPTION: Demonstrates how to set up proxy rotation using the new RoundRobinProxyStrategy class. The code loads proxies from environment variables, creates a rotation strategy, and tests each proxy by making multiple requests to httpbin.org/ip to verify IP addresses match the expected proxy IPs.

LANGUAGE: python
CODE:
import re
from crawl4ai import (
    AsyncWebCrawler,
    BrowserConfig,
    CrawlerRunConfig,
    CacheMode,
    RoundRobinProxyStrategy,
)
import asyncio
from crawl4ai.configs import ProxyConfig
async def main():
    # Load proxies and create rotation strategy
    proxies = ProxyConfig.from_env()
    #eg: export PROXIES="ip1:port1:username1:password1,ip2:port2:username2:password2"
    if not proxies:
        print("No proxies found in environment. Set PROXIES env variable!")
        return
        
    proxy_strategy = RoundRobinProxyStrategy(proxies)
    
    # Create configs
    browser_config = BrowserConfig(headless=True, verbose=False)
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        proxy_rotation_strategy=proxy_strategy
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        urls = ["https://httpbin.org/ip"] * (len(proxies) * 2)  # Test each proxy twice

        print("\nðŸ“ˆ Initializing crawler with proxy rotation...")
        async with AsyncWebCrawler(config=browser_config) as crawler:
            print("\nðŸš€ Starting batch crawl with proxy rotation...")
            results = await crawler.arun_many(
                urls=urls,
                config=run_config
            )
            for result in results:
                if result.success:
                    ip_match = re.search(r'(?:[0-9]{1,3}\.){3}[0-9]{1,3}', result.html)
                    current_proxy = run_config.proxy_config if run_config.proxy_config else None
                    
                    if current_proxy and ip_match:
                        print(f"URL {result.url}")
                        print(f"Proxy {current_proxy.server} -> Response IP: {ip_match.group(0)}")
                        verified = ip_match.group(0) == current_proxy.ip
                        if verified:
                            print(f"âœ… Proxy working! IP matches: {current_proxy.ip}")
                        else:
                            print("âŒ Proxy failed or IP mismatch!")
                    print("---")

asyncio.run(main())

----------------------------------------

TITLE: Text Processing Configuration Options
DESCRIPTION: Shows how to configure text processing parameters including word count threshold, text-only mode, and attribute handling to control the content extraction process.

LANGUAGE: python
CODE:
run_config = CrawlerRunConfig(
    word_count_threshold=10,   # Ignore text blocks <10 words
    only_text=False,           # If True, tries to remove non-text elements
    keep_data_attributes=False # Keep or discard data-* attributes
)

----------------------------------------

TITLE: Session Management Configuration
DESCRIPTION: Demonstrates how to use session_id parameter to maintain session state across multiple crawl operations, enabling multi-step tasks or stateful browsing.

LANGUAGE: python
CODE:
run_config = CrawlerRunConfig(
    session_id="my_session123"
)

----------------------------------------

TITLE: LLM Extraction with Filtering Example Command
DESCRIPTION: Complete example showing LLM-based extraction with filtering, including browser configuration, extraction configuration, schema, and filter settings.

LANGUAGE: bash
CODE:
crwl https://example.com \
    -B browser.yml \
    -e extract_llm.yml \
    -s llm_schema.json \
    -f filter_bm25.yml \
    -o json

----------------------------------------

TITLE: Using LLMConfig for Configuration of LLM-Powered Features
DESCRIPTION: Shows how to use the new LLMConfig parameter for configuring LLM-based extraction, filtering, and schema generation tasks. The example demonstrates creating a reusable LLM configuration and using it with LLMExtractionStrategy in a crawler.

LANGUAGE: python
CODE:
from crawl4ai.types import LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

# Example of using LLMConfig with LLMExtractionStrategy
llm_config = LLMConfig(provider="openai/gpt-4o", api_token="YOUR_API_KEY")
strategy = LLMExtractionStrategy(llm_config=llm_config, schema=...)

# Example usage within a crawler
async with AsyncWebCrawler() as crawler:
    result = await crawler.arun(
        url="https://example.com",
        config=CrawlerRunConfig(extraction_strategy=strategy)
    )

----------------------------------------

TITLE: Migrating to CacheMode Enum in crawl4ai
DESCRIPTION: This code example demonstrates how to migrate from the deprecated cache control flags to the new CacheMode enum in AsyncWebCrawler. The new approach provides more fine-grained control over caching behavior during crawl operations.

LANGUAGE: python
CODE:
# Old way:
crawler = AsyncWebCrawler(always_by_pass_cache=True)
result = await crawler.arun(url="https://example.com", bypass_cache=True)

LANGUAGE: python
CODE:
# New way:
from crawl4ai import CacheMode

crawler = AsyncWebCrawler(always_bypass_cache=True)
result = await crawler.arun(url="https://example.com", cache_mode=CacheMode.BYPASS)

----------------------------------------

TITLE: Screenshot, PDF and Media Options Configuration
DESCRIPTION: Shows how to configure media capture options including screenshots and PDFs, along with parameters for handling image analysis and filtering.

LANGUAGE: python
CODE:
run_config = CrawlerRunConfig(
    screenshot=True,             # Grab a screenshot as base64
    screenshot_wait_for=1.0,     # Wait 1s before capturing
    pdf=True,                    # Also produce a PDF
    image_description_min_word_threshold=5,  # If analyzing alt text
    image_score_threshold=3,                # Filter out low-score images
)

----------------------------------------

TITLE: LLM-based Extraction Configuration in YAML
DESCRIPTION: Defines an LLM-based extraction configuration in YAML format, specifying the provider, instruction, API token, and generation parameters like temperature and token limit.

LANGUAGE: yaml
CODE:
# extract_llm.yml
type: "llm"
provider: "openai/gpt-4"
instruction: "Extract all articles with their titles and links"
api_token: "your-token"
params:
  temperature: 0.3
  max_tokens: 1000

----------------------------------------

TITLE: Launching Playwright Chromium with Custom User Data Directory on Linux
DESCRIPTION: Command to launch the Playwright Chromium binary on Linux with a custom user data directory. This allows you to create and configure a persistent browser profile that can be reused for crawling.

LANGUAGE: bash
CODE:
~/.cache/ms-playwright/chromium-1234/chrome-linux/chrome \
    --user-data-dir=/home/<you>/my_chrome_profile

----------------------------------------

TITLE: Implementing Robots.txt Compliance in AsyncWebCrawler
DESCRIPTION: Demonstrates how to enable robots.txt checking in the crawler to ensure ethical and legal web crawling. The configuration uses check_robots_txt=True to validate each URL against robots.txt rules before proceeding with the crawl.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

async def main():
    urls = [
        "https://example1.com",
        "https://example2.com",
        "https://example3.com"
    ]
    
    config = CrawlerRunConfig(
        cache_mode=CacheMode.ENABLED,
        check_robots_txt=True,  # Will respect robots.txt for each URL
        semaphore_count=3      # Max concurrent requests
    )
    
    async with AsyncWebCrawler() as crawler:
        async for result in crawler.arun_many(urls, config=config):
            if result.success:
                print(f"Successfully crawled {result.url}")
            elif result.status_code == 403 and "robots.txt" in result.error_message:
                print(f"Skipped {result.url} - blocked by robots.txt")
            else:
                print(f"Failed to crawl {result.url}: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Streaming Example with arun_many in Python
DESCRIPTION: An example demonstrating how to use arun_many in streaming mode, which processes results as they become available using an async for loop. This approach is ideal for processing large numbers of URLs efficiently.

LANGUAGE: python
CODE:
config = CrawlerRunConfig(
    stream=True,  # Enable streaming mode
    cache_mode=CacheMode.BYPASS
)

# Process results as they complete
async for result in await crawler.arun_many(
    urls=["https://site1.com", "https://site2.com", "https://site3.com"],
    config=config
):
    if result.success:
        print(f"Just completed: {result.url}")
        # Process each result immediately
        process_result(result)

----------------------------------------

TITLE: Simple Crawl Example with REST API in Python
DESCRIPTION: Python code example showing how to perform a simple crawl request using the REST API with requests library and handle the response.

LANGUAGE: python
CODE:
import requests

crawl_payload = {
    "urls": ["https://example.com"],
    "browser_config": {"headless": True},
    "crawler_config": {"stream": False}
}
response = requests.post(
    "http://localhost:8000/crawl",
    # headers={"Authorization": f"Bearer {token}"},  # If JWT is enabled, more on this later
    json=crawl_payload
)
print(response.json())  # Print the response for debugging

----------------------------------------

TITLE: Launching Playwright Chromium with Custom User Data Directory on macOS
DESCRIPTION: Command to launch the Playwright Chromium binary on macOS with a custom user data directory. This creates a persistent browser profile that maintains cookies, local storage, and session data.

LANGUAGE: bash
CODE:
~/Library/Caches/ms-playwright/chromium-1234/chrome-mac/Chromium.app/Contents/MacOS/Chromium \
    --user-data-dir=/Users/<you>/my_chrome_profile

----------------------------------------

TITLE: Advanced Crawler Configuration Example for Crawl4AI
DESCRIPTION: JSON configuration for Crawl4AI specifying crawler parameters including cache mode, markdown generation, and content filtering.

LANGUAGE: json
CODE:
{
    "urls": ["https://example.com"],
    "crawler_config": {
        "type": "CrawlerRunConfig",
        "params": {
            "cache_mode": "bypass",
            "markdown_generator": {
                "type": "DefaultMarkdownGenerator",
                "params": {
                    "content_filter": {
                        "type": "PruningContentFilter",
                        "params": {
                            "threshold": 0.48,
                            "threshold_type": "fixed",
                            "min_word_threshold": 0
                        }
                    }
                }
            }
        }
    }
}

----------------------------------------

TITLE: Accessing Basic Crawl Information
DESCRIPTION: Examples of how to access basic crawl information fields like URL, success status, status code, and error messages from a CrawlResult object.

LANGUAGE: python
CODE:
print(result.url)  # e.g., "https://example.com/"

LANGUAGE: python
CODE:
if not result.success:
    print(f"Crawl failed: {result.error_message}")

LANGUAGE: python
CODE:
if result.status_code == 404:
    print("Page not found!")

LANGUAGE: python
CODE:
if not result.success:
    print("Error:", result.error_message)

----------------------------------------

TITLE: Using Interactive Profile Management
DESCRIPTION: Example of using the interactive profile management console provided by BrowserProfiler. This allows guided profile creation, listing, and deletion, with an optional callback for crawling directly with a selected profile.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import BrowserProfiler, AsyncWebCrawler, BrowserConfig

# Define a function to use a profile for crawling
async def crawl_with_profile(profile_path, url):
    browser_config = BrowserConfig(
        headless=True,
        use_managed_browser=True,
        user_data_dir=profile_path
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url)
        return result

async def main():
    # Create a profiler instance
    profiler = BrowserProfiler()
    
    # Launch the interactive profile manager
    # Passing the crawl function as a callback adds a "crawl with profile" option
    await profiler.interactive_manager(crawl_callback=crawl_with_profile)
    
asyncio.run(main())

----------------------------------------

TITLE: Using Magic Mode for Simplified Automation
DESCRIPTION: Example of using Magic Mode in Crawl4AI for quick automation without persistent identity. Magic Mode simulates human-like browsing by randomizing user agents, navigator properties, and interaction timings.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async with AsyncWebCrawler() as crawler:
    result = await crawler.arun(
        url="https://example.com",
        config=CrawlerRunConfig(
            magic=True,  # Simplifies a lot of interaction
            remove_overlay_elements=True,
            page_timeout=60000
        )
    )

----------------------------------------

TITLE: Implementing Semaphore-based Crawling with AsyncWebCrawler in Python
DESCRIPTION: Shows how to configure a SemaphoreDispatcher with rate limiting for fixed concurrency control. This approach allows precise control over the number of concurrent requests and includes a rate limiter to prevent overloading target servers.

LANGUAGE: python
CODE:
async def crawl_with_semaphore(urls):
    browser_config = BrowserConfig(headless=True, verbose=False)
    run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)
    
    dispatcher = SemaphoreDispatcher(
        semaphore_count=5,
        rate_limiter=RateLimiter(
            base_delay=(0.5, 1.0),
            max_delay=10.0
        ),
        monitor=CrawlerMonitor(
            max_visible_rows=15,
            display_mode=DisplayMode.DETAILED
        )
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        results = await crawler.arun_many(
            urls, 
            config=run_config,
            dispatcher=dispatcher
        )
        return results

----------------------------------------

TITLE: Initializing SlidingWindowChunking in Python
DESCRIPTION: Configuration for the SlidingWindowChunking class that creates overlapping chunks with a sliding window approach, specifying window size and step size.

LANGUAGE: python
CODE:
SlidingWindowChunking(
    window_size: int = 100,    # Window size in words
    step: int = 50             # Step size between windows
)

----------------------------------------

TITLE: Authenticating API Requests with Bearer Token in Bash
DESCRIPTION: Example of how to use a JWT authentication token with curl to make authenticated requests to the Crawl4AI API. The token is passed in the Authorization header using the Bearer scheme.

LANGUAGE: bash
CODE:
curl -H "Authorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGci..." http://localhost:8000/crawl

----------------------------------------

TITLE: Managing Browser Profiles with BrowserProfiler
DESCRIPTION: Comprehensive example showing how to create, list, and delete browser profiles using the BrowserProfiler class. This utility helps manage persistent browser profiles for identity-based browsing.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import BrowserProfiler

async def manage_profiles():
    # Create a profiler instance
    profiler = BrowserProfiler()
    
    # Create a profile interactively - opens a browser window
    profile_path = await profiler.create_profile(
        profile_name="my-login-profile"  # Optional: name your profile
    )
    
    print(f"Profile saved at: {profile_path}")
    
    # List all available profiles
    profiles = profiler.list_profiles()
    
    for profile in profiles:
        print(f"Profile: {profile['name']}")
        print(f"  Path: {profile['path']}")
        print(f"  Created: {profile['created']}")
        print(f"  Browser type: {profile['type']}")
    
    # Get a specific profile path by name
    specific_profile = profiler.get_profile_path("my-login-profile")
    
    # Delete a profile when no longer needed
    success = profiler.delete_profile("old-profile-name")
    
asyncio.run(manage_profiles())

----------------------------------------

TITLE: Configuration Options for CosineStrategy in Python
DESCRIPTION: Shows the complete set of parameters available when initializing the CosineStrategy, including content filtering, clustering parameters, and model configuration options.

LANGUAGE: python
CODE:
CosineStrategy(
    # Content Filtering
    semantic_filter: str = None,       # Keywords/topic for content filtering
    word_count_threshold: int = 10,    # Minimum words per cluster
    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)
    
    # Clustering Parameters
    max_dist: float = 0.2,            # Maximum distance for clustering
    linkage_method: str = 'ward',      # Clustering linkage method
    top_k: int = 3,                   # Number of top categories to extract
    
    # Model Configuration
    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model
    
    verbose: bool = False             # Enable logging
)

----------------------------------------

TITLE: Installing Crawl4AI with PyTorch Support
DESCRIPTION: Command for installing Crawl4AI with PyTorch dependencies, enabling advanced text clustering with CosineSimilarity cluster strategy.

LANGUAGE: bash
CODE:
pip install crawl4ai[torch]

----------------------------------------

TITLE: Setting up LLM Provider Configuration for crawl4ai
DESCRIPTION: Example of creating an LLMConfig object to specify which language model provider to use and how to authenticate with it. This configuration can be passed to various extraction strategies and filters throughout the crawler.

LANGUAGE: python
CODE:
llm_config = LLMConfig(provider="openai/gpt-4o-mini", api_token=os.getenv("OPENAI_API_KEY"))

----------------------------------------

TITLE: Development Docker Build with All Features
DESCRIPTION: Command to build a Crawl4AI Docker image with all features enabled for development purposes, including specific Python version and GPU support.

LANGUAGE: bash
CODE:
docker build -t crawl4ai
  --build-arg INSTALL_TYPE=all \
  --build-arg PYTHON_VERSION=3.10 \
  --build-arg ENABLE_GPU=true \
  .

----------------------------------------

TITLE: Accessing Browser Profiles with Legacy Methods
DESCRIPTION: Shows how to use the legacy methods on ManagedBrowser that delegate to the new BrowserProfiler class for backward compatibility. This example demonstrates listing available browser profiles.

LANGUAGE: python
CODE:
from crawl4ai.browser_manager import ManagedBrowser

# These methods still work but use BrowserProfiler internally
profiles = ManagedBrowser.list_profiles()

----------------------------------------

TITLE: Installing Crawl4AI with All Features
DESCRIPTION: Command for installing Crawl4AI with all available features and dependencies, including PyTorch, transformers, and other optional components.

LANGUAGE: bash
CODE:
pip install crawl4ai[all]

----------------------------------------

TITLE: Command-Line Interface Usage Examples for Crawl4AI
DESCRIPTION: Examples of using the new Crawl4AI CLI (crwl) for various tasks including basic crawling, generating markdown output, using configuration files, LLM-based extraction, and asking questions about crawled content.

LANGUAGE: bash
CODE:
# Basic crawl
crwl https://example.com

# Get markdown output
crwl https://example.com -o markdown

# Use a configuration file
crwl https://example.com -B browser.yml -C crawler.yml

# Use LLM-based extraction
crwl https://example.com -e extract.yml -s schema.json

# Ask a question about the crawled content
crwl https://example.com -q "What is the main topic?"

# See usage examples
crwl --example

----------------------------------------

TITLE: Server Configuration YAML for Crawl4AI
DESCRIPTION: YAML configuration file for Crawl4AI server settings including application details, rate limiting, security, crawler parameters, logging, and observability options.

LANGUAGE: yaml
CODE:
# Application Configuration
app:
  title: "Crawl4AI API"           # Server title in OpenAPI docs
  version: "1.0.0"               # API version
  host: "0.0.0.0"               # Listen on all interfaces
  port: 8000                    # Server port
  reload: True                  # Enable hot reloading (development only)
  timeout_keep_alive: 300       # Keep-alive timeout in seconds

# Rate Limiting Configuration
rate_limiting:
  enabled: True                 # Enable/disable rate limiting
  default_limit: "100/minute"   # Rate limit format: "number/timeunit"
  trusted_proxies: []          # List of trusted proxy IPs
  storage_uri: "memory://"     # Use "redis://localhost:6379" for production

# Security Configuration
security:
  enabled: false               # Master toggle for security features
  jwt_enabled: true            # Enable JWT authentication
  https_redirect: True         # Force HTTPS
  trusted_hosts: ["*"]         # Allowed hosts (use specific domains in production)
  headers:                     # Security headers
    x_content_type_options: "nosniff"
    x_frame_options: "DENY"
    content_security_policy: "default-src 'self'"
    strict_transport_security: "max-age=63072000; includeSubDomains"

# Crawler Configuration
crawler:
  memory_threshold_percent: 95.0  # Memory usage threshold
  rate_limiter:
    base_delay: [1.0, 2.0]      # Min and max delay between requests
  timeouts:
    stream_init: 30.0           # Stream initialization timeout
    batch_process: 300.0        # Batch processing timeout

# Logging Configuration
logging:
  level: "INFO"                 # Log level (DEBUG, INFO, WARNING, ERROR)
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# Observability Configuration
observability:
  prometheus:
    enabled: True              # Enable Prometheus metrics
    endpoint: "/metrics"       # Metrics endpoint
  health_check:
    endpoint: "/health"        # Health check endpoint

----------------------------------------

TITLE: Content Filtering Configurations in YAML
DESCRIPTION: Defines two types of content filtering configurations: BM25-based filtering with a query and threshold, and pruning-based filtering with its own query and threshold.

LANGUAGE: yaml
CODE:
# filter_bm25.yml
type: "bm25"
query: "target content"
threshold: 1.0

# filter_pruning.yml
type: "pruning"
query: "focus topic"
threshold: 0.48

----------------------------------------

TITLE: Installing Crawl4AI for Development
DESCRIPTION: Commands for cloning the Crawl4AI repository and installing it in development mode with all dependencies, suitable for contributors who plan to modify the source code.

LANGUAGE: bash
CODE:
git clone https://github.com/unclecode/crawl4ai.git
cd crawl4ai
pip install -e ".[all]"
playwright install # Install Playwright dependencies

----------------------------------------

TITLE: Runtime Docker Configuration for Crawl4AI
DESCRIPTION: Bash command for running the Crawl4AI Docker container with a custom configuration file mounted at runtime. Uses volume mounting to replace the default configuration with a custom one without rebuilding the image.

LANGUAGE: bash
CODE:
# Mount custom config at runtime
docker run -d -p 8000:8000 \
  -v $(pwd)/custom-config.yml:/app/config.yml \
  crawl4ai-server:prod

----------------------------------------

TITLE: Installing Synchronous Version of Crawl4AI
DESCRIPTION: Command to install the deprecated synchronous version of Crawl4AI that uses Selenium instead of Playwright.

LANGUAGE: bash
CODE:
pip install crawl4ai[sync]

----------------------------------------

TITLE: Verifying Crawl4AI Installation with a Sample Python Script
DESCRIPTION: Python script to verify that Crawl4AI installation is working correctly by crawling a sample website and printing the extracted content. Uses AsyncWebCrawler for asynchronous crawling.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(url="https://www.example.com")
        print(result.markdown[:500])  # Print first 500 characters

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Browser Configuration for Crawl4AI in YAML Format
DESCRIPTION: Defines browser settings in YAML format, including headless mode, viewport dimensions, user agent settings, and error handling options.

LANGUAGE: yaml
CODE:
# browser.yml
headless: true
viewport_width: 1280
user_agent_mode: "random"
verbose: true
ignore_https_errors: true

----------------------------------------

TITLE: Defining the SSLCertificate class structure in Python
DESCRIPTION: Class definition showing the main methods and properties of the SSLCertificate class used to represent and manipulate SSL certificates in various formats.

LANGUAGE: python
CODE:
class SSLCertificate:
    """
    Represents an SSL certificate with methods to export in various formats.

    Main Methods:
    - from_url(url, timeout=10)
    - from_file(file_path)
    - from_binary(binary_data)
    - to_json(filepath=None)
    - to_pem(filepath=None)
    - to_der(filepath=None)
    ...

    Common Properties:
    - issuer
    - subject
    - valid_from
    - valid_until
    - fingerprint
    """

----------------------------------------

TITLE: Complete Content Filtering Example
DESCRIPTION: Demonstrates combined usage of content selection with CSS selector and various filtering options for a comprehensive crawl configuration.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

async def main():
    config = CrawlerRunConfig(
        css_selector="main.content", 
        word_count_threshold=10,
        excluded_tags=["nav", "footer"],
        exclude_external_links=True,
        exclude_social_media_links=True,
        exclude_domains=["ads.com", "spammytrackers.net"],
        exclude_external_images=True,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://news.ycombinator.com", config=config)
        print("Cleaned HTML length:", len(result.cleaned_html))

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Authenticated Crawling with Storage State in Crawl4AI
DESCRIPTION: Shows how to use the storage_state parameter to load authentication data from a JSON file for authenticated crawling sessions. This feature simplifies handling of logged-in sessions without needing to repeatedly authenticate.

LANGUAGE: python
CODE:
result = await crawler.arun(
    url="https://example.com/protected",
    storage_state="my_storage_state.json"
)

----------------------------------------

TITLE: Implementing Agentic Crawler in Python
DESCRIPTION: Example of using the autonomous Agentic Crawler that can understand complex goals and automatically plan and execute multi-step crawling operations. It demonstrates features like automatic planning, custom steps execution, and execution monitoring.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler
from crawl4ai.agents import CrawlerAgent

async with AsyncWebCrawler() as crawler:
    agent = CrawlerAgent(crawler)
    
    # Automatic planning and execution
    result = await agent.arun(
        goal="Find research papers about quantum computing published in 2023 with more than 50 citations",
        auto_retry=True
    )
    print("Generated Plan:", result.executed_steps)
    print("Extracted Data:", result.data)
    
    # Using custom steps with automatic execution
    result = await agent.arun(
        goal="Extract conference deadlines from ML conferences",
        custom_plan=[
            "Navigate to conference page",
            "Find important dates section",
            "Extract submission deadlines",
            "Verify dates are for 2024"
        ]
    )
    
    # Monitoring execution
    print("Step Completion:", result.step_status)
    print("Execution Time:", result.execution_time)
    print("Success Rate:", result.success_rate)

----------------------------------------

TITLE: Loading SSL certificate from a URL manually
DESCRIPTION: Shows how to manually fetch an SSL certificate from a given URL using the from_url method and access its fingerprint property.

LANGUAGE: python
CODE:
cert = SSLCertificate.from_url("https://example.com")
if cert:
    print("Fingerprint:", cert.fingerprint)

----------------------------------------

TITLE: Default Social Media Domains Excluded
DESCRIPTION: List of social media domains that are excluded by default when exclude_social_media_links is set to True.

LANGUAGE: python
CODE:
[
    'facebook.com',
    'twitter.com',
    'x.com',
    'linkedin.com',
    'instagram.com',
    'pinterest.com',
    'tiktok.com',
    'snapchat.com',
    'reddit.com',
]

----------------------------------------

TITLE: Minimal BrowserConfig Usage Example in Python
DESCRIPTION: Shows a minimal example of using BrowserConfig with AsyncWebCrawler. The configuration uses Firefox in visible mode with text extraction enabled.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler, BrowserConfig

browser_conf = BrowserConfig(
    browser_type="firefox",
    headless=False,
    text_mode=True
)

async with AsyncWebCrawler(config=browser_conf) as crawler:
    result = await crawler.arun("https://example.com")
    print(result.markdown[:300])

----------------------------------------

TITLE: Loading SSL certificate from binary data
DESCRIPTION: Shows how to initialize an SSL certificate from raw binary data, useful when capturing certificates from sockets or other sources.

LANGUAGE: python
CODE:
cert = SSLCertificate.from_binary(raw_bytes)

----------------------------------------

TITLE: Working with Session and Response Header Data
DESCRIPTION: How to access session identification and HTTP response headers from a CrawlResult object, useful for tracking browser contexts and server information.

LANGUAGE: python
CODE:
# If you used session_id="login_session" in CrawlerRunConfig, see it here:
print("Session:", result.session_id)

LANGUAGE: python
CODE:
if result.response_headers:
    print("Server:", result.response_headers.get("Server", "Unknown"))

----------------------------------------

TITLE: Implementing Sliding Window Chunking in Python
DESCRIPTION: A Python class that creates overlapping chunks using a sliding window approach, enabling better context preservation between adjacent chunks.

LANGUAGE: python
CODE:
class SlidingWindowChunking:
    def __init__(self, window_size=100, step=50):
        self.window_size = window_size
        self.step = step

    def chunk(self, text):
        words = text.split()
        chunks = []
        for i in range(0, len(words) - self.window_size + 1, self.step):
            chunks.append(' '.join(words[i:i + self.window_size]))
        return chunks

# Example Usage
text = "This is a long text to demonstrate sliding window chunking."
chunker = SlidingWindowChunking(window_size=5, step=2)
print(chunker.chunk(text))

----------------------------------------

TITLE: Basic Crawl4AI CLI Usage Commands
DESCRIPTION: Demonstrates basic usage patterns for the Crawl4AI CLI, including simple crawling, output format selection, verbose JSON output with cache bypass, and accessing usage examples.

LANGUAGE: bash
CODE:
# Basic crawling
crwl https://example.com

# Get markdown output
crwl https://example.com -o markdown

# Verbose JSON output with cache bypass
crwl https://example.com -o json -v --bypass-cache

# See usage examples
crwl --example

----------------------------------------

TITLE: Using Single-call Approach for Multiple Content Loading Actions in Crawl4AI
DESCRIPTION: This code illustrates a more efficient approach for known interaction patterns, using a single arun() call with an advanced JavaScript snippet. The JS clicks multiple modules sequentially and waits between each click before returning control to Crawl4AI for extraction.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler, CacheMode

js_code = [
    # Example JS that clicks multiple modules:
    """
    (async () => {
      const modules = document.querySelectorAll('.module-item');
      for (let i = 0; i < modules.length; i++) {
        modules[i].scrollIntoView();
        modules[i].click();
        // Wait for each module's content to load, adjust 100ms as needed
        await new Promise(r => setTimeout(r, 100));
      }
    })();
    """
]

async with AsyncWebCrawler(headless=True, verbose=True) as crawler:
    result = await crawler.arun(
        url="https://example.com",
        js_code=js_code,
        wait_for="css:.final-loaded-content-class",
        cache_mode=CacheMode.BYPASS
    )

# `result` now contains all content after all modules have been clicked in one go.

----------------------------------------

TITLE: Processing Links from Crawl Results
DESCRIPTION: Example of iterating through internal links discovered during crawling, accessing link targets, text content, and contextual information.

LANGUAGE: python
CODE:
for link in result.links["internal"]:
    print(f"Internal link to {link['href']} with text {link['text']}")

----------------------------------------

TITLE: Processing Links from Crawl Results
DESCRIPTION: Example of iterating through internal links discovered during crawling, accessing link targets, text content, and contextual information.

LANGUAGE: python
CODE:
for link in result.links["internal"]:
    print(f"Internal link to {link['href']} with text {link['text']}")

----------------------------------------

TITLE: CSS/XPath-based Extraction Configuration in YAML
DESCRIPTION: Defines a CSS/XPath-based extraction configuration in YAML format, specifying the extraction type as JSON-CSS with verbose output.

LANGUAGE: yaml
CODE:
# extract_css.yml
type: "json-css"
params:
  verbose: true

----------------------------------------

TITLE: Running Crawl4AI Setup
DESCRIPTION: Command to run the setup process which installs required Playwright browsers, performs OS-level checks, and confirms the environment is ready for crawling.

LANGUAGE: bash
CODE:
crawl4ai-setup

----------------------------------------

TITLE: Deep Crawler Configuration for Crawl4AI
DESCRIPTION: JSON configuration for deep crawling with BFS strategy, content filters, domain filters, and URL scoring based on keywords and path depth.

LANGUAGE: json
CODE:
{
  "crawler_config": {
    "type": "CrawlerRunConfig",
    "params": {
      "deep_crawl_strategy": {
        "type": "BFSDeepCrawlStrategy",
        "params": {
          "max_depth": 3,
          "filter_chain": {
            "type": "FilterChain",
            "params": {
              "filters": [
                {
                  "type": "ContentTypeFilter",
                  "params": {
                    "allowed_types": ["text/html", "application/xhtml+xml"]
                  }
                },
                {
                  "type": "DomainFilter",
                  "params": {
                    "allowed_domains": ["blog.*", "docs.*"],
                  }
                }
              ]
            }
          },
          "url_scorer": {
            "type": "CompositeScorer",
            "params": {
              "scorers": [
                {
                  "type": "KeywordRelevanceScorer",
                  "params": {
                    "keywords": ["tutorial", "guide", "documentation"],
                  }
                },
                {
                  "type": "PathDepthScorer",
                  "params": {
                    "weight": 0.5,
                    "optimal_depth": 3  
                  }
                }
              ]
            }
          }
        }
      }
    }
  }
}

----------------------------------------

TITLE: Content Chunking Example in Python
DESCRIPTION: Example of using OverlappingWindowChunking with LLMExtractionStrategy to process long articles by breaking them into overlapping chunks. This facilitates LLM processing of content that exceeds token limits.

LANGUAGE: python
CODE:
from crawl4ai.chunking_strategy import OverlappingWindowChunking
from crawl4ai.types import LLMConfig

# Create chunking strategy
chunker = OverlappingWindowChunking(
    window_size=500,  # 500 words per chunk
    overlap=50        # 50 words overlap
)

# Use with extraction strategy
strategy = LLMExtractionStrategy(
    llm_config = LLMConfig(provider="ollama/llama2"),
    chunking_strategy=chunker
)

result = await crawler.arun(
    url="https://example.com/long-article",
    extraction_strategy=strategy
)

----------------------------------------

TITLE: Using LLM-Powered Schema Generation for Data Extraction
DESCRIPTION: Demonstrates how to use the new generate_schema method in JsonCssExtractionStrategy to automatically create extraction schemas using LLM. The example extracts product information from HTML using a Gemini model to generate the appropriate selectors based on a natural language query.

LANGUAGE: python
CODE:
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
from crawl4ai.types import LLMConfig

llm_config = LLMConfig(provider="gemini/gemini-1.5-pro", api_token="env:GEMINI_API_KEY")

schema = JsonCssExtractionStrategy.generate_schema(
    html="<div class='product'><h2>Product Name</h2><span class='price'>$99</span></div>",
    llm_config = llm_config,
    query="Extract product name and price"
)
print(schema)

LANGUAGE: json
CODE:
{
  "name": "ProductExtractor",
  "baseSelector": "div.product",
  "fields": [
      {"name": "name", "selector": "h2", "type": "text"},
      {"name": "price", "selector": ".price", "type": "text"}
    ]
 }

----------------------------------------

TITLE: Pre-Fetching Models for Crawl4AI
DESCRIPTION: Optional command to cache large models locally if required by the workflow.

LANGUAGE: bash
CODE:
crawl4ai-download-models

----------------------------------------

TITLE: Build-time Docker Configuration for Crawl4AI
DESCRIPTION: Bash command for building a Docker image with a custom configuration path passed as a build argument. Uses the --build-arg flag to specify the configuration file path during the build process.

LANGUAGE: bash
CODE:
# Build with custom config
docker build --platform=linux/amd64 --no-cache \
  --build-arg CONFIG_PATH=/path/to/custom-config.yml \ 
  -t crawl4ai:latest .

----------------------------------------

TITLE: Proxy Configuration Example in JSON
DESCRIPTION: Demonstrates the format for proxy configuration in BrowserConfig. The configuration includes server URL, username, and password for proxy authentication.

LANGUAGE: json
CODE:
{
    "server": "http://proxy.example.com:8080", 
    "username": "...", 
    "password": "..."
}

----------------------------------------

TITLE: Executing JavaScript and Extracting Structured Data without LLMs in Python
DESCRIPTION: This example shows how to use crawl4ai to execute JavaScript on a webpage and extract structured data using CSS selectors. The code defines a JSON schema for extraction, executes JavaScript to interact with tabs, and processes the results without requiring LLMs.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
import json

async def main():
    schema = {
    "name": "KidoCode Courses",
    "baseSelector": "section.charge-methodology .w-tab-content > div",
    "fields": [
        {
            "name": "section_title",
            "selector": "h3.heading-50",
            "type": "text",
        },
        {
            "name": "section_description",
            "selector": ".charge-content",
            "type": "text",
        },
        {
            "name": "course_name",
            "selector": ".text-block-93",
            "type": "text",
        },
        {
            "name": "course_description",
            "selector": ".course-content-text",
            "type": "text",
        },
        {
            "name": "course_icon",
            "selector": ".image-92",
            "type": "attribute",
            "attribute": "src"
        }
    ]
}

    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)

    browser_config = BrowserConfig(
        headless=False,
        verbose=True
    )
    run_config = CrawlerRunConfig(
        extraction_strategy=extraction_strategy,
        js_code=["""(async () => {const tabs = document.querySelectorAll("section.charge-methodology .tabs-menu-3 > div");for(let tab of tabs) {tab.scrollIntoView();tab.click();await new Promise(r => setTimeout(r, 500));}})();"""],
        cache_mode=CacheMode.BYPASS
    )
        
    async with AsyncWebCrawler(config=browser_config) as crawler:
        
        result = await crawler.arun(
            url="https://www.kidocode.com/degrees/technology",
            config=run_config
        )

        companies = json.loads(result.extracted_content)
        print(f"Successfully extracted {len(companies)} companies")
        print(json.dumps(companies[0], indent=2))


if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Basic Link Extraction with AsyncWebCrawler in Python
DESCRIPTION: Example demonstrating how to extract internal and external links from a crawled page using AsyncWebCrawler. This snippet shows how to access the links.internal and links.external fields from the crawl result and print their count and details.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler

async with AsyncWebCrawler() as crawler:
    result = await crawler.arun("https://www.example.com")
    if result.success:
        internal_links = result.links.get("internal", [])
        external_links = result.links.get("external", [])
        print(f"Found {len(internal_links)} internal links.")
        print(f"Found {len(internal_links)} external links.")
        print(f"Found {len(result.media)} media items.")

        # Each link is typically a dictionary with fields like:
        # { "href": "...", "text": "...", "title": "...", "base_domain": "..." }
        if internal_links:
            print("Sample Internal Link:", internal_links[0])
    else:
        print("Crawl failed:", result.error_message)

----------------------------------------

TITLE: Running Crawl4AI Docker Container
DESCRIPTION: Commands for running the Crawl4AI container in different configurations, including basic run, with LLM support, and with host environment variables.

LANGUAGE: bash
CODE:
docker run -d -p 8000:8000 --name crawl4ai crawl4ai

----------------------------------------

TITLE: Cloning CrawlerRunConfig in Python
DESCRIPTION: Example of using the clone() method to create variations of a CrawlerRunConfig. The example shows creating a base configuration and then streaming and debug configurations with modified parameters.

LANGUAGE: python
CODE:
# Create a base configuration
base_config = CrawlerRunConfig(
    cache_mode=CacheMode.ENABLED,
    word_count_threshold=200,
    wait_until="networkidle"
)

# Create variations for different use cases
stream_config = base_config.clone(
    stream=True,  # Enable streaming mode
    cache_mode=CacheMode.BYPASS
)

debug_config = base_config.clone(
    page_timeout=120000,  # Longer timeout for debugging
    verbose=True
)

----------------------------------------

TITLE: Initializing CosineStrategy in Python
DESCRIPTION: Configuration for the CosineStrategy class used for content similarity-based extraction and clustering. This strategy supports semantic filtering, word count thresholds, and configurable clustering parameters.

LANGUAGE: python
CODE:
CosineStrategy(
    # Content Filtering
    semantic_filter: str = None,        # Topic/keyword filter
    word_count_threshold: int = 10,     # Minimum words per cluster
    sim_threshold: float = 0.3,         # Similarity threshold
    
    # Clustering Parameters
    max_dist: float = 0.2,             # Maximum cluster distance
    linkage_method: str = 'ward',       # Clustering method
    top_k: int = 3,                    # Top clusters to return
    
    # Model Configuration
    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model
    
    verbose: bool = False              # Enable verbose logging
)

----------------------------------------

TITLE: Accessing Media Items from Crawl Results
DESCRIPTION: Example showing how to access and process images from crawl results. This snippet demonstrates how to retrieve images from the media dictionary in the result object and display information about each image.

LANGUAGE: python
CODE:
if result.success:
    images_info = result.media.get("images", [])
    print(f"Found {len(images_info)} images in total.")
    for i, img in enumerate(images_info[:5]):  # Inspect just the first 5
        print(f"[Image {i}] URL: {img['src']}")
        print(f"           Alt text: {img.get('alt', '')}")
        print(f"           Score: {img.get('score')}")
        print(f"           Description: {img.get('desc', '')}\n")

----------------------------------------

TITLE: Speed Comparison Between Crawl4AI and Firecrawl
DESCRIPTION: Conducts a performance comparison between Crawl4AI and Firecrawl by measuring execution time for crawling the same URL. The test includes both simple crawling and crawling with JavaScript execution.

LANGUAGE: python
CODE:
import os
from google.colab import userdata
os.environ['FIRECRAWL_API_KEY'] = userdata.get('FIRECRAWL_API_KEY')
import time
from firecrawl import FirecrawlApp

async def speed_comparison():
    # Simulated Firecrawl performance
    app = FirecrawlApp(api_key=os.environ['FIRECRAWL_API_KEY'])
    start = time.time()
    scrape_status = app.scrape_url(
    'https://www.nbcnews.com/business',
    params={'formats': ['markdown', 'html']}
    )
    end = time.time()
    print("Firecrawl (simulated):")
    print(f"Time taken: {end - start:.2f} seconds")
    print(f"Content length: {len(scrape_status['markdown'])} characters")
    print(f"Images found: {scrape_status['markdown'].count('cldnry.s-nbcnews.com')}")
    print()

    async with AsyncWebCrawler() as crawler:
        # Crawl4AI simple crawl
        start = time.time()
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
            word_count_threshold=0,
            bypass_cache=True,
            verbose=False
        )
        end = time.time()
        print("Crawl4AI (simple crawl):")
        print(f"Time taken: {end - start:.2f} seconds")
        print(f"Content length: {len(result.markdown)} characters")
        print(f"Images found: {result.markdown.count('cldnry.s-nbcnews.com')}")
        print()

        # Crawl4AI with JavaScript execution
        start = time.time()
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
            js_code=["const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More')); loadMoreButton && loadMoreButton.click();"],
            word_count_threshold=0,
            bypass_cache=True,
            verbose=False
        )
        end = time.time()
        print("Crawl4AI (with JavaScript execution):")
        print(f"Time taken: {end - start:.2f} seconds")
        print(f"Content length: {len(result.markdown)} characters")
        print(f"Images found: {result.markdown.count('cldnry.s-nbcnews.com')}")

await speed_comparison()

----------------------------------------

TITLE: Advanced Crawl4AI CLI Usage Example
DESCRIPTION: Shows an advanced usage example that crawls a webpage and extracts content according to a CSS-based schema, outputting the results in JSON format.

LANGUAGE: bash
CODE:
crwl "https://www.infoq.com/ai-ml-data-eng/" -e docs/examples/cli/extract_css.yml -s docs/examples/cli/css_schema.json -o json;

----------------------------------------

TITLE: Handling Iframe Content Configuration
DESCRIPTION: Shows how to configure CrawlerRunConfig to process iframe content and integrate it into the final output, useful for sites that embed content.

LANGUAGE: python
CODE:
config = CrawlerRunConfig(
    # Merge iframe content into the final output
    process_iframes=True,    
    remove_overlay_elements=True
)

----------------------------------------

TITLE: Media Structure Example in Crawl4AI Result Object
DESCRIPTION: Example data structure showing how media items are stored in the CrawlResult object. This snippet illustrates the format of images, videos, and audio entries, including properties like src, alt, desc, score, and dimensions.

LANGUAGE: python
CODE:
result.media = {
  "images": [
    {
      "src": "https://cdn.prod.website-files.com/.../Group%2089.svg",
      "alt": "coding school for kids",
      "desc": "Trial Class Degrees degrees All Degrees AI Degree Technology ...",
      "score": 3,
      "type": "image",
      "group_id": 0,
      "format": None,
      "width": None,
      "height": None
    },
    # ...
  ],
  "videos": [
    # Similar structure but with video-specific fields
  ],
  "audio": [
    # Similar structure but with audio-specific fields
  ]
}

----------------------------------------

TITLE: Implementing Regex-Based Chunking in Python
DESCRIPTION: A Python class that splits text into chunks using regular expression patterns. It defaults to paragraph separation but allows custom patterns to be defined.

LANGUAGE: python
CODE:
class RegexChunking:
    def __init__(self, patterns=None):
        self.patterns = patterns or [r'\n\n']  # Default pattern for paragraphs

    def chunk(self, text):
        paragraphs = [text]
        for pattern in self.patterns:
            paragraphs = [seg for p in paragraphs for seg in re.split(pattern, p)]
        return paragraphs

# Example Usage
text = """This is the first paragraph.

This is the second paragraph."""
chunker = RegexChunking()
print(chunker.chunk(text))

----------------------------------------

TITLE: LLM Schema Definition for Structured Data Extraction
DESCRIPTION: Defines a JSON schema for LLM-based extraction, specifying the structure of objects to be extracted with their properties and descriptions.

LANGUAGE: json
CODE:
// llm_schema.json
{
  "title": "Article",
  "type": "object",
  "properties": {
    "title": {
      "type": "string",
      "description": "The title of the article"
    },
    "link": {
      "type": "string",
      "description": "URL to the full article"
    }
  }
}

----------------------------------------

TITLE: Combining Multiple Filters in FilterChain for Web Crawling in Python
DESCRIPTION: This code shows how to combine multiple filters (URL pattern, domain, and content type) into a filter chain for precise targeting of web content. It demonstrates a sophisticated approach to limiting crawl scope.

LANGUAGE: python
CODE:
from crawl4ai.deep_crawling.filters import (
    FilterChain,
    URLPatternFilter,
    DomainFilter,
    ContentTypeFilter
)

# Create a chain of filters
filter_chain = FilterChain([
    # Only follow URLs with specific patterns
    URLPatternFilter(patterns=["*guide*", "*tutorial*"]),
    
    # Only crawl specific domains
    DomainFilter(
        allowed_domains=["docs.example.com"],
        blocked_domains=["old.docs.example.com"]
    ),
    
    # Only include specific content types
    ContentTypeFilter(allowed_types=["text/html"])
])

config = CrawlerRunConfig(
    deep_crawl_strategy=BFSDeepCrawlStrategy(
        max_depth=2,
        filter_chain=filter_chain
    )
)

----------------------------------------

TITLE: Extracting Structured Data with OpenAI and Crawl4AI
DESCRIPTION: Implements LLMExtractionStrategy using OpenAI's models to extract structured data from web pages. This example extracts OpenAI model pricing information and formats it according to a Pydantic schema.

LANGUAGE: python
CODE:
import os
from google.colab import userdata
os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')

class OpenAIModelFee(BaseModel):
    model_name: str = Field(..., description="Name of the OpenAI model.")
    input_fee: str = Field(..., description="Fee for input token for the OpenAI model.")
    output_fee: str = Field(..., description="Fee for output token for the OpenAI model.")

async def extract_openai_fees():
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url='https://openai.com/api/pricing/',
            word_count_threshold=1,
            extraction_strategy=LLMExtractionStrategy(
                provider="openai/gpt-4o", api_token=os.getenv('OPENAI_API_KEY'),
                schema=OpenAIModelFee.schema(),
                extraction_type="schema",
                instruction="""From the crawled content, extract all mentioned model names along with their fees for input and output tokens.
                Do not miss any models in the entire content. One extracted model JSON format should look like this:
                {\"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\"}."""
            ),
            bypass_cache=True,
        )
        print(len(result.extracted_content))

# Uncomment the following line to run the OpenAI extraction example
await extract_openai_fees()

----------------------------------------

TITLE: Implementing Sentence-Based Chunking with NLTK
DESCRIPTION: A Python class that uses NLTK's sentence tokenizer to divide text into individual sentences, which is ideal for processing content statement by statement.

LANGUAGE: python
CODE:
from nltk.tokenize import sent_tokenize

class NlpSentenceChunking:
    def chunk(self, text):
        sentences = sent_tokenize(text)
        return [sentence.strip() for sentence in sentences]

# Example Usage
text = "This is sentence one. This is sentence two."
chunker = NlpSentenceChunking()
print(chunker.chunk(text))

----------------------------------------

TITLE: Content Filtering Command with Crawl4AI CLI
DESCRIPTION: Shows how to apply content filtering to crawled data by specifying a filter configuration file and outputting the results in markdown-fit format.

LANGUAGE: bash
CODE:
crwl https://example.com -f filter_bm25.yml -o markdown-fit

----------------------------------------

TITLE: ReadMe Documentation Updates (Diff)
DESCRIPTION: Documentation updates to include usage and explanation for the new PruningContentFilter feature in the README file.

LANGUAGE: diff
CODE:
Updated to include usage and explanation for the PruningContentFilter.

----------------------------------------

TITLE: Building Docker Image for Crawl4AI
DESCRIPTION: Commands to clone the repository and build the Docker image for Crawl4AI with platform-specific options for both AMD64 and ARM64 architectures.

LANGUAGE: bash
CODE:
# Clone the repository
git clone https://github.com/unclecode/crawl4ai.git
cd crawl4ai/deploy

# Build the Docker image
docker build --platform=linux/amd64 --no-cache -t crawl4ai .

# Or build for arm64
docker build --platform=linux/arm64 --no-cache -t crawl4ai .

----------------------------------------

TITLE: Content Filtering & Exclusions Configuration
DESCRIPTION: Shows basic configuration options for filtering content based on thresholds, excluding tags, filtering links, and handling media to refine crawl results.

LANGUAGE: python
CODE:
config = CrawlerRunConfig(
    # Content thresholds
    word_count_threshold=10,        # Minimum words per block

    # Tag exclusions
    excluded_tags=['form', 'header', 'footer', 'nav'],

    # Link filtering
    exclude_external_links=True,    
    exclude_social_media_links=True,
    # Block entire domains
    exclude_domains=["adtrackers.com", "spammynews.org"],    
    exclude_social_media_domains=["facebook.com", "twitter.com"],

    # Media filtering
    exclude_external_images=True
)

----------------------------------------

TITLE: Using Score Threshold for Quality Filtering in crawl4ai
DESCRIPTION: Sets up a score threshold to only crawl high-quality pages when using BFS and DFS strategies. This example uses a KeywordRelevanceScorer to evaluate URLs and only follows links with scores above the specified threshold.

LANGUAGE: python
CODE:
# Only follow links with scores above 0.4
strategy = DFSDeepCrawlStrategy(
    max_depth=2,
    url_scorer=KeywordRelevanceScorer(keywords=["api", "guide", "reference"]),
    score_threshold=0.4  # Skip URLs with scores below this value
)

----------------------------------------

TITLE: Excluding Specific Domains in Crawl4AI
DESCRIPTION: Example showing how to exclude specific domains from crawling. This snippet demonstrates setting the exclude_domains parameter in CrawlerRunConfig to block certain sites while still allowing other external links.

LANGUAGE: python
CODE:
crawler_cfg = CrawlerRunConfig(
    exclude_domains=["suspiciousads.com"]
)

----------------------------------------

TITLE: Running Crawl4AI with Host Environment Variables
DESCRIPTION: Command to run Crawl4AI Docker container using the host environment variables, which is not recommended for production but useful for local testing.

LANGUAGE: bash
CODE:
docker run -d -p 8000:8000 \
  --env-file .llm.env \
  --env "$(env)" \
  --name crawl4ai \
  crawl4ai

----------------------------------------

TITLE: CSS-Based Selection with AsyncWebCrawler
DESCRIPTION: Demonstrates how to limit crawl results to a specific region of a webpage using the css_selector parameter in CrawlerRunConfig.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async def main():
    config = CrawlerRunConfig(
        # e.g., first 30 items from Hacker News
        css_selector=".athing:nth-child(-n+30)"  
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news.ycombinator.com/newest", 
            config=config
        )
        print("Partial HTML length:", len(result.cleaned_html))

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Running Crawl4AI Diagnostics
DESCRIPTION: Command to run diagnostics that check Python version compatibility, verify Playwright installation, and inspect environment variables or library conflicts.

LANGUAGE: bash
CODE:
crawl4ai-doctor

----------------------------------------

TITLE: Using KeywordRelevanceScorer with BestFirstCrawlingStrategy in Python
DESCRIPTION: This code demonstrates how to use KeywordRelevanceScorer with BestFirstCrawlingStrategy to prioritize crawling pages based on keyword relevance. Results are returned in order of relevance score for more efficient crawling of important content.

LANGUAGE: python
CODE:
from crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer
from crawl4ai.deep_crawling import BestFirstCrawlingStrategy

# Create a keyword relevance scorer
keyword_scorer = KeywordRelevanceScorer(
    keywords=["crawl", "example", "async", "configuration"],
    weight=0.7  # Importance of this scorer (0.0 to 1.0)
)

config = CrawlerRunConfig(
    deep_crawl_strategy=BestFirstCrawlingStrategy(
        max_depth=2,
        url_scorer=keyword_scorer
    ),
    stream=True  # Recommended with BestFirstCrawling
)

# Results will come in order of relevance score
async with AsyncWebCrawler() as crawler:
    async for result in await crawler.arun("https://example.com", config=config):
        score = result.metadata.get("score", 0)
        print(f"Score: {score:.2f} | {result.url}")

----------------------------------------

TITLE: GPU-Enabled Docker Build
DESCRIPTION: Command to build a Crawl4AI Docker image with GPU acceleration support, targeting a specific Dockerfile location.

LANGUAGE: bash
CODE:
docker build -t crawl4ai
  --build-arg ENABLE_GPU=true \
  deploy/docker/

----------------------------------------

TITLE: Installing Crawl4AI and Dependencies
DESCRIPTION: Installs the Crawl4AI library along with nest_asyncio for asynchronous operations and sets up Playwright for browser automation in a Python environment.

LANGUAGE: python
CODE:
# %%capture
!pip install crawl4ai
!pip install nest_asyncio
!playwright install  

----------------------------------------

TITLE: Blog Post Extraction Schema with Base Fields
DESCRIPTION: A schema that extracts blog post data including the URL from each post card via baseFields, plus the title, date, summary, and author. This demonstrates how to extract attributes from parent elements along with nested content.

LANGUAGE: python
CODE:
schema = {
  "name": "Blog Posts",
  "baseSelector": "a.blog-post-card",
  "baseFields": [
    {"name": "post_url", "type": "attribute", "attribute": "href"}
  ],
  "fields": [
    {"name": "title", "selector": "h2.post-title", "type": "text", "default": "No Title"},
    {"name": "date", "selector": "time.post-date", "type": "text", "default": ""},
    {"name": "summary", "selector": "p.post-summary", "type": "text", "default": ""},
    {"name": "author", "selector": "span.post-author", "type": "text", "default": ""}
  ]
}

----------------------------------------

TITLE: CSS-based Extraction Strategy Configuration for Crawl4AI
DESCRIPTION: JSON configuration defining a CSS-based extraction strategy that parses HTML elements according to a specified schema with base selector and field mappings.

LANGUAGE: json
CODE:
{
    "crawler_config": {
        "type": "CrawlerRunConfig",
        "params": {
            "extraction_strategy": {
                "type": "JsonCssExtractionStrategy",
                "params": {
                    "schema": {
                        "baseSelector": "article.post",
                        "fields": [
                            {"name": "title", "selector": "h1", "type": "text"},
                            {"name": "content", "selector": ".content", "type": "html"}
                        ]
                    }
                }
            }
        }
    }
}

----------------------------------------

TITLE: Chunking Strategy Configuration JSON Example
DESCRIPTION: JSON example demonstrating the strategy pattern in Crawl4AI configurations, showing how to configure a RegexChunking strategy with specific patterns.

LANGUAGE: json
CODE:
{
    "crawler_config": {
        "type": "CrawlerRunConfig",
        "params": {
            "chunking_strategy": {
                "type": "RegexChunking",      // Strategy implementation
                "params": {
                    "patterns": ["\n\n", "\\.\\s+"]
                }
            }
        }
    }
}

----------------------------------------

TITLE: CSS Schema Definition for Structured Data Extraction
DESCRIPTION: Defines a JSON schema for CSS-based extraction, including the base selector and field definitions with their respective selectors, types, and attributes.

LANGUAGE: json
CODE:
// css_schema.json
{
  "name": "ArticleExtractor",
  "baseSelector": ".article",
  "fields": [
    {
      "name": "title",
      "selector": "h1.title",
      "type": "text"
    },
    {
      "name": "link",
      "selector": "a.read-more",
      "type": "attribute",
      "attribute": "href"
    }
  ]
}

----------------------------------------

TITLE: CSS Schema Definition for Structured Data Extraction
DESCRIPTION: Defines a JSON schema for CSS-based extraction, including the base selector and field definitions with their respective selectors, types, and attributes.

LANGUAGE: json
CODE:
// css_schema.json
{
  "name": "ArticleExtractor",
  "baseSelector": ".article",
  "fields": [
    {
      "name": "title",
      "selector": "h1.title",
      "type": "text"
    },
    {
      "name": "link",
      "selector": "a.read-more",
      "type": "attribute",
      "attribute": "href"
    }
  ]
}

----------------------------------------

TITLE: Automated Schema Generation with LLM
DESCRIPTION: Example of using the schema generation utility from Crawl4AI to automatically generate extraction schemas using LLM. This code demonstrates how to use OpenAI's GPT-4 to create a schema from sample HTML with product information.

LANGUAGE: python
CODE:
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy, JsonXPathExtractionStrategy
from crawl4ai.types import LLMConfig

# Sample HTML with product information
html = """
<div class="product-card">
    <h2 class="title">Gaming Laptop</h2>
    <div class="price">$999.99</div>
    <div class="specs">
        <ul>
            <li>16GB RAM</li>
            <li>1TB SSD</li>
        </ul>
    </div>
</div>
"""

# Option 1: Using OpenAI (requires API token)
css_schema = JsonCssExtractionStrategy.generate_schema(
    html,
    schema_type="css", 
    llm_config = LLMConfig(provider="openai/gpt-4o",api_token="your-openai-token")
)

----------------------------------------

TITLE: Handling Dynamic Content with JavaScript Interaction in Crawl4AI
DESCRIPTION: This example shows how to interact with dynamic web content by clicking tab elements and extracting structured data using CSS selectors. It utilizes custom JavaScript execution to trigger UI interactions and wait for content to load before extraction.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def extract_structured_data_using_css_extractor():
    print("\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---")
    schema = {
        "name": "KidoCode Courses",
        "baseSelector": "section.charge-methodology .w-tab-content > div",
        "fields": [
            {
                "name": "section_title",
                "selector": "h3.heading-50",
                "type": "text",
            },
            {
                "name": "section_description",
                "selector": ".charge-content",
                "type": "text",
            },
            {
                "name": "course_name",
                "selector": ".text-block-93",
                "type": "text",
            },
            {
                "name": "course_description",
                "selector": ".course-content-text",
                "type": "text",
            },
            {
                "name": "course_icon",
                "selector": ".image-92",
                "type": "attribute",
                "attribute": "src",
            },
        ],
    }

    browser_config = BrowserConfig(headless=True, java_script_enabled=True)

    js_click_tabs = """
    (async () => {
        const tabs = document.querySelectorAll("section.charge-methodology .tabs-menu-3 > div");
        for(let tab of tabs) {
            tab.scrollIntoView();
            tab.click();
            await new Promise(r => setTimeout(r, 500));
        }
    })();
    """

    crawler_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        extraction_strategy=JsonCssExtractionStrategy(schema),
        js_code=[js_click_tabs],
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://www.kidocode.com/degrees/technology", config=crawler_config
        )

        companies = json.loads(result.extracted_content)
        print(f"Successfully extracted {len(companies)} companies")
        print(json.dumps(companies[0], indent=2))

async def main():
    await extract_structured_data_using_css_extractor()

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Creating Configuration Objects in Python for Crawl4AI
DESCRIPTION: Example showing how to create Crawl4AI configuration objects in Python and convert them to JSON for API calls using the dump() method.

LANGUAGE: python
CODE:
from crawl4ai import CrawlerRunConfig, PruningContentFilter

config = CrawlerRunConfig(
    content_filter=PruningContentFilter(threshold=0.48)
)
print(config.dump())  # Use this JSON in your API calls

----------------------------------------

TITLE: Extracting HTML Element Attributes with JSON Schema
DESCRIPTION: A JSON schema example showing how to extract attributes (like href, src, or data-xxx) from HTML elements. This approach can be used within the baseFields or nested field structures.

LANGUAGE: json
CODE:
{
  "name": "href",
  "type": "attribute",
  "attribute": "href",
  "default": null
}

----------------------------------------

TITLE: Capturing Full-Page Screenshots and PDFs using Crawl4AI in Python
DESCRIPTION: This Python script demonstrates how to use Crawl4AI to capture both PDF and screenshot versions of a large webpage. It navigates to a Wikipedia page, bypasses cache, and saves both the screenshot and PDF to local files. The example showcases the library's ability to handle extensive content reliably.

LANGUAGE: python
CODE:
import os, sys
import asyncio
from crawl4ai import AsyncWebCrawler, CacheMode

# Adjust paths as needed
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(parent_dir)
__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))

async def main():
    async with AsyncWebCrawler() as crawler:
        # Request both PDF and screenshot
        result = await crawler.arun(
            url='https://en.wikipedia.org/wiki/List_of_common_misconceptions',
            cache_mode=CacheMode.BYPASS,
            pdf=True,
            screenshot=True
        )
        
        if result.success:
            # Save screenshot
            if result.screenshot:
                from base64 import b64decode
                with open(os.path.join(__location__, "screenshot.png"), "wb") as f:
                    f.write(b64decode(result.screenshot))
            
            # Save PDF
            if result.pdf:
                pdf_bytes = b64decode(result.pdf)
                with open(os.path.join(__location__, "page.pdf"), "wb") as f:
                    f.write(pdf_bytes)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Accessing Internal Links from CrawlResult in Crawl4AI
DESCRIPTION: Shows how to access and print the first three internal links from the links dictionary in a CrawlResult object.

LANGUAGE: python
CODE:
print(result.links["internal"][:3])  # Show first 3 internal links

----------------------------------------

TITLE: Production Configuration Settings for Crawl4AI in YAML
DESCRIPTION: YAML configuration for Crawl4AI in a production environment. Settings include disabling hot reloading, setting appropriate timeouts, configuring Redis-based rate limiting, and enabling security features with domain restrictions.

LANGUAGE: yaml
CODE:
app:
  reload: False              # Disable reload in production
  timeout_keep_alive: 120    # Lower timeout for better resource management

rate_limiting:
  storage_uri: "redis://redis:6379"  # Use Redis for distributed rate limiting
  default_limit: "50/minute"         # More conservative rate limit

security:
  enabled: true                      # Enable all security features
  trusted_hosts: ["your-domain.com"] # Restrict to your domain

----------------------------------------

TITLE: Listing Dependencies for crawl4ai Project
DESCRIPTION: This requirements file specifies the Python packages needed for the crawl4ai project. It includes web frameworks (FastAPI, Uvicorn, Gunicorn), rate limiting functionality (slowapi), monitoring tools (Prometheus), caching with Redis, JWT for authentication, and validation libraries.

LANGUAGE: plaintext
CODE:
crawl4ai
fastapi
uvicorn
gunicorn>=23.0.0
slowapi>=0.1.9
prometheus-fastapi-instrumentator>=7.0.2
redis>=5.2.1
jwt>=1.3.1
dnspython>=2.7.0
email-validator>=2.2.0

----------------------------------------

TITLE: First Run: Performing Login and Saving State in Python
DESCRIPTION: Complete example showing how to perform a login once via a hook, then export the resulting session state to a file for later reuse. This script navigates to a login page, authenticates, and saves the resulting storage state.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CacheMode
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

async def on_browser_created_hook(browser):
    # Access the default context and create a page
    context = browser.contexts[0]
    page = await context.new_page()
    
    # Navigate to the login page
    await page.goto("https://example.com/login", wait_until="domcontentloaded")
    
    # Fill in credentials and submit
    await page.fill("input[name='username']", "myuser")
    await page.fill("input[name='password']", "mypassword")
    await page.click("button[type='submit']")
    await page.wait_for_load_state("networkidle")
    
    # Now the site sets tokens in localStorage and cookies
    # Export this state to a file so we can reuse it
    await context.storage_state(path="my_storage_state.json")
    await page.close()

async def main():
    # First run: perform login and export the storage_state
    async with AsyncWebCrawler(
        headless=True,
        verbose=True,
        hooks={"on_browser_created": on_browser_created_hook},
        use_persistent_context=True,
        user_data_dir="./my_user_data"
    ) as crawler:
        
        # After on_browser_created_hook runs, we have storage_state saved to my_storage_state.json
        result = await crawler.arun(
            url='https://example.com/protected-page',
            cache_mode=CacheMode.BYPASS,
            markdown_generator=DefaultMarkdownGenerator(options={"ignore_links": True}),
        )
        print("First run result success:", result.success)
        if result.success:
            print("Protected page HTML length:", len(result.html))

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Error Handling with CosineStrategy and AsyncWebCrawler
DESCRIPTION: Demonstrates comprehensive error handling when using CosineStrategy with AsyncWebCrawler, including checking for success, handling empty results, and catching exceptions.

LANGUAGE: python
CODE:
try:
    result = await crawler.arun(
        url="https://example.com",
        extraction_strategy=strategy
    )
    
    if result.success:
        content = json.loads(result.extracted_content)
        if not content:
            print("No relevant content found")
    else:
        print(f"Extraction failed: {result.error_message}")
        
except Exception as e:
    print(f"Error during extraction: {str(e)}")

----------------------------------------

TITLE: Storage State Structure Example in JSON
DESCRIPTION: Example JSON structure showing how to format a storage_state containing cookies and localStorage entries for use with Crawl4ai. This structure includes a session cookie and two localStorage tokens.

LANGUAGE: json
CODE:
{
  "cookies": [
    {
      "name": "session",
      "value": "abcd1234",
      "domain": "example.com",
      "path": "/",
      "expires": 1675363572.037711,
      "httpOnly": false,
      "secure": false,
      "sameSite": "None"
    }
  ],
  "origins": [
    {
      "origin": "https://example.com",
      "localStorage": [
        { "name": "token", "value": "my_auth_token" },
        { "name": "refreshToken", "value": "my_refresh_token" }
      ]
    }
  ]
}

----------------------------------------

TITLE: Installing Crawl4AI Basic Package with Playwright Dependencies
DESCRIPTION: Basic installation command for Crawl4AI using pip, including the Playwright dependencies required for web crawling functionality.

LANGUAGE: bash
CODE:
pip install crawl4ai
playwright install # Install Playwright dependencies

----------------------------------------

TITLE: Bypassing Anti-Bot Detection with Magic Mode
DESCRIPTION: Shows how to use Magic Mode to bypass anti-bot detection systems on protected websites, making crawling more reliable.

LANGUAGE: python
CODE:
async def magic_mode_demo():
    async with AsyncWebCrawler() as crawler:  # Enables anti-bot detection bypass
        result = await crawler.arun(
            url="https://www.reuters.com/markets/us/global-markets-view-usa-pix-2024-08-29/",
            magic=True  # Enables magic mode
        )
        print(result.markdown)  # Shows the full content in Markdown format

# Run the demo
await magic_mode_demo()

----------------------------------------

TITLE: Installing Crawl4AI and Dependencies
DESCRIPTION: Installs the Crawl4AI library, nest_asyncio for running async code in Jupyter, and Playwright for browser automation.

LANGUAGE: python
CODE:
!pip install crawl4ai
!pip install nest_asyncio
!playwright install

----------------------------------------

TITLE: Setting Up Asynchronous Environment
DESCRIPTION: Imports asyncio and nest_asyncio, then applies nest_asyncio to allow asynchronous code execution in environments that already have an event loop running, such as Jupyter notebooks.

LANGUAGE: python
CODE:
import asyncio
import nest_asyncio
nest_asyncio.apply()

----------------------------------------

TITLE: Implementing Question-Based Crawler in Python
DESCRIPTION: Example of using the Question-Based Crawler to extract web content based on natural language questions. The code demonstrates how to use SerpiAPI integration with relevancy scoring to automatically discover and extract relevant web content.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler
from crawl4ai.discovery import QuestionBasedDiscovery

async with AsyncWebCrawler() as crawler:
    discovery = QuestionBasedDiscovery(crawler)
    results = await discovery.arun(
        question="What are the system requirements for major cloud providers' GPU instances?",
        max_urls=5,
        relevance_threshold=0.7
    )
    
    for result in results:
        print(f"Source: {result.url} (Relevance: {result.relevance_score})")
        print(f"Content: {result.markdown}\n")

----------------------------------------

TITLE: Using Multiple Browser Types with Multi-Browser Support
DESCRIPTION: Demonstrates how to use different browser engines (Chromium, Firefox, WebKit) for web crawling, showing Firefox as an example.

LANGUAGE: python
CODE:
async def multi_browser_demo():
    async with AsyncWebCrawler(browser_type="firefox") as crawler:  # Using Firefox instead of default Chromium
        result = await crawler.arun(url="https://crawl4i.com")
        print(result.markdown)  # Shows content extracted using Firefox

# Run the demo
await multi_browser_demo()

----------------------------------------

TITLE: Implementing Heuristic Markdown Generation with Content Filtering in Python
DESCRIPTION: This example demonstrates how to use crawl4ai to generate markdown content from a webpage with content filtering strategies like PruningContentFilter and BM25ContentFilter. The code configures a browser instance, sets up caching, and applies content filtering thresholds.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

async def main():
    browser_config = BrowserConfig(
        headless=True,  
        verbose=True,
    )
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.ENABLED,
        markdown_generator=DefaultMarkdownGenerator(
            content_filter=PruningContentFilter(threshold=0.48, threshold_type="fixed", min_word_threshold=0)
        ),
        # markdown_generator=DefaultMarkdownGenerator(
        #     content_filter=BM25ContentFilter(user_query="WHEN_WE_FOCUS_BASED_ON_A_USER_QUERY", bm25_threshold=1.0)
        # ),
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://docs.micronaut.io/4.7.6/guide/",
            config=run_config
        )
        print(len(result.markdown.raw_markdown))
        print(len(result.markdown.fit_markdown))

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Technical Documentation Extraction with CosineStrategy
DESCRIPTION: Example of configuring CosineStrategy for extracting technical specifications and documentation, with stricter matching parameters for accurate content identification.

LANGUAGE: python
CODE:
strategy = CosineStrategy(
    semantic_filter="technical specifications documentation",
    word_count_threshold=30,
    sim_threshold=0.6,        # Stricter matching for technical content
    max_dist=0.3             # Allow related technical sections
)

----------------------------------------

TITLE: Combining Features for Advanced Web Content Extraction
DESCRIPTION: Demonstrates a comprehensive example that combines all features: Fit Markdown, Magic Mode, Multi-browser support, and Knowledge Graph Extraction with LLM integration.

LANGUAGE: python
CODE:
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel
import json, os
from typing import List

# Define classes for the knowledge graph structure
class Landmark(BaseModel):
    name: str
    description: str
    activities: list[str]  # E.g., visiting, sightseeing, relaxing

class City(BaseModel):
    name: str
    description: str
    landmarks: list[Landmark]
    cultural_highlights: list[str]  # E.g., food, music, traditional crafts

class TravelKnowledgeGraph(BaseModel):
    cities: list[City]  # Central Mexican cities to visit

async def combined_demo():
    # Define the knowledge graph extraction strategy
    strategy = LLMExtractionStrategy(
        # provider="ollama/nemotron",
        provider='openai/gpt-4o-mini', # Or any other provider, including Ollama and open source models
        pi_token=os.getenv('OPENAI_API_KEY'), # In case of Ollama just pass "no-token"
        schema=TravelKnowledgeGraph.schema(),
        instruction=(
            "Extract cities, landmarks, and cultural highlights for places to visit in Central Mexico. "
            "For each city, list main landmarks with descriptions and activities, as well as cultural highlights."
        )
    )

    # Set up the AsyncWebCrawler with multi-browser support, Magic Mode, and Fit Markdown
    async with AsyncWebCrawler(browser_type="firefox") as crawler:
        result = await crawler.arun(
            url="https://janineintheworld.com/places-to-visit-in-central-mexico",
            extraction_strategy=strategy,
            bypass_cache=True,
            magic=True
        )
        
        # Display main article content in Fit Markdown format
        print("Extracted Main Content:\n", result.fit_markdown)
        
        # Display extracted knowledge graph of cities, landmarks, and cultural highlights
        if result.extracted_content:
            travel_graph = json.loads(result.extracted_content)
            print("\nExtracted Knowledge Graph:\n", json.dumps(travel_graph, indent=2))

# Run the combined demo
await combined_demo()

----------------------------------------

TITLE: Running Crawl4AI with LLM Support
DESCRIPTION: Command to run Crawl4AI Docker container with LLM support by providing environment variables from a .llm.env file.

LANGUAGE: bash
CODE:
docker run -d -p 8000:8000 \
  --env-file .llm.env \
  --name crawl4ai \
  crawl4ai

----------------------------------------

TITLE: Generating XPath Schema with Ollama in Crawl4AI
DESCRIPTION: This code demonstrates how to generate an XPath extraction schema using Ollama as the LLM provider, which is an open-source alternative that doesn't require an API token. The generated schema can then be used for repeated extractions.

LANGUAGE: python
CODE:
xpath_schema = JsonXPathExtractionStrategy.generate_schema(
    html,
    schema_type="xpath",
    llm_config = LLMConfig(provider="ollama/llama3.3", api_token=None)  # Not needed for Ollama
)

# Use the generated schema for fast, repeated extractions
strategy = JsonCssExtractionStrategy(css_schema)

----------------------------------------

TITLE: Understanding Crawl4AI Configuration Structure in Python
DESCRIPTION: Python code snippet showing how to view the JSON structure of a Crawl4AI configuration object to understand the type-params pattern.

LANGUAGE: python
CODE:
from crawl4ai import BrowserConfig

# Create a config and see its structure
config = BrowserConfig(headless=True)
print(config.dump())

----------------------------------------

TITLE: Handling SSL Certificates in Crawl4AI
DESCRIPTION: Demonstrates how to fetch, verify, and export SSL certificates during crawling. The example retrieves the certificate from example.com and exports it in multiple formats (JSON, PEM, DER). This functionality is useful for compliance, debugging, or security analysis.

LANGUAGE: python
CODE:
import asyncio, os
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

async def main():
    tmp_dir = os.path.join(os.getcwd(), "tmp")
    os.makedirs(tmp_dir, exist_ok=True)
    
    config = CrawlerRunConfig(
        fetch_ssl_certificate=True,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://example.com", config=config)
        
        if result.success and result.ssl_certificate:
            cert = result.ssl_certificate
            print("\nCertificate Information:")
            print(f"Issuer (CN): {cert.issuer.get('CN', '')}")
            print(f"Valid until: {cert.valid_until}")
            print(f"Fingerprint: {cert.fingerprint}")

            # Export in multiple formats:
            cert.to_json(os.path.join(tmp_dir, "certificate.json"))
            cert.to_pem(os.path.join(tmp_dir, "certificate.pem"))
            cert.to_der(os.path.join(tmp_dir, "certificate.der"))
            
            print("\nCertificate exported to JSON/PEM/DER in 'tmp' folder.")
        else:
            print("[ERROR] No certificate or crawl failed.")

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Exporting SSL certificate in DER format
DESCRIPTION: Demonstrates exporting the SSL certificate in DER (binary ASN.1) format, either as bytes or saved to a file.

LANGUAGE: python
CODE:
der_bytes = cert.to_der()
cert.to_der("certificate.der")

----------------------------------------

TITLE: Installing Crawl4AI and Dependencies
DESCRIPTION: Installs Crawl4AI, Playwright, and nest_asyncio packages required for web crawling in a notebook environment.

LANGUAGE: python
CODE:
# Install Crawl4AI and dependencies
!pip install crawl4ai
!playwright install
!pip install nest_asyncio

----------------------------------------

TITLE: Setting API Keys for AI Services in Python
DESCRIPTION: Defines API key variables for multiple AI service providers including Groq, OpenAI, and Anthropic. These environment variables would be used to authenticate API requests to these services in the crawl4ai project.

LANGUAGE: python
CODE:
GROQ_API_KEY = "YOUR_GROQ_API"
OPENAI_API_KEY = "YOUR_OPENAI_API"
ANTHROPIC_API_KEY = "YOUR_ANTHROPIC_API"
# You can add more API keys here

----------------------------------------

TITLE: Implementing Robots.txt Compliance in Crawl4AI
DESCRIPTION: Shows how to enable robots.txt checking in Crawl4AI to ensure ethical crawling. The example configures the crawler to check and respect robots.txt rules before accessing a website, with built-in caching for efficiency and appropriate handling of disallowed URLs.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async def main():
    # Enable robots.txt checking in config
    config = CrawlerRunConfig(
        check_robots_txt=True  # Will check and respect robots.txt rules
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            "https://example.com",
            config=config
        )
        
        if not result.success and result.status_code == 403:
            print("Access denied by robots.txt")

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Error Handling in Extraction Processes in Python
DESCRIPTION: Example of implementing error handling when using extraction strategies with the crawler. This pattern checks for successful extraction and handles exceptions appropriately.

LANGUAGE: python
CODE:
try:
    result = await crawler.arun(
        url="https://example.com",
        extraction_strategy=strategy
    )
    if result.success:
        content = json.loads(result.extracted_content)
except Exception as e:
    print(f"Extraction failed: {e}")

----------------------------------------

TITLE: Setting up Asyncio Environment for Notebooks
DESCRIPTION: Imports and applies nest_asyncio to allow asyncio operations in notebook environments like Colab.

LANGUAGE: python
CODE:
# Import nest_asyncio and apply it to allow asyncio in Colab
import nest_asyncio
nest_asyncio.apply()

print('Setup complete!')

----------------------------------------

TITLE: Using arun() with CrawlerRunConfig in Python
DESCRIPTION: Demonstrates how to use the arun() method with a CrawlerRunConfig object to specify detailed crawling parameters such as cache mode, CSS selector, word count threshold, and screenshot options. This is the recommended modern approach.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import CrawlerRunConfig, CacheMode

run_cfg = CrawlerRunConfig(
    cache_mode=CacheMode.BYPASS,
    css_selector="main.article",
    word_count_threshold=10,
    screenshot=True
)

async with AsyncWebCrawler(config=browser_cfg) as crawler:
    result = await crawler.arun("https://example.com/news", config=run_cfg)
    print("Crawled HTML length:", len(result.cleaned_html))
    if result.screenshot:
        print("Screenshot base64 length:", len(result.screenshot))

----------------------------------------

TITLE: Enabling Image Lazy Loading in Crawl4AI
DESCRIPTION: Configuration to ensure the crawler waits for all images to fully load before proceeding, addressing the common issue of missing lazy-loaded content on modern websites.

LANGUAGE: python
CODE:
await crawler.crawl(
    url="https://example.com",
    wait_for_images=True  # Add this argument to ensure images are fully loaded
)

----------------------------------------

TITLE: Exporting SSL certificate in PEM format
DESCRIPTION: Shows how to export an SSL certificate in PEM format, either as a string or saved to a file.

LANGUAGE: python
CODE:
pem_str = cert.to_pem()              # in-memory PEM string
cert.to_pem("/path/to/cert.pem")     # saved to file

----------------------------------------

TITLE: Implementing Page Closing in Try-Finally Block to Prevent Memory Leaks in Python
DESCRIPTION: This code snippet adds a finally block to ensure that browser pages are properly closed when no session_id is provided, preventing memory leaks caused by lingering pages after a crawl operation completes.

LANGUAGE: python
CODE:
finally:
    # If no session_id is given we should close the page
    if not config.session_id:
        await page.close()

----------------------------------------

TITLE: Setting Up LLM Environment Variables
DESCRIPTION: Creates a .llm.env file with API keys for various language model providers to enable LLM support in Crawl4AI.

LANGUAGE: env
CODE:
# OpenAI
OPENAI_API_KEY=sk-your-key

# Anthropic
ANTHROPIC_API_KEY=your-anthropic-key

# DeepSeek
DEEPSEEK_API_KEY=your-deepseek-key

# Check out https://docs.litellm.ai/docs/providers for more providers!

----------------------------------------

TITLE: Dynamically Adjusting Viewport Size to Content
DESCRIPTION: Configuration to automatically resize the viewport based on page content dimensions, ensuring all content is properly loaded, especially for responsive layouts or conditional content loading.

LANGUAGE: python
CODE:
await crawler.crawl(
    url="https://example.com",
    adjust_viewport_to_content=True  # Dynamically adjusts the viewport
)

----------------------------------------

TITLE: Quick Test for Crawl4AI API
DESCRIPTION: Python code to test a Crawl4AI Docker deployment by submitting a crawl job to the API and polling for results.

LANGUAGE: python
CODE:
import requests

# Submit a crawl job
response = requests.post(
    "http://localhost:11235/crawl",
    json={"urls": "https://example.com", "priority": 10}
)
task_id = response.json()["task_id"]

# Continue polling until the task is complete (status="completed")
result = requests.get(f"http://localhost:11235/task/{task_id}")

----------------------------------------

TITLE: Using BM25ContentFilter for Relevance-Based Content Extraction in Python
DESCRIPTION: This snippet demonstrates how to use the new BM25ContentFilter strategy to extract content from web pages based on relevance to a user query. The AsyncWebCrawler utilizes this filter to identify chunks of text that are relevant to specified search terms.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler
from crawl4ai.content_filter_strategy import BM25ContentFilter

async def filter_content(url, query):
    async with AsyncWebCrawler() as crawler:
        content_filter = BM25ContentFilter(user_query=query)
        result = await crawler.arun(url=url, extraction_strategy=content_filter, fit_markdown=True)
        print(result.extracted_content)  # Or result.fit_markdown for the markdown version
        print(result.fit_html) # Or result.fit_html to show HTML with only the filtered content

asyncio.run(filter_content("https://en.wikipedia.org/wiki/Apple", "fruit nutrition health"))

----------------------------------------

TITLE: Importing Required Libraries for Crawl4AI
DESCRIPTION: Imports necessary modules including AsyncWebCrawler, extraction strategies, and utility libraries for working with Crawl4AI.

LANGUAGE: python
CODE:
import asyncio
import nest_asyncio
from crawl4ai import AsyncWebCrawler
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy
import json
import time
from pydantic import BaseModel, Field

nest_asyncio.apply()

----------------------------------------

TITLE: Configuring Browser and Crawler Settings in Crawl4AI
DESCRIPTION: Demonstrates how to use the new BrowserConfig and CrawlerRunConfig objects to configure browser settings and crawling behavior. This approach provides better control and cleaner code structure compared to passing multiple arguments directly to the arun method.

LANGUAGE: python
CODE:
from crawl4ai import BrowserConfig, CrawlerRunConfig, AsyncWebCrawler

browser_config = BrowserConfig(headless=True, viewport_width=1920, viewport_height=1080)
crawler_config = CrawlerRunConfig(cache_mode="BYPASS")

async with AsyncWebCrawler(config=browser_config) as crawler:
    result = await crawler.arun(url="https://example.com", config=crawler_config)
    print(result.markdown[:500])

----------------------------------------

TITLE: Health Check Request Example for Crawl4AI
DESCRIPTION: Bash command for checking the health status of the Crawl4AI server via the health endpoint.

LANGUAGE: bash
CODE:
curl http://localhost:8000/health

----------------------------------------

TITLE: Crawling Local Files and Raw HTML Content in Python
DESCRIPTION: This example shows how to use the AsyncWebCrawler to process local HTML files and raw HTML strings directly. The crawler supports 'file://' prefix for local files and 'raw:' prefix for HTML strings, enabling content extraction without web requests.

LANGUAGE: python
CODE:
async def crawl_local_or_raw(crawler, content, content_type):
    prefix = "file://" if content_type == "local" else "raw:"
    url = f"{prefix}{content}"
    result = await crawler.arun(url=url)
    if result.success:
        print(f"Markdown Content from {content_type.title()} Source:")
        print(result.markdown)

# Example usage with local file and raw HTML
async def main():
    async with AsyncWebCrawler() as crawler:
        # Local File
        await crawl_local_or_raw(
            crawler, os.path.abspath('tests/async/sample_wikipedia.html'), "local"
        )
        # Raw HTML
        await crawl_raw_html(crawler, "<h1>Raw Test</h1><p>This is raw HTML.</p>")
        

asyncio.run(main())

----------------------------------------

TITLE: BM25ContentFilter Test Enhancement (Diff)
DESCRIPTION: Extended testing for the BM25ContentFilter to cover additional edge cases and performance metrics, including malformed HTML handling.

LANGUAGE: diff
CODE:
Added tests for new extraction scenarios including malformed HTML.

----------------------------------------

TITLE: Implementing File Download Processing in AsyncWebCrawler (Python)
DESCRIPTION: Example code showing how to use the file download functionality in the AsyncWebCrawler. This feature allows specifying download folders and tracking downloaded files in the CrawlResult object.

LANGUAGE: python
CODE:
import asyncio
import os
from pathlib import Path
from crawl4ai import AsyncWebCrawler

async def download_example():
    downloads_path = os.path.join(Path.home(), ".crawl4ai", "downloads")
    os.makedirs(downloads_path, exist_ok=True)

    async with AsyncWebCrawler(
        accept_downloads=True, 
        downloads_path=downloads_path, 
        verbose=True
    ) as crawler:
        result = await crawler.arun(
            url="https://www.python.org/downloads/",
            js_code="""
                const downloadLink = document.querySelector('a[href$=".exe"]');
                if (downloadLink) { downloadLink.click(); }
            """,
            wait_for=5 # To ensure download has started
        )

        if result.downloaded_files:
            print("Downloaded files:")
            for file in result.downloaded_files:
                print(f"- {file}")

asyncio.run(download_example())

----------------------------------------

TITLE: Using Managed Browsers for Identity-Based Crawling
DESCRIPTION: Complete example showing how to use managed browsers in Crawl4AI with a persistent user data directory. This allows crawling as an authenticated user with all session data and preferences preserved.

LANGUAGE: python
CODE:
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def main():
    # 1) Reference your persistent data directory
    browser_config = BrowserConfig(
        headless=True,             # 'True' for automated runs
        verbose=True,
        use_managed_browser=True,  # Enables persistent browser strategy
        browser_type="chromium",
        user_data_dir="/path/to/my-chrome-profile"
    )

    # 2) Standard crawl config
    crawl_config = CrawlerRunConfig(
        wait_for="css:.logged-in-content"
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url="https://example.com/private", config=crawl_config)
        if result.success:
            print("Successfully accessed private data with your identity!")
        else:
            print("Error:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: Managing Browser Sessions with AsyncWebCrawler in Python
DESCRIPTION: This snippet demonstrates how to use the new browser management features in AsyncWebCrawler. It shows how to create persistent browser sessions between requests using the session_id parameter and maintain browser state across multiple crawl operations.

LANGUAGE: python
CODE:
async def browser_management_demo():
    user_data_dir = os.path.join(Path.home(), ".crawl4ai", "user-data-dir")
    os.makedirs(user_data_dir, exist_ok=True)  # Ensure directory exists
    async with AsyncWebCrawler(
        use_managed_browser=True,
        user_data_dir=user_data_dir,
        use_persistent_context=True,
        verbose=True
    ) as crawler:
        result1 = await crawler.arun(
            url="https://example.com", session_id="my_session"
        )
        result2 = await crawler.arun(
            url="https://example.com/anotherpage", session_id="my_session"
        )

asyncio.run(browser_management_demo())

----------------------------------------

TITLE: Creating Mermaid Diagram of Crawl4AI Strategic Roadmap
DESCRIPTION: A mermaid diagram visualizing the four main strategic areas of Crawl4AI: Advanced Crawling Systems, Specialized Features, Development Tools, and Community & Growth. The diagram shows completed and planned features in each area.

LANGUAGE: mermaid
CODE:
%%{init: {'themeVariables': { 'fontSize': '14px'}}}%%
graph TD
    subgraph A1[Advanced Crawling Systems ðŸ”§]
        A["`
        â€¢ Graph Crawler âœ“
        â€¢ Question-Based Crawler
        â€¢ Knowledge-Optimal Crawler
        â€¢ Agentic Crawler
        `"]
    end

    subgraph A2[Specialized Features ðŸ› ï¸]
        B["`
        â€¢ Automated Schema Generator
        â€¢ Domain-Specific Scrapers
        â€¢ 
        â€¢ 
        `"]
    end

    subgraph A3[Development Tools ðŸ”¨]
        C["`
        â€¢ Interactive Playground
        â€¢ Performance Monitor
        â€¢ Cloud Integration
        â€¢ 
        `"]
    end

    subgraph A4[Community & Growth ðŸŒ±]
        D["`
        â€¢ Sponsorship Program
        â€¢ Educational Content
        â€¢ 
        â€¢ 
        `"]
    end

    classDef default fill:#f9f9f9,stroke:#333,stroke-width:2px
    classDef section fill:#f0f0f0,stroke:#333,stroke-width:4px,rx:10
    class A1,A2,A3,A4 section

    %% Layout hints
    A1 --> A2[" "]
    A3 --> A4[" "]
    linkStyle 0,1 stroke:none

----------------------------------------

TITLE: PruningContentFilter implementation (Diff)
DESCRIPTION: Code changes showing the introduction of PruningContentFilter, a new content filtering strategy that removes less relevant nodes based on metrics like text and link density.

LANGUAGE: diff
CODE:
Implemented effective pruning algorithm with comprehensive scoring.

----------------------------------------

TITLE: Complex Browser Configuration JSON Example
DESCRIPTION: Example JSON structure showing how simple and complex values are handled in the Crawl4AI configuration system, including nested viewport settings.

LANGUAGE: json
CODE:
{
    "browser_config": {
        "type": "BrowserConfig",
        "params": {
            "headless": true,           // Simple boolean - direct value
            "viewport": {               // Complex dictionary - needs type-params
                "type": "dict",
                "value": {
                    "width": 1200,
                    "height": 800
                }
            }
        }
    }
}

----------------------------------------

TITLE: Installing Crawl4AI package and setting up dependencies
DESCRIPTION: Instructions for installing the Crawl4AI package using pip, setting up the environment, and verifying the installation. Also includes an alternative method for manually installing browser dependencies if needed.

LANGUAGE: bash
CODE:
# Install the package
pip install -U crawl4ai

# For pre release versions
pip install crawl4ai --pre

# Run post-installation setup
crawl4ai-setup

# Verify your installation
crawl4ai-doctor

LANGUAGE: bash
CODE:
python -m playwright install --with-deps chromium

----------------------------------------

TITLE: Text Citation Format for Crawl4AI
DESCRIPTION: Plain text citation format for referencing Crawl4AI in documentation or non-academic contexts, following a standard author-date citation style.

LANGUAGE: text
CODE:
UncleCode. (2024). Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper [Computer software]. 
GitHub. https://github.com/unclecode/crawl4ai

----------------------------------------

TITLE: PruningContentFilter Test Implementation (Diff)
DESCRIPTION: Addition of comprehensive test cases to ensure correct functionality of the PruningContentFilter across various scenarios.

LANGUAGE: diff
CODE:
Created test cases for various scenarios using the PruningContentFilter.

----------------------------------------

TITLE: Pre-build Docker Configuration for Crawl4AI
DESCRIPTION: Bash commands for customizing Crawl4AI configuration before building the Docker image. Involves copying and modifying the configuration file, then building a Docker image with the platform specified as linux/amd64.

LANGUAGE: bash
CODE:
# Copy and modify config before building
cd crawl4ai/deploy
vim custom-config.yml # Or use any editor

# Build with custom config
docker build --platform=linux/amd64 --no-cache -t crawl4ai:latest .

----------------------------------------

TITLE: Manual Playwright Installation Methods
DESCRIPTION: Two alternative methods to manually install Playwright if the automatic setup doesn't work properly.

LANGUAGE: bash
CODE:
playwright install

LANGUAGE: bash
CODE:
python -m playwright install chromium

----------------------------------------

TITLE: Text Attribution for Crawl4AI
DESCRIPTION: A simple text attribution line that can be added to documentation for projects using Crawl4AI, providing proper credit and reference to the original repository.

LANGUAGE: text
CODE:
This project uses Crawl4AI (https://github.com/unclecode/crawl4ai) for web data extraction.

----------------------------------------

TITLE: Development Installation of Crawl4AI
DESCRIPTION: Commands for contributors to install Crawl4AI in development mode from the source code, with options for different feature sets.

LANGUAGE: bash
CODE:
git clone https://github.com/unclecode/crawl4ai.git
cd crawl4ai
pip install -e .                    # Basic installation in editable mode

LANGUAGE: bash
CODE:
pip install -e ".[torch]"           # With PyTorch features
pip install -e ".[transformer]"     # With Transformer features
pip install -e ".[cosine]"          # With cosine similarity features
pip install -e ".[sync]"            # With synchronous crawling (Selenium)
pip install -e ".[all]"             # Install all optional features

----------------------------------------

TITLE: Finding Playwright Chromium Binary Location
DESCRIPTION: Commands to locate the Playwright Chromium binary on your system. This helps identify where Playwright keeps the Chromium browser installation that can be used for creating persistent profiles.

LANGUAGE: bash
CODE:
python -m playwright install --dry-run

LANGUAGE: bash
CODE:
playwright install --dry-run

----------------------------------------

TITLE: Extracting Structured Data with LLMs in Python
DESCRIPTION: This example demonstrates how to use crawl4ai with LLMs to extract structured data from webpages. It uses a Pydantic model to define the extraction schema, configures the LLM provider (with support for OpenAI or alternatives like Ollama), and provides custom extraction instructions.

LANGUAGE: python
CODE:
import os
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field

class OpenAIModelFee(BaseModel):
    model_name: str = Field(..., description="Name of the OpenAI model.")
    input_fee: str = Field(..., description="Fee for input token for the OpenAI model.")
    output_fee: str = Field(..., description="Fee for output token for the OpenAI model.")

async def main():
    browser_config = BrowserConfig(verbose=True)
    run_config = CrawlerRunConfig(
        word_count_threshold=1,
        extraction_strategy=LLMExtractionStrategy(
            # Here you can use any provider that Litellm library supports, for instance: ollama/qwen2
            # provider="ollama/qwen2", api_token="no-token", 
            llm_config = LLMConfig(provider="openai/gpt-4o", api_token=os.getenv('OPENAI_API_KEY')), 
            schema=OpenAIModelFee.schema(),
            extraction_type="schema",
            instruction="""From the crawled content, extract all mentioned model names along with their fees for input and output tokens. 
            Do not miss any models in the entire content. One extracted model JSON format should look like this: 
            {"model_name": "GPT-4", "input_fee": "US$10.00 / 1M tokens", "output_fee": "US$30.00 / 1M tokens"}."""
        ),            
        cache_mode=CacheMode.BYPASS,
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url='https://openai.com/api/pricing/',
            config=run_config
        )
        print(result.extracted_content)

if __name__ == "__main__":
    asyncio.run(main())

----------------------------------------

TITLE: BibTeX Citation Format for Crawl4AI
DESCRIPTION: BibTeX citation format for referencing Crawl4AI in academic papers or research projects. Includes author, title, publication details, and URL.

LANGUAGE: bibtex
CODE:
@software{crawl4ai2024,
  author = {UncleCode},
  title = {Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub Repository},
  howpublished = {\url{https://github.com/unclecode/crawl4ai}},
  commit = {Please use the commit hash you're working with}
}

----------------------------------------

TITLE: Implementing Automated Schema Generator in Python
DESCRIPTION: Example of using the Automated Schema Generator to create JsonCssExtractionStrategy schemas from natural language descriptions. This tool makes structured data extraction accessible by automatically generating extraction schemas from simple text descriptions.

LANGUAGE: python
CODE:
from crawl4ai import AsyncWebCrawler
from crawl4ai.schema import SchemaGenerator

# Generate schema from natural language description
generator = SchemaGenerator()
schema = await generator.generate(
    url="https://news-website.com",
    description="For each news article on the page, I need the headline, publication date, and main image"
)

# Use generated schema with crawler
async with AsyncWebCrawler() as crawler:
    result = await crawler.arun(
        url="https://news-website.com",
        extraction_strategy=schema
    )

# Example of generated schema:
"""
{
    "name": "News Article Extractor",
    "baseSelector": "article.news-item",
    "fields": [
        {
            "name": "headline",
            "selector": "h2.article-title",
            "type": "text"
        },
        {
            "name": "date",
            "selector": "span.publish-date",
            "type": "text"
        },
        {
            "name": "image",
            "selector": "img.article-image",
            "type": "attribute",
            "attribute": "src"
        }
    ]
}
"""